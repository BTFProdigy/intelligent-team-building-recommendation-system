Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1?11,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
On Dual Decomposition and Linear Programming Relaxations
for Natural Language Processing
Alexander M. Rush David Sontag Michael Collins Tommi Jaakkola
MIT CSAIL, Cambridge, MA 02139, USA
{srush,dsontag,mcollins,tommi}@csail.mit.edu
Abstract
This paper introduces dual decomposition as a
framework for deriving inference algorithms
for NLP problems. The approach relies on
standard dynamic-programming algorithms as
oracle solvers for sub-problems, together with
a simple method for forcing agreement be-
tween the different oracles. The approach
provably solves a linear programming (LP) re-
laxation of the global inference problem. It
leads to algorithms that are simple, in that they
use existing decoding algorithms; efficient, in
that they avoid exact algorithms for the full
model; and often exact, in that empirically
they often recover the correct solution in spite
of using an LP relaxation. We give experimen-
tal results on two problems: 1) the combina-
tion of two lexicalized parsing models; and
2) the combination of a lexicalized parsing
model and a trigram part-of-speech tagger.
1 Introduction
Dynamic programming algorithms have been re-
markably useful for inference in many NLP prob-
lems. Unfortunately, as models become more com-
plex, for example through the addition of new fea-
tures or components, dynamic programming algo-
rithms can quickly explode in terms of computa-
tional or implementational complexity.1 As a re-
sult, efficiency of inference is a critical bottleneck
for many problems in statistical NLP.
This paper introduces dual decomposition
(Dantzig and Wolfe, 1960; Komodakis et al, 2007)
as a framework for deriving inference algorithms in
NLP. Dual decomposition leverages the observation
that complex inference problems can often be
decomposed into efficiently solvable sub-problems.
The approach leads to inference algorithms with the
following properties:
1The same is true for NLP inference algorithms based on
other exact combinatorial methods, for example methods based
on minimum-weight spanning trees (McDonald et al, 2005), or
graph cuts (Pang and Lee, 2004).
? The resulting algorithms are simple and efficient,
building on standard dynamic-programming algo-
rithms as oracle solvers for sub-problems,2 to-
gether with a method for forcing agreement be-
tween the oracles.
? The algorithms provably solve a linear program-
ming (LP) relaxation of the original inference
problem.
? Empirically, the LP relaxation often leads to an
exact solution to the original problem.
The approach is very general, and should be appli-
cable to a wide range of problems in NLP. The con-
nection to linear programming ensures that the algo-
rithms provide a certificate of optimality when they
recover the exact solution, and also opens up the
possibility of methods that incrementally tighten the
LP relaxation until it is exact (Sherali and Adams,
1994; Sontag et al, 2008).
The structure of this paper is as follows. We
first give two examples as an illustration of the ap-
proach: 1) integrated parsing and trigram part-of-
speech (POS) tagging; and 2) combined phrase-
structure and dependency parsing. In both settings,
it is possible to solve the integrated problem through
an ?intersected? dynamic program (e.g., for integra-
tion of parsing and tagging, the construction from
Bar-Hillel et al (1964) can be used). However,
these methods, although polynomial time, are sub-
stantially less efficient than our algorithms, and are
considerably more complex to implement.
Next, we describe exact polyhedral formula-
tions for the two problems, building on connec-
tions between dynamic programming algorithms
and marginal polytopes, as described in Martin et al
(1990). These allow us to precisely characterize the
relationship between the exact formulations and the
2More generally, other exact inference methods can be
used as oracles, for example spanning tree algorithms for non-
projective dependency structures.
1
LP relaxations that we solve. We then give guaran-
tees of convergence for our algorithms by showing
that they are instantiations of Lagrangian relaxation,
a general method for solving linear programs of a
particular form.
Finally, we describe experiments that demonstrate
the effectiveness of our approach. First, we con-
sider the integration of the generative model for
phrase-structure parsing of Collins (2003), with the
second-order discriminative dependency parser of
Koo et al (2008). This is an interesting problem
in its own right: the goal is to inject the high per-
formance of discriminative dependency models into
phrase-structure parsing. The method uses off-the-
shelf decoders for the two models. We find three
main results: 1) in spite of solving an LP relax-
ation, empirically the method finds an exact solution
on over 99% of the examples; 2) the method con-
verges quickly, typically requiring fewer than 10 it-
erations of decoding; 3) the method gives gains over
a baseline method that forces the phrase-structure
parser to produce the same dependencies as the first-
best output from the dependency parser (the Collins
(2003) model has an F1 score of 88.1%; the base-
line method has an F1 score of 89.7%; and the dual
decomposition method has an F1 score of 90.7%).
In a second set of experiments, we use dual de-
composition to integrate the trigram POS tagger of
Toutanova and Manning (2000) with the parser of
Collins (2003). We again find that the method finds
an exact solution in almost all cases, with conver-
gence in just a few iterations of decoding.
Although the focus of this paper is on dynamic
programming algorithms?both in the experiments,
and also in the formal results concerning marginal
polytopes?it is straightforward to use other com-
binatorial algorithms within the approach. For ex-
ample, Koo et al (2010) describe a dual decompo-
sition approach for non-projective dependency pars-
ing, which makes use of both dynamic programming
and spanning tree inference algorithms.
2 Related Work
Dual decomposition is a classical method for solv-
ing optimization problems that can be decomposed
into efficiently solvable sub-problems. Our work is
inspired by dual decomposition methods for infer-
ence in Markov random fields (MRFs) (Wainwright
et al, 2005a; Komodakis et al, 2007; Globerson and
Jaakkola, 2007). In this approach, the MRF is de-
composed into sub-problems corresponding to tree-
structured subgraphs that together cover all edges
of the original graph. The resulting inference algo-
rithms provably solve an LP relaxation of the MRF
inference problem, often significantly faster than
commercial LP solvers (Yanover et al, 2006).
Our work is also related to methods that incorpo-
rate combinatorial solvers within loopy belief prop-
agation (LBP), either for MAP inference (Duchi et
al., 2007) or for computing marginals (Smith and
Eisner, 2008). Our approach similarly makes use
of combinatorial algorithms to efficiently solve sub-
problems of the global inference problem. However,
unlike LBP, our algorithms have strong theoretical
guarantees, such as guaranteed convergence and the
possibility of a certificate of optimality. These guar-
antees are possible because our algorithms directly
solve an LP relaxation.
Other work has considered LP or integer lin-
ear programming (ILP) formulations of inference in
NLP (Martins et al, 2009; Riedel and Clarke, 2006;
Roth and Yih, 2005). These approaches typically
use general-purpose LP or ILP solvers. Our method
has the advantage that it leverages underlying struc-
ture arising in LP formulations of NLP problems.
We will see that dynamic programming algorithms
such as CKY can be considered to be very effi-
cient solvers for particular LPs. In dual decomposi-
tion, these LPs?and their efficient solvers?can be
embedded within larger LPs corresponding to more
complex inference problems.
3 Background: Structured Models for NLP
We now describe the type of models used throughout
the paper. We take some care to set up notation that
will allow us to make a clear connection between
inference problems and linear programming.
Our first example is weighted CFG parsing. We
assume a context-free grammar, in Chomsky normal
form, with a set of non-terminals N . The grammar
contains all rules of the form A ? B C and A ?
w where A,B,C ? N and w ? V (it is simple
to relax this assumption to give a more constrained
grammar). For rules of the form A ? w we refer
to A as the part-of-speech tag for w. We allow any
non-terminal to be at the root of the tree.
2
Given a sentence with n words, w1, w2, . . . wn, a
parse tree is a set of rule productions of the form
?A ? B C, i, k, j? where A,B,C ? N , and
1 ? i ? k < j ? n. Each rule production rep-
resents the use of CFG rule A ? B C where non-
terminal A spans words wi . . . wj , non-terminal B
spans words wi . . . wk, and non-terminal C spans
words wk+1 . . . wj . There are O(|N |3n3) such rule
productions. Each parse tree corresponds to a subset
of these rule productions, of size n? 1, that forms a
well-formed parse tree.3
We now define the index set for CFG parsing as
I = {?A? B C, i, k, j?: A,B,C ? N ,
1 ? i ? k < j ? n}
Each parse tree is a vector y = {yr : r ? I},
with yr = 1 if rule r is in the parse tree, and yr =
0 otherwise. Hence each parse tree is represented
as a vector in {0, 1}m, where m = |I|. We use Y
to denote the set of all valid parse-tree vectors; the
set Y is a subset of {0, 1}m (not all binary vectors
correspond to valid parse trees).
In addition, we assume a vector ? = {?r : r ?
I} that specifies a weight for each rule production.4
Each ?r can take any value in the reals. The optimal
parse tree is y? = arg maxy?Y y ? ? where y ? ? =?
r yr?r is the inner product between y and ?.
We use yr and y(r) interchangeably (similarly for
?r and ?(r)) to refer to the r?th component of the
vector y. For example ?(A ? B C, i, k, j) is a
weight for the rule ?A? B C, i, k, j?.
We will use similar notation for other problems.
As a second example, in POS tagging the task is to
map a sentence of n words w1 . . . wn to a tag se-
quence t1 . . . tn, where each ti is chosen from a set
T of possible tags. We assume a trigram tagger,
where a tag sequence is represented through deci-
sions ?(A,B) ? C, i? where A,B,C ? T , and
i ? {3 . . . n}. Each production represents a tran-
sition where C is the tag of word wi, and (A,B) are
3We do not require rules of the form A ? wi in this repre-
sentation, as they are redundant: specifically, a rule production
?A ? B C, i, k, j? implies a rule B ? wi iff i = k, and
C ? wj iff j = k + 1.
4We do not require parameters for rules of the formA? w,
as they can be folded into rule production parameters. E.g.,
under a PCFG we define ?(A ? B C, i, k, j) = logP (A ?
B C | A) + ?i,k logP (B ? wi|B) + ?k+1,j logP (C ?
wj |C) where ?x,y = 1 if x = y, 0 otherwise.
the previous two tags. The index set for tagging is
Itag = {?(A,B)? C, i? : A,B,C ? T , 3 ? i ? n}
Note that we do not need transitions for i = 1 or i =
2, because the transition ?(A,B) ? C, 3? specifies
the first three tags in the sentence.5
Each tag sequence is represented as a vector z =
{zr : r ? Itag}, and we denote the set of valid tag
sequences, a subset of {0, 1}|Itag|, as Z . Given a
parameter vector ? = {?r : r ? Itag}, the optimal
tag sequence is arg maxz?Z z ? ?.
As a modification to the above approach, we will
find it convenient to introduce extended index sets
for both the CFG and POS tagging examples. For
the CFG case we define the extended index set to be
I ? = I ? Iuni where
Iuni = {(i, t) : i ? {1 . . . n}, t ? T}
Here each pair (i, t) represents word wi being as-
signed the tag t. Thus each parse-tree vector y will
have additional (binary) components y(i, t) spec-
ifying whether or not word i is assigned tag t.
(Throughout this paper we will assume that the tag-
set used by the tagger, T , is a subset of the set of non-
terminals considered by the parser, N .) Note that
this representation is over-complete, since a parse
tree determines a unique tagging for a sentence:
more explicitly, for any i ? {1 . . . n}, Y ? T , the
following linear constraint holds:
y(i, Y ) =
n?
k=i+1
?
X,Z?N
y(X ? Y Z, i, i, k) +
i?1?
k=1
?
X,Z?N
y(X ? Z Y, k, i? 1, i)
We apply the same extension to the tagging index
set, effectively mapping trigrams down to unigram
assignments, again giving an over-complete repre-
sentation. The extended index set for tagging is re-
ferred to as I ?tag.
From here on we will make exclusive use of ex-
tended index sets for CFG parsing and trigram tag-
ging. We use the set Y to refer to the set of valid
parse structures under the extended representation;
5As one example, in an HMM, the parameter ?((A,B) ?
C, 3) would be logP (A|??)+logP (B|?A)+logP (C|AB)+
logP (w1|A) + logP (w2|B) + logP (w3|C), where ? is the
start symbol.
3
each y ? Y is a binary vector of length |I ?|. We
similarly use Z to refer to the set of valid tag struc-
tures under the extended representation. We assume
parameter vectors for the two problems, ?cfg ? R|I
?|
and ?tag ? R|I
?
tag|.
4 Two Examples
This section describes the dual decomposition ap-
proach for two inference problems in NLP.
4.1 Integrated Parsing and Trigram Tagging
We now describe the dual decomposition approach
for integrated parsing and trigram tagging. First, de-
fine the set Q as follows:
Q = {(y, z) : y ? Y, z ? Z,
y(i, t) = z(i, t) for all (i, t) ? Iuni} (1)
Hence Q is the set of all (y, z) pairs that agree
on their part-of-speech assignments. The integrated
parsing and trigram tagging problem is then to solve
max
(y,z)?Q
(
y ? ?cfg + z ? ?tag
)
(2)
This problem is equivalent to
max
y?Y
(
y ? ?cfg + g(y) ? ?tag
)
where g : Y ? Z is a function that maps a parse
tree y to its set of trigrams z = g(y). The benefit of
the formulation in Eq. 2 is that it makes explicit the
idea of maximizing over all pairs (y, z) under a set
of agreement constraints y(i, t) = z(i, t)?this con-
cept will be central to the algorithms in this paper.
With this in mind, we note that we have effi-
cient methods for the inference problems of tagging
and parsing alone, and that our combined objective
almost separates into these two independent prob-
lems. In fact, if we drop the y(i, t) = z(i, t) con-
straints from the optimization problem, the problem
splits into two parts, each of which can be efficiently
solved using dynamic programming:
(y?, z?) = (arg max
y?Y
y ? ?cfg, arg max
z?Z
z ? ?tag)
Dual decomposition exploits this idea; it results in
the algorithm given in figure 1. The algorithm opti-
mizes the combined objective by repeatedly solving
the two sub-problems separately?that is, it directly
Set u(1)(i, t)? 0 for all (i, t) ? Iuni
for k = 1 to K do
y(k) ? arg max
y?Y
(y ? ?cfg ?
?
(i,t)?Iuni
u(k)(i, t)y(i, t))
z(k) ? arg max
z?Z
(z ? ?tag +
?
(i,t)?Iuni
u(k)(i, t)z(i, t))
if y(k)(i, t) = z(k)(i, t) for all (i, t) ? Iuni then
return (y(k), z(k))
for all (i, t) ? Iuni,
u(k+1)(i, t)? u(k)(i, t)+?k(y(k)(i, t)?z(k)(i, t))
return (y(K), z(K))
Figure 1: The algorithm for integrated parsing and tag-
ging. The parameters ?k > 0 for k = 1 . . .K specify
step sizes for each iteration, and are discussed further in
the Appendix. The two arg max problems can be solved
using dynamic programming.
solves the harder optimization problem using an ex-
isting CFG parser and trigram tagger. After each
iteration the algorithm adjusts the weights u(i, t);
these updates modify the objective functions for the
two models, encouraging them to agree on the same
POS sequence. In section 6.1 we will show that the
variables u(i, t) are Lagrange multipliers enforcing
agreement constraints, and that the algorithm corre-
sponds to a (sub)gradient method for optimization
of a dual function. The algorithm is easy to imple-
ment: all that is required is a decoding algorithm for
each of the two models, and simple additive updates
to the Lagrange multipliers enforcing agreement be-
tween the two models.
4.2 Integrating Two Lexicalized Parsers
Our second example problem is the integration of
a phrase-structure parser with a higher-order depen-
dency parser. The goal is to add higher-order fea-
tures to phrase-structure parsing without greatly in-
creasing the complexity of inference.
First, we define an index set for second-order un-
labeled projective dependency parsing. The second-
order parser considers first-order dependencies, as
well as grandparent and sibling second-order depen-
dencies (e.g., see Carreras (2007)). We assume that
Idep is an index set containing all such dependen-
cies (for brevity we omit the details of this index
set). For convenience we define an extended index
set that makes explicit use of first-order dependen-
4
cies, I ?dep = Idep ? Ifirst, where
Ifirst = {(i, j) : i ? {0 . . . n}, j ? {1 . . . n}, i 6= j}
Here (i, j) represents a dependency with head wi
and modifier wj (i = 0 corresponds to the root sym-
bol in the parse). We use D ? {0, 1}|I
?
dep| to denote
the set of valid projective dependency parses.
The second model we use is a lexicalized CFG.
Each symbol in the grammar takes the form A(h)
where A ? N is a non-terminal, and h ? {1 . . . n}
is an index specifying that wh is the head of the con-
stituent. Rule productions take the form ?A(a) ?
B(b) C(c), i, k, j? where b ? {i . . . k}, c ? {(k +
1) . . . j}, and a is equal to b or c, depending on
whether A receives its head-word from its left or
right child. Each such rule implies a dependency
(a, b) if a = c, or (a, c) if a = b. We take Ihead
to be the index set of all such rules, and I ?head =
Ihead?Ifirst to be the extended index set. We define
H ? {0, 1}|I
?
head| to be the set of valid parse trees.
The integrated parsing problem is then to find
(y?, d?) = arg max
(y,d)?R
(
y ? ?head + d ? ?dep
)
(3)
where R = {(y, d) : y ? H, d ? D,
y(i, j) = d(i, j) for all (i, j) ? Ifirst}
This problem has a very similar structure to the
problem of integrated parsing and tagging, and we
can derive a similar dual decomposition algorithm.
The Lagrange multipliers u are a vector in R|Ifirst|
enforcing agreement between dependency assign-
ments. The algorithm (omitted for brevity) is identi-
cal to the algorithm in figure 1, but with Iuni, Y , Z ,
?cfg, and ?tag replaced with Ifirst, H, D, ?head, and
?dep respectively. The algorithm only requires de-
coding algorithms for the two models, together with
simple updates to the Lagrange multipliers.
5 Marginal Polytopes and LP Relaxations
We now give formal guarantees for the algorithms
in the previous section, showing that they solve LP
relaxations of the problems in Eqs. 2 and 3.
To make the connection to linear programming,
we first introduce the idea of marginal polytopes in
section 5.1. In section 5.2, we give a precise state-
ment of the LP relaxations that are being solved
by the example algorithms, making direct use of
marginal polytopes. In section 6 we will prove that
the example algorithms solve these LP relaxations.
5.1 Marginal Polytopes
For a finite set Y , define the set of all distributions
over elements in Y as ? = {? ? R|Y| : ?y ?
0,
?
y?Y ?y = 1}. Each ? ? ? gives a vector of
marginals, ? =
?
y?Y ?yy, where ?r can be inter-
preted as the probability that yr = 1 for a y selected
at random from the distribution ?.
The set of all possible marginal vectors, known as
the marginal polytope, is defined as follows:
M = {? ? Rm : ?? ? ? such that ? =
?
y?Y
?yy}
M is also frequently referred to as the convex hull of
Y , written as conv(Y). We use the notation conv(Y)
in the remainder of this paper, instead ofM.
For an arbitrary set Y , the marginal polytope
conv(Y) can be complex to describe.6 However,
Martin et al (1990) show that for a very general
class of dynamic programming problems, the cor-
responding marginal polytope can be expressed as
conv(Y) = {? ? Rm : A? = b, ? ? 0} (4)
where A is a p?m matrix, b is vector in Rp, and the
value p is linear in the size of a hypergraph repre-
sentation of the dynamic program. Note that A and
b specify a set of p linear constraints.
We now give an explicit description of the re-
sulting constraints for CFG parsing:7 similar con-
straints arise for other dynamic programming algo-
rithms for parsing, for example the algorithms of
Eisner (2000). The exact form of the constraints, and
the fact that they are polynomial in number, is not
essential for the formal results in this paper. How-
ever, a description of the constraints gives valuable
intuition for the structure of the marginal polytope.
The constraints are given in figure 2. To develop
some intuition, consider the case where the variables
?r are restricted to be binary: hence each binary
vector ? specifies a parse tree. The second con-
straint in Eq. 5 specifies that exactly one rule must
be used at the top of the tree. The set of constraints
in Eq. 6 specify that for each production of the form
6For any finite set Y , conv(Y) can be expressed as {? ?
Rm : A? ? b} where A is a matrix of dimension p ?m, and
b ? Rp (see, e.g., Korte and Vygen (2008), pg. 65). The value
for p depends on the set Y , and can be exponential in size.
7Taskar et al (2004) describe the same set of constraints, but
without proof of correctness or reference to Martin et al (1990).
5
?r ? I?, ?r ? 0 ;
X
X,Y,Z?N
k=1...(n?1)
?(X ? Y Z, 1, k, n) = 1 (5)
?X ? N , ?(i, j) such that 1 ? i < j ? n and (i, j) 6= (1, n):
X
Y,Z?N
k=i...(j?1)
?(X ? Y Z, i, k, j) =
X
Y,Z?N
k=1...(i?1)
?(Y ? Z X, k, i? 1, j)
+
X
Y,Z?N
k=(j+1)...n
?(Y ? X Z, i, j, k) (6)
?Y ? T, ?i ? {1 . . . n} : ?(i, Y ) =
X
X,Z?N
k=(i+1)...n
?(X ? Y Z, i, i, k) +
X
X,Z?N
k=1...(i?1)
?(X ? Z Y, k, i? 1, i) (7)
Figure 2: The linear constraints defining the marginal
polytope for CFG parsing.
?X ? Y Z, i, k, j? in a parse tree, there must be
exactly one production higher in the tree that gener-
ates (X, i, j) as one of its children. The constraints
in Eq. 7 enforce consistency between the ?(i, Y )
variables and rule variables higher in the tree. Note
that the constraints in Eqs.(5?7) can be written in the
form A? = b, ? ? 0, as in Eq. 4.
Under these definitions, we have the following:
Theorem 5.1 Define Y to be the set of all CFG
parses, as defined in section 4. Then
conv(Y) = {? ? Rm : ? satisifies Eqs.(5?7)}
Proof: This theorem is a special case of Martin et al
(1990), theorem 2.
The marginal polytope for tagging, conv(Z), can
also be expressed using linear constraints as in Eq. 4;
see figure 3. These constraints follow from re-
sults for graphical models (Wainwright and Jordan,
2008), or from the Martin et al (1990) construction.
As a final point, the following theorem gives an
important property of marginal polytopes, which we
will use at several points in this paper:
Theorem 5.2 (Korte and Vygen (2008), page 66.)
For any set Y ? {0, 1}k, and for any vector ? ? Rk,
max
y?Y
y ? ? = max
??conv(Y)
? ? ? (8)
The theorem states that for a linear objective func-
tion, maximization over a discrete set Y can be
replaced by maximization over the convex hull
?r ? I?tag, ?r ? 0 ;
X
X,Y,Z?T
?((X,Y )? Z, 3) = 1
?X ? T , ?i ? {3 . . . n? 1}:
X
Y,Z?T
?((Y,Z)? X, i) =
X
Y,Z?T
?((Y,X)? Z, i+ 1)
?X ? T , ?i ? {3 . . . n? 2}:
X
Y,Z?T
?((Y,Z)? X, i) =
X
Y,Z?T
?((X,Y )? Z, i+ 2)
?X ? T,?i ? {3 . . . n} : ?(i,X) =
X
Y,Z?T
?((Y,Z)? X, i)
?X ? T : ?(1, X) =
X
Y,Z?T
?((X,Y )? Z, 3)
?X ? T : ?(2, X) =
X
Y,Z?T
?((Y,X)? Z, 3)
Figure 3: The linear constraints defining the marginal
polytope for trigram POS tagging.
conv(Y). The problem max??conv(Y) ? ?? is a linear
programming problem.
For parsing, this theorem implies that:
1. Weighted CFG parsing can be framed as a linear
programming problem, of the form max??conv(Y) ??
?, where conv(Y) is specified by a polynomial num-
ber of linear constraints.
2. Conversely, dynamic programming algorithms
such as the CKY algorithm can be considered to
be oracles that efficiently solve LPs of the form
max??conv(Y) ? ? ?.
Similar results apply for the POS tagging case.
5.2 Linear Programming Relaxations
We now describe the LP relaxations that are solved
by the example algorithms in section 4. We begin
with the algorithm in Figure 1.
The original optimization problem was to find
max(y,z)?Q
(
y ? ?cfg + z ? ?tag
)
(see Eq. 2). By the-
orem 5.2, this is equivalent to solving
max
(?,?)?conv(Q)
(
? ? ?cfg + ? ? ?tag
)
(9)
To formulate our approximation, we first define:
Q? = {(?, ?) : ? ? conv(Y), ? ? conv(Z),
?(i, t) = ?(i, t) for all (i, t) ? Iuni}
6
The definition of Q? is very similar to the definition
of Q (see Eq. 1), the only difference being that Y
and Z are replaced by conv(Y) and conv(Z) re-
spectively. Hence any point inQ is also inQ?. It fol-
lows that any point in conv(Q) is also inQ?, because
Q? is a convex set defined by linear constraints.
The LP relaxation then corresponds to the follow-
ing optimization problem:
max
(?,?)?Q?
(
? ? ?cfg + ? ? ?tag
)
(10)
Q? is defined by linear constraints, making this a
linear program. Since Q? is an outer bound on
conv(Q), i.e. conv(Q) ? Q?, we obtain the guaran-
tee that the value of Eq. 10 always upper bounds the
value of Eq. 9.
In Appendix A we give an example showing
that in general Q? includes points that are not in
conv(Q). These points exist because the agreement
between the two parts is now enforced in expecta-
tion (?(i, t) = ?(i, t) for (i, t) ? Iuni) rather than
based on actual assignments. This agreement con-
straint is weaker since different distributions over
assignments can still result in the same first order
expectations. Thus, the solution to Eq. 10 may be
in Q? but not in conv(Q). It can be shown that
all such solutions will be fractional, making them
easy to distinguish from Q. In many applications of
LP relaxations?including the examples discussed
in this paper?the relaxation in Eq. 10 turns out to
be tight, in that the solution is often integral (i.e., it
is in Q). In these cases, solving the LP relaxation
exactly solves the original problem of interest.
In the next section we prove that the algorithm
in Figure 1 solves the problem in Eq 10. A similar
result holds for the algorithm in section 4.2: it solves
a relaxation of Eq. 3, whereR is replaced by
R? = {(?, ?) : ? ? conv(H), ? ? conv(D),
?(i, j) = ?(i, j) for all (i, j) ? Ifirst}
6 Convergence Guarantees
6.1 Lagrangian Relaxation
We now show that the example algorithms solve
their respective LP relaxations given in the previ-
ous section. We do this by first introducing a gen-
eral class of linear programs, together with an op-
timization method, Lagrangian relaxation, for solv-
ing these LPs. We then show that the algorithms in
section 4 are special cases of the general algorithm.
The linear programs we consider take the form
max
x1?X1,x2?X2
(?1 ? x1 + ?2 ? x2) such that Ex1 = Fx2
The matricesE ? Rq?m andF ? Rq?l specify q lin-
ear ?agreement? constraints between x1 ? Rm and
x2 ? Rl. The setsX1,X2 are also specified by linear
constraints, X1 = {x1 ? Rm : Ax1 = b, x1 ? 0}
and X2 =
{
x2 ? Rl : Cx2 = d, x2 ? 0
}
, hence the
problem is an LP.
Note that if we set X1 = conv(Y), X2 =
conv(Z), and define E and F to specify the agree-
ment constraints ?(i, t) = ?(i, t), then we have the
LP relaxation in Eq. 10.
It is natural to apply Lagrangian relaxation in
cases where the sub-problems maxx1?X1 ?1 ?x1 and
maxx2?X2 ?2 ? x2 can be efficiently solved by com-
binatorial algorithms for any values of ?1, ?2, but
where the constraints Ex1 = Fx2 ?complicate? the
problem. We introduce Lagrange multipliers u ? Rq
that enforce the latter set of constraints, giving the
Lagrangian:
L(u, x1, x2) = ?1 ? x1 + ?2 ? x2 + u ? (Ex1 ? Fx2)
The dual objective function is
L(u) = max
x1?X1,x2?X2
L(u, x1, x2)
and the dual problem is to find minu?Rq L(u).
Because X1 and X2 are defined by linear con-
straints, by strong duality we have
min
u?Rq
L(u) = max
x1?X1,x2?X2:Ex1=Fx2
(?1 ? x1 + ?2 ? x2)
Hence minimizing L(u) will recover the maximum
value of the original problem. This leaves open the
question of how to recover the LP solution (i.e., the
pair (x?1, x
?
2) that achieves this maximum); we dis-
cuss this point in section 6.2.
The dual L(u) is convex. However, L(u) is
not differentiable, so we cannot use gradient-based
methods to optimize it. Instead, a standard approach
is to use a subgradient method. Subgradients are tan-
gent lines that lower bound a function even at points
of non-differentiability: formally, a subgradient of a
convex function L : Rn ? R at a point u is a vector
gu such that for all v, L(v) ? L(u) + gu ? (v ? u).
7
u(1) ? 0
for k = 1 to K do
x(k)1 ? arg maxx1?X1(?1 + (u
(k))TE) ? x1
x(k)2 ? arg maxx2?X2(?2 ? (u
(k))TF ) ? x2
if Ex(k)1 = Fx
(k)
2 return u
(k)
u(k+1) ? u(k) ? ?k(Ex
(k)
1 ? Fx
(k)
2 )
return u(K)
Figure 4: The Lagrangian relaxation algorithm.
By standard results, the subgradient for L at a point
u takes a simple form, gu = Ex?1 ? Fx
?
2, where
x?1 = arg max
x1?X1
(?1 + (u
(k))TE) ? x1
x?2 = arg max
x2?X2
(?2 ? (u
(k))TF ) ? x2
The beauty of this result is that the values of x?1 and
x?2, and by implication the value of the subgradient,
can be computed using oracles for the two arg max
sub-problems.
Subgradient algorithms perform updates that are
similar to gradient descent:
u(k+1) ? u(k) ? ?kg
(k)
where g(k) is the subgradient ofL at u(k) and ?k > 0
is the step size of the update. The complete sub-
gradient algorithm is given in figure 4. The follow-
ing convergence theorem is well-known (e.g., see
page 120 of Korte and Vygen (2008)):
Theorem 6.1 If limk?? ?k = 0 and
??
k=1 ?k =
?, then limk?? L(u(k)) = minu L(u).
The following proposition is easily verified:
Proposition 6.1 The algorithm in figure 1 is an in-
stantiation of the algorithm in figure 4,8 with X1 =
conv(Y), X2 = conv(Z), and the matrices E and
F defined to be binary matrices specifying the con-
straints ?(i, t) = ?(i, t) for all (i, t) ? Iuni.
Under an appropriate definition of the step sizes ?k,
it follows that the algorithm in figure 1 defines a
sequence of Lagrange multiplers u(k) minimizing a
dual of the LP relaxation in Eq. 10. A similar result
holds for the algorithm in section 4.2.
8with the caveat that it returns (x(k)1 , x
(k)
2 ) rather than u
(k).
6.2 Recovering the LP Solution
The previous section described how the method in
figure 4 can be used to minimize the dualL(u) of the
original linear program. We now turn to the problem
of recovering a primal solution (x?1, x
?
2) of the LP.
The method we propose considers two cases:
(Case 1) If Ex(k)1 = Fx
(k)
2 at any stage during
the algorithm, then simply take (x(k)1 , x
(k)
2 ) to be the
primal solution. In this case the pair (x(k)1 , x
(k)
2 ) ex-
actly solves the original LP.9 If this case arises in the
algorithm in figure 1, then the resulting solution is
binary (i.e., it is a member of Q), and the solution
exactly solves the original inference problem.
(Case 2) If case 1 does not arise, then a couple of
strategies are possible. (This situation could arise
in cases where the LP is not tight?i.e., it has a
fractional solution?or where K is not large enough
for convergence.) The first is to define the pri-
mal solution to be the average of the solutions en-
countered during the algorithm: x?1 =
?
k x
(k)
1 /K,
x?2 =
?
k x
(k)
2 /K. Results from Nedic? and Ozdaglar
(2009) show that as K ? ?, these averaged solu-
tions converge to the optimal primal solution.10 A
second strategy (as given in figure 1) is to simply
take (x(K)1 , x
(K)
2 ) as an approximation to the primal
solution. This method is a heuristic, but previous
work (e.g., Komodakis et al (2007)) has shown that
it is effective in practice; we use it in this paper.
In our experiments we found that in the vast ma-
jority of cases, case 1 applies, after a small number
of iterations; see the next section for more details.
7 Experiments
7.1 Integrated Phrase-Structure and
Dependency Parsing
Our first set of experiments considers the integration
of Model 1 of Collins (2003) (a lexicalized phrase-
structure parser, from here on referred to as Model
9We have that ?1 ? x
(k)
1 + ?2 ? x
(k)
2 = L(u
(k), x(k)1 , x
(k)
2 ) =
L(u(k)), where the last equality is because x(k)1 and x
(k)
2 are de-
fined by the respective argmax?s. Thus, (x(k)1 , x
(k)
2 ) and u
(k)
are primal and dual optimal.
10The resulting fractional solution can be projected back to
the setQ, see (Smith and Eisner, 2008; Martins et al, 2009).
8
Itn. 1 2 3 4 5-10 11-20 20-50 **
Dep 43.5 20.1 10.2 4.9 14.0 5.7 1.4 0.4
POS 58.7 15.4 6.3 3.6 10.3 3.8 0.8 1.1
Table 1: Convergence results for Section 23 of the WSJ
Treebank for the dependency parsing and POS experi-
ments. Each column gives the percentage of sentences
whose exact solutions were found in a given range of sub-
gradient iterations. ** is the percentage of sentences that
did not converge by the iteration limit (K=50).
1),11 and the 2nd order discriminative dependency
parser of Koo et al (2008). The inference problem
for a sentence x is to find
y? = arg max
y?Y
(f1(y) + ?f2(y)) (11)
where Y is the set of all lexicalized phrase-structure
trees for the sentence x; f1(y) is the score (log prob-
ability) under Model 1; f2(y) is the score under Koo
et al (2008) for the dependency structure implied
by y; and ? > 0 is a parameter dictating the relative
weight of the two models.12 This problem is simi-
lar to the second example in section 4; a very sim-
ilar dual decomposition algorithm to that described
in section 4.2 can be derived.
We used the Penn Wall Street Treebank (Marcus
et al, 1994) for the experiments, with sections 2-21
for training, section 22 for development, and section
23 for testing. The parameter ? was chosen to opti-
mize performance on the development set.
We ran the dual decomposition algorithm with a
limit of K = 50 iterations. The dual decomposi-
tion algorithm returns an exact solution if case 1 oc-
curs as defined in section 6.2; we found that of 2416
sentences in section 23, case 1 occurred for 2407
(99.6%) sentences. Table 1 gives statistics showing
the number of iterations required for convergence.
Over 80% of the examples converge in 5 iterations or
fewer; over 90% converge in 10 iterations or fewer.
We compare the accuracy of the dual decomposi-
tion approach to two baselines: first, Model 1; and
second, a naive integration method that enforces the
hard constraint that Model 1 must only consider de-
11We use a reimplementation that is a slight modification of
Collins Model 1, with very similar performance, and which uses
the TAG formalism of Carreras et al (2008).
12Note that the models f1 and f2 were trained separately,
using the methods described by Collins (2003) and Koo et al
(2008) respectively.
Precision Recall F1 Dep
Model 1 88.4 87.8 88.1 91.4
Koo08 Baseline 89.9 89.6 89.7 93.3
DD Combination 91.0 90.4 90.7 93.8
Table 2: Performance results for Section 23 of the WSJ
Treebank. Model 1: a reimplementation of the genera-
tive parser of (Collins, 2002). Koo08 Baseline: Model 1
with a hard restriction to dependencies predicted by the
discriminative dependency parser of (Koo et al, 2008).
DD Combination: a model that maximizes the joint score
of the two parsers. Dep shows the unlabeled dependency
accuracy of each system.
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50
Per
cen
tage
Maximum Number of Dual Decomposition Iterations
f score% certificates% match K=50
Figure 5: Performance on the parsing task assuming a
fixed number of iterations K. f-score: accuracy of the
method. % certificates: percentage of examples for which
a certificate of optimality is provided. % match: percent-
age of cases where the output from the method is identical
to the output when using K = 50.
pendencies seen in the first-best output from the de-
pendency parser. Table 2 shows all three results. The
dual decomposition method gives a significant gain
in precision and recall over the naive combination
method, and boosts the performance of Model 1 to
a level that is close to some of the best single-pass
parsers on the Penn treebank test set. Dependency
accuracy is also improved over the Koo et al (2008)
model, in spite of the relatively low dependency ac-
curacy of Model 1 alone.
Figure 5 shows performance of the approach as a
function ofK, the maximum number of iterations of
dual decomposition. For this experiment, for cases
where the method has not converged for k ? K,
the output from the algorithm is chosen to be the
y(k) for k ? K that maximizes the objective func-
tion in Eq. 11. The graphs show that values of K
less than 50 produce almost identical performance to
K = 50, but with fewer cases giving certificates of
optimality (with K = 10, the f-score of the method
is 90.69%; with K = 5 it is 90.63%).
9
Precision Recall F1 POS Acc
Fixed Tags 88.1 87.6 87.9 96.7
DD Combination 88.7 88.0 88.3 97.1
Table 3: Performance results for Section 23 of the WSJ.
Model 1 (Fixed Tags): a baseline parser initialized to the
best tag sequence of from the tagger of Toutanova and
Manning (2000). DD Combination: a model that maxi-
mizes the joint score of parse and tag selection.
7.2 Integrated Phrase-Structure Parsing and
Trigram POS tagging
In a second experiment, we used dual decomposi-
tion to integrate the Model 1 parser with the Stan-
ford max-ent trigram POS tagger (Toutanova and
Manning, 2000), using a very similar algorithm to
that described in section 4.1. We use the same train-
ing/dev/test split as in section 7.1. The two models
were again trained separately.
We ran the algorithm with a limit of K = 50 it-
erations. Out of 2416 test examples, the algorithm
found an exact solution in 98.9% of the cases. Ta-
ble 1 gives statistics showing the speed of conver-
gence for different examples: over 94% of the exam-
ples converge to an exact solution in 10 iterations or
fewer. In terms of accuracy, we compare to a base-
line approach of using the first-best tag sequence
as input to the parser. The dual decomposition ap-
proach gives 88.3 F1 measure in recovering parse-
tree constituents, compared to 87.9 for the baseline.
8 Conclusions
We have introduced dual-decomposition algorithms
for inference in NLP, given formal properties of the
algorithms in terms of LP relaxations, and demon-
strated their effectiveness on problems that would
traditionally be solved using intersections of dy-
namic programs (Bar-Hillel et al, 1964). Given the
widespread use of dynamic programming in NLP,
there should be many applications for the approach.
There are several possible extensions of the
method we have described. We have focused on
cases where two models are being combined; the
extension to more than two models is straightfor-
ward (e.g., see Komodakis et al (2007)). This paper
has considered approaches for MAP inference; for
closely related methods that compute approximate
marginals, see Wainwright et al (2005b).
A Fractional Solutions
We now give an example of a point (?, ?) ? Q?\conv(Q)
that demonstrates that the relaxation Q? is strictly larger
than conv(Q). Fractional points such as this one can arise
as solutions of the LP relaxation for worst case instances,
preventing us from finding an exact solution.
Recall that the constraints for Q? specify that ? ?
conv(Y), ? ? conv(Z), and ?(i, t) = ?(i, t) for all
(i, t) ? Iuni. Since ? ? conv(Y), ? must be a con-
vex combination of 1 or more members of Y; a similar
property holds for ?. The example is as follows. There
are two possible parts of speech, A and B, and an addi-
tional non-terminal symbol X . The sentence is of length
3, w1 w2 w3. Let ? be the convex combination of the
following two tag sequences, each with probability 0.5:
w1/A w2/A w3/A and w1/A w2/B w3/B. Let ? be
the convex combination of the following two parses, each
with probability 0.5: (X(A w1)(X(A w2)(B w3))) and
(X(A w1)(X(B w2)(A w3))). It can be verified that
?(i, t) = ?(i, t) for all (i, t), i.e., the marginals for single
tags for ? and ? agree. Thus, (?, ?) ? Q?.
To demonstrate that this fractional point is not in
conv(Q), we give parameter values such that this frac-
tional point is optimal and all integral points (i.e., ac-
tual parses) are suboptimal. For the tagging model, set
?(AA? A, 3) = ?(AB ? B, 3) = 0, with all other pa-
rameters having a negative value. For the parsing model,
set ?(X ? A X, 1, 1, 3) = ?(X ? A B, 2, 2, 3) =
?(X ? B A, 2, 2, 3) = 0, with all other rule parameters
being negative. For this objective, the fractional solution
has value 0, while all integral points (i.e., all points inQ)
have a negative value. By Theorem 5.2, the maximum of
any linear objective over conv(Q) is equal to the maxi-
mum over Q. Thus, (?, ?) 6? conv(Q).
B Step Size
We used the following step size in our experiments. First,
we initialized ?0 to equal 0.5, a relatively large value.
Then we defined ?k = ?0 ? 2??k , where ?k is the num-
ber of times that L(u(k
?)) > L(u(k
??1)) for k? ? k. This
learning rate drops at a rate of 1/2t, where t is the num-
ber of times that the dual increases from one iteration to
the next. See Koo et al (2010) for a similar, but less ag-
gressive step size used to solve a different task.
Acknowledgments MIT gratefully acknowledges the
support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government. Alexander Rush was supported under the GALE
program of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022. David Sontag was supported
by a Google PhD Fellowship.
10
References
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In Proc CONLL, pages 9?16.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. CoNLL, pages
957?961.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP, page 8.
M. Collins. 2003. Head-driven statistical models for nat-
ural language parsing. In Computational linguistics,
volume 29, pages 589?637.
G.B. Dantzig and P. Wolfe. 1960. Decomposition princi-
ple for linear programs. In Operations research, vol-
ume 8, pages 101?111.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. In NIPS, volume 19.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29?62.
A. Globerson and T. Jaakkola. 2007. Fixing max-
product: Convergent message passing algorithms for
MAP LP-relaxations. In NIPS, volume 21.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In International Conference on
Computer Vision.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. ACL/HLT.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual Decomposition for Parsing with Non-
Projective Head Automata. In Proc. EMNLP, pages
63?70.
B.H. Korte and J. Vygen. 2008. Combinatorial optimiza-
tion: theory and algorithms. Springer Verlag.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of English:
The Penn Treebank. In Computational linguistics, vol-
ume 19, pages 313?330.
R.K. Martin, R.L. Rardin, and B.A. Campbell. 1990.
Polyhedral characterization of discrete dynamic pro-
gramming. Operations research, 38(1):127?138.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP, pages 523?
530.
Angelia Nedic? and Asuman Ozdaglar. 2009. Approxi-
mate primal solutions and rate analysis for dual sub-
gradient methods. SIAM Journal on Optimization,
19(4):1757?1780.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. ACL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. EMNLP, pages 129?137.
D. Roth and W. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In Proc.
ICML, pages 737?744.
Hanif D. Sherali and Warren P. Adams. 1994. A hi-
erarchy of relaxations and convex hull characteriza-
tions for mixed-integer zero?one programming prob-
lems. Discrete Applied Mathematics, 52(1):83 ? 106.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. EMNLP, pages 145?156.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proc. UAI.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP,
pages 1?8.
K. Toutanova and C.D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. EMNLP, pages 63?70.
M. Wainwright and M. I. Jordan. 2008. Graphical Mod-
els, Exponential Families, and Variational Inference.
Now Publishers Inc., Hanover, MA, USA.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005a.
MAP estimation via agreement on trees: message-
passing and linear programming. In IEEE Transac-
tions on Information Theory, volume 51, pages 3697?
3717.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005b. A
new class of upper bounds on the log partition func-
tion. In IEEE Transactions on Information Theory,
volume 51, pages 2313?2335.
C. Yanover, T. Meltzer, and Y. Weiss. 2006. Linear
Programming Relaxations and Belief Propagation?An
Empirical Study. In The Journal of Machine Learning
Research, volume 7, page 1907. MIT Press.
11
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288?1298,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Dual Decomposition for Parsing with Non-Projective Head Automata
Terry Koo Alexander M. Rush Michael Collins Tommi Jaakkola David Sontag
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,srush,mcollins,tommi,dsontag}@csail.mit.edu
Abstract
This paper introduces algorithms for non-
projective parsing based on dual decomposi-
tion. We focus on parsing algorithms for non-
projective head automata, a generalization of
head-automata models to non-projective struc-
tures. The dual decomposition algorithms are
simple and efficient, relying on standard dy-
namic programming and minimum spanning
tree algorithms. They provably solve an LP
relaxation of the non-projective parsing prob-
lem. Empirically the LP relaxation is very of-
ten tight: for many languages, exact solutions
are achieved on over 98% of test sentences.
The accuracy of our models is higher than pre-
vious work on a broad range of datasets.
1 Introduction
Non-projective dependency parsing is useful for
many languages that exhibit non-projective syntactic
structures. Unfortunately, the non-projective parsing
problem is known to be NP-hard for all but the sim-
plest models (McDonald and Satta, 2007). There has
been a long history in combinatorial optimization of
methods that exploit structure in complex problems,
using methods such as dual decomposition or La-
grangian relaxation (Lemare?chal, 2001). Thus far,
however, these methods are not widely used in NLP.
This paper introduces algorithms for non-
projective parsing based on dual decomposition. We
focus on parsing algorithms for non-projective head
automata, a generalization of the head-automata
models of Eisner (2000) and Alshawi (1996) to non-
projective structures. These models include non-
projective dependency parsing models with higher-
order (e.g., sibling and/or grandparent) dependency
relations as a special case. Although decoding of full
parse structures with non-projective head automata
is intractable, we leverage the observation that key
components of the decoding can be efficiently com-
puted using combinatorial algorithms. In particular,
1. Decoding for individual head-words can be ac-
complished using dynamic programming.
2. Decoding for arc-factored models can be ac-
complished using directed minimum-weight
spanning tree (MST) algorithms.
The resulting parsing algorithms have the following
properties:
? They are efficient and easy to implement, relying
on standard dynamic programming and MST al-
gorithms.
? They provably solve a linear programming (LP)
relaxation of the original decoding problem.
? Empirically the algorithms very often give an ex-
act solution to the decoding problem, in which
case they also provide a certificate of optimality.
In this paper we first give the definition for non-
projective head automata, and describe the parsing
algorithm. The algorithm can be viewed as an in-
stance of Lagrangian relaxation; we describe this
connection, and give convergence guarantees for the
method. We describe a generalization to models
that include grandparent dependencies. We then in-
troduce a perceptron-driven training algorithm that
makes use of point 1 above.
We describe experiments on non-projective pars-
ing for a number of languages, and in particu-
lar compare the dual decomposition algorithm to
approaches based on general-purpose linear pro-
gramming (LP) or integer linear programming (ILP)
solvers (Martins et al, 2009). The accuracy of our
models is higher than previous work on a broad
range of datasets. The method gives exact solutions
to the decoding problem, together with a certificate
of optimality, on over 98% of test examples for many
of the test languages, with parsing times ranging be-
tween 0.021 seconds/sentence for the most simple
languages/models, to 0.295 seconds/sentence for the
1288
most complex settings. The method compares favor-
ably to previous work using LP/ILP formulations,
both in terms of efficiency, and also in terms of the
percentage of exact solutions returned.
While the focus of the current paper is on non-
projective dependency parsing, the approach opens
up new ways of thinking about parsing algorithms
for lexicalized formalisms such as TAG (Joshi and
Schabes, 1997), CCG (Steedman, 2000), and pro-
jective head automata.
2 Related Work
McDonald et al (2005) describe MST-based parsing
for non-projective dependency parsing models with
arc-factored decompositions; McDonald and Pereira
(2006) make use of an approximate (hill-climbing)
algorithm for parsing with more complex models.
McDonald and Pereira (2006) and McDonald and
Satta (2007) describe complexity results for non-
projective parsing, showing that parsing for a variety
of models is NP-hard. Riedel and Clarke (2006) de-
scribe ILP methods for the problem; Martins et al
(2009) recently introduced alternative LP and ILP
formulations. Our algorithm differs in that we do not
use general-purpose LP or ILP solvers, instead using
an MST solver in combination with dynamic pro-
gramming; thus we leverage the underlying struc-
ture of the problem, thereby deriving more efficient
decoding algorithms.
Both dual decomposition and Lagrangian relax-
ation have a long history in combinatorial optimiza-
tion. Our work was originally inspired by recent
work on dual decomposition for inference in graph-
ical models (Wainwright et al, 2005; Komodakis
et al, 2007). However, the non-projective parsing
problem has a very different structure from these
models, and the decomposition we use is very dif-
ferent in nature from those used in graphical mod-
els. Other work has made extensive use of de-
composition approaches for efficiently solving LP
relaxations for graphical models (e.g., Sontag et
al. (2008)). Methods that incorporate combinato-
rial solvers within loopy belief propagation (LBP)
(Duchi et al, 2007; Smith and Eisner, 2008) are
also closely related to our approach. Unlike LBP,
our method has strong theoretical guarantees, such
as guaranteed convergence and the possibility of a
certificate of optimality.
Finally, in other recent work, Rush et al (2010)
describe dual decomposition approaches for other
NLP problems.
3 Sibling Models
This section describes a particular class of models,
sibling models; the next section describes a dual-
decomposition algorithm for decoding these models.
Consider the dependency parsing problem for a
sentence with n words. We define the index set
for dependency parsing to be I = {(i, j) : i ?
{0 . . . n}, j ? {1 . . . n}, i 6= j}. A dependency
parse is a vector y = {y(i, j) : (i, j) ? I}, where
y(i, j) = 1 if a dependency with head word i and
modifier j is in the parse, 0 otherwise. We use i = 0
for the root symbol. We define Y to be the set of all
well-formed non-projective dependency parses (i.e.,
the set of directed spanning trees rooted at node 0).
Given a function f : Y 7? R that assigns scores to
parse trees, the optimal parse is
y? = argmax
y?Y
f(y) (1)
A particularly simple definition of f(y) is f(y) =
?
(i,j)?I y(i, j)?(i, j) where ?(i, j) is the score for
dependency (i, j). Models with this form are often
referred to as arc-factored models. In this case the
optimal parse tree y? can be found efficiently using
MST algorithms (McDonald et al, 2005).
This paper describes algorithms that compute y?
for more complex definitions of f(y); in this sec-
tion, we focus on algorithms for models that capture
interactions between sibling dependencies. To this
end, we will find it convenient to define the follow-
ing notation. Given a vector y, define
y|i = {y(i, j) : j = 1 . . . n, j 6= i}
Hence y|i specifies the set of modifiers to word i;
note that the vectors y|i for i = 0 . . . n form a parti-
tion of the full set of variables.
We then assume that f(y) takes the form
f(y) =
n?
i=0
fi(y|i) (2)
Thus f(y) decomposes into a sum of terms, where
each fi considers modifiers to the i?th word alone.
In the general case, finding y? =
argmaxy?Y f(y) under this definition of f(y)
is an NP-hard problem. However for certain
1289
definitions of fi, it is possible to efficiently compute
argmaxy|i?Zi fi(y|i) for any value of i, typically
using dynamic programming. (Here we use Zi to
refer to the set of all possible values for y|i: specifi-
cally, Z0 = {0, 1}n and for i 6= 0, Zi = {0, 1}n?1.)
In these cases we can efficiently compute
z? = argmax
z?Z
f(z) = argmax
z?Z
?
i
fi(z|i) (3)
where Z = {z : z|i ? Zi for i = 0 . . . n} by
simply computing z?|i = argmaxz|i?Zi fi(z|i) for
i = 0 . . . n. Eq. 3 can be considered to be an approx-
imation to Eq. 1, where we have replaced Y with
Z . We will make direct use of this approximation
in the dual decomposition parsing algorithm. Note
that Y ? Z , and in all but trivial cases, Y is a strict
subset of Z . For example, a structure z ? Z could
have z(i, j) = z(j, i) = 1 for some (i, j); it could
contain longer cycles; or it could contain words that
do not modify exactly one head. Nevertheless, with
suitably powerful functions fi?for example func-
tions based on discriminative models?z? may be a
good approximation to y?. Later we will see that
dual decomposition can effectively use MST infer-
ence to rule out ill-formed structures.
We now give the main assumption underlying sib-
ling models:
Assumption 1 (Sibling Decompositions) A model
f(y) satisfies the sibling-decomposition assumption
if: 1) f(y) =
?n
i=0 fi(y|i) for some set of functions
f0 . . . fn. 2) For any i ? {0 . . . n}, for any value
of the variables u(i, j) ? R for j = 1 . . . n, it is
possible to compute
argmax
y|i?Zi
?
?fi(y|i)?
?
j
u(i, j)y(i, j)
?
?
in polynomial time.
The second condition includes additional terms in-
volving u(i, j) variables that modify the scores of
individual dependencies. These terms are benign for
most definitions of fi, in that they do not alter de-
coding complexity. They will be of direct use in the
dual decomposition parsing algorithm.
Example 1: Bigram Sibling Models. Recall that
y|i is a binary vector specifying which words are
modifiers to the head-word i. Define l1 . . . lp to be
the sequence of left modifiers to word i under y|i,
and r1 . . . rq to be the set of right modifiers (e.g.,
consider the case where n = 5, i = 3, and we have
y(3, 1) = y(3, 5) = 0, and y(3, 2) = y(3, 4) = 1:
in this case p = 1, l1 = 2, and q = 1, r1 = 4). In
bigram sibling models, we have
fi(y|i) =
p+1?
k=1
gL(i, lk?1, lk) +
q+1?
k=1
gR(i, rk?1, rk)
where l0 = r0 = START is the initial state, and
lp+1 = rq+1 = END is the end state. The functions
gL and gR assign scores to bigram dependencies to
the left and right of the head. Under this model cal-
culating argmaxy|i?Zi
(
fi(y|i)?
?
j u(i, j)y(i, j)
)
takes O(n2) time using dynamic programming,
hence the model satisfies Assumption 1.
Example 2: Head Automata Head-automata
models constitute a second important model type
that satisfy the sibling-decomposition assumption
(bigram sibling models are a special case of head
automata). These models make use of functions
gR(i, s, s?, r) where s ? S, s? ? S are variables in a
set of possible states S, and r is an index of a word
in the sentence such that i < r ? n. The function
gR returns a cost for taking word r as the next depen-
dency, and transitioning from state s to s?. A similar
function gL is defined for left modifiers. We define
fi(y|i, s0 . . . sq, t0 . . . tp) =
q?
k=1
gR(i, sk?1, sk, rk) +
p?
k=1
gL(i, tk?1, tk, ll)
to be the joint score for dependencies y|i, and left
and right state sequences s0 . . . sq and t0 . . . tp. We
specify that s0 = t0 = START and sq = tp = END.
In this case we define
fi(y|i) = maxs0...sq ,t0...tp
fi(y|i, s0 . . . sq, t0 . . . tp)
and it follows that argmaxy|i?Zi fi(y|i) can be com-
puted inO(n|S|2) time using a variant of the Viterbi
algorithm, hence the model satisfies the sibling-
decomposition assumption.
4 The Parsing Algorithm
We now describe the dual decomposition parsing al-
gorithm for models that satisfy Assumption 1. Con-
sider the following generalization of the decoding
1290
Set u(1)(i, j)? 0 for all (i, j) ? I
for k = 1 to K do
y(k) ? argmax
y?Y
?
(i,j)?I
(
?(i, j) + u(k)(i, j)
)
y(i, j)
for i ? {0 . . . n},
z(k)|i ? argmax
z|i?Zi
(fi(z|i)?
?
j
u(k)(i, j)z(i, j))
if y(k)(i, j) = z(k)(i, j) for all (i, j) ? I then
return (y(k), z(k))
for all (i, j) ? I,
u(k+1)(i, j)? u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))
return (y(K), z(K))
Figure 1: The parsing algorithm for sibling decompos-
able models. ?k ? 0 for k = 1 . . .K are step sizes, see
Appendix A for details.
problem from Eq. 1, where f(y) =
?
i fi(y|i),
h(y) =
?
(i,j)?I ?(i, j)y(i, j), and ?(i, j) ? R for
all (i, j):1
argmax
z?Z,y?Y
f(z) + h(y) (4)
such that z(i, j) = y(i, j) for all (i, j) ? I (5)
Although the maximization w.r.t. z is taken over the
set Z , the constraints in Eq. 5 ensure that z = y for
some y ? Y , and hence that z ? Y .
Without the z(i, j) = y(i, j) constraints, the
objective would decompose into the separate max-
imizations z? = argmaxz?Z f(z), and y
? =
argmaxy?Y h(y), which can be easily solved us-
ing dynamic programming and MST, respectively.
Thus, it is these constraints that complicate the op-
timization. Our approach gets around this difficulty
by introducing new variables, u(i, j), that serve to
enforce agreement between the y(i, j) and z(i, j)
variables. In the next section we will show that these
u(i, j) variables are actually Lagrange multipliers
for the z(i, j) = y(i, j) constraints.
Our parsing algorithm is shown in Figure 1. At
each iteration k, the algorithm finds y(k) ? Y us-
ing an MST algorithm, and z(k) ? Z through sep-
arate decoding of the (n + 1) sibling models. The
u(k) variables are updated if y(k)(i, j) 6= z(k)(i, j)
1This is equivalent to Eq. 1 when ?(i, j) = 0 for all (i, j).
In some cases, however, it is convenient to have a model with
non-zero values for the ? variables; see the Appendix. Note that
this definition of h(y) allows argmaxy?Y h(y) to be calculated
efficiently, using MST inference.
for some (i, j); these updates modify the objective
functions for the two decoding steps, and intuitively
encourage the y(k) and z(k) variables to be equal.
4.1 Lagrangian Relaxation
Recall that the main difficulty in solving Eq. 4 was
the z = y constraints. We deal with these con-
straints using Lagrangian relaxation (Lemare?chal,
2001). We first introduce Lagrange multipliers u =
{u(i, j) : (i, j) ? I}, and define the Lagrangian
L(u, y, z) = (6)
f(z) + h(y) +
?
(i,j)?I
u(i, j)
(
y(i, j)? z(i, j)
)
If L? is the optimal value of Eq. 4 subject to the
constraints in Eq. 5, then for any value of u,
L? = max
z?Z,y?Y,y=z
L(u, y, z) (7)
This follows because if y = z, the right term in Eq. 6
is zero for any value of u. The dual objective L(u)
is obtained by omitting the y = z constraint:
L(u) = max
z?Z,y?Y
L(u, y, z)
= max
z?Z
(
f(z)?
?
i,j
u(i, j)z(i, j)
)
+max
y?Y
(
h(y) +
?
i,j
u(i, j)y(i, j)
)
.
Since L(u) maximizes over a larger space (y may
not equal z), we have that L? ? L(u) (compare this
to Eq. 7). The dual problem, which our algorithm
optimizes, is to obtain the tightest such upper bound,
(Dual problem) min
u?R|I|
L(u). (8)
The dual objective L(u) is convex, but not differen-
tiable. However, we can use a subgradient method
to derive an algorithm that is similar to gradient de-
scent, and which minimizes L(u). A subgradient of
a convex function L(u) at u is a vector du such that
for all v ? R|I|, L(v) ? L(u) + du ? (v ? u). By
standard results,
du(k) = y
(k) ? z(k)
is a subgradient for L(u) at u = u(k), where z(k) =
argmaxz?Z f(z)?
?
i,j u
(k)(i, j)z(i, j) and y(k) =
1291
argmaxy?Y h(y) +
?
i,j u
(k)(i, j)y(i, j). Subgra-
dient optimization methods are iterative algorithms
with updates that are similar to gradient descent:
u(k+1) = u(k) ? ?kdu(k) = u
(k) ? ?k(y
(k) ? z(k)),
where ?k is a step size. It is easily verified that the
algorithm in Figure 1 uses precisely these updates.
4.2 Formal Guarantees
With an appropriate choice of the step sizes ?k, the
subgradient method can be shown to solve the dual
problem, i.e.
lim
k??
L(u(k)) = min
u
L(u).
See Korte and Vygen (2008), page 120, for details.
As mentioned before, the dual provides an up-
per bound on the optimum of the primal problem
(Eq. 4),
max
z?Z,y?Y,y=z
f(z) + h(y) ? min
u?R|I|
L(u). (9)
However, we do not necessarily have strong
duality?i.e., equality in the above equation?
because the sets Z and Y are discrete sets. That
said, for some functions h(y) and f(z) strong du-
ality does hold, as stated in the following:
Theorem 1 If for some k ? {1 . . .K} in the al-
gorithm in Figure 1, y(k)(i, j) = z(k)(i, j) for all
(i, j) ? I, then (y(k), z(k)) is a solution to the max-
imization problem in Eq. 4.
Proof. We have that f(z(k)) + h(y(k)) =
L(u(k), z(k), y(k)) = L(u(k)), where the last equal-
ity is because y(k), z(k) are defined as the respective
argmax?s. Thus, the inequality in Eq. 9 is tight, and
(y(k), z(k)) and u(k) are primal and dual optimal.
Although the algorithm is not guaranteed to sat-
isfy y(k) = z(k) for some k, by Theorem 1 if it does
reach such a state, then we have the guarantee of an
exact solution to Eq. 4, with the dual solution u pro-
viding a certificate of optimality. We show in the
experiments that this occurs very frequently, in spite
of the parsing problem being NP-hard.
It can be shown that Eq. 8 is the dual of an LP
relaxation of the original problem. When the con-
ditions of Theorem 1 are satisfied, it means that the
LP relaxation is tight for this instance. For brevity
we omit the details, except to note that when the LP
relaxation is not tight, the optimal primal solution to
the LP relaxation could be recovered by averaging
methods (Nedic? and Ozdaglar, 2009).
5 Grandparent Dependency Models
In this section we extend the approach to consider
grandparent relations. In grandparent models each
parse tree y is represented as a vector
y = {y(i, j) : (i, j) ? I} ? {y?(i, j) : (i, j) ? I}
where we have added a second set of duplicate vari-
ables, y?(i, j) for all (i, j) ? I. The set of all valid
parse trees is then defined as
Y = {y : y(i, j) variables form a directed tree,
y?(i, j) = y(i, j) for all (i, j) ? I}
We again partition the variables into n + 1 subsets,
y|0 . . . y|n, by (re)defining
y|i = {y(i, j) : j = 1 . . . n, j 6= i}
?{y?(k, i) : k = 0 . . . n, k 6= i}
So as before y|i contains variables y(i, j) which in-
dicate which words modify the i?th word. In addi-
tion, y|i includes y?(k, i) variables that indicate the
word that word i itself modifies.
The set of all possible values of y|i is now
Zi = {y|i : y(i, j) ? {0, 1} for j = 1 . . . n, j 6= i;
y?(k, i) ? {0, 1} for k = 0 . . . n, k 6= i;
?
k
y?(k, i) = 1}
Hence the y(i, j) variables can take any values, but
only one of the y?(k, i) variables can be equal to 1
(as only one word can be a parent of word i). As be-
fore, we define Z = {y : y|i ? Zi for i = 0 . . . n}.
We introduce the following assumption:
Assumption 2 (GS Decompositions)
A model f(y) satisfies the grandparent/sibling-
decomposition (GSD) assumption if: 1) f(z) =
?n
i=0 fi(z|i) for some set of functions f0 . . . fn. 2)
For any i ? {0 . . . n}, for any value of the variables
u(i, j) ? R for j = 1 . . . n, and v(k, i) ? R for
k = 0 . . . n, it is possible to compute
argmax
z|i?Zi
(fi(z|i)?
?
j
u(i, j)z(i, j)?
?
k
v(k, i)z?(k, i))
in polynomial time.
1292
Again, it follows that we can approxi-
mate y? = argmaxy?Y
?n
i=0 fi(y|i) by
z? = argmaxz?Z
?n
i=0 fi(z|i), by defining
z?|i = argmaxz|i?Zi fi(z|i) for i = 0 . . . n. The
resulting vector z? may be deficient in two respects.
First, the variables z?(i, j) may not form a well-
formed directed spanning tree. Second, we may
have z??(i, j) 6= z
?(i, j) for some values of (i, j).
Example 3: Grandparent/Sibling Models An
important class of models that satisfy Assumption 2
are defined as follows. Again, for a vector y|i de-
fine l1 . . . lp to be the sequence of left modifiers to
word i under y|i, and r1 . . . rq to be the set of right
modifiers. Define k? to the value for k such that
y?(k, i) = 1. Then the model is defined as follows:
fi(y|i) =
p+1?
j=1
gL(i, k
?, lj?1, lj)+
q+1?
j=1
gR(i, k
?, rj?1, rj)
This is very similar to the bigram-sibling model, but
with the modification that the gL and gR functions
depend in addition on the value for k?. This al-
lows these functions to model grandparent depen-
dencies such as (k?, i, lj) and sibling dependencies
such as (i, lj?1, lj). Finding z?|i under the definition
can be accomplished inO(n3) time, by decoding the
model using dynamic programming separately for
each of the O(n) possible values of k?, and pick-
ing the value for k? that gives the maximum value
under these decodings.
A dual-decomposition algorithm for models that
satisfy the GSD assumption is shown in Figure 2.
The algorithm can be justified as an instance of La-
grangian relaxation applied to the problem
argmax
z?Z,y?Y
f(z) + h(y) (10)
with constraints
z(i, j) = y(i, j) for all (i, j) ? I (11)
z?(i, j) = y(i, j) for all (i, j) ? I (12)
The algorithm employs two sets of Lagrange mul-
tipliers, u(i, j) and v(i, j), corresponding to con-
straints in Eqs. 11 and 12. As in Theorem 1, if at any
point in the algorithm z(k) = y(k), then (z(k), y(k))
is an exact solution to the problem in Eq. 10.
Set u(1)(i, j)? 0, v(1)(i, j)? 0 for all (i, j) ? I
for k = 1 to K do
y(k) ? argmax
y?Y
?
(i,j)?I
y(i, j)?(i, j)
where ?(i, j) = ?(i, j) + u(k)(i, j) + v(k)(i, j).
for i ? {0 . . . n},
z(k)|i ? argmax
z|i?Zi
(fi(z|i) ?
?
j
u(k)(i, j)z(i, j)
?
?
j
v(k)(j, i)z?(j, i))
if y(k)(i, j) = z(k)(i, j) = z(k)? (i, j) for all (i, j) ? I
then
return (y(k), z(k))
for all (i, j) ? I,
u(k+1)(i, j)? u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))
v(k+1)(i, j)? v(k)(i, j)+?k(z
(k)
? (i, j)?y
(k)(i, j))
return (y(K), z(K))
Figure 2: The parsing algorithm for grandparent/sibling-
decomposable models.
6 The Training Algorithm
In our experiments we make use of discriminative
linear models, where for an input sentence x, the
score for a parse y is f(y) = w ? ?(x, y) where
w ? Rd is a parameter vector, and ?(x, y) ? Rd
is a feature-vector representing parse tree y in con-
junction with sentence x. We will assume that the
features decompose in the same way as the sibling-
decomposable or grandparent/sibling-decomposable
models, that is ?(x, y) =
?n
i=0 ?(x, y|i) for some
feature vector definition ?(x, y|i). In the bigram sib-
ling models in our experiments, we assume that
?(x, y|i) =
p+1?
k=1
?L(x, i, lk?1, lk) +
q+1?
k=1
?R(x, i, rk?1, rk)
where as before l1 . . . lp and r1 . . . rq are left and
right modifiers under y|i, and where ?L and ?R
are feature vector definitions. In the grandparent
models in our experiments, we use a similar defi-
nition with feature vectors ?L(x, i, k?, lk?1, lk) and
?R(x, i, k?, rk?1, rk), where k? is the parent for
word i under y|i.
We train the model using the averaged perceptron
for structured problems (Collins, 2002). Given the
i?th example in the training set, (x(i), y(i)), the per-
ceptron updates are as follows:
? z? = argmaxy?Z w ? ?(x
(i), y)
? If z? 6= y(i), w = w+?(x(i), y(i))??(x(i), z?)
1293
The first step involves inference over the set Z ,
rather than Y as would be standard in the percep-
tron. Thus, decoding during training can be achieved
by dynamic programming over head automata alone,
which is very efficient.
Our training approach is closely related to local
training methods (Punyakanok et al, 2005). We
have found this method to be effective, very likely
because Z is a superset of Y . Our training algo-
rithm is also related to recent work on training using
outer bounds (see, e.g., (Taskar et al, 2003; Fin-
ley and Joachims, 2008; Kulesza and Pereira, 2008;
Martins et al, 2009)). Note, however, that the LP re-
laxation optimized by dual decomposition is signifi-
cantly tighter than Z . Thus, an alternative approach
would be to use the dual decomposition algorithm
for inference during training.
7 Experiments
We report results on a number of data sets. For
comparison to Martins et al (2009), we perform ex-
periments for Danish, Dutch, Portuguese, Slovene,
Swedish and Turkish data from the CoNLL-X
shared task (Buchholz and Marsi, 2006), and En-
glish data from the CoNLL-2008 shared task (Sur-
deanu et al, 2008). We use the official training/test
splits for these data sets, and the same evaluation
methodology as Martins et al (2009). For com-
parison to Smith and Eisner (2008), we also re-
port results on Danish and Dutch using their alter-
nate training/test split. Finally, we report results on
the English WSJ treebank, and the Prague treebank.
We use feature sets that are very similar to those
described in Carreras (2007). We use marginal-
based pruning, using marginals calculated from an
arc-factored spanning tree model using the matrix-
tree theorem (McDonald and Satta, 2007; Smith and
Smith, 2007; Koo et al, 2007).
In all of our experiments we set the value K, the
maximum number of iterations of dual decompo-
sition in Figures 1 and 2, to be 5,000. If the al-
gorithm does not terminate?i.e., it does not return
(y(k), z(k)) within 5,000 iterations?we simply take
the parse y(k) with the maximum value of f(y(k)) as
the output from the algorithm. At first sight 5,000
might appear to be a large number, but decoding is
still fast?see Sections 7.3 and 7.4 for discussion.2
2Note also that the feature vectors ? and inner productsw ??
The strategy for choosing step sizes ?k is described
in Appendix A, along with other details.
We first discuss performance in terms of accu-
racy, success in recovering an exact solution, and
parsing speed. We then describe additional experi-
ments examining various aspects of the algorithm.
7.1 Accuracy
Table 1 shows results for previous work on the var-
ious data sets, and results for an arc-factored model
with pure MST decoding with our features. (We use
the acronym UAS (unlabeled attachment score) for
dependency accuracy.) We also show results for the
bigram-sibling and grandparent/sibling (G+S) mod-
els under dual decomposition. Both the bigram-
sibling and G+S models show large improvements
over the arc-factored approach; they also compare
favorably to previous work?for example the G+S
model gives better results than all results reported in
the CoNLL-X shared task, on all languages. Note
that we use different feature sets from both Martins
et al (2009) and Smith and Eisner (2008).
7.2 Success in Recovering Exact Solutions
Next, we consider how often our algorithms return
an exact solution to the original optimization prob-
lem, with a certificate?i.e., how often the algo-
rithms in Figures 1 and 2 terminate with y(k) = z(k)
for some value of k < 5000 (and are thus optimal,
by Theorem 1). The CertS and CertG columns in Ta-
ble 1 give the results for the sibling and G+S models
respectively. For all but one setting3 over 95% of the
test sentences are decoded exactly, with 99% exact-
ness in many cases.
For comparison, we also ran both the single-
commodity flow and multiple-commodity flow LP
relaxations of Martins et al (2009) with our mod-
els and features. We measure how often these re-
laxations terminate with an exact solution. The re-
sults in Table 2 show that our method gives exact
solutions more often than both of these relaxations.4
In computing the accuracy figures for Martins et al
only need to be computed once, thus saving computation.
3The exception is Slovene, which has the smallest training
set at only 1534 sentences.
4Note, however, that it is possible that the Martins et al re-
laxations would have given a higher proportion of integral solu-
tions if their relaxation was used during training.
1294
Ma09 MST Sib G+S Best CertS CertG TimeS TimeG TrainS TrainG
Dan 91.18 89.74 91.08 91.78 91.54 99.07 98.45 0.053 0.169 0.051 0.109
Dut 85.57 82.33 84.81 85.81 85.57 98.19 97.93 0.035 0.120 0.046 0.048
Por 92.11 90.68 92.57 93.03 92.11 99.65 99.31 0.047 0.257 0.077 0.103
Slo 85.61 82.39 84.89 86.21 85.61 90.55 95.27 0.158 0.295 0.054 0.130
Swe 90.60 88.79 90.10 91.36 90.60 98.71 98.97 0.035 0.141 0.036 0.055
Tur 76.34 75.66 77.14 77.55 76.36 98.72 99.04 0.021 0.047 0.016 0.036
Eng1 91.16 89.20 91.18 91.59 ? 98.65 99.18 0.082 0.200 0.032 0.076
Eng2 ? 90.29 92.03 92.57 ? 98.96 99.12 0.081 0.168 0.032 0.076
Sm08 MST Sib G+S ? CertS CertG TimeS TimeG TrainS TrainG
Dan 86.5 87.89 89.58 91.00 ? 98.50 98.50 0.043 0.120 0.053 0.065
Dut 88.5 88.86 90.87 91.76 ? 98.00 99.50 0.036 0.046 0.050 0.054
Mc06 MST Sib G+S ? CertS CertG TimeS TimeG TrainS TrainG
PTB 91.5 90.10 91.96 92.46 ? 98.89 98.63 0.062 0.210 0.028 0.078
PDT 85.2 84.36 86.44 87.32 ? 96.67 96.43 0.063 0.221 0.019 0.051
Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our first-
order baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via
dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al (2009). Sm08:
The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald
and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task
(Buchholz and Marsi, 2006) or in any column of Martins et al (2009, Table 1); note that the latter includes McDonald
and Pereira (2006), Nivre and McDonald (2008), and Martins et al (2008). CertS/CertG: Percent of test examples
for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for
test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing,
test decoding was carried out on identical machines with zero additional load; however, training was conducted on
machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1: UAS when testing on
the CoNLL-08 validation set, following Martins et al (2009). Eng2: UAS when testing on the CoNLL-08 test set.
(2009), we project fractional solutions to a well-
formed spanning tree, as described in that paper.
Finally, to better compare the tightness of our
LP relaxation to that of earlier work, we consider
randomly-generated instances. Table 2 gives results
for our model and the LP relaxations of Martins et al
(2009) with randomly generated scores on automata
transitions. We again recover exact solutions more
often than the Martins et al relaxations. Note that
with random parameters the percentage of exact so-
lutions is significantly lower, suggesting that the ex-
actness of decoding of the trained models is a special
case. We speculate that this is due to the high perfor-
mance of approximate decoding with Z in place of
Y under the trained models for fi; the training algo-
rithm described in section 6 may have the tendency
to make the LP relaxation tight.
7.3 Speed
Table 1, columns TimeS and TimeG, shows decod-
ing times for the dual decomposition algorithms.
Table 2 gives speed comparisons to Martins et al
(2009). Our method gives significant speed-ups over
 0
 5
 10
 15
 20
 25
 30
 0  1000  2000  3000  4000  5000%
 of 
He
ad 
Au
tom
ata
 Re
com
put
ed
Iterations of Dual Decomposition
% recomputed, g+s% recomputed, sib
Figure 3: The average percentage of head automata that
must be recomputed on each iteration of dual decompo-
sition on the PTB validation set.
the Martins et al (2009) method, presumably be-
cause it leverages the underlying structure of the
problem, rather than using a generic solver.
7.4 Lazy Decoding
Here we describe an important optimization in the
dual decomposition algorithms. Consider the algo-
rithm in Figure 1. At each iteration we must find
z(k)|i = argmax
z|i?Zi
(fi(z|i)?
?
j
u(k)(i, j)z(i, j))
1295
Sib Acc Int Time Rand
LP(S) 92.14 88.29 0.14 11.7
LP(M) 92.17 93.18 0.58 30.6
ILP 92.19 100.0 1.44 100.0
DD-5000 92.19 98.82 0.08 35.6
DD-250 92.23 89.29 0.03 10.2
G+S Acc Int Time Rand
LP(S) 92.60 91.64 0.23 0.0
LP(M) 92.58 94.41 0.75 0.0
ILP 92.70 100.0 1.79 100.0
DD-5000 92.71 98.76 0.23 6.8
DD-250 92.66 85.47 0.12 0.0
Table 2: A comparison of dual decomposition with lin-
ear programs described by Martins et al (2009). LP(S):
Linear Program relaxation based on single-commodity
flow. LP(M): Linear Program relaxation based on
multi-commodity flow. ILP: Exact Integer Linear Pro-
gram. DD-5000/DD-250: Dual decomposition with non-
projective head automata, with K = 5000/250. Upper
results are for the sibling model, lower results are G+S.
Columns give scores for UAS accuracy, percentage of so-
lutions which are integral, and solution speed in seconds
per sentence. These results are for Section 22 of the PTB.
The last column is the percentage of integral solutions on
a random problem of length 10 words. The (I)LP experi-
ments were carried out using Gurobi, a high-performance
commercial-grade solver.
for i = 0 . . . n. However, if for some i, u(k)(i, j) =
u(k?1)(i, j) for all j, then z(k)|i = z
(k?1)
|i . In
lazy decoding we immediately set z(k)|i = z
(k?1)
|i if
u(k)(i, j) = u(k?1)(i, j) for all j; this check takes
O(n) time, and saves us from decoding with the i?th
automaton. In practice, the updates to u are very
sparse, and this condition occurs very often in prac-
tice. Figure 3 demonstrates the utility of this method
for both sibling automata and G+S automata.
7.5 Early Stopping
We also ran experiments varying the value of K?
the maximum number of iterations?in the dual de-
composition algorithms. As before, if we do not find
y(k) = z(k) for some value of k ? K, we choose
the y(k) with optimal value for f(y(k)) as the final
solution. Figure 4 shows three graphs: 1) the accu-
racy of the parser on PTB validation data versus the
value for K; 2) the percentage of examples where
y(k) = z(k) at some point during the algorithm,
hence the algorithm returns a certificate of optimal-
ity; 3) the percentage of examples where the solution
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
Pe
rce
nta
ge
Maximum Number of Dual Decomposition Iterations
% validation UAS% certificates% match K=5000
Figure 4: The behavior of the dual-decomposition parser
with sibling automata as the value of K is varied.
Sib P-Sib G+S P-G+S
PTB 92.19 92.34 92.71 92.70
PDT 86.41 85.67 87.40 86.43
Table 3: UAS of projective and non-projective decoding
for the English (PTB) and Czech (PDT) validation sets.
Sib/G+S: as in Table 1. P-Sib/P-G+S: Projective versions
of Sib/G+S, where the MST component has been re-
placed with the Eisner (2000) first-order projective parser.
returned is the same as the solution for the algorithm
with K = 5000 (our original setting). It can be seen
for K as small as 250 we get very similar accuracy
to K = 5000 (see Table 2). In fact, for this set-
ting the algorithm returns the same solution as for
K = 5000 on 99.59% of the examples. However
only 89.29% of these solutions are produced with a
certificate of optimality (y(k) = z(k)).
7.6 How Good is the Approximation z??
We ran experiments measuring the quality of z? =
argmaxz?Z f(z), where f(z) is given by the
perceptron-trained bigram-sibling model. Because
z? may not be a well-formed tree with n dependen-
cies, we report precision and recall rather than con-
ventional dependency accuracy. Results on the PTB
validation set were 91.11%/88.95% precision/recall,
which is accurate considering the unconstrained na-
ture of the predictions. Thus the z? approximation is
clearly a good one; we suspect that this is one reason
for the good convergence results for the method.
7.7 Importance of Non-Projective Decoding
It is simple to adapt the dual-decomposition algo-
rithms in figures 1 and 2 to give projective depen-
dency structures: the set Y is redefined to be the set
1296
of all projective structures, with the argmax over Y
being calculated using a projective first-order parser
(Eisner, 2000). Table 3 shows results for projec-
tive and non-projective parsing using the dual de-
composition approach. For Czech data, where non-
projective structures are common, non-projective
decoding has clear benefits. In contrast, there is little
difference in accuracy between projective and non-
projective decoding on English.
8 Conclusions
We have described dual decomposition algorithms
for non-projective parsing, which leverage existing
dynamic programming and MST algorithms. There
are a number of possible areas for future work. As
described in section 7.7, the algorithms can be easily
modified to consider projective structures by replac-
ing Y with the set of projective trees, and then using
first-order dependency parsing algorithms in place
of MST decoding. This method could be used to
derive parsing algorithms that include higher-order
features, as an alternative to specialized dynamic
programming algorithms. Eisner (2000) describes
extensions of head automata to include word senses;
we have not discussed this issue in the current pa-
per, but it is simple to develop dual decomposition
algorithms for this case, using similar methods to
those used for the grandparent models. The gen-
eral approach should be applicable to other lexical-
ized syntactic formalisms, and potentially also to de-
coding in syntax-driven translation. In addition, our
dual decomposition approach is well-suited to paral-
lelization. For example, each of the head-automata
could be optimized independently in a multi-core or
GPU architecture. Finally, our approach could be
used with other structured learning algorithms, e.g.
Meshi et al (2010).
A Implementation Details
This appendix describes details of the algorithm,
specifically choice of the step sizes ?k, and use of
the ?(i, j) parameters.
A.1 Choice of Step Sizes
We have found the following method to be effec-
tive. First, define ? = f(z(1)) ? f(y(1)), where
(z(1), y(1)) is the output of the algorithm on the first
iteration (note that we always have ? ? 0 since
f(z(1)) = L(u(1))). Then define ?k = ?/(1 + ?k),
where ?k is the number of times that L(u(k
?)) >
L(u(k
??1)) for k? ? k. Hence the learning rate drops
at a rate of 1/(1+ t), where t is the number of times
that the dual increases from one iteration to the next.
A.2 Use of the ?(i, j) Parameters
The parsing algorithms both consider a general-
ized problem that includes ?(i, j) parameters. We
now describe how these can be useful. Re-
call that the optimization problem is to solve
argmaxz?Z,y?Y f(z) + h(y), subject to a set of
agreement constraints. In our models, f(z) can
be written as f ?(z) +
?
i,j ?(i, j)z(i, j) where
f ?(z) includes only terms depending on higher-
order (non arc-factored features), and ?(i, j) are
weights that consider the dependency between i
and j alone. For any value of 0 ? ? ?
1, the problem argmaxz?Z,y?Y f2(z) + h2(y) is
equivalent to the original problem, if f2(z) =
f ?(z) + (1 ? ?)
?
i,j ?(i, j)z(i, j) and h2(y) =
?
?
i,j ?(i, j)y(i, j). We have simply shifted the
?(i, j) weights from one model to the other. While
the optimization problem remains the same, the al-
gorithms in Figure 1 and 2 will converge at differ-
ent rates depending on the value for ?. In our ex-
periments we set ? = 0.001, which puts almost
all the weight in the head-automata models, but al-
lows weights on spanning tree edges to break ties in
MST inference in a sensible way. We suspect this is
important in early iterations of the algorithm, when
many values for u(i, j) or v(i, j) will be zero, and
where with ? = 0 many spanning tree solutions y(k)
would be essentially random, leading to very noisy
updates to the u(i, j) and v(i, j) values. We have
not tested other values for ?.
Acknowledgments MIT gratefully acknowledges the
support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government. A. Rush was supported by the GALE program of
the DARPA, Contract No. HR0011-06-C-0022. D. Sontag was
supported by a Google PhD Fellowship.
1297
References
H. Alshawi. 1996. Head Automata and Bilingual Tiling:
Translation with Minimal Representations. In Proc.
ACL, pages 167?176.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
CoNLL, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. EMNLP-
CoNLL, pages 957?961.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proc. EMNLP, pages
1?8.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007. Us-
ing Combinatorial Optimization within Max-Product
Belief Propagation. In NIPS, pages 369?376.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
T. Finley and T. Joachims. 2008. Training structural
svms when exact inference is intractable. In ICML,
pages 304?311.
A.K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. Handbook of Formal Languages: Beyond
Words, 3:69?123.
N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF
Optimization via Dual Decomposition: Message-
Passing Revisited. In Proc. ICCV.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured Prediction Models via the Matrix-Tree The-
orem. In Proc. EMNLP-CoNLL, pages 141?150.
B.H. Korte and J. Vygen. 2008. Combinatorial Opti-
mization: Theory and Algorithms. Springer Verlag.
A. Kulesza and F. Pereira. 2008. Structured learning
with approximate inference. In NIPS.
C. Lemare?chal. 2001. Lagrangian Relaxation. In Com-
putational Combinatorial Optimization, Optimal or
Provably Near-Optimal Solutions [based on a Spring
School], pages 112?156, London, UK. Springer-
Verlag.
A.F.T. Martins, D. Das, N.A. Smith, and E.P. Xing. 2008.
Stacking Dependency Parsers. In Proc. EMNLP,
pages 157?166.
A.F.T. Martins, N.A. Smith., and E.P. Xing. 2009. Con-
cise Integer Linear Programming Formulations for De-
pendency Parsing. In Proc. ACL, pages 342?350.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proc. EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the Complexity of
Non-Projective Data-Driven Dependency Parsing. In
Proc. IWPT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-Projective Dependency Parsing using Spanning
Tree Algorithms. In Proc. HLT-EMNLP, pages 523?
530.
O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson.
2010. Learning Efficiently with Approximate Infer-
ence via Dual Losses. In Proc. ICML.
A. Nedic? and A. Ozdaglar. 2009. Approximate
Primal Solutions and Rate Analysis for Dual Sub-
gradient Methods. SIAM Journal on Optimization,
19(4):1757?1780.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers. In
Proc. ACL, pages 950?958.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and Inference over Constrained Output. In
Proc. IJCAI, pages 1124?1129.
S. Riedel and J. Clarke. 2006. Incremental Integer Linear
Programming for Non-projective Dependency Parsing.
In Proc. EMNLP, pages 129?137.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On Dual Decomposition and Linear Program-
ming Relaxations for Natural Language Processing. In
Proc. EMNLP.
D.A. Smith and J. Eisner. 2008. Dependency Parsing by
Belief Propagation. In Proc. EMNLP, pages 145?156.
D.A. Smith and N.A. Smith. 2007. Probabilistic Mod-
els of Nonprojective Dependency Trees. In Proc.
EMNLP-CoNLL, pages 132?140.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP Relaxations for MAP
using Message Passing. In Proc. UAI.
M. Steedman. 2000. The Syntactic Process. MIT Press.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies.
In Proc. CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697?3717.
1298
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1013?1024,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Greed is Good if Randomized: New Inference for Dependency Parsing
Yuan Zhang
?
, Tao Lei
?
, Regina Barzilay, and Tommi Jaakkola
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{yuanzh, taolei, regina, tommi}@csail.mit.edu
Abstract
Dependency parsing with high-order fea-
tures results in a provably hard decoding
problem. A lot of work has gone into
developing powerful optimization meth-
ods for solving these combinatorial prob-
lems. In contrast, we explore, analyze, and
demonstrate that a substantially simpler
randomized greedy inference algorithm al-
ready suffices for near optimal parsing: a)
we analytically quantify the number of lo-
cal optima that the greedy method has to
overcome in the context of first-order pars-
ing; b) we show that, as a decoding algo-
rithm, the greedy method surpasses dual
decomposition in second-order parsing; c)
we empirically demonstrate that our ap-
proach with up to third-order and global
features outperforms the state-of-the-art
dual decomposition and MCMC sampling
methods when evaluated on 14 languages
of non-projective CoNLL datasets.
1
1 Introduction
Dependency parsing is typically guided by param-
eterized scoring functions that involve rich fea-
tures exerting refined control over the choice of
parse trees. As a consequence, finding the high-
est scoring parse tree is a provably hard combina-
torial inference problem (McDonald and Pereira,
2006). Much of the recent work on parsing has
focused on solving these problems using powerful
optimization techniques. In this paper, we follow a
different strategy, arguing that a much simpler in-
ference strategy suffices. In fact, we demonstrate
that a randomized greedy method of inference sur-
passes the state-of-the-art performance in depen-
dency parsing.
?
Both authors contributed equally.
1
Our code is available at https://github.com/
taolei87/RBGParser.
Our choice of a randomized greedy algorithm
for parsing follows from a successful track record
of such methods in other hard combinatorial prob-
lems. These conceptually simple and intuitive
algorithms have delivered competitive approxi-
mations across a broad class of NP-hard prob-
lems ranging from set cover (Hochbaum, 1982) to
MAX-SAT (Resende et al., 1997). Their success
is predicated on the observation that most realiza-
tions of problems are much easier to solve than the
worst-cases. A simpler algorithm will therefore
suffice in typical cases. Evidence is accumulating
that parsing problems may exhibit similar proper-
ties. For instance, methods such as dual decom-
position offer certificates of optimality when the
highest scoring tree is found. Across languages,
dual decomposition has shown to lead to a cer-
tificate of optimality for the vast majority of the
sentences (Koo et al., 2010; Martins et al., 2011).
These remarkable results suggest that, as a com-
binatorial problem, parsing appears simpler than
its broader complexity class would suggest. In-
deed, we show that a simpler inference algorithm
already suffices for superior results.
In this paper, we introduce a randomized greedy
algorithm that can be easily used with any rich
scoring function. Starting with an initial tree
drawn uniformly at random, the algorithm makes
only local myopic changes to the parse tree in an
attempt to climb the objective function. While a
single run of the hill-climbing algorithm may in-
deed get stuck in a locally optimal solution, mul-
tiple random restarts can help to overcome this
problem. The same algorithm is used both for
learning the parameters of the scoring function as
well as for parsing test sentences.
The success of a randomized greedy algorithm
is tied to the number of local maxima in the search
space. When the number is small, only a few
restarts will suffice for the greedy algorithm to
find the highest scoring parse. We provide an al-
1013
gorithm for explicitly counting the number of lo-
cal optima in the context of first-order parsing,
and demonstrate that the number is typically quite
small. Indeed, we find that a first-order parser
trained with exact inference or using our random-
ized greedy algorithm delivers basically the same
performance.
We hypothesize that parsing with high-order
scoring functions exhibits similar properties. The
main rationale is that, even in the presence of high-
order features, the resulting scoring function re-
mains first-order dominant. The performance of
a simple arc-factored first-order parser is only a
few percentage points behind higher-order parsers.
The higher-order features in the scoring function
offer additional refinement but only a few changes
above and beyond the first-order result. As a
consequence, most of the arc choices are already
determined by a much simpler, polynomial time
parser.
We use dual decomposition to show that the
greedy method indeed succeeds as an inference al-
gorithm even with higher-order scoring functions.
In fact, with second-order features, regardless of
which method was used for training, the random-
ized greedy method outperforms dual decomposi-
tion by finding higher scoring trees. For the sen-
tences that dual decomposition is optimal (obtains
a certificate), the greedy method finds the same
solution in over 99% of the cases. Our simple
inference algorithm is therefore likely to scale to
higher-order parsing and we demonstrate empiri-
cally that this is indeed so.
We validate our claim by evaluating the method
on the CoNLL dependency benchmark that com-
prises treebanks from 14 languages. Aver-
aged across all languages, our method out-
performs state-of-the-art parsers, including Tur-
boParser (Martins et al., 2013) and our earlier
sampling-based parser (Zhang et al., 2014). On
seven languages, we report the best published re-
sults. The method is not sensitive to initialization.
In fact, drawing the initial tree uniformly at ran-
dom results in the same performance as when ini-
tialized from a trained first-order distribution. In
contrast, sufficient randomization of the starting
point is critical. Only a small number of restarts
suffices for finding (near) optimal parse trees.
2 Related Work
Finding Optimal Structure in Parsing The use
of rich-scoring functions in dependency parsing
inevitably leads to the challenging combinatorial
problem of finding the maximizing parse. In fact,
McDonald and Pereira (2006) demonstrated that
the task is provably NP-hard for non-projective
second-order parsing. Not surprisingly, approx-
imate inference has been at the center of pars-
ing research. Examples of these approaches in-
clude easy-first parsing (Goldberg and Elhadad,
2010), inexact search (Johansson and Nugues,
2007; Zhang and Clark, 2008; Huang et al., 2012;
Zhang et al., 2013), partial dynamic program-
ming (Huang and Sagae, 2010) and dual decom-
position (Koo et al., 2010; Martins et al., 2011).
Our work is most closely related to the MCMC
sampling-based approaches (Nakagawa, 2007;
Zhang et al., 2014). In our earlier work, we devel-
oped a method that learns to take guided stochas-
tic steps towards a high-scoring parse (Zhang et
al., 2014). In the heart of that technique are so-
phisticated samplers for traversing the space of
trees. In this paper, we demonstrate that a sub-
stantially simpler approach that starts from a tree
drawn from the uniform distribution and uses hill-
climbing for parameter updates achieves similar or
higher performance.
Another related greedy inference method has
been used for non-projective dependency pars-
ing (McDonald and Pereira, 2006). This method
relies on hill-climbing to convert the highest scor-
ing projective tree into its non-projective approxi-
mation. Our experiments demonstrate that when
hill-climbing is employed as a primary learning
mechanism for high-order parsing, it exhibits dif-
ferent properties: the distribution for initialization
does not play a major role in the final outcome,
while the use of restarts contributes significantly
to the quality of the resulting tree.
Greedy Approximations for NP-hard Problems
There is an expansive body of research on greedy
approximations for NP-hard problems. Examples
of NP-hard problems with successful greedy ap-
proximations include the traveling saleman prob-
lem problem (Held and Karp, 1970; Rego et
al., 2011), the MAX-SAT problem (Mitchell et
al., 1992; Resende et al., 1997) and vertex
cover (Hochbaum, 1982). While some greedy
methods have poor worst-case complexity, many
1014
of them work remarkably well in practice. Despite
the apparent simplicity of these algorithms, un-
derstanding their properties is challenging: often
their ?theoretical analyses are negative and incon-
clusive? (Amenta and Ziegler, 1999; Spielman and
Teng, 2001). Identifying conditions under which
approximations are provably optimal is an active
area of research in computer science theory (Du-
mitrescu and T?oth, 2013; Jonsson et al., 2013).
In NLP, randomized and greedy approximations
have been successfully used across multiple ap-
plications, including machine translation and lan-
guage modeling (Brown et al., 1993; Ravi and
Knight, 2010; Daum?e III et al., 2009; Moore and
Quirk, 2008; Deoras et al., 2011). In this paper,
we study the properties of these approximations in
the context of dependency parsing.
3 Method
3.1 Preliminaries
Let x be a sentence and T (x) be the set of possi-
ble dependency trees over the words in x. We use
y ? T (x) to denote a dependency tree for x, and
y(m) to specify the head (parent) of the modifier
word indexed by m in tree y. We also use m to
denote the indexed word when there is no ambi-
guity. In addition, we define T (y,m) as the set
of ?neighboring trees? of y obtained by changing
only the head of the modifier, i.e. y(m).
The dependency trees are scored according to
S(x, y) = ? ? ?(x, y), where ? is a vector of pa-
rameters and ?(x, y) is a sparse feature vector rep-
resentation of tree y for sentence x. In this work,
?(x, y) will include up to third-order features as
well as a range of global features commonly used
in re-ranking methods (Collins, 2000; Charniak
and Johnson, 2005; Huang, 2008).
The parameters ? in the scoring function are
estimated on the basis of a training set D =
{(x?
i
, y?
i
)}
N
i=1
of sentences x?
i
and the correspond-
ing gold (target) trees y?
i
. We adopt a max-margin
framework for this learning problem. Specifically,
we aim to find parameter values that score the gold
target trees higher than others:
?i ? {1, ? ? ? , N}, y ? T (x?
i
),
S(x?
i
, y?
i
) ? S(x?
i
, y) + ?y?
i
? y?
1
? ?
i
where ?
i
? 0 is the slack variable (non-zero values
are penalized against) and ?y?
i
? y?
1
is the ham-
ming distance between the gold tree y?
i
and a can-
didate parse y.
In an online learning setup, parameters are up-
dated successively after each sentence. Each up-
date still requires us to find the ?strongest viola-
tion?, i.e., a candidate tree y? that scores higher
than the gold tree y?
i
:
y? = argmax
y?T (x?
i
)
{S(x?
i
, y) + ?y ? y?
i
?
1
}
The parameters are then revised so as to select
against the offending y?. Instead of a standard
parameter update based on y? as in perceptron,
stochastic gradient descent, or passive-aggressive
updates, our implementation follows Lei et al.
(2014) where the first-order parameters are broken
up into a tensor. Each tensor component is updated
successively in combination with the parameters
corresponding to MST features (McDonald et al.,
2005) and higher-order features (when included).
2
3.2 Algorithm
During training and testing, the key combinatorial
problem we must solve is that of decoding, i.e.,
finding the highest scoring tree y? ? T (x) for each
sentence x (or x?
i
). In our notation,
y? = argmax
y?T (x?
i
)
{? ? ?(x?
i
, y) + ?y ? y?
i
?
1
} (train)
y? = argmax
y?T (x)
{? ? ?(x, y)} (test)
While the decoding problem with feature sets sim-
ilar to ours has been shown to be NP-hard, many
approximation algorithms work remarkably well.
We commence with a motivating example.
Locality and Parsing One possible reason for
why greedy or other approximation algorithms
work well for dependency parsing is that typical
sentences and therefore the learned scoring func-
tions S(x, y) = ? ? ?(x, y) are primarily ?lo-
cal?. By this we mean that head-modifier deci-
sions could be made largely without considering
the surrounding structure (the context). For exam-
ple, in English an adjective and a determiner are
typically attached to the following noun.
We demonstrate the degree of locality in de-
pendency parsing by comparing a first-order tree-
based parser to the parser that predicts each head
word independently of others. Note that the in-
dependent prediction of dependency arcs does not
necessarily give rise to a tree. The parameters of
2
We refer the readers to Lei et al. (2014) for more details
about the tensor scoring function and the online update.
1015
Dataset Indp. Pred Tree Pred
Slovene 83.7 84.2
Arabic 79.0 79.2
Japanese 93.4 93.7
English 91.6 91.9
Average 86.9 87.3
Table 1: Head attachment accuracy of a first-order
local classifier (left) and a first-order structural
prediction model (right). The two types of mod-
els are trained using the same set of features.
Input: parameter ?, sentence x
Output: dependency tree y?
1: Randomly initialize tree y
(0)
;
2: t = 0;
3: repeat
4: list = bottom-up node list of y
(t)
;
5: for each word m in list do
6: y
(t+1)
= argmax
y?T (y
(t)
,m)
S(x, y);
7: t = t+ 1;
8: end for
9: until no change in this iteration
10: return y? = y
(t)
;
Figure 1: A randomized hill-climbing algorithm
for dependency parsing.
the two parsers, the independent prediction and
a tree-based parser, are trained separately with
the corresponding decoding algorithm but with the
same feature set.
Table 1 shows that the accuracy of the inde-
pendent prediction ranges from 79% to 93% on
four CoNLL datasets. The results are on par with
the first-order structured prediction model. This
experiment reinforces the conclusion in Liang et
al. (2008), where a local classifier was shown
to achieve comparable accuracy to a sequential
model (e.g. CRF) in POS tagging and named-
entity recognition.
Hill-Climbing with Random Restarts We
build here on the motivating example and explore
greedy algorithms as generalizations of purely lo-
cal decoding. Greedy algorithms break the decod-
ing problem into a sequence of simple local steps,
each required to improve the solution. In our case,
simple local steps correspond to choosing the head
for each modifier word.
We begin with a tree y
(0)
, which can be a sam-
ple drawn uniformly from T (x) (Wilson, 1996).
Our greedy algorithm then updates y
(t)
to a bet-
ter tree y
(t+1)
by revising the head of one modifier
word while maintaining the constraint that the re-
sulting structure is a tree. The modifiers are con-
sidered in the bottom-up order relative to the cur-
rent tree (the word furthest from the root is consid-
ered first). We provide an analysis to motivate this
bottom-up update strategy in Section 4.1. The al-
gorithm continues until the score can no longer be
improved by changing the head of a single word.
The resulting tree represents a locally optimal pre-
diction relative to a single-arc greedy algorithm.
Figure 1 gives the algorithm in pseudo-code.
There are many possible variations of the sim-
ple randomized greedy hill-climbing algorithm.
First, the Wilson sampling algorithm (Wilson,
1996) can be naturally extended to obtain i.i.d.
samples from any first-order distributions. There-
fore, we could initialize the tree y
(0)
with a tree
from a first-order parser, or draw the initial tree
from a first-order distribution other than uniform.
However, perhaps surprisingly, as we demon-
strate later, little is lost with uniform initializa-
tion. Second, since a single run of randomized
hill-climbing is relatively cheap and runs are in-
dependent to each other, it is easy to execute mul-
tiple runs independently in parallel. The final pre-
dicted tree is then simply the highest scoring tree
across the multiple runs. We demonstrate that only
a small number of parallel runs are necessary for
near optimal prediction.
4 Analysis
4.1 First-Order Parsing
We provide here a firmer basis for why the ran-
domized greedy algorithm can be expected to
work. While the focus of the rest of the paper
is on higher-order parsing, we limit ourselves in
this subsection to first-order parsing. The reasons
for this are threefold. First, a simple greedy algo-
rithm is already not guaranteed a priori to work in
the context of a first-order scoring function. The
conclusions from this analysis are therefore likely
to carry over to higher-order parsing scenarios as
well. Second, a first-order arc-factored scoring
provides us an easy way to ascertain when the ran-
domized greedy algorithm indeed found the high-
est scoring tree. Finally, we are able to count the
1016
Dataset Average Len.
# of local optima at percentile fraction of finding global optima (%)
50% 70% 90% 0 <Len.? 15 Len.> 15
Turkish 12.1 1 1 2 100 100
Slovene 15.9 2 20 3647 100 98.1
English 24.0 21 121 2443 100 99.3
Arabic 36.8 2 35 >10000 100 99.1
Table 2: The left part of the table shows the local optimum statistics of the first-order model. The
sentences are sorted by the number of local optima. Columns 3 to 5 show the number of local optima of
a sentence at different percentile of the sorted list. For example, on English 50% of the sentences have
no more than 21 local optimum trees. The right part shows the fraction of finding global optima using
300 uniform restarts for each sentence.
number of locally optimal solutions for a greedy
algorithm in the context of first-order parsing and
can therefore relate this property to the success
rates of the algorithm.
Reachability We begin by highlighting a basic
property of trees, namely that single arc changes
suffice for transforming any tree to any other tree
in a small number of steps while maintaining that
each intermediate structure is also a tree. In this
sense, a target tree is reachable from any start-
ing point using only single arc changes. More
formally, let y be any starting tree and y
?
the de-
sired target. Let m
1
,m
2
, ? ? ? ,m
n
be the bottom-
up list of words (modifiers) corresponding to tree
y, where m
1
is the word furthest from the root.
We can simply change each head y(m
i
) to that of
y
?
(m
i
) in this order i = 1, . . . , n. The bottom-up
order guarantees that no cycle is introduced with
respect to the remaining (yet unmodified) nodes of
y. The fact that y
?
is a valid tree implies no cycle
will appear with respect to the already modified
nodes.
Note that, according to this property, any tree
is reachable from any starting point using only k
modifications, where k is the number of head dif-
ferences, i.e. k = |{m : y(m) 6= y
?
(m)}|. The
result also suggests that it may be helpful to per-
form the greedy steps in the bottom-up order, a
suggestion that we follow in our implementation.
Broadly speaking, we have established that
the greedy algorithm is not inherently limited by
virtue of its basic steps. Of course, it is a differ-
ent question whether the scoring function supports
such local changes towards the correct target tree.
Locally Optimal Trees While greedy algo-
rithms are notoriously prone to getting stuck in
locally optimal solutions, we establish here that
Function CountOptima(G = ?V,E?)
V = {w
0
, w
1
, ? ? ? , w
n
} where w
0
is the
root
E = {e
ij
? R} are the arc scores
Return: the number of local optima
1: Let y(0) = ? and y(i) = argmax
j
e
ji
;
2: if y is a tree (no cycle) then return 1;
3: Find a cycle C ? V in y;
4: count = 0;
// contract the cycle
5: create a vertex w
?
;
6: ?j /? C : e
?j
= max
k?C
e
kj
;
7: for each vertex w
i
? C do
8: ?j /? C : e
j?
= e
ji
;
9: V
?
= V ? {w
?
} \ C;
10: E
?
= E ? {e
?j
, e
j?
| ?j /? C}
11: count += CountOptima(G
?
= ?V
?
, E
?
?);
12: end for
13: return count;
Figure 2: A recursive algorithm for counting lo-
cal optima for a sentence with words w
1
, ? ? ? , w
n
(first-order parsing). The algorithm resembles the
Chu-Liu-Edmonds algorithm for finding the max-
imum directed spanning tree (Chu and Liu, 1965).
decoding with learned scoring functions involves
only a small number of local optima. In our case,
a local optimum corresponds to a tree y where no
single change of head y(m) results in a higher
scoring tree. Clearly, the highest scoring tree is
also a local optimum in this sense. If there were
many such local optima, finding the one with the
highest score would be challenging for a greedy
algorithm, even with randomization.
We begin with a worst case analysis and estab-
1017
Dataset
Trained with Hill-Climbing (HC) Trained with Dual Decomposition (DD)
%Cert (DD) s
DD
>s
HC
s
DD
=s
HC
s
DD
<s
HC
%Cert (DD) s
DD
>s
HC
s
DD
=s
HC
s
DD
<s
HC
Turkish 98.7 0.0 99.8 0.2 98.7 0.0 100.0 0.0
Slovene 94.5 0.0 98.7 1.3 92.3 0.2 99.0 0.8
English 94.5 0.3 98.7 1.0 94.6 0.5 98.7 0.8
Arabic 78.8 3.4 93.9 2.7 75.3 4.7 88.4 6.9
Table 3: Decoding quality comparison between hill-climbing (HC) and dual decomposition (DD). Mod-
els are trained either with HC (left) or DD (right). s
HC
denotes the score of the tree retrieved by HC
and s
DD
gives the analogous score for DD. The columns show the percentage of all test sentences for
which one method succeeds in finding a higher or the same score. ?Cert? column gives the percentage
of sentences for which DD finds a certificate.
lish a tight upper bound on the number of local
optima for a first-order scoring function.
Theorem 1 For any first-order scoring function
that factorizes into the sum of arc scores S(x, y) =
?
S
arc
(y(m),m): (a) the number of locally op-
timal trees is at most 2
n?1
for n words; (b) this
upper bound is tight.
3
While the number of possible dependency trees
is (n + 1)
n?1
(Cayley?s formula), the number of
local optima is at most 2
n?1
. This is still too many
for longer sentences, suggesting that, in the worst
case, a randomized greedy algorithm is unlikely to
find the highest scoring tree. However, the scor-
ing functions we learn for dependency parsing are
considerably easier.
Average Case Analysis In contrast to the worst-
case analysis above, we will count here the actual
number of local optima per sentence for a first-
order scoring function learned from data with the
randomized greedy algorithm. Figure 2 provides
pseudo-code for our counting algorithm. The al-
gorithm is derived by tailoring the proof of Theo-
rem 1 to each sentence.
Table 2 shows the empirical number of locally
optimal trees estimated by our algorithm across 4
different languages. Decoding with trained scor-
ing functions in the average case is clearly sub-
stantially easier than the worst case. For exam-
ple, on the English test set more than 70% of the
sentences have at most 121 locally optimal trees.
Since the average sentence length is 24, the dis-
crepancy between the typical number (e.g., 121)
and the worst case (2
24?1
) is substantial. As a re-
sult, only a small number of restarts is likely to
suffice for finding optimal trees in practice.
Optimal Decoding We can easily verify
whether the randomized greedy algorithm indeed
3
A proof sketch is given in Appendix.
succeeds in finding the highest scoring trees with
a learned first-order scoring function. We have
established above that there are typically only a
small number of locally optimal trees. We would
therefore expect the algorithm to work. We show
the results in the second part of Table 2. For short
sentences of length up to 15, our method finds the
global optimum for all the test sentences. Success
rates remain high even for longer test sentences.
4.2 Higher-Order Parsing
Exact decoding with high-order features is known
to be provably hard (McDonald et al., 2005). We
begin our analysis here with a second-order (sib-
ling/grandparent) model, and compare our ran-
domized hill-climbing (HC) method to dual de-
composition (DD), re-implementing Koo et al.
(2010). Table 3 compares decoding quality for the
two methods across four languages. Overall, in
97.8% of the sentences, HC obtains the same score
as DD, in 1.3% of the cases HC finds a higher
scoring tree, and in 0.9% of cases DD results in
a better tree. The results follow the same pattern
regardless of which method was used to train the
scoring function. The average rate of certificates
for DD was 92%. In over 99% of these sentences,
HC reaches the same optimum.
We expect that these observations about the suc-
cess of HC carry over to other high-order parsing
models for several reasons. First, a large num-
ber of arcs are pruned in the initial stage, con-
siderably reducing the search space and minimiz-
ing the number of possible locally optimal trees.
Second, many dependencies can be determined
already with independent arc prediction (see our
motivating example above), predictions that are
readily achieved with a greedy algorithm. Finally,
high-order features represent smaller refinements,
i.e., suggest only a few changes above and be-
yond the dominant first-order scores. Greedy al-
1018
gorithms are therefore likely to be able to leverage
at least some of this potential. We demonstrate be-
low that this is indeed so.
Our methods are trained within the max-margin
framework. As a result, we are expected to find
the highest scoring competing tree for each train-
ing sentence (the ?strongest violation?). One may
question therefore whether possible sub-optimal
decoding for some training sentences (finding ?a
violation? rather than the ?strongest violation?)
impacts the learned parser. To this end, Huang et
al. (2012) have established that weaker violations
do suffice for separable training sets.
5 Experimental Setup
Dataset and Evaluation Measures We evalu-
ate our model on CoNLL dependency treebanks
for 14 different languages (Buchholz and Marsi,
2006; Surdeanu et al., 2008), using standard train-
ing and testing splits. We use part-of-speech tags
and the morphological information provided in the
corpus. Following standard practice, we use Unla-
beled Attachment Score (UAS) excluding punctu-
ation (Koo et al., 2010; Martins et al., 2013) as the
evaluation metric in all our experiments.
Baselines We compare our model with the Tur-
boParser (Martins et al., 2013) and our earlier
sampling-based parser (Zhang et al., 2014). For
both parsers, we directly compare with the re-
cent published results on the CoNLL datasets.
We also compare our parser against the best pub-
lished results for the individual languages in our
datasets. This comparison set includes four ad-
ditional parsers: Martins et al. (2011), Koo et al.
(2010), Zhang et al. (2013) and our tensor-based
parser (Lei et al., 2014).
Features We use the same feature templates as
in our prior work (Zhang et al., 2014; Lei et al.,
2014)
4
. Figure 3 shows the first- to third-order
feature templates that we use in our model. For
the global features we use right-branching, coor-
dination, PP attachment, span length, neighbors,
valency and non-projective arcs features.
Implementation Details Following standard
practices, we train our model using the passive-
aggressive online learning algorithm (MIRA)
and parameter averaging (Crammer et al., 2006;
4
We refer the readers to Zhang et al. (2014) and Lei et al.
(2014) for the detailed definition of each feature template.
arc!
head bigram!!h h m m+1h m consecutive sibling!h m s grandparent!g h mgrand-sibling!g h m s
tri-siblings!h m s t grand-grandparent!g h mgg
outer-sibling-grandchild!h m sgc h s gcminner-sibling-grandchild!
Figure 3: First- to third-order features.
Arabic Slovene English Chinese German
?2
?1
0
1
2
3
4
5 Len ? 15Len > 15
Figure 4: Absolute UAS improvement of our full
model over the first-order model. Sentences in the
test set are divided into 2 groups based on their
lengths.
Collins, 2002). By default we use an adaptive
strategy for running the hill-climbing algorithm
? for a given sentence we repeatedly run the al-
gorithm in parallel
5
until the best tree does not
change for K = 300 consecutive restarts. For
each restart, by default we initialize the tree y
(0)
by sampling from the first-order distribution us-
ing the current learned parameter values (and first-
order scores). We train our first-order and third-
order model for 10 epochs and our full model for
20 epochs for all languages, and report the average
performance across three independent runs.
6 Results
Comparison with the Baselines Table 4 sum-
marizes the results of our model, along with the
state-of-the-art baselines. On average across 14
languages, our full model with the tensor com-
ponent outperforms both TurboParser and the
sampling-based parser. The direct comparison
5
We use 8 threads in all the experiments.
1019
Our Model
Exact 1st
Turbo Sampling
Best Published
1st 3rd Full
w/o tensor
Full (MA13) (ZL14)
Arabic 78.98 79.95 79.38 80.24 79.22 79.64 80.12 81.12 (MS11)
Bulgarian 92.15 93.38 93.69 93.72 92.24 93.10 93.30 94.02 (ZH13)
Chinese 91.20 93.00 92.76 93.04 91.17 89.98 92.63 92.68 (LX14)
Czech 87.65 90.11 90.34 90.77 87.82 90.32 91.04 91.04 (ZL14)
Danish 90.50 91.43 91.66 91.86 90.56 91.48 91.80 92.00 (ZH13)
Dutch 84.49 86.43 87.04 87.39 84.79 86.19 86.47 86.47 (ZL14)
English 91.85 93.01 93.20 93.25 91.94 93.22 92.94 93.22 (MA13)
German 90.52 91.91 92.64 92.67 90.54 92.41 92.07 92.41 (MA13)
Japanese 93.78 93.80 93.35 93.56 93.74 93.52 93.42 93.74 (LX14)
Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10)
Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11)
Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14)
Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13)
Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10)
Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58
Table 4: Results of our model and several state-of-the-art systems. ?Best Published UAS? includes the
most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et
al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set
of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser
(Zhang et al., 2014) and tensor features (Lei et al., 2014).
Dataset
MAP-1st Uniform Rnd-1st
UAS Init. UAS Init. UAS Init.
Slovene 85.2 80.1 86.7 13.7 86.7 34.2
Arabic 78.8 75.1 79.7 12.4 80.2 32.8
English 91.1 82.0 93.3 39.6 93.3 55.6
Chinese 87.2 75.3 93.2 36.8 93.0 54.5
Dutch 84.8 79.5 87.0 26.9 87.4 45.6
Average 85.4 78.4 88.0 25.9 88.1 44.5
Table 5: Comparison between different initializa-
tion strategies: (a) MAP-1st: only the MAP tree
of the first-order score; (b) Uniform: random trees
are sampled from the uniform distribution; and
(c) Rnd-1st: random trees are sampled from the
first-order distribution. For each method, the table
shows the average accuracy of the initial tree and
the final parsing accuracy.
with TurboParser is achieved by restricting our
model to third order features which still outper-
forms TurboParser (89.10% vs 88.72%). To com-
pare against the sampling-based parser, we em-
ploy our model without the tensor component. The
two models achieve a similar average performance
(89.24% and 89.23% respectively). Since relative
parsing performance depends on a target language,
we also include comparison with the best pub-
lished results. The model achieves the best pub-
lished results for seven languages.
Another noteworthy comparison concerns first-
order parsers. As Table 4 shows, the exact and ap-
proximate versions of the first-order parser deliver
almost identical performance.
Impact of High-Order Features Table 4 shows
that the model can effectively utilize high-order
features. Comparing the average performance of
the model variants, we see that the accuracy on
the benchmark languages consistently improves
when higher-order features are added. This char-
acteristic of the randomized greedy parser is in
line with findings about other state-of-the-art high-
order parsers (Martins et al., 2013; Zhang et al.,
2014). Figure 4 breaks down these gains based
on the sentence length. As expected, on most lan-
guages high-order features are particularly helpful
when parsing longer sentences.
Impact of Initialization and Restarts Table 5
shows the impact of initialization on the model
performance for several languages. We consider
three strategies: the MAP estimate of the first-
order score from the model, uniform sampling and
sampling from the first-order distribution. The ac-
curacy of initial trees varies greatly, ranging from
78.4% for the MAP estimate to 25.9% and 44.5%
for the latter randomized strategies. However, the
resulting parsing accuracy is not determined by
the initial accuracy. In fact, the two sampling
strategies result in almost identical parsing perfor-
mance. While the first-order MAP estimate gives
the best initial guess, the overall parsing accuracy
of this method lags behind. This result demon-
strates the importance of restarts ? in contrast to
the randomized strategies, the MAP initialization
performs only a single run of hill-climbing.
1020
Length ? 15 Length > 15
Slovene 100 98.11
English 100 99.12
Table 6: Fractions (%) of the sentences that find
the best solution among 3,000 restarts within the
first 300 restarts.
0 200 400 600 800 1000
0.994
0.996
0.998
1
# Restarts
Scor
e
 
 
len?15len>15
(a) Slovene
0 200 400 600 800 1000
0.994
0.996
0.998
1
# Restarts
Scor
e
 
 
len?15len>15
(b) English
Figure 5: Convergence analysis on Slovene and
English datasets. The graph shows the normalized
score of the output tree as a function of the number
of restarts. The score of each sentence is normal-
ized by the highest score obtained for this sentence
after 3,000 restarts. We only show the curves up to
1,000 restarts because they all reach convergence
after around 500 restarts.
Convergence Properties Figure 5 shows the
score of the trees retrieved by our full model with
respect to the number of restarts, for short and long
sentences in English and Slovene. To facilitate the
comparison, we normalize the score of each sen-
tence by the maximal score obtained for this sen-
tence after 3,000 restarts. Overall, most sentences
converge quickly. This view is also supported by
Table 6 which shows the fraction of the sentences
that converge within the first 300 restarts. We can
see that all the short sentences (length up to 15)
reach convergence within the allocated restarts.
Perhaps surprisingly, more than 98% of the long
sentences also converge within 300 restarts.
Decoding Speed As the number of restarts im-
pacts the parsing accuracy, we can trade perfor-
mance for speed. Figure 6 shows that the model
2 4 6 8 10 12 14x 10?3
82
84
86
88
Sec/Tok
UAS
 
 
3rd?order ModelFull Model
(a) Slovene
2 4 6 8 10x 10?3
88
90
92
94
Sec/Tok
UAS
 
 
3rd?order ModelFull Model
(b) English
Figure 6: Trade-off between performance and
speed on Slovene and English datasets. The graph
shows the accuracy as a function of decoding
speed measured in second per token. Variations in
decoding speed is achieved by changing the num-
ber of restarts.
achieves high performance with acceptable pars-
ing speed. While various system implementation
issues such as programming language and com-
putational platform complicate a direct compari-
son with other parsing systems, our model deliv-
ers parsing time roughly comparable to other state-
of-the-art graph-based systems (for example, Tur-
boParser and MST parser) and the sampling-based
parser.
7 Conclusions
We have shown that a simple, generally appli-
cable randomized greedy algorithm for inference
suffices to deliver state-of-the-art parsing perfor-
mance. We argued that the effectiveness of such
greedy algorithms is contingent on having a small
number of local optima in the scoring function. By
algorithmically counting the number of locally op-
timal solutions in the context of first-order parsing,
we show that this number is indeed quite small.
Moreover, we show that, as a decoding algorithm,
the greedy method surpasses dual decomposition
in second-order parsing. Finally, we empirically
demonstrate that our approach with up to third-
order and global features outperforms the state-of-
the-art parsers when evaluated on 14 languages of
1021
non-projective CoNLL datasets.
Appendix
We provide here a more detailed justification for
the counting algorithm in Figure 2 and, by exten-
sion, a proof sketch of Theorem 1. The bullets
below follow the operation of the algorithm.
? Whenever independent selection of the heads
results in a valid tree, there is only 1 opti-
mum (Lines 1&2 of the algorithm). Other-
wise there must be a cycle C in y (Line 3 of
the algorithm)
? We claim that any locally optimal tree y
?
of
the graph G = (V,E) must contain |C| ? 1
arcs of the cycle C ? V . This can be shown
by contradiction. If y
?
contains less than
|C| ? 1 arcs of C, then (a) we can construct
a tree y
??
that contains |C| ? 1 arcs; (b) the
heads in y
??
are strictly better than those in
y
?
over the unused part of the cycle; (c) by
reachability, there is a path y
?
? y
??
so y
?
cannot be a local optimum.
? Any locally optimal tree in G must select an
arc inC and reassign it. The rest of the |C|?1
arcs will then result in a chain.
? By contracting cycle C we obtain a new
graph G
?
of size |G| ? |C| + 1 (Lines 5-11
of the algorithm). Easy to verify that (not
shown): any local optimum in G
?
is a local
optimum in G and vice versa.
The theorem follows as a corollary of these
steps. To see this, let F (G
m
) be the number of
local optima in the graph of size m:
F (G
m
) ? max
C?V (G)
?
i
F (G
(i)
m?c+1
)
where G
(i)
m?c+1
is the graph (of size m ? c + 1)
created by selecting the i
th
arc in cycleC and con-
tracting G
m
accordingly, and c = |C| is the size
of the cycle. Define
?
F (m) as the upper bound of
F (G
m
) for any graph of size m. By the above
formula, we know that
?
F (m) ? max
2?c<m
?
F (m? c+ 1)? c
By solving for
?
F (m) we get
?
F (m) ? 2
m?2
. Since
m = n+1 for a sentence with n words, the upper-
bound of local optima is 2
n?1
.
To show the tightness, for any n > 0, create
the graph G
n+1
with arc scores e
ij
= e
ji
= i for
any 0 ? i < j ? n. Note that w
n
? w
n?1
?
w
n
forms the circle C of size 2, it can be shown
by induction on n and F (G
n+1
) that F (G
n+1
) =
F (G
n
)? 2 = 2
n?1
.
Acknowledgments
This research is developed in collaboration with
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the IYAS project. The authors acknowl-
edge the support of the U.S. Army Research Of-
fice under grant number W911NF-10-1-0533, and
of the DARPA BOLT program. We thank the MIT
NLP group and the ACL reviewers for their com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
References
Nina Amenta and G?unter Ziegler, 1999. Deformed
Products and Maximal Shadows of Polytopes. Con-
temporary Mathematics. American Mathematics So-
ciety.
Peter F. Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Scientia
Sinica, 14(10):1396.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, ICML ?00, pages 175?182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
1022
Language Processing - Volume 10, EMNLP ?02. As-
sociation for Computational Linguistics.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
learning, 75(3):297?325.
Anoop Deoras, Tom?a?s Mikolov, and Kenneth Church.
2011. A fast re-scoring strategy to capture long dis-
tance dependencies. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 1116?1127. Associa-
tion for Computational Linguistics.
Adrian Dumitrescu and Csaba D T?oth. 2013. The trav-
eling salesman problem for lines, balls and planes.
In SODA, pages 828?843. SIAM.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750. Association for Computa-
tional Linguistics.
Michael Held and Richard M. Karp. 1970. The
traveling-salesman problem and minimum spanning
trees. Operations Research, 18(6):1138?1162.
Dorit S. Hochbaum. 1982. Approximation algo-
rithms for the set covering and vertex cover prob-
lems. SIAM Journal on Computing, 11(3):555?556.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077?
1086. Association for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151. Association for Computational Linguis-
tics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
EMNLP-CoNLL, pages 1134?1138.
Peter Jonsson, Victor Lagerkvist, Gustav Nordh, and
Bruno Zanuttini. 2013. Complexity of sat problems,
clone theory and the exponential time hypothesis. In
SODA, pages 1264?1277. SIAM.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Percy Liang, Hal Daum?e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592?599. ACM.
Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M?ario A. T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11.
Association for Computational Linguistics.
Andr?e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
David Mitchell, Bart Selman, and Hector Levesque.
1992. Hard and easy distributions of sat problems.
In AAAI, volume 92, pages 459?465. Citeseer.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statis-
tical machine translation. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 585?592. Association
for Computational Linguistics.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In EMNLP-CoNLL,
pages 952?956.
Sujith Ravi and Kevin Knight. 2010. Does giza++
make search errors? Computational Linguistics,
36(3):295?302.
C?esar Rego, Dorabela Gamboa, Fred Glover, and Colin
Osterman. 2011. Traveling salesman problem
heuristics: leading methods, implementations and
latest advances. European Journal of Operational
Research, 211(3):427?441.
1023
Mauricio G. C. Resende, L. S. Pitsoulis, and P. M.
Pardalos. 1997. Approximate solution of weighted
max-sat problems using grasp. Satisfiability prob-
lems, 35:393?405.
Daniel Spielman and Shang-Hua Teng. 2001.
Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. In Pro-
ceedings of the thirty-third annual ACM symposium
on Theory of computing, pages 296?305. ACM.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, CoNLL ?08. Association for
Computational Linguistics.
David B. Wilson. 1996. Generating random spanning
trees more quickly than the cover time. In Proceed-
ings of the twenty-eighth annual ACM symposium on
Theory of computing, pages 296?303. ACM.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562?571. Association for Computa-
tional Linguistics.
Hao Zhang, Liang Zhao, Kai Huang, and Ryan Mc-
Donald. 2013. Online learning for inexact hyper-
graph search. In Proceedings of EMNLP.
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Jaakkola, and Amir Globerson. 2014. Steps to ex-
cellence: Simple inference with refined scoring of
dependency trees. In Proceedings of the 52th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.
1024
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 197?207,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Steps to Excellence: Simple Inference with Refined Scoring of
Dependency Trees
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola
Massachusetts Institute of Technology
{yuanzh, taolei, regina, tommi}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
Much of the recent work on depen-
dency parsing has been focused on solv-
ing inherent combinatorial problems as-
sociated with rich scoring functions. In
contrast, we demonstrate that highly ex-
pressive scoring functions can be used
with substantially simpler inference pro-
cedures. Specifically, we introduce a
sampling-based parser that can easily han-
dle arbitrary global features. Inspired
by SampleRank, we learn to take guided
stochastic steps towards a high scoring
parse. We introduce two samplers for
traversing the space of trees, Gibbs and
Metropolis-Hastings with Random Walk.
The model outperforms state-of-the-art re-
sults when evaluated on 14 languages
of non-projective CoNLL datasets. Our
sampling-based approach naturally ex-
tends to joint prediction scenarios, such
as joint parsing and POS correction. The
resulting method outperforms the best re-
ported results on the CATiB dataset, ap-
proaching performance of parsing with
gold tags.
1
1 Introduction
Dependency parsing is commonly cast as a max-
imization problem over a parameterized scoring
function. In this view, the use of more expres-
sive scoring functions leads to more challenging
combinatorial problems of finding the maximiz-
ing parse. Much of the recent work on parsing has
been focused on improving methods for solving
the combinatorial maximization inference prob-
lems. Indeed, state-of-the-art results have been ob-
1
The source code for the work is available at
http://groups.csail.mit.edu/rbg/code/
global/acl2014.
tained by adapting powerful tools from optimiza-
tion (Martins et al, 2013; Martins et al, 2011;
Rush and Petrov, 2012). We depart from this view
and instead focus on using highly expressive scor-
ing functions with substantially simpler inference
procedures. The key ingredient in our approach is
how learning is coupled with inference. Our com-
bination outperforms the state-of-the-art parsers
and remains comparable even if we adopt their
scoring functions.
Rich scoring functions have been used for some
time. They first appeared in the context of rerank-
ing (Collins, 2000), where a simple parser is used
to generate a candidate list which is then reranked
according to the scoring function. Because the
number of alternatives is small, the scoring func-
tion could in principle involve arbitrary (global)
features of parse trees. The power of this method-
ology is nevertheless limited by the initial set of
alternatives from the simpler parser. Indeed, the
set may already omit the gold parse. We dispense
with the notion of a candidate set and seek to ex-
ploit the scoring function more directly.
In this paper, we introduce a sampling-based
parser that places few or no constraints on the
scoring function. Starting with an initial candi-
date tree, our inference procedure climbs the scor-
ing function in small (cheap) stochastic steps to-
wards a high scoring parse. The proposal distri-
bution over the moves is derived from the scoring
function itself. Because the steps are small, the
complexity of the scoring function has limited im-
pact on the computational cost of the procedure.
We explore two alternative proposal distributions.
Our first strategy is akin to Gibbs sampling and
samples a new head for each word in the sentence,
modifying one arc at a time. The second strat-
egy relies on a provably correct sampler for first-
order scores (Wilson, 1996), and uses it within a
Metropolis-Hastings algorithm for general scoring
functions. It turns out that the latter optimizes the
197
score more efficiently than the former.
Because the inference procedure is so simple,
it is important that the parameters of the scoring
function are chosen in a manner that facilitates
how we climb the scoring function in small steps.
One way to achieve this is to make sure that im-
provements in the scoring functions are correlated
with improvements in the quality of the parse.
This approach was suggested in the SampleRank
framework (Wick et al, 2011) for training struc-
tured prediction models. This method was origi-
nally developed for a sequence labeling task with
local features, and was shown to be more effec-
tive than state-of-the-art alternatives. Here we ap-
ply SampleRank to parsing, applying several mod-
ifications such as the proposal distributions men-
tioned earlier.
The benefits of sampling-based learning go be-
yond stand-alone parsing. For instance, we can
use the framework to correct preprocessing mis-
takes in features such as part-of-speech (POS)
tags. In this case, we combine the scoring func-
tion for trees with a stand-alone tagging model.
When proposing a small move, i.e., sampling a
head of the word, we can also jointly sample its
POS tag from a set of alternatives provided by
the tagger. As a result, the selected tag is influ-
enced by a broad syntactic context above and be-
yond the initial tagging model and is directly opti-
mized to improve parsing performance. Our joint
parsing-tagging model provides an alternative to
the widely-adopted pipeline setup.
We evaluate our method on benchmark multi-
lingual dependency corpora. Our method outper-
forms the Turbo parser across 14 languages on av-
erage by 0.5%. On four languages, we top the best
published results. Our method provides a more
effective mechanism for handling global features
than reranking, outperforming it by 1.3%. In terms
of joint parsing and tagging on the CATiB dataset,
we nearly bridge (88.38%) the gap between in-
dependently predicted (86.95%) and gold tags
(88.45%). This is better than the best published
results in the 2013 SPMRL shared task (Seddah et
al., 2013), including parser ensembles.
2 Related Work
Earlier works on dependency parsing focused on
inference with tractable scoring functions. For in-
stance, a scoring function that operates over each
single dependency can be optimized using the
maximum spanning tree algorithm (McDonald et
al., 2005). It was soon realized that using higher
order features could be beneficial, even at the cost
of using approximate inference and sacrificing op-
timality. The first successful approach in this arena
was reranking (Collins, 2000; Charniak and John-
son, 2005) on constituency parsing. Reranking
can be combined with an arbitrary scoring func-
tion, and thus can easily incorporate global fea-
tures over the entire parse tree. Its main disadvan-
tage is that the output parse can only be one of the
few parses passed to the reranker.
Recent work has focused on more powerful in-
ference mechanisms that consider the full search
space (Zhang and McDonald, 2012; Rush and
Petrov, 2012; Koo et al, 2010; Huang, 2008). For
instance, Nakagawa (2007) deals with tractabil-
ity issues by using sampling to approximate
marginals. Another example is the dual decompo-
sition (DD) framework (Koo et al, 2010; Martins
et al, 2011). The idea in DD is to decompose the
hard maximization problem into smaller parts that
can be efficiently maximized and enforce agree-
ment among these via Lagrange multipliers. The
method is essentially equivalent to linear program-
ming relaxation approaches (Martins et al, 2009;
Sontag et al, 2011), and also similar in spirit to
ILP approaches (Punyakanok et al, 2004).
A natural approach to approximate global in-
ference is via search. For instance, a transition-
based parsing system (Zhang and Nivre, 2011)
incrementally constructs a parsing structure us-
ing greedy beam-search. Other approaches op-
erate over full trees and generate a sequence
of candidates that successively increase the
score (Daum?e III et al, 2009; Li et al, 2013;
Wick et al, 2011). Our work builds on one such
approach ? SampleRank (Wick et al, 2011), a
sampling-based learning algorithm. In SampleR-
ank, the parameters are adjusted so as to guide the
sequence of candidates closer to the target struc-
ture along the search path. The method has been
successfully used in sequence labeling and ma-
chine translation (Haddow et al, 2011). In this
paper, we demonstrate how to adapt the method
for parsing with rich scoring functions.
3 Sampling-Based Dependency Parsing
with Global Features
In this section, we introduce our novel sampling-
based dependency parser which can incorporate
198
arbitrary global features. We begin with the no-
tation before addressing the decoding and learning
algorithms. Finally, we extend our model to a joint
parsing and POS correction task.
3.1 Notations
We denote sentences by x and the corresponding
dependency trees by y ? Y(x). Here Y(x) is the
set of valid (projective or non-projective) depen-
dency trees for sentence x. We use x
j
to refer
to the jth word of sentence x, and h
j
to the head
word of x
j
. A training set of size N is given as a
set of pairs D = {(x
(i)
, y
(i)
)}
N
i=1
where y
(i)
is the
ground truth parse for sentence x
(i)
.
We parameterize the scoring function s(x, y) as
s(x, y) = ? ? f(x, y) (1)
where f(x, y) is the feature vector associated with
tree y for sentence x. We do not make any assump-
tions about how the feature function decomposes.
In contrast, most state-of-the-art parsers operate
under the assumption that the feature function de-
composes into a sum of simpler terms. For exam-
ple, in the second-order MST parser (McDonald
and Pereira, 2006), all the feature terms involve
arcs or consecutive siblings. Similarly, parsers
based on dual decomposition (Martins et al, 2011;
Koo et al, 2010) assume that s(x, y) decomposes
into a sum of terms where each term can be maxi-
mized over y efficiently.
3.2 Decoding
The decoding problem consists of finding a valid
dependency tree y ? Y(x) that maximizes the
score s(x, y) = ? ? f(x, y) with parameters ?.
For scoring functions that extend beyond first-
order arc preferences, finding the maximizing non-
projective tree is known to be NP-hard (McDonald
and Pereira, 2006). We find a high scoring tree
through sampling, and (later) learn the parameters
? so as to further guide this process.
Our sampler generates a sequence of depen-
dency structures so as to approximate independent
samples from
p(y|x, T, ?) ? exp (s(x, y)/T ) (2)
The temperature parameter T controls how con-
centrated the samples are around the maximum
of s(x, y) (e.g., see Geman and Geman (1984)).
Sampling from target distribution p is typically as
hard as (or harder than) that maximizing s(x, y).
Inputs: ?, x, T
0
(initial temperature), c (temperature
update rate), proposal distribution q.
Outputs: y
?
T ? T
0
Set y
0
to some random tree
y
?
? y
0
repeat
y
?
? q(?|x, y
t
, T, ?)
if s(x, y
?
) > s(x, y
?
) then
y
?
? y
?
? = min
[
1,
p(y
?
)q(y
t
|y
?
)
p(y
t
)q(y
?
|y
t
)
]
Sample Bernouli variable Z with P [Z = 1] = ?.
if Z = 0 then
y
t+1
? y
t
else
y
t+1
? y
?
t? t+ 1
T ? c ? T
until convergence
return y
?
Figure 1: Sampling-based algorithm for decoding
(i.e., approximately maximizing s(x, y)).
We follow here a Metropolis-Hastings sampling
algorithm (e.g., see Andrieu et al (2003)) and
explore different alternative proposal distributions
q(y
?
|x, y, ?, T ). The distribution q governs the
small steps that are taken in generating a sequence
of structures. The target distribution p folds into
the procedure by defining the probability that we
will accept the proposed move. The general struc-
ture of our sampling algorithm is given in Figure 1.
3.2.1 Gibbs Sampling
Perhaps the most natural choice of the proposal
distribution q is a conditional distribution from p.
This is feasible if we restrict the proposed moves
to only small changes in the current tree. In our
case, we choose a word j randomly, and then sam-
ple its head h
j
according to p with the constraint
that we obtain a valid tree (when projective trees
are sought, this constraint is also incorporated).
For this choice of q, the probability of accepting
the new tree (? in Figure 1) is identically one.
Thus new moves are always accepted.
3.2.2 Exact First-Order Sampling
One shortcoming of the Gibbs sampler is that it
only changes one variable (arc) at a time. This
usually leads to slow mixing, requiring more sam-
ples to get close to the parse with maximum
score. Ideally, we would change multiple heads
in the parse tree simultaneously, and sample those
choices from the corresponding conditional distri-
bution of p. While in general this is increasingly
difficult with more heads, it is indeed tractable if
199
Inputs: x, y
t
, ?, K (number of heads to change).
Outputs: y
?
for i = 1 to |x| do
inTree[i]? false
ChangeNode[i]? false
Set ChangeNode to true for K random nodes.
head[0]? ?1
for i = 1 to |x| do
u? i
while not inTree[u] do
if ChangeNode[u] then
head[u]? randomHead(u, ?)
else
head[u]? y
t
(u)
u? head[u]
if LoopExist(head) then
EraseLoop(head)
u? i
while not inTree[u] do
inTree[u]? true
u? head[u]
return Construct tree y
?
from the head array.
Figure 2: A proposal distribution q(y
?
|y
t
) based
on the random walk sampler of Wilson (1996).
The function randomHead samples a new head for
node u according to the first-order weights given
by ?.
the model corresponds to a first-order parser. One
such sampling algorithm is the random walk sam-
pler of Wilson (1996). It can be used to obtain
i.i.d. samples from distributions of the form:
p(y) ?
?
i?j?y
w
ij
, (3)
where y corresponds to a tree with a spcified root
and w
ij
is the exponential of the first-order score.
y is always a valid parse tree if we allow multiple
children of the root and do not impose projective
constraint. The algorithm in Wilson (1996) iter-
ates over all the nodes, and for each node performs
a random walk according to the weights w
ij
until
the walk creates a loop or hits a tree. In the first
case the algorithm erases the loop and continues
the walk. If the walk hits the current tree, the walk
path is added to form a new tree with more nodes.
This is repeated until all the nodes are included in
the tree. It can be shown that this procedure gen-
erates i.i.d. trees from p(y).
Since our features do not by design correspond
to a first-order parser, we cannot use the Wilson
algorithm as it is. Instead we use it as the proposal
function and sample a subset of the dependen-
cies from the first-order distribution of our model,
while fixing the others. In each step we uniformly
sample K nodes to update and sample their new
1!
2!
not?Monday? not ssssssssssss" ?""" wasloop erased!Black?Monday?was
ROOT! It! was! not! Black! Monday!
2!
1!
3!
ROOT! It! was! not! Black! Monday!
(b) walk path:!
(c) walk path:!
(a) original tree!
ROOT! It! was! not! Black! Monday!
Figure 3: An illustration of random walk sam-
pler. The index on each edge indicates its order on
each walk path. The heads of the red words are
sampled while others are fixed. The blue edges
represent the current walk path and the black ones
are already in the tree. Note that the walk direc-
tion is opposite to the dependency direction. (a)
shows the original tree before sampling; (b) and
(c) show the walk path and how the tree is gener-
ated in two steps. The loop not? Monday? not
in (b) is erased.
heads using the Wilson algorithm (in the experi-
ments we use K = 4). Note that blocked Gibbs
sampling would be exponential in K, and is thus
very slow already at K = 4. The procedure is de-
scribed in Figure 2 with a graphic illustration in
Figure 3.
3.3 Training
In this section, we describe how to learn the
adjustable parameters ? in the scoring function.
The parameters are learned in an on-line fash-
ion by successively imposing soft constraints be-
tween pairs of dependency structures. We intro-
duce both margin constraints and constraints per-
taining to successive samples generated along the
search path. We demonstrate later that both types
of constraints are essential.
We begin with the standard margin constraints.
An ideal scoring function would always rank the
gold parse higher than any alternative. Moreover,
alternatives that are far from the gold parse should
score even lower. As a result, we require that
s(x
(i)
, y
(i)
)? s(x
(i)
, y) ? ?(y
(i)
, y) ?y (4)
where ?(y
(i)
, y) is the number of head mistakes
in y relative to the gold parse y
(i)
. We adopt here
a shorthand Err(y) = ?(y
(i)
, y), where the de-
200
pendence on y
(i)
is implied from context. Note
that Equation 4 contains exponentially many con-
straints and cannot be enforced jointly for general
scoring functions. However, our sampling proce-
dure generates a small number of structures along
the search path. We enforce only constraints cor-
responding to those samples.
The second type of constraints are enforced be-
tween successive samples along the search path.
To illustrate the idea, consider a parse y that dif-
fers from y
(i)
in only one arc, and a parse y
?
that
differs from y
(i)
in ten arcs. We cannot necessarily
assume that s(x, y) is greater than s(x, y
?
) without
additional encouragement. Thus, we can comple-
ment the constraints in Equation 4 with additional
pairwise constraints (Wick et al, 2011):
s(x
(i)
, y)? s(x
(i)
, y
?
) ? Err(y
?
)? Err(y) (5)
where similarly to Equation 4, the difference in
scores scales with the differences in errors with re-
spect to the target y
(i)
. We only enforce the above
constraints for y, y
?
that are consecutive samples
in the course of the sampling process. These con-
straints serve to guide the sampling process de-
rived from the scoring function towards the gold
parse.
We learn the parameters ? in an on-line fashion
to satisfy the above constraints. This is done via
the MIRA algorithm (Crammer and Singer, 2003).
Specifically, if the current parameters are ?
t
, and
we enforce constraint Equation 5 for a particular
pair y, y
?
, then we will find ?
t+1
that minimizes
min ||? ? ?
t
||
2
+ C?
s.t. ? ? (f(x, y)? f(x, y
?
)) ? Err(y
?
)? Err(y)? ?
(6)
The updates can be calculated in closed form. Fig-
ure 4 summarizes the learning algorithm. We re-
peatedly generate parses based on the current pa-
rameters ?
t
for each sentence x
(i)
, and use succes-
sive samples to enforce constraints in Equation 4
and Equation 5 one at a time.
3.4 Joint Parsing and POS Correction
It is easy to extend our sampling-based parsing
framework to joint prediction of parsing and other
labels. Specifically, when sampling the new heads,
we can also sample the values of other variables at
the same time. For instance, we can sample the
POS tag, the dependency relation or morphology
information. In this work, we investigate a joint
Inputs: D = {(x
(i)
, y
(i)
)}
N
i=1
.
Outputs: Learned parameters ?.
?
0
? 0
for e = 1 to #epochs do
for i = 1 toN do
y
?
? q(?|x
(i)
, y
t
i
i
, ?
t
)
y
+
= arg min
y?
{
y
t
i
i
,y
?
}
Err(y)
y
?
= arg max
y?
{
y
t
i
i
,y
?
}
Err(y)
y
t
i
+1
i
? acceptOrReject(y
?
, y
t
i
i
, ?
t
)
t
i
? t
i
+ 1
?f = f(x
(i)
, y
+
)? f(x
(i)
, y
?
)
?Err = Err(y
+
)? Err(y
?
)
if ?Err 6= 0 and ?
t
? ?f < ?Err then
?
t+1
? updateMIRA(?f,?Err, ?
t
)
t? t+ 1
?f
g
= f(x
(i)
, y
(i)
)? f(x
(i)
, y
t
i
i
)
if ?
t
? ?f
g
< Err(y
t
i
i
) then
?
t+1
? updateMIRA(?f
g
, Err(y
t
i
i
), ?
t
)
t? t+ 1
return Average of ?
0
, . . . , ?
t
parameters.
Figure 4: SampleRank algorithm for learning. The
rejection strategy is as in Figure 1. y
t
i
i
is the t
i
th
tree sample of x
(i)
. The first MIRA update (see
Equation 6) enforces a ranking constraint between
two sampled parses. The second MIRA update en-
forces constraints between a sampled parse and the
gold parse. In practice several samples are drawn
for each sentence in each epoch.
POS correction scenario in which only the pre-
dicted POS tags are provided in the testing phase,
while both gold and predicted tags are available
for the training set.
We extend our model such that it jointly learns
how to predict a parse tree and also correct the pre-
dicted POS tags for a better parsing performance.
We generate the POS candidate list for each word
based on the confusion matrix on the training set.
Let c(t
g
, t
p
) be the count when the gold tag is t
g
and the predicted one is t
p
. For each word w, we
first prune out its POS candidates by using the vo-
cabulary from the training set. We don?t prune
anything if w is unseen. Assuming that the pre-
dicted tag forw is t
p
, we further remove those tags
t if their counts are smaller than some threshold
c(t, t
p
) < ? ? c(t
p
, t
p
)
2
.
After generating the candidate lists for each
word, the rest of the extension is rather straight-
forward. For each sampling, let H be the set of
candidate heads and T be the set of candidate POS
tags. The Gibbs sampler will generate a new sam-
ple from the space H ? T . The other parts of the
algorithm remain the same.
2
In our work we choose ? = 0.003, which gives a 98.9%
oracle POS tagging accuracy on the CATiB development set.
201
arc!
head bigram!!h h m m+1arbitrary sibling!?!h m sh m consecutive sibling!h m s grandparent!g h m
grand-sibling!g h m s tri-siblings!h m s t grand-grandparent!g h mgg
outer-sibling-grandchild!h m sgc h s gcminner-sibling-grandchild!
Figure 5: First- to third-order features.
4 Features
First- to Third-Order Features The feature
templates of first- to third-order features are
mainly drawn from previous work on graph-
based parsing (McDonald and Pereira, 2006),
transition-based parsing (Nivre et al, 2006) and
dual decomposition-based parsing (Martins et al,
2011). As shown in Figure 5, the arc is the basic
structure for first-order features. We also define
features based on consecutive sibling, grandpar-
ent, arbitrary sibling, head bigram, grand-sibling
and tri-siblings, which are also used in the Turbo
parser (Martins et al, 2013). In addition to these
first- to third-order structures, we also consider
grand-grandparent and sibling-grandchild struc-
tures. There are two types of sibling-grandchild
structures: (1) inner-sibling when the sibling is
between the head and the modifier and (2) outer-
sibling for the other cases.
Global Features We used feature shown promis-
ing in prior reranking work Charniak and Johnson
(2005), Collins (2000) and Huang (2008).
? Right Branch This feature enables the model
to prefer right or left-branching trees. It counts
the number of words on the path from the root
node to the right-most non-punctuation word,
normalized by the length of the sentence.
? Coordination In a coordinate structure, the two
adjacent conjuncts usually agree with each other
on POS tags and their span lengths. For in-
stance, in cats and dogs, the conjuncts are both
short noun phrases. Therefore, we add differ-
ent features to capture POS tag and span length
consistency in a coordinate structure.
? PP Attachment We add features of lexical tu-
eat! with! knife! and! fork!
Figure 6: An example of PP attachment with coor-
dination. The arguments should be knife and fork,
not and.
ples involving the head, the argument and the
preposition of prepositional phrases. Generally,
this feature can be defined based on an instance
of grandparent structure. However, we also han-
dle the case of coordination. In this case, the ar-
guments should be the conjuncts rather than the
coordinator. Figure 6 shows an example.
? Span Length This feature captures the distribu-
tion of the binned span length of each POS tag.
It also includes flags of whether the span reaches
the end of the sentence and whether the span is
followed by the punctuation.
? Neighbors The POS tags of the neighboring
words to the left and right of each span, together
with the binned span length and the POS tag at
the span root.
? Valency We consider valency features for each
POS tag. Specifically, we add two types of va-
lency information: (1) the binned number of
non-punctuation modifiers and (2) the concate-
nated POS string of all those modifiers.
? Non-projective Arcs A flag indicating if a de-
pendency is projective or not (i.e. if it spans a
word that does not descend from its head) (Mar-
tins et al, 2011). This flag is also combined with
the POS tags or the lexical words of the head and
the modifier.
POS Tag Features In the joint POS correction
scenario, we also add additional features specifi-
cally for POS prediction. The feature templates
are inspired by previous feature-rich POS tagging
work (Toutanova et al, 2003). However, we are
free to add higher order features because we do
not rely on dynamic programming decoding. In
our work we use feature templates up to 5-gram.
Table 1 summarizes all POS tag feature templates.
5 Experimental Setup
Datasets We evaluate our model on standard
benchmark corpora ? CoNLL 2006 and CoNLL
2008 (Buchholz and Marsi, 2006; Surdeanu et al,
2008) ? which include dependency treebanks for
14 different languages. Most of these data sets
202
1-gram
?t
i
?, ?t
i
, w
i?2
?, ?t
i
, w
i?1
?, ?t
i
, w
i
?, ?t
i
, w
i+1
?,
?t
i
, w
i+2
?
2-gram
?t
i?1
, t
i
?, ?t
i?2
, t
i
?, ?t
i?1
, t
i
, w
i?1
?,
?t
i?1
, t
i
, w
i
?
3-gram
?t
i?1
, t
i
, t
i+1
?, ?t
i?2
, t
i
, t
i+1
, ?, ?t
i?1
, t
i
, t
i+2
?,
?t
i?2
, t
i
, t
i+2
?
4-gram
?t
i?2
, t
i?1
, t
i
, t
i+1
?, ?t
i?2
, t
i?1
, t
i
, t
i+2
?,
?t
i?2
, t
i
, t
i+1
, t
i+2
?
5-gram ?t
i?2
, t
i?1
, t
i
, t
i+1
, t
i+2
?
Table 1: POS tag feature templates. t
i
and w
i
de-
notes the POS tag and the word at the current posi-
tion. t
i?x
and t
i+x
denote the left and right context
tags, and similarly for words.
contain non-projective dependency trees. We use
all sentences in CoNLL datasets during training
and testing. We also use the Columbia Arabic
Treebank (CATiB) (Marton et al, 2013). CATiB
mostly includes projective trees. The trees are an-
notated with both gold and predicted versions of
POS tags and morphology information. Follow-
ing Marton et al (2013), for this dataset we use
12 core POS tags, word lemmas, determiner fea-
tures, rationality features and functional genders
and numbers.
Some CATiB sentences exceed 200 tokens. For
efficiency, we limit the sentence length to 70 to-
kens in training and development sets. However,
we do not impose this constraint during testing.
We handle long sentences during testing by apply-
ing a simple split-merge strategy. We split the sen-
tence based on the ending punctuation, predict the
parse tree for each segment and group the roots of
resulting trees into a single node.
Evaluation Measures Following standard prac-
tice, we use Unlabeled Attachment Score (UAS)
as the evaluation metric in all our experiments.
We report UAS excluding punctuation on CoNLL
datasets, following Martins et al (2013). For the
CATiB dataset, we report UAS including punctu-
ation in order to be consistent with the published
results in the 2013 SPMRL shared task (Seddah et
al., 2013).
Baselines We compare our model with the Turbo
parser and the MST parser. For the Turbo parser,
we directly compare with the recent published re-
sults in (Martins et al, 2013). For the MST parser,
we train a second-order non-projective model us-
ing the most recent version of the code
3
.
We also compare our model against a discrim-
inative reranker. The reranker operates over the
3
http://sourceforge.net/projects/mstparser/
top-50 list obtained from the MST parser
4
. We
use a 10-fold cross-validation to generate candi-
date lists for training. We then train the reranker
by running 10 epochs of cost-augmented MIRA.
The reranker uses the same features as our model,
along with the tree scores obtained from the MST
parser (which is a standard practice in reranking).
Experimental Details Following Koo and Collins
(2010), we always first train a first-order pruner.
For each word x
i
, we prune away the incoming
dependencies ?h
i
, x
i
? with probability less than
0.005 times the probability of the most likely head,
and limit the number of candidate heads up to 30.
This gives a 99% pruning recall on the CATiB
development set. The first-order model is also
trained using the algorithm in Figure 4. Af-
ter pruning, we tune the regularization parameter
C = {0.1, 0.01, 0.001} on development sets for
different languages. Because the CoNLL datasets
do not have a standard development set, we ran-
domly select a held out of 200 sentences from the
training set. We also pick the training epochs from
{50, 100, 150} which gives the best performance
on the development set for each language. After
tuning, the model is trained on the full training set
with the selected parameters.
We apply the Random Walk-based sampling
method (see Section 3.2.2) for the standard de-
pendency parsing task. However, for the joint
parsing and POS correction on the CATiB dataset
we do not use the Random Walk method because
the first-order features in normal parsing are no
longer first-order when POS tags are also vari-
ables. Therefore, the first-order distribution is not
well-defined and we only employ Gibbs sampling
for simplicity. On the CATiB dataset, we restrict
the sample trees to always be projective as de-
scribed in Section 3.2.1. However, we do not im-
pose this constraint for the CoNLL datasets.
6 Results
Comparison with State-of-the-art Parsers Ta-
ble 2 summarizes the performance of our model
and of the baselines. We first compare our model
to the Turbo parser using the Turbo parser fea-
ture set. This is meant to test how our learning
and inference methods compare to a dual decom-
position approach. The first column in Table 2
4
The MST parser is trained in projective mode for rerank-
ing because generating top-k list from second-order non-
projective model is intractable.
203
Our Model (UAS)
Turbo (UAS)
MST 2nd-Ord.
(UAS)
Best Published UAS
Top-50
Reranker
Top-500
RerankerTurbo Feat. Full Feat.
Arabic 79.86 80.21 79.64 78.75 81.12 (Ma11) 79.03 78.91
Bulgarian 92.97 93.30 93.10 91.56 94.02 (Zh13) 92.81 -
Chinese 92.06 92.63 89.98 91.77 91.89 (Ma10) 92.25 -
Czech 90.62 91.04 90.32 87.30 90.32 (Ma13) 88.14 -
Danish 91.45 91.80 91.48 90.50 92.00 (Zh13) 90.88 90.91
Dutch 85.83 86.47 86.19 84.11 86.19 (Ma13) 81.01 -
English 92.79 92.94 93.22 91.54 93.22 (Ma13) 92.41 -
German 91.79 92.07 92.41 90.14 92.41 (Ma13) 91.19 -
Japanese 93.23 93.42 93.52 92.92 93.72 (Ma11) 93.40 -
Portuguese 91.82 92.41 92.69 91.08 93.03 (Ko10) 91.47 -
Slovene 86.19 86.82 86.01 83.25 86.95 (Ma11) 84.81 85.37
Spanish 88.24 88.21 85.59 84.33 87.96 (Zh13) 86.85 87.21
Swedish 90.48 90.71 91.14 89.05 91.62 (Zh13) 90.53 -
Turkish 76.82 77.21 76.90 74.39 77.55 (Ko10) 76.35 76.23
Average 88.87 89.23 88.72 86.86 89.33 87.92 -
Table 2: Results of our model, the Turbo parser, and the MST parser. ?Best Published UAS? includes the
most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins
et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald
(2012) and Zhang et al (2013). Martins et al (2013) is the current Turbo parser. The last two columns
shows UAS of the discriminative reranker.
shows the result for our model with an average of
88.87%, and the third column shows the results
for the Turbo parser with an average of 88.72%.
This suggests that our learning and inference pro-
cedures are as effective as the dual decomposition
method in the Turbo parser.
Next, we add global features that are not used by
the Turbo parser. The performance of our model
is shown in the second column with an average of
89.23%. It outperforms the Turbo parser by 0.5%
and achieves the best reported performance on
four languages. Moreover, our model also outper-
forms the 88.80% average UAS reported in Mar-
tins et al (2011), which is the top performing sin-
gle parsing system (to the best of our knowledge).
Comparison with Reranking As column 6 of Ta-
ble 2 shows, our model outperforms the reranker
by 1.3%
5
. One possible explanation of this perfor-
mance gap between the reranker and our model is
the small number of candidates considered by the
reranker. To test this hypothesis, we performed
experiments with top-500 list for a subset of lan-
guages.
6
As column 7 shows, this increase in the
list size does not change the relative performance
of the reranker and our model.
Joint Parsing and POS Correction Table 3
shows the results of joint parsing and POS cor-
rection on the CATiB dataset, for our model and
5
Note that the comparison is conservative because we
can also add MST scores as features in our model as in
reranker. With these features our model achieves an average
UAS 89.28%.
6
We ran this experiment on 5 languages with small
datasets due to the scalability issues associated with rerank-
ing top-500 list.
state-of-the-art systems. As the upper part of the
table shows, the parser with corrected tags reaches
88.38% compared to the accuracy of 88.46% on
the gold tags. This is a substantial increase from
the parser that uses predicted tags (86.95%).
To put these numbers into perspective, the bot-
tom part of Table 3 shows the accuracy of the best
systems from the 2013 SPMRL shared task on
Arabic parsing using predicted information (Sed-
dah et al, 2013). Our system not only out-
performs the best single system (Bj?orkelund et
al., 2013) by 1.4%, but it also tops the ensem-
ble system that combines three powerful parsers:
the Mate parser (Bohnet, 2010), the Easy-First
parser (Goldberg and Elhadad, 2010) and the
Turbo parser (Martins et al, 2013)
Impact of Sampling Methods We compare two
sampling methods introduced in Section 3.2 with
respect to their decoding efficiency. Specifically,
we measure the score of the retrieved trees in test-
ing as a function of the decoding speed, measured
by the number of tokens per second. We change
the temperature update rate c in order to decode
with different speed. In Figure 7 we show the cor-
responding curves for two languages: Arabic and
Chinese. We select these two languages as they
correspond to two extremes in sentence length:
Arabic has the longest sentences on average, while
Chinese has the shortest ones. For both languages,
the tree score improves over time. Given sufficient
time, both sampling methods achieve the same
score. However, the Random Walk-based sam-
pler performs better when the quality is traded for
speed. This result is to be expected given that each
204
Dev. Set (? 70) Testing Set
POS Acc. UAS POS Acc. UAS
Gold - 90.27 - 88.46
Predicted 96.87 88.81 96.82 86.95
POS Correction 97.72 90.08 97.49 88.38
CADIM 96.87 87.4- 96.82 85.78
IMS-Single - - - 86.96
IMS-Ensemble - - - 88.32
Table 3: Results for parsing and corrective tagging
on the CATiB dataset. The upper part shows UAS
of our model with gold/predicted information or
POS correction. Bottom part shows UAS of the
best systems in the SPMRL shared task. IMS-
Single (Bj?orkelund et al, 2013) is the best single
parsing system, while IMS-Ensemble (Bj?orkelund
et al, 2013) is the best ensemble parsing system.
We also show results for CADIM (Marton et al,
2013), the second best system, because we use
their predicted features.
0 20 40 60 80 1002.648
2.65
2.652
2.654
2.656
2.658 x 104
Toks/sec
Score
 
 
GibbsRandom Walk
(a) Arabic
0 100 200 300 400 500 600 700 8001.897
1.898
1.899
1.9 x 10
4
Toks/sec
Score
 
 
GibbsRandom Walk
(b) Chinese
Figure 7: Total score of the predicted test trees as
a function of the decoding speed, measured in the
number of tokens per second.
iteration of this sampler makes multiple changes
to the tree, in contrast to a single-edge change of
Gibbs sampler.
The Effect of Constraints in Learning Our train-
ing method updates parameters to satisfy the pair-
wise constraints between (1) subsequent samples
on the sampling path and (2) selected samples and
the ground truth. Figure 8 shows that applying
both types of constraints is consistently better than
using either of them alone. Moreover, these re-
sults demonstrate that comparison between subse-
quent samples is more important than comparison
against the gold tree.
Decoding Speed Our sampling-based parser is an
Danish Japanese Portuguese Swedish89
90
91
92
93
94
UAS
(%)
 
 BothNeighborGold
Figure 8: UAS on four languages when train-
ing with different constraints. ?Neighbor? corre-
sponds to pairwise constraints between subsequent
samples, ?Gold? represents constraints between a
single sample and the ground truth, ?Both? means
applying both types of constraints.
anytime algorithm, and therefore its running time
can be traded for performance. Figure 7 illustrates
this trade-off. In the experiments reported above,
we chose a conservative cooling rate and contin-
ued to sample until the score no longer changed.
The parser still managed to process all the datasets
in a reasonable time. For example, the time that it
took to decode all the test sentences in Chinese and
Arabic were 3min and 15min, respectively. Our
current implementation is in Java and can be fur-
ther optimized for speed.
7 Conclusions
This paper demonstrates the power of combining a
simple inference procedure with a highly expres-
sive scoring function. Our model achieves the best
results on the standard dependency parsing bench-
mark, outperforming parsing methods with elabo-
rate inference procedures. In addition, this frame-
work provides simple and effective means for joint
parsing and corrective tagging.
Acknowledgments
This research is developed in collaboration with
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the IYAS project. The authors acknowledge
the support of the MURI program (W911NF-10-
1-0533, the DARPA BOLT program and the US-
Israel Binational Science Foundation (BSF, Grant
No 2012330). We thank the MIT NLP group and
the ACL reviewers for their comments.
205
References
Christophe Andrieu, Nando De Freitas, Arnaud
Doucet, and Michael I Jordan. 2003. An introduc-
tion to mcmc for machine learning. Machine learn-
ing, 50(1-2):5?43.
Anders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the SPMRL 2013 shared task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 135?
145, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In COLING,
pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, ICML ?00, pages 175?182.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
The Journal of Machine Learning Research, 3:951?
991.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
learning, 75(3):297?325.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. Pattern Analysis andMachine
Intelligence, IEEE Transactions on, (6):721?741.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750. Association for Computa-
tional Linguistics.
Barry Haddow, Abhishek Arun, and Philipp Koehn.
2011. Samplerank training for phrase-based ma-
chine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 261?
271. Association for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1?11. Association for
Computational Linguistics.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1288?1298. Association for Compu-
tational Linguistics.
Quannan Li, Jingdong Wang, Zhuowen Tu, and
David P Wipf. 2013. Fixed-point model for struc-
tured labeling. In Proceedings of the 30th Interna-
tional Conference on Machine Learning (ICML-13),
pages 214?221.
Andr?e FT Martins, Noah A Smith, and Eric P Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 342?350. Association for
Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M?ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approxi-
mate variational inference. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 34?44. Association for
Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Pedro MQ Aguiar,
and M?ario AT Figueiredo. 2011. Dual decompo-
sition with many overlapping components. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 238?249. As-
sociation for Computational Linguistics.
Andr?e FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Yuval Marton, Nizar Habash, Owen Rambow, and
Sarah Alkhulani. 2013. Spmrl13 shared task sys-
tem: The cadim arabic dependency parser. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 76?
80.
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL.
206
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 523?530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, pages 216?220. Association for
Computational Linguistics.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In EMNLP-CoNLL,
pages 952?956.
Joakim Nivre, Johan Hall, Jens Nilsson, G?uls?en Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 221?225. Association for Computational Lin-
guistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1346. Association for Computa-
tional Linguistics.
Alexander M Rush and Slav Petrov. 2012. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 498?507. Association for Computational Lin-
guistics.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D Choi, Rich?ard Farkas, Jennifer
Foster, Iakes Goenaga, Koldo Gojenola Gallete-
beitia, Yoav Goldberg, et al 2013. Overview of the
spmrl 2013 shared task: A cross-framework evalua-
tion of parsing morphologically rich languages. In
Proceedings of the Fourth Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
146?182.
D. Sontag, A. Globerson, and T. Jaakkola. 2011. In-
troduction to dual decomposition for inference. In
Optimization for Machine Learning, pages 219?254.
MIT Press.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 159?177. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Michael L. Wick, Khashayar Rohanimanesh, Kedar
Bellare, Aron Culotta, and Andrew McCallum.
2011. Samplerank: Training factor graphs with
atomic gradients. In Lise Getoor and Tobias Schef-
fer, editors, Proceedings of the 28th International
Conference on Machine Learning, ICML 2011,
pages 777?784.
David Bruce Wilson. 1996. Generating random span-
ning trees more quickly than the cover time. In
Proceedings of the twenty-eighth annual ACM sym-
posium on Theory of computing, pages 296?303.
ACM.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 320?331. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188?193. Association for Computational Linguis-
tics.
Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-
ald. 2013. Online learning for inexact hypergraph
search. In Proceedings of EMNLP.
207
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1381?1391,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Low-Rank Tensors for Scoring Dependency Structures
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{taolei, yuxin, yuanzh, regina, tommi}@csail.mit.edu
Abstract
Accurate scoring of syntactic structures
such as head-modifier arcs in dependency
parsing typically requires rich, high-
dimensional feature representations. A
small subset of such features is often se-
lected manually. This is problematic when
features lack clear linguistic meaning as
in embeddings or when the information is
blended across features. In this paper, we
use tensors to map high-dimensional fea-
ture vectors into low dimensional repre-
sentations. We explicitly maintain the pa-
rameters as a low-rank tensor to obtain low
dimensional representations of words in
their syntactic roles, and to leverage mod-
ularity in the tensor for easy training with
online algorithms. Our parser consistently
outperforms the Turbo and MST parsers
across 14 different languages. We also ob-
tain the best published UAS results on 5
languages.
1
1 Introduction
Finding an expressive representation of input sen-
tences is crucial for accurate parsing. Syntac-
tic relations manifest themselves in a broad range
of surface indicators, ranging from morphological
to lexical, including positional and part-of-speech
(POS) tagging features. Traditionally, parsing re-
search has focused on modeling the direct connec-
tion between the features and the predicted syntac-
tic relations such as head-modifier (arc) relations
in dependency parsing. Even in the case of first-
order parsers, this results in a high-dimensional
vector representation of each arc. Discrete fea-
tures, and their cross products, can be further com-
plemented with auxiliary information about words
1
Our code is available at https://github.com/
taolei87/RBGParser.
participating in an arc, such as continuous vector
representations of words. The exploding dimen-
sionality of rich feature vectors must then be bal-
anced with the difficulty of effectively learning the
associated parameters from limited training data.
A predominant way to counter the high dimen-
sionality of features is to manually design or select
a meaningful set of feature templates, which are
used to generate different types of features (Mc-
Donald et al, 2005a; Koo and Collins, 2010; Mar-
tins et al, 2013). Direct manual selection may be
problematic for two reasons. First, features may
lack clear linguistic interpretation as in distribu-
tional features or continuous vector embeddings of
words. Second, designing a small subset of tem-
plates (and features) is challenging when the rel-
evant linguistic information is distributed across
the features. For instance, morphological proper-
ties are closely tied to part-of-speech tags, which
in turn relate to positional features. These features
are not redundant. Therefore, we may suffer a per-
formance loss if we select only a small subset of
the features. On the other hand, by including all
the rich features, we face over-fitting problems.
We depart from this view and leverage high-
dimensional feature vectors by mapping them into
low dimensional representations. We begin by
representing high-dimensional feature vectors as
multi-way cross-products of smaller feature vec-
tors that represent words and their syntactic rela-
tions (arcs). The associated parameters are viewed
as a tensor (multi-way array) of low rank, and opti-
mized for parsing performance. By explicitly rep-
resenting the tensor in a low-rank form, we have
direct control over the effective dimensionality of
the set of parameters. We obtain role-dependent
low-dimensional representations for words (head,
modifier) that are specifically tailored for parsing
accuracy, and use standard online algorithms for
optimizing the low-rank tensor components.
The overall approach has clear linguistic and
1381
computational advantages:
? Our low dimensional embeddings are tailored
to the syntactic context of words (head, modi-
fier). This low dimensional syntactic abstrac-
tion can be thought of as a proxy to manually
constructed POS tags.
? By automatically selecting a small number of
dimensions useful for parsing, we can lever-
age a wide array of (correlated) features. Un-
like parsers such as MST, we can easily bene-
fit from auxiliary information (e.g., word vec-
tors) appended as features.
We implement the low-rank factorization model
in the context of first- and third-order depen-
dency parsing. The model was evaluated on 14
languages, using dependency data from CoNLL
2008 and CoNLL 2006. We compare our results
against the MST (McDonald et al, 2005a) and
Turbo (Martins et al, 2013) parsers. The low-rank
parser achieves average performance of 89.08%
across 14 languages, compared to 88.73% for the
Turbo parser, and 87.19% for MST. The power of
the low-rank model becomes evident in the ab-
sence of any part-of-speech tags. For instance,
on the English dataset, the low-rank model trained
without POS tags achieves 90.49% on first-order
parsing, while the baseline gets 86.70% if trained
under the same conditions, and 90.58% if trained
with 12 core POS tags. Finally, we demonstrate
that the model can successfully leverage word vec-
tor representations, in contrast to the baselines.
2 Related Work
Selecting Features for Dependency Parsing A
great deal of parsing research has been dedicated
to feature engineering (Lazaridou et al, 2013;
Marton et al, 2010; Marton et al, 2011). While
in most state-of-the-art parsers, features are se-
lected manually (McDonald et al, 2005a; McDon-
ald et al, 2005b; Koo and Collins, 2010; Mar-
tins et al, 2013; Zhang and McDonald, 2012a;
Rush and Petrov, 2012a), automatic feature selec-
tion methods are gaining popularity (Martins et al,
2011b; Ballesteros and Nivre, 2012; Nilsson and
Nugues, 2010; Ballesteros, 2013). Following stan-
dard machine learning practices, these algorithms
iteratively select a subset of features by optimizing
parsing performance on a development set. These
feature selection methods are particularly promis-
ing in parsing scenarios where the optimal feature
set is likely to be a small subset of the original set
of candidate features. Our technique, in contrast,
is suitable for cases where the relevant information
is distributed across a larger set of related features.
Embedding for Dependency Parsing A lot of
recent work has been done on mapping words into
vector spaces (Collobert and Weston, 2008; Turian
et al, 2010; Dhillon et al, 2011; Mikolov et al,
2013). Traditionally, these vector representations
have been derived primarily from co-occurrences
of words within sentences, ignoring syntactic roles
of the co-occurring words. Nevertheless, any such
word-level representation can be used to offset in-
herent sparsity problems associated with full lexi-
calization (Cirik and S?ensoy, 2013). In this sense
they perform a role similar to POS tags.
Word-level vector space embeddings have so
far had limited impact on parsing performance.
From a computational perspective, adding non-
sparse vectors directly as features, including their
combinations, can significantly increase the num-
ber of active features for scoring syntactic struc-
tures (e.g., dependency arc). Because of this is-
sue, Cirik and S?ensoy (2013) used word vectors
only as unigram features (without combinations)
as part of a shift reduce parser (Nivre et al, 2007).
The improvement on the overall parsing perfor-
mance was marginal. Another application of word
vectors is compositional vector grammar (Socher
et al, 2013). While this method learns to map
word combinations into vectors, it builds on ex-
isting word-level vector representations. In con-
trast, we represent words as vectors in a manner
that is directly optimized for parsing. This frame-
work enables us to learn new syntactically guided
embeddings while also leveraging separately esti-
mated word vectors as starting features, leading to
improved parsing performance.
Dimensionality Reduction Many machine
learning problems can be cast as matrix problems
where the matrix represents a set of co-varying
parameters. Such problems include, for example,
multi-task learning and collaborative filtering.
Rather than assuming that each parameter can be
set independently of others, it is helpful to assume
that the parameters vary in a low dimensional
subspace that has to be estimated together with the
parameters. In terms of the parameter matrix, this
corresponds to a low-rank assumption. Low-rank
constraints are commonly used for improving
1382
generalization (Lee and Seung, 1999; Srebro
et al, 2003; Srebro et al, 2004; Evgeniou and
Pontil, 2007)
A strict low-rank assumption can be restrictive.
Indeed, recent approaches to matrix problems de-
compose the parameter matrix as a sum of low-
rank and sparse matrices (Tao and Yuan, 2011;
Zhou and Tao, 2011). The sparse matrix is used to
highlight a small number of parameters that should
vary independently even if most of them lie on
a low-dimensional subspace (Waters et al, 2011;
Chandrasekaran et al, 2011). We follow this de-
composition while extending the parameter matrix
into a tensor.
Tensors are multi-way generalizations of ma-
trices and possess an analogous notion of rank.
Tensors are increasingly used as tools in spec-
tral estimation (Hsu and Kakade, 2013), includ-
ing in parsing (Cohen et al, 2012) and other NLP
problems (de Cruys et al, 2013), where the goal
is to avoid local optima in maximum likelihood
estimation. In contrast, we expand features for
parsing into a multi-way tensor, and operate with
an explicit low-rank representation of the associ-
ated parameter tensor. The explicit representa-
tion sidesteps inherent complexity problems asso-
ciated with the tensor rank (Hillar and Lim, 2009).
Our parameters are divided into a sparse set corre-
sponding to manually chosen MST or Turbo parser
features and a larger set governed by a low-rank
tensor.
3 Problem Formulation
We will commence here by casting first-order de-
pendency parsing as a tensor estimation problem.
We will start by introducing the notation used in
the paper, followed by a more formal description
of our dependency parsing task.
3.1 Basic Notations
Let A ? R
n?n?d
be a 3-dimensional tensor (a 3-
way array). We denote each element of the tensor
as A
i,j,k
where i ? [n], j ? [n], k ? [d] and [n]
is a shorthand for the set of integers {1, 2, ? ? ? , n}.
Similarly, we use M
i,j
and u
i
to represent the ele-
ments of matrix M and vector u, respectively.
We define the inner product of two tensors (or
matrices) as ?A,B? = vec(A)
T
vec(B), where
vec(?) concatenates the tensor (or matrix) ele-
ments into a column vector. The squared norm
of a tensor/matrix is denoted by ?A?
2
= ?A,A?.
The Kronecker product of three vectors is de-
noted by u?v?w and forms a rank-1 tensor such
that
(u? v ? w)
i,j,k
= u
i
v
j
w
k
.
Note that the vectors u, v, and w may be column
or row vectors. Their orientation is defined based
on usage. For example, u ? v is a rank-1 matrix
uv
T
when u and v are column vectors (u
T
v if they
are row vectors).
We say that tensor A is in Kruskal form if
A =
r
?
i=1
U(i, :)? V (i, :)?W (i, :) (1)
where U, V ? R
r?n
, W ? R
r?d
and U(i, :) is the
i
th
row of matrix U . We will directly learn a low-
rank tensor A (because r is small) in this form as
one of our model parameters.
3.2 Dependency Parsing
Let x be a sentence and Y(x) the set of possible
dependency trees over the words in x. We assume
that the score S(x, y) of each candidate depen-
dency tree y ? Y(x) decomposes into a sum of
?local? scores for arcs. Specifically:
S(x, y) =
?
h?m ? y
s(h? m) ?y ? Y(x)
where h ? m is the head-modifier dependency
arc in the tree y. Each y is understood as a col-
lection of arcs h ? m where h and m index
words in x.
2
For example, x(h) is the word cor-
responding to h. We suppress the dependence on
x whenever it is clear from context. For exam-
ple, s(h ? m) can depend on x in complicated
ways as discussed below. The predicted parse is
obtained as y? = arg max
y?Y(x)
S(x, y).
A key problem is how we parameterize the
arc scores s(h ? m). Following the MST
parser (McDonald et al, 2005a) we can define
rich features characterizing each head-modifier
arc, compiled into a sparse binary vector ?
h?m
?
R
L
that depends on the sentence x as well as the
chosen arc h? m (again, we suppress the depen-
dence on x). Based on this feature representation,
we define the score of each arc as s
?
(h ? m) =
2
Note that in the case of high-order parsing, the sum
S(x, y) may also include local scores for other syntactic
structures, such as grandhead-head-modifier score s(g ?
h ? m). See (Martins et al, 2013) for a complete list of
these structures.
1383
Unigram features:
form form-p form-n
lemma lemma-p lemma-n
pos pos-p pos-n
morph bias
Bigram features:
pos-p, pos
pos, pos-n
pos, lemma
morph, lemma
Trigram features:
pos-p, pos, pos-n
Table 1: Word feature templates used by our
model. pos, form, lemma and morph stand for
the fine POS tag, word form, word lemma and the
morphology feature (provided in CoNLL format
file) of the current word. There is a bias term that
is always active for any word. The suffixes -p and
-n refer to the left and right of the current word re-
spectively. For example, pos-p means the POS tag
to the left of the current word in the sentence.
??, ?
h?m
? where ? ? R
L
represent adjustable pa-
rameters to be learned, and L is the number of pa-
rameters (and possible features in ?
h?m
).
We can alternatively specify arc features in
terms of rank-1 tensors by taking the Kronecker
product of simpler feature vectors associated with
the head (vector ?
h
? R
n
), and modifier (vector
?
m
? R
n
), as well as the arc itself (vector ?
h,m
?
R
d
). Here ?
h,m
is much lower dimensional than
the MST arc feature vector ?
h?m
discussed ear-
lier. For example, ?
h,m
may be composed of only
indicators for binned arc lengths
3
. ?
h
and ?
m
, on
the other hand, are built from features shown in
Table 1. By taking the cross-product of all these
component feature vectors, we obtain the full fea-
ture representation for arc h? m as a rank-1 ten-
sor
?
h
? ?
m
? ?
h,m
? R
n?n?d
Note that elements of this rank-1 tensor include
feature combinations that are not part of the fea-
ture crossings in ?
h?m
. In this sense, the rank-1
tensor represents a substantial feature expansion.
The arc score s
tensor
(h? m) associated with the
3
In our current version, ?
h,m
only contains the binned
arc length. Other possible features include, for example, the
label of the arc h ? m, the POS tags between the head and
the modifier, boolean flags which indicate the occurence of
in-between punctutations or conjunctions, etc.
tensor representation is defined analogously as
s
tensor
(h? m) = ?A, ?
h
? ?
m
? ?
h,m
?
where the adjustable parametersA also form a ten-
sor. Given the typical dimensions of the compo-
nent feature vectors, ?
h
, ?
m
, ?
h,m
, it is not even
possible to store all the parameters in A. Indeed,
in the full English training set of CoNLL-2008, the
tensor involves around 8 ? 10
11
entries while the
MST feature vector has approximately 1.5 ? 10
7
features. To counter this feature explosion, we re-
strict the parameters A to have low rank.
Low-Rank Dependency Scoring We can repre-
sent a rank-r tensor A explicitly in terms of pa-
rameter matrices U , V , and W as shown in Eq. 1.
As a result, the arc score for the tensor reduces to
evaluating U?
h
, V ?
m
, and W?
h,m
which are all
r dimensional vectors and can be computed effi-
ciently based on any sparse vectors ?
h
, ?
m
, and
?
h,m
. The resulting arc score s
tensor
(h ? m) is
then
r
?
i=1
[U?
h
]
i
[V ?
m
]
i
[W?
h,m
]
i
(2)
By learning parameters U , V , andW that function
well in dependency parsing, we also learn context-
dependent embeddings for words and arcs. Specif-
ically,U?
h
(for a given sentence, suppressed) is an
r dimensional vector representation of the word
corresponding to h as a head word. Similarly,
V ?
m
provides an analogous representation for a
modifier m. Finally, W?
h,m
is a vector embed-
ding of the supplemental arc-dependent informa-
tion. The resulting embedding is therefore tied
to the syntactic roles of the words (and arcs), and
learned in order to perform well in parsing.
We expect a dependency parsing model to ben-
efit from several aspects of the low-rank tensor
scoring. For example, we can easily incorpo-
rate additional useful features in the feature vec-
tors ?
h
, ?
m
and ?
h,m
, since the low-rank assump-
tion (for small enough r) effectively counters the
otherwise uncontrolled feature expansion. More-
over, by controlling the amount of information
we can extract from each of the component fea-
ture vectors (via rank r), the statistical estimation
problem does not scale dramatically with the di-
mensions of ?
h
, ?
m
and ?
h,m
. In particular, the
low-rank constraint can help generalize to unseen
arcs. Consider a feature ?(x(h) = a) ? ?(x(m) =
1384
b) ? ?(dis(x, h,m) = c) which is non-zero only
for an arc a ? b with distance c in sentence x.
If the arc has not been seen in the available train-
ing data, it does not contribute to the traditional
arc score s
?
(?). In contrast, with the low-rank con-
straint, the arc score in Eq. 2 would typically be
non-zero.
Combined Scoring Our parsing model aims to
combine the strengths of both traditional features
from the MST/Turbo parser as well as the new
low-rank tensor features. In this way, our model
is able to capture a wide range of information in-
cluding the auxiliary features without having un-
controlled feature explosion, while still having the
full accessibility to the manually engineered fea-
tures that are proven useful. Specifically, we de-
fine the arc score s
?
(h? m) as the combination
(1? ?)s
tensor
(h? m) + ?s
?
(h? m)
= (1? ?)
r
?
i=1
[U?
h
]
i
[V ?
m
]
i
[W?
h,m
]
i
+ ? ??, ?
h?m
? (3)
where ? ? R
L
, U ? R
r?n
, V ? R
r?n
, and W ?
R
r?d
are the model parameters to be learned. The
rank r and ? ? [0, 1] (balancing the two scores)
represent hyper-parameters in our model.
4 Learning
The training set D = {(x?
i
, y?
i
)}
N
i=1
consists of N
pairs, where each pair consists of a sentence x
i
and the corresponding gold (target) parse y
i
. The
goal is to learn values for the parameters ?, U , V
and W that optimize the combined scoring func-
tion S
?
(x, y) =
?
h?m?y
s
?
(h ? m), defined
in Eq. 3, for parsing performance. We adopt a
maximum soft-margin framework for this learning
problem. Specifically, we find parameters ?, U , V ,
W , and {?
i
} that minimize
C
?
i
?
i
+ ???
2
+ ?U?
2
+ ?V ?
2
+ ?W?
2
s.t. S
?
(x?
i
, y?
i
) ? S
?
(x?
i
, y
i
) + ?y?
i
? y
i
?
1
? ?
i
?y
i
? Y(x?
i
), ?i. (4)
where ?y?
i
?y
i
?
1
is the number of mismatched arcs
between the two trees, and ?
i
is a non-negative
slack variable. The constraints serve to separate
the gold tree from other alternatives in Y(x?
i
) with
a margin that increases with distance.
The objective as stated is not jointly convex
with respect to U , V and W due to our explicit
representation of the low-rank tensor. However, if
we fix any two sets of parameters, for example, if
we fix V andW , then the combined score S
?
(x, y)
will be a linear function of both ? and U . As a re-
sult, the objective will be jointly convex with re-
spect to ? and U and could be optimized using
standard tools. However, to accelerate learning,
we adopt an online learning setup. Specifically,
we use the passive-aggressive learning algorithm
(Crammer et al, 2006) tailored to our setting, up-
dating pairs of parameter sets, (?, U), (?, V ) and
(?,W ) in an alternating manner. This method is
described below.
Online Learning In an online learning setup,
we update parameters successively based on each
sentence. In order to apply the passive-aggressive
algorithm, we fix two of U , V and W (say, for ex-
ample, V and W ) in an alternating manner, and
apply a closed-form update to the remaining pa-
rameters (here U and ?). This is possible since
the objective function with respect to (?, U) has a
similar form as in the original passive-aggressive
algorithm. To illustrate this, consider a training
sentence x
i
. The update involves finding first the
best competing tree,
y?
i
= arg max
y
i
?Y(x?
i
)
S
?
(x?
i
, y
i
) + ?y?
i
? y
i
?
1
(5)
which is the tree that violates the constraint in
Eq. 4 most (i.e. maximizes the loss ?
i
). We then
obtain parameter increments ?? and ?U by solv-
ing
min
??, ?U, ??0
1
2
????
2
+
1
2
??U?
2
+ C?
s.t. S
?
(x?
i
, y?
i
) ? S
?
(x?
i
, y?
i
) + ?y?
i
? y?
i
?
1
? ?
In this way, the optimization problem attempts to
keep the parameter change as small as possible,
while forcing it to achieve mostly zero loss on this
single instance. This problem has a closed form
solution
?? = min
{
C,
loss
?
2
?d??
2
+ (1? ?)
2
?du?
2
}
?d?
?U = min
{
C,
loss
?
2
?d??
2
+ (1? ?)
2
?du?
2
}
(1? ?)du
1385
where
loss = S
?
(x?
i
, y?
i
) + ?y?
i
? y?
i
?
1
? S
?
(x?
i
, y?
i
)
d? =
?
h?m ? y?
i
?
h?m
?
?
h?m ? y?
i
?
h?m
du =
?
h?m ? y?
i
[(V ?
m
) (W?
h,m
)]? ?
h
?
?
h?m ? y?
i
[(V ?
m
) (W?
h,m
)]? ?
h
where (u v)
i
= u
i
v
i
is the Hadamard (element-
wise) product. The magnitude of change of ? and
U is controlled by the parameterC. By varyingC,
we can determine an appropriate step size for the
online updates. The updates also illustrate how ?
balances the effect of the MST component of the
score relative to the low-rank tensor score. When
? = 0, the arc scores are entirely based on the low-
rank tensor and ?? = 0. Note that ?
h
, ?
m
, ?
h,m
,
and ?
h?m
are typically very sparse for each word
or arc. Therefore du and d? are also sparse and
can be computed efficiently.
Initialization The alternating online algorithm
relies on how we initializeU , V , andW since each
update is carried out in the context of the other
two. A random initialization of these parameters is
unlikely to work well, both due to the dimensions
involved, and the nature of the alternating updates.
We consider here instead a reasonable determinis-
tic ?guess? as the initialization method.
We begin by training our model without any
low-rank parameters, and obtain parameters ?.
The majority of features in this MST component
can be expressed as elements of the feature ten-
sor, i.e., as [?
h
? ?
m
? ?
h,m
]
i,j,k
. We can there-
fore create a tensor representation of ? such that
B
i,j,k
equals the corresponding parameter value
in ?. We use a low-rank version of B as the ini-
tialization. Specifically, we unfold the tensor B
into a matrix B
(h)
of dimensions n and nd, where
n = dim(?
h
) = dim(?
m
) and d = dim(?
h,m
).
For instance, a rank-1 tensor can be unfolded as
u ? v ? w = u ? vec(v ? w). We compute the
top-r SVD of the resulting unfolded matrix such
that B
(h)
= P
T
SQ. U is initialized as P . Each
right singular vector S
i
Q(i, :) is also a matrix in
R
n?d
. The leading left and right singular vectors
of this matrix are assigned to V (i, :) and W (i, :)
respectively. In our implementation, we run one
epoch of our model without low-rank parameters
and initialize the tensor A.
Parameter Averaging The passive-aggressive
algorithm regularizes the increments (e.g. ?? and
?U ) during each update but does not include any
overall regularization. In other words, keeping up-
dating the model may lead to large parameter val-
ues and over-fitting. To counter this effect, we use
parameter averaging as used in the MST and Turbo
parsers. The final parameters are those averaged
across all the iterations (cf. (Collins, 2002)). For
simplicity, in our algorithm we average U , V , W
and ? separately, which works well empirically.
5 Experimental Setup
Datasets We test our dependency model on 14
languages, including the English dataset from
CoNLL 2008 shared tasks and all 13 datasets from
CoNLL 2006 shared tasks (Buchholz and Marsi,
2006; Surdeanu et al, 2008). These datasets in-
clude manually annotated dependency trees, POS
tags and morphological information. Following
standard practices, we encode this information as
features.
Methods We compare our model to MST and
Turbo parsers on non-projective dependency pars-
ing. For our parser, we train both a first-order
parsing model (as described in Section 3 and 4)
as well as a third-order model. The third order
parser simply adds high-order features, those typ-
ically used in MST and Turbo parsers, into our
s
?
(x, y) = ??, ?(x, y)? scoring component. The
decoding algorithm for the third-order parsing is
based on (Zhang et al, 2014). For the Turbo
parser, we directly compare with the recent pub-
lished results in (Martins et al, 2013). For the
MST parser, we train and test using the most re-
cent version of the code.
4
In addition, we im-
plemented two additional baselines, NT-1st (first
order) and NT-3rd (third order), corresponding to
our model without the tensor component.
Features For the arc feature vector ?
h?m
, we
use the same set of feature templates as MST
v0.5.1. For head/modifier vector ?
h
and ?
m
, we
show the complete set of feature templates used
by our model in Table 1. Finally, we use a similar
set of feature templates as Turbo v2.1 for 3rd order
parsing.
To add auxiliary word vector representations,
we use the publicly available word vectors (Cirik
4
http://sourceforge.net/projects/mstparser/
1386
First-order only High-order
Ours NT-1st MST Turbo Ours-3rd NT-3rd MST-2nd Turbo-3rd Best Published
Arabic 79.60 78.71 78.3 77.23 79.95 79.53 78.75 79.64 81.12 (Ma11)
Bulgarian 92.30 91.14 90.98 91.76 93.50 92.79 91.56 93.1 94.02 (Zh13)
Chinese 91.43 90.85 90.40 88.49 92.68 92.39 91.77 89.98 91.89 (Ma10)
Czech 87.90 86.62 86.18 87.66 90.50 89.43 87.3 90.32 90.32 (Ma13)
Danish 90.64 89.80 89.84 89.42 91.39 90.82 90.5 91.48 92.00 (Zh13)
Dutch 84.81 83.77 82.89 83.61 86.41 86.08 84.11 86.19 86.19 (Ma13)
English 91.84 91.40 90.59 91.21 93.02 92.82 91.54 93.22 93.22 (Ma13)
German 90.24 89.70 89.54 90.52 91.97 92.26 90.14 92.41 92.41 (Ma13)
Japanese 93.74 93.36 93.38 92.78 93.71 93.23 92.92 93.52 93.72 (Ma11)
Portuguese 90.94 90.67 89.92 91.14 91.92 91.63 91.08 92.69 93.03 (Ko10)
Slovene 84.25 83.15 82.09 82.81 86.24 86.07 83.25 86.01 86.95 (Ma11)
Spanish 85.27 84.95 83.79 83.61 88.00 87.47 84.33 85.59 87.96 (Zh13)
Swedish 89.86 89.66 88.27 89.36 91.00 90.83 89.05 91.14 91.62 (Zh13)
Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10)
Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43
Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the
English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyper-
parameter ? = 0.3. To remove the tensor in our model, we ran experiments with ? = 1, corresponding
to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et
al. (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011a), Martins et al (2013),
Koo et al (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al (2013).
and S?ensoy, 2013), learned from raw data (Glober-
son et al, 2007; Maron et al, 2010). Three
languages in our dataset ? English, German and
Swedish ? have corresponding word vectors in this
collection.
5
The dimensionality of this representa-
tion varies by language: English has 50 dimen-
sional word vectors, while German and Swedish
have 25 dimensional word vectors. Each entry of
the word vector is added as a feature value into
feature vectors ?
h
and ?
m
. For each word in the
sentence, we add its own word vector as well as
the vectors of its left and right words.
We should note that since our model parameter
A is represented and learned in the low-rank form,
we only have to store and maintain the low-rank
projections U?
h
, V ?
m
andW?
h,m
rather than ex-
plicitly calculate the feature tensor ?
h
??
m
??
h,m
.
Therefore updating parameters and decoding a
sentence is still efficient, i.e., linear in the num-
ber of values of the feature vector. In contrast,
assume we take the cross-product of the auxiliary
word vector values, POS tags and lexical items of
a word and its context, and add the crossed val-
ues into a normal model (in ?
h?m
). The number
of features for each arc would be at least quadratic,
growing into thousands, and would be a significant
impediment to parsing efficiency.
Evaluation Following standard practices, we
train our full model and the baselines for 10
5
https://github.com/wolet/sprml13-word-embeddings
epochs. As the evaluation measure, we use un-
labeled attachment scores (UAS) excluding punc-
tuation. In all the reported experiments, the hyper-
parameters are set as follows: r = 50 (rank of the
tensor), C = 1 for first-order model and C = 0.01
for third-order model.
6 Results
Overall Performance Table 2 shows the per-
formance of our model and the baselines on 14
CoNLL datasets. Our model outperforms Turbo
parser, MST parser, as well as its own variants
without the tensor component. The improvements
of our low-rank model are consistent across lan-
guages: results for the first order parser are better
on 11 out of 14 languages. By comparing NT-1st
and NT-3rd (models without low-rank) with our
full model (with low-rank), we obtain 0.7% abso-
lute improvement on first-order parsing, and 0.3%
improvement on third-order parsing. Our model
also achieves the best UAS on 5 languages.
We next focus on the first-order model and
gauge the impact of the tensor component. First,
we test our model by varying the hyper-parameter
? which balances the tensor score and the tradi-
tional MST/Turbo score components. Figure 1
shows the average UAS on CoNLL test datasets
after each training epoch. We can see that the im-
provement of adding the low-rank tensor is con-
sistent across various choices of hyper parame-
1387
2 4 6 8 1084.0%
84.5%
85.0%
85.5%
86.0%
86.5%
87.0%
87.5%
88.0%
# Epochs 
 
?=0.0?=0.2?=0.3?=0.4NT?1st
Figure 1: Average UAS on CoNLL testsets af-
ter different epochs. Our full model consistently
performs better than NT-1st (its variation without
tensor component) under different choices of the
hyper-parameter ?.
no word vector with word vector
English 91.84 92.07
German 90.24 90.48
Swedish 89.86 90.38
Table 3: Results of adding unsupervised word vec-
tors to the tensor. Adding this information yields
consistent improvement for all languages.
ter ?. When training with the tensor component
alone (? = 0), the model converges more slowly.
Learning of the tensor is harder because the scor-
ing function is not linear (nor convex) with respect
to parameters U , V and W . However, the tensor
scoring component achieves better generalization
on the test data, resulting in better UAS than NT-
1st after 8 training epochs.
To assess the ability of our model to incorpo-
rate a range of features, we add unsupervised word
vectors to our model. As described in previous
section, we do so by appending the values of dif-
ferent coordinates in the word vector into ?
h
and
?
m
. As Table 3 shows, adding this information in-
creases the parsing performance for all the three
languages. For instance, we obtain more than
0.5% absolute improvement on Swedish.
Syntactic Abstraction without POS Since our
model learns a compressed representation of fea-
ture vectors, we are interested to measure its per-
formance when part-of-speech tags are not pro-
vided (See Table 4). The rationale is that given all
other features, the model would induce representa-
tions that play a similar role to POS tags. Note that
Our model NT-1st
-POS +wv. -POS +POS
English 88.89 90.49 86.70 90.58
German 82.63 85.80 78.71 88.50
Swedish 81.84 85.90 79.65 88.75
Table 4: The first three columns show parsing re-
sults when models are trained without POS tags.
The last column gives the upper-bound, i.e. the
performance of a parser trained with 12 Core POS
tags. The low-rank model outperforms NT-1st by
a large margin. Adding word vector features fur-
ther improves performance.
the performance of traditional parsers drops when
tags are not provided. For example, the perfor-
mance gap is 10% on German. Our experiments
show that low-rank parser operates effectively in
the absence of tags. In fact, it nearly reaches the
performance of the original parser that used the
tags on English.
Examples of Derived Projections We manu-
ally analyze low-dimensional projections to assess
whether they capture syntactic abstraction. For
this purpose, we train a model with only a ten-
sor component (such that it has to learn an accu-
rate tensor) on the English dataset and obtain low
dimensional embeddings U?
w
and V ?
w
for each
word. The two r-dimension vectors are concate-
nated as an ?averaged? vector. We use this vector
to calculate the cosine similarity between words.
Table 5 shows examples of five closest neighbors
of queried words. While these lists include some
noise, we can clearly see that the neighbors ex-
hibit similar syntactic behavior. For example, ?on?
is close to other prepositions. More interestingly,
we can consider the impact of syntactic context
on the derived projections. The bottom part of
Table 5 shows that the neighbors change substan-
tially depending on the syntactic role of the word.
For example, the closest words to the word ?in-
crease? are verbs in the context phrase ?will in-
crease again?, while the closest words become
nouns given a different phrase ?an increase of?.
Running Time Table 6 illustrates the impact of
estimating low-rank tensor parameters on the run-
ning time of the algorithm. For comparison, we
also show the NT-1st times across three typical
languages. The Arabic dataset has the longest av-
erage sentence length, while the Chinese dataset
1388
greatly profit says on when
actively earnings adds with where
openly franchisees predicts into what
significantly shares noted at why
outright revenue wrote during which
substantially members contends over who
increase will increase again an increase of
rise arguing gain
advance be prices
contest charging payment
halt gone members
Exchequer making subsidiary
hit attacks hit the hardest hit is
shed distributes monopolies
rallied stayed pills
triggered sang sophistication
appeared removed ventures
understate eased factors
Table 5: Five closest neighbors of the queried
words (shown in bold). The upper part shows our
learned embeddings group words with similar syn-
tactic behavior. The two bottom parts of the table
demonstrate that how the projections change de-
pending on the syntactic context of the word.
#Tok. Len.
Train. Time (hour)
NT-1st Ours
Arabic 42K 32 0.13 0.22
Chinese 337K 6 0.37 0.65
English 958K 24 1.88 2.83
Table 6: Comparison of training times across three
typical datasets. The second column is the number
of tokens in each data set. The third column shows
the average sentence length. Both first-order mod-
els are implemented in Java and run as a single
process.
has the shortest sentence length in CoNLL 2006.
Based on these results, estimating a rank-50 tensor
together with MST parameters only increases the
running time by a factor of 1.7.
7 Conclusions
Accurate scoring of syntactic structures such as
head-modifier arcs in dependency parsing typi-
cally requires rich, high-dimensional feature rep-
resentations. We introduce a low-rank factoriza-
tion method that enables to map high dimensional
feature vectors into low dimensional representa-
tions. Our method maintains the parameters as a
low-rank tensor to obtain low dimensional repre-
sentations of words in their syntactic roles, and to
leverage modularity in the tensor for easy train-
ing with online algorithms. We implement the
approach on first-order to third-order dependency
parsing. Our parser outperforms the Turbo and
MST parsers across 14 languages.
Future work involves extending the tensor com-
ponent to capture higher-order structures. In par-
ticular, we would consider second-order structures
such as grandparent-head-modifier by increasing
the dimensionality of the tensor. This tensor will
accordingly be a four or five-way array. The online
update algorithm remains applicable since each di-
mension is optimized in an alternating fashion.
8 Acknowledgements
The authors acknowledge the support of the MURI
program (W911NF-10-1-0533) and the DARPA
BOLT program. This research is developed in col-
laboration with the Arabic Language Technoligies
(ALT) group at Qatar Computing Research Insti-
tute (QCRI) within the LYAS project. We thank
Volkan Cirik for sharing the unsupervised word
vector data. Thanks to Amir Globerson, Andreea
Gane, the members of the MIT NLP group and
the ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
References
Miguel Ballesteros and Joakim Nivre. 2012. Mal-
tOptimizer: An optimization tool for MaltParser. In
EACL. The Association for Computer Linguistics.
Miguel Ballesteros. 2013. Effective morpholog-
ical feature selection with MaltOptimizer at the
SPMRL 2013 shared task. In Proceedings of
the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages. Association for
Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06.
Association for Computational Linguistics.
Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Par-
rilo, and Alan S Willsky. 2011. Rank-sparsity in-
coherence for matrix decomposition. SIAM Journal
on Optimization.
Volkan Cirik and H?usn?u S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task : Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages. Association for
Computational Linguistics.
1389
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1. Associ-
ation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing - Volume 10, EMNLP ?02. As-
sociation for Computational Linguistics.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of
semantic compositionality. In HLT-NAACL. The As-
sociation for Computational Linguistics.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multiview learning of word embeddings via
CCA. In Advances in Neural Information Process-
ing Systems.
A Evgeniou and Massimiliano Pontil. 2007. Multi-
task feature learning. In Advances in neural infor-
mation processing systems: Proceedings of the 2006
conference. The MIT Press.
Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftali Tishby. 2007. Euclidean embedding of co-
occurrence data. Journal of Machine Learning Re-
search.
Christopher Hillar and Lek-Heng Lim. 2009. Most
tensor problems are NP-hard. arXiv preprint
arXiv:0911.1393.
Daniel Hsu and Sham M Kakade. 2013. Learning mix-
tures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th
Conference on Innovations in Theoretical Computer
Science. ACM.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?10. Association for Com-
putational Linguistics.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Angeliki Lazaridou, Eva Maria Vecchi, and Marco
Baroni. 2013. Fish transporters and miracle
homes: How compositional distributional semantics
can help NP parsing. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-
of-speech induction. In Advances in Neural Infor-
mation Processing Systems.
Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M?ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M?ario A. T. Figueiredo. 2011a. Dual
decomposition with many overlapping components.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11. Association for Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Pedro MQ Aguiar,
and M?ario AT Figueiredo. 2011b. Structured spar-
sity in structured prediction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Andr?e FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow.
2010. Improving arabic dependency parsing with
lexical and inflectional morphological features. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, SPMRL ?10. Association for Computa-
tional Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow.
2011. Improving arabic dependency parsing with
form-based and functional morphological features.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
1390
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR.
Peter Nilsson and Pierre Nugues. 2010. Automatic
discovery of feature sets for dependency parsing. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010). Coling
2010 Organizing Committee.
Joakim Nivre, Johan Hall, Jens Nilsson, G?uls?en Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering.
Alexander Rush and Slav Petrov. 2012a. Vine pruning
for efficient multi-pass dependency parsing. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL ?12).
Alexander M Rush and Slav Petrov. 2012b. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compo-
sitional vector grammars. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics.
Nathan Srebro, Tommi Jaakkola, et al 2003. Weighted
low-rank approximations. In ICML.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola.
2004. Maximum-margin matrix factorization. In
Advances in neural information processing systems.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, CoNLL ?08. Association for
Computational Linguistics.
Min Tao and Xiaoming Yuan. 2011. Recovering low-
rank and sparse components of matrices from in-
complete and noisy observations. SIAM Journal on
Optimization.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?10. Association for Com-
putational Linguistics.
Andrew E Waters, Aswin C Sankaranarayanan, and
Richard Baraniuk. 2011. SpaRCS: Recovering low-
rank and sparse matrices from compressive mea-
surements. In Advances in Neural Information Pro-
cessing Systems.
Hao Zhang and Ryan McDonald. 2012a. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12. Association for Compu-
tational Linguistics.
Hao Zhang and Ryan McDonald. 2012b. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning.
Association for Computational Linguistics.
Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-
ald. 2013. Online learning for inexact hypergraph
search. In Proceedings of EMNLP.
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Jaakkola, and Amir Globerson. 2014. Steps to ex-
cellence: Simple inference with refined scoring of
dependency trees. In Proceedings of the 52th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.
Tianyi Zhou and Dacheng Tao. 2011. Godec: Ran-
domized low-rank & sparse matrix decomposition in
noisy case. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11).
1391
