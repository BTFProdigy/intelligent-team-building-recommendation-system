Proceedings of the 43rd Annual Meeting of the ACL, pages 515?522,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Phonotactic Language Model for Spoken Language Identification 
 Haizhou Li and Bin Ma 
Institute for Infocomm Research 
Singapore 119613 
{hli,mabin}@i2r.a-star.edu.sg 
 
Abstract 
We have established a phonotactic lan-
guage model as the solution to spoken 
language identification (LID). In this 
framework, we define a single set of 
acoustic tokens to represent the acoustic 
activities in the world?s spoken languages. 
A voice tokenizer converts a spoken 
document into a text-like document of 
acoustic tokens. Thus a spoken document 
can be represented by a count vector of 
acoustic tokens and token n-grams in the 
vector space. We apply latent semantic 
analysis to the vectors, in the same way 
that it is applied in information retrieval, 
in order to capture salient phonotactics 
present in spoken documents. The vector 
space modeling of spoken utterances con-
stitutes a paradigm shift in LID technol-
ogy and has proven to be very successful. 
It presents a 12.4% error rate reduction 
over one of the best reported results on 
the 1996 NIST Language Recognition 
Evaluation database. 
1 Introduction 
Spoken language and written language are similar 
in many ways. Therefore, much of the research in 
spoken language identification, LID, has been in-
spired by text-categorization methodology. Both 
text and voice are generated from language de-
pendent vocabulary. For example, both can be seen 
as stochastic time-sequences corrupted by a chan-
nel noise. The n-gram language model has 
achieved equal amounts of success in both tasks, 
e.g. n-character slice for text categorization by lan-
guage (Cavnar and Trenkle, 1994) and Phone Rec-
ognition followed by n-gram Language Modeling, 
or PRLM (Zissman, 1996) .  
Orthographic forms of language, ranging from 
Latin alphabet to Cyrillic script to Chinese charac-
ters, are far more unique to the language than their 
phonetic counterparts. From the speech production 
point of view, thousands of spoken languages from 
all over the world are phonetically articulated us-
ing only a few hundred distinctive sounds or pho-
nemes (Hieronymus, 1994). In other words, 
common sounds are shared considerably across 
different spoken languages. In addition, spoken 
documents1, in the form of digitized wave files, are 
far less structured than written documents and need 
to be treated with techniques that go beyond the 
bounds of written language. All of this makes the 
identification of spoken language based on pho-
netic units much more challenging than the identi-
fication of written language. In fact, the challenge 
of LID is inter-disciplinary, involving digital signal 
processing, speech recognition and natural lan-
guage processing.  
In general, a LID system usually has three fun-
damental components as follows:  
1) A voice tokenizer which segments incoming 
voice feature frames and associates the seg-
ments with acoustic or phonetic labels, called 
tokens; 
2) A statistical language model which captures 
language dependent phonetic and phonotactic 
information from the sequences of tokens; 
3) A language classifier which identifies the lan-
guage based on discriminatory characteristics 
of acoustic score from the voice tokenizer and 
phonotactic score from the language model.  
In this paper, we present a novel solution to the 
three problems, focusing on the second and third 
problems from a computational linguistic perspec-
tive. The paper is organized as follows: In Section 
2, we summarize relevant existing approaches to 
the LID task. We highlight the shortcomings of 
existing approaches and our attempts to address the 
                                                          
1 A spoken utterance is regarded as a spoken document in this 
paper. 
515
issues. In Section 3 we propose the bag-of-sounds 
paradigm to turn the LID task into a typical text 
categorization problem. In Section 4, we study the 
effects of different settings in experiments on the 
1996 NIST Language Recognition Evaluation 
(LRE) database2. In Section 5, we conclude our 
study and discuss future work. 
2 Related Work 
Formal evaluations conducted by the National In-
stitute of Science and Technology (NIST) in recent 
years demonstrated that the most successful ap-
proach to LID used the phonotactic content of the 
voice signal to discriminate between a set of lan-
guages (Singer et al, 2003). We briefly discuss 
previous work cast in the formalism mentioned 
above: tokenization, statistical language modeling, 
and language identification. A typical LID system 
is illustrated in Figure 1 (Zissman, 1996), where 
language dependent voice tokenizers (VT) and lan-
guage models (LM) are deployed in the Parallel 
PRLM architecture, or P-PRLM. 
 
 
Figure 1.  L monolingual phoneme recognition 
front-ends are used in parallel to tokenize the input 
utterance, which is analyzed by LMs to predict the 
spoken language 
2.1 Voice Tokenization 
A voice tokenizer is a speech recognizer that 
converts a spoken document into a sequence of 
tokens. As illustrated in Figure 2, a token can be of 
different sizes, ranging from a speech feature 
frame, to a phoneme, to a lexical word. A token is 
defined to describe a distinct acoustic/phonetic 
activity. In early research, low level spectral 
                                                          
2 http://www.nist.gov/speech/tests/ 
frames, which are assumed to be independent of 
each other, were used as a set of prototypical spec-
tra for each language (Sugiyama, 1991). By adopt-
ing hidden Markov models, people moved beyond 
low-level spectral analysis towards modeling a 
frame sequence into a larger unit such as a pho-
neme and even a lexical word.  
Since the lexical word is language specific, the 
phoneme becomes the natural choice when build-
ing a language-independent voice tokenization 
front-end. Previous studies show that parallel lan-
guage-dependent phoneme tokenizers effectively 
serve as the tokenization front-ends with P-PRLM 
being the typical example. However, a language-
independent phoneme set has not been explored 
yet experimentally. In this paper, we would like to 
explore the potential of voice tokenization using a 
unified phoneme set. 
 
 
Figure 2 Tokenization at different resolutions 
2.2 n-gram Language Model 
With the sequence of tokens, we are able to es-
timate an n-gram language model (LM) from the 
statistics. It is generally agreed that phonotactics, 
i.e. the rules governing the phone/phonemes se-
quences admissible in a language, carry more lan-
guage discriminative information than the 
phonemes themselves. An n-gram LM over the 
tokens describes well n-local phonotactics among 
neighboring tokens. While some systems model 
the phonotactics at the frame level (Torres-
Carrasquillo et al, 2002), others have proposed P-
PRLM. The latter has become one of the most 
promising solutions so far (Zissman, 1996).  
  A variety of cues can be used by humans and 
machines to distinguish one language from another. 
These cues include phonology, prosody, morphol-
ogy, and syntax in the context of an utterance. 
VT-1: Chinese 
VT-2: English 
VT-L: French 
 
 
 LM-L: French 
LM-1 ? LM-L 
 
 
 LM-L: French 
LM-1 ? LM-L 
 
 
 LM-L: French 
LM-1 ? LM-L 
language classifier 
spoken utterance  
hypothesized
language
word 
phoneme 
frame
516
However, global phonotactic cues at the level of 
utterance or spoken document remains unexplored 
in previous work. In this paper, we pay special at-
tention to it. A spoken language always contains a 
set of high frequency function words, prefixes, and 
suffixes, which are realized as phonetic token sub-
strings in the spoken document. Individually, those 
substrings may be shared across languages. How-
ever, the pattern of their co-occurrences discrimi-
nates one language from another.  
Perceptual experiments have shown (Mut-
husamy, 1994) that with adequate training, human 
listeners? language identification ability increases 
when given longer excerpts of speech.  Experi-
ments have also shown that increased exposure to 
each language and longer training sessions im-
prove listeners? language identification perform-
ance. Although it is not entirely clear how human 
listeners make use of the high-order phonotac-
tic/prosodic cues present in longer spans of a spo-
ken document, strong evidence shows that 
phonotactics over larger context provides valuable 
LID cues beyond n-gram, which will be further 
attested by our experiments in Section 4. 
2.3 Language Classifier 
The task of a language classifier is to make 
good use of the LID cues that are encoded in the 
model l?  to hypothesize from among L lan-
guages, ? , as the one that is actually spoken in a 
spoken document O. The LID model 
l?
l?  in P-
PRLM refers to extracted information from acous-
tic model and n-gram LM for language l.  We have 
and { ,AM } LLMl l l? ? ?=  ( 1,..., )l l? ?? = . A maxi-
mum-likelihood classifier can be formulated as 
follows: 
( ) (
? arg max ( / )
arg max / , /
l
l
AM LM
l l
l T
l P O
P O T P T
?
? ?
??
?? ??
=
? ? )
)
      (1) 
The exact computation in Eq.(1) involves sum-
ming over all possible decoding of token se-
quences T given O. In many implementations, 
it is approximated by the maximum over all se-
quences in the sum by finding the most likely to-
ken sequence, , for each language l, using the 
Viterbi algorithm: 
??
l?T
( ) (? ? ?arg max[ / , / ]AM LMl l l l
l
l P O T P T? ?
??
?         (2) 
Intuitively, individual sounds are heavily shared 
among different spoken languages due to the com-
mon speech production mechanism of humans. 
Thus, the acoustic score has little language dis-
criminative ability. Many experiments (Yan and 
Barnard, 1995; Zissman, 1996) have further at-
tested that the n-gram LM score provides more 
language discriminative information than their 
acoustic counterparts. In Figure 1, the decoding of 
voice tokenization is governed by the acoustic 
model AMl? to arrive at an acoustic score ( )?/ , AMl lP O T ?  and a token sequence . The n-
gram LM derives the n-local phonotactic score 
l?T
( )? / LMl lP T ? from the language model LMl? .  
Clearly, the n-gram LM suffers the major short-
coming of having not exploited the global phono-
tactics in the larger context of a spoken utterance. 
Speech recognition researchers have so far chosen 
to only use n-gram local statistics for primarily 
pragmatic reasons, as this n-gram is easier to attain. 
In this work, a language independent voice tokeni-
zation front-end is proposed, that uses a unified 
acoustic model  AM?  instead of multiple language 
dependent acoustic models AMl? .  The n-gram 
LM LMl? is generalized to model both local and 
global phonotactics. 
3 Bag-of-Sounds Paradigm 
The bag-of-sounds concept is analogous to the 
bag-of-words paradigm originally formulated in 
the context of information retrieval (IR) and text 
categorization (TC) (Salton 1971; Berry et al, 
1995; Chu-Caroll and Carpenter, 1999). One focus 
of IR is to extract informative features for docu-
ment representation. The bag-of-words paradigm 
represents a document as a vector of counts. It is 
believed that it is not just the words, but also the 
co-occurrence of words that distinguish semantic 
domains of text documents.   
Similarly, it is generally believed in LID that, al-
though the sounds of different spoken languages 
overlap considerably, the phonotactics differenti-
ates one language from another. Therefore, one can 
easily draw the analogy between an acoustic token 
in bag-of-sounds and a word in bag-of-words. 
Unlike words in a text document, the phonotactic 
information that distinguishes spoken languages is 
517
concealed in the sound waves of spoken languages. 
After transcribing a spoken document into a text 
like document of tokens, many IR or TC tech-
niques can then be readily applied. 
It is beyond the scope of this paper to discuss 
what would be a good voice tokenizer. We adopt 
phoneme size language-independent acoustic to-
kens to form a unified acoustic vocabulary in our 
voice tokenizer. Readers are referred to (Ma et al, 
2005) for details of acoustic modeling. 
3.1 Vector Space Modeling 
In human languages, some words invariably occur 
more frequently than others. One of the most 
common ways of expressing this idea is known as 
Zipf?s Law (Zipf, 1949). This law states that there 
is always a set of words which dominates most of 
the other words of the language in terms of their 
frequency of use. This is true both of written words 
and of spoken words. The short-term, or local pho-
notactics, is devised to describe Zipf?s Law.  
The local phonotactic constraints can be typi-
cally described by the token n-grams, or phoneme 
n-grams as in (Ng et al, 2000), which represents 
short-term statistics such as lexical constraints. 
Suppose that we have a token sequence, t1 t2 t3 t4. 
We derive the unigram statistics from the token 
sequence itself. We derive the bigram statistics 
from t1(t2) t2(t3) t3(t4) t4(#) where the token vo-
cabulary is expanded over the token?s right context. 
Similarly, we derive the trigram statistics from the 
t1(#,t2) t2(t1,t3) t3(t2,t4) t4(t3,#) to account for left 
and right contexts. The # sign is a place holder for 
free context. In the interest of manageability, we 
propose to use up to token trigram. In this way, for 
an acoustic system of Y  tokens, we have poten-
tially bigram and Y trigram in the vocabulary.  2Y 3
Meanwhile, motivated by the ideas of having 
both short-term and long-term phonotactic statis-
tics, we propose to derive global phonotactics in-
formation to account for long-term phonotactics: 
The global phonotactic constraint is the high-
order statistics of n-grams. It represents document 
level long-term phonotactics such as co-
occurrences of n-grams. By representing a spoken 
document as a count vector of n-grams, also called 
bag-of-sounds vector, it is possible to explore the 
relations and higher-order statistics among the di-
verse n-grams through latent semantic analysis 
(LSA).  
It is often advantageous to weight the raw 
counts to refine the contribution of each n-gram to 
LID. We begin by normalizing the vectors repre-
senting the spoken document by making each vec-
tor of unit length. Our second weighting is based 
on the notion that an n-gram that only occurs in a 
few languages is more discriminative than an n-
gram that occurs in nearly every document. We use 
the inverse-document frequency (idf) weighting 
scheme (Spark Jones, 1972), in which a word is 
weighted inversely to the number of documents in 
which it occurs, by means of 
( ) log / ( )idf w D d w=  , where w is a word in the 
vocabulary of W token n-grams. D is the total num-
ber of documents in the training corpus from L lan-
guages. Since each language has at least one 
document in the training corpus, we have D L? . 
is the number of documents containing the 
word w. Letting be the count of word w in 
document d, we have the weighted count as 
( )d w
,w dc
2 1/ 2
, , ,
1
( ) /( )w d w d w d
w W
c c idf w c ?
?? ?
? = ? ?  (3) 
and a vector to represent 
document d. A corpus is then represented by a 
term-document matrix
1, 2, ,{ , ,..., }
T
d d d W dc c c c? ? ?=
1 2{ , ,..., }DH c c c= of W D? .  
3.2 Latent Semantic Analysis 
The fundamental idea in LSA is to reduce the 
dimension of a document vector, W to Q, where 
Q W<< and Q D<<  , by projecting the problem 
into the space spanned by the rows of the closest 
rank-Q matrix to H in the Frobenius norm (Deer-
wester et al 1990).  Through singular value de-
composition (SVD) of H, we construct a modified 
matrix HQ from the Q-largest singular values: 
T
Q Q Q QH U S V=                         (4) 
QU is a W Q? left singular matrix with rows 
,1wu w W? ? QS; is a Q Q?  diagonal matrix of Q-
largest singular values of H; is QV D Q? right sin-
gular matrix with rows , 1 . dv d D? ?
With the SVD, we project the D document vec-
tors in H into a reduced space  , referred to as 
Q-space in the rest of this paper. A test document 
of unknown language ID is mapped to a 
pseudo-document in the Q-space by matrix  
QV
pc
pv QU
518
1T
p p p Qc v c U S
?? = Q   (5) 
After SVD, it is straightforward to arrive at a 
natural metric for the closeness between two spo-
ken documents  and in Q-space instead of  
their original W-dimensional space  and . 
iv jv
ic jc
( , ) cos( , )
|| || || ||
T
i j
i j i j
i j
v v
g c c v v
v v
?? = ?    (6) 
( , )i jg c c  indicates the similarity between two vec-
tors, which can be transformed to a distance meas-
ure . 1( , ) cos ( , )i j i jk c c g c c
?=
In the forced-choice classification, a test docu-
ment, supposedly monolingual, is classified into 
one of the L languages. Note that the test document 
is unknown to the H matrix. We assume consis-
tency between the test document?s intrinsic phono-
tactic pattern and one of the D patterns, that is 
extracted from the training data and is presented in 
the H matrix, so that the SVD matrices still apply 
to the test document, and Eq.(5) still holds for di-
mension reduction. 
3.3 Bag-of-Sounds Language Classifier 
The bag-of-sounds phonotactic LM benefits from 
several properties of vector space modeling and 
LSA.  
1) It allows for representing a spoken document 
as a vector of n-gram features, such as unigram, 
bigram, trigram, and the mixture of them; 
2) It provides a well-defined distance metric for 
measurement of phonotactic distance between 
spoken documents;  
3) It processes spoken documents in a lower di-
mensional Q-space, that makes the bag-of-
sounds phonotactic language modeling, LMl? , 
and classification computationally manageable. 
Suppose we have only one prototypical vector 
and its projection in the Q-space to represent 
language l. Applying LSA to the term-document 
matrix
lc lv
:H W L? , a minimum distance classifier is 
formulated: 
? arg min ( , )p l
l
l k v
??
= v    (7) 
In Eq.(7), is the Q-space projection of , a test 
document. 
pv pc
Apparently, it is very restrictive for each lan-
guage to have just one prototypical vector, also 
referred to as a centroid. The pattern of language 
distribution is inherently multi-modal, so it is 
unlikely well fitted by a single vector. One solution 
to this problem is to span the language space with 
multiple vectors. Applying LSA to a term-
document matrix :H W L?? , where L L as-
suming each language l is represented by a set of 
M vectors, 
M? = ?
l? , a new classifier, using k-nearest 
neighboring rule (Duda and Hart, 1973) , is formu-
lated, named k-nearest classifier (KNC): 
? arg min ( , )
l
p l
l l
l k
?
??? ??
= v v?               (8) 
where l? is the set of k-nearest-neighbor to  and  pv
l l? ? ? . 
Among many ways to derive the M centroid vec-
tors, here is one option. Suppose that we have a set 
of training documents Dl for language l , as subset 
of corpus ? ,  and . To derive 
the M vectors, we choose to carry out vector quan-
tization (VQ) to partition D
lD ?? 1Ll lD=? = ?
l
l  into M cells Dl,m in the 
Q-space such that 1 ,
M
m l mD D=? =  using similarity 
metric Eq.(6). All the documents in each cell 
,l mD can then be merged to form a super-document, 
which is further projected into a Q-space vector 
. This results in M prototypical centroids 
. Using KNC, a test vector is 
compared with M vectors to arrive at the k-nearest 
neighbors for each language, which can be compu-
tationally expensive when M is large. 
,l mv
, ( 1,...l m l )M??v m =
Alternatively, one can account for multi-modal 
distribution through finite mixture model. A mix-
ture model is to represent the M discrete compo-
nents with soft combination. To extend the KNC 
into a statistical framework, it is necessary to map 
our distance metric Eq.(6) into a probability meas-
ure. One way is for the distance measure to induce 
a family of exponential distributions with pertinent 
marginality constraints. In practice, what we need 
is a reasonable probability distribution, which 
sums to one, to act as a lookup table for the dis-
tance measure. We here choose to use the empiri-
cal multivariate distribution constructed by 
allocating the total probability mass in proportion 
to the distances observed with the training data. In 
short, this reduces the task to a histogram normali-
zation. In this way, we map the distance  
to a conditional probability distribution 
( , )i jk c c
( | )i jp v v  
519
subject to . Now that we are in the 
probability domain, techniques such as mixture 
smoothing can be readily applied to model a lan-
guage class with finer fitting. 
| |
1
( | ) 1i ji p v v
?
= =?
Let?s re-visit the task of L language forced-
choice classification. Similar to KNC, suppose we 
have M centroids  in the Q-
space for each language l. Each centroid represents 
a class.  The class conditional probability can be 
described as a linear combination of
,  ( 1,... )l m lv m?? = M
,( | )i l mp v v : 
,
1
( | ) ( ) ( | )
M
LM
i l l m i l m
m
,p v p v p v?
=
=? v
)
           (9) 
the probability ,( l mp v , functionally serves as a 
mixture weight of ,( | )i l mp v v . Together with a set 
of centroids , ,  ( 1,...l m lv m )?? = ,( | )i l mM p v v
)
and 
,( l mp v  define a mixture model 
LM
l? .  ,( | )i l mp v v  
is estimated by histogram normalization and 
,( l m )p v is estimated under the maximum likelihood 
criteria, , ,( ) /l m m l lp v C= C  , where C  is total 
number of documents in D
l
l, of which C docu-
ments fall into the cell m.  
,m l
An Expectation-Maximization iterative process 
can be devised for training of LMl?  to maximize the 
likelihood Eq.(9) over the entire training corpus: 
| |
1 1
( | ) ( | )
lDL
LM
d l
l d
p p v ?
= =
? ? =??            (10) 
Using the phonotactic LM score ( )? / LMl lP T for 
classification, with T  being represented by the 
bag-of-sounds vector v ,  Eq.(2) can be reformu-
lated as Eq.(11),  named mixture-model classifier 
(MMC): 
?
l?
p
, ,
1
? arg max ( | )
 arg max ( ) ( | )
LM
p l
l
M
l m p l m
l m
l p v
p v p v v
?
??
?? =
=
= ?  (11) 
To establish fair comparison with P-PRLM, as 
shown in Figure 3, we devise our bag-of-sounds 
classifier to solely use the LM score ( )? / LMl lP T ? for classification decision whereas the 
acoustic score ( )?/ , AMl lP O may potentially help 
as reported in (Singer et al, 2003).  
T ?                                                           
 
 
Figure 3.  A bag-of-sounds classifier. A unified 
front-end followed by L parallel bag-of-sounds 
phonotactic LMs. 
4 Experiments 
This section will experimentally analyze the per-
formance of the proposed bag-of-sounds frame-
work using the 1996 NIST Language Recognition 
Evaluation (LRE) data. The database was intended 
to establish a baseline of performance capability 
for language recognition of conversational tele-
phone speech. The database contains recorded 
speech of 12 languages: Arabic, English, Farsi, 
French, German, Hindi, Japanese, Korean, Manda-
rin, Spanish, Tamil and Vietnamese. We use the 
training set and development set from LDC Call-
Friend corpus3 as the training data. Each conversa-
tion is segmented into overlapping sessions of 
about 30 seconds each, resulting in about 12,000 
sessions for each language. The evaluation set con-
sists of 1,492 30-sec sessions, each distributed 
among the various languages of interest. We treat a 
30-sec session as a spoken document in both train-
ing and testing. We report error rates (ER) of the 
1,492 test trials. 
4.1 Effect of Acoustic Vocabulary 
The choice of n-gram affects the performance of 
LID systems. Here we would like to see how a bet-
ter choice of acoustic vocabulary can help convert 
a spoken document into a phonotactically dis-
criminative space. There are two parameters that 
determine the acoustic vocabulary: the choice of 
acoustic token, and the choice of n-grams. In this 
paper, the former concerns the size of an acoustic 
system Y in the unified front-end. It is studied in 
more details in (Ma et al, 2005). We set Y to 32 in 
3 See http://www.ldc.upenn.edu/. The overlap between 1996 
NIST evaluation data and CallFriend database has been re-
moved from training data as suggested in the 2003 NIST LRE 
website http://www.nist.gov/speech/tests/index.htm 
LM
l? LM-L:  French 
Unified VT
1
LM? LM-1: Chinese 
2
LM?  LM-2: English 
Language C
lassifier
spoken utterance
H
ypothesized language 
AM?
520
this experiment; the latter decides what features to 
be included in the vector space. The vector space 
modeling allows for multiple heterogeneous fea-
tures in one vector. We introduce three types of 
acoustic vocabulary (AV) with mixture of token 
unigram, bigram, and trigram:   
a) AV1: 32 broad class phonemes as unigram, 
selected from 12 languages, also referred to as 
P-ASM as detailed in (Ma et al, 2005) 
b) AV2: AV1 augmented by 32  bigrams of 
AV1, amounting to 1,056 tokens 
32?
c) AV3: AV2 augmented by 32  tri-
grams of AV1, amounting to 33,824 tokens 
32 32? ?
 
 AV1 AV2 AV3 
ER % 46.1 32.8 28.3 
Table 1.  Effect of acoustic vocabulary (KNC) 
 
We carry out experiments with KNC classifier 
of 4,800 centroids. Applying k-nearest-neighboring 
rule, k is empirically set to 3. The error rates are 
reported in Table 1 for the experiments over the 
three AV types. It is found that high-order token n-
grams improve LID performance.   This reaffirms 
many previous findings that n-gram phonotactics 
serves as a valuable cue in LID. 
4.2 Effect of Model Size 
As discussed in KNC, one would expect to im-
prove the phonotactic model by using more cen-
troids. Let?s examine how the number of centroid 
vectors M affects the performance of KNC. We set 
the acoustic system size Y to 128, k-nearest to 3, 
and only use token bigrams in the bag-of-sounds 
vector. In Table 2, it is not surprising to find that 
the performance improves as M increases. How-
ever, it is not practical to have large M be-
cause comparisons need to take place in 
each test trial.  
L L M? = ?
 
#M 1,200 2,400 4,800 12,000 
ER % 17.0 15.7 15.4 14.8 
Table 2. Effect of number of centroids (KNC) 
 
To reduce computation, MMC attempts to use 
less number of mixtures M to represent the phono-
tactic space. With the smoothing effect of the mix-
ture model, we expect to use less computation to 
achieve similar performance as KNC. In the ex-
periment reported in Table 3, we find that MMC 
(M=1,024) achieves 14.9% error rate, which al-
most equalizes the best result in the KNC experi-
ment (M=12,000) with much less computation.  
 
#M 4 16 64 256 1,024 
ER % 29.6 26.4 19.7 16.0 14.9 
Table 3. Effect of number of mixtures (MMC) 
4.3 Discussion 
The bag-of-sounds approach has achieved equal 
success in both 1996 and 2003 NIST LRE data-
bases. As more results are published on the 1996 
NIST LRE database, we choose it as the platform 
of comparison. In Table 4, we report the perform-
ance across different approaches in terms of error 
rate for a quick comparison. MMC presents a 
12.4% ER reduction over the best reported result4 
(Torres-Carrasquillo et al, 2002). 
It is interesting to note that the bag-of-sounds 
classifier outperforms its P-PRLM counterpart by a 
wide margin (14.9% vs 22.0%). This is attributed 
to the global phonotactic features in LMl? .  The 
performance gain in (Torres-Carrasquillo et al, 
2002; Singer et al, 2003) was obtained mainly by 
fusing scores from several classifiers, namely 
GMM, P-PRLM and SVM, to benefit from both 
acoustic and language model scores. Noting that 
the bag-of-sounds classifier in this work solely re-
lies on the LM score, it is believed that fusing with 
scores from other classifiers will further boost the 
LID performance.  
 
 ER % 
P-PRLM5 22.0 
P-PRLM + GMM acoustic5 19.5 
P-PRLM + GMM acoustic +  
GMM tokenizer5
17.0 
Bag-of-sounds classifier (MMC) 14.9 
Table 4. Benchmark of different approaches 
 
Besides the error rate reduction, the bag-of-
sounds approach also simplifies the on-line com-
puting procedure over its P-PRLM counterpart. It 
would be interesting to estimate the on-line com-
putational need of MMC. The cost incurred has 
two main components: 1) the construction of the 
                                                          
4 Previous results are also reported in DCF, DET, and equal 
error rate (EER). Comprehensive benchmarking for bag-of-
sounds phonotactic LM will be reported soon. 
5 Results extracted from (Torres-Carrasquillo et al, 2002) 
521
pseudo document vector, as done via Eq.(5); 2) 
vector comparisons. The computing 
cost is estimated to be  per test trial 
(Bellegarda, 2000). For typical values of Q, this 
amounts to less than 0.05 Mflops. While this is 
more expensive than the usual table look-up in 
conventional n-gram LM, the performance im-
provement is able to justify the relatively modest 
computing overhead. 
L L M? = ?
2( )QO
5 Conclusion 
We have proposed a phonotactic LM approach to 
LID problem. The concept of bag-of-sounds is in-
troduced, for the first time, to model phonotactics 
present in a spoken language over a larger context. 
With bag-of-sounds phonotactic LM, a spoken 
document can be treated as a text-like document of 
acoustic tokens. This way, the well-established 
LSA technique can be readily applied. This novel 
approach not only suggests a paradigm shift in LID, 
but also brings 12.4% error rate reduction over one 
of the best reported results on the 1996 NIST LRE 
data. It has proven to be very successful.  
We would like to extend this approach to other 
spoken document categorization tasks. In monolin-
gual spoken document categorization, we suggest 
that the semantic domain can be characterized by 
latent phonotactic features. Thus it is straightfor-
ward to extend the proposed bag-of-sounds frame-
work to spoken document categorization. 
Acknowledgement 
The authors are grateful to Dr. Alvin F. Martin of 
the NIST Speech Group for his advice when pre-
paring the 1996 NIST LRE experiments, to Dr G. 
M. White and Ms Y. Chen of Institute for Info-
comm Research for insightful discussions.  
References  
Jerome R. Bellegarda. 2000. Exploiting latent semantic 
information in statistical language modeling, In Proc. 
of the IEEE, 88(8):1279-1296. 
M. W. Berry, S.T. Dumais and G.W. O?Brien. 1995. 
Using Linear Algebra for intelligent information re-
trieval, SIAM Review, 37(4):573-595. 
William B. Cavnar, and John M. Trenkle. 1994. N-
Gram-Based Text Categorization, In Proc. of 3rd 
Annual Symposium on Document Analysis and In-
formation Retrieval, pp. 161-169. 
Jennifer Chu-Carroll, and Bob Carpenter. 1999. Vector-
based Natural Language Call Routing, Computa-
tional Linguistics, 25(3):361-388. 
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and 
R. Harshman, 1990, Indexing by latent semantic 
analysis, Journal of the American Society for Infor-
matin Science, 41(6):391-407 
Richard O. Duda and Peter E. Hart. 1973. Pattern Clas-
sification and scene analysis. John Wiley & Sons 
James L. Hieronymus. 1994. ASCII Phonetic Symbols 
for the World?s Languages: Worldbet. Technical Re-
port AT&T Bell Labs. 
Spark Jones, K. 1972. A statistical interpretation of 
term specificity and its application in retrieval, Jour-
nal of Documentation, 28:11-20 
Bin Ma, Haizhou Li and Chin-Hui Lee, 2005. An Acous-
tic Segment Modeling Approach to Automatic Lan-
guage Identification, submitted to Interspeech 2005 
Yeshwant K. Muthusamy, Neena Jain, and Ronald A.  
Cole. 1994. Perceptual benchmarks for automatic 
language identification, In Proc. of ICASSP 
Corinna Ng , Ross Wilkinson , Justin Zobel, 2000. 
, Speech Communication, 32(1-2):61-
77 
Ex-
periments in spoken document retrieval using pho-
neme n-grams
G. Salton, 1971. The SMART Retrieval System, Pren-
tice-Hall, Englewood Cliffs, NJ, 1971 
E. Singer, P.A. Torres-Carrasquillo, T.P. Gleason, W.M. 
Campbell and D.A. Reynolds. 2003. Acoustic, Pho-
netic and Discriminative Approaches to Automatic 
language recognition, In Proc. of Eurospeech 
Masahide Sugiyama. 1991. Automatic language recog-
nition using acoustic features, In Proc. of ICASSP. 
Pedro A. Torres-Carrasquillo, Douglas A. Reynolds, 
and J.R. Deller. Jr. 2002. Language identification us-
ing Gaussian Mixture model tokenization, in Proc. of 
ICASSP. 
Yonghong Yan, and Etienne Barnard. 1995. An ap-
proach to automatic language identification based on 
language dependent phone recognition, In Proc. of 
ICASSP. 
George K. Zipf. 1949. Human Behavior and the Princi-
pal of Least effort, an introduction to human ecology. 
Addison-Wesley, Reading, Mass. 
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of 
telephone speech, IEEE Trans. on Speech and Audio 
Processing, 4(1):31-44. 
522
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Cross-Entity Inference to Improve Event Extraction 
Yu Hong     Jianfeng Zhang     Bin Ma     Jianmin Yao     Guodong Zhou     Qiaoming Zhu 
School of Computer Science and Technology, Soochow University, Suzhou City, China 
{hongy, jfzhang, bma, jyao, gdzhou, qmzhu}@suda.edu.cn 
 
 
Abstract 
Event extraction is the task of detecting certain 
specified types of events that are mentioned in 
the source language data. The state-of-the-art 
research on the task is transductive inference 
(e.g. cross-event inference). In this paper, we 
propose a new method of event extraction by 
well using cross-entity inference. In contrast to 
previous inference methods, we regard entity-
type consistency as key feature to predict event 
mentions. We adopt this inference method to 
improve the traditional sentence-level event ex-
traction system. Experiments show that we can 
get 8.6% gain in trigger (event) identification, 
and more than 11.8% gain for argument (role) 
classification in ACE event extraction. 
1 Introduction 
The event extraction task in ACE (Automatic Con-
tent Extraction) evaluation involves three challeng-
ing issues: distinguishing events of different types, 
finding the participants of an event and determin-
ing the roles of the participants. 
The recent researches on the task show the 
availability of transductive inference, such as that 
of the following methods: cross-document, cross-
sentence and cross-event inferences. Transductive 
inference is a process to use the known instances to 
predict the attributes of unknown instances. As an 
example, given a target event, the cross-event in-
ference can predict its type by well using the re-
lated events co-occurred with it within the same 
document. From the sentence: 
(1)He left the company. 
it is hard to tell whether it is a Transport event in 
ACE, which means that he left the place; or an 
End-Position event, which means that he retired 
from the company. But cross-event inference can 
use a related event ?Then he went shopping? within 
the same document to identify it as a Transport 
event correctly. 
As the above example might suggest, the avail-
ability of transductive inference for event extrac-
tion relies heavily on the known evidences of an 
event occurrence in specific condition. However, 
the evidence supporting the inference is normally 
unclear or absent. For instance, the relation among 
events is the key clue for cross-event inference to 
predict a target event type, as shown in the infer-
ence process of the sentence (1). But event relation 
extraction itself is a hard task in Information Ex-
traction. So cross-event inference often suffers 
from some false evidence (viz., misleading by un-
related events) or lack of valid evidence (viz., un-
successfully extracting related events). 
In this paper, we propose a new method of 
transductive inference, named cross-entity infer-
ence, for event extraction by well using the rela-
tions among entities. This method is firstly 
motivated by the inherent ability of entity types in 
revealing event types. From the sentences: 
(2)He left the bathroom. 
(3)He left Microsoft. 
it is easy to identify the sentence (2) as a Transport 
event in ACE, which means that he left the place, 
because nobody would retire (End-Position type) 
from a bathroom. And compared to the entities in 
sentence (1) and (2), the entity ?Microsoft? in (3) 
would give us more confidence to tag the ?left? 
event as an End-Position type, because people are 
used to giving the full name of the place where 
they retired. 
The cross-entity inference is also motivated by 
the phenomenon that the entities of the same type 
often attend similar events. That gives us a way to 
predict event type based on entity-type consistency. 
From the sentence: 
(4)Obama beats McCain. 
it is hard to identify it as an Elect event in ACE, 
which means Obama wins the Presidential Election, 
1127
or an Attack event, which means Obama roughs 
somebody up. But if we have the priori knowledge 
that the sentence ?Bush beats McCain? is an Elect 
event, and ?Obama? was a presidential contender 
just like ?Bush? (strict type consistency), we have 
ample evidence to predict that the sentence (4) is 
also an Elect event. 
Indeed above cross-entity inference for event-
type identification is not the only use of entity-type 
consistency. As we shall describe below, we can 
make use of it at all issues of event extraction: 
y For event type: the entities of the same type 
are most likely to attend similar events. And the 
events often use consistent or synonymous trigger. 
y For event argument (participant): the enti-
ties of the same type normally co-occur with simi-
lar participants in the events of the same type. 
y For argument role: the arguments of the 
same type, for the most part, play the same roles in 
similar events. 
With the help of above characteristics of entity, 
we can perform a step-by-step inference in this 
order:  
y Step 1: predicting event type and labeling 
trigger given the entities of the same type. 
y Step 2: identifying arguments in certain event 
given priori entity type, event type and trigger that 
obtained by step 1. 
y Step 3: determining argument roles in certain 
event given entity type, event type, trigger and ar-
guments that obtained by step 1 and step 2. 
On the basis, we give a blind cross-entity infer-
ence method for event extraction in this paper. In 
the method, we first regard entities as queries to 
retrieve their related documents from large-scale 
language resources, and use the global evidences 
of the documents to generate entity-type descrip-
tions. Second we determine the type consistency of 
entities by measuring the similarity of the type de-
scriptions. Finally, given the priori attributes of 
events in the training data, with the help of the en-
tities of the same type, we perform the step-by-step 
cross-entity inference on the attributes of test 
events (candidate sentences). 
In contrast to other transductive inference meth-
ods on event extraction, the cross-entity inference 
makes every effort to strengthen effects of entities 
in predicting event occurrences. Thus the inferen-
tial process can benefit from following aspects: 1) 
less false evidence, viz. less false entity-type con-
sistency (the key clue of cross-entity inference), 
because the consistency can be more precisely de-
termined with the help of fully entity-type descrip-
tion that obtained based on the related information 
from Web; 2) more valid evidence, viz. more enti-
ties of the same type (the key references for the 
inference), because any entity never lack its con-
geners. 
2 Task Description 
The event extraction task we addressing is that of 
the Automatic Content Extraction (ACE) evalua-
tions, where an event is defined as a specific occur-
rence involving participants. And event extraction 
task requires that certain specified types of events 
that are mentioned in the source language data be 
detected. We first introduce some ACE terminol-
ogy to understand this task more easily: 
y Entity: an object or a set of objects in one of 
the semantic categories of interest, referred to in 
the document by one or more (co-referential) entity 
mentions. 
y Entity mention: a reference to an entity (typi-
cally, a noun phrase). 
y Event trigger: the main word that most clear-
ly expresses an event occurrence (An ACE event 
trigger is generally a verb or a noun). 
y Event arguments: the entity mentions that 
are involved in an event (viz., participants). 
y Argument roles: the relation of arguments to 
the event where they participate. 
y Event mention: a phrase or sentence within 
which an event is described, including trigger and 
arguments. 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 separate event types 
and do not consider the hierarchical structure 
among them. Besides, the ACE evaluation plan 
defines the following standards to determine the 
correctness of an event extraction: 
y A trigger is correctly labeled if its event type 
and offset (viz., the position of the trigger word in 
text) match a reference trigger. 
y An argument is correctly identified if its event 
type and offsets match any of the reference argu-
ment mentions, in other word, correctly recogniz-
ing participants in an event. 
y An argument is correctly classified if its role 
matches any of the reference argument mentions. 
Consider the sentence: 
1128
(5) It has refused in the last five years to revoke 
the license of a single doctor for committing medi-
cal errors.1
The event extractor should detect an End-
Position event mention, along with the trigger 
word ?revoke?, the position ?doctor?, the person 
whose license should be revoked, and the time dur-
ing which the event happened: 
 Event type End-Position 
Trigger revoke 
a single doctor Role=Person 
doctor Role=Position Arguments 
the last five years Role=Time-within 
Table 1: Event extraction example 
It is noteworthy that event extraction depends on 
previous phases like name identification, entity 
mention co-reference and classification. Thereinto, 
the name identification is another hard task in ACE 
evaluation and not the focus in this paper. So we 
skip the phase and instead directly use the entity 
labels provided by ACE. 
3 Related Work 
Almost all the current ACE event extraction sys-
tems focus on processing one sentence at a time 
(Grishman et al, 2005; Ahn, 2006; Hardyet al 
2006). However, there have been several studies 
using high-level information from a wider scope:  
Maslennikov and Chua (2007) use discourse 
trees and local syntactic dependencies in a pattern-
based framework to incorporate wider context to 
refine the performance of relation extraction. They 
claimed that discourse information could filter noi-
sy dependency paths as well as increasing the reli-
ability of dependency path extraction. 
Finkel et al (2005) used Gibbs sampling, a sim-
ple Monte Carlo method used to perform approxi-
mate inference in factored probabilistic models. By 
using simulated annealing in place of Viterbi de-
coding in sequence models such as HMMs, CMMs, 
and CRFs, it is possible to incorporate non-local 
structure while preserving tractable inference. 
They used this technique to augment an informa-
tion extraction system with long-distance depend-
ency models, enforcing label consistency and 
extraction template consistency constraints. 
Ji and Grishman (2008) were inspired from the 
hypothesis of ?One Sense Per Discourse? (Ya-
                                                          
1 Selected from the file ?CNN_CF_20030304.1900.02? in 
ACE-2005 corpus. 
rowsky, 1995); they extended the scope from a 
single document to a cluster of topic-related docu-
ments and employed a rule-based approach to 
propagate consistent trigger classification and 
event arguments across sentences and documents. 
Combining global evidence from related docu-
ments with local decisions, they obtained an appre-
ciable improvement in both event and event 
argument identification. 
Patwardhan and Riloff (2009) proposed an event 
extraction model which consists of two compo-
nents: a model for sentential event recognition, 
which offers a probabilistic assessment of whether 
a sentence is discussing a domain-relevant event; 
and a model for recognizing plausible role fillers, 
which identifies phrases as role fillers based upon 
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic 
model allows the two components to jointly make 
decisions based upon both the local evidence sur-
rounding each phrase and the ?peripheral vision?. 
Gupta and Ji (2009) used cross-event informa-
tion within ACE extraction, but only for recovering 
implicit time information for events. 
Liao and Grishman (2010) propose document 
level cross-event inference to improve event ex-
traction. In contrast to Gupta?s work, Liao do not 
limit themselves to time information for events, but 
rather use related events and event-type consis-
tency to make predictions or resolve ambiguities 
regarding a given event. 
4 Motivation 
In event extraction, current transductive inference 
methods focus on the issue that many events are 
missing or spuriously tagged because the local in-
formation is not sufficient to make a confident de-
cision. The solution is to mine credible evidences 
of event occurrences from global information and 
regard that as priori knowledge to predict unknown 
event attributes, such as that of cross-document 
and cross-event inference methods.  
However, by analyzing the sentence-level base-
line event extraction, we found that the entities 
within a sentence, as the most important local in-
formation, actually contain sufficient clues for 
event detection. It is only based on the premise that 
we know the backgrounds of the entities before-
hand. For instance, if we knew the entity ?vesu-
vius? is an active volcano, we could easily identify 
1129
the word ?erupt?, which co-occurred with the en-
tity, as the trigger of a ?volcanic eruption? event 
but not that of a ?spotty rash?. 
In spite of that, it is actually difficult to use an 
entity to directly infer an event occurrence because 
we normally don?t know the inevitable connection 
between the background of the entity and the event 
attributes. But we can well use the entities of the 
same background to perform the inference. In de-
tail, if we first know entity(a) has the same back-
ground with entity(b), and we also know that 
entity(a), as a certain role, participates in a specific 
event, then we can predict that entity(b) might par-
ticiptes in a similar event as the same role. 
Consider the two sentences2 from ACE corpus: 
(5) American case for war against Saddam. 
(6) Bush should torture the al Qaeda chief op-
erations officer. 
The sentences are two event mentions which 
have the same attributes: 
Event type Attack 
Trigger war 
American Role=Attacker 
(5) 
Arguments 
Saddam Role=Target 
Event type Attack 
Trigger torture 
Bush Role=Attacker 
(6) 
Arguments 
...Qaeda chief ... Role=Target 
Table 2: Cross-entity inference example 
From the sentences, we can find that the entities 
?Saddam? and ?Qaeda chief? have the same back-
ground (viz., terrorist leader), and they are both the 
arguments of Attack events as the role of Target. 
So if we previously know any of the event men-
tions, we can infer another one with the help of the 
entities of the same background. 
In a word, the cross-entity inference, we pro-
posed for event extraction, bases on the hypothesis: 
Entities of the consistent type normally partici-
pate in similar events as the same role. 
As we will introduce below, some statistical da-
ta from ACE training corpus can support the hy-
pothesis, which show the consistency of event type 
and role in event mentions where entities of the 
same type occur. 
4.1 Entity Consistency and Distribution 
Within the ACE corpus, there is a strong entity 
consistency: if one entity mention appears in a type 
                                                          
2 They are extracted from the files ?CNN_CF_20030305.1900. 
00-1? and ?CNN_CF_20030303.1900.06-1? respectively. 
of event, other entity mentions of the same type 
will appear in similar events, and even use the 
same word to trigger the events. To see this we 
calculated the conditional probability (in the ACE 
corpus) of a certain entity type appearing in the 33 
ACE event subtypes. 
0
50
100
150
200
250
Be?Born
M
arry
D
ivorce
Injure
D
ie
Transport
Transfer?
Transfer?
Start?O
rg
M
erge?
D
eclare?
End?O
rg
A
ttack
D
em
onstr
M
eet
Phone?
Start?
End?
N
om
inate
Elect
A
rrest?Jail
Release?
Trial?
Charge?
Sue
Convict
Sentence
Fine
Execute
Extradite
A
cquit
A
ppeal
Pardon
Event typeF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 1. Conditional probability of a certain entity 
type appearing in the 33 ACE event subtypes (Here 
only the probabilities of Population-Center, Ex-
ploding and Air entities as examples) 
0
50
100
150
200
250
Person
Place
Buyer
Seller
Beneficiary
Price
A
rtifact
O
rigin
D
estination
G
iver
Recipient
M
oney
O
rg
A
gent
Victim
Instrum
ent
Entity
A
ttacker
Target
D
efendant
A
djudicator
Prosecutor
Plaintiff
Crim
e
Position
Sentence
Vehicle
Tim
e?A
fter
Tim
e?Before
Tim
e?A
t?
Tim
e?A
t?End
Tim
e?
Tim
e?
Tim
e?H
olds
Tim
e?
RoleF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 2. Conditional probability of an entity type 
appearing as the 34 ACE role types (Here only the 
probabilities of Population-Center, Exploding and 
Air entities as examples) 
As there are 33 event subtypes and 43 entity 
types, there are potentially 33*43=1419 entity-
event combinations. However, only a few of these 
appear with substantial frequency. For example, 
the Population-Center entities only occur in 4 
types of event mentions with the conditional prob-
ability more than 0.05. From Table 3, we can find 
that only Attack and Transport events co-occur 
frequently with Population-Center entities (see 
Figure 1 and Table 3). 
Event Cond.Prob. Freq. 
Transport 0.368 197 
Attack 0.295 158 
Meet 0.073 39 
Die 0.069 37 
Table 3: Events co-occurring with Population-
Center with the conditional probability > 0.05 
Actually we find that most entity types appear in 
more restricted event mentions than Population-
Center entity. For example, Air entity only co-
occurs with 5 event types (Attack, Transport, Die, 
Transfer-Ownership and Injure), and Exploding 
1130
entity co-occurs with 4 event types (see Figure 1). 
Especially, they only co-occur with one or two 
event types with the conditional probability more 
than 0.05. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 24 7 12 
Freq. >10 37 4 2 
Freq. >50 41 1 1 
Table 4: Distribution of entity-event combination 
corresponding to different co-occurrence frequency 
Table 4 gives the distributions of whole ACE 
entity types co-occurring with event types. We can 
find that there are 37 types of entities (out of 43 in 
total) appearing in less than 5 types of event men-
tions when entity-event co-occurrence frequency is 
larger than 10, and only 2 (e.g. Individual) appear-
ing in more than 10 event types. And when the fre-
quency is larger than 50, there are 41 (95%) entity 
types co-occurring with less than 5 event types. 
These distributions show the fact that most in-
stances of a certain entity type normally participate 
in events of the same type. And the distributions 
might be good predictors for event type detection 
and trigger determination. 
Air (Entity type) 
Attack 
event 
Fighter plane (subtype 1): 
?MiGs? ?enemy planes? ?warplanes? ?allied 
aircraft? ?U.S. jets? ?a-10 tank killer? ?b-1 
bomber? ?a-10 warthog? ?f-14 aircraft? 
?apache helicopter? 
Spacecraft (subtype 2): 
?russian soyuz capsule? ?soyuz? 
Civil aviation (subtype 3): 
?airliners? ?the airport? ?Hooters Air execu-
tive? 
Transport 
event 
Private plane (subtype 4): 
?Marine One? ?commercial flight? ?private 
plane? 
Table 5: Event types co-occurred with Air entities 
Besides, an ACE entity type actually can be di-
vided into more cohesive subtypes according to 
similarity of background of entity, and such a sub-
type nearly always co-occur with unique event 
type. For example, the Air entities can be roughly 
divided into 4 subtypes: Fighter plane, Spacecraft, 
Civil aviation and Private plane, within which the 
Fighter plane entities all appear in Attack event 
mentions, and other three subtypes all co-occur 
with Transport events (see Table 5). This consis-
tency of entities in a subtype is helpful to improve 
the precision of the event type predictor. 
4.2 Role Consistency and Distribution 
The same thing happens for entity-role combina-
tions: entities of the same type normally play the 
same role, especially in the event mentions of the 
same type. For example, the Population-Center 
entities occur in ACE corpus as only 4 role types: 
Place, Destination, Origin and Entity respectively 
with conditional probability 0.615, 0.289, 0.093, 
0.002 (see Figure 2). And They mainly appear in 
Transport event mentions as Place, and in Attack 
as Destination. Particularly the Exploding entities 
only occur as Instrument and Artifact respectively 
with the probability 0.986 and 0.014. They almost 
entirely appear in Attack events as Instrument. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 32 5 6 
Freq. >10 38 3 2 
Freq. >50 42 1 0 
Table 6: Distribution of entity-role combination 
corresponding to different co-occurrence frequency 
Table 6 gives the distributions of whole entity-
role combinations in ACE corpus. We can find that 
there are 38 entity types (out of 43 in total) occur 
as less than 5 role types when the entity-role co-
occurrence frequency is larger than 10. There are 
42 (98%) when the frequency is larger than 50, and 
only 2 (e.g. Individual) when larger than 10. The 
distributions show that the instances of an entity 
type normally occur as consistent role, which is 
helpful for cross-entity inference to predict roles. 
5 Cross-entity Approach  
In this section we present our approach to using 
blind cross-entity inference to improve sentence-
level ACE event extraction. 
Our event extraction system extracts events in-
dependently for each sentence, because the defini-
tion of event mention constrains them to appear in 
the same sentence. Every sentence that at least in-
volves one entity mention will be regarded as a 
candidate event mention, and a randomly selected 
entity mention from the candidate will be the star-
ing of the whole extraction process. For the entity 
mention, information retrieval is used to mine its 
background knowledge from Web, and its type is 
determined by comparing the knowledge with 
those in training corpus. Based on the entity type, 
the extraction system performs our step-by-step 
cross-entity inference to predict the attributes of 
1131
the candidate event mention: trigger, event type, 
arguments, roles and whether or not being an event 
mention. The main frame of our event extraction 
system is shown in Figure 3, which includes both 
training and testing processes. 
 
Figure 3. The frame of cross-entity inference for event extraction (including training and testing processes) 
In the training process, for every entity type in 
the ACE training corpus, a clustering technique 
(CLUTO toolkit)3 is used to divide it into different 
cohesive subtypes, each of which only contains the 
entities of the same background. For instance, the 
Air entities will be divided into Fighter plane, 
Spacecraft, Civil aviation, Private plane, etc (see 
Table 5). And for each subtype, we mine event 
mentions where this type of entities appear from 
ACE training corpus, and extract all the words 
which trigger the events to establish corresponding 
trigger list. Besides, a set of support vector ma-
chine (SVM) based classifiers are also trained: 
y Argument Classifier: to distinguish arguments 
of a potential trigger from non-arguments4; 
y Role Classifier: to classify arguments by ar-
gument role; 
y Reportable-Event Classifier (Trigger Classi-
fier): Given entity types, a potential trigger, an 
event type, and a set of arguments, to determine 
whether there is a reportable event mention. 
                                                          
3http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=h
tml&identifier=ADA439508 
4 It is noteworthy that a sentence may include more than one 
event (more than one trigger). So it is necessary to distinguish 
arguments of a potential trigger from that of others. 
In the test process, for each candidate event 
mention, our event extraction system firstly pre-
dicts its triggers and event types: given an ran-
domly selected entity mention from the candidate, 
the system determines the entity subtype it belong-
ing to and the corresponding trigger list, and then 
all non-entity words in the candidate are scanned 
for a instance of triggers from the list. When an 
instance is found, the system tags the candidate as 
the event type that the most frequently co-occurs 
with the entity subtype in the events that triggered 
by the instance. Secondly the argument classifier is 
applied to the remaining mentions in the candidate; 
for any argument passing that classifier, the role 
classifier is used to assign a role to it. Finally, once 
all arguments have been assigned, the reportable-
event classifier is applied to the candidate; if the 
result is successful, this event mention is reported. 
5.1 Further Division of Entity Type  
One of the most important pretreatments before 
our blind cross-entity inference is to divide the 
ACE entity type into more cohesive subtype. The 
greater consistency among backgrounds of entities 
in such a subtype might be good to improve the 
precision of cross-entity inference.  
1132
For each ACE entity type, we collect all entity 
mentions of the type from training corpus, and re-
gard each such mention as a query to retrieve the 
50 most relevant documents from Web. Then we 
select 50 key words that the most weighted by 
TFIDF in the documents to roughly describe back-
ground of entity. After establishing the vector 
space model (VSM) for each entity mention of the 
type, we adopt a clustering toolkit (CLUTO) to 
further divide the mentions into different subtypes. 
Finally, for each subtype, we describe its centroid 
by using 100 key words which the most frequently 
occurred in relevant documents of entities of the 
subtype. 
In the test process, for an entity mention in a 
candidate event mention, we determine its type by 
comparing its background against all centroids of 
subtypes in training corpus, and the subtype whose 
centroid has the most Cosine similarity with the 
background will be assigned to the entity. It is 
noteworthy that global information from the Web 
is only used to measure the entity-background con-
sistency and not directly in the inference process. 
Thus our event extraction system actually still per-
forms a sentence-level inference based on local 
information. 
5.2 Cross-Entity Inference 
Our event extraction system adopts a step-by-
step cross-entity inference to predict event. As dis-
cussed above, the first step is to determine the trig-
ger in a candidate event mention and tag its event 
type based on consistency of entity type. Given the 
domain of event mention that restrained by the 
known trigger, event type and entity subtype, the 
second step is to distinguish the most probable ar-
guments that co-occurring in the domain from the 
non-arguments. Then for each of the arguments, 
the third step can use the co-occurring arguments 
in the domain as important contexts to predict its 
role. Finally, the inference process determines 
whether the candidate is a reportable event men-
tion according to a confidence coefficient. In the 
following sections, we focus on introducing the 
three classifiers: argument classifier, role classifier 
and reportable-event classifier. 
5.2.1   Cross-Entity Argument Classifier 
For a candidate event mention, the first step 
gives its event type, which roughly restrains the 
domain of event mentions where the arguments of 
the candidate might co-occur. On the basis, given 
an entity mention in the candidate and its type (see 
the pretreatment process in section 5.1), the argu-
ment classifier could predict whether other entity 
mentions co-occur with it in such a domain, if yes, 
all the mentions will be the arguments of the can-
didate. In other words, if we know an entity of a 
certain type participates in some event, we will 
think of what entities also should participate in the 
event. For instance, when we know a defendant 
goes on trial, we can conclude that the judge, law-
yer and witness should appear in court. 
Argument Classifier 
Feature 1: an event type (an event-mention domain) 
Feature 2: an entity subtype 
Feature 3: entity-subtype co-occurrence in domain 
Feature 4: distance to trigger 
Feature 5: distances to other arguments 
Feature 6: co-occurrence with trigger in clause 
Role Classifier 
Feature 1 and Feature 2 
Feature 7: entity-subtypes of arguments 
Reportable-Event Classifier 
Feature 1 
Feature 8: confidence coefficient of trigger in domain 
Feature 9: confidence coefficient of role in domain 
Table 7: Features selected for SVM-based cross-
entity classifiers 
A SVM-based argument classifier is used to de-
termine arguments of candidate event mention. 
Each feature of this classifier is the conjunction of: 
y The subtype of an entity 
y The event type we are trying to assign an ar-
gument to 
y A binary indicator of whether this entity sub-
type co-occurs with other subtypes in such an 
event type (There are 266 entity subtypes, and so 
266 features for each instance) 
Some minor features, such as another binary indi-
cator of whether arguments co-occur with trigger 
in the same clause (see Table 7). 
5.2.2 Cross-Entity Role Classifier 
For a candidate event mention, the arguments 
that given by the second step (argument classifier) 
provide important contextual information for pre-
dicting what role the local entity (also one of the 
arguments) takes on. For instance, when citizens 
(Arg1) co-occur with terrorist (Arg2), most likely 
the role of Arg1 is Victim. On the basis, with the 
help of event type, the prediction might be more 
1133
precise. For instance, if the Arg1 and Arg2 co-
occur in an Attack event mention, we will have 
more confidence in the Victim role of Arg1. 
Besides, as discussed in section 4, entities of the 
same type normally take on the same role in simi-
lar events, especially when they co-occur with sim-
ilar arguments in the events (see Table 2). 
Therefore, all instances of co-occurrence model 
{entity subtype, event type, arguments} in training 
corpus could provide effective evidences for pre-
dicting the role of argument in the candidate event 
mention. Based on this, we trained a SVM-based 
role classifier which uses following features: 
y Feature 1 and Feature 2 (see Table 7) 
y Given the event domain that restrained by the 
entity and event types, an indicator of what sub-
types of arguments appear in the domain. (266 en-
tity subtypes make 266 features for each instance) 
5.2.3 Reportable-Event Classifier 
At this point, there are still two issues need to be 
resolved. First, some triggers are common words 
which often mislead the extraction of candidate 
event mention, such as ?it?, ?this?, ?what?, etc. 
These words only appear in a few event mentions 
as trigger, but when they once appear in trigger list, 
a large quantity of noisy sentences will be regarded 
as candidates because of their commonness in sen-
tences. Second, some arguments might be tagged 
as more than one role in specific event mentions, 
but as ACE event guideline, one argument only 
takes on one role in a sentence. So we need to re-
move those with low confidence. 
A confidence coefficient is used to distinguish 
the correct triggers and roles from wrong ones. The 
coefficient calculate the frequency of a trigger (or a 
role) appearing in specific domain of event men-
tions and that in whole training corpus, then com-
bines them to represent its confidence degree, just 
like TFIDF algorithm. Thus, the more typical trig-
gers (or roles) will be given high confidence. 
Based on the coefficient, we use a SVM-based 
classifier to determine the reportable events. Each 
feature of this classifier is the conjunction of: 
y An event type (domain of event mentions) 
y Confidence coefficients of triggers in domain 
y Confidence coefficients of roles in the domain. 
6 Experiments 
We followed Liao (2010)?s evaluation and ran-
domly select 10 newswire texts from the ACE 
2005 training corpus as our development set, 
which is used for parameter tuning, and then con-
duct a blind test on a separate set of 40 ACE 2005 
newswire texts. We use the rest of the ACE train-
ing corpus (549 documents) as training data for our 
event extraction system.  
To compare with the reported work on cross-
event inference (Liao, 2010) and its sentence-level 
baseline system, we cross-validate our method on 
10 separate sets of 40 ACE texts, and report the 
optimum, worst and mean performances (see Table 
8) on the data by using Precision (P), Recall (R) 
and F-measure (F). In addition, we also report the 
performance of two human annotators on 40 ACE 
newswire texts (a random blind test set): one 
knows the rules of event extraction; the other 
knows nothing about it. 
6.1 Main Results  
From the results presented in Table 8, we can 
see that using the cross-entity inference, we can 
improve the F score of sentence-level event extrac-
tion for trigger classification by 8.59%, argument 
classification by 11.86%, and role classification by 
11.9% (mean performance). Compared to the 
cross-event inference, we gains 2.87% improve-
ment for argument classification, and 3.81% for 
role classification (mean performance). Especially, 
our worst results also have better performances 
than cross-event inference. 
Nonetheless, the cross-entity inference has 
worse F score for trigger determination. As we can 
see, the low Recall score weaken its F score (see 
Table 8). Actually, we select the sentence which at 
least includes one entity mention as candidate 
event mention, but lots of event mentions in ACE 
never include any entity mention. Thus we have 
missed some mentions at the starting of inference 
process. 
In addition, the annotator who knows the rules 
of event extraction has a similar performance trend 
with systems: high for trigger classification, mid-
dle for argument classification, and low for role 
classification (see Table 8). But the annotator who 
never works in this field obtains a different trend: 
higher performance for argument classification. 
This phenomenon might prove that the step-by-
step inference is not the only way to predicate 
event mention because human can determine ar-
guments without considering triggers and event 
types. 
1134
                            Performance 
System/Human Trigger (%) Argument (%) Role (%) 
 P R F P R F P R F 
Sentence-level baseline 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46
Cross-event inference 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55
Cross-entity inference (optimum) 73.4 66.2 69.61 56.96 55.1 56 49.3 46.59 47.9 
Cross-entity inference (worst) 71.3 64.17 66.1 51.28 50.3 50.78 46.3 44.3 45.28
Cross-entity inference (mean) 72.9 64.3 68.33 53.4 52.9 53.15 51.6 45.5 48.36
Human annotation 1 (blind) 58.9 59.1 59.0 62.6 65.9 64.2 50.3 57.69 53.74
Human annotation 2 (know rules) 74.3 76.2 75.24 68.5 75.8 71.97 61.3 68.8 64.86
Table 8: Overall performance on blind test data
6.2 Influence of Clustering on Inference  
A main part of our blind inference system is the 
entity-type consistency detection, which relies 
heavily on the correctness of entity clustering and 
similarity measurement. In training, we used 
CLUTO clustering toolkit to automatically gener-
ate different types of entities based on their back-
ground-similarities. In testing, we use K-nearest 
neighbor algorithm to determine entity type. 
Fighter plane (subtype 1 in Air entities): 
?warplanes? ?allied aircraft? ?U.S. jets? ?a-10 tank killer? 
?b-1 bomber? ?a-10 warthog? ?f-14 aircraft? ?apache heli-
copter? ?terrorist? ?Saddam? ?Saddam Hussein? ?Bagh-
dad??
Table 9: Noises in subtype 1 of ?Air? entities (The 
blod fonts are noises) 
We obtained 129 entity subtypes from training 
set. By randomly inspecting 10 subtypes, we found 
nearly every subtype involves no less than 19.2% 
noises. For example, the subtype 1 of ?Air? in Ta-
ble 5 lost the entities of ?MiGs? and ?enemy 
planes?, but involved ?terrorist?, ?Saddam?, etc 
(See Table 9). Therefore, we manually clustered 
the subtypes and retry the step-by-step cross-entity 
inference. The results (denoted as ?Visible 1?) are 
shown in Table 10, within which, we additionally 
show the performance of the inference on the 
rough entity types provided by ACE (denoted as 
?Visible 2?), such as the type of ?Air?, ?Popula-
tion-Center?, ?Exploding?, etc., which normally 
can be divided into different more cohesive sub-
types. And the ?Blind? in Table 10 denotes the 
performances on our subtypes obtained by CLUTO. 
It is surprised that the performances (see Table 
10, F-score) on ?Visible 1? entity subtypes are just 
a little better than ?Blind? inference. So it seems 
that the noises in our blind entity types (CLUTO 
clusters) don?t hurt the inference much. But by re-
inspecting the ?Visible 1? subtypes, we found that 
their granularities are not enough small: the 89 
manual entity clusters actually can be divided into 
more cohesive subtypes. So the improvements of 
inference on noise-free ?Visible 1? subtypes are 
partly offset by loss on weakly consistent entities 
in the subtypes. It can be proved by the poor per-
formances on ?Visible 2? subtypes which are much 
more general than ?Visible 1?. Therefore, a rea-
sonable clustering method is important in our in-
ference process. 
F-score Trigger  Argument Role 
Blind 68.33 53.15 48.36 
Visible 1 69.15 53.65 48.83 
Visible 2 51.34 43.40 39.95 
Table 10: Performances on visible VS blind  
7 Conclusions and Future Work  
We propose a blind cross-entity inference method 
for event extraction, which well uses the consis-
tency of entity mention to achieve sentence-level 
trigger and argument (role) classification. Experi-
ments show that the method has better perform-
ance than cross-document and cross-event 
inferences in ACE event extraction. 
The inference presented here only considers the 
helpfulness of entity types of arguments to role 
classification. But as a superior feature, contextual 
roles can provide more effective assistance to role 
determination of local argument. For instance, 
when an Attack argument appears in a sentence, a 
Target might be there. So if we firstly identify 
simple roles, such as the condition that an argu-
ment has only a single role, and then use the roles 
as priori knowledge to classify hard ones, may be 
able to further improve performance.
Acknowledgments 
We thank Ruifang He. And we acknowledge the 
support of the National Natural Science Founda-
tion of China under Grant Nos. 61003152, 
60970057, 90920004. 
1135
References  
David Ahn. 2006. The stages of event extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events.Sydney, Aus-
tralia. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling. In Proc. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363?370, 
Ann Arbor, MI, June. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
In Proc. ACE 2005 Evaluation Workshop, Gaithers-
burg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Using 
Surface Text Features. In Proc. AAAI06 Workshop on 
Event Extraction and Synthesis. Boston, MA. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254?262, Columbus, OH, 
June. 
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event 
Extraction. In Proc. ACL-2010, pages 789-797, Upp-
sala, Sweden, July. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi resolution Framework for Information Extrac-
tion from Free Text. In Proc. 45th Annual Meeting of 
the Association of Computational Linguistics, pages 
592?599, Prague, Czech Republic, June. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. In Proc. Joint Confer-
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 2007, pages 717?727, Prague, Czech Re-
public, June. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. Conference on Empirical 
Methods in Natural Language Processing 2009, 
(EMNLP-09). 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Proc. 
ACL 1995. Cambridge, MA. 
1136
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190?195,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Broadcast News Story Segmentation Using Manifold Learning on Latent
Topic Distributions
Xiaoming Lu1,2, Lei Xie1?, Cheung-Chi Leung2, Bin Ma2, Haizhou Li2
1School of Computer Science, Northwestern Polytechnical University, China
2Institute for Infocomm Research, A?STAR, Singapore
luxiaomingnpu@gmail.com, lxie@nwpu.edu.cn, {ccleung,mabin,hli}@i2r.a-star.edu.sg
Abstract
We present an efficient approach for
broadcast news story segmentation using a
manifold learning algorithm on latent top-
ic distributions. The latent topic distribu-
tion estimated by Latent Dirichlet Alloca-
tion (LDA) is used to represent each text
block. We employ Laplacian Eigenmap-
s (LE) to project the latent topic distribu-
tions into low-dimensional semantic rep-
resentations while preserving the intrinsic
local geometric structure. We evaluate t-
wo approaches employing LDA and prob-
abilistic latent semantic analysis (PLSA)
distributions respectively. The effects of
different amounts of training data and dif-
ferent numbers of latent topics on the two
approaches are studied. Experimental re-
sults show that our proposed LDA-based
approach can outperform the correspond-
ing PLSA-based approach. The proposed
approach provides the best performance
with the highest F1-measure of 0.7860.
1 Introduction
Story segmentation refers to partitioning a mul-
timedia stream into homogenous segments each
embodying a main topic or coherent story (Allan,
2002). With the explosive growth of multimedia
data, it becomes difficult to retrieve the most rel-
evant components. For indexing broadcast news
programs, it is desirable to divide each of them
into a number of independent stories. Manual seg-
mentation is accurate but labor-intensive and cost-
ly. Therefore, automatic story segmentation ap-
proaches are highly demanded.
Lexical-cohesion based approaches have been
widely studied for automatic broadcast news story
segmentation (Beeferman et al, 1997; Choi, 1999;
Hearst, 1997; Rosenberg and Hirschberg, 2006;
?corresponding author
Lo et al, 2009; Malioutov and Barzilay, 2006;
Yamron et al, 1999; Tur et al, 2001). In this
kind of approaches, the audio portion of the da-
ta stream is passed to an automatic speech recog-
nition (ASR) system. Lexical cues are extracted
from the ASR transcripts. Lexical cohesion is the
phenomenon that different stories tend to employ
different sets of terms. Term repetition is one of
the most common appearances.
These rigid lexical-cohesion based approach-
es simply take term repetition into consideration,
while term association in lexical cohesion is ig-
nored. Moreover, polysemy and synonymy are not
considered. To deal with these problems, some
topic model techniques which provide conceptu-
al level matching have been introduced to text and
story segmentation task (Hearst, 1997). Proba-
bilistic latent semantic analysis (PLSA) (Hofman-
n, 1999) is a typical instance and used widely.
PLSA is the probabilistic variant of latent seman-
tic analysis (LSA) (Choi et al, 2001), and offers a
more solid statistical foundation. PLSA provides
more significant improvement than LSA for story
segmentation (Lu et al, 2011; Blei and Moreno,
2001).
Despite the success of PLSA, there are con-
cerns that the number of parameters in PLSA
grows linearly with the size of the corpus. This
makes PLSA not desirable if there is a consid-
erable amount of data available, and causes seri-
ous over-fitting problems (Blei, 2012). To deal
with this issue, Latent Dirichlet Allocation (L-
DA) (Blei et al, 2003) has been proposed. LDA
has been proved to be effective in many segmenta-
tion tasks (Arora and Ravindran, 2008; Hall et al,
2008; Sun et al, 2008; Riedl and Biemann, 2012;
Chien and Chueh, 2012).
Recent studies have shown that intrinsic di-
mensionality of natural text corpus is significant-
ly lower than its ambient Euclidean space (Belkin
and Niyogi, 2002; Xie et al, 2012). Therefore,
190
Laplacian Eigenmaps (LE) was proposed to com-
pute corresponding natural low-dimensional struc-
ture. LE is a geometrically motivated dimen-
sionality reduction method. It projects data into
a low-dimensional representation while preserv-
ing the intrinsic local geometric structure infor-
mation (Belkin and Niyogi, 2002). The locali-
ty preserving property attempts to make the low-
dimensional data representation more robust to the
noise from ASR errors (Xie et al, 2012).
To further improve the segmentation perfor-
mance, using latent topic distributions and LE in-
stead of term frequencies to represent text blocks
is studied in this paper. We study the effects of
the size of training data and the number of latent
topics on the LDA-based and the PLSA-based ap-
proaches. Another related work (Lu et al, 2013)
is to use local geometric information to regularize
the log-likelihood computation in PLSA.
2 Our Proposed Approach
In this paper, we propose to apply LE on the L-
DA topic distributions, each of which is estimat-
ed from a text block. The low-dimensional vec-
tors obtained by LE projection are used to detect
story boundaries through dynamic programming.
Moreover, as in (Xie et al, 2012), we incorporate
the temporal distances between block pairs as a
penalty factor in the weight matrix.
2.1 Latent Dirichlet Allocation
Latent Dirichlet alocation (LDA) (Blei et al,
2003) is a generative probabilistic model of a cor-
pus. It considers that documents are represented
as random mixtures over latent topics, where each
topic is characterized by a distribution over terms.
In LDA, given a corpus D = {d1, d2, . . . , dM}
and a set of terms W = (w1, w2, . . . , wV ), the
generative process can be summarized as follows:
1) For each document d, pick a multinomial dis-
tribution ? from a Dirichlet distribution parameter
?, denoted as ? ? Dir(?).
2) For each term w in document d, select a topic
z from the multinomial distribution ?, denoted as
z ? Multinomial(?).
3) Select a term w from P (w|z, ?), which is a
multinomial probability conditioned on the topic.
An LDA model is characterized by two sets of
prior parameters ? and ?. ? = (?1, ?2, . . . , ?K)
represents the Dirichlet prior distributions for each
K latent topics. ? is aK?V matrix, which defines
the latent topic distributions over terms.
2.2 Construction of weight matrix in
Laplacian Eigenmaps
Laplacian Eigenmaps (LE) is introduced to project
high-dimensional data into a low-dimensional rep-
resentation while preserving its locality property.
Given the ASR transcripts of N text blocks, we ap-
ply LDA algorithm to compute the corresponding
latent topic distributions X = [x1, x2, . . . , xN ] in
RK , where K is the number of latent topics, name-
ly the dimensionality of LDA distributions.
We use G to denote an N-node (N is number of
LDA distributions) graph which represents the re-
lationship between all the text block pairs. If dis-
tribution vectors xi and xj come from the same
story, we put an edge between nodes i and j. We
define a weight matrix S of the graph G to denote
the cohesive strength between the text block pairs.
Each element of this weight matrix is defined as:
sij = cos(xi, xj)?|i?j|, (1)
where ?|i?j| serves the penalty factor for the dis-
tance between i and j. ? is a constant lower than
1.0 that we tune from a set of development data.
It makes the cohesive strength of two text blocks
dramatically decrease when their distance is much
larger than the normal length of a story.
2.3 Data projection in Laplacian Eigenmaps
Given the weight matrix S, we define C as the di-
agonal matrix with its element:
cij =
?K
i=1
sij . (2)
Finally, we obtain the Laplacian matrix L, which
is defined as:
L = C? S. (3)
We use Y = [y1, y2, . . . , yN ] (yi is a column
vector) to indicate the low-dimensional represen-
tation of the latent topic distributions X. The pro-
jection from the latent topic distribution space to
the target space can be defined as:
f : xi ? yi. (4)
A reasonable criterion for computing an optimal
mapping is to minimize the objective as follows:
K?
i=1
K?
j=1
? yi ? yj ?2 sij . (5)
Under this constraint condition, we can preserve
the local geometrical property in LDA distribu-
tions. The objective function can be transformed
191
as:
K?
i=1
K?
j=1
(yi ? yj)sij = tr(YTLY). (6)
Meanwhile, zero matrix and matrices with it-
s rank less than K are meaningless solutions for
our task. We impose YTLY = I to prevent this
situation, where I is an identity matrix. By the
Reyleigh-Ritz theorem (Lutkepohl, 1997), the so-
lution can obtained by the Q smallest eigenvalues
of the generalized eigenmaps problem:
XLXT y = ?XCXT y. (7)
With this formula, we calculate the mapping ma-
trix Y, and its row vectors y?1, y?2, . . . , y?Q are in the
order of their eigenvalues ?1 ? ?2 ? . . . ? ?Q.
y?i is a Q-dimensional (Q<K) eigenvectors.
2.4 Story boundary detection
In story boundary detection, dynamic program-
ming (DP) approach is adopted to obtain the glob-
al optimal solution. Given the low-dimensional se-
mantic representation of the test data, an objective
function can be defined as follows:
? =
Ns?
t=1
(
?
i,j?Segt
? yi ? yj ?2), (8)
where yi and yj are the latent topic distributions of
text blocks i and j respectively, and ? yi ? yj ?2
is the Euclidean distance between them. Segt in-
dicates these text blocks assigned to a certain hy-
pothesized story. Ns is the number of hypothe-
sized stories.
The story boundaries which minimize the ob-
jective function ? in Eq.(8) form the optimal re-
sult. Compared with classical local optimal ap-
proach, DP can more effectively capture the s-
mooth story shifts, and achieve better segmenta-
tion performance.
3 Experimental setup
Our experiments were evaluated on the ASR tran-
scripts provided in TDT2 English Broadcast news
corpus1, which involved 1033 news programs. We
separated this corpus into three non-overlapping
sets: a training set of 500 programs for parameter
estimation in topic modeling and LE, a develop-
ment set of 133 programs for empirical tuning and
a test set of 400 programs for performance evalu-
ation.
In the training stage, ASR transcripts with man-
ually labeled boundary tags were provided. Text
1http://projects.ldc.upenn.edu/TDT2/
streams were broken into block units according to
the given boundary tags, with each text block be-
ing a complete story. In the segmentation stage,
we divided test data into text blocks using the time
labels of pauses in the transcripts. If the pause du-
ration between two blocks last for more than 1.0
sec, it was considered as a boundary candidate. To
avoid the segmentation being suffered from ASR
errors and the out-of-vocabulary issue, phoneme
bigram was used as the basic term unit (Xie et al,
2012). Since the ASR transcripts were at word lev-
el, we performed word-to-phoneme conversion to
obtain the phoneme bigram basic units. The fol-
lowing approaches, in which DP was used in story
boundary detection, were evaluated in the experi-
ments:
? PLSA-DP: PLSA topic distributions were
used to compute sentence cohesive strength.
? LDA-DP: LDA topic distributions were used
to compute sentence cohesive strength.
? PLSA-LE-DP: PLSA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesive strength.
? LDA-LE-DP: LDA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesion strength.
For LDA, we used the implementation from
David M. Blei?s webpage2. For PLSA, we used
the Lemur Toolkit3.
F1-measure was used as the evaluation crite-
rion.We followed the evaluation rule: a detected
boundary candidate is considered correct if it lies
within a 15 sec tolerant window on each side of a
reference boundary. A number of parameters were
set through empirical tuning on the developent set.
The penalty factor was set to 0.8. When evaluating
the effects of different size of the training set, the
number of latent topics in topic modeling process
was set to 64. After the number of latent topics
was fixed, the dimensionality after LE projection
was set to 32. When evaluating the effects of d-
ifferent number of latent topics in topic modeling
computation, we fixed the size of the training set
to 500 news programs and changed the number of
latent topics from 16 to 256.
4 Experimental results and analysis
4.1 Effect of the size of training dataset
We used the training set from 100 programs to 500
programs (adding 100 programs in each step) to e-
2http://www.cs.princeton.edu/ blei/lda-c/
3http://www.lemurproject.org/
192
valuate the effects of different size of training data
in both PLSA-based and LDA-based approaches.
Figure 1 shows the results on the development set
and the test set.
0.55
0.6
0.65
0.7
0.75
0.8
100 200 300 400 500
F1-
me
asu
re
PLSA-LE-DP LDA-LE-DP
LDA-DP
PLSA-DP
Development Set
0.55
0.6
0.65
0.7
0.75
0.8
100 200 300 400 500
F1-
me
asu
re
Number of programs in training data
PLSA-LE-DP
LDA-LE-DP
PLSA-DP
LDA-DP
Test Set
Figure 1: Segmentation performance with differ-
ent amounts of training data
LDA-LE-DP approach achieved the best result
(0.7927 and 0.7860) on both the development and
the test sets, when there were 500 programs in the
training set. This demonstrates that LDA model
and LE projection used in combination is excellent
for the story segmentation task. The LE projection
applied on the latent topic representations made
relatively 9.88% and 10.93% improvement over
the LDA-based approach and the PLSA-based ap-
proach, respectively on the test set. We can reveal
that employing LE on PLSA and LDA topic dis-
tributions achieves much better performance than
the corresponding approaches without using LE.
We have compared the performances between
PLSA and LDA. We found that when the train-
ing data size was small, PLSA performed better
than LDA. Both PLSA-based and LDA-based ap-
proaches got better with the increase in the size of
the training data set. All the four approaches had
similar performances on the development set and
the test set.
With the increase in the size of the training da-
ta, the LDA-based approaches were improved dra-
matically. They even outperformed the PLSA-
based approaches when the training data contained
more than 300 programs. This may be attributed
to the fact that LDA needs more training data to
estimate the parameters. When the training data is
not enough, its parameters estimated in the train-
ing stage is not stable for the development and the
test data. Moreover, compared with PLSA, the pa-
rameters in LDA do not grow linearly with the size
of the corpus.
4.2 Effect of the number of latent topics
We evaluated the F1-measure of the four ap-
proaches with different number of latent topics
prior to LE projection. Figure 2 shows the cor-
responding results.
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
16 32 48 64 80 96 128 256
F1-m
easu
re
Number of latent topics
PLSA-DP
LDA-DP
PLSA-LE-DP
LDA-LE-DP
Figure 2: Segmentation performance with differ-
ent numbers of latent topics
The best performances (0.7816-0.7847) were
achieved at the number of latent topics between
64 and 96. When the number of latent topics was
increased from 16 to 64, F1-measure increased.
When the number of latent topics was larger than
96, F1-measure decreased gradually. We found
that the best results were achieved when the num-
ber of topics was close to the real number of top-
ics. There are 80 manually labeled main topics in
the test set.
We observe that LE projection makes the topic
model more stable with different numbers of latent
topics. The best and the worst performances dif-
fered by relatively 9.12% in LDA-DP and 7.97%
in PLSA-DP. However, the relative difference of
2.79% and 2.46% were observed in LDA-LE-DP
and PLSA-LE-DP respectively.
5 Conclusions
Our proposed approach achieves the best F1-
measure of 0.7860. In the task of story segmen-
tation, we believe that LDA can avoid data overfit-
ting problem when there is a sufficient amount of
training data. This is also applicable to LDA-LE-
LP. Moreover, we find that when we apply LE pro-
jection to latent topic distributions, the segmen-
tation performances become less sensitive to the
predefined number of latent topics.
193
Acknowledgments
This work is supported by the National Natu-
ral Science Foundation of China (61175018), the
Natural Science Basic Research Plan of Shaanx-
i Province (2011JM8009) and the Fok Ying Tung
Education Foundation (131059).
References
J. Allan. 2002. Topic Detection and Tracking: Event-
Based Information Organization. Kluwer Academic
Publisher, Norwell, MA.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A Model of Lexical Attraction and repulsion.
In Proceedings of the 8th Conference on European
Chapter of the Association for Computational Lin-
guistics (EACL), pp.373-380.
Freddy Y. Y. Choi. 2000. Advances in Domain In-
dependent Linear Text Segmentation. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference
(NAACL), pp.26-33.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pp.20-57.
Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
Haizhou Li. 2011. Probabilistic Latent Seman-
tic Analysis for Broadcast New Story Segmentation.
In Proceedings of the 11th Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pp.1109-1112.
David M. Blei. 2012. Probabilistic topic models.
Communication of the ACM, vol. 55, pp.77-84.
David M. Blei, Andrew Y. Ng, Michael I. Jordan.
2003. Latent Dirichlet Allocation. the Journal of
Machine Learning Research, vol. 3, pp.993-1022.
Marti A. Hearst. 1997. TextTiling: Segmenting Text
into Multiparagraph subtopic passages. Computa-
tional Liguistic, vol. 23, pp.33-64.
Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke,
Elizabeth Shriberg. 2001. Integrating Prosodic
and Lexicial Cues for Automatic Topic Segmenta-
tion. Computational Liguistic, vol. 27, pp.31-57.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
Segmentation of Broadcast News in English, Man-
darin and Aribic. In Proceedings of the 7th North
American Chapter of the Association for Compu-
tational Linguistics Conference (NAACL), pp.125-
128.
David M. Blei and Pedro J. Moreno. 2001. Topic Seg-
mentation with An Aspect Hidden Markov Model. In
Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrival (SIGIR), pp.343-348.
Wai-Kit Lo, Wenying Xiong, Helen Meng. 2009. Au-
tomatic Story Segmentation Using a Bayesian De-
cision Framwork for Statistical Models of Lexical
Chain Feature. In Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pp.357-364.
Igor Malioutov and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmenation. In Pro-
ceedings of the 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp.25-32.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, Juhanna
Moore. 2001. Latent Semantic Analysis for Tex-
t Segmentation. In Proceedings of the 2001 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), pp.109-117.
Rachit Arora and Balaraman Ravindran. 2008. Latent
Dirichlet Allocation Based Multi-document Summa-
rization. In Proceedings of the 2nd Workshop on
Analytics for Noisy Unstructured Text Data (AND),
pp.91-97.
David Hall, Daniel Jurafsky, Christopher D. Manning.
2008. Latent Studying the History Ideas Using Topic
Models. In Proceedings of the 2008 Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP), pp.363-371.
Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu. 2008.
Text Segmentation with LDA-based Fisher Kernel.
In Proceedings of the 46th Annual Meeting of the As-
socation for Computational Linguistics on Human
Language Technologies (HLT-ACL), pp.269-272.
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
Eigenmaps for Dimensionality Reduction and Da-
ta Representation. Neural Computation, vol. 15,
pp.1383-1396.
Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang.
2012. Laplacian Eigenmaps for Automatic Story
Segmentation of Broadcast News. IEEE Transaction
on Audio, Speech and Language Processing, vol. 20,
pp.264-277.
Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang
Zhai. 2008. Modeling Hidden Topics on Document
Manifold. In Proceedings of the 17th ACM Confer-
ence on Information and Knowledge Managemen-
t (CIKM), pp.911-120.
Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
and Haizhou Li. 2013. Broadcast News Story Seg-
mentation Using Latent Topics on Data Manifold. In
Proceedings of the 38th International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
194
J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van
Mulbregt. 1999. AHiddenMarkov Model Approach
to Text Segmenation and Event Tracking. In Pro-
ceedings of the 1999 International Conference on
Acoustics, Speech, and Signal Processing (ICASSP),
pp.333-336.
Martin Riedl and Chris Biemann. 2012. Text Segmen-
tation with Topic Models. the Journal for Language
Technology and Computational Linguistics, pp.47-
69.
P. Fragkou , V. Petridis , Ath. Kehagias. 2002. A Dy-
namic Programming algorithm for Linear Text Story
Segmentation. the Joural of Intelligent Information
Systems, vol. 23, pp.179-197.
H. Lutkepohl. 1997. Handbook of Matrices. Wiley,
Chichester, UK.
Jen-Tzung Chien and Chuang-Hua Chueh. 2012.
Topic-Based Hieraachical Segmentation. IEEE
Transaction on Audio, Speech and Language Pro-
cessing, vol. 20, pp.55-66.
195
