Proceedings of the Fourth International Natural Language Generation Conference, pages 25?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Individuality and Alignment in Generated Dialogues
Amy Isard and Carsten Brockmann and Jon Oberlander
School of Informatics, University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
{Amy.Isard, Carsten.Brockmann, J.Oberlander}@ed.ac.uk
Abstract
It would be useful to enable dialogue
agents to project, through linguistic
means, their individuality or personality.
Equally, each member of a pair of agents
ought to adjust its language (to a greater or
lesser extent) to match that of its interlocu-
tor. We describe CRAG, which generates
dialogues between pairs of agents, who are
linguistically distinguishable, but able to
align. CRAG-2 makes use of OPENCCG
and an over-generation and ranking ap-
proach, guided by a set of language mod-
els covering both personality and align-
ment. We illustrate with examples of out-
put, and briefly note results from user stud-
ies with the earlier CRAG-1, indicating
how CRAG-2 will be further evaluated.
Related work is discussed, along with cur-
rent limitations and future directions.
1 Introduction
A computer agent should be individual. Nass
and collaborators find that users? responses
to computer-agents are influenced by whether
the agent?s linguistic personality matches?or
mismatches?the personality of the user (Moon
and Nass, 1996; Nass and Lee, 2000). Similarly,
characters in virtual environments should be dis-
tinctive (Ball and Breese, 2000; Rist et al, 2003).
But an aspect of personality is how well you adjust
to other people (and their language use): align-
ment. Pickering and Garrod?s Interactive Align-
ment Model suggests that people tend to automat-
ically converge on lexical and syntactic choices,
via a low-level mechanism of interpersonal prim-
ing (Pickering and Garrod, 2004), and Brennan
has shown that people will align their language to-
wards that of computer agents (Brennan, 1996).
But it is an open issue as to whether some peo-
ple are better ?aligners? than others. Conversely,
alignment is only visible and interesting (among
computer agents) if they start out being individual.
We therefore set out to simulate both individ-
uality and alignment. The paper briefly surveys
the evidence for linguistic personality, for inter-
personal alignment, and for interaction between
them. It then sketches the current version of
CRAG. CRAG-2 makes use of OPENCCG and
an over-generation and ranking approach, guided
by a set of language models for personality and
alignment. We illustrate the differing linguis-
tic behaviours that it generates, and briefly note
promising results from user studies with the ear-
lier CRAG-1 system, indicating how CRAG-2 will
be further evaluated. Related work is discussed,
along with possible directions for future work.
2 Background
2.1 Personality and Language
Current work on personality traits is dominated by
Costa and McCrae?s five-factor model (Costa and
McCrae, 1992). The five factors, or dimensions,
are: Extraversion; Neuroticism; Openness; Agree-
ableness; and Conscientiousness (Matthews et al,
2003). It has been shown that scores on these di-
mensions correlate with some aspects of language
use (Scherer, 1979; Dewaele and Furnham, 1999).
In studies of text, the focus has been on lexical
choice, and Pennebaker and colleagues have anal-
ysed relative frequencies of use of word-stems in
a dictionary structured into semantic and syntac-
tic categories (Pennebaker et al, 2001). Amongst
other results, they have shown that High Extraverts
25
use: more social process talk, positive emotion
words and inclusives; and fewer negations, ten-
tative words, exclusives, causation words, nega-
tive emotion words, and articles (Pennebaker and
King, 1999; Pennebaker et al, 2002).
Computational linguistic exploitation of such
empirically-derived features has been limited. On
the one hand, in generation, there has been work
on personality-based generation. For instance, in
developing embodied conversational agents, re-
searchers have designed agents or teams of agents
with distinguishable linguistic personalities (Ball
and Breese, 2000; Rist et al, 2003; Piwek and
van Deemter, 2003; Gebhard, 2005). However,
the linguistic behaviour is usually informed by
rules based on personality stereotypes, rather than
on language statistics themselves. On the other
hand, in interpretation, more empirical work has
recently been carried out, to enable text classifi-
cation. Argamon et al (2005) attempted to clas-
sify authors as High or Low Extravert and High
or Low Neurotic, using Pennebaker and King?s
(1999) data. They report classification accuracies
of around 58% (with a 50% baseline). Oberlander
and Nowson (2006) undertake a comparable task,
using weblog data. They report classification ac-
curacies of roughly 85% (Neuroticism) and 94%
(Extraversion), and comparable figures for Agree-
ableness and Conscientiousness. Such studies can
provide ordered lists of linguistic features which
are useful for distinguishing language producers,
and we will return to this, below.
2.2 Alignment and Language
People converge with their interlocutors in linguis-
tic choices at a number of levels (Pickering and
Garrod, 2004). The phenomena can be seen in
both social and cognitive terms. On the social side,
co-operative processes such as audience design
are usually considered to be conscious, at least in
part (Bell, 1984). But on the cognitive side, co-
ordinative processes such as alignment are usu-
ally considered to be largely automatic (Garrod
and Doherty, 1994). Alignment can be probed
by psycholinguistic tests for interpersonal prim-
ing, establishing the extent to which participants
are more likely to use a lexical item or syntac-
tic construction after hearing their conversational
partner use it. Syntactic priming experiments in-
volve constructions such as passives, and ditransi-
tives (Pickering and Branigan, 1998).
It is possible that some people are stronger
aligners than others. Gill et al (2004) probed
syntactic priming for passives, and investigated
whether levels of Extraversion or Neuroticism
would affect the strength of priming effects. It
was found that Extraversion has no effect, but that
Neuroticism has a non-linear effect: both High and
Low levels of Neuroticism led to weaker priming;
Mid levels led to significantly stronger priming.
Given this, if a generation system is going to simu-
late alignment, it is probably worth designing it so
that it can simulate agents with differing propensi-
ties to align.
3 The CRAG System Overview
The system described in the following sections
(CRAG-2) is the successor to CRAG-1 which is
detailed in Isard et al (2005). The system gener-
ates a dialogue between two computer agents on
the subject of opinions about a film. CRAG-2 uses
the OPENCCG parsing and generation framework
(White, 2004; White, 2006). The realiser com-
ponent takes a logical form as input and outputs
a list of candidate sentences ranked using one or
more language models. In CRAG-2, we use the
OPENCCG generator to massively over-generate
paraphrases, and the combination of n-gram mod-
els described in Section 4 to choose the best ut-
terance according to a character?s personality and
agenda, and the dialogue history.
4 N-Grams: Personality and Alignment
Modelling
4.1 N-Gram Language Models
The basic assumption underlying CRAG-2 is that
personality, as well as alignment behaviour, can
be modelled by the combination of a variety of n-
gram language models.
Language models are trained on a corpus and
subsequently used to compute probability scores
of word sequences. An n-gram language model
approximates the probability of a word given its
history of the preceding n? 1 words. According
to the chain rule, probabilities are then combined
by multiplication. Equation (1) shows a trigram
model that takes into account two words of context
to predict the probability of a word sequence wn1:
(1) P(wn1)?
n
?
i=1
P(wi|wi?1i?2)
26
4.2 Avoiding the Length Effect
Because word probabilities are always less than 1
and therefore each multiplication decreases the to-
tal, if we use this standard model, longer sentences
will always receive lower scores (this is known as
the length effect). We therefore calculate the prob-
ability of a sentence as the geometric mean of the
probability of each word in the sentence as shown
in (2):
(2) P(wn1)?
n
?
i=1
P(wi|wi?1i?2)
1/n
4.3 Linear Combination of Language Models
OPENCCG supports the linear combination of
language models, where each model is assigned a
weight. For uniform interpolation of two language
models Pa and Pb, each receives equal weight:
(3) P(wi|wi?1i?2) =
Pa(wi|wi?1i?2)+Pb(wi|w
i?1
i?2)
2
In the more general case, the language models
are assigned weights ?i, the sum of which has to
be 1:
(4) P(wi|wi?1i?2) = ?1Pa(wi|wi?1i?2)+?2Pb(wi|wi?1i?2)
For example, setting ?1 = 0.9 and ?2 = 0.1 assigns
a high weight to the first language model.
4.4 OPENCCG N-Gram Ranking
In the OPENCCG framework, language models
can be used to influence the chart-based realisation
process. The agenda of edges is re-sorted accord-
ing to the score an edge receives with respect to a
language model. For CRAG-2, many paraphrases
are generated from a given logical form, and they
are then ranked in order of probability according
to the combination of n-gram models appropriate
for the character and stage of the dialogue.
5 CRAG-2 Personality and Alignment
Models
We use the SRILM toolkit (Stolcke, 2002) to com-
pute our language models. All models (except
for the cache language model described in Sec-
tion 5.4) are trigram models with backoff to bi-
grams and unigrams.
We have experimented with two strategies for
creating personality models. Since we want to
study the effects of alignment as well as person-
ality, it is essential that the two characters in a di-
alogue be distinct from one another, so that the ef-
fects of alignment can be seen. The first strategy
involves using typical language for each personal-
ity trait, and the second uses the language of one
individual. In both cases, the language models de-
scribed in the following sections are combined as
described in Section 5.5.
5.1 Building a Personality
Nowson (2006) performed a study on language
use in weblogs. The weblog authors were asked to
complete personality questionnaires based on the
five-factor model (see Section 2.1). All weblog au-
thors scored High or Medium on the Openness di-
mension, so we have no data for typical Low Open
language.
We divided the data into High, Medium and
Low for each personality dimension, and trained
language models so that we would be able to as-
sess the probability of a word sequence given a
personality type. This means that each individual
weblog is used 5 times, once for each dimension.
For each personality dimension, the system sim-
plifies a character?s personality setting x by assign-
ing a value of High (x > 70), Medium (30 < x ?
70) or Low (x ? 30). The five models correspond-
ing to the character?s assigned personality are uni-
formly interpolated to give the final personality
model. If the character has been given a low Open-
ness score, since we do not have a model for this
personality type, we simply interpolate the other
four models.
5.2 Borrowing a Personality
Our second strategy was to train n-gram models
on language of the individuals from the CRAG-1
corpus (Isard et al, 2005) and to use one of these
models for each character in the dialogue.
5.3 Base Language Model
In the case of building a personality, a base lan-
guage model is obtained by combining a language
model computed from the corpus collected for the
CRAG-1 system and a general language model
based on data from the Switchboard corpus (Stol-
cke et al, 2000). The combined base model alone
would rank the utterances without any bias for per-
sonality or alignment. When we are borrowing a
personality, the base model is calculated from the
Switchboard corpus alone.
27
5.4 Cache Language Model
We simulate alignment by computing a cache lan-
guage model based on the utterance that was gen-
erated immediately before. This dialogue history
cache model is the uniform interpolation of word-
and class-based n-gram models, where classes act
as a backoff mechanism when there is no exact
word match. Classes group together lexical items
with similar semantic properties, e.g.:
? good, bad: quality-adjective
? loved, hated: opinion-verb
Details of this approach can be found in Brock-
mann et al (2005).
5.5 Combining the Language Models
The system uses weights to combine all the mod-
els described above. First the base and person-
ality models are interpolated to produce a base-
personality model, and finally the cache model is
introduced to add alignment effects.
6 Dialogue and Utterance Specifications
6.1 Character Specification
Two computer characters are parameterised for
their personality by specifying values (on a scale
from 0 to 100) for the five dimensions: Extraver-
sion (E), Neuroticism (N), Openness (O), Agree-
ableness (A), and Conscientiousness (C). Their
alignment behaviour is set to a value between 0
(low propensity to align) and 1 (high propensity
to align). Also, each character receives an agenda
of topics they wish to discuss, along with polari-
ties (positive/negative) that indicate their opinion
on the respective topic.
6.2 Utterance Design
The character with the higher Extraversion score
begins the dialogue, and their first topic is se-
lected. Once an utterance has been generated, the
other character is selected, and the system applies
the algorithm shown in (5) to decide which topic
should come next. This process continues until
there are no topics left on the agenda of the cur-
rent speaker.
(5) if (A < 46) or (C < 46) or
(no. of utts about this topic = 2)
then take next topic from own agenda
else continue on same topic
The system creates a simple XML representa-
tion of the character?s utterance, using the speci-
fied topic and polarity. An example using the topic
music and polarity negative is shown in Figure 1.
At this point the system also decides which dis-
course connectives may be appropriate, based on
the previous topic and polarity.
<utterance>
<utt topic="music" polarity="dislike"
opp-polarity="like" so="no" right="no"
also="no" well="yes" and="no" but="no">
<pred adj="bad"/>
<opp-pred adj="good"/>
</utt>
</utterance>
Figure 1: Simple Utterance Specification
6.3 OPENCCG Logical Forms
Following the method described in Foster and
White (2004), the basic utterance specification is
transformed, using stylesheets written in the XSL
transformation language, into an OPENCCG log-
ical form. We make use of the facility for defin-
ing optional and alternative inputs and underspec-
ified semantics to massively over-generate candi-
date utterances. A fragment of the logical form
which results from the transformation of Figure 1
is shown in Figure 2. We also include some frag-
ments of canned text from the CRAG corpus in our
OPENCCG lexicon.
We also add optional interjections (i mean, you
know, sort of ) and conversational markers (right,
but, and, well) where appropriate given the dis-
course history.
When the full logical form is processed by the
OPENCCG system, the output consists of sen-
tences of the types shown below:
(I think) the music was bad.
(I think) the music was not (wasn?t)
good.
I did not (didn?t) like the music.
I hated the music.
One thing I did not (didn?t) like was the
music.
One thing I hated was the music.
The fragmentary logical form in Figure 2 would
create all possible paraphrases from:
(well) (you know) I (kind of) [liked/loved] the
[music/score]
By using synonyms (e.g., plot=story, com-
edy=humour) and combining the sentence types
28
<node id="l1:opinion" pred="like" tense="past">
<rel name="Speaker">
<node id="p1:person" pred="pro1" num="sg"/>
</rel>
<rel name="Content">
<node id="f1:cragtopic" pred="music"
det="the" num="sg"/>
</rel>
<opt>
<rel name="Modifier">
<node id="w1:adv" pred="well"/>
</rel>
<opt>
<opt>
<rel name="HasProp">
<node id="a2:proposition" pred="kind-of"/>
</rel>
</opt>
<opt>
<rel name="Modifier">
<node id="a1:adv" pred="you-know"/>
</rel>
</opt>
</node>
Figure 2: Fragment of Logical Form
Stan: E:53 N:48 A:57 C:46 O:65
agenda: film(neg), dialogue(neg),
music(pos)
other opinions: plot(neg), comedy(neg)
Eddie: E:51 N:43 A:57 C:41 O:65
agenda: plot(neg), comedy(neg),
dialogue(neg)
other opinions: music(pos), film(neg)
Figure 3: Stan and Eddie
and optional expressions, we create up to 3000
possibilities per utterance, and the best candidate
is chosen by the specific combination of n-gram
models appropriate for the given personality and
dialogue history, as described in Section 4.
Our OPENCCG lexicon is based on the core
English lexicon included with the system and we
have added vocabulary appropriate to the movie
domain, and extended the range of grammatical
constructions where necessary.
7 Output and Evaluation
7.1 Output
In this section, we provide some example out-
puts from the CRAG-2 system, using characters
based on participants from our corpus (see Sec-
tion 5.2). Stan is higher on the Extraversion, Neu-
roticism, and Conscientiousness scales than Ed-
die. The characters? personalities and agendas are
summarised in Figure 3.
We show three example dialogues between Stan
and Eddie. In the first (Figure 4) neither charac-
ter aligns with the other at all, while in the sec-
ond (Figure 5) Stan has a slight tendency towards
alignment and in the third (Figure 6) a more pro-
nounced tendency. In system terms, this means
that in the first dialogue the cache model was given
weight 0, while in the second and third the cache
model was given weights 0.05 and 0.1 respectively
for Stan?s utterances. It can be seen that in the
first dialogue, Eddie?s use of humour in utterance
4 is followed by Stan?s comedy, and dialogue in
utterance 6 is followed by script. In the second
dialogue, Stan aligns his first lexical choice with
Eddie?s but not his second, while in the third di-
alogue he aligns both. The syntactic structures of
Stan?s utterances in the second and third dialogue
also mirror Eddie?s progressively more closely.
1. Stan: I didn?t think much of the film you know.
2. Eddie: I mean I didn?t like kind of like the story.
3. Stan: Yeah there were so many little pathways that they
didn?t take you know.
4. Eddie: I mean I didn?t think the humour was really funny.
5. Stan: Yeah you know the comedy was a bit dull.
6. Eddie: I mean I didn?t like kind of like the dialogue.
7. Stan: I mean the script was a bit dull.
Figure 4: Zero Alignment
1. Stan: I didn?t think much of the film you know.
2. Eddie: I mean I didn?t like kind of like the story.
3. Stan: Yeah there were so many little pathways that they
didn?t take you know.
4. Eddie: I mean I didn?t think the humour was really funny.
5. Stan: I mean the humour was a bit dull.
6. Eddie: I mean I didn?t like kind of like the dialogue.
7. Stan: I mean the script was a bit dull.
Figure 5: Little Alignment from Stan
1. Stan: I didn?t think much of the film you know.
2. Eddie: I mean I didn?t like kind of like the story.
3. Stan: I mean the story was a bit dull.
4. Eddie: I mean I didn?t think the humour was really funny.
5. Stan: I mean the humour was a bit dull.
6. Eddie: I mean I didn?t like kind of like the dialogue.
7. Stan: I mean the dialogue was a bit dull.
Figure 6: More Alignment from Stan
To further illustrate the differences between the
dialogues with and without alignment, we provide
some utterance rankings. We show candidates
for the fifth utterance in each dialogue. Table 1
shows sentences from the example generated with-
out alignment, corresponding to utterance 5 (Stan)
29
1 .03317 Yeah you know the comedy was a
bit dull.
3 .03210 Yeah you know the humour was a bit
dull.
6 .03083 Yeah to be honest I didn?t think that
the comedy was very good either.
15 .02938 I didn?t think much of the comedy
either.
24 .02861 I thought that the comedy was a bit
dull too you know.
Table 1: Ranked Sentences with Zero Alignment
1 .05384 I mean the humour was a bit dull.
8 .05239 The humour wasn?t really funny you
know.
15 .04748 I mean I didn?t think that the humour
was very good either.
19 .04518 I didn?t think much of the humour
either you know.
21 .04478 I thought the humour was a bit dull
too you know.
Table 2: Ranked Sentences with Little Alignment
from Stan
from Figure 4. We show the first five occurrences
of different sentence structures (see Section 6.3),
with their rank and their geometric mean adjusted
scores.
Table 2 shows the the top five sentences from
the fifth utterance from Figure 5 (little alignment),
and Table 3 those from Figure 6 (more align-
ment). It can be seen that when more alignment
is present, the syntactic structure used by the pre-
vious speaker rises higher in the rankings.
7.2 Evaluation
We have not evaluated CRAG-2. However, we
have evaluated CRAG-1. The method was to gen-
erate a set of dialogues, systematically contrasting
characters with extreme settings for the personal-
ity dimensions (High/Low Extraversion, Neuroti-
cism, and Psychoticism1).
1CRAG-1 used the simpler PEN three factor personality
model.
1 .07081 I mean the humour was a bit dull.
2 .06432 The humour wasn?t really funny you
know.
15 .05516 I mean I didn?t think that the humour
was really funny either.
27 .05000 I thought the humour was a bit dull
too you know.
36 .04884 I mean I didn?t think much of the hu-
mour either.
Table 3: Ranked Sentences with More Alignment
from Stan
Human subjects were asked to fill in a question-
naire to determine their personality. They were
then given a selection of dialogues to read. After
each dialogue, they were asked to rate their per-
ception of the interaction and of the characters in-
volved by assigning scores to a number of adjec-
tives related to the personality dimensions.
It was found that subjects could recognise dif-
ferences in the Extraversion level of the language.
Also, the personality setting of a character influ-
enced the perception of its and its dialogue part-
ner?s personality (Kahn, 2006).
We plan a similar evaluation for CRAG-2 to be
able to compare human raters? impressions of di-
alogues generated by the two systems. We also
plan to evaluate CRAG-2 internally by varying the
weight given to the underlying language models,
and observing the effects this has on the resulting
ranking of the generated utterances.
8 Related Work
Related work in NLG involves either personality
or alignment. So far as we can tell, there is little
work on the latter. Varges (2005) suggests that ?a
word similarity-based ranker could align the gen-
eration output (i.e. the highest-ranked candidate)
with previous utterances in the discourse context?,
but there is no report yet on an implementation of
this proposal. A rather different approach is sug-
gested by Bateman and Paris (2005), who discuss
initial work on alignment, mediated by a process
of register-recognition. Regarding generation with
personality, the most influential work is probably
Hovy?s PAULINE system, which varies both con-
tent selection and realisation according to an indi-
vidual speaker?s goals and attitudes (Hovy, 1990).
In her extremely useful survey of work on affective
(particularly, emotional) natural language gener-
ation, Belz (2003) notes that the complexity of
PAULINE?s rule system means that numerous rule
interactions can lead to unpredictable side effects.
In response, Paiva and Evans (2004) take a more
empirical line on style generation, which is closer
to that pursued here. Other relevant work includes
Loyall and Bates (1997), who explicitly propose
that personality and emotion could be used in
generation, but Belz observes that technical de-
scriptions of Hap and the Oz project suggest that
the proposals were not implemented. Walker et
al.?s (1997) system produces linguistic behaviour
which is much more varied than our current sys-
30
tem is capable of; but there, variation is driven by
a model of social relations (based on Brown and
Levinson), rather than on personality. The NECA
project subsequently developed methods for gen-
erating scripts for pairs of dialogue agents (Piwek
and van Deemter, 2003), supported by the MIAU
platform (Rist et al, 2003). The VIRTUALHU-
MAN project is a logical successor to this work,
and its ALMA platform provides an integrated ap-
proach to affective generation, covering emotion,
mood and personality (Gebhard, 2005).
9 Conclusion and Next Steps
Our current system takes a much coarser-grained
approach to semantics and discourse goals than
the recent projects described above, in order to
take advantage of empirically-derived relations
between language and personality. It should be
feasible in principle to move to a more sophisti-
cated semantics, but still retain the massive over-
generation and ranking method. However, to
support more perceptible variation, we need to
exploit much larger personality-corpus resources
than have been available up to now, and our cur-
rent priority is to obtain a corpus at least an order
of magnitude larger than what is currently avail-
able. This interest in individual differences and
what corpora can (and cannot) tell us about them
is one we share with Reiter and colleagues (Reiter
and Sripada, 2004).
We also plan to integrate techniques from
CRAG-1 and CRAG-2, by passing the ranked out-
put of CRAG-2 through further processing and
ranking stages. Furthermore, we intend to inves-
tigate longer-ranging alignment processes, taking
into account more than one previous utterance,
with reduced weight by distance, to emulate mem-
ory effects.
With these enhancements, we will take further
steps towards our goal of simulating both individu-
ality and alignment in believable computer agents.
10 Acknowledgements
This research has been funded by Scottish Enter-
prise through the Edinburgh-Stanford Link project
?Critical Agent Dialogue? (CRAG). We would
like to thank Michael White and Scott Nowson for
their assistance and our anonymous reviewers for
their helpful comments.
References
Shlomo Argamon, Sushant Dhawle, Moshe Koppel,
and James W. Pennebaker. 2005. Lexical predic-
tors of personality type. In Proceedings of the 2005
Joint Annual Meeting of the Interface and the Clas-
sification Society of North America.
Gene Ball and Jack Breese. 2000. Emotion and per-
sonality in a conversational agent. In J. Cassell,
J. Sullivan, S. Prevost, and E. Churchill, editors, Em-
bodied Conversational Agents, pages 189?219. MIT
Press, Cambridge, MA, USA.
John A. Bateman and Ce?cile L. Paris. 2005. Adap-
tation to affective factors: architectural impacts for
natural language generation and dialogue. In Pro-
ceedings of the Workshop on Adapting the Interac-
tion Style to Affective Factors at the 10th Interna-
tional Conference on User Modeling (UM-05), Ed-
inburgh, UK.
Allan Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
Anja Belz. 2003. And now with feeling: Develop-
ments in emotional language generation. Techni-
cal Report ITRI-03-21, Information Technology Re-
search Institute, University of Brighton, Brighton.
Susan E. Brennan. 1996. Lexical entrainment in spon-
taneous dialog. In Proceedings of the 1996 Inter-
national Symposium on Spoken Dialogue (ISSD-96),
pages 41?44, Philadelphia, PA.
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Modelling alignment for af-
fective dialogue. In Proceedings of the Workshop on
Adapting the Interaction Style to Affective Factors at
the 10th International Conference on User Modeling
(UM-05), Edinburgh, UK.
Paul T. Costa and Robert R. McCrae, 1992. Re-
vised NEO Personality Inventory (NEO-PI-R) and
NEO Five-Factor Inventory (NEO-FFI): Profes-
sional Manual. Odessa, FL: Psychological Assess-
ment Resources.
Jean-Marc Dewaele and Adrian Furnham. 1999. Ex-
traversion: The unloved variable in applied linguis-
tic research. Language Learning, 49:509?544.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for Text Planning with XSLT. In Proc. of the
4th NLPXML Workshop.
Simon Garrod and Gwyneth Doherty. 1994. Conver-
sation, co-ordination and convention: an empirical
investigation of how groups establish linguistic con-
ventions. Cognition, 53(3):181?215.
Patrick Gebhard. 2005. Alma: a layered model of af-
fect. In AAMAS ?05: Proceedings of the Fourth In-
ternational Joint Conference on Autonomous Agents
and Multiagent Systems, pages 29?36, New York,
NY, USA. ACM Press.
31
Alastair J. Gill, Annabel J. Harrison, and Jon Ober-
lander. 2004. Interpersonality: Individual differ-
ences and interpersonal priming. In Proceedings of
the 26th Annual Conference of the Cognitive Science
Society, pages 464?469.
Eduard Hovy. 1990. Pragmatics and natural language
generation. Artificial Intelligence, 43.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2005. Re-creating dialogues from a corpus. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation at Corpus Linguistics
2005 (CL-05), pages 7?12, Birmingham, UK.
Adam S. Kahn. 2006. Master?s thesis, Stanford Uni-
versity.
A. Bryan Loyall and Joseph Bates. 1997. Personality-
rich believable agents that use language. In J. Lewis
and B. Hayes-Roth, editors, Proceedings of the
1st International Conference on Autonomous Agents
(Agents?97). ACM Press.
Gerald Matthews, Ian J. Deary, and Martha C. White-
man. 2003. Personality Traits. Cambridge Univer-
sity Press, Cambridge, 2nd edition.
Youngme Moon and Clifford Nass. 1996. How ?real?
are computer personalities? Communication Re-
search, 23:651?674.
Clifford Nass and Kwan Min Lee. 2000. Does
computer-generated speech manifest personality?
an experimental test of similarity-attraction. In
Proceedings of CHI 2000, The Hague, Amsterdam,
2000, pages 329?336.
Scott Nowson. 2006. The Language of Weblogs: A
study of genre and individual differences. Ph.D. the-
sis, University of Edinburgh.
Jon Oberlander and Scott Nowson. 2006. Whose
thumb is it anyway? Classifying author personality
from weblog text. In Proceedings of COLING/ACL-
06: 44th Annual Meeting of the Association for
Computational Linguistics and 21st International
Conference on Computational Linguistics, Sydney.
Daniel S. Paiva and Roger Evans. 2004. A framework
for stylistically controlled generation. In Proceed-
ings of the 3rd International Conference on Natural
Language Generation, pages 120?129.
James W. Pennebaker and Laura King. 1999. Lin-
guistic styles: Language use as an individual differ-
ence. Journal of Personality and Social Psychology,
77:1296?1312.
James W. Pennebaker, Martha E. Francis, and Roger J.
Booth. 2001. Linguistic Inquiry and Word Count
2001. Lawrence Erlbaum Associates, Mahwah, NJ.
James W. Pennebaker, Matthias R. Mehl, and Kate G.
Neiderhoffer. 2002. Psychological aspects of nat-
ural language use: Our words, our selves. Annual
Review of Psychology, 54:547?577.
Martin J. Pickering and Holly P. Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39(4):633?651.
Martin J. Pickering and Simon Garrod. 2004. Towards
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, 27:169?225.
Paul Piwek and Kees van Deemter. 2003. Dialogue as
discourse: Controlling global properties of scripted
dialogue. In Proceedings of the AAAI Spring Sym-
posium on Natural Language Generation in Spoken
and Written Dialogue.
Ehud Reiter and Somayajulu Sripada. 2004. Contex-
tual influences on near-synonym choice. In Pro-
ceedings of the Third International Conference on
Natural Language Generation, pages 161?170.
Thomas Rist, Elisabeth Andre?, and Stephan Baldes.
2003. A flexible platform for building applications
with life-like characters. In IUI ?03: Proceedings of
the 8th International Conference on Intelligent User
Interfaces, pages 158?165, New York, NY, USA.
ACM Press.
Klaus Scherer. 1979. Personality markers in speech.
In K. R. Scherer and H. Giles, editors, Social Mark-
ers in Speech, pages 147?209. Cambridge Univer-
sity Press, Cambridge.
Andreas Stolcke, Harry Bratt, John Butzberger, Hora-
cio Franco, Venkata Ramana Rao Gadde, Madelaine
Plauche?, Colleen Richey, Elizabeth Shriberg, Kemal
So?nmez, Fuliang Weng, and Jing Zheng. 2000. The
SRI March 2000 Hub-5 conversational speech tran-
scription system. In Proceedings of the 2000 Speech
Transcription Workshop, College Park, MD.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
International Conference on Spoken Language Pro-
cessing (ICSLP-02), pages 901?904, Denver, CO.
Sebastian Varges. 2005. Spatial descriptions as refer-
ring expressions in the MapTask domain. In Pro-
ceedings of the 10th European Workshop on Natural
Language Generation.
Marilyn A. Walker, Janet E. Cahn, and Steve J. Whit-
taker. 1997. Improvising linguistic style: So-
cial and affective bases for agent personality. In
J. Lewis and B. Hayes-Roth, editors, Proceedings
of the 1st International Conference on Autonomous
Agents (Agents?97), pages 96?105. ACM Press.
Michael White. 2004. Reining in CCG Chart Re-
alization. In Proceedings of the 3rd International
Conference on Natural Language Generation, pages
182?191.
Michael White. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language & Computation, on-
line first, March.
32
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471?481,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Evaluating language understanding accuracy with respect to objective
outcomes in a dialogue system
Myroslava O. Dzikovska and Peter Bell and Amy Isard and Johanna D. Moore
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh, United Kingdom
{m.dzikovska,peter.bell,amy.isard,j.moore}@ed.ac.uk
Abstract
It is not always clear how the differences
in intrinsic evaluation metrics for a parser
or classifier will affect the performance of
the system that uses it. We investigate the
relationship between the intrinsic evalua-
tion scores of an interpretation component
in a tutorial dialogue system and the learn-
ing outcomes in an experiment with human
users. Following the PARADISE method-
ology, we use multiple linear regression to
build predictive models of learning gain,
an important objective outcome metric in
tutorial dialogue. We show that standard
intrinsic metrics such as F-score alone do
not predict the outcomes well. However,
we can build predictive performance func-
tions that account for up to 50% of the vari-
ance in learning gain by combining fea-
tures based on standard evaluation scores
and on the confusion matrix entries. We
argue that building such predictive mod-
els can help us better evaluate performance
of NLP components that cannot be distin-
guished based on F-score alone, and illus-
trate our approach by comparing the cur-
rent interpretation component in the system
to a new classifier trained on the evaluation
data.
1 Introduction
Much of the work in natural language processing
relies on intrinsic evaluation: computing standard
evaluation metrics such as precision, recall and F-
score on the same data set to compare the perfor-
mance of different approaches to the same NLP
problem. However, once a component, such as
a parser, is included in a larger system, it is not
always clear that improvements in intrinsic eval-
uation scores will translate into improved over-
all system performance. Therefore, extrinsic or
task-based evaluation can be used to complement
intrinsic evaluations. For example, NLP com-
ponents such as parsers and co-reference resolu-
tion algorithms could be compared in terms of
how much they contribute to the performance of
a textual entailment (RTE) system (Sammons et
al., 2010; Yuret et al 2010); parser performance
could be evaluated by how well it contributes to
an information retrieval task (Miyao et al 2008).
However, task-based evaluation can be difficult
and expensive for interactive applications. Specif-
ically, task-based evaluation for dialogue systems
typically involves collecting data from a number
of people interacting with the system, which is
time-consuming and labor-intensive. Thus, it is
desirable to develop an off-line evaluation pro-
cedure that relates intrinsic evaluation metrics to
predicted interaction outcomes, reducing the need
to conduct experiments with human participants.
This problem can be addressed via the use of
the PARADISE evaluation methodology for spo-
ken dialogue systems (Walker et al 2000). In a
PARADISE study, after an initial data collection
with users, a performance function is created to
predict an outcome metric (e.g., user satisfaction)
which can normally only be measured through
user surveys. Typically, a multiple linear regres-
sion is used to fit a predictive model of the desired
metric based on the values of interaction param-
eters that can be derived from system logs with-
out additional user studies (e.g., dialogue length,
word error rate, number of misunderstandings).
PARADISE models have been used extensively
in task-oriented spoken dialogue systems to estab-
lish which components of the system most need
improvement, with user satisfaction as the out-
come metric (Mo?ller et al 2007; Mo?ller et al
2008; Walker et al 2000; Larsen, 2003). In tu-
torial dialogue, PARADISE studies investigated
471
which manually annotated features predict learn-
ing outcomes, to justify new features needed in
the system (Forbes-Riley et al 2007; Rotaru and
Litman, 2006; Forbes-Riley and Litman, 2006).
We adapt the PARADISE methodology to eval-
uating individual NLP components, linking com-
monly used intrinsic evaluation scores with ex-
trinsic outcome metrics. We describe an evalua-
tion of an interpretation component of a tutorial
dialogue system, with student learning gain as the
target outcome measure. We first describe the
evaluation setup, which uses standard classifica-
tion accuracy metrics for system evaluation (Sec-
tion 2). We discuss the results of the intrinsic sys-
tem evaluation in Section 3. We then show that
standard evaluation metrics do not serve as good
predictors of system performance for the system
we evaluated. However, adding confusion matrix
features improves the predictive model (Section
4). We argue that in practical applications such
predictive metrics should be used alongside stan-
dard metrics for component evaluations, to bet-
ter predict how different components will perform
in the context of a specific task. We demonstrate
how this technique can help differentiate the out-
put quality between a majority class baseline, the
system?s output, and the output of a new classifier
we trained on our data (Section 5). Finally, we
discuss some limitations and possible extensions
to this approach (Section 6).
2 Evaluation Procedure
2.1 Data Collection
We collected transcripts of students interacting
with BEETLE II (Dzikovska et al 2010b), a tu-
torial dialogue system for teaching conceptual
knowledge in the basic electricity and electron-
ics domain. The system is a learning environment
with a self-contained curriculum targeted at stu-
dents with no knowledge of high school physics.
When interacting with the system, students spend
3-5 hours going through pre-prepared reading ma-
terial, building and observing circuits in a simula-
tor, and talking with a dialogue-based computer
tutor via a text-based chat interface.
During the interaction, students can be asked
two types of questions. Factual questions require
them to name a set of objects or a simple prop-
erty, e.g., ?Which components in circuit 1 are in
a closed path?? or ?Are bulbs A and B wired
in series or in parallel?. Explanation and defi-
nition questions require longer answers that con-
sist of 1-2 sentences, e.g., ?Why was bulb A on
when switch Z was open?? (expected answer ?Be-
cause it was still in a closed path with the bat-
tery?) or ?What is voltage?? (expected answer
?Voltage is the difference in states between two
terminals?). We focus on the performance of the
system on these long-answer questions, since re-
acting to them appropriately requires processing
more complex input than factual questions.
We collected a corpus of 35 dialogues from
paid undergraduate volunteers interacting with the
system as part of a formative system evaluation.
Each student completed a multiple-choice test as-
sessing their knowledge of the material before and
after the session. In addition, system logs con-
tained information about how each student?s utter-
ance was interpreted. The resulting data set con-
tains 3426 student answers grouped into 35 sub-
sets, paired with test results. The answers were
then manually annotated to create a gold standard
evaluation corpus.
2.2 BEETLE II Interpretation Output
The interpretation component of BEETLE II uses
a syntactic parser and a set of hand-authored rules
to extract the domain-specific semantic represen-
tations of student utterances from the text. The
student answer is first classified with respect to its
domain-specific speech act, as follows:
? Answer: a contentful expression to which
the system responds with a tutoring action,
either accepting it as correct or remediating
the problems as discussed in (Dzikovska et
al., 2010a).
? Help request: any expression indicating that
the student does not know the answer and
without domain content.
? Social: any expression such as ?sorry? which
appears to relate to social interaction and has
no recognizable domain content.
? Uninterpretable: the system could not arrive
at any interpretation of the utterance. It will
respond by identifying the likely source of
error, if possible (e.g., a word it does not un-
derstand) and asking the student to rephrase
their utterance (Dzikovska et al 2009).
472
If the student utterance was determined to be an
answer, it is further diagnosed for correctness as
discussed in (Dzikovska et al 2010b), using a do-
main reasoner together with semantic representa-
tions of expected correct answers supplied by hu-
man tutors. The resulting diagnosis contains the
following information:
? Consistency: whether the student statement
correctly describes the facts mentioned in
the question and the simulation environment:
e.g., student saying ?Switch X is closed? is
labeled inconsistent if the question stipulated
that this switch is open.
? Diagnosis: an analysis of how well the stu-
dent?s explanation matches the expected an-
swer. It consists of 4 parts
? Matched: parts of the student utterance
that matched the expected answer
? Contradictory: parts of the student ut-
terance that contradict the expected an-
swer
? Extra: parts of the student utterance that
do not appear in the expected answer
? Not-mentioned: parts of the expected
answer missing from the student utter-
ance.
The speech act and the diagnosis are passed to
the tutorial planner which makes decisions about
feedback. They constitute the output of the inter-
pretation component, and its quality is likely to
affect the learning outcomes, therefore we need
an effective way to evaluate it. In future work,
performance of individual pipeline components
could also be evaluated in a similar fashion.
2.3 Data Annotation
The general idea of breaking down the student an-
swer into correct, incorrect and missing parts is
common in tutorial dialogue systems (Nielsen et
al., 2008; Dzikovska et al 2010b; Jordan et al
2006). However, representation details are highly
system specific, and difficult and time-consuming
to annotate. Therefore we implemented a simpli-
fied annotation scheme which classifies whole an-
swers as correct, partially correct but incomplete,
or contradictory, without explicitly identifying the
correct and incorrect parts. This makes it easier to
create the gold standard and still retains useful in-
formation, because tutoring systems often choose
the tutoring strategy based on the general answer
class (correct, incomplete, or contradictory). In
addition, this allows us to cast the problem in
terms of classifier evaluation, and to use standard
classifier evaluation metrics. If more detailed an-
notations were available, this approach could eas-
ily be extended, as discussed in Section 6.
We employed a hierarchical annotation scheme
shown in Figure 1, which is a simplification of
the DeMAND coding scheme (Campbell et al
2009). Student utterances were first annotated
as either related to domain content, or not con-
taining any domain content, but expressing the
student?s metacognitive state or attitudes. Utter-
ances expressing domain content were then coded
with respect to their correctness, as being fully
correct, partially correct but incomplete, contain-
ing some errors (rather than just omissions) or
irrelevant1. The ?irrelevant? category was used
for utterances which were correct in general but
which did not directly answer the question. Inter-
annotator agreement for this annotation scheme
on the corpus was ? = 0.69.
The speech acts and diagnoses logged by the
system can be automatically mapped into our an-
notation labels. Help requests and social acts
are assigned the ?non-content? label; answers
are assigned a label based on which diagnosis
fields were filled: ?contradictory? for those an-
swers labeled as either inconsistent, or contain-
ing something in the contradictory field; ?incom-
plete? if there is something not mentioned, but
something matched as well, and ?irrelevant? if
nothing matched (i.e., the entire expected answer
is in not-mentioned). Finally, uninterpretable ut-
terances are treated as unclassified, analogous to a
situation where a statistical classifier does not out-
put a label for an input because the classification
probability is below its confidence threshold.
This mapping was then compared against the
manually annotated labels to compute the intrin-
sic evaluation scores for the BEETLE II interpreter
described in Section 3.
3 Intrinsic Evaluation Results
The interpretation component of BEETLE II was
developed based on the transcripts of 8 sessions
1Several different subcategories of non-content utter-
ances, and of contradictory utterances, were recorded. How-
ever, they resulting classes were too small and so were col-
lapsed into a single category for purposes of this study.
473
Category Subcategory Description
Non-content Metacognitive and social expressions without domain content, e.g., ?I
don?t know?, ?I need help?, ?you are stupid?
Content The utterance includes domain content.
correct The student answer is fully correct
pc incomplete The student said something correct, but incomplete, with some parts of
the expected answer missing
contradictory The student?s answer contains something incorrect or contradicting the
expected answer, rather than just an omission
irrelevant The student?s statement is correct in general, but it does not answer the
question.
Figure 1: Annotation scheme used in creating the gold standard
Label Count Frequency
correct 1438 0.43
pc incomplete 796 0.24
contradictory 808 0.24
irrelevant 105 0.03
non content 232 0.07
Table 1: Distribution of annotated labels in the evalu-
ation corpus
of students interacting with earlier versions of the
system. These sessions were completed prior to
the beginning of the experiment during which our
evaluation corpus was collected, and are not in-
cluded in the corpus. Thus, the corpus constitutes
unseen testing data for the BEETLE II interpreter.
Table 1 shows the distribution of codes in
the annotated data. The distribution is unbal-
anced, and therefore in our evaluation results we
use two different ways to average over per-class
evaluation scores. Macro-average combines per-
class scores disregarding the class sizes; micro-
average weighs the per-class scores by class size.
The overall classification accuracy (defined as the
number of correctly classified instances out of all
instances) is mathematically equivalent to micro-
averaged recall; however, macro-averaging better
reflects performance on small classes, and is com-
monly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)).
The detailed evaluation results are presented
in Table 2. We will focus on two metrics: the
overall classification accuracy (listed as ?micro-
averaged recall? as discussed above), and the
macro-averaged F score.
The majority class baseline is to assign ?cor-
rect? to every instance. Its overall accuracy is
43%, the same as BEETLE II. However, this is
obviously not a good choice for a tutoring sys-
tem, since students who make mistakes will never
get tutoring feedback. This is reflected in a much
lower value of the F score (0.12 macroaverage F
score for baseline vs. 0.44 for BEETLE II). Note
also that there is a large difference in the micro-
and macro- averaged scores. It is not immediately
clear which of these metrics is the most important,
and how they relate to actual system performance.
We discuss machine learning models to help an-
swer this question in the next section.
4 Linking Evaluation Measures to
Outcome Measures
Although the intrinsic evaluation shows that the
BEETLE II interpreter performs better than the
baseline on the F score, ultimately system devel-
opers are not interested in improving interpreta-
tion for its own sake: they want to know whether
the time spent on improvements, and the compli-
cations in system design which may accompany
them, are worth the effort. Specifically, do such
changes translate into improvement in overall sys-
tem performance?
To answer this question without running expen-
sive user studies we can build a model which pre-
dicts likely outcomes based on the data observed
so far, and then use the model?s predictions as an
additional evaluation metric. We chose a multiple
linear regression model for this task, linking the
classification scores with learning gain as mea-
sured during the data collection. This approach
follows the general PARADISE approach (Walker
et al 2000), but while PARADISE is typically
used to determine which system components need
474
Label baseline BEETLE II
prec. recall F1 prec. recall F1
correct 0.43 1.00 0.60 0.93 0.52 0.67
pc incomplete 0.00 0.00 0.00 0.42 0.53 0.47
contradictory 0.00 0.00 0.00 0.57 0.22 0.31
irrelevant 0.00 0.00 0.00 0.17 0.15 0.16
non-content 0.00 0.00 0.00 0.91 0.41 0.57
macroaverage 0.09 0.20 0.12 0.60 0.37 0.44
microaverage 0.18 0.43 0.25 0.70 0.43 0.51
Table 2: Intrinsic Evaluation Results for the BEETLE II and a majority class baseline
the most improvement, we focus on finding a bet-
ter performance metric for a single component
(interpretation), using standard evaluation scores
as features.
Recall from Section 2.1 that each participant
in our data collection was given a pre-test and
a post-test, measuring their knowledge of course
material. The test score was equal to the propor-
tion of correctly answered questions. The normal-
ized learning gain, post?pre1?pre is a metric typically
used to assess system quality in intelligent tutor-
ing, and this is the metric we are trying to model.
Thus, the training data for our model consists of
35 instances, each corresponding to a single dia-
logue and the learning gain associated with it. We
can compute intrinsic evaluation scores for each
dialogue, in order to build a model that predicts
that student?s learning gain based on these scores.
If the model?s predictions are sufficiently reliable,
we can also use them for predicting the learning
gain that a student could achieve when interacting
with a new version of the interpretation compo-
nent for the system, not yet tested with users. We
can then use the predicted score to compare dif-
ferent implementations and choose the one with
the highest predicted learning gain.
4.1 Features
Table 4 lists the feature sets we used. We tried two
basic types of features. First, we used the eval-
uation scores reported in the previous section as
features. Second, we hypothesized that some er-
rors that the system makes are likely to be worse
than others from a tutoring perspective. For ex-
ample, if the student gives a contradictory answer,
accepting it as correct may lead to student miscon-
ceptions; on the other hand, calling an irrelevant
answer ?partially correct but incomplete? may be
less of a problem. Therefore, we computed sepa-
rate confusion matrices for each student. We nor-
malized each confusion matrix cell by the total
number of incorrect classifications for that stu-
dent. We then added features based on confusion
frequencies to our feature set.2
Ideally, we should add 20 different features to
our model, corresponding to every possible con-
fusion. However, we are facing a sparse data
problem, illustrated by the overall confusion ma-
trix for the corpus in Table 3. For example,
we only observed 25 instances where a contra-
dictory utterance was miscategorized as correct
(compared to 200 ?contradictory?pc incomplete?
confusions), and so for many students this mis-
classification was never observed, and predictions
based on this feature are not likely to be reliable.
Therefore, we limited our features to those mis-
classifications that occurred at least twice for each
student (i.e., at least 70 times in the entire cor-
pus). The list of resulting features is shown in the
?conf? row of Table 4. Since only a small num-
ber of features was included, this limits the appli-
cability of the model we derived from this data
set to the systems which make similar types of
confusions. However, it is still interesting to in-
vestigate whether confusion probabilities provide
additional information compared to standard eval-
uation metrics. We discuss how better coverage
could be obtained in Section 6.
4.2 Regression Models
Table 5 shows the regression models we obtained
using different feature sets. All models were ob-
tained using stepwise linear regression, using the
Akaike information criterion (AIC) for variable
2We also experimented with using % unclassified as an
additional feature, since % of rejections is known to be a
problem for spoken dialogue systems. However, it did not
improve the models, and we do not report it here for brevity.
475
Actual
Predicted contradictory correct irrelevant non-content pc incomplete
contradictory 175 86 3 0 43
correct 25 752 1 4 26
irrelevant 31 12 16 4 29
non-content 1 3 2 95 3
pc incomplete 200 317 40 28 419
Table 3: Confusion matrix for BEETLE II. System predicted values are in rows; actual values in columns.
selection implemented in the R stepwise regres-
sion library. As measures of model quality, we re-
port R2, the percentage of variance accounted for
by the models (a typical measure of fit in regres-
sion modeling), and mean squared error (MSE).
These were estimated using leave-one-out cross-
validation, since our data set is small.
We used feature ablation to evaluate the contri-
bution of different features. First, we investigated
models using precision, recall or F-score alone.
As can be seen from the table, precision is not pre-
dictive of learning gain, while F-score and recall
perform similarly to one another, withR2 = 0.12.
In comparison, the model using only confusion
frequencies has substantially higher estimated R2
and a lower MSE.3 In addition, out of the 3 con-
fusion features, only one is selected as predictive.
This supports our hypothesis that different types
of errors may have different importance within a
practical system.
The confusion frequency feature chosen by
the stepwise model (?predicted-pc incomplete-
actual-contradictory?) has a reasonable theoret-
ical justification. Previous research shows that
students who give more correct or partially cor-
rect answers, either in human-human or human-
computer dialogue, exhibit higher learning gains,
and this has been established for different sys-
tems and tutoring domains (Litman et al 2009).
Consequently, % of contradictory answers is neg-
atively predictive of learning gain. It is reasonable
to suppose, as predicted by our model, that sys-
tems that do not identify such answers well, and
therefore do not remediate them correctly, will do
worse in terms of learning outcomes.
Based on this initial finding, we investigated
the models that combined either F scores or the
3The decrease in MSE is not statistically significant, pos-
sibly because of the small data set. However, since we ob-
serve the same pattern of results across our models, it is still
useful to examine.
full set of intrinsic evaluation scores with confu-
sion frequencies. Note that if the full set of met-
rics (precision, recall, F score) is used, the model
derives a more complex formula which covers
about 33% of the variance. Our best models,
however, combine the averaged scores with con-
fusion frequencies, resulting in a higher R2 and
a lower MSE (22% relative decrease between the
?scores.f? and ?conf+scores.f? models in the ta-
ble). This shows that these features have comple-
mentary information, and that combining them in
an application-specific way may help to predict
how the components will behave in practice.
5 Using prediction models in evaluation
The models from Table 5 can be used to compare
different possible implementations of the inter-
pretation component, under the assumption that
the component with a higher predicted learning
gain score is more appropriate to use in an ITS.
To show how our predictive models can be used
in making implementation decisions, we compare
three possible choices for an interpretation com-
ponent: the original BEETLE II interpreter, the
baseline classifier described earlier, and a new de-
cision tree classifier trained on our data.
We built a decision tree classifier using the
Weka implementation of C4.5 pruned decision
trees, with default parameters. As features, we
used lexical similarity scores computed by the
Text::Similarity package4. We computed
8 features: the similarity between student answer
and either the expected answer text or the question
text, using 4 different scores: raw number of over-
lapping words, F1 score, lesk score and cosine
score. Its intrinsic evaluation scores are shown in
Table 6, estimated using 10-fold cross-validation.
We can compare BEETLE II and baseline clas-
sifier using the ?scores.all? model. The predicted
4http://search.cpan.org/dist/Text-Similarity/
476
Name Variables
scores.fm fmeasure.microaverage, fmeasure.macroaverage, fmeasure.correct,
fmeasure.contradictory, fmeasure.pc incomplete,fmeasure.non-content,
fmeasure.irrelevant
scores.precision precision.microaverage, precision.macroaverage, precision.correct,
precision.contradictory, precision.pc incomplete,precision.non-content,
precision.irrelevant
scores.recall recall.microaverage, recall.macroaverage, recall.correct, recall.contradictory,
recall.pc incomplete,recall.non-content, recall.irrelevant
scores.all scores.fm + scores.precision + scores.recall
conf Freq.predicted.contradictory.actual.correct,
Freq.predicted.pc incomplete.actual.correct,
Freq.predicted.pc incomplete.actual.contradictory
Table 4: Feature sets for regression models
Variables Cross-
validation
R2
Cross-
validation
MSE
Formula
scores.f 0.12
(0.02)
0.0232
(0.0302)
0.32
+ 0.56 ? fmeasure.microaverage
scores.precision 0.00
(0.00)
0.0242
(0.0370)
0.61
scores.recall 0.12
(0.02)
0.0232
(0.0310)
0.37
+ 0.56 ? recall.microaverage
conf 0.25
(0.03)
0.0197
(0.0262)
0.74
? 0.56 ?
Freq.predicted.pc incomplete.actual.contradictory
scores.all 0.33
(0.03)
0.0218
(0.0264)
0.63
+ 4.20 ? fmeasure.microaverage
? 1.30 ? precision.microaverage
? 2.79 ? recall.microaverage
? 0.07 ? recall.non? content
conf+scores.f 0.36
(0.03)
0.0179
(0.0281)
0.52
? 0.66 ?
Freq.predicted.pc incomplete.actual.contradictory
+ 0.42 ? fmeasure.correct
? 0.07 ? fmeasure.non? content
full
(conf+scores.all)
0.49
(0.02)
0.0189
(0.0248)
0.88
? 0.68 ?
Freq.predicted.pc incomplete.actual.contradictory
? 0.06 ? precision.non domain
+ 0.28 ? recall.correct
? 0.79 ? precision.microaverage
+ 0.65 ? fmeasure.microaverage
Table 5: Regression models for learning gain. R2 and MSE estimated with leave-one-out cross-validation.
Standard deviation in parentheses.
477
score for BEETLE II is 0.66. The predicted
score for the baseline is 0.28. We cannot use
the models based on confusion scores (?conf?,
?conf+scores.f? or ?full?) for evaluating the base-
line, because the confusions it makes are always
to predict that the answer is correct when the
actual label is ?incomplete? or ?contradictory?.
Such situations were too rare in our training data,
and therefore were not included in the models (as
discussed in Section 4.1). Additional data will
need to be collected before this model can rea-
sonably predict baseline behavior.
Compared to our new classifier, BEETLE II has
lower overall accuracy (0.43 vs. 0.53), but per-
forms micro- and macro- averaged scores. BEE-
TLE II precision is higher than that of the classi-
fier. This is not unexpected given how the system
was designed: since misunderstandings caused
dialogue breakdown in pilot tests, the interpreter
was built to prefer rejecting utterances as uninter-
pretable rather than assigning them to an incorrect
class, leading to high precision but lower recall.
However, we can use all our predictive models
to evaluate the classifier. We checked the the con-
fusion matrix (not shown here due to space lim-
itations), and saw that the classifier made some
of the same types of confusions that BEETLE II
interpreter made. On the ?scores.all? model, the
predicted learning gain score for the classifier is
0.63, also very close to BEETLE II. But with the
?conf+scores.all? model, the predicted score is
0.89, compared to 0.59 for BEETLE II, indicating
that we should prefer the newly built classifier.
Looking at individual class performance, the
classifier performs better than the BEETLE II in-
terpreter on identifying ?correct? and ?contradic-
tory? answers, but does not do as well for par-
tially correct but incomplete, and for irrelevant an-
swers. Using our predictive performance metric
highlights the differences between the classifiers
and effectively helps determine which confusion
types are the most important.
One limitation of this prediction, however, is
that the original system?s output is considerably
more complex: the BEETLE II interpreter explic-
itly identifies correct, incorrect and missing parts
of the student answer which are then used by the
system to formulate adaptive feedback. This is
an important feature of the system because it al-
lows for implementation of strategies such as ac-
knowledging and restating correct parts of the an-
Label prec. recall F1
correct 0.66 0.76 0.71
pc incomplete 0.38 0.34 0.36
contradictory 0.40 0.35 0.37
irrelevant 0.07 0.04 0.05
non-content 0.62 0.76 0.68
macroaverage 0.43 0.45 0.43
microaverage 0.51 0.53 0.52
Table 6: Intrinsic evaluation scores for our newly built
classifier.
swer. However, we could still use a classifier to
?double-check? the interpreter?s output. If the
predictions made by the original interpreter and
the classifier differ, and in particular when the
classifier assigns the ?contradictory? label to an
answer, BEETLE II may choose to use a generic
strategy for contradictory utterances, e.g. telling
the student that their answer is incorrect without
specifying the exact problem, or asking them to
re-read portions of the material.
6 Discussion and Future Work
In this paper, we proposed an approach for cost-
sensitive evaluation of language interpretation
within practical applications. Our approach is
based on the PARADISE methodology for dia-
logue system evaluation (Walker et al 2000).
We followed the typical pattern of a PARADISE
study, but instead of relying on a variety of fea-
tures that characterize the interaction, we used
scores that reflect only the performance of the
interpretation component. For BEETLE II we
could build regression models that account for
nearly 50% variance in the desired outcomes, on
par with models reported in earlier PARADISE
studies (Mo?ller et al 2007; Mo?ller et al 2008;
Walker et al 2000; Larsen, 2003). More impor-
tantly, we demonstrated that combining averaged
scores with features based on confusion frequen-
cies improves prediction quality and allows us to
see differences between systems which are not ob-
vious from the scores alone.
Previous work on task-based evaluation of NLP
components used RTE or information extraction
as target tasks (Sammons et al 2010; Yuret et al
2010; Miyao et al 2008), based on standard cor-
pora. We specifically targeted applications which
involve human-computer interaction, where run-
ning task-based evaluations is particularly expen-
478
sive, and building a predictive model of system
performance can simplify system development.
Our evaluation data limited the set of features
that we could use in our models. For most con-
fusion features, there were not enough instances
in the data to build a model that would reliably
predict learning gain for those cases. One way
to solve this problem would be to conduct a user
study in which the system simulates random er-
rors appearing some of the time. This could pro-
vide the data needed for more accurate models.
The general pattern we observed in our data
is that a model based on F-scores alone predicts
only a small proportion of the variance. If a full
set of metrics (including F-score, precision and
recall) is used, linear regression derives a more
complex equation, with different weights for pre-
cision and recall. Instead of the linear model, we
may consider using a model based on F? score,
F? = (1 + ?2) PR?2P+R , and fitting it to the data to
derive the ? weight rather than using the standard
F1 score. We plan to investigate this in the future.
Our method would apply to a wide range of
systems. It can be used straightforwardly with
many current spoken dialogue systems which rely
on classifiers to support language understanding
in domains such as call routing and technical sup-
port (Gupta et al 2006; Acomb et al 2007).
We applied it to a system that outputs more com-
plex logical forms, but we showed that we could
simplify its output to a set of labels which still
allowed us to make informed decisions. Simi-
lar simplifications could be derived for other sys-
tems based on domain-specific dialogue acts typ-
ically used in dialogue management. For slot-
based systems, it may be useful to consider con-
cept accuracy for recognizing individual slot val-
ues. Finally, for tutoring systems it is possible
to annotate the answers on a more fine-grained
level. Nielsen et al(2008) proposed an annota-
tion scheme based on the output of a dependency
parser, and trained a classifier to identify individ-
ual dependencies as ?expressed?, ?contradicted?
or ?unaddressed?. Their system could be evalu-
ated using the same approach.
The specific formulas we derived are not likely
to be highly generalizable. It is a well-known
limitation of PARADISE evaluations that models
built based on one system often do not perform
well when applied to different systems (Mo?ller et
al., 2008). But using them to compare implemen-
tation variants during the system development,
without re-running user evaluations, can provide
important information, as we illustrated with an
example of evaluating a new classifier we built for
our interpretation task. Moreover, the confusion
frequency feature that our models picked is con-
sistent with earlier results from a different tutor-
ing domain (see Section 4.2). Thus, these models
could provide a starting point when making sys-
tem development choices, which can then be con-
firmed by user evaluations in new domains.
The models we built do not fully account for
the variance in the training data. This is expected,
since interpretation performance is not the only
factor influencing the objective outcome: other
factors, such choosing the the appropriate tutor-
ing strategy, are also important. Similar models
could be built for other system components to ac-
count for their contribution to the variance. Fi-
nally, we could consider using different learning
algorithms. Mo?ller et al(2008) examined deci-
sion trees and neural networks in addition to mul-
tiple linear regression for predicting user satisfac-
tion in spoken dialogue. They found that neural
networks had the best prediction performance for
their task. We plan to explore other learning algo-
rithms for this task as part of our future work.
7 Conclusion
In this paper, we described an evaluation of an
interpretation component of a tutorial dialogue
system using predictive models that link intrin-
sic evaluation scores with learning outcomes. We
showed that adding features based on confusion
frequencies for individual classes significantly
improves the prediction. This approach can be
used to compare different implementations of lan-
guage interpretation components, and to decide
which option to use, based on the predicted im-
provement in a task-specific target outcome met-
ric trained on previous evaluation data.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation; and Christo-
pher Brew for helpful comments and discussion.
This work has been supported in part by the US
ONR award N000141010085.
479
References
Kate Acomb, Jonathan Bloom, Krishna Dayanidhi,
Phillip Hunter, Peter Krogh, Esther Levin, and
Roberto Pieraccini. 2007. Technical support dia-
log systems: Issues, problems, and solutions. In
Proceedings of the Workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Tech-
nologies, pages 25?31, Rochester, NY, April.
Gwendolyn C. Campbell, Natalie B. Steinhauser,
Myroslava O. Dzikovska, Johanna D. Moore,
Charles B. Callaway, and Elaine Farrow. 2009. The
DeMAND coding scheme: A ?common language?
for representing and analyzing student discourse. In
Proceedings of 14th International Conference on
Artificial Intelligence in Education (AIED), poster
session, Brighton, UK, July.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn E. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of the SIGDIAL 2009 Conference, pages
38?45, London, UK, September.
Myroslava Dzikovska, Diana Bental, Johanna D.
Moore, Natalie B. Steinhauser, Gwendolyn E.
Campbell, Elaine Farrow, and Charles B. Callaway.
2010a. Intelligent tutoring with natural language
support in the Beetle II system. In Sustaining TEL:
From Innovation to Learning and Practice - 5th Eu-
ropean Conference on Technology Enhanced Learn-
ing, (EC-TEL 2010), Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a sys-
tem for tutoring and computational linguistics ex-
perimentation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2010) demo session, Uppsala, Swe-
den, July.
Kate Forbes-Riley and Diane J. Litman. 2006. Mod-
elling user satisfaction and student learning in a
spoken dialogue tutoring system with generic, tu-
toring, and user affect parameters. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT-NAACL
?06), pages 264?271, Stroudsburg, PA, USA.
Kate Forbes-Riley, Diane Litman, Amruta Purandare,
Mihai Rotaru, and Joel Tetreault. 2007. Compar-
ing linguistic features for modeling learning in com-
puter tutoring. In Proceedings of the 2007 confer-
ence on Artificial Intelligence in Education: Build-
ing Technology Rich Learning Contexts That Work,
pages 270?277, Amsterdam, The Netherlands. IOS
Press.
Narendra K. Gupta, Go?khan Tu?r, Dilek Hakkani-Tu?r,
Srinivas Bangalore, Giuseppe Riccardi, and Mazin
Gilbert. 2006. The AT&T spoken language un-
derstanding system. IEEE Transactions on Audio,
Speech & Language Processing, 14(1):213?222.
Pamela W. Jordan, Maxim Makatchev, and Umarani
Pappuswamy. 2006. Understanding complex nat-
ural language explanations in tutorial applications.
In Proceedings of the Third Workshop on Scalable
Natural Language Understanding, ScaNaLU ?06,
pages 17?24.
Lars Bo Larsen. 2003. Issues in the evaluation of spo-
ken dialogue systems using objective and subjective
measures. In Proceedings of the 2003 IEEE Work-
shop on Automatic Speech Recognition and Under-
standing, pages 209?214.
David D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the workshop on Speech and Nat-
ural Language, HLT ?91, pages 312?318, Strouds-
burg, PA, USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural lan-
guage processing to analyze tutorial dialogue cor-
pora across domains and modalities. In Proceed-
ings of 14th International Conference on Artificial
Intelligence in Education (AIED), Brighton, UK,
July.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of ACL-08: HLT, pages 46?
54, Columbus, Ohio, June.
Sebastian Mo?ller, Paula Smeele, Heleen Boland, and
Jan Krebber. 2007. Evaluating spoken dialogue
systems according to de-facto standards: A case
study. Computer Speech & Language, 21(1):26 ?
53.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, and
Robert Schleicher. 2008. Predicting the quality and
usability of spoken dialogue services. Speech Com-
munication, pages 730?744.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Mihai Rotaru and Diane J. Litman. 2006. Exploit-
ing discourse structure for spoken dialogue perfor-
mance analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?06, pages 85?93, Strouds-
burg, PA, USA.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan
Roth. 2010. ?Ask not what textual entailment can
do for you...?. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1199?1208, Uppsala, Sweden, July.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
480
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
51?56, Uppsala, Sweden, July.
481
Creation of a New Domain and Evaluation of Comparison Generation in a
Natural Language Generation System
Matthew Marge
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
mrmarge@cs.cmu.edu
Amy Isard
ICCS/HCRC
School of Informatics
University of Edinburgh
Amy.Isard@ed.ac.uk
Johanna Moore
ICCS/HCRC
School of Informatics
University of Edinburgh
J.Moore@ed.ac.uk
Abstract
We describe the creation of a new domain for
the Methodius Natural Language Generation
System, and an evaluation of Methodius? pa-
rameterized comparison generation algorithm.
The new domain was based around music and
performers, and texts about the domain were
generated using Methodius. Our evaluation
showed that test subjects learned more from
texts that contained comparisons than from
those that did not. We also established that the
comparison generation algorithm could gener-
alize to the music domain.
1 Introduction
There has been research into tailoring natural lan-
guage to a user?s previous browsing history in a va-
riety of domains such as medicine, museum col-
lections, and animal descriptions (McKeown, 1985;
Milosavljevic, 1997; Dale et al, 1998; O?Donnell
et al, 2001). Another domain in which this could
be applied is automated disc jockeys (DJs) that ac-
company a music stream such as Pandora1 and dis-
cuss interesting trivia or facts about music tracks re-
cently played to the user. User modeling could make
these texts much more natural and less repetitive,
and comparisons and contrasts between music artists
or tracks could also provide users with a novel way
to explore their music collection.
The Methodius system (Isard, 2007) continues
in a line of research which began with ILEX
(O?Donnell et al, 2001) and continued with M-
PIRO (Isard et al, 2003) and now also NaturalOWL
1http://www.pandora.com
(Galanis and Androutsopoulos, 2007). Like these
other systems, Methodius creates customizable de-
scriptions of objects from an database, but it features
a novel algorithm for generating comparisons be-
tween a new object and objects that have previously
been encountered, which stands out from previous
research in this area because it uses several explicit
parameters to choose the most relevant and interest-
ing comparisons given the context (Isard, 2007).
There have been previous evaluations of some of
these systems, including (Cox et al, 1999; Karasi-
mos and Isard, 2004). Karasimos and Isard con-
ducted an evaluation of comparisons and aggrega-
tion in the M-PIRO system. The results showed that
participants learned more and perceived that they
learned more from texts that contained comparisons
and aggregations than they did from texts that did
not. In this study, we investigate whether these re-
sults generalize to our new domain, and we isolate
the effect of comparisons from that of aggregation.
2 Knowledge Base Construction
2.1 Corpus Collection
We collected a small corpus to investigate the type
of facts disc jockeys tend to say about music. We se-
lected two genres where music descriptions between
pieces were common, jazz and classical music. The
programmes we used were broadcast on BBC Ra-
dio Three2. We transcribed sixty-four discussions;
to maintain uniformity, we followed the Linguistic
Data Consortium?s transcription guidelines3. This
2http://www.bbc.co.uk/radio3
3http://projects.ldc.upenn.edu/Transcription/quick-trans
169
was not a thorough corpus collection; the purpose of
collecting examples was to gain a sense of what disc
jockeys tend to discuss and compare.
2.2 Ontology Design
Based on the transcribed examples, we selected and
hand-wrote twelve database entries for music tracks,
using the authoring tool developed by the M-PIRO
project (Androutsopoulos et al, 2007). We trans-
formed the output of this tool into files suitable for
Methodius using an ad-hoc collection of Perl and
XSLT scripts, which also added the necessary infor-
mation to the OpenCCG grammars (White, 2006)
used by Methodius. We discuss future plans in this
area in Section 5.
We created a single-inheritance ontology for a
knowledge base of music pieces. First, we listed
the high-level entity types in the music domain, such
as ?person?, ?instrument?, ?classical music period?,
and ?jazz music period?. We then added attributes
commonly found in our disc jockey transcriptions.
For each entity type, we defined a set of fields. For
example, the classical-period field must contain an
entity which expresses a classical music piece?s time
period. We also specified a microplanning expres-
sion for each field, which provides detail on how the
field?s information should be generated at the sen-
tence level. We then added all the lexical items nec-
essary for the music domain.
2.3 Ontology Population
We populated our domain with six classical music
pieces and six jazz music pieces from the allmu-
sic.com database4. The songs were selected to yield
at least two interesting comparisons when placed in
a specific order. We also added entities linked to the
twelve songs, for example, each song?s album, per-
former, and composer, and information about these
entities. One challenge inherent in selecting these
entities from a publicly available database was to
eliminate as much common knowledge as possi-
ble about the music. In order to decrease back-
ground knowledge as a potential factor in our ex-
periment, we selected songs that primarily did not
contain popular performers, composers, and con-
ductors. We were able to gauge the popularity of
4http://www.allmusic.com
"Avatar" was written by Gary Husband and it was
performed by Billy Cobham, who was influenced
by Miles Davis. Billy Cobham originated from
Panama City, Panama and he played the drums; he
was active from the 1970s to the 1990s and he
participated in the Mahavishnu Orchestra. He
was influenced by Miles Davis. "Avatar" was
written during the Fusion period.
Figure 1: A generated description without comparisons.
Unlike "Fracture" and "A Mystery in Town",
which were written by Eddie "Lockjaw" Davis and
were performed by Fats Navarro, "Avatar" was
written by Gary Husband and it was performed by
Billy Cobham. Cobham originated from Panama
City, Panama and he played the drums; he was
active from the 1970s to the 1990s and he
participated in the Mahavishnu Orchestra. He
was influenced by Miles Davis. "Avatar" was
written during the Fusion period.
Figure 2: A generated description with comparisons to
previously described songs.
artists by their ?popularity rank? in the allmusic.com
database. However, we had to maintain a careful
balance between obscure artists and the ability to
generate interesting comparisons. Obscure artists
had less detailed information in the allmusic.com
database than popular music artists, so were forced
to select a few popular music artists for our exper-
iment, as their music pieces had multiple possible
interesting comparisons.
3 Experiment
We tried to maintain as many conditions from the
previous, similar study (Karasimos and Isard, 2004)
as possible to allow us to directly compare our re-
sults to theirs. The previous study established that
people learned more and perceived that they learned
more from text enriched with comparisons and ag-
gregations of facts than from texts that contained
neither. Our experimental design was similar to
theirs but all conditions of our experiment contained
text generated with aggregations of facts; our aim
was to isolate the effects of comparisons from those
of sentence aggregation.
For jazz texts, comparisons between songs involv-
ing performers, albums, composers, and time peri-
ods were possible. Classical texts could produce
all four of these types of comparisons. In addi-
tion, classical texts could also include comparisons
of conductors. Although the potential similarities
170
for classical and jazz texts were not equal, we de-
cided to include the conductor as a potential com-
parison for classical music. This is because across
both text types, we maintained the same number of
generated comparisons for each text type by limit-
ing Methodius to generating only one comparison
or contrast per paragraph of text. We present exam-
ples of a paragraph of text generated by Methodius
without (Figure 1) and with (Figure 2) comparisons.
In both cases, we assume that the user has already
seen texts about the songs ?Fracture? and ?A Mys-
tery in Town?, which expressed the facts about these
previous songs which are used in the comparisons in
Figure 2; the comparison text does not contain more
new information.
3.1 Evaluation Design
For our user study, we created a web interface using
WebExp2 Experiment Design software5 that con-
tained text generated by Methodius from our music
fact knowledge base. Forty fluent English speak-
ers were recruited and directed to a web page that
gave detailed instructions. After providing some ba-
sic personal information including their name, age,
gender, occupation and native languages, subjects
started with a test page, where they read a sample
paragraph and responded to one factual question, to
make sure that they had understood the interface,
and they then proceeded to the main experiment.
Participants read 6 paragraphs about either jazz
or classical music, and answered 15 factual recall
questions. They then read a further 6 paragraphs
about the other type of music, followed by 15 fac-
tual recall questions on the second set of texts. Fi-
nally they completed a post-experimental survey of
12 Likert Scale questions (Likert, 1932). We used
a within-subjects design, where each subject saw
two sets of texts, one classical and one jazz, one
with and one without comparisons, and the order
in which text sets were presented was controlled.
The multiple choice questions did not change given
the condition; so every participant saw the same
two sets of 15 multiple-choice questions in random-
ized orders. Seven multiple-choice questions of each
fifteen-question set dealt with facts that may be rein-
forced by comparisons. The remaining eight ques-
5http://www.webexp.info
Group Texts with com-
parisons
Texts without
comparisons
A 4.15 (1.814) 3.35 (1.872)
B 4.45 (1.638) 3.10 (1.651)
All 4.30 (1.713) 3.23 (1.747)
Table 1: Mean multiple choice scores with standard devi-
ation in brackets.
tions in each section served as a control for this ex-
periment.
On each page, the interface presented an image of
a paragraph of text generated by Methodius. The
users proceeded to the next paragraph when they
were ready by pressing the ?Next song? or ?Next
piece? button, depending on whether the music type
was jazz or classical. The texts were presented as
images for two reasons: so that the presentation of
stimuli would remain consistent across the differ-
ent computers and to prevent the text from being
selected by the participant, thus discouraging them
from copying the text and placing it into another
window as a reference to answer the factual recall
questions asked later.
4 Results
A summary of the participants? multiple choice
scores are shown in Table 1. Group A read classi-
cal texts with comparisons and jazz texts without,
and Group B read jazz texts with comparisons and
classical texts without.
We performed a 2-way repeated measures
ANOVA on our data and found that participants per-
formed significantly better on questions about the
texts which had comparisons (F (1, 36) = 11.131,
p < .01). There were no ordering or grouping
effects?the performance of participants did not de-
pend on which type of texts they saw first, or on
which type of texts contained comparisons.
In general, the Likert scores showed no signifi-
cant differences between the texts which had com-
parisons and those which did not. Karasimos and
Isard (2004) did find significant differences, but in
their case, texts had either comparisons and sen-
tence aggregations, or neither. In our study, all the
texts had sentence aggregations, so it may be this
factor which contributed to their higher Likert re-
171
sults on questions such as ?I enjoyed reading about
these songs? and the binary ?Which text (quality,
fluency) did you like more? question, for which we
also found no significant difference. Details of re-
sults and statistics can be found in (Marge, 2007).
5 Conclusions and Future Work
We have shown that the Methodius comparison gen-
eration algorithm does generalize to new domains,
and that it is possible to quickly author a new domain
and generate fluent and readable text, using an ap-
propriate authoring tool. We have also confirmed the
findings of previous studies, and showed that the use
of comparisons in texts does significantly improve
participants? recall of the facts which they have read.
In future work, we would like to use the cur-
rent text generation in an automatic DJ system with
streaming music, and perform further user studies in
order to make the texts as interesting and relevant
as possible. We would also like to perform a study
in which we compare the output of the comparison
algorithm using different parameter settings, to see
whether users express a preference.
Since this work was carried out, Methodius has
been adapted to accept ontologies and sentence
plans written in OWL/RDF. These can be created
using the Prote?ge? editor6 with an NLG plugin de-
veloped at the Athens University of Economics and
Business as part of the NaturalOWL generation sys-
tem (Galanis and Androutsopoulos, 2007), which is
available as an open source package7. A more prin-
cipled method for the OpenCCG conversion process
than the one described in Section 2.2 is in develop-
ment, and we hope to publish a paper on this subject.
Acknowledgements
The authors would like to acknowledge the help
and advice given by Colin Matheson, Ellen Bard,
Keith Edwards, Ray Carrick, Frank Keller, and Neil
Mayo and the comments of the anonymous review-
ers. This work was funded in part by a grant from
the Edinburgh-Stanford Link and by the Saint An-
drew?s Society of the State of New York. The music
data in this study was used with the permission of
the All Music Guide.
6http://www.protege.stanford.edu
7http://www.aueb.gr/users/ion/software/NaturalOWL.tar.gz
References
I. Androutsopoulos, J. Oberlander, and V. Karkaletsis.
2007. Source authoring for multilingual generation
of personalised object descriptions. Natural Language
Engineering, 13:191?233.
R. Cox, M. O?Donnell, and J. Oberlander. 1999. Dy-
namic versus static hypermedia in museum education:
an evaluation of ILEX, the intelligent labelling ex-
plorer. In Proceedings of the Artificial Intelligence in
Education conference, Le Mans.
R. Dale, J. Green, M. Milosavljevic, C. Paris, C. Ver-
spoor, and S. Williams. 1998. The realities of gener-
ating natural language from databases. In Proceedings
of the 11th Australian Joint Conference on Artificial
Intelligence, Brisbane, Australia.
D. Galanis and I. Androutsopoulos. 2007. Generating
multilingual descriptions from linguistically annotated
OWL ontologies: the NaturalOWL system. In Pro-
ceedings of ENLG 2007.
A. Isard, J. Oberlander, I. Androutsopoulos, and C. Math-
eson. 2003. Speaking the users? languages. IEEE In-
telligent Systems, 18(1):40?45. Special Issue on Ad-
vances in Natural Language Processing.
A. Isard. 2007. Choosing the best comparison under
the circumstances. In Proceedings of the International
Workshop on Personalization Enhanced Access to Cul-
tural Heritage, Corfu, Greece.
A. Karasimos and A. Isard. 2004. Multi-lingual eval-
uation of a natural language generation system. In
Proceedings of the Fourth International Conference on
Language Resources and Evaluation, Lisbon, Portu-
gal.
R. Likert. 1932. A technique for the measurement of
attitudes. Archives of Psychology, 22(140):1?55.
M. Marge. 2007. An evaluation of comparison genera-
tion in the methodius natural language generation sys-
tem. Master?s thesis, University of Edinburgh.
K. McKeown. 1985. Text Generation: Using Discourse
Strategies and Focus Constraints to Generate Natu-
ral Language Text. Cambridge University Press, New
York, NY, USA.
M. Milosavljevic. 1997. Content selection in compari-
son generation. In 6th European Workshop on Natural
Language Generation), Duisburg, Germany.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7:225?250.
M.White. 2006. Efficient realization of coordinate struc-
tures in combinatory categorial grammar. Research on
Language and Computation, 4(1):39?75.
172
Situated Reference in a Hybrid Human-Robot Interaction System
Manuel Giuliani1 and Mary Ellen Foster2 and Amy Isard3
Colin Matheson3 and Jon Oberlander3 and Alois Knoll1
1Informatik VI: Robotics and Embedded Systems, Technische Universita?t Mu?nchen
2School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
3Institute for Communicating and Collaborative Systems, School of Informatics, University of Edinburgh
Abstract
We present the situated reference genera-
tion module of a hybrid human-robot in-
teraction system that collaborates with a
human user in assembling target objects
from a wooden toy construction set. The
system contains a sub-symbolic goal in-
ference system which is able to detect the
goals and errors of humans by analysing
their verbal and non-verbal behaviour. The
dialogue manager and reference genera-
tion components then use situated refer-
ences to explain the errors to the human
users and provide solution strategies. We
describe a user study comparing the results
from subjects who heard constant refer-
ences to those who heard references gener-
ated by an adaptive process. There was no
difference in the objective results across
the two groups, but the subjects in the
adaptive condition gave higher subjective
ratings to the robot?s abilities as a conver-
sational partner. An analysis of the objec-
tive and subjective results found that the
main predictors of subjective user satisfac-
tion were the user?s performance at the as-
sembly task and the number of times they
had to ask for instructions to be repeated.
1 Introduction
When two humans jointly carry out a mutual task
for which both know the plan?for example, as-
sembling a new shelf?it frequently happens that
one makes an error, and the other has to assist
and to explain what the error was and how it can
be solved. Humans are skilled at spotting errors
committed by another, as well as errors which
they made themselves. Recent neurological stud-
ies have shown that error monitoring?i.e., ob-
serving the errors made by oneself or by others?
plays an important role in joint activity. For ex-
ample, Bekkering et al (2009) have demonstrated
that humans show the same brain activation pat-
terns when they make an error themselves and
when they observe someone else making an error.
In this paper, we describe a human-robot inter-
action (HRI) system that is able both to analyse
the actions and the utterances of a human part-
ner to determine if the human made an error in
the assembly plan, and to explain to the human
what went wrong and what to do to solve the prob-
lem. This robot combines approaches from sub-
symbolic processing and symbolic reasoning in a
hybrid architecture based on that described in Fos-
ter et al (2008b).
During the construction process, it is frequently
necessary to refer to an object which is being used
to assemble the finished product, choosing an un-
ambigious reference to distinguish the object from
the others available. The classic reference gen-
eration algorithm, on which most subsequent im-
plementations are based, is the incremental algo-
rithm of Dale and Reiter (1995), which selects
a set of attributes of a target object to single it
out from a set of distractor objects. In real-world
tasks, the speaker and hearer often have more con-
text in common than just the knowledge of object
attributes, and several extensions have been pro-
posed, dealing with visual and discourse salience
(Kelleher and Kruijff, 2006) and the ability to pro-
duce multimodal references including actions such
as pointing (van der Sluis, 2005; Kranstedt and
Wachsmuth, 2005).
Foster et al (2008a) noted another type of mul-
timodal reference which is particularly useful in
embodied, task-based contexts: haptic-ostensive
reference, in which an object is referred to as it
is being manipulated by the speaker. Manipulat-
ing an object, which must be done in any case as
part of the task, also makes an object more salient
and therefore affords linguistic references that in-
Figure 1: The dialogue robot
dicate the increased accessibility of the referent.
This type of reference is similar to the placing-for
actions noted by Clark (1996).
An initial approach for generating referring ex-
pressions that make use of haptic-ostensive refer-
ence was described in (Foster et al, 2009a). With
this system, a study was conducted comparing the
new reference strategy to the basic Dale and Reiter
incremental algorithm. Na??ve users reported that it
was significantly easier to understand the instruc-
tions given by the robot when it used references
generated by the more sophisticated algorithm. In
this paper, we perform a similar experiment, but
making use of a more capable human-robot in-
teraction system and a more complete process for
generating situated references.
2 Hybrid Human-Robot Dialogue
System
The experiment described in this paper makes use
of a hybrid human-robot dialogue system which
supports multimodal human-robot collaboration
on a joint construction task. The robot (Figure 1)
has a pair of manipulator arms with grippers,
mounted in a position to resemble human arms,
and an animatronic talking head (van Breemen,
2005) capable of producing facial expressions,
rigid head motion, and lip-synchronised synthe-
sised speech. The subject and the robot work to-
gether to assemble wooden construction toys on
a common workspace, coordinating their actions
through speech (English or German), gestures, and
facial expressions.
The robot can pick up and move objects in the
workspace and perform simple assembly tasks. In
the scenario considered here, both of the partici-
pants know the assembly plan and jointly execute
it. The robot assists the human, explains necessary
assembly steps in case the human makes an error,
and offers pieces as required. The workspace is di-
vided into two areas?one belonging to the robot
and one to the human?to make joint action nec-
essary for task success.
The system has components which use both
sub-symbolic and symbolic processing. It in-
cludes a goal inference module based on dynamic
neural fields (Erlhagen and Bicho, 2006; Bicho
et al, 2009), which selects the robot?s next actions
based on the human user?s actions and utterances.
Given a particular assembly plan and the knowl-
edge of which objects the user has picked up, this
module can determine when the user has made
an error. The system also incorporates a dialogue
manager based on the TrindiKit dialogue manage-
ment toolkit (Larsson and Traum, 2000), which
implements the information-state based approach
to dialogue management. This unique combina-
tion of abilities means that when the robot detects
that its human partner has made an error?for ex-
ample, picking up or requesting an assembly piece
that is not needed in the current step of the building
plan?it can explain to the human what the error
was and what can be done to correct the mistake?
for example by picking up or indicating the correct
assembly piece.
Messages from all of the system?s input chan-
nels (speech, object recognition, and gesture
recognition) are processed and combined by a
multimodal fusion component based on (Giuliani
and Knoll, 2008), which is the link between the
symbolic and the sub-symbolic parts of the sys-
tem. The fusion component then communicates
with the goal inference module, which calculates
the next action instructions for the robot and also
determines if the user made an error. From there,
fusion combines the information from goal infer-
ence with the input data and sends unified hy-
potheses to the dialogue manager.
When it receives the fusion hypotheses, the dia-
logue manager uses the dialogue history and the
physical and task context to choose a response.
It then sends a high-level specification of the de-
1. System First we will build a windmill.
2. User Okay.
3. User {picks up a yellow cube, unnecessary piece for a
windmill}
4. System You don?t need a yellow cube to build a windmill.
5. System To build a windmill, you first need to build a
tower.
6. System [picking up and holding out red cube] To build
the tower, insert the green bolt through the end of this
red cube and screw it into the blue cube.
7. User [takes cube, performs action] Okay.
Figure 2: Sample human-robot dialogue, showing
adaptively-generated situated references
sired response to the output planner, which in turn
sends commands to each output channel: linguis-
tic content (including multimodal referring ex-
pressions), facial expressions and gaze behaviours
of the talking head, and actions of the robot ma-
nipulators. The linguistic outputs are realised us-
ing the OpenCCG surface realiser (White, 2006).
3 Reference Generation
In this system, two strategies were implemented
for generating references to objects in the world:
a constant version that uses only the basic incre-
mental algorithm (Dale and Reiter, 1995) to se-
lect properties, and an adaptive version that uses
more of the physical, dialogue and task context
to help select the references. The constant sys-
tem can produce a definite or indefinite reference,
and the most appropriate combination of attributes
according to the incremental algorithm. The adap-
tive system also generates pronominal and deictic
references, and introduces the concept of multiple
types of distractor sets depending on context.
Figure 2 shows a fragment of a sample interac-
tion in which the user picks up an incorrect piece:
the robot detects the error and describes the correct
assembly procedure. The underlined references
show the range of output produced by the adap-
tive reference generation module; for the constant
system, the references would all have been ?the
red cube?. The algorithms used by the adaptive
reference generation module are described below.
3.1 Reference Algorithm
The module stores a history of the referring ex-
pressions spoken by both the system and the user,
and uses these together with distractor sets to se-
lect referring expressions. In this domain there are
two types of objects which we need to refer to:
concrete objects in the world (everything which is
on the table, or in the robot?s or user?s hand), and
objects which do not yet exist, but are in the pro-
cess of being created. For non-existent objects we
do not build a distractor set, but simply use the
name of the object. In all other cases, we use one
of three types of distractor set:
? all the pieces needed to build a target object;
? all the objects referred to since the last men-
tion of this object; or
? all the concrete objects in the world.
The first type of set is used if the object under
consideration (OUC) is a negative reference to a
piece in context of the creation of a target object.
In all other cases, the second type is used if the
OUC has been mentioned before and the third type
if it has not.
When choosing a referring expression, we first
process the distractor set, comparing the proper-
ties of the OUC with the properties of all distrac-
tors. If a distractor has a different type from the
OUC, it is removed from the distractor set. With
all other properties, if the distractor has a different
value from the OUC, it is removed from the dis-
tractor set, and the OUC?s property value is added
to the list of properties to use.
We then choose the type of referring expression.
We first look for a previous reference (PR) to the
OUC, and if one exists, determine whether it was
in focus. Depending on the case, we use one of the
following reference strategies.
No PR If the OUC does not yet exist or we are
making a negative reference, we use an indef-
inite article. If the robot is holding the OUC,
we use a deictic reference. If the OUC does
exist and there are no distractors, we use a
definite; if there are distractors we use an in-
definite.
PR was focal If the PR was within the same turn,
we choose a pronoun for our next reference.
If it was in focus but in a previous turn, if
the robot is holding the OUC we use a deictic
reference, and if the robot is not holding it,
we use a pronoun.
PR was not focal If the robot is holding the
OUC, we make a deictic reference. Other-
wise, if the PR was a pronoun, definite, or de-
ictic, we use a definite article. If the PR was
indefinite and there are no distractors, we use
a definite article, if there are distractors, we
use an indefinite article.
If there are any properties in the list, and the
reference chosen is not a pronoun, we add them.
3.2 Examples of the Reference Algorithm
We will illustrate the reference-selection strategy
with two cases from the dialogue in Figure 2.
Utterance 4 ?a yellow cube?
This object is going to be referred to in a negative
context as part of a windmill under construction,
so the distractor set is the set of objects needed to
make a windmill: {red cube, blue cube, small slat,
small slat, green bolt, red bolt}.
We select the properties to use in describing the
object under consideration, processing the distrac-
tor set. We first remove all objects which do not
share the same type as our object under considera-
tion, which leaves {red cube, blue cube}. We then
compare the other attributes of our new object with
the remaining distractors - in this case ?colour?.
Since neither cube shares the colour ?yellow? with
the target object, both are removed from the dis-
tractor set, and ?yellow? is added to the list of
properties to use.
There is no previous reference to this object,
and since we are making a negative reference,
we automatically choose an indefinite article. We
therefore select the reference ?a yellow cube?.
Utterance 6 ?it? (a green bolt)
This object has been referred to before, earlier in
the same utterance, so the distractor set is all the
references between the earlier one and this one?
{red cube}. Since this object has a different type
from the bolt we want to describe, the distractor
set is now empty, and nothing is added to the list
of properties to use.
There is a previous definite reference to the ob-
ject in the same utterance: ?the green bolt?. This
reference was focal, so we are free to use a pro-
noun if appropriate. Since the previous reference
was definite, and the object being referred to does
exist, we choose to use a pronoun. We therefore
select the reference ?it?.
4 Experiment Design
In the context of the HRI system, a constant refer-
ence strategy is sufficient in that it makes it possi-
ble for the robot?s partner to know which item is
needed. On the other hand, while the varied forms
produced by the more complex mechanism can in-
crease the naturalness of the system output, they
may actually be insufficient if they are not used
in appropriate current circumstances?for exam-
ple, ?this cube? is not a particularly helpful refer-
ence if a user has no way to tell which ?this? is.
As a consequence, the system for generating such
references must be sensitive to the current state
of joint actions and?in effect?of joint attention.
The difference between the two systems is a test of
the adaptive version?s ability to adjust expressions
to pertinent circumstances. It is known that peo-
ple respond well to reduced expressions like ?this
cube? or ?it? when another person uses them ap-
propriately (Bard et al, 2008); we need to see if
the robot system can also achieve the benefits that
situated reference could provide.
To address this question, the human-robot di-
alogue system was evaluated through a user study
in which subjects interacted with the complete sys-
tem. Using a between-subjects design, this study
compared the two reference strategies, measuring
the users? subjective reactions to the system along
with their overall performance in the interaction.
Based on the findings from the user evaluation de-
scribed in (Foster et al, 2009a)?in which the pri-
mary effect of varying the reference strategy was
on the users? subjective opinion of the robot?the
main prediction for this study was as follows:
? Subjects who interact with a system using
adaptive references will rate the quality of
the robot?s conversation more highly than the
subjects who hear constant references.
We made no specific prediction regarding the
effect of reference strategy on any of the objec-
tive measures: based on the results of the user
evaluation mentioned above, there is no reason to
expect an effect either way. Note that?as men-
tioned above?if the adaptive version makes in-
correct choices, that may have a negative impact
on users? ability to understand the system?s gener-
ated references. For this reason, even a finding of
(a) Windmill (b) Railway signal
Figure 3: Target objects for the experiment
no objective difference would demonstrate that the
adaptive references did not harm the users? ability
to interact with the system, as long as it was ac-
companied by the predicted improvement in sub-
jective judgements.
4.1 Subjects
41 subjects (33 male) took part in this experiment.
The mean age of the subjects was 24.5, with a min-
imum of 19 and a maximum of 42. Of the subjects
who indicated an area of study, the two most com-
mon areas were Mathematics (14 subjects) and In-
formatics (also 14 subjects). On a scale of 1 to 5,
subjects gave a mean assessment of their knowl-
edge of computers at 4.1, of speech-recognition
systems at 2.0, and of human-robot systems at 1.7.
Subjects were compensated for their participation
in the experiment.
4.2 Scenario
This study used a between-subjects design with
one independent variable: each subject interacted
either with a system that used a constant strategy
to generate referring expressions (19 subjects), or
else with a system that used an adaptive strategy
(22 subjects).1
Each subject built two objects in collaboration
with the system, always in the same order. The
first target object was the windmill (Figure 3a);
after the windmill was completed, the robot and
human then built a railway signal (Figure 3b). For
both target objects, the user was given a building
plan (on paper). To induce an error, both of the
plans given to the subjects instructed them to use
an incorrect piece: a yellow cube instead of a red
cube for the windmill, and a long (seven-hole) slat
instead of a medium (five-hole) slat for the rail-
1The results of an additional three subjects in the constant-
reference condition could not be analysed due to technical
difficulties.
way signal. The subjects were told that the plan
contained an error and that the robot would cor-
rect them when necessary, but did not know the
nature of the error.
When the human picked up or requested an in-
correct piece during the interaction, the system de-
tected the error and explained to the human what
to do in order to assemble the target object cor-
rectly. When the robot explained the error and
when it handed over the pieces, it used referring
expressions that were generated using the constant
strategy for half of the subjects, and the adaptive
strategy for the other half of the subjects.
4.3 Experimental Set-up and Procedure
The participants stood in front of the table facing
the robot, equipped with a headset microphone for
speech recognition. The pieces required for the
target object?plus a set of additional pieces in or-
der to make the reference task more complex?
were placed on the table, using the same layout
for every participant. The layout was chosen to
ensure that there would be points in the interaction
where the subjects had to ask the robot for build-
ing pieces from the robot?s workspace, as well as
situations in which the robot automatically handed
over the pieces. Along with the building plan men-
tioned above, the subjects were given a table with
the names of the pieces they could build the ob-
jects with.
4.4 Data Acquisition
At the end of a trial, the subject responded to
a usability questionnaire consisting of 39 items,
which fell into four main categories: Intelligence
of the robot (13 items), Task ease and task suc-
cess (12 items), Feelings of the user (8 items),
and Conversation quality (6 items). The items on
the questionnaire were based on those used in the
user evaluation described in (Foster et al, 2009b),
but were adapted for the scenario and research
questions of the current study. The questionnaire
was presented using software that let the subjects
choose values between 1 and 100 with a slider. In
addition to the questionnaire, the trials were also
video-taped, and the system log files from all tri-
als were kept for further analysis.
5 Results
We analysed the data resulting from this study in
three different ways. First, the subjects? responses
Table 1: Overall usability results
Constant Adaptive M-W
Intell. 79.0 (15.6) 74.9 (12.7) p = 0.19, n.s.
Task 72.7 (10.4) 71.1 (8.3) p = 0.69, n.s.
Feeling 66.9 (15.9) 66.8 (14.2) p = 0.51, n.s.
Conv. 66.1 (13.6) 75.2 (10.7) p = 0.036, sig.
Overall 72.1 (11.2) 71.8 (9.1) p = 0.68, n.s.
to the questionnaire items were compared to de-
termine if there was a difference between the re-
sponses given by the two groups. A range of sum-
mary objective measures were also gathered from
the log files and videos?these included the dura-
tion of the interaction measured both in seconds
and in system turns, the subjects? success at build-
ing each of the target objects, the number of times
that the robot had to explain the construction plan
to the user, and the number of times that the users
asked the system to repeat its instructions. Finally,
we compared the results on the subjective and ob-
jective measures to determine which of the objec-
tive factors had the largest influence on subjective
user satisfaction.
5.1 Subjective Measures
The subjects in this study gave a generally positive
assessment of their interactions with the system on
the questionnaire?with a mean overall satisfac-
tion score of 72.0 out of 100?and rated the per-
ceived intelligence of the robot particularly highly
(overall mean of 76.8). Table 1 shows the mean
results from the two groups of subjects for each
category on the user-satisfaction questionnaire, in
all cases on a scale from 0?100 (with the scores
for negatively-posed questions inverted).
To test the effect of reference strategy on the
usability-questionnaire responses, we performed a
Mann-Whitney test comparing the distribution of
responses from the two groups of subjects on the
overall results, as well as on each sub-category of
questions. For most categories, there was no sig-
nificant difference between the responses of the
two groups, with p values ranging from 0.19 to
0.69 (as shown in Table 1). The only category
where a significant difference was found was on
the questionnaire items that asked the subjects to
assess the robot?s quality as a conversational part-
ner; for those items, the mean score from sub-
jects who heard the adaptive references was sig-
nificantly higher (p < 0.05) than the mean score
from the subjects who heard references generated
by the constant reference module. Of the six ques-
Table 2: Objective results (all differences n.s.)
Measure Constant Adaptive M-W
Duration (s.) 404.3 (62.8) 410.5 (94.6) p = 0.90
Duration (turns) 29.8 (5.02) 31.2 (5.57) p = 0.44
Rep requests 0.26 (0.45) 0.32 (0.78) p = 0.68
Explanations 2.21 (0.63) 2.41 (0.80) p = 0.44
Successful trials 1.58 (0.61) 1.55 (0.74) p = 0.93
tions that were related to the conversation quality,
the most significant impact was on the two ques-
tions which assessed the subjects? understanding
of what they were able to do at various points dur-
ing the interaction.
5.2 Objective Measures
Based on the log files and video recordings, we
computed a range of objective measures. These
measures were divided into three classes, based
on those used in the PARADISE dialogue-system
evaluation framework (Walker et al, 2000):
? Two dialogue efficiency measures: the mean
duration of the interaction as measured both
in seconds and in system turns;
? Two dialogue quality measures: the number
of times that the robot gave explanations, and
the number of times that the user asked for
instructions to be repeated; and
? One task success measure: how many of the
(two) target objects were constructed as in-
tended (i.e., as shown in Figure 3).
For each of these measures, we tested whether the
difference in reference strategy had a significant
effect, again via a Mann-Whitney test. Table 2 il-
lustrates the results on these objective measures,
divided by the reference strategy.
The results from the two groups of subjects
were very similar on all of these measures: on
average, the experiment took 404 seconds (nearly
seven minutes) to complete with the constant strat-
egy and 410 seconds with the adaptive, the mean
number of system turns was close to 30 in both
cases, just over one-quarter of all subjects asked
for instructions to be repeated, the robot gave just
over two explanations per trial, and about three-
quarters of all target objects (i.e. 1.5 out of 2)
were correctly built. The Mann-Whitney test con-
firms that none of the differences between the two
groups even came close to significance on any of
the objective measures.
5.3 Comparing Objective and Subjective
Measures
In the preceding sections, we presented results on
a number of objective and subjective measures.
While the subjects generally rated their experi-
ence of using the system positively, there was
some degree of variation, most of which could not
be attributed to the difference in reference strat-
egy. Also, the results on the objective measures
varied widely across the subjects, but again were
not generally affected by the reference strategy.
In this section, we examine the relationship be-
tween these two classes of measures in order to
determine which of the objective measures had the
largest effect on users? subjective reactions to the
HRI system.
Being able to predict subjective user satisfac-
tion from more easily-measured objective proper-
ties can be very useful for developers of interac-
tive systems: in addition to making it possible to
evaluate systems based on automatically available
data without the need for extensive experiments
with users, such a performance function can also
be used in an online, incremental manner to adapt
system behaviour to avoid entering a state that is
likely to reduce user satisfaction (Litman and Pan,
2002), or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
We employed the procedure used in the PAR-
ADISE evaluation framework (Walker et al,
2000) to explore the relationship between the sub-
jective and objective factors. The PARADISE
model uses stepwise multiple linear regression to
predict subjective user satisfaction based on mea-
sures representing the performance dimensions of
task success, dialogue quality, and dialogue effi-
ciency, resulting in a predictor function of the fol-
lowing form:
Satisfaction =
n
?
i=1
wi ?N (mi)
The mi terms represent the value of each measure,
while the N function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Table 3 shows the predictor functions that were
derived for each of the classes of subjective mea-
sures in this study, using all of the objective mea-
sures from Table 2 as initial factors. The R2 col-
umn indicates the percentage of the variance in the
target measure that is explained by the predictor
function, while the Significance column gives sig-
nificance values for each term in the function.
In general, the two factors with the biggest in-
fluence on user satisfaction were the number of
repetition requests (which had a uniformly neg-
ative effect on user satisfaction), and the num-
ber of target objects correctly built by the user
(which generally had a positive effect). Aside
from the questions on user feelings, the R2 values
are generally in line with those found in previous
PARADISE evaluations of other dialogue systems
(Walker et al, 2000; Litman and Pan, 2002), and
in fact are much higher than those found in a pre-
vious similar study (Foster et al, 2009b).
6 Discussion
The subjective responses on the relevant items
from the usability questionnaire suggest that
the subjects perceived the robot to be a bet-
ter conversational partner if it used contextually
varied, situationally-appropriate referring expres-
sions than if it always used a baseline, constant
strategy; this supports the main prediction for this
study. The result also agrees with the findings of
a previous study (Foster et al, 2009a)?this sys-
tem did not incorporate goal inference and had a
less-sophisticated reference strategy, but the main
effect of changing reference strategy was also on
the users? subjective opinions of the robot?s inter-
active ability. These studies together support the
current effort in the natural-language generation
community to devise more sophisticated reference
generation algorithms.
On the other hand, there was no significant dif-
ference between the two groups on any of the
objective measures: the dialogue efficiency, dia-
logue quality, and task success were nearly iden-
tical across the two groups of subjects. A de-
tailed analysis of the subjects? gaze and object-
manipulation behaviour immediately after various
forms of generated references from the robot also
failed to find any significant differences between
the various reference types. These overall results
are not particularly surprising: studies of human-
human dialogue in a similar joint construction task
(Bard et al, In prep.) have demonstrated that the
collaborators preserve quality of construction in
Table 3: PARADISE predictor functions for each category on the usability questionnaire
Measure Function R2 Significance
Intelligence 76.8+7.00?N (Correct)?5.51?N (Repeats) 0.39 Correct: p < 0.001,
Repeats: p < 0.005
Task 72.4+3.54?N (Correct)?3.45?N (Repeats)?2.17?N (Explain) 0.43 Correct: p < 0.005,
Repeats: p < 0.01,
Explain: p? 0.10
Feeling 66.9?6.54?N (Repeats)+4.28?N (Seconds) 0.09 Repeats: p < 0.05,
Seconds: p? 0.12
Conversation 71.0+5.28?N (Correct)?3.08?N (Repeats) 0.20 Correct: p < 0.01,
Repeats: p? 0.10
Overall 72.0+4.80?N (Correct)?4.27?N (Repeats) 0.40 Correct: p < 0.001,
Repeats: p < 0.005
all cases, though circumstances may dictate what
strategies they use to do this. Combined with the
subjective findings, this lack of an objective effect
suggests that the references generated by the adap-
tive strategy were both sufficient and more natural
than those generated by the constant strategy.
The analysis of the relationship between the
subjective and objective measures analysis has
also confirmed and extended the findings from a
similar analysis (Foster et al, 2009b). In that
study, the main contributors to user satisfaction
were user repetition requests (negative), task suc-
cess, and dialogue length (both positive). In the
current study, the primary factors were similar,
although dialogue length was less prominent as
a factor and task success was more prominent.
These findings are generally intuitive: subjects
who are able to complete the joint construction
task are clearly having more successful interac-
tions than those who are not able to complete the
task, while subjects who need to ask for instruc-
tions to be repeated are equally clearly not hav-
ing successful interactions. The findings add ev-
idence that, in this sort of task-based, embodied
dialogue system, users enjoy the experience more
when they are able to complete the task success-
fully and are able to understand the spoken contri-
butions of their partner, and also suggest that de-
signers should concentrate on these aspects of the
interaction when designing the system.
7 Conclusions
We have presented the reference generation mod-
ule of a hybrid human-robot interaction system
that combines a goal-inference component based
on sub-symbolic dynamic neural fields with a
natural-language interface based on more tradi-
tional symbolic techniques. This combination of
approaches results in a system that is able to work
together with a human partner on a mutual con-
struction task, interpreting its partner?s verbal and
non-verbal behaviour and responding appropri-
ately to unexpected actions (errors) of the partner.
We have then described a user evaluation of this
system, concentrating on the impact of different
techniques for generating situated references in
the context of the robot?s corrective feedback. The
results of this study indicate that using an adaptive
strategy to generate the references significantly in-
creases the users? opinion of the robot as a con-
versational partner, without having any effect on
any of the other measures. This result agrees with
the findings of the system evaluation described in
(Foster et al, 2009a), and adds evidence that so-
phisticated generation techniques are able to im-
prove users? experiences with interactive systems.
An analysis of the relationship between the ob-
jective and subjective measures found that the
main contributors to user satisfaction were the
users? task performance (which had a positive ef-
fect on most measures of satisfaction), and the
number of times the users had to ask for instruc-
tions to be repeated (which had a generally neg-
ative effect). Again, these results agree with the
findings of a previous study (Foster et al, 2009b),
and also suggest priorities for designers of this
type of task-based interactive system.
Acknowledgements
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka and Levent Kent
for help in running the experiment and analysing
the data.
2http://www.jast-project.eu/
3http://www.ics.forth.gr/indigo/
References
E. G. Bard, R. Hill, and M. E. Foster. 2008. What
tunes accessibility of referring expressions in
task-related dialogue? In Proceedings of the
30th Annual Meeting of the Cognitive Science
Society (CogSci 2008). Chicago.
E. G. Bard, R. L. Hill, M. E. Foster, and M. Arai.
In prep. How do we tune accessibility in joint
tasks: Roles and regulations.
H. Bekkering, E.R.A. de Bruijn, R.H. Cuijpers,
R. Newman-Norlund, H.T. van Schie, and
R. Meulenbroek. 2009. Joint action: Neurocog-
nitive mechanisms supporting human interac-
tion. Topics in Cognitive Science, 1(2):340?
352.
E. Bicho, L. Louro, N. Hipolito, and W. Erlhagen.
2009. A dynamic field approach to goal infer-
ence and error monitoring for human-robot in-
teraction. In Proceedings of the Symposium on
?New Frontiers in Human-Robot Interaction?,
AISB 2009 Convention. Heriot-Watt University
Edinburgh.
H. H. Clark. 1996. Using Language. Cambridge
University Press.
R. Dale and E. Reiter. 1995. Computational inter-
pretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
19(2):233?263.
W. Erlhagen and E. Bicho. 2006. The dynamic
neural field approach to cognitive robotics.
Journal of Neural Engineering, 3(3):R36?R54.
M. E. Foster, E. G. Bard, R. L. Hill, M. Guhe,
J. Oberlander, and A. Knoll. 2008a. The roles
of haptic-ostensive referring expressions in co-
operative, task-based human-robot dialogue. In
Proceedings of HRI 2008.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009a. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI-09.
M. E. Foster, M. Giuliani, and A. Knoll. 2009b.
Comparing objective and subjective measures
of usability in a human-robot dialogue system.
In Proceedings of ACL-IJCNLP 2009.
M. E. Foster, M. Giuliani, T. Mu?ller, M. Rickert,
A. Knoll, W. Erlhagen, E. Bicho, N. Hipo?lito,
and L. Louro. 2008b. Combining goal inference
and natural-language dialogue for human-robot
joint action. In Proceedings of the 1st Interna-
tional Workshop on Combinations of Intelligent
Methods and Applications at ECAI 2008.
M. Giuliani and A. Knoll. 2008. MultiML:
A general-purpose representation language for
multimodal human utterances. In Proceedings
of ICMI 2008.
J. D. Kelleher and G.-J. M. Kruijff. 2006. Incre-
mental generation of spatial referring expres-
sions in situated dialog. In Proceedings of
COLING-ACL 2006.
A. Kranstedt and I. Wachsmuth. 2005. Incremen-
tal generation of multimodal deixis referring to
objects. In Proceedings of ENLG 2005.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language
Engineering, 6(3&4):323?340.
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue system.
User Modeling and User-Adapted Interaction,
12(2?3):111?137.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of AISB 2005
Creative Robotics Symposium.
I. F. van der Sluis. 2005. Multimodal Reference:
Studies in Automatic Generation of Multimodal
Referring Expressions. Ph.D. thesis, University
of Tilburg.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387?416.
M. White. 2006. Efficient realization of co-
ordinate structures in Combinatory Categorial
Grammar. Research on Language and Compu-
tation, 4(1):39?75.
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 338?340,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
BEETLE II: an adaptable tutorial dialogue system
Myroslava O. Dzikovska and Amy Isard and Peter Bell and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore,amy.isard,peter.bell}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{gwendolyn.campbell,natalie.steihauser}@navy.mil
Abstract
We present BEETLE II, a tutorial dialogue sys-
tem which accepts unrestricted language in-
put and supports experimentation with differ-
ent dialogue strategies. Our first system eval-
uation compared two dialogue policies. The
resulting corpus was used to study the impact
of different tutoring and error recovery strate-
gies on user satisfaction and student interac-
tion style. It can also be used in the future to
study a wide range of research issues in dia-
logue systems.
1 Introduction
There has recently been much interest in develop-
ing tutorial dialogue systems that understand student
explanations (Graesser et al, 1999; Aleven et al,
2001; Nielsen et al, 2008; VanLehn et al, 2007),
because it has been shown that high percentages of
self-explanation and student contentful talk are cor-
related with better learning in human-human tutor-
ing (Chi et al, 1994; Litman et al, 2009). How-
ever, most existing systems use pre-authored tutor
responses for addressing student errors. The advan-
tage of this approach is that tutors can devise reme-
diation dialogues that are highly tailored to specific
misconceptions, providing step-by-step scaffolding
and potentially suggesting additional exercises. The
disadvantage is a lack of adaptivity and generality:
students often get the same remediation for the same
error regardless of their past performance or dia-
logue context. It also becomes more difficult to ex-
periment with different dialogue policies (including
error recovery and tutorial policies determining the
most appropriate feedback), due to the complexities
in applying tutoring strategies consistently in a large
number of hand-authored remediations.
The BEETLE II system architecture is designed to
overcome these limitations (Callaway et al, 2007).
It uses a deep parser and generator, together with
a domain reasoner and a diagnoser, to produce de-
tailed analyses of student utterances and to generate
feedback automatically. This allows the system to
consistently apply the same tutorial policy across a
range of questions. The system?s modular setup and
extensibility also make it a suitable testbed for both
computational linguistics algorithms and more gen-
eral questions about theories of learning.
The system is based on an introductory electric-
ity and electronics course developed by experienced
instructional designers, originally created for use in
a human-human tutoring study. The exercises were
then transferred into a computer system with only
minor adjustments (e.g., breaking down compound
questions into individual questions). This resulted
in a realistic tutoring setup, which presents interest-
ing challenges to language processing components,
involving a wide variety of language phenomena.
We demonstrate a version of the system that un-
derwent a user evaluation in 2009, which found sig-
nificant learning gains for students interacting with
the system. The experimental data collection com-
pared two different dialogue policies implemented
in the system, and resulted in a corpus supporting
research into a variety of questions about human-
computer dialogue interaction (Dzikovska et al,
2010a).
338
Figure 1: Screenshot of the BEETLE II system
2 Example Interaction
The BEETLE II system delivers basic electricity and
electronics tutoring to students with no prior knowl-
edge of the subject. A screenshot is shown in Figure
1. The student interface includes an area to display
reading material, a circuit simulator, and a dialogue
history window. Currently all interactions with the
system are typed. Students read pre-authored cur-
riculum slides and carry out exercises which in-
volve experimenting with the circuit simulator and
explaining the observed behaviour. The system also
asks some high-level questions, such as ?What is
voltage??.
An example dialogue with the system, taken from
the evaluation corpus, is shown in Figure 2. It shows
three key system properties: after the student?s first
turn, which was correct but incomplete, the system
rephrases the correct part of the student answer and
prompts for the missing information. In the second
turn, the system cannot interpret the student utter-
ance, so it responds with a targeted help message
and a hint about the object that needs to be men-
tioned. Finally, in the last turn the system com-
bines the information from the tutor?s hint and the
student?s answers and restates the complete answer
since the current answer was completed over multi-
ple turns.
3 Data Analysis and Future Work
The data collected with the BEETLE II system has
been used to investigate several research questions
regarding discourse and dialogue: the effectiveness
of different error recovery strategies (Dzikovska et
al., 2010b); the underlying dimensions of user sat-
isfaction and their relationship with learning gain
(Dzikovska et al, 2011); the relationship between
(student) alignment in dialogue and learning gain
(Steinhauser et al, 2011); and the differences be-
tween students? social and metacognitive statements
depending on the interaction style (Dzikovska et al,
2010a). We are currently annotating the data with
additional interaction parameters, including correct-
ness of student answers and appropriateness of sys-
tem hints. This will allow us to apply PARADISE
339
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I?m sorry, I?m having a problem un-
derstanding. I don?t understand the
word ?effect? here. Here?s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
Tutor: You?re on the right track. Here?s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 2: Example interaction with the system from our
corpus
methodology to get additional insight into which pa-
rameters of tutorial dialogue affect learning gain and
user satisfaction.
We are also adding a speech interface to the sys-
tem, which will open new and interesting research
questions. Students often fail to use domain termi-
nology correctly, and in the tutoring domain it is
important to teach students to use proper terminol-
ogy, so incorrect usage must be detected and cor-
rected. This means that grammar-based language
models are not appropriate for the language mod-
elling, and opens new questions about robust ASR
and language interpretation in such domains.
Acknowledgements
This work has been supported in part by US Of-
fice of Naval Research grants N000141010085 and
N0001410WX20278. We thank Katherine Harrison,
Leanne Taylor, Charles Scott, Simon Caine, Charles
Callaway and Elaine Farrow for their contributions
to this effort.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Charles B. Callaway, Myroslava Dzikovska, Elaine Far-
row, Manuel Marques-Pita, Colin Matheson, and Jo-
hanna D. Moore. 2007. The Beetle and BeeDiff tutor-
ing systems. In Proceedings of SLaTE?07 (Speech and
Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Myroslava Dzikovska, Natalie B. Steinhauser, Jo-
hanna D. Moore, Gwendolyn E. Campbell, Kather-
ine M. Harrison, and Leanne S. Taylor. 2010a. Con-
tent, social, and metacognitive statements: An em-
pirical study comparing human-human and human-
computer tutorial dialogue. In Proceedings of ECTEL-
2010, pages 93?108.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010b. The
impact of interpretation problems on tutorial dialogue.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics(ACL-2010).
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2011. Ex-
ploring user satisfaction in a tutorial dialogue system.
In Proceedings of the 12th annual SIGdial Meeting on
Discourse and Dialogue.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proc. of 14th International
Conference on Artificial Intelligence in Education.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Learning to assess low-level conceptual under-
standing. In Proceedings 21st International FLAIRS
Conference, Coconut Grove, Florida, May.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proceed-
ings of the 15th International Conference on Artificial
Intelligence in Education (AIED-2011).
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technol-
ogy in Education, Farmington, PA, October.
340
Proceedings of the SIGDIAL 2014 Conference, pages 243?250,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Using Ellipsis Detection and Word Similarity for Transformation of
Spoken Language into Grammatically Valid Sentences
Manuel Giuliani
fortiss GmbH
Munich, Germany
giuliani@fortiss.org
Thomas Marschall
fortiss GmbH
Munich, Germany
marschat@in.tum.de
Amy Isard
University of Edinburgh
Edinburgh, UK
amyi@inf.ed.ac.uk
Abstract
When humans speak they often use gram-
matically incorrect sentences, which is a
problem for grammar-based language pro-
cessing methods, since they expect in-
put that is valid for the grammar. We
present two methods to transform spoken
language into grammatically correct sen-
tences. The first is an algorithm for au-
tomatic ellipsis detection, which finds el-
lipses in spoken sentences and searches
in a combinatory categorial grammar for
suitable words to fill the ellipses. The sec-
ond method is an algorithm that computes
the semantic similarity of two words us-
ing WordNet, which we use to find alter-
natives to words that are unknown to the
grammar. In an evaluation, we show that
the usage of these two methods leads to
an increase of 38.64% more parseable sen-
tences on a test set of spoken sentences
that were collected during a human-robot
interaction experiment.
1 Introduction
Computer systems that are designed to interact
verbally with humans need to be able to recog-
nise and understand human speech. In this pa-
per we use as an example the robot bartender
JAMES (Joint Action for Multimodal Embodied
Social Systems),
1
shown in Figure 1. The robot
is able to take drink orders from customers and to
serve drinks. It is equipped with automatic speech
recognition, to understand what the human is say-
ing, and it has a grammar, to parse and process the
spoken utterances.
The JAMES robot grammar was initially very
restricted, and therefore during grammar devel-
opment as well as during the user studies that
1
http://www.james-project.eu
Figure 1: The robot bartender of the JAMES
project interacting with a customer.
we conducted (Foster et al., 2012; Giuliani et al.,
2013; Keizer et al., 2013), we experienced situa-
tions in which the robot was not able to process
the spoken input by humans, because they spoke
sentences with grammatical structures that could
not be parsed by the grammar, they used words
that were not part of the grammar, or they left out
words. We had for example cases where humans
approached the robot and used a sentence with an
ellipsis (?I want Coke?, but the grammar expected
a determiner in front of ?Coke?) or a sentence with
a word that was unknown to the grammar (?I need
a water?, but ?need? was not part of the gram-
mar?s word list). In these cases, the robot was un-
able to process and to respond to the spoken ut-
terance by the human. Of course, these shortcom-
ings can be overcome by extending the grammar,
but even with a much more sophisticated grammar
there will always be instances of unexpected lan-
guage, and we believe that our approach can be
very useful in extending the coverage of a gram-
mar during testing or user studies.
Therefore, we present an approach to transform
unparseable spoken language into sentences that a
given grammar can parse. For ellipsis detection,
243
we present in Section 3.1 a novel algorithm that
searches for ellipses in a sentence and computes
candidate words to fill the ellipsis with words from
a grammar. In Section 3.2, we show how we use
WordNet (Miller, 1995) to find replacements for
words that are not in the robot?s grammar. In Sec-
tion 4 we evaluate our approach with a test set
of 211 spoken utterances that were recorded in
a human-robot interaction (HRI) experiment, and
the grammar for processing used in the same ex-
periment.
2 Related Work
The work described in this paper draws on re-
search and techniques from three main areas: the
automatic detection of ellipses in sentences, cal-
culation of semantic similarity between two words
using WordNet, and spoken language processing.
This section provides a summary of relevant work
in these areas.
2.1 Ellipsis Detection
There is a wide range of research in ellipsis de-
tection in written language, where different types
of ellipses are widely defined, such as gapping,
stripping or verb phrase ellipsis (Lappin, 1996).
For example, an ellipsis occurs when a redundant
word is left out of succeeding sentences, such as
the words ?want to have? in the sentence ?I want
to have a water, and my friend a juice?, which are
omitted in the second part of the sentence.
The detection of verb phrase ellipses (VPE)
is a subfield of ellipsis detection that has re-
ceived much attention. For VPE detection, re-
searchers have used machine learning algorithms
which were trained on grammar-parsed corpora,
for example in the works of Hardt (1997), Nielsen
(2004a), Nielsen (2004b), and Smith and Rauchas
(2006). Other approaches for ellipsis detection
rely on symbolic processing of sentences, which is
similar to our work. Haddar and Hamadou (1998)
present a method for ellipsis detection in the Ara-
bic language, which is based on an augmented
transition network grammar. Egg and Erk (2001)
present a general approach for ellipsis detection
and resolution that uses a language for partial de-
scription of ?-terms called Constraint Language
for Lambda Structures.
2.2 WordNet-based Semantic Similarity
Calculation
WordNet is used in many varied natural language
processing applications, such as word sense dis-
ambiguation, determining the structure of texts,
text summarisation and annotation, information
extraction and retrieval, automatic indexing, lexi-
cal selection, and the automatic correction of word
errors in text. In our work, we use WordNet
to find similar or synonym words. In previous
work, researchers have proposed several methods
to generally compute the semantic relatedness of
two words using WordNet. Budanitsky and Hirst
(2006) review methods to determine semantic re-
latedness. Newer examples for WordNet-based
calculation of semantic similarity are the works
by Qin et al. (2009), Cai et al. (2010), Liu et al.
(2012), and Wagh and Kolhe (2012).
2.3 Spoken Language Processing
Our work addresses the processing of spoken lan-
guage, which differs from the processing of writ-
ten language in that spoken language is more of-
ten elliptical and grammatically incorrect. Previ-
ous work in this area has attempted to address this
issue at different levels of processing. Issar and
Ward (1993) presented the CMU speech process-
ing system that supports recognition for grammat-
ically ill-formed sentences. Lavie (1996) presents
GLR*, a grammar-based parser for spoken lan-
guage, which ignores unparseable words and sen-
tence parts and instead looks for the maximal sub-
set of an input sentence that is covered by the
grammar.
Other researchers in this area have designed
grammar-based approaches for incremental spo-
ken language processing: Brick and Scheutz
(2007) present RISE, the robotic incremental se-
mantic engine. RISE is able to process syntactic
and semantic information incrementally and to in-
tegrate this information with perceptual and lin-
guistic information. Kruijff et al. (2007) present
an approach for incremental processing of situ-
ated dialogue in human-robot interaction, which
maintains parallel interpretations of the current di-
alogue that are pruned by making use of the con-
text information. Schlangen and Skantze (2009)
describe a ?general, abstract model of incremental
dialogue processing?, where their goal is to pro-
vide principles for designing new systems for in-
cremental speech processing.
244
3 Approach
Our goal in this paper is to transform spoken utter-
ances which cannot be parsed by our grammar into
grammar-valid sentences. During this process, we
have to make sure that the changes to the input
sentence do not change its meaning. In this sec-
tion, we show how we implement ellipsis detec-
tion and semantic similarity computation in order
to achieve this goal. We present our ellipsis detec-
tion algorithm in Section 3.1. Section 3.2 explains
our implementation of WordNet-based word simi-
larity computation.
3.1 Ellipsis Detection Algorithm
We use the OpenCCG parser (White, 2006), which
is based on Combinatory Categorial Grammar
(Kruijff and Baldridge, 2004; Steedman, 2000), to
parse the output of our speech recognition system.
We use the properties of CCGs to solve a prob-
lem that often occurs during parsing of spoken lan-
guage. In our evaluation (Section 4) we use a test
set (Section 4.1) of spoken sentences that was col-
lected during one of our human-robot interaction
studies (Foster et al., 2012) and the CCG (Sec-
tion 4.2) that was used in the same study. In the
test set, we found that speakers leave out words.
For example, one speaker said I want water to
order a drink. The grammar used in the experi-
ment assumed that every noun is specified by an
article; the grammar was only able to parse the
sentence I want a water . Just to remind you,
of course this particular example could have been
solved by rewriting the grammar, but at the time
of running the experiment it was not possible to
us to change the grammar. Furthermore, we argue
that there will always be examples of the above
described situation where experiment participants
use grammatical structures or words that cannot be
processed by the used grammar. Thus, we present
an algorithm that automatically finds ellipses in
sentences and suggests words from the grammar
that could be used to fill the ellipses.
To illustrate our approach, we will use the ex-
ample sentence give me a water . Example (1)
shows the words of the example sentence with
their assigned categories from the used CCG, and
Example (2) shows the parsed sentence. In the ex-
amples, we use the category symbols s for sen-
tence, n for noun, and np for noun phrase. In Ex-
ample (2) the symbol> denotes the used CCG for-
ward application combinator.
(1) CCG lexicon entries
a. give := s / np / np
b. me := np
c. a := np / n
d. water := n
(2) Full parse of an example sentence
give me a water
s/np/np np np/n n
>
np
>
s/np
>
s
The algorithm consists of two parts: (i) search
for ellipses in the sentence and selection of the
most relevant ellipsis, and (ii) computation of the
category for the word that will fill the chosen el-
lipsis.
(i) Ellipsis Search
In order to find the ellipsis in the sentence, our al-
gorithm assumes that to the left and to the right of
the ellipsis, the spoken utterance consists of sen-
tence parts that the grammar can parse. In our ex-
ample, these sentence parts would be I want to the
left of the ellipsis and water to the right. In order
to automatically find the sentence part to the right,
we use the following algorithm, which we present
in Listing 1 in a Java-style pseudo code: The al-
gorithm uses the method tokenize() to split up the
string that contains the utterance into an array of
single words. It then iterates through the array and
builds a new sentence of the words in the array,
using the method buildString(). This sentence is
then processed by the parser. If the parser finds
a parse for the sentence, the algorithm returns the
result. Otherwise it cuts off the first word of the
sentence and repeats the procedure. This way, the
algorithm searches for a parseable sentence part
for the given utterance from the left to the right un-
til it either finds the right-most parseable sentence
part or there are no more words to parse. In order
to find the left-most parseable sentence part, we
implemented a method findParseReverse(), which
parses sentence parts from right to left.
One has to consider that our method for ellip-
sis detection can falsely identify ellipses in cer-
tain sentence constellations. For example, if the
word like in the sentence I would like a water
is left out and given to our ellipsis detection al-
gorithm, it would falsely find an ellipsis between
I and would , and another ellipsis between would
245
Listing 1: Ellipsis detection algorithm.
R e s u l t f i n d P a r s e ( S t r i n g u t t e r a n c e ) {
words [ ] = t o k e n i z e ( u t t e r a n c e ) ;
f o r ( i = 0 ; i < words . l e n g t h ; i ++) {
S t r i n g s e n t e n c e = b u i l d S t r i n g ( words [
i ] , words . l e n g t h ) ;
R e s u l t p a r s e = p a r s e ( s e n t e n c e ) ;
i f ( p a r s e != nu l l ) {
re turn p a r s e ;
}
}
re turn nu l l ;
}
and a. The reason for the detection of the first
ellipsis is that the categories for I and would can-
not be combined together. would and like have to
be parsed first to an auxiliary verb-verb construct.
This construct can then be combined with the pro-
noun I. To overcome this problem, we first com-
pute the category for each found ellipsis. Then we
find a word for the ellipsis with the simplest cate-
gory, which is either an atomic category or a func-
tional category with fewer functors than the other
found categories, add it to the original input sen-
tence, and parse the output sentence. If the output
sentence cannot be parsed, we repeat the step with
the next found ellipsis.
(ii) Ellipsis Category Computation
After the algorithm has determined the ellipsis in
an utterance, it computes the category of the word
that will fill the ellipsis. The goal here is to find a
category which the grammar needs to combine the
sentence parts to the left and right of the ellipsis.
For example, the left part of our example utterance
I want has the category s/np and the right part wa-
ter has the category n. Hence, the category for the
missing word needs to be np/n, because it takes
the category of the right sentence part as argument
and produces the category np, which is the argu-
ment of the category of the left sentence part.
Figure 2 shows the processing sequence dia-
gram of our algorithm for computing the category
of an ellipsis. In the diagram, left and right stand
for the categories of the sentence parts that are to
the left and right of the ellipsis. The predicates
symbolise functions: isEmpty(category) checks
if a category is empty, atom(category) checks
if a category is atomic, compl(category) checks
if a category is complex and has a slash oper-
ator that faces toward the ellipsis. The predi-
cate arg(category) returns the argument of a com-
s / right
true
isEmpty(left) isEmpty(right)false
s \ left
true
atom(left)atom(right)
s / right \ left
compl(left)compl(right)
false
false
true
true
atom(left)compl(right)
left \ arg(right)
false
true
atom(right)compl(left)
arg(left) / right
true
false
Figure 2: Processing sequence of the category
computation algorithm.
plex category. Rectangular boxes symbolise steps
where the algorithm builds the result category for
the missing word. The algorithm determines the
category with the following rules:
? if the categories to the left or to the right of
the ellipsis are empty, the ellipsis category is
s/right or s\left, respectively,
? if the categories to the right and to the left of
the ellipsis are atomic, the ellipsis category is
s/right\left,
? if the categories to the right and to the left
of the ellipsis are both complex and have a
slash operator facing toward the ellipsis, the
ellipsis category is s/right\left,
? if the category to the left of the ellipsis is
atomic and to the right of the ellipsis is com-
plex, the ellipsis category is left\arg(right),
? if the category to the right of the ellipsis is
atomic and to the left of the ellipsis is com-
plex, the ellipsis category is arg(left)\right.
After the computation of the ellipsis category,
we use the OpenCCG grammar to select words to
fill the ellipsis. This step is straightforward, be-
cause the grammar maintains a separate word list
with corresponding categories. Here, we benefit
from the usage of a categorial grammar, as the
usage of a top-down grammar formalism would
have meant a more complicated computation in
this step.
3.2 WordNet-based Word Substitution
Spoken language is versatile and there are many
ways to express one?s intentions by using differ-
246
ent expressions. Thus, in grammar-based spo-
ken language processing it often happens that sen-
tences cannot be parsed because of words that
are not in the grammar. To overcome this prob-
lem, we use WordNet (Miller, 1995) to find se-
mantically equivalent replacements for unknown
words. WordNet arranges words in sets of syn-
onyms called synsets which are connected to other
synsets by a variety of relations, which differ for
each word category. The most relevant relations
for our work are: for nouns and verbs hyperonyms
(e.g., drink is a hyperonym of juice) and hyponyms
(e.g., juice is a hyponym of drink), and for adjec-
tives we use the similar to relation.
Our implementation of word substitution exe-
cutes two steps if a word is unknown to the gram-
mar: (1) look-up of synonyms for the unknown
words. The unknown word can be substituted with
a semantically similar word directly, if the synset
of the unknown word contains a word, which is
known to the grammar. (2) Computation of simi-
lar words in the WordNet hyperonym/hyponym hi-
erarchy. If the synset of the unknown word does
not contain a substitution, we compute if one of
the hyperonyms of the unknown word has a hy-
ponym which is known to the grammar. Here, one
has to be careful not to move too far away from
the meaning of the unknown word in the Word-
Net tree, in order not to change the meaning of
the originally spoken sentence. Also, the compu-
tation of the similar word should not take too much
time. Therefore, in our implementation, we only
substitute an unknown word with one of its hyper-
onym/hyponym neighbours when the substitution
candidate is a direct hyponym of the direct hyper-
onym of the unknown word.
4 Evaluation
The goal of this evaluation is to measure how
many spoken sentences that our grammar cannot
parse can be processed after the transformation of
the sentences with our methods. In Section 4.1
we present the test set of spoken sentences that we
used in the evaluation. In Section 4.2 we give de-
tails of the used grammar. As mentioned above,
both, the test set as well as the grammar, were
taken from the human-robot interaction study re-
ported by Foster et al. (2012). Section 4.3 sum-
marises the details of the evaluation procedure. Fi-
nally, we present the evaluation results in Section
4.4 and discuss their meaning in Section 4.5.
4.1 Test Set
As test set for the evaluation, we used the spo-
ken utterances from the participants of the human-
robot interaction experiment reported by Foster et
al. (2012). In the experiment, 31 participants were
asked to order a drink from the robot bartender
shown in Figure 1. The experiment consisted of
three parts: in the first part, participants had to or-
der drinks on their own, in the second and third
part, participants were accompanied by a confed-
erate in order to have a multi-party interaction with
the robot. The spoken utterances in the test set
were annotated by hand from video recordings of
the 93 drink order sequences. Please refer to (Fos-
ter et al., 2012) for a detailed description of the
experiment.
Table 1 shows an overview of the test set. In
total, it contains 211 unique utterances; the exper-
iment participants spoke 531 sentences of which
some sentences were said repeatedly. We di-
vided the test set into the following speech acts
(Searle, 1965): Ordering (?I would like a juice
please.?), Question (?What do you have??), Greet-
ing (?Hello there.?), Polite expression (?Thank
you.?), Confirmation (?Yes.?), Other (?I am
thirsty.?).
4.2 Grammar
The grammar that we used in this evaluation was
also used in the robot bartender experiment (Fos-
ter et al., 2012). This grammar is limited in its
scope, because the domain of the experiment?the
robot hands out drinks to customers?was limited
as well. Overall, the lexicon of the grammar con-
tains 92 words, which are divided into the follow-
ing part of speech classes: 42 verbs, 11 nouns, 10
greetings, 6 pronouns, 5 prepositions, 4 adverbs, 4
determiners, 3 quantifiers, 3 confirmations, 2 rela-
tive pronouns, 1 conjunction, 1 polite expression.
4.3 Procedure
For the evaluation, we implemented a programme
that takes our test set and automatically parses
each sentence with four different settings, which
are also presented in Table 1: (1) parsing with
the grammar only, (2) application of ellipsis de-
tection and word filling before parsing, (3) appli-
cation of WordNet similarity substitution before
parsing, (4) application of a combination of both
methods before parsing. Please note that for al-
ternative (4) the sequence in which the methods
247
Speech act No. utt (1) CCG (2) Ell. det. (3) WordNet (4) Ell. + WordNet
Ordering 143 34 16 - 1
Question 19 1 - - -
Greeting 18 4 1 - -
Polite expression 14 1 - - -
Confirmation 5 4 - - -
Other 12 - - - -
Total 211 44 17 - 1
Table 1: Overview for test set and evaluation. Column No. utt contains the number of test utterances
per speech act. Column (1) CCG shows the number of utterances that were directly parsed by the
grammar. Columns (2) Ell. det., (3) WordNet, and (4) Ell. + WordNet show how many utterances were
additionally parsed using the ellipsis detection , WordNet substitution, and combination of both modules.
are applied to a given sentence can make a differ-
ence to the parsing result. In this evaluation, we
used both possible sequences: first ellipsis detec-
tion followed by WordNet substitution method or
vice versa.
4.4 Results
Table 1 shows the result of the evaluation proce-
dure. The grammar parses 44 sentences of the
211 test set sentences correctly. By using the el-
lipsis detection algorithm, 17 additional sentences
are parsed. The usage of the WordNet substitution
algorithm yields no additionally parsed sentences.
The combination of both methods (in this case,
first ellipsis detection, then WordNet substitution)
leads to the correct parse of one additional sen-
tence. None of the transformed sentences changed
its meaning when compared to the original sen-
tence.
4.5 Discussion
The evaluation results show that the application
of our ellipsis detection algorithm leads to an in-
crease of successfully parsed sentences of 38.64%.
In the class of ordering sentences, which was the
most relevant for the human-robot interaction ex-
periment from which we used the evaluation test
set, the number of successfully parsed sentences
increases by 47.06%. Compared to this, the us-
age of WordNet substitution alone does not lead to
an increase in parseable sentences. The one case
in which the combination of ellipsis detection and
WordNet substitution transformed an unparseable
sentence into a grammatically valid sentence is in-
teresting: here, the experiment participant said ?I
need Coke.? to order a drink from the robot. This
sentence contained the word ?need?, which was
not in the grammar. WordNet has the synonym
?want? in the synset for the word ?need?. How-
ever, the sentence ?I want Coke.? was also not
parseable, because the grammar expected an arti-
cle in front of every noun. The ellipsis detection
algorithm was able to find the missing article in the
sentence and filled it with an article ?a? from the
grammar, leading to a parseable sentence ?I want
a Coke.?.
Although we see an increase in parsed sen-
tences, 150 sentences of the test set were not trans-
formed by our approach. Therefore, we made an
analysis for the remaining utterances to find the
main causes for this weak performance. We found
that the following reasons cause problems for the
grammar (with number of cases in brackets behind
each reason):
? Word missing in grammar (81). The partic-
ipant used a word that was not in the gram-
mar. For example, users ordered drinks by
saying ?One water, please.? , but the gram-
mar did not contain ?one? as an article. This
result shows that the WordNet similarity sub-
stitution has the potential to lead to a large
increase in parseable sentences. However as
mentioned above, there is a risk of changing
the meaning of a sentence too much when al-
lowing the replacement of words which are
only vaguely similar to the unknown word.
? Sentence structure (25). Some participants
said sentences that were either grammatically
incorrect or had a sentence structure that was
not encoded in the grammar. For example
one participant tried to order a juice by saying
?Juice for me.?. Additionally, some partici-
pants asked questions (?Do you have coke??).
248
For the latter, please note that it was not part
of the HRI experiment, from which we use
the test set, that the experiment participants
should be allowed to ask questions to the
robot.
? Unnecessary fill words (22). Some experi-
ment participants used unnecessary fill words
that did not add meaning to the sentence, for
example one participant said ?Oh come on, I
only need water? to order a drink.
? Sentence not related to domain (22). Some
participants said sentences that were contrary
to the given experiment instructions. For ex-
ample, some participants asked questions to
the robot (?How old are you??) and to the ex-
perimenter (?Do I need to focus on the cam-
era??), or complained about the robot?s per-
formance (?You are not quite responsive right
now.?). Clearly, these sentences were out of
the scope of the grammar.
5 Conclusion
We presented two methods for transforming spo-
ken language into grammatically correct sen-
tences. The first of these two approaches is an
ellipsis detection, which automatically detects el-
lipses in sentences and looks up words in a gram-
mar that can fill the ellipsis. Our ellipsis de-
tection algorithm is based on the properties of
the combinatory categorial grammar, which as-
signs categories to each word in the grammar
and thus enables the algorithm to find suitable fill
words by calculating the category of the ellipsis.
The second approach for sentence transformation
was a WordNet-based word similarity computa-
tion and substitution. Here, we used the synsets of
WordNet to substitute words that are unknown to a
given grammar with synonyms for these words. In
an evaluation we showed that the usage of ellip-
sis detection leads to an increase of successfully
parsed sentences of up to 47.06% for some speech
acts. The usage of the WordNet similarity substi-
tution does not increase the number of parsed sen-
tences, although our analysis of the test set shows
that unknown words are the most common reason
that sentences cannot be parsed.
Our approach was specifically implemented to
help circumventing problems during development
and usage of grammars for spoken language pro-
cessing in human-robot interaction experiments,
and the example grammar was a very restricted
one. However, we believe that our method can
also be helpful with more extensive grammars, and
for developers of dialogue systems in other ar-
eas, such as telephone-based information systems
or offline versions of automatic smartphone assis-
tants like Apple?s Siri.
In the future, we will refine our methodology.
In particular, the WordNet similarity substitution
is too rigid in its current form. Here, we plan
to loosen some of the constraints that we ap-
plied to our algorithm. We will systematically test
how far away from a word one can look for suit-
able substitutes in the WordNet hierarchy, with-
out losing the meaning of a sentence. Further-
more, we plan to add a dialogue history to our
approach, which will provide an additional source
of information?besides the grammar?to the el-
lipsis detection method. Finally, we plan to work
with stop word lists to filter unnecessary fill words
from the input sentences, since these proved also
to be a reason for sentences to be unparseable.
Acknowledgements
This research was supported by the European
Commission through the project JAMES (FP7-
270435-STREP).
References
Timothy Brick and Matthias Scheutz. 2007. Incre-
mental natural language processing for hri. In Pro-
ceedings of the ACM/IEEE International Confer-
ence on Human-Robot Interaction, pages 263?270.
ACM New York, NY, USA.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Songmei Cai, Zhao Lu, and Junzhong Gu. 2010. An
effective measure of semantic similarity. In Ad-
vances in Wireless Networks and Information Sys-
tems, pages 9?17. Springer.
Markus Egg and Katrin Erk. 2001. A compositional
account of vp ellipsis. Technology, 3:5.
Mary Ellen Foster, Andre Gaschler, Manuel Giuliani,
Amy Isard, Maria Pateraki, and Ronald P. A. Pet-
rick. 2012. Two people walk into a bar: Dynamic
multi-party social interaction with a robot agent. In
Proceedings of the 14th ACM International Confer-
ence on Multimodal Interaction (ICMI 2012), Octo-
ber.
249
Manuel Giuliani, Ronald P.A. Petrick, Mary Ellen Fos-
ter, Andre Gaschler, Amy Isard, Maria Pateraki, and
Markos Sigalas. 2013. Comparing task-based and
socially intelligent behaviour in a robot bartender. In
Proceedings of the 15th International Conference on
Multimodal Interfaces (ICMI 2013), Sydney, Aus-
tralia, December.
Kais Haddar and A Ben Hamadou. 1998. An ellip-
sis detection method based on a clause parser for
the arabic language. In Proceedings of the Interna-
tional Florida Artificial Intelligence Research Soci-
ety FLAIRS98, pages 270?274.
Daniel Hardt. 1997. An empirical approach to vp el-
lipsis. Computational Linguistics, 23(4):525?541.
Sunil Issar and Wayne Ward. 1993. Cmu?s robust spo-
ken language understanding system. In Proceedings
of the Third European Conference on Speech Com-
munication and Technology (Eurospeech 1993).
Simon Keizer, Mary Ellen Foster, Oliver Lemon, An-
dre Gaschler, and Manuel Giuliani. 2013. Training
and evaluation of an MDP model for social multi-
user human-robot interaction. In Proceedings of the
SIGDIAL 2013 Conference, pages 223?232, Metz,
France, August.
Geert-Jan M. Kruijff and Jason Baldridge. 2004. Gen-
eralizing dimensionality in combinatory categorial
grammar. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), Geneva, Switzerland, August.
Geert-Jan M. Kruijff, Pierre Lison, Trevor Benjamin,
Henrik Jacobsson, and Nick Hawes. 2007. In-
cremental, multi-level processing for comprehend-
ing situated dialogue in human-robot interaction. In
Luis Seabra Lopes, Tony Belpaeme, and Stephen J.
Cowley, editors, Symposium on Language and
Robots (LangRo 2007), Aveiro, Portugal, December.
Shalom Lappin. 1996. The interpretation of ellipsis.
The Handbook of Contemporary Semantic Theory,
397:399.
Alon Lavie. 1996. GLR*: A Robust Grammar-
Focused Parser for Spontaneously Spoken Lan-
guage. Ph.D. thesis, Carnegie Mellon University.
Hongzhe Liu, Hong Bao, and De Xu. 2012. Concept
vector for semantic similarity and relatedness based
on wordnet structure. Journal of Systems and Soft-
ware, 85(2):370?381.
George A. Miller. 1995. Wordnet: A lexical
database for english. Communications of the ACM,
38(11):39?41.
Leif Arda Nielsen. 2004a. Verb phrase ellipsis detec-
tion using automatically parsed text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics, page 1093. Association for Com-
putational Linguistics.
Leif Arda Nielsen. 2004b. Verb phrase ellipsis detec-
tion using machine learning techniques. Amsterdam
Studies in the Theory and History of Linguistic Sci-
ence, page 317.
Peng Qin, Zhao Lu, Yu Yan, and Fang Wu. 2009. A
new measure of word semantic similarity based on
wordnet hierarchy and dag theory. In Proceedings
of the International Conference on Web Information
Systems and Mining2009 (WISM 2009), pages 181?
185. IEEE.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL-09).
John R. Searle. 1965. What is a speech act? In
Robert J. Stainton, editor, Perspectives in the Phi-
losophy of Language: A Concise Anthology, pages
253?268.
Lindsey Smith and Sarah Rauchas. 2006. A machine-
learning approach for the treatment of vp ellipsis.
In Proceedings of the Seventeenth Annual Sympo-
sium of the Pattern Recognition Association of South
Africa, November.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Kishor Wagh and Satish Kolhe. 2012. A new approach
for measuring semantic similarity in ontology and
its application in information retrieval. In Multi-
disciplinary Trends in Artificial Intelligence, pages
122?132. Springer.
Michael White. 2006. CCG chart realization from dis-
junctive inputs. In Proceedings of the Fourth Inter-
national Natural Language Generation Conference,
pages 12?19, Sydney, Australia, July. Association
for Computational Linguistics.
250
