Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623?633,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Towards a model of formal and informal address in English
Manaal Faruqui
Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
manaalfar@gmail.com
Sebastian Pad?
Institute of Computational Linguistics
Heidelberg University
Heidelberg, Germany
pado@cl.uni-heidelberg.de
Abstract
Informal and formal (?T/V?) address in dia-
logue is not distinguished overtly in mod-
ern English, e.g. by pronoun choice like
in many other languages such as French
(?tu?/?vous?). Our study investigates the
status of the T/V distinction in English liter-
ary texts. Our main findings are: (a) human
raters can label monolingual English utter-
ances as T or V fairly well, given sufficient
context; (b), a bilingual corpus can be ex-
ploited to induce a supervised classifier for
T/V without human annotation. It assigns
T/V at sentence level with up to 68% accu-
racy, relying mainly on lexical features; (c),
there is a marked asymmetry between lex-
ical features for formal speech (which are
conventionalized and therefore general) and
informal speech (which are text-specific).
1 Introduction
In many Indo-European languages, there are two
pronouns corresponding to the English you. This
distinction is generally referred to as the T/V di-
chotomy, from the Latin pronouns tu (informal, T)
and vos (formal, V) (Brown and Gilman, 1960).
The V form (such as Sie in German and Vous in
French) can express neutrality or polite distance
and is used to address social superiors. The T
form (German du, French tu) is employed towards
friends or addressees of lower social standing, and
implies solidarity or lack of formality.
English used to have a T/V distinction until the
18th century, using you as V pronoun and thou
for T. However, in contemporary English, you has
taken over both uses, and the T/V distinction is not
marked anymore. In NLP, this makes generation
in English and translation into English easy. Con-
versely, many NLP tasks suffer from the lack of
information about formality, e.g. the extraction of
social relationships or, notably, machine transla-
tion from English into languages with a T/V dis-
tinction which involves a pronoun choice.
In this paper, we investigate the possibility to
recover the T/V distinction for (monolingual) sen-
tences of 19th and 20th-century English such as:
(1) Can I help you, Sir? (V)
(2) You are my best friend! (T)
After describing the creation of an English corpus
of T/V labels via annotation projection (Section 3),
we present an annotation study (Section 4) which
establishes that taggers can indeed assign T/V la-
bels to monolingual English utterances in context
fairly reliably. Section 5 investigates how T/V is
expressed in English texts by experimenting with
different types of features, including words, seman-
tic classes, and expressions based on Politeness
Theory. We find word features to be most reliable,
obtaining an accuracy of close to 70%.
2 Related Work
There is a large body of work on the T/V distinc-
tion in (socio-)linguistics and translation studies,
covering in particular the conditions governing
T/V usage in different languages (Kretzenbacher
et al 2006; Sch?pbach et al 2006) and the diffi-
culties in translation (Ardila, 2003; K?nzli, 2010).
However, many observations from this literature
are difficult to operationalize. Brown and Levin-
son (1987) propose a general theory of politeness
which makes many detailed predictions. They as-
sume that the pragmatic goal of being polite gives
rise to general communication strategies, such as
avoiding to lose face (cf. Section 5.2).
In computational linguistics, it is a common
observation that for almost every language pair,
there are distinctions that are expressed overtly
623
Please permit me to ask 
you a question.
Darf ich Sie etwas fragen?
Step 2: copy T/V class 
label to English sentence
Step 1: German pronoun 
provides overt T/V label
V
V
projection
Figure 1: T/V label induction for English sentences in
a parallel corpus with annotation projection
in one language, but remain covert in the other.
Examples include morphology (Fraser, 2009) and
tense (Schiehlen, 1998). A technique that is often
applied in such cases is annotation projection, the
use of parallel corpora to copy information from a
language where it is overtly realized to one where
it is not (Yarowsky and Ngai, 2001; Hwa et al
2005; Bentivogli and Pianta, 2005).
The phenomenon of formal and informal ad-
dress has been considered in the contexts of transla-
tion into (Hobbs and Kameyama, 1990; Kanayama,
2003) and generation in Japanese (Bateman, 1988).
Li and Yarowsky (2008) learn pairs of formal and
informal constructions in Chinese with a para-
phrase mining strategy. Other relevant recent stud-
ies consider the extraction of social networks from
corpora (Elson et al 2010). A related study is
(Bramsen et al 2011) which considers another
sociolinguistic distinction, classifying utterances
as ?upspeak? and ?downspeak? based on the social
relationship between speaker and addressee.
This paper extends a previous pilot study
(Faruqui and Pad?, 2011). It presents more an-
notation, investigates a larger and better motivated
feature set, and discusses the findings in detail.
3 A Parallel Corpus of Literary Texts
This section discusses the construction of T/V gold
standard labels for English sentences. We obtain
these labels from a parallel English?German cor-
pus using the technique of annotation projection
(Yarowsky and Ngai, 2001) sketched in Figure 1:
We first identify the T/V status of German pro-
nouns, then copy this T/V information onto the
corresponding English sentence.
3.1 Data Selection and Preparation
Annotation projection requires a parallel corpus.
We found commonly used parallel corpora like EU-
ROPARL (Koehn, 2005) or the JRC Acquis corpus
(Steinberger et al 2006) to be unsuitable for our
study since they either contain almost no direct
address at all or, if they do, just formal address (V).
Fortunately, for many literary texts from the 19th
and early 20th century, copyright has expired, and
they are freely available in several languages.
We identified 110 stories and novels among the
texts provided by Project Gutenberg (English) and
Project Gutenberg-DE (German)1 that were avail-
able in both languages, with a total of 0.5M sen-
tences per language. Examples are Dickens? David
Copperfield or Tolstoy?s Anna Karenina. We ex-
cluded plays and poems, as well as 19th-century
adventure novels by Sir Walter Scott and James F.
Cooper which use anachronistic English for stylis-
tic reasons, including words that previously (until
the 16th century) indicated T (?thee?, ?didst?).
We cleaned the English and German novels man-
ually by deleting the tables of contents, prologues,
epilogues, as well as chapter numbers and titles
occurring at the beginning of each chapter to ob-
tain properly parallel texts. The files were then
formatted to contain one sentence per line using
the sentence splitter and tokenizer provided with
EUROPARL (Koehn, 2005). Blank lines were
inserted to preserve paragraph boundaries. All
novels were lemmatized and POS-tagged using
TreeTagger (Schmid, 1994).2 Finally, they were
sentence-aligned using Gargantuan (Braune and
Fraser, 2010), an aligner that supports one-to-many
alignments, and word-aligned in both directions
using Giza++ (Och and Ney, 2003).
3.2 T/V Gold Labels for English Utterances
As Figure 1 shows, the automatic construction of
T/V labels for English involves two steps.
Step 1: Labeling German Pronouns as T/V.
German has three relevant personal pronouns for
the T/V distinction: du (T), sie (V), and ihr (T/V).
However, various ambiguities makes their interpre-
tation non-straightforward.
The pronoun ihr can both be used for plural T
address or for a somewhat archaic singular or plu-
ral V address. In principle, these usages should
be distinguished by capitalization (V pronouns
are generally capitalized in German), but many
T instances in our corpora informal use are nev-
ertheless capitalized. Additional, ihr can be the
1http://www.gutenberg.org, http://gutenberg.spiegel.de/
2It must be expected that the tagger degrades on this
dataset; however we did not quantify this effect.
624
dative form of the 3rd person feminine pronoun sie
(she/her). These instances are neutral with respect
to T/V but were misanalysed by TreeTagger as in-
stances of the T/V lemma ihr. Since TreeTagger
does not provide person information, and we did
not want to use a full parser, we decided to omit
ihr/Ihr from consideration.3
Of the two remaining pronouns (du and sie), du
expresses (singular) T. A minor problem is pre-
sented by novels set in France, where du is used as
an nobiliary particle. These instances can be recog-
nised reliably since the names before and after du
are generally unknown to the German tagger. Thus
we do not interpret du as T if the word preceding
or succeeding it has ?unknown? as its lemma.
The V pronoun, sie, doubles as the pronoun for
third person (she/they) when not capitalized. We
therefore interpret only capitalized instances of Sie
as V. Furthermore, we ignore utterance-initial po-
sitions, where all words are capitalized. This is
defined as tokens directly after a sentence bound-
ary (POS $.) or after a bracket (POS $().
These rules concentrate on precision rather than
recall. They leave many instances of German sec-
ond person pronouns unlabeled; however, this is
not a problem since we do not currently aim at
obtaining complete coverage on the English side
of our parallel corpus. From the 0.5M German sen-
tences, about 14% of the sentences were labeled
as T or V (37K for V and 28K for T). In a random
sample of roughly 300 German sentences which
we analysed, we did not find any errors. This puts
the precision of our heuristics at above 99%.
Step 2: Annotation Projection. We now copy
the information over onto the English side. We
originally intended to transfer T/V labels between
German and English word-aligned pronouns. How-
ever, we pronouns are not necessarily translated
into pronouns; additionally, we found word align-
ment accuracy for pronouns to be far from perfect,
due to the variability in function word translation.
For these reason, we decided to look at T/V labels
at the level of complete sentences, ignoring word
alignment. This is generally unproblematic ? ad-
dress is almost always consistent within sentences:
of the 65K German sentences with T or V labels,
only 269 (< 0.5%) contain both T and V. Our pro-
jection on the English side results in 25K V and
3Instances of ihr as possessive pronoun occurred as well,
but could be filtered out on the basis of the POS tag.
Comparison No context In context
A1 vs. A2 75% (.49) 79% (.58)
A1 vs. GS 60% (.20) 70% (.40)
A2 vs. GS 65% (.30) 76% (.52)
(A1 ? A2) vs. GS 67% (.34) 79% (.58)
Table 1: Manual annotation for T/V on a 200-sentence
sample. Comparison among human annotators (A1 and
A2) and to projected gold standard (GS). All cells show
raw agreement and Cohen?s ? (in parentheses).
18K T sentences4, of which 255 (0.6%) are labeled
as both T and V. We exclude these sentences.
Note that this strategy relies on the direct cor-
respondence assumption (Hwa et al 2005), that
is, it assumes that the T/V status of an utterance is
not changed in translation. We believe that this is
a reasonable assumption, given that T/V is deter-
mined by the social relation between interlocutors;
but see Section 4 for discussion.
3.3 Data Splitting
Finally, we divided our English data into train-
ing, development and test sets with 74 novels
(26K sentences), 19 novels (9K sentences) and
13 novels (8K sentences), respectively. The cor-
pus is available for download at http://www.
nlpado.de/~sebastian/data.shtml.
4 Human Annotation of T/V for English
This section investigates how well the T/V distinc-
tion can be made in English by human raters, and
on the basis of what information. Two annotators
with near native-speaker competence in English
were asked to label 200 random sentences from
the training set as T or V. Sentences were first pre-
sented in isolation (?no context?). Subsequently,
they were presented with three sentences pre- and
post-context each (?in context?).
Table 1 shows the results of the annotation
study. The first line compares the annotations
of the two annotators against each other (inter-
annotator agreement). The next two lines compare
the taggers? annotations against the gold standard
labels projected from German (GS). The last line
compares the annotator-assigned labels to the GS
for the instances on which the annotators agree.
For all cases, we report raw accuracy and Co-
hen?s ? (1960), i.e. chance-corrected agreement.
4Our sentence aligner supports one-to-many alignments
and often aligns single German to multiple English sentences.
625
We first observe that the T/V distinction is con-
siderably more difficult to make for individual
sentences (no context) than when the discourse is
available. In context, inter-annotator agreement in-
creases from 75% to 79%, and agreement with the
gold standard rises by 10%. It is notable that the
two annotators agree worse with one another than
with the gold standard (see below for discussion).
On those instances where they agree, Cohen?s ?
reaches 0.58 in context, which is interpreted as
approaching good agreement (Fleiss, 1981). Al-
though far from perfect, this inter-annotator agree-
ment is comparable to results for the annotation
of fine-grained word sense or sentiment (Navigli,
2009; Bermingham and Smeaton, 2009).
An analysis of disagreements showed that many
sentences can be uttered in both T and V contexts
and cannot be labeled without context:
(3) ?And perhaps sometime you may see her.?
This case (gold label: V) is disambiguated by the
previous sentence which indicates a hierarchical
social relation between speaker and addressee:
(4) ?And she is a sort of relation of your lord-
ship?s,? said Dawson. . . .
Still, even a three-sentence window is often not
sufficient, since the surrounding sentences may be
just as uninformative. In these cases, more global
information about the situation is necessary. Even
with perfect information, however, judgments can
sometimes deviate, as there are considerable ?grey
areas? in T/V usage (Kretzenbacher et al 2006).
In addition, social rules like T/V usage vary
in time and between countries (Sch?pbach et al
2006). This helps to explain why annotators agree
better with one another than with the gold standard:
21st century annotators tend to be unfamiliar with
19th century T/V usage. Consider this example
from a book written in second person perspective:
(5) Finally, you acquaint Caroline with the
fatal result: she begins by consoling you.
?One hundred thousand francs lost! We
shall have to practice the strictest econ-
omy?, you imprudently add.5
Here, the author and translator use V to refer to the
reader, while today?s usage would almost certainly
5H. de Balzac: Petty Troubles of Married Life
be T, as presumed by both annotators. Conver-
sations between lovers or family members form
another example, where T is modern usage, but
the novels tend to use V:
(6) [...] she covered her face with the other
to conceal her tears. ?Corinne!?, said Os-
wald, ?Dear Corinne! My absence has
then rendered you unhappy!?6
In sum, our annotation study establishes that the
T/V distinction, although not realized by different
pronouns in English, can be recovered manually
from text, provided that discourse context is avail-
able. A substantial part of the errors is due to social
changes in T/V usage.
5 Monolingual T/V Modeling
The second part of the paper explores the auto-
matic prediction of the T/V distinction for English
sentences. Given the ability to create an English
training corpus with T/V labels with the annotation
projection methods described in Section 3.2, we
can phrase T/V prediction for English as a standard
supervised learning task. Our experiments have
a twin motivation: (a), on the NLP side, we are
mainly interested in obtaining a robust classifier
to assign the labels T and V to English sentences;
(b), on the sociolinguistic side, we are interested in
investigating through which features the categories
T and V are expressed in English.
5.1 Classification Framework
We phrase T/V labeling as a binary classification
task at the sentence level, performing the classifica-
tion with L2-regularized logistic regression using
the LibLINEAR library (Fan et al 2008). Logis-
tic regression defines the probability that a binary
response variable y takes some value as a logit-
transformed linear combination of the features fi,
each of which is assigned a coefficient ?i.
p(y = 1) =
1
1 + e?z
with z =
?
i
?ifi (7)
Regularization incorporates the size of the coef-
ficient vector ? into the objective function, sub-
tracting it from the likelihood of the data given the
model. This allows the user to trade faithfulness
to the data against generalization.7
6A.L.G. de Sta?l: Corinne
7We use LIBLINEAR?s default parameters and set the
cost (regularization) parameter to 0.01.
626
p(C|V )
p(C|T ) Words
4.59 Mister, sir, Monsieur, sirrah, . . .
2.36 Mlle., Mr., M., Herr, Dr., . . .
1.60 Gentlemen, patients, rascals, . . .
Table 2: 3 of the 400 clustering-based semantic classes
(classes most indicative for V)
5.2 Feature Types
We experiment with three features types that are
candidates to express the T/V English distinction.
Word Features. The intuition to use word fea-
tures draws on the parallel between T/V and infor-
mation retrieval tasks like document classification:
some words are presumably correlated with formal
address (like titles), while others should indicate
informal address (like first names). In a prelimi-
nary experiment, we noticed that in the absence of
further constraints, many of the most indicative fea-
tures are names of persons from particular novels
which are systematically addressed formally (like
Phileas Fogg from J. Vernes? Around the world in
eighty days) or informally (like Mowgli, Baloo,
and Bagheera from R. Kipling?s Jungle Book).
These features clearly do not generalize to new
books. We therefore added a constraint to remove
all features which did not occur in at least three
novels. To reduce the number of word features to a
reasonable order of magnitude, we also performed
a ?2-based feature selection (Manning et al 2008)
on the training set. Preliminary experiments es-
tablished that selecting the top 800 word features
yielded a model with good generalization.
Semantic Class Features. Our second feature
type is semantic class features. These can be seen
as another strategy to counteract the sparseness
at the level of word features. We cluster words
into 400 semantic classes on the basis of distribu-
tional and morphological similarity features which
are extracted from an unlabeled English collec-
tion of Gutenberg novels comprising more than
100M tokens, using the approach by Clark (2003).
These features measure how similar tokens are to
one another in terms of their occurrences in the
document and are useful in Named Entity Recog-
nition (Finkel and Manning, 2009). As features
in the T/V classification of a given sentence, we
simply count for each class the number of tokens
in this class present in the current sentence. For
illustration, Table 2 shows the three classes most
indicative for V, ranked by the ratio of probabilities
for T and V, estimated on the training set.
Politeness Theory Features. The third feature
type is based on the Politeness Theory (Brown
and Levinson, 1987). Brown and Levinson?s pre-
diction is that politeness levels will be detectable
in concrete utterances in a number of ways, e.g.
a higher use of conjunctive or hedges in polite
speech. Formal address (i.e., V as opposed to T) is
one such expression. Politeness Theory therefore
predicts that other politeness indicators should cor-
relate with the T/V classification. This holds in
particular for English, where pronoun choice is
unavailable to indicate politeness.
We constructed 16 features on the basis of Po-
liteness Theory predictions, that is, classes of ex-
pressions indicating either formality or informality.
From a computational perspective, the problem
with Politeness Theory predictions is that they are
only described qualitatively and by example, with-
out detailed lists. For each feature, we manually
identified around 10 words or multi-word relevant
expressions. Table 3 shows these 16 features with
their intended classes and some example expres-
sions. Similar to the semantic class features, the
value of each politeness feature is the sum of the
frequencies of its members in a sentence.
5.3 Context: Size and Type
As our annotation study in Section 4 found, con-
text is crucial for human annotators, and this pre-
sumably carries over to automatic methods human
annotators: if the features for a sentence are com-
puted just on that sentence, we will face extremely
sparse data. We experiment with symmetrical win-
dow contexts, varying the size between n = 0 (just
the target sentence) and n = 10 (target sentence
plus 10 preceding and 10 succeeding sentences).
This kind of simple ?sentence context? makes an
important oversimplification, however. It lumps to-
gether material from different speech turns as well
as from ?narrative? sentences, which may generate
misleading features. For example, narrative sen-
tences may refer to protagonists by their full names
including titles (strong features for V) even when
these protagonists are in T-style conversations:
(8) ?You are the love of my life?, said Sir
Phileas Fogg.8 (T)
8J. Verne: Around the world in 80 days
627
Class Example expressions Class Example expressions
Inclusion (T) let?s, shall we Exclamations (T) hey, yeah
Subjunctive I (T) can, will Subjunctive II (V) could, would
Proximity (T) this, here Distance (V) that, there
Negated question (V) didn?t I, hasn?t it Indirect question (V) would there, is there
Indefinites (V) someone, something Apologizing (V) bother, pardon
Polite adverbs (V) marvellous, superb Optimism (V) I hope, would you
Why + modal (V) why would(n?t) Impersonals (V) necessary, have to
Polite markers (V) please, sorry Hedges (V) in fact, I guess
Table 3: 16 Politeness theory-based features with intended classes and example expressions
Example (8) also demonstrates that narrative mate-
rial and direct speech may even be mixed within
individual sentences.
For these reasons, we introduce an alternative
concept of context, namely direct speech context,
whose purpose is to exclude narrative material. We
compute direct speech context in two steps: (a),
segmentation of sentences into chunks that are
either completely narrative or speech, and (b), la-
beling of chunks with a classifier that distinguishes
these two classes. The segmentation step (a) takes
place with a regular expression that subdivides sen-
tences on every occurrence of quotes (? , ? , ? , ?,
etc.). As training data for the classification step
(b), we manually tagged 1000 chunks from our
training data as either B-DS (begin direct speech),
I-DS (inside direct speech) and O (outside direct
speech, i.e. narrative material).9 We used this
dataset to train the CRF-based sequence tagger
Mallet (McCallum, 2002) using all tokens, includ-
ing punctuation, as features.10 This tagger is used
to classify all chunks in our dataset, resulting in
output like the following example:
(9)
(B-DS) ?I am going to see his Ghost!
(I-DS) It will be his Ghost not him!?
(O) Mr. Lorry quietly chafed the
hands that held his arm.11
Direct speech chunks belonging to the same sen-
tence are subsequently recombined.
We define the direct speech context of size n for
a given sentence as the n preceding and following
direct speech chunks that are labeled B-DS or I-DS
while skipping any chunks labeled O. Note that
this definition of direct speech context still lumps
9The labels are chosen after IOB notation conventions
(Ramshaw and Marcus, 1995).
10We also experimented with rule-based chunk labeling
based on quotes, but found the use of quotes too inconsistent.
11C. Dickens: A tale of two cities.
?
?
?
?
?
?
? ?
?
?
?
0 2 4 6 8 10
61
62
63
64
65
66
67
Context size (n)
Acc
ura
cy (
%)
?
? ?
?
?
? ? ? ? ? ?
Figure 2: Accuracy vs. number of sentences in context
(empty circles: sentence context; solid circles: direct
speech context)
together utterances by different speakers and can
therefore yield misleading features in the case of
asymmetric conversational situations, in addition
to possible direct speech misclassifications.
6 Experimental Evaluation
6.1 Evaluation on the Development Set
We first perform model selection on the develop-
ment set and then validate our results on the test
set (cf. Section 3.3).
Influence of Context. Figure 2 shows the influ-
ence of size and type of context, using only words
as features. Without context, we obtain a perfor-
mance of 61.1% (sentence context) and of 62.9%
(direct speech context). These numbers beat the
random baseline (50.0%) and the frequency base-
line (59.1%). The addition of more context further
improves performance substantially for both con-
text types. The ideal context size is fairly large,
namely 7 sentences and 8 direct speech chunks, re-
628
Model Accuracy
Random Baseline 50.0
Frequency Baseline 59.1
Words 67.0??
SemClass 57.5
PoliteClass 59.6
Words + SemClass 66.6??
Words + PoliteClass 66.4??
Words + PoliteClass + SemClass 66.2??
Raw human IAA (no context) 75.0
Raw human IAA (in context) 79.0
Table 4: T/V classification accuracy on the develop-
ment set (direct speech context, size 8). ??: Significant
difference to frequency baseline (p<0.01)
spectively. This indicates that sparseness is indeed
a major challenge, and context can become large
before the effects mentioned in Section 5.3 counter-
act the positive effect of more data. Direct speech
context outperforms sentence context throughout,
with a maximum accuracy of 67.0% as compared
to 65.2%, even though it shows higher variation,
which we attribute to the less stable nature of the
direct speech chunks and their automatically cre-
ated labels. From now on, we adopt a direct speech
context of size 8 unless specified differently.
Influence of Features. Table 4 shows the results
for different feature types. The best model (word
features only) is highly significantly better than
the frequency baseline (which it beats by 8%) as
determined by a bootstrap resampling test (Noreen,
1989). It gains 17% over the random baseline,
but is still more than 10% below inter-annotator
agreement in context, which is often seen as an
upper bound for automatic models.
Disappointingly, the comparison of the feature
groups yields a null result: We are not able to
improve over the results for just word features with
either the semantic class or the politeness features.
Neither feature type outperforms the frequency
baseline significantly (p>0.05). Combinations of
the different feature types also do worse than just
words. The differences between the best model
(just words) and the combination models are all
not significant (p>0.05). These negative results
warrant further analysis. It follows in Section 6.3.
6.2 Results on the Test Set
Table 5 shows the results of evaluating models
with the best feature set and with different context
sizes on the test set, in order to verify that we did
Model Accuracy ? to dev set
Frequency baseline 59.3 + 0.2
Words (no context) 62.5 - 0.4
Words (context size 6) 67.3 + 1.0
Words (context size 8) 67.5 + 0.5
Words (context size 10) 66.8 + 1.0
Table 5: T/V classification accuracy on the test set and
differences to dev set results (direct speech context)
not overfit on the development set when picking
the best model. The tendencies correspond well
to the development set: the frequency baseline is
almost identical, as are the results for the different
models. The differences to the development set
are all equal to or smaller than 1% accuracy, and
the best result at 67.5% is 0.5% better than on the
development set. This is a reassuring result, as our
model appears to generalize well to unseen data.
6.3 Analysis by Feature Types
The results from Section 6.1 motivate further anal-
ysis of the individual feature types.
Analysis of Word Features. Word features are
by far the most effective features. Table 6 lists
the top twenty words indicating T and V (ranked
by the ratio of probabilities for the two classes
on the training set). The list still includes some
proper names like Vrazumihin or Louis-Gaston
(even though all features have to occur in at least
three novels), but they are relatively infrequent.
The most prominent indicators for the formal class
V are titles (monsieur, (ma)?am) and instances of
formulaic language (Permit (me), Excuse (me)).
There are also some terms which are not straight-
forward indicators of formal address (angelic, stub-
bornness), but are associated with a high register.
There is a notable asymmetry between T and
V. The word features for T are considerably more
difficult to interpret. We find some forms of earlier
period English (thee, hast, thou, wilt) that result
from occasional archaic passages in the novels as
well first names (Louis-Gaston, Justine). Never-
theless, most features are not straightforward to
connect to specifically informal speech.
Analysis of Semantic Class Features. We
ranked the semantic classes we obtained by distri-
butional clustering in a similar manner to the word
features. Table 2 shows the top three classes in-
dicative for V. Almost all others of the 400 clusters
do not have a strong formal/informal association
629
Top 20 words for V Top 20 words for T
Word w P (w|V )P (w|T ) Word w
P (w|T )
P (w|V )
Excuse 36.5 thee 94.3
Permit 35.0 amenable 94.3
?ai 29.2 stuttering 94.3
?am 29.2 guardian 94.3
stubbornness 29.2 hast 92.0
flights 29.2 Louis-Gaston 92.0
monsieur 28.6 lease-making 92.0
Vrazumihin 28.6 melancholic 92.0
mademoiselle 26.5 ferry-boat 92.0
angelic 26.5 Justine 92.0
Allow 24.5 Thou 66.0
madame 21.2 responsibility 63.8
delicacies 21.2 thou 63.8
entrapped 21.2 Iddibal 63.8
lack-a-day 21.2 twenty-fifth 63.8
ma 21.0 Chic 63.8
duke 18.0 allegiance 63.8
policeman 18.0 Jouy 63.8
free-will 18.0 wilt 47.0
Canon 18.0 shall 47.0
Table 6: Most indicative word features for T or V
but mix formal, informal, and neutral vocabulary.
This tendency is already apparent in class 3: Gen-
tlemen is clearly formal, while rascals is informal.
patients can belong to either class. Even in class
1, we find Sirrah, a contemptuous term used in ad-
dressing a man or boy with a low formality score
(p(w|V )/p(w|T ) = 0.22). From cluster 4 onward,
none of the clusters is strongly associated with ei-
ther V or T (p(c|V )/p(c|T ) ? 1).
Our interpretation of these observations is that
in contrast to text categorization, there is no clear-
cut topical or domain difference between T and V:
both categories co-occur with words from almost
any domain. In consequence, semantic classes do
not, in general, represent strong unambiguous indi-
cators. Similar to the word features, the situation
is worse for T than for V: there still are reasonably
strong features for V, the ?marked? case, but it is
more difficult to find indicators for T.
Analysis of politeness features. A major reason
for the ineffectiveness of the Politeness Theory-
based features seems to be their low frequency:
in the best model, with a direct speech context of
size 8, only an average of 7 politeness features
was active for any given sentence. However, fre-
quency was not the only problem ? the politeness
features were generally unable to discriminate well
between T and V. For all features, the values of
p(f |V )/p(f |T ) are between 0.9 and 1.3, that is,
the features were only weakly indicative of one of
the classes. Furthermore, not all features turned
out to be indicative of the class we designed them
for. The best indicator for V was the Indefinites
feature (somehow, someone cf. Table 3), as ex-
pected. In contrast, the best indicator for T was the
Negation question feature which was supposedly
an indicator for V (didn?t I, haven?t we).
A majority of politeness features (13 of the 16)
had p(f |V )/p(f |T ) values above 1, that is, were
indicative for the class V. Thus for this feature type,
like for the others, it appears to be more difficult to
identify T than to identify V. This negative result
can be attributed at least in part to our method of
hand-crafting lists of expressions for these features.
The inadvertent inclusion of overly general terms
V might be responsible for the features? inability
to discriminate well, while we have presumably
missed specific terms which has hurt coverage.
This situation may in the future be remedied with
the semi-automatic acquisition of instantiations of
politeness features.
6.4 Analysis of Individual Novels
One possible hypothesis regarding the difficulty
of finding indicators for the class T is that indi-
cators for T tend to be more novel-specific than
indicators for V, since formal language is more
conventionalized (Brown and Levinson, 1987). If
this were the case, then our strategy of building
well-generalizing models by combining text from
different novels would naturally result in models
that have problems with picking up T features.
To investigate this hypothesis, we trained mod-
els with the best parameters as before (8-sentence
direct speech context, words as features). How-
ever, this time we trained novel-specific models,
splitting each novel into 50% training data and
50% testing data. We required novels to contain
more than 200 labeled sentences. This ruled out
most short stories, leaving us with 7 novels in the
test set. The results are shown in Table 7 and show
a clear improvement. The accuracy is 13% higher
than in our main experiment (67% vs. 80%), even
though the models were trained on considerably
less data. Six of the seven novels perform above
the 67.5% result from the main experiment.
The top-ranked features for T and V show a
much higher percentage of names for both T and
V than in the main experiment. This is to be ex-
630
Novel Accuracy
H. Beecher-Stove: Uncle Tom?s Cabin 90.0
J. Spyri: Cornelli 88.3
E. Zola: Lourdes 83.9
H. de Balzac: Cousin Pons 82.3
C. Dickens: The Pickwick Papers 77.7
C. Dickens: Nicholas Nickleby 74.8
F. Hodgson Burnett: Little Lord 61.6
All (micro average) 80.0
Table 7: T/V prediction models for individual novels
(50% of each novel for training and 50% testing)
pected, since this experiment does not restrict itself
to features that occurred in at least three novels.
The price we pay for this is worse generalization to
other novels. There is also still a T/V asymmetry:
more top features are shared among the V lists of
individual novels and with the main experiment
V list than on the T side. Like in the main exper-
iment (cf. Section 6.3), V features indicate titles
and other features of elevated speech, while T fea-
tures mostly refer to novel-specific protagonists
and events. In sum, these results provide evidence
for a difference in status of T and V.
7 Discussion and Conclusions
In this paper, we have studied the distinction
between formal and information (T/V) address,
which is not expressed overtly through pronoun
choice or morphosyntactic marking in modern En-
glish. Our hypothesis was that the T/V distinction
can be recovered in English nevertheless. Our man-
ual annotation study has shown that annotators can
in fact tag monolingual English sentences as T or
V with reasonable accuracy, but only if they have
sufficient context. We exploited the overt informa-
tion from German pronouns to induce T/V labels
for English and used this labeled corpus to train a
monolingual T/V classifier for English. We exper-
imented with features based on words, semantic
classes, and Politeness Theory predictions.
With regard to our NLP goal of building a T/V
classifier, we conclude that T/V classification is
a phenomenon that can be modelled on the basis
of corpus features. A major factor in classifica-
tion performance is the inclusion of a wide context
to counteract sparse data, and more sophisticated
context definitions improve results. We currently
achieve top accuracies of 67%-68%, which still
leave room for improvement. We next plan to
couple our T/V classifier with a machine trans-
lation system for a task-based evaluation on the
translation of direct address into German and other
languages with different T/V pronouns.
Considering our sociolinguistic goal of deter-
mining the ways in which English realizes the T/V
distinction, we first obtained a negative result: only
word features perform well, while semantic classes
and politeness features do hardly better than a fre-
quency baseline. Notably, there are no clear ?topi-
cal? divisions between T and V, like for example
in text categorization: almost all words are very
weakly correlated with either class, and seman-
tically similar words can co-occur with different
classes. Consequently, distributionally determined
semantic classes are not helpful for the distinction.
Politeness features are difficult to operationalize
with sufficiently high precision and recall.
An interesting result is the asymmetry between
the linguistic features for V and T at the lexical
level. V language appears to be more convention-
alized; the models therefore identified formulaic
expressions and titles as indicators for V. On the
other hand, very few such generic features exist for
the class T; consequently, the classifier has a hard
time learning good discriminating and yet generic
features. Those features that are indicative of T,
such as first names, are highly novel-specific and
were deliberately excluded from the main exper-
iment. When we switched to individual novels,
the models picked up such features, and accuracy
increased ? at the cost of lower generalizability
between novels. A more technical solution to this
problem would be the training of a single-class
classifier for V, treating T as the ?default? class
(Tax and Duin, 1999).
Finally, an error analysis showed that many er-
rors arise from sentences that are too short or un-
specific to determine T or V reliably. This points
to the fact that T/V should not be modelled as a
sentence-level classification task in the first place:
T/V is not a choice made for each sentence, but
one that is determined once for each pair of inter-
locutors and rarely changed. In future work, we
will attempt to learn social networks from novels
(Elson et al 2010), which should provide con-
straints on all instances of communication between
a speaker and an addressee. However, the big ? and
unsolved, as far as we know ? challenge is to au-
tomatically assign turns to interlocutors, given the
varied and often inconsistent presentation of direct
speech turns in novels.
631
References
John Ardila. 2003. (Non-Deictic, Socio-Expressive)
T-/V-Pronoun Distinction in Spanish/English Formal
Locutionary Acts. Forum for Modern Language
Studies, 39(1):74?86.
John A. Bateman. 1988. Aspects of clause politeness in
Japanese: An extended inquiry semantics treatment.
In Proceedings of ACL, pages 147?154, Buffalo,
New York.
Luisa Bentivogli and Emanuele Pianta. 2005. Ex-
ploiting parallel texts in the creation of multilingual
semantically annotated resources: the MultiSemCor
Corpus. Journal of Natural Language Engineering,
11(3):247?261.
Adam Bermingham and Alan F. Smeaton. 2009. A
study of inter-annotator agreement for opinion re-
trieval. In Proceedings of ACM SIGIR, pages 784?
785.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of ACL/HLT, pages 773?782, Portland, OR.
Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for symmet-
rical and asymmetrical parallel corpora. In Coling
2010: Posters, pages 81?89, Beijing, China.
Roger Brown and Albert Gilman. 1960. The pronouns
of power and solidarity. In Thomas A. Sebeok, edi-
tor, Style in Language, pages 253?277. MIT Press,
Cambridge, MA.
Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some Universals in Language Usage. Num-
ber 4 in Studies in Interactional Sociolinguistics.
Cambridge University Press.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL, pages 59?66, Bu-
dapest, Hungary.
J. Cohen. 1960. A Coefficient of Agreement for Nomi-
nal Scales. Educational and Psychological Measure-
ment, 20(1):37?46.
David Elson, Nicholas Dames, and Kathleen McKe-
own. 2010. Extracting social networks from literary
fiction. In Proceedings of ACL, pages 138?147, Up-
psala, Sweden.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Manaal Faruqui and Sebastian Pad?. 2011. ?I Thou
Thee, Thou Traitor?: Predicting formal vs. infor-
mal address in English literature. In Proceedings of
ACL/HLT 2011, pages 467?472, Portland, OR.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In Proceedings of
EMNLP, pages 141?150, Singapore.
Joseph L. Fleiss. 1981. Statistical methods for rates
and proportions. John Wiley, New York, 2nd edi-
tion.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In Proceedings of the EACL MT workshop, pages
115?119, Athens, Greece.
Jerry Hobbs and Megumi Kameyama. 1990. Trans-
lation by abduction. In Proceedings of COLING,
pages 155?161, Helsinki, Finland.
Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across parallel
texts. Journal of Natural Language Engineering,
11(3):311?325.
Hiroshi Kanayama. 2003. Paraphrasing rules for au-
tomatic evaluation of translation into Japanese. In
Proceedings of the Second International Workshop
on Paraphrasing, pages 88?93, Sapporo, Japan.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit, pages 79?86,
Phuket, Thailand.
Heinz L. Kretzenbacher, Michael Clyne, and Doris
Sch?pbach. 2006. Pronominal Address in German:
Rules, Anarchy and Embarrassment Potential. Aus-
tralian Review of Applied Linguistics, 39(2):17.1?
17.18.
Alexander K?nzli. 2010. Address pronouns as a prob-
lem in French-Swedish translation and translation
revision. Babel, 55(4):364?380.
Zhifei Li and David Yarowsky. 2008. Mining and
modeling relations between formal and informal Chi-
nese phrases from web corpora. In Proceedings of
EMNLP, pages 1031?1040, Honolulu, Hawaii.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
UK, 1st edition.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Roberto Navigli. 2009. Word Sense Disambiguation:
a survey. ACM Computing Surveys, 41(2):1?69.
Eric W. Noreen. 1989. Computer-intensive Methods
for Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceed-
ing of the 3rd ACL Workshop on Very Large Corpora,
Cambridge, MA.
Michael Schiehlen. 1998. Learning tense transla-
tion from bilingual corpora. In Proceedings of
ACL/COLING, pages 1183?1187, Montreal, Canada.
632
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Doris Sch?pbach, John Hajek, Jane Warren, Michael
Clyne, Heinz Kretzenbacher, and Catrin Norrby.
2006. A cross-linguistic comparison of address pro-
noun use in four European languages: Intralingual
and interlingual dimensions. In Proceedings of the
Annual Meeting of the Australian Linguistic Society,
Brisbane, Australia.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma? Erjavec, and Dan Tufis. 2006.
The JRC-Acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In Proceedings of LREC,
pages 2142?2147, Genoa, Italy.
David M. J. Tax and Robert P. W. Duin. 1999. Sup-
port vector domain description. Pattern Recognition
Letters, 20:1191?1199.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL, pages 200?207, Pittsburgh, PA.
633
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462?471,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Improving Vector Space Word Representations
Using Multilingual Correlation
Manaal Faruqui and Chris Dyer
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui, cdyer}@cs.cmu.edu
Abstract
The distributional hypothesis of Harris
(1954), according to which the meaning
of words is evidenced by the contexts
they occur in, has motivated several effec-
tive techniques for obtaining vector space
semantic representations of words using
unannotated text corpora. This paper ar-
gues that lexico-semantic content should
additionally be invariant across languages
and proposes a simple technique based
on canonical correlation analysis (CCA)
for incorporating multilingual evidence
into vectors generated monolingually. We
evaluate the resulting word representations
on standard lexical semantic evaluation
tasks and show that our method produces
substantially better semantic representa-
tions than monolingual techniques.
1 Introduction
Data-driven learning of vector-space word embed-
dings that capture lexico-semantic properties is
a technique of central importance in natural lan-
guage processing. Using cooccurrence statistics
from a large corpus of text (Deerwester et al.,
1990; Turney and Pantel, 2010),
1
it is possible
to construct high-quality semantic vectors ? as
judged by both correlations with human judge-
ments of semantic relatedness (Turney, 2006;
Agirre et al., 2009) and as features for downstream
applications (Turian et al., 2010).
The observation that vectors representing cooc-
currence tendencies would capture meaning is ex-
pected according to the distributional hypothe-
sis (Harris, 1954), famously articulated by Firth
1
Related approaches use the internal representations from
neural network models of word sequences (Collobert and We-
ston, 2008) or continuous bags-of-context wordsels (Mikolov
et al., 2013a) to arrive at vector representations that likewise
capture cooccurence tendencies and meanings.
(1957) as You shall know a word by the company
it keeps. Although there is much evidence in fa-
vor of the distributional hypothesis, in this paper
we argue for incorporating translational context
when constructing vector space semantic models
(VSMs). Simply put: knowing how words trans-
late is a valuable source of lexico-semantic infor-
mation and should lead to better VSMs.
Parallel corpora have long been recognized as
valuable for lexical semantic applications, in-
cluding identifying word senses (Diab, 2003;
Resnik and Yarowsky, 1999) and paraphrase and
synonymy relationships (Bannard and Callison-
Burch, 2005). The latter work (which we build on)
shows that if different words or phrases in one lan-
guage often translate into a single word or phrase
type in a second language, this is good evidence
that they are synonymous. To illustrate: the En-
glish word forms aeroplane, airplane, and plane
are observed to translate into the same Hindi word:
vAy  yAn (vaayuyaan). Thus, even if we did not
know the relationship between the English words,
this translation fact is evidence that they all have
the same meaning.
How can we exploit information like this when
constructing VSMs? We propose a technique that
first constructs independent VSMs in two lan-
guages and then projects them onto a common
vector space such that translation pairs (as deter-
mined by automatic word alignments) should be
maximally correlated (?2). We review latent se-
mantic analysis (LSA), which serves as our mono-
lingual VSM baseline (?3), and a suite of stan-
dard evaluation tasks that we use to measure the
quality of the embeddings (?4). We then turn to
experiments. We first show that our technique
leads to substantial improvements over monolin-
gual LSA (?5), and then examine how our tech-
nique fares with vectors learned using two dif-
ferent neural networks, one that models word se-
quences and a second that models bags-of-context
462
Figure 1: Cross-lingual word vector projection us-
ing CCA.
words. We observe substantial improvements over
the sequential model using multilingual evidence
but more mixed results relative to using the bags-
of-contexts model (?6).
2 Multilingual Correlation with CCA
To gain information from the translation of a given
word in other languages the most basic thing to do
would be to just append the given word represen-
tation with the word representations of its transla-
tion in the other language. This has three draw-
backs: first, it increases the number of dimensions
in the vector; second, it can pull irrelevant infor-
mation from the other language that doesn?t gen-
eralize across languages and finally the given word
might be out of vocabulary of the parallel corpus
or dictionary.
To counter these problems we use CCA
2
which
is a way of measuring the linear relationship be-
tween two multidimensional variables. It finds two
projection vectors, one for each variable, that are
optimal with respect to correlations. The dimen-
sionality of these new projected vectors is equal to
or less than the smaller dimensionality of the two
variables.
Let ? ? R
n
1
?d
1
and ? ? R
n
2
?d
2
be vector
2
We use the MATLAB module for CCA: http://www.
mathworks.com/help/stats/canoncorr.html
space embeddings of two different vocabularies
where rows represent words. Since the two vo-
cabularies are of different sizes (n
1
and n
2
) and
there might not exist translation for every word
of ? in ?, let ?
?
? ? where every word in ?
?
is translated to one other word
3
in ?
?
? ? and
? ? R
n?d
1
and ? ? R
n?d
2
.
Let x and y be two corresponding vectors from
?
?
and ?
?
, and v and w be two projection direc-
tions. Then, the projected vectors are:
x
?
= xv y
?
= yw
(1)
and the correlation between the projected vectors
can be written as:
?(x
?
,y
?
) =
E[x
?
y
?
]
?
E[x
?
2
]E[y
?
2
]
(2)
CCA maximizes ? for the given set of vectors ?
?
and ?
?
and outputs two projection vectors v and
w:
v,w = CCA(x,y)
= arg max
v,w
?(xv,yw)
(3)
Using these two projection vectors we can project
the entire vocabulary of the two languages ? and
? using equation 1. Summarizing:
V ,W = CCA(?
?
,?
?
) (4)
?
?
= ?V ?
?
= ?W (5)
where, V ? R
d
1
?d
, W ? R
d
2
?d
con-
tain the projection vectors and d =
min{rank(V ), rank(W )}. Thus, the result-
ing vectors cannot be longer than the original
vectors. Since V and W can be used to project
the whole vocabulary, CCA also solves the
problem of not having translations of a particular
word in the dictionary. The schema of performing
CCA on the monolingual word representations of
two languages is shown in Figure 1.
Further Dimensionality Reduction: Since
CCA gives us correlations and corresponding
projection vectors across d dimensions which
can be large, we perform experiments by taking
projections of the original word vectors across
only the top k correlated dimensions. This is
trivial to implement as the projection vectors V ,
3
Further information on how these one-to-one translations
are obtained in ?5
463
W in equation 4 are already sorted in descending
order of correlation. Therefore in,
?
?
k
= ?V
k
?
?
k
= ?W
k
(6)
?
?
k
and?
?
k
are now word vector projections along
the top k correlated dimensions, where, V
k
and
W
k
are the column truncated matrices.
3 Latent Semantic Analysis
We perform latent semantic analysis (Deerwester
et al., 1990) on a word-word co-occurrence ma-
trix. We construct a word co-occurrence frequency
matrix F for a given training corpus where each
row w, represents one word in the corpus and ev-
ery column c, is the context feature in which the
word is observed. In our case, every column is
a word which occurs in a given window length
around the target word. For scalability reasons, we
only select words with frequency greater than 10
as features. We also remove the top 100 most fre-
quent words (mostly stop words) from the column
features.
We then replace every entry in the sparse fre-
quency matrix F by its pointwise mutual infor-
mation (PMI) (Church and Hanks, 1990; Turney,
2001) resulting in X . PMI is designed to give a
high value to x
ij
where there is a interesting rela-
tion between w
i
and c
j
, a small or negative value
of x
ij
indicates that the occurrence of w
i
in c
j
is
uninformative. Finally, we factorize the matrix X
using singular value decomposition (SVD). SVD
decomposes X into the product of three matrices:
X = U?V
>
(7)
where, U and V are in column orthonormal
form and ? is a diagonal matrix of singular val-
ues (Golub and Van Loan, 1996). We obtain a re-
duced dimensional representation of words from
size |V | to k:
A = U
k
?
k
(8)
where k can be controlled to trade off between re-
construction error and number of parameters, ?
k
is the diagonal matrix containing the top k singular
values, U
k
is the matrix produced by selecting the
corresponding columns from U and A represents
the new matrix containing word vector representa-
tions in the reduced dimensional space.
4 Word Representation Evaluation
We evaluate the quality of our word vector repre-
sentations on a number of tasks that test how well
they capture both semantic and syntactic aspects
of the representations.
4.1 Word Similarity
We evaluate our word representations on four dif-
ferent benchmarks that have been widely used to
measure word similarity. The first one is the WS-
353 dataset (Finkelstein et al., 2001) containing
353 pairs of English words that have been assigned
similarity ratings by humans. This data was fur-
ther divided into two fragments by Agirre et al.
(2009) who claimed that similarity (WS-SIM) and
relatedness (WS-REL) are two different kinds of
relations and should be dealt with separately. We
present results on the whole set and on the individ-
ual fragments as well.
The second and third benchmarks are the RG-
65 (Rubenstein and Goodenough, 1965) and the
MC-30 (Miller and Charles, 1991) datasets that
contain 65 and 30 pairs of nouns respectively and
have been given similarity rankings by humans.
These differ from WS-353 in that it contains only
nouns whereas the former contains all kinds of
words. The fourth benchmark is the MTurk-287
(Radinsky et al., 2011) dataset that constitutes of
287 pairs of words and is different from the above
two benchmarks in that it has been constructed by
crowdsourcing the human similarity ratings using
Amazon Mechanical Turk.
We calculate similarity between a given pair
of words by the cosine similarity between their
corresponding vector representation. We then re-
port Spearman?s rank correlation coefficient (My-
ers and Well, 1995) between the rankings pro-
duced by our model against the human rankings.
4.2 Semantic Relations (SEM-REL)
Mikolov et al. (2013a) present a new semantic re-
lation dataset composed of analogous word pairs.
It contains pairs of tuples of word relations that
follow a common semantic relation. For example,
in England : London :: France : Paris, the two
given pairs of words follow the country-capital re-
lation. There are three other such kinds of rela-
tions: country-currency, man-woman, city-in-state
and overall 8869 such pairs of words
4
.
The task here is to find a word d that best fits
the following relationship: a : b :: c : d given a, b
and c. We use the vector offset method described
4
107 pairs were out of vocabulary for our vectors and
were ignored.
464
in Mikolov et al. (2013a) that computes the vector
y = x
a
? x
b
+ x
c
where, x
a
,x
b
and x
c
are word
vectors of a, b and c respectively and returns the
vector x
w
from the whole vocabulary which has
the highest cosine similarity to y:
x
w
= arg max
x
w
x
w
? y
|x
w
| ? |y|
It is worth noting that this is a non-trivial |V |-way
classification task where V is the size of the vo-
cabulary.
4.3 Syntactic Relations (SYN-REL)
This dataset contains word pairs that are differ-
ent syntactic forms of a given word and was pre-
pared by Mikolov et al. (2013a). For exam-
ple, in walking and walked, the second word is
the past tense of the first word. There are nine
such different kinds of relations: adjective-adverb,
opposites, comaparative, superlative, present-
participle, nation-nationality, past tense, plural
nouns and plural verbs. Overall there are 10675
such syntactic pairs of word tuples. The task here
again is identifying a word d that best fits the fol-
lowing relationship: a : b :: c : d and we solve it
using the method described in ?4.2.
5 Experiments
5.1 Data
For English, German and Spanish we used the
WMT-2011
5
monolingual news corpora and for
French we combined the WMT-2011 and 2012
6
monolingual news corpora so that we have around
300 million tokens for each language to train the
word vectors.
For CCA, a one-to-one correspondence be-
tween the two sets of vectors is required. Obvi-
ously, the vocabulary of two languages are of dif-
ferent sizes and hence to obtain one-to-one map-
ping, for every English word we choose a word
from the other language to which it has been
aligned the maximum number of times
7
in a paral-
lel corpus. We got these word alignment counts
using cdec (Dyer et al., 2010) from the paral-
lel news commentary corpora (WMT 2006-10)
combined with the Europarl corpus for English-
{German, French, Spanish}.
5
http://www.statmt.org/wmt11/
6
http://www.statmt.org/wmt12/
7
We also tried weighted average of vectors across all
aligned words and did not observe any significant difference
in results.
5.2 Methodology
We construct LSA word vectors of length 640
8
for
English, German, French and Spanish. We project
the English word vectors using CCA by pairing
them with German, French and Spanish vectors.
For every language pair we take the top k cor-
related dimensions (cf. equation 6), where k ?
10%, 20%, . . . 100% and tune the performance on
WS-353 task. We then select the k that gives
us the best average performance across language
pairs, which is k = 80%, and evaluate the cor-
responding vectors on all other benchmarks. This
prevents us from over-fitting k for every individual
task.
5.3 Results
Table 1 shows the Spearman?s correlation ratio ob-
tained by using word vectors to compute the sim-
ilarity between two given words and compare the
ranked list against human rankings. The first row
in the table shows the baseline scores obtained
by using only the monolingual English vectors
whereas the other rows correspond to the multi-
lingual cases. The last row shows the average per-
formance of the three language pairs. For all the
tasks we get at least an absolute gain of 20 points
over the baseline. These results are highly assur-
ing of our hypothesis that multilingual context can
help in improving the semantic similarity between
similar words as described in the example in ?1.
Results across language pairs remain almost the
same and the differences are most of the times sta-
tistically insignificant.
Table 1 also shows the accuracy obtained on
predicting different kinds of relations between
word pairs. For the SEM-REL task the average
improvement in accuracy is an absolute 30 points
over the baseline which is highly statistically sig-
nificant (p < 0.01) according to the McNemar?s
test (Dietterich, 1998). The same holds true for
the SYN-REL task where we get an average im-
provement of absolute 8 points over the baseline
across the language pairs. Such an improvement
in scores across these relation prediction tasks fur-
ther enforces our claim that cross-lingual context
can be exploited using the method described in ?2
and it does help in encoding the meaning of a word
better in a word vector than monolingual informa-
tion alone.
8
See section 5.5 for further discussion on vector length.
465
Lang Dim WS-353 WS-SIM WS-REL RG-65 MC-30 MTurk-287 SEM-REL SYN-REL
En 640 46.7 56.2 36.5 50.7 42.3 51.2 14.5 36.8
De-En 512 68.0 74.4 64.6 75.5 81.9 53.6 43.9 45.5
Fr-En 512 68.4 73.3 65.7 73.5 81.3 55.5 43.9 44.3
Es-En 512 67.2 71.6 64.5 70.5 78.2 53.6 44.2 44.5
Average ? 56.6 64.5 51.0 62.0 65.5 60.8 44 44.7
Table 1: Spearman?s correlation (left) and accuracy (right) on different tasks.
Figure 2: Monolingual (top) and multilingual (bottom; marked with apostrophe) word projections of the
antonyms (shown in red) and synonyms of ?beautiful?.
5.4 Qualitative Example
To understand how multilingual evidence leads to
better results in semantic evaluation tasks, we plot
the word representations obtained in ?3 of sev-
eral synonyms and antonyms of the word ?beau-
tiful? by projecting both the transformed and un-
transformed vectors onto R
2
using the t-SNE
tool (van der Maaten and Hinton, 2008). The
untransformed LSA vectors are in the upper part
of Fig. 2, and the CCA-projected vectors are in
the lower part. By comparing the two regions,
we see that in the untransformed representations,
the antonyms are in two clusters separated by the
synonyms, whereas in the transformed representa-
tion, both the antonyms and synonyms are in their
own cluster. Furthermore, the average intra-class
distance between synonyms and antonyms is re-
duced.
Figure 3: Performance of monolingual and mul-
tilingual vectors on WS-353 for different vector
lengths.
5.5 Variation in Vector Length
In order to demonstrate that the gains in perfor-
mance by using multilingual correlation sustains
466
for different number of dimensions, we compared
the performance of the monolingual and (German-
English) multilingual vectors with k = 80% (cf.
?5.2). It can be see in figure 3 that the perfor-
mance improvement for multilingual vectors re-
mains almost the same for different vector lengths
strengthening the reliability of our approach.
6 Neural Network Word Representations
Other kinds of vectors shown to be useful in many
NLP tasks are word embeddings obtained from
neural networks. These word embeddings capture
more complex information than just co-occurrence
counts as explained in the next section. We test
our multilingual projection method on two types
of such vectors by keeping the experimental set-
ting exactly the same as in ?5.2.
6.1 RNN Vectors
The recurrent neural network language model
maximizes the log-likelihood of the training cor-
pus. The architecture (Mikolov et al., 2013b) con-
sists of an input layer, a hidden layer with recur-
rent connections to itself, an output layer and the
corresponding weight matrices. The input vector
w(t) represents input word at time t encoded us-
ing 1-of-N encoding and the output layer y(t) pro-
duces a probability distribution over words in the
vocabulary V . The hidden layer maintains a repre-
sentation of the sentence history in s(t). The val-
ues in the hidden and output layer are computed as
follows:
s(t) = f(Uw(t) + Ws(t? 1)) (9)
y(t) = g(V s(t)) (10)
where, f and g are the logistic and softmax func-
tions respectively. U and V are weight matri-
ces and the word representations are found in the
columns of U . The model is trained using back-
propagation. Training such a purely lexical model
will induce representations with syntactic and se-
mantic properties. We use the RNNLM toolkit
9
to
induce these word representations.
6.2 Skip Gram Vectors
In the RNN model (?6.1) most of the complexity
is caused by the non-linear hidden layer. This is
avoided in the new model proposed in Mikolov
9
http://www.fit.vutbr.cz/
?
imikolov/
rnnlm/
et al. (2013a) where they remove the non-linear
hidden layer and there is a single projection layer
for the input word. Precisely, each current word is
used as an input to a log-linear classifier with con-
tinuous projection layer and words within a cer-
tain range before and after the word are predicted.
These vectors are called the skip-gram (SG) vec-
tors. We used the tool
10
for obtaining these word
vectors with default settings.
6.3 Results
We compare the best results obtained by using dif-
ferent types of monolingual word representations
across all language pairs. For brevity we do not
show the results individually for all language pairs
as they follow the same pattern when compared to
the baseline for every vector type. We train word
vectors of length 80 because it was computation-
ally intractable to train the neural embeddings for
higher dimensions. For multilingual vectors, we
obtain k = 60% (cf. ?5.2).
Table 2 shows the correlation ratio and the accu-
racies for the respective evaluation tasks. For the
RNN vectors the performance improves upon in-
clusion of multilingual context for almost all tasks
except for SYN-REL where the loss is statistically
significant (p < 0.01). For MC-30 and SEM-
REL the small drop in performance is not statis-
tically significant. Interestingly, the performance
gain/loss for the SG vectors in most of the cases is
not statistically significant, which means that in-
clusion of multilingual context is not very helpful.
In fact, for SYN-REL the loss is statistically sig-
nificant (p < 0.05) which is similar to the perfor-
mance of RNN case. Overall, the best results are
obtained by the SG vectors in six out of eight eval-
uation tasks whereas SVD vectors give the best
performance in two tasks: RG-65, MC-30. This is
an encouraging result as SVD vectors are the eas-
iest and fastest to obtain as compared to the other
two vector types.
To further understand why multilingual context
is highly effective for SVD vectors and to a large
extent for RNN vectors as well, we plot (Figure 4)
the correlation ratio obtained by varying the length
of word representations by using equation 6 for the
three different vector types on two word similarity
tasks: WS-353 and RG-65.
SVD vectors improve performance upon the in-
crease of the number of dimensions and tend to
10
https://code.google.com/p/word2vec/
467
Vectors Dim Lang WS-353 WS-SIM WS-REL RG-65 MC-30 MTurk SEM-REL SYN-REL
SVD
80 Mono 34.8 45.5 23.4 30.8 21.0 46.6 13.5 24.4
48 Multi 58.1 65.3 52.7 62.7 67.7 62.1 23.4 33.2
RNN
80 Mono 23.6 35.6 17.5 26.2 47.7 32.9 4.7 18.2
48 Multi 35.4 47.3 29.8 36.6 46.5 43.8 4.1 12.2
SG
80 Mono 63.9 69.9 60.9 54.6 62.8 66.9 47.8 47.8
48 Multi 63.1 70.4 57.6 54.9 64.7 58.7 46.5 44.2
Table 2: Spearman?s correlation (left) and accuracy (right) on different tasks. Bold indicates best result
across all vector types. Mono: monolingual and Multi: multilingual.
	 ? WS-353 RG-65 
SVD 
RNN 
SG 
Number of dimensions 
Correlati
on ratio 
(%) 
Figure 4: Performance as a function of vector length on word similarity tasks. The monolingual vectors
always have a fixed length of 80, they are just shown in the plots for comparison.
468
saturate towards the end. For all the three lan-
guage pairs the SVD vectors show uniform pat-
tern of performance which gives us the liberty to
use any language pair at hand. This is not true
for the RNN vectors whose curves are signifi-
cantly different for every language pair. SG vec-
tors show a uniform pattern across different lan-
guage pairs and the performance with multilin-
gual context converges to the monolingual perfor-
mance when the vector length becomes equal to
the monolingual case (k = 80). The fact that both
SG and SVD vectors have similar behavior across
language pairs can be treated as evidence that se-
mantics or information at a conceptual level (since
both of them basically model word cooccurrence
counts) transfers well across languages (Dyvik,
2004) although syntax has been projected across
languages as well (Hwa et al., 2005; Yarowsky and
Ngai, 2001). The pattern of results in the case of
RNN vectors are indicative of the fact that these
vectors encode syntactic information as explained
in ?6 which might not generalize well as compared
to semantic information.
7 Related Work
Our method of learning multilingual word vectors
is most closely associated to Zou et al. (2013) who
learn bilingual word embeddings and show their
utility in machine translation. They optimize the
monolingual and the bilingual objective together
whereas we do it in two separate steps and project
to a common vector space to maximize correla-
tion between the two. Vuli?c and Moens (2013)
learn bilingual vector spaces from non parallel
data induced using a seed lexicon. Our method
can also be seen as an application of multi-view
learning (Chang et al., 2013; Collobert and We-
ston, 2008), where one of the views can be used
to capture cross-lingual information. Klementiev
et al. (2012) use a multitask learning framework
to encourage the word representations learned by
neural language models to agree cross-lingually.
CCA can be used for dimension reduction and
to draw correspondences between two sets of
data.Haghighi et al. (2008) use CCA to draw trans-
lation lexicons between words of two different lan-
guages using only monolingual corpora. CCA
has also been used for constructing monolingual
word representations by correlating word vectors
that capture aspects of word meaning and dif-
ferent types of distributional profile of the word
(Dhillon et al., 2011). Although our primary ex-
perimental emphasis was on LSA based monolin-
gual word representations, which we later gener-
alized to two different neural network based word
embeddings, these monolingual word vectors can
also be obtained using other continuous models of
language (Collobert and Weston, 2008; Mnih and
Hinton, 2008; Morin and Bengio, 2005; Huang et
al., 2012).
Bilingual representations have previously been
explored with manually designed vector space
models (Peirsman and Pad?o, 2010; Sumita, 2000)
and with unsupervised algorithms like LDA and
LSA (Boyd-Graber and Blei, 2012; Zhao and
Xing, 2006). Bilingual evidence has also been ex-
ploited for word clustering which is yet another
form of representation learning, using both spec-
tral methods (Zhao et al., 2005) and structured
prediction approaches (T?ackstr?om et al., 2012;
Faruqui and Dyer, 2013).
8 Conclusion
We have presented a canonical correlation anal-
ysis based method for incorporating multilingual
context into word representations generated using
only monolingual information and shown its ap-
plicability across three different ways of generat-
ing monolingual vectors on a variety of evalua-
tion benchmarks. These word representations ob-
tained after using multilingual evidence perform
significantly better on the evaluation tasks com-
pared to the monolingual vectors. We have also
shown that our method is more suitable for vec-
tors that encode semantic information than those
that encode syntactic information. Our work sug-
gests that multilingual evidence is an important
resource even for purely monolingual, semanti-
cally aware applications. The tool for projecting
word vectors can be found at http://cs.cmu.
edu/
?
mfaruqui/soft.html.
Acknowledgements
We thanks Kevin Gimpel, Noah Smith, and David
Bamman for helpful comments on earlier drafts
of this paper. This research was supported by the
NSF through grant IIS-1352440.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
469
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In Pro-
ceedings of North American Chapter of the Associ-
ation for Computational Linguistics, NAACL ?09,
pages 19?27, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL.
Jordan L. Boyd-Graber and David M. Blei. 2012. Mul-
tilingual topic models for unaligned text. CoRR,
abs/1205.2657.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22?29, March.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, ICML ?08, pages 160?167, New
York, NY, USA. ACM.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, pages 199?207.
Mona Talat Diab. 2003. Word sense disambiguation
within a multilingual framework. Ph.D. thesis, Uni-
versity of Maryland at College Park, College Park,
MD, USA. AAI3115805.
Thomas G. Dietterich. 1998. Approximate statis-
tical tests for comparing supervised classification
learning algorithms. Neural Computation, 10:1895?
1923.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In In Proceedings of ACL System Demonstrations.
Helge Dyvik. 2004. Translations as semantic mir-
rors: from parallel corpus to wordnet. Language
and Computers, 49(1):311?326.
Manaal Faruqui and Chris Dyer. 2013. An informa-
tion theoretic approach to bilingual word clustering.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 777?783, Sofia, Bulgaria, Au-
gust.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In WWW ?01: Proceedings of the
10th international conference on World Wide Web,
pages 406?414, New York, NY, USA. ACM Press.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis, pages 1?32.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
computations (3rd ed.). Johns Hopkins University
Press, Baltimore, MD, USA.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:11?311.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In In NIPS.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
AISTATS05, pages 246?252.
470
Jerome L. Myers and Arnold D. Well. 1995. Research
Design & Statistical Analysis. Routledge, 1 edition,
June.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 921?929,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
WWW ?11, pages 337?346, New York, NY, USA.
ACM.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evalua-
tion methods for word sense disambiguation. Nat.
Lang. Eng., 5(2):113?133, June.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?00, pages 425?431, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In The 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 1, page 11. Association
for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning : Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
pages 141?188.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the 12th
European Conference on Machine Learning, EMCL
?01, pages 491?502, London, UK, UK. Springer-
Verlag.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Comput. Linguist., 32(3):379?416, Septem-
ber.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2579?2605, November.
Ivan Vuli?c and Marie-Francine Moens. 2013. A study
on bootstrapping bilingual vector spaces from non-
parallel data (and nothing else). In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1613?1624, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies, NAACL ?01, pages 1?
8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL06.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005.
Bilingual word spectral clustering for statistical ma-
chine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, ParaText
?05, pages 25?32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
471
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467?472,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
?I Thou Thee, Thou Traitor?:
Predicting Formal vs. Informal Address in English Literature
Manaal Faruqui
Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
manaalfar@gmail.com
Sebastian Pad?
Computational Linguistics
Heidelberg University
Heidelberg, Germany
pado@cl.uni-heidelberg.de
Abstract
In contrast to many languages (like Russian or
French), modern English does not distinguish
formal and informal (?T/V?) address overtly,
for example by pronoun choice. We describe
an ongoing study which investigates to what
degree the T/V distinction is recoverable in
English text, and with what textual features it
correlates. Our findings are: (a) human raters
can label English utterances as T or V fairly
well, given sufficient context; (b), lexical cues
can predict T/V almost at human level.
1 Introduction
In many Indo-European languages, such as French,
German, or Hindi, there are two pronouns corre-
sponding to the English you. This distinction is
generally referred to as the T/V dichotomy, from
the Latin pronouns tu (informal, T) and vos (formal,
V) (Brown and Gilman, 1960). The V form can
express neutrality or polite distance and is used to
address socially superiors. The T form is employed
for friends or addressees of lower social standing,
and implies solidarity or lack of formality. Some
examples for V pronouns in different languages are
Sie (German), Vous (French), andaAp [Aap] (Hindi).
The corresponding T pronouns are du, tu, and t
 
m
[tum].
English used to have a T/V distinction until the
18th century, using you as V and thou as T pronoun.
However, in contemporary English, you has taken
over both uses, and the T/V distinction is not marked
morphosyntactically any more. This makes gener-
ation in English and translation into English easy.
Conversely, the extraction of social information from
texts, and translation from English into languages
with a T/V distinction is very difficult.
In this paper, we investigate the possibility to re-
cover the T/V distinction based on monolingual En-
glish text. We first demonstrate that annotators can
assign T/V labels to English utterances fairly well
(but not perfectly). To identify features that indicate
T and V, we create a parallel English?German corpus
of literary texts and preliminarily identify features
that correlate with formal address (like titles, and
formulaic language) as well as informal address. Our
results could be useful, for example, for MT from
English into languages that distinguish T and V, al-
though we did not test this prediction with the limits
of a short paper.
From a Natural Language Processing point of view,
the recovery of T/V information is an instance of a
more general issue in cross-lingual NLP and ma-
chine translation where for almost every language
pair, there are distinctions that are not expressed
overtly in the source language, but are in the target
language, and must therefore be recovered in some
way. Other examples from the literature include
morphology (Fraser, 2009) and tense (Schiehlen,
1998). The particular problem of T/V address has
been considered in the context of translation into
Japanese (Hobbs and Kameyama, 1990; Kanayama,
2003) and generation (Bateman, 1988), but only
on the context of knowledge-rich methods. As for
data-driven studies, we are only aware of Li and
Yarowsky?s (2008) work, who learn pairs of formal
and informal constructions in Chinese where T/V is
expressed mainly in construction choice.
467
Naturally, there is a large body of work on T/V
in (socio-)linguistics and translation science, cover-
ing in particular the conditions governing T/V use
in different languages (Kretzenbacher et al, 2006;
Sch?pbach et al, 2006) and on the difficulties in
translating them (Ardila, 2003; K?nzli, 2010). How-
ever, these studies are generally not computational in
nature, and most of their observations and predictions
are difficult to operationalize.
2 A Parallel Corpus of Literary Texts
2.1 Data Selection
We chose literary texts to build a parallel corpus for
the investigation of the T/V distinction. The main
reason is that commonly used non-literary collections
like EUROPARL (Koehn, 2005) consist almost ex-
clusively of formal interactions and are therefore of
no use to us. Fortunately, many 18th and 19th century
texts are freely available in several languages.
We identified 115 novels among the texts pro-
vided by Project Gutenberg (English) and Project
Gutenberg-DE (German) that were available in both
languages, with a total of 0.5M sentences per lan-
guage.1 Examples include Dickens? David Copper-
field or Tolstoy?s Anna Karenina. We decided to
exclude plays and poems as they often include partial
sentences and structures that are difficult to align.
2.2 Data Preparation
As the German and English novels come from two
different websites, they were not coherent in their
structure. They were first manually cleaned by delet-
ing the index, prologue, epilogue and Gutenberg li-
cense from the beginning and end of the files. To
some extent the chapter numbers and titles occurring
at the beginning of each chapter were cleared as well.
The files were then formatted to contain one sentence
per line and a blank line was inserted to preserve the
segmentation information.
The sentence splitter and tokenizer provided with
EUROPARL (Koehn, 2005) were used. We ob-
tained a comparable corpus of English and German
novels using the above pre-processing. The files
in the corpus were sentence-aligned using Gargan-
tuan (Braune and Fraser, 2010), an aligner that sup-
ports one-to-many alignments. After obtaining the
1http://www.gutenberg.org and http://gutenberg.spiegel.de/
ID Position Lemma Cap Category
(1) any du any T
(2) non-initial sie yes V
(3) non-initial ihr no T
(4) non-initial ihr yes V
Table 1: Rules for T/V determination for German personal
pronouns. (Cap: Capitalized)
sentence aligned corpus we computed word align-
ments in both English to German and German to En-
glish directions using Giza++ (Och and Ney, 2003).
The corpus was lemmatized and POS-tagged using
TreeTagger (Schmid, 1994). We did not apply a full
parser to keep processing as efficient as possible.
2.3 T/V Gold Labels for English Utterances
The goal of creating our corpus is to enable the in-
vestigation of contextual correlates of T/V in English.
In order to do this, we need to decide for as many
English utterances in our corpus as possible whether
they instantiate formal or informal address. Given
that we have a parallel corpus where the German side
overtly realizes T and V, this is a classical case of
annotation projection (Yarowsky and Ngai, 2001):
We transfer the German T/V information onto the
English side to create an annotated English corpus.
This allows us to train and evaluate a monolingual
English classifier for this phenomenon. However,
two problems arise on the way:
Identification of T/V in German pronouns. Ger-
man has three relevant personal pronouns: du, sie,
and ihr. These pronouns indicate T and V, but due to
their ambiguity, it is impossible to simply interpret
their presence or absense as T or V. We developed
four simple disambiguation rules based on position
on the sentence and capitalization, shown in Table 1.
The only unambiguous pronoun is du, which ex-
presses (singular) T (Rule 1). The V pronoun for
singular, sie, doubles as the pronoun for third person
(singular and plural), which is neutral with respect
to T/V. Since TreeTagger does not provide person
information, the only indicator that is available is
capitalization: Sie is 2nd person V. However, since
all words are capitalized in utterance-initial positions,
we only assign the label V in non-initial positions
468
(Rule 2).2
Finally, ihr is also ambiguous: non-capitalized, it
is used as T plural (Rule 3); capitalized, it is used as
an archaic alternative to Sie for V plural (Rule 4).
These rules leave a substantial number of instances
of German second person pronouns unlabeled; we
cover somewhat more than half of all pronouns. In
absolute numbers, from 0.5M German sentences we
obtained about 15% labeled sentences (45K for V
and 30K for T). However, this is not a fundamental
problem, since we subsequently used the English
data to train a classifier that is able to process any
English sentence.
Choice of English units to label. On the German
side, we assign the T/V labels to pronouns, and the
most straightforward way of setting up annotation
projection would be to label their word-aligned En-
glish pronouns as T/V. However, pronouns are not
necessarily translated into pronouns; additionally, we
found word alignment accuracy for pronouns, as a
function of word class, to be far from perfect. For
these reasons, we decided to treat complete sentences
as either T or V. This means that sentence alignment
is sufficient for projection, but English sentences can
receive conflicting labels, if a German sentence con-
tains both a T and a V label. However, this occurs
very rarely: of the 76K German sentences with T or
V pronouns, only 515, or less than 1%, contain both.
Our projection on the English side results in 53K V
and 35K T sentences, of which 731 are labeled as
both T and V.3
Finally, from the English labeled sentences we ex-
tracted a training set with 72 novels (63K sentences)
and a test set with 21 novels (15K sentences).4
3 Experiment 1: Human Annotation
The purpose of our first experiment is to investigate
how well the T/V distinction can be made in English
by human raters, and on the basis of what information.
We extracted 100 random sentences from the training
set. Two annotators with advanced knowledge of
2An initial position is defined as a position after a sentence
boundary (POS ?$.?) or after a bracket (POS ?$(?).
3Our sentence aligner supports one-to-many alignments and
often aligns single German to multiple English sentences.
4The corpus can be downloaded for research purposes from
http://www.nlpado.de/~sebastian/data.shtml.
Acc (Ann1) Acc (Ann2) IAA
No context 63 65 68
In context 70 69 81
Table 2: Manual annotation for T/V on a 100-sentence
sample (Acc: Accuracy, IAA: Inter-annotator agreement)
English were asked to label these sentences as T or V.
In a first round, the sentences were presented in isola-
tion. In a second round, the sentences were presented
with three sentences pre-context and three sentences
post-context. The results in Table 2 show that it is
fairly difficult to annotate the T/V distinction on indi-
vidual sentences since it is not expressed systemati-
cally. At the level of small discourses, the distinction
can be made much more confidently: In context, av-
erage agreement with the gold standard rises from
64% to 70%, and raw inter-annotator agreement goes
up from 68% to 81%.
Concerning the interpretation of these findings, we
note that the two taggers were both native speakers
of languages which make an overt T/V distinction.
Thus, our present findings cannot be construed as
firm evidence that English speakers make a distinc-
tion, even if implicitly. However, they demonstrate
at least that native speakers of such languages can
recover the distinction based solely on the clues in
English text.
An analysis of the annotation errors showed that
many individual sentences can be uttered in both T
and V situations, making it impossible to label them
in isolation:
(1) ?And perhaps sometime you may see her.?
This case (gold label: V) is however disambiguated
by looking at the previous sentence, which indicates
the social relation between speaker and addressee:
(2) ?And she is a sort of relation of your lord-
ship?s,? said Dawson.
Still, a three-sentence window is often not sufficient,
since the surrounding sentences may be just as unin-
formative. In these cases, global information about
the situation would be necessary.
A second problem is the age of the texts. They are
often difficult to label because they talk about social
situations that are unfamiliar to modern speakers (as
469
between aristocratic friends) or where the usage has
changed (as in married couples).
4 Experiment 2: Statistical Modeling
Task Setup. In this pilot modeling experiment, we
explore a (limited) set of cues which can be used to
predict the V vs. T dichotomy for English sentences.
Specifically, we use local words (i.e. information
present within the current sentence ? similar to the
information available to the human annotators in the
?No context? condition of Experiment 1). We ap-
proach the task by supervised classification, applying
a model acquired from the training set on the test
set. Note, however, that the labeled training data are
acquired automatically through the parallel corpus,
without the need for human annotation.
Statistical Model. We train a Naive Bayes classi-
fier, a simple but effective model for text categoriza-
tion (Domingos and Pazzani, 1997). It predicts the
class c for a sentence s by maximising the product
of the probabilities for the features f given the class,
multiplied by the class probability:
c? = argmax
c
P (c|s) = argmax
c
P (c)P (s|c) (3)
= argmax
c
P (c)
?
f?s
P (f |c) (4)
We experiment with three sets of features. The first
set consists of words, following the intuition that
some words should be correlated with formal ad-
dress (like titles), while others should indicate infor-
mal address (like first names). The second set con-
sists of part of speech bigrams, to explore whether
this more coarse-grained, but at the same time less
sparse, information can support the T/V decision.
The third set consists of one feature that represents a
semantic class, namely a set of 25 archaic verbs and
pronouns (like hadst or thyself ), which we expect
to correlate with old-fashioned T use. All features
are computed by MLE with add-one smoothing as
P (f |c) = freq(f,c)+1freq(c)+1 .
Results. Accuracies are shown in Table 3. A ran-
dom baseline is at 50%, and the majority class (V)
corresponds to 60%. The Naive Bayes models signif-
icantly outperform the frequency baselines at up to
67.0%; however, only the difference between the best
Model Accuracy
Random BL 50.0
Frequency BL 60.1
Words 66.1
Words + POS 65.0
Words + Archaic 67.0
Human (no context) 64
Human (in context) 70
Table 3: NB classifier results for the T/V distinction
(Words+Archaic) and the worst (Words+POS) model
is significant according to a ?2 test. Thus, POS fea-
tures tend to hurt, and the archaic feature helps, even
though it technically overcounts evidence.5
The Naive Bayes model notably performs at a
roughly human level, better than human annotators
on the same setup (no context sentences), but worse
than humans that have more context at their disposal.
Overall, however, the T/V distinction appears to be a
fairly difficult one. An important part of the problem
is the absence of strong indicators in many sentences,
in particular short ones (cf. Example 1). In contrast
to most text categorization tasks, there is no topi-
cal difference between the two categories: T and V
can both co-occur with words from practically any
domain.
Table 4, which lists the top ten words for T and
V (ranked by the ratio of probabilities for the two
classes), shows that among these indicators, many
are furthermore names of persons from particular
novels which are systematically addressed formally
(like Phileas Fogg from Jules Vernes? In eighty days
around the world) or informally (like Mowgli, Baloo,
and Bagheera from Rudyard Kipling?s Jungle Book).
Nevertheless, some features point towards more
general patterns. In particular, we observe ti-
tles among the V-indicators (gentlemen, madam,
ma+?am) as well as formulaic language (Permit
(me)). Indicators for T seem to be much more general,
with the expected exception of archaic thou forms.
5 Conclusions and Future Work
In this paper, we have reported on an ongoing study
of the formal/informal (T/V) address distinction in
5We experimented with logistic regression models, but were
unable to obtain better performance, probably because we intro-
duced a frequency threshold to limit the feature set size.
470
Top 10 words for V Top 10 words for T
Word w P (w|V )P (w|T ) Word w
P (w|T )
P (w|V )
Fogg 49.7 Thee 67.2
Oswald 32.5 Trot 46.8
Ma 31.8 Bagheera 37.7
Gentlemen 25.2 Khan 34.7
Madam 24.2 Mowgli 33.2
Parfenovitch 23.2 Baloo 30.2
Monsieur 22.6 Sahib 30.2
Fix 22.5 Clare 29.7
Permit 22.5 didst 27.7
?am 22.4 Reinhard 27.2
Table 4: Words that are indicative for T or V
modern English, where it is not determined through
pronoun choice or other overt means. We see this task
as an instance of the general problem of recovering
?hidden? information that is not expressed overtly.
We have created a parallel German-English cor-
pus and have used the information provided by the
German pronouns to induce T/V labels for English
sentences. In a manual annotation study for English,
annotators find the form of address very difficult to
determine for individual sentences, but can draw this
information from broader English discourse context.
Since our annotators are not native speakers of En-
glish, but of languages that make the T/V distinction,
we can conclude that English provides lexical cues
that can be interpreted as to the form of address, but
cannot speak to the question whether English speak-
ers in fact have a concept of this distinction.
In a first statistical analysis, we found that lexical
cues from the sentence can be used to predict the
form of address automatically, although not yet on a
very satisfactory level.
Our analyses suggest a number of directions for
future research. On the technical level, we would like
to apply a sequence model to account for the depen-
decies among sentences, and obtain more meaningful
features for formal and informal address. In order
to remove idiosyncratic features like names, we will
only consider features that occur in several novels;
furthermore, we will group words using distributional
clustering methods (Clark, 2003) and predict T/V
based on cluster probabilities.
The conceptually most promising direction, how-
ever, is the induction of social networks in such nov-
els (Elson et al, 2010): Information on the social re-
lationship between a speaker and an addressee should
provide global constraints on all instances of com-
munications between them, and predict the form of
address much more reliably than word features can.
Acknowledgments
Manaal Faruqui has been partially supported by a
Microsoft Research India Travel Grant.
References
John Ardila. 2003. (Non-Deictic, Socio-Expressive) T-
/V-Pronoun Distinction in Spanish/English Formal Lo-
cutionary Acts. Forum for Modern Language Studies,
39(1):74?86.
John A. Bateman. 1988. Aspects of clause politeness in
japanese: An extended inquiry semantics treatment. In
Proceedings of the 26th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 147?154,
Buffalo, New York.
Fabienne Braune and Alexander Fraser. 2010. Improved
unsupervised sentence alignment for symmetrical and
asymmetrical parallel corpora. In Coling 2010: Posters,
pages 81?89, Beijing, China.
Roger Brown and Albert Gilman. 1960. The pronouns
of power and solidarity. In Thomas A. Sebeok, editor,
Style in Language, pages 253?277. MIT Press, Cam-
bridge, MA.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 59?66, Budapest, Hungary.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple Bayesian classifier under zero-
one loss. Machine Learning, 29(2?3):103?130.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138?147,
Uppsala, Sweden.
Alexander Fraser. 2009. Experiments in morphosyntactic
processing for translating to and from German. In Pro-
ceedings of the Fourth Workshop on Statistical Machine
Translation, pages 115?119, Athens, Greece.
Jerry Hobbs and Megumi Kameyama. 1990. Translation
by abduction. In Proceedings of the 13th International
Conference on Computational Linguistics, Helsinki,
Finland.
471
Hiroshi Kanayama. 2003. Paraphrasing rules for auto-
matic evaluation of translation into japanese. In Pro-
ceedings of the Second International Workshop on Para-
phrasing, pages 88?93, Sapporo, Japan.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Proceedings of the 10th
Machine Translation Summit, pages 79?86, Phuket,
Thailand.
Heinz L. Kretzenbacher, Michael Clyne, and Doris Sch?p-
bach. 2006. Pronominal Address in German: Rules,
Anarchy and Embarrassment Potential. Australian Re-
view of Applied Linguistics, 39(2):17.1?17.18.
Alexander K?nzli. 2010. Address pronouns as a problem
in French-Swedish translation and translation revision.
Babel, 55(4):364?380.
Zhifei Li and David Yarowsky. 2008. Mining and mod-
eling relations between formal and informal Chinese
phrases from web corpora. In Proceedings of the 2008
Conference on Empirical Methods in Natural Language
Processing, pages 1031?1040, Honolulu, Hawaii.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Michael Schiehlen. 1998. Learning tense translation
from bilingual corpora. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Com-
putational Linguistics, pages 1183?1187, Montreal,
Canada.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Doris Sch?pbach, John Hajek, Jane Warren, Michael
Clyne, Heinz Kretzenbacher, and Catrin Norrby. 2006.
A cross-linguistic comparison of address pronoun use in
four European languages: Intralingual and interlingual
dimensions. In Proceedings of the Annual Meeting of
the Australian Linguistic Society, Brisbane, Australia.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the 2nd Meeting of the North American Chapter of
the Association of Computational Linguistics, pages
200?207, Pittsburgh, PA.
472
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 777?783,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Information Theoretic Approach to Bilingual Word Clustering
Manaal Faruqui and Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui, cdyer}@cs.cmu.edu
Abstract
We present an information theoretic objec-
tive for bilingual word clustering that in-
corporates both monolingual distributional
evidence as well as cross-lingual evidence
from parallel corpora to learn high qual-
ity word clusters jointly in any number of
languages. The monolingual component
of our objective is the average mutual in-
formation of clusters of adjacent words in
each language, while the bilingual com-
ponent is the average mutual information
of the aligned clusters. To evaluate our
method, we use the word clusters in an
NER system and demonstrate a statisti-
cally significant improvement in F1 score
when using bilingual word clusters instead
of monolingual clusters.
1 Introduction
A word cluster is a group of words which ideally
captures syntactic, semantic, and distributional
regularities among the words belonging to the
group. Word clustering is widely used to reduce
the number of parameters in statistical models
which leads to improved generalization (Brown et
al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo
et al, 2008; Turian et al, 2010), and multilingual
clustering has been proposed as a means to im-
prove modeling of translational correspondences
and to facilitate projection of linguistic resource
across languages (Och, 1999; Ta?ckstro?m et al,
2012). In this paper, we argue that generally more
informative clusters can be learned when evidence
from multiple languages is considered while cre-
ating the clusters.
We propose a novel bilingual word clustering
objective (?2). The first term deals with each
language independently and ensures that the data
is well-explained by the clustering in a sequence
model (?2.1). The second term ensures that the
cluster alignments induced by a word alignment
have high mutual information across languages
(?2.2). Since the objective consists of terms rep-
resenting the entropy monolingual data (for each
language) and parallel bilingual data, it is partic-
ularly attractive for the usual situation in which
there is much more monolingual data available
than parallel data. Because of its similarity to the
variation of information metric (Meila?, 2003), we
call this bilingual term in the objective the aligned
variation of information.
2 Word Clustering
A word clustering C is a partition of a vocabulary
? = {x1, x2, . . . , x|?|} into K disjoint subsets,
C1, C2, . . . , CK . That is, C = {C1, C2, . . . , CK};
Ci ? Cj = ? for all i 6= j and ?Kk=1Ck = ?.
2.1 Monolingual objective
We use the average surprisal in a probabilistic se-
quence model to define the monolingual clustering
objective. Let ci denote the word class of word
wi. Our objective assumes that the probability of
a word sequence w = ?w1, w2, . . . , wM ? is
p(w) =
M?
i=1
p(ci | ci?1)? p(wi | ci), (2.1)
where c0 is a special start symbol. The term p(ci |
ci?1) is the probability of class ci following class
ci?1, and p(wi | ci) is the probability of class ci
emitting word wi. Using the MLE esitmates after
taking the negative logarithm, this term reduces to
777
the following as shown in (Brown et al, 1992):
H(C;w) = 2
K?
k=1
#(Ck)
M log
#(Ck)
M
?
?
i
?
j 6=i
#(Ci, Cj)
M log
#(Ci, Cj)
M
where #(Ck) is the count of Ck in the corpus w
under the clustering C, #(Ci, Cj) is the count of
the number of times that cluster Ci precedes Cj
and M is the size of the corpus. Using the mono-
lingual objective to cluster, we solve the following
search problem:
C? = arg min
C
H(C;w). (2.2)
2.2 Bilingual objective
Now let us suppose we have a second lan-
guage with vocabulary ? = {y1, y2, . . . , y|?|},
which is clustered into K disjoint subsets D =
{D1, D2, . . . , DK}, and a corpus of text in the
second language, v = ?v1, v2, . . . , vN ?. Obvi-
ously we can cluster both languages using the
monolingual objective above:
C?, D? = arg min
C,D
H(C;w) +H(D; v).
This joint minimization for the clusterings for both
languages clearly has no benefit since the two
terms of the objective are independent. We must
alter the object by further assuming that we have
a priori beliefs that some of the words in w and v
have the same meaning.
To encode this belief, we introduce the notion
of a weighted vocabulary alignment A, which is
a function on pairs of words in vocabularies ? and
? to a value greater than or equal to 0, i.e., A :
?? ? 7? R?0. For concreteness, A(x, y) will be
the number of times that x is aligned to y in a word
aligned parallel corpus. By abuse of notation, we
write marginal weights A(x) = ?y??A(x, y)
and A(y) = ?x??A(x, y). We also define the
set marginals A(C,D) = ?x?C
?
y?DA(x, y).Using this weighted vocabulary alignment, we
state an objective that encourages clusterings to
have high average mutual information when align-
ment links are followed; that is, on average how
much information does knowing the cluster of a
word x ? ? impart about the clustering of y ? ?,
and vice-versa?
C DC
Figure 1: Factor graphs of the monolingual (left)
& proposed bilingual clustering problem (right).
We call this quantity the aligned variation of
information (AVI).
AVI(C,D;A) =
EA(x,y) [? log p(cx | dy)? log p(dy | cx)]
Writing out the expectation and gathering terms,
we obtain
AVI(C,D;A) = ?
?
x??
?
y??
A(x, y)
A(?, ?) ?
[
2 log A(C,D)A(?, ?) ? log p(C)? log p(D)
]
,
where it is assumed that 0 log x = 0.
Our bilingual clustering objective can therefore
be stated as the following search problem over a
linear combination of the monolingual and bilin-
gual objectives:
arg min
C,D
monolingual? ?? ?
H(C;w) +H(D; v) +
??bilingual? ?? ?
?AVI(C,D) .
(2.3)
Understanding AVI. Intuitively, we can imag-
ine sampling a random alignment from the distri-
bution obtained by normalizing A(?, ?). AVI gives
us a measure of how much information do we ob-
tain, on average, from knowing the cluster in one
language about the clustering of a linked element
chosen at random proportional to A(x, ?) (or con-
ditioned the other way around). In the following
sections, we denote AVI(C,D;A) by AVI(C,D).
To further understand AVI, we remark that AVI re-
duces to the VI metric when the alignment maps
words to themselves in the same language. As a
proper metric, VI has a number of attractive prop-
erties, and these can be generalized to AVI (with-
out restriction on the alignment map), namely:
? Non-negativity: AVI(C,D) ? 0;
? Symmetry: AVI(C,D) = AVI(D,C);
? Triangle inequality:
AVI(C,D) + AVI(D,E) ? AVI(C,E);
778
? Identity of indiscernables:
AVI(C,D) = 0 iff C ? D.1
2.3 Example
Figure 2 provides an example illustrating the dif-
ference between the bilingual vs. monolingual
clustering objectives. We compare two different
clusterings of a two-sentence Arabic-English par-
allel corpus (the English half of the corpus con-
tains the same sentence, twice, while the Ara-
bic half has two variants with the same mean-
ing). While English has a relatively rigid SVO
word order, Arabic can alternate between the tradi-
tional VSO order and an more modern SVO order.
Since our monolingual clustering objective relies
exclusively on the distribution of clusters before
and after each token, flexible word order alterna-
tions like this can cause unintuitive results. To
further complicate matters, verbs can inflect dif-
ferently depending on whether their subject pre-
cedes or follows them (Haywood and Nahmad,
1999), so a monolingual model, which knows
nothing about morphology and may only rely
on distributional clues, has little chance of per-
forming well without help. This is indeed what
we observe in the monolingual objective opti-
mal solution (center), in which AwlAd (boys) and
yElbwn (play+PRES + 3PL) are grouped into a
single class, while yElb (play+PRES + 3SG) is in
its own class. However, the AVI term (which is of
course not included) has a value of 1.0, reflecting
the relatively disordered clustering relative to the
given alignment. On the right, we see the optimal
solution that includes the AVI term in the cluster-
ing objective. This has an AVI of 0, indicating that
knowing the clustering of any word is completely
informative about the words it is aligned to. By in-
cluding this term, a slightly worse monolingual so-
lution is chosen, but the clustering corresponds to
the reasonable intuition that words with the same
meaning (i.e., the two variants of to play) should
be clustered together.
2.4 Inference
Figure 1 shows the factor graph representation
of our clustering models. Finding the optimal
clustering under both the monolingual and bilin-
gual objectives is a computationally hard combi-
natorial optimization problem (Och, 1995). We
use a greedy hill-climbing word exchange algo-
rithm (Martin et al, 1995) to find a minimum
1C ? D iff ?i|{D(y)|?(x, y) ? A, C(x) = i}| = 1
value for our objective. We terminate the opti-
mization procedure when the number of words
exchanged at the end of one complete iteration
through both the languages is less than 0.1% of
the sum of vocabulary of the two languages and
at least five complete iterations have been com-
pleted.2 For every language the word clusters are
initialised in a round robin order according to the
token frequency.
3 Experiments
Evaluation of clustering is not a trivial problem.
One branch of work seeks to recast the problem
as the of part-of-speech (POS) induction and at-
tempts to match linguistic intuitions. However,
hard clusters are particularly useful for down-
stream tasks (Turian et al, 2010). We therefore
chose to focus our evaluation on the latter prob-
lem. For our evaluation, we use our word clusters
as an input to a named entity recognizer which
uses these clusters as a source of features. Our
evaluation task is the German corpus with NER
annotation that was created for the shared task
at CoNLL-2003 3. The training set contains ap-
proximately 220,000 tokens and the development
set and test set contains 55,000 tokens each. We
use Stanford?s Named Entity Recognition system4
which uses a linear-chain conditional random field
to predict the most likely sequence of NE la-
bels (Finkel and Manning, 2009).
Corpora for Clustering: We used parallel cor-
pora for {Arabic, English, French, Korean &
Turkish}-German pairs from WIT-3 corpus (Cet-
tolo et al, 2012) 5, which is a collection of trans-
lated transcriptions of TED talks. Each language
pair contained around 1.5 million German words.
The corpus was word aligned in two directions
using an unsupervised word aligner (Dyer et al,
2013), then the intersected alignment points were
taken.
Monolingual Clustering: For every language
pair, we train German word clusters on the mono-
lingual German data from the parallel data. Note
that the parallel corpora are of different sizes and
hence the monolingual German data from every
parallel corpus is different. We treat the F1 score
2In practice, the number of exchanged words drops of exponentially,
so this threshold is typically reached in not many iterations.
3http://www.cnts.ua.ac.be/conll2003/ner/
4http://nlp.stanford.edu/ner/index.shtml
5https://wit3.fbk.eu/mt.php?release=2012-03
779
The boys are playing
Al- AwlAd ylEbwn
Al- AwlAdylEb
) ?*()'& ?$?"? (
) &'(, ?$?"? (
ylEb
ylEbwn
Al-
AwlAd
are
playing
boys
The
Al-
ylEb
are
playing
boys
The
AwlAd
ylEbwn
H(D;v) = 4H(C;w) = 4.56
H(C;w) = 4H(D;v) = 3.88
H(C;w) + H(D;v)= 8.56H(C;w) + H(D;v)= 7.88
AVI(C,D)= 0AVI(C,D)= 1.0
Figure 2: A two-sentence English-Arabic parallel corpus (left); a 3-class clustering that maximizes the
monolingual objective (? = 0; center); and a 3-class clustering that maximizes the joint monolingual
and bilingual objective (any ? > 0.68; right).
obtained using monolingual word clusters (? = 0)
as the baseline. Table 1 shows the F1 score of
NER6 when trained on these monolingual German
word clusters.
Bilingual Clustering: While we have formu-
lated a joint objective that enables using both
monolingual and bilingual evidence, it is possible
to create word clusters using the bilingual signal
only by removing the first term in Eq. 2.3. Ta-
ble 1 shows the performance of NER when the
word clusters are obtained using only the bilingual
information for different language pairs. As can
be seen, these clusters are helpful for all the lan-
guage pairs. For Turkish the F1 score improves
by 1.0 point over when there are no distributional
clusters which clearly shows that the word align-
ment information improves the clustering quality.
We now need to supplement the bilingual infor-
mation with monolingual information to see if the
improvement sustains.
We varied the weight of the bilingual objec-
tive (?) from 0.05 to 0.9 and observed the ef-
fect in NER performance on English-German lan-
guage pair. The F1 score is maximum for ? =
0.1 and decreases monotonically when ? is ei-
ther increased or decreased. This indicates that
bilingual information is helpful, but less valuable
than monolingual information. Preliminary exper-
iments showed that the value of ? = 0.1 is fairly
robust across other language pairs and hence we
fix it to that for all the experiments.
We run our bilingual clustering model (? =
6Faruqui and Pado? (2010) show that for the size of our generalization
data in German-NER, K = 100 should give us the optimum value.
0.1) across all language pairs and note the F1
scores. Table 1 (unrefined) shows that except for
Arabic-German & French-German, all other lan-
guage pairs deliver a better F1 score than only us-
ing monolingual German data. In case of Arabic-
German there is a drop in score by 0.25 points.
Although, we have observed improvement in F1
score over the monolingual case, the gains do
not reach significance according to McNemar?s
test (Dietterich, 1998).
Thus we propose to further refine the quality of
word alignment links as follows: Let x be a word
in language ? and y be a word in language ? and
let there exists an alignment link between x and
y. Recall that A(x, y) is the count of the align-
ment links between x and y observed in the par-
allel data, and A(x) and A(y) are the respective
marginal counts. Then we define an edge associ-
ation weight e(x, y) = 2?A(x,y)A(x)+A(y) This quantityis an association of the strength of the relationship
between x and y, and we use it to remove all align-
ment links whose e(x, y) is below a given thresh-
old before running the bilingual clustering model.
We vary e from 0.1 to 0.7 and observe the new F1
scores on the development data. Table 1 (refined)
shows the results obtained by our refined model.
The values shown in bold are the highest improve-
ments over the monolingual model.
For English and Turkish we observe a statisti-
cally significant improvement over the monolin-
gual model (cf. Table 1) with p < 0.007 and
p < 0.001 according to McNemar?s test. Ara-
bic improves least with just an improvement of
0.02 F1 points over the monolingual baseline. We
780
Dev Test
Language Pair ? ? = 0 ? = 0.1 ? = 0.1 ? = 0 ? = 0.1(only bi) (only mono) (unrefined) (refined) (only mono) (refined)
No clusters 68.27 72.32
En-De 68.95 70.04 70.33 70.64? 72.30 72.98?
Fr-De 69.16 69.74 69.69 69.89 72.66 72.83
Ar-De 69.01 69.65 69.40 69.67 72.90 72.37
Tr-De 69.29 69.46 69.64 70.05? 72.41 72.54
Ko-De 68.95 69.70 69.78 69.95 72.71 72.54
Average 69.07 69.71 69.76 70.04? 72.59 72.65
Table 1: NER performance using different word clustering models. Bold indicates an improvement over
the monolingual (? = 0) baseline; ? indicates a significant improvement (McNemar?s test, p < 0.01).
see that the optimal value of e changes from one
language pair to another. For French and English
e = 0.1 gives the best results whereas for Turk-
ish and Arabic e = 0.5 and for Korean e = 0.7.
Are these thresholds correlated with anything? We
suggest that higher values of e correspond to more
intrinsically noisy alignments. Since alignment
models are parameterized based on the vocabu-
laries of the languages they are aligning, larger
vocabularies are more prone to degenerate solu-
tions resulting from overfitting. So we are not
surprised to see that sparser alignments (resulting
from higher values of e) are required by languages
like Korean, while languages like French and En-
glish make due with denser alignments.
Evaluation on Test Set: We now verify our re-
sults on the test set. We take the best bilin-
gual word clustering model obtained for every lan-
guage pair (e = 0.1 for En, Fr. e = 0.5 for Ar,
Tr. e = 0.7 for Ko) and train NER classifiers
using these. Table 1 shows the performance of
German NER classifiers on the test set. All the
values shown in bold are better than the mono-
lingual baselines. English again has a statistically
significant improvement over the baseline. French
and Turkish show the next best improvements.
The English-German cluster model performs bet-
ter than the mkcls7 tool (72.83%).
4 Related Work
Our monolingual clustering model is purely distri-
butional in nature. Other extensions to word clus-
tering have incorporated morphological and or-
thographic information (Clark, 2003). The work
of Snyder and Barzilay (2010), which focused on
POS induction is very closely related. The ear-
liest work on bilingual word clustering was pro-
posed by (Och, 1999) which, like us, uses a lan-
7http://www.statmt.org/moses/giza/mkcls.html
guage modeling approach (Brown et al, 1992;
Kneser and Ney, 1993) for monolingual optimiza-
tion and a similarity function for bilingual simi-
larity. Ta?ckstro?m et al (2012) use cross-lingual
word clusters to show transfer of linguistic struc-
ture. While their clustering method is superficially
similar, the objective function is more heuristic in
nature than our information-theoretic conception
of the problem. Multilingual learning has been
applied to a number of unsupervised and super-
vised learning problems, including word sense dis-
ambiguation (Diab, 2003; Guo and Diab, 2010),
topic modeling (Mimno et al, 2009; Boyd-Graber
and Blei, 2009), and morphological segmenta-
tion (Snyder and Barzilay, 2008).
Also closely related is the technique of cross-
lingual annotation projection. This has been
applied to bootstrapping syntactic parsers (Hwa
et al, 2005; Smith and Smith, 2007; Co-
hen et al, 2011), morphology (Fraser, 2009),
tense (Schiehlen, 1998) and T/V pronoun us-
age (Faruqui and Pado?, 2012).
5 Conclusions
We presented a novel information theoretic model
for bilingual word clustering which seeks a clus-
tering with high average mutual information be-
tween clusters of adjacent words, and also high
mutual information across observed word align-
ment links. We have shown that improvement in
clustering can be obtained across a range of lan-
guage pairs, evaluated in terms of their value as
features in an extrinsic NER task. Our model can
be extended for clustering any number of given
languages together in a joint framework, and in-
corporate both monolingual and parallel data.
Acknowledgement: We woud like to thank W.
Ammar, V. Chahuneau and W. Ling for valuable
discussions.
781
References
J. Boyd-Graber and D. M. Blei. 2009. Multilingual
topic models for unaligned text. In Proceedings of
the Twenty-Fifth Conference on Uncertainty in Arti-
ficial Intelligence, UAI ?09, pages 75?82, Arlington,
Virginia, United States. AUAI Press.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,
and J. C. Lai. 1992. Class-based n-gram models
of natural language. Comput. Linguist., 18(4):467?
479, December.
M. Cettolo, C. Girardi, and M. Federico. 2012. Wit3:
Web inventory of transcribed and translated talks. In
Proceedings of the 16th Conference of the European
Association for Machine Translation (EAMT), pages
261?268, Trento, Italy, May.
A. Clark. 2003. Combining distributional and mor-
phological information for part of speech induction.
In Proceedings of the tenth conference on Euro-
pean chapter of the Association for Computational
Linguistics - Volume 1, EACL ?03, pages 59?66,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 50?61, Stroudsburg, PA,
USA. Association for Computational Linguistics.
M. T. Diab. 2003. Word sense disambiguation within a
multilingual framework. Ph.D. thesis, University of
Maryland at College Park, College Park, MD, USA.
AAI3115805.
T. G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning al-
gorithms. Neural Computation, 10:1895?1923.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013.
A simple, fast, and effective reparameterization of
IBM Model 2. In Proc. NAACL.
M. Faruqui and S. Pado?. 2010. Training and Evalu-
ating a German Named Entity Recognizer with Se-
mantic Generalization. In Proceedings of KON-
VENS 2010, Saarbru?cken, Germany.
M. Faruqui and S. Pado?. 2012. Towards a model of for-
mal and informal address in english. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1 - Volume 1, EMNLP ?09,
pages 141?150, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Fraser. 2009. Experiments in morphosyntactic pro-
cessing for translating to and from German. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 115?119, Athens, Greece,
March. Association for Computational Linguistics.
W. Guo and M. Diab. 2010. Combining orthogonal
monolingual and multilingual sources of evidence
for all words wsd. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?10, pages 1542?1551, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
J. A. Haywood and H. M. Nahmad. 1999. A new
Arabic grammar of the written language. Lund
Humphries Publishers.
R. Hwa, P. Resnik, A. Weinberg, C. I. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural Language
Engineering, pages 311?325.
R. Kneser and H. Ney. 1993. Forming word classes
by statistical clustering for statistical language mod-
elling. In R. Khler and B. Rieger, editors, Contri-
butions to Quantitative Linguistics, pages 221?226.
Springer Netherlands.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proc. of
ACL.
S. Martin, J. Liermann, and H. Ney. 1995. Algorithms
for bigram and trigram word clustering. In Speech
Communication, pages 1253?1256.
M. Meila?. 2003. Comparing Clusterings by the Varia-
tion of Information. In Learning Theory and Kernel
Machines, pages 173?187.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A.
Smith, and A. McCallum. 2009. Polylingual topic
models. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 2 - Volume 2, EMNLP ?09, pages 880?
889, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
F. J. Och. 1995. Maximum-Likelihood-Scha?tzung von
Wortkategorien mit Verfahren der kombinatorischen
Optimierung. Studienarbeit, University of Erlangen.
F. J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 71?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
M. Schiehlen. 1998. Learning tense translation from
bilingual corpora.
D. A. Smith and N. A. Smith. 2007. Probabilis-
tic Models of Nonprojective Dependency Trees.
In Proceedings of the 2007 Joint Conference on
782
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 132?140, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
B. Snyder and R. Barzilay. 2008. Unsupervised mul-
tilingual learning for morphological segmentation.
In In The Annual Conference of the Association for
Computational Linguistics.
B. Snyder and R. Barzilay. 2010. Climbing the tower
of babel: Unsupervised multilingual learning. In
J. Frnkranz and T. Joachims, editors, Proceedings
of the 27th International Conference on Machine
Learning (ICML-10), June 21-24, 2010, Haifa, Is-
rael, pages 29?36. Omnipress.
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of
linguistic structure. In The 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, volume 1, page 11. Association for Com-
putational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 384?394, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
783
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 19?24,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Community Evaluation and Exchange of Word Vectors
at wordvectors.org
Manaal Faruqui and Chris Dyer
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui, cdyer}@cs.cmu.edu
Abstract
Vector space word representations are use-
ful for many natural language process-
ing applications. The diversity of tech-
niques for computing vector representa-
tions and the large number of evaluation
benchmarks makes reliable comparison a
tedious task both for researchers devel-
oping new vector space models and for
those wishing to use them. We present
a website and suite of offline tools that
that facilitate evaluation of word vectors
on standard lexical semantics benchmarks
and permit exchange and archival by users
who wish to find good vectors for their
applications. The system is accessible at:
www.wordvectors.org.
1 Introduction
Data-driven learning of vector-space word embed-
dings that capture lexico-semantic properties is
a technique of central importance in natural lan-
guage processing. Using co-occurrence statistics
from a large corpus of text (Deerwester et al.,
1990; Turney and Pantel, 2010), it is possible
to construct high-quality semantic vectors ? as
judged by both correlations with human judge-
ments of semantic relatedness (Turney, 2006;
Agirre et al., 2009) and as features for down-
stream applications (Turian et al., 2010). A num-
ber of approaches that use the internal representa-
tions from models of word sequences (Collobert
and Weston, 2008) or continuous bags-of-context
wordsets (Mikolov et al., 2013) to arrive at vector
representations have also been shown to likewise
capture co-occurrence tendencies and meanings.
With an overwhelming number of techniques
to obtain word vector representations the task of
comparison and choosing the vectors best suitable
for a particular task becomes difficult. This is
further aggravated by the large number of exist-
ing lexical semantics evaluation benchmarks be-
ing constructed by the research community. For
example, to the best of our knowledge, for evaluat-
ing word similarity between a given pair of words,
there are currently at least 10 existing bench-
marks
1
that are being used by researchers to prove
the effectiveness of their word vectors.
In this paper we describe an online application
that provides the following utilities:
? Access to a suite of word similarity evalua-
tion benchmarks
? Evaluation of user computed word vectors
? Visualizing word vectors in R
2
? Evaluation and comparison of the available
open-source vectors on the suite
? Submission of user vectors for exhaustive of-
fline evaluation and leader board ranking
? Publicly available repository of word vectors
with performance details
Availability of such an evaluation system will
help in enabling better consistency and uniformity
in evaluation of word vector representations as
well as provide an easy to use interface for end-
users in a similar spirit to Socher et al. (2013a),
a website for text classification.
2
Apart from the
online demo version, we also provide a software
that can be run in an offline mode on the command
line. Both the online and offline tools will be kept
updated with continuous addition of new relevant
tasks and vectors.
1
www.wordvectors.org/suite.php
2
www.etcml.com
19
2 Word Similarity Benchmarks
We evaluate our word representations on 10 dif-
ferent benchmarks that have been widely used to
measure word similarity. The first one is the WS-
353
3
dataset (Finkelstein et al., 2001) containing
353 pairs of English words that have been assigned
similarity ratings by humans. This data was fur-
ther divided into two fragments by Agirre et al.
(2009) who claimed that similarity (WS-SIM) and
relatedness (WS-REL)
4
are two different kinds
of relations and should be dealt with separately.
The fourth and fifth benchmarks are the RG-65
(Rubenstein and Goodenough, 1965) and the MC-
30 (Miller and Charles, 1991) datasets that contain
65 and 30 pairs of nouns respectively and have
been given similarity rankings by humans. These
differ from WS-353 in that it contains only nouns
whereas the former contains all kinds of words.
The sixth benchmark is the MTurk-287
5
(Radinsky et al., 2011) dataset that constitutes 287
pairs of words and is different from the previ-
ous benchmarks in that it has been constructed
by crowdsourcing the human similarity ratings
using Amazon Mechanical Turk (AMT). Simi-
lar in spirit is the MTruk-771
6
(Halawi et al.,
2012) dataset that contains 771 word pairs whose
similarity was crowdsourced from AMT. An-
other, AMT created dataset is the MEN
7
bench-
mark (Bruni et al., 2012) that consists of 3000
word pairs, randomly selected from words that
occur at least 700 times in the freely available
ukWaC and Wackypedia
8
corpora combined.
The next two benchmarks were created to put
emphasis on different kinds of word types. To
specifically emphasize on verbs, Yang and Pow-
ers (2006) created a new benchmark YP-130 of
130 verb pairs with human similarity judgements.
Since, most of the earlier discussed datasets con-
tain word pairs that are relatively more frequent in
a corpus, Luong et al. (2013) create a new bench-
3
http://www.cs.technion.ac.il/
?
gabr/
resources/data/wordsim353/
4
http://alfonseca.org/eng/research/
wordsim353.html
5
http://tx.technion.ac.il/
?
kirar/
Datasets.html
6
http://www2.mta.ac.il/
?
gideon/
mturk771.html
7
http://clic.cimec.unitn.it/
?
elia.
bruni/MEN.html
8
http://wacky.sslmit.unibo.it/doku.
php?id=corpora
mark (Rare-Word)
9
that contains rare-words by
sampling words from different frequency bins to a
total of 2034 word pairs.
We calculate similarity between a given pair
of words by the cosine similarity between their
corresponding vector representation. We then re-
port Spearman?s rank correlation coefficient (My-
ers and Well, 1995) between the rankings pro-
duced by our model against the human rankings.
Multilingual Benchmarks. As is the case with
most NLP problems, the lexical semantics evalua-
tion benchmarks for languages other than English
have been limited. Currently, we provide a link
to some of these evaluation benchmarks from our
website and in future will expand the website to
encompass vector evaluation for other languages.
3 Visualization
The existing benchmarks provide ways of vector
evaluation in a quantitative setting. To get an idea
of what kind of information the vectors encode it is
important to see how these vectors represent words
in n-dimensional space, where n is the length
of the vector. Visualization of high-dimensional
data is an important problem in many different do-
mains, and deals with data of widely varying di-
mensionality. Over the last few decades, a variety
of techniques for the visualization of such high-
dimensional data have been proposed (de Oliveira
and Levkowitz, 2003).
Since visualization in n dimensions is hard
when n >= 3, we use the t-SNE (van der Maaten
and Hinton, 2008) tool
10
to project our vectors into
R
2
. t-SNE converts high dimensional data set into
a matrix of pairwise similarities between individ-
ual elements and then provides a way to visual-
ize these distances in a way which is capable of
capturing much of the local structure of the high-
dimensional data very well, while also revealing
global structure such as the presence of clusters at
several scales.
In the demo system, we give the user an option
to input words that they need to visualize which
are fed to the t-SNE tool and the produced images
are shown to the user on the webpage. These im-
ages can then be downloaded and used. We have
9
http://www-nlp.stanford.edu/
?
lmthang/
morphoNLM/
10
http://homepage.tudelft.nl/19j49/
t-SNE_files/tsne_python.zip
20
Figure 1: Antonyms (red) and synonyms (green) of beautiful represented by Faruqui and Dyer (2014)
(left) and Huang et al. (2012) (right).
included two datasets by default which exhibit dif-
ferent properties of the language:
? Antonyms and synonyms of beautiful
? Common male-female nouns and pronouns
In the first plot, ideally the antonyms (ugly,
hideous, . . . ) and synonyms (pretty, gorgeous,
. . . ) of beautiful should form two separate clus-
ters in the plot. Figure 1 shows the plots of the
antonyms and synonyms of the word beautiful for
two available embeddings. The second default
word plot is the gender data set, every word in
which has a male and a female counterpart (ex.
grandmother and grandfather), this data set ex-
hibits both local and global properties. Locally,
the male and female counterparts should occur in
pairs together and globally there should be two
separate clusters of male and female.
4 Word Vector Representations
4.1 Pre-trained Vectors
We haves collected several standard pre-trained
word vector representations freely available for re-
search purposes and provide a utility for the user
to test them on the suite of benchmarks, as well
as try out the visualization functionality. The user
can also choose the option to choose two different
types of word vectors and compare their perfor-
mance on the benchmarks. We will keep adding
word vectors on the website as and when they are
released. The following word vectors have been
included in our collection:
Metaoptimize. These word embeddings
11
have
been trained in (Turian et al., 2010) using a neu-
ral network language model and were shown to
be useful for named entity recognition (NER) and
phrase chunking.
SENNA. It is a software
12
which outputs a host
of predictions: part-of-speech (POS) tags, chunk-
ing, NER etc (Collobert et al., 2011). The soft-
ware uses neural word embeddings trained over
Wikipedia data for over 2 months.
RNNLM. The recurrent neural network lan-
guage modeling toolkit
13
comes with some
pre-trained embeddings on broadcast news
data (Mikolov et al., 2011).
Global Context. Huang et al. (2012) present a
model to incorporate document level information
into embeddings to generate semantically more in-
formed word vector representations. These em-
beddings
14
capture both local and global context
of the words.
Skip-Gram. This model is a neural network lan-
guage model except for that it does not have a
hidden layer and instead of predicting the target
word, it predicts the context given the target word
(Mikolov et al., 2013). These embeddings are
much faster to train
15
than the other neural em-
beddings.
11
http://metaoptimize.com/projects/
wordreprs/
12
http://ronan.collobert.com/senna/
13
http://rnnlm.org/
14
http://nlp.stanford.edu/
?
socherr/
ACL2012_wordVectorsTextFile.zip
15
https://code.google.com/p/word2vec/
21
Figure 2: Vector selection interface (right) of the demo system.
Multilingual. Faruqui and Dyer (2014) propose
a method based on canonical correlation analy-
sis to produce more informed monolingual vec-
tors using multilingual knowledge. Their method
is shown to perform well for both neural embed-
dings and LSA (Deerwester et al., 1990) based
vectors.
16
4.2 User-created Vectors
Our demo system provides the user an option to
upload their word vectors to perform evaluation
and visualization. However, since the size of the
word vector file will be huge due to a lot of in-
frequent words that are not useful for evaluation,
we give an option to filter the word vectors file
to only include the words required for evaluation.
The script and the vocabulary file can be found on
the website online.
5 Offline Evaluation & Public Access
We provide an online portal where researchers can
upload their vectors which are then be evaluated
on a variety of NLP tasks and then placed on the
leader board.
17
The motivation behind creating
such a portal is to make it easier for a user to se-
lect the kind of vector representation that is most
suitable for their task. In this scenario, instead of
asking the uploader to filter their word vectors for
a small vocabulary, they will be requested to up-
load their vectors for the entire vocabulary.
16
http://cs.cmu.edu/
?
mfaruqui/soft.html
17
We provide an initial list of some such tasks to which we
will later add more tasks as they are developed.
5.1 Offline Evaluation
Syntactic & semantic relations. Mikolov et al.
(2013) present a new semantic and syntactic re-
lation dataset composed of analogous word pairs
of size 8869 and 10675 pairs resp.. It contains
pairs of tuples of word relations that follow a com-
mon relation. For example, in England : Lon-
don :: France : Paris, the two given pairs of words
follow the country-capital relation. We use the
vector offset method (Mikolov et al., 2013) to
compute the missing word in these relations. This
is non-trivial |V |-way classification task where V
is the size of the vocabulary.
Sentence Completion. The Microsoft Research
sentence completion dataset contains 1040 sen-
tences from each of which one word has been re-
moved. The task is to correctly predict the miss-
ing word from a given list of 5 other words per
sentence. We average the word vectors of a given
sentence q
sent
=
?
N
i=1,i 6=j
q
w
i
/N , where w
j
is
the missing word and compute the cosine similar-
ity of q
sent
vector with each of the options. The
word with the highest similarity is chosen as the
missing word placeholder.
Sentiment Analysis Socher et al. (2013b) have
created a treebank which contains sentences an-
notated with fine-grained sentiment labels on both
the phrase and sentence level. They show that
compositional vector space models can be used
to predict sentiment at these levels with high ac-
curacy. The coarse-grained treebank, containing
only positive and negative classes has been split
into training, development and test datasets con-
22
Figure 3: Screenshot of the command line version showing word similarity evaluation.
taining 6920, 872 and 1821 sentences respectively.
We train a logistic regression classifier with L2
regularization on the average of the word vectors
of a given sentence to predict the coarse-grained
sentiment tag at the sentence level.
TOEFL Synonyms. These are a set of 80 ques-
tions compiled by Landauer and Dutnais (1997),
where a given word needs to be matched to its
closest synonym from 4 given options. A num-
ber of systems have reported their results on this
dataset.
18
We use cosine similarity to identify the
closest synonym.
5.2 Offline Software
Along with the web demo system we are making
available a software which can be downloaded and
be used for evaluation of vector representations of-
fline on all the benchmarks listed above. Since, we
cannot distribute the evaluation benchmarks along
with the software because of licensing issues, we
would give links to the resources which should be
downloaded prior to using the software. This soft-
ware can be run on a command line interface. Fig-
ure 3 shows a screenshot of word similarity evalu-
ation using the software.
5.3 Public Access
Usually corpora that the vectors are trained upon
are not available freely because of licensing issues
but it is easier to release the vectors that have been
trained on them. In the system that we have devel-
oped, we give the user an option to either make the
vectors freely available for everyone to use under a
GNU General Public License
19
or a Creative Com-
mons License.
20
If the user chooses not to make
the word vectors available, we would evaluate the
18
http://aclweb.org/aclwiki/index.php?
title=TOEFL_Synonym_Questions_(State_of_
the_art)
19
https://www.gnu.org/copyleft/gpl.html
20
https://creativecommons.org/licenses/
by-nc-sa/4.0/legalcode
vectors and give it a position in the leader board
with proper citation to the publications/softwares.
6 Conclusion
In this paper we have presented a demo system that
supports rapid and consistent evaluation of word
vector representations on a variety of tasks, visual-
ization with an easy-to-use web interface and ex-
change and comparison of different word vector
representations. The system also provides access
to a suite of evaluation benchmarks both for En-
glish and other languages. The functionalities of
the system are aimed at: (1) Being a portal for
systematic evaluation of lexical semantics tasks
that heavily rely on word vector representation, (2)
Making it easier for an end-user to choose the most
suitable vector representation schema.
Acknowledgements
We thank members of Noah?s Ark and c-lab for
their helpful comments about the demo system.
Thanks to Devashish Thakur for his help in set-
ting up the website. This work was supported by
the NSF through grant IIS-1352440.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL, NAACL ?09, pages 19?27, Strouds-
burg, PA, USA.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in technicolor. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 136?145,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
23
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, ICML ?08, pages 160?167, New
York, NY, USA. ACM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Maria Cristina Ferreira de Oliveira and Haim Lev-
kowitz. 2003. From visual data exploration to vi-
sual data mining: A survey. IEEE Trans. Vis. Com-
put. Graph., 9(3):378?394.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Gothenburg, Sweden, April.
Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In WWW ?01: Proceedings of the
10th international conference on World Wide Web,
pages 406?414, New York, NY, USA. ACM Press.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of word
relatedness with constraints. In KDD, pages 1406?
1414.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th ACL: Long
Papers-Volume 1, pages 873?882.
Thomas K Landauer and Susan T. Dutnais. 1997. A
solution to platos problem: The latent semantic anal-
ysis theory of acquisition, induction, and represen-
tation of knowledge. Psychological review, pages
211?240.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In CoNLL, Sofia, Bulgaria.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and J Cernock`y. 2011. Rnnlm?
recurrent neural network language modeling toolkit.
Proc. of the 2011 ASRU Workshop, pages 196?201.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Jerome L. Myers and Arnold D. Well. 1995. Research
Design & Statistical Analysis. Routledge, 1 edition,
June.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
WWW ?11, pages 337?346, New York, NY, USA.
ACM.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
Richard Socher, Romain Paulus, Bryan McCann,
Kai Sheng Tai, and Andrew Y. Hu, JiaJi Ng. 2013a.
etcml.com - easy text classification with machine
learning. In Advances in Neural Information Pro-
cessing Systems (NIPS 2013).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631?1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th ACL, ACL ?10, pages 384?394, Stroudsburg,
PA, USA.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning : Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
pages 141?188.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Comput. Linguist., 32(3):379?416, Septem-
ber.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2579?2605, November.
Dongqiang Yang and David M. W. Powers. 2006. Verb
similarity on the taxonomy of wordnet. In In the 3rd
International WordNet Conference (GWC-06), Jeju
Island, Korea.
24
Acquiring entailment pairs across languages and domains:
A data analysis
Manaal Faruqui
Dept. of Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
manaal.iitkgp@gmail.com
Sebastian Pad?
Seminar f?r Computerlinguistik
Universit?t Heidelberg
Heidelberg, Germany
pado@cl.uni-heidelberg.de
Abstract
Entailment pairs are sentence pairs of a premise and a hypothesis, where the premise textually
entails the hypothesis. Such sentence pairs are important for the development of Textual Entailment
systems. In this paper, we take a closer look at a prominent strategy for their automatic acquisition
from newspaper corpora, pairing first sentences of articles with their titles. We propose a simple
logistic regression model that incorporates and extends this heuristic and investigate its robustness
across three languages and three domains. We manage to identify two predictors which predict
entailment pairs with a fairly high accuracy across all languages. However, we find that robustness
across domains within a language is more difficult to achieve.
1 Introduction
Semantic processing has become a major focus of attention in NLP. However, different applications
such as Question Answering, Information Extraction and Machine Translation often adopt very different,
task-specific semantic processing strategies. Textual entailment (TE) was introduced by Dagan et al
(2006) as a ?meta-task? that can subsume a large part of the semantic processing requirements of such
applications by providing a generic concept of inference that corresponds to ?common sense? reasoning
patterns. Textual Entailment is defined as a relation between two natural language utterances (a Premise
P and a Hypothesis H) that holds if ?a human reading P would infer that H is most likely true?. See,
e.g., the ACL ?challenge paper? by Sammons et al (2010) for further details.
The successive TE workshops that have taken place yearly since 2005 have produced annotation for
English which amount to a total of several thousand entailing Premise-Hypothesis sentence pairs, which
we will call entailment pairs:
(1) P: Swedish bond yields end 21 basis points higher.
H: Swedish bond yields rose further.
From the machine learning perspective assumed by many approaches to TE, this is a very small number
of examples, given the complex nature of entailment. Given the problems of manual annotation, therefore,
Burger and Ferro (2005) proposed to take advantage of the structural properties of a particular type of
discourse ? namely newspaper articles ? to automatically harvest entailment pairs. They proposed to pair
the title of each article with its first sentence, interpreting the first sentence as Premise and the title as
Hypothesis. Their results were mixed, with an average of 50% actual entailment pairs among all pairs
constructed in this manner. SVMs which identified ?entailment-friendly? documents based on their bags
of words lead to an accuracy of 77%. Building on the same general idea, Hickl et al (2006) applied a
simple unsupervised filter which removes all entailment pair candidates that ?did not share an entity (or
an NP)?. They report an accuracy of 91.8% on a manually evaluated sample ? considerably better Burger
and Ferro. The article however does not mention the size of the original corpus, and whether ?entity? is to
95
be understood as named entity, so it is difficult to assess what its recall is, and whether it presupposes a
high-quality NER system.
In this paper, we model the task using a logistic regression model that allows us to synchronously
analyse the data and predict entailment pairs, and focus on the question of how well these results generalize
across domains and languages, for many of which no entailment pairs are available at all. We make three
main contributions: (a), we define an annotation scheme based on semantic and discourse phenomena that
can break entailment and annotate two datasets with it; (b), we idenfiy two robust properties of sentence
pairs that correlate strongly with entailment and which are robust enough to support high-precision
entailment pair extraction; (c), we find that cross-domain differences are actually larger than cross-lingual
differences, even for languages as different as German and Hindi.
Plan of the paper. Section 2 defines our annotation scheme. In Section 3, we sketch the logistic
regression framework we use for analysis, and motivate our choice of predictors. Sections 4 and 5 present
the two experiments on language and domain comparisons, respectively. We conclude in Section 6.
2 A fine-grained annotation scheme for entailment pairs
The motivation of our annotation scheme is to better understand why entailment breaks down between
titles and first sentences of newswire articles. We subdivide the general no entailment category of earlier
studies according to an inventory of reasons for non-entailment that we collected from an informal
inspection of some dozen articles from an English-language newspaper. Additionally, we separate out
sentences that are ill-formed in the sense of not forming one proposition.
2.1 Subtypes of non-entailment
No-par (Partial entailment). The Premise entails the Hypothesis almost, but not completely, in one of
two ways: (a), The Hypothesis is a conjunction and the Premise entails just one conjunct; or (b),
Premise and Hypothesis share the main event, but the Premise is missing an argument or adjunct
that forms part of the Hypothesis. Presumably, in our setting, such information is provided by the
other sentences in the article than the first one. In Ex. (1), if P and H were switched, this would be
the case for the size of the rise.
No-pre (Presupposition): The Premise uses a construction which can only be understood with informa-
tion from the Hypothesis, typically a definite description or an adjunct. This category arises because
the title stands before the first sentence and is available as context. In the following example, the
Premise NP ?des Verbandes? can only be resolved through the mention of ?VDA? (the German car
manufacturer?s association) in the Hypothesis.
(2) P: Herzog
Herzog
wird
will
in
in
dem
the
vierk?pfigen
four-head
F?hrungsgremium
management board
des
of the
Verbands
association
f?r
for
die
the
Teile-
parts
und
and
Zubeh?rindustrie
accessory business
zust?ndig
resposible
sein.
be.
H: Martin
Martin
Herzog
Herzog
wird
becomes
VDA-Gesch?ftsf?hrer.
VDA manager.
No-con (Contradiction): Direct contradiction of Premise and Hypothesis.
(3) P: Wie
How
die
the
innere
biological
Uhr
clock
[...]
[...]
funktioniert,
works,
ist
is
noch
still
weitgehend
mostly
unbekannt.
unknown.
H: Licht
Light
stellt
regulates
die
the
innere
biological
Uhr.
clock.
96
No-emb (Embedding): The Premise uses an embedding that breaks entailment (e.g., modal adverbials or
non-factural embedding verb). In the following pair, the proposition in the Hypothesis is embedded
under ?expect?.
(4) P: An Arkansas gambling amendment [...] is expected to be submitted to the state Supreme
Court Monday for a rehearing, a court official said.
H: Arkansas gaming petition goes before court again Monday
No-oth (Other): All other negative examples where Premise and Hypothesis are well-formed, and which
could not be assigned to a more specific category, are included under this tag. In this sense, ?Other?
is a catch-all category. Often, Premise and Hypothesis, taken in isolation, are simply unrelated:
(5) P: Victor the Parrot kept shrieking "Voda, Voda" ? "Water, Water".
H: Thirsty jaguar procures water for Bulgarian zoo.
2.2 Ill-formed sentence pairs
Err (Error): These cases arise due to errors in sentence boundary detection: Premise or Hypothesis may
be cut off in the middle of the sentence.
Ill (Ill-formed): Often, the titles are not single grammatical sentences and can therefore not be interpreted
sensibly as the Hypothesis of an entailment pair. They can be incomplete proposition such as NPs
or PPs (?Beautiful house situated in woods?), or, frequently, combinations of multiple sentences
(?RESEARCH ALERT - Mexico upped, Chile cut.?).
3 Modeling entailment with logistic regression
We will model the entailment annotation labels on candidate sentence pairs using a logistic regression
model. From a machine learning point of view, logistic regression models can be seen as a rather simple
statistical classifier which can be used to acquire new entailment pairs. From a linguistic point of view,
they can be used to explain the phenomena in the data, see e.g., Bresnan et al (2007).
Formally, logistic regression models assume that datapoints consist of a set of predictors x and a
binary response variable y. They have the form
p(y = 1) = 1
1 + e?z with z =
?
i
?ixi (1)
where p is the probability of a datapoint x, ?i is the coefficient assigned to the linguistically motivated
factor xi. Model estimation sets the parameters ? so that the likelihood of the observed data is maximized.
From the linguistics perspective, we are most interested in analysing the importance of the different
predictors: for each predictor xi, the comparison of the estimated value of its coefficient ?i can be
compared to its estimated standard error, and it is possible to test the hypothesis that ?i = 0, i.e., the
predictor does not significantly contribute to the model. Furthermore, the absolute value of ?i can be
interpreted as the log odds ? that is, as the change in the probability of the response variable being positive
depending on xi being positive.
e?i = P (y = 1|x = 1, . . . )/P (y = 0|x = 1, . . . )P (y = 1|x = 0, . . . )/P (y = 0|x = 0, . . . ) (2)
The fact that z is just a linear combination of predictor weights encodes the assumption that the log odds
combine linearly among factors.
From the natural language processing perspective, we would like to create predictions for new
observations. Note, however, that simply assessing the significance of predictors on some dataset, as
97
provided by the logistic regression model, corresponds to an evaluation of the model on the training set,
which is prone to the problem of overfitting. We will therefore in our experiments always apply the models
acquired from one dataset on another to see how well they generalize.
3.1 Choice of Predictors
Next, we need a set of plausible predictors that we can plug into the logistic regression framework. These
predictors should ideally be language-independent. We analyse the categories of our annotation, as an
inventory of phenomena that break entailment, to motivate a small set of robust predictors.
Following early work on textual entailment, we use word overlap as a strong indicator of entail-
ment (Monz and de Rijke, 2001). Our weighted overlap predictor uses the well-known tf/idf weighting
scheme to compute the overlap between P and H (Manning et al, 2008):
weightedOverlap(T,H,D) =
?
w?T?H tf-idf(w,D)
?
w?H tf-idf(w,D)
(3)
where we treat each article as a separate document and the whole corpus as document collection D. We
expect that No-oth pairs have generally the lowest weighted overlap, followed by No-par pairs, while Yes
pairs have the highest weighted overlap. We also use a categorical version of this observation in the form
of our strict noun match predictor. This predictor is similar in spirit to the proposal by Hickl et al (2006)
mentioned in Section 1. The boolean strict noun match predictor is true if all Hypothesis nouns are present
in the Premise, and is therefore a predictor that is geared at precision rather than recall. A third predictor
that was motivated by the No-par and No-oth categories was the number of words in the article: No-oth
sentence pairs often come from long articles, where the first sentence provides merely an introduction. For
this predictor, log num words, we count the total number of words in the article and logarithmize it.1 The
remaining subcategories of No were more difficult to model. No-pre pairs should be identifiable by testing
whether the Premise contains a definite description that cannot be accommodated, a difficult problem
that seems to require world knowledge. Similarly, the recognition of contradictions, as is required to find
No-con pairs, is very difficult in itself (de Marneffe et al, 2008). Finally, No-emb requires the detection
of a counterfactual context in the Premise. Since we do not currently see robust, language-independent
ways of modelling these phenomena, we do not include specific predictors to address them.
The situation is similar with regard to the Err category. While it might be possible to detect incomplete
sentences with the help of a parser, this again involves substantial knowledge about the language. The Ill
category, however, appears easier to target: at least cases of Hypotheses consisting of multiple phrases
case be detected easily by checking for sentence end markers in the middle of the Hypothesis (full stop,
colon, dash). We call this predictor punctuation.
4 Experiment 1: Analysis by Language
4.1 Data sources and preparation
This experiment performs a cross-lingual comparison of three newswire corpora. We use English, German,
and Hindi. All three belong to the Indo-European language family, but English and German are more
closely related.
For English and German, we used the Reuters RCV2 Multilingual Corpus2. RCV2 contains over
487,000 news stories in 13 different languages. Almost all news stories cover the business and politics
domains. The corpus marks the title of each article; we used the sentence splitter provided by Treetag-
ger (Schmid, 1995) to extract the first sentences. Our Hindi corpus is extracted from the text collection
of South Asian languages prepared by the EMILLE project (Xiao et al, 2004)3. We use the Hindi
1This makes the coefficiently easier to interpret. The predictive difference is minimal.
2http://trec.nist.gov/data/reuters/reuters.html
3http://www.elda.org/catalogue/en/text/W0037.html
98
No. of sentence pairs English German Hindi
Original 473,874 (100%) 112,259 (100%) 20,209 (100%)
Filtered 264.711 (55.8%) 50.039 (44.5%) 10.475 (51.8%)
Table 1: Pair extraction statistics
Corpus err ill no-con no-emb no-oth no-par no-pre yes
English Reuters 3.5 2.9 0 0.2 3.7 7.4 0 82.3
German Reuters 2.1 11.0 0.4 0.2 4.3 2.1 0.2 79.7
Hindi Emille 1.1 2.5 0 0.3 14.7 5.7 0 75.7
Table 2: Exp.1: Distribution of annotation categories (in percent)
monolingual data, which was crawled from Webdunia,4 an Indian daily online newspaper. The articles
are predominantly political, with a focus on Indo-Pakistani and Indo-US affairs. We identify sentence
boudaries with the Hindi sentence marker (?|?), which is used exclusively for this purpose.
We preprocessed the data by extracting the title and the first sentence, treating the first sentence as
Premise and the title as Hypothesis. We applied a filter to remove pairs where the chance of entailment
was impossible or very small. Specifically, our filter keeps only sentence pairs that (a) share at least one
noun and where (b) both sentences include at least one verb and are not questions. Table 1 shows the
corpus sizes before and after filtering. Note that the percentage of selected sentences across the languages
are all in the 45%-55% range. This filter could presumably be improved by requiring a shared named
entity, but since language-independent NER is still an open research issue, we did not follow up on this
avenue. We randomly sampled 1,000 of the remaining sentence pairs per language for manual annotation.
4.2 Distribution of annotation categories
First, we compared the frequencies of the annotation categories defined in Section 3.1. The results are
shown in Table 2. We find our simple preprocessing filter results in an accuracy of between 75 and 82%.
This is still considerably below the results of Hickl et al, who report 92% accuracy on their English data.5
Even though the overall percentage of ?yes? cases is quite similar among languages, the details of the
distribution differ. One fairly surprising observation was the fairly large number of ill-formed sentence
pairs. As described in Section 2, this category comprises cases where the Hypothesis (i.e., a title) is not a
grammatical sentence. Further analysis of the category shows that the common patterns are participle
constructions (Ex. (6)) and combinations of multiple statements (Ex. (7)). The participle construction is
particularly prominent in German.
(6) Glencoe Electric, Minn., rated single-A by Moody?s.
(7) Wieder
Again
K?mpfe
fights
in
in
S?dlibanon
Southern Lebanon
-
-
Israeli
Israeli
get?tet.
killed.
The ?no?-categories make up a total of 11.3% (English), 6.6% (German), and 20.7% (Hindi). The ?other?
and ?partial? categories clearly dominate. This is to be expected, in particular the high number of partial
entailments. The ?other? category mostly consists of cases where the title summarizes the whole article,
but the first sentence provides only a gentle introduction to the topic:
(8) P: One automotive industry analyst has dubbed it the ?Lincoln Town Truck?.
H: Ford hopes Navigator will lure young buyers to Lincoln.
As regards the high ratio of ?no-other? cases in the Hindi corpus, we found a high number of instances
where the title states the gist of the article too differently from the first sentence to preserve entailment:
4http://www.webdunia.com
5We attribute the difference to the filtering scheme which is difficult to reconstruct from Hickl et al (2006).
99
Predictor German sig English sig Hindi sig
weighted overlap -0.77 ** -2.30 *** -3.35 ***
log num words -0.05 ? -0.03 ? -0.17 ?
punctuation -1.04 *** -0.43 ** -0.35 **
strict noun match -0.12 ? -0.19 ? -0.38 **
Table 3: Exp. 1: Predictors in the logreg model (*: p<0.05; **: p<0.01; ***: p<0.001)
(9) P: aAj BF E?\ss XAynA kF lokE?ytA km nhF\ h
 
I h{ .
Even today, Princess Diana?s popularity has not decreased.
H: E?\ss XAynA k p/ aOr kAX nFlAm ho\g .
Bidding on Princess Diana?s letter and cards would take place.
The remaining error categories (embedding, presupposition, contradiction) were, disappointingly, almost
absent. Another sizable category is formed by errors, though. We find the highest percentage for English,
where our sentence splitter misinterpreted full stops in abbreviations as sentence boundaries.
4.3 Modelling the data
We estimated logistic regression models on each dataset, using the predictors from Section 3.1. Consider-
ing the eventual goal of extracting entailment pairs, we use the decision yes vs. everything else as our
response variable. The analysis was performed with R, using the rms6 and ROCR7 packages.
Analysis of predictors. The coefficients for the predictors and their significances are shown in Table 3.
There is considerable parallelism between the languages. In all three languages, weighted overlap between
H and P is a significant predictor: high overlap indicates entailment, and vice versa. Its effect size is large
as well: Perfect overlap increases the probability of entailment for German by a factor of e0.77 = 2.16, for
English by 10, and for Hindi even by 28. Similarly, the punctuation predictor comes out as a significant
negative effect for all three languages, presumably by identifying ill-formed sentence pairs. In contrast,
the length of the article (log num words) is not a significant predictor. This is a surprising result, given
our hypothesis that long articles often involve an ?introduction? which reduces the chance for entailment
between the title and the first sentence. The explanation is that the two predictors, log num words and
weighted overlap, are highly significantly correlated in all three corpora. Since weighted overlap is the
predictive of the two, the model discards article length.
Finally, strict noun match, which requires that all nouns match between H and P, is assigned a
positive coefficient for each language, but only reaches significance for Hindi. This is the only genuine
cross-lingual difference: In our Hindi corpus, the titles are copied more verbatim from the text than for
English and German (median weighted overlap: Hindi 0.76, English 0.72, German 0.69). Consequently,
in English and German the filter discards too many entailment instances. For all three languages, though,
the coefficient is small ? for Hindi, where it is largest, it increases the odds by a factor of e0.39 ? 1.4.
Evaluation. We trained models on the three corpora, using only the two predictors that contributed
significantly in all languages (weighted overlap and punctuation), in order to avoid overfitting on the
individual datasets.8 We applied each model to each dataset. How such models should be evaluated
depends on the intended purpose of the classification. We assume that it is fairly easy to obtain large
corpora of newspaper text, which makes precision an issue rather than recall. The logistic regression
classifier assigns a probability to each datapoint, so we can trade off recall and precision. We fix recall at
a reasonable value (30%) and compare precision values.
6http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/Design
7http://rocr.bioinf.mpi-sb.mpg.de/
8Subsequent analysis of ?full? models (with all features) showed that they did not generally improve over two-feature models.
100
PPPPPPPP
Data
Models German model English model Hindi model
German data 91.6 88.8 88.8
English data 93.2 94.3 94.6
Hindi data 98.7 98.7 99.1
Table 4: Exp. 1: Precision for the class ?yes? (entailment) at 30% Recall
Our expectation is that each model will perform best on its own corpus (since this is basically the
training data), and worse on the other languages. The size of the drop for the other languages reflects the
differences between the corpora as well as the degree of overfitting models show to their training data.
The actual results are shown in Table 4.3. The precision is fairly high, generally over 90%, and well
above the baseline percentage of entailment pairs. The German data is modelled best by the German
model, with the two other models performing 3 percent worse. The situation is similar, although less
pronounced, on Hindi data, where the Hindi-trained model is 0.4% better than the two other models. For
English, the Hindi model even outperforms the English model by 0.3%9, which in turn works about 1%
better than the German model. In sum, the logistic regression models can be applied very well across
languages, with little loss in precision. The German data with its high ratio of ill-formed headlines (cf.
Table 2) is most difficult to model. Hindi is simplest, due to the tendency of title and first sentence to be
almost identical (cf. the large weight for the overlap predictor).
5 Experiment 2: Analysis by Domain of German corpora
5.1 Data
This experiment compares three German corpora from different newspapers to study the impact of domain
differences: Reuters, ?Stuttgarter Zeitung?, and ?Die Zeit?. These corpora differ in domain and in style.
The Reuters corpus was already described in Section 4.1. ?Stuttgarter Zeitung? (StuttZ) is a daily regional
newspaper which covers international business and politics like Reuters, but does not draw its material
completely from large news agencies and gives more importance to regional and local events. Its style is
therefore less consistent. Our corpus covers some 80,000 sentences of text from StuttZ. The third corpus
comprises over 4 million sentences of text from ?Die Zeit?, a major German national weekly. The text is
predominantly from the 2000s, plus selected articles from the 1940s through 1990s. ?Die Zeit? focuses on
op-ed pieces and general discussions of political and social issues. It also covers arts and science, which
the two other newspapers rarely do.
5.2 Distribution of annotation categories
We extracted and annotated entailment pair candidates in the same manner as before (cf. Section 4.1).
The new breakdown of annotation categories in Table (10) shows, in comparison to the cross-lingual
results in Table 2, a higher incidence of errors, which we attribute to formatting problems of these corpora.
Compared to the German Reuters corpus we considered in Exp. 1, StuttZ and Die Zeit contain considerably
fewer entailment pairs, most notably Die Zeit, where the percentage of entailment pairs is just 21.6% in
our sample, compared to 82.3% for Reuters. Notably, there are almost no cases where the first sentence
represents a partial entailment; in contrast, for more than one third of the examples (33.9%), there is no
entailment relation between the title and the first sentence. This seems to be a domain-dependent, or even
stylistic, effect: in ?Die Zeit?, titles are often designed solely as ?bait? to interest readers in the article:
(10) P: Sat.1
Sat.1
sah
watched
[...]
[...]
Doris
Doris
dabei zu ,
,
wie
how
sie
she
[...]
[...]
Auto fahren
to drive
lernte.
learned.
9The English model outperforms the Hindi model at higher recall levels, though.
101
Corpus err ill no-con no-emb no-oth no-par no-pre yes
Reuters 3.5 2.9 0 0.2 3.7 7.4 0 82.3
StuttZ 6.2 3.6 0.5 2.8 12.4 3.0 0.6 70.7
Die Zeit 2.3 39.0 0.5 1.8 33.9 0.9 0.0 21.6
Table 5: Exp. 2: Distribution of annotation categories on German corpora (in percent)
Predictor Reuters sig StuttZ sig Die Zeit sig
weighted overlap -0.77 ** -1.82 *** -2.60 ***
log num words -0.05 ? -0.24 ? -0.20 ?
punctuation -1.04 *** -0.01 ? -1.21 ***
strict noun match -0.12 ? -0.20 ? -0.01 ?
Table 6: Exp. 2: Predictors in the logreg model (*: p<0.05; **: p<0.01; ***: p<0.001)
PPPPPPPP
Data
Models Reuters StuttZ Die Zeit
Reuters 91.6 85.4 91.6
StuttZ 83.0 83.0 82.6
Die Zeit 45.2 45.2 46.7
Table 7: Exp. 2: Precision for the class ?yes? at 30% recall
H: Doris,
Doris,
es
it
ist
is
gr?n!
green!
Other titles are just noun or verb phrases, which accounts for the large number (39%) of ill-formed pairs.
5.3 Modelling the data
Predictors and evaluation. The predictors of the logistic regression models for the three German
corpora are shown in Table 6. The picture is strikingly similar to the results of Exp. 1 (Table 3): weighted
overlap and punctuation are highly significant predictors for all three corpora (except punctuation, which
is insignificant for StuttZ); even the effect sizes are roughly similar. Again, neither sentence length
nor strict noun match are significant. This indicates that the predictors we have identified work fairly
robustly. Unfortunately, this does not imply that they always work well. Table 6 shows the precision of
the predictors in Exp. 2, again at 30% Recall. Here, the difference to Exp. 1 (Table 4.3) is striking. First,
overfitting of the predictors is worse across domains, with losses of 5% on Reuters and Die Zeit when they
are classified with models trained on other corpora even though use just two generic features. Second, and
more seriously, it is much more difficult to extract entailment pairs from the Stuttgarter Zeitung corpus
and, especially, the Die Zeit corpus. For the latter, we can obtain a precision of at most 46.7%, compared
to >90% in Exp. 1.
We interpret this result as evidence that domain adaptation may be an even greater challenge than
multilinguality in the acquisition of entailment pairs. More specifically, our impression is that the heuristic
of pairing title and first sentence works fairly well for a particular segment of newswire text, but not
otherwise. This segment consists of factual, ?no-nonsense? articles provided by large news agencies such
as Reuters, which tend to be simple in their discourse structure and have an informative title. In domains
where articles become longer, and the intent to entertain becomes more pertinent (as for Die Zeit), the
heuristic fails very frequently. Note that the weighted overlap predictor cannot recover all negative cases.
Example (10) is a case in point: one of the two informative words in H, ?Doris? and ?gr?n?, is in fact in P.
Domain specificity. The fact that it is difficult to extract entailment pairs from some corpora is serious
exactly because, according to our intuition, the ?easier? news agency corpora (like Reuters) are domain-
102
Corpus D( ? | deWac) words w with highest P (w)/Q(w)
Reuters 0.98 H?ndler (trader), B?rse (exchange), Prozent (per cent), erkl?rte (stated)
StuttZ 0.93 DM (German Mark), Prozent (per cent), Millionen (millions), Gesch?fts-
jahr (fiscal year), Milliarden (billions)
Die Zeit 0.64 hei?t (means), wei? (knows), l??t (leaves/lets)
Table 8: Exp. 2: Domain specificity (KL distance from deWac); typical content words
specific. We quantify this intuition with an approach by Ciaramita and Baroni (2006), who propose
to model the representativeness of web-crawled corpora as the KL divergence between their Laplace-
smoothed unigram distribution P and that of a reference corpus, Q (w ? W are vocabulary words):
D(P,Q) =
?
w?W
P (w) log P (w)Q(w) (4)
We use the deWac German web corpus (Baroni et al, 2009) as reference, making the idealizing assumption
that it is representative for the German language. We interpret a large distance from deWac as domain
specificity. The results in Table 8 bear out our hypothesis: Die Zeit is less domain specific than StuttZ,
which in turn is less specific than Reuters. The table also lists the content words (nouns/verbs) that are
most typical for each corpus, i.e., which have the highest value of P (w)/Q(w). The lists bolster the
interpretation that Reuters and StuttZ concentrate on the economical domain, while the typical terms of
Die Zeit show an argumentative style, but no obvious domain bias. In sum, domain specificity is inversely
correlated with the difficulty of extracting entailment pairs: from a representativity standpoint, we should
draw entailment pairs from Die Zeit.
6 Conclusion
In this paper, we have discussed the robustness of extracting entailment pairs from the title and first
sentence of newspaper articles. We have proposed a logistic regression model and have analysed its
performance on two datasets that we have created: a cross-lingual one a cross-domain one. Our cross-
lingual experiment shows a positive result: despite differences in the distribution of annotation categories
across domains and languages, the predictors of all logistic regression models look remarkably similar. In
particular, we have found two predictors which are correlated significantly with entailment across (almost)
all languages and domains. These are (a), a tf/idf measure of word overlap between the title and the first
sentence; and (b), the presence of punctuation indicating that the title is not a single grammatical sentence.
These predictors extract entailment pairs from newswire text at a precision of > 90%, at a recall of 30%,
and represent a simple, cross-lingually robust method for entailment pair acquisition.
The cross-domain experiment, however, forces us to qualify this positive result. On two other German
corpora from different newspapers, we see a substantial degradation of the model?s performance. It may
seem surprising that cross-domain robustness is a larger problem than cross-lingual robustness. Our
interpretation is that the limiting factor is the degree to which the underlying assumption, namely that
first sentence entails the title, is true. If the assumption is true only for a minority of sentences, our
predictors cannot save the day. This assumption holds well in the Reuters corpora, but less so for the
other newspapers. Unfortunately, we also found that the Reuters corpora are at the same time thematically
constrained, and therefore only of limited use for extracting a representative corpus of entailment pairs. A
second problem is that the addition of features we considered beyond the two mentioned above threatens
to degrade the classifier due to overfitting, at least across domains.
Given these limitation of the present headline-based approach, other approaches that are more
generally applicable may need to be explored. Entailment pairs have for example been extracted from
Wikipedia (Bos et al, 2009). Another direction is to build on methods to extract paraphrases from
comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where
entailment holds in one, but not the other, direction.
103
Acknowledgments. The first author would like to acknowledge the support of a WISE scholarship
granted by DAAD (German Academic Exchange Service).
References
Baroni, M., S. Bernardini, A. Ferraresi, and E. Zanchetta (2009). The wacky wide web: A collection
of very large linguistically processed web-crawled corpora. Journal of Language Resources and
Evaluation 43(3), 209?226.
Barzilay, R. and L. Lee (2003). Learning to paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL, Edmonton, AL, pp. 16?23.
Bos, J., M. Pennacchiotti, and F. M. Zanzotto (2009). Textual entailment at EVALITA 2009. In
Proceedings of IAAI, Reggio Emilia.
Bresnan, J., A. Cueni, T. Nikitina, and H. Baayen (2007). Predicting the dative alternation. In G. Bouma,
I. Kraemer, and J. Zwarts (Eds.), Cognitive Foundations of Interpretation, pp. 69?94. Royal Netherlands
Academy of Science.
Burger, J. and L. Ferro (2005). Generating an entailment corpus from news headlines. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pp. 49?54.
Ciaramita, M. and M. Baroni (2006). A figure of merit for the evaluation of web-corpus randomness. In
Proceedings of EACL, Trento, Italy, pp. 217?224.
Dagan, I., O. Glickman, and B. Magnini (2006). The PASCAL recognising textual entailment challenge.
In Machine Learning Challenges, Volume 3944 of Lecture Notes in Computer Science, pp. 177?190.
Springer.
de Marneffe, M.-C., A. N. Rafferty, and C. D. Manning (2008). Finding contradictions in text. In
Proceedings of the ACL, Columbus, Ohio, pp. 1039?1047.
Hickl, A., J. Williams, J. Bensley, K. Roberts, B. Rink, and Y. Shi (2006). Recognizing textual entailment
with LCC?s Groundhog system. In Proceedings of the Second PASCAL Challenges Workshop.
Manning, C. D., P. Raghavan, and H. Sch?tze (2008). Introduction to Information Retrieval (1st ed.).
Cambridge University Press.
Monz, C. and M. de Rijke (2001). Light-weight entailment checking for computational semantics. In
Proceedings of ICoS, Siena, Italy, pp. 59?72.
Sammons, M., V. Vydiswaran, and D. Roth (2010). ?Ask Not What Textual Entailment Can Do for You...?.
In Proceedings of ACL, Uppsala, Sweden, pp. 1199?1208.
Schmid, H. (1995). Improvements in part-of-speech tagging with an application to german. In Proceedings
of the SIGDAT Workshop at ACL, Cambridge, MA.
Xiao, Z., T. McEnery, P. Baker, and A. Hardie (2004). Developing Asian language corpora: Standards
and practice. In In Proceedings of the Fourth Workshop on Asian Language Resources, Sanya, China,
pp. 1?8.
104
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 279?287,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Identifying the L1 of non-native writers: the CMU-Haifa system
Yulia Tsvetkov? Naama Twitto? Nathan Schneider? Noam Ordan?
Manaal Faruqui? Victor Chahuneau? Shuly Wintner? Chris Dyer?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA
cdyer@cs.cmu.edu
?Department of Computer Science
University of Haifa
Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
We show that it is possible to learn to identify, with
high accuracy, the native language of English test
takers from the content of the essays they write.
Our method uses standard text classification tech-
niques based on multiclass logistic regression, com-
bining individually weak indicators to predict the
most probable native language from a set of 11 pos-
sibilities. We describe the various features used for
classification, as well as the settings of the classifier
that yielded the highest accuracy.
1 Introduction
The task we address in this work is identifying the
native language (L1) of non-native English (L2) au-
thors. More specifically, given a dataset of short
English essays (Blanchard et al, 2013), composed
as part of the Test of English as a Foreign Lan-
guage (TOEFL) by authors whose native language is
one out of 11 possible languages?Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, or Turkish?our task is to identify
that language.
This task has a clear empirical motivation. Non-
native speakers make different errors when they
write English, depending on their native language
(Lado, 1957; Swan and Smith, 2001); understand-
ing the different types of errors is a prerequisite for
correcting them (Leacock et al, 2010), and systems
such as the one we describe here can shed interest-
ing light on such errors. Tutoring applications can
use our system to identify the native language of
students and offer better-targeted advice. Forensic
linguistic applications are sometimes required to de-
termine the L1 of authors (Estival et al, 2007b; Es-
tival et al, 2007a). Additionally, we believe that the
task is interesting in and of itself, providing a bet-
ter understanding of non-native language. We are
thus equally interested in defining meaningful fea-
tures whose contribution to the task can be linguis-
tically interpreted. Briefly, our features draw heav-
ily on prior work in general text classification and
authorship identification, those used in identifying
so-called translationese (Volansky et al, forthcom-
ing), and a class of features that involves determin-
ing what minimal changes would be necessary to
transform the essays into ?standard? English (as de-
termined by an n-gram language model).
We address the task as a multiway text-
classification task; we describe our data in ?3 and
classification model in ?4. As in other author attri-
bution tasks (Juola, 2006), the choice of features for
the classifier is crucial; we discuss the features we
define in ?5. We report our results in ?6 and con-
clude with suggestions for future research.
2 Related work
The task of L1 identification was introduced by Kop-
pel et al (2005a; 2005b), who work on the Inter-
national Corpus of Learner English (Granger et al,
2009), which includes texts written by students from
5 countries, Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts range from 500 to
850 words in length. Their classification method
is a linear SVM, and features include 400 standard
function words, 200 letter n-grams, 185 error types
and 250 rare part-of-speech (POS) bigrams. Ten-
279
fold cross-validation results on this dataset are 80%
accuracy.
The same experimental setup is assumed by Tsur
and Rappoport (2007), who are mostly interested
in testing the hypothesis that an author?s choice of
words in a second language is influenced by the
phonology of his or her L1. They confirm this hy-
pothesis by carefully analyzing the features used by
Koppel et al, controlling for potential biases.
Wong and Dras (2009; 2011) are also motivated
by a linguistic hypothesis, namely that syntactic er-
rors in a text are influenced by the author?s L1.
Wong and Dras (2009) analyze three error types sta-
tistically, and then add them as features in the same
experimental setup as above (using LIBSVM with a
radial kernel for classification). The error types are
subject-verb disagreement, noun-number disagree-
ment and misuse of determiners. Addition of these
features does not improve on the results of Kop-
pel et al. Wong and Dras (2011) further extend
this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic struc-
ture. This improves the results significantly, yielding
78% accuracy compared with less than 65% using
only lexical features.
Kochmar (2011) uses a different corpus, the Cam-
bridge Learner Corpus, in which texts are 200-400
word long, and are authored by native speakers of
five Germanic languages (German, Swiss German,
Dutch, Swedish and Danish) and five Romance lan-
guages (French, Italian, Catalan, Spanish and Por-
tuguese). Again, SVMs are used as the classification
device. Features include POS n-grams, character n-
grams, phrase-structure rules (extracted from parse
trees), and two measures of error rate. The classi-
fier is evaluated on its ability to distinguish between
pairs of closely-related L1s, and the results are usu-
ally excellent.
A completely different approach is offered by
Brooke and Hirst (2011). Since training corpora for
this task are rare, they use mainly L1 (blog) cor-
pora. Given English word bigrams ?e1, e2?, they try
to assess, for each L1, how likely it is that an L1 bi-
gram was translated literally by the author, resulting
in ?e1, e2?. Working with four L1s (French, Span-
ish, Chinese, and Japanese), and evaluating on the
International Corpus of Learner English, accuracy is
below 50%.
3 Data
Our dataset in this work consists of TOEFL essays
written by speakers of eleven different L1s (Blan-
chard et al, 2013), distributed as part of the Na-
tive Language Identification Shared Task (Tetreault
et al, 2013). The training data consists of 1000
essays from each native language. The essays are
short, consisting of 10 to 20 sentences each. We
used the provided splits of 900 documents for train-
ing and 100 for development. Each document is an-
notated with the author?s English proficiency level
(low, medium, high) and an identification (1 to 8) of
the essay prompt. All essays are tokenized and split
into sentences. In table 1 we provide some statistics
on the training corpora, listed by the authors? profi-
ciency level. All essays were tagged with the Stan-
ford part-of-speech tagger (Toutanova et al, 2003).
We did not parse the dataset.
Low Medium High
# Documents 1,069 5,366 3,456
# Tokens 245,130 1,819,407 1,388,260
# Types 13,110 37,393 28,329
Table 1: Training set statistics.
4 Model
For our classification model we used the creg re-
gression modeling framework to train a 11-class lo-
gistic regression classifier.1 We parameterize the
classifier as a multiclass logistic regression:
p?(y | x) =
exp
?
j ? jh j(x, y)
Z?(x)
,
where x are documents, h j(?) are real-valued feature
functions of the document being classified, ? j are the
corresponding weights, and y is one of the eleven L1
class labels. To train the parameters of our model,
we minimized the following objective,
L = ?
`2 reg.
?????
j
?2j ?
?
{(xi,yi)}
|D|
i=1
(
log likelihood
?          ??          ?
log p?(yi | xi) +
?Ep?(y? |xi) log p?(y
? | xi)
?                      ??                      ?
?conditional entropy
)
,
1https://github.com/redpony/creg
280
which combines the negative log likelihood of the
training dataset D, an `2 (quadratic) penalty on the
magnitude of ? (weighted by ?), and the negative en-
tropy of the predictive model (weighted by ?). While
an `2 weight penalty is standard in regression prob-
lems like this, we found that the the additional en-
tropy term gave more reliable results. Intuitively,
the entropic regularizer encourages the model to re-
main maximally uncertain about its predictions. In
the metaphor of ?maximum entropy?, the entropic
prior finds a solution that has more entropy than the
?maximum? model that is compatible with the con-
straints.
The objective cannot be minimized in closed
form, but it does have a unique minimum and
is straightforwardly differentiable, so we used L-
BFGS to find the optimal weight settings (Liu et al,
1989).
5 Feature Overview
We define a large arsenal of features, our motivation
being both to improve the accuracy of classification
and to be able to interpret the characteristics of the
language produced by speakers of different L1s.
While some of the features were used in prior
work (?2), we focus on two broad novel categories
of features: those inspired by the features used
to identify translationese by Volansky et al (forth-
coming) and those extracted by automatic statisti-
cal ?correction? of the essays. Refer to figure 1 to
see the set of features and their values that were ex-
tracted from an example sentence.
POS n-grams Part-of-speech n-grams were used in
various text-classification tasks.
Prompt Since the prompt contributes information
on the domain, it is likely that some words (and,
hence, character sequences) will occur more fre-
quently with some prompts than with others. We
therefore use the prompt ID in conjunction with
other features.
Document length The number of tokens in the text
is highly correlated with the author?s level of flu-
ency, which in turn is correlated with the author?s
L1.
Pronouns The use of pronouns varies greatly
among different authors. We use the same list
of 25 English pronouns that Volansky et al (forth-
coming) use for identifying translationese.
Punctuation Similarly, different languages use
punctuation differently, and we expect this to taint
the use of punctuation in non-native texts. Of
course, character n-grams subsume this feature.
Passives English uses passive voice more fre-
quently than other languages. Again, the use of
passives in L2 can be correlated with the author?s
L1.
Positional token frequency The choice of the first
and last few words in a sentence is highly con-
strained, and may be significantly influenced by
the author?s L1.
Cohesive markers These are 40 function words
(and short phrases) that have a strong discourse
function in texts (however, because, in fact,
etc.). Translators tend to spell out implicit utter-
ances and render them explicitly in the target text
(Blum-Kulka, 1986). We use the list of Volansky
et al (forthcoming).
Cohesive verbs This is a list of manually compiled
verbs that are used, like cohesive markers, to spell
out implicit utterances (indicate, imply, contain,
etc.).
Function words Frequent tokens, which are mostly
function words, have been used successfully for
various text classification tasks. Koppel and Or-
dan (2011) define a list of 400 such words, of
which we only use 100 (using the entire list was
not significantly different). Note that pronouns
are included in this list.
Contextual function words To further capitalize
on the ability of function words to discriminate,
we define pairs consisting of a function word from
the list mentioned above, along with the POS tag
of its adjacent word. This feature captures pat-
terns such as verbs and the preposition or particle
immediately to their right, or nouns and the deter-
miner that precedes them. We also define 3-grams
consisting of one or two function words and the
POS tag of the third word in the 3-gram.
Lemmas The content of the text is not considered a
good indication of the author?s L1, but many text
categorization tasks use lemmas (more precisely,
the stems produced by the tagger) as features ap-
proximating the content.
Misspelling features Learning to perceive, pro-
duce, and encode non-native phonemic contrasts
281
Firstly the employers live more savely because they are going to have more money to spend for luxury .
Presence Considered alternatives/edits
Characters
"CHAR_l_y_ ": log 2 + 1
"CharPrompt_P5_g_o_i": log 1 + 1
"MFChar_e_ ": log 1 + 1
"Punc_period": log 1 + 1
"DeleteP_p_.": 1.0
"InsertP_p_,": 1.0
"MID:SUBST:v:f": log 1 + 1
"SUBST:v:f": log 1 + 1
Words
"DocLen_": log 19 + 1
"MeanWordRank": 422.6
"CohMarker_because": log 1 + 1
"MostFreq_have": log 1 + 1
"PosToken_last_luxury": log 1 + 1
"Pronouns_they": log 1 + 1
"MSP:safely": log 1 + 1
"Match_p_to": 0.5
"Delete_p_to": 0.5
"Delete_p_are": 1.0
"Delete_p_because": 1.0
"Delete_p_for": 1.0
POS "POS
_VBP_VBG_TO": log 1 + 1
"POS_p_VBP_VBG_TO": 0.059
Words + POS "VBP
_VBG_to": log 1 + 1
"FW__more RB": log 1 + 1
Figure 1: Some of the features extracted for an L1 German sentence.
is extremely difficult for L2 learners (Hayes-Harb
and Masuda, 2008). Since English?s orthogra-
phy is largely phonemic?even if it is irregular
in many places, we expect leaners whose na-
tive phoneme contrasts are different from those
of English to make characteristic spelling errors.
For example, since Japanese and Korean lack a
phonemic /l/-/r/ contrast, we expect native speak-
ers of those languages to be more likely to make
spelling errors that confuse l and r relative to
native speakers of languages such as Spanish in
which that pair is contrastive. To make this in-
formation available to our model, we use a noisy
channel spelling corrector (Kernighan, 1990) to
identify and correct misspelled words in the train-
ing and test data. From these corrections, we ex-
tract minimal edit features that show what inser-
tions, deletions, substitutions and joinings (where
two separate words are written merged into a sin-
gle orthographic token) were made by the author
of the essay.
Restored tags We focus on three important token
classes defined above: punctuation marks, func-
tion words and cohesive verbs. We first remove
words in these classes from the texts, and then
recover the most likely hidden tokens in a se-
quence of words, according to an n-gram lan-
guage model trained on all essays in the training
corpus corrected with a spell checker and con-
taining both words and hidden tokens. This fea-
ture should capture specific words or punctuation
marks that are consistently omitted (deletions),
or misused (insertions, substitutions). To restore
hidden tokens we use the hidden-ngram util-
ity provided in SRI?s language modeling toolkit
(Stolcke, 2002).
Brown clusters (Brown et al, 1992) describe an al-
gorithm that induces a hierarchical clustering of
a language?s vocabulary based on each vocabu-
lary item?s tendency to appear in similar left and
right contexts in a training corpus. While origi-
nally developed to reduce the number of parame-
ters required in n-gram language models, Brown
clusters have been found to be extremely effective
as lexical representations in a variety of regres-
sion problems that condition on text (Koo et al,
2008; Turian et al, 2010; Owoputi et al, 2013).
Using an open-source implementation of the al-
gorithm,2 we clustered 8 billion words of English
into 600 classes.3 We included log counts of all
4-grams of Brown clusters that occurred at least
100 times in the NLI training data.
5.1 Main Features
We use the following four feature types as the base-
line features in our model. For features that are sen-
sitive to frequency, we use the log of the (frequency-
plus-one) as the feature?s value. Table 2 reports the
accuracy of using each feature type in isolation (with
2https://github.com/percyliang/brown-cluster
3http://www.ark.cs.cmu.edu/cdyer/en-600/
cluster_viewer.html
282
Feature Accuracy (%)
POS 55.18
FreqChar 74.12
CharPrompt 65.09
Brown 72.26
DocLen 11.81
Punct 27.41
Pron 22.81
Position 53.03
PsvRatio 12.26
CxtFxn (bigram) 62.79
CxtFxn (trigram) 62.32
Misspell 37.29
Restore 47.67
CohMark 25.71
CohVerb 22.85
FxnWord 42.47
Table 2: Independent performance of feature types de-
tailed in ?5.1, ?5.2 and ?5.3. Accuracy is averaged over
10 folds of cross-validation on the training set.
10-fold cross-validation on the training set).
POS Part-of-speech n-grams. Features were ex-
tracted to count every POS 1-, 2-, 3- and 4-gram
in each document.
FreqChar Frequent character n-grams. We exper-
imented with character n-grams: To reduce the
number of parameters, we removed features only
those character n-grams that are observed more
than 5 times in the training corpus, and n ranges
from 1 to 4. High-weight features include:
TUR:<Turk>; ITA:<Ital>; JPN:<Japa>.
CharPrompt Conjunction of the character n-gram
features defined above with the prompt ID.
Brown Substitutions, deletions and insertions
counts of Brown cluster unigrams and bigrams in
each document.
The accuracy of the classifier on the development set
using these four feature types is reported in table 3.4
5.2 Additional Features
To the basic set of features we now add more spe-
cific, linguistically-motivated features, each adding
a small number of parameters to the model. As
above, we indicate the accuracy of each feature type
in isolation.
4For experiments in this paper combining multiple types of
features, we used Jonathan Clark?s workflow management tool,
ducttape (https://github.com/jhclark/ducttape).
Feature Group # Params Accuracy (%) `2
POS 540,947 55.18 1.0
+ FreqChar 1,036,871 79.55 1.0
+ CharPrompt 2,111,175 79.82 1.0
+ Brown 5,664,461 81.09 1.0
Table 3: Dev set accuracy with main feature groups,
added cumulatively. The number of parameters is always
a multiple of 11 (the number of classes). Only `2 regular-
ization was used for these experiments; the penalty was
tuned on the dev set as well.
DocLen Document length in tokens.
Punct Counts of each punctuation mark.
Pron Counts of each pronoun.
Position Positional token frequency. We use the
counts for the first two and last three words be-
fore the period in each sentence as features. High-
weight features for the second word include:
ARA:2<,>; CHI:2<is>; HIN:2<can>.
PsvRatio The proportion of passive verbs out of all
verbs.
CxtFxn Contextual function words. High-weight
features include: CHI:<some JJ>;
HIN:<as VBN>.
Misspell Spelling correction edits. Features
included substitutions, deletions, insertions,
doubling of letters and missing doublings of
letters, and splittings (alot?a lot), as well as the
word position where the error occurred.
High-weight features include: ARA:DEL<e>,
ARA:INS<e>, ARA:SUBST<e>/<i>;
GER:SUBST<z>/<y>; JPN:SUBST<l>/<r>,
JPN:SUBST<r>/<l>; SPA:DOUBLE<s>,
SPA:MID_INS<s>, SPA:INS<s>.
Restore Counts of substitutions, deletions and
insertions of predefined tokens that we restored
in the texts. High-weight features include:
CHI:DELWORD<do>; GER:DELWORD<on>;
ITA:DELWORD<be>
Table 4 reports the empirical improvement that each
of these brings independently when added to the
main features (?5.1).
5.3 Discarded Features
We also tried several other feature types that did not
improve the accuracy of the classifier on the devel-
opment set.
CohMark Counts of each cohesive marker.
283
Feature Group # Params Accuracy (%) `2
main + Position 6,153,015 81.00 1.0
main + PsvRatio 5,664,472 81.00 1.0
main 5,664,461 81.09 1.0
main + DocLen 5,664,472 81.09 1.0
main + Pron 5,664,736 81.09 1.0
main + Punct 5,664,604 81.09 1.0
main + Misspell 5,799,860 81.27 5.0
main + Restore 5,682,589 81.36 5.0
main + CxtFxn 7,669,684 81.73 1.0
Table 4: Dev set accuracy with main features plus addi-
tional feature groups, added independently. `2 regulariza-
tion was tuned as in table 3 (two values, 1.0 and 5.0, were
tried for each configuration; more careful tuning might
produce slightly better accuracy). Results are sorted by
accuracy; only three groups exhibited independent im-
provements over the main feature set.
CohVerb Counts of each cohesive verb.
FxnWord Counts of function words. These features
are subsumed by the highly discriminative CxtFxn
features.
6 Results
The full model that we used to classify the test set
combines all features listed in table 4. Using all
these features, the accuracy on the development set
is 84.55%, and on the test set it is 81.5%. The values
for ? and ? were tuned to optimize development set
performance, and found to be ? = 5, ? = 2.
Table 5 lists the confusion matrix on the test set,
as well as precision, recall and F1-score for each L1.
The largest error type involved predicting Telugu
when the true label was Hindi, which happened 18
times. This error is unsurprising since many Hindi
and Telugu speakers are arguably native speakers of
Indian English.
Production of L2 texts, not unlike translating from
L1 to L2, involves a tension between the impos-
ing models of L1 (and the source text), on the one
hand, and a set of cognitive constraints resulting
from the efforts to generate the target text, on the
other. The former is called interference in Trans-
lation Studies (Toury, 1995) and transfer in second
language acquisition (Selinker, 1972). Volansky et
al. (forthcoming) designed 32 classifiers to test the
validity of the forces acting on translated texts, and
found that features sensitive to interference consis-
tently yielded the best performing classifiers. And
indeed, in this work too, we find fingerprints of the
source language are dominant in the makeup of L2
texts. The main difference, however, between texts
translated by professionals and the texts we address
here, is that more often than not professional trans-
lators translate into their mother tongue, whereas L2
writers write out of their mother tongue by defini-
tion. So interference is ever more exaggerated in
this case, for example, also phonologically (Tsur and
Rappoport, 2007).
We explore the effects of interference by analyz-
ing several patterns we observe in the features. Our
classifier finds that the character sequence alot is
overrepresented in Arabic L2 texts. Arabic has no
indefinite article and we speculate that Arabic speak-
ers conceive a lot as a single word; the Arabic equiv-
alent for a lot is used adverbially like an -ly suffix
in English. For the same reason, another promi-
nent feature is a missing definite article before nouns
and adjectives. Additionally, Arabic, being an Ab-
jad language, rarely indicates vowels, and indeed we
find many missing e?s and i?s in the texts of Arabic
speakers. Phonologically, because Arabic conflates
/I/ and /@/ into /i/ (at least in Modern Standard Ara-
bic), we see that many e?s are indeed substituted for
i?s in these texts.
We find that essays that contain hyphens are more
likely to be from German authors. We again find
evidence of interference from the native language
here. First, relative clauses are widely used in Ger-
man, and we see this pattern in L2 English of L1
German speakers. For example, any given rational
being ? let us say Immanual Kant ? we find that.
Another source of extra hyphens stems from com-
pounding convention. So, for example, we find well-
known, community-help, spare-time, football-club,
etc. Many of these reflect an effort to both connect
and separate connected forms in the original (e.g.,
Fussballklub, which in English would be more natu-
rally rendered as football club). Another unexpected
feature of essays by native Germans is a frequent
substitution of the letter y for z and vice versa. We
suspect this owes to their switched positions on Ger-
man keyboards.
Lexical item frequency also provides clues to the
L1 of the essay writers. The word that occurs more
frequently in the texts of German L1 speakers. We
284
true? ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision (%) Recall (%) F1 (%)
ARA 80 0 2 1 3 4 1 0 4 2 3 80.8 80.0 80.4
CHI 3 80 0 1 1 0 6 7 1 0 1 88.9 80.0 84.2
FRE 2 2 81 5 1 2 1 0 3 0 3 86.2 81.0 83.5
GER 1 1 1 93 0 0 0 1 1 0 2 87.7 93.0 90.3
HIN 2 0 0 1 77 1 0 1 5 9 4 74.8 77.0 75.9
ITA 2 0 3 1 1 87 1 0 3 0 2 82.1 87.0 84.5
JPN 2 1 1 2 0 1 87 5 0 0 1 78.4 87.0 82.5
KOR 1 5 2 0 1 0 9 81 1 0 0 80.2 81.0 80.6
SPA 2 0 2 0 1 8 2 1 78 1 5 77.2 78.0 77.6
TEL 0 1 0 0 18 1 2 1 1 73 3 85.9 73.0 78.9
TUR 4 0 2 2 0 2 2 4 4 0 80 76.9 80.0 78.4
Table 5: Official test set confusion matrix with the full model. Accuracy is 81.5%.
hypothesize that in English it is optional in rela-
tive clauses whereas in German it is not, so Ger-
man speakers are less comfortable using the non-
obligatory form. Also, often is over represented. We
hypothesize that since it is cognate of German oft, it
is not cognitively expensive to retrieve it. We find
many times?a literal translation of muchas veces?
in Spanish essays.
Other informative features that reflect L1 features
include frequent misspellings involving confusions
of l and r in Japanese essays. More mysteriously,
the characters r and s are misused in Chinese and
Spanish, respectively. The word then is dominant
in the texts of Hindi speakers. Finally, it is clear
that authors refer to their native cultures (and, conse-
quently, native languages and countries); the strings
Turkish, Korea, and Ita were dominant in the texts of
Turkish, Korean and Italian native speakers, respec-
tively.
7 Discussion
We experimented with different classifiers and a
large set of features to solve an 11-way classifica-
tion problem. We hope that studying this problem
will improve to facilitate human assessment, grad-
ing, and teaching of English as a second language.
While the core features used are sparse and sensitive
to lexical and even orthographic features of the writ-
ing, many of them are linguistically informed and
provide insight into how L1 and L2 interact.
Our point of departure was the analogy between
translated texts as a genre in its own and L2 writ-
ers as pseudo translators, relying heavily on their
mother tongue and transferring their native models
to a second language. In formulating our features,
we assumed that like translators, L2 writers will
write in a simplified manner and overuse explicit
markers. Although this should be studied vis-?-vis
comparable outputs of mother tongue writers in En-
glish, we observe that the best features of our clas-
sifiers are of the ?interference? type, i.e. phonolog-
ical, morphological and syntactic in nature, mostly
in the form of misspelling features, restoration tags,
punctuation and lexical and syntactic modeling.
We would like to stress that certain features indi-
cating a particular L1 have no bearing on the quality
of the English produced. This has been discussed
extensively in Translation Studies (Toury, 1995),
where interference is observed by the overuse or un-
deruse of certain features reflecting the typological
differences between a specific pair of languages, but
which is still within grammatical limits. For exam-
ple, the fact that Italian native speakers favor the
syntactic sequence of determiner + adjective + noun
(e.g., a big risk or this new business) has little pre-
scriptive value for teachers.
A further example of how L2 quality and the
ability to predict L1 are uncorrelated, we noted
that certain L2 writers often repeat words appear-
ing in their essay prompts, and including informa-
tion about whether the writer was reusing prompt
words improved classification accuracy. We suggest
this reflects different educational backgrounds. This
feature says nothing about the quality of the text, just
as the tendency of Korean and Italian writers to men-
tion their home country more often does not.
285
Acknowledgments
This research was supported by a grant from the Is-
raeli Ministry of Science and Technology.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native English. Technical report, Edu-
cational Testing Service.
Shoshana Blum-Kulka. 1986. Shifts of cohesion and co-
herence in translation. In Juliane House and Shoshana
Blum-Kulka, editors, Interlingual and intercultural
communication Discourse and cognition in translation
and second language acquisition studies, volume 35,
pages 17?35. Gunter Narr Verlag.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4).
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007a. Author profil-
ing for English emails. In Proc. of PACLING, pages
263?272, Melbourne, Australia.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007b. TAT: An author
profiling tool with application to Arabic emails. In
Proc. of the Australasian Language Technology Work-
shop, pages 21?30, Melbourne, Australia, December.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English. Presses universitaires de Louvain,
Louvain-la-Neuve.
Rachel Hayes-Harb and Kyoko Masuda. 2008. Devel-
opment of the ability to lexically encode novel second
language phonemic contrasts. Second Language Re-
search, 24(1):5?33.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233?
334.
Mark D. Kernighan. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proc. of
COLING.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of ACL-HLT, pages 1318?
1326, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining
a text for errors. In Proc. of KDD, pages 624?628,
Chicago, IL. ACM.
Robert Lado. 1957. Linguistics across cultures: applied
linguistics for language teachers. University of Michi-
gan Press, Ann Arbor, Michigan, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan and
Claypool.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory BFGS method
for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL.
Larry Selinker. 1972. Interlanguage. International
Review of Applied Linguistics in Language Teaching,
10(1?4):209?232.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Procedings of Interna-
tional Conference on Spoken Language Processing,
pages 901?904.
Michael Swan and Bernard Smith. 2001. Learner En-
glish: A Teacher?s Guide to Interference and Other
Problems. Cambridge Handbooks for Language
Teachers. Cambridge University Press.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proc. of the Eighth Workshop on Inno-
vative Use of NLP for Building Educational Applica-
tions, Atlanta, GA, USA, June. Association for Com-
putational Linguistics.
Gideon Toury. 1995. Descriptive Translation Studies
and beyond. John Benjamins, Amsterdam / Philadel-
phia.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180, Edmonton, Canada,
June. Association for Computational Linguistics.
286
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proc. of
the Workshop on Cognitive Aspects of Computational
Language Acquisition, pages 9?16, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Vered Volansky, Noam Ordan, and Shuly Wintner. forth-
coming. On the features of translationese. Literary
and Linguistic Computing.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Proc.
of the Australasian Language Technology Association
Workshop, pages 53?61, Sydney, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. of EMNLP, pages 1600?1610, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
287
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
A Framework for (Under)specifying Dependency Syntax
without Overloading Annotators
Nathan Schneider?? Brendan O?Connor? Naomi Saphra? David Bamman?
Manaal Faruqui? Noah A. Smith? Chris Dyer? Jason Baldridge?
?School of Computer Science, Carnegie Mellon University
?Department of Linguistics, The University of Texas at Austin
Abstract
We introduce a framework for lightweight
dependency syntax annotation. Our for-
malism builds upon the typical represen-
tation for unlabeled dependencies, per-
mitting a simple notation and annotation
workflow. Moreover, the formalism en-
courages annotators to underspecify parts
of the syntax if doing so would streamline
the annotation process. We demonstrate
the efficacy of this annotation on three lan-
guages and develop algorithms to evaluate
and compare underspecified annotations.
1 Introduction
Computational representations for natural lan-
guage syntax are borne of competing design con-
siderations. When designing such representations,
there may be a tradeoff between parsimony and
expressiveness. A range of linguistic theories at-
tract support due to differing purposes and aes-
thetic principles (Chomsky, 1957; Tesni?re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988,
inter alia). Formalisms concerned with tractable
computation may care chiefly about learnabil-
ity or parsing efficiency (Shieber, 1992; Sleator
and Temperly, 1993; Kuhlmann and Nivre, 2006).
Further considerations may include psychologi-
cal and evolutionary plausibility (Croft, 2001;
Tomasello, 2003; Steels et al, 2011; Fossum and
Levy, 2012), integration with other representa-
tions such as semantics (Steedman, 2000; Bergen
and Chang, 2005), or suitability for particular ap-
plications (e.g., translation).
Here we elevate ease of annotation as a pri-
mary design concern for a syntactic annotation
formalism. Currently, a lack of annotated data
is a huge bottleneck for robust NLP, standing in
the way of parsers for social media text (Foster
et al, 2011) and many low-resourced languages
(to name two examples). Traditional syntactic an-
notation projects like the Penn Treebank (Marcus
?Corresponding author: nschneid@cs.cmu.edu
et al, 1993) or Prague Dependency Treebank (Ha-
jic?, 1998) require highly trained annotators and
huge amounts of effort. Lowering the cost of an-
notation, by making it easier and more accessi-
ble, could greatly facilitate robust NLP in new lan-
guages and genres.
To that end, we design and test new, lightweight
methodologies for syntactic annotation. We pro-
pose a formalism, Fragmentary Unlabeled De-
pendency Grammar (FUDG) for unlabeled de-
pendency syntax that addresses some of the most
glaring deficiencies of basic unlabeled dependen-
cies (?2), with little added burden on annotators.
FUDG requires minimal theoretical commitments,
and can be supplemented with a project-specific
style guide (we provide a brief one for English).
We contribute a simple ASCII markup language?
Graph Fragment Language (GFL; ?3)?that al-
lows annotations to be authored using any text ed-
itor, along with tools for validating, normalizing,
and visualizing GFL annotations.1
An important characteristic of our framework is
annotator flexibility. The formalism supports this
by allowing underspecification of structural por-
tions that are unclear or unnecessary for the pur-
poses of a project. Fully leveraging this power re-
quires new algorithms for evaluation, e.g., of inter-
annotator agreement, where annotations are par-
tial; such algorithms are presented in ?4.2
Finally, small-scale case studies (?5) apply our
framework (formalism, notation, and evaluations)
to syntactically annotate web text in English, news
in Malagasy, and dialogues in Kinyarwanda.
2 A Dependency Grammar for
Annotation
Although dependency-based approaches to syntax
play a major role in computational linguistics, the
nature of dependency representations is far from
uniform. Exemplifying one end of the spectrum
is the Prague Dependency Treebank, which articu-
lates an elaborate dependency-based syntactic the-
1https://github.com/brendano/gfl_syntax/
2Parsing algorithms are left for future work.
51
Found the scarriest mystery door in my school . I?M SO CURIOUS D:
Found** < (the scarriest mystery door*)
Found < in < (my > school)
I?M** < (SO > CURIOUS)
D:**
my = I?M
thers still like 1 1/2 hours till Biebs bday here :P
thers** < still
thers < ((1 1/2) > hours < till < (Biebs > bday))
(thers like 1 1/2 hours)
thers < here
:P**
Figure 1: Two tweets with example GFL annotations. (The formalism and notation are described in ?3.)
ory in a rich, multi-tiered formalism (Hajic?, 1998;
B?hmov? et al, 2003). On the opposite end of
the spectrum are the structures used in dependency
parsing research which organize all the tokens of
a sentence into a tree, sometimes with category la-
bels on the edges (K?bler et al, 2009). Insofar as
they reflect a theory of syntax, these vanilla de-
pendency grammars provide a highly reduction-
ist view of structure?indeed, parses used to train
and evaluate dependency parses are often simpli-
fications of Prague-style parses, or else converted
from constituent treebanks.
In addition to the binary dependency links of
vanilla dependency representations, we offer three
devices to capture certain linguistic phenomena
more straightforwardly:3
1. We make explicit the meaningful lexical units
over which syntactic structure is represented. Our
approach (a) allows punctuation and other extrane-
ous tokens to be excluded so as not to distract from
the essential structure; and (b) permits tokens to be
grouped into shallow multiword lexical units.4
2. Coordination is problematic to represent with
unlabeled dependencies due to its non-binary na-
ture. A coordinating conjunction typically joins
multiple expressions (conjuncts) with equal sta-
tus, and other expressions may relate to the com-
pound structure as a unit. There are several differ-
ent conventions for forcing coordinate structures
into a head-modifier straightjacket (Nivre, 2005;
de Marneffe and Manning, 2008; Marec?ek et al,
2013). Conjuncts, coordinators, and shared de-
pendents can be distinguished with edge labels;
we equivalently use a special notation, permitting
the coordinate structure to be automatically trans-
formed with any of the existing conventions.5
3Some of this is inspired by the conventions of Reed-
Kellogg sentence diagramming, a graphical dependency an-
notation system for English pedagogy (Reed and Kellogg,
1877; Kolln and Funk, 1994; Florey, 2006).
4The Stanford representation supports a limited notion of
multiword expressions (de Marneffe and Manning, 2008).
For simplicity, our formalism treats multiwords as unana-
lyzed (syntactically opaque) wholes, though some multiword
expressions may have syntactic descriptions (Baldwin and
Kim, 2010).
5Tesni?re (1959) and Hudson (1984) similarly use
special structures for coordination (Schneider, 1998;
3. Following Tesni?re (1959), our formalism
offers a simple facility to express anaphora-
antecedent relations (a subset of semantic relation-
ships) that are salient in particular syntactic phe-
nomena such as relative clauses, appositives, and
wh-expressions.
Underspecification. Our desire to facilitate
lightweight annotation scenarios requires us to
abandon the expectation that syntactic informants
provide a complete parse for every sentence. On
one hand, an annotator may be uncertain about the
appropriate parse due to lack of expertise, insuf-
ficiently mature annotation conventions, or actual
ambiguity in the sentence. On the other hand, an-
notators may be indifferent to certain phenomena.
This can happen for a variety of reasons:
? Some projects may only need annotations of
specific constructions. For example, building a
semantic resource for events may require anno-
tation of syntactic verb-argument relations, but
not internal noun phrase structure.
? As a project matures, it may be more useful to
annotate only infrequent lexical items.
? Semisupervised learning from partial annota-
tions may be sufficient to learn complete parsers
(Hwa, 1999; Clark and Curran, 2006).
? Beginning annotators may wish to focus on eas-
ily understood syntactic phenomena.
? Different members of a project may wish to spe-
cialize in different syntactic phenomena, reduc-
ing training cost and cognitive load.
Rather than treating annotations as invalid unless
and until they are complete trees, we formally rep-
resent and reason about partial parse structures.
Annotators produce annotations, which encode
constraints on the (inferred) analysis, the parse
structure, of a sentence. We say that a valid anno-
tation supports (is compatible with) one or more
analyses. Both annotations and analyses are rep-
resented as graphs (the graph representation is de-
scribed below in ?3.2). We require that the di-
rected edges in an analysis graph must form a tree
over all the lexical items in the sentence.6 Less
Sangati and Mazza, 2009).
6While some linguistic phenomena (e.g., relative clauses,
control constructions) can be represented using non-tree
52
stringent well-formedness constraints on the an-
notation graph leave room for underspecification.
Briefly, an annotation can be underspecified in
two ways: (a) an expression may not be attached to
any parent, indicating it might depend on any non-
descendant in a full analysis?this is useful for an-
notating sentences piece by piece; and (b) multiple
expressions may be grouped together in a fudge
expression (?3.3), a constraint that the elements
form a connected subgraph in the full analysis
while leaving the precise nature of that subgraph
indeterminate?this is useful for marking relation-
ships between chunks (possibly constituents).
A formalism, not a theory. Our framework for
dependency grammar annotation is a syntactic
formalism, but it is not sufficiently comprehen-
sive to constitute a theory of syntax. Though
it standardizes the basic treatment of a few ba-
sic phenomena, simplicity of the formalism re-
quires us to be conservative about making such
extensions. Therefore, just as with simpler for-
malisms, language- and project-specific conven-
tions will have to be developed for specific linguis-
tic phenomena. By embracing underspecified an-
notation, however, our formalism aims to encour-
age efficient corpus coverage in a nascent anno-
tation project, without forcing annotators to make
premature decisions.
3 Syntactic Formalism and GFL
In our framework, a syntactic annotation of a sen-
tence follows an extended dependency formalism
based on the desiderata enumerated in the previ-
ous section. We call our formalism Fragmentary
Unlabeled Dependency Grammar (FUDG).
To make it simple to create FUDG annotations
with a text editor, we provide a plain-text de-
pendency notation called Graph Fragment Lan-
guage (GFL). Fragments of the FUDG graph?
nodes and dependencies linking them?are en-
coded in this language; taken together, these frag-
ments describe the annotation in its entirety. The
ordering of GFL fragments, and of tokens within
each fragment, is of no formal consequence. Since
the underlying FUDG representation is transpar-
ently related to GFL constructions, GFL notation
will be introduced alongside the discussion of each
kind of FUDG node.7
structures, we find that being able to alert annotators when
they inadvertently violate the tree constraint is more useful
than the expressive flexibility.
7In principle, FUDG annotations could be created with
3.1 Tokens
We expect a tokenized string, such as a sentence
or short message. The provided tokenization is re-
spected in the annotation. For human readability,
GFL fragments refer to tokens as strings (rather
than offsets), so all tokens that participate in an
annotation must be unambiguous in the input.8 A
token may be referenced multiple times in the an-
notation.
3.2 Graph Encoding
Directed arcs. As in other dependency
formalisms, dependency arcs are directed
links indicating the syntactic headedness
relationship between pairs of nodes. In
GFL, directed arcs are indicated with an-
gle brackets pointing from the dependent to
its head, as in black > cat or (equivalently)
cat < black. Multiple arcs can be chained to-
gether: the > cat < black < jet describes three
arcs. Parentheses help group portions of a chain:
(the > cat < black < jet) > likes < fish (the
structure black < jet > likes, in which jet
appears to have two heads, is disallowed). Note
that another encoding for this structure would be
to place the contents of the parentheses and the
chain cat > likes < fish on separate lines. Curly
braces can be used to list multiple dependents of
the same head: {cat fish} > likes.
Anaphoric links. These undirected links join
coreferent anaphora to each other and to their an-
tecedent(s). In English this includes personal pro-
nouns, relative pronouns (who, which, that), and
anaphoric do and so (Leo loves Ulla and so does
Max). This introduces a bit of semantics into our
annotation, though at present we do not attempt to
mark non-anaphoric coreference. It also allows a
more satisfying treatment of appositives and rel-
ative clauses than would be possible from just the
directed tree (the third example in figures 2 and 3).
Lexical nodes. Whereas in vanilla dependency
grammar syntactic links are between pairs of to-
ken nodes, FUDG abstracts away from the indi-
vidual tokens in the input. The lowest level of a
FUDG annotation consists of lexical nodes, i.e.,
an alternative mechanism such as a GUI, as in Hajic? et al
(2001).
8If a word is repeated within the sentence, it must be in-
dexed in the input string in order to be referred to from a
fragment. In our notation, successive instances of the same
word are suffixed with ~1, ~2, ~3, etc. Punctuation and other
tokens omitted from an annotation do not need to be indexed.
53
'll
If
's
I wake_up
restin' it~1
it~2
weapons
Our three
are
$a
fear surprise efficiency
ruthless
and~1 and~2
are
We knights
the
who
say
Ni
Figure 2: FUDG graphs corresponding to the examples in figure 3. The two special kinds of directed edges are for attaching
conjuncts (bolded) and their coordinators (dotted) in a coordinate structure. Anaphoric links are undirected. The root node of
each sentence is omitted.
If it~1 's restin' I 'll wake it~2 up .
If < (it~1 > 's < restin')
I > 'll < [wake up] < it~2
If > 'll**
it~1 = it~2
Our three weapons are fear and~1 surprise and~2
ruthless efficiency ...
{Our three} > weapons > are < $a
$a :: {fear surprise efficiency} :: {and~1 and~2}
ruthless > efficiency
We are the knights who say ... Ni !
We > are < knights < the
knights < (who > say < Ni)
who = knights
Figure 3: GFL for the FUDG graphs in figure 2.
lexical item occurrences. Every token node maps
to 0 or 1 lexical nodes (punctuation, for instance,
can be ignored).
A multiword is a lexical node incorporating
more than one input token and is atomic (does
not contain internal structure). A multiword node
may group any subset of input tokens; this allows
for multiword expressions which are not neces-
sarily contiguous in the sentence (e.g., the verb-
particle construction make up in make the story
up). GFL notates multiwords with square brack-
ets, e.g., [break a leg].
Coordination nodes. Coordinate structures re-
quire at least two kinds of dependents: co-
ordinators (i.e., lexical nodes for coordinat-
ing conjunctions?at least one per coordina-
tion node) and conjuncts (heads of the con-
joined subgraphs?at least one per coordination
node). The GFL annotation has three parts:
a variable representing the node, a set of con-
juncts, and a set of coordinator nodes. For in-
stance, $a :: {[peanut butter] honey} :: {and}
(peanut butter and honey) can be embedded
within a phrase via the coordination node
variable $a; a [fresh [[peanut butter] and
honey] sandwich] snack would be formed with
{fresh $a} > sandwich > snack < a. A graphical
example of coordination can be seen in figure 2?
note the bolded conjunct edges and the dotted co-
ordinator edges. If the conjoined phrase as a whole
takes modifiers, these are attached to the coordina-
tion node with regular directed arcs. For example,
in Sam really adores kittens and abhors puppies.,
the shared subject Sam and adverb really attach to
the entire conjoined phrase. In GFL:
$a :: {adores abhors} :: {and}
Sam > $a < really
adores < kittens abhors < puppies
Root node. This is a special top-level node used
to indicate that a graph fragment constitutes a stan-
dalone utterance or a discourse connective. For an
input with multiple utterances, the head of each
should be designated with ** to indicate that it at-
taches to the root.
3.3 Means of Underspecification
As discussed in ?2, our framework distinguishes
annotations from full syntactic analyses. With re-
spect to dependency structure (directed edges), the
former may underspecify the latter, allowing the
annotator to commit only to a partial analysis.
For an annotationA, we define support(A) to be
the set of full analyses compatible with that anno-
tation. A full analysis is required to be a directed
rooted tree over all lexical nodes in the annotation.
An annotation is valid if its support is non-empty.
The 2 mechanisms for dependency underspeci-
fication are unattached nodes and fudge nodes.
Unattached nodes. For any node in an annota-
tion, the annotator is free to simply leave it not
attached to any head. This is interpreted as al-
lowing its head to be any other node (including
the root node), subject to the tree constraint. We
call a node?s possible heads its supported par-
ents. Formally, for an unattached node v in an-
notation A, suppParentsA(v) = nodes(A) \ ({v} ?
descendants(v)).
Fudge nodes. Sometimes, however, it is desir-
able to represent a sort of skeletal structure with-
out filling in all the details. A fudge expres-
sion (FE) asserts that a group of nodes (the ex-
pression?s members) belong together in a con-
nected subgraph, while leaving the internal struc-
ture of that subgraph unspecified.9 The notation
9This underspecification semantics is, to the best of our
knowledge, novel, though it has been proposed that con-
nected dependency subgraphs (known as catenae) are of the-
oretical importance in syntax (Osborne et al, 2012).
54
FN2
a
b
f
FN1
c d e
f
b
f
b
b c
b
b a
a
d
a
c a
d
a
d
c db e fe c e fe
a
d d
cf
e e f
c
Figure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments
((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?
log 6
log 75
= .816.
for this is a list of two or more nodes within
parentheses: an annotation for Few if any witches
are friends with Maria. might contain the FE
(Few if any) so as to be compatible with the
structures Few < if < any, Few > if > any, etc.?
but not, for instance, Few > witches < any. In
the FUDG graph, this is represented with a fudge
node to which members are attached by special
member arcs. Fudge nodes may be linked to other
nodes: the GFL fragment (Few if any) > witches
is compatible with (Few < if < any) > witches,
(Few < (if > any)) > witches, and so forth.
Properties. Let f be a fudge expression. From
the connected subgraph definition and the tree
constraint on analyses, it follows that:
? Exactly 1 member of f must, in any compatible
analysis, have a parent that is not a member of f.
Call this node the top of the fudge expression,
denoted f ?. f ? dominates all other members of
f; it can be considered f?s ?internal head.?
? f does not necessarily form a full subtree. Any
of its members may have dependents that are
not themselves members of the fudge expres-
sion. (Such dependencies can be specified in
additional GFL fragments.)
Top designation. A single member of a fudge
expression may optionally be designated as its top
(internal head). This is specified with an asterisk:
(Few* if any) > witches indicates that Few must
attach to witches and also dominate both if and
any. In the FUDG graph, this is represented with
a special top arc as depicted in bold in figure 4.
Nesting. One fudge expression may nest
within another, e.g. (Few (if any)) > witches;
the word analyzed as attaching to witches might
be Few or whichever of (if any) heads the other.
A nested fudge expression can be designated as
top: (Vanishingly few (if any)*).
Modifiers. An arc attaching a node to a
fudge expression as a whole asserts that the
external node should modify the top of the fudge
expression (whether or not that top is designated
in the annotation). For instance, two of the
interpretations of British left waffles on Falklands
would be preserved by specifying British > left
and (left waffles) < on < Falklands. Analyses
British > left < waffles < on < Falklands and
(British > left < on < Falklands) > waffles
would be excluded because the preposition does
not attach to the head of (left waffles).10
Multiple membership. A node may be a mem-
ber of multiple fudge expressions, or a member
of an FE while attached to some other node via
an explicit arc. Each connected component of
the FUDG graph is therefore a polytree (not nec-
essarily a tree). The annotation graph minus all
member edges of fudge nodes and all (undirected)
anaphoric links must be a directed tree or forest.
Enumerating supported parents. Fudge ex-
pressions complicate the procedure for listing a
node?s supported parents (see above). Consider an
FE f having some member v. v might be the top
of f (unless some other node is so designated), in
which case anything the fudge node can attach to
is a potential parent of v. If some node other than
v might be the top of f, then v?s head could be any
member of f. Below (?4.1) we develop an algo-
rithm for enumerating supported parents for any
annotation graph node.
4 Annotation Evaluation Measures
For an annotation task which allows for a great
deal of latitude?as in our case, where a syntac-
tic annotation may be full or partial?quantitative
evaluation of data quality becomes a challenge. In
the context of our formalism, we propose mea-
sures that address:
? Annotation efficiency, quantified in terms of
annotator productivity (tokens per hour).
? The amount of information in an underspeci-
fied annotation. Intuitively, an annotation that
flirts with many full analyses conveys less syn-
tactic information than one which supports few
analyses. We define an annotation?s promiscu-
ity to be the number of full analyses it supports,
and develop an algorithm to compute it (?4.1).
10Not all attachment ambiguities can be precisely encoded
in FUDG. For instance, there is no way to forbid an attach-
ment to a word that lies along the path between the pos-
sible heads. The best that can be done given a sentence
like They conspired to defenestrate themselves on Tuesday. is
They > conspired < to < defenestrate < themselves and
(conspired* to defenestrate (on < Tuesday)).
55
? Inter-annotator agreement between two par-
tial annotations. Our measures for dependency
structure agreement (?4.2) incorporate the no-
tion of promiscuity.
We test these evaluations on our pilot annotation
data in the case studies (?5).
4.1 Promiscuity vs. Commitment
Given a FUDG annotation of a sentence, we quan-
tify the extent to which it underspecifies the full
structure by counting the number of analyses that
are compatible with the constraints in the annota-
tion. We call this number the promiscuity of the
annotation. Each analysis tree is rooted with the
root node and must span all lexical nodes.11
A na?ve algorithm for computing promiscuity
would be to enumerate all directed spanning trees
over the lexical nodes, and then check each of
them for compatibility with the annotation. But
this quickly becomes intractable: for n nodes,
one of which is designated as the root, there are
nn?2 spanning trees. However, we can filter out
edges that are known to be incompatible with
the annotation before searching for spanning
trees. Our ?upward-downward? method for
constructing a graph of supported edges first
enumerates a set of candidate top nodes for every
fudge expression, then uses that information
to infer a set of supported parents for every
node.12 The supported edge graph then consists
of vertices lexnodes(A) ? {root} and edges
?
v?lexnodes(A) {(v? v?) ? v? ? suppParentsA(v)}.
From this graph we can count all directed span-
ning trees in cubic time using Kirchhoff?s matrix
tree theorem (Chaiken and Kleitman, 1978; Smith
and Smith, 2007; Margoliash, 2010).13 If some
lexical node has no supported parents, this reflects
conflicting constraints in the annotation, and no
spanning tree will be found.
Promiscuity will tend to be higher for longer
sentences. To control for this, we define a second
quantity, the annotation?s commitment quotient
(commitment being the opposite of promiscuity),
11This measure assumes a fixed lexical analysis (set of lex-
ical nodes) and does not consider anaphoric links. Coordinate
structures are simplified into ordinary dependencies, with co-
ordinate phrases headed by the coordinator?s lexical node. If
a coordination node has multiple coordinators, one is arbi-
trarily chosen as the head and the others as its dependents.
12Python code for these algorithms appears in Schneider
et al (2013) and the accompanying software release.
13Due to a technicality with non-member attachments to
fudge nodes, for some annotations this is only an upper bound
on promiscuity; see Schneider et al (2013).
which normalizes for the number of possible span-
ning trees given the sentence length. The commit-
ment quotient for an annotation of a sentence with
n?1 lexical nodes and one root node is given by:
com(A) = 1 ?
log prom(A)
log nn?2
(the logs are to attenuate the dominance of the ex-
ponential term). This will be 1 if only a single
tree is supported by the annotation, and 0 if the
annotation does not constrain the structure at all.
(If the constraints in the annotation are internally
inconsistent, then promiscuity will be 0 and com-
mitment undefined.) In practice, there is a trade-
off between efficiency and commitment: more de-
tailed annotations require more time. The value of
minimizing promiscuity will therefore depend on
the resources and goals of the annotation project.
4.2 Inter-Annotator Agreement
FUDG can encode flat groupings and coreference
at the lexical level, as well as syntactic structure
over lexical items. Inter-annotator agreement can
be measured separately for each of these facets.
Pilot annotator feedback indicated that our initial
lexical-level guidelines were inadequate, so we fo-
cus here on measuring structural agreement pend-
ing further clarification of the lexical conventions.
Attachment accuracy, a standard measure for
evaluating dependency parsers, cannot be com-
puted between two FUDG annotations if either of
them underspecifies any part of the dependency
structure. One solution is to consider the inter-
section of supported full trees, in the spirit of
our promiscuity measure. For annotations A1 and
A2 of sentence s, one annotation?s supported an-
alyses can be enumerated and then filtered sub-
ject to the constraints of the other annotation.
The tradeoff between inter-annotator compatibil-
ity and commitment can be accounted for by tak-
ing their product, i.e. comPrec(A1 | A2) =
com(A1)
|supp(A1)?supp(A2)|
|supp(A1)|
.
A limitation of this support-intersection ap-
proach is that if the two annotations are not
compatible, the intersection will be empty. A
more fine-grained approach is to decompose
the comparison by lexical node: we general-
ize attachment accuracy with softComPrec(A1 |
A2) = com(A1)
?
`?s
?
i?{1,2} suppParentsAi (`)?
`?s suppParentsA1 (`)
, comput-
ing com(?) and suppParents(?) as in the previous
section. As lexical nodes may differ between the
two annotations, a reconciliation step is required
56
Language Tokens Rate (tokens/hr)
English Tweets (partial) 667 430
English Tweets (full) 388 250
Malagasy 4,184 47
Kinyarwanda 8,036 80
Table 1: Productivity estimates from pilot annotation project.
All annotators were native speakers of English.
to compare the structures: multiwords proposed in
only one of the two annotations are converted to
fudge expressions. Tokens annotated by neither
annotator are ignored. Like with the promiscuity
measure, we simplify coordinate structures to or-
dinary dependencies (see footnote 11).
5 Case Studies
5.1 Annotation Time
To estimate annotation efficiency, we performed
a pilot annotation project consisting of annotating
several hundred English tweets, about 1,000 sen-
tences in Malagasy, and a further 1,000 sentences
in Kinyarwanda.14 Table 1 summarizes the num-
ber of tokens annotated and the effort required. For
the two Twitter cases, the same annotator was first
permitted to do partial annotation of 100 tweets,
and then spend the same amount of time doing a
complete annotation of all tokens. Although this is
a very small study, the results clearly suggest she
was able to make much more rapid progress when
partial annotation was an option.15
This pilot study helped us to identify linguistic
phenomena warranting specific conventions: these
include wh-expressions, comparatives, vocatives,
discourse connectives, null copula constructions,
and many others. We documented these cases in a
20-page style guide for English,16 which informed
the subsequent pilot studies discussed below.
5.2 Underspecification and Agreement
We annotated 2 small English data samples in
order to study annotators? use of underspecifica-
tion. The first is drawn from Owoputi et al?s 2013
Twitter part-of-speech corpus; the second is from
the Reviews portion of the English Web Treebank
14Malagasy is a VOS Austronesian language spoken by 15
million people, mostly in Madagascar. Kinyarwanda is an
SVO Bantu language spoken by 12 million people mostly in
Rwanda. All annotations were done by native speakers of En-
glish. The Kinyarwanda and Malagasy annotators had basic
proficiency in these languages.
15As a point of comparison, during the Penn Treebank
project, annotators corrected the syntactic bracketings pro-
duced by a high-quality hand-written parser (Fidditch) and
achieved a rate of only 375 tokens/hour using a specialized
GUI interface (Marcus et al, 1993).
16Included with the data and software release (footnote 1).
Omit. prom Hist. Mean
1Ws MWs Tkns FEs 1 >1 ?10 ?102 com
Tweets 60 messages, 957 tokens
A 597 56 304 23 43 17 11 5 .96
B 644 47 266 28 37 23 12 6 .95
Reviews 55 sentences, 778 tokens
A 609 33 136 2 53 2 2 1 1.00
C ? D 643 19 116 114 11 44 38 21 .82
T 704 ? 74 ? 55 0 0 0 1
Table 2: Measures of our annotation samples. Note that
annotator ?D? specialized in noun phrase?internal structure,
while annotator ?C? specialized in verb phrase/clausal phe-
nomena; C ? D denotes the combination of their annotation
fragments. ?T? denotes our dependency conversion of the
English Web Treebank parses. (The value 1.00 was rounded
up from .9994.)
(EWTB) (Bies et al, 2012). (Our annotators only
saw the tokenized text.) Both datasets are infor-
mal and conversational in nature, and are dom-
inated by short messages/sentences. In spite of
their brevity, many of the items were deemed to
contain multiple ?utterances,? which we define to
include discourse connectives and emoticons (at
best marginal parts of the syntax); utterance heads
are marked with ** in figure 1.
Table 2 indicates the sizes of the two data sam-
ples, and gives statistics over the output of each
annotator: total counts of single-word and mul-
tiword lexical nodes, tokens not represented by
any lexical node, and fudge nodes; as well as
a histogram of promiscuity counts and the aver-
age of commitment quotients (see ?4.1). For in-
stance, the two sets of annotations obtained for the
Tweets sample used underspecification in 17/60
and 23/60 tweets, respectively, though the promis-
cuity rarely exceeded 100 compatible trees per an-
notation. Examples can be seen in figure 1, where
annotator ?A? marked only the noun phrase head
for the scarriest mystery door, opted not to choose
a head within the quantity 1 1/2, and left ambigu-
ous the attachment of the hedge like. The strong
but not utter commitment to the dependency struc-
ture is reflected in the mean commitment quotients
for this dataset, both of which exceed 0.95.
Inter-annotator agreement (IAA) is quantified in
table 3. The row marked A ? B, for instance,
considers the agreement between annotator ?A?
and annotator ?B?. Measuring IAA on the depen-
dency structure requires a common set of lexical
nodes, so a lexical reconciliation step ensures that
(a) any token used by either annotation is present
in both, and (b) no multiword node is present
in only one annotation?solved by relaxing in-
compatible multiwords to FEs (which increases
promiscuity). For Tweets, lexical reconciliation
57
thus reduces the commitment averages for each
annotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?
marked more multiwords. An analysis fully com-
patible with both annotations exists for only 27/60
sentences; the finer-grained softComPrec measure
(?4.2), however, offers insight into the balance be-
tween commitment and agreement.
Qualitatively, we observe three leading causes
of incompatibilities (disagreements): obvious an-
notator mistakes (such as the marked as a head);
inconsistent handling of verbal auxiliaries; and un-
certainty whether to attach expressions to a verb
or the root node, as with here in figure 1.17 An-
notators noticed occasional ambiguous cases and
attempted to encode the ambiguity with fudge ex-
pressions: again in the tweet maybe put it off un-
til you feel like ~ talking again ? is one example.
More often, fudge expressions proved useful for
syntactically difficult constructions, such as those
shown in figure 1 as well as: 2 shy of breaking it,
asked what tribe I was from, a $ 13 / day charge,
you two, and the most awkward thing ever.
5.3 Annotator Specialization
As an experiment in using underspecification for
labor division, two of the annotators of Reviews
data were assigned specific linguistic phenomena
to focus on. Annotator ?D? was tasked with the in-
ternal structure of base noun phrases, including re-
solving the antecedents of personal pronouns. ?C?
was asked to mark the remaining phenomena?
i.e., utterance/clause/verb phrase structure?but to
mark base noun phrases as fudge expressions,
leaving their internal structure unspecified. Both
annotators provided a full lexical analysis. For
comparison, a third individual, ?A,? annotated the
same data in full. The three annotators worked
completely independently.
Of the results in tables 2 and 3, the most notable
difference between full and specialized annotation
is that the combination of independent specialized
annotations (C ? D) produces somewhat higher
promiscuity/lower commitment. This is unsurpris-
ing because annotators sometimes overlook rela-
tionships that fall under their specialty.18 Still, an-
notators reported that specialization made the task
17Another example: Some uses of conjunctions like and
and so can be interpreted as either phrasal coordinators or dis-
course connectives (cf. The PDTB Research Group, 2007).
18A more practical and less error-prone approach might be
for specialists to work sequentially or collaboratively (rather
than independently) on each sentence.
com softComPrec
IAA 1 2 N|?|>0 1|2 2|1 F1
Tweets (N=60)
A ? B .82 .91 27 .57 .72 .63
Reviews (N=55)
A ? (C ? D) .95 .76 30 .64 .40 .50
A ? T .92 1 26 .48 .91 .63
(C ? D) ? T .73 1.00 28 .33 .93 .49
Table 3: Measures of inter-annotator agreement. Annotator
labels are as in table 2. Per-annotator com (with lexical rec-
onciliation) and inter-annotator softComPrec are aggregated
over sentences by arithmetic mean.
less burdensome, and the specialized annotations
did prove complementary to each other.19
5.4 Treebank Comparison
Though the annotators in our study were native
speakers well acquainted with representations of
English syntax, we sought to quantify their agree-
ment with the expert treebankers who created the
EWTB (the source of the Reviews sentences). We
converted the EWTB?s constituent parses to de-
pendencies via the PennConverter tool (Johansson
and Nugues, 2007),20 then removed punctuation.
Agreement with the converted treebank parses
appears in the bottom two rows of table 3. Be-
cause the EWTB commits to a single analysis,
precision scores are quite lopsided. Most of its
attachments are consistent with our annotations
(softComPrec > 0.9), but these allow many ad-
ditional analyses (hence the scores below 0.5).
6 Conclusion
We have presented a framework for simple depen-
dency annotation that overcomes some of the rep-
resentational limitations of unlabeled dependency
grammar and embraces the practical realities of
resource-building efforts. Pilot studies (in multiple
languages and domains, supported by a human-
readable notation and a suite of open-source tools)
showed this approach lends itself to rapid annota-
tion with minimal training.
The next step will be to develop algorithms ex-
ploiting these representations for learning parsers.
Other future extensions might include additional
expressive mechanisms (e.g., multi-headedness,
labels), crowdsourcing of FUDG annotations
(Snow et al, 2008), or even a semantic counter-
part to the syntactic representation.
19In fact, for only 2 sentences did ?C? and ?D? have in-
compatible annotations, and both were due to simple mis-
takes that were then fixed in the combination.
20We ran PennConverter with options chosen to emulate
our annotation conventions; see Schneider et al (2013).
58
Acknowledgments
We thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-
jay John, Lori Levin, Andr? Martins, and several anony-
mous reviewers for their insights. This research was sup-
ported in part by the U. S. Army Research Laboratory and
the U. S. Army Research Office under contract/grant number
W911NF-10-1-0533 and by NSF grant IIS-1054319.
References
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.
Benjamin K. Bergen and Nancy Chang. 2005. Embod-
ied Construction Grammar in simulation-based lan-
guage understanding. In Jan-Ola ?stman and Mir-
jam Fried, editors, Construction grammars: cog-
nitive grounding and theoretical extensions, pages
147?190. John Benjamins, Amsterdam.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, Barbora
Hladk?, and Anne Abeill?. 2003. The Prague De-
pendency Treebank: a three-level annotation sce-
nario. In Treebanks: building and using parsed cor-
pora, pages 103?127. Springer.
Seth Chaiken and Daniel J. Kleitman. 1978. Matrix
Tree Theorems. Journal of Combinatorial Theory,
Series A, 24(3):377?381.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
La Haye.
Stephen Clark and James Curran. 2006. Partial training
for a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL (HLT-NAACL 2006), pages 144?151. As-
sociation for Computational Linguistics, New York
City, USA.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Oxford
University Press, Oxford.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies man-
ual. http://nlp.stanford.edu/downloads/
dependencies_manual.pdf.
Kitty Burns Florey. 2006. Sister Bernadette?s Barking
Dog: The quirky history and lost art of diagramming
sentences. Melville House, New York.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incre-
mental sentence processing. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL 2012), pages 61?69. As-
sociation for Computational Linguistics, Montr?al,
Canada.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS Tagging and Parsing the Twitter-
verse. In Proceedings of the 2011 AAAI Workshop
on Analyzing Microtext, pages 20?25. AAAI Press,
San Francisco, CA.
The PDTB Research Group. 2007. The Penn Discourse
Treebank 2.0 annotation manual. Technical Report
IRCS-08-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania, Philadelphia, PA.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Hajic?ov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Jan Hajic?, Barbora Vidov? Hladk?, and Petr Pajas.
2001. The Prague Dependency Treebank: anno-
tation structure and support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114. University of Pennsylvania, Philadelphia,
USA.
Richard A. Hudson. 1984. Word Grammar. Blackwell,
Oxford.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-99), pages 73?79. Association for Computa-
tional Linguistics, College Park, Maryland, USA.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of the
16th Nordic Conference of Computational Linguis-
tics (NODALIDA-2007), pages 105?112. Tartu, Es-
tonia.
Martha Kolln and Robert Funk. 1994. Understanding
English Grammar. Macmillan, New York.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool, San Rafael, CA.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 507?514. Association for Computa-
tional Linguistics, Sydney, Australia.
59
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Marec?ek, Martin Popel, Loganathan Ramasamy,
Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,
and Jan Hajic?. 2013. Cross-language study on in-
fluence of coordination style on dependency parsing
performance. Technial Report 49, ?FAL MFF UK.
Jonathan Margoliash. 2010. Matrix-Tree Theorem for
directed graphs. http://www.math.uchicago.
edu/~may/VIGRE/VIGRE2010/REUPapers/
Margoliash.pdf.
Igor Aleksandrovic? Mel?c?uk. 1988. Dependency Syn-
tax: Theory and Practice. SUNY Press, Albany,
NY.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical Report MSI report 05133,
V?xj? University School of Mathematics and Sys-
tems Engineering, V?xj?, Sweden.
Timothy Osborne, Michael Putnam, and Thomas Gro?.
2012. Catenae: introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390. Association for Computational Lin-
guistics, Atlanta, Georgia, USA.
Alonzo Reed and Brainerd Kellogg. 1877. Work on
English grammar & composition. Clark & Maynard.
Federico Sangati and Chiara Mazza. 2009. An English
dependency treebank ? la Tesni?re. In Marco Pas-
sarotti, Adam Przepi?rkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eigth
International Workshop on Treebanks and Linguistic
Theories, pages 173?184. EDUCatt, Milan, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Mas-
ter?s thesis, University of Zurich.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. arXiv:1306.2091
[cs.CL]. arxiv.org/pdf/1306.2091.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel, Dordrecht and
Academia, Prague.
Stuart M. Shieber. 1992. Constraint-Based Grammar
Formalisms. MIT Press, Cambridge, MA.
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the
Third International Workshop on Parsing Technol-
ogy (IWPT?93), pages 277?292. Tilburg, Nether-
lands.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 132?140. Associa-
tion for Computational Linguistics, Prague, Czech
Republic.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2008), pages 254?263. As-
sociation for Computational Linguistics, Honolulu,
Hawaii.
Mark Steedman. 2000. The Syntatic Process. MIT
Press, Cambridge, MA.
Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.
2011. Design patterns in Fluid Construction Gram-
mar. Number 11 in Constructional Approaches to
Language. John Benjamins, Amsterdam.
Lucien Tesni?re. 1959. El?ments de Syntaxe Struc-
turale. Klincksieck, Paris.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
60
