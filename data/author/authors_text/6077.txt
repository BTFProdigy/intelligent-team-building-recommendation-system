 Word Sense Disambiguation using Static and Dynamic Sense 
Vectors 
 
Jong-Hoon Oh, and Key-Sun Choi 
Computer Science Division, Dept. of EECS, Korea Advanced Institute of Science & Technology 
(KAIST) / Korea Terminology Research Center for Language and Knowledge Engineering 
(KORTERM), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Korea 
Email: {rovellia,kschoi}@world.kaist.ac.kr 
 
Abstract  
It is popular in WSD to use contextual 
information in training sense tagged data. 
Co-occurring words within a limited 
window-sized context support one sense 
among the semantically ambiguous ones of 
the word. This paper reports on word sense 
disambiguation of English words using 
static and dynamic sense vectors. First, 
context vectors are constructed using 
contextual words 1  in the training sense 
tagged data. Then, the words in the context 
vector are weighted with local density. 
Using the whole training sense tagged data, 
each sense of a target word2 is represented 
as a static sense vector in word space, which 
is the centroid of the context vectors. Then 
contextual noise is removed using a 
automatic selective sampling. A automatic 
selective sampling method use information 
retrieval technique, so as to enhance the 
discriminative power. In each test case, a 
automatic selective sampling method 
retrieves N relevant training samples to 
reduce noise. Using them, we construct 
another sense vectors for each sense of the 
target word. They are called dynamic sense 
vectors because they are changed according 
to a target word and its context. Finally, a 
word sense of a target word is determined 
using static and dynamic sense vectors. The 
English SENSEVAL test suit is used for this 
experimentation and our method produces 
relatively good results. 
                                                     
1
 ?Contextual words? is defined as a list of content 
words in context. 
2
 In this paper, a target word ?Wt? is a semantically 
1. Introduction 
It is popular in WSD to use contextual 
information in training data (Agirre, et al, 
19963; Escudero, et al, 2000; Gruber, 1991; 
Schutze, 1998). Co-occurring words within a 
limited window-sized context support one sense 
among the semantically ambiguous ones of the 
word. The problem is to find the most effective 
patterns in order to capture the right sense. It is 
true that they have similar context and 
co-occurrence information when words are used 
with the same sense (Rigau, et al, 1997). It is 
also true that contextual words nearby an 
ambiguous word give more effective patterns or 
features than those far from it (Chen, et al, 
1998). In this paper, we represent each sense of 
a word as a vector in word space. First, 
contextual words in the training sense tagged 
data4 are represented as context vectors. Then, 
                                                                               
ambiguous word in a given context of ?Wt?. This 
context may consist of several sentences and it is 
represented by ?contextual words?. 
3
 Agirre et al, (1996) defines a term ?conceptual 
density? based on how many nodes are hit between 
WordNet node and target words+contexts. Unlike 
?Conceptual density?, ?local density? used in this 
paper does not use any semantic net like WordNet 
but use only the contextual words surrounding the 
given target word.. 
4
 In this paper, the English SENSEVAL-2 data for 
the lexical sample task is used as training sense 
tagged data. It is sampled from BNC-2, the Penn 
Treebank (comprising components from the Wall 
Street Journal, Brown, and IBM manuals) and so on. 
All items in the lexical sample are specific to one 
word class; noun, verb or adjective. Training sense 
tagged data is composed of training samples that 
support a certain sense of a target word. They contain 
 the words in the context vector are weighted 
with local density. Then, each sense of a target 
word can be represented as a sense vector, which 
is the centroid of the context vectors in word 
space. 
However, if training samples contain noise, it is 
difficult to capture effective patterns for WSD 
(Atsushi, et al, 1998). Word occurrences in the 
context are too diverse to capture the right 
pattern for WSD. It means that the dimension of 
contextual words will be very large when we 
will use all words in the training samples for 
WSD. To avoid the problems, we use an 
automatized hybrid version of selective 
sampling that will be called ?automatic selective 
sampling?. This automatization is based on 
cosine similarity for the selection.  For a given 
target word and its context, this method retrieves 
N-best relevant training samples using the cosine 
similarity. Using them, we can construct another 
sense vectors for each sense of the target word. 
The relevant training samples are retrieved by 
comparing cosine similarities between given 
contexts and indexed context vectors of training 
samples. The ?automatic selective sampling? 
method makes it possible to use traning samples 
which have higher discriminative power.  
 
This paper is organized as follows: section 2 
shows details of our method. Section 3 deals 
with experiments. Conclusion and future works 
are drawn in sections 4. 
2 Word Sense Disambiguation 
Method 
2.1 Overall System Description 
Figure 1 shows the overall system description. 
The system is composed of a training phase and 
a test phase. In the training phase, words in the 
limited context window of training samples, 
which contains a target word and its sense, are 
extracted and the words are weighted with local 
density concept (section 2.2). Then, context 
vectors, which represent each training sample, 
are indexed and static sense vectors for each 
                                                                               
a target word , its sense and its context. But the sense 
of contexual words is not annotated in the training 
samples (SENSEVAL-2, 2001) 
 
sense are constructed. A static sense vector is the 
centroid of context vectors of training samples 
where a target word is used as a certain sense 
(section 2.3). For example, two sense vectors of 
?bank? can be constructed using context vectors 
of training samples where ?bank? is used as 
?business establishment? and those where ?bank? 
is used as ?artificial embankment?. Each context 
vector is indexed for  ?automatic selective 
sampling?. 
Training samples
Word extraction in 
local context
Term weighting with 
local density
Indexing context 
vector
Context vectors
for each training 
sample
Constructing static 
sense vectors for 
each sense
Index for 
each training 
sample
Static sense 
vectors for 
each sense
Automatic selective 
sampling
Estimating a word 
sense
Dynamic sense 
vectors for the given 
context
Test samples
Word Senses
Constructing
Dynamic vectors
Retrieved N training 
samples
Training Testing
Morphological
analyzer
 
Fig. 1 The overall system description 
In the test phase, contextual words are extracted 
with the same manner as in the training phase 
(section 2.5). Then, the ?automatic selective 
sampling? module retrieves top-N training 
samples. Cosine similarity between indexed 
context vectors of training samples, and the 
context vector of a given test sample provides 
relevant training samples. Then we can make 
another sense vectors for each sense using the 
retrieved context vectors. Since, the sense 
vectors produced by the automatic selective 
sampling method are changed according to test 
samples and their context, we call them dynamic 
sense vectors in this paper (section 2.4) (Note 
that, the sense vectors produced in the training 
phase are not changed according to test samples. 
Thus, we call them static sense vectors.) 
The similarities between dynamic sense vectors, 
and a context vector of a test sample, and those 
between static sense vectors and the context 
vector of the test sample are estimated by cosine 
measure. The sense with the highest similarity is 
selected as the relevant word sense. 
 
Our proposed method can be summarized as 
follows 
Training Phase 
 1) Constructing context vectors using 
contextual words in training sense 
tagged data. 
2) Local density to weight terms in 
context vectors. 
3) Creating static sense vectors, which 
are the centroid of the context 
vectors. 
Test Phase 
1) Constructing context vectors using 
contextual words in test data. 
2) Automatic selective sampling of 
training vectors in each test case to 
reduce noise. 
3) Creating dynamic sense vectors, 
which are the centroid of the 
training vectors for each sense. 
4) Estimating word senses using static 
and dynamic sense vectors. 
2.2 Representing Training Samples as a 
Context Vector with Local Density 
In WSD, context must reflect various contextual 
characteristics5. If the window size of context is 
too large, the context cannot contain relevant 
information consistently (Kilgarriff et al, 2000). 
Words in this context window6 can be classified 
into nouns, verbs, and adjectives. The classified 
words within the context window are assumed to 
show the co-occurring behaviour with the target 
word. They provide a supporting vector for a 
certain sense. Contextual words nearby a target 
word give more relevant information to decide 
its sense than those far from it. Distance from a 
target word is used for this purpose and it is 
calculated by the assumption that the target 
words in the context window have the same 
sense (Yarowsky, 1995).  
Each word in the training samples can be 
weighted by formula (1). Let Wij(tk) represent a 
weighting function for a term tk, which appears 
in the jth training sample for the ith sense, tfijk 
                                                     
5  POS, collocations, semantic word associations, 
subcategorization information, semantic roles, 
selectional preferences and frequency of senses are 
useful for WSD (Agirre et al, 2001). 
6  Since, the length of context window was 
considered when SENSEVAL-2 lexical sample data 
were constructed, we use a training sample itself as 
context window. 
represent the frequency of a term tk in the jth 
training sample for the ith sense, dfik  represent 
the number of training samples for the ith sense 
where a term tk appears, Dijk represent the 
average distance of a term tk from the target 
word in the jth training sample for the ith sense, 
and Ni represent the number of training samples 
for the ith sense.  
 
ij
ijk
kij Z
Z
tW =)(     (1) 
where,  
???
?
???
?
???=
ik
ik
ijk
ijkijk N
N
DF
df
D
tfZ 1  
?? ==
sensesall
ikk
sensesall
i dfDFNN
__
   ,
 
( )?
=
=
termof
k
ijkij ZZ
__#
1
2
 
In formula (1), Z is a normalization factor, 
which forces all values of Wij(tk) to fall into 
between 0 and 1, inclusive (Salton et al, 1983). 
Formula (1) is a variation of tf-idf. We regard 
each training sample as indexed documents, 
which we want to retrieve and a test sample as a 
query in information retrieval system. Because 
we know a target word in training samples and 
test samples, we can restrict search space into 
training samples, which contain the target word 
when we find relevant samples. We also take 
into account distance from the target word.  
Dijk and dfik in formula (1) support a local 
density concept. In this paper, ?local density? of 
a target word ?Wt? is defined by the density 
among contextual words of ?Wt? in terms of their 
in-between distance and relative frequency. First, 
the distance factor is one of the important clues 
because contextual words surrounding a target 
word frequently support a certain sense: for 
example, ?money? in ?money in a bank?. 
Second, if contextual words frequently co-occur 
with a target word of a certain sense, they may 
be a strong evidence to decide what word sense 
is correct. Therefore, contextual words, which 
more frequently appear near a target word and 
appear with a certain sense of a target word, 
have a higher local density. 
With the local density concept, context of 
training samples can be represented by a vector 
 with context words and their weight, such that 
(wij(t1),wij(t2),?.,wij(tn)). When Wij(tk) is 1, it 
means that tk is strong evidence for the ith sense. 
(Zijk are much larger than others.) 
2.3 Constructing Static Sense Vectors  
Now, we can represent each training sample as 
context vectors using contextual words such that 
vij=(wij(t1),wij(t2),?.,wij(tn)) where vij represents a 
context vector of the jth training sample for the ith 
sense and wij(tk) is the weight of a term tk 
calculated by formula (1). 
||
||
1
i
N
j
ij
i N
v
SV
i?
=
=
    (2) 
Context vectors 
for Sense 1 
Context vectors 
for Sense 2
Context vector 
for Sense n
?
2SV
1SV
nSV
Fig.2 A graphical representation of static sense 
vectors 
 
Throughout clustering the context vectors, each 
sense can be represented as sense vectors. Let Ni 
represent the number of training samples for the 
ith sense, and vij represent the context vector of 
the jth training sample for the ith sense. The static 
sense vector for the ith sense, SVi, can be 
represented by formula (2) (Park, 1997). In 
formula (2), SVi is the centroid of context 
vectors of training samples for the ith sense as 
shown in figure 2. In figure 2, there are n senses 
and context vectors, which represent each 
training sample. We can categorize each context 
vector according to a sense of a target word. 
Then, each sense vectors are acquired using 
formula (2). Because the sense vectors are not 
changed according to test samples, we call them 
a static sense vector in this paper (note that sense 
vectors, which we will describe in section 2.4, 
are changed depending on the context of test 
samples). 
2.4 Automatic selective sampling: Dynamic 
Sense Vectors  
It is important to capture effective patterns and 
features from the training sense tagged data in 
WSD. However, if there is noise in the training 
sense tagged data, it makes difficult to 
disambiguate word senses effectively. To reduce 
its negative effects, we use a automatic selective 
sampling method using cosine similarity. Figure 
3 shows the process of a automatic selective 
sampling method. The upper side shows 
retrieval process and the lower side shows a 
graphical representation of dynamic sense 
vectors.  
Sense 1 Sense 2 Sense n
..
Retrieved 
Training 
Samples
..
..
A target word
DSV1 DSV2 DSVn
?
?
A Context vector for a test sample
Indexed 
Training 
Samples 
Context vectors 
for Sense 1
Context vectors 
for Sense 2
Context vectors 
for Sense n
?2DSV
1DSV
nDSV A context vector 
of a test sample
Retrieved top-N 
training sample
Fig. 3 A graphical representation of an automatic 
selective sampling method  
 
For example, let ?bank? have two senses 
(?business establishment?, ?artificial 
embankment?). Now, there are indexed training 
samples for the two senses. Then top-N training 
samples can be acquired for a given test sample 
containing a target word ?bank?. The retrieved 
 training samples can be clustered as Dynamic 
Sense Vectors according to a sense of their 
target word. Since, the sense vectors produced 
by a automatic selective sampling method are 
changed according to the context vector of a test 
sample, we call them dynamic sense vectors in 
this paper.  
Let RTi represent the number of training samples 
for the ith sense in the retrieved top-N, and vij 
represent a context vector of the jth training 
sample for the ith sense in the top-N. The 
dynamic sense vector for the ith sense of a target 
word, DSVi, is formulated by formula (3). In 
formula (3), DSVi means the centroid of the 
retrieved context vectors of training samples for 
the ith sense as shown in the lower side of 
figure.3 
||
||
1
i
RT
j
ij
i RT
v
DSV
i?
=
=
   (3) 
2.5 Context Vectors of a Test Sample 
Contextual words in a test sample are extracted 
as the same manner as in the training phase. The 
classified words in the limited window size ? 
nouns, verbs, and adjectives ? offer components 
of context vectors. When a term tk appears in the 
test sample, the value of tk in a context vector of 
the test sample will be 1, in contrary, when tk 
does not appear in the test sample, the value of tk 
in a context vector of the test sample will be 0. 
Let contextual words of a test sample be ?bank?, 
?river? and ?water?, and dimension of context 
vector be (?bank?, ?commercial?, ?money?, 
?river?, ?water?). Then we can acquire a context 
vector, CV =(1,0,0,1,1), from the test sample. 
Henceforth we will denote CVi as a context 
vector for the ith test sample. 
2.6 Estimating a Word Sense: Comparing 
Similarity 
We described the method for constructing static 
sense vectors, dynamic sense vectors and 
context vectors of a test sample. Next, we will 
describe the method for estimating a word sense 
using them. The similarity in information 
retrieval area is the measure of how alike two 
documents are, or how alike a document and a 
query are. In a vector space model, this is 
usually interpreted as how close their 
corresponding vector representations are to each 
other. A popular method is to compute the 
cosine of the angle between the vectors (Salton 
et al, 1983). Since our method is based on a 
vector space model, the cosine measure (formula 
(4)) will be used as the similarity measure. 
Throughout comparing similarity between SVi 
and CVj and between DSVi and CVj for the ith 
sense and the jth test sample, we can estimate the 
relevant word sense for the given context vector 
of the test sample. Formula (5) shows a 
combining method of sim(SVi,CVj) and 
sim(DSVi,CVj). Let CVj represent the context 
vector of the jth test sample, si represent the ith 
sense of a target word, and Score(si,CVj) 
represent score between the ith  sense and the 
context vector of the jth test sample. 
 
??
?
==
=
=
N
i i
N
i i
N
i ii
wv
wv
wvsim
1
2
1
2
1),(  (4) 
where, N represents the dimension of the vector 
space, v and w represent vectors.  
 
 
),()1(
),(
),(maxarg
ji
ji
ji
s
CVDSVsim
CVSVsim
CVsScore
i
??
+?
=
?
?
  (5) 
where ?  is a weighting parameter.  
  
Because the value of cosine similarity falls into 
between 0 and 1, that of Score(si,CVj) also exists 
between 0 and 1. When similarity value is 1 it 
means perfect consensus, in contrary, when 
similarity value is 0 it means there is no part of 
agreement at all. After all, the sense having 
maximum similarity by formula (5) is decided as 
the answer.  
3. Experiment 
3.1 Experimental Setup 
In this paper, we compared six systems as 
follows. 
The system that assigns a word sense 
which appears most frequently in the 
training samples (Baseline) 
The system by the Na?ve Bayesian 
method (A) (Gale, et al, 1992) 
 The system that is trained by 
co-occurrence information directly 
without changing. (only with term 
frequency) (B) 
The system with local density and 
without automatic selective sampling 
(C)  
The system with automatic selective 
sampling and without local density (D)  
The system with local density and 
automatic selective sampling (E) 
System A was used to compare our method with 
the other method. System B, C, D, and E will 
show the performance of each component in our 
proposed method. To evaluate performance in 
the condition of ?without local density (system B 
and D)?, we weight each word with its frequency 
in the context of training samples.  
The test suit used is the English lexical samples 
released for SENSEVAL-2 in 2001. This test 
suit supplies training sense tagged data and test 
data for noun, verb and adjective 
(SENSEVAL-2, 2001). 
Cross-validation on training sense tagged data is 
used to determine the parameters ? ?  in 
formula (5) and top-N in constructing dynamic 
sense vectors. We divide training sense tagged 
data into ten folds with the equal size, and 
determine each parameter, which makes the best 
result in average from ten-fold validation. The 
values, we used, are 2.0=? , and 50 =N . 
The results were evaluated by precision rates 
(Salton, et al, 1983). The precision rate is 
defined as the proportion of the correct answers 
to the generated results. 
3.2 Experimental Results 
 Noun Verb Adjective Total 
Baseline 50.97% 40.34% 58.04% 47.60% 
A 44.04% 32.48% 43.43% 39.09% 
B 24.33% 21.31% 26.92% 23.50% 
C 44.44% 33.81% 45.38% 40.15% 
D 65.47% 49.64% 66.84% 59.09% 
E 66.89% 53.74% 70.74% 62.07% 
Table 1. Experimental results  
Table 1 shows experimental results. In the result , 
all systems and baseline show higher 
performance on noun and adjective than verb. 
This indicates that the disambiguation of verb is 
more difficult than others in this test suit. In 
analysing errors, we found that we did not 
consider important information for 
disambiguating verb senses such as adverbs, 
which can be used as idioms with the verbs. For 
example, ?carry out?, ?pull out? and so on. It is 
necessary to handle them for more effective 
WSD.  
System B, C, D, and E show how effective local 
density and dynamic vectors are in WSD. The 
performance increase was shown about 70% 
with local density (system C) and about 150% 
with dynamic vectors (system D), when they are 
compared with system B ? without local density 
and dynamic vectors. This shows that local 
density is more effective than term frequency. 
This also shows that automatic selective 
sampling of training samples in each test sample 
is very important. 
Combining local density and dynamic vectors 
(system E), we acquire about 62% performance. 
Our method also shows higher performance than 
baseline and system A (the Na?ve Bayesian 
method) ? about 30% for baseline and about 
58% for system A. 
As a result of this experiment, we proved that 
co-occurrence information throughout the local 
density and the automatic selective sampling is 
more suitable and discriminative in WSD. This 
techniques lead up to 70% ~ 150% performance 
improvement in the experimentation comparing 
the system without local density and automatic 
selective sampling. 
4. Conclusion 
This paper reported about word sense 
disambiguation for English words using static 
and dynamic sense vectors. Content words ? 
noun, verb, and adjective ? in the context were 
selected as contextual words. Local density was 
used to weight words in the contextual window. 
Then we constructed static sense vectors for 
each sense. A automatic selective sampling 
method was used to construct dynamic sense 
vectors, which had more discriminative power, 
by reducing the negative effects of noise in the 
training sense tagged data. The answer was 
decided by comparing similarity. Our method is 
simple but effective for WSD.  
Our method leads up to 70~150% precision 
improvement in the experimentation comparing 
 the system without local density and automatic 
selective sampling. We showed that our method 
is simple but effective. Our method was 
somewhat language independent, because our 
method used only POS information. Syntactic 
and semantic features such as dependency 
relations, approximated word senses of 
contextual words and so on may be useful to 
improve the performance of our method. 
References  
Agirre, E. and G. Rigau (1996) Word Sense 
Disambiguation using Conceptual Density, 
Proceedings of 16th International Conference on 
Computational Linguistics(COLING96), 
Copenhagen, Denmark. 
Agirre, E. and D. Martinez, (2001) Knowledge 
Sources for Word Sense Disambiguation, 
Proceedings of the Fourth International 
Conference (TSD 2001). 
Fujii, Atsushi , Kentaro Inui, Takenobu Tokunaga, 
and Hozumi Tanaka, (1998) Selective Sampling for 
Example-based Word Sense Disambiguation, 
Computational Linguistics, 24(4), pp. 573-597. 
Escudero, G., L. M?rquez and G. Rigau (2000) 
Boosting Applied to Word Sense Disambiguation, 
Proceedings of the 11th European Conference on 
Machine Learning (ECML 2000) Barcelona, Spain. 
2000. Lecture Notes in Artificial Intelligence 1810. 
R. L. de M?ntaras and E. Plaza (Eds.). Springer 
Verlag. 
Gale, William A., Kenneth W. Church, and David 
Yarowsky (1992) A Method for Disambiguating 
Word Senses in a Large Corpus. Computers and 
Humanities, 26, 415-439. 
Gruber, T. R. (1991) Subject-Dependent 
Co-occurrence and Word Sense Disambiguation, 
Proceedings of 29th Annual Meeting of the 
Association for Computational Linguistics. 
Schutze, Hinrich (1998) Automatic Word Sense 
Discrimination. Computational Linguistics, 24(1), 
97-123.  
Chen , Jen Nan and Jason S. Chang (1998) A 
Concept-based Adaptive Approach to Word Sense 
Disambiguation, Proceedings of 36th Annual 
Meeting of the Association for Computational 
Linguistics and  17th International Conference on 
Computational Linguistics (COLING/ACL-98) pp 
237-243. 
Kilgarriff, A. and J. Rosenzweig, (2000) English 
SENSEVAL: Report and Results, Proceedings of 
2nd International Conference on Language 
Resources & Evaluation (LREC 2000), Athens.  
Park,Y.C (1997) ?Building word knowledge for 
information retrieval using statistical information?, 
Ph.D. thesis, Department of Computer Science, 
Korea Advanced Institute of Science and 
Technology. 
Rigau, G., J. Atserias and E. Agirre, (1997) 
Combining Unsupervised Lexical Knowledge 
Methods for Word Sense Disambiguation, 
Proceedings of joint 35th Annual Meeting of the 
Association for Computational Linguistics and 8th 
Conference of the European Chapter of the 
Association for Computational Linguistics 
(ACL/EACL?97), Madrid, Spain. 
Salton, G. and M.  McGill, (1983) Introduction to 
Modern Information Retrieval, McGraw-Hill, New 
York. 
SENSEVAL-2 (2001)http://www.sle.sharp.co.uk/ 
senseval2/ 
Yarowsky, D. (1995) Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods, In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics, 
Cambridge, MA, 189-196. 
 An English-Korean Transliteration Model Using Pronunciation and 
Contextual Rules 
 
Jong-Hoon Oh, and Key-Sun Choi 
Computer Science Division, Dept. of EECS, Korea Advanced Institute of Science & Technology 
(KAIST) / Korea Terminology Research Center for Language and Knowledge Engineering 
(KORTERM), 373-1, Kusong-dong, Yusong-gu, Taejon, 305-701, Korea 
Email: {rovellia,kschoi}@world.kaist.ac.kr 
 
Abstract  
There is increasing concern about 
English-Korean (E-K) transliteration 
recently. In the previous works, direct 
converting methods from English 
alphabets to Korean alphabets were a 
main research topic. In this paper, we 
present an E-K transliteration model using 
pronunciation and contextual rules. Unlike 
the previous works, our method uses 
phonetic information such as phoneme 
and its context. We also use word 
formation information such as English 
words of Greek origin. With them, our 
method shows significant performance 
increase about 31% in word accuracy.  
1.Introduction 
In Korean, many technical terms in a domain 
specific text, especially science and engineering 
are from foreign origin. Sometimes they are 
written in their original forms and sometimes 
they are transliterated into Korean words in 
various forms. This makes difficult to handle 
them in natural language processing. Especially 
information retrieval, words with the same 
meanings are treated as different ones because of 
their different forms.  
One possible solution can be a dictionary, which 
contains English words and their possible 
transliterated forms. However, this is not a 
practical solution because technical terms, which 
mainly cause the problem, usually have rich 
productivity. The other solution can be 
automatic transliteration. There have been works 
on automatic transliteration from English to 
other languages ? English to Japanese (Kang et 
al., 1996; Knight et al, 1997), and English to 
Korean (Kang et al, 2000; Kang et al, 2001; 
Kim et al, 1999; Lee et al, 1998).  
In E-K transliteration, direct converting methods 
from English alphabet to Korean alphabet were a 
main research topic (Kang et al, 2000; Kang et 
al., 2001; Kim et al, 1999; Lee et al, 1998). In 
the works, machine learning techniques such as 
a decision tree and a neural network were used.  
However, transliteration is more phonetic 
process than orthographic process: ?h? in the 
Johnson does not make any Korean character 
(Knight et al, 1997). Therefore, patterns for E-K 
transliteration acquired from English/Korean 
alphabets as in the previous works, may not be 
effective. In the previous works, they did not 
consider origin of English ? pure English (e.g., 
board), English words with Greek origin (e.g., 
hernia) and so on In E-K transliteration, origin 
of English words determine the way of 
transliteration. Our method uses phonetic 
information such as phoneme and its context as 
well as orthography. English words of Greek 
origin are also considered in transliteration. 
This paper organized as follows. In section 2, we 
survey related works. In section 3, we will 
describe the details of our method. In section 4, 
the results of experiments are represented. 
Finally, the conclusion follows in section 5. 
2. Related works 
2.1 Probability based transliteration 
(Lee et al, 1998) used formula (1) to generate a 
transliterated Korean word ?K? for a given 
English word ?E?. Lee et al (1998) defined a 
pronunciation unit. It is a chunk of graphemes or 
alphabets that can be mapped to phoneme. They 
divided an English word into pronunciation units 
 (PUs) for transliteration. For example, an 
English word ?board (/B AO R D/)? can be 
divided into ?b/B/: oa/AO/: r/R/: d/D/?1 ? ?b?, 
?oa?, ?r? and ?d? are PUs. An English word ?E? 
was represented as ?E=epu1,epu2,?,epun? where 
epui was the ith PU. Sequences of Korean PUs, 
K1,K2,?,Km, where  ?Ki= kpui1,kpui2,?,kpuin? 
were generated according to epui. Lee et al 
(1998) considered all possible English PU 
sequences and corresponding Korean PU 
sequences for a given English word, because its 
pronunciation was not determined. For example, 
?data? can have PU sequences such as ?d :at :a?, 
?da :ta?, ?d :a :t :a? and so on. If the total number 
of English PU in E is N and the average number 
of kpui generated by epui is M, the total number 
of generated Korean PU sequences will be about 
N*M. Then he selected the best result among 
them as a Korean transliteration word. 
)|()(maxarg)|(maxarg KEpKpEKp
KK
= (1) 
?
=
?
?
n
i
ii kpukpupkpupKP
2
11 )|()()(      (2) 
?
=
?
n
i
ii kpuepupKEP
1
)|()|(       (3) 
Kim et al, (1999) used the same formula as 
Lee?s (1998) except P(E|K) (formula(4)). He 
used additional information ? Korean PUs kpui-1 
and kpui+1 ? and used a neural network to 
approximate P(E|K). 
?
=
+??
n
i
iiii kpukpukpuepupKEP
1
11 ),,|()|(  (4) 
Probability based transliteration showed about 
40% precision on E-K transliteration with 1,500 
E-K pairs for training and 150 E-K pairs for 
testing. 
2.2 Decision Tree based transliteration 
Kang, et al (2000; 2001) proposed an English 
alphabet-to-Korean alphabet conversion method 
based on a decision tree. This method used six 
attribute values ? left three English alphabets 
and right three English alphabets ? for 
determining Korean alphabets corresponding to 
English alphabets. For each English alphabet, its 
corresponding decision trees are constructed. 
Table 1 shows an example of transliteration for 
an English word ?data?. In table 1, (E) represents 
                                                     
1
 Henthforce, ?:? will be used as a PU boundary 
a current English alphabet, K represents 
generated Korean alphabets by decision trees. 
L3 L2 L1 (E) R1 R2 R3  K 
< < < d a t a  ?d? 
< < d a t a >  ?e-i? 
< d a t a > >  ?t? 
d a t a > > >  ?a? 
Table 1. An example of decision tree based 
transliteration 
This method showed about 49% precision for 
6,185 E-K pairs for training and 1,000 E-K pairs 
for testing. 
 
Though the previous works showed relatively 
good results, they also showed some limitations. 
Because they focused on a converting method 
from English alphabet to Korean alphabet, they 
did not consider phonetic features such as 
phoneme and word formation features such as 
origin of English. This makes some errors when 
pronunciation and origin of English were 
important clues for transliteration - ?Mcdonald? 
(pronunciation is needed) and ?amylase? (origin 
of English word is needed). 
3. An English-Korean Transliteration 
Model using Pronunciation and 
Contextual Rules 
3.1 Overall System Description 
Figure1 shows the overall system description. 
Our method is composed of two phases ? 
alignment (section 3.2) and transliteration 
(section 3.3, 3.4, 3.5 and 3.6).  
First an English pronunciation unit2 (hearafter, 
EPU) and its corresponding phoneme are 
aligned. EPU-to-Phoneme alignment is to find 
out the most phonetically probable 
correspondence between an English 
pronunciation unit and phoneme. EPU to 
phoneme aligned results acquired from the 
alignment algorithm offer training data for 
estimating pronunciation of English words, 
which are not registered in a pronunciation 
dictionary, for example ?zinkenite?. Second, 
English words are transliterated into Korean 
words through several steps. Using an English 
                                                     
2
 The term ?pronunciation unit? will be used as the same 
meaning as in the Lee?s (Lee et al, 1998) 
 pronunciation dictionary (P-DIC), we can assign 
pronunciation to a given English word. When it 
is not registered in P-DIC, we investigate that it 
has a complex word form (section 3.3). For 
detecting a complex word form, we divide a 
given English word into two words 
(word+word)3 using entries of P-DIC. If both of 
them are in P-DIC, we can assign pronunciation 
to the given word otherwise we should estimate 
pronunciation (section 3.5). Then, we check 
whether the English word is from Greek origin 
or not (section 3.4). Because a way of E-K 
transliteration for the English words of Greek 
origin is different from that for pure English 
words, it is important to detect them. 
Pronunciation for English words, which are not 
registered in a P-DIC, is estimated (section 3.5) 
in the next step. Finally, Korean transliterated 
words are generated using conversion rules 
(section 3.6). The right side of figure 1 shows a 
transliteration example for an English word, 
?cutline?. 
Pronunciation
dictionary English  words
Dictionary Search
Detecting Complex Word forms
Detecting English words
of Greek origin
no
no
Estimating
pronunciation 
for E-class
Phoneme to Korean conversion
yes
yes
Estimating
pronunciation 
for G-class
noyes
EPU-P 
alignment
EPU-P 
Alignment
results
Training data
for estimating
pronunciation 
( E and G  
class) 
Detecting
English words
of Greek origin
Korean transliterated words
cutline
keo-teu-la-in
Complex word forms ?
(Yes)
Registered in 
a pronunciation
dictionary? (No)
Pronunciation to 
Korean conversion
[C/K]:[u/AH]:[T/T]
[L/L]:[I/AY]:[ne/N]
Fig. 1 Overall system description 
3.2 EPU-to-Phoneme Alignment 
EPU-to-Phoneme (hereafter, EPU-P) alignment 
is to find out the most phonetically probable 
correspondence between an English 
pronunciation unit and phoneme. For example, 
one of the possible alignment for an English 
word ?board? and its pronunciation ?/B AO R 
D/?4 is as follows. 
                                                     
3
 ?broadcasting? may be divided into three words : ?broad?, 
?cast? and ?ing?. But from the training corpus and 
pronunciation dictionary, all of complex word is divided 
into two words like ?broad? and ?casting?. 
4 (www.cs.cmu.edu/~laura/pages/arpabet.ps):  ARPAbet 
symbol will be used for representing phonemes. ARPAbet 
English b oa r d 
 | | | | 
Pronunciation /B/ /AO/ /R/ /D/ 
Table 2. One possible alignment between English 
word ?board? and its pronunciation 
For automatic EPU-P alignment, we used the 
modified version of Kang?s E-K alignment 
algorithm (Kang et al, 2000; Kang et al, 2001). 
It is based on Covington?s algorithm (Covington, 
1996). Covington views an alignment as a way 
of stepping through two words ? a word in one 
side and a word in the other side ? while 
performing ?match? or ?skip? operation on each 
step. Kang added ?forward bind? and ?backward 
bind? operations to consider one-to-many, 
many-to- one and many-to-many alignments 
Operation Condition Penalty 
Similar C/CP 0 
V/VP 0 
V/SVP or C/SVP 30 
Dissimilar C/CP 240 
Match 
V/CP or C/VP 250 
Similar C/CP 0 
V/VP 0 
V/SVP or C/SVP 30 
Dissimilar C/CP 190 
Bind 
V/CP or C/VP 200 
Table 3. Penalty metrics: C, V, CP, VP, and SVP 
represent consonants, vowels, consonant 
phonemes, vowel phonemes 5  and semi-vowel 
phonemes respectively. 
English b o a r D Total 
Operation M M < M M Penalty 
Pronunciation B AO < R D  
Penalty +0 +0 +0 +0 +0 0 
Table 4. The best alignment result for an English 
word ?board?. ?M? represents ?match?, and ?<? 
represents ?backward bind?. 
Unlike the previous alignment algorithm, we 
combine ?skip? and ?bind? operations because 
the ?skip? operation can be replaced with the 
?bind? operation. This makes all PUs to be 
mapped into phoneme. It means that our 
algorithm does not allow null-to-phoneme 
alignment or PU-to-null alignment. All the valid 
alignments that are possible by ?match?, and 
?bind? operations can be generated. Alignment 
                                                                               
is one of the method for coding phonemes into ASCII 
chracters. 
5
 In this paper, vowel pronunciation includes diphthongs. 
 may be interpreted as finding the best result 
among them. To find the best result, a penalty 
scheme is used ? the best alignment result is one 
that has the least penalty values. Since Kang?s 
method focused on an E-K character alignment, 
a penalty scheme and an E-K 
character-matching table were restricted to an 
E-K alignment. Instead of Kang?s E-K character 
penalty scheme, we developed an EPU-P penalty 
scheme and an EPU-P matching table using 
manually aligned EPU-P data. We assume that 
all vowels can be aligned with all vowel 
phonemes without penalty. Table 3 shows our 
penalty metrics and table 4 shows an example of 
EPU-P alignment. 
We aligned about 120,000 English word and 
Pronunciation pairs in ?The CMU Pronouncing 
Dictionary?. For evaluating performance of the 
alignment, we randomly selected 100 results. 
The performnance of EPU-P alignment is 99%. 
3.3 Dealing with a Complex word form 
Some English words are not in P-DIC, because 
they are in a complex word form. In this paper, 
we define words in a complex word form as 
those composed of two base nouns in P-DIC. 
When a given word is not in P-DIC, it is 
segmented into all possible two words. If the 
two words are in P-DIC, we can assign their 
pronunciation. For example, ?cutline? can be 
segmented into ?c+utline?, ?cu+tline?, ?cut+line? 
and so on. ?cut+line? is the correct segmentation 
of ?cutline?, because ?cut? and ?line? are in the 
P-DIC. If words are not in P-DIC and they are 
not in a complex word form, we should estimate 
their pronunciation. The details of estimating 
pronunciation will be described in the section 
3.5. 
3.4 Detecting English words of Greek origin  
In Korean, there are two methods for E-K 
transliteration ? ?written word transliteration? 
and ?spoken word transliteration? (Lee et al, 
1998). The two methods use similar mechanism 
for consonant transliteration. However, ?written 
word transliteration? uses its character and 
?spoken word transliteration? uses its phoneme 
when they transliterate vowels. For example, ?a? 
in ?piano? can be transliterated into ?pi-a-no? 
with its character and ?pi-e-no? with its phoneme. 
Since, a vowel in a pure English word is usually 
transliterated using its phoneme and that in an 
English word of Greek origin is usually 
transliterated with its character in E-K 
transliteration- for example, ?hernia? 
(he-reu-ni-a), ?acacia? (a-ka-si-a), ?adenoid? 
(a-de-no-i-deu) and so on -, it is important to 
detect them. We use suffix and prefix patterns 
for detecting English words of Greek origin 
(Luschnig, 2001) 6  and table 5 7  shows the 
patterns. If words have the affixes in table 5, we 
determine them as words of Greek origin 
otherwise pure English words. 
Suffix -ic, -tic, -ac, -ics, -ical, -oid, -ite, -ast, 
-isk, -iscus, -ia, -sis, -me, -ma 
Prefix amphi-, ana-, anti-, apo-, dia-, dys-, ec-, 
ecto-, enantio-, endo-, epi-, cata-, cat-, 
meta-, met-, palin-, pali-, para-, par-, 
peri-, pros-, hyper-, hypo-, hyp- 
Table 5. Suffix and prefix patterns for detecting 
English words of Greek origin. 
3.5 Estimating Pronunciation  
Estimating pronunciation is composed of two 
steps. Using aligned EPU-P pairs as training 
data, we can find EPUs in the given English 
word (Chunking EPU) and assign their 
appropriate phoneme (EPU-to-Phoneme 
assignment). For dealing with English words of 
Greek origin, we categorize EPU-P aligned data 
into pure English words (E-class) and English 
words of Greek origin (G-class). Then we 
construct the ?Chunking EPU? module and the 
?EPU-to-Phoneme assignment? module for each 
class.  
?Chunking EPU? is to find out boundaries of 
EPUs in English words. For example, we can 
find EPUs in ?board? as ?b:oa:r:d?. For chunking 
EPU, we used C4.5 (Quilan, 1993) with ten 
attributes ? left five alphabets and right five 
alphabets and the setting shows the best result 
among various settings such as eight attributes 
(left four and right four - 87.2% ) and so on. 8. 
                                                     
6
 
38 Grek affixes out of 249 Latin and Greek affixes in 
120 categories described in (John, 1953) are used. 63 out of 
the 120 categories share the meaning though their form is 
somewhat different  
7
 In this paper, some Greek affixes are not used, because 
they such as prefix ?a-?, ?an-?, and postfix ?-y?, ?-m? may 
cause error. 
8
 C4.5 is one of the popular method for recognizing 
boundary of chunks. Unlike Kang et al, (2000)?s method, 
 We use 90% of EPU-P aligned data as training 
data and 10% of those as test data. Our 
?Chunking EPU? module shows 91.7% 
precision. 
)|()(maxarg)|(maxarg PEpPpEPp
PP
=   (5) 
?
=
?
?
n
i
ii pppppPP
2
11 )|()()(       (6) 
?
=
?
n
i
ii pepupPEP
1
)|()|(       (7) 
Then we can assign phoneme to each EPU. For 
the given EPU sequence ?E=epu1,epu2,?,epun? 
and its possible phoneme sequences P1,..,Pm 
where ?Pi=pi1,pi2,?,pin?, the ?EPU-to-Phoneme 
assignment? task is to find out the most probable 
phomene sequence ?Pi=pi1,pi2,?,pin?. It can be 
represented as formula (5). p(P) and p(E|P) are 
approximated as formula (6) and (7). 
3.6 Phoneme-to-Korean Conversion 
Our Phoneme-to-Korean (P-K) conversion 
method is based on English-to-Korean Standard 
Conversion Rule (EKSCR) (Ministry, 1995). 
EKSCR is composed of nine general rules and 
five rules for specific cases ? each rule contains 
several sub-rules. It describes a transliteration 
method from English alphabets or phonemes to 
Korean alphabets. It uses English phoneme as a 
transliteration condition ? if a phoneme is A 
then transliterate into a Korean alphabet B. 
However, EKSCR does not contain enough rules 
to generate correct Korean words for 
corresponding English words, because it mainly 
focuses on a way of mapping from one English 
phoneme to one Korean character without 
context of phonemes and PUs. For example, an 
English word ?board? and its pronunciation ?/B 
AO R D/?, are transliterated into ?bo-reu-deu? by 
EKSCR ? the correct transliteration is ?bo-deu?. 
In E-K transliteration, the phoneme ?R? before 
consonant phonemes and after vowel phonemes 
is rarely transliterated into Korean characters 
(Note that the phoneme ?R? in English words of 
Greek origin is transliterated into a Korean 
                                                                               
our method produces EPU and it phoneme. This makes 
possible for a E-K conversion method (in section 3.6) to 
use context of EPU and its phoneme. Because an 
alphabet-to-alphabet mapping method did not use EPU and 
its phoneme, it may show some errors when phoneme and 
its context are the most importnat clues, for example, 
?Mcdonald?. 
consonant ?r? frequently.) These contextual rules 
are very important to generate correct Korean 
transliterated words. 
 
We capture contextual rules by observing errors 
in the results, which are generated by applying 
EKSCR to 200 randomly selected words from 
the CMU pronunciation dictionary. The selected 
words are not in the test data in the experiment. 
Among the generated rules, we selected 27 
contextual rules with high frequency (above 5). 
Table 6 shows some rules and their conditions in 
which rules will be fired. There are three 
conditions ? ?Context?, ?TPU (Target PU)?, and 
?TP (Target Phoneme)?. In context condition, 
?[]?, ?{}?, C, VP, and CP represent phoneme, 
pronunciation unit, consonant, vowel phonemes 
and consonant phonemes respectively. The rule 
with context condition, ?[R] after VP and before 
CP?, is not fired for the English words of Greek 
origin. Except it, all rules are applied to both 
classes (E-class and G-class). 
Condition 
Context  TPU TP 
Korean 
Characters 
C+ {le} ?le? AH L ?eul? 
{or} in the end of 
a word 
?or? ER ?eo? 
{or} in a word ?or? ER ?eu? 
{sm} in the end 
of a word 
?sm? S AH M ?jeum? 
[R] after VP and 
before CP 
?r? ?R? ?eu? 
Table 6. Some contextual rules 
4.Experiment 
4.1 Experimental Setup 
We use two data sets for an accuracy test. Test 
Set I (Lee et al, 1998) is composed of 1,650 
E-K pairs. Since, the test set was used as a 
common testbed for (Lee et al, 1998; Kim et al, 
1999; Kang et al, 2000; Kang et al, 2001), we 
use them as a testbed for comparison between 
our method and other methods. For comparison, 
1,500 pairs are used as training data for other 
methods and 150 pairs are used as test data for 
our method and other methods. Test set II (Kang 
et al, 2000) consists of 7,185 E-K pairs ? the 
number of training data is 6,185 and that of test 
data is 1,000. We use Test set II to compare our 
 method with (Kang et al, 2000), which shows 
the best result among the previous works. 
Evaluation is performed by word accuracy (W. 
A.) and character accuracy (C.A.), which were 
used as the evaluation measure in the previous 
works (Lee and Choi 1998; Kim and Choi 1999; 
Kang and Choi 2000). 
wordsgeneratedof
wordscorrectofAW
  #
  #
.. =   (8) 
L
sdiLAC )(.. ++?=    (9) 
where L represents the length of the original 
string, and di, , and s  represent the number 
of insertion, deletion and substitution 
respectively. If )( sdiL ++< , we consider it 
as zero (Hall and Dowling, 1980). 
We perform the three experiments as follows. 
Comparison Test: Comparison between 
our method and the previous works 
Dictionary Test: Performance of 
transliteration for words in a 
pronunciation dictionary and that for 
others 
Component Test: Effectiveness of each 
component 
4.2 Experimental results 
4.2.1 Comparison Test 
Method C.A W.A 
[Lee et al, 1998] 69.3% 40.7%9 
[Kim et al, 1999] 79.0% 35.1% 
[Kang et al, 2000] 78.1% 37.6% 
Our method 90.82% 56.0% 
Table 7 Comparison test results for Test set I 
Method C.A W.A 
[Kang et al, 2000, 2001] 81.8% 48.7% 
Our method 92.86% 63.0% 
Table 8 Comparison test results for Test set II. 
Table 7 and 8 show results of comparison test 
for Test set I and Test set II respectively. In the 
tables our method shows higher performance 
especially in W.A. Moreover, our method shows 
higher performance in C.A. It means that the 
generated words by our method are more similar 
to the correct transliteration, when they are not 
the correct answer. 
                                                     
9
 with 20 higher rank results. 
4.2.2 Dictionary Test 
For the dictionary test, we use test data of Test 
set II. In the result, ?registered? words show 
higher performance. It can be analysed that 
contextual rules are constructed using registered 
words in a P-DIC and estimating pronunciation 
module makes some errors. However, ?not 
registered? words also show relatively good 
performance. 
 C.A W.A # of words 
Registered 93.49% 67.83% 687 
Not 
registered 
91.47% 52.40% 313 
Table 9. Dictionary test results. 
4.2.3 Component Test 
For the component test, we use words, which are 
?not registered? in Test set II. Components, 
which are tested in ?Component test? are 
?Dealing with words in a complex word 
form?[C], ?Detecting English words of Greek 
origin? [G], and ?Contextual rules? [R]. In the 
result, [G] and [R] show good results in contrary 
to [C]. There are so few words in complex word 
forms that [C] does not show significant 
performance improvement though the 
performance is relatively good ?about 70% W.A. 
for 43 words (43 words out of total 313 words). 
For the effective comparison, it will be 
necessary to consider the number of words, 
which each component handles. Our method 
shows better performance than ?W/O 
[R]?(EKSCR). It indicates that contextual rules 
are important. 
Method C.A. W.A. 
W/O [C], [G], and [R] 87.90% 23.96% 
W/O [G] 88.45% 36.10% 
W/O [C] 91.99% 50.16% 
W/O [R] 89.78% 44.41% 
[C]+[G]+[R] (proposed) 91.47% 52.40% 
Table 10. Component test results. 
4.3. Discussion 
The previous works focused on an 
alphabet-to-alphabet mapping method. Because, 
how the transliteration is more phonetic than 
orthographic, without phonetic infomation10 it 
                                                     
10
 Hangul alphabet has phonetic as well as orthographic. It 
may be adopted to our method as phoneme. Because one 
 may be difficult to acquire more relevant result. 
In the result, ?crepe(keu-le-i-peu/ keu-le-pe) 11?, 
?dealer (dil-leo/ di-eol-leo)?, ?diode (da-i-o-deu/ 
di-o-deu)?, and ?pheromone (pe-ro-mon/ 
pe-eo-o-mon)? etc. produce errors in the 
previous works because they are transliterated 
into Korean with pronunciation and the patterns 
can not be acquired from an 
alphabet-to-alphabet mapping method. For 
example, ?e? before ?p? in ?crepe? is 
transliterated into Korean chracters ?e-i? but it is 
usually transliterated into ?e? in training data. 
Origin of English word also contributes 
performance improvement. For example, words 
such as ?hittite (hi-ta-i-teu /ha-i-ta-i-teu)?, 
?hernia (he-leu-ni-a/ heo-ni-a)?, ?cafeteria 
(ka-pe-te-li-a/ ka-pi-te-ri-a)?.  In summary, E-K 
transliteration is not an alphabet-to-alphabet 
mapping problem but a problem that can be 
solved with mixed use of alphabet, phoneme, 
and word formation information. 
In the experiments, we find that vowel  
transliteration is the main reason of errors rather 
than consonant transliteration in E-K 
transliteration. Especially, ?AH? is the most 
ambiguous phoneme because it can be several 
Korean characters such as ?eo?, ?e?, ?u?, and so 
on. To improve performance of E-K 
transliteration, more specific rules may be 
necessary to handle vowel transliteration.  
5. Conclusion 
We propose an English-Korean transliteration 
model using pronunciation and contextual rules. 
Unlike the previous works, our method use 
phonetic and orthographic information for 
transliteration. With them our method showed 
significant performance increase about 31%. We 
also showed that origin of English words was 
important in E-K transliteration.  
In future works, a study is attempting to develop 
a method for handling English of various foreign 
origin, which this paper did not handle. To 
improve accuracy, contextual rules must be 
added using larger data. Our method may be 
useful to many NLP applications such as 
                                                                               
EPU may produce many phonemes, it may be difficult to 
acquire a good result without context of phoneme and EPU. 
11
 English word (correct transliteration / transliteration by 
the previous works) 
automatic bi-lingual dictionary construction, 
information retrieval, machine translation, 
speech recognition and so on. 
References  
Brown, P. F. and et al (1990), ?A Statistical 
Approach to Machine Translation,? Computational 
Linguistics, Vol 16 (2), June. 
Covington, M. A., (1996). ?An algorithm to align 
words for historical comparison?, Computational 
Linguistics, 22. 
Hall, P., and G. Dowling, (1980), ?Approximate 
string matching,? Computing Surveys, 12(4), 
381-402. 
John Hough, (1953) ?Scientific Terminology?  New 
York: Rhinehart & Company, Inc. 
Kang, Y. and A. A. Maciejewski, (1996). ?An 
algorithm for Generating a Dictionary of Japanese 
Scientific Terms?, Literary and Linguistic 
Computing, 11(2). 
Kang B.J. and K-S. Choi (2000), ?Automatic 
Transliteration and Back-transliteration by 
Decision Tree Learning?, In Proceedings of the 
2nd International Conference on Language 
Resources and Evaluation, Athens, Greece. 
Kang B.J. and  Key-Sun Choi, (2001) ?Two 
approaches for the resolution of word mismatch 
problem caused by English words and foreign 
words in Korean information retrieval?, 
International journal of computer processing of 
oriental language vol 14/No 2, 109-131 
Kim J.J., J.S. Lee, and K-S. Choi., (1999). 
?Pronunciation unit based automatic 
English-Korean transliteration model using neural 
network?, In Proceedings of Korea Cognitive 
Science Association (in Korean) 
Knight, K. and J. Graehl, (1997). ?Machine 
Transliteration?. In Proceedings. of the 35th 
Annual Meetings of the Association for 
Computational Linguistics (ACL) Madrid, Spain. 
Lee, J. S. and K. S. Choi, 1998. English to Korean 
Statistical transliteration for information retrieval. 
Computer Processing of Oriental Languages, 
12(1):17-37. 
Luschnig, C.A.E. (2001) English word origin, 
http://www.ets.uidaho.edu/luschnig/EWO 
Ministry of culture and tourism, Republic of Korea, 
?English-to-Korean Standard conversion rule?, 
1995 (in Korean) 
Quinlan,J.R. (1993), ?C4.5: Programs for Machine 
Learning?, Morgan Kauffman. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 450 ? 461, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
An Ensemble of Grapheme and Phoneme  
for Machine Transliteration 
Jong-Hoon Oh and Key-Sun Choi 
Department of Computer Science, KAIST/KORTERM/BOLA, 
373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea 
{rovellia, kschoi}@world.kaist.ac.kr 
Abstract. Machine transliteration is an automatic method to generate characters 
or words in one alphabetical system for the corresponding characters in another 
alphabetical system. There has been increasing concern on machine translitera-
tion as an assistant of machine translation and information retrieval. Three ma-
chine transliteration models, including ?grapheme-based model?, ?phoneme-
based model?, and ?hybrid model?, have been proposed. However, there are 
few works trying to make use of correspondence between source grapheme and 
phoneme, although the correspondence plays an important role in machine 
transliteration. Furthermore there are few works, which dynamically handle 
source grapheme and phoneme. In this paper, we propose a new transliteration 
model based on an ensemble of grapheme and phoneme. Our model makes use 
of the correspondence and dynamically uses source grapheme and phoneme. 
Our method shows better performance than the previous works about 15~23% 
in English-to-Korean transliteration and about 15~43% in English-to-Japanese 
transliteration. 
1   Introduction 
Machine transliteration is an automatic method to generate characters or words in one 
alphabetical system for the corresponding characters in another alphabetical system. 
For example, English word data is transliterated into Korean ?deita? 1 and Japanese 
?deeta?. Transliteration is used to phonetically translate proper names and technical 
terms especially from languages in Roman alphabets to languages in non-Roman 
alphabets such as from English to Korean, Japanese, and Chinese and so on. There 
has been increasing concern on machine transliteration as an assistant of Machine 
Translation (MT) [2], [10], mono-lingual information retrieval (MLIR) [8], [11] and 
cross-lingual information retrieval (CLIR) [6]. In the area of MLIR and CLIR, ma-
chine transliteration bridges the gap between a transliterated localized form and its 
original form by generating all possible transliterated forms from each original form. 
Especially for CLIR, machine transliteration gives a help to query translation where 
proper names and technical terms frequently appear in source language queries. In the 
area of MT, machine transliteration prevents translation failure when translations of 
                                                          
1
  In this paper, target language transliterations are represented with their Romanization form in 
a quotation mark (??) .  
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 451 
proper names and technical terms are not registered in a translation dictionary. A 
machine transliteration system, therefore, may affect the performance of MT, MLIR, 
and CLIR system. 
Three machine transliteration models have been studied: called ?grapheme2-based 
transliteration model (?G)? [7], [8], [9], [11], [12], [13], ?phoneme3-based translit-
eration model (?P)? [10], [12], and ?hybrid transliteration model (?H)? [2], [4], 
[12]. ?G and ?P are classified in terms of units to be transliterated. ?G is referred to 
the direct model because it directly transforms source language graphemes to target 
language graphemes without any phonetic knowledge of source language words. ?P is 
called the pivot model because it makes use of phonemes as a pivot during a translit-
eration process. Therefore ?P usually needs two steps; the first step is to produce 
phonemes from source language graphemes, and the second step is to produce target 
language graphemes from phonemes. ?H combines ?G and ?P with the linear interpo-
lation style. Hereafter, we will use a source grapheme for a source language grapheme 
and a target grapheme for a target language grapheme. 
Though transliteration is the phonetic process (?P) rather than the orthographic one 
(?G) [10], we should consider both source grapheme and phoneme to achieve high 
performance in machine transliteration because the standard transliterations are not 
restricted to phoneme-based transliterations4. However, many previous works make 
use of either source grapheme or phoneme. They simplify a machine transliteration 
problem into either ?G or ?P assuming that one of ?G and ?P is able to cover all trans-
literation behaviors. However, transliteration is a complex process, which does not 
rely on either source grapheme or phoneme. For example, the standard Korean trans-
literations of amylase and data are grapheme-based transliteration ?amillaaje? and 
phoneme-based transliteration ?deiteo?, respectively. A machine transliteration model, 
therefore, should reflect the dynamic transliteration behaviors in order to produce the 
correct transliterations.  
?H has the limited power for producing the correct transliterations because it just 
combines ?G and ?P with the linear interpolation style. ?H does not consider corre-
spondence between source grapheme and phoneme during the transliteration process. 
However the correspondence plays important roles in machine transliteration. For 
example, phoneme /AH/5 produces high ambiguities since it can be mapped to almost 
every single vowels in source language and target language (the underlined grapheme 
corresponds to /AH/: cinema, hostel, holocaust in English, ?sinema?, ?hostel?, ?hol-
lokoseuteu? in their Korean counterparts, and ?sinema?, ?hoseuteru?, ?horokoosuto? in 
                                                          
2
  Graphemes refer to the basic units (or the smallest contrastive units) of written language: for 
example, English has 26 graphemes or letters, Korean has 24, and German has 30. 
3
  Phonemes are the simplest significant unit of sound (or the smallest contrastive units of the 
spoken language): for example, the /M/, /AE/, and /TH/ in math. 
4
  In an English-to-Korean transliteration test set [14], we find that about 60% are phoneme-
based transliterations, while about 30% are grapheme-based ones. The others are translitera-
tions generated by combining ?G and ?P. 
5
  ARPAbet symbol will be used for representing phonemes. ARPAbet is one of the methods 
used for coding phonemes into ASCII characters (www.cs.cmu.edu/~laura/pages/arpabet.ps). 
In this paper, we will denote phonemes and pronunciation with two slashes like so : /AH/.  
Pronunciation represented in this paper is based on The CMU Pronunciation Dictionary and 
The American Heritage(r) Dictionary of the English Language.  
452 J.-H. Oh and K.-S. Choi 
their Japanese counterparts). If we know the correspondence between source graph-
eme and phoneme in this context, then we can more easily infer the correct translitera-
tion of /AH/, since a target grapheme of /AH/ usually depends on a source grapheme 
corresponding to /AH/. Korean transliterations of source grapheme a is various such 
as ?a?, ?ei?, ?o?, ?eo? and so on. Like the previous example, correspondence makes it 
possible to reduce transliteration ambiguities like Table 1. In Table 1, the underlined 
source grapheme a in the example column is pronounced as the phoneme in the pho-
neme column. The correct Korean transliterations of source grapheme a can be more 
easily found, like in the Korean grapheme column, by means of phonemes in the 
phoneme column. 
Table 1. Examples of Korean graphemes derived from source grapheme a and its correspond-
ing phoneme: the underline indicates source graphemes corresponding to each phoneme in the 
phoneme column 
Korean grapheme  Phoneme  Example 
?a? /AA/ adagio,  safari, vivace 
?ae? /AE/ advantage, alabaster, travertine 
?ei? /EY/ chamber, champagne, chaos 
?i? /IH/ advantage, average, silage 
?o? /AO/ allspice, ball, chalk 
In this paper, we propose a new machine transliteration model based on an ensem-
ble of source grapheme and phoneme, symbolized as ?C (?correspondence-based 
transliteration model?). ?C has two strong points over ?G, ?P, and ?H. First, ?C can 
produce transliterations by considering correspondence between source grapheme and 
phoneme. As described above, correspondence is very useful for reducing translitera-
tion ambiguities. From the viewpoint of reducing the ambiguities, ?C has an advan-
tage over ?G, ?P, and ?H because ?C can more easily reduce the ambiguities by con-
sidering the correspondence. Second, ?C can dynamically handle source grapheme 
and phoneme according to their contexts. Because of this property, ?C can produce 
grapheme-based transliterations as well as phoneme-based transliterations. It can also 
produce a transliteration, where one part is a grapheme-based transliteration and the 
other part is a phoneme-based transliteration. For example, the Korean transliteration 
of neomycin, ?neomaisin?, where ?neo? is a grapheme-based transliteration and 
?maisin? is a phoneme-based transliteration. 
2   Correspondence-Based Machine Transliteration Model 
Correspondence-based transliteration model (?C) is composed of two component 
functions (?C: ?p??t). In this paper, we refer to ?p as a function for ?producing pro-
nunciation? and ?t as a function for ?producing target grapheme?. First, ?p pro-
duces pronunciation and then ?t produces target graphemes with correspondence be-
tween source grapheme and phoneme produced by ?p. The goal of the ?p is to produce 
the most probable sequence of phonemes corresponding to source graphemes. For 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 453 
example, ?p produces /B/, /AO/, /~/6, /R/, and /D/ for each source grapheme, b, o, a, r, 
and d in board (see ?The result of ?p? in the right side of Fig 1). In this step, pronun-
ciation is generated through two ways; pronunciation dictionary search and pro-
nunciation estimation. A pronunciation dictionary contains the correct pronunciation 
corresponding to English words. Therefore, English words are first investigated 
whether they are registered in the dictionary otherwise their pronunciation is esti-
mated by pronunciation estimation. The goal of ?t is to produce the most probable 
sequence of target graphemes with correspondence between source grapheme and 
phoneme, which is the result of ?p. For example, ?t produces ?b?, ?o?, ?~?, ?~?, and 
?deu? using the result of ?p, b-/B/, o-/AO/, a-/~/, r-/R/, and d-/D/ (see ?The result of ?t? 
in the right side of Fig 1). Finally, the target language transliteration, such as the Ko-
rean transliteration ?bodeu? for board, can be acquired by concatenating the sequence 
of target graphemes in the result of ?t. 
English word
Training Data 
for PE
Training Data for
 t
Dictionary searchi ti r  r
 tt
Transliterations
Pronunciation 
Dictionary
Pronunciation 
Estimation
r ti  
ti ti
board
/D//R//~//AO//B/
draob
/D//R//~//AO//B/
?deu?~~?o??b?
draob
 p
 p
 tt
Result of  p
The result of  p r t f p
The result of  t r lt f t
?bodeu?
 p: Producing Pronunciation
 t : Producing Target Grapheme
p: r i  r i ti
t : r i  t r
 
Fig. 1. The overall system architecture 
Table 2. Feature types used for correspondence-based transliteration model: where S is a set of 
source graphemes (e.g. English alphabets), P is a set of phonemes defined in ARPABET, T is a 
set of target graphemes. Note that fS,GS is a symbol for indicating both fS and fGS. fP,GP is a sym-
bol for indicating both fP and fGP. 
Feature Type Description Possible feature values 
fS,GS fS Source graphemes Source grapheme in S; 26 alphabets 
for English  
 fGS Source grapheme type Consonant (C), and Vowel (V) 
fP,GP fP Phonemes  Phonemes in P (/AA/, /AE/, etc.) 
 fGP Phoneme type Consonant (C), Vowel (V), Semi-
vowel (SV) and silence (/~/) 
 fT Target graphemes Target graphemes in T 
Pronunciation estimation in ?p and ?t are trained by machine learning algorithms. 
To train each component function, we need features that represent training instance 
                                                          
6
  In this paper, ?/~/? represents silence and ?~? represents null target grapheme. 
454 J.-H. Oh and K.-S. Choi 
and data. Table 2 shows five feature types, fS,  fP, fGS, fGP, and fT that our model uses. 
Depending on component functions, different feature types are used. For example, 
?p(si) uses (fS, fGS, fP) and ?t(si, ?p(si)) does (fS,  fP, fGS, fGP, fT). 
2.1   Producing Pronunciation (?p) 
Producing pronunciation (?p:S?P) is a function that finds phonemes in a set P for 
each source grapheme, where P is a set of phonemes defined in ARPABET, and S is a 
set of source graphemes (e.g. English alphabets). The results of this step can be repre-
sented as a sequence of correspondences between source grapheme and phoneme. We 
will denote it as GP={gp1,gp2,?,gpn; gpi=(si,?p(si))} where si is the ith source graph-
eme of SW=s1,s2,...,sn. Producing pronunciation is composed of two steps. The first 
step involves a search in the pronunciation dictionary, which contains English words 
and their pronunciation. This paper uses The CMU Pronouncing Dictionary7, which 
contains 120,000 English words and their pronunciation. The second step involves 
pronunciation estimation. If an English word is not registered in the pronunciation 
dictionary, we must estimate its pronunciation.  
Table 3. An example of pronunciation estimation for b in board 
Feature type L3 L2 L1 C0 R1 R2 R3 ?p(C0) 
fS $ $ $ b o a r 
fGS $ $ $ C V V C 
/B/ 
fP $ $ $      
Let SW=s1,s2,...,sn be an English word, and PSW= p1,p2,...,pn be SW?s pronunciation, 
where si represents the ith grapheme and pi=?p(si). Pronunciation estimation is a task to 
find the most relevant phoneme among a set of all possible phonemes, which can be 
derived from source grapheme si. Table 3 shows an example of pronunciation estima-
tion for b in board. In Table 3, L1~L3 and R1~R3 represent the left contexts and right 
contexts, respectively. C0 means the current context (or focus). ?p(C0) means the esti-
mated phoneme of C0. $ is a symbol for representing the start of words. The result can 
be interpreted as follows. The most relevant phoneme of b, /B/, can be produced with 
the context, fS, fGS, and fP in contexts of L1~L3, C0, and R1~R3. Other phonemes for o, 
a, r, and d in board are produced in the same manner. Thus, we can get the pronuncia-
tion of board as /B AO R D/ by concatenating the phoneme sequence. 
2.2   Producing Target Graphemes (?t) 
Producing target graphemes (?t:S?P?T) is a function that finds the target grapheme 
in T for each gpi that is a result of ?p. A result of this step, GT, is represented by a 
sequence of gpi and its corresponding target graphemes generated by ?t, like GT={gt1, 
gt2 ,?, gtn; gti=(gpi,?t(gpi))}. 
                                                          
7
 Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 455 
Table 4. An example of ?t for b in board 
Feature type  L3 L2 L1 C0 R1 R2 R3 ?t(C0) 
fS $ $ $ b o a r ?b? 
fP $ $ $ /B/ /AO/ /~/ /R/  
fGS $ $ $ C V V C  
fGP $ $ $ C V /~/ C  
fT $ $ $      
Let SW=s1,s2,...,sn be a source language word, PSW= p1,p2,...,pn be SW?s pronuncia-
tion and TSW= t1, t2,...,tn be a target language word of SW, where si, ?p(si)=pi and ?t(gpi) 
= ti represent the ith source grapheme, phoneme corresponding to si, and target graph-
eme corresponding to gpi, respectively. ?t finds the most probable target grapheme 
among a set of all possible target graphemes, which can be derived from gpi. ?t pro-
duces target graphemes with source grapheme (fS), phoneme (fP), source grapheme type 
(fGS), phoneme type (fGP) and ?t?s previous output (fT) in the context window. Table 4 
shows an example of ?t for b in board. ?t produces the most probable sequence of tar-
get graphemes (e.g. Korean), like ?t(gp1)= ?b?, ?t(gp2)= ?o?, ?t(gp3)=?~?, ?t(gp4)=?~?, 
and ?t(gp5)=?deu? for board. Finally, the target language transliteration of board as 
?bodeu? can be acquired by concatenating the sequence of produced target graphemes.  
3   Machine Learning Algorithms for Each Component Function 
In this section we will describe a way of modeling component functions using three 
machine learning algorithms (maximum entropy model, decision tree, and memory-
based learning).  
3.1   Maximum Entropy Model 
The maximum entropy model (MEM) is a widely used probability model that can 
incorporate heterogeneous information effectively [3]. In the maximum entropy 
model, an event ev is usually composed of a target event (te) and a history event (he), 
say ev=<te, he>. Event ev is represented by a bundle of feature functions, fei(ev), 
which represent the existence of a certain characteristic in event ev. A feature function 
is a binary valued function. It is activated (fei(ev)=1) when it meets its activating 
condition, otherwise it is deactivated (fei(ev)=0) [3].  
?p and ?t based on the maximum entropy model can be represented as formula (1). 
History events in each component function are made from the left, right and current 
context. For example, history events for ?t are composed of fS,GS (i-3,i+3), fP,GP (i-3,i+3), and 
fT (i-3,i-1) where i is a index of the current source grapheme and phoneme to be translit-
erated and fX(l,m) represents features of feature type fX located from position l to posi-
tion m. Target events are a set of target graphemes (phonemes) derived from history 
events of ?t (?p). Given history events, ?t (?p) finds the most probable target grapheme 
(phoneme), which maximizes formula (1). One important thing in designing a model 
456 J.-H. Oh and K.-S. Choi 
based on the maximum entropy model is to determine feature functions which effec-
tively support certain decision of the model. Our basic philosophy of feature function 
design for each component function is that context information collocated with the 
unit of interest is an important factor. With the philosophy, we determined the history 
events (or activating conditions) of the feature functions by combinations of features 
in feature types. Possible feature combinations for history events are between features 
in the same feature type and between features in different feature types. The used 
feature combinations in each component function are listed in Table 5. 
Table 5. Used feature combinations for history events  
?p ?t 
Between features in the same feature 
type 
Between features in different feature 
types 
z fS,GS  and fP 
Between features in the same feature 
type 
Between features in different feature 
types  
z fS,GS  and fP,GP  
z fS,GS  and fT 
z fP,GP  and fT 
In formula (1), history events of ?p and ?t are defined by the conditions described 
in Table 5. Target events of ?
 t are all possible target graphemes derived from its his-
tory events; while those of ?p are all possible phonemes derived from its history 
events. In order to model each component function based on MEM, Zhang?s maxi-
mum entropy modeling tool is used [16].  
),|(maxarg)(
),,|(maxarg))(,(
3,3,1,3
3,3,3,3,1,3
+???
+?+???
=
=
iiGSSiiPiip
iiGPPiiGSSiiTiipit
ffpps
ffftpss
?
??
 (1) 
3.2   Decision Tree 
Decision tree learning is one of the most widely used and well-known methods for 
inductive inference [15]. ID3, which is a greedy algorithm and constructs decision 
trees in a top-down manner, adopts a statistical measure called information gain that 
measures how well a given feature (or attribute) separates training examples accord-
ing to their target class [15]. We use C4.5 [15], which is a well-known tool for deci-
sion tree learning and implementation of Quinlan?s ID3 algorithm.  
Training data for each component function is represented by features of feature 
types in the context of L3~L1, C0, and R1~R3 as described in Table 3. Fig. 2 shows a 
fraction of our decision trees for ?p and ?t in English-to-Korean transliteration (note 
that the left side represents the decision tree for ?p and the right side represents the 
decision tree for ?t). A set of the target classes in the decision tree for ?p will be a set 
of phonemes and that for
 
?t will be a set of target graphemes. In Fig. 2, rectangles 
indicate a leaf node and circles indicate a decision node. In order to simplify our  
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 457 
examples, we just use fS and fP in Fig. 2. Intuitively, the most effective feature for ?p 
and
 
?t may be located in C0 among L3~L1, C0, and R1~R3 because the correct out-
puts of ?p and ?t strongly depend on source grapheme or phoneme in the C0 position. 
As we expected, the most effective feature in the decision trees is located in the C0 
position like C0(fS) for ?p and C0(fP) for ?t (Note that the first feature to be tested is 
the most effective feature). In Fig. 2, the decision tree for ?p outputs phoneme /AO/ 
for the instance x(SP) by retrieving the decision nodes C(fS)=o, R1(fS)=a, and R2(fS)=r 
represented with ?*?.  With the similar manner, the decision tree for ?t produces target 
grapheme (Korean grapheme) ?o? for the instance x(SPT) by retrieving the decision 
nodes from C0(fP)=/AO/ to R1(fP)=/~/ represented with ?*?.  
C0(fS):o(*)(
R1(fS): yS  R1(fS): e or q( S)    R1(fS): a(*)(  
/OW// / /OY// / /AA// /
R1(fS): xS  ??
R2(fS): d(fS): R2(fS): r(*)(f ): ( ) R2(fS): others(fS): t rR2(fS): $(fS): 
/OW// /OW///AO/(*)/AO/(*)
R1(fS): bfS : 
L2(fS): af )  L2(fS): r( : rL2(fS): $f )  ??
fS
Feature typex(SP)
?
/AO/draob$$
?pR3R2R1C0L1L2L3
Decision tree for ?p 
C0(fP): /AO/ (*))   
C0(fS): aS): C0(fS): e(  C0(fS): o(*)f )  
?o?? ? ?a?? ? ?eu?? ?
C0(fS): othersfS)  ??
R1(fP): /R/(f ): / / R1(fS): /~/(*)(f ): / /( ) R1(fP): others(f ): t r
?o??o? (*)?o? (*)
C0(fS): i(  
L2(fS): a(  L2(fS): rS : rL2(fS): $(  ??
?o?
?
draob$$fS
x(SPT)
fP
Feature type
/D//R//~//AO//B/$$
?tR3R2R1C0L1L2L3
Decision tree for ?ti i  t
 
Fig. 2. Decision tree for ?p and?t 
3.3   Memory-Based Learning 
Memory-based learning (MBL) is an example-based learning method. It is also called 
instance-based learning and case-based learning method.  It is based on a k-nearest 
neighborhood algorithm [1], [5]. MBL represents a training data as a vector. In the train-
ing phase, MBL puts all training data as examples in memory, and clusters some exam-
ples with a k-nearest neighborhood principle. It then outputs a target class using similar-
ity-based reasoning between test data and examples in the memory. Let test data be x 
and a set of examples in a memory be Y, the similarity between x and Y is estimated by a 
distance function, ?(x,Y). MBL selects an example yi or a cluster of examples that are 
most similar to x, then assign a target class of the example to x?s class. We use a mem-
ory-based learning tool called TiMBL (Tilburg memory-based learner) version 5.0 [5].  
Training data for each component function is represented by features of feature 
types in the context of L3~L1, C0, and R1~R3 as described in Table 4. Fig. 3 shows 
examples of ?p and ?t based on MBL in English-to-Korean transliteration. In order to 
simplify our examples, we just use fS and fP in Fig. 3. All training data are represented 
with their features in the context of L3~L1, C0, and R1~R3 and their target classes for 
?p and ?t. They are stored in the memory through a training phase. Feature weighting 
for dealing with features of differing importance is also performed in the training 
phase. In Fig. 3, ?p based on MBL outputs the phoneme /AO/ for x(SP) by comparing 
the similarities between x(SP) and Y using distance metric ?(x(SP),Y). With the simi-
lar manner, ?t based on MBL outputs the target grapheme ?o?. 
458 J.-H. Oh and K.-S. Choi 
x(SP)
/AO/
?
draob$$fS
Feature type ?pR3R2R1C0L1L2L3
Training instances in a memory (?p)i    
0.51/UW/$tuobaefS8
0.16/AO/$$waskcfS7
0.75/W/draode$fS6
0.73/AO/sraoc$$fS5
0.81/OW/$taob$$fS4
0.81/OW/tsaob$$fS3
0.38/OW/$$$obahfS2
0.93/AO/draoba$fS1*
yi Feature 
type
?p(C0) ?(x(SP),yi)R3R2R1C0L1L2L3
/D//R//~//W//D//~/$fP
/S//R//~//OW//K/$$fP
0.31?u?$tuobaefS4
$/T//~//UW//B//~//IY/fP
3
2
1*
yi
fS
fS
fP
fS
Feature 
type
0.55?u?draode$
0.63?o?sraoc$$
?o?
?t(C0)
/D//R//~//AO//B//AH/$
0.89draoba$
? (x(SPT),yi)R3R2R1C0L1L2L3
x(SPT)
?o?
?
draob$$fS
fP
Feature type
/D//R//~//AO//B/$$
?tR3R2R1C0L1L2L3
Training instances in a memory (?t) i  t
 
Fig. 3. Memory-based learning for ?p and ?t 
4   Experiments 
We perform experiments for English-to-Korean and English-to-Japanese translitera-
tion. English-to-Korean test set (EKSet) [14] consists of 7,185 English-Korean pairs ? 
the number of training data is 6,185 and that of test data is 1,000. EKSet contains no 
transliteration variations. English-to-Japanese test set (EJSet), which is an English-
katakana pair in EDICT8, consists of 10,398 ? 1,000 for test and the rest for training. 
EJSet contains transliteration variations, like (micro, ?maikuro?) and (micro, ?mi-
kuro?); the average number of Japanese transliterations for an English word is 1.15. 
Evaluation is performed by word accuracy (W. A.) in formula (2).  
wordsgeneratedof
wordscorrectofAW
  #
  #
.. =  (2) 
We perform two experiments called ?Comparison test? and ?Context window 
size test?. In the ?Comparison test?, we compare our ?C with the previous works. In 
?Context window size test?, we evaluate the performance of our transliteration model 
depending on context window size. 
4.1   Experimental Results 
Table 6 shows results of ?Comparison test?. MEM, DT, and MBL represent ?C 
based on maximum entropy model, decision tree, and memory-based learning, respec-
tively. GDT [8], GPC [9], GMEM [7] and HWFST [4], which are one of the best 
machine transliteration methods in English-to-Korean transliteration and English-to-
Japanese transliteration, are compared with ?C. Table 7 shows the key feature of each 
method in the viewpoint of information type (SG, PH, COR) and information usage 
(Context size, POut). Information type indicates that each transliteration method be-
longs to which transliteration model. For example, GDT, GPC, and GMEM will be-
long to ?G because they use only the source grapheme; while HWFST belongs to ?H. 
Information usage gives information about what kinds of information each translitera-
tion method can deal with. From the viewpoint of information type, phoneme and 
correspondence, which most previous works do not consider, is the key point of the 
performance gap between our method and the previous works.  
                                                          
8
 http://www.csse.monash.edu.au/~jwb/j_edict.html 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 459 
Table 6. Evaluation results of ?Comparison test? 
Method EKSet  EJSet  
  W.A Chg % W.A Chg % 
GDT 51.4% 23.2% 50.3% 43.5% 
GPC  55.1% 17.6% 53.2% 35.7% 
GMEM  55.9% 16.4% 56.2% 28.5% 
HWFST 58.3% 14.7% 62.5% 15.5% 
DT  62.0% 7.3% 66.8% 8.1% 
MEM  63.3% 5.4% 67.0% 7.8% 
MBL  66.9% 0% 72.2% 0% 
Table 7. Key features of our machine transliteration model and the previous works: SG, PH, 
COR and POut represent source grapheme, phoneme, correspondence and previous output, 
respectively 
Method  SG PH COR Context size POut  
GDT  O X X <-3, +3> X 
GPC  O X X Unbounded O 
GMEM  O X X <-3, +3> O 
HWFST O O X - - 
Ours  O O O <-3, +3> O 
From the viewpoint of information usage, if a transliteration model adopts wide 
context window and considers previous outputs, it tends to show better performance. 
For example, GMEM that satisfies the conditions gives more accurate results  
than GDT which does not satisfy one of them. Because machine transliteration is 
sensitive to context, wider contexts give more powerful transliteration ability to 
machine transliteration systems. Note that the previous works, however, limit their 
context window size to 3, because the context window size over 3 degrades the  
performance [8] or does not change the performance of their transliteration model 
[9]. Determining reasonable context window size, therefore, is very important for 
machine transliteration.  
For ?Context window size test?, we use ?C based on MBL, which shows the best 
performance among three machine learning algorithms in Table 6. Experiments are 
performed by changing the context window size from 1 to 5. Table 8 shows results of 
context window size test. The results indicate that the best performance is shown 
when the context window size is 3. When the context window size is 1, there are 
many cases where the correct transliterations are not produced due to lack of informa-
tion. For example, in order to produce the correct target grapheme of t in -tion, we 
need the right three graphemes of t, -ion. When the context window size is over 3, it is 
difficult to generalize the training data because of increase of variety of the training 
data. With the two reasons, our system shows the best performance when the context 
window size is 3. Table 8 also shows that context size should be at least 2 to avoid 
significant decrease of performance due to lack of contextual information. 
460 J.-H. Oh and K.-S. Choi 
Table 8.  Evaluation results of ?Context window size test? 
Context Size EKSet EJSet 
1 54.5% 62.7% 
2 63.3% 70.0% 
3 66.9% 72.2% 
4 63.9% 70.7% 
5 63.8% 69.3% 
In summary, our method shows significant performance improvement, about 
15%~23%, in English-to-Korean transliteration, and about 15%~ 43% in English-to-
Japanese transliteration. Experiments show that a good transliteration system should 
consider; 1) source grapheme and phoneme along with their correspondence simulta-
neously and 2) reasonable context size and previous output. Our transliteration model 
satisfies the two conditions, thus it shows higher performance than the previous works.  
5   Conclusion  
This paper has described a correspondence-based machine transliteration model (?C). 
Unlike the previous transliteration models, ?C uses correspondence between source 
grapheme and phoneme. The correspondence makes it possible for ?C to effectively 
produce both grapheme-based transliterations and phoneme-based transliterations. 
Moreover, the correspondence helps ?C to reduce transliteration ambiguities more 
easily. Experiments show that ?C is more powerful transliteration model than the 
previous transliteration models (?C shows significant performance improvement, 
about 15%~23%, in English-to-Korean transliteration, and about 15%~ 43% in Eng-
lish-to-Japanese transliteration).  
In future work, we will apply our transliteration model to English-to-Chinese trans-
literation model. In order to prove usefulness of our method in NLP applications, we 
need to apply our system to applications such as automatic bi-lingual dictionary con-
struction, information retrieval, machine translation, speech recognition and so on. 
Acknowledgement 
This work was supported by the Korea Ministry of Science and Technology, the Ko-
rea Ministry of Commerce, Industry and Energy, and the Korea Science and Engi-
neering Foundation (KOSEF). 
References 
1. Aha, D. W. Lazy learning: Special issue editorial. Artificial Intelligence Review, 11:710, 
(1997). 
2. Al-Onaizan Y. and Kevin Knight, ?Translating Named Entities Using Monolingual and 
Bilingual Resources?, In the Proceedings of  ACL 2002, (2002) 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 461 
3. Berger, A., S. Della Pietra, and V. Della Pietra. , A maximum entropy approach to natural 
language processing. Computational Linguistics, 22(1), (1996), 39?71 
4. Bilac Slaven and Hozumi Tanaka. "Improving Back-Transliteration by Combining Infor-
mation Sources". In Proc. of IJC-NLP2004, (2004) 542?547 
5. Daelemans, W., Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch, 2002, Timble 
TiMBL: Tilburg Memory Based Learner, version 4.3, Reference Guide, ILK Technical 
Report 02-10, (2002). 
6. Fujii, Atsushi and Tetsuya, Ishikawa. Japanese/English Cross-Language Information Re-
trieval: Exploration of Query Translation and Transliteration. Computers and the Humani-
ties, Vol.35, No.4, (2001) 389?420 
7. Goto, I., N. Kato, N. Uratani and T. Ehara, Transliteration Considering Context Informa-
tion Based on the Maximum Entropy Method, In Proceedings of MT-Summit IX, (2003) 
8. Kang B.J. and K-S. Choi, "Automatic Transliteration and Back-transliteration by Decision 
Tree Learning", In Proceedings of the 2nd International Conference on Language Re-
sources and Evaluation, (2000) 
9. Kang, I.H. and G.C. Kim, "English-to-Korean Transliteration using Multiple Unbounded 
Overlapping Phoneme Chunks", In Proceedings of the 18th International Conference on 
Computational Linguistics, (2000). 
10. Knight, K. and J. Graehl, "Machine Transliteration". In Proceedings. of the 35th Annual 
Meetings of the Association for Computational Linguistics (ACL), (1997)  
11. Lee, J. S. and K. S. Choi, English to Korean Statistical transliteration for information re-
trieval. Computer Processing of Oriental Languages, 12(1), (1998), 17-37. 
12. Lee, J.S., An English-Korean transliteration and Retransliteration model for Cross-lingual 
information retrieval, PhD Thesis, Computer Science Dept., KAIST, (1999) 
13. Li Haizhou, Min Zhang and Jian Su , A Joint Source-Channel Model for Machine Trans-
literation , ACL 2004, (2004), 159?166 
14. Nam, Y.S., Foreign dictionary, Sung-An-Dang publisher, (1997) 
15. Quinlan, J.R., "C4.5: Programs for Machine Learning", Morgan Kauffman, (1993) 
16. Zhang, Le. Maximum Entropy Modeling Toolkit for Python and C++. 
http://www.nlplab.cn/zhangle/, (2004) 
Automatic Extraction of English-Korean Translations for Constitu-
ents of Technical Terms 
Jong-Hoon Oh and Key-Sun Choi 
Department of Computer Science, Division of EECS, KAIST/KORTERM/BOLA 
373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea  
{rovellia,kschoi}@world.kaist.ac.kr 
 
Abstract*
Technical terms are linguistic realiza-
tion of a domain concept and their con-
stituents are a component used for 
representing the concept. Many techni-
cal terms are usually multi-word terms 
and their meaning can be inferred from 
their constituents. Because a term con-
stituent is usually a morphological unit 
rather than a conceptual unit in Korean 
technical terms, we need to first iden-
tify conceptual units and then to re-
solve the proper meaning of the 
conceptual units in order to properly 
translate technical terms. For natural 
language applications to properly han-
dle technical terms, it is necessary to 
give information about conceptual units 
and their meaning including homonym, 
synonym and domain dependency. In 
this paper, we propose a term constitu-
ent alignment algorithm, which extracts 
such information from bilingual techni-
cal term pairs. Our algorithm regards 
English term constituents as a concep-
tual unit and then finds its Korean 
counterpart. Our method shows about 
6.1% AER. 
1 Introduction 
Technical terms are linguistic realization of a 
domain specific concept and their constituents 
are a component used for representing the con-
cept (Sager, 1997). Technical terms can be clas-
sified into single-word terms, and complex term 
                                                          
                                                          
* The first author's current affiliation is with Computational 
Linguistics Group, National Institute of Information and 
Communications Technology, 3-5 Hikaridai, Seika-cho, 
Soraku-gun, Kyoto, 619-0289 Japan 
(or multi-word term) according to the number of 
their constituents. Single-word terms have one 
term constituent while complex terms have more 
than one term constituent. Many Korean techni-
cal terms are usually complex terms and their 
meaning can be inferred from their constituents 
(Sager, 1997).  Therefore it is helpful to identify 
constituents of technical terms and their mean-
ing in order to understand the meaning of the 
technical terms and to translate the technical 
term from one?s language to the other. However, 
a term constituent is usually a morphological 
unit rather than a conceptual unit1  in Korean 
technical terms. Due to the mismatch between a 
term constituent and a conceptual unit, we need 
to first identify conceptual units which is a 
chunk of term constituents representing a do-
main specific concept (?chunking conceptual 
units?) and then to resolve the proper meaning 
of the conceptual unit (?resolving meanings?) in 
order to properly understand the meaning of 
technical terms and to translate them.  
In the ?chunking conceptual units? stage, it is 
necessary to determine whether one term con-
stituent represents a concept or not. The decision 
depends on contexts of term constituents. For 
example, a Korean technical term, ?seong? can 
be a conceptual unit by itself when it represents 
sex. But ?seong? in the context of ?hyang-
chuk+seong / bun-yeol+jo-jik? 2  (representing 
adaxial meristem) should be recognized as a 
conceptual unit along with its neighborhood 
?hyang-chuk? such as ?hyang-chuk+seong? (ad-
axial). If ?seong? is recognized as a conceptual 
unit by itself in the context, like ?hyang-chuk 
(adaxial) / seong (sex) / bun-yeol+jo-jik (meris-
1 In this paper, a conceptual unit is defined as the linguistic 
unit representing a domain specific concept. 
2 In this paper, Romanized Korean transcriptions are repre-
sented in the quotation mark. In the transcriptions, ?+? 
represents the boundary of term constituents, ?-? represents 
the syllable boundary and ?/? represents the boundary of 
conceptual units. 
67
tem)?, we can neither understand the designated 
meaning of ?hyang-chuk+seong / bun-yeol+jo-
jik? (?meristem of a leaf cell in the adaxial 
area?) nor properly translate it.  
In the ?resolving meanings? stage, homonym, 
synonym, and domain dependency of conceptual 
units should be considered. Sino-Korean affixes 
are frequently used for coining Korean technical 
terms and are used as a conceptual unit like sin-
gle words. Moreover, they are usually homonym. 
For example, a suffix ?-gi? is used as a term con-
stituent in a biology domain with four senses 
like group (?), period (?), stage (?), and or-
gan (?). Therefore, disambiguating the sense of 
such affixes is very important for understanding 
a Korean technical term. 
Many Korean technical terms are from for-
eign origin. These technical terms become Ko-
rean technical terms with various translation 
ways ? 1) translation with pure Korean words, 
2) translation with Sino-Korean words, 3) trans-
literation, 4) combinations of the three ways. 
Moreover, each translation way produces some 
variations. For example, abdominal is translated 
into three different Korean terms like ?bok-bu?, 
?bok?, and ?bae?, but they indicate the same 
meaning; in other words, they are synonym. ab-
dominal is translated into two Sino-Korean 
terms like ?bok-bu (??)? and ?bok (?)?, and 
one pure Korean term, ?bae?. Capturing syno-
nym, therefore, is important for understanding 
meaning of technical terms. 
Depending on domain of technical terms, 
translations of conceptual units can be different. 
For example, the meaning of cell in chemistry, 
physics, and electricity is usually ?A single unit 
that converts radiant energy into electric energy?, 
while that in biology is usually ?The smallest 
structural unit of an organism?. In each case, 
cell is differently translated into Korean terms 
?jeon-ji? (in chemistry, physics, and electricity 
domain), and ?se-po? (biology domain). 
For natural language applications to properly 
handle technical terms, it is necessary to give 
information about conceptual units and their 
meaning including homonym, synonym and 
domain dependency. In this paper, we propose a 
term constituent alignment algorithm, which 
extracts such information from bilingual techni-
cal term pairs. In our algorithm, one or more 
than one English term constituents are regarded 
as a conceptual unit. Therefore, the main objec-
tive of our algorithm is to recognize conceptual 
units of Korean technical terms corresponding to 
an English term constituent in English-Korean 
translation pairs of technical terms.  
The recognized bilingual conceptual units 
give contextual information, which supports de-
cision whether certain term constituent tends to 
be used as a conceptual unit by itself or not. 
Homonym and synonym can be handled by find-
ing the correspondence between English and 
Korean conceptual units. Because English and 
Korean conceptual units indicating the same 
concept will be linked to each other, we can eas-
ily find homonym and synonym from the rela-
tions. For example, the homonym ?gi? will be 
linked to four different English conceptual units. 
In the same manner, we can capture three rela-
tions between the English conceptual unit ab-
dominal and its counterparts ?bok-bu?, ?bok?, 
and ?bae?. The three Korean counterparts can be 
clustered as synonyms by means of their corre-
sponding English conceptual unit, like {?bok-
bu?, ?bok?, ?bae?}. Moreover, domain depend-
ency of conceptual units can be handled by the 
relations because extracted relations for certain 
English conceptual unit, which has domain de-
pendency, will be different depending on do-
mains.  
This paper organized as follows. In section 2, 
we will describe the related works. Section 3 
shows details of our method. Section 4 deals 
with experiments. Conclusion and future works 
are drawn in sections 5. 
2 Related Works 
One of the well-known alignment techniques is 
the one based on statistical machine translation 
models. It was initially proposed by (Brown et 
al., 1993) and, more recently, have been inten-
sively studied by several research groups (Ger-
mann et al, 2001; Och et al, 2003). It is used 
for finding sentence, phrase, and word-level cor-
respondences from parallel texts. It can be 
formulated as equation (1). For the give source 
text, S, it finds the most probable alignment set, 
A, and target text, T.  
?
?
=
Aa
SaTpSTp )|,()|(    (1) 
Brown (Brown et al, 1993) proposed five 
alignment models, called IBM Model, for an 
English-French alignment task based on equa-
68
tion (1). Equation (2) describes the IBM Model 
1. It is modeled by two assumptions - P(F|E) 
depends on word translation probability t(fj|ei) 
and one English word was aligned to one French 
word (1:1 alignment). t(fj|ei) is estimated by EM 
algorithm.  
??
= =
=
m
j
l
i
ijml eftCEFp
1 1
, )|()|(  (2) 
where, m represents the length of F, l represents 
the length of E, and Cl,m is a constant value de-
termined by l (the length of E) and m (the length 
of F).  
IBM Model 2 considers distortion (How 
likely is a source language word in position i to 
align to a target language word in position j). 
IBM Model 3 adopts fertility (How likely is a 
source language word to align to k target lan-
guage words) as its parameter for 1:n alignment. 
IBM Model 4 and 5 make use of relative distor-
tion, word classes and variables to avoid defi-
ciency.  
There is another stream of studies on align-
ment. (Chen et al, 1993; Gale et al, 1993) pro-
posed sentence alignment techniques based on 
dynamic programming, using sentence length 
and lexical mapping information. (Haruno et al, 
1996; Kay et al, 1993) applied iterative refine-
ment algorithms to sentence level alignment 
tasks.  
In this paper, we propose an alignment algo-
rithm between English and Korean conceptual 
units (or between English and Korean term con-
stituents) in English-Korean technical term pairs 
based on IBM Model (Brown et al, 1993). 
Unlike IBM Model, our alignment model can 
deal with n:1 alignment. While the IBM Model 
aimed to word-level alignment of parallel texts, 
our method focuses on word- and morphology-
level alignment of English-Korean term pairs. 
Moreover, our algorithm reflects the translation 
properties of English-to-Korean technical term 
pairs in a bilingual dictionary. 
3 Term Constituent Alignment 
For term constituent alignment, we use biology, 
chemistry and physics dictionaries where term 
constituents are manually segmented and their 
part-of-speech is manually assigned. For exam-
ple, the Korean counterpart of crop growth rate 
is ?jak-mul + seng-jang + yul? and its three term 
constituents are ?jak-mul?, ?seng-jang?, and ?yul? 
where the first two are a noun and the last one is 
a suffix.  
The problem can be defined as finding corre-
spondence between English and Korean term 
constituents as described in equation (3). For a 
given English term E=e1,?,en, composed of n 
English term constituents and its corresponding 
Korean term K=k1,?,km, composed of m Korean 
term constituents, the task is to find alignment 
set, A={a1,....,at;ap=(ei,i+w(p),, kj(p))}, maximizing 
probability P(A|K,E), where ei is the ith term 
constituent of E, kj is the jth term constituent of 
K, and ap represents the pth alignment relation 
between English and Korean term constituents. 
Note that ap=(ei,i+w(p), kj(p)) (w ? 0) represents an 
alignment relation between English term con-
stituents ei ,?,ei+w  and Korean term constituent 
kj. For example, there are two alignment rela-
tions for English term female sex hormone and 
Korean term ?ja-seong + ho-leu-mon?, like a1 
=(e1,2(1)=female sex, k1(1)=?ja-seong?) and a2 
=(e1(2)=hormone, k2(2)=?ho-leu-mon?) 
),|(maxarg* EKAPA
A
=   (3) 
3.1  Statistical Modeling 
In this section, first, we describe two translation 
properties (or constraints), derived from analysis 
of the alignment tendency between English-
Korean term constituents and then describe how 
to apply these properties to statistical modeling 
of term constituent alignment.  
We randomly sample 20% data of English-
Korean term pairs in each technical dictionary 
and finds two properties ?Cross alignment ap-
pears in some conditions?3 and ?Null Alignment 
hardly appears?4 by analyzing the sampled data. 
Constraint 1: Cross alignment is partly al-
lowed. 
Let algnment units in a source language be si, 
sj(i<j), where i and j are the index of the source 
language, and those in a target language be tq, tr 
(q<r), where q and r are the index in the target 
language. Then alignment ai=(si,tr), and 
aj=(sj,tq) are called cross alignment. Because a 
sentence structure of Korean is different from 
                                                          
3 Among analyzed data, 1.3% for biology, 0.1% for physics 
and 5.65% for chemistry show cross alignment. 
4 Among analyzed data, 0.8% for biology, 0.2% for physics 
and 0.1% for chemistry show null alignment. 
69
that of English, cross-alignment between Eng-
lish and Korean words frequently occurs in par-
allel sentences (Shin et al, 1995). For alignment 
between term constituents, however, most 
alignment relations are derived from sequential 
alignment because technical terms, which are 
usually noun phrases, share the similar structure, 
say modifier and modifee, in both languages. 
Sometimes there is cross-alignment because of 
the preposition in an English term such as of. In 
that case, we allow cross-alignment. For exam-
ple, there is a cross-alignment relation such as a1 
= (e2 = blood, k1 = ?hyeol-aek?) and a2 = (e1 = 
clotting, k2 = ?eung-go?) between the English 
term clotting of blood and its Korean translation 
?hyeol-aek + eung-go?. Note that we do not con-
sider the preposition of as an alignment unit in 
that case. English-Korean term pairs represent-
ing a name of chemical compounds usually 
show cross-alignment and 1:1 alignment. To 
deal with this case, we allow cross-alignment 
when the number of English term constituents 
and that of Korean term constituents are same. 
With the constraint 1, sequential alignment is 
performed except the above two cases.  
Constraint 2:  Null Alignment is not allowed. 
Constraint 2 means that all English and Korean 
term constituents should be aligned. Because, 
term pairs consist of an English term and its 
translated Korean term, we assume that all con-
stituents should be aligned. Null alignment 
means that an alignment unit in one side is 
aligned to nothing in the other side. For example, 
for Dutch elm disease and ?ne-deol-lan-deu 
(Dutch) / neu-leup-na-mu (elm) / che-gwan 
(sieve tube) / byeong (disease)?, there is no Eng-
lish term constituent to be aligned to the Korean 
term constituent ?che-gwan (sieve tube)?. Be-
cause, null alignment, however, does not fre-
quently appear in term constituent alignment 
(only the 0.1%~0.8% data among analyzed data), 
we do not consider null alignment in our algo-
rithm. 
),,,|(),|(
),|(
1
)(,)( tmnjiaekap
EKAP
t
l
lwiiljl?
=
+ ?=  (4) 
),|()|(
),,|(
),|(
,,
)(,)()(
)(,)(
wiij
t
j
w
wiij
t
lwiilj
w
lj
t
l
lwiiljl
ekkpekp
ekkap
ekap
++
+
+
??
=  (5) 
By the constraints, equation (3) can be repre-
sented as equation (4). In equation (4), n, m, and 
t represent the number of English term constitu-
ents, the number of Korean term constituents 
and the number of alignment relations between 
term constituents. In equation (4), a(i|j,n,t) 
represents position information, which is a bi-
nary-valued function and supports the constraint 
1. a(i|j,n,m,t) = 0 when ap= a(ei,i+w(p),kj(p)) is 
cross-alignment, which is not allowed by con-
straint 1, otherwise a(i|j,n,m,t) = 1. 
In equation (4), p(al|kj(l),ei,i+w(l)) are estimated 
by equation (5). In equation (5), kj(l) is repre-
sented by kwj and ktj where kwj and ktj are lexical 
information and part of speech information of 
the jth Korean term constituent, respectively.  
3.2 Parameter Estimation with EM Algo-
rithm 
Parameters, p(ktj|ei,i+w) and p(kwj| ktj, ei,i+w), in 
equation (5) are estimated with EM (Expecta-
tion-Maximization) algorithm. EM algorithm is 
the technique for parameter estimation of ge-
neric statistical distributions in presence of in-
complete data (Dempster et al, 1997). The main 
goal of EM is to obtain the estimated parameters 
that give maximum likelihood to the input (in-
complete) data. The basic idea underlying the 
EM algorithm is to iterate through a series of 
expectation (E-step) and maximization (M-step) 
steps where the estimation of the parameters of 
the model is progressively refined until conver-
gence (Lopez et al, 1999).  
In this paper, parameters are estimated 
through two steps, called ?initial parameter es-
timation? and ?iterative parameter estimation?. 
In the initial parameter estimation step, the ini-
tial parameters are determined by seed data. 
Seed data, which contains alignment relations 
derived from E=e1,?,en  and E?s Korean transla-
tion K=k1,?,km, where n =1 or m = 1, was se-
lected among data for term constituent 
alignment. In the condition of n = 1 or m = 1, 
English technical terms or Korean technical 
terms are a conceptual unit by itself. In other 
words, alignment relations can be directly ex-
tracted from the English-Korean term pairs if 
70
there is only one English term constituent or 
only one Korean term constituent. With the seed 
data we can get the initial alignment relation set 
A(0) and then the initial parameter ?(0) is esti-
mated with A(0), where A(k) represents the 
alignment relation set and ?(k) represents the 
estimated parameter set derived from the kth it-
eration. Note that A={a1,....,at;ap=(ei,i+w(p),kj(p))} 
and ?={p(ktj|ei,i+w), p(kwj| ktj, ei,i+w)}.  
In the iterative parameter estimation step, 
A(k) is determined by ?(k-1) in E-step and ?
(k) is estimated by A(k) in M-step using the 
whole data until ?(k) converges. E-step and M-
step can be represented as equation (6) 
))1(;,|(maxarg)(: ?=? kKEApkAstepE A ?
))(|(maxarg)(: kApkstepM ?? ?=?  (6) 
p(ktj|ei) and p(kwj| ktj, ei) are estimated in the 
kth iteration as equation (7) and (8), respectively. 
In order to prevent zero probability, the Laplace 
smoothing method (Manning et al, 1999) is ap-
plied to equation (7) and (8). 
))(;(||
))(;,(1
))(;|(
,
,
, kAeCE
kAekC
kAekp
wii
wiij
t
wiij
t
+
+
+ +
+=       (7) 
))(;,(||||
))(;,,(1
))(;,|(
,
,
, kAekCET
kAekkC
kAekkp
wiij
t
wiij
t
j
w
wiij
t
j
w
+
+
+ ++
+= (8) 
where C(x) represents frequency of x, |E| repre-
sents the number of unique English term con-
stituents in A(k), |T| represents the number of 
unique POS tags of Korean term constituents in 
A(k). 
4 Experiments 
For experiments we use three kinds of technical 
dictionary. They are biology, chemistry, and 
physics technical dictionaries where Korean 
term constituents are manually analyzed. The 
characteristics of experimental data are summa-
rized as Table 1 (Ministry, 2002).  
Domain Seed data Test data Total  
Biology  8,163 5,668 13,831
Physics 2,757 8,047 10,804
Chemistry  5,353 10,024 15,377
Table 1. Characteristics experimental data (the 
number of bilingual term pairs) 
We compare our model with IBM Model 2 
(IBM-2), and IBM Model 4 (IBM-4) imple-
mented by GIZA++ (Och et al, 2003). We 
evaluate results with the alignment error rate 
(AER) of Och and Ney (Och et al, 2003), which 
measures agreement at the level of pairs of term 
constituents.5
||||
||21
GA
GAAER +
???=    (9) 
where A is the set of term constituent pairs 
aligned by the automatic system, and G is the set 
aligned in the gold standard. 
4.1 Experimental results 
Table 2 shows evaluation results for IBM-2, 
IBM-4 and our proposed method. In the results 
precision and AER of our proposed method is 
higher than those of IBM-4. But recall of our 
proposed method is lower than that of IBM-4. 
IBM-4 has strong points in handling cross-
alignment and null alignment while our model 
has strong points in handling n:1 alignment. The 
difference between our model and IBM-4 causes 
the performance gap. Because most alignment 
type found in the gold standard is 1:1 alignment 
and 1:n alignment rather than cross-alignment, 
null alignment, and n:1 alignment as described 
in Table 3, the performance gap between our 
method and IBM-4 is not so big. IBM-2 shows 
the worst performance because it can not deal 
with 1:n alignment. In other words, IBM-2 does 
not consider fertility as its parameter for estimat-
ing the translation probability. Note that 1:n 
alignment in the gold standard is about 
18%~22% (see Table 3).  
Domain IBM-2 IBM-4 Proposed 
Biology 25.0% 7.4% 6.5% 
Physics 30.0% 9.6% 5.2% 
Chemistry 28.7% 7.6% 6.5% 
Table 2. Experimental Results  
Type Biology  Physics Chem. 
Null alignment 0.6% 0.2% 0.2% 
Cross alignment 2.1% 0.2% 4.4% 
n:1 alignment 2.1% 1.6% 1.2% 
1:n alignment 16.5% 21.4% 19.0% 
1:1 alignment 78.7% 76.7% 75.3% 
Table 3. Alignment types found in the gold 
standard 
When we analyze errors caused by our 
method, errors are mainly caused by n:1 align-
ment and cross-alignment. In order to produce 
relevant alignment results for n:1 alignment, we 
need information indicating that more than one 
                                                          
5 While (Och et al, 2003) differentiates sure and possible 
hand-annotated alignment, our gold-standard comes in only 
one variety. 
71
English term constituents are used as a concep-
tual unit. Due to lack of the information, our 
model has limitation on recovering errors caused 
by n:1 alignment. It is necessary to use domain 
specific corpus as a way of relaxing the problem. 
Cross alignment, which our model does not al-
low due to constrain 1, makes errors. Due to the 
cross alignment, the performance of our method 
in chemistry and biology is lower than that in 
physics, where there are few cross alignments in 
the gold standard. 
5 Conclusion 
In this paper, we have described an alignment 
algorithm between English and Korean term 
constituents. Our alignment algorithm can han-
dle cross alignment, n:1 alignment and 1:n 
alignment between term constituents. Our 
method shows about 94.7% precision, 93.2% 
recall and 6.1% alignment error rate. However, 
there are scopes to improve performance still 
further. Constraints should be relaxed in order to 
generalize our model and overcome errors 
caused by them.  
Our method can be applied to handle techni-
cal terms in three aspects. First, alignment re-
sults produced by our alignment algorithm help 
a machine translation system to consistently 
translate new English technical terms to Korean 
terms by considering domain of the technical 
terms. Second, alignment results between term 
constituents can be used for constructing term 
formation patterns or word formation patterns. 
Because relations between conceptual units can 
be extracted from the alignment results, we can 
construct concept-level term formation patterns 
using them. Third, the alignment results can be 
used as a resource for recognizing term varia-
tions. Because alignment relations acquired by 
our alignment model offer information about 
homonym, synonym and domain dependency, 
term variations related to certain term constitu-
ent can be recognized using them. 
Acknowledgement 
This work was supported by the Korea Ministry 
of Science and Technology, the Korea Ministry 
of Commerce, Industry and Energy, and the Ko-
rea Science and Engineering Foundation 
(KOSEF). 
References 
Brown P.F., V.S.A. Della Petra, V.J. Della Pietra and 
R.L. Mercer, ?The mathematics of statistical ma-
chine translation: parameter estimation?, Compu-
tational Linguistics, Vol. 19 No 2, (1993) 263?
311 
Chen, S, F., Aligning Sentences in Bilingual Corpora 
Using Lexical information, in proceedings of 31st 
ACL, (1993) 9?16 
Dempster A.P., N.M. Laird, and D.B. Rubin. Maxi-
mum likelihood from incomplete data via the EM 
algorithm. Journal of Royal Statistical Society, 
39(1):138, (1977) 
Gale, W. A. And Church K.W.  A program for al-
ingning sentences in Bilingual Corpora, Computa-
tional linguistics, vol 19, no 1, (1993), 75?102 
Germann, U. M.Jahr, Knight, K., Marcu, D. And 
Yamada, K. Fast Decoding and Optimal Decoding 
for Machine translation, in proceedings of 39th 
ACL, (2001) 228?235  
Haruno M., and Yamazaki, T. High-performance 
Bilingual Text alignment using Statistical and Dic-
tionary information, in proceedings of 34th ACL, 
(1996) 131?138 
Kay, M. and Roscheisen, M. Text-Translation Align-
ment, Computational Linguistics, Vol 19, No 1, 
(1993) 121?142  
L?pez de Teruel P. E., Jos? M. Garc?a and Manuel E. 
Acacio. The Parallel EM Algorithm and its Appli-
cations in Computer Vision. Parallel and Distrib-
uted Processing Techniques and Applications, 
(1999). 
Manning, C.D. and H. Schutze, Foundations of statis-
tical natural language processing, MIT Press 
(1999) 
Ministry of Culture and Tourism, "Forming the foun-
dation of Terminology Standardization?, 
http://www.korterm.or.kr/, (2002) 
Och, Franz Josef and Hermann Ney. A Systematic 
Comparison of Various Statistical Alignment 
Models, Computational Linguistics, Vol 29 (1), 
(2003), 19?51  
Sager, J.C. ?Section 1.2.1 Term formation?, in Hand-
book of terminology management Vol.1, John 
Benjamins publishing company, (1997) 
Shin Jung Ho and Key-Sun Choi (1995), Aligning a 
parallel Korean-English corpus at word and phrase 
level, Proceedings of the 3rd Natural Language 
Processing Pacific Rim Symposium (NLPRS'95), 
(1995)  223?227 
72
Term Recognition Using Technical Dictionary Hierarchy 
 
Jong-Hoon Oh, KyungSoon Lee, and Key-Sun Choi 
Computer Science Dept., Advanced Information TechnologyResearch Center (AITrc), and 
Korea Terminology Research Center for Language and Knowledge Engineering (KORTERM) 
Korea Advanced Institute of Science & Technology (KAIST)  
Kusong-Dong, Yusong-Gu Taejon, 305-701 Republic of Korea  
{rovellia,kslee,kschoi}@world.kaist.ac.kr  
 
 
 
Abstract  
In recent years, statistical approaches on 
ATR (Automatic Term Recognition) have 
achieved good results. However, there are 
scopes to improve the performance in 
extracting terms still further. For example, 
domain dictionaries can improve the 
performance in ATR. This paper focuses on 
a method for extracting terms using a 
dictionary hierarchy. Our method produces 
relatively good results for this task. 
Introduction 
In recent years, statistical approaches on ATR 
(Automatic Term Recognition) (Bourigault, 
1992; Dagan et al 1994; Justeson and Katz, 
1995; Frantzi, 1999) have achieved good results. 
However, there are scopes to improve the 
performance in extracting terms still further. For 
example, the additional technical dictionaries 
can be used for improving the accuracy in 
extracting terms. Although, the hardship on 
constructing an electronic dictionary was major 
obstacles for using an electronic technical 
dictionary in term recognition, the increasing 
development of tools for building electronic 
lexical resources makes a new chance to use 
them in the field of terminology. From these 
endeavour, a number of electronic technical 
dictionaries (domain dictionaries) have been 
acquired.  
Since newly produced terms are usually made 
out of existing terms, dictionaries can be used as 
a source of them. For example, ?distributed 
database? is composed of ?distributed? and 
?database? that are terms in a computer science 
domain. Further, concepts and terms of a domain 
are frequently imported from related domains. 
For example, the term ?Geographical 
Information System (GIS)? is used not only in a 
computer science domain, but also in an 
electronic domain. To use these properties, it is 
necessary to build relationships between 
domains. The hierarchical clustering method 
used in the information retrieval offers a good 
means for this purpose. A dictionary hierarchy 
can be constructed by the hierarchical clustering 
method. The hierarchy helps to estimate the 
relationships between domains. Moreover the 
estimated relationships between domains can be 
used for weighting terms in the corpus. For 
example, a domain of electronics may have a 
deep relationship to that of computer science. As 
a result, terms in the dictionary of electronics 
domain have a higher probability to be terms of 
computer science domain than terms in the 
dictionary of others do (Felber, 1984).  
The recent works on ATR identify the 
candidate terms using shallow syntactic 
information and score the terms using statistical 
measure such as frequency. The candidate terms 
are ranked by the score and are truncated by the 
thresholds. However, the statistical method 
solely may not give accurate performance in 
case of small sized corpora or very specialized 
domains, where the terms may not appear 
repeatedly in the corpora. 
In our approach, a dictionary hierarchy is 
used to avoid these limitations. In the next 
section, we describe the overall method 
description. In section 2, section 3, and section 4, 
we describe primary methods and its details. In 
section 5, we describe experiments and results 
1 Method Description 
 
The description of the proposed method is 
shown in figure 1. There are three main steps in 
our method. In the first stage, candidate terms 
that are complex nominal are extracted by a 
linguistic filter and a dictionary hierarchy is 
constructed. In the second stage, candidate terms 
are scored by each weighting scheme. In 
dictionary weighing scheme, candidate terms are 
scored based on the kind of domain dictionary 
where terms appear. In statistical weighting 
scheme, terms are scored by their frequency in 
the given corpus. In transliterated word 
weighting scheme, terms are scored by the 
number of transliterated foreign words in the 
terms. In the third stage, each weight is 
normalized and combined to Term weight 
(Wterm), and terms are extracted by Term weight.   
Figure 1. The method description 
2 Dictionary Hierarchy 
2.1 Resource 
Field 
Agrochemical, Aerology, Physics, Biology, 
Mathematics, Nutrition, Casting, Welding, 
Dentistry, Medical, Electronical engineering, 
Computer science, Electronics, Chemical 
engineering, Chemistry.... and so on. 
Table 1. The fragment of a list: dictionaries of 
domains used for constructing the hierarchy. 
A dictionary hierarchy is constructed using 
bi-lingual dictionaries (English to Korean) of the 
fifty-seven domains. Table 1 lists the domains 
that are used for constructing the dictionary 
hierarchy. The dictionaries belong to domains of 
science and technology. Moreover, terms that do 
not appear in any dictionary (henceforth we call 
them unregistered terms) are complemented by a 
domain tagged corpus. We use a corpus, called 
ETRI-KEMONG test collection, with the 
documents of seventy-six domains to 
complement unregistered terms and to eliminate 
common term.  
2.2 Constructing Dictionary Hierarchy  
The clustering method is used for constructing 
a dictionary hierarchy. The clustering is a 
statistical technique to generate a category 
structure using the similarity between 
documents (Anderberg, 1973). Among the 
clustering methods, a reciprocal nearest 
neighbor (RNN) algorithm (Murtaugh, 1983) 
based on a hierarchical clustering model is used, 
since it joins the cluster minimizing the increase 
in the total within-group error sum of squares at 
each stage and tends to make a symmetric 
hierarchy (Lorr, 1983). The algorithm to form a 
cluster can be described as follows:  
 
1. Determine all inter-object (or 
inter-dictionary) dissimilarity. 
2. Form cluster from two closest objects 
(dictionaries) or clusters. 
3. Recalculate dissimilarities between new 
cluster created in the step2 and other 
object (dictionary) or cluster already 
made. (all other inter-point dissimilarities 
are unchanged). 
4. Return to Step2, until all objects 
(including cluster) are in the one cluster. 
 
In the algorithm, all objects are treated as a 
vector such as Di = (xi1, xi2, ... , xiL ). In the step 
1, inter-object dissimilarity is calculated based 
on the Euclidian distance. In the step2, the 
closest object is determined by a RNN. For 
given object i and object j, we can define that 
there is a RNN relationship between i and j 
when the closest object of i is object j and the 
closest object of j is object i. This is the reason 
why the algorithm is called a RNN algorithm. A 
dictionary hierarchy is constructed by the 
algorithm, as shown in figure 2. There are ten 
domains in the hierarchy ? this is a fragment of 
whole hierarchy. 
 
Technical
Dictionaries
Domain 
tagged
Documents 
?.A CB D ?.
Constructing  
hierarchy
POS-tagged
Corpus Linguistic filter
Abbreviation and
Translation pairs
extraction
Candidate term
Frequency based
Weighing
Transliterated
Word detection
Transliterated word
Based Weighting
Complement 
Unregistered Term
Scoring by hierarchy
Eliminate
Common Word
Dictionary based 
Weighting
Statistical
Weight
Transliterated
Word Weight
Dictionary
Weight
Term Recognition
 
Figure 2. The fragment of whole dictionary 
hierarchy : The hierarchy shows that domains 
clustered in the terminal node such as chemical 
engineering and chemistry are highly related. 
2.3 Scoring Terms Using Dictionary 
Hierarchy 
The main idea for scoring terms using the 
hierarchy is based on the premise that terms in 
the dictionaries of the target domain and terms 
in the dictionary of the domain related to the 
target domain act as a positive indicator for 
recognizing terms. Terms in the dictionaries of 
the domains that are not related to the target 
domain act as a negative indicator for 
recognizing terms. We apply the premise for 
scoring terms using the hierarchy. There are 
three steps to calculate the score. 
 
1. Calculating the similarity between the 
domains using the formula (2.1) (Maynard 
and Ananiadou, 1998) 
 
where  
Depthi: the depth of the domaini node in the 
hierarchy 
Commonij: the depth of the deepest node 
sharing between the domaini and the 
domainj in the path from the root. 
 
In the formula (2.1), the depth of the node 
is defined as a distance from the root ? the 
depth of a root is 1. For example, let the 
parent node of C1 and C8 be the root of 
hierarchy in figure 2. The similarity between 
?Chemistry? and ?Chemical engineering? is 
calculated as shown below in table 2: 
 
Domain Chemistry Chemical 
Engineering 
Path from 
the root 
Root->C8-> 
C9->Chemistry 
Root->C8->C9-> 
Chemical 
Engineering 
Depthi 4 4 
Common ij 3 3 
Similarity 
ij 
2*3/(4+4) =0.75 2*3/(4+4) =0.75 
Table 2. Similarityij  calculation: The table shows 
an example in caculating similarity using formula 
(2.1). In the example, Chemical engineering 
domain and Chemistry domain are used. Path, 
Depth, and Common are calculated according to 
figure 1. Then similarity between domains are 
determined to 0.75. 
2.Term scoring by distance between a target 
domain and domains where terms appear: 
 
  
where  
N: the number of dictionaries where a 
term appear  
Similarityti: the similarity between the 
target domain and the domain dictionary 
where a term appears  
 
For example, in figure 2, let the target 
domain be physics and a term ?radioactive? 
appear in physics, chemistry and astronomy 
domain dictionaries. Then similarity between 
physics and the domains where the term 
?radioactive? appears can be estimated by 
formula (2.1) as shown below. Finally, 
Score(radioactive) is calculated by formula 
(2.2) ? score is (0.4+1+0.7)/3.:  
 
N 3 
similarity physics-chemistry 0.4 
similarity physics-physics 1 
similarity physics-astronomy 0.7 
Score(radioactive) 2.1*1/3 = 0.7  
Table 3. Scoring terms based on similarity 
between domains 
 
3. Complementing unregistered terms and 
common terms by domain tagged corpora.  
 
)1.2(2
ji
ij
ij depthdepth
Commonsimilarity
+
?
=
)2.2(1)(
1
?
=
=
N
i
tisimilarityNtermScore
 
where 
W: the number of words in the term ??? 
dofi: the number of domain that words in 
the term appear in the domain tagged 
corpus. 
 
Consider two exceptional possible cases. First, 
there are unregistered terms that are not 
contained in any dictionaries. Second, some 
commonly used terms can be used to describe a 
special concept in a specific domain dictionary.  
Since an unregistered term may be a newly 
created term of domains, it should be considered 
as a candidate term. In contrast with an 
unregistered term, common terms should be 
eliminated from candidate terms. Therefore, the 
score calculated in the step 2 should be 
complemented for these purposes. In our method, 
the domain tagged corpus (ETRI 1997) is used. 
Each word in the candidate terms ? they are 
composed of more than one word ? can appear 
in the domain tagged corpus. We can count the 
number of domains where the word appears. If 
the number is large, we can determine that the 
word have a tendency to be a common word. If 
the number is small, we can determine that the 
word have a high probability to be a valid term. 
In this paper, the score calculated by the 
dictionary hierarchy is called Dictionary Weight 
(WDic). 
3. Statistical Method 
The statistical method is divided into two 
elements. The first element, the Statistical 
Weight, is based on the frequencies of terms. 
The second element, the Transliterated word 
Weight, which is based on the number of 
transliterated foreign word in the candidate term. 
This section describes the above two elements.  
3.1. Statistical Weight: Frequency Based 
Weight 
In the Statistical Weight, not only 
abbreviation pairs and translation pairs in a 
parenthetical expression but also frequencies of 
terms are considered. Abbreviation pairs and 
translation pairs are detected using the following 
simple heuristics: 
 
For a given parenthetical expression A(B), 
1. Check on a fact that A and B are 
abbreviation pairs. The capital letter of A is 
compared with that of B. If the half of the 
capital letter are matched for each other 
sequentially, A and B are determined to 
abbreviation pairs (Hisamitsu et. al, 1998). 
For example, ?ISO? and ?International 
Standardization Organization? is detected as 
an abbreviation in a parenthetical expression 
?ISO (International Standardization 
Organization)?. 
 
2. Check on a fact that A and B are translation 
pairs. Using the bi-lingual dictionary, it is 
determined. 
 
After detecting abbreviation pairs and 
translation pairs, the Statistical Weight (WStat) of 
the terms is calculated by the formula (3.1). 
 
where  
?: a candidate term 
|?|: the length of a term??? 
S (?): abbreviation and translation pairs of 
??? 
T(?): The set of candidate terms that nest 
??? 
f(?): the frequency of ?? ? 
C(T(?)): The number of elements in T(?) 
 
In the formula (3.1), the nested relation is 
defined as follows: let A and B be a candidate 
term. If A contains B, we define that A nests B.  
The formula implies that abbreviation pairs 
and translation pairs related to ??? is counted as 
well as ??? itself and productivity of words in 
the nested expression containing ??? gives more 
weight, when the generated expression contains 
???. Moreover, formula (1) deals with a single- 
word term, since an abbreviation such as GUI 
(Graphical User Interface) is single word term 
and English multi-word term usually translated 
to Korean single-word term ? (e.g. distributed 
database => bunsan deitabeisu) 
)3.2(*)1)(()( 1W
dof
ScoreW
W
i
i
Dic
?
=+= ??
( )
??
?
?
??
?
?
?
??
?
?
?
??
?
?
?
??
?
?
?
??
?
?
?
+?
?
=
?
?
?
??
?
??
}{)(
)(
}{)(
)1.3())((
)(
)(
)(
)(
???
??
???
?
?
??
???
?
S
T
S
Stat
otherwiseTC
f
f
nestedisiff
W
3.2 Transliterated word Weight: By 
Automatic Extraction of Transliterated 
words 
Technical terms and concepts are created in 
the world that must be translated or transliterated. 
Transliterated terms are one of important clues 
to identify the terms in the given domain. We 
observe dictionaries of computer science and 
chemistry domains to investigate the 
transliterated foreign words. In the result of 
observation, about 53% of whole entries in a 
dictionary of a computer science domain are 
transliterated foreign words and about 48% of 
whole entries in a dictionary of a chemistry 
domain are transliterated foreign words. Because 
there are many possible transliterated forms and 
they are usually unregistered terms, it is difficult 
to detect them automatically.  
In our method, we use HMM (Hidden Markov 
Model) for this task (Oh, et al, 1999). The main 
idea for extracting a foreign word is that the 
composition of foreign words would be different 
from that of pure Korean words, since the 
phonetic system for the Korean language is 
different from that of the foreign language. 
Especially, several English consonants that 
occur frequently in English words, such as 
?p?, ?t?, ?c?, and ?f?, are transliterated into Korean 
consonants ?p?, ?t?, ?k?, and ?p? respectively. 
Since these consonants of Korean are not used in 
pure Korean words frequently, this property can 
be used as an important clue for extracting a 
foreign word from Korean. For example, in a 
word, ?si-seu-tem? (system), the syllable ?tem? 
have a high probability to be a syllable of 
transliterated foreign word, since the consonant 
of ?t? in the syllable ?tem? is usually not used in 
a pure Korean word. Therefore, the consonant 
information which is acquired from a corpus can 
be used to determine whether a syllable in the 
given term is likely to be the part of a foreign 
word or not.  
Using HMM, a syllable is tagged with ?K? or 
?F?. A syllable tagged with ?K? means that it is 
part of a pure Korean word. A syllable tagged 
with ?F? means that it is part of a transliterated 
word. For example, ?si-seu-tem-eun (system is)? 
is tagged with  ?si/F + seu/F + tem/F + eun/K?. 
We use consonant information to detect a 
transliterated word like lexical information in 
part-of-speech-tagging. The formula (3.2) is 
used for extracting a transliterated word and the 
formula (3.3) is used for calculating the 
Transliterated Word Weight (WTrl). The formula 
(3.3) implies that terms have more transliterated 
foreign words than common words do. 
 
where  
si: i-th consonant in the given word. 
ti: i-th tag (?F? or ?K?) of the syllable in the 
given word. 
 
 
where  
|?| is the number of words in the term ? 
trans(?) is the number of transliterated 
words in the term ? 
4.Term Weighting 
The three individual weights described above 
are combined according to the following 
formula (4.1) called Term Weight (WTerm) for 
identifying the relevant terms.  
 
Where 
?: a candidate term ??? 
f,g,h : normalization function 
?+?+? = 1 
 
In the formula (4.1), the three individual 
weights are normalized by the function f, g, and 
h respectively and weighted parameter ?,?, and 
?. The parameter ?,?, and ? are determined by 
experiment with the condition ?+?+? = 1. Each 
value which is used in this paper is ?=0.6, ? 
=0.1, and ?=0.3 respectively. 
 
)3.3()()(
?
?
?
transWTrl =
)2.3()|(),|(
)|()()()|(
13
21
121
??
???
???
???
?
=
??
==
??
n
i
ii
n
i
iii tsptttp
ttptpSPSTP
)1.4())(())((
))(()(
????
???
StatTrl
Dicterm
WhWg
WfW
?+?
+?=
5. Experiment 
The proposed method is tested on a corpus of 
computer science domains, called the KT test 
collection. The collection contains 4,434 
documents and 67,253 words and contains 
documents about the abstract of the paper (Park. 
et al, 1996). It was tagged with a part-of-speech 
tagger for evaluation. We examined the 
performance of the Dictionary Weight (WDic) to 
show its usefulness. Moreover, we examined 
both the performance of the C-value that is 
based on the statistical method (Frantzi. et al, 
1999) and the performance of the proposed 
method. 
5.1 Evaluation Criteria 
Two domain experts manually carry out the 
assessment of the list of terms extracted by the 
proposed method. The results are accepted as the 
valid term when both of the two experts agree on 
them. This prevents the evaluation from being 
carried out subjectively, when one expert 
assesses the results. The results are evaluated by 
a precision rate. A precision rate means that the 
proportion of correct answers to the extracted 
results by the system. 
5.2 Evaluation by Dictionary Weight 
(WDic) 
In this section, the evaluation is performed 
using only WDic to show the usefulness of a 
dictionary hierarchy to recognize the relevant 
terms The Dictionary Weight is based on the 
premise that the information of the target 
domain is a good indicator for identifying terms. 
The term in the dictionaries of the target domain 
and the domain related to the target domain acts 
as a positive indicator for recognizing terms. 
The term in the dictionaries of the domains, 
which are not related to the target domain acts as 
a negative indicator for recognizing terms. The 
dictionary hierarchy is constructed to estimate 
the similarity between one domain and another. 
 
 Top 10% Bottom 10% 
The Valid Term 94% 54.8% 
Non-Term 6% 45.2% 
Table 4.  terms and non-terms by Dictionary 
Weight 
The result, depicted in table 4, can be 
interpreted as follows: In the top 10% of the 
extracted terms, 94% of them are the valid terms 
and 6% of them are non-terms. In the bottom 
10% of the extracted terms, 54.8% of them are 
the valid terms and 45.2% of them are non-terms. 
This means that the relevant terms are much 
more than non-terms in the top 10% of the result, 
while non-terms are much more than the 
relevant terms in the bottom 10% of the result.  
 
The results are summarized as follow:  
 
!"According as a term has a high 
Dictionary Weight (WDic), it is apt 
to be valid. 
!"More valid terms have a high 
Dictionary Weight (WDic) than 
non-terms do 
 
5.3 Overall Performance 
Table 5 and figure 3 show the performance of 
the proposed method and of the C-value method. 
By dividing the ranked lists into 10 equal 
sections, the results are compared. Each section 
contains the 1291 terms and is evaluated 
independently. 
 
C-value The proposed 
method 
Section # of 
term 
Precision # of 
term 
Precision 
1 1181 91.48% 1241 96.13% 
2 1159 89.78% 1237 95.82% 
3 1207 93.49% 1213 93.96% 
4 1192 92.33% 1174 90.94% 
5 1206 93.42% 1154 89.39% 
6 981 75.99% 1114 86.29% 
7 934 72.35% 1044 80.87% 
8 895 69.33% 896 69.40% 
9 896 69.40% 780 60.42% 
10 578 44.77% 379 29.36% 
Table 5.  Precision rates of C-value and the 
proposed method : Section contain 1291 terms and 
precision is evaluated independently. For example, 
in section 1, since there are 1291 candidate terms 
and 1241 relevant terms by the proposed method, 
the precision rate in section 1 is 96.13% . 
The result can be interpreted as follows. In the 
top sections, the proposed method shows the 
higher precision rate than the C-value does. The 
distribution of valid terms is also better for the 
proposed method, since there is a downward 
tendency from section 1 to section 10. This 
implies that the terms with higher weight scored 
by our method have a higher probability to be 
valid terms. Moreover, the precision rate of our 
method shows the rapid decrease from section 6 
to section 10. This indicates that most of valid 
terms are located in the top sections. 
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Section
Pre
cis
ion
The Proposed method C-value
Figure 2. The performance of C-value and the 
proposed method in each section 
The results can be summarized as follow : 
 
!"The proposed method extracts a valid 
term more accurate than C-value does. 
!"Most of the valid terms are in the top 
section extracted by the proposed 
method. 
Conclusion 
In this paper, we have described a method for 
term extraction using a dictionary hierarchy. It is 
constructed by clustering method and is used for 
estimating the relationships between domains. 
Evaluation shows improvement over the C-value. 
Especially, our approach can distinguish the 
valid terms efficiently ? there are more valid 
terms in the top sections and less valid terms in 
the bottom sections. Although the method 
targets Korean, it can be applicable to English 
by slight change on the Tweight (WTrl).  
However, there are many scopes for further 
extensions of this research. The problems of 
non-nominal terms (Klavans and Kan, 1998), 
term variation (Jacquemin et al, 1997), and  
relevant contexts (Maynard and Ananiadou, 
1998), can be considered for improving the 
performance. Moreover, it is necessary to apply 
our method to practical NLP systems, such as an 
information retrieval system and a 
morphological analyser. 
Acknowledgements 
KORTERM is sponsored by the Ministry of Culture 
and Tourism under the program of King Sejong 
Project. Many fundamental researches are supported 
by the fund of Ministry of Science and Technology 
under a project of plan STEP2000. And this work 
was partially supported by the KOSEF through the 
?Multilingual Information Retrieval? project at the 
AITrc. 
References  
Anderberg, M.R. (1973) Cluster Analysis for 
Applications. New York: Academic 
Bourigault, D. (1992) Surface grammatical analysis 
for the extraction of terminological noun phrases. 
In Proceedings of the 14th International Conference 
on Computational Linguistics, COLING?92 pp. 
977-981. 
Dagan, I. and K. Church. (1994) Termight: 
Identifying and terminology In Proceedings of the 
4th Conference on Applied Natural Language 
Processing, Stuttgart/Germany, 1994. Association 
for Computational Linguistics. 
ETRI (1997) Etri-Kemong set 
Felber Helmut (1984) Terminology Manual, 
International Information Centre for Terminology 
(Infoterm) 
Frantzi, K.T. and S.Ananiadou (1999) The 
C-value/NC-value domain independent method for 
multi-word term extraction. Journal of Natural 
Language Processing, 6(3) pp. 145-180 
Hisamitsu, Toru and Yoshiki Niwa (1998) Extraction 
of useful terms from parenthetical expressions by 
using simple rules and statistical measures. In First 
Workshop on Computational Terminology 
Computerm?98, pp 36-42 
Jacquemin, C., Judith L.K. and Evelyne, T. (1997) 
Expansion of Muti-word Terms for indexing and 
Retrieval Using Morphology and Syntax, 35th 
Annual Meeting of the Association for 
Computational Linguistics, pp 24-30 
Justeson, J.S. and S.M. Katz (1995) Technical 
terminology : some linguistic properties and an 
algorithm for identification in text. Natural 
Language Engineering, 1(1) pp. 9-27  
Klavans, J. and Kan M.Y (1998) Role of Verbs in 
Document Analysis, In Proceedings of the 17th 
International Conference on Computational 
Linguistics, COLING?98 pp. 680-686. 
Lauriston, A. (1996) Automatic Term Recognition : 
performance of Linguistic and Statistical 
Techniques. Ph.D. thesis, University of Manchester 
Institute of Science and Technology. 
Lorr, M. (1983) Cluster Analysis and Its Application, 
Advances in Information System Science,8 , 
pp.169-192 
Murtagh, F. (1983) A Survey of Recent Advances in 
Hierarchical Clustering Algorithms, Computer 
Journal, 26, 354-359 
Maynard, D. and Ananiadou, S. (1998) Acquiring 
Context Information for Term Disambiguation In 
First Workshop on Computational Terminology 
Computerm?98, pp 86-90 
Oh, J.H. and K.S. Choi (1999) Automatic extraction 
of a transliterated foreign word using hidden 
markov model , In Proceedings of the 11th Korean 
and Processing of Korean Conference pp. 137-141 
(In Korean). 
Park, Y.C., K.S. Choi, J.K.Kim and Y.H. Kim (1996). 
Development of the KT test collection for 
researchers in information retrieval. In the 23th 
KISS Spring Conference (in Korean) 
  
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 658?667,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Can Chinese Phonemes Improve Machine Transliteration?:
A Comparative Study of English-to-Chinese Transliteration Models
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{rovellia,uchimoto,torisawa}@nict.go.jp
Abstract
Inspired by the success of English
grapheme-to-phoneme research in speech
synthesis, many researchers have pro-
posed phoneme-based English-to-Chinese
transliteration models. However, such ap-
proaches have severely suffered from the
errors in Chinese phoneme-to-grapheme
conversion. To address this issue,
we propose a new English-to-Chinese
transliteration model and make system-
atic comparisons with the conventional
models. Our proposed model relies on
the joint use of Chinese phonemes and
their corresponding English graphemes
and phonemes. Experiments showed that
Chinese phonemes in our proposed model
can contribute to the performance im-
provement in English-to-Chinese translit-
eration.
1 Introduction
1.1 Motivation
Transliteration, i.e., phonetic translation, is com-
monly used to translate proper names and techni-
cal terms across languages. A variety of English-
to-Chinese machine transliteration models has
been proposed in the last decade (Meng et al,
2001; Gao et al, 2004; Jiang et al, 2007; Lee
and Chang, 2003; Li et al, 2004; Li et al, 2007;
Wan and Verspoor, 1998; Virga and Khudanpur,
2003). They can be categorized into those based
on Chinese phonemes (Meng et al, 2001; Gao
et al, 2004; Jiang et al, 2007; Lee and Chang,
2003; Wan and Verspoor, 1998; Virga and Khu-
danpur, 2003) and those that don?t rely on Chinese
phonemes (Li et al, 2004; Li et al, 2007).
Inspired by the success of English grapheme-to-
phoneme research in speech synthesis, many re-
searchers have proposed phoneme-based English-
to-Chinese transliteration models. In these ap-
proaches, Chinese phonemes are generated from
English graphemes or phonemes, and then the
Chinese phonemes are converted into Chinese
graphemes (or characters), where Chinese Pinyin
strings1 are used for representing a syllable-level
Chinese phoneme sequence. Despite its high ac-
curacy in generating Chinese phonemes from En-
glish, this approach has severely suffered from er-
rors in Chinese phoneme-to-grapheme conversion,
mainly caused by Chinese homophone confusion
? one Chinese Pinyin string can correspond to sev-
eral Chinese characters (Li et al, 2004). For ex-
ample, the Pinyin string ?LI? corresponds to such
different Chinese characters as,, and. For
this reason, it has been reported that English-to-
Chinese transliteration without Chinese phonemes
outperforms that with Chinese phonemes (Li et al,
2004).
Then ?Can Chinese phonemes improve
English-to-Chinese transliteration, if we can re-
duce the errors in Chinese phoneme-to-grapheme
conversion?? Our research starts from this
question.
1.2 Our Approach
Previous approaches using Chinese phonemes
have relied only on Chinese phonemes in Chi-
nese phoneme-to-grapheme conversion. However,
the simple use of Chinese phonemes doesn?t al-
ways provide a good clue to reduce the ambi-
guity in Chinese phoneme-to-grapheme conver-
sion. Let us explain with an example, the Chinese
transliteration of Greeley in Table 1, where Chi-
nese phonemes are represented in terms of Chi-
nese Pinyin strings and English phonemes are rep-
resented by ARPAbet symbols2.
In Table 1, Chinese Pinyin string ?LI? corre-
sponds to two different Chinese characters, and
1Pinyin, the most commonly used Romanization sys-
tem for Chinese characters, faithfully represents Chinese
658
Table 1: Chinese Pinyin string ?LI? and its corre-
sponding Chinese characters in Chinese transliter-
ation of Greeley
English grapheme g ree ley
English phoneme G R IY L IY
Chinese Pinyin GE LI LI
Chinese character   
. It seems difficult to find evidence for select-
ing the correct Chinese character corresponding to
each Chinese Pinyin string ?LI? by just looking
at the sequence of Chinese Pinyin strings ?GE LI
LI.? However, English graphemes (ree and ley) or
phonemes (?R IY? and ?L IY?) corresponding to
Chinese Pinyin string ?LI?, especially their conso-
nant parts (r and l in the English graphemes and
?R? and ?L? in the English phonemes), provide
strong evidence to resolve the ambiguity. Thus,
we can easily find rules for the conversion from
Chinese Pinyin string ?LI? to and as follows:
? ? ?R IY?, LI ? ?
? ? ?L IY?, LI ? ?
Based on the observation, we propose an
English-to-Chinese transliteration model based on
the joint use of Chinese phonemes and their corre-
sponding English graphemes and phonemes. We
define a set of English-to-Chinese transliteration
models and categorize them into the following
three classes:
? M
I
: Models Independent of Chinese
phonemes
? M
S
: Models based on Simple use of Chinese
phonemes
? M
J
: Models based on Joint use of Chi-
nese phonemes and English graphemes and
phonemes that correspond to our proposed
model.
Our comparison among the three types of translit-
eration models can be summarized as follows.
? The M
I
models relying on either English
graphemes or phonemes could not outper-
form those based on both English graphemes
and phonemes.
phonemes and syllables (Yin and Felley, 1990).
2http://www.cs.cmu.edu/
?
laura/pages/
arpabet.ps
? The M
S
models always showed the worst
performance due to the severe error rate in
Chinese phoneme-to-grapheme conversion.
? The M
J
models significantly reduced er-
rors in Chinese phoneme-to-grapheme con-
version; thus they achieved the best perfor-
mance.
The rest of this paper is organized as follows.
Section 2 introduces the notations used through-
out this paper. Section 3 describes the translitera-
tion models we compared. Section 4 describes our
tests and results. Section 5 concludes the paper
with a summary.
2 Preliminaries
Let E
G
be an English word composed of n English
graphemes, and let E
P
be a sequence of English
phonemes that represents the pronunciation of E
G
.
Let C
G
be a sequence of Chinese graphemes cor-
responding to the Chinese transliteration of E
G
,
and let C
P
be a sequence of Chinese phonemes
that represents the pronunciation of C
G
.
C
P
corresponds to a sequence of the Chinese
Pinyin strings of C
G
. Because a Chinese Pinyin
string represents the pronunciation of a sylla-
ble consisting of consonants and vowels, we di-
vide a Chinese Pinyin string into consonant and
vowel parts like ?L+I?, ?L+I+N?, and ?SH+A.?
In this paper, we define a Chinese phoneme
as the vowel and consonant parts in a Chinese
Pinyin string (e.g., ?L?, ?SH?, and ?I?). A Chi-
nese character usually corresponds to multiple
English graphemes, English phonemes, and Chi-
nese phonemes (i.e.,  corresponds to English
graphemes ree, English phonemes ?R IY?, and
Chinese phonemes ?L I? in Table 1). To repre-
sent these many-to-one correspondences, we use
the well-known BIO labeling scheme to represent
a Chinese character, where B and I represent the
beginning and inside/end of the Chinese charac-
ters, respectively, and O is not used. Each Chi-
nese phoneme corresponds to a Chinese character
with B and I labels. For example, Chinese charac-
ter ?? in Table 1 can be represented as ?:B?
and ?:I?, where ?:B? and ?:I? correspond
to Chinese phonemes ?L? and ?I?, respectively. In
this paper, we define a Chinese grapheme as a Chi-
nese character represented with a BIO label, e.g.,
?:B? and ?:I.?
659
Table 2: eg
i
and its corresponding ep
i
, cp
i
, and cg
i
in Greeley and its corresponding Chinese translit-
eration ??
i 1 2 3 4 5 6 7
E
G
g r e e l e y
E
P
G R IY ? L IY ?
C
P
GE L I ? L I ?
GE LI ? LI ?
C
G
:B :B :I ? :B :I ?
  ?  ?
Then E
P
, C
P
, and C
G
can be segmented into a
series of sub-strings, each of which corresponds to
an English grapheme in E
G
. We can thus write
? E
G
= eg
1
, ? ? ? , eg
n
= eg
n
1
? E
P
= ep
1
, ? ? ? , ep
n
= ep
n
1
? C
P
= cp
1
, ? ? ? , cp
n
= cp
n
1
? C
G
= cg
1
, ? ? ? , cg
n
= cg
n
1
where eg
i
, ep
i
, cp
i
, and cg
i
represent the ith
English grapheme, English phonemes, Chinese
phonemes, and Chinese graphemes corresponding
to eg
i
, respectively.
Based on the definition, we model English-
to-Chinese transliteration so that each English
grapheme is tagged with its corresponding En-
glish phonemes, Chinese phonemes, and Chinese
graphemes. Table 2 illustrates eg
i
, ep
i
, cp
i
, and
cg
i
with the same example listed in Table 1 (En-
glish word Greeley and its corresponding Chinese
transliteration ??)3, where ? represents an
empty string.
3 Transliteration Model
We defined eighteen transliteration models to be
compared. These transliteration models are clas-
sified into three classes, M
I
, M
S
, andM
J
as de-
scribed in Section 1.2; each class has three basic
transliteration models and three hybrid ones. In
this section, we first describe the basic translit-
eration models in each class by focusing on the
main difference among the three classes and then
describe the hybrid transliteration models.
3We performed alignment between E
G
and E
P
and be-
tween E
P
and C
P
in a similar manner presented in Li et al
(2004). Then the two alignment results were merged using
E
P
as a pivot. Finally, we made a correspondence relation
among eg
i
, ep
i
, cp
i
, and cg
i
using the merged alignment re-
sult and the Pinyin table.
3.1 Basic Transliteration Models
The basic transliteration models in each class are
denoted as M(x, y).
? (x, y) ? X ? Y
? x ? X = {E
G
, E
P
, E
GP
}
? y ? Y = {?, C
P
, JC
P
}
x is an English-side parameter representing En-
glish grapheme (E
G
), English phoneme (E
P
), and
the joint use of English grapheme and phoneme
(E
GP
= ?E
G
, E
P
?) that contributes to generat-
ing Chinese phonemes or Chinese graphemes in
a transliteration model. y is a Chinese-phoneme
parameter that represents a way of using Chinese
phonemes to generate Chinese graphemes in a
transliteration model. Since M(x, ?) represents
a transliteration model that does not rely on Chi-
nese phonemes, it falls intoM
I
, while M(x, C
P
)
corresponds to a transliteration model in M
S
that
only uses Chinese phonemes in Chinese phoneme-
to-grapheme conversion. M(x, JC
P
) is a translit-
eration model in theM
J
class that generates Chi-
nese transliterations based on joint use of x and
Chinese phoneme C
P
, where x ? X . Thus,
M(x, JC
P
) can be rewritten as M(x, ?x, C
P
?),
where the joint representation of x and C
P
,
?x, C
P
?, is used in Chinese phoneme-to-grapheme
conversion. The three basic models inM
J
can be
interpreted as follows:
? M(E
G
, JC
P
) = M(E
G
, ?E
G
, C
P
?)
? M(E
P
, JC
P
) = M(E
P
, ?E
P
, C
P
?)
? M(E
GP
, JC
P
) = M(E
GP
, ?E
GP
, C
P
?)
M(E
G
, JC
P
) directly converts English
graphemes into Chinese phonemes without
the help of English phonemes and then gener-
ates Chinese transliterations based on the joint
representation of English graphemes and Chi-
nese phonemes. The main difference between
M(E
P
, JC
P
) and M(E
GP
, JC
P
) lies in the
use of English graphemes to generate Chinese
phonemes and graphemes. English graphemes
are only used in English grapheme-to-phoneme
conversion, and English phonemes play a crucial
role for generating Chinese transliteration in
M(E
P
, JC
P
). Chinese phoneme-to-grapheme
conversion that relies on the joint use of English
graphemes, English phonemes, and Chinese
660
PM(E
G
,JC
P
)
(C
G
|E
G
) =
?
?C
P
P (C
P
|E
G
)? P (C
G
|E
G
, C
P
) (1)
P
M(E
P
,JC
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
P
)? P (C
G
|E
P
, C
P
) (2)
P
M(E
GP
,JC
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
G
, E
P
)? P (C
G
|E
G
, E
P
, C
P
) (3)
P
M(E
G
,C
P
)
(C
G
|E
G
) =
?
?C
P
P (C
P
|E
G
)? P (C
G
|C
P
) (4)
P
M(E
P
,C
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
P
)? P (C
G
|C
P
) (5)
P
M(E
GP
,C
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
G
, E
P
)? P (C
G
|C
P
) (6)
phonemes is the key feature of M(E
GP
, JC
P
).
Because M(x, JC
P
) can be interpreted as
M(x, ?x, C
P
?), English-side parameter x de-
termines the English graphemes and phonemes,
or both jointly used with Chinese phonemes in
Chinese phoneme-to-grapheme conversion. Then
we can represent the three basic transliteration
models as in Eqs. (1)?(3), where P (C
G
|E
G
, C
P
),
P (C
G
|E
P
, C
P
), and P (C
G
|E
G
, E
P
, C
P
) are the
key points in our proposed models,M
J
.
The three basic transliteration models in M
S
? M(E
G
, C
P
), M(E
P
, C
P
), and M(E
GP
, C
P
) ?
are formulated as Eqs. (4)?(6). Chinese phoneme-
based transliteration models in the literature fall
into either M(E
G
, C
P
) or M(E
P
, C
P
) (Meng et
al., 2001; Gao et al, 2004; Jiang et al, 2007; Lee
and Chang, 2003; Wan and Verspoor, 1998; Virga
and Khudanpur, 2003). The three basic transliter-
ation models inM
S
are identical as those inM
J
,
except for the Chinese phoneme-to-grapheme con-
version method. They only depend on Chinese
phonemes in Chinese phoneme-to-grapheme con-
version represented as P (C
G
|C
P
) in Eqs. (4)?(6).
P
M(E
G
,?)
(C
G
|E
G
) = P (C
G
|E
G
) (7)
P
M(E
P
,?)
(C
G
|E
G
) (8)
=
?
?E
P
P (E
P
|E
G
)? P (C
G
|E
P
)
P
M(E
GP
,?)
(C
G
|E
G
) (9)
=
?
?E
P
P (E
P
|E
G
)? P (C
G
|E
G
, E
P
)
The three basic transliteration models in M
I
are
represented in Eqs. (7)?(9). Because theM
I
mod-
els are independent of Chinese phonemes, they are
the same as the transliteration models in the lit-
erature used for machine transliteration from En-
glish to other languages without relying on target-
language phonemes (Karimi et al, 2007; Malik,
2006; Oh et al, 2006; Sherif and Kondrak, 2007;
Yoon et al, 2007). Note that M(E
G
, ?) is the
same transliteration model as the one proposed by
Li et al (2004).
3.2 Hybrid Transliteration Models
The hybrid transliteration models in each class
are defined by discrete mixture between the prob-
ability distribution of the two basic transliter-
ation models, as in Eq. (10) (Al-Onaizan and
Knight, 2002; Oh et al, 2006), where 0 < ? <
1. We denote a hybrid transliteration model be-
tween two basic transliteration models M(x
1
, y)
and M(x
2
, y) as M(x
1
+ x
2
, y, ?), where y ?
Y = {?, C
P
, JC
P
}, x
1
6= x
2
, and x
1
, x
2
?
X = {E
G
, E
P
, E
GP
}. In this paper, we define
three types of hybrid transliteration models in each
class: M(E
G
+ E
P
, y, ?), M(E
G
+ E
GP
, y, ?),
and M(E
P
+ E
GP
, y, ?).
P
M(x
1
+x
2
,y,?)
(C
G
|E
G
) (10)
= ? ? P
M(x
1
,y)
(C
G
|E
G
)
+ (1? ?) ? P
M(x
2
,y)
(C
G
|E
G
)
3.3 Probability Estimation
Because Eqs. (1)?(9) can be estimated in a similar
way, we limit our focus to Eq. (3) in this section.
Assuming that P (E
P
|E
G
), P (C
P
|E
G
, E
P
), and
P (C
G
|E
G
, E
P
, C
P
) in Eq. (3) depend on the size
of the context window, k (k = 3 in this paper),
661
Table 3: Feature functions for P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
) with an example in Table 2, where i = 2
f
1
gram
3
(eg
i
) eg
i+2
i
= ?ree? cg
i
= ?:B?
f
2
pair
11
(cp
i?1
, cg
i?1
) cp
i?1
= ?G?, cg
i?1
= ?:B? cg
i
= ?:B?
f
3
pair
12
(cg
i?1
, cp
i?1
) cp
i
i?1
= ?GE L?, cg
i?1
= ?:B? cg
i
= ?:B?
f
4
pair
22
(cp
i?1
, cg
i?2
) eg
i
i?1
= ?gr?, epi
i?1
= ?G R? cg
i
= ?:B?
f
5
triple
1
(eg
i
, cp
i
, cg
i?1
) eg
i
= ?r?, cp
i?1
= ?GE?, cg
i?1
= ?:B? cg
i
= ?:B?
f
6
triple
2
(eg
i?1
, cg
i?1
, cp
i?1
) eg
i?1
= ?g?, cpi
i?1
= ?GE L?, cg
i?1
= ?:B? cg
i
= ?:B?
they can be simplified into a series of products in
Eqs. (11)?(13).
The maximum entropy model is used to esti-
mate the probabilities in Eqs. (11)?(13) (Berger
et al, 1996). Generally, a conditional maxi-
mum entropy model is an exponential model that
gives the conditional probability, as described in
Eq. (14), where ?
i
is the parameter to be estimated
and f
i
(a, b) is a feature function corresponding to
?
i
(Berger et al, 1996; Ratnaparkhi, 1997):
P (E
P
|E
G
) ?
?
i
P (ep
i
|ep
i?1
i?k
, eg
i+k
i?k
) (11)
P (C
P
|E
G
, E
P
) (12)
?
?
i
P (cp
i
|cp
i?1
i?k
, ?eg, ep?
i+k
i?k
)
P (C
G
|E
G
, E
P
, C
P
) (13)
?
?
i
P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
)
P (b|a) =
exp(
?
i
?
i
f
i
(a, b))
?
b
?
exp(
?
i
?
i
f
i
(a, b
?
))
(14)
f
i
(a, b) is a binary function returning TRUE
or FALSE based on context a and output b.
If f
i
(a, b)=1, its corresponding model parame-
ter ?
i
contributes toward conditional probability
P (b|a) (Berger et al, 1996; Ratnaparkhi, 1997).
The feature functions used here are defined in
terms of context predicates ? a function return-
ing TRUE or FALSE that depends on the presence
of the information in the current context (Ratna-
parkhi, 1997). Context predicates and their de-
scriptions used are given in Table 4.
N-GRAM includes gram
1
(u
j
), gram
2
(u
j
), and
gram
3
(u
j
) corresponding to a unigram, a bigram,
and a trigram, respectively. PAIR includes a pair of
unigrams (pair
11
), unigram and bigram (pair
12
),
and bigrams (pair
22
). TRIPLE includes a triple of
three unigrams (triple
1
) and a triple of two uni-
grams and one bigram (triple
2
). Note that if dif-
ferent context predicates represent the same con-
text, we accept one of them and ignore the others
Table 4: Context predicates and their descriptions
Category Context predicates Description
N-GRAM gram
1
(u
j
) u
j
gram
2
(u
j
) uj+1
j
gram
3
(u
j
) uj+2
j
PAIR pair
11
(u
j
, v
k
) u
j
, v
k
pair
12
(u
j
, v
k
) u
j
, v
k+1
k
pair
22
(u
j
, v
k
) u
j+1
j
, v
k+1
k
TRIPLE triple
1
(u
j
, v
k
, w
l
) u
j
, v
k
, w
l
triple
2
(u
j
, v
k
, w
l
) u
j
, v
k
, w
l+1
l
(e.g., pair
12
(u
j
, u
j+1
) = trigram(u
j
) = u
j+2
j
).
Table 3 represents the examples of feature func-
tions for P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
).
We used the ?Maximum Entropy Modeling
Toolkit?4 to estimate the probabilities and the
LBFGS algorithm to find ?
i
in Eq. (14). For
each transliteration model, we produced n-best
transliterations using a stack decoder (Schwartz
and Chow, 1990).
3.4 Summary
In this paper, we defined eighteen transliteration
models to be compared. There are six translitera-
tion models, three basic and three hybrid ones, in
each class, M
I
, M
S
, and M
J
. We compared the
transliteration models from the viewpoint of Chi-
nese phonemes or the class of transliteration mod-
els in our experiments.
4 Testing and Results
We used the same test set used in Li et al (2004)
for our testing5. It contains 37,694 pairs of English
words and their official Chinese transliterations
4Available at http://homepages.inf.ed.ac.
uk/s0450736/maxent_toolkit.html
5This test set was also used in ?NEWS09 machine translit-
eration shared task? for English-to-Chinese transliteration (Li
et al, 2009)
662
extracted from the ?Chinese Transliteration of For-
eign Personal Names? (Xinhua News Agency,
1992), which includes names in English, French,
German, and many other foreign languages (Li et
al., 2004). We used the same test data as in Li et
al. (2004). But we randomly selected 90% of the
training data used in Li et al (2004) as our training
data and the remainder as the development data, as
shown in Table 5.
Table 5: Number of English-Chinese translitera-
tion pairs in each data set
Ours Li et al (2004)
Training data 31,299 34,777
Development data 3,478 N/A
Blind test data 2,896 2,896
We used the training data for training the
transliteration models. For each model, we tuned
the parameters including the number of iterations
for training the maximum entropy model and a
Gaussian prior for smoothing the maximum en-
tropy model using the development data. Further,
the development data was used to select param-
eter ? of the hybrid transliteration models. We
varied parameter ? from 0 to 1 in 0.1 intervals
(i.e., ?=0, 0.1, 0.2, ? ? ? ,1) and tested the perfor-
mance of the hybrid models with the development
data. Then we chose ? that showed the best per-
formance in each hybrid model. The blind test
data was used for evaluating the performance of
each transliteration model. The CMU Pronounc-
ing Dictionary6, which contains about 120,000
English words and their pronunciations, was used
for estimating P (E
P
|E
G
).
We conducted two experiments. First, we com-
pared the overall performance of the translitera-
tion models. Second, we investigated the effect
of training data size on the performance of each
transliteration model.
The evaluation was done for word accuracy
in top-1 (ACC), Chinese pronunciation accuracy
(CPA) and a mean reciprocal rank (MRR) met-
ric (Kantor and Voorhees, 2000; Li et al, 2009;
Chang et al, 2009). ACC measures how many
correct transliterations appeared in the top-1 re-
sult of each system. CPA measures the Chinese
pronunciation accuracy in the top-1 of the n-best
Chinese pronunciation. We used CPA for com-
6Available at http://www.speech.cs.cmu.edu/
cgi-bin/cmudict
paring the performance between systems based on
Chinese phonemes. MRR, mean reciprocal ranks
of n-best results of each system over the test en-
tries, is an evaluation measure for n-best translit-
erations. If a transliteration generated by a system
matches a reference transliteration7 at the rth posi-
tion of the n-best results, its reciprocal rank equals
1/r; otherwise its reciprocal rank equals 0, where
1 ? r ? n. We produced 10-best Chinese translit-
erations for each English word in our experiments.
4.1 Comparison of the Overall Performance
Table 6 represents the overall performance of one
system in a previous work (Li et al, 2004) and
eighteen systems based on the transliteration mod-
els defined in this paper. ACC, MRR, and CPA
represent the evaluation results for each model
trained by our training data. To test transliteration
models without the errors introduced by incorrect
Chinese phonemes, we carried out the experiments
with the correct Chinese pronunciation (or the
correct Chinese phoneme sequence) in Chinese
phoneme-to-grapheme conversion. In the exper-
iment, we put the correct Chinese pronunciation
into the top-1 of the n-best Chinese pronunciation
with the highest probability, say P (C
P
|E
G
)=1;
thus CPA was assumed to be 100%. The ACC
of the transliteration models under this condition
is denoted as ACC? in Table 6. TRAIN represents
the evaluation results of the transliteration mod-
els trained by our training data. To compare Li
et al (2004) and transliteration models defined in
this paper under the same condition, we also car-
ried out experiments with the same training data
in Li et al (2004). Since the training data used
in Li et al (2004) is identical as the union of
our training and development data, we denoted it
as TRAIN+DEV in Table 6. In both TRAIN and
TRAIN+DEV, we used the same parameter setting
that was obtained by using the development data.
LI04 represents a system in Li et al (2004),
and its ACC? in TRAIN+DEV is taken from the
literature. The systems based on the translitera-
tion models defined in our paper are represented
from the second row in Table 6. The phoneme-
based transliteration models in the literature cor-
respond to either M(E
G
, C
P
) (Wan and Verspoor,
1998; Lee and Chang, 2003; Jiang et al, 2007) or
M(E
P
, C
P
) (Meng et al, 2001; Gao et al, 2004;
7In our test set, an English word corresponds to one refer-
ence Chinese transliteration.
663
Table 6: Comparison of the overall performance
Class Model TRAIN TRAIN+DEV
ACC MRR CPA ACC? ACC MRR CPA ACC?
LI04 N/A N/A N/A N/A 70.1 N/A N/A N/A
M(E
G
, JC
P
) 71.9 80.4 72.3 88.2 72.3 80.7 73.1 88.9
M(E
P
, JC
P
) 61.1 70.3 62.4 82.8 61.1 70.6 63.1 83.8
M
J
M(E
GP
, JC
P
) 72.3 80.9 73.2 89.6 73.5 81.5 73.9 90.4
M(E
G
+E
P
, JC
P
, 0.7) 72.8 80.7 73.8 89.7 73.2 81.0 74.7 90.5
M(E
G
+E
GP
, JC
P
, 0.6) 73.5 81.7 74.2 90.6 73.7 81.8 74.8 91.2
M(E
P
+E
GP
, JC
P
, 0.1) 71.6 80.3 73.3 89.8 72.5 80.8 73.8 90.1
M(E
G
, ?) 70.0 78.5 N/A N/A 70.6 79.0 N/A N/A
M(E
P
, ?) 58.5 69.3 N/A N/A 59.4 70.1 N/A N/A
M
I
M(E
GP
, ?) 71.2 79.9 N/A N/A 72.3 80.7 N/A N/A
M(E
G
+E
P
, ?, 0.7) 70.7 79.1 N/A N/A 72.0 80.0 N/A N/A
M(E
G
+E
GP
, ?, 0.4) 72.0 80.3 N/A N/A 72.8 80.9 N/A N/A
M(E
P
+E
GP
, ?, 0.1) 71.0 79.6 N/A N/A 72.0 80.4 N/A N/A
M(E
G
, C
P
) 58.9 70.2 72.3 78.4 59.1 70.4 73.1 78.4
M(E
P
, C
P
) 50.2 62.3 62.4 78.4 50.4 62.6 63.1 78.5
M
S
M(E
GP
, C
P
) 59.1 70.4 73.2 78.4 59.3 70.5 73.9 78.5
M(E
G
+E
P
, C
P
, 0.8) 59.7 71.3 73.8 79.0 60.3 71.7 74.7 79.0
M(E
G
+E
GP
, C
P
, 0.6) 59.8 71.7 74.2 78.9 60.6 72.1 74.8 78.9
M(E
P
+E
GP
, C
P
, 0.1) 58.8 70.4 73.3 78.9 59.4 70.7 73.8 78.8
Virga and Khudanpur, 2003).
A comparison between the basic and hybrid
transliteration models showed that the hybrid
ones usually performed better (the exception was
M(E
P
+E
GP
, y, ?) but the performance still com-
parable to the basic ones in each class). Es-
pecially, the hybrid ones based on the best two
basic transliteration models, M(E
G
+E
GP
, y, ?),
showed the best performance.
A comparison among the M
I
, M
S
, and
M
J
models showed that Chinese phonemes did
contribute to the performance improvement of
English-to-Chinese transliteration when Chinese
phonemes were used together with their corre-
sponding English graphemes and phonemes in
Chinese phoneme-to-grapheme conversion. A
one-tail paired t-test between the M
I
and M
J
models showed that the results of the M
J
mod-
els were always significantly better than those
of the M
I
models if the M
I
and M
J
models
shared the same English-side parameter, x ?
{E
G
, E
P
, E
GP
} (level of significance = 0.001).
In the results obtained by the M
S
and M
J
mod-
els, the figures in CPA are the same when theM
S
and our M
J
models share the same English-side
parameter. Moreover, the difference between the
figures in ACC and CPA can be interpreted as
the error rate of Chinese phoneme-to-grapheme
conversion. Our proposed M
J
models gener-
ated Chinese transliterations with a very low er-
ror rate in Chinese phoneme-to-grapheme conver-
sion, while theM
S
models suffered from a signif-
icant error rate in Chinese phoneme-to-grapheme
conversion. ACC? showed that the M
J
models
still outperformed the M
S
models even without
errors in generating Chinese pronunciation from
the English words. These results indicate that the
joint use of Chinese phonemes and their corre-
sponding English graphemes and phonemes sig-
nificantly improved the performance in Chinese
phoneme-to-grapheme conversion and English-to-
Chinese transliteration.
Table 7 shows the Chinese transliterations gen-
erated by M(E
G
, ?), M(E
GP
, ?), M(E
G
, JC
P
),
and M(E
GP
, JC
P
) where English or Chinese
phonemes contributed to the correct translitera-
tion. In this table, the first column show the
English words and their English phonemes, and
the second and third columns represent the Chi-
nese transliterations and their phonemes. Note
that the Chinese phonemes in the second and third
columns of theM
I
models are not used in translit-
eration. They are shown in the table to indicate
the difference in the Chinese phonemes of Chinese
664
Table 7: Top-1 results of M(E
G
, ?), M(E
GP
, ?),
M(E
G
, JC
P
), and M(E
GP
, JC
P
), where * rep-
resents incorrect transliterations
M(EGP,JCP)M(EG,JCP)MJ models
????*
(LAI YIN HA TE)
????*
(LAI YIN HA TE)
Reinhardt
(R AI N HH AA R T)
??
(AI WEI)
??*
(YI WEI)
Ivy
(AY V IY)
???*
(AI MI LI)
???*
(AI MI LI)
Emily
(EH M IH L IY)
????
LAI YIN HA TE
????
LAI YIN HA TE
Reinhardt
(R AI N HH AA R T)
??
AI WEI
??*
YI WEI
Ivy
(AY V IY)
???
AI MI LI
???
AI MI LI
Emily
(EH M IH L IY)
M(EGP,?)M(EG,?)MI models
transliterations between theM
I
andM
J
models.
For Emily and Reinhardt, the M
J
models gen-
erated correct Chinese transliterations, but theM
I
models did not. Figure 1 shows the probabil-
ity distribution when a transliteration model gen-
erates the first Chinese character in the Chinese
transliteration of Reinhardt with and without Chi-
nese phonemes. Two Chinese characters,  and
, were strong candidates and  is the correct
one in this case. Without Chinese phonemes,
M(E
G
, ?), which is based on P(cg|Reinhardt)
in Figure 1(a) preferring  to , generated the
incorrect transliteration as shown in Table 7. How-
ever, Figure 1(b) shows that  can be selected
if the correct Chinese phoneme sequence ?LAI
YIN ...? is given. Three Chinese phoneme se-
quences starting with ?LAI YIN ...?, ?LAI NA
...?, and ?LAI NEI ...? were generated from Rein-
hardt, where ?LAI YIN ...? was the best Chinese
phoneme sequence based on the probability distri-
bution in Figure 1(c). As a result, M(E
G
, JC
P
),
which jointly used Chinese phonemes with En-
glish graphemes, generated the correct Chinese
transliteration of Reinhardt based on two probabil-
ity distribution in Figures 1(b) and 1(c). In the case
of Ivy, English phonemes contributed to generat-
ing the correct transliteration in the M(E
GP
, ?)
and M(E
GP
, JC
P
) models.
Chinese transliterations sometimes reflect the
English word?s pronunciation as well as the Chi-
nese character?s meaning (Li et al, 2007). Li
0
0.2
0.4
0.6
0.8
P(
?
|Reinhardt) P(
?
|Reinhardt)
(a) Probability distribution when Chi-
nese phonemes are not given
0
0.2
0.4
0.6
0.8
1
?
?
P(cg|Reinhardt, "LAI YIN ..") P(cg|Reinhardt, "LAI NA ..")
P(cg|Reinhardt, "LAI NEI ..")
(b) Probability distribution when Chinese phonemes are
given
0
0.2
0.4
0.6
0.8
1
P("LAI YIN .."|Reinhardt) P(?"LAI YIN .."|Reinhardt)
(c) Probability distribution for Chinese phoneme se-
quence ?LAI YIN ...? and others
Figure 1: Probability distribution for the first Chi-
nese character in the Chinese transliteration of
Reinhardt: M(E
G
, ?) vs. M(E
G
, JC
P
)
et al (2007) defined such a Chinese transliter-
ation as a phonetic-semantic transliteration (se-
mantic transliteration) to distinguish it from a
usual phonetic transliteration. One fact that
affects semantic transliteration is gender asso-
ciation (Li et al, 2007). For example, 
(meaining jasmine) is frequently used in Chi-
nese transliterations of female names but sel-
dom in common person names. Because Emily
is often used in female names, the results ob-
tained by the M(E
G
, JC
P
) and M(E
GP
, JC
P
)
models are acceptable. This indicates that Chi-
nese phonemes coupled with English graphemes
or those coupled with English graphemes and
phonemes could provide evidence required for se-
mantic transliteration as well as phonetic translit-
eration. As a result, M(E
GP
, ?), M(E
G
, JC
P
),
665
and M(E
GP
, JC
P
), which used phonemes cou-
pled with English graphemes, achieved higher per-
formance than M(E
G
, ?), which relied only on
English graphemes.
4.2 Effect of Training Data Size
 80
 70
 60
 50
 40
 30
 20
 80 60 40 20
M
R
R
Training Data Size (%)
M(EG,?)
M(EP,?)
M(EGP,?)
M(EG,CP)
M(EP,CP)
M(EGP,CP)
M(EG,JCP)
M(EP,JCP)
M(EGP,JCP)
(a) Basic transliteration models
 80
 70
 60
 50
 40
 30
 80 60 40 20
M
R
R
Training Data Size (%)
M(EG+EP,?,0.7)
M(EG+EGP,?,0.4)
M(EP+EGP,?,0.1)
M(EG+EP,CP,0.8)
M(EG+EGP,CP,0.6)
M(EP+EGP,CP,0.1)
M(EG+EP,JCP,0.7)
M(EG+EGP,JCP,0.6)
M(EP+EGP,JCP,0.1)
(b) Hybrid transliteration models
Figure 2: Performance of each system with differ-
ent training data size
We investigated the effect of training data size
on the performance of each transliteration model.
We randomly selected training data with ratios
from 10 to 90% and compared the performance
of each system trained by different sizes of train-
ing data. The results for the basic translitera-
tion models in Figure 2(a) can be categorized into
three groups. M(E
GP
, ?) and M(E
GP
, JC
P
)
fall into the best group, where they showed the
best performance regardless of training data size.
M(E
G
, ?) and M(E
G
, JC
P
) belong to the mid-
dle group, where they showed lower performance
than the best group if the training data size is
small, but their performance is comparable to the
best group if the size of the training data is large
enough. The others always showed lower perfor-
mance than both the best and middle groups. Fig-
ure 2(b) shows that hybrid transliteration models,
on average, were less sensitive to the training data
size than the basic ones, because the two differ-
ent basic transliteration models used in the hybrid
ones boosted transliteration performance by com-
plementing each other?s weak points.
5 Conclusion
We proposed a new English-to-Chinese transliter-
ation model based on Chinese phonemes and their
corresponding English graphemes and phonemes.
We defined eighteen English-to-Chinese translit-
eration models including our proposed model and
classified them into three classes based on the role
of Chinese phonemes in the transliteration mod-
els. Experiments showed that Chinese phonemes
in our proposed model can contribute to the
performance improvement in English-to-Chinese
transliteration.
Now we can answer Yes to this paper?s key ques-
tion, ?Can Chinese phonemes improve machine
transliteration?? Actually, this is the second time
the same question has been answered. The pre-
vious answer, which was unfortunately reported
as No by Li et al (2004), has been accepted as
true for the last five years; the research issue has
been considered closed. In this paper, we found
a new answer that contradicts the previous an-
swer. We hope that our answer promotes research
on phoneme-based English-to-Chinese translitera-
tion.
Appendix: Illustration of Basic
Transliteration Models inM
J
andM
S
EG
CPEG EP
EG EP
CG
CP
CP
CG
CG:)JC,?(? PG
:)JC,?(?
PP
:)JC,?(?
PGP
(a) M
J
models
EG
CPEG EP
EG EP
CG
CP
CP
CG
CG
:)C,?(?
PGP
:)C,?(?
PP
:)C,?(?
PG
(b) M
S
models
666
References
Y. Al-Onaizan and Kevin Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proc. of ACL ?02, pages 400?408.
A. L. Berger, S. D. Pietra, and V. J. D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for translit-
eration discovery. In Proceedings of NAACL HLT?
09.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. of IJCNLP 2004, pages
110?119.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In Proc. of IJCAI ?07, pages
1629?1634.
Paul B. Kantor and Ellen M. Voorhees. 2000. The trec-
5 confusion track: Comparing retrieval methods for
scanned text. Information Retrieval, 2:165?176.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2007. Collapsed consonant and vowel models: New
approaches for English-Persian transliteration and
back-transliteration. In Proceedings of ACL ?07,
pages 648?655.
Chun-Jen Lee and Jason S. Chang. 2003. Acqui-
sition of English-Chinese transliterated word pairs
from parallel-aligned texts using a statistical ma-
chine transliteration model. In Proc. of HLT-NAACL
2003 Workshop on Building and Using Parallel
Texts, pages 96?103.
Haizhou Li, Min Zhang, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the 42th Annual Meeting of the As-
sociation of Computational Linguistics, pages 160?
167.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic transliteration of personal
names. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. of
ACL-IJCNLP 2009 Named Entities Workshop.
M.G. Abbas Malik. 2006. Punjabi machine translit-
eration. In Proceedings of the COLING/ACL 2006,
pages 1137?1144.
H.M. Meng, Wai-Kit Lo, Berlin Chen, and K. Tang.
2001. Generating phonetic cognates to handle
named entities in English-Chinese cross-language
spoken document retrieval. In Proc. of Auto-
matic Speech Recognition and Understanding, 2001.
ASRU ?01, pages 311?314.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models. Journal of Artificial Intelligence Re-
search (JAIR), 27:119?151.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximal entropy models. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 1?
10.
Richard Schwartz and Yen-Lu Chow. 1990. The N-
Best algorithm: an efficient procedure for finding
top N sentence hypotheses. In Proc. of ICASSP ?90,
pages 81?84.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of ACL ?07,
pages 944?951.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. of ACL 2003 Workshop on Multi-
lingual and Mixed-language Named Entity Recogni-
tion, pages 57?64.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. of
COLING ?98, pages 1352?1356.
Xinhua News Agency. 1992. Chinese transliteration
of foreign personal names. The Commercial Press.
Binyong Yin and Mary Felley. 1990. Chinese Roman-
ization: Pronunciation and Orthography. Sinolin-
gua.
Su-Youn Yoon, Kyoung-Young Kim, and Richard
Sproat. 2007. Multilingual transliteration using
feature based phonetic method. In Proceedings of
ACL?07, pages 112?119.
667
Hypothesis Selection in Machine Transliteration: A Web Mining Approach
Jong-Hoon Oh and Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai,Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{rovellia,isahara}@nict.go.jp
Abstract
We propose a new method of selecting hy-
potheses for machine transliteration. We
generate a set of Chinese, Japanese, and Ko-
rean transliteration hypotheses for a given
English word. We then use the set of translit-
eration hypotheses as a guide to finding rel-
evant Web pages and mining contextual in-
formation for the transliteration hypotheses
from the Web page. Finally, we use the
mined information for machine-learning al-
gorithms including support vector machines
and maximum entropy model designed to
select the correct transliteration hypothesis.
In our experiments, our proposed method
based on Web mining consistently outper-
formed systems based on simple Web counts
used in previous work, regardless of the lan-
guage.
1 Introduction
Machine transliteration has been a great challenge
for cross-lingual information retrieval and machine
translation systems. Many researchers have devel-
oped machine transliteration systems that accept a
source language term as input and then output its
transliteration in a target language (Al-Onaizan and
Knight, 2002; Goto et al, 2003; Grefenstette et al,
2004; Kang and Kim, 2000; Li et al, 2004; Meng et
al., 2001; Oh and Choi, 2002; Oh et al, 2006; Qu
and Grefenstette, 2004). Some of these have used
the Web to select machine-generated transliteration
hypotheses and have obtained promising results (Al-
Onaizan and Knight, 2002; Grefenstette et al, 2004;
Oh et al, 2006; Qu and Grefenstette, 2004). More
precisely, they used simple Web counts, estimated as
the number of hits (Web pages) retrieved by a Web
search engine.
However, there are several limitations imposed on
the ability of Web counts to select a correct translit-
eration hypothesis. First, the assumption that hit
counts approximate the Web frequency of a given
query usually introduces noise (Lapata and Keller,
2005). Moreover, some Web search engines disre-
gard punctuation and capitalization when matching
search terms (Lapata and Keller, 2005). This can
cause errors if such Web counts are relied on to se-
lect transliteration hypotheses. Second, it is not easy
to consider the contexts of transliteration hypothe-
ses with Web counts because Web counts are esti-
mated based on the number of retrieved Web pages.
However, as our preliminary work showed (Oh et
al., 2006), transliteration or translation pairs often
appear as parenthetical expressions or tend to be in
close proximity in texts; thus context can play an im-
portant role in selecting transliteration hypotheses.
For example, there are several Chinese, Japanese,
and Korean (CJK) transliterations and their counter-
parts in a parenthetical expression, as follows.
1) 
1

2
(Adrienne
1
Clarkson
2
)
2) ?
1
	?

2
(glucose
1
oxidase
2
)
3) 
1
	

2
(diphenol
1
oxidase
2
)
Note that the subscripted numbers in all examples
represent the correspondence between the English
word and its CJK counterpart. These parentheti-
cal expressions are very useful in selecting translit-
233
eration hypotheses because it is apparent that they
are translation pairs or transliteration pairs. How-
ever, we cannot fully use such information with Web
counts.
To address these problems, we propose a new
method of selecting transliteration hypotheses. We
were interested in how to mine information relevant
to the selection of hypotheses and how to select cor-
rect transliteration hypotheses using the mined in-
formation. To do this, we generated a set of CJK
transliteration hypotheses for a given English word.
We then used the set of transliteration hypotheses
as a guide to finding relevant Web page and min-
ing contextual information for the transliteration hy-
potheses from the Web page. Finally, we used
the mined information for machine-learning algo-
rithms including support vector machines (SVMs)
and maximum entropy model designed to select the
correct transliteration hypothesis.
This paper is organized as follows. Section 2 de-
scribes previous work based on simple Web counts.
Section 3 describes a way of generating transliter-
ation hypotheses. Sections 4 and 5 introduce our
methods of Web mining and selecting transliteration
hypotheses. Sections 6 and 7 deal with our exper-
iments and the discussion. Conclusions are drawn
and future work is discussed in Section 8.
2 Related work
Web counts have been used for selecting translit-
eration hypotheses in several previous work (Al-
Onaizan and Knight, 2002; Grefenstette et al, 2004;
Oh et al, 2006; Qu and Grefenstette, 2004). Be-
cause the Web counts are estimated as the number of
hits by a Web search engine, they greatly depend on
queries sent to a search engine. Previous work has
used three types of queries?monolingual queries
(MQs) (Al-Onaizan and Knight, 2002; Grefen-
stette et al, 2004; Oh et al, 2006), bilingual
simple queries (BSQs) (Oh et al, 2006; Qu and
Grefenstette, 2004), and bilingual bigram queries
(BBQs) (Oh et al, 2006). If we let S be a source
language term and H = {h
1
, ? ? ? , hr} be a set of
machine-generated transliteration hypotheses of S,
the three types of queries can be defined as
MQ: hi (e.g., ,?, and	).
BSQ: s and hi without quotations (e.g., Clinton 
 , Clinton ?, and Clinton 
	).
BBQ: Quoted bigrams composed of S and hi (e.g.,
?Clinton ?, ?Clinton ??, and
?Clinton	?).
MQ is not able to determine whether hi is a counter-
part of S, but whether hi is a frequently used target
term in target-language texts. BSQ retrieves Web
pages if S and hi are present in the same document
but it does not take the distance between S and hi
into consideration. BBQ retrieves Web pages where
?S hi? or ?hi S? are present as a bigram. The rel-
ative order of Web counts over H makes it possible
to select transliteration hypotheses in the previous
work.
3 Generating Transliteration Hypotheses
Let S be an English word, P be a pronuncia-
tion of S, and T be a target language translitera-
tion corresponding to S. We implement English-
to-CJK transliteration systems based on three dif-
ferent transliteration models ? a grapheme-based
model (S ? T ), a phoneme-based model (S ? P
and P ? T ), and a correspondence-based model
(S ? P and (S, P ) ? T ) ? as described in our
preliminary work (Oh et al, 2006). P and T are seg-
mented into a series of sub-strings, each of which
corresponds to a source grapheme. We can thus
write S = s
1
, ? ? ? , sn = sn
1
, P = p
1
, ? ? ? , pn = pn
1
,
and T = t
1
, ? ? ? , tn = tn
1
, where si, pi, and ti rep-
resent the ith English grapheme, English phonemes
corresponding to si, and target language graphemes
corresponding to si, respectively. Given S, our
transliteration systems generate a sequence of ti cor-
responding to either si (in Eq. (1)) or pi (in Eq. (2))
or both of them (in Eq. (3)).
PrG(T |S) = Pr(tn
1
|sn
1
) (1)
PrP (T |S) = Pr(pn
1
|sn
1
)? Pr(tn
1
|pn
1
) (2)
PrC(T |S) = Pr(pn
1
|sn
1
)? Pr(tn
1
|sn
1
, pn
1
) (3)
The maximum entropy model was used to estimate
probabilities in Eqs. (1)?(3) (Oh et al, 2006). We
produced the n-best transliteration hypotheses using
a stack decoder (Schwartz and Chow, 1990). We
234
then created a set of transliteration hypotheses com-
prising the n-best transliteration hypotheses.
4 Web Mining
Let S be an English word and H = {h
1
, ? ? ? , hr} be
its machine-generated set of transliteration hypothe-
ses. We use S and H to generate queries sent to a
search engine1 to retrieve the top-100 snippets. A
correct transliteration and its counterpart tend to be
in close proximity on CJK Web pages. Our goal in
Web mining was to find such Web pages and mine
information that would help to select transliteration
hypotheses from these pages.
To find these Web pages, we used three kinds of
queries, Q
1
=(S and hi), Q2=S, and Q3=hi, where
Q
1
is the same as BSQ?s query and Q
3
is the same
as MQ?s. The three queries usually result in different
sets of Web pages. We categorize the retrieved Web
pages by Q
1
, Q
2
, and Q
3
into W
1
, W
2
, and W
3
. We
extract three kinds of features from Wl as follows,
where l = 1, 2, 3.
? Freq(hi,Wl): the number of occurrences of hi
in Wl
? DFreqk(hi,Wl): Co-occurrence of S and hi
with distance dk ? D in the same snippet of
Wl.
? PFreqk(hi,Wl): Co-occurrence of S and hi
as parenthetical expressions with distance dk ?
D in the same snippet of Wl. Parenthetical ex-
pressions are detected when either S or hi is in
parentheses.
We define D = {d
1
, d
2
, d
3
} with three ranges of
distances between S and hi, where d1(d < 5),
d
2
(5 ? d < 10), and d
3
(10 ? d ? 15). We counted
distance d with the total number of characters (or
words)2 between S and hi. Here, we can take the
contexts of transliteration hypotheses into account
using DFreq and PFreq; while Freq is counted
regardless of the contexts of the transliteration hy-
potheses.
Figure 1 shows examples of how to calculate
Freq, DFreqk, and PFreqk, where S = Clinton,
1We used Google (http://www.google.com)
2Depending on whether the languages had spacing units,
words (for English and Korean) or characters (for Chinese and
Japanese) were chosen to calculate d.
????????
1
(Bill Clinton1)????????????
????????????????????????????
??(My Life)????
2
?????????????????
???????????
3
(Hillary Rodham Clinton
2
)??1997
????? ...
1
(Bill Clinton1)
(My Life)
2
3
(Hillary Rodham Clinton
2
) 1997
...
W1: Q1=(Clinton ???)
::???
4
?Clinton
3
????????
1
??Kerry?::
?
2
??John Kerry???????????????????
??????????
5
?Clinton
4
????????????
?????????????????????Bush??"??
?"???? ???
6
?Clinton
5
???
3
??Kerry? ...
::
4
Clinton
3 1
Kerry ::
2
John Kerry
5
Clinton
4
Bush "
"
6
Clinton
5 3
Kerry ...
Snippet1
Snippet2
Figure 1: Web corpora collected by Clinton and 

Snippet
1

1

2

3
Clinton
1
1 41 68
Clinton
2
72 29 2
Snippet
2

4

5

6
Clinton
3
0 36 81
Clinton
4
40 0 37
Clinton
5
85 41 0
Snippet
2

1

2

3
Clinton
3
6 9 85
Clinton
4
32 29 42
Clinton
5
77 74 1
Table 1: Distance between Clinton and Chinese
transliteration hypotheses in Fig. 1
hi= in W1 collected by Q1=(Clinton 
). The subscripted numbers of Clinton and 
 were used to indicate how many times they oc-
curred in W
1
. In Fig. 1,  occurs six times
thus Freq(hi,W1) = 6. Table 1 lists the dis-
tance between Clinton andwithin each snip-
pet of W
1
. We can obtain DFreq
1
(hi,W1) =
5. PFreq
1
(hi,Wl) is calculated by detecting
parenthetical expressions between S and hi when
DFreq
1
(hi,Wl) is counted. Because all S in
W
1
(Clinton
1
to Clinton
5
) are in parentheses,
PFreq
1
(hi,W1) is the same as DFreq1(hi,W1).
We ignore Freq, DFreqk, and PFreqk when hi
is a substring of other transliteration hypotheses be-
cause hi usually has a higher Freq, DFreqk, and
PFreqk than hj if hi is a substring of hj . Let a
235
set of transliteration hypotheses for S = Clinton
be H= {h
1
= , h
2
= }. Here, h
2
is a
substring of h
1
. In Fig. 1, h
2
appears six times as
a substring of h
1
and three times independently in
Snippet
2
. Moreover, independently used h
2
(
1
,

2
, and 
3
) and S (Clinton
3
and Clinton
5
) are
sufficiently close to count DFreqk and PFreqk.
Therefore, the Freq, DFreqk, and PFreqk of h1
will be lower than those of h
2
if we do not take
the substring relation between h
1
and h
2
into ac-
count. Considering the substring relation, we ob-
tain Freq(h
2
,W
1
) = 3, DFreq
1
(h
2
,W
1
) = 1,
DFreq
2
(h
2
,W
1
) = 2, PFreq
1
(h
2
,W
1
) = 1, and
PFreq
2
(h
2
,W
1
) = 2.
5 Hypothesis Selection
We select transliteration hypotheses by ranking
them. A set of transliteration hypotheses, H =
{h
1
, h
2
, ? ? ? , hr}, is ranked to enable a correct hy-
pothesis to be identified. We devise a rank function,
g(hi) in Eq. (4), that ranks a correct transliteration
hypothesis higher and the others lower.
g(hi) : H ? {R : R is ordering of hi ? H} (4)
Let xi ? X be a feature vector of hi ? H, yi =
{+1,?1} be the training label for xi, and T D =
{td
1
=< x
1
, y
1
>, ? ? ? , tdz =< xz, yz >} be the
training data for g(hi). We prepare the training data
for g(hi) as follows.
1. Given each English word S in the training-set,
generate transliteration hypotheses H.
2. Given hi ? H, assign yi by looking for S and
hi in the training-set ? yi = +1 if hi is a cor-
rect transliteration hypothesis corresponding to
S, otherwise yi = ?1.
3. For each pair (S, hi), generate its feature vector
xi.
4. Construct a training data set, T D:
? T D = T D
+
?
T D
?
? T D
+
 tdi where yi = +1
? T D
?
 tdj where yj = ?1
We used two machine-learning algorithms, sup-
port vector machines (SVMs)3 and maximum en-
tropy model4 for our implementation of g(hi). The
SVMs assign a value to each transliteration hypoth-
esis (hi) using
gSVM (hi) = w ? xi + b (5)
where w denotes a weight vector. Here, we use the
predicted value of gSVM (hi) rather than the pre-
dicted class of hi given by SVMs because our rank-
ing function, as represented by Eq. (4), determines
the relative ordering between hi and hj in H. A
ranking function based on the maximum entropy
model assigns a probability to hi using
gMEM (hi) = Pr(yi = +1|xi) (6)
We can finally obtain a ranked list for the given H?
the higher the g(hi) value, the better the hi.
5.1 Features
We represent the feature vector, xi, with two types
of features. The first is the confidence scores of hi
given by Eqs. (1)?(3) and the second is Web-based
features ? Freq, DFreqk, and PFreqk. To nor-
malize Freq, DFreqk, and PFreqk, we use their
relative frequency over H as in Eqs. (7)?(9), where
k = 1, 2, 3 and l = 1, 2, 3.
RF (hi,Wl) =
Freq(h
i
,W
l
)
?
h
j
?H
Freq(h
j
,W
l
)
(7)
RDFk(hi,Wl) =
DFreq
k
(h
i
,W
l
)
?
h
j
?H
DFreq
k
(h
j
,W
l
)
(8)
RPFk(hi,Wl) =
PFreq
k
(h
i
,W
l
)
?
h
j
?H
PFreq
k
(h
j
,W
l
)
(9)
Figure 2 shows how to construct feature vector
xi from a given English word, Rachel, and its Chi-
nese hypotheses, H, generated from our translitera-
tion systems. We can obtain r Chinese translitera-
tion hypotheses and classify them into positive and
negative samples according to yi. Note that yi = +1
if and only if hi is registered as a counterpart of S
in the training data. The bottom of Fig. 2 shows our
feature set representing xi. There are three confi-
dence scores in P (hi|S) according to transliteration
models and the three Web-based features Web(W
1
),
Web(W
2
), and Web(W
3
).
3SVM light (Joachims, 2002)
4
?Maximum Entropy Modeling Toolkit? (Zhang, 2004)
236
??????????????????
hr?h5h4h3h2h1H
-1-1-1-1-1+1
yr?y5y4y3y2y1Y
Rachel
RF(hi,W1)
RDF1(hi,W1) 
RDF2(hi,W1)
RDF3(hi,W1)
RPF1(hi,W1) 
RPF2(hi,W1)
RPF3(hi,W1)
Web (W1)
RF(W3)
RDF1(hi,W3) 
RDF2(hi,W3)
RDF3(hi,W3)
RPF1(hi,W3) 
RPF2(hi,W3)
RPF3(hi,W3)
RF(hi,W2)
RDF1(hi,W2) 
RDF2(hi,W2)
RDF3(hi,W2)
RPF1(hi,W2) 
RPF2(hi,W2)
RPF3(hi,W2)
PrG(hi|S) 
PrP(hi|S)
PrC(hi|S)
Web (W3)Web (W2)Pr(hi|S)xi
td1 ? TD+ td2, td3,  td4, td5,?,tdr? TD-
xr?x5x4x3x2x1X
Figure 2: Feature vectors
6 Experiments
We evaluated the effectiveness of our system in se-
lecting CJK transliteration hypotheses. We used the
same test set used in Li et al (2004) (ECSet) for Chi-
nese transliterations (Xinhua News Agency, 1992)
and those used in Oh et al (2006) for Japanese
and Korean transliterations ? EJSET and EK-
SET (Breen, 2003; Nam, 1997). We divided the test
ECSet EJSet EKSet
Training Set 31,299 8,335 5,124
Development Set 3,478 1,041 1,024
Blind Test Set 2,896 1,041 1,024
Total 37,694 10,417 7,172
Table 2: Test data sets
data into training, development, and blind test sets
as in Table 2. The training set was used to train our
three transliteration models to generate the n-best
transliteration hypotheses5. The development set
was used to train hypothesis selection based on sup-
port vector machines and maximum entropy model.
We used the blind test set for evaluation. The eval-
uation was done in terms of word accuracy (WA).
WA is the proportion of correct transliterations in
the best hypothesis by a system to correct transliter-
ations in the blind test set.
System ECSet EJSet EKSet
KANG00 N/A N/A 54.1
GOTO03 N/A 54.3 N/A
LI04 70.1 N/A N/A
GM 69.0 61.6 59.0
PM 56.6 54.4 56.7
CM 69.9 65.0 65.1
Table 3: WA of individual transliteration systems
(%)
6.1 Results: Web counts vs. Web mining
We compared our transliteration system with three
previous ones, all of which were based on a
grapheme-based model (Goto et al, 2003; Kang and
Kim, 2000; Li et al, 2004). LI046 is an English-
to-Chinese transliteration system, which simultane-
ously takes English and Chinese contexts into con-
sideration (Li et al, 2004). KANG00 is an English-
to-Korean transliteration system and GOTO03 is an
English-to-Japanese one ? they segment a chunk of
English graphemes and identify the most relevant
sequence of target graphemes corresponding to the
chunk (Goto et al, 2003; Kang and Kim, 2000) 7.
GM, PM, and CM, which are respectively based
on Eqs. (1)?(3), are the transliteration systems we
used for generating transliteration hypotheses. Our
transliteration systems showed comparable or better
performance than the previous ones regardless of the
language.
We compared simple Web counts with our Web
mining for hypothesis selection. We used the same
set of transliteration hypotheses H then compared
their performance in hypothesis selection with two
measures, relative frequency and g(hi). Tables 4 and
5 list the results. Here, ?Upper bound? is a system
that always selects the correct transliteration hypoth-
esis if there is a correct one inH. ?Upper bound? can
5We set n = 10 for the n-best. Thus, n ? r ? 3? n where
H = {h
1
, h
2
, ? ? ? , h
r
}
6The WA of LI04 was taken from the literature, where the
training data were the same as the union of our training set and
the development set while the test data were the same as in our
test set. In other words, LI04 used more training data than ours
did. With the same setting as LI04, our GM, PM, and CM pro-
duced respective WAs of 70.0, 57.7, and 71.7.
7We implemented KANG00 (Kang and Kim, 2000) and
GOTO03 (Goto et al, 2003), and tested them with the same
data as ours.
237
System ECSet EJSet EKSet
WC
MQ 16.1 40.4 34.7
BSQ 45.8 74.0 72.4
BBQ 34.9 78.1 79.3
WM
RF (W
1
) 62.9 78.4 77.1
RDF (W
1
) 70.8 80.4 80.2
RPF (W
1
) 73.5 79.7 79.4
RF (W
2
) 63.5 76.2 74.8
RDF (W
2
) 67.1 79.2 78.9
RPF (W
2
) 69.6 79.1 78.4
RF (W
3
) 37.9 53.9 55.8
RDF (W
3
) 76.4 69.0 70.2
RPF (W
3
) 76.8 68.3 68.7
Upper bound 94.6 93.5 93.2
Table 4: Web counts (WC) vs. Web mining (WM):
hypothesis selection by relative frequency (%)
System ECSet EJSet EKSet
WC MEMWC 74.7 86.1 85.6
SVMWC 74.8 86.9 86.5
WM MEMWM 82.0 88.2 85.8
SVMWM 83.9 88.5 86.7
Upper bound 94.6 93.5 93.2
Table 5: Web counts (WC) vs. Web mining (WM):
hypothesis selection by g(hi) (%)
also be regarded as the ?Coverage? of H generated
by our transliteration systems. MQ, BSQ, and BBQ
in the upper section of Table 4, represent hypothesis
selection systems based on the relative frequency of
Web counts over H, the same measure used in Oh et
al. (2006):
WebCountsx(hi)
?
h
j
?H
WebCountsx(hj)
(10)
where WebCountsx(hi) is a function returning
Web counts retrieved by x ? {MQ,BSQ,BBQ}
RF (Wl), RDF (Wl), and RPF (Wl) in Table 4 rep-
resent hypothesis selection systems with their rela-
tive frequency, where RDF (Wl) and RPF (Wl) use
?
3
k=1 RDFk(hj ,Wl) and
?
3
k=1 RPFk(hj ,Wl),
respectively. The comparison in Table 4 shows
which is best for selecting transliteration hy-
potheses when each relative frequency is used
alone. Table 5 compares Web counts with fea-
tures mined from the Web when they are used
as features in g(hi) ? {Pr(hi|S), Web(Wl)} in
MEMWM and SVMWM (our proposed method),
while {Pr(hi|S), WebCountsx(hi)} in MEMWC
and SVMWC . Here, Web(Wl) is a set of mined
features from Wl as described in Fig .2.
????????(a Man To Call My Own) ??
???????????- ????????(a Man To Call 
My Own), ????ranchhouse???????????????
??????????????? ????????????
?????????????...
(a Man To Call My Own) 
- (a Man To Call 
My Own), ranchhouse
...
??????(4/03)????
???????,?????????,???????,???
??????????????????,????????,?
???? ... ???????(Academy)??????????,
???????????????...
(4/03)
, , ,
, ,
... (Academy) ,
...
Snippet1 retrieved by BSQ: Aman ????
Snippet2 retrieved by MQ: ???? (meaning Agard)
?????|Cliff De Young| ??| ??| ??| EO????
????? | The Secret Life of Zoey (TV) ?????2002 ???
????????????? , ????? , ????? , ??
???? , Avery Raskin. ??????Larry Carter. ???4.92?
|Cliff De Young| | | | EO
| The Secret Life of Zoey (TV) 2002 
, , , 
, Avery Raskin. Larry Carter. 4.92
UNESCO. General Conference; 32nd; Election of member
????????????????. ?. 1987--1991. ???????
????????????. ????. (1976). 1987--1991. ????
??????????. 2001--2005. ????. 1993--1997....
UNESCO. General Conference; 32nd; Election of e ber
? ? ? . . 1987--1991. ?
? ? . . (1976). 1987--1991. ?
? . 2001--2005. . 1993--1997....
Snippet3 retrieved by MQ: ?????? (meaning Rawcliffe)
Snippet4 retrieved by MQ: ?????? (meaning Aldersey)
Figure 3: Snippets causing errors in Web counts
The results in the tables show that our systems
consistently outperformed systems based on Web
counts, especially for Chinese. This was due to the
difference between languages. Japanese and Chi-
nese do not use spaces between words. However,
Japanese is written using three different alphabet
systems, called Hiragana, Katakana, and Kanji, that
assist word segmentation. Moreover, words written
in Katakana are usually Japanese transliterations of
foreign words. This makes it possible for a Web
search engine to effectively retrieve Web pages con-
taining given Japanese transliterations. Like En-
glish, Korean has spaces between words (or word
phrases). As the spaces in the languages reduce am-
biguity in segmenting words, a Web search engine
can correctly identify Web pages containing given
Korean transliterations. In contrast, there is a se-
vere word-segmentation problem with Chinese that
causes Chinese Web search engines to incorrectly
retrieve Web pages, as shown in Fig. 3. For example,
Snippet
1
is not related to ?Aman? but to ?a man?.
238
Snippet
2
contains a super-string of a given Chinese
query, which corresponds to ?Academy? rather than
to ?Agard?, which is the English counterpart of the
Chinese transliteration. Moreover, Web search
engines ignore punctuation marks in Chinese. In
Snippet
3
and Snippet
4
, ?,? and ?? in the under-
lined terms are disregarded, so the Web counts based
on such Web documents are noisy. Thus, noise in
the Chinese Web counts causes systems based on
Web counts to produce more errors than our sys-
tems do. Our proposed method can filter out such
noise because our systems take punctuation marks
and the contexts of transliterations in Web mining
into consideration. Thus, our systems based on fea-
tures mined from the Web were able to achieve the
best performance. The results revealed that our sys-
tems based on the Web-mining technique can effec-
tively be used to select transliteration hypotheses re-
gardless of the language.
6.2 Contribution of Web corpora
ECSet EJSet EKSet
SVM MEM SVM MEM SVM MEM
Base 73.3 73.8 67.0 66.1 66.0 66.4
W
1
81.7 79.7 87.6 87.3 86.1 85.1
W
2
80.8 79.5 86.9 86.0 83.8 82.1
W
3
77.2 76.7 83.0 82.8 79.8 77.3
W
1+2
83.8 82.3 88.5 87.9 86.3 85.9
W
1+3
81.9 80.1 87.6 87.8 86.1 84.7
W
2+3
81.4 79.8 88.0 87.7 85.1 84.3
W
All
83.9 82.0 88.5 88.2 86.7 85.8
Table 6: Contribution of Web corpora
In Web mining, we used W
1
, W
2
, and W
3
, col-
lected by respective queries Q
1
=(S and hi), Q2=S,
and Q
3
=hi. To investigate their contribution, we
tested our proposed method with different combina-
tions of Web corpora. ?Base? is a baseline system
that only uses Pr(hi|S) as features but does not use
features mined from the Web. We added features
mined from different combinations of Web corpora
to ?Base? from W
1
to WAll.
In Table 6, we can see that W
1
, a set of Web pages
retrieved by Q
1
, tends to give more relevant infor-
mation than W
2
and W
3
, because Q
1
can search
more Web pages containing both S and hi in the top-
100 snippets if S and hi are a correct transliteration
pair. Therefore, its performance tends to be superior
in Table 6 if W
1
is used, especially for ECSet. How-
ever, as W
1
occasionally retrieves few snippets, it is
not able to provide sufficient information. Using W
2
or W
3
, we can address the problem. Thus, combina-
tions of W
1
and others (W
1+2
, W
1+3
, WAll) pro-
vided better WA than W
1
.
7 Discussion
Several Web mining techniques for translitera-
tion lexicons have been developed in the last few
years (Jiang et al, 2007; Oh and Isahara, 2006).
The main difference between ours and those previ-
ous ones is in the way a set of transliteration hy-
potheses (or candidates) is created.
Jiang et al (2007) generated Chinese transliter-
ations for given English words and searched the
Web using the transliterations. They generated only
the best transliteration hypothesis and focused on
Web mining to select transliteration lexicons rather
than selecting transliteration hypotheses. The best
transliteration hypothesis was used to guide Web
searches. Then, transliteration candidates were
mined from the retrieved Web pages. Therefore,
their performance greatly depended on their abil-
ity to mine transliteration candidates from the Web.
However, this system might create errors if it can-
not find a correct transliteration candidate from the
retrieved Web pages. Because of this, their sys-
tem?s coverage and WA were relatively poor than
ours 8. However, our transliteration process was able
to generate a set of transliteration hypotheses with
excellent coverage and could thus achieve superior
WA.
Oh and Isahara (2006) searched the Web using
given source words and mined the retrieved Web
pages to find target-language transliteration candi-
dates. They extracted all possible sequences of
target-language characters from the retrieved Web
snippets as transliteration candidates for which the
beginnings and endings of the given source word
8Since both Jiang et al?s (2007) and ours used Chinese
transliterations of personal names as a test set, we can indirectly
compare our coverage and WA with theirs (Jiang et al, 2007).
Jiang et al (2007) achieved a 74.5% coverage of transliteration
candidates and 47.5% WA, while ours achieved a 94.6% cov-
erage of transliteration hypotheses and 82.0?83.9% WA
239
and the extracted transliteration candidate were pho-
netically similar. However, while this can exponen-
tially increase the number of transliteration candi-
dates, ours used the n-best transliteration hypothe-
ses but still achieved excellent coverage.
8 Conclusion
We have described a novel approach to selecting
transliteration hypotheses based on Web mining. We
first generated CJK transliteration hypotheses for a
given English word and retrieved Web pages us-
ing the transliteration hypotheses and the given En-
glish word as queries for a Web search engine. We
then mined features from the retrieved Web pages
and trained machine-learning algorithms using the
mined features. Finally, we selected transliteration
hypotheses by ranking them. Our experiments re-
vealed that our proposed method worked well re-
gardless of the language, while simple Web counts
were not effective, especially for Chinese.
Because our method was very effective in select-
ing transliteration pairs, we expect that it will also
be useful for selecting translation pairs. We plan to
extend our method in future work to selecting trans-
lation pairs.
References
Y. Al-Onaizan and Kevin Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proc. of ACL ?02, pages 400?408.
J. Breen. 2003. EDICT Japanese/English dictionary .le.
The Electronic Dictionary Research and Development
Group, Monash University. http://www.csse.
monash.edu.au/
?
jwb/edict.html.
I. Goto, N. Kato, N. Uratani, and T. Ehara. 2003.
Transliteration considering context information based
on the maximum entropy method. In Proc. of MT-
Summit IX, pages 125?132.
Gregory Grefenstette, Yan Qu, and David A. Evans.
2004. Mining the Web to create a language model
for mapping between English names and phrases and
Japanese. In Proc. of Web Intelligence, pages 110?
116.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with Web min-
ing and transliteration. In Proc. of IJCAI, pages 1629?
1634.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
I. H. Kang and G. C. Kim. 2000. English-to-Korean
transliteration using multiple unbounded overlapping
phoneme chunks. In Proc. of COLING ?00, pages
418?424.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech Lang. Process., 2(1):3.
H. Li, M. Zhang, and J. Su. 2004. A joint source-channel
model for machine transliteration. In Proc. of ACL
?04, pages 160?167.
H.M. Meng, Wai-Kit Lo, Berlin Chen, and K. Tang.
2001. Generating phonetic cognates to handle named
entities in English-Chinese cross-language spoken
document retrieval. In Proc. of Automatic Speech
Recognition and Understanding, 2001. ASRU ?01,
pages 311?314.
Y. S. Nam. 1997. Foreign dictionary. Sung An Dang.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation and
contextual rules. In Proc. of COLING2002, pages
758?764.
Jong-Hoon Oh and Hitoshi Isahara. 2006. Mining the
Web for transliteration lexicons: Joint-validation ap-
proach. In Web Intelligence, pages 254?261.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models. Journal of Artificial Intelligence Re-
search (JAIR), 27:119?151.
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in
Latin script via language identification and corpus val-
idation. In Proc. of ACL ?04, pages 183?190.
Richard Schwartz and Yen-Lu Chow. 1990. The N-best
algorithm: An efficient and exact procedure for finding
the N most likely sentence hypothesis. In Procs. of
ICASSP ?90, pages 81?84.
Xinhua News Agency. 1992. Chinese transliteration of
foreign personal names. The Commercial Press.
L. Zhang. 2004. Maximum entropy model-
ing toolkit for python and C++. http:
//homepages.inf.ed.ac.uk/s0450736/
software/maxent/manual.pdf.
240
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 432?440,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Bilingual Co-Training for Monolingual Hyponymy-Relation Acquisition
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{rovellia,uchimoto,torisawa}@nict.go.jp
Abstract
This paper proposes a novel framework
called bilingual co-training for a large-
scale, accurate acquisition method for
monolingual semantic knowledge. In
this framework, we combine the indepen-
dent processes of monolingual semantic-
knowledge acquisition for two languages
using bilingual resources to boost perfor-
mance. We apply this framework to large-
scale hyponymy-relation acquisition from
Wikipedia. Experimental results show
that our approach improved the F-measure
by 3.6?10.3%. We also show that bilin-
gual co-training enables us to build classi-
fiers for two languages in tandem with the
same combined amount of data as required
for training a single classifier in isolation
while achieving superior performance.
1 Motivation
Acquiring and accumulating semantic knowledge
are crucial steps for developing high-level NLP
applications such as question answering, although
it remains difficult to acquire a large amount of
highly accurate semantic knowledge. This pa-
per proposes a novel framework for a large-scale,
accurate acquisition method for monolingual se-
mantic knowledge, especially for semantic rela-
tions between nominals such as hyponymy and
meronymy. We call the framework bilingual co-
training.
The acquisition of semantic relations between
nominals can be seen as a classification task of se-
mantic relations ? to determine whether two nom-
inals hold a particular semantic relation (Girju et
al., 2007). Supervised learning methods, which
have often been applied to this classification task,
have shown promising results. In those methods,
however, a large amount of training data is usually
required to obtain high performance, and the high
costs of preparing training data have always been
a bottleneck.
Our research on bilingual co-training sprang
from a very simple idea: perhaps training data in a
language can be enlarged without much cost if we
translate training data in another language and add
the translation to the training data in the original
language. We also noticed that it may be possi-
ble to further enlarge the training data by trans-
lating the reliable part of the classification results
in another language. Since the learning settings
(feature sets, feature values, training data, corpora,
and so on) are usually different in two languages,
the reliable part in one language may be over-
lapped by an unreliable part in another language.
Adding the translated part of the classification re-
sults to the training data will improve the classifi-
cation results in the unreliable part. This process
can also be repeated by swapping the languages,
as illustrated in Figure 1. Actually, this is nothing
other than a bilingual version of co-training (Blum
and Mitchell, 1998).
Language 1 Language 2
Iteration
Manually Prepared 
Training Data
for Language 1
Classifier Classifier
Training Training
Enlarged 
Training Data
for Language 1
Enlarged 
Training Data
for Language 2
Manually Prepared 
Training Data
for Language 2
ClassifierClassifier
Further Enlarged 
Training Data
for Language 1
Further Enlarged 
Training Data
for Language 2
Translate
reliable parts of
classification 
results
Training
Training Training
Training
?..
?..
Translate
reliable parts of
classification 
results
Figure 1: Concept of bilingual co-training
Let us show an example in our current task:
hyponymy-relation acquisition from Wikipedia.
Our original approach for this task was super-
432
vised learning based on the approach proposed by
Sumida et al (2008), which was only applied for
Japanese and achieved around 80% in F-measure.
In their approach, a common substring in a hyper-
nym and a hyponym is assumed to be one strong
clue for recognizing that the two words constitute
a hyponymy relation. For example, recognizing a
proper hyponymy relation between two Japanese
words, (kouso meaning enzyme) and
 (kasuibunkaikouso meaning hydrolase), is
relatively easy because they share a common suf-
fix: kouso. On the other hand, judging whether
their English translations (enzyme and hydrolase)
have a hyponymy relation is probably more dif-
ficult since they do not share any substrings. A
classifier for Japanese will regard the hyponymy
relation as valid with high confidence, while a
classifier for English may not be so positive. In
this case, we can compensate for the weak part of
the English classifier by adding the English trans-
lation of the Japanese hyponymy relation, which
was recognized with high confidence, to the En-
glish training data.
In addition, if we repeat this process by swap-
ping English and Japanese, further improvement
may be possible. Furthermore, the reliable parts
that are automatically produced by a classifier can
be larger than manually tailored training data. If
this is the case, the effect of adding the transla-
tion to the training data can be quite large, and the
same level of effect may not be achievable by a
reasonable amount of labor for preparing the train-
ing data. This is the whole idea.
Through a series of experiments, this paper
shows that the above idea is valid at least for one
task: large-scale monolingual hyponymy-relation
acquisition from English and Japanese Wikipedia.
Experimental results showed that our method
based on bilingual co-training improved the per-
formance of monolingual hyponymy-relation ac-
quisition about 3.6?10.3% in the F-measure.
Bilingual co-training also enables us to build clas-
sifiers for two languages in tandem with the same
combined amount of data as would be required
for training a single classifier in isolation while
achieving superior performance.
People probably expect that a key factor in the
success of this bilingual co-training is how to
translate the training data. We actually did transla-
tion by a simple look-up procedure in the existing
translation dictionaries without any machine trans-
lation systems or disambiguation processes. De-
spite this simple approach, we obtained consistent
improvement in our task using various translation
dictionaries.
This paper is organized as follows. Section 2
presents bilingual co-training, and Section 3 pre-
cisely describes our system. Section 4 describes
our experiments and presents results. Section 5
discusses related work. Conclusions are drawn
and future work is mentioned in Section 6.
2 Bilingual Co-Training
Let S and T be two different languages, and let
CL be a set of class labels to be obtained as a re-
sult of learning/classification. To simplify the dis-
cussion, we assume that a class label is binary; i.e.,
the classification results are ?yes? or ?no.? Thus,
CL = {yes, no}. Also, we denote the set of all
nonnegative real numbers by R+.
Assume X = XS ? XT is a set of instances in
languages S and T to be classified. In the con-
text of a hyponymy-relation acquisition task, the
instances are pairs of nominals. Then we assume
that classifier c assigns class label cl in CL and
confidence value r for assigning the label, i.e.,
c(x) = (x, cl, r), where x ? X , cl ? CL, and
r ? R+. Note that we used support vector ma-
chines (SVMs) in our experiments and (the abso-
lute value of) the distance between a sample and
the hyperplane determined by the SVMs was used
as confidence value r. The training data are de-
noted by L ? X?CL, and we denote the learning
by function LEARN ; if classifier c is trained by
training data L, then c = LEARN(L). Particu-
larly, we denote the training sets for S and T that
are manually prepared by LS and LT , respectively.
Also, bilingual instance dictionary DBI is defined
as the translation pairs of instances in XS and XT .
Thus, DBI = {(s, t)} ? XS ? XT . In the case
of hyponymy-relation acquisition in English and
Japanese, (s, t) ? DBI could be (s=(enzyme, hy-
drolase), t=( (meaning enzyme),
 (meaning hydrolase))).
Our bilingual co-training is given in Figure 2. In
the initial stage, c0S and c0T are learned with manu-
ally labeled instances LS and LT (lines 2?5). Then
ciS and ciT are applied to classify instances in XS
and XT (lines 6?7). Denote CRiS as a set of the
classification results of ciS on instances XS that is
not in LiS and is registered in DBI . Lines 10?18
describe a way of selecting from CRiS newly la-
433
1: i = 0
2: L0S = LS ; L
0
T = LT
3: repeat
4: ciS := LEARN(L
i
S)
5: ciT := LEARN(L
i
T )
6: CRiS := {c
i
S(xS)|xS ? XS ,
?cl (xS , cl) /? LiS , ?xT (xS , xT ) ? DBI}
7: CRiT := {c
i
T (xT )|xT ? XT ,
?cl (xT , cl) /? LiT , ?xS (xS , xT ) ? DBI}
8: L(i+1)S := L
i
S
9: L(i+1)T := L
i
T
10: for each (xS , clS , rS) ? TopN(CRiS) do
11: for each xT such that (xS , xT ) ? DBI
and (xT , clT , rT ) ? CRiT do
12: if rS > ? then
13: if rT < ? or clS = clT then
14: L(i+1)T := L
(i+1)
T ? {(xT , clS)}
15: end if
16: end if
17: end for
18: end for
19: for each (xT , clT , rT ) ? TopN(CRiT ) do
20: for each xS such that (xS , xT ) ? DBI
and (xS , clS , rS) ? CRiS do
21: if rT > ? then
22: if rS < ? or clS = clT then
23: L(i+1)S := L
(i+1)
S ? {(xS , clT )}
24: end if
25: end if
26: end for
27: end for
28: i = i + 1
29: until a fixed number of iterations is reached
Figure 2: Pseudo-code of bilingual co-training
beled instances to be added to a new training set
in T . TopN(CRiS) is a set of ciS(x), whose rS
is top-N highest in CRiS . (In our experiments,
N = 900.) During the selection, ciS acts as a
teacher and ciT as a student. The teacher instructs
his student in the class label of xT , which is actu-
ally a translation of xS by bilingual instance dic-
tionary DBI , through clS only if he can do it with
a certain level of confidence, say rS > ?, and
if one of two other condition meets (rT < ? or
clS = clT ). clS = clT is a condition to avoid
problems, especially when the student also has a
certain level of confidence in his opinion on a class
label but disagrees with the teacher: rT > ? and
clS 6= clT . In that case, the teacher does nothing
and ignores the instance. Condition rT < ? en-
ables the teacher to instruct his student in the class
label of xT in spite of their disagreement in a class
label. If every condition is satisfied, (xT , clS) is
added to existing labeled instances L(i+1)T . The
roles are reversed in lines 19?27 so that ciT be-
comes a teacher and ciS a student.
Similar to co-training (Blum and Mitchell,
1998), one classifier seeks another?s opinion to se-
lect new labeled instances. One main difference
between co-training and bilingual co-training is
the space of instances: co-training is based on dif-
ferent features of the same instances, and bilin-
gual co-training is based on different spaces of in-
stances divided by languages. Since some of the
instances in different spaces are connected by a
bilingual instance dictionary, they seem to be in
the same space. Another big difference lies in
the role of the two classifiers. The two classifiers
in co-training work on the same task, but those
in bilingual co-training do the same type of task
rather than the same task.
3 Acquisition of Hyponymy Relations
from Wikipedia
Our system, which acquires hyponymy relations
from Wikipedia based on bilingual co-training,
is described in Figure 3. The following three
main parts are described in this section: candidate
extraction, hyponymy-relation classification, and
bilingual instance dictionary construction.
Classifier in E Classifier in J
Labeled 
instances
Labeled 
instances 
Wikipedia
Articles in E
Wikipedia
Articles in J
Candidates
in J
Candidates
in E
Acquisition of 
translation dictionary
Bilingual Co-Training
Unlabeled 
instances in J
Unlabeled 
instances in E
Bilingual instance dictionary
Newly labeled 
instances for E 
Newly labeled 
instances for J
Translation 
dictionary 
Hyponymy-relation 
candidate extraction
Hyponymy-relation 
candidate extraction
Figure 3: System architecture
3.1 Candidate Extraction
We follow Sumida et al (2008) to extract
hyponymy-relation candidates from English and
Japanese Wikipedia. A layout structure is chosen
434
(a) Layout structure
of article TIGER
Range
Siberian tiger
Bengal tiger
Subspecies
Taxonomy
Tiger
Malayan tiger
(b) Tree structure of
Figure 4(a)
Figure 4: Wikipedia article and its layout structure
as a source of hyponymy relations because it can
provide a huge amount of them (Sumida et al,
2008; Sumida and Torisawa, 2008)1, and recog-
nition of the layout structure is easy regardless of
languages. Every English and Japanese Wikipedia
article was transformed into a tree structure like
Figure 4, where layout items title, (sub)section
headings, and list items in an article were used
as nodes in a tree structure. Sumida et al (2008)
found that some pairs consisting of a node and one
of its descendants constituted a proper hyponymy
relation (e.g., (TIGER, SIBERIAN TIGER)), and
this could be a knowledge source of hyponymy
relation acquisition. A hyponymy-relation candi-
date is then extracted from the tree structure by re-
garding a node as a hypernym candidate and all
its subordinate nodes as hyponym candidates of
the hypernym candidate (e.g., (TIGER, TAXON-
OMY) and (TIGER, SIBERIAN TIGER) from Fig-
ure 4). 39 M English hyponymy-relation candi-
dates and 10 M Japanese ones were extracted from
Wikipedia. These candidates are classified into
proper hyponymy relations and others by using the
classifiers described below.
3.2 Hyponymy-Relation Classification
We use SVMs (Vapnik, 1995) as classifiers for
the classification of the hyponymy relations on the
hyponymy-relation candidates. Let hyper be a hy-
pernym candidate, hypo be a hyper?s hyponym
candidate, and (hyper, hypo) be a hyponymy-
relation candidate. The lexical, structure-based,
and infobox-based features of (hyper, hypo) in Ta-
ble 1 are used for building English and Japanese
classifiers. Note that SF
3
?SF
5
and IF were not
1Sumida et al (2008) reported that they obtained 171 K,
420 K, and 1.48 M hyponymy relations from a definition sen-
tence, a category system, and a layout structure in Japanese
Wikipedia, respectively.
used in Sumida et al (2008) but LF
1
?LF
5
and
SF
1
?SF
2
are the same as their feature set.
Let us provide an overview of the feature
sets used in Sumida et al (2008). See Sum-
ida et al (2008) for more details. Lexical fea-
tures LF
1
?LF
5
are used to recognize the lexi-
cal evidence encoded in hyper and hypo for hy-
ponymy relations. For example, (hyper,hypo) is
often a proper hyponymy relation if hyper and
hypo share the same head morpheme or word.
In LF
1
and LF
2
, such information is provided
along with the words/morphemes and the parts of
speech of hyper and hypo, which can be multi-
word/morpheme nouns. TagChunk (Daume? III et
al., 2005) for English and MeCab (MeCab, 2008)
for Japanese were used to provide the lexical fea-
tures. Several simple lexical patterns2 were also
applied to hyponymy-relation candidates. For ex-
ample, ?List of artists? is converted into ?artists?
by lexical pattern ?list of X.? Hyponymy-relation
candidates whose hypernym candidate matches
such a lexical pattern are likely to be valid (e.g.,
(List of artists, Leonardo da Vinci)). We use LF
4
for dealing with these cases. If a typical or fre-
quently used section heading in a Wikipedia arti-
cle, such as ?History? or ?References,? is used as
a hyponym candidate in a hyponymy-relation can-
didate, the hyponymy-relation candidate is usually
not a hyponymy relation. LF
5
is used to recognize
these hyponymy-relation candidates.
Structure-based features are related to the
tree structure of Wikipedia articles from which
hyponymy-relation candidate (hyper,hypo) is ex-
tracted. SF
1
provides the distance between hyper
and hypo in the tree structure. SF
2
represents the
type of layout items from which hyper and hypo
are originated. These are the feature sets used in
Sumida et al (2008).
We also added some new items to the above
feature sets. SF
3
represents the types of tree
nodes including root, leaf, and others. For exam-
ple, (hyper,hypo) is seldom a hyponymy relation
if hyper is from a root node (or title) and hypo
is from a hyper?s child node (or section head-
ings). SF
4
and SF
5
represent the structural con-
texts of hyper and hypo in a tree structure. They
can provide evidence related to similar hyponymy-
relation candidates in the structural contexts.
An infobox-based feature, IF , is based on a
2We used the same Japanese lexical patterns in Sumida et
al. (2008) to build English lexical patterns with them.
435
Type Description Example
LF
1
Morphemes/words hyper: tiger?, hypo: Siberian, hypo: tiger?
LF
2
POS of morphemes/words hyper: NN?, hypo: NP, hypo: NN?
LF
3
hyper and hypo, themselves hyper: Tiger, hypo: Siberian tiger
LF
4
Used lexical patterns hyper: ?List of X?, hypo: ?Notable X?
LF
5
Typical section headings hyper: History, hypo: Reference
SF
1
Distance between hyper and hypo 3
SF
2
Type of layout items hyper: title, hypo: bulleted list
SF
3
Type of tree nodes hyper: root node, hypo: leaf node
SF
4
LF
1
and LF
3
of hypo?s parent node LF
3
:Subspecies
SF
5
LF
1
and LF
3
of hyper?s child node LF
3
: Taxonomy
IF Semantic properties of hyper and hypo hyper: (taxobox,species), hypo: (taxobox,name)
Table 1: Feature type and its value. ? in LF
1
and LF
2
represent the head morpheme/word and its POS.
Except those in LF
4
and LF
5
, examples are derived from (TIGER, SIBERIAN TIGER) in Figure 4.
Wikipedia infobox, a special kind of template, that
describes a tabular summary of an article subject
expressed by attribute-value pairs. An attribute
type coupled with the infobox name to which it
belongs provides the semantic properties of its
value that enable us to easily understand what
the attribute value means (Auer and Lehmann,
2007; Wu and Weld, 2007). For example, in-
fobox template City Japan in Wikipedia article
Kyoto contains several attribute-value pairs such
as ?Mayor=Daisaku Kadokawa? as attribute=its
value. What Daisaku Kadokawa, the attribute
value of mayor in the example, represents is hard
to understand alone if we lack knowledge, but
its attribute type, mayor, gives a clue?Daisaku
Kadokawa is a mayor related to Kyoto. These
semantic properties enable us to discover seman-
tic evidence for hyponymy relations. We ex-
tract triples (infobox name, attribute type, attribute
value) from the Wikipedia infoboxes and encode
such information related to hyper and hypo in our
feature set IF .3
3.3 Bilingual Instance Dictionary
Construction
Multilingual versions of Wikipedia articles are
connected by cross-language links and usually
have titles that are bilinguals of each other (Erd-
mann et al, 2008). English and Japanese articles
connected by a cross-language link are extracted
from Wikipedia, and their titles are regarded as
translation pairs4. The translation pairs between
3We obtained 1.6 M object-attribute-value triples in
Japanese and 5.9 M in English.
4197 K translation pairs were extracted.
English and Japanese terms are used for building
bilingual instance dictionary DBI for hyponymy-
relation acquisition, where DBI is composed of
translation pairs between English and Japanese
hyponymy-relation candidates5.
4 Experiments
We used the MAY 2008 version of English
Wikipedia and the JUNE 2008 version of
Japanese Wikipedia for our experiments. 24,000
hyponymy-relation candidates, randomly selected
in both languages, were manually checked to build
training, development, and test sets6. Around
8,000 hyponymy relations were found in the man-
ually checked data for both languages7. 20,000 of
the manually checked data were used as a train-
ing set for training the initial classifier. The rest
were equally divided into development and test
sets. The development set was used to select the
optimal parameters in bilingual co-training and the
test set was used to evaluate our system.
We used TinySVM (TinySVM, 2002) with a
polynomial kernel of degree 2 as a classifier. The
maximum iteration number in the bilingual co-
training was set as 100. Two parameters, ? and
TopN , were selected through experiments on the
development set. ? = 1 and TopN=900 showed
5We also used redirection links in English and Japanese
Wikipedia for recognizing the variations of terms when we
built a bilingual instance dictionary with Wikipedia cross-
language links.
6It took about two or three months to check them in each
language.
7Regarding a hyponymy relation as a positive sample and
the others as a negative sample for training SVMs, ?positive
sample:negative sample? was about 8,000:16,000=1:2
436
the best performance and were used as the optimal
parameter in the following experiments.
We conducted three experiments to show ef-
fects of bilingual co-training, training data size,
and bilingual instance dictionaries. In the first two
experiments, we experimented with a bilingual in-
stance dictionary derived from Wikipedia cross-
language links. Comparison among systems based
on three different bilingual instance dictionaries is
shown in the third experiment.
Precision (P ), recall (R), and F
1
-measure (F
1
),
as in Eq (1), were used as the evaluation measures,
where Rel represents a set of manually checked
hyponymy relations and HRbyS represents a set
of hyponymy-relation candidates classified as hy-
ponymy relations by the system:
P = |Rel ? HRbyS|/|HRbyS| (1)
R = |Rel ? HRbyS|/|Rel|
F
1
= 2 ? (P ? R)/(P + R)
4.1 Effect of Bilingual Co-Training
ENGLISH JAPANESE
P R F
1
P R F
1
SYT 78.5 63.8 70.4 75.0 77.4 76.1
INIT 77.9 67.4 72.2 74.5 78.5 76.6
TRAN 76.8 70.3 73.4 76.7 79.3 78.0
BICO 78.0 83.7 80.7 78.3 85.2 81.6
Table 2: Performance of different systems (%)
Table 2 shows the comparison results of the four
systems. SYT represents the Sumida et al (2008)
system that we implemented and tested with the
same data as ours. INIT is a system based on ini-
tial classifier c0 in bilingual co-training. We trans-
lated training data in one language by using our
bilingual instance dictionary and added the trans-
lation to the existing training data in the other
language like bilingual co-training did. The size
of the English and Japanese training data reached
20,729 and 20,486. We trained initial classifier c0
with the new training data. TRAN is a system
based on the classifier. BICO is a system based
on bilingual co-training.
For Japanese, SYT showed worse performance
than that reported in Sumida et al (2008), proba-
bly due to the difference in training data size (ours
is 20,000 and Sumida et al (2008) was 29,900).
The size of the test data was also different ? ours
is 2,000 and Sumida et al (2008) was 1,000.
Comparison between INIT and SYT shows the
effect of SF
3
?SF
5
and IF , newly introduced
feature types, in hyponymy-relation classification.
INIT consistently outperformed SYT, although the
difference was merely around 0.5?1.8% in F
1
.
BICO showed significant performance im-
provement (around 3.6?10.3% in F
1
) over SYT,
INIT, and TRAN regardless of the language. Com-
parison between TRAN and BICO showed that
bilingual co-training is useful for enlarging the
training data and that the performance gain by
bilingual co-training cannot be achieved by sim-
ply translating the existing training data.
 81
 79
 77
 75
 73
 60 55 50 45 40 35 30 25 20
F 1
Training Data (103)
English
Japanese
Figure 5: F
1
curves based on the increase of train-
ing data size during bilingual co-training
Figure 5 shows F
1
curves based on the size
of the training data including those manually tai-
lored and automatically obtained through bilin-
gual co-training. The curve starts from 20,000 and
ends around 55,000 in Japanese and 62,000 in En-
glish. As the training data size increases, the F
1
curves tend to go upward in both languages. This
indicates that the two classifiers cooperate well
to boost their performance through bilingual co-
training.
We recognized 5.4 M English and 2.41 M
Japanese hyponymy relations from the classifi-
cation results of BICO on all hyponymy-relation
candidates in both languages.
4.2 Effect of Training Data Size
We performed two tests to investigate the effect of
the training data size on bilingual co-training. The
first test posed the following question: ?If we build
2n training samples by hand and the building cost
is the same in both languages, which is better from
the monolingual aspects: 2n monolingual training
samples or n bilingual training samples?? Table 3
and Figure 6 show the results.
437
In INIT-E and INIT-J, a classifier in each lan-
guage, which was trained with 2n monolingual
training samples, did not learn through bilingual
co-training. In BICO-E and BICO-J, bilingual co-
training was applied to the initial classifiers trained
with n training samples in both languages. As
shown in Table 3, BICO, with half the size of the
training samples used in INIT, always performed
better than INIT in both languages. This indicates
that bilingual co-training enables us to build clas-
sifiers for two languages in tandem with the same
combined amount of data as required for training
a single classifier in isolation while achieving su-
perior performance.
 81
 79
 77
 75
 73
 71
 69
 67
 65
 20000 15000 10000 7500 5000 2500
F 1
Training Data Size
INIT-E
INIT-J
BICO-E
BICO-J
Figure 6: F
1
based on training data size:
with/without bilingual co-training
n
2n n
INIT-E INIT-J BICO-E BICO-J
2500 67.3 72.3 70.5 73.0
5000 69.2 74.3 74.6 76.9
10000 72.2 76.6 76.9 78.6
Table 3: F
1
based on training data size:
with/without bilingual co-training (%)
The second test asked: ?Can we always im-
prove performance through bilingual co-training
with one strong and one weak classifier?? If the
answer is yes, then we can apply our framework
to acquisition of hyponymy-relations in other lan-
guages, i.e., German and French, without much
effort for preparing a large amount of training
data, because our strong classifier in English or
Japanese can boost the performance of a weak
classifier in other languages.
To answer the question, we tested the perfor-
mance of classifiers by using all training data
(20,000) for a strong classifier and by changing the
training data size of the other from 1,000 to 15,000
({1,000, 5,000, 10,000, 15,000}) for a weak clas-
sifier.
INIT-E BICO-E INIT-J BICO-J
1,000 72.2 79.6 64.0 72.7
5,000 72.2 79.6 73.1 75.3
10,000 72.2 79.8 74.3 79.0
15,000 72.2 80.4 77.0 80.1
Table 4: F
1
based on training data size: when En-
glish classifier is strong one
INIT-E BICO-E INIT-J BICO-J
1,000 60.3 69.7 76.6 79.3
5,000 67.3 74.6 76.6 79.6
10,000 69.2 77.7 76.6 80.1
15,000 71.0 79.3 76.6 80.6
Table 5: F
1
based on training data size: when
Japanese classifier is strong one
Tables 4 and 5 show the results, where ?INIT?
represents a system based on the initial classifier
in each language and ?BICO? represents a sys-
tem based on bilingual co-training. The results
were encouraging because the classifiers showed
better performance than their initial ones in every
setting. In other words, a strong classifier always
taught a weak classifier well, and the strong one
also got help from the weak one, regardless of the
size of the training data with which the weaker one
learned. The test showed that bilingual co-training
can work well if we have one strong classifier.
4.3 Effect of Bilingual Instance Dictionaries
We tested our method with different bilingual in-
stance dictionaries to investigate their effect. We
built bilingual instance dictionaries based on dif-
ferent translation dictionaries whose translation
entries came from different domains (i.e., gen-
eral domain, technical domain, and Wikipedia)
and had a different degree of translation ambigu-
ity. In Table 6, D1 and D2 correspond to sys-
tems based on a bilingual instance dictionary de-
rived from two handcrafted translation dictionar-
ies, EDICT (Breen, 2008) (a general-domain dic-
tionary) and ?The Japan Science and Technology
Agency Dictionary,? (a translation dictionary for
technical terms) respectively. D3, which is the
same as BICO in Table 2, is based on a bilingual
438
instance dictionary derived from Wikipedia. EN-
TRY represents the number of translation dictio-
nary entries used for building a bilingual instance
dictionary. E2J (or J2E) represents the average
translation ambiguities of English (or Japanese)
terms in the entries. To show the effect of these
translation ambiguities, we used each dictionary
under two different conditions, ?=5 and ALL. ?=5
represents the condition where only translation en-
tries with less than five translation ambiguities are
used; ALL represents no restriction on translation
ambiguities.
DIC F
1
DIC STATISTICS
TYPE E J ENTRY E2J J2E
D1 ?=5 76.5 78.4 588K 1.80 1.77
D1 ALL 75.0 77.2 990K 7.17 2.52
D2 ?=5 76.9 78.5 667K 1.89 1.55
D2 ALL 77.0 77.9 750K 3.05 1.71
D3 ?=5 80.7 81.6 197K 1.03 1.02
D3 ALL 80.7 81.6 197K 1.03 1.02
Table 6: Effect of different bilingual instance dic-
tionaries
The results showed that D3 was the best and
that the performances of the others were sim-
ilar to each other. The differences in the F
1
scores between ?=5 and ALL were relatively small
within the same system triggered by translation
ambiguities. The performance gap between D3
and the other systems might explain the fact that
both hyponymy-relation candidates and the trans-
lation dictionary used in D3 were extracted from
the same dataset (i.e., Wikipedia), and thus the
bilingual instance dictionary built with the trans-
lation dictionary in D3 had better coverage of
the Wikipedia entries consisting of hyponymy-
relation candidates than the other bilingual in-
stance dictionaries. Although D1 and D2 showed
lower performance than D3, the experimental re-
sults showed that bilingual co-training was always
effective no matter which dictionary was used
(Note that F
1
of INIT in Table 2 was 72.2 in En-
glish and 76.6 in Japanese.)
5 Related Work
Li and Li (2002) proposed bilingual bootstrapping
for word translation disambiguation. Similar to
bilingual co-training, classifiers for two languages
cooperated in learning with bilingual resources in
bilingual bootstrapping. However, the two clas-
sifiers in bilingual bootstrapping were for a bilin-
gual task but did different tasks from the monolin-
gual viewpoint. A classifier in each language is for
word sense disambiguation, where a class label (or
word sense) is different based on the languages.
On the contrary, classifiers in bilingual co-training
cooperate in doing the same type of tasks.
Bilingual resources have been used for mono-
lingual tasks including verb classification and
noun phrase semantic interpolation (Merlo et al,
2002; Girju, 2006). However, unlike ours, their fo-
cus was limited to bilingual features for one mono-
lingual classifier based on supervised learning.
Recently, there has been increased interest in se-
mantic relation acquisition from corpora. Some
regarded Wikipedia as the corpora and applied
hand-crafted or machine-learned rules to acquire
semantic relations (Herbelot and Copestake, 2006;
Kazama and Torisawa, 2007; Ruiz-casado et al,
2005; Nastase and Strube, 2008; Sumida et al,
2008; Suchanek et al, 2007). Several researchers
who participated in SemEval-07 (Girju et al,
2007) proposed methods for the classification of
semantic relations between simple nominals in
English sentences. However, the previous work
seldom considered the bilingual aspect of seman-
tic relations in the acquisition of monolingual se-
mantic relations.
6 Conclusion
We proposed a bilingual co-training approach and
applied it to hyponymy-relation acquisition from
Wikipedia. Experiments showed that bilingual
co-training is effective for improving the perfor-
mance of classifiers in both languages. We fur-
ther showed that bilingual co-training enables us
to build classifiers for two languages in tandem,
outperforming classifiers trained individually for
each language while requiring no more training
data in total than a single classifier trained in iso-
lation.
We showed that bilingual co-training is also
helpful for boosting the performance of a weak
classifier in one language with the help of a strong
classifier in the other language without lowering
the performance of either classifier. This indicates
that the framework can reduce the cost of prepar-
ing training data in new languages with the help of
our English and Japanese strong classifiers. Our
future work focuses on this issue.
439
References
So?ren Auer and Jens Lehmann. 2007. What have
Innsbruck and Leipzig in common? Extracting se-
mantics from wiki content. In Proc. of the 4th
European Semantic Web Conference (ESWC 2007),
pages 503?517. Springer.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT?
98: Proceedings of the eleventh annual conference
on Computational learning theory, pages 92?100.
Jim Breen. 2008. EDICT Japanese/English dictionary
file, The Electronic Dictionary Research and Devel-
opment Group, Monash University.
Hal Daume? III, John Langford, and Daniel Marcu.
2005. Search-based structured prediction as classi-
fication. In Proc. of NIPS Workshop on Advances in
Structured Learning for Text and Speech Processing,
Whistler, Canada.
Maike Erdmann, Kotaro Nakayama, Takahiro Hara,
and Shojiro Nishio. 2008. A bilingual dictionary
extracted from the Wikipedia link structure. In Proc.
of DASFAA, pages 686?689.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 13?18.
Roxana Girju. 2006. Out-of-context noun phrase se-
mantic interpretation with cross-linguistic evidence.
In CIKM ?06: Proceedings of the 15th ACM inter-
national conference on Information and knowledge
management, pages 268?276.
Aurelie Herbelot and Ann Copestake. 2006. Acquir-
ing ontological relationships from Wikipedia using
RMRS. In Proc. of the ISWC 2006 Workshop on
Web Content Mining with Human Language Tech-
nologies.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proc. of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 698?707.
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proc. of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 343?351.
MeCab. 2008. MeCab: Yet another part-of-speech
and morphological analyzer. http://mecab.
sourceforge.net/.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 207?214.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia categories for knowledge acquisition. In
Proc. of AAAI 08, pages 1219?1224.
Maria Ruiz-casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic extraction of semantic
relationships for Wordnet by means of pattern learn-
ing from Wikipedia. In Proc. of NLDB, pages 67?
79. Springer Verlag.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proc. of the 16th international conference
on World Wide Web, pages 697?706.
Asuka Sumida and Kentaro Torisawa. 2008. Hack-
ing Wikipedia for hyponymy relation acquisition. In
Proc. of the Third International Joint Conference
on Natural Language Processing (IJCNLP), pages
883?888, January.
Asuka Sumida, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in Wikipedia. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation.
TinySVM. 2002. http://chasen.org/
?
taku/
software/TinySVM.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In CIKM ?07: Proceedings
of the sixteenth ACM conference on Conference on
information and knowledge management, pages 41?
50.
440
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 36?39,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Machine Transliteration using Target-Language Grapheme and
Phoneme: Multi-engine Transliteration Approach
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{rovellia,uchimoto,torisawa}@nict.go.jp
Abstract
This paper describes our approach to
?NEWS 2009 Machine Transliteration
Shared Task.? We built multiple translit-
eration engines based on different combi-
nations of two transliteration models and
three machine learning algorithms. Then,
the outputs from these transliteration en-
gines were combined using re-ranking
functions. Our method was applied to all
language pairs in ?NEWS 2009 Machine
Transliteration Shared Task.? The official
results of our standard runs were ranked
the best for four language pairs and the
second best for three language pairs.
1 Outline
This paper describes our approach to ?NEWS
2009 Machine Transliteration Shared Task.?
Our approach was based on two transliteration
models ? TM-G (Transliteration model based
on target-language Graphemes) and TM-GP
(Transliteration model based on target-language
Graphemes and Phonemes). The difference
between the two models lies in whether or
not a machine transliteration process depends
on target-language phonemes. TM-G directly
converts source-language graphemes into target-
language graphemes, while TM-GP first trans-
forms source language graphemes into target-
language phonemes and then target-language
phonemes coupled with their corresponding
source-language graphemes are converted into
target-language graphemes. We used three dif-
ferent machine learning algorithms (conditional
random fields (CRFs), margin infused relaxed al-
gorithm (MIRA), and maximum entropy model
(MEM)) (Berger et al, 1996; Crammer and
Singer, 2003; Lafferty et al, 2001) for build-
ing multiple machine transliteration engines. We
attempted to improve the transliteration quality
by combining the outputs of different machine
transliteration engines operating on the same in-
put. Our approach was applied to all language
pairs in ?NEWS 2009 Machine Transliteration
Shared Task.? The official results of our approach
were ranked as the best for four language pairs and
the second best for three language pairs (Li et al,
2009a).
2 Transliteration Model
Let S be a source-language word and T be a target-
language transliteration of S. T is represented in
two ways ? TG, a sequence of target-language
graphemes, and TP , a sequence of target-language
phonemes. Here, a target-language grapheme is
defined as a target-language character. We regard
consonant and vowel parts in the romanized form
of a target language grapheme as a target-language
phoneme. Then TM-G and TM-GP are formu-
lated as Eq (1) and (2), respectively.
PTM?G(T |S) = P (TG|S) (1)
PTM?GP (T |S) (2)
=
?
?T
P
P (TP |S) ? P (TG|TP , S)
Ja
Ch
En
?:I?:I?:B?:I?:I?:B?:B TG
NUDNILKETP
?:B ?:I?:B ?:B ?:I?:B ?:B TG
NOTNIRKUTP
notnilCS   
Clinton
KELINDUN KURINTON
??? ?????
ClintonClinton
??? ?????
Clinton
TM-G TM-GP
Figure 1: Illustration of the two transliteration
models
36
Figure 1 illustrates the two transliteration mod-
els with examples, Clinton and its Chinese
and Japanese transliterations. Target language
graphemes are represented in terms of the BIO no-
tation. This makes it easier to represent many-
to-one correspondence between target language
phoneme and grapheme.
3 Machine Learning Algorithms
A machine transliteration problem can be con-
verted into a sequential labeling problem, where
each source-language grapheme is tagged with its
corresponding target-language grapheme. This
section briefly describes the machine learning al-
gorithms used for building multiple transliteration
engines.
3.1 Maximum Entropy Model
Machine transliteration based on the maximum
entropy model was described in detail in Oh et al
(2006) along with comprehensive evaluation of its
performance. We used the same way as that pro-
posed by Oh et al (2006), thus its full description
is not presented here.
3.2 Conditional Random Fields (CRFs)
CRFs, a statistical sequence modeling framework,
was first introduced by Lafferty et al (2001).
CRFs has been used for sequential labeling prob-
lems such as text chunking and named entity
recognition (McCallum and Li, 2003). CRF++1
was used in our experiment.
3.3 Margin Infused Relaxed Algorithm
The Margin Infused Relaxed Algorithm (MIRA)
has been introduced by Crammer and Singer
(2003) for large-margin multi-class classification.
Kruengkrai et al (2008) proposed a discriminative
model for joint Chinese segmentation and POS
tagging, where MIRA was used as their machine
learning algorithm. We used the same model for
our machine transliteration, exactly joint syllabi-
cation2 and transliteration.
3.4 Features
We used the following features within the ?3 con-
text window3 for the above mentioned three ma-
1Available at http://crfpp.sourceforge.net/
2A syllable in English is defined as a sequence of English
grapheme corresponding to one target-language grapheme.
3The unit of context window is source-language
grapheme or syllable.
chine learning algorithms.
? Left-three and right-three source-language
graphemes (or syllables)
? Left-three and right-three target-language
phonemes
? Target-language graphemes assigned to the
previous three source-language graphemes
(or syllables)
4 Multi-engine Transliteration
4.1 Individual Transliteration Engine
The main aim of the multi-engine transliteration
approach is to combine the outputs of multiple en-
gines so that the final output is better in quality
than the output of each individual engine. We
designed four transliteration engines using dif-
ferent combinations of source-language translit-
eration units, transliteration models, and machine
learning algorithms as listed in Table 1. We named
four transliteration engines as CRF-G, MEM-G,
MEM-GP, and MIRA-G. Here, the prefixes rep-
resent applied machine learning algorithms (max-
imum entropy model (MEM), CRFs, and MIRA),
while G and GP in the suffix represent the translit-
eration models, TM-G and TM-GP, respectively.
Each individual engine produces 30-best translit-
erations for a given source-language word.
Source-language transliteration unit
Grapheme Syllable
TM-G ME-G, CRF-G MIRA-G
TM-GP ME-GP N/A
Table 1: Design strategy for multiple translitera-
tion engines
4.2 Combining Methodology
We combined the outputs of multiple translitera-
tion engines by means of a re-ranking function,
g(x). Let X be a set of transliterations gener-
ated by multiple transliteration engines for source-
language word s and ref be a reference translit-
eration of s. A re-ranking function is defined as
Eq. (3), where it ranks ref in X higher and the
others lower (Oh and Isahara, 2007).
g(x) : X ? {r : r is ordering of x ? X} (3)
We designed two types of re-ranking functions by
using the rank of each individual engine and ma-
chine learning algorithm.
37
4.2.1 Re-ranking Based on the Rank of
Individual Engines
Two re-ranking functions based on the rank of
each individual engine, grank and gFscore(x),
are used for combining the outputs of multiple
transliteration engines. Let X be a set of outputs
of N transliteration engines for the same input.
grank(x) re-ranks x ? X in the manner shown
in Eq. (4), where Ranki(x) is the position of x in
the n-best list generated by the ith transliteration
engine. grank(x) can be interpreted as the average
rank of x over outputs of each individual engine.
If x is not in the n-best list of the ith transliteration
engine, 1Rank
i
(x) = 0.
grank(x) =
1
N
N
?
i=1
1
Ranki(x)
(4)
gFscore(x) is based on grank(x) and the F-
score measure, which is one of the evaluation met-
rics in the ?NEWS 2009 Machine Transliteration
Shared Task? (Li et al, 2009b). We considered
the top three outputs of each individual engine
as reference transliterations and defined them as
virtual reference transliterations. We calculated
the F-score measure between the virtual reference
transliteration and each output of multiple translit-
eration engines. gFscore(x) is defined by Eq. (5),
where VRef is a set of virtual reference transliter-
ations, and Fscore(vr, x) is a function that restores
the F-score measure between vr and x.
gFscore(x) = grank(x) ?MF (x) (5)
MF (x) =
1
|V Ref |
?
vr?V Ref
Fscore(vr, x)
Since the F-score measure is calculated in terms of
string similarity, x gets a high score from gMF (x)
when it is orthographically similar to virtual refer-
ence transliterations.
4.2.2 Re-ranking based on Machine Learning
Algorithm
We used the maximum entropy model for learn-
ing re-ranking function gME(x). Let ref be a ref-
erence transliteration of source-language word s,
feature(x) be a feature vector of x ? X , and
y ? {ref, wrong} be the training label for x.
gME(x) assigns a probability to x ? X as shown
in Eq. (6).
gME(x) = P (ref |feature(x)) (6)
A feature vector of x is composed of
? ?grank(x), gFscore(x), 1Rank
i
(x) , P (T |S)?
where 1Rank
i
(x) and P (T |S) of each individual en-
gine are used as a feature.
We estimated P (ref |feature(x)) by using the
development data.
5 Our Results
5.1 Individual Engine
CRF-G MEM-G MEM-GP MIRA-G
EnCh 0.628 0.686 0.715 0.684
EnHi 0.455 0.469 0.469 0.412
EnJa 0.514 0.517 0.519 0.490
EnKa 0.386 0.380 0.380 0.338
EnKo 0.460 0.438 0.447 0.367
EnRu 0.600 0.561 0.566 0.568
EnTa 0.453 0.459 0.459 0.412
JnJk N/A 0.532 N/A 0.571
Table 2: ACC of individual engines on the test data
Table 2 presents ACC4 of individual translit-
eration engines, which was applied to all lan-
guage pairs in ?NEWS 2009 Machine Translit-
eration Shared Task? (Li et al, 2004; Kumaran
and Kellner, 2007; The CJK Dictionary Institute,
2009). CRF-G was the best transliteration engine
in EnKa, EnKo, and EnRu. Owing to the high
training costs of CRFs, we trained CRF-G in EnCh
with a very small number of iterations5. Hence,
the performance of CRF-G was poorer than that
of the other engines in EnCh. MEM-GP was the
best transliteration engine in EnCh, EnHi, EnJa,
and EnTa. These results indicate that joint use
of source language graphemes and target language
phonemes were very useful for improving perfor-
mance. MIRA-G was sensitive to the training data
size, because it was based on joint syllabication
and transliteration. Therefore, the performance of
MIRA-G was relatively better in EnCh and EnJa,
whose training data size is bigger than other lan-
guage pairs. CRF-G could not be applied to JnJk,
mainly due to too long training time. Further,
MEM-GP could not be applied to JnJk, because
transliteration in JnJk can be regarded as conver-
sion of target language phonemes to target lan-
guage graphemes. MEM-G and MIRA-G were
4Word accuracy in Top-1 (Li et al, 2009b)
5We applied over 100 iterations to other language pairs
but only 30 iterations to EnCh.
38
applied to JnJk and MIRA-G showed the best per-
formance in JnJK.6
5.2 Combining Multiple Engines
grank gFscore gME I-BEST
EnCh 0.730 0.731 0.731 0.715
EnHi 0.481 0.475 0.483 0.469
EnJa 0.535 0.535 0.537 0.519
EnKa 0.393 0.399 0.398 0.386
EnKo 0.461 0.444 0.473 0.460
EnRu 0.602 0.605 0.600 0.600
EnTa 0.470 0.478 0.474 0.459
JnJk 0.597 0.593 0.590 0.571
Table 3: Multi-engine transliteration results on the
test data: the underlined figures are our official re-
sult
Table 3 presents the ACC of our multi-engine
transliteration approach and that of the best in-
dividual engine (I-BEST) in each language pair.
gME gave the best performance in EnCh, EnHi,
EnJa, and EnKo, while gFscore did in EnCh, EnKa,
EnRu, and EnTa. Comparison between the best
individual transliteration engine and our multi-
engine transliteration showed that grank and gME
consistently showed better performance except in
EnRu, while gFscore showed the poorer perfor-
mance in EnKo. The results to be submitted as
?the standard run? were selected among the re-
sults listed in Table 3 by using cross-validation on
the development data. We submitted the results of
gME as the standard run to ?NEWS 2009 Machine
Transliteration Shared Task? for the six language
pairs in Table 3, while the result of gFscore is sub-
mitted as the standard run for EnRu. The official
results of our standard runs were ranked the best
for EnCh, EnJa, EnKa, and EnTa, and the second
best for EnHi, EnKo, and EnRu (Li et al, 2009a).
6 Conclusion
In conclusion, we have applied multi-engine
transliteration approach to ?NEWS 2009 Machine
Transliteration Shared Task.? We built multiple
transliteration engines based on different com-
binations of transliteration models and machine
learning algorithms. We showed that the translit-
eration model, which is based on target language
6We submitted the results of MEM-G as a standard run for
JnJk because we had only one transliteration engine for JnJK
before the submission deadline of the NEWS 2009 machine
transliteration shared task.
graphemes and phonemes, and our multi-engine
transliteration approach are effective, regardless of
the nature of the language pairs.
References
A. L. Berger, S. D. Pietra, and V. J. D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Canasai Kruengkrai, Jun?ichi Kazama, Kiyotaka Uchi-
moto, Kentaro Torisawa, and Hitoshi Isahara. 2008.
A discriminative hybrid model for joint Chinese
word segmentation and pos tagging. In Proc. of The
11th Oriental COCOSDA Workshop.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. of
SIGIR ?07, pages 721?722.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML01, pages 282?289.
Haizhou Li, Min Zhang, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proc. of ACL ?04, pages 160?167.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report on NEWS 2009 machine
transliteration shared task. In Proc. of ACL-IJCNLP
2009 Named Entities Workshop.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. of
ACL-IJCNLP 2009 Named Entities Workshop.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. of CoNLL ?03, pages 188?191.
Jong-Hoon Oh and Hitoshi Isahara. 2007. Machine
transliteration using multiple transliteration engines
and hypothesis re-ranking. In Proc. of the 11th Ma-
chine Translation Summit, pages 353?360.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models. Journal of Artificial Intelligence Re-
search (JAIR), 27:119?151.
The CJK Dictionary Institute. 2009. http://www.
cjk.org.
39
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 842?850,
Beijing, August 2010
Co-STAR: A Co-training Style Algorithm for Hyponymy Relation
Acquisition from Structured and Unstructured Text
Jong-Hoon Oh, Ichiro Yamada, Kentaro Torisawa, and Stijn De Saeger
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
{rovellia,iyamada,torisawa,stijn}@nict.go.jp
Abstract
This paper proposes a co-training style
algorithm called Co-STAR that acquires
hyponymy relations simultaneously from
structured and unstructured text. In Co-
STAR, two independent processes for hy-
ponymy relation acquisition ? one han-
dling structured text and the other han-
dling unstructured text ? collaborate by re-
peatedly exchanging the knowledge they
acquired about hyponymy relations. Un-
like conventional co-training, the two pro-
cesses in Co-STAR are applied to dif-
ferent source texts and training data.
We show the effectiveness of this al-
gorithm through experiments on large-
scale hyponymy-relation acquisition from
Japanese Wikipedia and Web texts. We
also show that Co-STAR is robust against
noisy training data.
1 Introduction
Acquiring semantic knowledge, especially se-
mantic relations between lexical terms, is re-
garded as a crucial step in developing high-level
natural language applications. This paper pro-
poses Co-STAR (a Co-training STyle Algorithm
for hyponymy Relation acquisition from struc-
tured and unstructured text). Similar to co-
training (Blum and Mitchell, 1998), two hy-
ponymy relation extractors in Co-STAR, one for
structured and the other for unstructured text, it-
eratively collaborate to boost each other?s perfor-
mance.
Many algorithms have been developed to auto-
matically acquire semantic relations from struc-
tured and unstructured text. Because term pairs
are encoded in structured and unstructured text in
different styles, different kinds of evidence have
been used for semantic relation acquisition:
Evidence from unstructured text: lexico-
syntactic patterns and distributional similar-
ity (Ando et al, 2004; Hearst, 1992; Pantel
et al, 2009; Snow et al, 2006; De Saeger et
al., 2009; Van Durme and Pasca, 2008);
Evidence from structured text: topic hierarchy,
layout structure of documents, and HTML
tags (Oh et al, 2009; Ravi and Pasca, 2008;
Sumida and Torisawa, 2008; Shinzato and
Torisawa, 2004).
Recently, researchers have used both structured
and unstructured text for semantic-relation acqui-
sition, with the aim of exploiting such different
kinds of evidence at the same time. They ei-
ther tried to improve semantic relation acquisition
by putting the different evidence together into a
single classifier (Pennacchiotti and Pantel, 2009)
or to improve the coverage of semantic relations
by combining and ranking the semantic relations
obtained from two source texts (Talukdar et al,
2008).
In this paper we propose an algorithm called
Co-STAR. The main contributions of this work
can be summarized as follows.
? Co-STAR is a semi-supervised learning
method composed of two parallel and iter-
ative processes over structured and unstruc-
tured text. It was inspired by bilingual co-
training, which is a framework for hyponymy
relation acquisition from source texts in two
languages (Oh et al, 2009). Like bilingual
co-training, two processes in Co-STAR op-
erate independently on structured text and
unstructured text. These two processes are
trained in a supervised manner with their
initial training data and then each of them
tries to enlarge the existing training data of
the other by iteratively exchanging what they
842
have learned (more precisely, by transfer-
ring reliable classification results on com-
mon instances to one another) (see Section
4 for comparison Co-STAR and bilingual
co-training). Unlike the ensemble semantic
framework (Pennacchiotti and Pantel, 2009),
Co-STAR does not have a single ?master?
classifier or ranker to integrate the differ-
ent evidence found in structured and unstruc-
tured text. We experimentally show that, at
least in our setting, Co-STAR works better
than a single ?master? classifier.
? Common relation instances found in both
structured and unstructured text act as a
communication channel between the two ac-
quisition processes. Each process in Co-
STAR classifies common relation instances
and then transfers its high-confidence classi-
fication results to training data of the other
process (as shown in Fig. 1), in order to im-
prove classification results of the other pro-
cess. Moreover, the efficiency of this ex-
change can be boosted by increasing the
?bandwidth? of this channel. For this pur-
pose each separate acquisition process auto-
matically generates a set of relation instances
that are likely to be negative. In our experi-
ments, we show that the above idea proved
highly effective.
? Finally, the acquisition algorithm we propose
is robust against noisy training data. We
show this by training one classifier in Co-
STAR with manually labeled data and train-
ing the other with automatically generated
but noisy training data. We found that Co-
STAR performs well in this setting. This is-
sue is discussed in Section 6.
This paper is organized as follows. Sections 2
and 3 precisely describe our algorithm. Section 4
describes related work. Sections 5 and 6 describe
our experiments and present their results. Conclu-
sions are drawn in Section 7.
2 Co-STAR
Co-STAR consists of two processes that simul-
taneously but independently extract and classify
Structured	 ?Texts	 Unstructured	 ?Texts	
Itera?on	
Training	 ?Data	 ?for	 ?Structured	 ?Texts	
Classifier	 Classifier	Training	 Training	
Enlarged	 ?	 ?Training	 ?Data	 ?for	 ?Structured	 ?Text	
Enlarged	 ?	 ?Training	 ?Data	 ?for	 ?Unstructured	 ?Texts	
Training	 ?Data	 ?For	 ?Unstructured	 ?Texts	
Classifier	Classifier	
Further	 ?Enlarged	 ?Training	 ?Data	 ?for	 ?Structured	 ?Texts	
Further	 ?Enlarged	 ?Training	 ?Data	 ?for	 ?Unstructured	 ?Texts	
Training	
Training	 Training	
Training	
?..	 ?..	Common	 ?instances	Transferring	 ?reliable	 ?classifica?on	 ?results	 ?of	 ?classifiers	
Transferring	 ?reliable	 ?classifica?on	 ?results	 ?of	 ?classifiers	
Figure 1: Concept of Co-STAR.
hyponymy relation instances from structured and
unstructured text. The core of Co-STAR is the
collaboration between the two processes, which
continually exchange and compare their acquired
knowledge on hyponymy relations. This collabo-
ration is made possible through common instances
shared by both processes. These common in-
stances are classified separately by each process,
but high-confidence classification results by one
process can be transferred as new training data to
the other.
2.1 Common Instances
Let S and U represent a source (i.e. corpus)
of structured and unstructured text, respectively.
In this paper, we use the hierarchical layout of
Wikipedia articles and the Wikipedia category
system as structured text S (see Section 3.1), and
a corpus of ordinary Web pages as unstructured
text U . Let XS and XU denote a set of hyponymy
relation candidates extracted from S and U , re-
spectively. XS is extracted from the hierarchi-
cal layout of Wikipedia articles (Oh et al, 2009)
and XU is extracted by lexico-syntactic patterns
for hyponymy relations (i.e., hyponym such as hy-
ponymy) (Ando et al, 2004) (see Section 3 for a
detailed explanation)
We define two types of common instances,
called ?genuine? common instances (G) and ?vir-
tual? common instances (V ). The set of common
instances is denoted by Y = G ? V . Genuine
common instances are hyponymy relation candi-
dates found in both S and U (G = XS ?XU ). On
843
the other hand, term pairs are obtained as virtual
common instances when:
? 1) they are extracted as hyponymy relation
candidates in either S or U and;
? 2) they do not seem to be a hyponymy rela-
tion in the other text
The first condition corresponds to XS ? XU .
Term pairs satisfying the second condition are de-
fined as RS and RU , where RS ? XS = ? and
RU ?XU = ?.
RS contains term pairs that are found in the
Wikipedia category system but neither term ap-
pears as ancestor of the other1. For example, (nu-
trition,protein) and (viruses,viral disease), respec-
tively, hold a category-article relation, where nu-
trition is not ancestor of viruses and vice versa in
the Wikipedia category system. Here, term pairs,
such as (nutrition, viruses) and (viral disease, nu-
trition), can be ones in RS .
RU is a set of term pairs extracted from U
when:
? they are not hyponymy relation candidates in
XU and;
? they regularly co-occur in the same sentence
as arguments of the same verb (e.g., A cause
B or A is made by B);
As a result, term pairs in RU are thought as hold-
ing some other semantic relations (e.g., A and B
in ?A cause B? may hold a cause/effect relation)
than hyponymy relation. Finally, virtual common
instances are defined as:
? V = (XS ?XU ) ? (RS ?RU )
The virtual common instances, from the view-
point of either S or U , are unlikely to hold a hy-
ponymy relation even if they are extracted as hy-
ponymy relation candidates in the other text. Thus
many virtual common instances would be a nega-
tive example for hyponymy relation acquisition.
On the other hand, genuine common instances
(hyponymy relation candidates found in both S
1A term pair often holds a hyponymy relation if one term
in the term pair is a parent of the other in the Wikipedia cat-
egory system (Suchanek et al, 2007).
and U ) are more likely to hold a hyponymy re-
lation than virtual common instances.
In summary, genuine and virtual common in-
stances can be used as different ground for collab-
oration as well as broader collaboration channel
between the two processes than genuine common
instances used alone.
2.2 Algorithm
We assume that classifier c assigns class label
cl ? {yes, no} (?yes? (hyponymy relation) or
?no? (not a hyponymy relation)) to instances in
x ? X with confidence value r ? R+, a non-
negative real number. We denote the classifica-
tion result by classifier c as c(x) = (x, cl, r). We
used support vector machines (SVMs) in our ex-
periments and the absolute value of the distance
between a sample and the hyperplane determined
by the SVMs as confidence value r.
1: Input: Common instances (Y = G ? V ) and
the initial training data (L0S and L0U )
2: Output: Two classifiers (cnS and cnU )
3: i = 0
4: repeat
5: ciS := LEARN(LiS)
6: ciU := LEARN(LiU )
7: CRiS := {ciS(y)|y ? Y , y /? LiS ? LiU}
8: CRiU := {ciU (y)|y ? Y , y /? LiS ? LiU}
9: for each (y, clS , rS) ? TopN(CRiS) and
(y, clU , rU ) ? CRiU do
10: if (rS > ? and rU < ?)
or (rS > ? and clS = clU ) then
11: L(i+1)U := L
(i+1)
U ? {(y, clS)}
12: end if
13: end for
14: for each (y, clU , rU ) ? TopN(CRiU ) and
(y, clS , rS) ? CRiS do
15: if (rU > ? and rS < ?)
or (rU > ? and clS = clU ) then
16: L(i+1)S := L
(i+1)
S ? {(y, clU )}
17: end if
18: end for
19: i = i+ 1
20: until stop condition is met
Figure 2: Co-STAR algorithm
844
The Co-STAR algorithm is given in Fig. 2. The
algorithm is interpreted as an iterative procedure
1) to train classifiers (ciU , ciS) with the existing
training data (LiS and LiU ) and 2) to select new
training instances from the common instances to
be added to existing training data. These are re-
peated until stop condition is met.
In the initial stage, two classifiers c0S and c0U
are trained with manually prepared labeled in-
stances (or training data) L0S and L0U , respec-
tively. The learning procedure is denoted by
c = LEARN(L) in lines 5?6, where c is a re-
sulting classifier. Then ciS and ciU are applied
to classify common instances in Y (lines 7?8).
We denote CRiS as a set of the classification re-
sults of ciS for common instances, which are not
included in the current training data LiS ? LiU .
Lines 9?13 describe a way of selecting instances
in CRiS to be added to the existing training data
in U . During the selection, ciS acts as a teacher
and ciU as a student. TopN(CRiS) is a set of
ciS(y) = (y, clS , rS), whose rS is the top-N high-
est in CRiS . (In our experiments, N = 900.) The
teacher instructs his student the class label of y if
the teacher can decide the class label of y with a
certain level of confidence (rS > ?) and the stu-
dent satisfies one of the following two conditions:
? the student agrees with the teacher on class
label of y (clS = clU ) or
? the student?s confidence in classifying y is
low (rU < ?)
rU < ? enables the teacher to instruct his student
in spite of their disagreement over a class label.
If one of the two conditions is satisfied, (y, clS)
is added to existing labeled instances L(i+1)U . The
roles are reversed in lines 14?18, so that ciU be-
comes the teacher and ciS the student.
The iteration stops if the change in the differ-
ence between the two classifiers is stable enough.
The stability is estimated by d(ciS , ciU ) in Eq. (1),
where ?i represents the change in the average
difference between the confidence values of the
two classifiers in classifying common instances.
We terminate the iteration if d(ciS , ciU ) is smaller
than 0.001 in three consecutive rounds (Wang and
Zhou, 2007).
d(ciS , ciU ) = |?i ? ?(i?1)|/|?(i?1)| (1)
3 Hyponymy Relation Acquisition
In this section we explain how each process ex-
tracts hyponymy relations from its respective text
source either Wikipedia or Web pages. Each pro-
cess extracts hyponymy relation candidates (de-
noted by (hyper,hypo) in this section). Because
there are many non-hyponymy relations in these
candidates2, we classify hyponymy relation can-
didates into correct hyponymy relation or not. We
used SVMs (Vapnik, 1995) for the classification
in this paper.
3.1 Acquisition from Wikipedia
(a) Layout structure
Range
Siberian tiger
Bengal tiger
Subspecies
Taxonomy
Tiger
Malayan tiger
(b) Tree structure
Figure 3: Example borrowed from Oh et al
(2009): Layout and tree structures of Wikipedia
article TIGER
We follow the method in Oh et al (2009) for
acquiring hyponymy relations from the Japanese
Wikipedia. Every article is transformed into a tree
structure as shown in Fig. 3, based on the items in
its hierarchical layout including title, (sub)section
headings, and list items. Candidate relations are
extracted from this tree structure by regarding a
node as a hypernym candidate and all of its subor-
dinate nodes as potential hyponyms of the hyper-
nym candidate (e.g., (TIGER, TAXONOMY) and
(TIGER, SIBERIAN TIGER) from Fig. 3). We ob-
tained 1.9?107 Japanese hyponymy relation can-
didates from Wikipedia.
2Only 25?30% of candidates was true hyponymy relation
in our experiments.
845
Type Description
Feature from Wikipedia Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves
(?WikiFeature?) Structure Distance between hyper and hypo in a tree structure;
Lexical patterns for article or section names, where listed items often appear;
Frequently used section headings in Wikipedia (e.g., ?Reference?);
Layout item type (e.g., section or list); Tree node type (e.g., root or leaf);
Parent and children nodes of hyper and hypo
Infobox Attribute type and its value obtained from Wikipedia infoboxes
Feature from Web texts Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves
(?WebFeature?) Pattern Lexico-syntactic patterns applied to hyper and hypo;
PMI score between pattern and hyponymy relation candidate (hyper,hypo)
Collocation PMI score between hyper and hypo
Noun Class Noun classes relevant to hyper and hypo
Table 1: Feature sets (WikiFeature and WebFeature): hyper and hypo represent hypernym and hyponym
parts of hyponymy relation candidates, respectively.
As features for classification we used lex-
ical, structure, and infobox information from
Wikipedia (WikiFeature), as shown in Table 1.
Because they are the same feature sets as those
used in Oh et al (2009), here we just give a brief
overview of the feature sets. Lexical features3
are used to recognize the lexical evidence for
hyponymy relations encoded in hyper and hypo.
For example, the common head morpheme tiger
in (TIGER, BENGAL TIGER) can be used as the
lexical evidence. Such information is provided
along with the words/morphemes and the parts of
speech of hyper and hypo, which can be multi-
word/morpheme nouns.
Structure features provide evidence found in
layout or tree structures for hyponymy relations.
For example, hyponymy relations (TIGER, BEN-
GAL TIGER) and (TIGER,MALAYAN TIGER) can
be obtained from tree structure ?(root node, chil-
dren nodes of Subspecies)? in Fig 3.
3.2 Acquisition from Web Texts
As the target for hyponymy relation acquisition
from the Web, we used 5 ? 107 pages from
the TSUBAKI corpus (Shinzato et al, 2008),
a 108 page Japanese Web corpus that was de-
pendency parsed with KNP (Kurohashi-Nagao
Parser) (Kurohashi and Kawahara, 2005). Hy-
ponymy relation candidates are extracted from the
corpus based on the lexico-syntactic patterns such
as ?hypo nado hyper (hyper such as hypo)? and
?hypo to iu hyper (hyper called hypo)? (Ando
3MeCab (http://mecab.sourceforge.net/)
was used to provide the lexical features.
et al, 2004). We extracted 6 ? 106 Japanese
hyponymy relation candidates from the Japanese
Web texts. Features (WebFeature) used for classi-
fication are summarized in Table 1. Similar to the
hyponymy relation acquisition from Wikipedia,
lexical features are used to recognize the lexical
evidence for hyponymy relations.
Lexico-syntactic patterns for hyponymy rela-
tion show different coverage and accuracy in hy-
ponymy relation acquisition (Ando et al, 2004).
Further if multiple lexico-syntactic patterns sup-
port acquisition of hyponymy relation candidates,
these candidates are more likely to be actual hy-
ponymy relations. The pattern feature of hy-
ponymy relation candidates is used for these ev-
idence.
We use PMI (point-wise mutual information)
of hyponymy relation candidate (hyper, hypo) as
a collocation feature (Pantel and Ravichandran,
2004), where we assume that hyper and hypo in
candidates would frequently co-occur in the same
sentence if they hold a hyponymy relation.
Semantic noun classes have been regarded as
useful information in semantic relation acquisi-
tion (De Saeger et al, 2009). EM-based clus-
tering (Kazama and Torisawa, 2008) is used for
obtaining 500 semantic noun classes4 from 5 ?
105 nouns (including single-word and multi-word
ones) and their 4? 108 dependency relations with
5 ? 105 verbs and other nouns in our target Web
4Because EM clustering provides a probability distri-
bution over noun class nc, we obtain discrete classes of
each noun n with a probability threshold p(nc|n) ?
0.2 (De Saeger et al, 2009).
846
Co-training Bilingual co-training Co-STAR
(Blum and Mitchell, 1998) (Oh et al, 2009) (Proposed method)
Instance space Same Different Almost different
Feature space Split by human decision Split by languages Split by source texts
Common instances Genuine-common Genuine-common Genuine-common and
(or All unlabeled) instances instances (Translatable) virtual-common instances
Table 2: Differences among co-training, bilingual co-training, and Co-STAR
corpus. For example, noun class C311 includes
biological or chemical substances such as tatou
(polysaccharide) and yuukikagoubutsu (organic
compounds). Noun classes (i.e., C311) relevant to
hyper and hypo, respectively, are used as a noun
class feature.
4 Related Work
There are two frameworks, which are most rele-
vant to our work ? bilingual co-training and en-
semble semantics.
The main difference between bilingual co-
training and Co-STAR lies in an instance space.
In bilingual co-training, instances are in different
spaces divided by languages while, in Co-STAR,
many instances are in different spaces divided by
their source texts. Table 2 shows differences be-
tween co-training, bilingual co-training and Co-
STAR.
Ensemble semantics is a relation acquisition
framework, where semantic relation candidates
are extracted from multiple sources and a single
ranker ranks or classifies the candidates in the fi-
nal step (Pennacchiotti and Pantel, 2009). In en-
semble semantics, one ranker is in charge of rank-
ing all candidates extracted from multiple sources;
while one classifier classifies candidates extracted
from one source in Co-STAR.
5 Experiments
We used the July version of Japanese Wikipedia
(jawiki-20090701) as structured text. We ran-
domly selected 24,000 hyponymy relation candi-
dates from those identified in Wikipedia and man-
ually checked them. 20,000 of these samples were
used as training data for our initial classifier, the
rest was equally divided into development and test
data for Wikipedia. They are called ?WikiSet.?
As unstructured text, we used 5 ? 107 Japanese
Web pages in the TSUBAKI corpus (Shinzato et
al., 2008). Here, we manually checked 9,500
hyponymy relation candidates selected randomly
from Web texts. 7,500 of these were used as train-
ing data. The rest was split into development and
test data. We named this data ?WebSet?.
In both classifiers, the development data was
used to select the optimal parameters, and the test
data was used to evaluate our system. We used
TinySVM (TinySVM, 2002) with a polynomial
kernel of degree 2 as a classifier. ? (the threshold
value indicating high confidence), ? (the thresh-
old value indicating low confidence), and TopN
(the maximum number of training instances to be
added to the existing training data in each iter-
ation) were selected through experiments on the
development set. The combination of ? = 1,
? = 0.3, and TopN=900 showed the best perfor-
mance and was used in the following experiments.
Evaluation was done by precision (P ), recall (R),
and F-measure (F ).
5.1 Results
We compare six systems. Three of these, B1?B3,
show the effect of different feature sets (?Wik-
iFeature? and ?WebFeature? in Table 1) and dif-
ferent training data. We trained two separate clas-
sifiers in B1 and B2, while we integrated feature
sets and training data for training a single classi-
fier in B3. The classifiers in these three systems
are trained with manually prepared training data
(?WikiSet? and ?WebSet?). For the purpose of our
experiment, we consider B3 as the closest possible
approximation of the ensemble semantics frame-
work (Pennacchiotti and Pantel, 2009).
? B1 consists of two completely independent
classifiers. Both S and U classifiers are
trained and tested on their own feature and
data sets (respectively ?WikiSet + WikiFea-
ture? and ?WebSet + WebFeature?).
847
? B2 is the same as B1, except that both clas-
sifiers are trained with all available training
data ? WikiSet and WebSet are combined
(27,500 training instances in total). However,
each classifier only uses its own feature set
(WikiFeature or WebFeature)5.
? B3 adds a master classifier to B1. This third
classifier is trained on the complete 27,500
training instances (same as B2) using all
available features from Table 1, including
each instance?s SVM scores obtained from
the two B1 classifiers6. The verdict of the
master classifier is considered to be the final
classification result.
The other three systems, BICO, Co-B, and Co-
STAR (our proposed method), are for compari-
son between bilingual co-training (Oh et al, 2009)
(BICO) and variants of Co-STAR (Co-B and Co-
STAR). Especially, we prepared Co-B and Co-
STAR to show the effect of different configura-
tions of common instances on the Co-STAR al-
gorithm. We use both B1 and B2 as the initial
classifiers of Co-B and Co-STAR. We notate Co-
B and Co-STAR without ??? when B1 is used as
their initial classifier and those with ??? when B2
is used.
? BICO implements the bilingual co-training
algorithm of (Oh et al, 2009), in which
two processes collaboratively acquire hy-
ponymy relations in two different languages.
For BICO, we prepared 20,000 English and
20,000 Japanese training samples (Japanese
ones are the same as training data in the
WikiSet) by hand.
? Co-B is a variant of Co-STAR that uses only
the genuine-common instances as common
instances (67,000 instances)7, to demonstrate
5Note that training instances from WebSet (or WikiSet)
can have WikiFeature (or WebFeature) if they also appear
in Wikipedia (or Web corpus). But they can always have
lexical feature, the common feature set between WikiFeature
and WebFeature.
6SVM scores are assigned to the instances in training data
in a 10-fold cross validation manner.
7Co-B can be considered as conventional co-
training (Blum and Mitchell, 1998) in the sense that
two classifiers collaborate through actual common instances.
the effectiveness of the virtual common in-
stances.
? Co-STAR is our proposed method, which
uses both genuine-common and virtual-
common instances (643,000 instances in to-
tal).
WebSet WikiSet
P R F P R F
B1 84.3 65.2 73.5 87.8 74.7 80.7
B2 83.4 69.6 75.9 87.4 79.5 83.2
B3 82.2 72.0 76.8 86.1 77.7 81.7
BICO N/A N/A N/A 84.5 81.8 83.1
Co-B 86.2 63.5 73.2 89.7 74.1 81.2
Co-B? 85.5 69.9 77.0 89.6 76.5 82.5
Co-STAR 85.9 76.0 80.6 88.0 81.8 84.8
Co-STAR? 83.3 80.7 82.0 87.6 81.8 84.6
Table 3: Comparison of different systems
Table 3 summarizes the result. Features for
common instances in Co-B and Co-STAR are pre-
pared in the same way as training data in B2, so
that both classifiers can classify the common in-
stances with their trained feature sets.
Comparison between B1?B3 shows that B2 and
B3 outperform B1 in F-measure. More train-
ing data used in B2?B3 (27,500 instances for
both WebSet and WikiSet) results in higher per-
formance than that of B1 (7,500 and 20,000 in-
stances used separately). We think that the lexical
features, assigned regardless of source text to in-
stances in B2?B3, are mainly responsible for the
performance gain over B1, as they are the least
domain-dependent type of features. B2?B3 are
composed of different number of classifiers, each
of which is trained with different feature sets and
training instances. Despite this difference, B2 and
B3 showed similar performance in F-measure.
Co-STAR outperformed the algorithm similar
to the ensemble semantics framework (B3), al-
though we admit that a more extensive com-
parison is desirable. Further Co-STAR outper-
formed BICO. While the manual cost for build-
ing the initial training data used in Co-STAR
and BICO is hard to quantify, Co-STAR achieves
better performance with fewer training data in
total (27,500 instances) than BICO (40,000 in-
stances). The difference in performance between
Co-B and Co-STAR shows the effectiveness of
848
the automatically generated virtual-common in-
stances. From these comparison, we can see that
virtual-common instances coupled with genuine-
common instances can be leveraged to enable
more effective collaboration between the two clas-
sifiers in Co-STAR.
As a result, our proposed method outperforms
the others in F-measure by 1.4?8.5%. We ob-
tained 4.3 ? 105 hyponymy relations from Web
texts and 4.6? 106 ones from Wikipedia8.
6 Co-STAR with Automatically
Generated Training Data
For Co-STAR, we need two sets of manually pre-
pared training data, one for structured text and the
other for unstructured text. As in any other su-
pervised system, the cost of preparing the training
data is an important issue. We therefore investi-
gated whether Co-STAR can be trained for a lower
cost by generating more of its training data auto-
matically.
We automatically built training data for Web
texts by using definition sentences9 and category
names in the Wikipedia articles, while we stuck to
manually prepared training data for Wikipedia. To
obtain hypernyms from Wikipedia article names,
we used definition-specific lexico-syntactic pat-
terns such as ?hyponym is hypernym? and ?hy-
ponym is a type of hypernym? (Kazama and Tori-
sawa, 2007; Sumida and Torisawa, 2008). Then,
we extracted hyponymy relations consisting of
pairs of Wikipedia category names and their mem-
ber articles when the Wikipedia category name
and the hypernym obtained from the definition
of the Wikipedia article shared the same head
word. Next, we selected a subset of the extracted
hyponymy relations that are also hyponymy re-
lation candidates in Web texts, as positive in-
stances for hyponymy relation acquisition from
Web text. We obtained around 15,000 positive in-
stances in this way. Negative instances were cho-
sen from virtual-common instances, which also
originated from the Wikipedia category system
and hyponymy relation candidates in Web texts
8We obtained them with 90% precision by setting the
SVM score threshold to 0.23 for Web texts and 0.1 for
Wikipedia.
9The first sentences of Wikipedia articles.
(around 293,000 instances).
The automatically built training data was noisy
and its size was much bigger than manually pre-
pared training data in WebSet. Thus 7,500 in-
stances as training data (the same number of man-
ually built training data in WebSet) were ran-
domly chosen from the positive and negative in-
stances with a positive:negative ratio of 1:410.
WebSet WikiSet
P R F P R F
B1 81.0 47.6 60.0 87.8 74.7 80.7
B2 80.0 55.4 65.5 87.1 79.5 83.1
B3 82.0 33.7 47.8 87.1 75.6 81.0
Co-STAR 82.2 60.8 69.9 87.3 80.7 83.8
Co-STAR? 79.2 69.6 74.1 87.0 81.8 84.4
Table 4: Results with automatically generated
training data
With the automatically built training data for
Web texts and manually prepared training data for
Wikipedia, we evaluated B1?B3 and Co-STAR,
which are the same systems in Table 3. The results
in Table 4 are encouraging. Co-STAR was robust
even when faced with noisy training data. Further
Co-STAR showed better performance than B1?
B3, although its performance in Table 4 dropped a
bit compared to Table 3. This result shows that we
can reduce the cost of manually preparing training
data for Co-STAR with only small loss of the per-
formance.
7 Conclusion
This paper proposed Co-STAR, an algorithm for
hyponymy relation acquisition from structured
and unstructured text. In Co-STAR, two indepen-
dent processes of hyponymy relation acquisition
from structured texts and unstructured texts, col-
laborate in an iterative manner through common
instances. To improve this collaboration, we in-
troduced virtual-common instances.
Through a series of experiments, we showed
that Co-STAR outperforms baseline systems and
virtual-common instances can be leveraged to
achieve better performance. We also showed that
Co-STAR is robust against noisy training data,
which requires less human effort to prepare it.
10We select the ratio by testing different ratio from 1:2 to
1:5 with our development data in WebSet and B1.
849
References
Ando, Maya, Satoshi Sekine, and Shun Ishiza. 2004.
Automatic extraction of hyponyms from Japanese
newspaper using lexico-syntactic patterns. In Proc.
of LREC ?04.
Blum, Avrim and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT? 98: Proceedings of the eleventh annual con-
ference on Computational learning theory, pages
92?100.
De Saeger, Stijn, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proc. of ICDM 2009, pages 764?769.
Hearst, Marti A. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics,
pages 539?545.
Kazama, Jun?ichi and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proc. of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 698?707.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Kurohashi, Sadao and Daisuke Kawahara. 2005. KNP
(Kurohashi-Nagao Parser) 2.0 users manual.
Oh, Jong-Hoon, Kiyotaka Uchimoto, and Kentaro
Torisawa. 2009. Bilingual co-training for mono-
lingual hyponymy-relation acquisition. In Proc. of
ACL-09: IJCNLP, pages 432?440.
Pantel, Patrick and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In Proc. of
HLT-NAACL ?04, pages 321?328.
Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP ?09, pages 938?947.
Pennacchiotti, Marco and Patrick Pantel. 2009. En-
tity extraction via ensemble semantics. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 238?247.
Ravi, Sujith and Marius Pasca. 2008. Using structured
text for large-scale attribute extraction. In CIKM-
08, pages 1183?1192.
Shinzato, Keiji and Kentaro Torisawa. 2004. Ex-
tracting hyponyms of prespecified hypernyms from
itemizations and headings in web documents. In
Proceedings of COLING ?04, pages 938?944.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008. Tsubaki: An open search engine infrastruc-
ture for developing new information access. In Pro-
ceedings of IJCNLP ?08, pages 189?196.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2006. Semantic taxonomy induction from heteroge-
nous evidence. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 801?808.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proc. of WWW ?07, pages 697?706.
Sumida, Asuka and Kentaro Torisawa. 2008. Hack-
ing Wikipedia for hyponymy relation acquisition.
In Proc. of the Third International Joint Conference
on Natural Language Processing (IJCNLP), pages
883?888, January.
Talukdar, Partha Pratim, Joseph Reisinger, Marius
Pasca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proc. of EMNLP08, pages 582?590.
TinySVM. 2002. http://chasen.org/?taku/
software/TinySVM.
Van Durme, Benjamin and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In Proc. of AAAI08, pages
1243?1248.
Vapnik, Vladimir N. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Wang, Wei and Zhi-Hua Zhou. 2007. Analyzing co-
training style algorithms. In ECML ?07: Proceed-
ings of the 18th European conference on Machine
Learning, pages 454?465.
850
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1423?1434, Dublin, Ireland, August 23-29 2014.
Million-scale Derivation of Semantic Relations
from a Manually Constructed Predicate Taxonomy
Motoki Sano
?
Kentaro Torisawa
?
Julien Kloetzer
?
Chikara Hashimoto
?
Istv
?
an Varga
?
Jong-Hoon Oh
?
? ? ? ? ?
National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan
?
NEC Knowledge Discovery Research Laboratories, Kanagawa, 211-8666, Japan
{
?
msano,
?
torisawa,
?
julien,
?
ch,
?
rovellia}@nict.go.jp,
?
vistvan@az.jp.nec.com
Abstract
We manually created a semantic taxonomy called Phased Predicate Template Taxonomy (PPTT)
that covers 12,023 predicate templates (i.e., predicates with one argument slot like ?rescue X?)
and derived from it various semantic relations between these templates on a million-instance
scale (70%-80% precision level). The derived relations include entailment (e.g., rescue X?X is
alive), happens-before (e.g., buy X?drink X), and a novel relation type anomalous obstruction
(e.g., X is sold out;cannot buy X). Such derivation became possible thanks to PPTT?s design
and the use of statistical methods.
1 Introduction
Databases of various semantic relations between natural language expressions are indispensable knowl-
edge for many NLP applications. For instance, entailment relations are crucial in information extraction
and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014).
Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are im-
portant for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire
those relations with a broad coverage. Although many sophisticated machine learning techniques have
been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky,
2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al.,
2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness
in the input data. In this work we take a completely different approach: we manually construct a seman-
tic lexicon called Phased Predicate Template Taxonomy (PPTT), and derive various types of semantic
relations on a large-scale by using it. Our target language is Japanese, but examples are given in English
for simplicity throughout this paper.
PPTT is a taxonomy of predicate templates (predicates with one argument slot like rescue X, ?Tem-
plate? hereafter) that classifies templates according to phases of story concerning an entity denoted by
X. In the story, or the ?life? of the entity X, X can be anticipated, created, then execute its function and
finally it may collapse and become deficient. Anticipation, creation, execution, collapse, deficiency of X
can be seen as such phases of story concerning X, and PPTT classifies templates into 41 semantic classes
each of which corresponds to a phase. In other words, PPTT provides a way to describe the stories of var-
ious entities that constitute this world, and we believe that PPTT (partly) reflects how we understand the
world and its entities. Accordingly, PPTT can also provide a way to derive various semantic knowledge
about this world such as the happens-before relation between events involving an entity, e.g., since the
creation phase usually occurs before the execution phase, invent X (creation phase) is likely to happen-
before use X (execution phase). In addition, entailment relations can be derived: since the creation phase
of an object X must have occurred if X is in its execution phase, it implies that use X is likely to entail
invent X.
In addition, there are ups and downs in stories; some entities suffer setbacks in their stories. PPTT de-
scribes such ?ups and downs? by means of a recently proposed semantic polarity, excitation (Hashimoto
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1423
et al., 2012). Excitation classifies templates into excitatory, inhibitory, and neutral; an excitatory tem-
plate like install X and buy X indicates that the main function, effect, purpose or role of the entity referred
to by the X of the template is activated, enhanced, or prepared,
1
while an inhibitory template like unin-
stall X and X is canceled roughly indicates that it is deactivated or suppressed. Neutral templates are
neither excitatory nor inhibitory (e.g., consider X). Roughly speaking, an excitatory template expresses
the events that contribute to turn on the function of X, while an inhibitory template expresses the events
that contribute to turn off or not to turn on the function of X. Then, in PPTT, excitatory and inhibitory
respectively correspond to ?ups? and ?downs? in the story of X. The phases in PPTT are marked accord-
ing to these ups and downs. Accordingly, PPTT can derive many antonymous contradiction pairs like
install X?uninstall X, as Hashimoto et al. did, though we omit the detail for space limitation. Moreover,
PPTT can derive a huge volume of anomalous obstruction, a contradiction-like novel semantic relation
that we propose in this paper, like X is canceled;(cannot) buy X and X is sold out;(cannot) buy X,
which indicate that if X is canceled or sold out, you cannot buy X. Anomalous obstruction should be
used for Why-type QA (Oh et al., 2013), as well as a novel system that warns a user who wants to buy
a commercial product that the product is started to be sold out or canceled in various e-commerce sites
without any application-specific coding.
As suggested, a story has a temporal order between its phases, which we call the canonical temporal
order. In addition, some phases in a story would enable or necessitate another phase in the same story to
occur. In PPTT, these relations are embodied in various temporal-semantic links between phases. Note
that each link between two phases does not guarantee that every possible pair of templates taken from
the two phases has such semantic relations; it just indicates that there exists such tendencies. Despite the
absence of the guarantee, PPTT?s links enable a million-scale derivation of semantic relations with the
help of distributional similarity. In existing resources such as WordNet (Fellbaum, 1998), the links are
assumed to be 100% correct, but it would be hard to have such absolutely correct links in a million-scale.
Hence, we believe that our approximate links are more useful for a large-scale relation derivation.
Note that the goal of our PPTT project is to derive a wide range of semantic relations on a large scale,
rather than to complete a comprehensive template taxonomy. As such, PPTT lacks some templates as
described in later sections. Nevertheless, we believe that our design brings much more good than harm,
since we could generate various semantic relations on a million scale thanks to PPTT. Our experimental
results show that we can derive 4.4 million happens-before relation instances with 79.5% precision,
0.5 million entailment relation instances with 70.0% precision, and one million anomalous obstruction
relation instances with 73.5% precision. Constructing the PPTT taxonomy requires a manual labor cost,
which amounted to three man-months in our case; however, we believe that this cost is lower than the
cost for developing highly-precise automatic acquisition methods for all of happens-before, entailment,
contradiction, and anomalous obstruction relations.
We plan to release PPTT and the derived relation instances after the manual annotation of the derived
instances to the NLP community.
2 Related Works
PPTT might resemble other semantic lexicons created in the long history of NLP (Levin, 1993; Kipper
et al., 2006; Fellbaum, 1998; Bond et al., 2009; Fillmore, 1976; Baker et al., 1998; Halliday, 1985;
Pustejovsky et al., 2003; Puscasu and Mititelu, 2008; Bejar et al., 1991; Jurgens et al., 2012). PPTT
is different in that it primarily aims at deriving various types of semantic relations on a large scale ex-
ploiting the notion of the phase of story, rather than being a comprehensive taxonomy like those existing
semantic lexicons. As a result, PPTT can derive more varieties of semantic relations between templates
than any one of those existing lexicons. From WordNet (Fellbaum, 1998; Bond et al., 2009), we can de-
rive entailment and contradiction relations using synsets and synset-links that represent relations such as
?troponym?, ?antonym? and ?entailment?. However, happens-before and anomalous obstruction relations
1
The above definition is slightly different from the original one in Hashimoto et al. (2012). We inserted the verb ?prepared?
into the original definition. This clarifies that various preparation processes for X, such as buy X, can be regarded as excitatory
templates. We also assume that such templates as X exists and have X, which mean little more than just existence, are regarded
as excitatory templates in PPTT based on the assumption that existence can be regarded as preparation for the function of X.
1424
cannot be derived from it, since there is no information on temporal ordering except that on causality.
From VerbNet (Levin, 1993; Kipper et al., 2006), the hyponymy/synonymy type of entailment relations
may be derived using templates in the same verb classes constructed based on shared syntactic behavior,
possibly with the help of statistical methods. However, the other types of relations that can be derived
from PPTT cannot be derived from VerbNet, since there is no link representing relationships between the
verb classes. FrameNet (Fillmore, 1976; Baker et al., 1998) was used to derive hyponymy/synonymy
types of entailment (Coyne and Rambow, 2009; Aharon et al., 2010) using information such as a Frame-
to-frame relation ?Inheritance? (is-a relation). In addition, happens-before relations can be derived using
?Precedes? (Later-Earlier relations). However, since it does not contain semantic constraints like en-
ablement and necessity that PPTT contains, it is not trivial to derive presupposition type of entailment
or anomalous obstruction instances from it. TimeML (Pustejovsky et al., 2003; Puscasu and Mititelu,
2008) contains various temporal information and can be used to derive context-dependent happens-before
relations such as the relation between ?leaves? and ?will not hear? in the sentence ?If Graham leaves to-
day, he will not hear Sabine? through TLINK (Pustejovsky et al., 2003) annotated manually; thus, it is
difficult to derive context-independent relations from it, while they can be derived from PPTT. Besides,
since it covers only temporal information, it is difficult to derive other types of relations from it. From
Bejar et al.?s semantic relation taxonomy of lexical pairs (Bejar et al., 1991; Jurgens et al., 2012),
using semantic relation categories such as ?act: act attribute? (e.g., creep:slow), lexical entailment rela-
tions were extracted (Turney and Mohammad, 2014). However, it is not trivial to derive happens-before
or anomalous obstruction relations from it since it does not contain information on temporal sequences
between verbs.
Furthermore, our work differs from automatic methods for extracting temporal or causal relations
(Szpektor et al., 2007; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Talukdar et al.,
2012; Hashimoto et al., 2012; Hashimoto et al., 2014) in that our method does not require that target
pairs co-occur in a document, unlike the previous methods. Hence, our method is likely to be immune
to data sparseness. We could actually derive a wide range of relation instances that were rarely written
in documents because they were too commonsensical (e.g., X is constructed happens-before sew (some-
thing) at X). Needless to say, such commonsensical knowledge is often needed to develop intelligent
systems.
3 PPTT Design
In PPTT, templates are organized hierarchically into three levels. In each level, there are classes that
correspond to phases of stories, which we call Level-0 (L0), Level-1 (L1), and Level-2 (L2) classes.
Each template belongs to only one class at each level. In the following, we describe each level.
3.1 L0-Classes and L0-Links
First we divided the entire story concerning an entity X into five phases: non-existence, existence, func-
tioning, non-existence to existence transition and existence to non-existence transition. Then we created
the five L0-classes listed below, each of which corresponds to one of these five phases.
Non-existence Class The class of templates that do not entail the existence of X, e.g., plan
X.
2
Existence Class The class of templates that entail X?s existence but does not imply the execu-
tion of its main function or the achievement of its objectives, e.g., buy X, X exists.
Functioning Class The class of templates that imply the execution of X?s main function or
the achievement of its objectives, e.g., use X, eat X.
Non-existence to Existence Transition Class (NET Class) The class of templates that ex-
press the transition from a situation in which X does not exist to a situation in which it
exists, e.g., manufacture X.
2
One might think the definition of the Non-existence Class should be ?the templates that DO entail X?s NON-EXISTENCE?.
We did not use such a definition because it would overlook many templates that are consistent with X?s NON-EXISTENCE but
DO NOT entail X?s NON-EXISTENCE, like order X.
1425
Existence to Non-existence Transition Class (ENT Class) The class of templates that ex-
press the transition from a situation in which X exists to a situation in which it does not
exist, e.g., dismantle X.
!"#$%&'()%#*%+,-.((+!"#"$%&'()%*%
/&'()%#*%+,-.((+!"#"$%+,-%*% 01#*2"#'#3+,-.((+!"#"$%,.!%*%
/&'()%#*%+)"+!"#$%&'()%#*%++45.#('2"#+6/!47+,-.((++!"#"$%/0.1()2'!%*%
!"#$%&'()%#*%+)"+/&'()%#*%++45.#('2"#+6!/47+,-.((+!"#"$%1(),3(42,5!%*%
Figure 1: L0-links among L0-classes.
As mentioned in the introduction, we assume
a canonical temporal order among L0-classes.
For instance, templates in the NET class (e.g.,
manufacture X) should refer to events that usu-
ally happen before those events referred to by
templates in the Existence class (e.g., buy X),
Functioning class (e.g., use X) and ENT class
(e.g., dismantle X). We enumerated such tem-
poral restrictions, each of which is represented
by a link in Figure 1, which we call L0-links
and used them for deriving relations. Note that
we did not set any L0-link between the Exis-
tence class and the Functioning class because
the events described by them may happen in various orders or have temporal overlap. For example, X
exists should have temporal overlap with use X.
Of course, such metaphysical notions as the canonical temporal order and the phases must have many
complications and exceptions. First, many templates that have the neutral excitation polarity (Hashimoto
et al., 2012) did not seem to follow the canonical temporal order among L0-classes. For instance, since
the neutral template think about X does not entail the existence of X, it belongs to the Non-existence
class but one can consider X while X exists or while it is functioning or even after it is collapsed and
violate canonical temporal ordering. For this reason, we excluded neutral templates from PPTT and will
deal with them in a different framework as a future work. In addition, although we did not assume a
temporal order between the Existence class and the Functioning class, some templates in these classes
have a happens-before relation as special cases (e.g., buy X in the Existence class happens before eat
X in the Functioning class). The proposed L0-links also cause problems. For instance, order X (Non-
Existence class) may not always happen before create X (NET class) even though the L0-links indicate
a happens-before relation between their classes. We dealt as far as possible with such cases in level 2
with L2-classes, which are finer than L0-classes. Nonetheless, we stress that the overall plausibility of
the canonical temporal order among L0-classes was experimentally confirmed through the derivation of
happens-before relations only using L0-links. Note that the design of the L0-classes was inspired by the
Generative Lexicon (Pustejovsky, 1998) and Aristotle?s Entelecheia (Aristotle, 1987).
3.2 L1-Classes
Excitation
L0-class Excitatory Inhibitory
POTENTIAL class FORECLOSING class
Non-existence class e.g., plan X e.g., prevent X
ENABLING class INCOMMODE class
Existence class e.g., buy X e.g., weaken X
ACTUALIZING class DISORDERING class
Functioning class e.g., X functions e.g., X loses
GENERATING class
NET class e.g., X is born N/A
CORRUPTING class
ENT class N/A e.g., destroy X
Table 1: L1-classes.
Next, we divided some L0-classes into
L1-classes using the excitation polar-
ity (Hashimoto et al., 2012) to intro-
duce ?ups and downs? to PPTT, which
enables to capture semantic inconsis-
tencies between templates (e.g., in-
stall X?uninstall X) and negative in-
teraction between the events referred
to by the templates in PPTT (e.g., X
is canceled;(cannot) hold X). Excita-
tion was originally proposed for recog-
nizing contradictions and causal rela-
tions between templates and then was
successfully applied to other deep se-
mantic processing (Oh et al., 2013; Varga et al., 2013; Kloetzer et al., 2013; Hashimoto et al., 2014).
1426
As shown in Table 1, we divided each of three L0-classes (Non-existence class, Existence class and
Functioning class) into two L1-classes, each of which corresponds to excitatory and inhibitory. Since
the transition to an existence situation can be interpreted as an enhancement of an entity?s function, we
assumed that all the templates in the NET classes are excitatory because they express a transition of
entity X from a non-existence situation to an existence situation. Similarly, we assume all the templates
from the ENT class are inhibitory. Also, L1-classes do not have specific links between them beside the
L0-links from their parent classes.
3.3 L2-Classes and L2-Links
Finally, we divided L1-classes into 41 L2-classes. Specifically, we first roughly grouped together seman-
tically similar templates from the same L1-class and identified the common semantic properties among
them. Note that in the rough grouping, we classified templates so that the resulting groups fit into fine-
grained phases in the story concerning X.
After this initial grouping, we classified all the templates into the L2-classes that are listed in Table
3 alongside the classification criteria and the number of templates in each class. As the classification
criteria, we used the identified common semantic properties among members of each class. Note that
some L2-classes can be regarded as a subset of another L2-class. For instance, the PROHIBIT L2-class
can be seen as a subset of the PREVENTION L2-class. When a template meets the classification criteria
of both a subset class and its superset class, we classified it into the subset class.
We also made links called L2-links between the L2-classes. The motivation behind this is to capture
finer temporal-semantic constraints that could not be specified at Level-0 and Level-1 as well as to
capture the temporal-semantic constraints inside a single L0 or L1-class. For example, the temporal
order between buy X and eat X is encoded in a L2-link between the ACQUISITION and EXECUTION L2-
classes, while there is no L0-link between the Existence L0-class (class of buy X) and the Functioning
L0-class (class of eat X). This exemplifies that the L2- and L0-links complement each other.
Each L2-link has one of the six types of temporal-semantic links that are summarized in Table 2 with
the number of links of each type. The link types were designed to capture how the events referred to by
the templates in a class affect the occurrence or non-occurrence of the events referred to by the templates
in a class in the past, present, or future. C
1
and C
2
being two L2-classes, C
1
?s effect on the occurrence or
non-occurrence of C
2
is represented by Positive (
+
) and Negative (
?
) links, respectively, while C
1
?s effect
on the past, present, or future phase of X expressed by C
2
is represented by Past, Present, and Future
links, respectively. For instance, the Past
+
link from the ABANDONMENT class to the ACQUISITION
class indicates that a template from the ACQUISITION class (e.g., obtain X) must occur before a template
from the ABANDONMENT class (e.g., get rid of X), and the Future
?
link from the PROHIBIT class to
the EXECUTION class indicates that templates from the PROHIBIT class (e.g., ban X) disable templates
from the EXECUTION class (e.g., utilize X). Notice that L2-links represent such semantic constraints as
enablement and necessity in addition to temporal order, and they are useful for deriving various kinds of
semantic relations including entailment and anomalous obstruction, as shown in a later section. The first
author of this paper hand-labeled the links between every combination of L2-class pairs by considering
the name of the classes and a few example templates in each.
Positive Negative
Past If C
1
occurred, C
2
must have occurred.
e.g.,FORGETTING
Past
+
? RECOGNITION; X is forgotten
Past
+
? X is
recognized (55 links)
If C
1
occurred, C
2
COULD NOT have occurred.
e.g.,CREATION
Past
?
? PREVENTION; X is generated
Past
?
? X
is prevented (438 links)
Present While C
1
is taking place, C
2
must be taking place.
e.g.,INITIATION
Present
+
? BEING; X is started
Present
+
? X
exists (73 links)
While C
1
is taking place, C
2
CANNOT take place.
e.g.,ENHANCEMENT
Present
?
? DEGRADATION; X is enhanced
Present
?
? X is deteriorated (496 links)
Future C
1
enables C
2
to occur. e.g.,PREPARATION
Future
+
? EXECUTION;
X is customized
Future
+
? X is executed (90 links)
C
1
DISABLEs C
2
to occur. e.g.,
DEFICIENCY
Future
?
? PROVISION; X does not exist
Future
?
?
X is provided (210 links)
Table 2: Types and numbers of L2-links in PPTT. Link direction is C
1
? C
2
.
1427
Non-existence L0-class: Potential L1-class (578) / Foreclosing L1-class (178)
DESIRE entails that X is desired but unlike PLANNING or DEMAND, it does not entail that X is planned or requested, e.g.,
desire X, want X (48).
PLANNING entails that X is planned but does not entail that X is requested. Unlike DEMAND, it does not assume that a person
other than the Planner will carry out X, e.g., plan X, conspire X (72).
DEMAND entails that X is requested. Unlike PLANNING, it assumes that a person other than the Demander will carry out X,
e.g., order X (252).
APPROVAL entails that X is approved or permitted and that there was a plan or a demand before approving, e.g., permit X, accept
X (80).
FEAR entails that X is expected and that X is a source of anxiety or fear, e.g., fear X, worry about X (13).
ANTICIPATION entails that X is expected but unlike FEAR, does not entail that X is a source of anxiety or fear, e.g., forecast X, predict
X (24).
SEARCH entails that X is searched for but unlike DESIRE or DEMAND, does not entail that X is desired or requested, e.g.,
search for X (89).
PREVENTION entails that X is prevented. Unlike CANCELATION, it does not entail that there was a plan or a demand before
preventing, e.g., preclude X (54).
CANCELATION entails that X is canceled and that there was a plan or demand before canceling, e.g., cancel X, give up X (34).
PROHIBIT entails that X is prohibited. X?s right or ability to be generated or used is taken away. e.g., ban X, forbid X (39).
POSTPONE entails that X is postponed, e.g., postpone X, defer X (15).
DEFICIENCY entails that X does not exist but does not entail that it is prevented, canceled, prohibited, or postponed, as in other
L2-classes of Foreclosing L1-class. e.g., lack X, X is absent (36).
NET L0-class: Generating L1-class (596)
SYMBOLIZATION entails that X transits from non-existence to existence as a kind of (semiotic) representation, e.g., write X, compose
(music) X (13).
CREATION entails that X transits from non-existence to existence. Unlike SYMBOLIZATION, X is not limited to a semiotic
representation, and unlike TRANSFORMATION, it focuses less on transformation from another entity. generate X,
cause X (509).
TRANSFORMATION entails that X transits from non-existence to existence as a result of transformation. Unlike CREATION, it focuses on
the transformation from another entity, e.g., turn into X (74).
ENT L0-class: Corrupting L1-class (622)
COLLAPSE entails that X transits from existence to non-existence by dying, being eliminated, or being destroyed. Unlike CON-
VERSION , it focuses less on transformation, e.g., destroy X, kill X (588).
CONVERSION entails that X transits from existence to non-existence by transforming X into an another entity, e.g., turned from X,
changed from X (34).
Existence L0-class: Enabling L1-class (3,536) / Incommode L1-class (1,355)
RECOGNITION entails that X is recognized or sensed, e.g., find X, feel X (308).
SELECTION entails that X is selected, e.g., appoint X, choose X (139).
ENCOUNTER entails that X emerges as a result of transportation, e.g., send X, X arrives (407).
ACQUISITION entails that X is obtained and possessed, e.g., buy X, catch X (482).
PROVISION entails that X is handed to be possessed, e.g., sell X, render X (422).
ENHANCEMENT entails that X is extended, improved, or supported, e.g., increase X, help X (880).
PREPARATION entails that X is arranged, connected, or qualified in preparation to execute its function, e.g., cook X, install X (822).
BEING entails that X is existing or living but does not entail that X is recognized, selected, encountered, acquired, enhanced,
or prepared, as in other L2-classes of the Enabling L1-class, e.g., X exists, X lives (76).
UNRECOGNIZING entails that X is not recognized or sensed but unlike FORGETTING, does not entail that X was previously recognized,
e.g., overlook X (8).
FORGETTING entails that X is forgotten and that X was once recognized, e.g., forget X, lose memory of X (8).
UNSELECTING entails that X is not selected, e.g., alternate X, reject X (46).
SEPARATION entails that X is left or separated as a result of transportation, e.g., X leaves, send X away (114).
ABANDONMENT entails that X is not possessed as a result of being thrown away, e.g., throw X away, renounce X (58).
DEPRIVATION entails that X was taken away without the permission of a possessor, e.g., steal X, take X away (102).
DEGRADATION entails that X is reduced, deteriorated, or interrupted, e.g., X is weakened, attack X (908).
UNPREPARED entails that X is unprepared, disconnected, or unqualified, e.g., X is uninstalled, X is disconnected (111).
Functioning L0-class: Actualizing L1-class (4,460) / Disordering L1-class (698)
EXECUTION entails that the function of X is executed but unlike WORKING, does not entail that X successfully satisfies its function,
e.g., ignite X (966).
WORKING entails that the function of X is carried out and that X successfully satisfies its function, e.g., X functions, cleaned by
X (3,106).
INITIATION entails that X is started or continued, e.g., start X, open X (185).
SUCCESS entails that X accomplished its goal and the result of the execution of its function is evaluated positively, e.g., accom-
plish X, X wins (203).
SUSPENSION entails that the function of X is suspended but unlike FINISHING, does not entail that its function is terminated, e.g.,
suspend X (133).
DYSFUNCTION entails that the function of X is executed but X is performing poorly, e.g., X is sluggish, bored by X (196).
FINISHING entails that X is terminated, e.g., end X, finish X. (110).
FAILURE entails that X fails to accomplish its goal and the result of the execution of its function is evaluated negatively, e.g., X
is defeated (259).
Table 3: PPTT classes. The number in parentheses indicates the number of templates in PPTT.
1428
Note that the existence of an L2-link does not guarantee that the semantic properties specified by it
hold for all the possible template pairs taken from the class pair it connects. The cost of hand-labelling
the links with such guarantees is prohibitively high because we would have to check all of the template
combinations. We empirically evaluated the validity of the links in our experiments below although this
is not a direct evaluation since the relations we derived are different from the ones given to the links.
4 Construction of PPTT and Relation Derivation
Using the automatic acquisition method proposed by Hashimoto et al. (2012), we collected 10,825 can-
didates of excitatory/inhibitory templates from a 600-million-page web corpus (hereafter, WCorpus).
Hashimoto et al.?s method constructs a network of templates based on their co-occurrence in sentences
with a small number of seed templates of which excitation polarity are assigned manually, and infers the
polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et
al., 2005). Then, we added the 20,000 most frequent templates in the corpus that could not be extracted
automatically for a total of 30,825 templates.
Three human annotators (not the authors) judged the polarity of the templates, and we included the
excitatory and the inhibitory templates but excluded the neutral templates in PPTT due to the reason
discussed in Section 3.1. We also excluded templates whose variable X is the subject of a transitive verb.
This is because the subject position is often occupied by living things, and since the functions/objectives
of such subjects seem difficult to identify, it is often difficult to judge whether such templates should be
classified into the Functioning class or another. After applying these two restrictions, the first author
classified the remaining 12,023 templates in PPTT.
In this work, we derived happens-before, entailment and anomalous obstruction relations among tem-
plates from PPTT. The target data is the set of all the template pairs such that a noun exists with which
both templates of the pair co-occur at least 100 times in WCorpus. We denote this set of the template
pairs by TP100, and all the relation derivations pick up template pairs as relation instances from it. This
is because in our preliminary experiments, we found that the relation instance candidates taken from
outside of TP100 had much lower precision. The relation derivation itself is quite simple and consists
of the following two steps.
Step 1 Select L0-links or types of L2-links that are expected to represent a target semantic
relation (e.g., Present
+
links are expected to represent entailment, since they represent
the relations between classes where ?While C
1
is taking place, C
2
must be taking place?.)
and extract all the class pairs connected by the selected links (e.g., INITIATION L2-class
Present
+
? BEING L2-class). Enumerate all the template pairs from the intersection between
TP100 and the extracted class pairs (e.g., X is started
Present
+
? X exists).
Step 2 If necessary, rank the relation instance candidates that are extracted in Step1 by distri-
butional similarity scores between the templates that compose the candidates, computed
with WCorpus.
5 Experiments
This section reports our experiments on semantic relation derivation. Derived relation instances were
marked by three human annotators (not the authors) who voted to break ties. Unless stated otherwise,
we asked them to mark a template pair as negative if they found any noun that can be placed in both
templates? argument slots and makes the template pair a negative sample for the target relation, and
positive otherwise.
5.1 Happens-Before Relation
Following Regneri et al. (2010), we assumed template
1
(T
1
) has a happens-before relationwith template
2
(T
2
) iff one event expressed by T
1
normally happens before another expressed by T
2
, provided that both
events occur. Below are our four methods to derive happens-before relation instances, each of which
uses different links. Note that we did not use distributional similarity in this experiment.
1429
H1 uses the 55 pairs of L2-classes connected by L2-link Past
+
, meaning that a template in a
class must occur before another.
H2 uses the 90 pairs of L2-classes connected by L2-link Future
+
, i.e., a template in a class
often enables another to occur.
H3 uses the 474 pairs of L2-classes connected by one of the seven L0-links in Figure 1, i.e.,
the canonical temporal order links.
All is the union of H1-H3 results.
We prepared two baselines; HB-Ptn is a pattern-based method based on Chklovski and Pantel (2004a).
It extracts template pairs in TP100 that were connected in WCorpus by one of manually collected 73
conjunctives expressing temporal order, such as after and before, and which either shared the same
argument or the second template was filled by the pronouns it, this, or that. Random is a random
sampling from TP100.
Three annotators annotated 200 random samples from each method?s output. Fleiss? kappa was .56
(moderate agreement). The results of their majority vote are summarized in Table 4. The recall was
estimated against the number of positive samples in TP100 based on the precision of Random. The
precision of all of our four methods is reasonably high for such a difficult task, and the number of
relations derived by All reached about 4.4 million. The recall of All exceeds 65%, which we believe is
quite high. HB-Ptn suffered from low recall, probably due to the data sparseness in WCorpus. Table 5
shows examples of the derived happens-before relations alongside L2-classes of the templates, the L2-
links between the classes and the original Japanese templates. The acquired relations included many
unexpected but correct happens-before relations, like compose (a piece of music) X?relax by X.
Actually, it is difficult to fairly compare our work and previous works on temporal relation acqui-
sition, due to differences in language, the data used, and the methodologies. Nonetheless, our result
with 79.5% precision is at least five times larger than the English data released by Chambers et al.
(cs.stanford.edu/people/nc/schemas), which contains around 870,000 ?before? relation candidates and
happens-before database in the VerbOcean (Chklovski and Pantel, 2004a) that covers 4,205 relations.
Considering our method is completely different from theirs, we believe that our contribution is valuable.
Setting/Method Precision (%) # of Pairs Recall (%)
H1 83.5 1,113,280 18.0
H2 70.5 1,524,557 20.8
H3 67.0 3,837,116 49.7
All 79.5 4,387,781 67.5
HB-Ptn 53.0 32,288 0.3
Random 18.0 28,717,454 100.0
Table 4: Happens-before derivation performance.
boil X?eat X
PREPARATION Class
Future
+
? EXECUTION Class
X wo niru? X wo taberu
compose (a piece of music) X?relax by X
SYMBOLIZATION Class
Past
+
? WORKING Class
X wo sakkyoku-suru? X de rirakkusu-suru
Table 5: Examples of happens-before relation.
5.2 Entailment Relation
Below are our proposed methods to derive entailment relations.
Present+.DIFF extracts the 32 class pairs that are composed of DIFFERENT L2-classes and
are connected by the Present
+
links, meaning that a template in a class must occur simul-
taneously with another template in another class, and ranks all the possible template pairs
taken from each class pair using Hashimoto et al.?s (2009) conditional probability based
similarity measure for entailment recognition.
Present+.SAME extracts the 41 class pairs that are composed of the SAME L2-classes and
are connected with the Present
+
links, and ranks all the template pairs from each class
pair using Hashimoto et al.?s similarity.
Past+ extracts the 55 pairs of L2-classes that are connected with the Past
+
links, meaning that
a template in a class must occur before another, and ranks all the template pairs from each
class pair using Hashimoto et al.?s similarity.
1430
Baseline-HAS is our baseline which is our implementation of Hashimoto et al. (2009) for entailment
recognition; it ranks all the template pairs in TP100 by Hashimoto et al.?s score. Our methods can be
seen as the restrictions of the output of the baseline method using the extracted PPTT?s class pairs.
0e+00 2e+04 4e+04 6e+04 8e+04 1e+05
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Number of template pairs (sorted by score)
Pr
ec
isi
on
Present+.DIFFPresent+.SAMEPast+Baseline-HAS
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
Figure 2: Entailment derivation performance.
Three annotators hand-labeled 500
random samples from the top 100,000
template pairs for each method. The
kappa was .59 (moderate agreement),
and the results of their majority vote
are presented in Figure 2. Table 6
shows examples of Proposed methods?
outputs. The restriction of the class
pairs in our method contributed to much
higher precision than using the state-of-
the-art method alone.
Since the precision of Past+ is quite
high for the top 100,000 pairs, we an-
notated an additional 500 random sam-
ples from the top 500,000 pairs. Accord-
ing to this annotation, the top 408,610
pairs had 70% precision, implying that
after merging all the top pairs extracted
by Present+.DIFF, Present+.SAME and Past+ whose precisions exceeded 70%, we had 0.49 million
entailment pairs with 70% precision. With Baseline-HAS, we derived only 24,000 with the same preci-
sion. Also, the JapaneseWordNet (v.1.1) covers only 2.4% of the pairs in the manually annotated positive
samples from our proposed methods through the ?synsets? or any ?synlinks?. We analyzed 200 samples
from the positive samples not covered by WordNet and found that 49.5% are the hyponymy type (e.g.,
boil X?heat X), 39.0% are the backward presupposition type (e.g., complete X?start X), and 11.5% are
the synonymy type (e.g., X passes away?X dies). This seems to imply that our methods are better at
deriving all types of entailment, while WordNet might be effective for only the synonymy type. In addi-
tion, by analyzing all the positive samples, we confirmed that the different types of entailment pairs were
derived with different L2-links; 88.1% of the positive samples from Present+.DIFF and Present+.SAME
require that two events referred to by the two templates occur with temporal overlap (e.g., equip X?X
exists, i.e. X is equipped while X exists), while 96.7% of those from Past+ were the backward presuppo-
sition type, in which an event entails another event that happened before it. This shows that the L2-links
were useful for deriving various fine-grained types of entailment.
get X?X exists (X wo nyuushu-suru ? X ga sonzai-suru ) ACQUISITION Class
Present
+
? BEING Class
evolve into X?change into X (X ni shinka-suru ? X ni kawaru ) TRANSFORMATION Class
Present
+
? TRANSFORMATION Class
close (a shop) X?make X (X wo heiten-suru ? X wo tsukuru ) FINISHING Class
Past
+
? CREATION Class
Table 6: Examples of entailment.
5.3 Anomalous Obstruction Relation
We assumed that template
1
(T
1
) like X is sold out has an anomalous obstruction relation with template
2
(T
2
) like buy X (denoted as X is sold out;(cannot) buy X) iff: (A) the event expressed by T
1
prevents
the event expressed by T
2
from occurring; (B) T
1
expresses an event that should not happen if everything
about the variable X goes as expected; and (C) T
2
expresses another event in which the function of X is
executed, enhanced, or prepared. We derived anomalous obstructions, by generating all of the possible
template pairs from the 88 L2-class pairs connected by Future
?
L2-links. These indicate that the events
expressed by the templates in the first class of a pair disable the events expressed by the templates in the
second class. Also, to confirm that the templates of the first class in a pair express an unexpected event,
1431
we required the disabler class to have the inhibitory polarity and the disabled class to be excitatory.
Otherwise, we would obtain such pairs as INITIATION;PLANNING (e.g., start X;schedule X), which
indeed express the prevention relation (Barker and Szpakowicz, 1995), i.e., ?scheduling X would not
occur after starting X,? which is different from anomalous obstruction.
Three annotators annotated 200 random samples for each method, and the results of their majority
vote are summarized in Table 7, where Random refers to a random baseline using TP100. The recall
was estimated using the number of positive samples provided by Random. The kappa was .60 (moderate
agreement). 73.5% precision, 26.4% recall against the positive samples in TP100, and more than one
million outputs of our proposed method are reasonably high/large results for this difficult task. Table 8
shows examples of Proposed?s outputs. ?(cannot)? was attached to disabled templates for readability.
Setting/Method Precision # of Pairs Recall
Proposed 73.5 1,081,405 26.4
Random 10.5 28,717,454 100.0
Table 7: Performance of anomalous obstruc-
tion derivation.
prohibit X;(cannot) exhibit X PROHIBIT Class
Future
?
? EXECUTION Class
X wo kinshi-suru;X wo kookai-suru
break X;(cannot) utilize X COLLAPSE CLASS
Future
?
? EXECUTION CLASS
X wo kowasu;X wo riyo-suru
Table 8: Examples of anomalous obstruction.
6 Conclusion
In this work, we manually constructed a Phased Predicate Template Taxonomy (PPTT), which is a net-
work of semantically coherent classes of templates and derived semantic relations including entailment
from it in a million-instance scale. Future work will extend PPTT to cover non-excitatory/non-inhibitory
templates and generate richer structural knowledge similar to full-fledged scripts (Schank and Abelson,
1977) and narrative schemas (Chambers and Jurafsky, 2011).
Acknowledgements
We would like to thank three anonymous reviewers for many useful comments and advices on the
manuscript of this paper.
References
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010. Generating entailment rules from framenet. In Pro-
ceedings of the ACL 2010 Conference Short Papers, ACLShort ?10, pages 241?246, Stroudsburg, PA, USA.
ACL.
Aristotle. 1987. De Anima (Translated by Hugh Lawson-Tancred). Penguin Classics, London.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of
the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference
on Computational Linguistics - Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA, USA. ACL.
Ken Barker and Stan Szpakowicz. 1995. Interactive semantic analysis of clause level relationships. In Proceedings
of PACLING ?95, Brisbane.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cognitive and Psychometric Analysis of Analogical Problem
Solving. Springer-Verlag, New York.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure
optimization. Comput. Linguist., 38(1):73?111, March.
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka Uchimoto, Takayuki Kuribayashi, and Kyoko Kanzaki.
2009. Enhancing the japanese wordnet. In Proceedings of the 7th Workshop on Asian Language Resources,
ALR7, pages 1?8, Stroudsburg, PA, USA. ACL.
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of
ACL-08: HLT, pages 789?797, Columbus, Ohio, June. ACL.
1432
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ?09, pages 602?610,
Stroudsburg, PA, USA. ACL.
Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 976?986, Portland, Oregon, USA, June. ACL.
Timothy Chklovski and Patrick Pantel. 2004a. Path analysis for refining verb relations. In In Proceedings of KDD
Workshop on Link Analysis and Group Detection (LinkKDD-04), Seattle, WA.
Timothy Chklovski and Patrick Pantel. 2004b. Verbocean: Mining the web for fine-grained semantic verb rela-
tions. In Dekang Lin and Dekai Wu, editors, Proceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?04, pages 33?40, Barcelona, Spain, July. ACL.
Robert Coyne and Owen Rambow. 2009. Lexpar: A freely available english paraphrase lexicon automatically
extracted from framenet. In Proceedings of the Third IEEE International Conference on Seman- tic Computing.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational,
evaluation and approaches. Natural Language Engineering, 15(4):i?xvii.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.
Charles J. Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of
Sciences: Conference on the Origin and Development of Language and Speech, 280(1):20?32.
Michael A.K. Halliday. 1985. An Introduction to Functional Grammar. Arnold, 1st edition.
Chikara Hashimoto, Kentaro Torisawa, KowKuroda, Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama. 2009.
Large-scale verb entailment acquisition from the Web. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1172?1181, Singapore, August. ACL.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012. Excitatory
or inhibitory: a new semantic orientation extracts contradiction and causality from the web. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 619?630, Stroudsburg, PA, USA. ACL.
Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer, Motoki Sano, Istv?an Varga, Jong-Hoon Oh, and Yutaka
Kidawara? 2014. Toward future scenario generation: Extracting event causality exploiting semantic relation,
context, and association features. In Proceedings of the 52nd Annual Meeting of the Association for Computa-
tional Linguistics, Baltimore, USA, June. Association for Computational Linguistics.
David A. Jurgens, Peter D. Turney, Saif M. Mohammad, and Keith J. Holyoak. 2012. Semeval-2012 task 2: Mea-
suring degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computa-
tional Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Semantic Evaluation, SemEval ?12, pages 356?364, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending verbnet with novel verb
classes. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC
2006), pages 731?738, Genoa, Italy, June.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa, Chikara Hashimoto, Jong-Hoon Oh, Motoki Sano, and Kiy-
onori Ohtake. 2013. Two-stage method for large-scale acquisition of contradiction pattern pairs using entail-
ment. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages
693?703, Seattle, Washington, USA, October. ACL.
Beth Levin. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago and London.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake. 2013.
Why-question answering using intra- and inter-sentential causal relations. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1733?1743, Sofia,
Bulgaria, August. ACL.
1433
Georgiana Puscasu and Verginica Barbu Mititelu. 2008. Annotation of wordnet verbs with timeml event classes.
In Bente Maegaard Joseph Mariani Jan Odijk Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, editor, Proceedings of the Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
James Pustejovsky, Jos Castao, Robert Ingria, Roser Saur, Robert Gaizauskas, Andrea Setzer, and Graham Katz.
2003. Timeml: Robust specification of event and temporal expressions in text. In in Fifth International Work-
shop on Computational Semantics (IWCS-5.
James Pustejovsky. 1998. The Generative Lexicon. MIT Press, Cambridge.
Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experi-
ments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
979?988, Uppsala, Sweden, July. ACL.
Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding: an Inquiry into Human
Knowledge Structures. L. Erlbaum, Hillsdale, NJ.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456?463,
Prague, Czech Republic, June. ACL.
Hiroya Takamura, Takashi Inui, andManabu Okumura. 2005. Extracting semantic orientations of words using spin
model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),
pages 133?140, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell. 2012. Acquiring temporal constraints between relations.
In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM
?12, pages 992?1001, New York, NY, USA. ACM.
P. D. Turney and S. M. Mohammad. 2014. Experiments with three approaches to recognizing lexical entailment.
Natural Language Engineering, FirstView:1?40, 5.
Istv?an Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon
Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1619?1629, Sofia, Bulgaria, August. ACL.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Language Learning, pages 194?204, Jeju Island, Korea,
July. ACL.
1434
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 825?835,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Acquisition using Word Classes and Partial Patterns
Stijn De Saeger?? Kentaro Torisawa? Masaaki Tsuchida? Jun?ichi Kazama?
Chikara Hashimoto? Ichiro Yamada? Jong Hoon Oh? Istva?n Varga? Yulan Yan?
? Information Analysis Laboratory, National Institute of
Information and Communications Technology, 619-0289 Kyoto, Japan
{stijn,torisawa,kazama,ch,rovellia,istvan,yulan}@nict.go.jp
? Information and Media Processing Laboratories, NEC Corporation, 630-0101 Nara, Japan
m-tsuchida@cq.jp.nec.com
? Human & Information Science Research Division,
NHK Science & Technology Research Laboratories, 157-8510 Tokyo, Japan
yamada.i-hy@nhk.or.jp
Abstract
This paper proposes a semi-supervised rela-
tion acquisition method that does not rely on
extraction patterns (e.g. ?X causes Y? for
causal relations) but instead learns a combi-
nation of indirect evidence for the target re-
lation ? semantic word classes and partial
patterns. This method can extract long tail
instances of semantic relations like causality
from rare and complex expressions in a large
JapaneseWeb corpus? in extreme cases, pat-
terns that occur only once in the entire cor-
pus. Such patterns are beyond the reach of cur-
rent pattern based methods. We show that our
method performs on par with state-of-the-art
pattern based methods, and maintains a rea-
sonable level of accuracy even for instances
acquired from infrequent patterns. This abil-
ity to acquire long tail instances is crucial for
risk management and innovation, where an ex-
haustive database of high-level semantic rela-
tions like causation is of vital importance.
1 Introduction
Pattern based relation acquisition methods rely on
lexico-syntactic patterns (Hearst, 1992) for extract-
ing relation instances. These are templates of natu-
ral language expressions such as ?X causes Y ? that
signal an instance of some semantic relation (i.e.,
causality). Pattern based methods (Agichtein and
Gravano, 2000; Pantel and Pennacchiotti, 2006b;
Pas?ca et al, 2006; De Saeger et al, 2009) learn many
? This work was done when all authors were at the National
Institute of Information and Communications Technology.
such patterns to extract new instances (word pairs)
from the corpus.
However, since extraction patterns are learned us-
ing statistical methods that require a certain fre-
quency of observations, pattern based methods fail
to capture relations from complex expressions in
which the pattern connecting the two words is rarely
observed. Consider the following sentence:
?Curing hypertension alleviates the deteriora-
tion speed of the renal function, thereby lower-
ing the risk of causing intracranial bleeding?
Humans can infer that this sentence expresses a
causal relation between the underlined noun phrases.
But the actual pattern connecting them, i.e., ?Cur-
ing X alleviates the deterioration speed of the re-
nal function, thereby lowering the risk of causing
Y ?, is rarely observed more than once even in a 108
page Web corpus. In the sense that the term pat-
tern implies a recurring event, this expression con-
tains no pattern for detecting the causal relation be-
tween hypertension and intracranial bleeding. This
is what we mean by ?long tail instances? ? words
that co-occur infrequently, and only in sparse extrac-
tion contexts.
Yet an important application of relation extraction
is mining the Web for so-called unknown unknowns
? in the words of D. Rumsfeld, ?things we don?t
know we don?t know? (Torisawa et al, 2010). In
knowledge discovery applications like risk manage-
ment and innovation, the usefulness of relation ex-
traction lies in its ability to find many unexpected
remedies for diseases, causes of social problems,
and so on. To give an example, our relation extrac-
825
tion system found a blog post mentioning Japanese
automaker Toyota as a hidden cause of Japan?s de-
flation. Several months later the same connection
was made in an article published in an authoritative
economic magazine.
We propose a semi-supervised relation extraction
method that does not rely on direct pattern evidence
connecting the two words in a sentence. We argue
that the role of binary patterns can be replaced by a
combination of two types of indirect evidence: se-
mantic class information about the target relation
and partial patterns, which are fragments or sub-
patterns of binary patterns. The intuition is this: if
a sentence like the example sentence above contains
some wordX belonging to the class of medical con-
ditions and another word Y from the class of trau-
mas, and X matches the partial pattern ?. . . causing
X?, there is a decent chance that this sentence ex-
presses a causal relation between X and Y . We
show that just using this combination of indirect
evidence we can pick up semantic relations with
roughly 50% precision, regardless of the complexity
or frequency of the expression in which the words
co-occur. Furthermore, by combining this idea with
a straightforward machine learning approach, the
overall performance of our method is on par with
state-of-the-art pattern based methods. However,
our method manages to extract a large number of
instances from sentences that contain no pattern that
can be learned by pattern induction methods.
Our method is a two-stage system. Figure 1
presents an overview. In Stage 1 we apply a state-
of-the-art pattern based relation extractor to a Web
corpus to obtain an initial batch of relation instances.
In Stage 2 a supervised classifier is built from vari-
ous components obtained from the output of Stage
1. Given the output of Stage 1 and access to a
Web corpus, the Stage 2 extractor is completely
self-sufficient, and the whole method requires no
supervision other than a handful of seed patterns
to start the first stage extractor. The whole proce-
dure is therefore minimally supervised. Semantic
word classes and partial patterns play a crucial role
throughout all steps of the process.
We evaluate our method on three relation acqui-
sition tasks (causation, prevention and material re-
lations) using a 600 million Japanese Web page cor-
Figure 1: Proposed method: data flow.
pus (Shinzato et al, 2008) and show that our sys-
tem can successfully acquire relations from both
frequent and infrequent patterns. Our system ex-
tracted 100,000 causal relations with 84.6% preci-
sion, 50,000 prevention relations with 58.4% preci-
sion and 25,000 material relations with 76.1% preci-
sion. In the extreme case, we acquired several thou-
sand word pairs co-occurring only in patterns that
appear once in the entire corpus. We call such pat-
terns single occurrence (SO) patterns. Word pairs
that co-occur only with SO patterns represent the
theoretical limiting case of relations that cannot be
acquired using existing pattern based methods. In
this sense our method can be seen as complemen-
tary with pattern based approaches, and merging our
method?s output with that of a pattern based method
may be beneficial.
2 Stage 1 Extractor
This section introduces our Stage 1 extractor: the
pattern based method from (De Saeger et al, 2009),
which we call CDP for ?class dependent patterns?.
We give a brief overview below, and refer the reader
to the original paper for a more comprehensive ex-
planation.
CDP takes a set of seed patterns as input, and au-
tomatically learns new class dependent patterns as
paraphrases of the seed patterns. Class dependent
patterns are semantic class restricted versions of or-
dinary lexico-syntactic patterns. Existing methods
use class independent patterns such as ?X causes
Y ? to learn causal relations betweenX and Y . Class
dependent patterns however place semantic class re-
826
strictions on the noun pairs they may extract, like
?Yaccidents causes Xincidents?. The accidents and
incidents subscripts specify the semantic class of the
X and Y slot fillers.
These class restrictions make it possible to distin-
guish between multiple senses of highly ambiguous
patterns (so-called ?generic? patterns). For instance,
given the generic pattern ?Y by X?, if we restrict
Y and X in to the semantic classes of injuries and
accidents (as in ?death by drowning?), the class de-
pendent pattern ?Yinjuries by Xaccidents? becomes a
valid paraphrase of ?X causes Y ? and can safely be
used to extract causal relations, whereas other class
dependent versions of the same generic pattern (e.g.,
?Yproducts byXcompanies?, as in ?iPhone by Apple?)
may not.
CDP ranks each noun pair in the corpus accord-
ing to a score that reflects its likelihood of being
a proper instance of the target relation, by calcu-
lating the semantic similarity of a set of seed pat-
terns to the class dependent patterns this noun pair
co-occurs with. The output of CDP is a list of noun
pairs ranked by score, together with the highest scor-
ing class dependent pattern each noun pair co-occurs
with. This list becomes the input to Stage 2 of our
method, as shown in Figure 1. We adopted CDP as
Stage 1 extractor because, besides having generally
good performance, the class dependent patterns pro-
vide the two fundamental ingredients for Stage 2 of
our method ? the target semantic word classes for a
given relation (in the form of the semantic class re-
strictions attached to patterns), and partial patterns.
To obtain fine-grained semantic word classes we
used the large scale word clustering algorithm from
(Kazama and Torisawa, 2008), which uses the EM
algorithm to compute the probability that a word w
belongs to class c, i.e., P (c|w). Probabilistic cluster-
ing defines no discrete boundary between members
and non-members of a semantic class, so we simply
assume w belongs to c whenever P (c|w) ? 0.2. For
this work we clustered 106 nouns into 500 classes.
Finally, we adopt the structural representation of
patterns introduced in (Lin and Pantel, 2001). All
sentences in our corpus are dependency parsed, and
patterns consist of words on the path of dependency
relations connecting two nouns.
3 Stage 2 Extractor
We use CDP as our Stage 1 extractor, and the top
N noun pairs along with the class dependent pat-
terns that extract them are given as input to Stage 2,
which represents the main contribution of this work.
As shown in Figure 1, Stage 2 consists of three mod-
ules: a candidate generator, a training data gener-
ator and a supervised classifier. The training data
generator builds training data for the classifier from
the top N output of CDP and sentences retrieved
from the Web corpus. This classifier then scores and
ranks the candidate relations generated by the can-
didate relation generator. We introduce each module
below.
Candidate Generator This module generates
sentences containing candidate word pairs for the
target relation from the corpus. It does so using the
semantic class restrictions and partial patterns ob-
tained from the output of CDP. The set of all seman-
tic class pairs obtained from the class dependent pat-
terns that extracted the topN results become the tar-
get semantic class pairs from which new candidate
instances are generated. We extract all sentences
containing a word pair belonging to one of the target
class pairs from the corpus.
From these sentences we keep only those that con-
tain a trace of evidence for the target semantic re-
lation. For this we decompose the class dependent
patterns from the Stage 1 extractor into partial pat-
terns. As mentioned previously, patterns consist of
words on the path of dependency relations connect-
ing the two target words in a syntactic tree. To obtain
partial patterns we split this dependency path into its
two constituent branches, each one leading from the
leaf word (i.e. variable) to the syntactic head of the
pattern. For example, ?X subj?? causes obj?? Y ? is
split into two partial patterns ?X subj?? causes? and
?causes obj?? Y ?. These partial patterns capture the
predicate structures in binary patterns.1 We discard
partial patterns with syntactic heads other than verbs
or adjectives.
The candidate genarator retrieves all sentences
from the corpus in which two nouns belonging to
one of the target semantic classes co-occur and
1 In Japanese, case information is encoded in post-positions
attached to the noun.
827
where at least one of the nouns matches a partial pat-
tern. As shown in Figure 1, these sentences and the
candidate noun pairs they contain (called (noun pair,
sentence) triples hereafter) are submitted to the clas-
sifier for scoring. Restricting candidate noun pairs
by this combination of semantic word classes and
partial pattern matching proved to be quite powerful.
For instance, in the case of causal relations we found
that close to 60% of the (noun pair, sentence) triples
produced by the candidate generator were correct
(Figure 6).
Training Data Generator As shown in Figure 1,
the (noun pair, sentence) triples used as training data
for the SVM classifier were generated from the top
results of the Stage 1 extractor and the corpus. We
consider the noun pairs in the top N output of the
Stage 1 extractor as true instances of the target re-
lation (even though they may contain erroneous ex-
tractions), and retrieve from the corpus all sentences
in which these noun pairs co-occur and that match
one of the partial patterns mentioned above. In our
experiments we set N to 25, 000. We randomly se-
lect positive training samples from this set of (noun
pair, sentence) triples.
Negative training samples are also selected ran-
domly, as follows. If one member of the target noun
pair in the positive samples above matches a partial
pattern but the other does not, we randomly replace
the latter by another noun found in the same sen-
tence, and generate this new (noun pair, sentence)
triple as a negative training sample. In the causal
relation experiments this approach had about 5%
chance of generating false negatives ? noun pairs
contained in the top N results of the Stage 1 extrac-
tor. Such samples were discarded. Our experimen-
tal results show that this scheme works quite well in
practice. We randomly sample M positive and neg-
ative samples from the autogenerated training data
to train the SVM. M was empirically set to 50,000
in our experiments.
SVM Classifier We used a straightforward fea-
ture set for training the SVM classifier. Because
our classifier will be faced with sentences contain-
ing long and infrequent patterns where the distance
between the two target nouns may be quite large,
we did not try to represent lexico-syntactic patterns
as features but deliberately restricted the feature set
to local context features of the candidate noun pair
in the target sentence. Concretely, we looked at bi-
grams and unigrams surrounding both nouns of the
candidate relation, as the local context around the
target words may contain many telling expressions
like ?increase in X? or ?X deficiency? which are use-
ful clues for causal relations. Also, in Japanese case
information is encoded in post-positions attached to
the noun, which is captured by the unigram features.
In addition to these base features, we include the
semantic classes to which the candidate noun pair
belongs, the partial patterns they match in this sen-
tence, and the infix words inbetween the target noun
pair. Note that this feature set is not intended to
be optimal beyond the actual claims of this paper,
and we have deliberately avoided exhaustive fea-
ture engineering so as not to obscure the contribu-
tion of semantic classes and partial pattern to our
approach. Clearly an optimal classifier will incorpo-
rate many more advanced features (see GuoDong et
al. (2005) for a comprehensive overview), but even
without sophisticated feature engineering our clas-
sifier achieved sufficient performance levels to sup-
port our claims. An overview of the feature set is
given in Table 1. The relative contribution of each
type of features is discussed in section 4. In prelim-
inary experiments we found a polynomial kernel of
degree 3 gave the best results, which suggests the ef-
fectiveness of combining different types of indirect
evidence.
The SVM classifier outputs (noun pair, sentence)
triples, ranked by SVM score. To obtain the final
output of our method we assign each unique noun
pair the maximum score from all (noun pair, sen-
tence) triples it occurs in, and discard all other sen-
tences for this noun pair. In section 4 below we eval-
uate the acquired noun pairs in the context of the
sentence that maximizes their score.
4 Evaluation
We demonstrate the effectiveness of semantic word
classes and partial pattern matching for relation ex-
traction by showing that the method proposed in this
paper performs at the level of other state-of-the-art
relation acquisition methods. In addition we demon-
strate that our method can successfully extract re-
lation instances from infrequent patterns, and we
828
Feature type Description Number of features
Morpheme features Unigram and bigram morphemes surrounding both target words. 554,395
POS features Coarse- and fine-grained POS tags of the noun pair and morpheme features. 2,411
Semantic features Semantic word classes of the target noun pair. 1000 (500 classes ?2)
Infix word features Morphemes found inbetween the target noun pair. 94,448
Partial patterns Partial patterns matching the target noun pair. 86
Table 1: Feature set used in the Stage 2 classifier, and their number for the causal relation experiments.
explore several criteria for what constitutes an in-
frequent pattern ? including the theoretical limit-
ing case of patterns observed only once in the en-
tire corpus. These instances are impossible to ac-
quire by pattern based methods. The ability to ac-
quire relations from extremely infrequent expres-
sions with decent accuracy demonstrates the utility
of combining semantic word classes with partial pat-
tern matching.
4.1 Experimental Setting
We evaluate our method on three semantic relation
acquisition tasks: causality, prevention and mate-
rial. Two concepts stand in a causal relation when
the source concept (the ?cause?) is directly or indi-
rectly responsible for the subsequent occurrence of
the target concept (its ?effect?). In a prevention rela-
tion the source concept directly or indirectly acts to
avoid the occurrence of the target concept, and in a
material relation the source concept is a material or
ingredient of the target concept.
For our experiments we used the latest version
of the TSUBAKI corpus (Shinzato et al, 2008),
a collection of 600 million Japanese Web pages
dependency parsed by the Japanese dependency
parser KNP2. In our implementation of CDP, lexico-
syntactic patterns consist of words on the path con-
necting two nouns in a dependency parse tree. We
discard patterns from dependency paths longer than
8 constituent nodes. Furthermore, we estimated pat-
tern frequencies in a subset of the corpus (50 million
pages, or 1/12th of the entire corpus) and discarded
patterns that co-occur with less than 10 unique noun
pairs in this smaller corpus. These restrictions do
not apply to the proposed method, which can extract
noun pairs connected by patterns of arbitrary length,
even if found only once in the corpus. For our pur-
2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
pose we treat dependency paths whose observed fre-
quency is below this threshold as insufficiently fre-
quent to be considered as ?patterns?. This threshold
is of course arbitrary, but in section 4 we show that
our results are not affected by these implementation
details.
We asked three human judges to evaluate ran-
dom (noun pair, sentence) triples, i.e. candidate
noun pairs in the context of some corpus sentence
in which they co-occur. If the judges find the sen-
tence contains sufficient evidence that the target re-
lation holds between the candidate nouns, they mark
the noun pair correct. To evaluate the performance
of each method we use two evaluation criteria: strict
(all judges must agree the candidate relation is cor-
rect) and lenient (decided by the judges? majority
vote). Over all experiments the interrater agreement
(Kappa) ranged between 0.57 and 0.82 with an aver-
age of 0.72, indicating substantial agreement (Lan-
dis and Koch, 1977).
4.1.1 Methods Compared
We compare our results to two pattern based
methods: CDP (the Stage 1 extractor) and Espresso
(Pantel and Pennacchiotti, 2006a).
Espresso is a popular bootstrapping based method
that uses a set of seed instances to induce extraction
patterns for the target relation and then acquire new
instances in an iterative bootstrapping process. In
each iteration Espresso performs pattern induction,
pattern ranking and selection using previously ac-
quired instances, and uses the newly acquired pat-
terns to extraction new instances. Espresso com-
putes a reliability score for both instances and pat-
terns based on their pointwise mutual information
(PMI) with the top-scoring patterns and instances
from the previous iteration.3 We refer to (Pantel and
3 In our implementation of Espresso we found that, despite
the many parameters for controlling the bootstrapping process,
829
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 2: Precision of acquired relations (causality). L
and S denote lenient and strict evaluation.
Pennacchiotti, 2006a) for a more detailed descrip-
tion.
For all methods compared we rank the acquired
noun pairs by their score and evaluated 500 random
samples from the top 100,000 results. For noun pairs
acquired by CDP and Espresso we select the pattern
that extracted this noun pair (in the case of Espresso,
the pattern with the highest PMI for this noun pair),
and randomly select a sentence in which the noun
pair co-occurs with that pattern from our corpus. For
the proposed method we evaluate noun pairs in the
context of the (noun pair, sentence) triple with the
highest SVM score.
4.2 Results and Discussion
The performance of each method on the causality,
prevention and material relations are shown in Fig-
ures 2, 3 and 4 respectively. In the causality exper-
iments (Figure 2) the proposed method performs on
par with CDP for the top 25,000 results, both achiev-
ing close to 90% precision. But whereas CDP?s per-
it remains very difficult to prevent semantic drift (Komachi et
al., 2008) from occurring. One small adjustment to the al-
gorithm stabilized the bootstrapping process considerably and
gave overall better results. In the pattern induction step (sec-
tion 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso com-
putes a reliability score for each candidate pattern based on the
weighted PMI of the pattern with all instances extracted so far.
As the number of extracted instances increases this dispropor-
tionally favours high frequency (i.e. generic) patterns, so in-
stead of using all instances for computing pattern reliability we
only use the m most reliable instances from the previous iter-
ation, which were used to extract the candidate patterns of the
current iteration (m = 200, like the original).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 3: Precision of acquired relations (prevention). L
and S denote lenient and strict evaluation.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 4: Precision of acquired relations (material). L
and S denote lenient and strict evaluation.
formance drops from there our method maintains
the same high precision throughout (84.6%, lenient).
Both our method and CDP outperform Espresso by
a large margin.
For the prevention relation (Figure 3), precision
is considerably lower for all methods except the top
10,000 of CDP (82% precision, lenient). The pro-
posed method peaks at around 20,000 results (67%
precision, lenient) and performance remains more or
less constant from there on. The proposed method
overtakes CDP?s performance around the top 45,000
mark, which suggests that combining the results of
both methods may be beneficial.
In the material relations the proposed method
slightly outperforms both pattern based methods
in the top 10,000 results (92% precision, lenient).
830
However for this relation our method produced only
35,409 instances. The reason is that the top 25,000
results of CDP were all extracted by just 12 patterns,
and these contained many patterns whose syntactic
head is not a verb or adjective (like ?Y rich in X? or
?Y containing X?). Only 12 partial patterns were ob-
tained, which greatly reduced the output of the pro-
posed method. Taking into account the high perfor-
mance of CDP for material relations, this suggests
that for some relations our method?s N and M pa-
rameters could use some tuning. In conclusion, in
all three relations our method performs at a level
comparable to state-of-the-art pattern based meth-
ods, which is remarkable given that it only uses in-
direct evidence.
Dealing with Difficult Extractions How does our
method handle noun pairs that are difficult to ac-
quire by pattern based methods? The graphs marked
?Prop. w/o CDP? (Proposed without CDP) in Fig-
ures 2 , 3 and 4 show the number and precision of
evaluated samples from the proposed method that do
not co-occur in our corpus with any of the patterns
that extracted the top N results of the first stage ex-
tractor. These graphs show that our method is not
simply regenerating CDP?s top results but actually
extracts many noun pairs that do not co-occur in pat-
terns that are easily learned. Figure 2 shows that
roughly two thirds of the evaluated samples are in
this category, and that their performance is not sig-
nificantly worse than the overall result. The same
conclusion holds for the prevention results (Figure
3), where over 80% of the proposed method?s sam-
ples are noun pairs that do not co-occur with eas-
ily learnable patterns. Their precision is about 5%
worse than all samples from the proposed method.
For material relations (Figure 4) about half of all
evaluated samples are in this category, but their pre-
cision is markedly worse compared to all results.
For genuinely infrequent patterns, the graphs
marked ?Prop. w/o pattern? (Proposed without pat-
tern) in Figures 2 , 3 and 4 show the number and
precision of noun pairs evaluated for the proposed
method that were acquired from sentences without
any discernible pattern. As explained in section 4
above, these constitute noun pairs co-occurring in a
sentence in which the path of dependency relations
connecting them is either longer than 8 nodes or can
 0
 5
 10
 15
 20
1 2 32 1024 32768 1.05x106 3.36x107
% o
f al
l sa
mp
les
# of noun pairs co-occurring with patterns
Pattern frequency, CDPPattern frequency, ProposedPattern frequency, Espresso
Figure 5: Frequencies of patterns in the evaluation data
(causation).
extract fewer than 10 noun pairs in 50 million Web
pages. Note that in theory it is possible that these
noun pairs could not be acquired by pattern based
methods due to this threshold ? patterns must be
able to extract more than 10 different noun pairs in
a subset of our corpus, while the proposed method
does not have this constraint. So at least in the-
ory, pattern based methods might be able to acquire
all noun pairs obtained by our method by lowering
this threshold. To see that this is unlikely to be the
case, consider Figure 5, which shows the pattern fre-
quency of the patterns induced by CDP and Espresso
for the causality experiment. The x-axis represents
pattern frequency in terms of the number of unique
noun pairs a pattern co-occurs with in our corpus (on
a log scale), and the y-axis shows the percentage of
samples that was extracted by patterns of a given fre-
quency.4 Figure 5 shows that for the pattern based
methods, the large majority of noun pairs was ex-
tracted by patterns that co-occur with several thou-
sand different noun pairs. Extrapolating the original
frequency threshold of 10 nounpairs to the size of
our entire corpus roughly corresponds to about 120
distinct noun pairs (10 times in 1/12th of the entire
corpus). In Figure 5, the histograms for the pattern
based methods CDP and Espresso start around 1000
noun pairs, which is far above this new lowerbound.
4 In the case of CDP we ignore semantic class restrictions
on the patterns when comparing frequencies. For Espresso, the
most frequent pattern (?Y by X? at the 24,889,329 data point
on the x-axis) extracted up to 53.8% of the results, but the graph
was cut at 20% for readability.
831
Cau
sali
ty
??????? ??????????????????????????????????????????[????]??????
Because ?catecholamine? causes a rapid increase of heart rate, the change of circulation inside the blood vessels leads to blood vessel
disorders and promotes [thrombus generation].
????? ??????????????????????????????????? [????]?????????????
When we injected Xylocaine during a ?tachycardia seizure?, the patient suddenly lost consciousness and fell into a fit of [convulsions].
???????? ????????? ????????? [???]???????????????
(. . . ) The reason is that by taking a lot of ?animal proteins? the causative agents of [tragomaschalia] increase.
*???????????? ?????? ?????????????????????? [???]?
* [Radon] heightens the (body?s) antioxidative function and is effective for eliminating activated oxygen, which is a cause of aging and
?lifestyle-related? diseases.
Pre
ven
tion
???????????????????? ???? ??????????????[???]?????????????
Because the fatty meat of tuna contains DHA and ?EPA? in abundance, it is effective for preventing [neuralgia].
??????? ????? ??????? [????]?????????
If you use ?nitrogen gas? instead of air you may prevent [dust explosions].
??????????? ??????? ???????????????????????? [???]???????????
In ancient Europe ?orthosiphon aristatus? tea was called a ?diet tea?, and supposedly it helps preventing triglycerides and [adult diseases].
* ?? ???????????????????????? [????]????????
* (It) is something that prevents [scratches] on the screen if the ?calash? gets stuck between the screens during storage.
Table 2: Causality and Prevention relations acquired from Single Occurrence (SO) patterns. ?X? and [Y] indicate the
relation instance?s source and target words, and ?*? indicates erroneous extractions.
Thus, pattern based methods naturally tend to induce
patterns that are much more frequent than the range
of patterns our method can capture, and it is unlikely
that this is a result of implementation details like pat-
tern frequency threshold.
The precision of noun pairs in the category ?Prop.
w/o pattern? is clearly lower than the overall re-
sults, but the graphs demonstrate that our method
still handles these difficult cases reasonably well.
The 500 samples evaluated contained 155 such in-
stances for causality, 403 for prevention and 276 for
material. For prevention, the high ratio of these noun
pairs helps explain why the overall performance was
lower than for the other relations.
Finally, the theoretical limiting case for pattern
based algorithms consists of patterns that only co-
occur with a single noun pair in the entire corpus
(single occurrence or SO patterns). Pattern based
methods learn new patterns that share many noun
pairs with a set of reliable patterns in order to extract
new relation instances. If a noun pair that co-occurs
with a SO pattern also co-occurs with more reliable
patterns there is no need to learn the SO pattern. If
that same noun pair does not co-occur with any other
reliable pattern, the SO pattern is beyond the reach
of any pattern induction method. Thus, SO patterns
are effectively useless for pattern based methods.
For the 500 samples evaluated from the causality
and prevention relations acquired by our method we
found 7 causal noun pairs that co-occur only in SO
patterns and 29 such noun pairs for prevention. The
precision of these instances was 42.9% and 51.7%
respectively. In total we found 8,716 causal noun
pairs and 7,369 prevention noun pairs that co-occur
only with SO patterns. Table 2 shows some example
relations from our causality and prevention experi-
ments that were extracted from SO patterns. To con-
clude, our method is able to acquire correct relations
even from the most extreme infrequent expressions.
Semantic Classes, Partial Patterns or Both? In
the remainder of this section we look at how the
combination of semantic word classes and partial
patterns benefits our method. For each relation we
evaluated 1000 random (noun pair, sentence) triples
satisfying the two conditions from section 3 ?
matching semantic class pairs and partial patterns.
Surprisingly, the precision of these samples was
59% for causality, 40% for prevention and 50.4%
for material, showing just how compelling these two
types of indirect evidence are in combination.
To estimate the relative contribution of each
heuristic we compared our candidate generation
method against two baselines. The first baseline
evaluates the precision of random noun pairs from
832
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 6: Contribution of feature sets (causality).
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 7: Contribution of feature sets (prevention).
the target semantic classes that co-occur in a sen-
tence. The second baseline does the same for the
second heuristic, selecting random sentences con-
taining a noun pair that matches some partial pat-
tern. Evaluating 100 samples for causality and pre-
vention, we found the precision of the semantic class
baseline was 16% for causality and 5% for preven-
tion. The pattern fragment baseline gave 9% for
causality and 22% for prevention. This is consid-
erably lower than the precision of random samples
that satisfy both the semantic class and partial pat-
tern conditions, showing that the combination of se-
mantic classes and partial patterns is more effective
than either one individually.
Finally, we investigated the effect of the various
feature sets used in the classifier. Figures 6, 7 and
8 show the results for the respective semantic re-
lations. The ?Base features? graph shows the per-
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 8: Contribution of feature sets (material).
formance the unigram, bigram and part-of-speech
features. ?All features? uses all features in Table
1. The other graphs show the effect of removing
one type of features. These graphs suggest that the
contribution of the individual feature types (seman-
tic class information, partial patterns or infix words)
to the classification performance is relatively minor,
but in combination they do give a marked improve-
ment over the base features, at least for some rela-
tions like causation and material. In other words,
the main contribution of semantic word classes and
partial patterns to our method?s performance lies not
in the final classification step but seems to occur at
earlier stages of the process, in the candidate and
training data generation steps.
5 Related Work
Using lexico-syntactic patterns to extract semantic
relations was first explored by Hearst (Hearst, 1992),
and has inspired a large body of work on semi-
supervised relation acquisition methods (Berland
and Charniak, 1999; Agichtein and Gravano, 2000;
Etzioni et al, 2004; Pantel and Pennacchiotti,
2006b; Pas?ca et al, 2006; De Saeger et al, 2009),
two of which were used in this work.
Some researchers have addressed the sparse-
ness problems inherent in pattern based methods.
Downey et al (2007) starts from the output of
the unsupervised information extraction system Tex-
tRunner (Banko and Etzioni, 2008), and uses lan-
guage modeling techniques to estimate the reliabil-
ity of sparse extractions. Pas?ca et al (2006) alle-
833
viates pattern sparseness by using infix patterns that
are generalized using classes of distributionally sim-
ilar words. In addition, their method employs clus-
tering based semantic similarities to filter newly ex-
tracted instances in each iteration of the bootstrap-
ping process. A comparison with our method would
have been instructive, but we were unable to imple-
ment their method because the original paper con-
tains insufficient detail to allow replication.
There is a large body of research in the super-
vised tradition that does not use explicit pattern rep-
resentations ? kernel based methods (Zelenko et
al., 2003; Culotta, 2004; Bunescu and Mooney,
2005) and CRF based methods (Culotta et al, 2006).
These approaches are all fully supervised, whereas
in our work the automatic generation of candi-
dates and training data is an integral part of the
method. An interesting alternative is distant super-
vision (Mintz et al, 2009), which trains a classi-
fier using an existing database (Freebase) containing
thousands of semantic relations, with millions of in-
stances. We believe our method is more general, as
depending on external resources like a database of
semantic relations limits both the range of seman-
tic relations (i.e., Freebase contains only relations
between named entities, and none of the relations
in this work) and languages (i.e., no resource com-
parable to Freebase exists for Japanese) to which
the technology can be applied. Furthermore, it is
unclear whether distant supervision can deal with
noisy input such as automatically acquired relation
instances.
Finally, inference based methods (Carlson et al,
2010; Schoenmackers et al, 2010; Tsuchida et al,
2010) are another attempt at relation acquisition that
goes beyond pattern matching. Carlson et al (2010)
proposed a method based on inductive logic pro-
gramming (Quinlan, 1990). Schoenmackers et al
(2010) takes relation instances produced by Tex-
tRunner (Banko and Etzioni, 2008) as input and in-
duces first-order Horn clauses, and new instances are
infered using a Markov Logic Network (Richardson
and Domingo, 2006; Huynh and Mooney, 2008).
Tsuchida et al (2010) generated new relation hy-
potheses by substituting words in seed instances
with distributionally similar words. The difference
between these works and ours lies in the treatment
of evidence. While the above methods learn infer-
ence rules to acquire new relation instances from in-
dependent information sources scattered across dif-
ferent Web pages, our method takes the other option
of working with all the clues and indirect evidence a
single sentence can provide. In the future, a combi-
nation of both approaches may prove beneficial.
6 Conclusion
We have proposed a relation acquisition method that
is able to acquire semantic relations from infrequent
expressions by focusing on the evidence provided by
semantic word classes and partial pattern matching
instead of direct extraction patterns. We experimen-
tally demonstrated the effectiveness of this approach
on three relation acquisition tasks, causality, preven-
tion and material relations. In addition we showed
our method could acquire a significant number of
relation instances that are found in extremely infre-
quent expressions, the most extreme case of which
are single occurrence patterns, which are beyond
the reach of existing pattern based methods. We be-
lieve this ability is of crucial importance for acquir-
ing valuable long tail instances. In future work we
will investigate whether the current framework can
be extended to acquire inter-sentential relations.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proc. of the fifth ACM conference on Digital li-
braries, pages 85?94.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proc. of the 46th ACL-08:HLT, pages 28?36.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 57?64, College Park, Mary-
land, USA, June.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT ?05), pages 724?731.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for neverend-
ing language learning. In Proc of the 24th AAAI, pages
1306?1313.
834
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT/NAACL), pages 296?303.
Aron Culotta. 2004. Dependency tree kernels for rela-
tion extraction. In In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-04, pages 423?429.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large Scale
Relation Acquisition Using Class Dependent Patterns.
In Proc. of the 9th International Conference on Data
Mining (ICDM), pages 764?769.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2007).
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll. In Proc. of
the 13th international conference on World Wide Web
(WWW04), pages 100?110.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extrac-
tion. In Proc. of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05, pages
419?444.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING?92), pages 539?545.
Tuyen N. Huynh and Raymond J. Mooney. 2008.
Discriminative structure and parameter learning for
markov logic networks. In Proc. of the 25th ICML,
pages 416?423.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08: HLT), pages 407?415.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in espresso-like bootstrapping algorithms.
In Proc. of EMNLP?08. Honolulu, USA, pages 1011?
1020.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proc. of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 323?328.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and Similarities on
the Web: Fact Extraction in the Fast Lane. In Proc. of
the COLING-ACL06, pages 809?816.
Patrick Pantel and Marco Pennacchiotti. 2006a.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-06,
pages 113?120.
Patrick Pantel and Pennacchiotti Pennacchiotti, Marco.
2006b. Espresso: Leveraging generic patterns for au-
tomatically harvesting semantic relations. In Proc. of
the COLING-ACL06, pages 113?120.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239?266.
Matthew Richardson and Pedro Domingo. 2006.
Markov logic networks. Machine Learning, 26:107?
136.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proc. of EMNLP2010, pages
1088?1098.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access. In Proc. of IJC-
NLP, pages 189?196.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion to
discover unknown unknowns. New Generation Com-
puting, 28(3):217?236.
Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa,
Masaki Murata, Jun?ichi Kazama, Kow Kuroda, and
Hayato Ohwada. 2010. Large scale similarity-based
relation expansion. In Proc of the 4th IUCS, pages
140?147.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
pages 1083?1106.
835
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 368?378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Why Question Answering using Sentiment Analysis and Word Classes
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ?
Takuya Kawada? Stijn De Saeger? Jun?ichi Kazama? Yiou Wang??
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? rovellia,? torisawa,? ch,? tkawada,?stijn,? kazama,??wangyiou}@nict.go.jp
Abstract
In this paper we explore the utility of sen-
timent analysis and semantic word classes
for improving why-question answering on a
large-scale web corpus. Our work is moti-
vated by the observation that a why-question
and its answer often follow the pattern that if
something undesirable happens, the reason is
also often something undesirable, and if some-
thing desirable happens, the reason is also of-
ten something desirable. To the best of our
knowledge, this is the first work that intro-
duces sentiment analysis to non-factoid ques-
tion answering. We combine this simple idea
with semantic word classes for ranking an-
swers to why-questions and show that on a set
of 850 why-questions our method gains 15.2%
improvement in precision at the top-1 answer
over a baseline state-of-the-art QA system that
achieved the best performance in a shared task
of Japanese non-factoid QA in NTCIR-6.
1 Introduction
Question Answering (QA) research for factoid ques-
tions has recently achieved great success as demon-
strated by IBM?s Watson at Jeopardy: its accuracy
has been reported to be around 85% on factoid ques-
tions (Ferrucci et al2010). Although recent shared
QA tasks (Voorhees, 2004; Pe?as et al2011; Fuku-
moto et al2007) have stimulated the research com-
munity to move beyond factoid QA, comparatively
little attention has been paid to QA for non-factoid
questions such as why questions and how to ques-
tions, and the performance of the state-of-art non-
factoid QA systems reported in the literature (Mu-
rata et al2007; Surdeanu et al2011; Verberne et
al., 2010) remains considerably lower than that of
factoid QA (i.e., 34% in MRR at top-150 results on
why-questions (Verberne et al2010)).
In this paper we explore the utility of sentiment
analysis (Pang et al2002; Turney, 2002; Nakagawa
et al2010) and semantic word classes for improv-
ing why-question answering (why-QA) on a large-
scale web corpus. The inspiration behind this work
is the observation that why-questions and their an-
swers often have the following tendency:
? if something undesirable happens, the reason is
often also something undesirable, and
? if something desirable happens, its reason is of-
ten also desirable.
Consider the following question Q1, and its an-
swer candidates A1-1 and A1-2.
? Q1: Why does cancer occur?
? A1-1: Carcinogens such as nitrosamine and
benzopyrene may increase the risk of cancer by
altering DNA in cells.
? A1-2: Maintaining a healthy weight may lower
the risk of various types of cancer.
Here A1-1 describes an undesirable event related to
cancer, while A1-2 suggests a desirable action for
its prevention. Our hypothesis suggests that A1-1
is more appropriate for answering Q1. If this hy-
pothesis holds, we can obtain a significant improve-
ment in performance on why-QA tasks by exploiting
the sentiment orientation1 of expressions obtainable
1 In this paper we denote the desirable/undesirable polar-
ity of an expression by the term ?sentiment orientation? instead
of ?semantic orientation? to avoid confusion with our different
notion of ?semantic word classes.?
368
by automatic sentiment analysis of questions and an-
swers.
A second observation motivating this work is that
there are often significant associations between the
lexico-semantic classes of words in a question and
those in its answer sentence. For instance, questions
concerning diseases like Q1 often have answers that
include references to specific semantic word classes
such as chemicals (like A1-1), viruses, body parts,
and so on. Capturing such statistical correlations be-
tween diseases and harmful substances may lead to
higher why-QA performance. For this purpose we
use classes of semantically similar words that were
automatically acquired from a large web corpus us-
ing an EM-based clustering method (Kazama and
Torisawa, 2008).
Another issue is that simply introducing the sen-
timent orientation of words or phrases in question
and answer sentences in a naive way is insufficient,
since answer candidate sentences may contain mul-
tiple sentiment expressions with different polarities
in answer candidates (i.e., about 33% of correct an-
swers had such multiple sentiment expressions with
different polarities in our test set). For example, if
A1-2 contained a second sentiment expression with
negative polarity like the example below,
?Trusting a specific food is not effective
for preventing cancer, but maintaining a
healthy weight may help lower the risk of
various types of cancer.?
both A1-1 and A1-2 would contain sentiment ex-
pressions with the same polarity as that of Q1. Thus,
it is difficult to expect that the sentiment orientation
alone will work well for recognizing A1-1 as a cor-
rect answer to Q1. To address this problem, we con-
sider the combination of sentiment polarity and the
contents of sentiment expressions associated with
the polarity in questions and their answer candidates
as well. To deal with the data sparseness problem
arising in using the content of sentiment expressions,
we developed a feature set that combines the polar-
ity and the semantic word classes effectively.
We exploit these two main ideas (concerned with
the sentiment orientation and the semantic classes
described so far) for training a supervised classi-
fier to rank answer candidates to why-questions.
Through a series of experiments on 850 Japanese
why-questions, we showed that the proposed seman-
tic features were effective in identifying correct an-
swers, and our proposed method obtained more than
15% improvement in precision of its top answer
(P@1) over our baseline, which achieved the best
performance in the non-factoid QA task in NTCIR-
6 (Murata et al2007). We also show that our
method can potentially perform with high precision
(64.8% in P@1) when answer candidates containing
at least one correct answer are given to our re-ranker.
2 Approach
Our proposed method is composed of answer re-
trieval and answer re-ranking. The first step, an-
swer retrieval, extracts a set of answer candidates to
a why-question from 600 million Japanese Web cor-
pus. The answer retrieval is our implementation of
the state-of-art method that has shown the best per-
formance in the shared task of Japanese non-factoid
QA in NTCIR-6 (Murata et al2007; Fukumoto et
al., 2007). The second step, answer re-ranking, is
the focus of this work.
2.1 Answer Retrieval
We use Solr2 to retrieve documents from a 600 mil-
lion Japanese Web page corpus3for a given why-
question. Let a set of content words in a why-
question be T = {t1, ? ? ? , tn}. Two boolean queries
for a why-question, ?t1 AND ? ? ? AND tn? and ?t1
OR ? ? ? OR tn,? are given to Solr and top-300 doc-
uments for each query are retrieved. Note that re-
trieved documents by each query have different cov-
erage and relevance to a given why-question. To
keep balance between the coverage and relevance of
retrieved documents, we use a set of retrieved doc-
uments by these two queries for obtaining answer
candidates. Each document in the result of docu-
ment retrieval is split into a set of answer candi-
dates consisting of five subsequent sentences4. Sub-
sequent answer candidates can share up to two sen-
tences to avoid errors due to wrong document seg-
mentation.
2 http://lucene.apache.org/solr
3 To the best of our knowledge, few Japanese non-factoid
QA systems in the literature have used such a large-scale cor-
pus.
4 The length of acceptable answer candidates for why-
QA in the literature ranges from one sentence to two para-
graphs (Fukumoto et al2007; Murata et al2007; Higashinaka
and Isozaki, 2008; Verberne et al2007; Verberne et al2010).
369
Answer candidate ac for question q is ranked
according to scoring function S(q, ac) given in
Eq. (1) (Murata et al2007). Murata et al2007)?s
method uses text search to look for answer candi-
dates containing terms from the question with ad-
ditional clue terms referring to ?reason? or ?cause.?
Following the original method we used riyuu (rea-
son), genin (cause) and youin (cause) as clue terms.
The top-20 answer candidates for each question are
passed on to the next step, which is answer re-
ranking. S(q, ac) assigns a score to answer candi-
dates like tf -idf , where 1/dist(t1, t2) functions like
tf and 1/df(t2) is idf for given terms t1 and t2 that
are shared by q and ac.
S(q, ac) = maxt1?T
?
t2?T
?? log(ts(t1, t2)) (1)
ts(t1, t2) =
N
2? dist(t1, t2)? df(t2)
Here T is a set of terms including nouns, verbs, and
adjectives in question q that appear in answer can-
didate ac. Note that the clue terms are added to T
if they exist in ac. N is the total number of docu-
ments (600 million), dist(t1, t2) represents the dis-
tance (the number of characters) between t1 and t2
in answer candidate ac, df(t) is the document fre-
quency of term t, and ? ? {0, 1} is an indicator,
where ? = 1 if ts(t1, t2) > 1, ? = 0 otherwise.
2.2 Answer Re-ranking
Our re-ranker is a supervised classifier (SVMs)
(Vapnik, 1995) that uses three types of feature
sets: features expressing morphological and syn-
tactic analysis of questions and answer candidates,
features representing semantic word classes appear-
ing in questions and answer candidates, and features
from sentiment analysis. All answer candidates of a
question are ranked in a descending order of their
score given by SVMs. We trained and tested the
re-ranker using 10-fold cross validation on a cor-
pus composed of 850 why-questions and their top-
20 answer candidates provided by the answer re-
trieval procedure in Section 2.1. The answer candi-
dates were manually annotated by three human an-
notators (not by the authors). Our corpus construc-
tion method is described in more detail in Section 4.
3 Features for Answer Re-ranking
This section describes our feature sets for answer
re-ranking: features expressing morphological and
syntactic analysis (MSA), features representing se-
mantic word class (SWC), and features indicat-
ing sentiment analysis (SA). MSA, which has been
widely used for re-ranking answers in the literature,
is used to identify associations between questions
and answers at the morpheme, word phrase, and syn-
tactic dependency levels. The other two feature sets
are proposed in this paper. SWC is devised for iden-
tifying semantic word class associations between
questions and answers. SA is used for identify-
ing sentiment orientation associations between ques-
tions and answers as well as expressing the combi-
nation of each sentiment expression and its polarity.
Table 1 summarizes the respective feature sets, each
of which is described in detail below.
3.1 Morphological and Syntactic Analysis
MSA including n-grams of morphemes, words, and
syntactic dependencies has been widely used for re-
ranking answers in non-factoid QA (Higashinaka
and Isozaki, 2008; Surdeanu et al2011; Verberne
et al2007; Verberne et al2010). We use MSA as
a baseline feature set in this work.
We represent all sentences in a question and
its answer candidate in three ways: morphemes,
word phrases (bunsetsu5) and syntactic dependency
chains. These are obtained using a morphological
analyzer6 and a dependency parser7. From each
question and answer candidate we extract n-grams
of morphemes, word phrases, and syntactic depen-
dencies, where n ranges from 1 to 3. Syntactic de-
pendency n-grams are defined as a syntactic depen-
dency chain containing n word phrases. Syntactic
dependency 1-grams coincide with word phrase 1-
grams, so they are ignored.
Table 1 defines four types of MSA (MSA1 to
MSA4). MSA1 is n-gram features from all sen-
tences in a question and its answer candidates and
distinguishes an n-gram feature found in a ques-
tion from that same feature found in answer candi-
dates. MSA2 contains n-grams found in the answer
5 A bunsetsu is a syntactic constituent composed of a content
word and several function words such as post-positions and case
markers. It is the smallest unit of syntactic analysis in Japanese.
6 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
7 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
370
MSA1 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in a question and its answer candidate, where n ranges
from 1 to 3. n-grams in a question and those in an answer candidate are distinguished.
MSA2 MSA1?s n-grams in an answer candidate that contain a question term.
MSA3 MSA1?s n-grams that contain a clue term including riyuu (reason), genin (cause) and youin (cause). These n-grams in a question and
those in an answer candidate are distinguished.
MSA4 The ratio of the number of question terms in an answer candidate to the total number of question terms.
SWC1 Word class n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are distin-
guished.
SWC2 SWC1?s n-grams in an answer candidate whose original MSA1?s n-grams contain any question term.
SA@W1 Word polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W2 SA@W1?s n-grams in an answer candidate whose original MSA1 n-grams contain any question term.
SA@W3 Joint class-polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W4 SA@W3?s n-grams in an answer candidates whose original MSA1 n-grams contain any question term.
SA@P1 The indicator for polarity agreement between sentiment phrases, one in a question and the other in an answer candidate: 1 if any pair of
such sentiment phrases has polarity in agreement, 0 otherwise.
SA@P2 The phrase-polarity, positive or negative, of a pair of sentiment phrases for which the indicator in SA@P1 is 1.
SA@P3 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in sentiment phrases are coupled with their phrase-polarity,
where n ranges from 1 to 3. These n-grams in a question and those in an answer candidate are distinguished.
SA@P4 SA@P3?s n-grams in an answer candidates that contain a question term.
SA@P5 The ratio of the number of question terms in sentences that have sentiment phrases in answer candidates to the total number of question
terms.
SA@P6 Word class n-grams in sentiment phrases are coupled with phrase-polarity. These n-grams in a question and those in an answer candidate
are distinguished.
SA@P7 SA@P6?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P8 Joint class-polarity n-grams in sentiment phrases of a question and its answer candidate are coupled with phrase-polarity of the sentiment
phrases. These n-grams in a question and those in an answer candidate are distinguished.
SA@P9 SA@P8?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P10 A pair of SA@P6?s n-grams, one from sentiment phrases in a question and the other from those in an answer candidate, where the two
sentiment phrases should have the same sentiment orientation.
Table 1: Features used in training our re-ranker
candidates that themselves contain a term from the
question (e.g., ?types of cancer? in example A1-2).
MSA3 is the n-gram feature that contains one of the
clue terms used for answer retrieval (riyuu (reason),
genin (cause) or youin (cause)). Here too, n-grams
obtained from the questions and answer candidates
are distinguished. Finally, MSA4 is the percentage
of the question terms found in an answer candidate.
3.2 Semantic Word Class
Semantic word classes are sets of semantically simi-
lar words. We construct these semantic word classes
by using the noun clustering algorithm proposed in
Kazama and Torisawa (2008). The algorithm fol-
lows the distributional hypothesis, which states that
semantically similar words tend to appear in simi-
lar contexts (Harris, 1954). By treating syntactic de-
pendency relations between words as ?contexts,? the
clustering method defines a probabilistic model of
noun-verb dependencies with hidden classes as:
p(n, v, r) =
?
c
p(n|c)p(?v, r?|c)p(c) (2)
Here, n is a noun, v is a verb or noun on which n de-
pends via a grammatical relation r (post-positions in
Japanese), and c is a hidden class. Dependency rela-
tion frequencies were obtained from our 600-million
page web corpus, and model parameters p(n|c),
p(?v, r?|c) and p(c) were estimated using the EM
algorithm (Hofmann, 1999). We successfully clus-
tered 5.5 million nouns into 500 classes. For each
noun n, EM clustering estimates a probability dis-
tribution over hidden variables representing seman-
tic classes. From this distribution we obtained dis-
crete semantic word classes by assigning each noun
n to semantic class c = argmaxc? p(c?|n). The
resulting classes actually form clean semantic cat-
egories such as chemicals, nutrients, diseases and
conditions, in our examples of Q1 and Q2. The fol-
lowing are the top-10 words (English translation) ac-
cording to p(c|n) for these classes.
chemicals: acetylene, hydrogenation product,
phosphoric monoester, methacrylate, levoglu-
cosan, ammonium salt, halogenated organic
compound, benzonitrile, alkyne, nitrosamine
371
nutrients: glucide, carbonhydrate, mineral, salt,
sugar, water, fat, vitamin, nutrients, protein
diseases: pneumonia, neuritis, cancer, oral leuko-
plakia, pachymeningitis, acidosis, encephalitis,
abdominal injury, valvulitis, gingivitis
conditions: proficiency, decrepitude, deficiency,
impurity, abnormalities, floated, crisis, dis-
placement, condition, shortage
Semantic word class (SWC) features are used to
capture associations between semantic classes of
words in the question and those in the answer candi-
dates. For example:
? Q2: Why does rickets (Wdisease) occur in chil-
dren?
? A2: Deficiency (Wcondition) of vitamin D
(Wnutrients) can cause rickets (Wdisease).
Wcondition, Wdisease and Wnutrients represent se-
mantic word classes of conditions, diseases and nu-
trients, respectively. If this question-answer pair is
given to the classifier as a positive training sample,
we expect it to learn that if a disease name appears
in a question then, everything else being equal, an-
swers including nutrient names are more likely to be
correct. Note that in principle the same association
could be learned between word pairs, i.e., rickets and
vitamin D. However, we found that word level asso-
ciations are often too specific, and because of data
sparseness this knowledge cannot easily be general-
ized to unseen questions. This is our main motiva-
tion for introducing broad coverage semantic word
classes into the feature set.
We call the feature set with the word classes SWC
and use two types of SWC, as shown in Table 1. To
obtain the first type (SWC1), we convert all nouns
in the MSA1 n-grams into their respective word
classes, and keep all n-grams that contain at least
one word class. We call these features word class
n-grams. Again, word class n-grams obtained from
questions are distinguished from the ones in answer
candidates. For example, we extract ?Wdisease oc-
cur? as a word class 2-gram from Q2.
The second type of SWC, SWC2, represents word
class n-grams in an answer candidate, in which
question terms are replaced by their respective se-
mantic word classes. For example, Wdisease in word
class 2-gram ?cause Wdisease? from A2 is the se-
mantic word class of rickets, one of the question
terms. These features capture the correspondence
between semantic word classes in the question and
answer candidates.
3.3 Sentiment Analysis
Sentiment analysis (SA) features are classified into
word-polarity and phrase-polarity features. We use
opinion extraction tool8 and sentiment orientation
lexicon in the tool for these features.
3.3.1 Opinion Extraction Tool
Opinion extraction tool is a software, the im-
plementation of Nakagawa et al2010). It ex-
tracts linguistic expressions representing opinions
(henceforth, we call them sentiment phrases) from
a Japanese sentence and then identifies the polarity
of these sentiment phrases using machine learning
techniques. For example, rickets occur in Q2 and
Deficiency of vitamin D can cause rickets in A2 can
be identified as sentiment phrases with a negative
polarity. The tool identifies sentiment phrases and
their polarity by using polarities of words and de-
pendency subtrees as evidence, where these polari-
ties are given in a word polarity dictionary.
In this paper, we use a trained model and a word
polarity dictionary (containing about 35,000 entries)
distributed via the ALAGIN forum9 for our sen-
timent analysis. Table 2 shows the performance
of opinion extraction tool, precision (P), recall (R)
and F-value (F), in this setting (reported in the
Japanese homepage of this tool). In the evaluation of
sentiment-phrase extraction, an extracted sentiment
phrase is determined as correct if its head word is
the same as one in the gold standard. Polarity clas-
sification is evaluated under the condition that all of
the sentiment phrases are correctly extracted.
P R F
Sentiment-phrase extraction 0.602 0.408 0.486
Polarity classification (pos.) 0.873 0.893 0.883
Polarity classification (neg.) 0.866 0.842 0.854
Table 2: The performance of opinion extraction tool
3.3.2 Word Polarity (SA@W)
Polarities of words are identified by simply look-
ing up the word polarity dictionary of opinion ex-
8 Available at http://alaginrc.nict.go.jp/opinion/index_e.html
9 http://www.alagin.jp/index-e.html. Only the members of
the ALAGIN forum can access these resources.
372
traction tool. Word polarity features are used
for identifying associations between the polarity of
words in a question and that in a correct answer. For
example:
? Q2: Why does rickets (W?) occur in children?
? A2: Deficiency (W?) of vitamin D can cause
rickets (W?).
Here, W? represents negative word polarities. We
expect our classifier to learn from this question and
answer pair that if a word with negative polarity ap-
pears in a question then its correct answer is likely
to contain a negative polarity word as well.
SA@W1 and SA@W2 in Table 1 are sentiment
analysis features from word polarity n-grams, which
contain at least one word that has word polarities.
We obtain these n-grams by converting all nouns in
MSA1 n-grams into their word polarities through
dictionary lookup. For example, from Q2 in the
above example we extract ?W? occur? as a word
polarity 2-gram. SA@W1 is concerned with all
word polarity n-grams in questions and answer can-
didates. For SA@W2, we restrict word polarity
n-grams from SA@W1 to those whose original n-
gram include a question term.
Furthermore, word polarities are coupled with se-
mantic word classes so that our classifier can iden-
tify meaningful combinations of both. For example,
deficiency in A2 can be represented asW?condition by
its respective semantic word class and word polar-
ity, which allows for the representation of undesir-
able conditions. This in turn lets our system learn
meaningful correlations between words expressing
these kind of negative conditions and their connec-
tion to questions asking about diseases. SA@W3
and SA@W4 are features from this combination.
They are defined in the same way as SA@W1 and
SA@W2 except that word polarities are replaced
with the combination of semantic word classes and
word polarities. We call n-grams in SA@W3 and
SA@W4 joint (word) class-polarity n-grams.
3.3.3 Phrase Polarity (SA@P)
Opinion extraction tool is applied to question and
its answer candidate to identify sentiment phrases
and their phrase-polarities. In preliminary tests we
found that sentiment phrases do not help to iden-
tify correct answers if answer sentences including
the sentiment phrases do not have any term from the
question. So we restrict the target sentiment phrases
to those acquired from sentences containing at least
one question term. From these sentiment phrases we
extract three categories of features.
First, SA@P1 and SA@P2 are features concerned
with phrase-polarity agreement between sentiment
phrases in a question and its answer candidate. We
consider all possible pairs of sentiment phrases from
the question and answer. If any such pair agrees
in phrase-polarity, an indicator for the agreement
and its polarity in the agreement become features
SA@P1 and SA@P2, respectively.
Secondly, following the original hypothesis un-
derlying this paper, we assume that sentiment
phrases often represent the core part of the cor-
rect answer (e.g., A2 above) and it is important
to express the content of the sentiment phrases in
features. SA@P3 and SA@P4 were devised for
this purpose. SA@P3 represents this sentiment
phrase contents as n-grams of morphemes, words,
and syntactic dependencies of sentiment phrases,
together with their phrase-polarity. Furthermore,
SA@P4 is the subset of SA@P3 n-grams restricted
to those that include terms found in the question,
and SA@P5 indicates the percentage of sentiment
n-grams from the question that are found in a given
answer candidate.
Finally, features SA@P6 through SA@P9 use se-
mantic word classes to generalize the content fea-
tures mentioned above. These features consist of
word class n-grams and joint class-polarity n-grams
taken from sentiment phrases, together with their
phrase polarity. Similar to the definition of SA@P4,
for SA@P7 and SA@P9 we restrict ourselves to n-
grams containing a question term. SA@P10 repre-
sents the semantic content of two sentiment phrases
with the same sentiment orientation (one from a
question and the other from an answer candidate)
using word class n-grams, together with the phrase-
polarity in agreement.
4 Test Set
We prepared three sets of why-questions (QS1, QS2
and QS3) and used these questions to build two test
sets for our experiments.
Why-questions in QS1 are taken from the
Japanese version of Yahoo! Answers (called Ya-
hoo! Chiebukuro)10. We automatically extracted
10 We used ?Yahoo! Chiebukuro Data (2nd edition)? which is
373
questions consisting of a single sentence and con-
taining the interrogative naze (why), and our anno-
tators verified that these questions are meaningful
without further context. For example, they discarded
questions like ?Why doesn?t the WBC (world box-
ing council) make an objection to the WBC (World
baseball classic)?? (the object of the objection is
unclear) and ?Why do minors trade at the auction
even though it is disallowed by the rules? (informa-
tion about which auction is not provided).
Because questions in Yahoo! Answers are aimed
at human readers, users often ?set the stage? by giv-
ing lots of background information about their ques-
tion. This often leads to large stylistic differences
between the questions in Yahoo! Answers and those
typically posed to a QA system. We therefore cre-
ated a second set of why-questions, QS2, whose
style should be more appropriate for a QA system
(examples showing these differences are given in the
supplementary materials of this paper). Six human
annotators (not the authors) were asked to create
why-questions in their own words, keeping in mind
that the questions they create are for a QA system. In
addition, the annotators were asked to verify on the
Web that the questions they created ask about some
real event or phenomena. For example, a question
like ?Why does Mars appear blue?? is disallowed in
QS2 because ?Mars appears blue? is false. Note that
the correct answer to these questions does not have
to be either in our target corpus or in real-world Web
texts. These two sets of why-questions, QS1 and
QS2, are used to build a test set for evaluating our
proposed method.
Finally, QS3 contains why-questions that have at
least one answer in our target corpus (600 million
Japanese Web page corpus). For creating such why-
questions, four human annotators (not the authors)
were given a text passage composed of three contin-
uous sentences and asked to locate the reasons for
some event as described in this passage. Then they
created a why-question for which the description is a
correct answer. Because randomly selected passages
from our target corpus have little chance of generat-
ing good why-questions we extracted passages from
our target corpus that include at least one of the clue
terms used in our answer retrieval step (i.e. riyuu
(reason), genin (cause), or youin (cause)). This set-
provided by Yahoo Japan Corporation and contains 16 million
questions asked from April, 2004 to April 2009.
ting may not necessarily reflect a ?real world? dis-
tribution of why-questions, in which ideally a wide
range of people ask questions that may or may not
have an answer in our corpus. However, QS3 al-
lows us to evaluate our method under the idealized
conditions where we have a perfect answer retrieval
module whose answer candidates always contain at
least one correct answer (the source passage used
for creating the why-question). This setting allows
us to estimate the ideal-case performance of our
method. Under these circumstances we found that
our method achieves almost 65% precision in P@1,
which suggests that it can potentially perform with
high precision if the answer candidates given by the
answer retrieval module contain at least one correct
answer. This is the main purpose of QS3. Addition-
ally, we use QS3 for building training data, to check
whether questions that do not reflect the real-world
distribution of why-questions are useful for improv-
ing the system?s performance on ?real-world? ques-
tions (see Section 5.1).
In addition, we checked QS1, QS2 and QS3 for
questions having the same topic, to avoid the pos-
sibility that the distribution of questions is biased
towards certain topics. We manually extracted the
questions? topic words and randomly selected a sin-
gle representative question from all questions with
the same topic. For example, ?Why does Twitter
only allow 140 characters?? and ?Why is Twitter
so popular?? both have as topic Twitter. In the end
we obtained 250 questions in QS1, 250 questions in
QS2 and 350 questions in QS3.
For evaluation we prepared two test sets, Set1 and
Set2. Set1 contains question-answer pairs whose
questions are taken from QS1 and QS2. In our ex-
periment, we evaluate systems with 10-fold cross
validation on Set1. Set2 has question-answer pairs
whose questions are from QS3. Set2 is mainly used
for estimating estimate the ideal-case performance
of our method with a perfect answer retrieval mod-
ule. Furthermore Set2 is used as additional training
data in evaluating systems with 10-fold cross vali-
dation on Set1. We used our answer retrieval sys-
tem to obtain the top-20 answer candidates for each
question, and all question-answer (candidate) pairs
were checked by three annotators, where their inter-
rater agreement (Fleiss? kappa) was 0.634, indicat-
ing substantial agreement. Finally, correct answers
to each question were determined by majority vote.
374
Q1:???????????????????????????????????????????
(Why does the increase of greenhouse gases such as carbon dioxide in the atmosphere lead to a rise of ocean level?)
A1: .. ????????????????????????????????????????????????????????
??????????????????????????????????????? ... ???????????????????
???????????????????????
(The burning of fossil fuels contributes to the increase of atmospheric concentrations of greenhouse gases and this makes the atmosphere absorb more
thermal radiation. As a result, Earth?s average surface temperature increases. This is global warming. ... There are warnings that the increase of sea
water and melting of polar ice due to the global warming may cause sea-surface height to rise by 9?88 cm on average.
Q2:?????????????????????????????
(Why does hemoglobin deficiency cause lack of oxygen in the human body?)
A2:... ????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????..
(... Hemoglobin has an important role in the human body of carrying oxygen to the organs and transferring carbon dioxide back to the lungs, to be
dispensed from the organism. If the amount of hemoglobin produced by the body is insufficient due to iron deficiency, the amount of oxygen delivered
throughout the body decreases, causing oxygen deficiency. ... )
Table 3: Correct question-answer pairs in our test set
Table 3 shows a sample of correct question-answer
pairs in our test set. Please see the supplementary
materials of this paper for more examples.
Note that word and phrase polarities are not con-
sidered by the annotators in building our test sets
and these polarities are automatically identified us-
ing a word polarity dictionary and opinion extraction
tool. We confirmed that about 35% of questions and
40% of answer candidates had at least one sentiment
phrase by opinion extraction tool, and about 45% of
questions and 85% of answer candidates contained
at least one word having polarity by a word polarity
dictionary.
5 Experiments
We use TinySVM11 with a linear kernel for training
our re-ranker. Evaluation was done by P@1 (Pre-
cision of the top answer) and MAP (Mean Average
Precision). P@1 measures how many questions have
a correct top answer candidate. MAP, widely used in
evaluation of IR systems, measures the overall qual-
ity of the top-n answer candidates (n=20 in this ex-
periment) using the formula:
MAP =
1
|Q|
?
q?Q
?n
k=1(Prec(k)? rel(k))
|Aq|
(3)
Here Q is a set of why-questions, Aq is a set of cor-
rect answers to why-question q ? Q, Prec(k) is the
precision at cut-off k in the top-n answer candidates,
rel(k) is an indicator, 1 if the item at rank k is a cor-
rect answer in Aq, 0 otherwise.
We evaluated all systems using 10-fold cross val-
idation in two ways. In the first setting we per-
formed 10-fold cross validation on Set1. Set1 con-
11 http://chasen.org/?taku/software/TinySVM/
sists of 10,000 question-answer pairs (500 questions
with their 20 answer candidates), and was parti-
tioned into 10 subsamples such that the questions
in one subsample do not overlap with those of the
other subsamples. 9 subsamples (9,000 question-
answer pairs) were used as training data and the
remaining subsample (1,000 question-answer pairs)
was retained as test data. This experiment is called
CV(Set1). It shows the effect of answer re-ranking
when evaluating our proposed method with train-
ing data built with real world why-questions alone.
In the second setting, we used the same 10 sub-
samples of Set1 as in CV(Set1) and exploited Set2
(composed of 7,000 question-answer pairs) as ad-
ditional training data for 10-fold cross validation.
As a result, in each fold 16,000 question-answer
pairs (9,000 from Set1 and 7,000 from Set2) were
used as training data for re-rankers, and all systems
were evaluated on the remaining 1,000 question-
answer pair subsample from Set1. We call this set-
ting CV(Set1+Set2). It verifies whether training
data that does not necessarily reflect a real-world
distribution of why-questions can improve why-QA
performance on real-world questions.
5.1 Results
Table 4 shows the evaluation results of six different
systems. For each system, we represent the perfor-
mance in P@1 and MAP. B-QA is a system of our
answer retrieval and the other five re-rank top-20 an-
swer candidates using their own re-ranker.
B-QA: our answer retrieval system, our implemen-
tation of Murata et al2007).
B-Ranker: a system that has a re-ranker trained
with morphological and syntactic analysis
(MSA) features alone.
375
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
B-QA 0.222 (0.368) 0.270 (0.447) 0.222 (0.368) 0.270 (0.447)
B-Ranker 0.256 (0.424) 0.319 (0.528) 0.274 (0.454) 0.323 (0.535)
B-Ranker+CR 0.262 (0.434) 0.319 (0.528) 0.278 (0.460) 0.325 (0.538)
B-Ranker+WN 0.257 (0.425) 0.320 (0.530) 0.275 (0.455) 0.325 (0.538)
Proposed 0.336 (0.56) 0.377 (0.624) 0.374 (0.619) 0.391 (0.647)
UpperBound 0.604 (1) 0.604 (1) 0.604 (1) 0.604 (1)
Table 4: Comparison of systems
B-Ranker+CR: a system has a re-ranker trained
with our MSA features and the causal relation
(CR) features used in Higashinaka and Isozaki
(2008). The CR features include binary fea-
tures indicating whether an answer candidate
contains a causal relation pattern, which causal
relation pattern the answer candidate has, and
whether the question-answer pair contains a
causal relation instance ? cause in the answer,
effect in the question). We acquired causal
relation instances from our target corpus us-
ing the method from (De Saeger et al2009),
and exploited the top-100,000 causal relation
instances and the patterns that extracted them
for CR features. Note that these CR features
are introduced only for comparing our semantic
features with ones in Higashinaka and Isozaki
(2008) and they are not a part of our method.
B-Ranker+WN: its re-ranker is trained with our
MSA features and the WordNet features in Ver-
berne et al2010). The WordNet features in-
clude the percentage of the question terms and
their synonyms in WordNet synsets found in
an answer candidate and the semantic related-
ness score between a question and its answer
candidate, the average of the concept similar-
ity between each question term and all of the
answer terms by WordNet::Similarity (Peder-
sen et al2004). We used the Japanese Word-
Net 1.1 (Bond et al2009) for these WordNet
features. Note that the Japanese WordNet 1.1
has 93,834 Japanese words linked to 57,238
WordNet synsets, while the English WordNet
3.0 covers 155,287 words linked to 117,659
synsets. Due to this lower coverage, the Word-
Net features in Japanese may have a less power
for finding a correct answer than those in En-
glish used in Verberne et al2010).
Proposed: our proposed method. All of the MSA,
SWC and SA features are used for training our
re-ranker.
UpperBound: a system that ranks all n correct an-
swers as the top n results of the 20 answer can-
didates if there are any. This indicates the per-
formance upperbound in this experiment. The
relative performance of each system compared
to UpperBound is shown in parentheses.
The proposed method achieved the best perfor-
mance both in CV(Set1) and CV(Set1+Set2). Our
method shows a significant improvement (11.4?
15.2% in P@1 and 10.7?12.1% in MAP) over our
answer retrieval method, B-QA. Its improvement
over B-Ranker, B-Ranker+CR and B-Ranker+WN
(7.6?10% in P@1 and 5.7?6.6% in MAP) shows
the effectiveness of our proposed feature set over
the features used in previous works. Both B-
Ranker+CR and B-Ranker+WN did not show signif-
icant performance improvement over B-Ranker. At
least in our setting, the causal relation and WordNet
features did not prove effective. The performance
gap between B-Ranker and B-QA (3.4?5.2% in P@1
and 4.9?5.3% in MAP) suggests the effectiveness
of re-ranking. All systems consistently show better
performance in CV(Set1+Set2) than CV(Set1). This
suggests that training data built with why-questions
that does not reflect real-world distribution of why-
questions is useful in training re-rankers.
We investigate the contribution of each type of
features to the performance by removing one fea-
ture set from the all feature sets in training our re-
ranker. In this experiment, we split SA into SA@W
(features expressing words and their polarity) and
SA@P (features expressing phrases and their po-
larity) to investigate their contribution either. The
results are summarized in Table 5.
In Table 5, MSA+SWC+SA represents our pro-
posed method using all feature sets. The perfor-
mance gap between MSA+SWC+SA and the others
confirms that all the features contributed to a higher
376
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
SWC+SA 0.302 0.324 0.314 0.332
MSA+SWC 0.308 0.349 0.318 0.358
MSA+SA 0.300 0.352 0.314 0.364
MSA+SWC+SA@W 0.312 0.358 0.325 0.365
MSA+SWC+SA@P 0.323 0.369 0.358 0.384
MSA+SWC+SA 0.336 0.377 0.374 0.391
UpperBound 0.604 0.604 0.604 0.604
Table 5: Evaluation with different combination of feature
sets used in training our re-ranker
performance. The significant performance improve-
ment by SA (features from sentiment analysis) and
SWC (features from semantic word classes) (The
gap between MSA+SWC+SA and MSA+SWC was
2.8?6% and that between MSA+SWC+SA and
MSA+SA was 3.6%?6% in P@1) supports the hy-
pothesis for sentiment analysis and semantic word
classes in this paper.
Though the performance gap between
MSA+SWC+SA and MSA+SWC+SA@P
(1.3%?1.6% in P@1) shows that SA@W is
useful in training our re-ranker, we found that
MSA+SWC+SA@W made only 0.4?0.7% im-
provement over MSA+SWC. We believe that this
is mainly because SA@W and SWC are based on
semantic and sentiment information at the word
level, and these often capture a similar type of
information. For instance, disease names that are
grouped together into one class in SWC are typi-
cally classified as negative in SA@W. Therefore the
similarity in the information provided by SA@W
and SWC causes a classifier trained with both of
these features to obtain only a minor improvement
over a classifier using only one of the features.
To estimate the ideal-case performance of our
proposed method, we made another experiment by
using Set1 as training data for our re-ranker and
Set2 as test data for evaluating our proposed method.
Here, we assume a perfect answer retrieval module
that adds the source passage that was used for gener-
ating the original why-question in Set2 as a correct
answer to the set of existing answer candidates, giv-
ing 21 answer candidates. The performance of our
method in this setting was 64.8% in P@1 and 66.6%
in MAP. This evaluation result suggests that our re-
ranker can potentially perform with high precision
when at least one correct answer in answer candi-
dates is given by the answer retrieval module.
6 Related Work
In the QA literature, Higashinaka and Isozaki
(2008), Verberne et al2010), and Surdeanu et al
(2011) are closest to our work. The first two deal
with why-questions, the last with how-questions.
Similar to our method, they use machine learn-
ing techniques to re-rank answer candidates to non-
factoid questions based on various combinations of
syntactic, semantic and other statistical features such
as the density and frequency of question terms in the
answer candidates and patterns for causal relations
in the answer candidates. Especially for why-QA,
Higashinaka and Isozaki (2008) used causal relation
features and Verberne et al2010) exploited Word-
Net features as a kind of semantic features for train-
ing their re-ranker, where we used these features, re-
spectively, for B-Ranker+CR and B-Ranker+WN in
our experiment.
Our work differs from the above approaches in
that we propose semantic word classes and senti-
ment analysis as a new type of semantic features,
and show their usefulness in why-QA. Sentiment
analysis has been used before on the slightly un-
usual task of opinion question answering, where the
system is asked to answer subjective opinion ques-
tions (Stoyanov et al2005; Dang, 2008; Li et al
2009). To the best of our knowledge though, no pre-
vious work has systematically explored the use of
sentiment analysis in a general QA setting beyond
opinion questions.
7 Conclusion
In this paper, we have explored the utility of senti-
ment analysis and semantic word classes for ranking
answer candidates to why-questions. We proposed a
set of semantic features that exploit sentiment anal-
ysis and semantic word classes obtained from large-
scale noun clustering, and used them to train an an-
swer candidate re-ranker. Through a series of exper-
iments on 850 why-questions, we showed that the
proposed semantic features were effective in identi-
fying correct answers, and our proposed method ob-
tained more than 15% improvement in precision of
its top answer (P@1) over our baseline, a state-of-
the-art IR based QA system. We plan to use new se-
mantic knowledge such as semantic orientation, ex-
citatory or inhibitory, proposed in Hashimoto et al
(2012) for improving why-QA.
377
References
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka
Uchimoto, Takayuki Kuribayashi, and Kyoko Kan-
zaki. 2009. Enhancing the japanese wordnet. In Pro-
ceedings of the 7th Workshop on Asian Language Re-
sources, pages 1?8.
Hoa Tran Dang. 2008. Overview of the TAC 2008 opin-
ion question answering and summarization tasks. In
Proc. TAC 2008.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large scale
relation acquisition using class dependent patterns. In
Proc. of ICDM 2009, pages 764?769.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building Watson: An overview of the DeepQA
project. AI Magazine, 31(3):59?79.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An overview of the 4th ques-
tion answering challenge (QAC-4) at NTCIR work-
shop 6. In Proc. of NTCIR-6.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, and Jun?ichi Kazama. 2012. Excita-
tory or inhibitory: A new semantic orientation extracts
contradiction and causality from the web. In Proceed-
ings of EMNLP-CoNLL 2012.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-questions.
In Proc. of IJCNLP, pages 418?425.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proc. of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?99, pages 50?57.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL-
08: HLT, pages 407?415.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, pages
737?745.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A system
for answering non-factoid Japanese questions by using
passage retrieval weighted based on type of answer. In
Proc. of NTCIR-6.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 786?794, Los An-
geles, California, June. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41.
Anselmo Pe?as, Eduard H. Hovy, Pamela Forner, ?lvaro
Rodrigo, Richard F. E. Sutcliffe, Corina Forascu, and
Caroline Sporleder. 2011. Overview of QA4MRE at
CLEF 2011: Question answering for machine reading
evaluation. In CLEF.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 923?
930.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 417?424.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Evaluating discourse-based an-
swer extraction for why-question answering. In SIGIR,
pages 735?736.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2010. What is not in the bag of words
for why-QA? Computational Linguistics, 36:229?
245.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
378
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 619?630, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Excitatory or Inhibitory: A New Semantic Orientation Extracts
Contradiction and Causality from the Web
Chikara Hashimoto? Kentaro Torisawa? Stijn De Saeger?
Jong-Hoon Oh? Jun?ichi Kazama?
National Institute of Information and Communications Technology
Kyoto, 619-0289, JAPAN
{? ch, ? torisawa, ? stijn, ? rovellia, ?kazama}@nict.go.jp
Abstract
We propose a new semantic orientation, Ex-
citation, and its automatic acquisition method.
Excitation is a semantic property of predicates
that classifies them into excitatory, inhibitory
and neutral. We show that Excitation is useful
for extracting contradiction pairs (e.g., destroy
cancer ? develop cancer) and causality pairs
(e.g., increase in crime ? heighten anxiety).
Our experiments show that with automatically
acquired Excitation knowledge we can extract
one million contradiction pairs and 500,000
causality pairs with about 70% precision from
a 600 million page Web corpus. Furthermore,
by combining these extracted causality and
contradiction pairs, we can generate one mil-
lion plausible causality hypotheses that are not
written in any single sentence in our corpus
with reasonable precision.
1 Introduction
Recognizing semantic relations between events in
texts is crucial for such NLP tasks as question an-
swering (QA). For example, to answer the question
?What ruined the crops in Japan?? a QA system
must recognize that the sentence ?the Fukushima
nuclear power plant caused radioactive pollution
and contaminated the crops in Japan? contains a
causal relation and that contaminate crops entails
ruin crops but contradicts preserve crops.
To facilitate the acquisition of causality, contra-
diction, paraphrase and entailment relations between
events we propose a new semantic orientation, Ex-
citation, that classifies unary predicates (templates,
hereafter) into excitatory, inhibitory and neutral. An
excitatory template entails that the main function or
effect of the referent of its argument is activated or
enhanced (e.g., cause X, preserve X), while an in-
hibitory template entails that it is deactivated or sup-
pressed (e.g., ruin X, contaminate X, prevent X).
Excitation is useful for extracting contradiction;
if two templates with similar distributional profiles
have opposite Excitation polarities, they tend to be
contradictions (e.g., contaminate crops and preserve
crops). With extracted contradictions we can distin-
guish paraphrases from contradictions among distri-
butionally similar phrases. Furthermore, contradic-
tion in itself is important knowledge for Recogniz-
ing Textual Entailment (RTE) (Voorhees, 2008).
Excitation is also a powerful indicator of causal-
ity. In the physical world, the activation or de-
activation of one thing often causes the activation
or deactivation of another. Two excitatory or in-
hibitory templates that co-occur in some temporal
or logical order in the same narrative often describe
a causal chain of events, like ?the Fukushima nu-
clear power plant caused radioactive pollution and
contaminated crops in Japan?.
In this paper we propose both the concept of Ex-
citation and an automatic method for its acquisition.
Our method acquires Excitation templates based on
certain natural, language independent constraints on
narrative structures found in text. We also propose
acquisition methods for contradiction and causal-
ity relations based on Excitation. Our methods ex-
tract one million contradiction pairs with over 70%
precision, and 500,000 causality pairs with about
70% precision from a 600 million page Web corpus.
Moreover, by combining these extracted causality
pairs and contradiction pairs, we generated one mil-
lion plausible causality hypotheses that were not
619
written in any single sentence in our corpus with rea-
sonable precision. For example, a causality hypoth-
esis prevent radioactive pollution ? preserve crops
can be generated from an extracted causality cause
radioactive pollution ? contaminate crops.
We target the Japanese language in this paper.
2 What is Excitation?
Excitation classifies templates into excitatory, in-
hibitory, and neutral, as explained below.
excitatory templates entail that the function, ef-
fect, purpose or role of their argument?s refer-
ent is activated or enhanced. (e.g., cause X, buy
X, produce X, import X, increase X, enable X)
inhibitory templates entail that the function, ef-
fect, purpose or role of their argument?s refer-
ent is deactivated or suppressed. (e.g., prevent
X, discard X, remedy X, decrease X, disable X)
neutral templates are neither excitatory nor in-
hibitory. (e.g., consider X, proportional to X,
related to X, evaluate X, close to X)
For example, when fire fills the X slot of cause X,
it suggests that the effect of fire is activated. If pre-
vent X?s slot is filled with flu, the effect of flu is sup-
pressed. In this study, we aim to acquire excitatory
and inhibitory templates that are useful for extract-
ing contradiction and causality, though neutral tem-
plates are the most frequent in our data (See Section
5.1). Collectively we call excitatory and inhibitory
templates Excitation templates, and excitatory and
inhibitory two opposite polarities.
Excitation is independent of the good/bad seman-
tic orientation. (Hatzivassiloglou and McKeown,
1997; Turney, 2002; Rao and Ravichandran, 2009).
For example, sophisticate X and complicate X are
both excitatory, but only the former has a positive
connotation. Similarly, remedy X and degrade X are
both inhibitory but only the latter is negative.
General Inquirer (Stone et al1966) deals with
semantic factors some of which were proposed by
Osgood et al1957). Their ?activity? factor involves
binary opposition between ?active? and ?passive.?
Notice that activity and Excitation are independent.
In General Inquirer, both accelerate X and abolish
X are active, but only the former is excitatory. Both
accept X and abate X are passive, but only the lat-
ter is inhibitory. Pustejovsky (1995) proposed telic
and agentive roles, which inspired our excitatory no-
tion, but they have no corresponding notion of in-
hibitory. Andreevskaia and Bergler (2006) acquired
the increase/decrease semantic orientation, which is
a subclass of Excitation.
Excitation is inverted if a template?s predicate is
negated. For example, preserve X is excitatory,
while don?t preserve X is inhibitory. We acknowl-
edge that this may seem somewhat counter-intuitive
and will address this issue in future work.
3 Excitation Template Acquisition
This section presents our acquisition method of Ex-
citation templates. We introduce constraints in the
co-occurrence of templates in text that seem both ro-
bust and language independent in Section 3.1. Our
method exploits these constraints for the acquisition
of Excitation templates. First we construct a tem-
plate network where nodes are templates and links
represent that two connected templates have either
SAME or OPPOSITE polarities. Given 46 manually
prepared seed templates we calculate the Excitation
value of each template, a value in range [?1, 1] that
is positive if the template is excitatory and negative
if it is inhibitory. Technically, our method treats all
templates as excitatory or inhibitory, and, upon com-
pletion, regards templates with small absolute Exci-
tation values as neutral.
The whole method is a bootstrapping process.
Each iteration expands the network and the Excita-
tion value of each template is (re-)calculated.
3.1 Characteristics of Excitation Templates
Our method exploits natural discourse constraints on
the possible combinations of (a) the polarity of co-
occurring templates, (b) the nouns that fill their ar-
gument slots and (c) the connectives that link the
templates in a given sentence. Table 1 shows the
constraints and Figure 1 shows examples that will
be explained shortly. Though our target is Japanese
we believe these constraints are universal discourse
principles, and as such not language dependent. Ex-
amples are given in English for ease of explanation.
We first identify two categories of connectives
in our target sentences: AND/THUS-type (e.g., and,
thus and since) and BUT-type (e.g., but and though).
Both types suggest a sort of consistency or inconsis-
tency between predicates. We manually classified
169 frequently used connectives into AND/THUS-
620
(1) He smoked cigarettes, AND/THUS he suffered lung
cancer. (Both smoke X and suffer X are excitatory.)
(2) He quit cigarettes, AND/THUS was immune from lung
cancer. (quit X and immune from X are inhibitory.)
(3) He smoked cigarettes, BUT didn?t suffer lung cancer.
(smoke X is excitatory, not suffer X is inhibitory.)
(4) He quit cigarettes, BUT he suffered lung cancer. (quit
X is inhibitory, but suffer X is excitatory.)
(5) He underwent cancer treatment, AND/THUS he could
cure the cancer. (undergo X is excitatory, cure X is
inhibitory.)
(6) He underwent cancer treatment, BUT still had cancer.
(Both undergo X and have X are excitatory.)
(7) Unnatural: He smoked cigarettes, BUT he suffered
lung cancer. (smoke X and suffer X are excitatory.)
Figure 1: Examples of constraints: (cigarettes, lung can-
cer) is PNP and (cancer treatment, cancer) is NNP.
PNPs NNPs others
AND/THUS SAME OPPOSITE N/A
BUT OPPOSITE SAME N/A
Table 1: Constraint matrix.
and BUT-type (See supplementary materials).
Next we extract sentences from the Web in which
two templates co-occur and are joined by one of
these connectives, and then classify the noun pairs
filling the templates? argument slots into ?positively-
associated? and ?negatively-associated? noun pairs
(PNPs and NNPs). Mirroring our definition of Excita-
tion, PNPs are noun pairs in which the referent of the
first noun facilitates the emergence of the referent
of the second noun. PNPs can range from causally
related noun pairs like (cigarettes, lung cancer) to
?material-product? relation pairs like (semiconduc-
tor, electronic circuit). We found that PNPs only
fill the argument slots of (a) same Excitation polar-
ity templates connected by AND/THUS-type connec-
tives (examples 1 and 2 in Figure 1), or (b) opposite
Excitation polarity templates connected by a BUT-
type connectives (examples 3 and 4). Violating such
constraints (example 7) seems unnatural. Similarly,
NNPs are noun pairs in which the referent of one
noun suppresses the emergence of the referent of the
other noun. Examples include such ?inverse causal-
ity? pairs as (cancer treatment, cancer). NNPs only
fill the argument slots of (a) opposite Excitation po-
larity templates connected by AND/THUS-type con-
nectives (example 5), or (b) same polarity templates
connected by a BUT-type connective (example 6).
All these constraints are summarized in Table 1,
which we will call the constraint matrix. Accord-
ing to the constraint matrix, we can know whether
two templates? polarities are the same or opposite if
we know whether a noun pair filling the two tem-
plates? slots is PNP or NNP. Conversely, we can
know whether a noun pair is PNP or NNP if we know
whether two templates whose slots are filled with
the noun pair have the same or opposite polarities.
We believe these constraints capture certain univer-
sal principles of discourse, since it is difficult in any
language to produce natural sounding sentences that
violate these constraints. We empirically confirm
their validity for Japanese in Section 5.1.
3.2 Bootstrapping Approach to Excitation
Template Acquisition
To calculate the Excitation values for the templates,
we construct a template network where templates
are connected by links indicating polarity agreement
between two connected templates (either SAME or
OPPOSITE polarity), as determined by the constraint
matrix. Excitation values are determined by spread-
ing activation applied to the network, given a small
number of manually prepared seed templates.
However, we cannot construct the network unless
we know whether each noun pair is PNP or NNP, due
to the configuration of the constraint matrix, and cur-
rently we have no feasible method to classify all of
them into PNPs and NNPs in advance. We therefore
adopt a bootstrapping method (Figure 2) that starts
from manually prepared excitatory and inhibitory
seed templates (Step 1 in Figure 2). Our method
begins by extracting noun pairs from the Web that
co-occur with two seed templates connected by a
AND/THUS- or BUT-type connective, and classifies
these noun pairs into PNPs and NNPs based on the
constraint matrix (Steps 2 and 3). Next, we automat-
ically extract additional (non-seed) template pairs
from the Web that co-occur with these PNPs and
NNPs. Links (either SAME or OPPOSITE) between
all template pairs are determined by the constraint
matrix (Step 4), and we construct a template network
from both seed and non-seed template pairs (Step 5).
Our method calculates the Excitation values for
all the templates in the network by first assign-
ing Excitation values +1 and ?1 to the excitatory
and inhibitory seed templates, and applies a spread-
ing activation method proposed by Takamura et al
(2005) (Step 6) to the network. This method calcu-
621
1. Prepare initial seed templates with fixed excitation values (either
+1 or ?1).
2. Make seed template pairs that are combinations of two seed tem-
plates and a connective (either AND/THUS-type or BUT-type).
3. Extract noun pairs that co-occur with one of the seed template
pairs from the Web. Classify the noun pairs into PNPs and NNPs
based on the constraints matrix. Filter out those noun pairs that
appear as both PNP and NNP on the Web or those whose occur-
rence frequency is less than or equal to F, which is set to 5.
4. Extract additional (non-seed) template pairs that are filled by one
of the PNPs or NNPs from the Web. Determine the link type
(SAME or OPPOSITE) for each template pair based on the con-
straint matrix. If a template pair appears on the Web as having
both link types, we determine its link type by majority vote.
5. Construct the template network from all the template pairs. Re-
move from the network those templates whose number of linked
templates is less than D, which is set to 5.
6. Apply Takamura et al method to the network and fix the Exci-
tation value of each template.
7. Extract the top- and bottom-ranked N ? i templates from the
result of Takamura et al method. N is a constant, which is
set to 30. i is the iteration number. They are used as additional
seed templates for the next iteration. The top-ranked templates
are given Excitation value +1 and the bottom-ranked templates
are assigned ?1. Go to Step 2.
Figure 2: Bootstrapping for template acquisition.
lates all templates? excitation values by solving the
network constraints imposed by the SAME and OP-
POSITE links, and the Excitation values of the seed
templates (This method is detailed in Section 3.3).
In each iteration i, our method selects the N ? i top-
ranked and bottom-ranked templates as additional
seed templates for the next iteration (N is set to 30)
(Step 7). Our method then constructs a new tem-
plate network using the augmented seed templates
and restarts the calculation process. Figure 2 sum-
marizes our bootstrapping process.
Bootstrapping stops after M iterations, with M
set to 7 based on our preliminary experiments.
To prepare the initial seed templates we con-
structed a maximal template network that could in
theory be created by our bootstrapping method. This
maximal network consists of any two templates that
co-occur in a sentence with any connective, regard-
less of their arguments. We manually selected 36
excitatory and 10 inhibitory seed templates from
among 114 templates with the most links in the net-
work (See supplementary materials).
3.3 Determining Excitation in the Network
This section details Step 6 of our bootstrapping
method, i.e., how Takamura et al method calcu-
lates the Excitation value of each template. Their
method is based on the spin model in physics, where
each electron has a spin of either up or down. We
chose this method due to the straightforward parallel
between the spin model and our Excitation template
model. Both models capture the spreading of acti-
vation (either spin direction or excitation polarity)
between neighboring objects in a network. Deter-
mining the optimal algorithm for this task is beyond
our current scope, but for the purpose of our experi-
ments we found that Takamura et al method gave
satisfactory results.
The spin model defines an energy function on a
spin network, and each electron?s spin can be esti-
mated by minimizing this function:
E(x,W ) = ?1/2? ?ijwijxixj
Here, xi, xj ? x are spins of electrons i and j, and
matrix W = {wij} assigns weights to links between
electrons. We regard templates as electrons and Ex-
citation polarities as their spins (up and down corre-
spond to excitatory and inhibitory). We define the
weight wij of the link between templates i and j as:
wij =
{
1/
?
d(i)d(j) if SAME(i, j)
?1/
?
d(i)d(j) if OPPOSITE(i, j)
Here, d(i) denotes the number of templates linked
to i. SAME(i, j) (OPPOSITE(i, j)) indicates a SAME
(OPPOSITE) link exists between i and j. We obtain
excitation values by minimizing the above energy
function. Note that after minimizing E, xi and xj
tend to get the same polarity when wij is positive.
When wij is negative, xi and xj tend to have op-
posite polarities. Initially seed templates are given
values +1 or ?1 depending on whether they are ex-
citatory or inhibitory, and others are given 0.
We used SUPPIN (http://www.lr.pi.titech.
ac.jp/?takamura/pubs/SUPPIN-0.01.tar.gz),
an implementation of Takamura et al method. Its
parameter ? is set to the default value (0.75).
4 Knowledge Acquisition by Excitation
This section shows how the concept of Excitation
can be used for automatic knowledge acquisition.
4.1 Contradiction Extraction
Our first knowledge acquisition method extracts
contradiction pairs like destroy cancer ? develop
cancer, based on our assumption that they often con-
sist of distributionally similar templates that have a
sharp contrast in Excitation value. Concretely, we
622
extract two phrases as a contradiction pair if (a)
their templates have opposite Excitation polarities,
(b) they share the same argument noun, and (c) the
part-of-speech of their predicates is the same. Then
the contradiction pairs are ranked by Ct:
Ct(p1, p2) = |s1| ? |s2| ? sim(t1, t2)
Here p1 and p2 are two phrases that satisfy condi-
tions (a), (b) and (c) above, t1 and t2 are their re-
spective templates, and |s1| and |s2| are the absolute
values of t1 and t2?s excitation values. sim(t1, t2) is
the distributional similarity proposed by Lin (1998).
Note that ?contradiction? here includes what we
call ?quasi-contradiction.? This consists of two
phrases such that, if the tendencies of the events they
describe get stronger, they eventually become con-
tradictions. For example, the pair emit smells ? re-
duce smells is not logically contradictory since the
two events can happen at the same time. However,
they become almost contradictory when their ten-
dencies get stronger (i.e., emit smells more strongly
? thoroughly reduce smells). We believe quasi-
contradictions are useful for NLP tasks.
4.2 Causality Extraction
Our second knowledge acquisition method extracts
causality pairs like increase in crime ? heighten
anxiety that co-occur with AND/THUS-type connec-
tives in a sentence. The assumption is that if two
templates (t1 and t2) with a strong Excitation ten-
dency are connected by an AND/THUS-type connec-
tive in a sentence, the event described by t1 and its
argument n1 tends to be a cause of the event de-
scribed by t2 and its argument n2. Here, Excitation
strength is expressed by absolute Excitation values.
The intuition is that, if the referent of n1 is strongly
activated or suppressed, it tends to have some causal
effect on the referent of n2 in the same sentence.
We focus on extracting causality pairs that co-
occur with only ?non-causal connectives? like and,
which are AND/THUS-type connectives that do NOT
explicitly signal causality, since causal connectives
like thus can mask the effectiveness of Excitation.
We prepared 139 non-causal connectives (See sup-
plementary materials). We extract two templates
such as increase in X and heighten Y co-occurring
with only non-causal connectives, as well as the
noun pair that fills the two templates? slots (e.g.,
(crime, anxiety)) to obtain causal phrase pairs. In
Japanese, the temporal order between events is usu-
ally determined by precedence in the sentence. Cs
ranks the obtained causality pairs:
Cs(p1, p2) = |s1| ? |s2|
Here p1 and p2 are the phrases of causality pair, and
|s1| and |s2| are absolute Excitation values of p1?s
and p2?s templates. As is common in the literature,
this notion of causality should be interpreted prob-
abilistically rather than logically, i.e., we interpret
causality A ? B as ?if A happens, the probability of
B increases?. This interpretation is often more use-
ful for NLP tasks than a strict logical interpretation.
4.3 Causality Hypothesis Generation
Our third knowledge acquisition method generates
plausible causality hypotheses that are not written in
any single sentence using the previously extracted
contradiction and causality pairs. We assume that if
a causal relation (e.g., increase in crime ? heighten
anxiety ) is valid, its inverse (e.g., decrease in crime
? diminish anxiety ) is often valid as well. From
a logical definition of causation, taking the inverse
of an implication obviously does not preserve valid-
ity. However, at least under our probabilistic inter-
pretation, taking the inverse of a given causality pair
using the extracted contradiction pairs proves to be
a viable strategy for generating non-trivial causality
hypotheses, as our experiments in Section 5.4 show.
For an extracted causality pair, we generate its
inverse as a causality hypothesis by replacing both
phrases in the original pair with their contradiction
counterparts. For instance, a causality hypothesis
decrease in crime ? diminish anxiety is generated
from a causality increase in crime ? heighten anxi-
ety by two contradictions, decrease in crime ? in-
crease in crime and diminish anxiety ? heighten
anxiety. Since we are interested in finding new
causal hypotheses, we filter out hypotheses whose
phrase pair co-occurs in a sentence in our corpus.
Remaining causality hypotheses are ranked by Hp.
Hp(q1, q2) = Ct(p1, q1)? Ct(p2, q2)? Cs?(p1, p2)
Here, q1 and q2 are two phrases of a causality hy-
pothesis. p1 and p2 are two phrases of a hypothesis?s
original causality. That is, p1 ? q1 and p2 ? q2 are
contradiction pairs, and Ct(p1, q1) and Ct(p2, q2)
are their contradiction scores. Cs?(p1, p2) is the
original causality?s causality score. Cs? can be Cs
623
from Section 4.2, but based on preliminary experi-
ments we found the following score works better:
Cs?(p1, p2) = |s1| ? |s2| ? npfreq(n1, n2)
|s1| and |s2| are absolute Excitation values of p1?s
and p2?s templates, whose slots are filled with n1 and
n2. npfreq(n1, n2) is the co-occurrence frequency
of (n1, n2) with polarity-identical template pairs (if
(n1, n2) is PNP) or with polarity-opposite template
pairs (if (n1, n2) is NNP). Thus, npfreq indicates a
sort of association strength between two nouns.
5 Experiments
This section shows that our template acquisition
method acquired many Excitation templates. More-
over, using only the acquired templates we extracted
one million contradiction pairs with more than 70%
precision, and 500,000 causality pairs with about
70% precision. Further, using only these extracted
contradiction and causality pairs we generated one
million causality hypotheses with 57% precision.
In our experiments we removed evaluation sam-
ples containing the initial seed templates and exam-
ples used for annotation instruction from the evalua-
tion data. Three annotators (not the authors) marked
all evaluation samples, which were randomly shuf-
fled so that they could not identify which sample was
produced by which method. Information about the
predicted labels or ranks was also removed from the
evaluation data. Final judgments were made by ma-
jority vote between the annotators. They were non-
experts without formal training in linguistics or se-
mantics. See supplementary materials for our anno-
tation manuals (translated into English).
We used 600 million Japanese Web pages
(Akamine et al2010) parsed by KNP (Kawahara
and Kurohashi, 2006) as a corpus. We restricted
the argument positions of templates to ha (topic),
ga (nominative), wo (accusative), ni (dative), and de
(instrumental). We discarded templates appearing
fewer than 20 times in compound sentences (regard-
less of connectives) in our corpus.
5.1 Excitation Template Acquisition
We show that our proposed method for template ex-
traction (PROPtmp) successfully acquired many Ex-
citation templates from which we obtained a huge
number of contradiction and causality pairs, and that
Excitation is a reasonably comprehensible notion
even for non-experts. We also show that PROPtmp
outperformed two baselines by a large margin.
The template network constructed by PROPtmp
contained 10,825 templates. Among these, the boot-
strapping process classified 8,685 templates as exci-
tatory and 2,140 as inhibitory. Note that these can-
didates in fact also contain neutral templates, as ex-
plained at the beginning of Section 3.
Baselines The baseline methods are ALLEXC and
SIM. ALLEXC regards all templates that are ran-
domly extracted from the Web as excitatory, since in
our data excitatory templates outnumber inhibitory
ones. Actually, in our data neutral templates rep-
resent the most frequent class, but since our objec-
tive is to acquire excitatory and inhibitory templates,
a baseline marking all templates as neutral would
make little sense. SIM is a distributional similarity
baseline that takes as input the same 10,825 tem-
plates of PROPtmp above, constructs a network by
connecting two templates whose distributional simi-
larity is greater than zero, and regards two connected
templates as having the same polarity. The weight
of the links between templates is set to their distri-
butional similarity based on Lin (1998). Then SIM
is given the same initial seed templates as PROPtmp,
by which it calculates the Excitation values of tem-
plates using Takamura et al method. As a result,
SIM assigned positive Excitation values to all tem-
plates, and except for the 10 inhibitory initial seed
templates no templates were regarded inhibitory.
Evaluation scheme We randomly sampled 100
templates each from PROPtmp?s 8,685 excitatory
candidates, PROPtmp?s 2,140 inhibitory candidates,
all the ALLEXC?s templates, and all the SIM?s tem-
plates, i.e., 400 templates in total. To make the an-
notators? judgements easier, we randomly filled the
argument slot of each template with a noun filling its
argument slot in our Web corpus. Three annotators
labeled each sample (a combination of a template
and a noun) as ?excitatory,? ?inhibitory,? ?neutral,? or
?undecided? if they were not sure about its label.
Results for excitatory In the top graph in Fig-
ure 3, ?Proposed? shows PROPtmp?s precision curve.
The curve is drawn from its 100 samples whose X-
axis positions represent their ranks. We plot a dot for
every 5 samples. Among the 100 samples, 37 were
judged as excitatory, 6 as inhibitory, 45 as neutral,
and 6 as ?undecided?. For the remaining 6 samples,
624
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Pre
cisio
n
?Proposed?
?Sim?
?Allexc?
 0.4
 0.6
 0.8
 1
 0  500  1000  1500  2000  2500
Pre
cisio
n
Top-N
?Proposed?
Figure 3: Precision of template acquisition: excitatory
(top) and inhibitory (bottom).
the three annotators gave three different labels and
the label was not fixed (?split-votes? hereafter). For
calculating precision, only the 37 samples labeled
excitatory were regarded as correct. PROPtmp out-
performed all baselines by a large margin, with an
estimated 70% precision for the top 2,000 templates.
?Allexc? and ?Sim? in Figure 3 denote ALLEXC and
SIM. Among ALLEXC?s 100 samples, 19 were
judged as excitatory, 5 as inhibitory, 74 as neutral,
and 2 as ?undecided?. SIM?s low performance re-
flects the fact that templates with opposite polarities
are sometimes distributionally similar, and as a re-
sult get connected by SAME links.
Results for inhibitory ?Proposed? in the bottom
graph in Figure 3 shows the precision curve drawn
from the 100 samples of PROPtmp?s inhibitory can-
didates. Among the 100 samples, 41 were judged as
inhibitory, 15 as excitatory, 32 as neutral, 4 as ?unde-
cided?, and 8 as ?split-votes?. Only the 41 inhibitory
samples were regarded as correct. From the curve
we estimate that PROPtmp achieved about 70% pre-
cision for the top 500. Note that SIM could not ac-
quire any inhibitory templates, yet we can think of
no other reasonable baseline for this task.
Inter-annotator agreement The Fleiss? kappa
(Fleiss, 1971) of annotator judgements was 0.48
(moderate agreement (Landis and Koch, 1977)). For
training, the annotators were given a one-page anno-
tation manual (see supplementary materials), which
basically described the same contents in Section 2,
in addition to 14 examples of excitatory, 14 exam-
ples of inhibitory, and 6 examples of neutral tem-
plates that were manually prepared by the authors.
Using the manual and the examples, we instructed
all the annotators face-to-face for a few hours. We
also made sure the evaluation data did not contain
any examples used during instruction.
Observations about argument positions Among
the 200 evaluation samples of PROPtmp (for both ex-
citatory and inhibitory evaluations), 52 were judged
as excitatory, 47 as inhibitory, and 77 as neutral. For
the excitatory templates, the numbers of nominative,
topic, accusative, dative, and instrumental argument
positions are 15, 11, 10, 8, and 8, respectively. For
the inhibitory templates, the numbers are 17, 11, 16,
3, and 0. For the neutral templates, the numbers are
8, 23, 17, 21, and 8. Accordingly, we found no no-
ticeable bias with regard to their numbers. Likewise,
we found no noticeable bias regarding their useful-
ness for contradiction and causality acquisition re-
ported shortly, too.
Summary PROPtmp works well, as it outperforms
the baselines. Its performance demonstrates the va-
lidity of our constraint matrix in Table 1. Besides,
since our annotators were non-experts but showed
moderate agreement, we conclude that Excitation is
a reasonably comprehensible notion.
5.2 Contradiction Extraction
This section shows that our proposed method for
contradiction extraction (PROPcont) extracted one
million contradiction pairs with more than 70% pre-
cision, and that Excitation values are useful for con-
tradiction ranking. As input for PROPcont we took
the top 2,000 excitatory and the top 500 inhibitory
templates from the previous experiment (i.e., the
other templates were regarded as neutral).
Baselines Our baseline methods are RANDcont
and PROPcont-NE. RANDcont randomly combines
two phrases, each consisting of a template and a
noun that they share. It does not rank its output.
PROPcont-NE is the same as PROPcont except that it
does not use Excitation values; ranking is based only
on sim(t1, t2). PROPcont-NE does combine phrases
with opposite template polarities, just like PROPcont.
Evaluation scheme We randomly sampled 200
phrase pairs from the top one million results of each
625
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200000  400000  600000  800000  1e+06
Pre
cisio
n
Top-N
?Proposed?
?Proposed-ne?
?Random?
Figure 4: Precision of contradiction extraction.
PROPcont and PROPcont-NE, and 100 samples from
the output of RANDcont?s output, giving 500 sam-
ples. Three annotators labeled whether the samples
are contradictions. Fleiss? kappa was 0.78 (substan-
tial agreement).
Results ?Proposed? in Figure 4 shows the preci-
sion curve of PROPcont. PROPcont achieved an esti-
mated 70% precision for its top one million results.
Readers might wonder whether PROPcont?s output
consists of a small number of template pairs that are
filled with many different nouns. If this were the
case, PROPcont?s performance would be somewhat
misleading. However, we found that PROPcont?s 200
samples contained 194 different template pairs, sug-
gesting that our method can acquire a large variety
of contradiction phrases. ?Proposed-ne? is the pre-
cision curve for PROPcont-NE. Its precision is more
than 10% lower than PROPcont at the top one million
results. ?Random? shows that RANDcont?s precision
is only 4%. Table 2 shows examples of PROPcont?s
outputs and their English translation. The labels
?Cont,? ?Quasi? and ?6? denote whether a pair is con-
tradictory, quasi-contradictory, or not contradictory.
Among PROPcont?s 145 samples judged by the an-
notators as contradiction, 46 were judged as quasi-
contradictory by one of the authors. The first 6
case in Table 2 was caused by the template, X??
??? (improve X). It is tricky since it is excitatory
when taking arguments like function, while it is in-
hibitory when taking arguments like disorder. How-
ever, PROPtmp currently cannot distinguish these us-
ages and judged it as inhibitory in our experiments
in Section 5.1, though it must be interpreted as ex-
citatory for the 6 case. The second 6 case was due
to PROPtmp?s error; it incorrectly judged the neutral
template, X????? (related to X), as inhibitory.
Rank Contradiction Pairs Label
8,767 ??????????? ????????????? Cont
repair imbalance ? become imbalanced
103,581 ?????? ??????? Cont
assist the driver ? disturb the driver
151,338 ????????? ?????? Quasi
calm tension ? feel tension
184,014 ??????? ??????? 6
improve function ? boost function
316,881 ?????? ???????? Cont
yen depreciation stops ? yen depreciation develops
317,028 ???????? ???????? Cont
noise gets worse ? noise abates
334,642 ????? ??????? Cont
a sour taste is augmented ? a sour taste is lost
487,496 ??????? ??????? Quasi
feel pain ? reduce pain
529,173 ???????? ?????????? Cont
access occurs ? curb access
555,049 ?????? ??????? Cont
lose nuclear plants ? augment nuclear plants
608,895 ????????? ??????? Quasi
radioactivity is released ? radioactivity is reduced
638,092 ???????? ????????? Cont
Euro falls ? Euro gets strong
757,423 ??????? ????????? Quasi
have share (in market) ? share decreases
833,941 ?????????? ?????????? 6
generate active oxygen ? related to active oxygen
848,331 ??????? ????????? Cont
destroy cancer ? develop cancer
982,980 ????????? ??????????? Cont
virus becomes extinct ? virus is activated
Table 2: Examples of PROPcont?s outputs.
Summary PROPcont is a low cost but high perfor-
mance method, since it acquired one million con-
tradiction pairs with over 70% precision from only
the 46 initial seed templates. Besides, Excitation
contributes to contradiction ranking since PROPcont
outperformed PROPcont-NE by a 10% margin for the
top one million results. Thus we conclude that our
assumption on contradiction extraction is valid.
5.3 Causality Extraction
We show that our method for causality extraction
(PROPcaus) extracted 500,000 causality pairs with
about 70% precision, and that Excitation values con-
tribute to the ranking of causal pairs. PROPcaus took
as input all 10,825 templates classified by PROPtmp.
Baselines RANDcaus randomly extracts two
phrases that co-occur in a sentence with one of the
AND/THUS-type connectives, i.e., it uses not only
non-causal connectives but also causal ones like
thus. FREQ is the same as PROPcaus except that it
ranks its output by the phrase pair co-occurrence
frequency rather than Excitation values.
626
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200000  400000  600000  800000  1e+06
Pre
cisio
n
Top-N
?Proposed?
?Freq?
?Random?
Figure 5: Precision of causality extraction.
Evaluation scheme We randomly sampled 100
pairs each from the top one million results of
PROPcaus and FREQ, and all RANDcaus?s output.
The annotators were shown the original sentences
from which the samples were extracted. Fleiss?
kappa was 0.68 (substantial agreement).
Results ?Proposed? in Figure 5 is the precision
curve for PROPcaus. From this curve the estimated
precision of PROPcaus is about 70% around the top
500,000. Note that PROPcaus outperformed FREQ
by a large margin, and extracted a large variety of
causal pairs since its 100 samples contained 91 dif-
ferent template pairs. Table 3 shows examples of
PROPcaus?s output along with English translations.
The labels ?4? and ?6? denote whether a pair is
causality or not. The 6 cases in Table 3 were
exceptions to our assumption described in Section
4.2; even if two Excitation templates co-occur in a
sentence with an AND/THUS-type connective, they
sometimes do not constitute causality. Actually, the
first 6 case consists of two phrases that co-occurred
in a sentence with a (non-causal) AND/THUS-type
connective but described two events that happen as
the effects of introducing the RAID storage system;
both are caused by the third event. In the second 6
case, the two phrases co-occurred in a sentence with
a (non-causal) AND/THUS-type connective but just
described two opposing events.
Summary PROPcaus performs well since it ex-
tracted 500,000 causality pairs with about 70%
precision. Moreover, Excitation values contribute
to causality ranking since PROPcaus outperformed
FREQ by a large margin. Then we conclude that our
assumption on causality extraction is confirmed.
Rank Causality Pairs Label
1,036 ?????????????????? 4
increase basal metabolism ? enhance fat-burning ability
2,128 ?????????????????? 4
increase desire to learn ? facilitate self-learning
6,471 ?????????????? 6
improve reliability ? increase capacity
29,638 ???????????????????????? 4
circulating thyroid hormone level increases ? improves metabolism
56,868 ??????????????? 4
exports increase ? GDP grows
267,364 ???????????????? 4
promote blood circulation ? improve metabolism
268,670 ???????????????? 4
BSE outbreak occurs ? import ban (on beef) is issued
290,846 ????????????????? 4
improve the view ? improve the efficiency of work
322,121 ??????????????????? 4
giant earthquake occurs ? meltdown is triggered
532,106 ??????????????? 4
good at thermal efficiency ? enhance heating efficiency
563,462 ???????????????? 4
promote inflation (in Japan) ? yen depreciation develops
591,175 ???????????????? 6
bring profit ? bring detriment
657,676 ?????????????? 4
physical strength declines ? immune system weakens
676,902 ?????????????????? 4
sharp fall in government bond futures occurs ? interest rates increase
914,101 ?????????????? 4
have a margin of error ? cause trouble
Table 3: Examples of PROPcaus?s outputs.
5.4 Causality Hypothesis Generation
Here we show that our causality hypothesis genera-
tion method in Section 4.3 (PROPhyp) extracted one
million hypotheses with about 57% precision.
This experiment took the top 100,000 results of
PROPcaus as input, generated hypotheses from them,
and randomly selected 100 samples from the top one
million hypotheses. We evaluated only PROPcaus,
since we could not think of any reasonable baseline
for this task. Randomly coupling two phrases might
be a baseline, but it would perform so poorly that it
could not be a reasonable baseline.
The annotators judged each sample in the same
way as Section 5.3, except that we presented them
with source causality pairs from which hypotheses
were generated, as well as the original sentences of
these source pairs. Fleiss? kappa was 0.51 (moderate
agreement).
As a result, PROPhyp generated one million hy-
potheses with 57% precision. It generated various
kinds of hypotheses, since these 100 samples con-
tained 99 different template pairs. Table 4 shows
some causal hypotheses generated by PROPhyp. The
source causal pair is shown in parentheses. The la-
627
bels ?4? and ?6? denote whether a pair is causality
or not. The first 6 case was due to an error made by
Rank Causality Hypotheses (and their Origin) Label
18,886 ?????????????????? 4
(???????????????) 4
alleviate stress ? remedy insomnia
(increase stress ? continue to have insomnia)
93,781 ???????????????? 4
(????????????) 4
halt deflation ? tax revenue increases
(deflation is promoted ? tax revenes declines)
121,163 ?????????????????? 4
(???????????????) 4
enjoyment increases ? stress decreases
(enjoyment decreases ? stress grows)
205,486 ?????????????? 4
(??????????????) 4
decrease in crime ? diminish anxiety
(increase in crime ? heighten anxiety)
253,531 ????????????????? 4
(????????????????????) 4
reduce chlorine ? bacteria grow
(generate chlorine ? bacteria extinct)
450,353 ???????????????? 4
(????????????) 4
expand demand ? decrease unemployment rate
(decrease demand ? increase unemployment rate)
464,546 ??????????????????? 6
(??????????????????) 6
(ability of) digestion deteriorates ? cholesterol increases
(aid digestion ? decrease cholesterol)
538,310 ??????????????? 4
(?????????????) 4
relieve fatigue ? improve immunity
(feel fatigued ? immunity is weakened)
789,481 ??????????????? 4
(????????????????) 4
conditions improve ? prevent troubles
(conditions become bad ? cause troubles)
837,850 ????????????????? 6
(????????????????) 4
control economic conditions ? accompany problems
(economic conditions improve ? problems are solved)
Table 4: Examples of causality hypotheses.
our causality extraction method PROPcaus; the case
was erroneous since its original causality was erro-
neous. The second 6 case was due to the fact that
one of the contradiction phrase pairs used to gener-
ate the hypothesis was in fact not contradictory (?
?????????? 6? ??????? ?con-
trol economic conditions 6? economic conditions im-
prove?).
From these results, we conclude that our assump-
tion on causality hypothesis generation is valid.
6 Related Work
While the semantic orientation involving good/bad
(or desirable/undesirable) has been extensively stud-
ied (Hatzivassiloglou and McKeown, 1997; Turney,
2002; Rao and Ravichandran, 2009; Velikovich et
al., 2010), we believe Excitation represents a gen-
uinely new semantic orientation.
Most previous methods of contradiction extrac-
tion require either thesauri like Roget?s or WordNet
(Harabagiu et al2006; Mohammad et al2008; de
Marneffe et al2008) or large training data for su-
pervision (Turney, 2008). In contrast, our method
requires only a few seed templates. Lin et al2003)
used a few ?incompatibility? patterns to acquire
antonyms, but they did not report their method?s per-
formance on the incompatibility identification task.
Many methods for extracting causality or script-
like knowledge between events exist (Girju, 2003;
Torisawa, 2005; Torisawa, 2006; Abe et al2008;
Chambers and Jurafsky, 2009; Do et al2011; Shi-
bata and Kurohashi, 2011), but none uses a notion
similar to Excitation. As we have shown, we expect
that Excitation will improve their performance.
Regarding the acquisition of semantic knowledge
that is not explicitly written in corpora, Tsuchida et
al. (2011) proposed a novel method to generate se-
mantic relation instances as hypotheses using auto-
matically discovered inference rules. We think that
automatically generating plausible semantic knowl-
edge that is not written (explicitly) in corpora as hy-
potheses and augmenting semantic knowledge base
is important for the discovery of so-called ?unknown
unknowns? (Torisawa et al2010), among others.
7 Conclusion
We proposed a new semantic orientation, Excitation,
and its acquisition method. Our experiments showed
that Excitation allows to acquire one million con-
tradiction pairs with over 70% precision, as well as
causality pairs and causality hypotheses of the same
volume with reasonable precision from the Web. We
plan to make all our acquired knowledge resources
available to the research community soon (Visit
http://www.alagin.jp/index-e.html).
We will investigate additional applications of Ex-
citation in future work. For instance, we expect that
Excitation and its related semantic knowledge ac-
quired in this study will improve the performance
of Why-QA system like the one proposed by Oh et
al. (2012).
628
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phrased event relation acquisition: Coupling the
relation-oriented and argument-oriented approaches.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
1?8.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Alina Andreevskaia and Sabine Bergler. 2006. Semantic
tag extraction from wordnet glosses. In Proceedings
of the 5th International Conference on Language Re-
sources and Evaluation (LREC 2006).
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP (ACL-
IJCNLP 2009), pages 602?610.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradiction
in text. In Proceedings of the 48th Annual Meeting of
the Association of Computational Linguistics: Human
Language Technologies (ACL-08: HLT), pages 1039?
1047.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), pages 294?303.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), Workshop on Multi-
lingual Summarization and Question Answering - Ma-
chine Learning and Beyond, pages 76?83.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence (AAAI-06), pages 755?
762.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35 Annual Meeting of
the Association for Computational Linguistics and the
8the Conference of the European Chapter of the Asso-
ciation of Computational Linguistics, pages 174?181.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for Japanese syntactic
and case structure analysis. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT-NAACL2006),
pages 176?183.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL1998), pages 768?
774.
Saif Mohammad, Bonnie Dorr, and Greame Hirst. 2008.
Computing word-pair antonymy. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 982?991.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Junichi Kazama, and
Yiou Wang. 2012. Why question answering using
sentiment analysis and word classes. In Proceedings
of EMNLP-CoNLL 2012: Conference on Empirical
Methods in Natural Language Processing and Natural
Language Learning.
Charles E. Osgood, George J. Suci, and Percy H. Tannen-
baum. 1957. The measurement of meaning. Univer-
sity of Illinois Press.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL, pages 675?682.
Tomohide Shibata and Sadao Kurohashi. 2011. Acquir-
ing strongly-related events using predicate-argument
co-occurring statistics and case frames. In Proceed-
ings of the 5th International Joint Conference on Natu-
ral Language Processing (IJCNLP 2011), pages 1028?
1036.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
629
spin model. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 133?140.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion
to discover unknown unknowns. New Generation
Computing (Special Issue on Information Explosion),
28(3):217?236.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556?560.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL
(HLT-NAACL2006), pages 57?64.
Masaaki Tsuchida, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, Jun?ichi Kazama, Chikara Hashimoto,
and Hayato Ohwada. 2011. Toward finding semantic
relations not written in a single sentence: An inference
method using auto-discovered rules. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 902?910.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2002), pages 417?424.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (COLING 2008), pages 905?912.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the ACL, pages 777?785.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics: Human Language
Technologies (ACL-08: HLT), pages 63?71.
630
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 693?703,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Two-stage Method for Large-scale Acquisition of
Contradiction Pattern Pairs using Entailment
Julien Kloetzer? Stijn De Saeger? Kentaro Torisawa? Chikara Hashimoto?
Jong-Hoon Oh? Motoki Sano? Kiyonori Ohtake??
Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Kyoto, Japan
{?julien, ? stijn, ? torisawa, ? ch, ?rovellia, ?msano, ??kiyonori.ohtake}@nict.go.jp
Abstract
In this paper we propose a two-stage method
to acquire contradiction relations between
typed lexico-syntactic patterns such as Xdrug
prevents Ydisease and Ydisease caused by
Xdrug . In the first stage, we train an SVM
classifier to detect contradiction pattern pairs
in a large web archive by exploiting the exci-
tation polarity (Hashimoto et al, 2012) of the
patterns. In the second stage, we enlarge the
first stage classifier?s training data with new
contradiction pairs obtained by combining the
output of the first stage?s classifier and that of
an entailment classifier. We acquired this way
750,000 typed Japanese contradiction pattern
pairs with an estimated precision of 80%. We
plan to release this resource to the NLP com-
munity.
1 Introduction
The ability to detect contradictory information in
text has many practical applications. Among those,
Murakami et al (2009) pointed out that a contra-
diction recognition system can detect conflicts and
anomalies in large bodies of texts and flag them to
help users identify unreliable information. For ex-
ample, many Japanese web pages claim that agari-
cus prevents cancer, where agaricus is a species of
mushroom found in a variety of commercial prod-
ucts. Although this has been accepted by many
Japanese people, by Googling keywords ?agaricus?,
?promotes? and ?cancer?, we can find pages claim-
ing that ?agaricus promotes cancer?, some of which
point to a study authorized by the Japanese Min-
istry of Health, Labour and Welfare1 reporting that
1 http://www.mhlw.go.jp/topics/bukyoku/iyaku/syoku-
anzen/qa/060213-1.html
a commercial product containing agaricus promoted
cancer. Obviously, the existence of these pages casts
serious doubt on the ability of agaricus to prevent
cancer and encourages readers to dig more about this
subject.
The above example suggests that recognizing
contradictory information can guide users to a true
fact. Likewise, we believe that contradiction recog-
nition is also useful when dealing with non-factual
information that occupy most of our daily lives. For
instance, there is a big controversy recently whether
Japan should join an economic partnership agree-
ment called the Trans Pacific Partnership (TPP), and
quite serious but contradictory claims are plentiful in
the mass media and on the web, e.g., TPP will wipe
out Japan?s agricultural businesses and TPP will
strengthen Japan?s agricultural businesses. Neither
of these are facts; they are predictions that can only
be realized or disputed after the underlying decision-
making is done: joining or refusing the TPP.
Furthermore, after reading documents including
contradictory predictions, one should notice that
each of them is supported by a convincing the-
ory that has no obvious defect, e.g., ?Exports of
Japan?s agricultural products will increase thanks to
the TPP? or ?A large amount of low-price agricul-
tural products will be imported to Japan due to the
TPP?. Even if one of these predictions may just hap-
pen to be true because of unexpected reasons such as
minor fluctuations in the Japanese yen, we must sur-
vey such theories that support contradictory predic-
tions, conduct balanced decision-making, and pre-
pare counter measures for the expected problems af-
ter examining multiple viewpoints. Contradiction
recognition should be useful to select documents to
be surveyed.
693
Figure 1: Method workflow
We have developed a method for recog-
nizing pairs of contradictory binary patterns
such as ??X promotes Y?, ?X prevents Y?? and
??X will wipe out Y?, ?X will strengthen Y??. To
solve the problem described above, we can easily
develop a system that can find contradictory text
fragments from the web like ?agaricus promotes
cancer? and ?agaricus prevents cancer? from the
discovered contradictory pattern pairs.
Our method is a two-stage procedure with three
supervised classifiers (Fig. 1). In the first stage,
we build a classifier BASE to recognize contradic-
tions between binary patterns, and a classifier ENT
to recognize entailment. In the second stage, we
combine the contradiction pairs recognized by BASE
and the entailment pairs recognized by ENT to ex-
pand BASE?s training data and train a new contra-
diction classifier, EXP. This expansion using en-
tailment is one key idea of this work: we acquired
750,000 contradiction pairs with 80% precision us-
ing the expanded training data, more than doubling
the 285,000 pairs acquired at the same precision
level without expansion. We also demonstrate that
this result is not trivial by showing that our method
outperforms an alternative one based on Integer Lin-
ear Programming inspired by the successful entail-
ment recognition method of Berant et al (2011).
As another technical contribution of this work, we
exploit the recently proposed semantic polarity of
excitation (Hashimoto et al, 2012) to recognize con-
tradictions between binary patterns. Hashimoto et
al. (2012) previously showed that excitation polari-
ties are useful to recognize contradictions between
phrases that consist of a noun and a predicate, such
as ?promote cancer? and ?prevent cancer?. While
it is trivial to extend this framework to contradic-
tions between unary patterns such as ?promote X?
and ?prevent X? by replacing the common nouns
in each pair with a variable, the information rep-
resented in unary patterns is often vague, and it is
unlikely that a contradiction between unary patterns
directly leads to the discovery of unreliable infor-
mation to be flagged or to a meaningful survey of
complex problems. As exemplified by the agaricus
and TPP examples, contradictions between binary
patterns that include two variables such as ?X pro-
motes Y? or ?X will wipe out Y? are more useful
than those between unary patterns. We also show
that it is not trivial to recognize contradictions be-
tween binary patterns using contradictions between
unary patterns.
Most works dealing with contradiction recogni-
tion up till now (Harabagiu et al, 2006; Bobrow
et al, 2007; Kawahara et al, 2008; Kawahara et
al., 2010; Ohki et al, 2011) focus on recognizing
contradictions between full sentences or documents,
not text fragments that match our relatively short
patterns (survey in Section 5). We expect that the
contradictory pattern pairs we acquired can be used
as building blocks in such full-fledged contradiction
recognition for full sentences or documents, simi-
larly to antonym pairs in Harabagiu et al (2006).
Also, we should emphasize that our method
focuses on the most challenging part of contra-
diction recognition according to the classification
of De Marneffe et al (2008). Since we discard
patterns with negations, an evident source of contra-
dictions like ??X causes Y?, ?X does not cause Y??,
most of our output are non-trivial contradic-
tions related to high-level semantic phenomena,
e.g., contradiction pairs related to antonyms
like ??X? Y?????, ?X? Y??????
(??X increases Y?, ?X decreases Y??), lexical contra-
dictions like ??X? Y????, ?Y? X?????
(??X wins against Y?, ?Y wins against X??), or
contradictions due to common-sense knowledge
like ??X? Y???????, ?X? Y??????
(??X reassures Y?, ?X betrays Y??). We believe
acquiring such contradictions in a large scale is a
valuable contribution.
The following is the outline of this paper. Sec-
tion 2 details our target and our proposed method.
Evaluation results are discussed in Section 3. Sec-
694
Figure 2: Detailed data flow
tion 4 details our features set, and Section 5 related
work. Section 6 provides a conclusion.
2 Proposed method
As showed in Figure 1, our method consists of
three supervised classifiers. Classifiers BASE and
EXP recognize contradiction relations between bi-
nary patterns, and ENT recognizes entailment rela-
tions between binary patterns. The contradiction
pairs recognized by BASE and the entailment pairs
recognized by ENT are combined to generate new
contradiction pairs, part of which are then added to
BASE training data to train the EXP classifier. Our
final output is the set of all binary pattern pairs re-
garded as contradictions by EXP. Since the depen-
dencies between these three classifiers, their distinct
sets of training data, and the two data sets to be clas-
sified (we describe those in the two sections below)
is a bit complex, we show a complete description of
the whole process in Figure 2.
The key idea is in the scheme that expands the
training data. Logically speaking, patterns p and r
are contradictory if there exists a pattern q such that
p entails q and q contradicts r. For example, since
?X causes Y? entails ?X promotes Y? and ?X pro-
motes Y? contradicts ?X prevents Y?, then ?X causes
Y? contradicts ?X prevents Y?. Hence, by combin-
ing entailment and contradiction pairs, we can ob-
tain more contradiction pairs.
Following this property of contradiction relations,
we collect a set of pattern pairs {?p, r?} for which
there exists a pattern q such that ENT recognizes that
p entails q and BASE recognizes that q contradicts r.
Then we rank these pairs based on a novel scoring
function called Contradiction Derivation Precision
(CDP) and expand BASE training data by adding to
it the top-ranked pairs according to CDP in order to
train EXP. This ranking scheme selects highly accu-
rate contradiction pairs and prevents errors caused
by BASE and ENT from being propagated to EXP.
In the following, after defining the patterns for
which we acquire contradiction relations, we de-
scribe BASE, EXP, ENT, and our expansion scheme.
2.1 Patterns
In this work, a binary pattern is a word sequence
on the path of dependency relations connecting two
nouns in a syntactic dependency tree, like ?X causes
Y?, and we say a noun pair co-occurs with a pattern
if the two nouns are connected by this pattern in the
dependency tree of a sentence in the corpus.
We focus on typed binary patterns, which place
semantic class restrictions on the noun pairs they
co-occur with, e.g., ?Yorganization is in Xlocation?.
Subscripts organization and location indicate the se-
mantic classes of the X and Y slots. Since typed
patterns can distinguish between multiple senses
of ambiguous patterns, they greatly reduce errors
due to pattern ambiguity (De Saeger et al, 2009;
Schoenmackers et al, 2010; Berant et al, 2011).
We automatically induced semantic classes from our
corpus using the EM-based noun clustering algo-
695
rithm presented in Kazama and Torisawa (2008),
and clustered one million nouns into 500 rela-
tively clean semantic classes, including for example
classes of diseases and of chemical substances.
The binary patterns and their co-occurring noun
pairs were extracted from our corpus of 600 mil-
lion Japanese web pages dependency parsed with
KNP (Kurohashi and Nagao, 1994). We restricted
our patterns to the most frequent 3.9 million pat-
terns of the form ?X-[case particle] Y-[case parti-
cle] predicate? such as ?X-ga Y-ni aru? (?X is in Y?)
which do not contain any negation, number, symbol
or punctuation character. Based on our observation
that patterns in meaningful contradiction and entail-
ment pairs tend to share many co-occurring noun
pairs, we used as input to our classifiers the set Pall
of 792 million pattern pairs for which both patterns
share three co-occurring noun pairs.
2.2 BASE: First stage Classifier for
Contradiction
Below, we detail BASE: its training data and input
data to be classified, and some experimental results.
Our first stage classifier for contradictions, BASE,
is an SVM that uses commonsensical surface and
lexical resources based features, such as n-grams ex-
tracted from patterns, which will be detailed in Sec-
tion 4. An important point to be stressed here is
that we restricted the pattern pairs to be classified
by BASE by exploiting their excitation polarity, a
semantic orientation proposed by Hashimoto et al
(2012). Excitation characterizes unary patterns as
excitatory, inhibitory, or neutral. Excitatory unary
patterns, such as ?cause X? or ?increase X?, entail
that the function, effect, purpose, or role of their ar-
gument?s referent is activated or enhanced, and in-
hibitory unary patterns, such as ?prevent X? or ?X
disappears?, entail that the function, effect, purpose,
or role of their argument?s referent is deactivated or
suppressed. Neutral unary patterns like ?close to X?
are neither excitatory nor inhibitory.
We exploited excitation to restrict the input of
BASE. Based on the result of Hashimoto et al
(2012) showing that two unary patterns with op-
posite polarity have a higher chance to be a con-
tradiction, we extracted from set Pall the set Popp
of binary pattern pairs that contain unary patterns
with opposite excitation polarities as sub-patterns.
??Y cause X?, ?Y prevent X?? is an example of such
a pair since the unary sub-patterns ?cause X? and
?prevent X? are respectively excitatory and in-
hibitory. We used here 6,470 excitation unary pat-
terns hand-labeled as either excitatory (4,882 pat-
terns) or inhibitory (1,588 patterns). Set Popp con-
tains 8 million pattern pairs with roughly 38% true
contradiction pairs, and is the input to BASE. We
will show in experiments at the end of this section
that this restriction is necessary to obtain good per-
formance for BASE. We also tried to add the excita-
tion polarities in BASE?s feature set and classify Pall,
but the performance was worse.
Training Data Another key feature of BASE is
that it is distantly supervised. We did not use
training samples that are directly manually anno-
tated. Instead we automatically generated training
data from a smaller set of (non-)contradiction unary
pattern pairs. We first prepared a set of roughly
800 unary pattern pairs hand-labeled by three human
annotators as contradictions (238 pairs) and non-
contradictions (558 pairs) using majority vote. The
inter-annotator agreement was 0.78 (Fleiss?kappa).
Inspired by Hashimoto et al (2012), we selected
these unary pattern pairs among pairs with high dis-
tributional similarity, with and without restricting
them to having opposite excitation polarity, such as
to get a fair distribution of contradictions and non-
contradictions.
We then extracted from set Pall all 256,000 pat-
tern pairs containing a contradictory unary pattern
pair, and all 5.2 million pattern pairs containing a
non-contradictory unary pattern pair, which we re-
spectively used as positive and negative training data
(estimated 79% and 73% accuracy from 200 hand-
labeled samples). Table 1 shows some examples.
The optimal composition of training data for
BASE was determined according to preliminary ex-
periments using our development set (1,000 manu-
ally labelled samples. See Section 3.1). We trained
20 different classifiers using from 6,250 to 50,000
positive samples (4 sets) and from 12,500 to 200,000
negative samples (5 sets), doubling the amounts in
each step, for a total of 20 configurations. We could
not try a larger training data due to long training time
but we do not expect it to be a problem because the
worst performance was observed with large train-
696
Table 1: Examples of training samples for BASE obtained from unary pattern pairs
Binary pattern pair (the unary pattern pair that extracted it is underlined) Unary pattern pair label
Y ? X ??? (X is bad in Y too) - Y ?? X ??? (X is good even in Y) contradiction
Y ? X ???? (Y too heads toward X) - Y ? X ??? (Y too comes out of X) contradiction
X ?Y ? ??? (add Y to X) - X ?Y ? ??? (insert X into Y) non-contradiction
Y ? X ??? (Y too comes to X) - Y ?? X ??? (go to X with Y) non-contradiction
Figure 3: Effect of the restriction using excitation
ing data (25,000 positives and 200,000 negatives;
the difference from the optimal setting was 2.3% in
average precision). The optimal training data set,
Trainbase, consists of 12,500 positives and 100,000
negatives samples as described above and is the one
we use in our experiments below and in Section 3.
Since BASE input for classification data is Popp
we also tried sampling Trainbase from Popp. We
obtained 56.27% average precision for our classi-
fier BASE, and 52.99% when restricting the source
of training data to pairs in Popp. We believe that the
difference lies in the size of the sets from which we
sampled our training data: while there are 5.46 mil-
lion binary pattern pairs in Pall with a hand-labeled
unary pattern pair in Pall, there are only 237,000
pairs in Popp. We believe this much smaller sam-
ple source lead to a lower performance because it
included much less variations of the patterns.
To train BASE and other classifiers mentioned in
this paper, we used the SVM tool TinySVM2 with
a polynomial kernel of degree 2, the setting which
showed the best performance during our preliminary
experiments.
Effect of Excitation Polarities We also empiri-
cally examined the effect of the restriction on the
patterns using excitation polarities. We used our test
set (2,000 manually annotated samples described in
2 http://chasen.org/?taku/software/TinySVM/
Section 3.1) and 250 manually annotated samples
(majority vote from 3 annotators) from top ranked
pairs of Pall to draw precision curves for BASE over
the top 2 million binary pairs from both Popp and
Pall. In each case we assumed that pairs were dis-
tributed uniformly (i.e., with a constant interval) in
the ranked list of pairs of Popp and Pall, and com-
puted precision accordingly. Since the pairs sets
are reasonably large and were sampled randomly we
thought this was a reasonable hypothesis. The pre-
cision over Popp is higher than that over Pall with
a large margin, suggesting that the restriction using
excitation polarities is beneficial.
2.3 ENT: First stage Classifier for Entailment
ENT is an SVM classifier for entailment trained us-
ing 27,500 hand-annotated binary pattern pairs (set
Trainent, 45% of positive entailment pairs) created
for some previous work (Kloetzer et al, 2013). It es-
sentially uses the same feature set as that for BASE
with the addition of several distributional similar-
ity measures (see Section 4 below for more details).
This classifier is given all pairs of Pall as input and
scores each of them. For this study, we considered
the 44.5 million pattern pairs with a positive SVM
score as entailment pairs. Manual annotation of 200
random samples revealed that the precision of these
pairs was 63% and that the top 7.1 million pairs had
80% precision (result interpolated from the top 16%
of the annotated samples).
2.4 Second stage: Training Data Expansion
and Classifier EXP
Below, we show how we combine BASE?s top output
(hereafter C) and ENT?s top output (hereafter E) in
the second stage of our method to expand Trainbase
and train a new classifier, EXP.
The training data expansion process is based on
the following logical constraint: if a pattern p entails
a pattern q and pattern q contradicts a third pattern r,
then p must contradict r. For example, because ?X
697
Table 2: Examples of triplets ?p, q,r? where p entails q, q contradicts r, and hence p contradicts r
Pattern p Pattern q Pattern r X/Y examples SV M Score(p, r) CDP (p, r)
Y ?? X ???? Y ?? X ????? Y ? X ???? ??/? 0.3 0.98X disappears from Y X vanishes from Y Y is full of X anger/eye
Y ? X ????? Y ? X ???? Y ?? X ???? ??/?? -0.3 0.61stop X in Y finish X in Y start X in Y April/activity
X ? Y ??? X ? Y ??? X ? Y ??? ???/?? 0.07 0.45X shows Y X have Y X loses Y team/confidence
Algorithm 1 Training data expansion: C is the top 5%
output of BASE, E is the top output of ENT (score > 0)
1: procedure EXPAND(C, E)
2: Compute the set of expanded pairs C? = {?p, r? |
?q : ?p, q?? E,?q, r?? C}.
3: Rank the pairs in C? using CDP.
4: Add the N top-ranked pairs in C? \ C as new positive
samples to Trainbase.
5: Remove incoherent negative training samples using
negative cleaning.
6: end procedure
causes Y? (pattern p) entails ?X promotes Y? (pattern
q) and the latter contradicts ?X prevents Y? (pattern
r), we conclude that ?X causes Y? (p) contradicts
?X prevents Y? (r). We call the former contradic-
tion ?q, r? a source contradiction pair, and the later
pair ?p, r? an expanded contradiction pair. Based on
this idea, we combine C and E to aggressively ex-
pand Trainbase. This process is described in Al-
gorithm 1, and Table 2 shows examples of triples
?p, q,r? obtained in our experiments.
Expanding pairs fromC andE compounds the er-
rors made by BASE and ENT, hence it is crucial to
select a highly precise subset of the expanded pairs.
Taking the top pairs according to their SVM score
would achieve this, but since BASE already handles
correctly such pairs, they should not help much as
new training data. We therefore propose a new scor-
ing function for selecting highly precise expanded
pairs: Contradiction Derivation Precision (CDP ).
CDP was designed according to the following
assumption: a source contradiction pair that derives
correct expanded pairs with a high precision should
be reliable. Probably, all the expanded pairs derived
from such a reliable source pair will be correct and
should be included in the new training data .
In our formulation of CDP , correctness of an ex-
panded pair is judged according to the pair?s SVM
score using BASE. In other words, we regard an
expanded pair that has an SVM score above some
threshold ? as a true contradiction. A source contra-
diction pair that derives true contradiction pairs with
a high precision is regarded as a reliable source con-
tradiction pair. CDP , which is defined over a ex-
panded pairs, is the maximum precision among that
of the source contradiction pairs that derive a given
expanded pair.
We first define CDPsub(q, r) over a source con-
tradiction pair ?q, r? as the ratio of expanded pairs
obtained from ?q, r? whose SVM score is above
threshold ?. This ratio corresponds to the precision
of the expanded pairs derived from the source con-
tradiction pair ?q, r?.
CDPsub(q, r) = |{?p, r? ? Ex(q, r) | Sc(p, r) > ?}|
|Ex(q, r)|
HereEx(q, r) is the set of expanded pairs derived
from a source pair ?q, r?, and Sc is the SVM score
given by BASE. In our experiments, we set ? = 0.46
such that pattern pairs for which BASE gives a score
over ? corresponds to the top 5% of BASE?s output.
CDP (p, r) over an expanded pair is defined as fol-
lows, where Source(p, r) is the set of source con-
tradiction pairs that were derived into the expanded
pair ?p, r?.
CDP (p, r) = max?q,r??Source(p,r)CDPsub(q, r)
We then expand the top 5% contradictions of
BASE?s output (set C) and pattern pairs scored pos-
itively by ENT (set E), rank all expanded pairs not
already in C according to CDP, and add the top N
pairs with the highest CDP values as positives to
Trainbase to train EXP. The value of N shall be
determined empirically in later experiments using
a development set. Note that, since CDP (p, r) is
independent of ?p, r??s SVM score, even pairs that
were assigned a negative score by BASE can become
highly ranked by CDP (second triplet in Table 2)
698
and be added to train EXP, hence we expect EXP to
learn something new from these pairs.
Finally, after the addition of expanded pairs, we
remove incoherent training samples. We propose to
remove from the negative training samples of EXP
any pattern pair that may conflict with the newly
added positives; we call this step negative cleaning.
Intuitively, since the content word pairs in a pattern
pair should present some of the strongest evidence
for determining the patterns (non-)contradiction sta-
tus, we remove any negative sample that shares a
content word pair with one of the added expanded
pairs. The final training data for EXP, set Trainexp,
consists of the following: (1) positive samples from
Trainbase, (2) (positive) expanded pairs, and (3)
negative training samples from Trainbase, cleaned
using negative cleaning. We confirmed in our exper-
iments that negative cleaning was necessary to train
a strong EXP classifier (details omitted for reason of
space).
After training EXP with Trainexp, we classify
Popp with EXP to produce the final output of the
whole method. Note that while this expansion pro-
cess can be re-iterated with EXP?s output, our exper-
iments failed to show any improvement with subse-
quent iterations.
3 Evaluation
This section presents our experimental results. We
describe first how we constructed test and develop-
ment data, and then report comparison results be-
tween our method and others including BASE and an
Integer Linear Programming-based (ILP) method.
3.1 Development and Test Data
We asked three human annotators to label 3,000 bi-
nary pattern pairs randomly sampled from Popp as
contradiction or non-contradiction to be used as de-
velopment (1,000 pairs) and test (2,000 pairs) sets.
We considered a pattern pair as a true contradic-
tion relation if at least two out of the three annota-
tors marked it as positive. The inter-rater agreement
score (Fleiss Kappa) was 0.523, indicating moderate
agreement (Landis and Koch, 1977). As a definition
of contradiction, we used the notion of incompati-
bility (i.e., two statements are extremely unlikely to
be simultaneously true) proposed by De Marneffe et
Figure 4: Precision of all the compared methods
al. (2008). We then say binary patterns such as ?X
causes Y? and ?X prevents Y? are contradictory if
the above definition holds for any noun pair that can
instantiate the patterns? variables in the provided se-
mantic class pair.
Because our semantic classes are obtained by au-
tomatic clustering and have no meaningful labels,
we followed Szpektor et al (2007) and provided the
annotators with three random noun pairs that co-
occur with the patterns as a proxy for the class pair.
The annotators marked a given pattern pair as posi-
tive if the contradiction relation between the patterns
held for all three noun pairs presented.
3.2 Experimental Results
Here we show how our proposed method outper-
forms baseline methods. We compare the following
four methods:
? PROPOSED: our proposed method. N , the
number of newly added positive training sam-
ples during the training data expansion pro-
cess, was set to 6,000 according to preliminary
experiments using the development set. We
tried 50 different values of N from 1,000 up to
50,000, adding 1,000 each time, and chose the
N value giving the highest average precision
against our development set (1,000 samples).
? BASE: our first stage classifier.
? PROP-SCORE: same as PROPOSED except for
the use of BASE?s SVM score instead of CDP .
N was set to 30,000 in the same way we set N
for PROPOSED.
? HAS: an adaptation of the contradiction ex-
traction method presented in Hashimoto et al
699
(2012). For a binary pattern pair we first
extracted its unary pattern pair with opposite
polarity (or one at random in case there are
two) and scored it based on our implementa-
tion of Hashimoto et al (2012); the score is
based on the distributional similarity between
unary patterns and an excitation score obtained
using a minimally supervised method based on
the spin model. We then scored the binary pat-
tern pair by the score of this unary pattern pair.
We ranked the pattern pairs of our test set (2,000
random pairs from set Popp) based on the score pro-
duced by each method. For each tested method we
assumed that pairs in the test set were distributed
uniformly like explained in Section 2.2. The pre-
cision curves we obtained are shown in Figure 4.
PROPOSED clearly outperformed BASE and ac-
quired around 750,000 contradiction pattern pairs
with an estimated precision of 80%, out of which
some examples are shown in Table 3. These pairs
cover 26,941 content word pairs and reduce to
272,164 untyped pairs, showing that PROPOSED
does not just acquire a handful of contradictions in
many different class pairs. Also, when matching
these pairs against an antonyms database (extracted
from the dictionary of the morphical analyzer JU-
MAN) we found that only 100,886 of these pattern
pairs contain an antonym pair, which means that
most of the extracted pairs? contradictions are due
to more complex phenomena than simple antonymy.
With the same precision, BASE and PROP-SCORE
acquired only 285,000 pairs (covering 11,794 con-
tent word pairs) and 636,000 pairs respectively. This
implies that our two-stage method can more than
double the number of highly precise contradiction
pairs we acquire as well as increasing their vari-
ety, and that ranking expanded pairs using our scor-
ing function CDP is better than with SVM score,
though even PROP-SCORE performs better than
BASE in our setting. Finally, the poor performance
of HAS suggests that extending the Hashimoto et
al.?s framework to recognition of binary patterns is
not a trivial task.
As to why adding only 6,000 top pairs ranked
by CDP performs better than adding 30,000 pairs
ranked by SVM score, the pattern pairs added in
PROP-SCORE had high SVM scores given by BASE
and as such are already handled nicely by BASE.
Table 3: Examples of pairs acquired by PROPOSED: con-
tradiction (label +) and non-contradiction (label -)
Lab. Pattern pairs (with rank) X/Y example
Y ? X ???? - Y ?? X ????? ??/??
+ X finished Y - X started from Y sale/yesterday
Rank 228,039
X ? Y ??? - Y ? X ??? ??/????
+ X wins against Y - Y wins against X Japan/Vietnam
Rank: 258,068
X ? Y ??? - X ?? Y ??? ?/??
- X lose Y - Have Y in X people/interest
Rank 474,143
Y ? X ???? - Y ?? X ??? ??/??
+ Lose X in Y - Have X in Y too confidence/
Rank 522,534 oneself
Y ? X ????? - X ? Y ???? 9 ?/??
- Y falls down to X - raise Y to X 9th/ranking
Rank 538,901
X ? Y ????? - X ?? Y ??? ?/????
+ Y exists in X - Keep Y out X inside/virus
Rank 620,430
X ?? Y ??? - X ? Y ???? ?/?
- Remove Y off X - X answer with Y I (or me)/eyes
Rank 652,530
Y ? X ?????? - X ? Y ??? ?/??
+ Kick out Y from X - Y remains in X body/fatigue
Rank 697,177
Y ? X ?????? - Y ? X ????? ?/??
+ X reassures Y - X betrays Y I/her
Rank: 749,916
Hence, we think the effect of adding a new sam-
ple from PROP-SCORE is smaller than that in PRO-
POSED, because in PROPOSED we add to the train-
ing data pattern pairs with both high and low (possi-
bly negative) SVM scores.
Finally, while the quality of the entailment pairs
plays a very important role in the assumption that
was the base of CDP , these results show that even
a simple rule such as ?Use entailment pairs with
SVM score over 0 to expand contradictions before
ranking them with CDP ? is sufficient to make the
method work. Though it may be possible to design
a more complex CDP formula which takes entail-
ment score into account, we did not explore this di-
rection in this work.
Comparison with an ILP-based method Finally,
we would like to compare our method with an ILP-
based method. The interaction between contradic-
tion and entailment that forms the basis for our ex-
pansion method has a natural interpretation as an op-
timization problem. We thus compared our method
to the following ILP formulation of this interaction
inspired by Berant et al (2011), using our test set:
700
Figure 5: Comparison between PROPOSED, BASE and
BASE+ILP on a restricted test set (1,306 samples)
(1) G = argmax
?
p6=q
(e(p, q)??)?Epq +(c(p, q)??)?Cpq
(2) s.t. ?p,q,r Epq + Cqr ? Cpr ? 1
(3) ?p,q Epq + Cpq ? 1
(4) ?p,q Epq ? {0, 1} (5) ?p,q Cpq ? {0, 1}
The objective in Equation (1) is a sum over the
weights of every pair of patterns ?p, q?, where Epq
indicates whether a pair ?p, q? is an entailment pair
(Equation (4)), andCpq indicates whether it is a con-
tradiction pair (Equation (5)). e(p, q) and c(p, q) are
the score given respectively by ENT and BASE, and
? is a prior defining the weight of a pair as neither
entailment nor contradiction that shall be set before
any experimentation. Equation (2) states the tran-
sitivity relation which is the basis of our expansion
method. Finally, Equation (3) states that a given pat-
tern pair cannot be a contradiction pair and an entail-
ment pair at the same time. Since our patterns are
class-dependent, we solved separate ILP instances
for each semantic class pair.
We drew a precision curve for each of BASE,
PROPOSED and BASE+ILP. To draw the curve for
BASE+ILP, we incrementally raised the sample?s
non-contradiction non-entailment prior ? (more de-
tails in Berant et al (2011)). Because of the com-
putational difficulty of ILP (NP-complete) and the
size of our data, the computation for the ILP-based
method ran out of memory on a 72GB machine for
116 class pairs out of the 1,031 that our test set cov-
ers. For this reason, we only used the 1,306 samples
of the test set covered by the remaining 915 class
pairs. We also measured the performance of BASE
and PROPOSED on the same restricted test set.
Figure 5 shows that under these conditions the
ILP-based method performance resembles BASE
and is worse than PROPOSED on all data points.
PROPOSED performs slightly worse in this setting
compared to when classifying the whole of Popp,
but this only means that its performance is good for
the 116 class pairs we ignored in this experiment.
While this comparison is only made in a restricted
setting, our expansion method still outperforms ILP
and is clearly more scalable. The ILP results could
be improved by adding more constraints (contradic-
tion is symmetric, entailment is transitive), but this
would also make the problem even more intractable
in terms of computational costs.
4 Features
In this section we present the features used in our
classifiers, which are mainly categorized into three:
surface features (i.e., those reflecting the patterns?
content itself), features based on external lexical re-
sources, and distributional similarity based features;
all features are listed in Table 4. ENT uses all the
features while BASE and EXP use all except for the
distributional similarity based ones. The optimality
of the feature sets was confirmed through ablation
tests using the development set (results omitted for
the sake of space).
Since patterns with a contradiction or entailment
relation are often superficially similar, for instance,
in case structure or inflection, we use a number of
surface features based on string similarity measures,
extending the feature sets used by Malakasiotis and
Androutsopoulos (2007) for entailment recognition.
They include bag-of-words features such as n-grams
and similarity scores concerning the bag-of-words
such as their Euclidian distance.
To complement the surface features with knowl-
edge about the content words, we used lexi-
cal databases including such as antonymy, syn-
onymy, entailment, or allography. The presence
of such word pairs is usually a good indicator of
(non-)contradiction or (non-)entailment at the pat-
tern level. More specifically, for any word pair
?wp,wq? taken from a pattern pair ?p, q? we mark
the presence of ?wp,wq? in each of the lexical re-
sources as a binary feature. We used the Japanese
lexical resources distributed by the ALAGIN Fo-
rum3: the verb entailment database (117,000 verb
3 http://www.alagin.jp/
701
Table 4: Features summary, computed over a pair of patterns ?p, q?
su
rfa
ce Similarity measures: common elements ratios, Dice coefficient, Jaccard and discounted Jaccard scores, Cosine, Euclidian, Manhattan, Levenshteinand Jaro distances; computed over: the patterns? 1-, 2- and 3-grams sets of: characters, morphemes, their stems & POS; content words and stems
binary feature for each of the patterns? subtrees, 1- and 2-grams ; patterns? lengths and length ratios
le
x.
r. entries in databases of verb entailments and non-entailments, synonyms, antonyms, allographs ; checked over: pairs of content words,
pairs of content word stems, same for the reverse pattern pair ?q, p?
di
s.s
. Distributional similarity measures: Common elements ratios, Jaccard and discounted Jaccard scores, sets and sets intersection cardinality,
DIRT (Lin and Pantel, 2001), Weeds (Weeds and Weir, 2003) and Hashimoto (Hashimoto et al, 2009) scores; computed over: patterns?
co-occurring noun pairs, POS tags of those, nouns co-occurring in each variable slot, nouns co-occurring with each unary sub-patterns
ot
he
r binary feature for each semantic class pair and individual semantic classes
patterns frequency rank in the given semantic class pair
pairs; Alagin ID A-2), the databases of synonyms,
antonyms and meronyms (respectively 111,000,
5000 and 2500 pairs; Alagin ID A-9), and the al-
lographic word database (2.7 million pairs; Alagin
ID A-7). We also used the information concerning
allographic words in the dictionary of the morpho-
logical analyzer JUMAN4.
Distributional similarity values between patterns
are based on the idea that patterns that appear in
similar contexts tend to have similar meanings and
as such are useful to recognize entailment (Lin and
Pantel, 2001). We computed as features several dis-
tributional similarity measures on the sets of each
pattern?s co-occurring noun pairs and their POS
tags, of nouns co-occurring in each variable slot, and
with each of the pattern?s unary sub-patterns.
We also added a few more uncategorizable fea-
tures. See Table 4 for more details.
5 Related Work
A number of previous work dealt with the recogni-
tion of contradictions between sentences. Harabagiu
et al (2006) proposed a contradiction detection
method that focuses on negation, antonymy and
some discourse information. Kawahara et al (2010)
also used negations and antonyms to extract con-
trastive/contradictory statements from the web to
present users with a bird ?s-eye view of statements
about a given topic. Bobrow et al (2007) showed
a method using logical forms with relatively precise
results. Ohki et al (2011) proposed a method to rec-
ognize confinment, a novel semantic relation related
to both entailment and contradiction. While we do
not deal ourselves directly with sentences, we expect
that the binary pattern pairs we acquire can play a
role similar to that of basic linguistic resources such
4 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
as antonyms and negations in these works. Closer
to our work, Ritter et al (2008) presented a method
for detecting contradictions between functional re-
lations like ?X was born in Y?, but these constitute
only a part of the semantic relations expressed by the
binary patterns we deal with in this paper.
Other works analyzed contradictions from lin-
guistic/semantic viewpoints. Voorhees (2008) ana-
lyzed the contradiction recognition-task of the RTE3
contest. Magnini and Cabrio (2010) examined rela-
tions between contradictions and textual entailment
samples. De Marneffe et al (2008) presented a
typology of contradictions, and showed that con-
tradictions can arise from a multitude of phenom-
ena. They showed contradictions based on lexical or
world knowledge are challenging and require a high-
level understanding of language and/or the world.
As stated in the introduction, these are the types of
contradictions our method focuses on.
6 Conclusion
This paper showed how to acquire a large number of
contradiction pairs between lexico-syntactic binary
patterns by exploiting (1) the interaction between
contradiction and entailment, and (2) excitation po-
larities. In the end, we could acquire 750,000 typed
contradiction pattern pairs with an estimated 80%
precision. The resulting contradiction pairs cov-
ered ones deeply related to world knowledge such
as the pair ??X reassures Y?, ?X betrays Y??. We ex-
pect our work to lead to a high level analysis of
textual information, such as flagging unreliable in-
formation or identifying important documents to be
surveyed for understanding complex social prob-
lems. We plan to release the data we acquired to
the NLP community through the ALAGIN Forum5.
5 http://www.alagin.jp/
702
References
J. Berant, I. Dagan, and J. Goldberger. 2011. Global
learning of typed entailment rules. In Proceedings of
ACL 2011, pages 610?619.
D. G. Bobrow, C. Condoravdi, R. Crouch, V. De Paiva,
L. Karttunen, T. H. King, R. Nairn, L. Price, and
A. Zaenen. 2007. Precision-focused textual inference.
In Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, page 16?21.
M.-C. De Marneffe, A. N. Rafferty, and C. D. Manning.
2008. Finding contradictions in text. Proceedings of
ACL 2008, page 1039?1047.
S. De Saeger, K. Torisawa, J. Kazama, K. Kuroda, and
M. Murata. 2009. Large scale relation acquisition us-
ing class dependent patterns. In Proceedings of ICDM
2009, page 764?769.
S.M. Harabagiu, A. Hickl, and V.F. Lacatusu. 2006.
Negation, contrast and contradiction in text process-
ing. In Proceedings of AAAI 2006, pages 755?762.
C. Hashimoto, K. Torisawa, K. Kuroda, S. De Saeger,
M. Murata, and J. Kazama. 2009. Large-scale verb
entailment acquisition from the web. In Proceedings
of EMNLP 2009, volume 3, page 1172?1181.
C. Hashimoto, K. Torisawa, S. De Saeger, J.-H. Oh, and
J. Kazama. 2012. Excitatory or inhibitory: A new se-
mantic orientation extracts contradiction and causality
from the web. In Proceedings of EMNLP 2012.
D. Kawahara, S. Kurohashi, and K. Inui. 2008. Grasp-
ing major statements and their contradictions toward
information credibility analysis of web contents. In
Proceedings of WI-IAT 2008, volume 1, page 393?
397.
D. Kawahara, K. Inui, and S. Kurohashi. 2010. Iden-
tifying contradictory and contrastive relations between
statements to outline web information on a given topic.
In Proceedings of COLING 2010, page 534?542.
J. Kazama and K. Torisawa. 2008. Inducing gazetteers
for named entity recognition by large-scale clustering
of dependency relations. Proceedings of ACL 2008,
page 407?415.
J. Kloetzer, S. De Saeger, K. Torisawa, M. Sano,
C. Hashimoto, and J. Gotoh. 2013. Large-scale acqui-
sition of entailment pattern pairs. In Information Pro-
cessing Society of Japan (IPSJ) Kansai-Branch Con-
vention.
S. Kurohashi and M. Nagao. 1994. KN parser: Japanese
dependency/case structure analyzer. In Proceedings
of the Workshop on Sharable Natural Language Re-
sources, page 48?55.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
page 159?174.
D. Lin and P. Pantel. 2001. Dirt - discovery of inference
rules from text. In Proceedings of the ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 323?328.
B. Magnini and E. Cabrio. 2010. Contradiction-focused
qualitative evaluation of textual entailment. In Pro-
ceedings of the Workshop on Negation and Speculation
in Natural Language Processing, page 86?94.
P. Malakasiotis and I. Androutsopoulos. 2007. Learning
textual entailment using SVMs and string similarity
measures. In Proceedings of the ACL- PASCAL Work-
shop on Textual Entailment and Paraphrasing, page 42
?47.
K. Murakami, E. Nichols, S. Matsuyoshi, A. Sumida,
S. Masuda, K. Inui, and Y. Matumoto. 2009. State-
ment map: assisting information crediblity analysis
by visualizing arguments. In Proceedings of the 3rd
workshop on Information credibility on the web, page
43?50. ACM.
M. Ohki, S. Matsuyoshi, J. Mizuno, K. Inui, E. Nichols,
K. Murakami, S. Masuda, and Y. Matsumoto. 2011.
Recognizing confinement in web texts. In the Pro-
ceedings of the Ninth International Conference on
Computational Semantics, page 215?224.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction?no, it?s not: a case study
using functional relations. In Proceedings of EMNLP
2008, pages 11?20.
S. Schoenmackers, O. Etzioni, D. S Weld, and J. Davis.
2010. Learning first-order horn clauses from web text.
In Proceedings of EMNLP 2010, page 1088?1098.
I. Szpektor, E. Shnarch, and I. Dagan. 2007. Instance-
based evaluation of entailment rule acquisition. In
Proceedings of ACL 2007, volume 45, page 456?463.
E. M. Voorhees. 2008. Contradictions and justifications:
Extensions to the textual entailment task. In Proceed-
ings of ACL 2008, page 63?71.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP
2003, page 81?88. Association for Computational
Linguistics.
703
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1619?1629,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Aid is Out There:
Looking for Help from Tweets during a Large Scale Disaster
Istva?n Varga? Motoki Sano? Kentaro Torisawa? Chikara Hashimoto?
Kiyonori Ohtake? Takao Kawai? Jong-Hoon Oh? Stijn De Saeger?
?Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Japan
{istvan, msano, torisawa, ch, kiyonori.ohtake, rovellia, stijn}@nict.go.jp
?Knowledge Discovery Research Laboratories, NEC Corporation, Japan
t-kawai@bx.jp.nec.com
Abstract
The 2011 Great East Japan Earthquake
caused a wide range of problems, and as
countermeasures, many aid activities were
carried out. Many of these problems and
aid activities were reported via Twitter.
However, most problem reports and corre-
sponding aid messages were not success-
fully exchanged between victims and lo-
cal governments or humanitarian organi-
zations, overwhelmed by the vast amount
of information. As a result, victims could
not receive necessary aid and humanitar-
ian organizations wasted resources on re-
dundant efforts. In this paper, we propose
a method for discovering matches between
problem reports and aid messages. Our
system contributes to problem-solving in
a large scale disaster situation by facilitat-
ing communication between victims and
humanitarian organizations.
1 Introduction
The 2011 Great East Japan Earthquake in March
11, 2011 killed 15,883 people and destroyed over
260,000 households (National Police Agency of
Japan, 2013). Accustomed way of living suddenly
became unmanageable and people found them-
selves in extreme conditions for months.
Just after the disaster, many people used Twitter
for posting problem reports and aid messages as
it functioned while most communication channels
suffered disruptions (Winn, 2011; Acar and Mu-
raki, 2011; Sano et al, 2012). Examples of such
problem reports and aid messages, translated from
Japanese tweets, are given below (P1, A1).
P1 My friend said infant formula is sold out. If
somebody knows shops in Sendai-city where
they still have it in stock, please let us know.
A1 At Jusco supermarket in Sendai, you can still
buy water and infant formula.
If A1 would have been forwarded to the sender
of P1, it could have helped since it would help
the ?friend? to obtain infant formula. But in re-
ality, the majority of such reports/messages, es-
pecially unforeseen ones went unnoticed amongst
the mass of information (Ohtake et al, 2013). In
addition, there were cases where many humani-
tarian organizations responded to the same prob-
lems and wasted precious resources. For instance,
many volunteers responded to problems which
were heavily reported by public media, leading
to oversupply (Saijo, 2012). Such waste of re-
sources could have been avoided if the organiza-
tions would have successfully shared the aid mes-
sages for the same problems.
Such observations motivated this work. We de-
veloped methods for recognizing problem reports
and aid messages in tweets and finding proper
matches between them. By browsing the discov-
ered matches, victims can be assisted to over-
come their problems, and humanitarian organiza-
tions can avoid redundant relief efforts. We define
problem reports, aid messages and their successful
matches as follows.
Problem report: A tweet that informs about the
possibility or emergence of a problem that re-
quires a treatment or countermeasure.
Aid message: A tweet that (1) informs about sit-
uations or actions that can be a remedy or so-
lution for a problem, or (2) informs that the
problem is solved or is about to be solved.
Problem-aid tweet match: A tweet pair is a
problem-aid tweet match (1) if the aid mes-
sage informs how to overcome the problem,
(2) if the aid message informs about the set-
1619
tlement of the problem, or (3) if the aid mes-
sage provides information which contributes
to the settlement of the problem.
In this work we excluded direct requests, such as
?Send us food!?, from problem reports. This is be-
cause it is relatively easy to recognize such direct
requests by checking mood types (i.e., imperative)
and their behavior is quite different from prob-
lem reports like ?People in Sendai are starving?.
Problem reports in this work do not directly state
which actions are required, only implying the ne-
cessity of a countermeasure through claiming the
existence of problems.
An underlying assumption of our method is that
we can find a noun-predicate dependency relation
that works as an indicator of problems and aids in
problem reports and aid messages, which we refer
to as problem nucleus and aid nucleus.1 An exam-
ple of problem nucleus is ?infant formula is sold
out? in P1, and that of aid nucleus is ?(can) buy
infant formula? in A1. Many problem-aid tweet
matches can be recognized through problem and
aid nuclei pairs.
We also assume that if the problem and aid nu-
clei match, they share the same noun. Then, the
semantics of predicates in the nuclei is the main
factor that decides whether the nuclei constitute
a match. We introduce a semantic classification
of predicates according to the framework of ex-
citation polarities proposed in Hashimoto et al
(2012). Our hypothesis is that excitation polarities
along with trouble expressions can characterize
problem reports, aid messages and their matches.
We developed a supervised method encoding such
information into its features.
An evident alternative to this approach is to use
sentiment analysis (Mandel et al, 2012; Tsagkali-
dou et al, 2011) assuming that problem reports
should include something ?bad? while aid mes-
sages describe something ?good?. However, we
will show that this does not work well in our exper-
iments. We think this is due to mismatch between
the concepts of problem/aid and sentiment polar-
ity. Note that previous work on ?demand? recogni-
tion also found similar tendencies (Kanayama and
Nasukawa, 2008).
Another issue in this task is, of course, the
context surrounding problem/aid nuclei. The fol-
1We found that out of 500 random tweets only 4.5% of
problem reports and 9.1% of aid messages did not contain
any problem report/aid message nuclei.
lowing (imaginary) tweets exemplify the problems
caused by contexts.
FP1 I do not believe infant formula is sold out
in Sendai.
FA1 At Jusco supermarket in Iwaki, you can still
buy infant formula.
The problem nuclei of FP1 and P1 are the same
but FP1 is not a problem report because of the ex-
pression ?I do not believe?. The aid nuclei of FA1
and A1 are the same but FA1 does not constitute
a proper match with P1 because FA1 and P1 re-
fer to different cities, ?Iwaki? and ?Sendai?. In
this work, the problems concerning the modality
and other semantic modifications to problem/aid
nuclei by context are dealt with by the introduc-
tion of features representing the text surrounding
the nuclei in machine learning. As for the loca-
tion problem, we apply a location recognizer to all
tweets and restrict the matching candidates to the
tweet pairs referring to the same location.
2 Approach
!"#$%"&"'()*&+*,*&-.(.+*,/+/0$0,/0,)-+$*#.(,'+
!"#$%&'("&!#")("&*#+,-.&"(
12001.+ 12001+/0$0,/0,)-+#0&*3",+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+
/-0('&11/+&("&*#+,-.&"(
*(/+!0..*'0++*(/+,5)&05.+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+ *(/+!0..*'0+*(/+,5)&05.+
!"#$%&'2/-0()3&&)('/)*4(
$#"4&0!+*,/+*(/+,"5,+*#0+1%0+.*!06+.*!0+'0"'#*$%()*&+&")*3",+
&")*3",+#0)"',(70#+
!"#$%&'2/-0('/)*4("&*#+,-.&"(
Figure 1: Problem-aid matching system overview.
We developed machine learning based systems
to recognize problem reports, aid messages and
problem-aid tweet matches. Figure 1 illustrates
the whole system. First, location names in tweets
are identified by matching tweets against our loca-
tion dictionary, described in Section 3. Then, each
tweet is paired with each dependency relation in
the tweet, which is a candidate of problem/aid nu-
clei and given to the problem report and aid mes-
sage recognizers. A tweet-nucleus-candidate pair
judged as problem report is combined with another
tweet-nucleus-candidate pair recognized as an aid
message if the two nuclei share the same noun and
the tweets share the same location name, and given
to the problem-aid match recognizer.
1620
In the following, problem and aid nuclei are
denoted by a noun-template pair. A template is
composed of a predicate and its argument posi-
tion. For instance, ?water supply stopped? in P2
is a problem nucleus, ?water supply recovered? in
A2 is an aid nucleus and they are denoted by the
noun-template pairs ?water supply, X stopped? and
?water supply, X recovered?.
P2 In Sendai city, water supply stopped.
A2 In Sendai city, water supply recovered.
Roughly speaking, we regard the tasks of prob-
lem report recognition and aid message recogni-
tion as the tasks of finding proper problem/aid
nuclei in tweets and our method performs these
tasks based on the semantic properties of nouns
and templates in problem/aid nucleus candidates
and their surrounding contexts.
The basic intuition behind this approach can
be explained using excitation polarity proposed in
Hashimoto et al (2012). Excitation polarity differ-
entiates templates into ?excitatory? or ?inhibitory?
with regard to the main function or effect of en-
tities referred to by their argument noun. While
excitatory templates (e.g., cause X, buy X, suf-
fer from X) entail that the main function or ef-
fect is activated or enhanced, inhibitory templates
(e.g., ruin X, prevent X, X runs out) entail that
the main function or effect is deactivated or sup-
pressed. The templates that do not fit into the
above categorization are classified as ?neutral?.
We observed that problem reports in general
included either of (A) a dependency relation be-
tween a noun referring to some trouble and an
excitatory template or (B) a dependency rela-
tion between a noun not referring to any trouble
and an inhibitory template. Examples of (A) in-
clude ?carbon monoxide poisoning, suffer from
X?, ?false rumor, spread X?. They refer to events
that activate troubles. On the other hand, (B) is
exemplified by ?school, X is collapsed?, ?battery,
X runs out?, which imply that some non-trouble
objects such as resources, appliances and facilities
are dysfunctional. We assume that if we can find
such dependency relations in tweets, the tweets are
likely to be problem reports.
Contrary, a tweet is more likely to be an aid
message when it includes either (C) a dependency
relation between a noun referring to some trouble
and an inhibitory template or (D) a dependency re-
lation between a noun not referring to any trou-
trouble non-trouble
excitatory (A) problem nucleus (D) aid nucleus
inhibitory (C) aid nucleus (B) problem nucleus
Table 1: Problem/aid-excitation matrix.
ble and an excitatory template. Examples of (C)
are ?flu, X was eradicated (in some shelter)? and
?debris, remove X?. They represent the dysfunc-
tion of troubles and can mean the solution or the
settlement of troubles. On the other hand, exam-
ples of (D) include ?school, X re-build? and ?baby
formula, buy X?. They entail that some resources
function properly or become available. These for-
mulations are summarized in Table 1.
As an interesting consequence of such a view
on problem/aid nucleus, we can say the following
regarding problem-aid tweet matchings: when a
problem nucleus and an aid nucleus are an ade-
quate match, the excitation polarities of their tem-
plates are opposite. Consider the following tweets.
P3 Some people were going back to Iwaki, but the
water system has not come back yet. It?s ter-
rible that bath is unusable.
A3 We open the bath for the public, located on
the 2F of Iwaki Kuhon temple. If you?re stay-
ing at a relief shelter and would like to take a
bath, you can use it.
?Bath is unusable? in P3 is a problem nucleus
while ?open the bath? in A3 is an aid nucleus.
Since the problem reported in P3 can be solved
with A3, they are a successful match. The in-
hibitory template ?X is unusable? indicates that
the function of ?bath?, a non-trouble expression,
is suppressed. The excitatory template ?open X?
indicates that the function of ?bath? is activated.
The same holds when we consider the noun re-
ferring to troubles like ?flu?. The polarity of the
template in a problem nucleus should be excita-
tory like ?flu is raging? while that of an aid nucleus
should be inhibitory like ?flu, X was eradicated?.
These examples keep the constraint that the prob-
lem and aid nucleus should have opposite polari-
ties when they constitute a match.
Note that the formulations of problem report,
aid message and their matches or the excitation
matrix (Table 1) were not presented to our anno-
tators and our test/training data may contain data
that contradict with the formulations. These for-
mulations constitute the hypothesis to be validated
in this work.
1621
An important point to be stressed here is that
there are problem-aid tweet matches that do not
fit into our formulations. For instance, we as-
sume that the problem nucleus and aid nucleus in
a proper match share the same noun. However,
tweet pairs such as ?There are many injured people
in Sendai city? and ?We are sending ambulances
to Sendai? can constitute a proper match, but there
is no proper problem-aid nuclei pair that share the
same noun in these tweets. (We can find the de-
pendency relations sharing ?Sendai? but they do
not express anything about the contents of prob-
lem and aid.) The point is that the tweet pairs can
be judged because people know ambulances can
be a countermeasure to injured people as world
knowledge. Introducing such world knowledge is
beyond the scope of this current study.
Also, we exclude direct requests from problem
reports. As mentioned in the introduction, identi-
fying direct requests is relatively easy, hence we
excluded them from our target.
3 Problem Report and Aid Message
Recognizers
We recognize problem reports and aid messages in
given tweets using a supervised classifier, SVMs
with linear kernel, which worked best in our pre-
liminary experiments. The feature set given to
the SVMs are summarized in the top part of Ta-
ble 2. Note that we used a common feature
set for both the problem report recognizer and
aid message recognizer and that it is categorized
into several types: features concerning trouble
expressions (TR), excitation polarity (EX), their
combination (TREX1) and word sentiment polar-
ity (WSP), features expressing morphological and
syntactic structures of nuclei and their context sur-
rounding problem/aid nuclei (MSA), features con-
cerning semantic word classes (SWC) appearing
in nuclei and their context, request phrases, such
as ?Please help us?, appearing in tweets (REQ),
and geographical locations in tweets recognized
by our location recognizer (GL). MSA is used to
express the modality of nuclei and other contex-
tual information surrounding nuclei. REQ was in-
troduced based on our observation that if there are
some requests in tweets, problem nuclei tend to
appear as justification for the requests.
We also attempted to represent nucleus template
IDs, noun IDs and their combinations directly in
our feature set to capture typical templates fre-
TR Whether the nucleus noun is a trouble/non-trouble expression.
EX1 The excitation polarity and the value of the excitation score of the
nucleus template.
TREX1 All possible combinations of trouble/non-trouble of TR and exci-
tation polarities of EX1.
WSP1 Whether the nucleus noun is positive/negative/not in theWord Sen-
timent Polarity (WSP) dictionary.
WSP2 Whether the nucleus template is positive/negative/not in the WSP
dictionary.
WSP3 Whether the nucleus template is followed by a positive/negative
word within the tweet.
MSA1 Morpheme n-grams, syntactic dependency n-grams in the tweet
and morpheme n-grams before and after the nucleus template.
(1 ? n ? 3)
MSA2 Character n-grams of the nucleus template to capture conjugation
and modality variations. (1 ? n ? 3)
MSA3 Morpheme and part-of-speech n-grams within the bunsetsu con-
taining the nucleus template to capture conjugation and modality
variations. (1 ? n ? 3) (A bunsetsu is a syntactic constituent
composed of a content word and several function words, the small-
est unit of syntactic analysis in Japanese.)
MSA4 The part-of-speech of the nucleus template?s head to capture
modality variations outside the nucleus template?s bunsetsu.
MSA5 The number of bunsetsu between the nucleus noun and the nucleus
template. We found that a long distance between the noun and the
template suggests parsing errors.
MSA6 Re-occurrence of the nucleus noun?s postpositional particle be-
tween the nucleus noun and the nucleus template. We found
that the re-occurrence of the same postpositional particle within
a clause suggests parsing errors.
SWC1 The semantic class n-grams in the tweet.
SWC2 The semantic class(es) of the nucleus noun.
REQ Presence of a request phrase in the tweet, identified from within
426 manually collected request phrases.
GL Geographical locations in the tweet identified using our location
recognizer. Existence/non-existence of locations in tweets are also
encoded.
EX2 Whether the problem and aid nucleus templates have the same or
opposite excitation polarities.
EX3 Product of the values of the excitation scores for the problem and
the aid nucleus template.
TREX2 All possible combinations of trouble/non-trouble of TR, excitation
polarity EX1 of the problem nucleus template and excitation po-
larity EX1 of the aid nucleus template.
SIM1 Common semantic word classes of the problem report and aid mes-
sage.
SIM2 Whether there are common nouns modifying the common nucleus
noun or not in the problem report and aid message.
SIM3 Whether the words in the same word class modify the common
nucleus noun or not in the problem report and aid message.
SIM4 The semantic similarity score between the problem nucleus tem-
plate and the aid nucleus template.
CTP Whether the problem nucleus template and the aid nucleus tem-
plate are in contradiction relation dictionary or not.
SSR1 Problem report recognizer?s SVM score of problem nucleus tem-
plate.
SSR2 Problem report recognizer?s SVM score of aid nucleus template.
SSR3 Aid message recognizer?s SVM score of the problem nucleus tem-
plate.
SSR4 Aid message recognizer?s SVM score of the aid nucleus template.
Table 2: Features used with the problem re-
port recognizer and the aid message recognizer
(above); additional features used in training the
problem-aid match recognizer (below).
quently appearing in problem and aid nuclei, but
since there was no improvement we omit them.
The other feature types need some non-trivial
dictionaries. In the following, we explain how we
created the dictionaries for each feature type along
with the motivation behind their introduction.
Trouble Expressions (TR) As mentioned previ-
ously, trouble expressions work as good evidence
for recognizing problem reports and aid messages.
The TR feature indicates whether the noun in the
problem/aid nucleus candidate is a trouble ex-
1622
pression or not. For this purpose, we created
a list of trouble expressions following the semi-
supervised procedure presented in De Saeger et al
(2008). After manual validation of the list, we ob-
tained 20,249 expressions referring to some trou-
bles, such as ?tsunami? and ?flu?. The value of the
TR feature is determined by checking whether the
nucleus noun is contained in the list.
Excitation Polarities (EX) The excitation po-
larities are also important in recognizing problem
reports and aid messages as mentioned before. For
constructing the dictionary for excitation polarities
of templates, we applied the bootstrapping proce-
dure in Hashimoto et al (2012) to 600 millionWeb
pages. Hashimoto?s method provides the value of
the excitation score in [?1, 1] for each template
indicating the polarities and their strength. Posi-
tive value indicates excitatory, negative value in-
hibitory and small absolute value neutral. After
manual checking of the results by the majority
vote of three human annotators (other than the au-
thors), we limited the templates to the ones that
have score values consistent with the majority vote
of the annotators, obtaining a dictionary consisting
of 7,848 excitatory, 836 inhibitory and 7,230 neu-
tral templates. The Fleiss? (1971) kappa-score was
0.48 (moderate agreement). We used the excita-
tion score values as feature values. Excitation has
already been used in many works, such as causal-
ity and contradiction extraction (Hashimoto et al,
2012) or Why-QA (Oh et al, 2013).
Word Sentiment Polarity (WSP) As we sug-
gested before, full-fledged sentiment analysis to
recognize the expressions, including clauses and
phrases, that refer to something good or bad was
not effective in our task. However, the sentiment
polarity, assigned to single words turned out to
be effective. To identify the sentiment polarity
of words, we employed the word sentiment polar-
ity dictionary used with a sentiment analysis tool
for Japanese, the Opinion Extraction Tool soft-
ware2, which is an implementation of Nakagawa
et al (2010). The dictionary includes 9,030 posi-
tive and 27,951 negative words. Note that we used
the Opinion Extraction Tool in the experiments to
check the effectiveness of the full-fledged senti-
ment analysis in this task.
Semantic Word Class (SWC) We assume that
nouns in the same semantic class behave simi-
2Provided at the ALAGIN Forum (http://www.alagin.jp/).
larly in crisis situations. For example, if ?infec-
tion? appears in a problem report, the tweets in-
cluding ?pulmonary embolism? are also likely to
be problem reports. Semantic word class features
are used to capture such tendencies. We applied
an EM-style word clustering algorithm in Kazama
and Torisawa (2008) to 600 millionWeb pages and
clustered 1 million nouns into 500 classes. This
algorithm has been used in many works, such as
relation extraction (De Saeger et al, 2011) and
Why-QA (Oh et al, 2012), and can generate vari-
ous kinds of semantically clean word classes, such
as foods, disease names, and natural disasters. We
used the word classes in tweets as features.3
Geographical Locations (GL) Our location
recognizer matches tweets against our loca-
tion dictionary. Location names and their
existence/non-existence in tweets constitute evi-
dence, thus we encoded such information into our
features. The location dictionary was created from
the Japan Post code data4 and Wikipedia, contain-
ing 2.7 million location names including cities,
schools and other facilities (Kazama et al, 2013).
4 Problem-Aid Match Recognizer
After problem report and aid message recogni-
tion, the positive outputs of the respective classi-
fiers are used as input in this step. The problem-
aid match recognizer classifies an aid message-
nucleus pair together with the problem report-
nucleus pair employing SVMs with linear ker-
nel, which performed best in this task again. The
problem-aid match recognizer uses all the features
used in the problem report recognizer and the aid
message recognizer along with additional features
regarding: excitation polarity (EX) and trouble
expressions (TR), distributional similarity (SIM),
contradiction (CTP) and SVM-scores of the prob-
lem report and aid message recognizers (SSR).
Here also we attempted to capture typical or fre-
quent matches of nuclei using template and noun
IDs and their combinations, but we did not observe
any improvement so we omit them from the fea-
ture set. The bottom part of Table 2 summarizes
the additional feature set, some of which are de-
scribed below in more detail.
3There is a slight complication here. For each noun n, EM
clustering estimates a probability distribution P (n|c?) for n
and semantic class c?. From this distribution we obtained
discrete semantic word classes by assigning each noun n to
semantic class c = argmaxc? p(c?|n).
4http://www.post.japanpost.jp/zipcode/download.html
1623
As for TR and EX, our intuition is that if a prob-
lem nucleus and an aid nucleus are an adequate
match, their excitation polarities are opposite, as
described in Section 2. We encode whether the ex-
citation polarities of nuclei templates are the same
or not in our features. Also, the excitation polar-
ities of problem and aid nuclei and TR are com-
bined (TREX1, TREX2) so that the classifier can
know whether the nuclei follow the constraint for
adequate matches described in Section 2.
As for SIM, if an aid message matches a prob-
lem report, besides the common nucleus noun, it is
reasonable to assume that certain contexts are se-
mantically similar. We capture this characteristic
in three ways. SIM1 looks for common semantic
word classes in the problem report and aid mes-
sage. SIM2 and SIM3 target the modifiers of the
common nucleus noun if they exist.
We also observed that if an aid message matches
a problem report, the problem nucleus template
and aid nucleus template are often distributionally
similar. A typical example is ?X is sold out? and
?buy X?. SIM4 captures this tendency. As the dis-
tributional similarity between templates, we used
a Bayesian distributional similarity measure pro-
posed by Kazama et al (2010).5
CTP indicates whether the problem and aid nu-
clei are in contradiction relation or not. This fea-
ture was implemented based on the observation
that when problem and aid nuclei are in contradic-
tion relation, they are often proper matches (e.g.,
?blackout, ?X starts?? and ?blackout, ?X ends??).
CTP indicates whether nucleus pairs are in the
one million contradiction phrase pairs6 automat-
ically obtained by applying a method proposed by
Hashimoto et al (2012) to 600 million Web pages.
5 Experiments
We evaluated our problem report recognizer and
problem-aid match recognizer. For the sake of
space, we give only the performance figures of the
aid message recognizer at the end of Section 5.1.
We collected tweets posted during and after
the 2011 Great East Japan Earthquake, between
March 10 and April 4, 2011. After applying
keyword-based filtering with a list of over 300
5The original similarity was defined over noun pairs and it
was estimated from dependency relations. Obtaining similar-
ity between template pairs, not noun pairs, is straightforward
given the same dependency relations. We used 600 million
Web pages for this similarity estimation.
6The precision of the pairs was reported as around 70%.
disaster related keywords, we obtained 55 million
tweets. After dependency parsing7, we used them
in our evaluation.
5.1 Problem Report Recognition
Firstly, we evaluated our problem report recog-
nizer. Particularly, we assessed the effect of ex-
citation polarities and trouble expressions in two
settings. The first is against a naturally distributed
gold standard data. The second targets problem
reports with problem nuclei unseen in the training
data.
In both experiments we observed that the per-
formance drops when excitation polarities and
trouble expressions are removed from the feature
set. The performance drop was larger in the sec-
ond experiment which suggests that the excitation
polarities and trouble expressions are more effec-
tive against unseen problem reports.
Training and test data for problem report recog-
nition consist of tweet-nucleus candidate pairs
randomly sampled from our 55 million tweet data.
The training data (R) and test data (T ) consist of
13,000 and 1,000 pairs, respectively, manually la-
beled by three annotators (other than the authors)
as problem or other. Final judgment was made by
majority vote. The Fleiss? kappa score for train-
ing and test data for annotation judgement is 0.74
(substantial agreement).
Our problem report recognizer and its variants
are listed in Table 3. Table 4 shows the evalua-
tion results. The proposed method achieved about
44% recall and nearly 80% precision, outperform-
ing all other systems in terms of precision, F-score
and average precision8. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.05).9 Note that F-measure dropped
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here EX and TR denote all excitation po-
larity and trouble expression related features, respectively,
including their combinations (TREX1).
PROPOSED+OET: The proposed method incorporating the
classification results of problem nucleus candidates by the
Opinion Extraction Tool as additional binary features.
RULE-BASED: The method that regards only nuclei satisfying
the constraint in Table 1 as problem nuclei.
Table 3: Evaluated problem report recognizers.
7http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
8We calculate average precision using the formula: aP =?n
k=1
(Prec(k)?rel(k))
n , where Prec(k) is the precision atcut-off k and rel(k) is an indicator function equaling 1 if
the item at rank k is relevant, zero otherwise.
9Throughout this paper we performed two-tailed test of
1624
Recognition system R (%) P (%) F (%) aP (%)
PROPOSED 44.26 79.41 56.83 71.82
PROPOSED-TR&EX 45.08 74.83 56.26 69.67
PROPOSED-EX 44.67 74.66 55.89 69.90
PROPOSED-TR 43.85 74.31 55.15 69.44
PROPOSED-MSA 28.69 70.71 40.81 57.74
PROPOSED-SWC 43.42 75.97 55.25 70.61
PROPOSED-WSP 43.14 77.83 55.50 70.45
PROPOSED-REQ 42.64 76.16 55.50 54.67
PROPOSED-GL 44.14 78.34 55.50 56.46
PROPOSED+OET 44.24 79.41 56.82 71.81
RULE-BASED 30.32 67.96 41.93 n/a
Table 4: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem report rec-
ognizers.
whenever each type of feature was removed, im-
plying that each type of feature is effective in this
task. Especially note the performance drop if we
remove excitation polarities (EX), trouble expres-
sion (TR) and both excitation and trouble expres-
sion features (TR&EX), confirming that they are
crucial in recognizing problem reports with high
accuracy. Also note that the performance of PRO-
POSED+OET was actually slightly worse than that
of the proposed method. This suggests that full-
fledged sentiment analysis is not effective at least
in this setting. The rule-based method achieved
relatively high precision despite of the low recall,
demonstrating the importance of problem and aid
nuclei formulations described in Section 1.
The second experiment assessed the efficiency
of our problem report recognizer against unseen
problem nuclei under the condition that every tem-
plate in nuclei has excitation polarity. We sampled
the training and test data so that the problem nu-
cleus nouns and templates in the training and test
data are disjoint. First we created a subset of the
test data by selecting the samples which had nu-
clei with excitation templates. We call this sub-
set T ?. Next, we removed samples from training
data R if either of their problem nouns or tem-
plates appeared in the nuclei of T ?. The result-
ing new training data (called R?) and test data (T ?)
consist of 6,484 and 407 tweet-nucleus candidate
pairs, respectively. We trained our problem report
recognizer using R? and tested its performance us-
ing T ?. Figure 2 shows the precision-recall curves
obtained by changing the threshold on the SVM
scores. The effectiveness of excitation polarities
and trouble expressions was more evident in this
setting. The PROPOSED?s performance was ac-
tually better in this setting (almost 50% recall at
population proportion (Ott and Longnecker, 2010) using
SVM-threshold=0.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pre
cisi
on
Recall
PROPOSED-TRPROPOSED-EX PROPOSED-TR&EXPROPOSED
Figure 2: Precision-recall curves of problem re-
port recognizers against unseen problem nuclei.
more that 80% precision), than the previous set-
ting, showing that excitation templates and trouble
expressions are crucial in achieving high perfor-
mance especially for unseen problem nuclei. The
same was confirmed when we removed excitation
polarity and trouble expression related features,
with performance dropping by 7.43 points in terms
of average precision. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.01). This implies, assuming that we
have a wide-coverage dictionary of templates with
excitation polarities, that excitation polarities are
important in dealing with unexpected problems in
disaster situations.
We also evaluated the aid-message recognizer,
using tweet-nucleus pairs in R and T as train-
ing and test data and the annotation scheme was
also the same. The average Fleiss? kappa score
was 0.55 (moderate agreement). Our recognizer
achieved 53.82% recall and 65.67% precision and
showed similar tendencies with the problem report
recognizer, with the excitation polarities and trou-
ble expressions contributing to higher accuracy.
We can conclude that excitation polarities and
trouble expressions are important in identifying
problem reports and aid messages during disaster
situations.
5.2 Problem-Aid Matching
Next, we evaluated the performance of the
problem-aid match recognizer. We applied our
problem report recognizer and aid message recog-
nizer to all 55 million tweets and combined the
tweet-nucleus pairs judged as problem reports and
aid messages, respectively, to create the training
and test data.
The training data consists of two parts (M1 and
M2). M1 includes many variations of the aid
messages for each problem report, while M2 en-
1625
sures diversity in nouns and templates in problem
nuclei. For M1, we randomly picked up problem
reports from the output of the problem report rec-
ognizer and to each we attached up to 30 randomly
picked, distinct aid messages that have the same
nucleus noun. Building M2 follows the construc-
tion method of M1, except that: (1) we used up
to 30 distinct problem nuclei for each noun; (2)
for each problem report we attached only one ran-
domly picked aid message.
In creating the test data T2, we followed the
construction method used for M2 to assess the
performance of our proposal with a large variety
of problems. M1, M2 and T2 consist of 3,000,
6,000 and 1,000 samples, respectively. The an-
notation was done by majority vote of three hu-
man annotators (other than the authors), the aver-
age Fleiss? kappa-score for training and test data
was 0.63 (substantial agreement).
We trained the problem-aid match recognizers
of Table 5 with M1 and M2. The evaluation
results performed on T2 are shown in Table 6.
We can observe that, among the nuclei related
features, the trouble expression (TR) and excita-
tion polarity (EX) features and their combination
(TR&EX) contribute most to the performance, al-
though the contribution of nuclei related features
is less in comparison to the problem report and aid
message recognition. The improvement in preci-
sion when using TR&EX is marginally significant
(p = 0.056). Instead, morphological and syntactic
analysis (MSA) and semantic word class (SWC)
features greatly improved performance.
As the final experiments, we evaluated top-
ranking matches of our problem-aid match recog-
nizer, where the recognizer classified all the possi-
ble combinations of tweet-nuclei pairs taken from
55 million tweets. In addition, we assessed the ef-
fectiveness of excitation polarities and trouble ex-
pressions by comparing all positive matches pro-
duced by our full problem-aid match recognizer
(PROPOSED) and those produced by the problem-
aid match recognizer (PROPOSED-TR&EX) that
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here also EX and TR denote all excitation
polarity and trouble expression related features, respec-
tively, including their combinations (TREX1 and TREX2).
RULE-BASED: The method that judges only problem-aid nuclei
combinations with opposite excitation polarities as proper
matches.
Table 5: Evaluated problem-aid match recogniz-
ers.
Matching system R (%) P (%) F (%) aP (%)
PROPOSED 30.67 70.42 42.92 55.16
PROPOSED-TR&EX 28.83 67.14 40.33 53.99
PROPOSED-EX 31.29 67.11 42.68 54.19
PROPOSED-TR 30.56 69.33 42.42 54.85
PROPOSED-MSA 13.50 53.66 21.57 44.52
PROPOSED-SWC 26.99 67.69 38.59 52.23
PROPOSED-WSP 30.61 69.51 42.50 54.81
PROPOSED-CTP 30.06 70.00 42.05 54.94
PROPOSED-SIM 29.95 70.11 41.97 54.98
PROPOSED-REQ 30.58 70.25 42.61 54.67
PROPOSED-GL 30.61 70.31 42.65 55.02
PROPOSED-SSR 30.67 69.44 42.72 54.91
RULE-BASED 15.33 17.36 16.28 n/a
Table 6: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem-aid match
recognizers.
did not use excitation polarities and trouble ex-
pressions in its feature set. Note that PROPOSED-
TR&EX was fed by the problem report and aid
message recognizers that didn?t use excitation po-
larities and trouble expressions. For both systems?
training data we used R for the problem report
and aid message recognizers; M1 and M2 for the
problem-aid matching recognizers.
PROPOSED and PROPOSED-TR&EX output 15.2
million and 13.4 million positive matches, cover-
ing 1,691 and 1,442 nucleus nouns, respectively.
Table 7 shows match samples identified with PRO-
POSED. We observed that the output of each sys-
tem was dominated by just a handful of frequent
nucleus nouns, such as ?water? or ?gasoline?. We
preferred to assess the performance of our system
against a large variation of problem-aid nuclei,
thus we restricted the number of matches to 10
for each noun10. After this restriction the number
of matches found by PROPOSED and PROPOSED-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000  9000
Pre
cisi
on
Rank
PROPOSED (unseen)PROPOSED-TR&EX (unseen)PROPOSED (all)PROPOSED-TR&EX (all)
Figure 3: Problem-aid match recognition perfor-
mance for ?all? and ?unseen? problem reports.
10Note that this setting is a pessimistic estimation of our
system?s overall performance, since according to our obser-
vations problem reports with very frequent nucleus nouns had
proper matches with a higher accuracy than problem reports
with less frequent nucleus nouns.
1626
Problem report: ???????????????????
??????????????????????????
??????????????????????????
(Starting from the 17th, the Iwaki Joban Hospital, the Iwaki
Urology Clinique, the Takebayashi Sadakichi Memorial Clin-
ique and the Izumi Central Clinique have all suspended dial-
ysis sessions. Patients are advised to urgently make contact.)
Aid message: ???????????????????
??????????????????????????
(Restart of dialysis sessions: short dialysis sessions are
available at the Iwaki Urology Clinique between 9 AM and
4 PM.)
Problem report: ??????????????????
?????????????????????????
?????????????????????????
?????
(Please spread this message. According to my father in
Sendai, there are more and more people whose phones ran
out of battery. We need phone chargers!)
Aid message: ???????????????????
???????????
([Please spread] At the City Hall of Wakabayashi-ku, Sendai,
you can recharge your phone battery.)
Table 7: Examples from the output of the proposed
method in the ?all? setting. Problem report and aid
message nuclei are boldfaced in the English trans-
lations.
TR&EX was 8,484 and 7,363, respectively.
The performance of PROPOSED and
PROPOSED-TR&EX were assessed in two
settings: ?all? and ?unseen?. For ?all?, we selected
400 problem-aid matches from the outputs of the
respective systems after applying the 10-match
restriction. For ?unseen?, first we removed the
samples from the systems? outputs if either the
nucleus noun or template pair appear in the nuclei
of the problem-aid match recognizers? training
data. Next we applied the same sampling process
as with ?all?. Three annotators (other than the
authors) manually labeled the sample sets, final
judgment being made by majority vote. The
Fleiss? kappa score for all test data was 0.73
(substantial agreement).
Figure 3 shows the systems? precision curves,
drawn from the samples whose X-axis positions
represent the ranks according to SVM scores. In
both scenarios we can confirm that excitation po-
larity and trouble expression related features con-
tribute to this task. In the ?all? setting in terms
of average precision calculated over the top 7,200
matches, PROPOSED?s 62.36% is 10.48 points
higher than that of PROPOSED-TR&EX. For un-
seen problem/aid nuclei PROPOSED method?s av-
erage precision of 58.57% calculated at the top
3,800 matches is 5.47 points higher than that of
PROPOSED-TR&EX at the same data point. The
improvement in precision when using TR&EX is
statistically significant in both settings (p < 0.01).
6 Related Work
Twitter has been observed as a platform for situ-
ational awareness during various crisis situations
(Starbird et al, 2010; Vieweg et al, 2010), as sen-
sors for an earthquake reporting system (Sakaki et
al., 2010; Okazaki and Matsuo, 2010) or to de-
tect epidemics (Aramaki et al, 2011). Besides
Twitter, blogs or forums have also been the tar-
get of community response analysis (Qu et al,
2009; Torrey et al, 2007). Similar to our work
are the ones of Neubig et al (2011) and Ishino et
al. (2012), who tackle specific problems that occur
during disasters (i.e., safety information and trans-
portation information, respectively); and Munro
(2011), who extracted ?actionable messages? (re-
quests and aids, indiscriminately), matching being
performed manually. Our work differs from (Neu-
big et al, 2011) and (Ishino et al, 2012) in that we
do not restrict the range of problem reports, and as
opposed to (Munro, 2011), matching is automatic.
Systems such as that of Seki (2011)11 or Munro
(2013)12 are successful examples of crisis crowd-
sourcing, but these require extensive human inter-
vention to coordinate useful information.
Another category of related work relevant to our
task is troubleshooting. Baldwin et al (2007) and
Raghavan et al (2010) use discussion forums to
solve technical problems using supervised learn-
ing methods, but these approaches presume that
the solution of a specific problem is within the
same thread. In our work we do not employ struc-
tural characteristics of tweets as restrictions (e.g.,
a problem report and its aid message need to be in
the same tweet chain).
7 Conclusions
In this paper, we proposed a method to dis-
cover matches between problem reports and aid
messages from tweets in large-scale disasters.
Through a series of experiments, we demonstrated
that the performance of the problem-aid match-
ing can be improved with the usage of seman-
tic orientation of excitation polarities, proposed in
(Hashimoto et al, 2012), and trouble expressions.
We are planning to deploy our system and re-
lease model files of the classifiers to assist relief
efforts in future crisis scenarios.
11http://www.sinsai.info/
12http://www.mission4636.org/
1627
References
Adam Acar and Yuya Muraki. 2011. Twitter for cri-
sis communication: Lessons learned from Japan?s
tsunami disaster. International Journal of Web
Based Communities, 7(3):392?402.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using Twitter. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 1568?1576.
Timothy Baldwin, David Martinez, and Richard B.
Penman. 2007. Automatic thread classification for
Linux user forum information access. In Proceed-
ings of the 12th Australasian Document Computing
Symposium (ADCS 2007), pages 72?79.
Stijn De Saeger, Kentaro Torisawa, and Jun?ichi
Kazama. 2008. Looking for trouble. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING 2008), pages 185?
192.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 825?835.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 5:378?382.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jong-Hoon Oh, and Jun?ichi Kazama.
2012. Excitatory or inhibitory: A new semantic
orientation extracts contradiction and causality from
the web. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2012), pages 619?630.
Aya Ishino, Shuhei Odawara, Hidetsugu Nanba, and
Toshiyuki Takezawa. 2012. Extracting transporta-
tion information and traffic problems from tweets
during a disaster: Where do you evacuate to? In
Proceedings of the Second International Conference
on Advances in Information Mining and Manage-
ment (IMMM 2012), pages 91?96.
Hiroshi Kanayama and Tetsuya Nasukawa. 2008. Tex-
tual demand analysis: Detection of users? wants and
needs from opinions. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING 2008), pages 409?416.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407?
415.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
Bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 247?256.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
Jun Goto, and Istva?n Varga. 2013. Saigaiji jouhou
e no shitsumon outo shisutemu no tekiyou no koko-
romi. (An attempt for applying question-answering
system on disaster related information). In Pro-
ceeding of the Nineteenth Annual Meeting of The
Association for Natural Language Processing. (in
Japanese).
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during Hurricane Irene. In Proceedings of the Sec-
ond Workshop on Language Analysis in Social Me-
dia (LASM 2012), pages 27?36.
Robert Munro. 2011. Subword and spatiotempo-
ral models for identifying actionable information in
Haitian Kreyol. In Proceedings of the Fifteenth
Conference on Computational Natural Language
Learning (CoNLL-2011), pages 68?77.
Robert Munro. 2013. Crowdsourcing and the
crisis-affected community. Information Retrieval,
16(2):210?266.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL HLT
2010), pages 786?794.
National Police Agency of Japan. 2013. Damage sit-
uation and public countermeasures associated with
2011 Tohoku district ? off the Pacific Ocean Earth-
quake. http://www.npa.go.jp/archive/
keibi/biki/higaijokyo_e.pdf. (accessed
on 30 April, 2013).
Graham Neubig, Yuichiroh Matsubayashi, Masato
Hagiwara, and Koji Murakami. 2011. Safety infor-
mation mining? what can NLP do in a disaster?.
In Proceedings of the 5th International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 965?973.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL 2012), pages 368?378.
1628
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013).
Kiyonori Ohtake, Kentaro Torisawa, Jun Goto, and
Stijn De Saeger. 2013. Saigaiji ni okeru hi-
saisha to kyuuen kyuujosha kan no souhoko komyu-
nikeeshon. (Bi-directional communication between
victims and rescures during a crisis). In Proceeding
of the Nineteenth Annual Meeting of The Association
for Natural Language Processing. (in Japanese).
Makoto Okazaki and Yutaka Matsuo. 2010. Seman-
tic Twitter: Analyzing tweets for real-time event
notification. In Proceedings of the 2008/2009 in-
ternational conference on Social software: Re-
cent trends and developments in social software
(BlogTalk 2008), pages 63?74.
R. Lyman Ott and Michael T. Longnecker, 2010. An
Introduction to Statistical Methods and Data Analy-
sis, chapter 10.2. Brooks Cole, 6th edition.
Yan Qu, Philip Fei Wu, and Xiaoqing Wang. 2009.
Online community response to major disaster: A
study of Tianya forum in the 2008 Sichuan Earth-
quake. In 42st Hawaii International International
Conference on Systems Science (HICSS-42), pages
1?11.
Preethi Raghavan, Rose Catherine, Shajith Ikbal,
Nanda Kambhatla, and Debapriyo Majumdar. 2010.
Extracting problem and resolution information from
online discussion forums. In Proceedings of the
16th International Conference on Management of
Data (COMAD 2010).
Takeo Saijo. 2012. Hito-o tasukeru sungoi shikumi. (A
stunning system that saves people). Diamond Inc.
(in Japanese).
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web (WWW 2010), pages 851?860.
Motoki Sano, Istva?n Varga, Jun?ichi Kazama, and Ken-
taro Torisawa. 2012. Requests in tweets dur-
ing a crisis: A systemic functional analysis of
tweets on the Great East Japan Earthquake and
the Fukushima Daiichi nuclear disaster. In Pa-
pers from the 39th International Systemic Func-
tional Congress (ISFC39), pages 135?140.
Haruyuki Seki. 2011. Higashi-nihon daishinsai fukkou
shien platform sinsai.info no naritachi to kongo no
kadai. (The organizational structure of sinsai.info
restoration support platform for the 2011 Great East
Japan Earthquake and future challenges). Journal of
digital practices, 2(4):237?241. (in Japanese).
Kate Starbird, Leysia Palen, Amanda L. Hughes, and
Sarah Vieweg. 2010. Chatter on the red: What
hazards threat reveals about the social life of mi-
croblogged information. In Proceedings of The
2010 ACM Conference on Computer Supported Co-
operative Work (CSCW 2010), pages 241?250.
Cristen Torrey, Moira Burke, Matthew L. Lee,
Anind K. Dey, Susan R. Fussell, and Sara B. Kiesler.
2007. Connected giving: Ordinary people coordi-
nating disaster relief on the Internet. In Proceedings
of the 40th Annual Hawaii International Conference
on System Sciences (HICSS-40), pages 179?188.
Katerina Tsagkalidou, Vassiliki Koutsonikola, Athena
Vakali, and Konstantinos Kafetsios. 2011. Emo-
tional aware clustering on micro-blogging sources.
In Proceedings of the 4th international conference
on Affective computing and intelligent interaction
(ACII 2011), pages 387?396.
Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and
Leysia Palen. 2010. Microblogging during two nat-
ural hazards events: What Twitter may contribute
to situational awareness. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems (CHI 2010), pages 1079?1088.
Patrick Winn. 2011. Japan tsunami disaster: As Japan
scrambles, Twitter reigns. GlobalPost, 18 March.
1629
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733?1743,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Why-Question Answering
using Intra- and Inter-Sentential Causal Relations
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ? Motoki Sano?
Stijn De Saeger? Kiyonori Ohtake?
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{?rovellia,? torisawa,? ch,? msano,?stijn,?kiyonori.ohtake}@nict.go.jp
Abstract
In this paper, we explore the utility of
intra- and inter-sentential causal relations
between terms or clauses as evidence for
answering why-questions. To the best of
our knowledge, this is the first work that
uses both intra- and inter-sentential causal
relations for why-QA. We also propose
a method for assessing the appropriate-
ness of causal relations as answers to a
given question using the semantic orienta-
tion of excitation proposed by Hashimoto
et al (2012). By applying these ideas
to Japanese why-QA, we improved preci-
sion by 4.4% against all the questions in
our test set over the current state-of-the-
art system for Japanese why-QA. In addi-
tion, unlike the state-of-the-art system, our
system could achieve very high precision
(83.2%) for 25% of all the questions in the
test set by restricting its output to the con-
fident answers only.
1 Introduction
?Why-question answering? (why-QA) is a task to
retrieve answers from a given text archive for a
why-question, such as ?Why are tsunamis gen-
erated?? The answers are usually text fragments
consisting of one or more sentences. Although
much research exists on this task (Girju, 2003;
Higashinaka and Isozaki, 2008; Verberne et al,
2008; Verberne et al, 2011; Oh et al, 2012), its
performance remains much lower than that of the
state-of-the-art factoid QA systems, such as IBM?s
Watson (Ferrucci et al, 2010).
In this work, we propose a quite straightfor-
ward but novel approach for such difficult why-
QA task. Consider the sentence A1 in Table 1,
which represents the causal relation between the
cause, ?the ocean?s water mass ..., waves are gen-
A1 [Tsunamis that can cause large coastal inundation
are generated]effect because [the ocean?s water
mass is displaced and, much like throwing a stone
into a pond, waves are generated.]cause
A2 [Earthquake causes seismic waves which set up
the water in motion with a large force.]cause
This causes [a tsunami.]effect
A3 [Tsunamis]effect are caused by [the sudden dis-
placement of huge volumes of water.]cause
A4 [Tsunamis weaken as they pass through
forests]effect because [the hydraulic resistance of
the trees diminish their energy.]cause
A5 [Automakers in Japan suspended production for an
array of vehicles]effect because [the magnitude 9
earthquake and tsunami hit their country on Friday,
March 11, 2011.]cause
Table 1: Examples of intra/inter-sentential causal
relations. Cause and effect parts of each causal re-
lation, marked with [..]cause and [..]effect, are con-
nected by the underlined cue phrases for causality,
such as because, this causes, and are caused by.
erated,? and its effect, ?Tsunamis ... are gener-
ated.? This is a good answer to the question, ?Why
are tsunamis generated??, since the effect part is
more or less equivalent to the (propositional) con-
tent of the question. Our method finds text frag-
ments that include such causal relations with an
effect part that resembles a given question and pro-
vides them as answers.
Since this idea looks quite intuitive, many peo-
ple would probably consider it as a solution to
why-QA. However, to our surprise, we could not
find any previous work on why-QA that took this
approach. Some methods utilized the causal re-
lations between terms as evidence for finding an-
swers (i.e., matching a cause term with an answer
text and its effect term with a question) (Girju,
2003; Higashinaka and Isozaki, 2008). Other ap-
proaches utilized such clue terms for causality as
?because? as evidence for finding answers (Mu-
rata et al, 2007). However, these algorithms did
not check whether an answer candidate, i.e., a text
fragment that may be provided as an answer, ex-
plicitly contains a complex causal relation sen-
1733
tence with the effect part that resembles a ques-
tion. For example, A5 in Table 1 is an incorrect an-
swer to ?Why are tsunamis generated??, but these
previous approaches would probably choose it as a
proper answer due to ?because? and ?earthquake?
(i.e., a cause of tsunamis). At least in our exper-
imental setting, our approach outperformed these
simpler causality-based QA systems.
Perhaps this approach was previously deemed
infeasible due to two non-trivial technical chal-
lenges. The first challenge is to accurately iden-
tify a wide range of causal relations like those in
Table 1 in answer candidates. To meet this chal-
lenge, we developed a sequence labeling method
that identifies not only intra-sentential causal re-
lations, i.e., the causal relations between two
terms/phrases/clauses expressed in a single sen-
tence (e.g., A1 in Table 1), but also the inter-
sentential causal relations, which are the causal
relations between two terms/phrases/clauses ex-
pressed in two adjacent sentences (e.g., A2) in a
given text fragment.
The second challenge is assessing the appropri-
ateness of each identified causal relation as an an-
swer to a given question. This is important since
the causal relations identified in the answer candi-
dates may have nothing to do with a given ques-
tion. In this case, we have to reject these causal
relations because they are inappropriate as an an-
swer to the question. When a single answer candi-
date contains many causal relations, we also have
to select the appropriate ones. Consider the causal
relations in A1?A4. Those in A1?A3 are appro-
priate answers to ?Why are tsunamis generated??,
but not the one in A4. To assess the appropri-
ateness, the system must recognize textual entail-
ment, i.e., ?tsunamis (are) generated? in the ques-
tion is entailed by all ?tsunamis are generated? in
A1, ?cause a tsunami? in A2 and ?tsunamis are
caused? in A3 but not by ?tsunamis weaken? in
A4. This quite difficult task is currently being
studied by many researchers in the RTE field (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
al., 2010; Shima et al, 2011; Bentivogli et al,
2011). To meet this challenge, we developed a
relatively simple method that can be seen as a
lightweight approximation for this difficult RTE
task, using excitation polarities (Hashimoto et al,
2012).
Through our experiments on Japanese why-QA,
we show that a combination of the above methods
can improve why-QA accuracy. In addition, our
proposed method can be successfully combined
with other approaches to why-QA and can con-
tribute to higher accuracy. As a final result, we im-
proved the precision by 4.4% against all the ques-
tions in our test set over the current state-of-the-art
system of Japanese why-QA (Oh et al, 2012). The
difference in the performance became much larger
when we only compared the highly confident an-
swers of each system. When we made our sys-
tem provide only its confident answers according
to their confidence score given by our system, the
precision of these confident answers was 83.2%
for 25% of all the questions in our test set. In the
same setting, the precision of the state-of-the-art
system (Oh et al, 2012) was only 62.4%.
2 Related Work
Although there were many previous works on the
acquisition of intra- and inter-sentential causal re-
lations from texts (Khoo et al, 2000; Girju, 2003;
Inui and Okumura, 2005; Chang and Choi, 2006;
Torisawa, 2006; Blanco et al, 2008; De Saeger et
al., 2009; De Saeger et al, 2011; Riaz and Girju,
2010; Do et al, 2011; Radinsky et al, 2012), their
application to why-QA was limited to causal re-
lations between terms (Girju, 2003; Higashinaka
and Isozaki, 2008).
As previous attempts to improve why-QA per-
formance, such semantic knowledge as Word-
Net synsets (Verberne et al, 2011), semantic
word classes (Oh et al, 2012), sentiment analy-
sis (Oh et al, 2012), and causal relations between
terms (Girju, 2003; Higashinaka and Isozaki,
2008) has been used. These previous studies took
basically bag-of-words approaches and used the
semantic knowledge to identify certain seman-
tic associations using terms and n-grams. On
the other hand, our method explicitly identifies
intra- and inter-sentential causal relations between
terms/phrases/clauses that have complex struc-
tures and uses the identified relations to answer
a why-question. In other words, our method
considers more complex linguistic structures than
those used in the previous studies. Note that our
method can complement the previous approaches.
Through our experiments, we showed that it is
possible to achieve a higher precision by combin-
ing our proposed method with bag-of-words ap-
proaches considering semantic word classes and
sentiment analysis in our previous work (Oh et al,
1734
Document	 ?retrieval	 ?from	 ?Japanese	 ?web	 ?texts	 ?
Answer	 ?candidate	 ?extrac?on	
Answer	 ?candidate	 ?extrac?on	 ?	 ?from	 ?the	 ?retrieved	 ?documents	 ?
Answer	 ?re-??ranker	 ?
Answer	 ?re-??ranking	
top-n answer	 ?candidates	 ?by	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?causal	 ?rela?on	 ?recogni?on	 ?
Causal	 ?rela?on	 ?recogni?on	 ?model	 ?top-n answer	 ?candidates	 ?
training	 ?
training	 ?
Why-??ques?on	 ?
recogni?on	 ?	 ?
recogni?on	 ?	 ?
Figure 1: System architecture
2012).
3 System Architecture
We first describe the system architecture of
our QA system before describing our proposed
method. It is composed of two components: an-
swer candidate extraction and answer re-ranking
(Fig. 1). This architecture is basically the same as
that used in our previous work (Oh et al, 2012).
We extended our previous work by introducing
causal relations recognized from answer candi-
dates to the answer re-ranking. The features used
in our previous work are very different from those
in this work, and we found that combining both
improves accuracy.
Answer candidate extraction: In our previous
work, we implemented the method of Murata et
al. (2007) for our answer candidate extractor. We
retrieved documents from Japanese web texts us-
ing Boolean AND and OR queries generated from
the content words in why-questions. Then we ex-
tracted passages of five sentences from these re-
trieved documents and ranked them with the rank-
ing function proposed by Murata et al (2007).
This method ranks a passage higher when it con-
tains more query terms that are closer to each other
in the passage. We used a set of clue terms, includ-
ing the Japanese counterparts of cause and reason,
as query terms for the ranking. The top ranked
passages are regarded as answer candidates in the
answer re-ranking. See Murata et al (2007) for
more details.
Answer re-ranking: Re-ranking the answer
candidates is done by a supervised classifier
(SVMs) (Vapnik, 1995). In our previous work, we
employed three types of features for training the
re-ranker: morphosyntactic features (n-grams of
morphemes and syntactic dependency chains), se-
mantic word class features (semantic word classes
obtained by automatic word clustering (Kazama
and Torisawa, 2008)) and sentiment polarity fea-
tures (word and phrase polarities). Here, we used
semantic word classes and sentiment polarities for
identifying such semantic associations between a
why-question and its answer as ?if a disease?s
name appears in a question, then answers that in-
clude nutrient names are more likely to be correct?
by semantic word classes and ?if something un-
desirable happens, the reason is often also some-
thing undesirable? by sentiment polarities. In this
work, we propose causal relation features gener-
ated from intra- and inter-sentential causal rela-
tions in answer candidates and use them along
with the features proposed in our previous work
for training our re-ranker.
4 Causal Relations for Why-QA
We describe causal relation recognition in Sec-
tion 4.1 and describe the features (of our re-ranker)
generated from causal relations in Section 4.2.
4.1 Causal Relation Recognition
We restrict causal relations to those expressed by
such cue phrases for causality as (the Japanese
counterparts of) because and as a result like in
the previous work (Khoo et al, 2000; Inui and
Okumura, 2005) and recognize them in the fol-
lowing two steps: extracting causal relation candi-
dates and recognizing causal relations from these
candidates.
4.1.1 Extracting Causal Relation Candidates
We identify cue phrases for causality in answer
candidates using the regular expressions in Ta-
ble 2. Then, for each identified cue phrase, we
extract three sentences as a causal relation candi-
date, where one contains the cue phrase and the
other two are the previous and next sentences in
the answer candidates. When there is more than
one cue phrase in an answer candidate, we use
all of them for extracting the causal relation can-
didates, assuming that each of the cue phrases is
linked to different causal relations. We call a cue
phrase used for extracting a causal relation candi-
date a c-marker (causality marker) of the candi-
date to distinguish it from the other cue phrases in
the same causal relation candidate.
1735
Regular expressions Examples
(D|?)? ?? P? ?? (for),??? (for),????
(as a result),???? (for)
?? ?? (since or because of)
?? (??|?) ???? (from the fact that),??
? (by the fact that)
(??|??) C ??? (because),??? (It is be-
cause)
D? RCT (P|C)+ ??? (the reason is), ???
(is the cause),?????? (from
this reason)
Table 2: Regular expressions for identifying cue
phrases for causality. D, P and C represent
demonstratives (e.g., ?? (this) and ?? (that)),
postpositions (including case markers such as ?
(nominative), ? (genitive)), and copula (e.g., ?
? (is) and ??? (is)) in Japanese, respectively.
RCT, which represents Japanese terms meaning
reason, cause, or thanks to, is defined as fol-
lows: RCT = {?? (reason), ?? (cause), ?
? (cause), ??? (cause), ??? (thanks to),
?? (thanks to),?? (reason) }.
4.1.2 Recognizing Causal Relations
Next, we recognize the spans of the cause and ef-
fect parts of a causal relation linked to a c-marker.
We regard this task as a sequence labeling problem
and use Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001) as a machine learning frame-
work. In our task, CRFs take three sentences
of a causal relation candidate as input and gen-
erate their cause-effect annotations with a set of
possible cause-effect IOB labels, including Begin-
Cause (B-C), Inside-Cause (I-C), Begin-Effect (B-
E), Inside-Effect (I-E), and Outside (O). Fig 2
shows an example of such sequence labeling. Al-
though this example is about sequential labeling
shown on English sentences for ease of explana-
tion, it was actually done on Japanese sentences.
We used the three types of feature sets in Table 3
for training the CRFs, where j is in the range of
i? 4 ? j ? i+4 for current position i in a causal
relation candidate.
Type Features
Morphological feature mj , mj+1j , posj , posj+1j
Syntactic feature sj , sj+1j , bj , bj+1j
C-marker feature (mj , cm), (mj+1j , cm)
(sj , cm), (sj+1j , cm)
Table 3: Features for training CRFs, where
xj+1j = xjxj+1
Morphological features: mj and posj in Ta-
ble 3 represent the jth morpheme and the POS tag.
S1:	 ?Earthquake	 ?causes	 ?seismic	 ?waves	 ?which	 ?set	 ?up	 ?the	 ?water	 ?in	 ?mo?on	 ?with	 ?a	 ?large	 ?force.	 ?EOS	 ?S2:	 ?This	 ?causes	 ?a	 ?tsunami.	 ?EOS	 ?S3:	 ?EOA	 ?
S1	 ? Earthquake	 ? causes	 ? ?	 ? with	 ? a	 ? large	 ? force	 ? .	 ? EOS	 ?IOB	 ? B-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? O	 ?S2	 ? This	 ? causes	 ? a	 ? tsunami	 ? .	 ? EOS	 ?IOB	 ? O	 ? O	 ? B-??E	 ? I-??E	 ? I-??E	 ? O	 ?S3	 ? EOA	 ?IOB	 ? O	 ?
CRFs	 ?
A	 ?causal	 ?rela?on	 ?candidate	 ?from	 ?A2	 ?
Figure 2: Recognizing causal relations by se-
quence labeling: Underlined text This causes rep-
resents a c-marker, and EOS and EOA represent
end-of-sentence and end-of-answer candidates.
??	 ? ??	 ? ???	 ? ???	 ? ???????	 ? ???	 ? ??	 ? ????????	 ?
water	 ? ice	 ? if	 ?(it)	 ?becomes	 ? its	 ?volume	 ? because	 ?(it)	 ?	 ?increases	 ? an	 ?iceberg	 ? water	 ? float	 ?on	 ?(water)	 ?
Subtree	 ?informa?on	 ?used	 ?for	 ?syntac?c	 ?features	 ?	 ?
subtree	 ? subtree	 ? child	 ? child	 ? c-??marker	 ? subtree-??of-??parent	 ?
subtree-??of-??parent	 ?
parent	 ?
???????	 ?
???	 ?
[??????????????]cause???[?????????????]effect	 ?(Because	 ?[the	 ?volume	 ?of	 ?the	 ?water	 ?increases	 ?if	 ?it	 ?becomes	 ?ice]cause,	 ?[an	 ?iceberg	 ?floats	 ?on	 ?water]effect.)	 ?
????????	 ?
root	 ?
c-??marker	 ?node	 ?
Figure 3: Example of syntactic information related
to a c-marker used for syntactic features
We use JUMAN1, a Japanese morphological ana-
lyzer, for generating our morphological features.
Syntactic features: The span of the causal rela-
tions in a given causal relation candidate strongly
depends on the c-marker in the candidate. Es-
pecially for intra-sentential causal relations, their
cause and effect parts often appear in the subtrees
of the c-marker?s node or those of the c-marker?s
parent node in a syntactic dependency tree struc-
ture. Fig. 3 shows an example that follows this ob-
servation, where the c-marker node is represented
in a hexagon and the other nodes are in a rectan-
gle. Note that each node in Fig. 3 is a word phrase
(called a bunsetsu), which is the smallest unit of
syntactic analysis in Japanese. A bunsetsu is a
syntactic constituent composed of a content word
and several function words such as postpositions
and case markers. Syntactic dependency is repre-
sented by an arrow in Fig. 3. For example, there
is syntactic dependency from word phrase ??
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
1736
(water) to??? (if (it) becomes), i.e.,?? dep???
???. We encode this subtree information into
sj , which is the syntactic information of a word
phrase to which the jth morpheme belongs. sj
only has one of six values: 1) the c-marker?s node
(c-marker), 2) the c-marker?s child node (child),
3) the c-marker?s parent node (parent), 4) in the c-
marker?s subtree but not the c-marker?s child node
(subtree), 5) in the subtree of the c-marker?s par-
ent node but not the c-marker?s node (subtree-of-
parent) and 6) the others (others). bj is the word
phrase information of the jth morpheme (mj) that
represents whether mj is in the beginning or in-
side a word phrase. For generating our syntactic
features, we use KNP2, a Japanese syntactic de-
pendency parser.
C-marker features: As our c-marker features,
we use a pair composed of c-marker cm and one
of the following: mj , mj+1j , sj , or sj+1j .
4.2 Causal Relation Features
We use terms, partial trees (in a syntactic depen-
dency tree structure), and the semantic orienta-
tion of excitation (Hashimoto et al, 2012) to as-
sess the appropriateness of each causal relation ob-
tained by our causal relation recognizer as an an-
swer to a given question. Finding answers with
term matching and partial tree matching has been
used in the literature of question answering (Girju,
2003; Narayanan and Harabagiu, 2004; Moschitti
et al, 2007; Higashinaka and Isozaki, 2008; Ver-
berne et al, 2008; Surdeanu et al, 2011; Verberne
et al, 2011; Oh et al, 2012), while that with the
excitation polarity is proposed in this work.
We use three types of features. Each fea-
ture type expresses the causal relations in an an-
swer candidate that are determined to be appro-
priate as answers to a given question by term
matching (tf1?tf4), partial tree matching (pf1?
pf4) and excitation polarity matching (ef1?ef4).
We call these causal relations used for generating
our causal relation features candidates of an ap-
propriate causal relation in this section. Note that
if one answer candidate has more than one candi-
date of an appropriate causal relation found by one
matching method, we generated features for each
appropriate candidate and merged all of them for
the answer candidate.
2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
Type Description
tf1 word n-grams of causal relations
tf2 word class version of tf1
tf3 indicator for the existence of candidates of an
appropriate causal relation identified by term
matching in an answer candidate
tf4 number of matched terms in candidates of an ap-
propriate causal relation
pf1 syntactic dependency n-grams (n dependency
chain) of causal relations
pf2 word class version of pf1
pf3 indicator for the existence of candidates of an ap-
propriate causal relation identified by partial tree
matching in an answer candidate
pf4 number of matched partial trees in candidates of
an appropriate causal relation
ef1 types of noun-polarity pairs shared by causal re-
lations and the question
ef2 ef1 coupled with each noun?s word class
ef3 indicator for the existence of candidates of an ap-
propriate causal relation identified by excitation
polarity matching in an answer candidate
ef4 number of noun-polarity pairs shared by the
question and the candidates of an appropriate
causal relation
Table 4: Causal relation features: n in n-grams
is n = {2, 3} and n-grams in an effect part are
distinguished from those in a cause part.
4.2.1 Term Matching
Our term matching method judges that a causal re-
lation is a candidate of an appropriate causal rela-
tion if its effect part contains at least one content
word (nouns, verbs, and adjectives) in the ques-
tion. For example, all the causal relations of A1?
A4 in Table 1 are candidates of an appropriate
causal relation to the question, ?Why is a tsunami
generated??, by term matching with question term
tsunami.
tf1?tf4 are generated from candidates of an ap-
propriate causal relation identified by term match-
ing. The n-grams of tf1 and tf2 are restricted
to those containing at least one content word in
a question. We distinguish this matched word
from the other words by replacing it with QW, a
special symbol representing a word in the ques-
tion. For example, word 3-gram ?this/cause/QW?
is extracted from This causes tsunamis in A2 for
?Why is a tsunami generated?? Further, we cre-
ate a word class version of word n-grams by con-
verting the words in these word n-grams into their
corresponding word class using the semantic word
classes (500 classes for 5.5 million nouns) from
our previous work (Oh et al, 2012). These word
classes were created by applying the automatic
word clustering method of Kazama and Torisawa
(2008) to 600 million Japanese web pages. For
example, the word class version of word 3-gram
1737
?this/cause/QW? is ?this/cause/QW,WCtsunami?,
where WCtsunami represents the word class of
a tsunami. tf3 is a binary feature that indi-
cates the existence of candidates of an appropri-
ate causal relation identified by term matching in
an answer candidate. tf4 represents the degree
of the relevance of the candidates of an appro-
priate causal relation measured by the number of
matched terms: one, two, and more than two.
4.2.2 Partial Tree Matching
Our partial tree matching method judges a causal
relation as a candidate of an appropriate causal re-
lation if its effect part contains at least one par-
tial tree in a question, where the partial tree covers
more than one content word. For example, only
the causal relation A1 among A1?A4 is a can-
didate of an appropriate causal relation for ques-
tion ?Why are tsunamis generated?? by partial
tree matching because only its effect part contains
partial tree ?tsunamis dep??? (are) generated? of the
question.
pf1?pf4 are generated from candidates of an
appropriate causal relation identified by the par-
tial tree matching. The syntactic dependency n-
grams in pf1 and pf2 are restricted to those that
contain at least one content word in a question. We
distinguish this matched content word from the
other content words in the n-gram by converting
it to QW, which represents a content word in the
question. For example, syntactic dependency 2-
gram ?QW dep??? cause? and its word class version
?QW,WCtsunami dep??? cause? are extracted from
Tsunamis that can cause in A1. pf3 is a binary
feature that indicates whether an answer candidate
contains candidates of an appropriate causal rela-
tion identified by partial tree matching. pf4 rep-
resents the degree of the relevance of the candi-
date of an appropriate causal relation measured by
the number of matched partial trees: one, two, and
more than two.
4.2.3 Excitation Polarity Matching
Hashimoto et al (2012) proposed a semantic ori-
entation called excitation polarities. It classifies
predicates with their argument position (called
templates) into excitatory, inhibitory and neu-
tral. In the following, we denote a template
as ?[argument position,predicate].? According to
Hashimoto?s definition, excitatory templates im-
ply that the function, effect, purpose, or the role of
an entity filling an argument position in the tem-
plates is activated/enhanced. On the contrary, in-
hibitory templates imply that the effect, purpose
or the role of an entity is deactivated/suppressed.
Neutral templates are those that neither activate
nor suppress the function of an argument.
We assume that the meanings of a text can
be roughly captured by checking whether each
noun in the text is activated or suppressed in the
sense of the excitation polarity framework, where
the activation and suppression of each entity (or
noun) can be detected by looking at the excita-
tion polarities of the templates that are filled by
the entity. For instance, effect part ?tsunamis
that can cause large coastal inundation are gen-
erated? of A1 roughly means that ?tsunamis? are
activated and ?inundation? is (or can be) acti-
vated. This activation/suppression configuration
of the nouns is consistent with sentence ?tsunamis
are caused? in which ?tsunamis? are activated.
This consistency suggests that A1 is a good an-
swer to question ?Why are tsunamis caused??, al-
though the ?tsunamis? are modified by different
predicates; ?cause? and ?generate.? On the other
hand, effect part ?tsunamis weaken as they pass
through forests? of A4 implies that ?tsunamis?
are suppressed. This suggests that A4 is not
a good answer to ?Why are tsunamis caused??
Note that the consistency checking between ac-
tivation/suppression configurations of nouns3 in
texts can be seen as a rough but lightweight ap-
proximation of the recognition of textual entail-
ments or paraphrases.
Following the definition of excitation polarity
in Hashimoto et al (2012), we manually classi-
fied templates4 to each polarity type and obtained
8,464 excitatory templates, such as [?, ???]
([subject, increase]) and [?, ????] ([sub-
ject, improve]), 2,262 inhibitory templates, such
as [?, ??] ([object, prevent]) and [?, ??]
([subject, die]), and 7,230 neutral templates such
as [?, ???] ([object, consider]). With these
templates, we obtain activation/suppression con-
figurations (including neutral) for the nouns in the
causal relations in the answer candidates and ques-
3 Because the activation/suppression configurations of
nouns come from an excitation polarity of templates, ?[argu-
ment position,predicate],? the semantics of verbs in the tem-
plates are implicitly considered in this consistency checking.
4 Varga et al (2013) has used the same templates as ours,
except they restricted their excitation/inhibitory templates to
those whose polarity is consistent with that given by the au-
tomatic acquisition method of Hashimoto et al (2012).
1738
tions.
Next, we assume that a causal relation is ap-
propriate as an answer to a question if the effect
part of the causal relation and the question share
at least one common noun with the same polarity.
More detailed information concerning the config-
urations of all the nouns in all the candidates of an
appropriate causal relation (including their cause
parts) and the question are encoded into our fea-
ture set ef1?ef4 in Table 4 and the final judgment
is done by our re-ranker.
For generating ef1 and ef2, we classified all the
nouns coupled with activation/suppression/neutral
polarities in a causal relation into three types:
SAME (the question contains the same noun with
the same polarity), DiffPOL (the question con-
tains the same noun with different polarity), and
OTHER (the others). ef1 indicates whether each
type of noun-polarity pair exists in a causal rela-
tion. Note that the types for the effect and cause
parts are represented in distinct features. ef2 is the
same as ef1 except that the types are augmented
with the word classes of the corresponding nouns.
In other words, ef2 indicates whether each type
of noun-polarity pair exists in the causal relation
for each word class. ef3 indicates the existence of
candidates of an appropriate causal relation iden-
tified by this matching scheme, and ef4 repre-
sents the number of noun-polarity pairs shared by
the question and the candidates of an appropriate
causal relations (one, two, and more than two).
5 Experiments
We experimented with causal relation recognition
and why-QA with our causal relation features.
5.1 Data Set for Why-Question Answering
For our experiments, we used the same why-QA
data set as the one used in our previous work (Oh
et al, 2012). This why-QA data set is composed
of 850 Japanese why-questions and their top-20
answer candidates obtained by answer candidate
extraction from 600 million Japanese web pages.
Three annotators checked the top-20 answer can-
didates of these 850 questions and the final judg-
ment was made by their majority vote. Their inter-
rater agreement by Fleiss? kappa reported in Oh et
al. (2012) was substantial (? = 0.634). Among the
850 questions, 250 why-questions were extracted
from the Japanese version of Yahoo! Answers,
and another 250 were created by annotators. In
our previous work, we evaluated the system with
these 500 questions and their answer candidates as
training and test data in 10-fold cross-validation.
The other 350 why-questions were manually built
from passages describing the causes or reasons of
events/phenomena. These questions and their an-
swer candidates were used as additional training
data for testing subsamples in each fold during the
10-fold cross-validation. In our why-QA experi-
ments, we evaluated our why-QA system with the
same settings.
5.2 Data Set for Causal Relation Recognition
We built a data set composed of manually anno-
tated causal relations for evaluating our causal re-
lation recognition. As source data for this data set,
we used the same 10-fold data that we used for
evaluating our why-QA (500 questions and their
answer candidates). We extracted the causal re-
lation candidates from the answer candidates in
each fold, and then our annotator (not an author)
manually marked the span of the cause and effect
parts of a causal relation for each causal relation
candidate, keeping in mind that the causal rela-
tion must be expressed in terms of a c-marker in
a given causal relation candidate. Finally, we had
a data set made of 16,051 causal relation candi-
dates, 8,117 of which had a true causal relation;
the number of intra- and inter-sentential causal re-
lations were 7,120 and 997, respectively.
Note that this data set can be partitioned into ten
folds by using the 10-fold partition of its source
data. We performed 10-fold cross validation to
evaluate our causal relation recognition with this
10-fold data.
5.3 Causal Relation Recognition
We used CRF++5 for training our causal relation
recognizer. In our evaluation, we judged a sys-
tem?s output as correct if both spans of the cause
and effect parts overlapped those in the gold stan-
dard. Evaluation was done by precision, recall,
and F1.
Precision Recall F1
BASELINE 41.9 61.0 49.7
INTRA-SENT 84.5 75.4 79.7
INTER-SENT 80.2 52.6 63.6
ALL 83.8 71.1 77.0
Table 5: Results of causal relation recognition (%)
Table 5 shows the result. BASELINE represents
5 http://code.google.com/p/crfpp/
1739
the result for our baseline system that recognizes
a causal relation by simply taking the two phrases
adjacent to a c-marker (i.e., before and after) as
cause and effect parts of the causal relation. We
assumed that the system had an oracle for judging
correctly whether each phrase is a cause part or an
effect part. In other words, we judged that a causal
relation recognized by BASELINE is correct if both
cause and effect parts in the gold standard are adja-
cent to a c-marker. INTRA-SENT and INTER-SENT
represent the results for intra- and inter-sentential
causal relations and ALL represents the result for
the both causal relations by our method. From
these results, we confirmed that our method rec-
ognized both intra- and inter-sentential causal rela-
tions with over 80% precision, and it significantly
outperformed our baseline system in both preci-
sion and recall rates.
Precision Recall F1
ALL-?MORPH? 80.8 66.4 72.9
ALL-?SYNTACTIC? 82.9 67.0 74.1
ALL-?C-MARKER? 76.3 51.4 61.4
ALL 83.8 71.1 77.0
Table 6: Ablation test results for causal relation
recognition (%)
We also investigated the contribution of the
three types of features used in our causal rela-
tion recognition to the performance. We evalu-
ated the performance when we removed one of
the three types of features (ALL-?MORPH?, ALL-
?SYNTACTIC? and ALL-?C-MARKER?) and com-
pared the results in these settings with the one
when all the feature sets were used (ALL). Ta-
ble 6 shows the result. We confirmed that all the
feature sets improved the performance, and we got
the best performance when using all of them. We
used the causal relations obtained from the 10-fold
cross validation for our why-QA experiments.
5.4 Why-Question Answering
We performed why-QA experiments to confirm
the effectiveness of intra- and inter-sentential
causal relations in a why-QA task. In
this experiment, we compared five systems:
four baseline systems (MURATA, OURCF, OH
and OH+PREVCF) and our proposed method
(PROPOSED).
MURATA corresponds to our answer candidate
extraction.
OURCF uses a re-ranker trained with only our
causal relation features.
OH, which represents our previous work (Oh et
al., 2012), has a re-ranker trained with mor-
phosyntactic, semantic word class, and senti-
ment polarity features.
OH+PREVCF is a system with a re-ranker
trained with the features used in OH and with
the causal relation feature proposed in Hi-
gashinaka and Isozaki (2008). The causal re-
lation feature includes an indicator that deter-
mines whether the causal relations between
two terms appear in a question-answer pair;
cause in an answer and its effect in a question.
We acquired the causal relation instances (be-
tween terms) from 600 million Japanese web
pages using the method of De Saeger et al
(2009) and exploited the top-100,000 causal
relation instances in this system.
PROPOSED has a re-ranker trained with our
causal relation features as well as the three
types of features proposed in Oh et al (2012).
Comparison between OH and PROPOSED re-
veals the contribution of our causal relation
features to why-QA.
We used TinySVM6 with a linear kernel
for training the re-rankers in OURCF, OH,
OH+PREVCF and PROPOSED. Evaluation was
done by P@1 (Precision of the top-answer) and
Mean Average Precision (MAP); they are the same
measures used in Oh et al (2012). P@1 measures
how many questions have a correct top-answer
candidate. MAP measures the overall quality of
the top-20 answer candidates. As mentioned in
Section 5.1, we used 10-fold cross-validation with
the same setting as the one used in Oh et al (2012)
for our experiments.
P@1 MAP
MURATA 22.2 27.0
OURCF 27.8 31.4
OH 37.4 39.1
OH+PREVCF 37.4 38.9
PROPOSED 41.8 41.0
Table 7: Why-QA results (%)
Table 7 shows the evaluation results. Our pro-
posed method outperformed the other four sys-
tems and improved P@1 by 4.4% over OH, which
is the-state-of-the-art system for Japanese why-
6 http://chasen.org/?taku/software/TinySVM/
1740
QA. OURCF showed the performance improve-
ment over MURATA. Although this suggests the
effectiveness of our causal relation features, the
overall performance of OURCF was lower than
that of OH. OH+PREVCF outperformed neither
OH nor PROPOSED. This suggests that our ap-
proach is more effective than previous causality-
based approaches (Girju, 2003; Higashinaka and
Isozaki, 2008), at least in our setting.
 20
 30
 40
 50
 60
 70
 80
 90
 100
 10  20  30  40  50  60  70  80  90  100
P
re
ci
si
on
 (%
)
% of questions
PROPOSED
OH
OurCF
Figure 4: Effect of causal relation features on the
top-answers
We also compared confident answers of
OURCF, OH, and PROPOSED by making each sys-
tem provide only the k confident top-answers (for
k questions) selected by their SVM scores given
by each system?s re-ranker. This reduces the num-
ber of questions that can be answered by a system,
but the top-answers become more reliable as k de-
creases. Fig. 4 shows this result, where the x axis
represents the percentage of questions (against all
the questions in our test set) whose top-answers
are given by each system, and the y axis repre-
sents the precision of the top-answers at a certain
point on the x axis. When both systems provided
top-answers for 25% of all the questions in our test
set, our method achieved 83.2% precision, which
is much higher than OH?s (62.4%). This exper-
iment confirmed that our causal relation features
were also effective in improving the quality of the
highly confident answers.
However, the high precision by our method was
bound to confident answers for a small number
of questions, and the difference in the precision
between OH and PROPOSED in Fig. 4 became
smaller as we considered more answers with lower
confidence. We think that one of the reasons is the
relatively small coverage of the excitation polarity
lexicon, a core resource in our excitation polarity
matching. We are planning to enlarge the lexicon
to deal with this problem.
Next, we investigated the contribution of the
intra- and inter-sentential causal relations to the
performance of our method. We used only one
of the two types of causal relations for generating
causal relation features (INTRA-SENT and INTER-
SENT) for training our re-ranker and compared the
results in these settings with the one when both
were used (ALL (PROPOSED)). Table 8 shows
the result. Both intra- and inter-sentential causal
relations contributed to the performance improve-
ment.
P@1 MAP
INTER-SENT 39.0 39.7
INTRA-SENT 40.4 40.5
ALL (PROPOSED) 41.8 41.0
Table 8: Results with/without intra- and inter-
sentential causal relations (%)
We also investigated the contributions of the
three types of causal relation features by ablation
tests (Table 9). When we do not use the fea-
tures by excitation polarity matching (ALL-{ef1?
ef4}), the performance is the worst. This implies
that the contribution of excitation polarity match-
ing exceeds the other two.
P@1 MAP
ALL-{tf1?tf4} 40.8 40.7
ALL-{pf1?pf4} 41.0 40.9
ALL-{ef1?ef4} 39.6 40.5
ALL (PROPOSED) 41.8 41.0
Table 9: Ablation test results for why-QA (%)
6 Conclusion
In this paper, we explored the utility of intra- and
inter-sentential causal relations for ranking answer
candidates to why-questions. We also proposed a
method for assessing the appropriateness of causal
relations as answers to a given question using the
semantic orientation of excitation. Through ex-
periments, we confirmed that these ideas are ef-
fective for improving why-QA, and our proposed
method achieved 41.8% P@1, which is 4.4% im-
provement over the current state-of-the-art system
of Japanese why-QA. We also showed that our
system achieved 83.2% precision for its confident
answers, when it only provided its confident an-
swers for 25% of all the questions in our test set.
1741
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search (JAIR), 38(1):135?187.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. In Pro-
ceedings of TAC.
E. Blanco, N. Castell, and Dan I. Moldovan. 2008.
Causal relation extraction. In Proceedings of
LREC?08.
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, 42(3):662?678.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 16(1):1?17.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of ICDM ?09, pages 764?769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong Hoon Oh, Istv?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of EMNLP ?11,
pages 825?835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of EMNLP ?11, pages 294?303.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Roxana Girju. 2003. Automatic detection of causal
relations for question answering. In Proceedings of
the ACL 2003 workshop on Multilingual summariza-
tion and question answering, pages 76?83.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orientation
extracts contradiction and causality from the web. In
Proceedings of EMNLP-CoNLL ?12.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-
questions. In Proceedings of IJCNLP ?08, pages
418?425.
Takashi Inui and Manabu Okumura. 2005. Investigat-
ing the characteristics of causal relations in Japanese
text. In In Annual Meeting of the Association
for Computational Linguistics (ACL) Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medical
database using graphical patterns. In Proceedings of
ACL ?00, pages 336?343.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML ?01, pages
282?289.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of ACL ?07,
pages 776?783.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A sys-
tem for answering non-factoid Japanese questions
by using passage retrieval weighted based on type
of answer. In Proceedings of NTCIR-6.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of COLING ?04, pages 693?701.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of EMNLP-CoNLL ?12, pages 368?378.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proceedings of WWW
?12, pages 909?918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In ICSC
?10, pages 361?368.
Hideki Shima, Hiroshi Kanayama, Cheng wei Lee,
Chuan jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of NTCIR-9 RITE: Recognizing Inference in TExt.
In Proceedings of NTCIR-9.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
1742
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL ?06, pages 57?64.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Istvan Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of ACL ?13.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2008. Using syntactic infor-
mation for improving why-question answering. In
Proceedings of COLING ?08, pages 953?960.
Suzan Verberne, Lou Boves, and Wessel Kraaij. 2011.
Bringing why-qa to web search. In Proceedings of
ECIR ?11, pages 491?496.
1743
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 987?997,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Toward Future Scenario Generation: Extracting Event Causality
Exploiting Semantic Relation, Context, and Association Features
Chikara Hashimoto? Kentaro Torisawa? Julien Kloetzer? Motoki Sano?
Istva?n Varga? Jong-Hoon Oh? Yutaka Kidawara??
? ? ? ? ? ??National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan
?NEC Knowledge Discovery Research Laboratories, Nara, 630-0101, Japan
{
? ch, ? torisawa, ? julien, ? msano, ? rovellia, ??kidawara}@nict.go.jp
Abstract
We propose a supervised method of
extracting event causalities like conduct
slash-and-burn agriculture?exacerbate
desertification from the web using se-
mantic relation (between nouns), context,
and association features. Experiments
show that our method outperforms base-
lines that are based on state-of-the-art
methods. We also propose methods of
generating future scenarios like conduct
slash-and-burn agriculture?exacerbate
desertification?increase Asian dust (from
China)?asthma gets worse. Experi-
ments show that we can generate 50,000
scenarios with 68% precision. We also
generated a scenario deforestation con-
tinues?global warming worsens?sea
temperatures rise?vibrio parahaemolyti-
cus fouls (water), which is written in no
document in our input web corpus crawled
in 2007. But the vibrio risk due to global
warming was observed in Baker-Austin
et al (2013). Thus, we ?predicted? the
future event sequence in a sense.
1 Introduction
The world can be seen as a network of causal-
ity where people, organizations, and other kinds
of entities causally depend on each other. This
network is so huge and complex that it is almost
impossible for humans to exhaustively predict the
consequences of a given event. Indeed, after the
Great East Japan Earthquake in 2011, few ex-
pected that it would lead to an enormous trade
deficit in Japan due to a sharp increase in en-
ergy imports. For effective decision making that
carefully considers any form of future risks and
chances, we need a system that helps humans do
scenario planning (Schwartz, 1991), which is a
decision-making scheme that examines possible
future events and assesses their potential chances
and risks. Our ultimate goal is to develop a system
that supports scenario planning through generat-
ing possible future events using big data, which
would contain what Donald Rumsfeld called ?un-
known unknowns?1 (Torisawa et al, 2010).
To this end, we propose a supervised method
of extracting such event causality as conduct
slash-and-burn agriculture?exacerbate desertifi-
cation and use its output to generate future sce-
narios (scenarios), which are chains of causal-
ity that have been or might be observed in
this world like conduct slash-and-burn agricul-
ture?exacerbate desertification?increase Asian
dust (from China)?asthma gets worse. Note that,
in this paper, A?B denotes that A causes B, which
means that ?if A happens, the probability of B in-
creases.? Our notion of causality should be inter-
preted probabilistically rather than logically.
Our method extracts event causality based on
three assumptions that are embodied as features
of our classifier. First, we assume that two nouns
(e.g. slash-and-burn agriculture and desertifica-
tion) that take some specific binary semantic rela-
tions (e.g. A CAUSES B) tend to constitute event
causality if combined with two predicates (e.g.
conduct and exacerbate). Note that semantic re-
lations are not restricted to those directly relevant
to causality like A CAUSES B but can be those that
might seem irrelevant to causality like A IS AN
INGREDIENT FOR B (e.g. plutonium and atomic
bomb as in plutonium is stolen?atomic bomb is
made). Our underlying intuition is the observation
that event causality tends to hold between two en-
tities linked by semantic relations which roughly
entail that one entity strongly affects the other.
Such semantic relations can be expressed by (oth-
erwise unintuitive) patterns like A IS AN INGRE-
DIENT FOR B. As such, semantic relations like the
MATERIAL relation can also be useful. (See Sec-
1http://youtu.be/GiPe1OiKQuk
987
tion 3.2.1 for a more intuitive explanation.)
Our second assumption is that there are gram-
matical contexts in which event causality is more
likely to appear. We implement what we con-
sider likely contexts for event causality as con-
text features. For example, a likely context of
event causality (underlined) would be: CO2 levels
rose, so climatic anomalies were observed, while
an unlikely context would be: It remains uncertain
whether if the recession is bottomed the declining
birth rate is halted. Useful context information in-
cludes the mood of the sentences (e.g., the uncer-
tainty mood expressed by uncertain above), which
is represented by lexical features (Section 3.2.2).
The last assumption embodied in our associa-
tion features is that each word of the cause phrase
must have a strong association (i.e., PMI, for ex-
ample) with that of the effect phrase as slash-and-
burn agriculture and desertification in the above
example, as in Do et al (2011).
Our method exploits these features on top of our
base features such as nouns and predicates. Exper-
iments using 600 million web pages (Akamine et
al., 2010) show that our method outperforms base-
lines based on state-of-the-art methods (Do et al,
2011; Hashimoto et al, 2012) by more than 19%
of average precision.
We require that event causality be self-
contained, i.e., intelligible as causality without the
sentences from which it was extracted. For ex-
ample, omit toothbrushing?get a cavity is self-
contained, but omit toothbrushing?get a girl-
friend is not since this is not intelligible without a
context: He omitted toothbrushing every day and
got a girlfriend who was a dental assistant of den-
tal clinic he went to for his cavity. This is im-
portant since future scenarios, which are gener-
ated by chaining event causality as described be-
low, must be self-contained, unlike Hashimoto et
al. (2012). To make event causality self-contained,
we wrote guidelines for manually annotating train-
ing/development/test data. Annotators regarded
as event causality only phrase pairs that were
interpretable as event causality without contexts
(i.e., self-contained). From the training data, our
method seemed to successfully learn what self-
contained event causality is.
Our scenario generation method generates sce-
narios by chaining extracted event causality; gen-
erating A?B?C from A?B and B?C. The chal-
lenge is that many acceptable scenarios are over-
looked if we require the joint part of the chain (B
above) to be an exact match. To increase the num-
ber of acceptable scenarios, our method identifies
compatibility w.r.t causality between two phrases
by a recently proposed semantic polarity, exci-
tation (Hashimoto et al, 2012), which properly
relaxes the chaining condition (Section 3.1 de-
scribes it). For example, our method can iden-
tify the compatibility between sea temperatures
are high and sea temperatures rise to chain global
warming worsens?sea temperatures are high
and sea temperatures rise?vibrio parahaemolyti-
cus2 fouls (water). Accordingly, we generated
a scenario deforestation continues?global warm-
ing worsens?sea temperatures rise?vibrio para-
haemolyticus fouls (water), which is written in
no document in our input web corpus that was
crawled in 2007, but the vibrio risk due to global
warming has actually been observed in the Baltic
sea and reported in Baker-Austin et al (2013). In
a sense, we ?predicted? the event sequence re-
ported in 2013 by documents written in 2007. Our
experiments also show that we generated 50,000
scenarios with 68% precision, which include con-
duct terrorist operations?terrorist bombing oc-
curs?cause fatalities and injuries?cause eco-
nomic losses and the above ?slash-and-burn agri-
culture? scenario (Section 5.2). Neither is written
in any document in our input corpus.
In this paper, our target language is Japanese.
However, we believe that our ideas and methods
are applicable to many languages. Examples are
translated into English for ease of explanation.
Supplementary notes of this paper are available
at http://khn.nict.go.jp/analysis/
member/ch/acl2014-sup.pdf.
2 Related Work
For event causality extraction, clues used by
previous methods can roughly be categorized
as lexico-syntactic patterns (Abe et al, 2008;
Radinsky et al, 2012), words in context (Oh et
al., 2013), associations among words (Torisawa,
2006; Riaz and Girju, 2010; Do et al, 2011), and
predicate semantics (Hashimoto et al, 2012). Be-
sides features similar to those described above, we
propose semantic relation features3 that include
those that are not obviously related to causality.
We show that such thorough exploitation of new
and existing features leads to high performance.
2A bacterium in the sea causing food-poisoning.
3Radinsky et al (2012) and Tanaka et al (2012) used se-
mantic relations to generalize acquired causality instances.
988
Other clues include shared arguments (Torisawa,
2006; Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009), which we ignore since we tar-
get event causality about two distinct entities.
To the best of our knowledge, future scenario
generation is a new task, although previous works
have addressed similar tasks (Radinsky et al,
2012; Radinsky and Horvitz, 2013). Neither in-
volves chaining and restricts themselves to only
one event causality step. Besides, the events they
predict must be those for which similar events
have previously been observed, and their method
only applies to news domain.
Some of the scenarios we generated are written
on no page in our input web corpus. Similarly,
Tsuchida et al (2011) generated semantic knowl-
edge like causality that is written in no sentence.
However, their method cannot combine more than
two pieces of knowledge unlike ours, and their tar-
get knowledge consists of nouns, but ours consists
of verb phrases, which are more informative.
Tanaka et al (2013)?s web information analy-
sis system provides a what-happens-if QA service,
which is based on our scenario generation method.
3 Event Causality Extraction Method
This section describes our event causality extrac-
tion method. Section 3.1 describes how to extract
event causality candidates, and Section 3.2 details
our features. Section 3.3 shows how to rank event
causality candidates.
3.1 Event Causality Candidate Extraction
We extract the event causality between two events
represented by two phrases from single sentences
that are dependency parsed.4 We obtained sen-
tences from 600 million web pages. Each phrase
in the event causality must consist of a predicate
with an argument position (template, hereafter)
like conduct X and a noun like slash-and-burn
agriculture that completes X. We also require the
predicate of the cause phrase to syntactically de-
pend on the effect phrase in the sentence from
which the event causality was extracted; we guar-
antee this by verifying the dependencies of the
original sentence. In Japanese, since the tempo-
ral order between events is usually determined by
precedence in a sentence, we require the cause
phrase to precede the effect phrase. For context
4We used a Japanese dependency parser called J.DepP
(Yoshinaga and Kitsuregawa, 2009), available at http://
www.tkl.iis.u-tokyo.ac.jp/?ynaga/jdepp/.
feature extraction, the event causality candidates
are accompanied by the original sentences from
which they were extracted.
Excitation We only keep the event causality
candidates each phrase of which consists of exci-
tation templates, which have been shown to be ef-
fective for causality extraction (Hashimoto et al,
2012) and other semantic NLP tasks (Oh et al,
2013; Varga et al, 2013; Kloetzer et al, 2013a).
Excitation is a semantic property of templates that
classifies them into excitatory, inhibitory, and neu-
tral. Excitatory templates such as cause X entail
that the function, effect, purpose or role of their ar-
gument?s referent is activated, enhanced, or man-
ifested, while inhibitory templates such as lower
X entail that it is deactivated or suppressed. Neu-
tral ones like proportional to X belong to neither
of them. We collectively call both excitatory and
inhibitory templates excitation templates. We ac-
quired 43,697 excitation templates by Hashimoto
et al?s method and the manual annotation of exci-
tation template candidates.5 We applied the exci-
tation filter to all 272,025,401 event causality can-
didates from the web and 132,528,706 remained.
After applying additional filters (see Section A
in the supplementary notes) including those based
on a stop-word list and a causal connective list
to remove unlikely event causality candidates that
are not removed by the above filter, we finally ac-
quired 2,451,254 event causality candidates.
3.2 Features for Event Causality Classifier
3.2.1 Semantic Relation Features
We hypothesize that two nouns with some particu-
lar semantic relations are more likely to constitute
event causality. Below we describe the semantic
relations that we believe are likely to constitute
event causality.
CAUSATION is the causal relation between two
entities and is expressed by binary patterns like
A CAUSES B. Deforestation and global warming
might complete the A and B slots. We manually
collected 748 binary patterns for this relation. (See
Section B in the supplementary notes for examples
of our binary patterns.)
MATERIAL is the relation between a material
and a product made of it (e.g. plutonium and
5Hashimoto et al?s method constructs a network of tem-
plates based on their co-occurrence in web sentences with a
small number of polarity-assigned seed templates and infers
the polarity of all the templates in the network by a constraint
solver based on the spin model (Takamura et al, 2005).
989
atomic bomb) and can be expressed by A IS MADE
OF B. Its relation to event causality might seem
unclear, but a material can be seen as a ?cause?
of a product. Indeed materials can participate
in event causality with the help of such template
pairs as A is stolen?B is made as in plutonium is
stolen?atomic bomb is made. We manually col-
lected 187 binary patterns for this relation.
NECESSITY?s patterns include A IS NECES-
SARY FOR B, which can be filled with verbal apti-
tude and ability to think. Noun pairs with this rela-
tion can constitute event causality when combined
with template pairs like improve A?cultivate B.
We collected 257 patterns for this relation.
USE is the relation between means (or instru-
ments) and the purpose for using them. A IS USED
FOR B is a pattern of the relation, which can be
filled with e-mailer and exchanges of e-mail mes-
sages. Note that means can be seen as ?causing?
or ?realizing? the purpose of using the means in
this relation, and actually event causality can be
obtained by incorporating noun pairs of this rela-
tion into template pairs like activate A?conduct
B. 2,178 patterns were collected for this relation.
PREVENTION is the relation expressed by pat-
terns like A PREVENTS B, which can be filled with
toothbrushing and periodontal disease. This rela-
tion is, so to speak, ?negative CAUSATION? since
the entity denoted by the noun completing the A
slot makes the entity denoted by the B noun NOT
realized. Such noun pairs mean event causality
by substituting them into template pairs like omit
A?get B. The number of patterns is 490.
The experiments in Section 5.1.1 show that not
only CAUSATION and PREVENTION (?negative
CAUSATION?) but the other relations are also ef-
fective for event causality extraction.
In addition, we invented the EXCITATION rela-
tion that is expressed by binary patterns made of
excitatory and inhibitory templates (Section 3.1).
For instance, we make binary patterns A RISES B
and A LOWERS B from excitatory template rise X
and inhibitory template lower X respectively. The
EXCITATION relation roughly means that A acti-
vates B (excitatory) or suppresses it (inhibitory).
We simply add an additional argument position to
each template in the 43,697 excitation templates to
make binary patterns. We restricted the argument
positions (represented by Japanese postpositions)
of the A slot to either ha (topic marker), ga (nomi-
native), or de (instrumental) and those of the B slot
to either ha, ga, de, wo (accusative), or ni (dative),
SR1: Binary pattern of our semantic relations that co-
occurs with two nouns of an event causality candi-
date in our web corpus.
SR2: Semantic relation types (e.g CAUSATION and EN-
TAILMENT) of the binary pattern of SR1. EXCITA-
TION is divided into six sub types based on the ex-
citation polarity of the binary patterns, the argument
positions, and the existence of causative markers. A
CAUSATION pattern, B BY A, constitutes an indepen-
dent relation called the BY relation.
Table 1: Semantic relation features.
and obtained 55,881 patterns.
Moreover, for broader coverage, we acquired
binary patterns that entail or are entailed by one
of the patterns of the above six semantic relations.
Those patterns were acquired from our web cor-
pus by Kloetzer et al (2013b)?s method, which ac-
quired 185 million entailment pairs with 80% pre-
cision from our web corpus and was used for con-
tradiction acquisition (Kloetzer et al, 2013a). We
acquired 335,837 patterns by this method. They
are class-dependent patterns, which have seman-
tic class restrictions on arguments. The semantic
classes were obtained from our web corpus based
on Kazama and Torisawa (2008). See De Saeger
et al (2009), De Saeger et al (2011) and Kloet-
zer et al (2013a) for more on our patterns. They
collectively constitute the ENTAILMENT relation.
Table 1 shows our semantic relation features. To
use them, we first make a database that records
which noun pairs co-occur with each binary pat-
tern. Then we check a noun pair (the nouns of the
cause and effect phrases) for each event causality
candidate, and give the candidate all the patterns
in the database that co-occur with the noun pair.
3.2.2 Context Features
We believe that contexts exist where event causal-
ity candidates are more likely to appear, as de-
scribed in Section 1. We developed features that
capture the characteristics of likely contexts for
Japanese event causality (See Section C in the sup-
plementary notes). In a nutshell, they represent a
connective (C1 and C2 in Section C), the distance
between the elements of event causality candidate
(C3 and C4), words in context (C5 to C8), the ex-
istence of adnominal modifier (9 to C10), and the
existence of additional arguments of cause and ef-
fect predicates (C13 to C20), among others.
3.2.3 Association Features
These features measure the association strength
between slash-and-burn agriculture and deser-
990
AC1: The CEA value, the sum of AC2, AC3, and AC4.
AC2: Do et al?s S
pp
. This is the association measure
between predicates, which is the product of AC5,
AC6 and AC7 below. They are calculated from the
132,528,706 event causality candidates in Section
3.1. We omit Do et al?s Dist, which is a constant
since we set our window size to one.
AC3: Do et al?s S
pa
. This is the association measure be-
tween arguments and predicates, which is the sum
of AC8 and AC9. They are calculated from the
132,528,706 event causality candidates.
AC4: Do et al?s S
aa
, which is PMI between arguments.
We obtained it in the same way as Filter 5 in the sup-
plementary notes.
AC5: PMI between predicates.
AC6 / AC7: Do et al?s max / IDF .
AC8: PMI between a cause noun and an effect predicate.
AC9: PMI between a cause predicate and an effect noun.
Table 2: CEA-based association features.
tification in conduct slash-and-burn agricul-
ture?exacerbate desertification for instance and
consist of CEA-, Wikipedia-, definition-, and web-
based features. CEA-based features are based
on the Cause Effect Association (CEA) measure
of Do et al (2011). It consists of association
measures like PMI between arguments (nouns),
between arguments and predicates, and between
predicates (Table 2). Do et al used it (along
with discourse relations) to extract event causality.
Wikipedia-based features are the co-occurrence
counts and the PMI values between cause and ef-
fect nouns calculated using Wikipedia (as of 2013-
Sep-19). We also checked whether an Wikipedia
article whose title is a cause (effect) noun con-
tains its effect (cause) noun, as detailed in Section
D.1 in the supplementary notes. Definition-based
features, as detailed in Section D.2 in the sup-
plementary notes, resemble the Wikipedia-based
features except that the information source is the
definition sentences automatically acquired from
our 600 million web pages using the method of
Hashimoto et al (2011). Web-based features
provide association measures between nouns us-
ing various window sizes in the 600 million web
pages. See Section D.3 for detail. Web-based as-
sociation measures were obtained from the same
database as AC4 in Table 2.
3.2.4 Base Features
Base features represent the basic properties of
event causality like nouns, templates, and their ex-
citation polarities (See Section E in the supple-
mentary notes). For B3 and B4, 500 semantic
classes were obtained from our web corpus using
the method of Kazama and Torisawa (2008).
3.3 Event Causality Scoring
Using the above features, a classifier6 classifies
each event causality candidate into causality and
non-causality. An event causality candidate is
given a causality score CScore, which is the SVM
score (distance from the hyperplane) that is nor-
malized to [0, 1] by the sigmoid function 1
1+e
?x
.
Each event causality candidate may be given mul-
tiple original sentences, since a phrase pair can ap-
pear in multiple sentences, in which case it is given
more than one SVM score. For such candidates,
we give the largest score and keep only one origi-
nal sentence that corresponds to the largest score.7
Original sentences are also used for scenario gen-
eration, as described below.
4 Future Scenario Generation Method
Our future scenario generation method creates
scenarios by chaining event causalities. A naive
approach chains two phrase pairs by exact match-
ing. However, this approach would overlook many
acceptable scenarios as discussed in Section 1. For
example, global warming worsens?sea tempera-
tures are high and sea temperatures rise?vibrio
parahaemolyticus fouls (water) can be chained to
constitute an acceptable scenario, but the joint part
is not the same string. Note that the two phrases
are not simply paraphrases; temperatures may be
rising but remain cold, or they may be decreasing
even though they remain high.
What characterizes two phrases that can be the
joint part of acceptable scenarios? Although we
have no definite answer yet, we name it the causal-
compatibility of two phrases and provide its pre-
liminary characterization based on the excitation
polarity. Remember that excitatory templates like
cause X entail that X?s function or effect is acti-
vated, but inhibitory templates like lower X entail
that it is suppressed (Section 3.1). Two phrases
are causally-compatible if they mention the same
entity (typically described by a noun) that is pred-
icated by the templates of the same excitation po-
larity. Indeed, both X rise and X are high are ex-
citatory and hence sea temperatures are high and
sea temperatures rise are causally-compatible.8
6We used SVMlight with the polynominal kernel (d = 2),
available at http://svmlight.joachims.org.
7Future work will exploit other original sentences, as sug-
gested by an anonymous reviewer.
8Using other knowledge like verb entailment (Hashimoto
et al, 2009) can be helpful too, which is further future work.
991
Scenarios (scs) generated by chaining causally-
compatible phrase pairs are scored by Score(sc),
which embodies our assumption that an acceptable
scenario consists of plausible event causality pairs:
Score(sc) =
?
cs?CAUS(sc)
CScore(cs)
where CAUS(sc) is a set of event causality
pairs that constitutes sc and cs is a member of
CAUS(sc). CScore(cs), which is cs?s score,
was described in Section 3.3.
Our method optionally applies the following
two filters to scenarios for better precision: An
original sentence filter removes a scenario if two
event causality pairs that are chained in it are ex-
tracted from original sentences between which no
word overlap exists other than words constituting
causality pairs. In this case, the two event causal-
ity pairs tend to be about different topics and con-
stitute an incoherent scenario. A common argu-
ment filter removes a scenario if a joint part con-
sists of two templates that share no argument in
our ?argument, template? database, which is com-
piled from the syntactic dependency data between
arguments and templates extracted from our web
corpus. Such a scenario tends to be incoherent too.
5 Experiments
5.1 Event Causality Extraction
Next we describe our experiments on event causal-
ity extraction and show (a) that most of our fea-
tures are effective and (b) that our method outper-
forms the baselines based on state-of-the-art meth-
ods (Do et al, 2011; Hashimoto et al, 2012). Our
method achieved 70% precision at 13% recall; we
can extract about 69,700 event causality pairs with
70% precision, as described below.
For the test data, we randomly sampled 23,650
examples of ?event causality candidate, origi-
nal sentence? among which 3,645 were positive
from 2,451,254 event causality candidates ex-
tracted from our web corpus (Section 3.1). For
the development data, we identically collected
11,711 examples among which 1,898 were posi-
tive. These datasets were annotated by three anno-
tators (not the authors), who annotated the event
causality candidates without looking at the origi-
nal sentences. The final label was determined by
majority vote. The training data were created
by the annotators through our preliminary experi-
ments and consists of 112,110 among which 9,657
Method Ave. prec. (%)
Proposed 46.27
w/o Context features 45.68
w/o Association features 45.66
w/o Semantic relation features 44.44
Base features only 41.29
Table 3: Ablation tests.
Semantic relations Ave. prec. (%)
All semantic relations (Proposed) 46.27
CAUSATION 45.86
CAUSATION and PREVENTION 45.78
None (w/o Semantic relation features) 44.44
Table 4: Ablation tests on semantic relations.
were positive. The Kappa (Fleiss, 1971) of their
judgments was 0.67 (substantial agreement (Lan-
dis and Koch, 1977)). These three datasets have
no overlap in terms of phrase pairs. About nine
man-months were required to prepare the data.
Our evaluation is based on average precision;9
we believe that it is important to rank the plausible
event causality candidates higher.
5.1.1 Ablation Tests
We evaluated the features of our method by ab-
lation tests. Table 3 shows the results of remov-
ing the semantic relation, the context, and the as-
sociation features from our method. All the fea-
ture types are effective and contribute to the per-
formance gain that was about 5% higher than the
Base features only. Proposed achieved 70% pre-
cision at 13% recall. We then estimated that, with
the precision rate, we can extract 69,700 event
causality pairs from the 2,451,254 event causality
candidates, among which the estimated number of
positive examples is 377,794.
Next we examined whether the semantic rela-
tions that do not seem directly relevant to causality
like MATERIAL are effective. Table 4 shows that
the performance degraded (46.27 ? 45.86) when
we only used the CAUSATION binary patterns and
their entailing and entailed patterns compared to
Proposed. Even when adding the PREVENTION
(?negative CAUSATION?) patterns and their entail-
ing and entailed patterns, the performance was still
slightly worse than Proposed. The performance
was even worse when using no semantic relation
(?None? in Table 4). Consequently we conclude
that not only semantic relations directly relevant
9It is obtained by computing the precision for each point
in the ranked list where we find a positive sample and aver-
aging all the precision figures (Manning and Schu?tze, 1999).
992
Method Ave. prec. (%)
w/o Wikipedia-based features 46.52
Proposed 46.27
w/o definition-based features 46.21
w/o Web-based features 46.15
w/o CEA-based features 45.80
Table 5: Ablation tests on association features.
Method Ave. prec. (%)
Proposed 46.27
Proposed-CEA 45.80
CEA
sup
21.77
CEA
uns
16.57
Table 6: Average precision of our proposed meth-
ods and baselines using CEA.
to causality like CAUSATION but also those that
seem to lack direct relevance to causality like MA-
TERIAL are somewhat effective.
Finally, Table 5 shows the performance drop
by removing the Wikipedia-, definition-, web-,
and CEA-based features. The CEA-based fea-
tures were the most effective, while the Wikipedia-
based ones slightly degraded the performance.
5.1.2 Comparison to Baseline Methods
We compared our method and two baselines based
on Do et al (2011): CEA
uns
is an unsupervised
method that uses CEA to rank event causality can-
didates, and CEA
sup
is a supervised method us-
ing SVM and the CEA features, whose ranking is
based on the SVM scores. The baselines are not
complete implementations of Do et al?s method
which uses discourse relations identified based on
Lin et al (2010) and exploits them with CEA
within an ILP framework. Nonetheless, we believe
that this comparison is informative since CEA can
be seen as the main component; they achieved a
F1 of 41.7% for extracting causal event relations,
but with only CEA they still achieved 38.6%.
Table 6 shows the average precision of the com-
pared methods. Proposed is our proposed method.
Proposed-CEA is Proposed without the CEA-
features and shows their contribution. Proposed
is the best and the CEA features slightly contribute
to the performance, as Proposed-CEA indicates.
We observed that CEA
sup
and CEA
uns
performed
poorly and tended to favor event causality candi-
dates whose phrase pairs were highly relevant to
each other but described the contrasts of events
rather than event causality (e.g. build a slow mus-
cle and build a fast muscle) probably because their
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
re
ci
si
on
Recall
	
	



Figure 1: Precision-recall curves of proposed
methods and baselines using CEA.
Method Ave. prec. (%)
Proposed 49.64
Cs
uns
30.38
Cs
sup
27.49
Table 7: Average precision of our proposed
method and baselines using Cs.
main components are PMI values. Figure 1 shows
their precision-recall curves.
Next we compared our method with the base-
lines based on Hashimoto et al (2012). They de-
veloped an automatic excitation template acqui-
sition method that assigns each template an ex-
citation value in range [?1, 1] that is positive if
the template is excitatory and negative if it is in-
hibitory. They ranked event causality candidates
by Cs(p
1
, p
2
) = |s
1
| ? |s
2
|, where p
1
and p
2
are
the two phrases of event causality candidates, and
|s
1
| and |s
2
| are the absolute excitation values of
p
1
?s and p
2
?s templates. The baselines are as fol-
lows: Cs
uns
is an unsupervised method that uses
Cs for ranking, and Cs
sup
is a supervised method
using SVM with Cs as the only feature that uses
SVM scores for ranking. Note that some event
causality candidates were not given excitation val-
ues for their templates, since some templates were
acquired by manual annotation without Hashimoto
et al?s method. To favor the baselines for fairness,
the event causality candidates of the development
and test data were restricted to those with excita-
tion values. Since Cs
sup
performed slightly better
when using all of the training data in our prelimi-
nary experiments, we used all of it.
Table 7 shows the average precision of the com-
pared methods. Proposed is our method. Its av-
erage precision is different from that in Table 6
due to the difference in test data described above.
Cs
uns
and Cs
sup
did not perform well. Many
993
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
re
ci
si
on
Recall

	

	
Figure 2: Precision-recall curves of proposed
methods and baselines using Cs.
phrase pairs described two events that often hap-
pen in parallel but are not event causality (e.g. re-
duce the intake of energy and increase the energy
consumption) in the highly ranked event causality
candidates of Cs
uns
and Cs
sup
. Figure 2 shows
their precision-recall curves.
Hashimoto et al (2012) extracted 500,000 event
causalities with about 70% precision. However, as
described in Section 1, our event causality crite-
ria are different; since they regarded phrase pairs
that were not self-contained as event causality
(their annotators checked the original sentences of
phrase pairs to see if they were event causality),
their judgments tended to be more lenient than
ours, which explains the performance difference.
In preliminary experiments, since our proposed
method?s performance degraded when Cs was in-
corporated, we did not use it in our method.
5.2 Future Scenario Generation
To show that our future scenario generation meth-
ods can generate many acceptable scenarios with
reasonable precision, we experimentally com-
pared four methods: Proposed, our scenario
generation method without the two filters, Pro-
posed+Orig, our method with the original sen-
tence filter, Proposed+Orig+Comm, our method
with the original sentence and common argument
filters, and Exact, a method that chains event
causality by exact matching.
Beginning events As the beginning event of a
scenario, we extracted nouns that describe social
problems (social problem nouns, e.g. deforesta-
tion) from Wikipedia to focus our evaluation on
the ability to generate scenarios about them, which
is a realistic use-case of scenario generation. We
extracted 557 social problem nouns and used the
cause phrases of the event causality candidates that
Two-step Three-step
Exact 1,000 (44.10) 1,000 (23.50)
Proposed 2,000 (32.25) 2,000 (12.55)
Proposed+Orig 995 (36.28) 602 (17.28)
Proposed+Orig+Comm 708 (38.70) 339 (17.99)
Table 8: Number of scenario samples and their
precision (%) in parentheses.
consisted of one of the social problem nouns as the
scenario?s beginning event.
Event causality We applied our event causality
extraction method to 2,451,254 candidates (Sec-
tion 3.1) and culled the top 1,200,000 phrase pairs
from them (See Section F in the supplementary
notes for examples). Some phrase pairs have the
same noun pairs and the same template polar-
ity pairs (e.g. omit toothbrushing?get a cavity
and neglect toothbrushing?have a cavity, where
omit X and neglect X are inhibitory and get X and
have X are excitatory). We removed such phrase
pairs except those with the highest CScore, and
960,561 phrase pairs remained, from which we
generated two- or three-step scenarios that con-
sisted of two or three phrase pairs.
Evaluation samples The numbers of two- and
three-step scenarios generated by Proposed were
217,836 and 5,288,352, while those of Exact were
22,910 and 72,746. We sampled 2,000 from Pro-
posed?s two- and three-step scenarios and 1,000
from those of Exact. We applied the filters to the
sampled scenarios of Proposed, and the results
were regarded as the sample scenarios of Pro-
posed+Orig and Proposed+Orig+Comm. Table
8 shows the number and precision of the samples.
Note that, for the diversity of the sampled scenar-
ios, our sampling proceeded as follows: (i) Ran-
domly sample a beginning event phrase from the
generated scenarios. (ii) Randomly sample an ef-
fect phrase for the beginning event phrase from the
scenarios. (iii) Regarding the effect phrase as a
cause phrase, randomly sample an effect phrase
for it, and repeat (iii) up to the specified number
of steps (2 or 3). The samples were annotated by
three annotators (not the authors), who were in-
structed to regard a sample as acceptable if each
event causality that constitutes it is plausible and
the sample as a whole constitutes a single coherent
story. Final judgment was made by majority vote.
Fleiss? kappa of their judgments was 0.53 (moder-
ate agreement), which is lower than the kappa for
the causality judgment. This is probably because
994
Two-step Three-step
Exact 2,085 1,237
Proposed 5,773 0
Proposed+Orig 4,107 0
Proposed+Orig+Comm 3,293 21,153
Table 9: Estimated number of acceptable scenar-
ios with a 70% precision rate.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10000 20000 30000 40000 50000 60000 70000
P
re
ci
si
on
Estimated number of acceptable scenarios

	

	

	

Figure 3: Precision-scenario curves (2-step).
scenario judgment requires careful consideration
about various possible futures for which individ-
ual annotators tend to draw different conclusions.
Result 1 Table 9 shows the estimated number
of acceptable scenarios generated with 70% pre-
cision. The estimated number is calculated as the
product of the recall at 70% precision and the
number of acceptable scenarios in all the gener-
ated scenarios, which is estimated by the anno-
tated samples. Figures 3 and 4 show the precision-
scenario curves for the two- and three-step sce-
narios, which illustrate how many acceptable sce-
narios can be generated with what precision. The
curve is drawn in the same way as the precision-
recall curve except that the X-axis indicates the
estimated number of acceptable scenarios. At
70% precision, all of the proposed methods out-
performed Exact in the two-step setting, and Pro-
posed+Orig+Comm outperformed Exact in the
three-step setting.
Result 2 To evaluate the top-ranked scenarios
of Proposed+Orig+Comm in the three-step set-
ting with more samples, the annotators labeled 500
samples from the top 50,000 of its output. 341
(68.20%) were acceptable, and the estimated num-
ber of acceptable scenarios at a precision rate of
70% and 80% are 26,700 and 5,200 (See Section H
in the supplementary notes). The ?terrorist oper-
ations? scenario and the ?slash-and-burn agricul-
ture? scenario in Section 1 were ranked 16,386th
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  100000 200000 300000 400000 500000 600000 700000
P
re
ci
si
on
Estimated number of acceptable scenarios

	

	

	

Figure 4: Precision-scenario curves (3-step).
and 21,968th. Next we examined how many of
the top 50,000 scenarios were acceptable and non-
trivial, i.e., found in no page in our input web cor-
pus, using the 341 acceptable samples. A scenario
was regarded as non-trivial if its nouns co-occur in
no page of the corpus. 22 among the 341 samples
were non-trivial. Accordingly, we estimate that
we can generate 2,200 (50,000?22
500
) acceptable and
non-trivial scenarios from the top 50,000. (See
Section G in the supplementary notes for exam-
ples of the generated scenarios.)
Discussion Scenario deforestation contin-
ues?global warming worsens?sea temperatures
rise?vibrio parahaemolyticus fouls (water)
was generated by Proposed+Orig+Comm. It
is written in no page in our input web corpus,
which was crawled in 2007.10 But we did find
a paper Baker-Austin et al (2013) that observed
the emerging vibrio risk in the Baltic sea due to
global warming. In a sense, we ?predicted? an
event observed in 2013 from documents written
in 2007, although the scenario was ranked as low
as 240,738th.
6 Conclusion
We proposed a supervised method for event
causality extraction that exploits semantic rela-
tion, context, and association features. We also
proposed methods for our new task, future sce-
nario generation. The methods chain event causal-
ity by causal-compatibility. We generated non-
trivial scenarios with reasonable precision, and
?predicted? future events from web documents.
Increasing their rank is future work.
10The corpus has pages where global warming, sea tem-
peratures, and vibrio parahaemolyticus happen to co-occur.
But they are either diaries where the three words appear sep-
arately in different topics or lists of arbitrary words.
995
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phrased event relation acquisition: Coupling
the relation-oriented and argument-oriented ap-
proaches. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING
2008), pages 1?8.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu,
Takuya Kawada, Kentaro Inui, Sadao Kurohashi,
and Yutaka Kidawara. 2010. Organizing informa-
tion on the web to support user judgments on in-
formation credibility. In Proceedings of 2010 4th
International Universal Communication Symposium
Proceedings (IUCS 2010), pages 122?129.
Craig Baker-Austin, Joaquin A. Trinanes, Nick G. H.
Taylor, Rachel Hartnell, Anja Siitonen, and Jaime
Martinez-Urtaza. 2013. Emerging vibrio risk at
high latitudes in response to ocean warming. Nature
Climate Change, 3:73?77.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation of Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 789?
797.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meet-
ing of the ACL and the 4th IJCNLP of the AFNLP
(ACL-IJCNLP 2009), pages 602?610.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of the IEEE International
Conference on Data Mining (ICDM 2009), pages
764?769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 825?835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), pages 294?303.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Masaki Murata, and Jun?ichi Kazama. 2009. Large-
scale verb entailment acquisition from the web. In
Proceedings of EMNLP 2009: Conference on Em-
pirical Methods in Natural Language Processing,
pages 1172?1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jun?ichi Kazama, and Sadao Kuro-
hashi. 2011. Extracting paraphrases from definition
sentences on the web. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1087?1097.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orienta-
tion extracts contradiction and causality from the
web. In Proceedings of EMNLP-CoNLL 2012: Con-
ference on Empirical Methods in Natural Language
Processing and Natural Language Learning, pages
619?630.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407?
415.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa,
Chikara Hashimoto, Jong-Hoon Oh, and Kiyonori
Ohtake. 2013a. Two-stage method for large-scale
acquisition of contradiction pattern pairs using en-
tailment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2013), pages 693?703.
Julien Kloetzer, Kentaro Torisawa, Stijn De Saeger,
Motoki Sano, Chikara Hashimoto, and Jun Gotoh.
2013b. Large-scale acquisition of entailment pattern
pairs. In Information Processing Society of Japan
(IPSJ) Kansai-Branch Convention 2013.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
pdtb-styled end-to-end discourse parser. Technical
report, School of Computing, National University of
Singapore.
Chris Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pages 1733?
1743.
996
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of Sixth
ACM International Conference on Web Search and
Data Mining (WSDM 2013), pages 255?264.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for news
events prediction. In Proceedings of International
World Wide Web Conference 2012 (WWW 2012),
pages 909?918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In 2010
IEEE Fourth International Conference on Semantic
Computing, pages 361?368.
Peter Schwartz. 1991. The Art of the Long View. Dou-
bleday.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pages 133?140.
Shohei Tanaka, Naoaki Okazaki, and Mitsuru Ishizuka.
2012. Acquiring and generalizing causal inference
rules from deverbal noun constructions. In Proceed-
ings of 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1209?
1218.
Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake,
Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii,
and Kentaro Torisawa. 2013. WISDOM2013: A
large-scale web information analysis system. In
Companion Volume of the Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2013) (Demo Track), pages
45?48.
Kentaro Torisawa, Stijn de Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kak-
izawa, Masaki Murata, Kow Kuroda, and Ichiro Ya-
mada. 2010. Organizing the web?s information ex-
plosion to discover unknown unknowns. New Gen-
eration Computing (Special Issue on Information
Explosion), 28(3):217?236.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT-NAACL2006), pages 57?64.
Masaaki Tsuchida, Kentaro Torisawa, Stijn De
Saeger, Jong Hoon Oh, Jun?ichi Kazama, Chikara
Hashimoto, and Hayato Ohwada. 2011. Toward
finding semantic relations not written in a single sen-
tence: An inference method using auto-discovered
rules. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP 2011), pages 902?910.
Istva?n Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 1619?1629.
Naoki Yoshinaga and Masaru Kitsuregawa. 2009.
Polynomial to linear: Efficient classification with
conjunctive features. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2009), pages 542?1551.
997
