Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 159?163,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Dependency Parsers using Combinatory Categorial Grammar
Bharat Ram Ambati Tejaswini Deoskar
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
bharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.uk
Mark Steedman
Abstract
Subcategorization information is a useful
feature in dependency parsing. In this
paper, we explore a method of incorpo-
rating this information via Combinatory
Categorial Grammar (CCG) categories
from a supertagger. We experiment with
two popular dependency parsers (Malt
and MST) for two languages: English
and Hindi. For both languages, CCG
categories improve the overall accuracy
of both parsers by around 0.3-0.5% in
all experiments. For both parsers, we
see larger improvements specifically on
dependencies at which they are known to
be weak: long distance dependencies for
Malt, and verbal arguments for MST. The
result is particularly interesting in the case
of the fast greedy parser (Malt), since im-
proving its accuracy without significantly
compromising speed is relevant for large
scale applications such as parsing the web.
1 Introduction
Dependency parsers can recover much of the
predicate-argument structure of a sentence, while
being relatively efficient to train and extremely
fast at parsing. Dependency parsers have been
gaining in popularity in recent times due to
the availability of large dependency treebanks
for several languages and parsing shared tasks
(Buchholz and Marsi, 2006; Nivre et al., 2007a;
Bharati et al., 2012).
Ambati et al. (2013) showed that the perfor-
mance of Malt (Nivre et al., 2007b) on the free
word order language, Hindi, is improved by using
lexical categories from Combinatory Categorial
Grammar (CCG) (Steedman, 2000). In this paper,
we extend this work and show that CCG categories
are useful even in the case of English, a typolog-
ically different language, where parsing accuracy
of dependency parsers is already extremely high.
In addition, we also demonstrate the utility of
CCG categories to MST (McDonald et al., 2005)
for both languages. CCG lexical categories
contain subcategorization information regarding
the dependencies of predicates, including long-
distance dependencies. We show that providing
this subcategorization information in the form of
CCG categories can help both Malt and MST on
precisely those dependencies for which they are
known to have weak rates of recovery. The result
is particularly interesting for Malt, the fast greedy
parser, as the improvement in Malt comes without
significantly compromising its speed, so that it
can be practically applied in web scale parsing.
Our results apply both to English, a fixed word
order and morphologically simple language, and
to Hindi, a free word order and morphologically
rich language, indicating that CCG categories
from a supertagger are an easy and robust way
of introducing lexicalized subcategorization
information into dependency parsers.
2 Related Work
Parsers using different grammar formalisms
have different strengths and weaknesses, and
prior work has shown that information from one
formalism can improve the performance of a
parser in another formalism. Sagae et al. (2007)
achieved a 1.4% improvement in accuracy over a
state-of-the-art HPSG parser by using dependen-
cies from a dependency parser for constraining
wide-coverage rules in the HPSG parser. Coppola
and Steedman (2013) incorporated higher-order
dependency features into a cube decoding phrase-
structure parser and obtained significant gains
on dependency recovery for both in-domain and
out-of-domain test sets.
Kim et al. (2012) improved a CCG parser using
dependency features. They extracted n-best parses
from a CCG parser and provided dependency
159
Pierre Vinken will join the board as a nonexecutive director Nov. 29
N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N
> > > >
N NP N (S\NP)\(S\NP)
T >
NP NP
> >
(S[b]\NP)/PP PP
>
S[b]\NP
<
S[b]\NP
>
S[dcl]\NP
>
S[dcl]
Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence.
features from a dependency parser to a re-ranker
with an improvement of 0.35% in labelled F-score
of the CCGbank test set. Conversely, Ambati
et al. (2013) showed that a Hindi dependency
parser (Malt) could be improved by using CCG
categories. Using an algorithm similar to Cakici
(2005) and Uematsu et al. (2013), they first cre-
ated a Hindi CCGbank from a Hindi dependency
treebank and built a supertagger. They provided
CCG categories from a supertagger as features to
Malt and obtained overall improvements of 0.3%
and 0.4% in unlabelled and labelled attachment
scores respectively.
3 Data and Tools
Figure 1 shows a CCG derivation with CCG
lexical categories for each word and Stanford
scheme dependencies (De Marneffe et al., 2006)
for an example English sentence. (Details of CCG
and dependency parsing are given by Steedman
(2000) and K?ubler et al. (2009).)
3.1 Treebanks
In English dependency parsing literature, Stanford
and CoNLL dependency schemes are widely
popular. We used the Stanford parser?s built-in
converter (with the basic projective option) to
generate Stanford dependencies and Penn2Malt
1
to generate CoNLL dependencies from Penn
Treebank (Marcus et al., 1993). We used standard
splits, training (sections 02-21), development
(section 22) and testing (section 23) for our
experiments. For Hindi, we worked with the
Hindi Dependency Treebank (HDT) released
as part of Coling 2012 Shared Task (Bharati et
al., 2012). HDT contains 12,041 training, 1,233
development and 1,828 testing sentences.
We used the English (Hockenmaier and Steed-
man, 2007) and Hindi CCGbanks (Ambati et al.,
1
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
2013) for our experiments. For Hindi we used two
lexicons: a fine-grained one (with morphological
information) and a coarse-grained one (without
morphological information).
3.2 Supertaggers
We used Clark and Curran (2004)?s supertagger
for English, and Ambati et al. (2013)?s supertag-
ger for Hindi. Both are Maximum Entropy based
CCG supertaggers. The Clark and Curran (2004)
supertagger uses different features like word, part-
of-speech, and contextual and complex bi-gram
features to obtain a 1-best accuracy of 91.5% on
the development set. In addition to the above
mentioned features, Ambati et al. (2013) em-
ployed morphological features useful for Hindi.
The 1-best accuracy of Hindi supertagger for fine-
grained and coarse-grained lexicon is 82.92% and
84.40% respectively.
3.3 Dependency Parsers
There has been a significant amount of work on
parsing English and Hindi using the Malt and
MST parsers in the recent past (Nivre et al.,
2007a; Bharati et al., 2012). We first run these
parsers with previous best settings (McDonald et
al., 2005; Zhang and Nivre, 2012; Bharati et
al., 2012) and treat them as our baseline. In
the case of English, Malt uses arc-standard and
stack-projective parsing algorithms for CoNLL
and Stanford schemes respectively and LIBLIN-
EAR learner (Fan et al., 2008) for both the
schemes. MST uses 1st-order features, and a pro-
jective parsing algorithm with 5-best MIRA train-
ing for both the schemes. For Hindi, Malt uses
the arc-standard parsing algorithm with a LIBLIN-
EAR learner. MST uses 2nd-order features, non-
projective algorithm with 5-best MIRA training.
For English, we assigned POS-tags using a per-
ceptron tagger (Collins, 2002). For Hindi, we also
did all our experiments using automatic features
160
Language Experiment
Malt MST
UAS LAS UAS LAS
English
Stanford Baseline 90.32 87.87 90.36 87.18
Stanford + CCG 90.56** (2.5) 88.16** (2.5) 90.93** (5.9) 87.73** (4.3)
CoNLL Baseline 89.99 88.73 90.94 89.69
CoNLL + CCG 90.38** (4.0) 89.19** (4.1) 91.48** (5.9) 90.23** (5.3)
Hindi
Baseline 88.67 83.04 90.52 80.67
Fine CCG 88.93** (2.2) 83.23* (1.1) 90.97** (4.8) 80.94* (1.4)
Coarse CCG 89.04** (3.3) 83.35* (1.9) 90.88** (3.8) 80.73* (0.4)
Table 1: Impact of CCG categories from a supertagger on dependency parsing. Numbers in brackets
are percentage of errors reduced. McNemar?s test compared to baseline, * = p < 0.05 ; ** = p < 0.01
(Hindi Malt results (grey background) are from Ambati et al. (2013)).
(POS, chunk and morphological information)
extracted using a Hindi shallow parser
2
.
4 CCG Categories as Features
Following Ambati et al. (2013), we used supertags
which occurred at least K times in the training
data, and backed off to coarse POS-tags otherwise.
For English K=1, i.e., when we use CCG cate-
gories for all words, gave the best results. K=15
gave the best results for Hindi due to sparsity is-
sues, as the data for Hindi is small. We provided
a supertag as an atomic symbol similar to a POS
tag and didn?t split it into a list of argument and
result categories. We explored both Stanford and
CoNLL schemes for English and fine and coarse-
grained CCG categories for Hindi. All feature and
parser tuning was done on the development data.
We assigned automatic POS-tags and supertags to
the training data.
4.1 Experiments with Supertagger output
We first used gold CCG categories extracted from
each CCGbank as features to the Malt and MST,
to get an upper bound on the utility of CCG cate-
gories. As expected, gold CCG categories boosted
the Unlabelled Attachment Score (UAS) and La-
belled Attachment Score (LAS) by a large amount
(4-7% in all the cases).
We then experimented with using automatic
CCG categories from the English and Hindi su-
pertaggers as a feature to Malt and MST. With au-
tomatic categories from a supertagger, we got sta-
tistically significant improvements (McNemar?s
test, p < 0.05 for Hindi LAS and p < 0.01 for the
rest) over the baseline parsers, for all cases (Table
1). Since the CCGbanks used to train the supertag-
gers are automatically generated from the con-
stituency or dependency treebanks used to train
2
http://ltrc.iiit.ac.in/analyzer/hindi/
the dependency parsers, the improvements are
indeed due to reparameterization of the model to
include CCG categories and not due to additional
hand annotations in the CCGbanks. This shows
that the rich subcategorization information pro-
vided by automatically assigned CCG categories
can help Malt and MST in realistic applications.
For English, in case of Malt, we achieved
0.3% improvement in both UAS and LAS for
Stanford scheme. For CoNLL scheme, these
improvements were 0.4% and 0.5% in UAS and
LAS respectively. For MST, we got around 0.5%
improvements in all cases.
In case of Hindi, fine-grained supertags gave
larger improvements for MST. We got final
improvements of 0.5% and 0.3% in UAS and LAS
respectively. In contrast, for Malt, Ambati et al.
(2013) had shown that coarse-grained supertags
gave larger improvements of 0.3% and 0.4% in
UAS and LAS respectively. Due to better handling
of error propagation in MST, the richer informa-
tion in fine-grained categories may have surpassed
the slightly lower supertagger performance,
compared to coarse-grained categories.
4.2 Analysis: English
We analyze the impact of CCG categories on
different labels (label-wise) and distance ranges
(distance-wise) for CoNLL scheme dependencies
(We observed a similar impact for the Stanford
scheme dependencies as well). Figure 2a shows
the F-score for three major dependency labels,
namely, ROOT (sentence root), SUBJ (subject),
OBJ (object). For Malt, providing CCG categories
gave an increment of 1.0%, 0.3% for ROOT and
SUBJ labels respectively. For MST, the improve-
ments for ROOT and SUBJ were 0.5% and 0.8%
respectively. There was no significant improve-
ment for OBJ label, especially in the case of Malt.
161
87.7
92.5
88.7
92.893.4 92.5
88.2
93.9 93.3
88.58990
9192
9394
95 MaltMalt + CCGMSTMST + CCG
86.5 86.5
8687
88
ROOT SUBJ DOBJ
(a) Label-wise impact
98.2 
78.6 80.8 
98.3 
79.2 81.7 
98.4 
80.8 
84.5 
98.5 
81.8 
85.5 
78 
83 
88 
93 
98 
1-5 6-10 >10 
Malt Malt + CCG MST MST + CCG 
(b) Distance-wise impact
Figure 2: Label-wise and Distance-wise impact of supertag features on Malt and MST for English
Figure 2b shows the F-score of dependencies
based on the distance ranges between words. The
percentage of dependencies in the 1?5, 6?10 and
>10 distance ranges are 88.5%, 6.6% and 4.9% re-
spectively out of the total of around 50,000 depen-
dencies. For both Malt and MST, there was very
slight improvement for short distance dependen-
cies (1?5) but significant improvements for longer
distances (6?10 and >10). For Malt, there was
an improvement of 0.6% and 0.9% for distances
6?10, and >10 respectively. For MST, these
improvements were 1.0% and 1.0% respectively.
4.3 Analysis: Hindi
In the case of Hindi, for MST, providing CCG
categories gave an increment of 0.5%, 0.4% and
0.3% for ROOT, SUBJ and OBJ labels respec-
tively in F-score over the baseline. Ambati et al.
(2013) showed that for Hindi, providing CCG
categories as features improved Malt in better
handling of long distance dependencies.
The percentage of dependencies in the 1?5,
6?10 and >10 distance ranges are 82.2%,
8.6% and 9.2% respectively out of the total of
around 40,000 dependencies. Similar to English,
there was very slight improvement for short
distance dependencies (1?5). But for longer
distances, 6?10, and >10, there was significant
improvement of 1.3% and 1.3% respectively
for MST. Ambati et al. (2013) reported similar
improvements for Malt as well.
4.4 Discussion
Though valency is a useful feature in dependency
parsing (Zhang and Nivre, 2011), Zhang and Nivre
(2012) showed that providing valency information
dynamically, in the form of the number of depen-
dencies established in a particular state during
parsing, did not help Malt. However, as we have
shown above, providing this information as a static
lexical feature in the form of CCG categories does
help Malt. In addition to specifying the number of
arguments, CCG categories also contain syntactic
type and direction of those arguments. However,
providing CCG categories as features to zpar
(Zhang and Nivre, 2011) didn?t have significant
impact as it is already using similar information.
4.5 Impact on Web Scale Parsing
Greedy parsers such as Malt are very fast and are
practically useful in large-scale applications such
as parsing the web. Table 2, shows the speed of
Malt, MST and zpar on parsing English test data
in CoNLL scheme (including POS-tagging and
supertagging time). Malt parses 310 sentences per
second, compared to 35 and 11 of zpar and MST
respectively. Clearly, Malt is orders of magnitude
faster than MST and zpar. After using CCG
categories from the supertagger, Malt parses 245
sentences per second, still much higher than other
parsers. Thus we have shown a way to improve
Malt without significantly compromising speed,
potentially enhancing its usefulness for web scale
parsing.
Parser Ave. Sents / Sec Total Time
MST 11 3m 36s
zpar 35 1m 11s
Malt 310 0m 7.7s
Malt + CCG 245 0m 10.2s
Table 2: Time taken to parse English test data.
5 Conclusion
We have shown that informative CCG categories,
which contain both local subcategorization infor-
mation and capture long distance dependencies
elegantly, improve the performance of two de-
pendency parsers, Malt and MST, by helping
in recovering long distance relations for Malt
and local verbal arguments for MST. This is
true both in the case of English (a fixed word
order language) and Hindi (free word order and
morphologically richer language), extending the
result of Ambati et al. (2013). The result is
particularly interesting in the case of Malt which
cannot directly use valency information, which
CCG categories provide indirectly. It leads to an
improvement in performance without significantly
compromising speed and hence promises to be
applicable to web scale processing.
162
References
Bharat Ram Ambati, Tejaswini Deoskar, and Mark
Steedman. 2013. Using CCG categories to improve
Hindi dependency parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
604?609, Sofia, Bulgaria.
Akshar Bharati, Prashanth Mannem, and Dipti Misra
Sharma. 2012. Hindi Parsing Shared Task. In Pro-
ceedings of Coling Workshop on Machine Transla-
tion and Parsing in Indian Languages, Kharagpur,
India.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164,
New York City, New York.
Ruken Cakici. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of the ACL
Student Research Workshop, pages 73?78, Ann Ar-
bor, Michigan.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282?288.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical methods in natural
language processing, EMNLP ?02, pages 1?8.
Greg Coppola and Mark Steedman. 2013. The effect
of higher-order dependency features in discrimina-
tive phrase-structure parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
610?616, Sofia, Bulgaria.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Sunghwan Mac Kim, Dominick Ng, Mark Johnson,
and James Curran. 2012. Improving combina-
tory categorial grammar parse reranking with depen-
dency grammar features. In Proceedings of COL-
ING 2012, pages 1441?1458, Mumbai, India.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Clay-
pool Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91?98, Ann Arbor, Michigan.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 624?
631, Prague, Czech Republic.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,
Yusuke Miyao, and Hideki Mima. 2013. Inte-
grating multiple dependency corpora for inducing
wide-coverage Japanese CCG resources. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1042?1051, Sofia, Bulgaria.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391?1400,
Mumbai, India, December.
163
Proceedings of the ACL 2010 Student Research Workshop, pages 103?108,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Importance of linguistic constraints in statistical dependency parsing 
 
Bharat Ram Ambati 
Language Technologies Research Centre, IIIT-Hyderabad, 
Gachibowli, Hyderabad, India ? 500032. 
ambati@research.iiit.ac.in 
 
 
Abstract 
Statistical systems with high accuracy are very 
useful in real-world applications. If these sys-
tems can capture basic linguistic information, 
then the usefulness of these statistical systems 
improve a lot. This paper is an attempt at in-
corporating linguistic constraints in statistical 
dependency parsing. We consider a simple 
linguistic constraint that a verb should not 
have multiple subjects/objects as its children 
in the dependency tree. We first describe the 
importance of this constraint considering Ma-
chine Translation systems which use depen-
dency parser output, as an example applica-
tion. We then show how the current state-of-
the-art dependency parsers violate this con-
straint. We present two new methods to handle 
this constraint. We evaluate our methods on 
the state-of-the-art dependency parsers for 
Hindi and Czech. 
1 Introduction 
Parsing is one of the major tasks which helps in 
understanding the natural language. It is useful in 
several natural language applications. Machine 
translation, anaphora resolution, word sense dis-
ambiguation, question answering, summarization 
are few of them. This led to the development of 
grammar-driven, data-driven and hybrid parsers.  
Due to the availability of annotated corpora in 
recent years, data driven parsing has achieved 
considerable success. The availability of phrase 
structure treebank for English (Marcus et al, 
1993) has seen the development of many effi-
cient parsers.  Using the dependency analysis, a 
similar large scale annotation effort for Czech, 
has been the Prague Dependency Treebank (Ha-
jicova, 1998). Unlike English, Czech is a free-
word-order language and is also morphologically 
very rich. It has been suggested that free-word-
order languages can be handled better using the 
dependency based framework than the constitu-
ency based one (Hudson, 1984; Shieber, 1985; 
Mel??uk, 1988, Bharati et al, 1995). The basic 
difference between a constituent based represen-
tation and a dependency representation is the 
lack of nonterminal nodes in the latter. It has also 
been noted that use of appropriate edge labels 
gives a level of semantics. It is perhaps due to 
these reasons that the recent past has seen a surge 
in the development of dependency based tree-
banks. 
Due to the availability of dependency tree-
banks, there are several recent attempts at build-
ing dependency parsers. Two CoNLL shared 
tasks (Buchholz and Marsi, 2006; Nivre et al, 
2007a) were held aiming at building state-of-the-
art dependency parsers for different languages. 
Recently in NLP Tools Contest in ICON-2009 
(Husain, 2009 and references therein), rule-
based, constraint based, statistical and hybrid 
approaches were explored towards building de-
pendency parsers for three Indian languages 
namely, Telugu, Hindi and Bangla. In all these 
efforts, state-of-the-art accuracies are obtained 
by two data-driven parsers, namely, Malt (Nivre 
et al, 2007b) and MST (McDonald et al, 2006). 
The major limitation of both these parsers is that 
they won't take linguistic constraints into account 
explicitly. But, in real-world applications of the 
parsers, some basic linguistic constraints are very 
useful. If we can make these parsers handle lin-
guistic constraints also, then they become very 
useful in real-world applications.  
This paper is an effort towards incorporating 
linguistic constraints in statistical dependency 
parser. We consider a simple constraint that a 
verb should not have multiple subjects/objects as 
its children. In section 2, we take machine trans-
lation using dependency parser as an example 
and explain the need of this linguistic constraint. 
In section 3, we propose two approaches to han-
dle this case. We evaluate our approaches on the 
state-of-the-art dependency parsers for Hindi and 
Czech and analyze the results in section 4. Gen-
eral discussion and future directions of the work 
are presented in section 5. We conclude our pa-
per in section 6. 
103
2 Motivation 
In this section we take Machine Translation 
(MT) systems that use dependency parser output 
as an example and explain the need of linguistic 
constraints. We take a simple constraint that a 
verb should not have multiple subjects/objects as 
its children in the dependency tree. Indian Lan-
guage to Indian Language Machine Transtion 
System1 is one such MT system which uses de-
pendency parser output. In this system the gener-
al framework has three major components. a) 
dependency analysis of the source sentence. b) 
transfer from source dependency tree to target 
dependency tree, and c) sentence generation 
from the target dependency tree. In the transfer 
part several rules are framed based on the source 
language dependency tree. For instance, for Te-
lugu to Hindi MT system, based on the depen-
dency labels of the Telugu sentence post-
positions markers that need to be added to the 
words are decided. Consider the following ex-
ample, 
 
(1) 
Telugu:  raamu     oka      pamdu       tinnaadu 
               ?Ramu?   ?one?    ?fruit?        ?ate? 
 
Hindi:   raamu     ne      eka      phala    khaayaa 
             ?Ramu?  ?ERG?   ?one?   ?fruit?    ?ate? 
 
English:  ?Ramu ate a fruit?. 
 
In the above Telugu sentence, ?raamu? is the 
subject of the verb ?tinnaadu?. While translating 
this sentence to Hindi, the post-position marker 
?ne? is added to the subject. If the dependency 
parser marks two subjects, both the words will 
have ?ne? marker. This affects the comprehensi-
bility. If we can avoid such instances, then the 
output of the MT system will be improved.  
This problem is not due to morphological 
richness or free-word-order nature of the target 
language. Consider an example of free-word-
order language to fixed-word-order language MT 
system like Hindi to English MT system. The 
dependency labels help in identifying the posi-
tion of the word in the target sentence. Consider 
the example sentences given below. 
 
(2a)    raama   seba      khaatha  hai  
      ?Ram?   ?apple?   ?eats?     ?is? 
          ?Ram eats an apple? 
                                                 
1 http://sampark.iiit.ac.in/ 
 
 
(2b)    seba        raama       khaatha  hai  
      ?apple?    ?Ram?        ?eats?     ?is? 
          ?Ram eats an apple? 
 
Though the source sentence is different, the 
target sentence is same. Even though the source 
sentences are different, the dependency tree is 
same for both the sentences. In both the cases, 
?raama? is the subject and ?seba? is the object of 
the verb ?khaatha?. This information helps in 
getting the correct translation. If the parser for 
the source sentence assigns the label ?subject? to 
both ?raama? and ?seba?, the MT system can not 
give the correct output. 
There were some attempts at handling these 
kind of linguistic constraints using integer pro-
gramming approaches (Riedel et al, 2006; Bha-
rati et al, 2008). In these approaches dependency 
parsing is formulated as solving an integer pro-
gram as McDonald et al (2006) has formulated 
dependency parsing as MST problem. All the 
linguistic constraints are encoded as constraints 
while solving the integer program. In other 
words, all the parses that violate these constraints 
are removed from the solution list. The parse 
with satisfies all the constraints is considered as 
the dependency tree for the sentence. In the fol-
lowing section, we describe two new approaches 
to avoid multiple subjects/objects for a verb. 
3 Approaches 
In this section, we describe the two different ap-
proaches for avoiding the cases of a verb having 
multiple subjects/objects as its children in the 
dependency tree. 
3.1 Naive Approach (NA) 
In this approach we first run a parser on the input 
sentence. Instead of first best dependency label, 
we extract the k-best labels for each token in the 
sentence. For each verb in the sentence, we 
check if there are multiple children with the de-
pendency label ?subject?. If there are any such 
cases, we extract the list of all the children with 
label ?subject?. we find the node in this list which 
appears left most in the sentence with respect to 
other nodes. We assign ?subject? to this node. For 
the rest of the nodes in this list we assign the 
second best label and remove the first best label 
from their respective k-best list of labels. We 
check recursively, till all such instances are 
104
avoided. We repeat the same procedure for ?ob-
ject?. 
Main criterion to avoid multiple sub-
jects/objects in this approach is position of the 
node in the sentence. Consider the following ex-
ample,  
 
Eg. 3: raama   seba      khaatha  hai  
            ?Ram?   ?apple?   ?eats?     ?is? 
            ?Ram eats an apple? 
Suppose the parser assigns the label ?subject? 
to both the nouns, ?raama? and ?seba?. Then 
naive approach assigns the label subject to ?raa-
ma? and second best label to ?seba? as ?raama? 
precedes ?seba?.  
In this manner we can avoid a verb having 
multiple children with dependency labels sub-
ject/object.  
Limitation to this approach is word-order. The 
algorithm described here works well for fixed 
word order languages. For example, consider a 
language with fixed word order like English. 
English is a SVO (Subject, Verb, Object) lan-
guage. Subject always occurs before the object. 
So, if a verb has multiple subjects, based on posi-
tion we can say that the node that occurs first 
will be the subject. But if we consider a free-
word order language like Hindi, this approach 
wouldn't work always.  
Consider (2a) and (2b). In both these exam-
ples, ?raama? is the subject of the verb ?khaatha? 
and ?seba? is the object of the verb ?khaatha?. 
The only difference in these two sentences is the 
order of the word. In (2a), subject precedes ob-
ject. Whereas in (2b), object precedes subject. 
Suppose the parser identifies both ?raama? and 
?seba? as subjects. NA can correctly identify 
?raama? as the subject in case of (2a). But in case 
of (2b), ?seba? is identified as the subject. To 
handle these kind of instances, we use a proba-
bilistic approach. 
3.2 Probabilistic Approach (PA) 
The probabilistic approach is similar to naive 
approach except that the main criterion to avoid 
multiple subjects/objects in this approach is 
probability of the node having a particular label. 
Whereas in naive approach, position of the node 
is the main criterion to avoid multiple sub-
jects/objects. In this approach, for each node in 
the sentence, we extract the k-best labels along 
with their probabilities. Similar to NA, we first 
check for each verb if there are multiple children 
with the dependency label ?subject?. If there are 
any such cases, we extract the list of all the 
children with label ?subject?. We find the node in 
this list which has the highest probability value. 
We assign ?subject? to this node. For the rest of 
the nodes in this list we assign the second best 
label and remove the first best label from their 
respective k-best list of labels. We check recur-
sively, till all such instances are avoided. We 
repeat the same procedure for ?object?. 
Consider (2a) and (2b). Suppose the parser 
identifies both ?raama? and ?seba? as subjects. 
Probability of ?raama? being a subject will be 
more than ?seba? being a subject. So, the proba-
bilistic approach correctly marks ?raama? as sub-
ject in both (2a) and (2b). But, NA couldn't iden-
tify ?raama? as subject in (2b). 
4 Experiments 
We evaluate our approaches on the state-of-the-
art parsers for two languages namely, Hindi and 
Czech. First we calculate the instances of mul-
tiple subjects/objects in the output of the state-of-
the-art parsers for these two languages. Then we 
apply our approaches and analyze the results. 
4.1 Hindi 
Recently in NLP Tools Contest in ICON-2009 
(Husain, 2009 and references herein), rule-based, 
constraint based, statistical and hybrid approach-
es were explored for parsing Hindi. All these 
attempts were at finding the inter-chunk depen-
dency relations, given gold-standard POS and 
chunk tags. The state-of-the-art accuracy of 
74.48% LAS (Labeled Attachment Score) is 
achieved by Ambati et al (2009) for Hindi.  
They used two well-known data-driven parsers, 
Malt2 (Nivre et al, 2007b), and MST3 (McDo-
nald et al, 2006) for their experiments. As the 
accuracy of the labeler of MST parser is very 
low, they used maximum entropy classification 
algorithm, MAXENT4 for labeling. 
For Hindi, dependency annotation is done us-
ing paninian framework (Begum et al, 2008; 
Bharati et al, 1995). So, in Hindi, the equivalent 
labels for subject and object are ?karta (k1)? and 
?karma (k2)?. ?karta? and ?karma? are syntactico-
semantic labels which have some properties of 
both grammatical roles and thematic roles. k1 
behaves similar to subject and agent. k2 behaves 
similar to object and patient (Bharati et al, 1995; 
Bharati et al, 2009). Here, by object we mean 
                                                 
2 Malt Version 1.3.1 
3 MST Version 0.4b 
4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm
l 
105
only direct object. Thus we consider only k1 and 
k2 labels which are equivalent of subject and di-
rect object. Annotation scheme is such that there 
wouldn?t be multiple subjects/objects for a verb 
in any case (Bharati et al, 2009). For example, 
even in case of coordination, coordinating con-
junction is the head and conjuncts are children of 
the coordinating conjunction. The coordinating 
conjunction is attached to the verb with k1/k2 
label and the conjuncts get attached to the coor-
dinating conjunction with a dependency label 
?ccof?.  
We replicated the experiments of Ambati et al 
(2009) on test set (150 sentences) of Hindi and 
analyzed the outputs of Malt and MST+MaxEnt. 
We consider this as the baseline. In the output of 
Malt, there are 39 instances of multiple sub-
jects/objects. There are 51 such instances in the 
output of MST+MAXENT. 
Malt is good at short distance labeling and 
MST is good at long distance labeling (McDo-
nald and Nivre, 2007). As ?k1? and ?k2? are short 
distance labels, Malt could able predict these la-
bels more accurately than MST. Because of this 
output of MST has higher number of instances of 
multiple subjects/objects than Malt. 
 
 Total Instances 
Malt 39 
MST + MAXENT 51 
 
Table 1: Number of instances of multiple subjects or 
objects in the output of the state-of-the-art parsers for 
Hindi 
 
Both the parsers output first best label for each 
node in the sentence. In case of Malt, we mod-
ified the implementation to extract all the possi-
ble dependency labels with their scores. As Malt 
uses libsvm for learning, we couldn't able to get 
the probabilities. Though interpreting the scores 
provided by libsvm as probabilities is not the 
correct way, that is the only option currently 
available with Malt. In case of MST+MAXENT, 
labeling is performed by MAXENT. We used a 
java version of MAXENT5  to extract all possible 
tags with their scores. We applied both the naive 
and probabilistic approaches to avoid multiple 
subjects/objects. We evaluated our experiments 
based on unlabeled attachment score (UAS), la-
beled attachment score (LAS) and labeled score 
                                                 
5 http://maxent.sourceforge.net/ 
(LS) (Nivre et al, 2007a). Results are presented 
in Table 2. 
As expected, PA performs better than NA. 
With PA we got an improvement of 0.26% in 
LAS over the previous best results for Malt. In 
case of MST+MAXENT we got an improvement 
of 0.61% in LAS over the previous best results. 
Note that in case of MST+MAXENT, the slight 
difference between state-of-the-art results of 
Ambati et al (2009) and our baseline accuracy is 
due different MAXENT package used.  
 Malt MST+MAXENT 
UAS LAS LS UAS LAS LS 
Baseline 90.14 74.48 76.38 91.26 72.75 75.26 
NA 90.14 74.57 76.38 91.26 72.84 75.26 
PA 90.14 74.74 76.56 91.26 73.36 75.87 
 
Table 2: Comparison of NA and PA with previous 
best results for Hindi 
 
Improvement in case of MST+MAXENT is 
greater than that of Malt. One reason is because 
of more number of instances of multiple sub-
jects/objects in case of MST+MAXENT. Other 
reason is use of probabilities in case 
MST+MAXENT. Whereas in case of Malt, we 
interpreted the scores as probabilities which is 
not a good way to do. But, in case of Malt, that is 
the only option available. 
4.2 Czech 
In case of Czech, we replicated the experiments 
of Hall et al (2007) using latest version of Malt 
(version 1.3.1) and analyzed the output. We con-
sider this as the baseline. The minor variation of 
the baseline results from the results of CoNLL-
2007 shared task is due to different version Malt 
parser being used. Due to practical reasons we 
couldn't use the older version. In the output of 
Malt, there are 39 instances of multiple sub-
jects/objects out of 286 sentences in the testing 
data. In case of Czech, the equivalent labels for 
subject and object are ?agent? and ?theme?. 
Czech is a free-word-order language similar to 
Hindi. So as expected, PA performed better than 
NA. Interestingly, accuracy of PA is lower than 
the baseline. Main reason for this is scores of 
libsvm of Malt. We explain the reason for this 
using the following example, consider a verb ?V? 
has two children ?C1? and ?C2? with dependency 
label subject. Assume that the label for ?C1? is 
subject and the label of ?C2? is object in the gold-
data. As the parser marked ?C1? with subject, this 
106
adds to the accuracy of the parser. While avoid-
ing multiple subjects, if ?C1? is marked as sub-
ject, then the accuracy doesn't drop. If ?C2? is 
marked as object then the accuracy increases. 
But, if ?C2? is marked as subject and ?C1? is 
marked as object then the accuracy drops. This 
could happen if probability of ?C1? having sub-
ject as label is lower than ?C1? having subject as 
the label. This is because of two reasons, (a) 
parser itself wrongly predicted the probabilities, 
and (b) parser predicted correctly, but due to the 
limitation of libsvm, we couldn't get the scores 
correctly.  
 
 UAS LAS LS 
Baseline 82.92 76.32 83.69 
NA 82.92 75.92 83.35 
PA 82.92 75.97 83.40 
 
Table 3: Comparison of NA and PA with previous 
best results for Czech 
 
5 Discussion and Future Work 
Results show that the probabilistic approach per-
forms consistently better than the naive ap-
proach. For Hindi, we could able to achieve an 
improvement 0.26% and 0.61% in LAS over the 
previous best results using Malt and MST re-
spectively. We couldn?t able to achieve any im-
provement in case of Czech due to the limitation 
of libsvm learner used in Malt. 
We plan to evaluate our approaches on all the 
data-sets of CoNLL-X and CoNLL-2007 shared 
tasks using Malt. Settings of MST parser are 
available only for CoNLL-X shared task data 
sets. So, we plan to evaluate our approaches on 
CoNLL-X shared task data using MST also. Malt 
has the limitation for extracting probabilities due 
to libsvm learner. Latest version of Malt (version 
1.3.1) provides option for liblinear learner also. 
Liblinear provides option for extracting probabil-
ities. So we can also use liblinear learning algo-
rithm for Malt and explore the usefulness of our 
approaches. Currently, we are handling only two 
labels, subject and object. Apart from subject and 
object there can be other labels for which mul-
tiple instances for a single verb is not valid. We 
can extend our approaches to handle such labels 
also. We tried to incorporate one simple linguis-
tic constraint in the statistical dependency pars-
ers. We can also explore the ways of incorporat-
ing other useful linguistic constraints. 
6 Conclusion 
Statistical systems with high accuracy are very 
useful in practical applications. If these systems 
can capture basic linguistic information, then the 
usefulness of the statistical system improves a 
lot. In this paper, we presented a new method of 
incorporating linguistic constraints into the sta-
tistical dependency parsers. We took a simple 
constraint that a verb should not have multiple 
subjects/objects as its children. We proposed two 
approaches, one based on position and the other 
based on probabilities to handle this. We eva-
luated our approaches on state-of-the-art depen-
dency parsers for Hindi and Czech. 
 
Acknowledgments 
I would like to express my gratitude to Prof. Joa-
kim Nivre and Prof. Rajeev Sangal for their 
guidance and support. I would also like to thank 
Mr. Samar Husain for his valuable suggestions. 
References  
B. R. Ambati, P. Gadde and K. Jindal. 2009. Experi-
ments in Indian Language Dependency Parsing. In 
Proceedings of the ICON09 NLP Tools Contest: 
Indian Language Dependency Parsing, pp 32-37.  
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, 
and R. Sangal. 2008. Dependency annotation 
scheme for Indian languages. In Proceedings of 
IJCNLP-2008. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natu-
ral Language Processing: A Paninian Perspective, 
Prentice-Hall of India, New Delhi, pp. 65-106. 
A. Bharati, S. Husain, D. M. Sharma, and R. Sangal. 
2008. A Two-Stage Constraint Based Dependency 
Parser for Free Word Order Languages. In Pro-
ceedings of the COLIPS International Conference 
on Asian Language Processing 2008 (IALP). 
Chiang Mai, Thailand. 
S. Buchholz and E. Marsi. 2006. CoNLL-X shared 
task on multilingual dependency parsing. In Proc. 
of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL). 
E. Hajicova. 1998. Prague Dependency Treebank: 
From Analytic to Tectogrammatical Annotation. In 
Proc. TSD?98. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, 
M. Nilsson and M. Saers. 2007. Single Malt or 
Blended? A Study in Multilingual Parser Optimiza-
tion. In Proceedings of the CoNLL Shared Task 
Session of EMNLP-CoNLL. 
R. Hudson. 1984. Word Grammar, Basil Blackwell, 
108 Cowley Rd, Oxford, OX4 1JF, England. 
107
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proceedings of ICON09 NLP Tools 
Contest: Indian Language Dependency Parsing. 
Hyderabad, India. 
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 
1993. Building a large annotated corpus of English: 
The Penn Treebank, Computational Linguistics 
1993. 
I. A. Mel'?uk. 1988. Dependency Syntax: Theory and 
Practice, State University, Press of New York. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of the Tenth 
Conference on Computational Natural Language 
Learning (CoNLL-X), pp. 216?220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models. 
In Proc. of EMNLP-CoNLL. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  
S. Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceed-
ings of EMNLP/CoNLL-2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltPars-
er: A language-independent system for data-driven 
dependency parsing. Natural Language Engineer-
ing, 13(2), 95-135. 
S. Riedel, Ruket ?ak?c? and Ivan Meza-Ruiz. 2006. 
Multi-lingual Dependency Parsing with Incremen-
tal Integer Linear Programming. In Proceedings of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL-X). 
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. In Linguistics and 
Philosophy, p. 8, 334?343. 
108
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?609,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using CCG categories to improve Hindi dependency parsing
Bharat Ram Ambati Tejaswini Deoskar
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
bharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.uk
Mark Steedman
Abstract
We show that informative lexical cate-
gories from a strongly lexicalised for-
malism such as Combinatory Categorial
Grammar (CCG) can improve dependency
parsing of Hindi, a free word order lan-
guage. We first describe a novel way to
obtain a CCG lexicon and treebank from
an existing dependency treebank, using a
CCG parser. We use the output of a su-
pertagger trained on the CCGbank as a
feature for a state-of-the-art Hindi depen-
dency parser (Malt). Our results show that
using CCG categories improves the accu-
racy of Malt on long distance dependen-
cies, for which it is known to have weak
rates of recovery.
1 Introduction
As compared to English, many Indian languages
including Hindi have a freer word order and are
also morphologically richer. These characteristics
pose challenges to statistical parsers. Today, the
best dependency parsing accuracies for Hindi are
obtained by the shift-reduce parser of Nivre et
al. (2007) (Malt). It has been observed that Malt
is relatively accurate at recovering short distance
dependencies, like arguments of a verb, but is less
accurate at recovering long distance dependencies
like co-ordination, root of the sentence, etc
(Mcdonald and Nivre, 2007; Ambati et al, 2010).
In this work, we show that using CCG lexical
categories (Steedman, 2000), which contain sub-
categorization information and capture long dis-
tance dependencies elegantly, can help Malt with
those dependencies. Section 2 first shows how we
extract a CCG lexicon from an existing Hindi de-
pendency treebank (Bhatt et al, 2009) and then
use it to create a Hindi CCGbank. In section 3, we
develop a supertagger using the CCGbank and ex-
plore different ways of providing CCG categories
from the supertagger as features to Malt. Our re-
sults show that using CCG categories can help
Malt by improving the recovery of long distance
relations.
2 A CCG Treebank from a Dependency
Treebank
There have been some efforts at automatically ex-
tracting treebanks of CCG derivations from phrase
structure treebanks (Hockenmaier and Steedman,
2007; Hockenmaier, 2006; Tse and Curran, 2010),
and CCG lexicons from dependency treebanks
(C?ak?c?, 2005). Bos et al (2009) created a
CCGbank from an Italian dependency treebank by
converting dependency trees into phrase structure
trees and then applying an algorithm similar
to Hockenmaier and Steedman (2007). In this
work, following C?ak?c? (2005), we first extract a
Hindi CCG lexicon from a dependency treebank.
We then use a CKY parser based on the CCG
formalism to automatically obtain a treebank
of CCG derivations from this lexicon, a novel
methodology that may be applicable to obtaining
CCG treebanks in other languages as well.
2.1 Hindi Dependency Treebank
In this paper, we work with a subset of the Hindi
Dependency Treebank (HDT ver-0.5) released
as part of Coling 2012 Shared Task on parsing
(Bharati et al, 2012). HDT is a multi-layered
dependency treebank (Bhatt et al, 2009) an-
notated with morpho-syntactic (morphological,
part-of-speech and chunk information) and
syntactico-semantic (dependency) information
(Bharati et al, 2006; Bharati et al, 2009).
Dependency labels are fine-grained, and mark de-
pendencies that are syntactico-semantic in nature,
such as agent (usually corresponding to subject),
patient (object), and time and place expressions.
There are special labels to mark long distance
relations like relative clauses, co-ordination etc
604
(Bharati et al, 1995; Bharati et al, 2009).
The treebank contains 12,041 training, 1,233
development and 1,828 testing sentences with an
average of 22 words per sentence. We used the
CoNLL format1 for our purposes, which contains
word, lemma, pos-tag, and coarse pos-tag in the
WORD, LEMMA, POS, and CPOS fields respectively
and morphological features and chunk information
in the FEATS column.
2.2 Algorithm
We first made a list of argument and adjunct
dependency labels in the treebank. For e.g.,
dependencies with the label k1 and k2 (corre-
sponding to subject and object respectively) are
considered to be arguments, while labels like
k7p and k7t (corresponding to place and time
expressions) are considered to be adjuncts. For
readability reasons, we will henceforth refer to
dependency labels with their English equivalents
(e.g., SUBJ, OBJ, PURPOSE, CASE for k1, k2,
rt, lwg psp respectively).
Starting from the root of the dependency tree,
we traverse each node. The category of a node de-
pends on both its parent and children. If the node
is an argument of its parent, we assign the chunk
tag of the node (e.g., NP, PP) as its CCG category.
Otherwise, we assign it a category of X|X, where
X is the parent?s result category and | is direction-
ality (\ or /), which depends on the position of
the node w.r.t. its parent. The result category of
a node is the category obtained once its arguments
are resolved. For example, S, is the result category
for (S\NP)\NP. Once we get the partial category
of a node based on the node?s parent information,
we traverse through the children of the node. If
a child is an argument, we add that child?s chunk
tag, with appropriate directionality, to the node?s
category. The algorithm is sketched in Figure 1
and an example of a CCG derivation for a simple
sentence (marked with chunk tags; NP and VGF
are the chunk tags for noun and finite verb chunks
respectively.) is shown in Figure 2. Details of
some special cases are described in the following
subsections.
We created two types of lexicon. In Type 1,
we keep morphological information in noun cate-
gories and in Type 2, we don?t. For example, con-
sider a noun chunk ?raam ne (Ram ERG)?. In Type
1, CCG categories for ?raam? and ?ne? are NP and
1http://nextens.uvt.nl/depparse-wiki/DataFormat
ModifyTree(DependencyTree tree);
for (each node in tree):
handlePostPositionMarkers(node);
handleCoordination(node);
handleRelativeClauses(node);
if (node is an argument of parent):
cat = node.chunkTag;
else:
prescat = parent.resultCategory;
cat = prescat + getDir(node, parent) + prescat;
for(each child of node):
if (child is an argument of node):
cat = cat + getDir(child, node) + child.chunkTag;
Figure 1: Algorithm for extracting CCG lexicon
from a dependency tree.
ROOT mohan ne raam ke lie kitaab khariidii
Mohan Erg Ram for book buy-past-fem
ROOT
OBJ
PURPOSE
CASE
SUBJ
CASE
[NP mohan ne] [NP raam ke lie] [NP kitaab] [VGF khariidii]
NP NP\NP NP (VGF/VGF)\NP NP (VGF\NP)\NP
< < <NP VGF/VGF VGF\NP
< B?VGF\NP
<VGF?Mohan bought a book for Ram.?
Figure 2: An example dependency tree with its
CCG derivation (Erg = Ergative case).
NP[ne]\NP respectively. In Type 2, respective
CCG categories for ?raam? and ?ne? are NP and
NP\NP. Morphological information such as case
(e.g., Ergative case - ?ne?) in noun categories is ex-
pected to help with determining their dependency
labels, but makes the lexicon more sparse.
2.3 Morphological Markers
In Hindi, morphological information is encoded in
the form of post-positional markers on nouns, and
tense, aspect and modality markers on verbs. A
post-positional marker following a noun plays the
role of a case-marker (e.g., ?raam ne (Ram ERG)?,
here ?ne? is the ergative case marker) and can also
have a role similar to English prepositions (e.g.,
?mej par (table on)?). Post-positional markers on
nouns can be simple one word expressions like
?ne? or ?par? or can be multiple words as in ?raam
ke lie (Ram for)?. Complex post position markers
as a whole give information about how the head
noun or verb behaves. We merged complex post
position markers into single words like ?ke lie? so
605
that the entire marker gets a single CCG category.
For an adjunct like ?raam ke lie (for Ram)?
in Figure 2, ?raam? can have a CCG category
VGF/VGF as it is the head of the chunk and
?ke lie? a category of (VGF/VGF)\(VGF/VGF).
Alternatively, if we pass the adjunct information
to the post-position marker (?ke lie?), and use the
chunk tag ?NP? as the category for the head word
(?raam?), then categories of ?raam? and ?ke lie? are
NP and (VGF/VGF)\NP respectively. Though
both these analysis give the same semantics, we
chose the latter as it leads to a less sparse lexi-
con. Also, adjuncts that modify adjacent adjuncts
are assigned identical categories X/X making use
of CCG?s composition rule and following C?ak?c?
(2005).
2.4 Co-ordination
The CCG category of a conjunction is (X\X)/X,
where a conjunction looks for a child to its right
and then a child to its left. To handle conjunc-
tion with multiple children, we modified the de-
pendency tree, as follows.
For the example given below, in the original de-
pendency tree, the conjunction ora ?and? has three
children ?Ram? , ?Shyam? and ?Sita?. We modified
the original dependency tree and treat the comma
?,? as a conjunction. As a result, ?,? will have ?Ram?
and ?Shyam? as children and ?and? will have ?,? and
?Sita? as children. It is straightforward to convert
this tree into the original dependency tree for the
purpose of evaluation/comparison with other de-
pendency parsers.
ROOT raam , shyam ora siitaa skoola gaye
ROOT
DEST
SUBJ
COORD
COORD
COORDCOORD
raam , shyam ora siitaa skoola gayeRam , Shyam and Sita school went
NP (NP\NP)/NP NP (NP\NP)/NP NP NP (VGF\NP)\NP
> > <NP\NP NP\NP VGF\NP
<NP <NP <VGF?Ram , Shyam and Sita went to school.?
2.5 Relative Clauses
In English, relative clauses have the category type
NP\NP, where they combine with a noun phrase
on the left to give a resulting noun phrase. Hindi,
due to its freer word order, has relative clauses of
the type NP\NP or NP/NP based on the position of
the relative clause with respect to the head noun.
Similar to English, the relative pronoun has a CCG
category of (NP|NP)|X where directionality de-
pends on the position of the relative pronoun in the
clause and the category X depends on the gram-
matical role of the relative pronoun. In the follow-
ing example, X is VGF\NP
ROOT vaha ladakaa jo baithaa hai raam hai
ROOT
OBJ
SUBJ
DEM RELCSUBJ AUX
vaha ladakaa jo baithaa hai raam haithat boy who sitting be-1P-pres Ram be-1P-pres
NP/NP NP (NP\NP)/X VGF\NP VGF\VGF NP (VGF\NP)\NP
> > B? <NP VGF\NP VGF\NP
>NP\NP
>NP <VGF
?The boy who is sitting is Ram?
2.6 CCG Lexicon to Treebank conversion
We use a CCG parser to convert the CCG lexicon
to a CCG treebank as conversion to CCG trees
directly from dependency trees is not straight-
forward. Using the above algorithm, we get one
CCG category for every word in a sentence. We
then run a non-statistical CKY chart parser based
on the CCG formalism2, which gives CCG deriva-
tions based on the lexical categories. This gives
multiple derivations for some sentences. We rank
these derivations using two criteria. The first cri-
terion is correct recovery of the gold dependency
tree. Derivations which lead to gold dependencies
are given higher weight. In the second criterion,
we prefer derivations which yield intra-chunk de-
pendencies (e.g., verb and auxiliary) prior to inter-
chunk (e.g., verb and its arguments). For exam-
ple, morphological markers (which lead to intra-
chunk dependencies) play a crucial role in identi-
fying correct dependencies . Resolving these de-
pendencies first helps parsers in better identifica-
tion of inter-chunk dependencies such as argument
structure of the verb (Ambati, 2011). We thus ex-
tract the best derivation for each sentence and cre-
ate a CCGbank for Hindi. Coverage, i.e., number
of sentences for which we got at least one com-
plete derivation, using this lexicon is 96%. The
remaining 4% are either cases of wrong annota-
tions in the original treebank, or rare constructions
which are currently not handled by our conversion
algorithm.
2http://openccg.sourceforge.net/
606
3 Experiments
In this section, we first describe the method of de-
veloping a supertagger using the CCGbank. We
then describe different ways of providing CCG
categories from the supertagger as features to a
state-of-the-art Hindi Dependency parser (Malt).
We did all our experiments using both gold fea-
tures (pos, chunk and morphological information)
provided in the treebank and automatic features
extracted using a Hindi shallow parser3. We re-
port results with automatic features but we also
obtained similar improvements with gold features.
3.1 Category Set
For supertagging, we first obtained a category set
from the CCGbank training data. There are 2,177
and 718 category types in Type 1 (with morph. in-
formation) and Type 2 (without morph. informa-
tion) data respectively. Clark and Curran (2004)
showed that using a frequency cutoff can signif-
icantly reduce the size of the category set with
only a small loss in coverage. We explored dif-
ferent cut-off values and finally used a cutoff of
10 for building the tagger. This reduced the cat-
egory types to 376 and 202 for Type 1 and Type
2 respectively. The percent of category tokens in
development data that don?t appear in the category
set entrailed by this cut-off are 1.39 & 0.47 for
Type 1 and Type 2 respectively.
3.2 Supertagger
Following Clark and Curran (2004), we used
a Maximum Entropy approach to build our su-
pertagger. We explored different features in the
context of a 5-word window surrounding the tar-
get word. We used features based on WORD (w),
LEMMA (l), POS (p), CPOS (c) and the FEATS (f )
columns of the CoNLL format. Table 1 shows the
impact of different features on supertagger perfor-
mance. Experiments 1, 2, 3 have current word (wi)
features while Experiments 4, 5, 6 show the im-
pact of contextual and complex bi-gram features.
Accuracy of the supertagger after Experiment 6
is 82.92% and 84.40% for Type 1 and Type 2 data
respectively. As the number of category types in
Type 1 data (376) are much higher than in Type 2
(202), it is not surprising that the performance of
the supertagger is better for Type 2 as compared to
Type 1.
3http://ltrc.iiit.ac.in/analyzer/hindi/
Experiments: Features Accuracy
Type 1 Type 2
Exp 1: wi, pi 75.14 78.47
Exp 2: Exp 1 + li, ci 77.58 80.17
Exp 3: Exp 2 + fi 80.43 81.88
Exp 4: Exp 3 +wi?1,wi?2, pi?1,pi?2, 82.72 84.15
wi+1, wi+2, pi+1, pi+2
Exp 5: Exp 4 + wipi, wici, wifi, pifi 82.81 84.29
Exp 6: Exp 5 + wi?2wi?1, wi?1wi, 82.92 84.40
wiwi+1, wi+1wi+2, pi?2pi?1,
pi?1pi, pipi+1, pi+1pi+2
Table 1: Impact of different features on the su-
pertagger performance for development data.
3.3 Dependency Parsing
There has been a significant amount of work on
Hindi dependency parsing in the recent past (Hu-
sain, 2009; Husain et al, 2010; Bharati et al,
2012). Out of all these efforts, state-of-the-art ac-
curacy is achieved using the Malt parser. We first
run Malt with previous best settings (Bharati et
al., 2012) which use the arc-standard parsing al-
gorithm with a liblinear learner, and treat this as
our baseline. We compare and analyze results af-
ter adding supertags as features with this baseline.
3.4 Using Supertags as Features to Malt
C?ak?c? (2009) showed that using gold CCG cate-
gories extracted from dependency trees as features
to MST parser (McDonald et al, 2006) boosted
the performance for Turkish. But using automatic
categories from a supertagger radically decreased
performance in their case as supertagger accuracy
was very low. We have explored different ways
of incorporating both gold CCG categories and
supertagger-provided CCG categories into depen-
dency parsing. Following C?ak?c? (2009), instead
of using supertags for all words, we used supertags
which occurred at least K times in the training
data, and backed off to coarse POS-tags otherwise.
We experimented with different values of K and
found that K=15 gave the best results.
We first provided gold CCG categories as fea-
tures to the Malt parser and then provided the out-
put of the supertagger described in section 3.2. We
did all these experiments with both Type 1 and
Type 2 data. Unlabelled Attachment Scores (UAS)
and Labelled Attachment Scores (LAS) for Malt
607
are shown in Table 2. As expected, gold CCG
categories boosted UAS and LAS by around 6%
and 7% respectively, for both Type 1 and Type 2
data. This clearly shows that the rich subcatego-
rization information provided by CCG categories
can help a shift-reduce parser. With automatic cat-
egories from a supertagger, we also got improve-
ments over the baseline, for both Type 1 and Type
2 data. All the improvements are statistically sig-
nificant (McNemar?s test, p < 0.01).
With gold CCG categories, Type 1 data gave
slightly better improvements over Type 2 as Type
1 data has richer morphological information. But,
in the case of supertagger output, Type 2 data
gave more improvements over the baseline Malt
as compared to Type 1. This is because the perfor-
mance of the supertagger on Type 2 data is slightly
better than that of Type 1 data (see Table 1).
Experiment Development Testing
UAS LAS UAS LAS
Malt: Baseline 89.09 83.46 88.67 83.04
Malt + Type 1 Gold 95.87* 90.79* 95.27* 90.22*
Malt + Type 2 Gold 95.73* 90.70* 95.26* 90.18*
Malt + Type 1 ST 89.54* 83.68* 88.93* 83.23*
Malt + Type 2 ST 89.90* 83.96* 89.04* 83.35*
Table 2: Supertagger impact on Hindi dependency
parsing (ST=Supertags). McNemar?s test, * = p <
0.01.
It is interesting to notice the impact of using
automatic CCG categories from a supertagger on
long distance dependencies. It is known that Malt
is weak at long-distance relations (Mcdonald and
Nivre, 2007; Ambati et al, 2010). Providing
CCG categories as features improved handling of
long-distance dependencies for Malt. Figure 3
shows the F-score of the impact of CCG categories
on three dependency labels, which take the ma-
jor share of long distance dependencies, namely,
ROOT, COORD, and RELC, the labels for sentence
root, co-ordination, and relative clause respec-
tively. For these relations, providing CCG cate-
gories gave an increment of 1.2%, 1.4% and 1.6%
respectively over the baseline.
We also found that the impact of CCG cate-
gories is higher when the span of the dependency
is longer. Figure 4 shows the F-score of the impact
of CCG categories on dependencies based on the
distance between words. Using CCG categories
73.12 80.9 
29.89 
74.35 82.28 
31.46 
25 
50 
75 
100 
ROOT COORD RELC 
F-sco
re 
Dependency Labels 
Malt Malt + Type 2 ST 
Figure 3: Label-wise impact of supertag features.
does not have much impact on short distance de-
pendencies (1?5), which Malt is already good at.
For longer range distances, 6?10, and >10, there
is an improvement of 1.8% and 1.4% respectively.
97.3 
79.0 76.4 
97.5 
80.8 77.8 
70 
80 
90 
100 
1 - 5 6 - 10 > 10  
F-sco
re 
Distance ranges  
Malt Malt + Type 2 ST 
Figure 4: Impact of supertags on distance ranges.
4 Conclusion and Future Direction
We have presented an approach for automatically
extracting a CCG lexicon from a dependency tree-
bank for Hindi. We have also presented a novel
way of creating a CCGbank from a dependency
treebank using a CCG parser and the CCG lex-
icon. Unlike previous work, we obtained im-
provements in dependency recovery using auto-
matic supertags, as well as gold information. We
have shown that informative CCG categories im-
prove the performance of a shift-reduce depen-
dency parser (Malt) in recovering some long dis-
tance relations. In future work, we would like to
directly train a CCG shift-reduce parser (such as
Zhang and Clark (2011)?s English parser) on the
Hindi CCGbank. We would also like to see the
impact of generalisation of our lexicon using the
free-word order formalism for CCG categories of
Baldridge (2002).
Acknowledgements
We would like to thank three anonymous review-
ers for their useful suggestions. This work was
supported by ERC Advanced Fellowship 249520
GRAMPLUS.
608
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010.
Two Methods to Incorporate ?Local Morphosyntac-
tic? Features in Hindi Dependency Parsing. In Pro-
ceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages, pages 22?30, Los Angeles, CA, USA, June.
Bharat Ram Ambati. 2011. Hindi Dependency Parsing
and Treebank Validation. Master?s Thesis, Interna-
tional Institute of Information Technology - Hyder-
abad, India.
Jason M. Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh, UK.
Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal.
1995. Natural Language Processing: A Paninian
Perspective. Prentice-Hall of India, pages 65?106.
Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma,
and Lakshmi Bai. 2006. AnnCorra: Annotating
Corpora Guidelines for POS and Chunk Annotation
for Indian Languages. In Technical Report (TR-
LTRC-31), LTRC, IIIT-Hyderabad.
Akshar Bharati, Dipti Misra Sharma, Samar
Husain, Lakshmi Bai, Rafiya Begum, and
Rajeev Sangal. 2009. AnnCorra: Tree-
Banks for Indian Languages, Guidelines for
Annotating Hindi TreeBank (version 2.0).
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf.
Akshar Bharati, Prashanth Mannem, and Dipti Misra
Sharma. 2012. Hindi Parsing Shared Task. In Pro-
ceedings of Coling Workshop on Machine Transla-
tion and Parsing in Indian Languages, Kharagpur,
India.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for Hindi/Urdu. In Proceedings of the
Third Linguistic Annotation Workshop at 47th ACL
and 4th IJCNLP, pages 186?189, Suntec, Singapore.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a Dependency Treebank to a Cat-
egorial Grammar Treebank for Italian. In M. Pas-
sarotti, Adam Przepio?rkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8), pages 27?38, Milan, Italy.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of Student Re-
search Workshop, 43rd Annual Meeting of the ACL,
pages 73?78.
Ruket C?ak?c?. 2009. Parser Models for a Highly In-
flected Language. Ph.D. thesis, University of Edin-
burgh, UK.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282?288.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396, Septem-
ber.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 505?512, Sydney, Aus-
tralia.
Samar Husain, Prashanth Mannem, Bharat Ram Am-
bati, and Phani Gadde. 2010. The ICON-
2010 Tools Contest on Indian Language Depen-
dency Parsing. In Proceedings of ICON-2010 Tools
Contest on Indian Language Dependency Parsing,
Kharagpur, India.
Samar Husain. 2009. Dependency Parsers for Indian
Languages. In Proceedings of the ICON09 NLP
Tools Contest: Indian Language Dependency Pars-
ing, India.
Ryan Mcdonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
and Natural Language Learning.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 216?220, New
York City, New York.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the Penn
Chinese Treebank. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1083?1091, Beijing, China.
Yue Zhang and Stephen Clark. 2011. Shift-Reduce
CCG Parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
683?692, Portland, Oregon, USA, June.
609
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 22?30,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Two methods to incorporate local morphosyntactic features in Hindi de-
pendency parsing 
 
Bharat Ram Ambati, Samar Husain, Sambhav Jain, Dipti Misra Sharma 
and Rajeev Sangal 
Language Technologies Research Centre, IIIT-Hyderabad, India - 500032. 
{ambati,samar}@research.iiit.ac.in, sambhav-
jain@students.iiit.ac.in,{dipti,sangal}@mail.iiit.ac.in 
 
 
Abstract 
In this paper we explore two strategies to in-
corporate local morphosyntactic features in 
Hindi dependency parsing. These features are 
obtained using a shallow parser. We first ex-
plore which information provided by the shal-
low parser is most beneficial and show that 
local morphosyntactic features in the form of 
chunk type, head/non-head information, 
chunk boundary information, distance to the 
end of the chunk and suffix concatenation are 
very crucial in Hindi dependency parsing. We 
then investigate the best way to incorporate 
this information during dependency parsing. 
Further, we compare the results of various ex-
periments based on various criterions and do 
some error analysis. All the experiments were 
done with two data-driven parsers, MaltParser 
and MSTParser, on a part of multi-layered and 
multi-representational Hindi Treebank which 
is under development. This paper is also the 
first attempt at complete sentence level pars-
ing for Hindi. 
1 Introduction 
The dependency parsing community has since a 
few years shown considerable interest in parsing 
morphologically rich languages with flexible word 
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages, 
but it is also motivated by the observation that the 
performance obtained for these languages have not 
been very high (Nivre et al, 2007a). Attempts at 
handling various non-configurational aspects in 
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty 
and Sima'an, 2008; Eryigit et al, 2008; Seddah et 
al., 2009; Husain et al, 2009; Gadde et al, 2010). 
Among other things, it has been pointed out that 
the use of language specific features may play a 
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways, 
and it has been hypothesized that the integration of 
morphological and syntactic information could be 
a key to better accuracy. However, it has also been 
noted that incorporating these language specific 
features in parsing is not always straightforward 
and many intuitive features do not always work in 
expected ways. 
In this paper we explore various strategies to in-
corporate local morphosyntactic features in Hindi 
dependency parsing. These features are obtained 
using a shallow parser. We conducted experiments 
with two data-driven parsers, MaltParser (Nivre et 
al., 2007b) and MSTParser (McDonald et al, 
2006). We first explore which information pro-
vided by the shallow parser is most beneficial and 
show that local morphosyntactic features in the 
form of chunk type, head/non-head information, 
chunk boundary information, distance to the end of 
the chunk and suffix concatenation are very crucial 
in Hindi dependency parsing. We then investigate 
the best way to incorporate this information during 
dependency parsing. All the experiments were 
done on a part of multi-layered and multi-
representational Hindi Treebank (Bhatt et al, 
2009)1.  
The shallow parser performs three tasks, (a) it 
gives the POS tags for each lexical item, (b) pro-
vides morphological features for each lexical item, 
and (c) performs chunking. A chunk is a minimal 
(non-recursive) phrase consisting of correlated, 
inseparable words/entities, such that the intra-
chunk dependencies are not distorted (Bharati et 
                                                          
1 This Treebank is still under development. There are currently 
27k tokens with complete sentence level annotation.  
22
al., 2006). Together, a group of lexical items with 
some POS tag and morphological features within a 
chunk can be utilized to automatically compute 
local morphosyntactic information. For example, 
such information can represent the postposi-
tion/case-marking in the case of noun chunks, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. In 
the experiments conducted for this paper such local 
information is automatically computed and incor-
porated as a feature to the head of a chunk. In gen-
eral, local morphosyntactic features correspond to 
all the parsing relevant local linguistic features that 
can be utilized using the notion of chunk. Previous-
ly, there have been some attempts at using chunk 
information in dependency parsing. Attardi and 
Dell?Orletta (2008) used chunking information in 
parsing English. They got an increase of 0.35% in 
labeled attachment accuracy and 0.47% in unla-
beled attachment accuracy over the state-of-the-art 
dependency parser. 
Among the three components (a-c, above), the 
parsing accuracy obtained using the POS feature is 
taken as baseline. We follow this by experiments 
where we explore how each of morph and chunk 
features help in improving dependency parsing 
accuracy. In particular, we find that local morpho-
syntactic features are the most crucial. These expe-
riments are discussed in section 2. In section 3 we 
will then see an alternative way to incorporate the 
best features obtained in section 2. In all the pars-
ing experiments discussed in section 2 and 3, at 
each step we explore all possible features and ex-
tract the best set of features. Best features of one 
experiment are used when we go to the next set of 
experiments. For example, when we explore the 
effect of chunk information, all the relevant morph 
information from previous set of experiments is 
taken into account.  
This paper is also the first attempt at complete 
sentence level parsing for Hindi. Due to the availa-
bility of dependency treebank for Hindi (Begum et 
al., 2008), there have been some previous attempts 
at Hindi data-driven dependency parsing (Bharati 
et al, 2008; Mannem et al, 2009; Husain et al, 
2009). Recently in ICON-09 NLP Tools Contest 
(Husain, 2009; and the references therein), rule-
based, constraint based, statistical and hybrid ap-
proaches were explored for dependency parsing. 
Previously, constraint based approaches to Indian 
language (IL) dependency parsing have also been 
explored (Bharati et al, 1993, 1995, 2009b, 
2009c). All these attempts, however, were finding 
inter-chunk dependency relations, given gold-
standard POS and chunk tags. Unlike these pre-
vious parsers, the dependencies in this work are 
between lexical items, i.e. the dependency tree is 
complete.  
The paper is arranged as follows, in section 2 
and 3, we discuss the parsing experiments. In sec-
tion 4, we describe the data and parser settings. 
Section 5 gives the results and discusses some re-
lated issues. General discussion and possible future 
work is mentioned in section 6. We conclude the 
paper in section 7. 
2 Getting the best linguistic features  
As mentioned earlier, a shallow parser consists of 
three main components, (a) POS tagger, (b) mor-
phological analyzer and (c) chunker. In this section 
we systematically explore what is the effect of 
each of these components. We?ll see in section 2.3 
that the best features of a-c can be used to compute 
local morphosyntactic features that, as the results 
show, are extremely useful. 
2.1 Using POS as feature (PaF): 
In this experiment we only use the POS tag infor-
mation of individual words during dependency 
parsing. First a raw sentence is POS-tagged. This 
POS-tagged sentence is then given to a parser to 
predict the dependency relations. Figure 1, shows 
the steps involved in this approach for (1). 
 
(1)  raama   ne         eka     seba        khaayaa  
  ?Ram?   ERG    ?one?  ?apple?      ?ate? 
       ?Ram ate an apple? 
 
Figure 1: Dependency parsing using only POS informa-
tion from a shallow parser. 
23
 
In (1) above, ?NN?, ?PSP?, ?QC?, ?NN? and ?VM? 
are the POS tags2 for raama, ne, eka, seba and 
khaayaa respectively. This information is provided 
as a feature to the parser. The result of this experi-
ment forms our baseline accuracy. 
2.2 Using Morph as feature (MaF): 
In addition to POS information, in this experiment 
we also use the morph information for each token. 
This morphological information is provided as a 
feature to the parser. Morph has the following in-
formation 
 
? Root: Root form of the word 
? Category: Course grained POS 
? Gender: Masculine/Feminine/Neuter 
? Number: Singular/Plural 
? Person: First/Second/Third person 
? Case: Oblique/Direct case 
? Suffix: Suffix of the word 
 
Take raama in (1), its morph information com-
prises of root = ?raama?, category = ?noun? gender 
= ?masculine?, number = ?singular?, person = 
?third?, case = ?direct?, suffix = ?0?. Similarly, 
khaayaa (?ate?) has the following morph informa-
tion. root = ?khaa?, category = ?verb? gender = 
?masculine?, numer = ?singular?, person = ?third?, 
case = ?direct?, suffix = ?yaa?. 
Through a series of experiments, the most cru-
cial morph features were selected. Root, case and 
suffix turn out to be the most important features. 
Results are discussed in section 5. 
2.3 Using local morphosyntax as feature 
(LMSaF) 
Along with POS and the most useful morph fea-
tures (root, case and suffix), in this experiment we 
also use local morphosyntactic features that reflect 
various chunk level information. These features 
are: 
? Type of the chunk 
? Head/non-head of the chunk 
                                                          
2 NN: Common noun, PSP: Post position, QC: Cardinal, VM: 
Verb. A list of complete POS tags can be found here: 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/POS-Tag-
List.pdf. The POS/chunk tag scheme followed in the Treebank 
is described in Bharati et al (2006). 
? Chunk boundary information 
? Distance to the end of the chunk 
? Suffix concatenation 
 
In example 1 (see section 2.1), there are two 
noun chunks and one verb chunk. raama and seba 
are the heads of the noun chunks. khaayaa is the 
head of the verb chunk. We follow standard IOB3 
notation for chunk boundary. raama,  eka and 
khaayaa are at the beginning (B) of their respective 
chunks. ne and seba are inside (I) their respective 
chunks. raama is at distance 1 from the end of the 
chunk and ne is at a distance 0 from the end of the 
chunk. 
Once we have a chunk and morph feature like 
suffix, we can perform suffix concatenation auto-
matically. A group of lexical items with some POS 
tags and suffix information within a chunk can be 
utilized to automatically compute this feature. This 
feature can, for example, represent the postposi-
tion/case-marking in the case of noun chunk, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. 
Note that, this feature becomes part of the lexical 
item that is the head of a chunk. Take (2) as a case 
in point: 
 
(2) [NP raama/NNP   ne/PSP]     [NP seba/NN]        
              ?Ram?           ERG                ?apple?   
      [VGF khaa/VM     liyaa/VAUX] 
                 ?eat?           ?PRFT? 
      ?Ram ate an apple? 
 
The suffix concatenation feature for khaa, which 
is the head of the VGF chunk, will be ?0+yaa? and 
is formed by concatenating the suffix of the main 
verb with that of its auxiliary. Similarly, the suffix 
concatenation feature for raama, which is head of 
the NP chunk, will be ?0+ne?. This feature turns 
out to be very important. This is because in Hindi 
(and many other Indian languages) there is a direct 
correlation between the TAM markers and the case 
that appears on some nominals (Bharati et al, 
1995). In (2), for example, khaa liyaa together 
gives the past perfective aspect for the verb khaa-
naa ?to eat?. Since, Hindi is split ergative, the sub-
ject of the transitive verb takes an ergative case 
marker when the verb is past perfective. Similar 
                                                          
3 Inside, Outside, Beginning of the chunk. 
24
correlation between the case markers and TAM 
exist in many other cases. 
3 An alternative approach to use best fea-
tures: A 2-stage setup (2stage) 
So far we have been using various information 
such as POS, chunk, etc. as features. Rather than 
using them as features and doing parsing at one go, 
we can alternatively follow a 2-stage setup. In par-
ticular, we divide the task of parsing into:  
 
? Intra-chunk dependency parsing 
? Inter-chunk dependency parsing 
 
We still use POS, best morphological features 
(case, suffix, root) information as regular features 
during parsing. But unlike LMSaF mentioned in 
section 2.3, where we gave local morphosyntactic 
information as a feature, we divided the task of 
parsing into sub-tasks. A similar approach was also 
proposed by Bharati et al (2009c). During intra-
chunk dependency parsing, we try to find the de-
pendency relations of the words within a chunk. 
Following which, chunk heads of each chunk with-
in a sentence are extracted. On these chunk heads 
we run an inter-chunk dependency parser. For each 
chunk head, in addition to POS tag, useful morpho-
logical features, any useful intra-chunk information 
in the form of lexical item, suffix concatenation, 
dependency relation are also given as a feature. 
 
Figure 2: Dependency parsing using chunk information: 
2-stage approach. 
Figure 2 shows the steps involved in this ap-
proach for (1). There are two noun chunks and one 
verb chunk in this sentence. raama and seba are 
the heads of the noun chunks. khaaya is the head 
of the verb chunk. The intra-chunk parser attaches 
ne to raama and eka to seba with dependency la-
bels ?lwg__psp? and ?nmod__adj?4 respectively. 
Heads of each chunk along with its POS, morpho-
logical features, local morphosyntactic features and 
intra-chunk features are extracted and given to in-
ter-chunk parser. Using this information the inter-
chunk dependency parser marks the dependency 
relations between chunk heads. khaaya becomes 
the root of the dependency tree. raama and seba 
are attached to khaaya with dependency labels ?k1? 
and ?k2?5 respectively. 
4 Experimental Setup 
In this section we describe the data and the parser 
settings used for our experiments.  
4.1 Data 
For our experiments we took 1228 dependency 
annotated sentences (27k tokens), which have 
complete sentence level annotation from the new 
multi-layered and multi-representational Hindi 
Treebank (Bhatt et al, 2009). This treebank is still 
under development. Average length of these sen-
tences is 22 tokens/sentence and 10 
chunks/sentence. We divided the data into two 
sets, 1000 sentences for training and 228 sentences 
for testing.  
4.2 Parsers and settings 
All experiments were performed using two data-
driven parsers, MaltParser6 (Nivre et al, 2007b), 
and MSTParser7 (McDonald et al, 2006).
                                                          
4 nmod__adj is an intra-chunk label for quantifier-noun mod-
ification. lwg__psp is the label for post-position marker. De-
tails of the labels can be seen in the intra-chunk guidelines 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/IntraChunk-
Dependency-Annotation-Guidelines.pdf 
5 k1 (karta) and k2 (karma) are syntactico-semantic labels 
which have some properties of both grammatical roles and 
thematic roles. k1 behaves similar to subject and agent. k2 
behaves similar to object and patient (Bharati et al, 1995; 
Vaidya et al, 2009). For complete tagset, see (Bharati et al, 
2009). 
6 Malt Version 1.3.1 
7 MST Version 0.4b 
25
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 89.4 78.2 80.5 90.4 80.1 82.4 86.3 75.1 77.9 87.9 77.0 79.3 
MaF 89.6 80.5 83.1 90.4 81.7 84.1 89.1 79.2 82.5 90.0 80.9 83.9 
LMSaF 91.5 82.7 84.7 91.8 84.0 86.2 90.8 79.8 82.0 92.0 81.8 83.8 
2stage 91.8 83.3 85.3 92.4 84.4 86.3 92.1 82.2 84.3 92.7 84.0 86.2 
Table 1: Results of all the four approaches using gold-standard shallow parser information. 
 
Malt is a classifier based shift/reduce parser. It 
provides option for six parsing algorithms, namely, 
arc-eager, arc-standard, convington projective, co-
vington non-projective, stack projective, stack ea-
ger and stack lazy. The parser also provides option 
for libsvm and liblinear learning model. It uses 
graph transformation to handle non-projective trees 
(Nivre and Nilsson, 2005). MST uses Chu-Liu-
Edmonds (Chu and Liu, 1965; Edmonds, 1967) 
Maximum Spanning Tree algorithm for non-
projective parsing and Eisner's algorithm for pro-
jective parsing (Eisner, 1996). It uses online large 
margin learning as the learning algorithm (McDo-
nald et al, 2005). In this paper, we use MST only 
for unlabeled dependency tree and use a separate 
maximum entropy model8 (MaxEnt) for labeling. 
Various combination of features such as node, its 
parent, siblings and children were tried out before 
arriving at the best results. 
As the training data size is small we did 5-fold 
cross validation on the training data for tuning the 
parameters of the parsers and for feature selection. 
Best settings obtained using cross-validated data 
are applied on test set. We present the results both 
on cross validated data and on test data.  
For the Malt Parser, arc-eager algorithm gave 
better performance over others in all the approach-
es. Libsvm consistently gave better performance 
over liblinear in all the experiments. For SVM set-
tings, we tried out different combinations of best 
SVM settings of the same parser on different lan-
guages in CoNLL-2007 shared task (Hall et al, 
2007) and applied the best settings. For feature 
model, apart from trying best feature settings of the 
same parser on different languages in CoNLL-
2007 shared task (Hall et al, 2007), we also tried 
out different combinations of linguistically intui-
tive features and applied the best feature model. 
The best feature model is same as the feature mod-
el used in Ambati et al (2009a), which is the best 
                                                          
8 http://maxent.sourceforge.net/ 
performing system in the ICON-2009 NLP Tools 
Contest (Husain, 2009). 
For the MSTParser, non-projective algorithm, 
order=2 and training-k=5 gave best results in all 
the approaches. For the MaxEnt, apart from some 
general useful features, we experimented consider-
ing different combinations of features of node, par-
ent, siblings, and children of the node.  
5 Results and Analysis 
All the experiments discussed in section 2 and 3 
were performed considering both gold-standard 
shallow parser information and automatic shallow 
parser9 information. Automatic shallow parser uses 
a rule based system for morph analysis, a 
CRF+TBL based POS-tagger and chunker. The 
tagger and chunker are 93% and 87% accurate re-
spectively. These accuracies are obtained after us-
ing the approach of PVS and Gali, (2007) on larger 
training data. In addition, while using automatic 
shallow parser information to get the results, we 
also explored using both gold-standard and auto-
matic information during training. As expected, 
using automatic shallow parser information for 
training gave better performance than using gold 
while training.  
Table 1 and Table 2 shows the results of the four 
experiments using gold-standard and automatic 
shallow parser information respectively. We eva-
luated our experiments based on unlabeled attach-
ment score (UAS), labeled attachment score (LAS) 
and labeled score (LS) (Nivre et al, 2007a). Best 
LAS on test data is 84.4% (with 2stage) and 75.4% 
(with LMSaF) using gold and automatic shallow 
parser information respectively. These results are 
obtained using MaltParser. In the following sub-
section we discuss the results based on different 
criterion.
                                                          
9 http://ltrc.iiit.ac.in/analyzer/hindi/ 
26
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 82.2 69.3  73.4  84.6  72.9  76.5  79.4  66.5  70.7  81.6  69.4  73.1  
MaF 82.5 71.6  76.1  84.0  73.6  77.6  82.3  70.4  75.4  83.4  72.7  77.3  
LMSaF 83.2 73.0  77.0  85.5  75.4  78.9  82.6  71.3  76.1  85.0  73.4  77.3  
2stage 79.0 69.5 75.6 79.6 71.1 76.8 78.8  66.6  72.6 80.1  69.7  75.4  
Table 2: Results of all the four experiments using automatic shallow parser information. 
 
POS tags provide very basic linguistic informa-
tion in the form of broad grained categories. The 
best LAS for PaF while using gold and automatic 
tagger were 80.1% and 72.9% respectively. The 
morph information in the form of case, suffix and 
root information proved to be the most important 
features. But surprisingly, gender, number and per-
son features didn?t help. Agreement patterns in 
Hindi are not straightforward. For example, the 
verb agrees with k2 if the k1 has a post-position; it 
may also sometimes take the default features. In a 
passive sentence, the verb agrees only with k2. The 
agreement problem worsens when there is coordi-
nation or when there is a complex verb. It is un-
derstandable then that the parser is unable to learn 
the selective agreement pattern which needs to be 
followed.  
LMSaF on the other hand encode richer infor-
mation and capture some local linguistic patterns. 
The first four features in LMSaF (chunk type, 
chunk boundary, head/non-head of chunk and dis-
tance to the end of chunk) were found to be useful 
consistently. The fifth feature, in the form of suffix 
concatenation, gave us the biggest jump, and cap-
tures the correlation between the TAM markers of 
the verbs and the case markers on the nominals. 
5.1 Feature comparison: PaF, MaF vs. 
LMSaF 
Dependency labels can be classified as two types 
based on their nature, namely, inter-chunk depen-
dency labels and intra-chunk labels. Inter-chunk 
dependency labels are syntacto-semantic in nature. 
Whereas intra-chunk dependency labels are purely 
syntactic in nature.  
Figure 3, shows the f-measure for top six inter-
chunk and intra-chunk dependency labels for PaF, 
MaF, and LMSaF using Maltparser on test data 
using automatic shallow parser information. The 
first six labels (k1, k2, pof, r6, ccof, and k7p) are 
the top six inter-chunk labels and the next six la-
bels (lwg__psp, lwg__aux, lwg__cont, rsym, 
nmod__adj, and pof__cn) are the top six intra-
chunk labels. First six labels (inter-chunk) corres-
pond to 28.41% and next six labels (intra-chunk) 
correspond to 48.81% of the total labels in the test 
data. The figure shows that with POS information 
alone, f-measure for top four intra-chunk labels 
reached more than 90% accuracy. The accuracy 
increases marginally with the addition of morph 
and local morphosytactic features. The results cor-
roborates with our intuition that intra-chunk de-
pendencies are mostly syntactic.  For example, 
consider an intra-chunk label ?lwg__psp?. This is 
the label for postposition marker. A post-position 
marker succeeding a noun is attached to that noun 
with the label ?lwg__psp?. POS tag for post-
position marker is PSP. So, if a NN (common 
noun) or a NNP (proper noun) is followed by a 
PSP (post-position marker), then the PSP will be 
attached to the preceding NN/NNP with the de-
pendency label ?lwg_psp?. As a result, providing 
POS information itself gave an f-measure of 98.3% 
for ?lwg_psp?.  With morph and local morphosy-
tactic features, this got increased to 98.4%. How-
ever, f-measure for some labels like ?nmod__adj? 
is around 80% only. ?nmod__adj? is the label for 
adjective-noun, quantifier-noun modifications. 
Low accuracy for these labels is mainly due to two 
reasons. One is POS tag errors. And the other is 
attachment errors due to genuine ambiguities such 
as compounding. 
For inter-chunk labels (first six columns in the 
figure 3), there is considerable improvement in the 
f-measure using morph and local morphosytactic 
features. As mentioned, local morphosyntactic fea-
tures provide local linguistic information. For ex-
ample, consider the case of verbs. At POS level, 
there are only two tags ?VM? and ?VAUX? for 
main verbs and auxiliary verbs respectively (Bha-
rati et al, 2006). Information about finite/non-
finiteness is not present in the POS tag. But, at 
chunk level there are four different chunk tags for
27
30
40
50
60
70
80
90
100
k1 k2 pof r6 ccof k7p lwg__psp lwg__vaux lwg__cont rsym nmod__adj pof__cn
PaF
MaF
LMaF
Figure 3: F-measure of top 6, inter-chunk and intra-chunk labels for PaF, MaF and LMSaF approaches using Malt-
parser on test data using automatic shallow parser information. 
 
verbs, namely VGF, VGNF, VGINF and VGNN. 
They are respectively, finite, non-finite, infinitival 
and gerundial chunk tags. The difference in the 
verbal chunk tag is a good cue for helping the 
parser in identifying different syntactic behavior of 
these verbs. Moreover, a finite verb can become 
the root of the sentence, whereas a non-finite or 
infinitival verb can?t. Thus, providing chunk in-
formation also helped in improving the correct 
identification of the root of the sentence. 
Similar to Prague Treebank (Hajicova, 1998), 
coordinating conjuncts are heads in the treebank 
that we use. The relation between a conjunct and 
its children is shown using ?ccof? label. A coordi-
nating conjuct takes children of similar type only. 
For example, a coordinating conjuct can have two 
finite verbs or two non-finite verbs as its children, 
but not a finite verb and a non-finite verb. Such 
instances are also handled more effectively if 
chunk information is incorporated. The largest in-
crease in performance, however, was due to the 
?suffix concatenation? feature. Significant im-
provement in the core inter-chunk dependency la-
bels (such as k1, k2, k4, etc.) due to this feature is 
the main reason for the overall improvement in the 
parsing accuracy. As mentioned earlier, this is be-
cause this feature captures the correlation between 
the TAM markers of the verbs and the case mark-
ers on the nominals. 
5.2 Approach comparison: LMSaF vs. 2stage 
Both LMSaF and 2stage use chunk information. In 
LMSaF, chunk information is given as a feature 
whereas in 2stage, sentence parsing is divided into 
intra-chunk and inter-chunk parsing. Both the ap-
proaches have their pros and cons. In LMSaF as 
everything is done in a single stage there is much 
richer context to learn from. In 2stage, we can pro-
vide features specific to each stage which can?t be 
done in a single stage approach (McDonald et al, 
2006). But in 2stage, as we are dividing the task, 
accuracy of the division and the error propagation 
might pose a problem. This is reflected in the re-
sults where the 2-stage performs better than the 
single stage while using gold standard information, 
but lags behind considerably when the features are 
automatically computed.  
During intra-chunk parsing in the 2stage setup, 
we tried out using both a rule-based approach and 
a statistical approach (using MaltParser). The rule 
based system performed slightly better (0.1% 
LAS) than statistical when gold chunks are consi-
dered. But, with automatic chunks, the statistical 
approach outperformed rule-based system with a 
difference of 7% in LAS. This is not surprising 
because, the rules used are very robust and mostly 
based on POS and chunk information. Due to er-
rors induced by the automatic POS tagger and 
chunker, the rule-based system couldn?t perform 
well. Consider a small example chunk given be-
low. 
 ((    NP 
 meraa ?my?   PRP  
 bhaaii ?brother? NN 
)) 
As per the Hindi chunking guidelines (Bharati et 
al., 2006), meraa and bhaaii should be in two sepa-
rate chunks. And as per Hindi dependency annota-
tion guidelines (Bharati et al, 2009), meraa is 
attached to bhaaii with a dependency label ?r6?10. 
When the chunker wrongly chunks them in a single 
                                                          
10?r6? is the dependency label for genitive relation. 
28
chunk, intra-chunk parser will assign the depen-
dency relation for meraa. Rule based system can 
never assign ?r6? relation to meraa as it is an inter-
chunk label and the rules used cannot handle such 
cases. But in a statistical system, if we train the 
parser using automatic chunks instead of gold 
chunks, the system can potentially assign ?r6? la-
bel.  
5.3 Parser comparison: MST vs. Malt 
In all the experiments, results of MaltParser are 
consistently better than MST+MaxEnt. We know 
that Maltparser is good at short distance labeling 
and MST is good at long distance labeling (McDo-
nald and Nivre, 2007). The root of the sentence is 
better identified by MSTParser than MaltParser. 
Our results also confirm this. MST+MaxEnt and 
Malt could identify the root of the sentence with an 
f-measure of 89.7% and 72.3% respectively. Pres-
ence of more short distance labels helped Malt to 
outperform MST. Figure 5, shows the f-measure 
relative to dependency length for both the parsers 
on test data using automatic shallow parser infor-
mation for LMSaF.  
30
40
50
60
70
80
90
100
0 5 10 15+
Dependency Length
f-
m
ea
su
re
Malt
MST+MaxEnt
 
Figure 5: Dependency arc f-measure relative to depen-
dency length. 
6 Discussion and Future Work 
We systematically explored the effect of various 
linguistic features in Hindi dependency parsing. 
Results show that POS, case, suffix, root, along 
with local morphosyntactic features help depen-
dency parsing. We then described 2 methods to 
incorporate such features during the parsing 
process. These methods can be thought as different 
paradigms of modularity. For practical reasons (i.e. 
given the POS tagger/chunker accuracies), it is 
wiser to use this information as features rather than 
dividing the task into two stages.  
As mentioned earlier, this is the first attempt at 
complete sentence level parsing for Hindi. So, we 
cannot compare our results with previous attempts 
at Hindi dependency parsing, due to, (a) The data 
used here is different and (b) we produce complete 
sentence parses rather than chunk level parses. 
As mentioned in section 5.1, accuracies of intra-
chunk dependencies are very high compared to 
inter-chunk dependencies. Inter-chunk dependen-
cies are syntacto-semantic in nature. The parser 
depends on surface syntactic cues to identify such 
relations. But syntactic information alone is always 
not sufficient, either due to unavailability or due to 
ambiguity. In such cases, providing some semantic 
information can help in improving the inter-chunk 
dependency accuracy. There have been attempts at 
using minimal semantic information in dependency 
parsing for Hindi (Bharati et al, 2008). Recently, 
Ambati et al (2009b) used six semantic features 
namely, human, non-human, in-animate, time, 
place, and abstract for Hindi dependency parsing. 
Using gold-standard semantic features, they 
showed considerable improvement in the core in-
ter-chunk dependency accuracy. Some attempts at 
using clause information in dependency parsing for 
Hindi (Gadde et al, 2010) have also been made. 
These attempts were at inter-chunk dependency 
parsing using gold-standard POS tags and chunks. 
We plan to see their effect in complete sentence 
parsing using automatic shallow parser information 
also.  
7 Conclusion 
In this paper we explored two strategies to incorpo-
rate local morphosyntactic features in Hindi de-
pendency parsing. These features were obtained 
using a shallow parser. We first explored which 
information provided by the shallow parser is use-
ful  and showed that local morphosyntactic fea-
tures in the form of chunk type, head/non-head 
info, chunk boundary info, distance to the end of 
the chunk and suffix concatenation are very crucial 
for Hindi dependency parsing. We then investi-
gated the best way to incorporate this information 
during dependency parsing. Further, we compared 
the results of various experiments based on various 
criterions and did some error analysis. This paper 
was also the first attempt at complete sentence lev-
el parsing for Hindi. 
29
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing. In 
Proc of the ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, pp 32-37.  
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. 
Effect of Minimal Semantics on Dependency Pars-
ing. In Proc of RANLP09 student paper workshop. 
G. Attardi and F. Dell?Orletta. 2008. Chunking and De-
pendency Parsing. In Proc of LREC Workshop on 
Partial Parsing: Between Chunking and Deep Pars-
ing. Marrakech, Morocco. 
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. In Proc of IJCNLP-2008. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural 
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi. 
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 
and R. Sangal. 2008. Two semantic features make all 
the difference in parsing accuracy. In Proc of ICON. 
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. 
AnnCorra: Annotating Corpora Guidelines for POS 
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad. 
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam 
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi 
TreeBank. 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
2009b. Two stage constraint based hybrid approach 
to free word order language dependency parsing. In 
Proc. of IWPT. 
A. Bharati, S. Husain, M. Vijay, K. Deepak, D. M. 
Sharma and R. Sangal. 2009c. Constraint Based Hy-
brid Approach to Parsing Indian Languages. In Proc 
of PACLIC 23. Hong Kong. 2009. 
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 
M. Sharma and F. Xia. 2009. Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu. In 
Proc. of the Third LAW at 47th ACL and 4th IJCNLP. 
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400. 
J. Edmonds. 1967. Optimum branchings. Journal of 
Research of the National Bureau of Standards, 
71B:233?240. 
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc of 
COLING-96, pp. 340?345. 
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency 
Parsing of Turkish. Computational Linguistics 34(3), 
357-389. 
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. 
Sangal. 2010. Improving Data Driven Dependency 
Parsing using Clausal Information. In Proc of 
NAACL-HLT 2010, Los Angeles, CA. 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation. In Proc of 
TSD?98. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson and M. Saers. 2007. Single Malt or Blended? 
A Study in Multilingual Parser Optimization. In Proc 
of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, 933?939. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proc of ICON09 NLP Tools Contest: In-
dian Language Dependency Parsing. Hyderabad, 
India. 
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. 
Sangal. 2009. A modular cascaded approach to com-
plete parsing. In Proc. of the COLIPS IALP. 
P. Mannem, A. Abhilash and A. Bharati. 2009. LTAG-
spinal Treebank and Parser for Hindi. In Proc of In-
ternational Conference on NLP, Hyderabad. 2009. 
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In 
Proc of ACL. pp. 91?98. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. In Proc of the Tenth (CoNLL-X), pp. 
216?220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models. In 
Proc. of EMNLP-CoNLL. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proc of 
EMNLP/CoNLL-2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 
13(2), 95-135. 
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL-2005, pp. 99?106. 
Avinesh PVS and K. Gali. 2007. Part-Of-Speech Tag-
ging and Chunking Using Conditional Random 
Fields and Transformation Based Learning. In Proc 
of the SPSAL workshop during IJCAI '07. 
D. Seddah, M. Candito and B. Crabb?. 2009. Cross 
parser evaluation: a French Treebanks study. In Proc. 
of IWPT, 150-161. 
R. Tsarfaty and K. Sima'an. 2008. Relational-
Realizational Parsing. In Proc. of CoLing, 889-896. 
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 
2009. A karaka-based dependency annotation scheme 
for English. In Proc. of CICLing, 41-52. 
30
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 94?102,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
On the Role of Morphosyntactic Features in Hindi Dependency Parsing 
 
Bharat Ram Ambati*, Samar Husain*, Joakim Nivre? and Rajeev Sangal* 
*Language Technologies Research Centre, IIIT-Hyderabad, India. 
?Department of Linguistics and Philology, Uppsala University, Sweden. 
{bharat,samar}@research.iiit.ac.in, joakim.nivre@lingfil.uu.se, san-
gal@mail.iiit.ac.in 
 
 
 
 
Abstract 
This paper analyzes the relative importance of 
different linguistic features for data-driven 
dependency parsing of Hindi, using a feature 
pool derived from two state-of-the-art parsers.  
The analysis shows that the greatest gain in 
accuracy comes from the addition of morpho-
syntactic features related to case, tense, aspect 
and modality. Combining features from the 
two parsers, we achieve a labeled attachment 
score of 76.5%, which is 2 percentage points 
better than the previous state of the art. We fi-
nally provide a detailed error analysis and 
suggest possible improvements to the parsing 
scheme. 
1 Introduction 
The dependency parsing community has since a 
few years shown considerable interest in parsing 
morphologically rich languages with flexible word 
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages, 
but it is also motivated by the observation that the 
performance obtained for these languages has not 
been very high (Nivre et al, 2007a). Attempts at 
handling various non-configurational aspects in 
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty 
and Sima'an, 2008; Eryigit et al, 2008; Seddah et 
al., 2009; Husain et al, 2009; Gadde et al, 2010). 
Among other things, it has been pointed out that 
the use of language specific features may play a 
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways, 
and it has been hypothesized that the integration of 
morphological and syntactic information could be 
a key to better accuracy. However, it has also been 
noted that incorporating these language specific 
features in parsing is not always straightforward 
and many intuitive features do not always work in 
expected ways. 
In this paper, we are concerned with Hindi, an 
Indian language with moderately rich morphology 
and relatively free word order.  There have been 
several previous attempts at parsing Hindi as well 
as other Indian languages (Bharati et al, 1995, 
Bharati et al, 2009b). Many techniques were tried 
out recently at the ICON09 dependency parsing 
tools contest (Husain, 2009). Both the best per-
forming system (Ambati et al, 2009a) and the sys-
tem in second place (Nivre, 2009b) used a 
transition-based approach to dependency parsing, 
as implemented in MaltParser (Nivre et al, 2007b). 
Other data driven parsing efforts for Indian lan-
guages in the past have been Bharati et al (2008), 
Husain et al (2009), Mannem et al (2009b) and 
Gadde et al (2010). 
In this paper, we continue to explore the transi-
tion-based approach to Hindi dependency parsing, 
building on the state-of-the-art results of Ambati et 
al. (2009a) and Nivre (2009b) and exploring the 
common pool of features used by those systems. 
Through a series of experiments we select features 
incrementally to arrive at the best parser features. 
The primary purpose of this investigation is to 
study the role of different morphosyntactic features 
in Hindi dependency parsing, but we also want to 
improve the overall parsing accuracy. Our final 
results are 76.5% labeled and 91.1% unlabeled at-
tachment score, improving previous results by 2 
and 1 percent absolute, respectively. In addition to 
this, we also provide an error analysis, isolating 
specific linguistic phenomena and/or other factors 
that impede the overall parsing performance, and 
suggest possible remedies for these problems. 
94
2 The Hindi Dependency Treebank 
Hindi is a free word order language with SOV as 
the default order. This can be seen in (1), where 
(1a) shows the constituents in the default order, 
and the remaining examples show some of the 
word order variants of (1a). 
 
(1) a. malaya  ne     sameer      ko     kitaba   dii.  
          Malay   ERG  Sameer    DAT   book    gave 
        ?Malay gave the book to Sameer? (S-IO-DO-V)1 
       b. malaya ne kitaba sameer ko dii. (S-DO-IO-V) 
       c. sameer ko malaya ne kitaba dii. (IO-S-DO-V) 
       d. sameer ko kitaba malaya ne dii. (IO-DO-S-V) 
       e. kitaba malaya ne sameer ko dii. (DO-S-IO-V) 
        f. kitaba sameer ko malaya ne dii.  (DO-IO-S-V) 
 
Hindi also has a rich case marking system, al-
though case marking is not obligatory. For exam-
ple, in (1), while the subject and indirect object are 
explicitly marked for the ergative (ERG) and da-
tive (DAT) cases, the direct object is unmarked for 
the accusative.  
The Hindi dependency treebank (Begum et al, 
2008) used for the experiment was released as part 
of the ICON09 dependency parsing tools contest 
(Husain, 2009). The dependency framework (Bha-
rati et al, 1995) used in the treebank is inspired by 
Panini?s grammar of Sanskrit. The core labels, 
called karakas, are syntactico-semantic relations 
that identify the participant in the action denoted 
by the verb. For example, in (1), ?Malay? is the 
agent, ?book? is the theme, and ?Sameer? is the be-
neficiary in the activity of ?give?. In the treebank, 
these three labels are marked as k1, k2, and k4 re-
spectively. Note, however, that the notion of kara-
ka does not capture the ?global? semantics of 
thematic roles; rather it captures the elements of 
the ?local semantics? of a verb, while also taking 
cues from the surface level morpho-syntactic in-
formation (Vaidya et al, 2009).  The syntactic re-
lational cues (such as case markers) help identify 
many of the karakas. In general, the highest availa-
ble karaka,2 if not case-marked, agrees with the 
verb in an active sentence.  In addition, the tense, 
                                                          
1 S=Subject; IO=Indirect Object; DO=Direct Object; 
V=Verb; ERG=Ergative; DAT=Dative 
2 These are the karta karaka (k1) and karma karaka (k2). k1 
and k2 can be roughly translated as ?agent? and ?theme? re-
spectively. For a complete description of the tagset and the 
dependency scheme, see Begum et al (2008) and Bharati et al 
(2009a).  
aspect and modality (TAM) marker can many a 
times control the case markers that appear on k1. 
For example, in (1) ?Malay? takes an ergative case 
because of the past perfective TAM marker (that 
appears as a suffix in this case) of the main verb 
?gave?. Many dependency relations other than ka-
rakas are purely syntactic. These include relations 
such as noun modifier (nmod), verb modifier 
(vmod), conjunct relation (ccof), etc. 
Each sentence is manually chunked and then an-
notated for dependency relations. A chunk is a mi-
nimal, non-recursive structure consisting of 
correlated groups of words (Bharati et al, 2006). A 
node in a dependency tree represents a chunk head. 
Each lexical item in a sentence is also annotated 
with its part-of-speech (POS).  For all the experi-
ments described in this paper we use gold POS and 
chunk tags. Together, a group of lexical items with 
some POS tags within a chunk can be utilized to 
automatically compute coarse grained morphosyn-
tactic information. For example, such information 
can represent the postposition/case-marking in the 
case of noun chunks, or it may represent the TAM 
information in the case of verb chunks. In the ex-
periments conducted for this paper this local in-
formation is automatically computed and 
incorporated as a feature of the head of a chunk. 
As we will see later, such information proves to be 
extremely crucial during dependency parsing. 
For all the experiments discussed in section 4, 
the training and development data size was 1500 
and 150 sentences respectively. The training and 
development data consisted of ~22k and ~1.7k 
words respectively. The test data consisted of 150 
sentences (~1.6k words). The average sentence 
length is 19.85. 
3 Transition-Based Dependency Parsing 
A transition-based dependency parser is built of 
two essential components (Nivre, 2008): 
 
? A transition system for mapping sentences to 
dependency trees 
? A classifier for predicting the next transition for 
every possible system configuration 
 
 
 
 
95
 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS 
Stack:        top 1 5 1 7  9  
Input:        next 1 5 1 7  9  
Input:        next+1 2 5 6 7    
Input:        next+2 2       
Input:        next+3 2       
Stack:       top-1 3       
String:      predecessor of top 3       
Tree:        head of top 4       
Tree:        leftmost dep of next 4 5 6     
Tree:        rightmost dep of top     8   
Tree:        left sibling of rightmost dep of top     8   
Merge:     PTAG of top and next       10 
Merge:     CTAM and DEPREL of top       10 
 
Table 1. Feature pool based on selection from Ambati et al (2009a) and Nivre (2009b). 
 
Given these two components, dependency parsing 
can be realized as deterministic search  through the 
transition system, guided by the classifier. With 
this technique, parsing can be performed in linear 
time for projective dependency trees. Like Ambati 
et al (2009a) and Nivre (2009b), we use MaltPars-
er, an open-source implementation of transition-
based dependency parsing with a variety of transi-
tion systems and customizable classifiers.3 
3.1 Transition System 
Previous work has shown that the arc-eager projec-
tive transition system first described in Nivre 
(2003) works well for Hindi (Ambati et al, 2009a; 
Nivre, 2009b). A parser configuration in this sys-
tem contains a stack holding partially processed 
tokens, an input buffer containing the remaining 
tokens, and a set of arcs representing the partially 
built dependency tree. There are four possible tran-
sitions (where top is the token on top of the stack 
and next is the next token in the input buffer): 
 
? Left-Arc(r): Add an arc labeled r from next to 
top; pop the stack. 
? Right-Arc(r): Add an arc labeled r from top to 
next; push next onto the stack. 
? Reduce: Pop the stack. 
? Shift: Push next onto the stack. 
 
Although this system can only derive projective 
dependency trees, the fact that the trees are labeled 
                                                          
3 MaltParser is available at http://maltparser.org. 
allows non-projective dependencies to be captured 
using the pseudo-projective parsing technique pro-
posed in Nivre and Nilsson (2005). 
3.2 Classifiers 
Classifiers can be induced from treebank data us-
ing a wide variety of different machine learning 
methods, but all experiments reported below use 
support vector machines with a polynomial kernel, 
as implemented in the LIBSVM package (Chang 
and Lin, 2001) included in MaltParser. The task of 
the classifier is to map a high-dimensional feature 
vector representation of a parser configuration to 
the optimal transition out of that configuration. The 
features used in our experiments represent the fol-
lowing attributes of input tokens: 
 
? PTAG: POS tag of chunk head. 
? CTAG: Chunk tag. 
? FORM: Word form of chunk head. 
? LEMMA: Lemma of chunk head. 
? DEPREL: Dependency relation of chunk. 
? CTAM: Case and TAM markers of chunk.  
 
The PTAG corresponds to the POS tag associated 
with the head of the chunk, whereas the CTAG 
represent the chunk tag. The FORM is the word 
form of the chunk head, and the LEMMA is auto-
matically computed with the help of a morphologi-
cal analyzer. CTAM gives the local 
morphosyntactic features such as case markers 
(postpositions/suffixes) for nominals and TAM 
markers for verbs (cf. Section 2). 
96
The pool of features used in the experiments are 
shown in Table 1, where rows denote tokens in a 
parser configuration ? defined relative to the stack, 
the input buffer, the partially built dependency tree 
and the input string ? and columns correspond to 
attributes. Each non-empty cell represents a fea-
ture, and features are numbered for easy reference. 
4 Feature Selection Experiments 
Starting from the union of the feature sets used by 
Ambati et al (2009a and by Nivre (2009b), we 
first used 5-fold cross-validation on the combined 
training and development sets from the ICON09 
tools contest to select the pool of features depicted 
in Table 1, keeping all features that had a positive 
effect on both labeled and unlabeled accuracy. We 
then grouped the features into 10 groups (indicated 
by numbers 1?10 in Table 1) and reran the cross-
validation, incrementally adding different feature 
groups in order to analyze their impact on parsing 
accuracy. The result is shown in Figure 1. 
30
36
42
48
54
60
66
72
78
84
90
Ex
p1
Ex
p2
Ex
p3
Ex
p4
Ex
p5
Ex
p6
Ex
p7
Ex
p8
Ex
p9
Ex
p1
0
UAS
LAS
 
Figure 1. UAS and LAS of experiments 1-10; 5-fold 
cross-validation on training and development data of the 
ICON09 tools contest. 
 
Experiment 1: Experiment 1 uses a baseline 
model with only four basic features: PTAG and 
FORM of top and next. This results in a labeled 
attachment score (LAS) of 41.7% and an unlabeled 
attachment score (UAS) of 68.2%. 
Experiments 2?3: In experiments 2 and 3, the 
PTAG of contextual words of next and top are 
added. Of all the contextual words, next+1, 
next+2, next+3, top-1 and predecessor of top were 
found to be useful.4 Adding these contextual fea-
tures gave a modest improvement to 45.7% LAS 
and 72.7% UAS. 
Experiment 4: In experiment 4, we used the 
PTAG information of nodes in the partially built 
tree, more specifically the syntactic head of top 
and the leftmost dependent of next. Using these 
features gave a large jump in accuracy to 52% 
LAS and 76.8% UAS. This is because partial in-
formation is helpful in making future decisions. 
For example, a coordinating conjunction can have 
a node of any PTAG category as its child. But all 
the children should be of same category. Knowing 
the PTAG of one child therefore helps in identify-
ing other children as well. 
Experiments 5?7: In experiments 5, 6 and 7, 
we explored the usefulness of CTAG, FORM, and 
LEMMA attributes. These features gave small in-
cremental improvements in accuracy; increasing 
LAS to 56.4% and UAS to 78.5%. It is worth not-
ing in particular that the addition of LEMMA 
attributes only had a marginal effect on accuracy, 
given that it is generally believed that this type of 
information should be beneficial for richly in-
flected languages. 
Experiment 8: In experiment 8, the DEPREL of 
nodes in the partially formed tree is used. The 
rightmost child and the left sibling of the rightmost 
child of top were found to be useful. This is be-
cause, if we know the dependency label of one of 
the children, then the search space for other child-
ren gets reduced. For example, a verb cannot have 
more than one k1 or k2. If we know that the parser 
has assigned k1 to one of its children, then it 
should use different labels for the other children. 
The overall effect on parsing accuracy is neverthe-
less very marginal, bringing LAS to 56.5% and 
UAS to 78.6%. 
Experiment 9: In experiment 9, the CTAM 
attribute of top and next is used. This gave by far 
the greatest improvement in accuracy with a huge 
jump of around 10% in LAS (to 66.3%) and 
slightly less in UAS (to 84.7%). Recall that CTAM 
consists of two important morphosyntactic fea-
tures, namely, case markers (as suffixes or postpo-
sitions) and TAM markers. These feature help 
because (a) case markers are important surface  
                                                          
4 The predecessor of top is the word occurring immediately 
before top in the input string, as opposed to top-1, which is the 
word immediately below top in the current stack. 
97
 
Figure 2. Precision and Recall of some important dependency labels. 
 
cues that help identify various dependency rela-
tions, and (b) there exists a direct mapping be-
tween many TAM labels and the nominal case 
markers because TAMs control the case markers of 
some nominals. As expected, our experiments 
show that the parsing decisions are certainly more 
accurate after using these features. In particular, (a) 
and (b) are incorporated easily in the parsing 
process.  
In a separate experiment we also added some 
other morphological features such as gender, num-
ber and person for each node. Through these fea-
tures we expected to capture the agreement in 
Hindi. The verb agrees in gender, number and per-
son with the highest available karaka. However, 
incorporating these features did not improve pars-
ing accuracy and hence these features were not 
used in the final setting. We will have more to say 
about agreement in section 5. 
Experiment 10: In experiment 10, finally, we 
added conjoined features, where the conjunction of 
POS of next and top and of CTAM and DEPREL 
of top gave slight improvements. This is because a 
child-parent pair type can only take certain labels. 
For example, if the child is a noun and the parent is 
a verb, then all the dependency labels reflecting 
noun, adverb and adjective modifications are not 
relevant. Similarly, as noted earlier, certain case-
TAM combinations demand a particular set of la-
bels only. This can be captured by the combination 
tried in this experiment. 
Experiment 10 gave the best results in the cross-
validation experiments. The settings from this ex-
periment were used to get the final performance on 
the test data. Table 2 shows the final results along 
with the results of the first and second best per-
forming systems in the ICON09 tools contest. We 
see that our system achieved an improvement of 2 
percentage points in LAS and 1 percentage point in 
UAS over the previous state of the art reported in 
Ambati et al (2009a). 
 
System LAS UAS 
Ambati et al (2009a) 74.5 90.1 
Nivre (2009b) 73.4 89.8 
Our system 76.5 91.1 
 
Table 2. Final results on the test data from the ICON09 
tools contest. 
5 Error Analysis 
In this section we provide a detailed error analysis 
on the test data and suggest possible remedies for 
problems noted. We note here that other than the 
reasons mentioned in this section, small treebank 
size could be another reason for low accuracy of 
the parser. The training data used for the experi-
ments only had ~28.5k words. With recent work on 
Hindi Treebanking (Bhatt et al, 2009) we expect 
to get more annotated data in the near future. 
Figure 2 shows the precision and recall of some 
important dependency labels in the test data. The 
labels in the treebank are syntacto-semantic in na-
ture. Morph-syntactic features such as case mark-
ers and/or TAM labels help in identifying these 
labels correctly. But lack of nominal postpositions 
can pose problems. Recall that many case mark-
ings in Hindi are optional. Also recall that the verb 
agrees with the highest available karaka. Since 
agreement features do not seem to help, if both k1 
and k2 lack case markers, k1-k2 disambiguation 
becomes difficult (considering that word order in-
formation cannot help in this disambiguation). In 
the case of k1 and k2, error rates for instances that 
lack post-position markers are 60.9% (14/23) and 
65.8% (25/38), respectively. 
 
 
98
 Correct Incorrect 
  k1 k1s k2 pof k7p k7t k7 others 
k1 184 5 3 8 3  1  3 
k1s 12 6  1 6    1 
k2 126 14  1 7 5   11 
pof 54 1 8 4      
k7p 54 3  7   1 2 3 
k7t 27 3  3 3  1  10 
k7 3 2   2 4    
 
Table 3. Confusion matrix for important labels. The 
diagonal under ?Incorrect? represents attachment errors. 
 
Table 3 shows the confusion matrix for some 
important labels in the test data. As the present 
information available for disambiguation is not 
sufficient, we can make use of some semantics to 
resolve these ambiguities. Bharati et al (2008) and 
Ambati et al (2009b) have shown that this ambi-
guity can be reduced using minimal semantics. 
They used six semantic features: human, non-
human, in-animate, time, place and abstract. Using 
these features they showed that k1-k2 and k7p-k7t 
ambiguities can be resolved to a great extent. Of 
course, automatically extracting these semantic 
features is in itself a challenging task, although 
?vrelid (2008) has shown that animacy features 
can be induced automatically from data. 
In section 4 we mentioned that a separate expe-
riment explored the effectiveness of morphological 
features like gender, number and person. Counter 
to our intuitions, these features did not improve the 
overall accuracy. Accuracies on cross-validated 
data while using these features were less than the 
best results with 66.2% LAS and 84.6% UAS. 
Agreement patterns in Hindi are not straightfor-
ward. For example, the verb agrees with k2 if the 
k1 has a post-position; it may also sometimes take 
the default features. In a passive sentence, the verb 
agrees only with k2. The agreement problem wor-
sens when there is coordination or when there is a 
complex verb. It is understandable then that the 
parser is unable to learn the selective agreement 
pattern which needs to be followed. Similar prob-
lems with agreement features have also been noted 
by Goldberg and Elhadad (2009). 
In the following sections, we analyze the errors 
due to different constructions and suggest possible 
remedies.  
 
 
5.1 Simple Sentences 
A simple sentence is one that has only one main 
verb. In these sentences, the root of the dependen-
cy tree is the main verb, which is easily identified 
by the parser. The main problem is the correct 
identification of the argument structure. Although 
the attachments are mostly correct, the dependency 
labels are error prone. Unlike in English and other 
more configurational languages, one of the main 
cues that help us identify the arguments is to be 
found in the nominal postpositions. Also, as noted 
earlier these postpositions are many times con-
trolled by the TAM labels that appear on the verb. 
There are four major reasons for label errors in 
simple sentences: (a) absence of postpositions, (b) 
ambiguous postpositions, (c) ambiguous TAMs, 
and (d) inability of the parser to exploit agreement 
features. For example in (2), raama and phala are 
arguments of the verb khaata. Neither of them has 
any explicit case marker. This makes it difficult for 
the parser to identify the correct label for these 
nodes. In (3a) and (3b) the case marker se is ambi-
guous. It signifies ?instrument? in (3b) and ?agent? 
in (3a). 
 
(2) raama    phala    khaata    hai 
     ?Ram?    ?fruit?    ?eat?       ?is? 
     ?Ram eats a fruit? 
 
(3) a. raama   se     phala  khaayaa nahi    gaya 
         ?Ram? INST ?fruit?  ?eat?      ?not?  ?PAST? 
         ?Ram could not eat the fruit? 
     b. raama  chamach   se     phala    khaata  hai 
         ?Ram?  ?spoon?   INST ?fruit?    ?eat?     ?is? 
          ?Ram eats fruit with spoon? 
5.2 Embedded Clauses 
Two major types of embedded constructions in-
volve participles and relative clause constructions. 
Participles in Hindi are identified through a set of 
TAM markers. In the case of participle embed-
dings, a sentence will have more than one verb, 
i.e., at least one participle and the matrix verb. 
Both the matrix (finite) verb and the participle can 
take their own arguments that can be identified via 
the case-TAM mapping discussed earlier. Howev-
er, there are certain syntactic constraints that limit 
the type of arguments a participle can take.  There 
99
are two sources of errors here: (a) argument shar-
ing, and (b) ambiguous attachment sites.  
Some arguments such as place/time nominals 
can be shared. Shared arguments are assigned to 
only one verb in the dependency tree. So the task 
of identifying the shared arguments, if any, and 
attaching them to the correct parent is a complex 
task. Note that the dependency labels can be identi-
fied based on the morphosyntactic features. The 
task becomes more complex if there is more than 
one participle in a sentence. 12 out of 130 in-
stances (9.23%) of shared arguments has an incor-
rect attachment.  
Many participles are ambiguous and making the 
correct attachment choice is difficult. Similar par-
ticiples, depending on the context, can behave as 
adverbials and attach to a verb, or can behave as 
adjectives and attach to a noun. Take (4) as a case 
in point.  
 
(4) maine     daurte   hue        kutte  ko   dekhaa 
     ?I?-ERG  (while) running   dog   ACC ?saw?             
 
In (4) based on how one interprets ?daurte hue?, 
one gets either the reading that ?I saw a running 
dog? or that ?I saw a dog while running?. In case of 
the adjectival participle construction (VJJ), 2 out of 
3 errors are due to wrong attachment. 
5.3 Coordination 
Coordination poses problems as it often gives rise 
to long-distance dependencies. Moreover, the tree-
bank annotation treats the coordinating conjunction 
as the head of the coordinated structure. Therefore, 
a coordinating conjunction can potentially become 
the root of the entire dependency tree. This is simi-
lar to Prague style dependency annotation (Hajico-
va, 1998). Coordinating conjunctions pose 
additional problems in such a scenario as they can 
appear as the child of different heads. A coordinat-
ing conjunction takes children of similar POS cat-
egory, but the parent of the conjunction depends on 
the type of the children.  
 
 
(5) a. raama  aur  shyaama   ne     khaana khaayaa                                    
        ?Ram? ?and? ?Shyam? ?ERG?  ?food?   ?ate?          
        ?Ram and Shyam ate the food.? 
 
 
     b. raama   ne    khaanaa  khaayaa  aur  paanii    
          ?Ram?  ?ERG? ?food?     ?ate?     ?and? ?water?  
         piyaa 
         ?drank? 
         ?Ram ate food and drank water.? 
 
In (5a), raama and shyaama are children of the 
coordinating conjunction aur, which gets attached 
to the main verb khaayaa with the label k1. In ef-
fect, syntactically aur becomes the argument of the 
main verb. In (5b), however, the verbs khaayaa 
and piyaa are the children of aur. In this case, aur 
becomes the root of the sentence. Identifying the 
nature of the conjunction and its children becomes 
a challenging task for the parser. Note that the 
number of children that a coordinating conjunction 
can take is not fixed either. The parser could iden-
tify the correct head of the conjunctions with an 
accuracy of 75.7% and the correct children with an 
accuracy of 85.7%. 
The nature of the conjunction will also affect the 
dependency relation it has with its head. For ex-
ample, if the children are nouns, then the conjunc-
tion behaves as a noun and can potentially be an 
argument of a verb. By contrast, if the children are 
finite verbs, then it behaves as a finite verb and can 
become the root of the dependency tree. Unlike 
nouns and verbs, however, conjunctions do not 
have morphological features. So a child-to-head 
feature percolation should help make a coordinat-
ing node more transparent. For example, in (5a) the 
Ergative case ne is a strong cue for the dependency 
label k1. If we copy this information from one of 
its children (here shyaama) to the conjunct, then 
the parser can possibly make use of this informa-
tion. 
5.4 Complex Predicates 
Complex predicates are formed by combining a 
noun or an adjective with a verbalizer kar or ho. 
For instance, in taariif karanaa ?to praise?, taariif 
?praise? is a noun and karanaa ?to do? is a verb. 
Together they form the main verb. Complex predi-
cates are highly productive in Hindi. Combination 
of the light verb and the noun/adjective is depen-
dent on not only syntax but also semantics and 
therefore its automatic identification is not always 
straightforward (Butt, 1995).  A noun-verb com-
plex predicate in the treebank is linked via the de-
pendency label pof. The parser makes mistakes in 
100
identifying pof or misclassifies other labels as pof. 
In particular, the confusion is with k2 and k1s 
which are object/theme and noun complements of 
k1, respectively. These labels share similar contex-
tual features like the nominal element in the verb 
complex. Table 3 includes the confusion matrix for 
pof errors.  
5.5 Non-Projectivity 
As noted earlier, MaltParser?s arc-eager parsing 
algorithm can be combined with the pseudo-
projective parsing techniques proposed in Nivre 
and Nilsson (2005), which potentially helps in 
identifying non-projective arcs. The Hindi treebank 
has ~14% non-projective arcs (Mannem et al, 
2009a). In the test set, there were a total of 11 non-
projective arcs, but the parser did not find any of 
them. This is consistent with earlier results show-
ing that pseudo-projective parsing has high preci-
sion but low recall, especially when the percentage 
of non-projective relations is small (Nilsson et al 
2007). 
Non-projectivity has proven to be one of the ma-
jor problems in dependency parsing, especially for 
free word-order languages. In Hindi, the majority 
of non-projective arcs are inter-clausal (Mannem et 
al., 2009a), involving conjunctions and relative 
clauses. There have been some attempts at han-
dling inter-clausal non-projectivity in Hindi. Hu-
sain et al (2009) proposed a two-stage approach 
that can handle some of the inter-clausal non-
projective structures. 
5.6 Long-Distance Dependencies  
Previous results on parsing other languages have 
shown that MaltParser has lower accuracy on long-
distance dependencies. Our results confirm this. 
Errors in the case of relative clauses and coordina-
tion can mainly be explained in this way. For ex-
ample, there are 8 instances of relative clauses in 
the test data. The system could identify only 2 of 
them correctly. These two are at a distance of 1 
from its parent. For the remaining 6 instances the 
distance to the parent of the relative clause ranges 
from 4 to 12. 
Figure 3 shows how parser performance de-
creases with increasing distance between the head 
and the dependent. Recently, Husain et al (2009) 
have proposed a two-stage setup to parse inter-
clausal and intra-clausal dependencies separately. 
They have shown that most long distance relations 
are inter-clausal, and therefore, using such a clause 
motivated parsing setup helps in maximizing both 
short distance and long distance dependency accu-
racy. In a similar spirit, Gadde et al (2010) showed 
that using clausal features helps in identifying long 
distance dependencies. They have shown that pro-
viding clause information in the form of clause 
boundaries and clausal heads can help a parser 
make better predictions about long distance depen-
dencies. 
0
20
40
60
80
100
0 2 4 6 8 10 12
Dependency Length
D
ep
en
de
nc
y 
Pr
ec
is
io
n
 
0
20
40
60
80
100
0 1 2 3 4 5 6 7 8 9 10 11 12
Dependency Length
D
ep
en
de
nc
y 
Re
ca
ll
 
Figure 3. Dependency arc precision/recall relative to 
dependency length, where the length of a dependency 
from wi to wj is |i-j| and roots are assumed to have dis-
tance 0 to their head. 
6 Conclusion 
In this paper we have analyzed the importance of 
different linguistic features in data-driven parsing 
of Hindi and at the same time improved the state of 
the art. Our main finding is that the combination of 
case markers on nominals with TAM markers on 
verbs is crucially important for syntactic disambig-
uation, while the inclusion of features such as per-
son, number gender that help in agreement has not 
yet resulted in any improvement. We have also 
presented a detailed error analysis and discussed 
possible techniques targeting different error 
classes. We plan to use these techniques to im-
prove our results in the near future. 
101
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing. 
Proc. of ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, 32-37.  
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. 
Effect of Minimal Semantics on Dependency Pars-
ing. Proc. of RANLP Student Research Workshop.  
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. Proc. of IJCNLP. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural 
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi. 
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 
and R. Sangal. 2008. Two semantic features make all 
the difference in parsing accuracy. Proc. of ICON. 
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. 
AnnCorra: Annotating Corpora Guidelines for POS 
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad. 
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam 
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi 
TreeBank. 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
2009b. Two stage constraint based hybrid approach 
to free word order language dependency parsing. In 
Proc. of IWPT. 
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 
M. Sharma and F. Xia. 2009. Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu. Proc. 
of the Third LAW at ACL-IJCNLP, 186-189. 
M. Butt. 1995. The Structure of Complex Predicates in 
Urdu. CSLI Publications. 
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency 
Parsing of Turkish. Computational Linguistics 34(3), 
357-389. 
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. 
Sangal. 2010. Improving Data Driven Dependency 
Parsing using Clausal Information. Proc. of NAACL-
HLT. 
Y. Goldberg and M. Elhadad. 2009. Hebrew Dependen-
cy Parsing: Initial Results. Proc. of IWPT, 129-133. 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation.  Proc. of 
TSD.  
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson and M. Saers. 2007. Single Malt or Blended? 
A Study in Multilingual Parser Optimization. Proc. 
of EMNLP-CoNLL, 933-939. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. Proc. of ICON09 NLP Tools Contest: Indian 
Language Dependency Parsing.  
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. 
Sangal. 2009. A modular cascaded approach to com-
plete parsing. Proc. of the COLIPS International 
Conference on Asian Language Processing.  
P. Mannem, H. Chaudhry, and A. Bharati. 2009a. In-
sights into non-projectivity in Hindi. Proc. of ACL-
IJCNLP Student Research Workshop. 
P. Mannem, A. Abhilash and A. Bharati. 2009b. LTAG-
spinal Treebank and Parser for Hindi. Proceedings of 
International Conference on NLP, Hyderabad. 2009. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. Proc. of CoNLL, 216-220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models.  
Proc. of EMNLP-CoNLL, 122-131. 
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York. 
J. Nilsson, J. Nivre and J. Hall. 2007. Generalizing Tree 
Transformations for Inductive Dependency Parsing. 
Proc. of ACL, 968-975. 
J. Nivre. 2008. Algorithms for Deterministic Incremen-
tal Dependency Parsing. Computational Linguistics 
34(4), 513-553. 
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In 
Proceedings of ACL-HLT, pp. 950-958. 
J. Nivre. 2009a.  Non-Projective Dependency Parsing in 
Expected Linear Time. Proc. of ACL-IJCNLP, 351-
359. 
J. Nivre. 2009b. Parsing Indian Languages with Malt-
Parser. Proc. of ICON09 NLP Tools Contest: Indian 
Language Dependency Parsing, 12-18. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. Proc. of 
EMNLP/CoNLL, 915-932. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. NLE, 13(2), 95-135. 
L. ?vrelid. 2008. Argument Differentiation. Soft con-
straints and data-driven models. PhD Thesis, Uni-
versity of Gothenburg. 
D. Seddah, M. Candito and B. Crabb?. 2009. Cross 
parser evaluation: a French Treebanks study. Proc. of 
IWPT, 150-161. 
R. Tsarfaty and K. Sima'an. 2008. Relational-
Realizational Parsing. Proc. of CoLing, 889-896. 
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 
2009. A karaka-based dependency annotation scheme 
for English. Proc. of CICLing, 41-52. 
102
