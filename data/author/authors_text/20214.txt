Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 118?123, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ECNUCS: Recognizing Cross-lingual Textual Entailment Using Multiple
Text Similarity and Text Difference Measures
Jiang ZHAO
Department of Computer
Science and Technology
East China Normal University
Shanghai, P.R.China
51121201042@ecnu.cn
Man LAN?
Department of Computer
Science and Technology
East China Normal University
Shanghai, P.R.China
mlan@cs.ecnu.edu.cn
Zheng-Yu NIU
Baidu Inc.
Beijing, P.R.China
niuzhengyu@baidu.com
Abstract
This paper presents our approach used for
cross-lingual textual entailment task (task 8)
organized within SemEval 2013. Cross-
lingual textual entailment (CLTE) tries to de-
tect the entailment relationship between two
text fragments in different languages. We
solved this problem in three steps. Firstly,
we use a off-the-shelf machine translation
(MT) tool to convert the two input texts into
the same language. Then after performing a
text preprocessing, we extract multiple feature
types with respect to surface text and gram-
mar. We also propose novel feature types
regarding to sentence difference and seman-
tic similarity based on our observations in the
preliminary experiments. Finally, we adopt a
multiclass SVM algorithm for classification.
The results on the cross-lingual data collec-
tions provided by SemEval 2013 show that (1)
we can build portable and effective systems
across languages using MT and multiple ef-
fective features; (2) our systems achieve the
best results among the participants on two test
datasets, i.e., FRA-ENG and DEU-ENG.
1 Introduction
The Cross-lingual Textual Entailment (CLTE) task
in SemEval 2013 consists in detecting the entail-
ment relationship between two topic-related text
fragments (usually called T(ext) and H(ypothesis))
in different languages, which is a cross-lingual ex-
tension of TE task in (Dagan and Glickman, 2004).
We say T entails H if the meaning of H can be in-
ferred from the meaning of T. Mehdad et al (2010b)
firstly proposed this problem within a new challeng-
ing application scenario, i.e., content synchroniza-
tion. In consideration of the directionality, the task
needs to assign one of the following entailment judg-
ments to a pair of sentences (1) forward: unidirec-
tional entailment from T to H; (2) backward: unidi-
rectional entailment from H to T; (3) bidirectional:
the two fragments entail each other (i.e., semantic
equivalence); (4) non-entailment: there is no entail-
ment between T and H.
During the last decades, many researchers and
communities have paid a lot of attention to resolve
the TE detection (e.g., seven times of the Rec-
ognizing Textual Entailment Challenge, i.e., from
RTE1 to RET7, have been held) since identifying
the relationship between two sentences is at the core
of many NLP applications, such as text summa-
rization (Lloret et al, 2008) or question answer-
ing (Harabagiu and Hickl, 2006). For example,
in text summarization, a redundant sentence should
be omitted from the summary if this sentence can
be entailed from other expressions in the summary.
CLTE extends those tasks with lingual dimension-
ality, where more than one language is involved.
Although it is a relatively new task, a basic solu-
tion has been provided in (Mehdad et al, 2010b),
which brings the problem back to monolingual sce-
nario using MT to translate H into the language of
T. The promising performance indicates the poten-
tialities of such a simple approach which integrates
MT and monolingual TE algorithms (Castillo, 2011;
Jimenez et al, 2012; Mehdad et al, 2010a).
In this work, we regard CLTE as a multiclass clas-
sification problem, in which multiple feature types
are used in conjunction with a multiclass SVM clas-
sifier. Specifically, our approach can be divided
into three steps. Firstly, following (Espla`-Gomis
et al, 2012; Meng et al, 2012), we use MT to
118
bridge the gap of language differences between T
and H. Secondly, we perform a preprocessing pro-
cedure to maximize the similarity of the two text
fragments so as to make a more accurate calcula-
tion of surface text similarity measures. Besides sev-
eral features described in previous work (Malakasi-
otis, 2009; Espla`-Gomis et al, 2012), we also pro-
pose several novel features regarding to sentence dif-
ference and semantic similarity. Finally, all these
features are combined together and serves as input
of a multiclass SVM classifier. After analyzing of
the results obtained in preliminary experiments, we
also cast this problem as a hierarchical classification
problem.
The remainder of the paper is organized as fol-
lows. Section 2 describes different features used in
our systems. Section 3 presents the system settings
including the datasets and preprocessing. Section 4
shows the results of different systems on different
language pairs. Finally, we conclude this paper with
future work in Section 5.
2 Features
In this section, we will describe a variety of feature
types used in our experiments.
2.1 Basic features
The BC feature set consists of length measures on
variety sets including |A|, |B|, |A?B|, |B?A|, |A?
B|, |A ? B|, |A|/|B| and |B|/|A|, where A and B
represent two texts, and the length of set is the num-
ber of non-repeated elements in this set. Once we
view the text as a set of words, A?B means the set
of words found in A but not in B, A ? B means the
set of words found in either A or B and A?B means
the set of shared words found in both A and B.
Given a pair of texts, i.e., <T,H>, which are in
different languages, we use MT to translate one of
them to make them in the same language. Thus,
we can get two pairs of texts, i.e., <Tt,H> and
<T,H t>. We apply the above eight length measures
to the two pairs, resulting in a total of 16 features.
2.2 Surface Text Similarity features
Following (Malakasiotis and Androutsopoulos,
2007), the surface text similarity (STS) feature set
contains nine similarity measures:
Jaccard coefficient: It is defined as |A?B|
|A?B| , where
|A ?B| and |A ?B| are as in the BC.
Dice coefficient: Defined as 2?|A?B|
|A|+|B| .
Overlap coefficient: This is the following quantity,
Overlap(A,B) = |A?B|
|A| .
Weighted overlap coefficient: We assign the tf*idf
value to each word in the sentence to distinguish
the importance of different words. The weighted
overlap coefficient is defined as follows:
WOverlap(A,B) =
?
w
i
?A?B Wwi
?
w
i
?AWwi
,
where Ww
i
is the weight of word wi.
Cosine similarity: cos(??x ,??y ) =
??x ???y
?
??x ?????y ? , where
??x and ??y are vectorial representations of texts (i.e.
A and B) in tf ? idf schema.
Manhattan distance: Defined as M(??x ,??y ) =
n
?
i=1
|xi ? yi|.
Euclidean distance: Defined as E(??x ,??y ) =
?
n
?
i=1
(xi ? yi)2.
Edit distance: This is the minimum number of op-
erations needed to transform A to B. We define an
operation as an insertion, deletion or substitution of
a word.
Jaro-Winker distance: Following (Winkler and
others, 1999), the Jaro-Winkler distance is a mea-
sure of similarity between two strings at the word
level.
In total, we can get 11 features in this feature set.
2.3 Sematic Similarity features
Almost every previous work used the surface texts
or exploited the meanings of words in the dictio-
nary to calculate the similarity of two sentences
rather than the actual meaning in the sentence. In
this feature set (SS), we introduce a latent model
to model the semantic representations of sentences
since latent models are capable of capturing the
contextual meaning of words in sentences. We
used weighted textual matrix factorization (WTMF)
(Guo and Diab, 2012) to model the semantics of
the sentences. The model factorizes the original
term-sentence matrix X into two matrices such that
Xi,j ? P T
?,iQ?,j , where P?,i is a latent semantics
119
vector profile for word wi and Q?,j is the vector pro-
file that represents the sentence sj . The weight ma-
trix W is introduced in the optimization process in
order to model the missing words at the right level
of emphasis. We propose three similarity measures
according to different strategies:
wtw: word-to-word based similarity defined as
sim(A,B) = lg
?
w
i
?A
W
w
i
?max
w
j
?B
(P
?,i
,P
?,j
)
?
w
i
?A
W
w
i
.
wts: word-to-sentence based similarity defined as
sim(A,B) = lg
?
w
i
?A
W
w
i
?P
?,i
?Q
?,k
?
w
i
?A
W
w
i
.
sts: sentence-to-sentence based similarity defined as
sim(A,B) = lg (Q
?,i ?Q?,j).
Also we calculate the cosine similarity, Euclidean
and Manhattan distance, weighted overlap coeffi-
cient using those semantics vectors, resulting in 10
features.
2.4 Sentence Difference features
Most of those above measures are symmetric and
only a few are asymmetric, which means they may
not be very suitable for the task that requires dealing
with directional problems. We solve this problem by
introducing sentence difference measures.
We observed that many entailment relationships
between two sentences are determined by only tiny
parts of the sentences. As a result, the similarity of
such two sentences by using above measures will be
close to 1, which may mislead the classifier. Fur-
thermore, almost all similarity measures in STS are
symmetric, which means the same similarity has no
help to distinguish the different directions. Based on
the above considerations, we propose a novel sen-
tence difference (SD) feature set to discover the dif-
ferences between two sentences and tell the classi-
fier the possibility the entailment should not hold.
The sentence difference features are extracted as
follows. Firstly, a word in one sentence is consid-
ered as matched if we can find the same word in the
other sentence. Then we find all matched words and
count the number of unmatched words in each sen-
tence, resulting in 2 features. If one sentence has
no unmatched words, we say that this sentence can
be entailed by the other sentence. That is, we can
infer the entailment class through the number of un-
matched words. We regard this label as our third
feature type. Secondly, different POS types of un-
matched words may have different impacts on the
classification, therefore we count the number of un-
matched words in each sentence that belong to a
small set of POS tags (here consider only NN, JJ,
RB, VB and CD tags), which produces 10 features,
resulting in a total of 13 sentence difference features.
2.5 Grammatical Relationship features
The grammatical relationship feature type (GR) is
designed to capture the grammatical relationship be-
tween two sentences. We first replace the words in a
sentence with their part-of-speech (POS) tags, then
apply the STS measures on this new ?sentence?.
In addition, we use the Stanford Parser to get the
dependency information represented in a form of re-
lation units (e.g. nsubj(example, this)). We calculate
the BC measures on those units and the overlap co-
efficients together with the harmonic mean of them.
Finally, we get 22 features.
2.6 Bias features
The bias features (BS) are to check the differences
between two sentences in certain special aspects,
such as polarity and named entity. We use a method
based on subjectivity of lexicons (Loughran and Mc-
Donald, 2011) to get the polarity of a sentence by
simply comparing the numbers of positive and neg-
ative words. If the numbers are the same, then we
set the feature to 1, otherwise -1. Also, we check
whether one sentence entails the other using only
the named entity information. We consider four cat-
egories of named entities, i.e., person, organization,
location, number, which are recognized by using the
Stanford NER toolkit. We set the feature to 1 if the
named entities in one sentence are found in the other
sentence, otherwise -1. As a result, this feature set
contains 9 features.
3 Experimental Setting
We evaluated our approach using the data sets
provided in the task 8 of SemEval 2013 (Ne-
gri et al, 2013). The data sets consist of a
collection of 1500 text fragment pairs (1000 for
training consisting of training and test set in Se-
mEval 2012 and 500 for test) in each language
pair. Four different language pairs are provided:
German-English, French-English, Italian-English
and Spanish-English. See (Negri et al, 2013) for
more detailed description.
120
3.1 Preprocess
We performed the following text preprocessing.
Firstly, we employed the state-of-the-art Statistical
Machine Translator, i.e., Google translator, to trans-
late each pair of texts <T,H> into <Tt,H> and
<T,H t>, thus they were in the same language. Then
we extracted all above described feature sets from
the pair <T t,H> (note that <T,Ht> are also used
in BC), so the below steps were mainly operated on
this pair. After that, all sentences were tokenized
and lemmatized using the Stanford Lemmatizer and
all stop words were removed, followed by the equiv-
alent replacement procedure. The replacement pro-
cedure consists of the following 3 steps:
Abbreviative replacement. Many phrases or orga-
nizations can be abbreviated to a set of capitalized
letters, e.g. ?New Jersey? is usually wrote as ?NJ?
for short. In this step, we checked every word whose
length is 2 or 3 and if it is the same as the ?word?
consisting of the first letters of the successive words
in another sentence, then we replaced it by them.
Semantic replacement. We observed that although
some lemmas in H and T were in the different forms,
they actually shared the same meaning, e.g. ?hap-
pen? and ?occur?. Here, we focused on replacing a
lemma in one sentence with another lemma in the
other sentence if they were: 1) in the same syn-
onymy set; or 2) gloss-related. Two lemmas were
gloss-related if a lemma appeared in the gloss of the
other. For example, the gloss of ?trip? is ?a jour-
ney for some purpose? (WordNet 2.1 was used for
looking up the synonymy and gloss of a lemma), so
the lemma ?journey? is gloss-related with ?trip?. No
word sense disambiguation was performed and all
synsets for a particular lemma were considered.
Context replacement. The context of a lemma
is defined as the non-stopword lemmas around it.
Given two text fragments, i.e., T. ...be erroneously
label as a ?register sex offender.? and H. ...be mis-
takenly inscribe as a ?register sex offender?., af-
ter the semantic replacement, we can recognize the
lemma ?erroneously? was replaceable by ?mistak-
enly?. However, WordNet 2.1 cannot recognize the
lemmas ?label? and ?inscribe? which can also be
replaceable. To address this problem, we simply as-
sumed that two lemmas surrounded by the same con-
text can be replaceable as well. In the experiments,
we set the window size of context replacement as 3.
This step is the foundation of the extraction of
the sentence different features and can also allevi-
ate the imprecise similarity measure problem exist-
ing in STS caused by the possibility of the lemmas
in totally different forms sharing the same sense.
3.2 System Configuration
We selected 500 samples from the training data as
development set (i.e. test set in SemEval 2012) and
performed a series of preliminary experiments to
evaluate the effectiveness of different feature types
in isolation and also in different combinations. Ac-
cording to the results on the development set, we
configured five different systems on each language
pair as our final submissions with different feature
types and classification strategies. Table1 shows the
five configurations of those systems.
System Feature Set Description
1 all flat, SVM
2 best feature sets flat, SVM
3 best feature sets flat, Majority Voting
4 best feature sets flat, only 500 instancesfor train, SVM
5 best feature sets hierarchical, SVM
Table 1: System configurations using different strategies
based on the results of preliminary experiments.
Among them, System 1 serves as a baseline that
used all features and was trained using a flat SVM
while System 2 used only the best feature combi-
nations. In our preliminary experiments, different
language pairs had different best feature combina-
tions (showed in Table 2). In System 3 we per-
formed a majority voting strategy to combine the
results of different algorithm (i.e. MaxEnt, SVM,
liblinear) to further improve performance. System
4 is a backup system that used only the training set
in SemEval 2012 to explore the influence of the dif-
ferent size of train set. Based on the analysis of the
preliminary results on development set, we also find
that the misclassification mainly occur between the
class of backward and others. So in System 5, we
adopted hierarchical classification technique to filter
out backward class in the first level using a binary
classifier and then conducted multi-class classifica-
tion among the remaining three classes.
121
We used a linear SVM with the trade-off parame-
ter C=1000 (also in liblinear). The parameters in SS
are set as below: the dimension of sematic space is
100, the weight of missing words is 100 and the reg-
ularization factor is 0.01. In the hierarchical classifi-
cation, we use the liblinear (Fan et al, 2008) to train
a binary classifier and SVM for a multi-class classi-
fier with the same parameters in other Systems.
4 Results and discussion
Table 2 lists the final results of our five systems on
the test samples in terms of four language pairs. The
best feature set combinations for different language
pairs are also shown. The last two rows list the re-
sults of the best and runner-up team among six par-
ticipants, which is released by the organizers.
From this table, we have some interesting find-
ings.
Firstly, the feature types BC and SD appear in all
best feature combinations. This indicates that the
length and sentence difference information are good
and effective label indicators.
Secondly, based on the comparison between Sys-
tem 1 and System 2, we find that the behavior of the
best feature sets of different language pairs on test
and development datasets is quite different. Specif-
ically, the best feature set performs better on FRA-
ENG and DEU-ENG data sets than the full feature
set. However, the full feature set performs the best
on SPA-ENG and ITA-ENG data sets. The reason
may be the different distribution properties of test
and development data sets.
Thirdly, although the only difference between
System 2 and System 4 is the size of training sam-
ples, System 4 trained on a small number of training
instances even makes a 1.6% improvement in accu-
racy over System 2 on DEU-ENG data set. This
is beyond our expectation and it indicates that the
CLTE may not be sensitive to the size of data set.
Fourthly, by adopting a majority voting scheme,
System 3 achieves the best results on two data sets
among five systems and obtains 45.8% accuracy on
FRA-ENG which is the best result among all partic-
ipants. This indicates the majority voting strategy is
a effective way to boost the performance.
Fifthly, System 5 which adopts hierarchical clas-
sification technique fails to make further improve-
ment. But it still outperforms the runner-up system
in this task on FRA-ENG and DEU-ENG. We spec-
ulate that the failure of System 5 may be caused by
the errors sensitive to hierarchical structure in hier-
archical classification.
In general, our approaches obtained very good
results on all the language pairs. On FRA-ENG
and DEU-ENG, we achieved the best results among
the 16 systems with the accuracy 45.8% and 45.3%
respectively and largely outperformed the runner-
up. The results on SPA-ENG and ITA-ENG were
also promising, achieving the second and third place
among the 16 systems.
5 Conclusion
We have proposed several effectively features con-
sisting of sentence semantic similarity and sentence
difference, which work together with other features
presented by the previous work to solve the cross-
lingual textual entailment problem. With the aid
of machine translation, we can handle the cross-
linguality. We submitted five systems on each lan-
guage pair and obtained the best result on two data
sets, i.e., FRA-ENG and DEU-ENG, and ranked the
2nd and the 3rd on other two language pairs respec-
tively. Interestingly, we find some simple feature
types like BC and SD are good class indicators and
can be easily acquired. In future work, we will in-
vestigate the discriminating power of different fea-
ture types in the CLTE task on different languages.
Acknowledgements
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improves the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No. 20090076120029)
and Shanghai Knowledge Service Platform Project
(No. ZF1213).
References
Julio Javier Castillo. 2011. A wordnet-based seman-
tic approach to textual entailment and cross-lingual
122
System SPA-ENG ITA-ENG FRA-ENG DEU-ENG
1 0.428 0.426 0.438 0.422
2 0.404 0.420 0.450 0.436
3 0.408 0.426 0.458 0.432
4 0.422 0.416 0.436 0.452
5 0.392 0.402 0.442 0.426
Best
feature set
BC+STS+SS
+GR+SD
BC+SD+SS
+GR+BS SD+BC+STS
BC+STS+SS
+BS+SD
Best 0.434 0.454 0.458 0.452
runner-up 0.428 0.432 0.426 0.414
Table 2: The accuracy results of our systems on different language pairs released by the organizer.
textual entailment. International Journal of Machine
Learning and Cybernetics, 2(3):177?189.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In Proceedings of the PASCAL Workshop
on LearningMethods for Text Understanding andMin-
ing.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. Ualacant: Using online ma-
chine translation for cross-lingual textual entailment.
In Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 472?476,
Montre?al, Canada, 7-8 June.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics.
Sanda Harabagiu and Andrew Hickl. 2006. Methods for
using textual entailment in open-domain question an-
swering. InProceedings of the 21st InternationalCon-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912, Sydney, Australia, July.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality+ ml: Learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Elena Lloret, Oscar Ferra?ndez, Rafael Munoz, and
Manuel Palomar. 2008. A text summarization ap-
proach under the influence of textual entailment. In
Proceedings of the 5th International Workshop on
Natural Language Processing and Cognitive Science
(NLPCS 2008), pages 22?31.
Tim Loughran and Bill McDonald. 2011. When is a
liability not a liability? textual analysis, dictionaries,
and 10-ks. The Journal of Finance, 66(1):35?65.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using svms and
string similarity measures. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 42?47.
Prodromos Malakasiotis. 2009. Paraphrase recognition
using machine learning to combine similarity mea-
sures. In Proceedings of the ACL-IJCNLP 2009 Stu-
dent Research Workshop, pages 27?35.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010a. Syntactic/semantic structures
for textual entailment recognition. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 1020?1028.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010b. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 321?
324, Los Angeles, California, June.
Fandong Meng, Hao Xiong, and Qun Liu. 2012. Ict:
A translation based method for cross-lingual textual
entailment. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 715?720, Montre?al, Canada, 7-8 June.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2013. Semeval-2013 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 7th InternationalWorkshop
on Semantic Evaluation (SemEval 2013).
William E Winkler et al 1999. The state of record link-
age and current research problems.
123
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 259?264,
Dublin, Ireland, August 23-24, 2014.
ECNU: Expression- and Message-level Sentiment Orientation
Classification in Twitter Using Multiple Effective Features
Jiang Zhao
?
, Man Lan
?
, Tian Tian Zhu
?
Department of Computer Science and Technology
East China Normal University
?
51121201042,51111201046@ecnu.cn;
?
mlan@cs.ecnu.edu.cn
Abstract
Microblogging websites (such as Twitter,
Facebook) are rich sources of data for
opinion mining and sentiment analysis. In
this paper, we describe our approaches
used for sentiment analysis in twitter (task
9) organized in SemEval 2014. This task
tries to determine whether the sentiment
orientations conveyed by the whole tweets
or pieces of tweets are positive, negative
or neutral. To solve this problem, we ex-
tracted several simple and basic features
considering the following aspects: surface
text, syntax, sentiment score and twitter
characteristic. Then we exploited these
features to build a classifier using SVM
algorithm. Despite the simplicity of fea-
tures, our systems rank above the average.
1 Introduction
Microblogging services such as Twitter
1
, Face-
book
2
today play an important role in expressing
opinions on a variety of topics, discussing current
issues or sharing one?s feelings about different ob-
jects in our daily life (Agarwal and Sabharwal,
2012). Therefore, Twitter (and other platforms)
has become a valuable source of users? sentiments
and opinions and with the continuous and rapid
growth of the number of tweets, analyzing the sen-
timents expressed in twitter has attracted more and
more researchers and communities, for example,
the sentiment analysis task in twitter was held in
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
2
http://facebook.com/
SemEval 2013 (Nakov et al., 2013). It will bene-
fit lots of real applications such as simultaneously
businesses, media outlets, and help investors to
discover product trends, identify customer pref-
erences and categorize users by analyzing these
tweets (Becker et al., 2013).
The task of sentiment analysis in twitter in Se-
mEval 2014 (Sara et al., 2014) aims to classify
whether a tweet?s sentiment is positive, negative or
neutral at expression level or message level. The
expression-level subtask (i.e., subtask A) is to de-
termine the sentiment of a marked instance of a
word or phrase in the context of a given message,
while the message-level subtask (i.e., subtask B)
aims to determine the sentiment of a whole mes-
sage. Previous work (Nakov et al., 2013) showed
that message-level sentiment classification is more
difficult than that of expression-level (i.e., 0.690 vs
0.889 in terms of F-measure) since a message may
be composed of inconsistent sentiments.
To date, lots of approaches have been proposed
for conventional blogging sentiment analysis and
a very broad overview is presented in (Pang and
Lee, 2008). Inspired by that, many features used
in microblogging mining are adopted from tradi-
tional blogging sentiment analysis task. For ex-
ample, n-grams at the character or word level,
part-of-speech tags, negations, sentiment lexicons
were used in most of current work (Agarwal et
al., 2011; Barbosa and Feng, 2010; Zhu et al.,
2013; Mohammad et al., 2013; K?okciyan et al.,
2013). They found that n-grams are still effective
in spite of the short length nature of microblog-
ging and the distributions of different POS tags
in tweets with different polarities are highly dif-
ferent (Pak and Paroubek, 2010). Compared with
formal blog texts, tweets often contain many in-
formal writings including slangs, emoticons, cre-
259
ative spellings, abbreviations and special marks
(i.e., mentions @ and hashtags #), and thus many
twitter-specific features are proposed to character-
ize this phenomena. For example, features record
the number of emoticons, elongated words and
hashtags were used in (Mohammad et al., 2013;
Zhu et al., 2013; K?okciyan et al., 2013). In this
work, we adopted many features from previous
work and then these features were fed to SVM to
perform classification.
The remainder of this paper is organized as fol-
lows. Section 2 describes our systems including
preprocessing, feature representations, data sets,
etc. Results of two subtasks and discussions are
reported in Section 3. Finally, we conclude this
paper in Section 4.
2 Our Systems
We extracted eight types of features and the first
six types were used in subtask A and all features
were used in subtask B. Then, several classifica-
tion algorithms were examined on the develop-
ment data set and the algorithm with the best per-
formance was chosen in our final submitted sys-
tems.
2.1 Preprocessing
In order to remedy as many informal texts as
possible, we recovered the elongated words to
their normal forms, e.g., ?goooooood? to ?good?
and collected about five thousand slangs or ab-
breviations from Internet to convert each slang
to its complete form, e.g., ?1dering? to ?won-
dering?, ?2g2b4g? to ?to good to be forgotten?.
Then these preprocessed texts were used to extract
non twitter-specific features (i.e., POS, lexicon, n-
grams, word cluster and indicator feature).
2.2 Feature Representations
2.2.1 POS Features
(Pak and Paroubek, 2010) found that POS tags
help to identify the sentiments of tweets and they
pointed out that objective tweets often contain
more nouns than subjective tweets and subjec-
tive tweets may carry more adjectives and adverbs
than objective tweets. Therefore, we used Stan-
ford POS Tagger
3
and recorded the number of
four different tags for each tweet: noun (the cor-
responding POS tags are ?NN?, ?NNP?, ?NNS?
and ?NNPS?), verb (the corresponding POS tags
3
http://nlp.stanford.edu/software/tagger.shtml
are ?VB?, ?VBD?, ?VBG?, ?VBN?, ?VBP? and
?VBZ?), adjective (the corresponding POS tags
are ?JJ?, ?JJR? and ?JJS?) and adverb (the corre-
sponding POS tags are ?RB?, ?RBR? and ?RBS?).
Then we normalized them by the length of given
instance or message.
2.2.2 Sentiment Lexicon-based Features
Sentiment lexicons are widely used to calculate
the sentiment scores of phrases or messages in pre-
vious work (Nakov et al., 2013; Mohammad et al.,
2013) and they are proved to be very helpful to
detect the sentiment orientation. Given a phrase
or message, we calculated the following six fea-
ture values: (1) the ratio of positive words to all
words, i.e., the number of positive words divided
by the number of total words; (2) the ratio of neg-
ative words to all words; (3) the ratio of objective
words to all words; (4) the ratio of positive senti-
ment score to the total score (i.e., the sum of the
positive and negative score); (5) the ratio of nega-
tive sentiment score to the total score; (6) the ratio
of positive score to negative score, if the negative
score is zero, which means this phrase or message
has a very strong positive sentiment orientation,
we set ten times of positive score as its value.
During the calculation, we also considered the
effects of negation words since they may reverse
the sentiment orientation in most cases. To do so,
we defined the negation context as a snippet of a
tweet that starts with a negation word and ends
with punctuation marks. If a non-negation word
is in a negation context and also in the sentiment
lexicon, we reverse its polarity. For example, the
word ?bad? in phrase ?not bad? originally has a
negative score of 0.625, after reversal, this phrase
has a positive score of 0.625. A manually made
list containing 29 negation words (e.g., no, hardly,
never, etc) was used in our experiment.
Four sentiment lexicons were used to decide
whether a word is subjective or objective and ob-
tain its sentiment score.
MPQA (Wilson et al., 2009). This subjectiv-
ity lexicon contains about 8000 subjective words
and each word has two types of sentiment strength:
strong subjective and weak subjective, and four
kinds of sentiment polarities: positive, negative,
both (positive and negative) and neutral. We used
this lexicon to determine whether a word is posi-
tive, negative or objective and assign a value of 0.5
or 1 if it is weak or strong subjective (i.e., positive
or negative) respectively.
260
SentiWordNet(SWN) (Baccianella et al.,
2010). This sentiment lexicon contains about
117 thousand items and each item corresponds
to a synset of WordNet. Three sentiment scores:
positivity, negativity, objectivity are provided and
the sum of these three scores is always 1, for
example, living#a#3, positivity: 0.5, negativity:
0.125, objectivity: 0.375. In experiment we used
the most common sense of a word.
NRC (Mohammad et al., 2013). Mohammad et
al. collected two sets of tweets and each tweet con-
tains the seed hashtags or emoticons and then they
labeled the sentiment orientation for each tweet
according to its hashtags or emoticons. They used
pointwise mutual information (PMI) to calculate
the sentiment score for each word and obtained
two sentiment lexicons (i.e., hashtag lexicon and
emoticon lexicon).
IMDB. We generated an unigram lexicon by
ourselves from a large movie review data set from
IMDB website (Maas et al., 2011) which con-
tains 25,000 positive and 25,000 negative movie
reviews by calculating their PMI scores.
2.2.3 Word n-Gram
Words in themselves in tweets usually carry out
the original sentiment orientation, so we con-
sider word n-grams as one feature. We removed
URLs, mentions, hashtags, stopwords from tweet
and then all words were stemmed using the nltk
4
toolkit. For subtask A, only unigram was used and
we used word frequency as feature values. For
subtask B, both unigram and bigram were used.
Besides, weighted unigram was also used where
we replaced word frequency with their sentiment
scores using the hashtag lexicon and emoticon lex-
icon in NRC.
2.2.4 Twitter-specific Features
Punctuation Generally, punctuation may express
users? sentiment in a certain extent. Therefore we
recorded the frequency of the following four types
of punctuation: exclamation (!), question (?), dou-
ble (?) and single marks (?). In addition, we also
recorded the number of contiguous sequences of
exclamation marks, question marks, and both of
themwhich appeared at the end of a phrase or mes-
sage.
Emoticon Emoticons are widely used to directly
express the sentiment of users and thus we counted
4
http://nltk.org/
the number of positive emoticons, negative emoti-
cons and the sum of positive and negative emoti-
cons. To identify the polarities of emoticons, we
collected 36 positive emoticons and 33 negative
emoticons from the Internet.
Hashtag A hashtag is a short phrase that con-
catenates more than one words together without
white spaces and users usually use hashtags to
label the subject topic of a tweet, e.g., #toobad,
#ihateschool, #NewGlee. Since a hashtag may
contain a strong sentiment orientation, we first
used the Viterbi algorithm (Berardi et al., 2011)
to split hashtags and then calculated the sentiment
scores of hashtags using the hashtag and emoticon
lexicon in NRC.
2.2.5 Word Cluster
Apart from n-gram, we presented another word
representations based on word clusters to explore
shallow semantic meanings and reduced the spar-
sity of the word space. 1000 word clusters pro-
vided by CMU pos-tagging tool
5
were used to rep-
resent tweet contents. For each tweet we recorded
the number of words from each cluster, resulting
in 1000 features.
2.2.6 Indicator Features
We observed that the polarity of a message some-
times is revealed by some special individual posi-
tive or negative words in a certain degree. How-
ever the sentiment lexicon based features where
a synthetical sentiment score of a message is cal-
culated may hide this information. Therefore, we
directly used several individual sentiment scores
as features. Specifically, we created the following
sixteen features for each message where the hash-
tag and emoticon lexicons were used to obtain sen-
timent scores: the sentiment scores of the first and
last sentiment-bearing words, the three highest and
lowest sentiment scores.
2.3 Data sets and Evaluation Metric
The organizers provide tweet ids and a script for
all participants to collect data. Table 1 shows the
statistics of the data set used in our experiments.
To examine the generalization of models trained
on tweets, the test data provided by the organiz-
ers consists of instances from different domains
for both subtasks. Specifically, five corpora are in-
cluded: LiveJournal(2014) is a collection of com-
ments from LiveJournal blogs, SMS2013 is a SMS
5
http://www.ark.cs.cmu.edu/TweetNLP/
261
data set directly from last year, Twitter2013 is a
twitter data set directly from last year, Twitter2014
is a new twitter data set and Twitter2014Sarcasm
is a collection of tweets that contain sarcasm. No-
tice that the data set SMS2013 and Twitter2013
were also used as our development set. Form Ta-
ble 1, we find that (1) the class distributions of test
data sets almost agree with training data sets for
both subtasks, (2) the percentages of class neutral
in two subtasks are significantly different (4.7%
vs 45.5%), which reflects that a sentence which is
composed of different sentiment expressions may
act neutrality, (3) Twitter2014Sarcasm data set is
very small. According to the guideline, we did not
use any development data for training in the eval-
uation period.
data set Positive Negative Neutral Total
subtask A:
train 3,609(61%) 2,023(34%) 265(5%) 5,897
dev 2,734(62%) 1,541(35%) 160(3%) 4,435
test
LiveJournal 660(50%) 511(39%) 144(11%) 1,315
SMS2013 1,071(46%) 1,104(47%) 159( 7%) 2,334
Twitter2013 2,734(62%) 1,541(35%) 160(3%) 4,435
Twitter2014 1,807(73%) 578(23%) 88( 4%) 2,473
Twitter2014S 82(66%) 37(30%) 5(4%) 124
all 6,354(59%) 3,771(35%) 556(6%) 10,681
subtask B:
train 3,069(36%) 1,313(15%) 4,089(49%) 8,471
dev 1,572(41%) 601(16%) 1,640(43%) 3,813
test
LiveJournal 427(37%) 304(27%) 411(36%) 1,142
SMS2013 492(24%) 394(19%) 1,207(57%) 2,093
Twitter2013 1,572(41%) 601(16%) 1,640(43%) 3,813
Twitter2014 982(53%) 202(11%) 669(36%) 1,853
Twitter2014S 33(38%) 40(47%) 13(15%) 86
all 3,506(39%) 1,541(17%) 3,940(44%) 8,987
Table 1: Statistics of data sets in training (train),
development (dev), test (test) set. Twitter2014S
stands for Twitter2014Sarcasm.
We used macro-averaged F-measure of positive
and negative classes (without neutral since it is
margin in training data) to evaluate the perfor-
mance of our systems and the averaged F-measure
of five corpora was used to rank the final results.
2.4 Submitted System Configurations
For each subtask, each team can submit two runs:
(1) constrained: only the provided data set can be
used for training and no additional annotated data
is allowed for training, however other resources
such as lexicons are allowed; (2) unconstrained:
any additional data can be used for training. We
explored several classification algorithms on the
development set and configured our final systems
as follows. For constrained system, we used SVM
and logistic regression algorithm implemented in
scikit-learn toolkit (Pedregosa et al., 2011) to ad-
dress two subtasks respectively and used self-
training strategy to conduct unconstrained system.
Self-training is a semi-supervised learning method
where a classifier is first trained with a small
amount of labeled data and then we repeat the fol-
lowing procedure: the most confident predictions
by the current classifier are added to training pool
and then the classifier is retrained(Zhu, 2005). The
parameters in constrained models and the growth
size k and iteration number T in self-training are
listed in Table 2 according to the results of prelim-
inary experiments.
task constrained unconstrained
subtask A SVM, kernel=rbf, c=500 k=100, T=40
subtask B LogisticRegression, c=1 k=90, T=40
Table 2: System configurations for the constrained
and unconstrained runs in two subtasks.
3 Results and Discussion
3.1 Results
We submitted four systems as described above and
their final results are shown in Table 3, as well as
the top-ranked systems released by the organizers.
From the table, we observe the following findings.
Firstly, we find that the results of message-level
polarity classification are much worse than the re-
sults of expression-level polarity disambiguation
(82.93 vs 61.22) on both constrained and uncon-
strained systems, which is consistent with the pre-
vious work (Nakov et al., 2013). The low per-
formance of message-level task may result from
two possible reasons: (1) a message may con-
tain mixed sentiments and (2) the strength of
sentiments is different. In contrast, the texts in
expression-level task are usually short and contain
a single sentiment orientation, which leads to bet-
ter performance.
Secondly, whether on constrained or uncon-
strained systems, the performance on Twit-
ter2014Sarcasm data set is much worse than the
performance on the other four data sets. This is
because that sarcasm often expresses the opposite
meaning of what it seems to say, that means the
actual sentiment orientation of a word is opposite
to its original orientation. Moreover, even for our
human it is a challenge to identify whether it is a
sarcasm or not.
Thirdly, the results on LiveJournal and SMS
are comparable to the results on Twitter2013 and
Twitter2014 in both subtasks, which indicates that
262
online comments and SMS share some common
characteristics with tweets (e.g., emoticons and
punctuation). Therefore, in case of lack of labeled
online comments or SMS data, we can use the ex-
isting tweets as training data instead.
Fourthly, our unconstrained systems exploit the
test data of year 2014 in training stage and perform
a worse result in subtask B. We speculate that the
failure of using self-training on message-level data
set is because that the performance of initial clas-
sifier was low and thus in the following iterations
more and more noisy instances were selected to
add the training pool, which eventually resulted in
a final weak classifier.
In summary, we adopted some simple and ba-
sic features to classify the polarities of expressions
and messages and they were promising. For sub-
task A, our systems rank 5th out of 19 submissions
under the constrained setting and rank 2nd out of 6
submissions under the unconstrained setting. For
subtask B, our systems rank 16th out of 42 submis-
sions under the constrained setting and rank 5th
out of 8 submissions under the unconstrained set-
ting.
3.2 Feature Combination Experiments
To explore the effectiveness of different feature
types, we conducted a series of feature combina-
tion experiments using the constrained setting as
shown in Table 2 for both subtasks. For each time
we repeated to add one feature type to current fea-
ture set and then selected the best one until all the
feature types were processed. Table 4 shows the
results of different feature combinations and the
best results are shown in bold font.
From Table 4, we find that (1) MPQA, n-gram
and Word cluster are the most effective feature
types to identify the polarities; (2) The POS tags
make margin contribution to improve the perfor-
mance since Stanford parser is designed for for-
mal texts and in the future we may use specific
parser instead; (3) The lexicon IMDB extracted
from movie reviews has negative effects to clas-
sify twitter data, which indicates that there exist
differences in the way of expressing sentiments
between these two domains; (4) Twitter-specific
features, i.e., hashtag and emoticon, are not as ef-
fective as expected. This is because they are sparse
in the data sets. In subtask Awith 16578 instances,
only 292 instances (1.76%) have hashtags and 419
instances (2.52%) have emoticons. In subtask B
with 17458 messages, more instances have hash-
tags (16.72%) and emoticons (26.70%). (5) For
subtask A MPQA, n-gram, NRC and punctuation
features achieve the best performance and for sub-
task B the best performance is achieved by using
almost all features.
In summary, we find that n-gram and some lex-
icons such as MPQA are the most effective while
twitter-specific features (i.e., hashtag and emoti-
con) are not as discriminating as expected and the
main reason for this is that they are sparse in the
data sets.
Feature Subtask A Feature Subtask B
MPQA 77.49 Word cluster 53.50
.+n-gram 80.08(2.59) .+MPQA 58.35(4.85)
.+NRC 82.42(2.34) .+W1Gram 60.22(1.87)
.+Pun. 83.83(1.41) .+Pun. 60.99(0.77)
.+POS 83.83(0) .+Indicator 61.38(0.39)
.+Emoticon 83.49(-0.34) .+SWN 61.51(0.13)
.+Hashtag 83.54(0.05) .+Hashtag 61.54(0.03)
.+IMDB 83.51(-0.03) .+n-gram 61.56(0.02)
.+SWN 82.92(-0.59) .+Emoticon 61.69(0.13)
- - .+POS 61.71(0.02)
- - .+IMDB 61.11(-0.6)
- - .+NRC 61.23(0.12)
Table 4: The results of feature combination exper-
iments. The numbers in the brackets are the per-
formance increments compared with the previous
results. ?.+? means to add current feature to the
previous feature set.
4 Conclusion
In this paper we used several basic feature types to
identify the sentiment polarity at expression level
or message level and these feature types include
n-gram, sentiment lexicon and twitter-specific fea-
tures, etc. Although they are simple, our systems
are still promising and rank above average (e.g.,
rank 5th out of 19 and 16th out of 42 in subtask A
and B respectively under the constrained setting).
For the future work, we would like to analyze the
distributions of different sentiments in sentences.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Apoorv Agarwal and Jasneet Sabharwal. 2012. End-
to-end sentiment analysis of twitter data. In Pro-
263
Systems LiveJournal SMS2013 Twitter2013 Twitter2014 Twitter2014S Average
A-constrained (expression-level) 81.67 89.31 87.28 82.67 73.71 82.93
A-unconstrained 81.69 89.26 87.29 82.93 73.71 82.98
NRC-Canada-A-constrained
?
85.49 88.03 90.14 86.63 77.13 85.48
Think Positive-A-unconstrained
?
80.90 87.65 88.06 82.05 76.74 83.08
B-constrained(message-level) 69.44 59.75 62.31 63.17 51.43 61.22
B-unconstrained 64.08 56.73 63.72 63.04 49.33 59.38
NRC-Canada-B-constrained
?
74.84 70.28 70.75 69.85 58.16 68.78
Think Positive-B-unconstrained
?
66.96 63.20 68.15 67.04 47.85 62.64
Table 3: Performance of our systems and the top-ranked systems (marked with asterisk).
ceedings of the Workshop on Information Extraction
and Entity Analytics on Social Media Data, pages
39?44, Mumbai, India, December. The COLING
2012 Organizing Committee.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ?11, pages
30?38. Association for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 36?44. Association for Computational Lin-
guistics.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. Avaya: Sentiment analysis on
twitter with self-training and polarity lexicon expan-
sion. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 333?340. Association for Computational Lin-
guistics, June.
Giacomo Berardi, Andrea Esuli, Diego Marcheggiani,
and Fabrizio Sebastiani. 2011. Isti@ trec microblog
track 2011: Exploring the use of hashtag segmenta-
tion and text quality ranking. In TREC.
Nadin K?okciyan, Arda C?elebi, Arzucan
?
Ozg?ur, and
Suzan
?
Usk?udarli. 2013. Bounce: Sentiment classifi-
cation in twitter using rich feature sets. In Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 554?561.
Association for Computational Linguistics, June.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142?150. As-
sociation for Computational Linguistics.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 321?327. Asso-
ciation for Computational Linguistics, June.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 312?320. Association for Computational Lin-
guistics, June.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Rosenthal Sara, Ritter Alan, Veselin Stoyanov, and
Nakov Preslav. 2014. Semeval-2014 task 9: Sen-
timent analysis in twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14). Association for Computational
Linguistics, August.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, pages 399?433.
Tian Tian Zhu, Fang Xi Zhang, and Man Lan. 2013.
Ecnucs: A surface information based system de-
scription of sentiment analysis in twitter in the
semeval-2013 (task 2). In Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), page 408.
Xiaojin Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
264
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271?277,
Dublin, Ireland, August 23-24, 2014.
ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for
Semantic Relatedness and Textual Entailment
Jiang Zhao, Tian Tian Zhu, Man Lan
?
Department of Computer Science and Technology
East China Normal University
51121201042,51111201046@ecnu.cn; mlan@cs.ecnu.edu.cn
?
Abstract
This paper presents our approach to se-
mantic relatedness and textual entailment
subtasks organized as task 1 in SemEval
2014. Specifically, we address two ques-
tions: (1) Can we solve these two sub-
tasks together? (2) Are features proposed
for textual entailment task still effective
for semantic relatedness task? To address
them, we extracted seven types of features
including text difference measures pro-
posed in entailment judgement subtask, as
well as common text similarity measures
used in both subtasks. Then we exploited
the same feature set to solve the both sub-
tasks by considering them as a regression
and a classification task respectively and
performed a study of influence of differ-
ent features. We achieved the first and the
second rank for relatedness and entailment
task respectively.
1 Introduction
Distributional Semantic Models (DSMs)(surveyed
in (Turney et al., 2010)) exploit the co-occurrences
of other words with the word being modeled to
compute the semantic meaning of the word un-
der the distributional hypothesis: ?similar words
share similar contexts? (Harris, 1954). Despite
their success, DSMs are severely limited to model
the semantic of long phrases or sentences since
they ignore grammatical structures and logical
words. Compositional Distributional Semantic
Models (CDSMs)(Zanzotto et al., 2010; Socher et
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
al., 2012) extend DSMs to sentence level to cap-
ture the compositionality in the semantic vector
space, which has seen a rapidly growing interest
in recent years. Although several CDSMs have
been proposed, benchmarks are lagging behind.
Previous work (Grefenstette and Sadrzadeh, 2011;
Socher et al., 2012) performed experiments on
their own datasets or on the same datasets which
are limited to a few hundred instances of very short
sentences with a fixed structure.
To provide a benchmark so as to compare dif-
ferent CDSMs, the sentences involving composi-
tional knowledge task in SemEval 2014 (Marelli et
al., 2014) develops a large dataset which is full of
lexical, syntactic and semantic phenomena. It con-
sists of two subtasks: semantic relatedness task,
which measures the degree of semantic relatedness
of a sentence pair by assigning a relatedness score
ranging from 1 (completely unrelated) to 5 (very
related); and textual entailment (TE) task, which
determines whether one of the following three re-
lationships holds between two given sentences A
and B: (1) entailment: the meaning of B can be
inferred from A; (2) contradiction: A contradicts
B; (3) neutral: the truth of B cannot be inferred on
the basis of A.
Semantic textual similarity (STS) (Lintean and
Rus, 2012) and semantic relatedness are closely
related and interchangeably used in many liter-
atures except that the concept of semantic simi-
larity is more specific than semantic relatedness
and the latter includes concepts as antonymy and
meronymy. In this paper we regard the semantic
relatedness task as a STS task. Besides, regardless
of the original intention of this task, we adopted
the mainstream machine learning methods instead
of CDSMs to solve these two tasks by extracting
heterogenous features.
271
Like semantic relatedness, TE task (surveyed
in (Androutsopoulos and Malakasiotis, 2009)) is
also closely related to STS task since in TE task
lots of similarity measures at different levels are
exploited to boost classification. For example,
(Malakasiotis and Androutsopoulos, 2007) used
ten string similarity measures such as cosine sim-
ilarity at the word and the character level. There-
fore, the first fundamental question arises, i.e.,
?Can we solve both of these two tasks together??
At the same time, since high similarity does not
mean entailment holds, the TE task also utilizes
other features besides similarity measures. For ex-
ample, in our previous work (Zhao et al., 2014)
text difference features were proposed and proved
to be effective. Therefore, the second question sur-
faces here, i.e., ?Are features proposed for TE task
still effective for STS task?? To answer the first
question, we extracted seven types of features in-
cluding text similarity and text difference and then
fed them to classifiers and regressors to solve TE
and STS task respectively. Regarding the second
question, we conducted a series of experiments
to study the performance of different features for
these two tasks.
The rest of the paper is organized as follows.
Section 2 briefly describes the related work on
STS and TE tasks. Section 3 presents our systems
including features, learning methods, etc. Section
4 shows the experimental results on training data
and Section 5 reports the results of our submitted
systems on test data and gives a detailed analysis.
Finally, Section 6 concludes this paper with future
work.
2 Related Work
Existing work on STS can be divided into 4
categories according to the similarity measures
used (Gomaa and Fahmy, 2013): (1) string-based
method (B?ar et al., 2012; Malakasiotis and An-
droutsopoulos, 2007) which calculates similarities
using surface strings at either character level or
word level; (2) corpus-based method (Li et al.,
2006) which measures word or sentence similar-
ities using the information gained from large cor-
pora, including Latent Semantic Analysis (LSA),
pointwise mutual information (PMI), etc. (3)
knowledge-based method (Mihalcea et al., 2006)
which estimates similarities with the aid of ex-
ternal resources, such as WordNet
1
; (4) hybrid
1
http://wordnet.princeton.edu/
method (Zhu and Lan, 2013; Croce et al., 2013)
which integrates multiple similarity measures and
adopts supervised machine learning algorithms to
learn the different contributions of different fea-
tures.
The approaches to the task of TE can be roughly
divided into two groups: (1) logic inference
method (Bos and Markert, 2005) where automatic
reasoning tools are used to check the logical repre-
sentations derived from sentences and (2) machine
learning method (Zhao et al., 2013; Gomaa and
Fahmy, 2013) where a supervised model is built
using a variety of similarity scores.
Unlike previous work which separately ad-
dressed these two closely related tasks by using
simple feature types, in this paper we endeavor to
simultaneously solve these two tasks by using het-
erogenous features.
3 Our Systems
We consider the two tasks as one by exploiting the
same set of features but using different learning
methods, i.e., classification and regression. Seven
types of features are extracted and most of them
are based on our previous work on TE (Zhao et
al., 2014) and STS (Zhu and Lan, 2013). Many
learning algorithms and parameters are examined
and the final submitted systems are configured ac-
cording to the preliminary results on training data.
3.1 Preprocessing
Three text preprocessing operations were per-
formed before we extracted features, which in-
cluded: (1) we converted the contractions to their
formal writings, for example, doesn?t is rewrit-
ten as does not. (2) the WordNet-based Lemma-
tizer implemented in Natural Language Toolkit
2
was used to lemmatize all words to their nearest
base forms in WordNet, for example, was is lem-
matized to be. (3) we replaced a word from one
sentence with another word from the other sen-
tence if the two words share the same meaning,
where WordNet was used to look up synonyms.
No word sense disambiguation was performed and
all synsets for a particular lemma were considered.
3.2 Feature Representations
3.2.1 Length Features (len)
Given two sentences A and B, this feature type
records the length information using the follow-
2
http://nltk.org/
272
ing eight measure functions:
|A|, |B|, |A?B|, |B ?A|, |A ?B|, |A ?B|,
(|A|?|B|)
|B|
,
(|B|?|A|)
|A|
where |A| stands for the number of non-repeated
words in sentence A , |A?B| means the number of
unmatched words found in A but not in B , |A ?B|
stands for the set size of non-repeated words found
in either A or B and |A ? B| means the set size of
shared words found in both A and B .
Moreover, in consideration of different types of
words make different contributions to text similar-
ity, we also recorded the number of words in set
A?B and B ?A whose POS tags are noun, verb,
adjective and adverb respectively. We used Stan-
ford POS Tagger
3
for POS tagging. Finally, we
collected a total of sixteen features.
3.2.2 Surface Text Similarity (st)
As shown in Table 1, we adopted six commonly
used functions to calculate the similarity between
sentence A and B based on their surface forms,
where
??
x and
??
y are vectorial representations of
sentences A and B in tf ? idf schema.
Measure Definition
Jaccard S
jacc
= |A ? B|/|A ? B|
Dice S
dice
= 2 ? |A ? B|/(|A|+ |B|)
Overlap S
over
= |A ? B|/|A| and |A ? B|/|B|
Cosine S
cos
=
??
x ?
??
y /(?
??
x ? ? ?
??
y ?)
Manhattan M(
??
x ,
??
y ) =
n
?
i=1
|x
i
? y
i
|
Euclidean E(
??
x ,
??
y ) =
?
n
?
i=1
(x
i
? y
i
)
2
Table 1: Surface text similarity measures and their
definitions used in our experiments.
We also used three statistical correlation coef-
ficients (i.e., Pearson, Spearmanr, Kendalltau) to
measure similarity by regarding the vectorial rep-
resentations as different variables. Thus we got ten
features at last.
3.2.3 Semantic Similarity (ss)
The above surface text similarity features only
consider the surface words rather than their ac-
tual meanings in sentences. In order to build the
semantic representations of sentences, we used a
latent model to capture the contextual meanings
of words. Specifically, we adopted the weighted
textual matrix factorization (WTMF) (Guo and
Diab, 2012) to model the semantics of sentences
due to its reported good ability to model short
texts. This model first factorizes the original term-
sentence matrix X into two matrices such that
3
http://nlp.stanford.edu/software/tagger.shtml
X
i,j
? P
T
?,i
.Q
?,j
, where P
?,i
is a latent seman-
tic vector profile for word w
i
and Q
?,j
is a vector
profile that represents the sentence s
j
. Then we
employed the new representations of sentences,
i.e., Q, to calculate the semantic similarity be-
tween sentences using Cosine, Manhattan, Eu-
clidean, Pearson, Spearmanr, Kendalltau measures
respectively, which results in six features.
3.2.4 Grammatical Relationship (gr)
The grammatical relationship feature measures
the semantic similarity between two sentences
at the grammar level and this feature type was
also explored in our previous work (Zhao et al.,
2013; Zhu and Lan, 2013). We used Stanford
Parser
4
to acquire the dependency information
from sentences and the grammatical information
are represented in the form of relation unit, e.g.
nsubj(example, this), where nsubj stands for a de-
pendency relationship between example and this.
We obtained a sequence of relation units for each
sentence and then used them to estimate similarity
by adopting eight measure functions described in
Section 3.2.1, resulting in eight features.
3.2.5 Text Difference Measures (td)
There are two types of text difference measures.
The first feature type is specially designed for
the contradiction entailment relationship, which
is based on the following observation: there ex-
ist antonyms between two sentences or the nega-
tion status is not consistent (i.e., one sentence has
a negation word while the other does not have) if
contradiction holds. Therefore we examined each
sentence pair and set this feature as 1 if at least one
of these conditions is met, otherwise -1. WordNet
was used to look up antonyms and a negation list
with 28 words was used.
The second feature type is extracted from two
word sets A?B and B?A as follows: we first cal-
culated the similarities between every word from
A ? B and every word from B ? A , then took the
maximum, minimum and average value of them as
features. In our experiments, four WordNet-based
similarity measures (i.e., path, lch, wup, jcn (Go-
maa and Fahmy, 2013)) were used to calculate the
similarity between two words.
Totally, we got 13 text difference features.
4
http://nlp.stanford.edu/software/lex-parser.shtml
273
3.2.6 String Features (str)
This set of features is taken from our previous
work (Zhu and Lan, 2013) due to its superior per-
formance.
Longest common sequence (LCS) We computed
the LCS similarity on the original and lemmatized
sentences. It was calculated by finding the maxi-
mum length of a common contiguous subsequence
of two strings and then dividing it by the smaller
length of two strings to eliminate the impacts of
length imbalance.
Jaccard similarity using n-grams We obtained
n-grams at three different levels, i.e., the origi-
nal word level, the lemmatized word level and the
character level. Then these n-grams were used for
calculating Jaccard similarity defined in Table 1.
In our experiments, n = {1, 2, 3} were used for
the word level and n = {2, 3, 4} were used for the
character level.
Weighted word overlap (WWO) Since not all
words are equally important, the traditional Over-
lap similarity may not be always reasonable. Thus
we used the information content of word w to es-
timate the importance of word w as follows:
ic(w) = ln
?
w
?
?C
freq(w
?
)
freq(w)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the
corpus. To compute ic(w), we used the Web 1T
5-gram Corpus
5
. Then the WWO similarity of
two sentence s
1
and s
2
was calculated as follows:
Sim
wwo
(s
1
, s
2
) =
?
w?s
1
?s
2
ic(w)
?
w
?
?s
2
ic(w
?
)
Due to its asymmetry, we used the harmonic mean
of Sim
wwo
(s
1
, s
2
) and Sim
wwo
(s
2
, s
1
) as the fi-
nal WWO similarity. The WWO similarity is cal-
culated on the original and lemmatized strings re-
spectively.
Finally, we got two LCS features, nine Jaccard
n-gram features and two WWO features.
3.2.7 Corpus-based Features (cps)
Two types of corpus-based feature are also bor-
rowed from our previous work (Zhu and Lan,
2013), i.e., vector space sentence similarity and
co-occurrence retrieval model (CRM), which re-
sults in six features.
5
https://catalog.ldc.upenn.edu/LDC2006T13
Co-occurrence retrieval model (CRM) The
CRM word similarity is calculated as follows:
Sim
CRM
(w
1
, w
2
) =
2 ? |c(w
1
) ? c(w
2
)|
|c(w
1
)|+ |c(w
2
)|
where c(w) is the set of words that co-occur with
word w. We used the 5-gram part of the Web 1T
5-gram Corpus to obtain c(w). We only consid-
ered the word w with |c(w)| > T and then took
the top 200 co-occurring words ranked by the co-
occurrence frequency as its c(w). In our experi-
ment, we set T = {50, 200}. To propagate the
similarity from words to sentences, we adopted
the best alignment strategy used in (Banea et al.,
2012) to align two sentences.
Vector space sentence similarity This feature set
is taken from (
?
Sari?c et al., 2012), which is based
on distributional vectors of words. First we per-
formed latent semantic analysis (LSA) over two
corpora, i.e., the New York Times Annotated Cor-
pus (NYT) (Sandhaus, 2008) andWikipedia, to es-
timate the distributions of words. Then we used
two strategies to convert the distributional mean-
ings of words to sentence level: (i) simply sum-
ming up the distributional vector of each word w
in the sentence, (ii) using the information content
ic(w) to weigh the LSA vector of each wordw and
summing them up. Then we used cosine similarity
to measure the similarity of two sentences.
3.3 Learning Algorithms
We explored several classification algorithms to
classify entailment relationships and regression
algorithms to predict similarity scores using the
above 72 features after performing max-min stan-
dardization procedure by scaling them to [-1,1].
Five supervised learning methods were explored:
Support Vector Machine (SVM) which makes the
decisions according to the hyperplanes, Random
Forest (RF) which constructs a multitude of de-
cision trees at training time and selects the mode
of the classes output by individual trees, Gradient
Boosting (GB) that produces a prediction model
in the form of an ensemble of weak prediction
models, k-nearest neighbors (kNN) that decides
the class labels with the aid of the classes of k
nearest neighbors, and Stochastic Gradient De-
scent (SGD) which uses SGD technique to min-
imize loss functions. These supervised learning
methods are implemented in scikit-learn toolkit
(Pedregosa et al., 2011). Besides, we also used
a semi-supervised learning strategy for both tasks
274
in order to make full use of unlabeled test data.
Specifically, the co-training algorithm was used to
address TE task according to (Zhao et al., 2014).
Its strategy is to train two classifiers with two data
views and to add the top confident predicted in-
stances by one classifier to expand the training set
of another classifier and then to re-train the two
classifiers on the expanded training sets. For STS
task, we utilized CoReg algorithm (Zhou and Li,
2005) which uses two kNN regressors to perform
co-training paradigm.
3.4 Evaluation Measures
In order to evaluate the performance of differ-
ent algorithms, we adopted the official evaluation
measures, i.e., Pearson correlation coefficient for
STS task and accuracy for TE task.
4 Experiments on Training Data
To make a reasonable comparison between differ-
ent algorithms, we performed 5-fold cross valida-
tion on training data with 5000 sentence pairs. The
parameters tuned in different algorithms are listed
below: the trade-off parameter c in SVM, the num-
ber of trees n in RF, the number of boosting stages
n in GB, the number of nearest neighbors k in kNN
and the number of passes over the training data n
in SGD. The rest parameters are set to be default.
Algorithm
STS task TE task
Pearson para. Accuracy para.
SVM .807?.058 c=10 83.46?2.09 c=100
RF .805?.052 n=40 83.16?2.64 n=30
GB .806?.055 n=210 83.22?2.48 n=140
kNN .797?.062 k=25 82.54?2.45 k=17
SGD .765?.064 n=29 78.88?1.99 n=15
Table 2: The 5-fold cross validation results on
training data with mean and standard deviation for
each algorithm.
Table 2 reports the experimental results of 5-
fold cross validation with mean and standard devi-
ation and the optimal parameters on training data.
The results of semi-supervised learning methods
are not listed because only a few parameters are
tried due to the limit of time. From this table we
see that SVM, RF and GB perform comparable re-
sults to each other.
5 Results on Test Data
5.1 Submitted System Configurations
According to the above preliminary experimental
results, we configured five final systems for each
task. Table 3 presents the classification and regres-
sion algorithms with their parameters used in the
five systems for each task.
System STS task TE task
1 SVR, c=10 SVC, c=100
2 GB, n=210 GB, n=140
3 RF, n=40 RF, n=30
4 CoReg, k=13 co-training, k=40
5 majority voting majority voting
Table 3: Five system configurations for test data
for two tasks.
Among them, System 1 acts as our primary
and baseline system that employs SVM algorithm
and as comparison System 2 and System 3 exploit
GB and RF algorithm respectively. Unlike super-
vised settings in the aforementioned systems, Sys-
tem 4 employs a semi-supervised learning strategy
to make use of unlabeled test data. For CoReg,
the number of iteration and the number of near-
est neighbors are set as 100 and 13 respectively,
and for each iteration in co-training, the number
of confident predictions is set as 40. To further
improve performance, System 5 combines the re-
sults of 5 different algorithms (i.e. MaxEnt, SVM,
kNN, GB, RF) through majority voting. We used
the averaged values of the outputs from different
regressors as final similarity scores for semantic
similarity measurement task and chose the major
class label for entailment judgement task.
5.2 Results and Discussion
Table 4 lists the final results officially released by
the organizers in terms of Pearson and accuracy.
The best performance among these five systems is
shown in bold font. All participants can submit a
maximum of five runs for each task and only one
primary system is involved in official ranking. The
lower part of Table 4 presents the top 3 results and
the results with ? are achieved by our systems.
System STS task TE task(%)
1 0.8279 83.641
2 0.8389 84.128
3 0.8414 83.945
4 0.8210 81.165
5 0.8349 83.986
rank 1st 0.8279* 84.575
rank 2nd 0.8272 83.641*
rank 3rd 0.8268 83.053
Table 4: The results of our five systems for two
tasks and the officially top-ranked systems.
From this table, we found that (1) System 3 (us-
275
ing GB algorithm) and System 2 (using RF algo-
rithm) achieve the best performance among three
supervised systems in STS and TE task respec-
tively. However, there is no significant difference
among these systems. (2) Surprisingly, the semi-
supervised system (i.e., System 4) that employs
the co-training strategy to make use of test data
performs the worst, which is beyond our expecta-
tion. Based on our further observation in TE task,
the possible reason is that a lot of misclassified ex-
amples are added into the training pool in the ini-
tial iteration, which results in worse models built
in the subsequent iterations. And we speculate that
the weak learner kNN employed in CoReg may
lead to poor performance as well. (3) The major-
ity voting strategy fails to boost the performance
since GB and RF algorithm obtain the best perfor-
mance among these algorithms. (4) Our systems
obtain very good results on both STS and TE task,
i.e., we rank 1st out of 17 participants in STS task
and rank 2nd out of 18 participants in TE task ac-
cording to the results of primary systems and as
shown in Table 4 our primary system (i.e., System
1) do not achieve the best performance.
In a nutshell, our systems rank first and second
in STS and TE task respectively. Therefore the
answer to the first question raised in Section 1 is
yes. For two tasks, i.e., STS and TE, which are
very closely related but slightly different, we can
use the same features to solve them together.
5.3 Feature Combination Experiments
To answer the second question and explore the in-
fluences of different feature types, we performed
a series of experiments under the best system set-
ting. Table 5 shows the results of different feature
combinations where for each time we selected and
added one best feature type. From this table, we
find that for STS the most effective feature is cps
and for TE task is td. Almost all feature types have
positive effects on performance. Specifically, td
alone achieves 81.063% in TE task which is quite
close to the best performance (84.128%) and cps
alone achieves 0.7544 in STS task. Moreover, the
td feature proposed for TE task is quite effective
in STS task as well, which suggests that text se-
mantic difference measures are also crucial when
measuring sentence similarity.
Therefore the answer to the second question is
yes. It is clear that the features proposed for TE are
also effective for STS and heterogenous features
yield better performance than a single feature type.
len st ss gr td str cps result
+ 0.7544 (STS)
+ + 0.8057(+5.13)
+ + + 0.8280(+2.23)
+ + + + 0.8365(+0.85)
+ + + + + 0.8426(+0.61)
+ + + + + + 0.8432(+0.06)
+ + + + + + + 0.8429(-0.03)
+ 81.063 (TE)
+ + 82.484(+1.421)
+ + + 82.992(+0.508)
+ + + + 83.844(+0.852)
+ + + + + 83.925(+0.081)
+ + + + + + 84.067(+0.142)
+ + + + + + + 84.128(+0.061)
Table 5: Results of feature combinations, the num-
bers in the brackets are the performance incre-
ments compared with the previous results.
6 Conclusion
We set up five state-of-the-art systems and each
system employs different classifiers or regressors
using the same feature set. Our submitted systems
rank the 1st out of 17 teams in STS task with the
best performance of 0.8414 in terms of Pearson
coefficient and rank the 2nd out of 18 teams in
TE task with 84.128% in terms of accuracy. This
result indicates that (1) we can use the same fea-
ture set to solve these two tasks together, (2) the
features proposed for TE task are also effective
for STS task and (3) heterogenous features out-
perform a single feature. For future work, we may
explore the underlying relationships between these
two tasks to boost their performance by each other.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entail-
ment methods. arXiv preprint arXiv:0912.3747.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt:a supervised synergistic
approach to semantictext similarity. In First Joint
Conference on Lexical and Computational Seman-
tics (*SEM.
276
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the First
Joint Conference on Lexical and Computational Se-
mantics, pages 435?440. Association for Computa-
tional Linguistics.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 628?635. Association for Compu-
tational Linguistics.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. Unitor-core typed: Combining text similarity
and semantic filters through sv regression. In Pro-
ceedings of the 2nd Joint Conference on Lexical and
Computational Semantics, page 59.
Wael H Gomaa and Aly A Fahmy. 2013. A survey of
text similarity approaches. International Journal of
Computer Applications, 68(13):13?18.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics.
Zellig S Harris. 1954. Distributional structure. The
Philosophy of Linguistics,.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O?shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138?1150.
Mihai C. Lintean and Vasile Rus. 2012. Measuring se-
mantic similarity in short texts through greedy pair-
ing and word semantics. In FLAIRS Conference.
AAAI Press.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using svms and
string similarity measures. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 42?47. Association for Com-
putational Linguistics.
M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,
S. Menini, and R. Zamparelli. 2014. Semeval-2014
task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775?780.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Evan Sandhaus. 2008. The new york times annotated
corpus ldc2008t19. Philadelphia: Linguistic Data
Consortium.
Socher, Richard, Huval Brody, Manning Christopher,
and Ng Andrew. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, Jeju Island, Korea.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 441?448, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
Jiang Zhao, Man Lan, and Zheng-Yu Niu. 2013. Ec-
nucs: Recognizing cross-lingual textual entailment
using multiple text similarity and text difference
measures. In Proceedings of the Seventh Interna-
tional Workshop on Semantic Evaluation (SemEval
2013), pages 118?123, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Jiang Zhao, Man Lan, Zheng-Yu Niu, and Donghong
Ji. 2014. Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In The 2014 International Joint Conference
on Neural Networks (IJCNN2014). IEEE.
Zhi-Hua Zhou and Ming Li. 2005. Semi-supervised
regression with co-training. In IJCAI, pages 908?
916.
Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measur-
ing short text semantic equivalence using multiple
similarity measurements. In Proceedings of the 2nd
Joint Conference on Lexical and Computational Se-
mantics, page 124.
277
