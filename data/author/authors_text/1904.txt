Coling 2008: Companion volume ? Posters and Demonstrations, pages 91?94
Manchester, August 2008
Rank Distance as a Stylistic Similarity
Marius Popescu
University of Bucharest
Department of Computer Science
Academiei 14, Bucharest, Romania
mpopescu@phobos.cs.unibuc.ro
Liviu P. Dinu
University of Bucharest
Department of Computer Science
Academiei 14, Bucharest, Romania
ldinu@funinf.cs.unibuc.ro
Abstract
In this paper we propose a new distance
function (rank distance) designed to reflect
stylistic similarity between texts. To assess
the ability of this distance measure to cap-
ture stylistic similarity between texts, we
tested it in two different machine learning
settings: clustering and binary classifica-
tion.
1 Introduction
Computational stylistics investigates texts from the
standpoint of individual style (author identifica-
tion) or functional style (genres, registers). Be-
cause in all computational stylistic studies / ap-
proaches, a process of comparison of two or more
texts is involved, in a way or another, there was
always a need for a distance function to measure
similarity (more precisely dissimilarity) of texts
from the stylistic point of view. Such distance
measures were proposed and used for example in
authorship identification (Labb?e and Labb?e, 2001;
Burrows, 2002) or clustering texts by genre (Luy-
ckx et al, 2006).
In this paper we propose a new distance mea-
sure designed to reflect stylistic similarity between
texts. As style markers we used the function word
frequencies. Function words are generally con-
sidered good indicators of style because their use
is very unlikely to be under the conscious con-
trol of the author and because of their psycholog-
ical and cognitive role (Chung and Pennebaker,
2007). Also function words prove to be very effec-
tive in many author attribution studies. The nov-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
elty of our approach resides in the way we use
information given by the function word frequen-
cies. Given a fixed set of function words (usually
the most frequent ones), a ranking of these func-
tion words according to their frequencies is built
for each text; the obtained ranked lists are subse-
quently used to compute the distance between two
texts. To calculate the distance between two rank-
ings we used Rank distance (Dinu, 2003), an ordi-
nal distance tightly related to the so-called Spear-
man?s footrule (Diaconis and Graham, 1977).
Usage of the ranking of function words in the
calculation of the distance instead of the actual val-
ues of the frequencies may seem as a loss of infor-
mation, but we consider that the process of rank-
ing makes the distance measure more robust acting
as a filter, eliminating the noise contained in the
values of the frequencies. The fact that a specific
function word has the rank 2 (is the second most
frequent word) in one text and has the rank 4 (is
the fourth most frequent word) in another text can
be more relevant than the fact that the respective
word appears 349 times in the first text and only
299 times in the second.
To assess the ability of this distance function to
capture stylistic similarity between texts, we tested
it in two different machine learning settings: clus-
tering and binary classification.
Compared with other machine learning and sta-
tistical approaches, clustering was relatively rarely
used in stylistic investigations. However, few re-
searchers (Labb?e and Labb?e, 2001; Luyckx et al,
2006) have recently proved that clustering can be
a useful tool in computational stylistic studies.
Apart of this, clustering is a very good test bed for
a distance measure behavior. We plugged our dis-
tance function into a standard hierarchical cluster-
ing algorithm and test it on a collection of 21 nine-
91
teenth century English books (Koppel et al, 2007).
The results are very encouraging. The family trees
produced grouped together texts according to their
author, genre, even gender.
Also a distance measure can be used to solve
classification problems if it is coupled with proper
learning algorithm. One of the simplest such algo-
rithms is nearest neighbor classification algorithm.
We chose nearest neighbor algorithm because its
performance is entirely based on the appropriate-
ness to the data of the distance function on which it
relies. In this way the accuracy of the classification
will reflect the adequacy of the distance measure to
data and domain on which the method was applied.
We used the new distance function in conjunction
with nearest neighbor classification algorithm and
tested it on the well known case of authorship of
disputed Federalist papers. The method attributed
all disputed papers to Madison, the result being
consistent with that of Mosteller and Wallace.
To check if the usage of ranks of function words
is better suited for capturing stylistic differences
than the usage of actual frequencies of the function
words, we repeated the above experiments on clus-
tering and binary classification with the standard
euclidean distance between the vectors of frequen-
cies of the same function words that were used in
computing the rank distance. The comparison is in
favor of rank distance.
2 Rank Distance and Its Use as a Stylistic
Distance Between Texts
Rank distance (Dinu, 2003) is an ordinal met-
ric able to compare different rankings of a set of
objects. It is tightly related to the Spearman?s
footrule (Diaconis and Graham, 1977), and it had
already been successfully used in computational
linguistics, in such problems as the similarity of
Romance languages (Dinu and Dinu, 2005).
A ranking of a set of n objects can be repre-
sented as a permutation of the integers 1, 2, . . . , n,
? ? S
n
. ?(i) will represent the place (rank) of the
object i in the ranking. The Rank distance in this
case is simply the distance induced by L
1
norm:
D(?
1
, ?
2
) =
n
?
i=1
|?
1
(i)? ?
2
(i)| (1)
This is a distance between what is called full rank-
ings. However, in real situations, the problem of
tying arises, when two or more objects claim the
same rank (are ranked equally). For example, two
a been had its one that was
all but has may only the were
also by have more or their what
an can her must our then when
and do his my shall there which
any down if no should things who
are even in not so this will
as every into now some to with
at for is of such up would
be from it on than upon your
Table 1: Function words used in computing the
distance
or more function words can have the same fre-
quency in a text and any ordering of them would
be arbitrary.
The Rank distance allocates to tied objects a
number which is the average of the ranks the tied
objects share. For instance, if two objects claim the
rank 2, then they will share the ranks 2 and 3 and
both will receive the rank number (2+3)/2 = 2.5.
In general, if k objects will claim the same rank
and the first x ranks are already used by other ob-
jects, then they will share the ranks x + 1, x +
2, . . . , x + k and all of them will receive as rank
the number:
(x+1)+(x+2)+...+(x+k)
k
= x+
k+1
2
. In
this case, a ranking will be no longer a permutation
(?(i) can be a non integer value), but the formula
(1) will remain a distance (Dinu, 2003).
Rank distance can be used as a stylistic distance
between texts in the following way:
First a set of function word must be fixed. The
most frequent function words may be selected or
other criteria may be used for selection. In all our
experiments we used the set of 70 function words
identified by Mosteller and Wallace (Mosteller and
Wallace, 1964) as good candidates for author-
attribution studies. The set is given in Table 1.
Once the set of function words is established,
for each text a ranking of these function words is
computed. The ranking is done according to the
function word frequencies in the text. Rank 1 will
be assigned to the most frequent function word,
rank 2 will be assigned to the second most frequent
function word, and so on. The ties are resolved as
we discussed above. If some function words from
the set don?t appear in the text, they will share the
last places (ranks) of the ranking.
The distance between two texts will be the Rank
distance between the two rankings of the function
words corresponding to the respective texts.
3 Clustering Experiments
One good way to test the virtues of a distance mea-
sure is to use it as a base for a hierarchical cluster-
92
Group Author Book
American Novelists Hawthorne Dr. Grimshawe?s Secret
House of Seven Gables
Melville Redburn
Moby Dick
Cooper The Last of the Mohicans
The Spy
Water Witch
American Essayists Thoreau Walden
A Week on Concord
Emerson Conduct Of Life
English Traits
British Playwrights Shaw Pygmalion
Misalliance
Getting Married
Wilde An Ideal Husband
Woman of No Importance
Bronte Sisters Anne Agnes Grey
Tenant Of Wildfell Hall
Charlotte The Professor
Jane Eyre
Emily Wuthering Heights
Table 2: The list of books used in the experiment
ing algorithm. The family trees (dendogram) thus
obtained can reveal a lot about the distance mea-
sure behavior.
In our experiments we used an agglomerative hi-
erarchical clustering algorithm (Duda et al, 2001)
with average linkage.
In the first experiment we cluster a collection
of 21 nineteenth century English books written by
10 different authors and spanning a variety of gen-
res (Table 2). The books were used by Koppel et
al. (Koppel et al, 2007) in their authorship verifi-
cation experiments.
The resulted dendogram is shown in Figure 1.
As can be seen, the family tree produced is a very
good one, accurately reflecting the stylistic rela-
tions between books. The books were grouped
in three big clusters (the first three branches of
the tree) corresponding to the three genre: dramas
(lower branch), essays (middle branch) and novels
(upper branch). Inside each branch the works were
first clustered according to their author. The only
exceptions are the two essays of Emerson which
instead of being first cluster together and after that
merged in the cluster of essays, they were added
one by one to this cluster. Apart of this, the family
tree is perfect. Even more, in the cluster of novels
one may distinguished two branches clearly sepa-
rated that can correspond to the gender or national-
ity of the authors: female English (lower part) and
male American (upper part).
For comparison, the dendogram in Figure 2
show the same books clustered with the same al-
gorithm, but using the standard euclidean distance
instead of the rank distance as measure of stylis-
tic similarity. The same set of function words as
in the case of rank distance was used. This time
though, each text was represented as a vector of
Figure 1: Dendogram of 21 nineteenth century En-
glish books (Rank Distance)
Figure 2: Dendogram of 21 nineteenth century En-
glish books (Euclidean Distance)
93
relative frequencies of these function words in the
text. The relative frequency of a particular func-
tion word in a text is calculated as the number of
appearances of the respective function word in the
text divided by the length (in tokens) of the text.
The distance between two texts is given by the eu-
clidean distance between the corresponding vec-
tors of relative frequencies of function words. In
the family tree obtained using euclidean distance,
most of the books are still grouped according to
their author, but the distinct clusters corresponding
to genre and gender disappeared and the novels of
Melville were separated: one being clustered with
the essays of Thoreau (Moby Dick) and the other
with the novels of Hawthorne.
4 Binary Classification Experiments
When a distance measure is available, the most
natural choice of a classification algorithm is the
nearest neighbor algorithm (Duda et al, 2001).
We tested the nearest neighbor classification al-
gorithm combined with both rank distance and eu-
clidean distance on the case of the 12 disputed fed-
eralist papers (Mosteller and Wallace, 1964). In
our experiments we followed the Mosteller and
Wallace setting, treating the problem as a binary
classification problem. Each one of the 12 dis-
puted papers has to be classified as being written
by Hamilton or Madison. For training are used the
51 papers written by Hamilton and the 14 papers
written by Madison.
Tested on disputed papers, the nearest neighbor
classification algorithm combined with rank dis-
tance attributed all the 12 papers to Madison. This
matches the results obtained by Mosteller and Wal-
lace and is in agreement with today accepted thesis
that the disputed papers belong to Madison. When
the nearest neighbor classification algorithm was
combined with euclidean distance only 11 papers
were attributed to Madison, the paper 56 was at-
tributed to Hamilton.
5 Discussion
In this paper we have proposed a new distance
measure based on the ranking of function words,
designed to capture stylistic similarity between
texts. We have tested it in two different machine
learning settings: clustering and binary classifica-
tion; we have compared its performance with that
of standard euclidean distance on vectors of fre-
quencies of the function words. Though testing on
more data is needed, the initial experiments shown
that the new distance measure is indeed a good in-
dicator of stylistic similarity and better suited for
capturing stylistic differences between texts than
the standard euclidean distance.
In future work it would be useful to test this dis-
tance measure on other data sets and especially in
other machine learning paradigms like one-class
classification to solve authorship verification prob-
lems (Koppel et al, 2007).
Acknowledgments Research supported by
MEdC-ANCS, PNII-Idei, project 228 and Univer-
sity of Bucharest.
References
Burrows, John. 2002. ?delta?: a measure of stylistic
difference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3):267?287.
Chung, Cindy K. and James W. Pennebaker. 2007. The
psychological function of function words. In Fiedler,
K., editor, Social communication: Frontiers of social
psychology, pages 343?359. Psychology Press, New
York.
Diaconis, P. and R.L. Graham. 1977. Spearman?s
footrule as a measure of disarray. Journal of the
Royal Statistical Society, Series B (Methodological),
39(2):262?268.
Dinu, Anca and Liviu Petrisor Dinu. 2005. On the syl-
labic similarities of romance languages. In CICLing-
2005, pages 785?788.
Dinu, Liviu Petrisor. 2003. On the classification and
aggregation of hierarchies with different constitutive
elements. Fundamenta Informaticae, 55(1):39?50.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern
Classification (2nd ed.). Wiley-Interscience Publi-
cation.
Koppel, Moshe, Jonathan Schler, and Elisheva
Bonchek-Dokow. 2007. Measuring differentiabil-
ity: Unmasking pseudonymous authors. Journal of
Machine Learning Research, 8:1261?1276.
Labb?e, Cyril and Dominique Labb?e. 2001. Inter-
textual distance and authorship attribution corneille
and moliere. Journal of Quantitative Linguistics,
8(3):213?231.
Luyckx, Kim, Walter Daelemans, and Edward Van-
houtte. 2006. Stylogenetics: Clustering-based
stylistic analysis of literary corpora. In Proceedings
of LREC-2006, the fifth International Language Re-
sources and Evaluation Conference, pages 30?35.
Mosteller, Frederick and David L. Wallace. 1964. In-
ference and Disputed Authorship: The Federalist.
Addison-Wesley, Massachusetts.
94
Proceedings of the Workshop on Linguistic Distances, pages 109?116,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Total rank distance and scaled total rank distance:
two alternative metrics in computational linguistics
Anca Dinu
University of Bucharest,
Faculty of Foreign Languages/
Edgar Quinet 17,
Bucharest, Romania
anca d dinu@yahoo.com
Liviu P. Dinu
University of Bucharest, Faculty of
Mathematics and Computer Science/
Academiei 14, 010014,
Bucharest, Romania
ldinu@funinf.cs.unibuc.ro
Abstract
In this paper we propose two metrics to be
used in various fields of computational lin-
guistics area. Our construction is based on
the supposition that in most of the natural
languages the most important information
is carried by the first part of the unit. We
introduce total rank distance and scaled to-
tal rank distance, we prove that they are
metrics and investigate their max and ex-
pected values. Finally, a short application
is presented: we investigate the similarity
of Romance languages by computing the
scaled total rank distance between the di-
gram rankings of each language.
1 Introduction
Decision taking processes are common and fre-
quent tasks for most of us in our daily life.
The ideal case would be that when the decisions
can be taken deterministically, based on some
clear, quantifiable and unambiguous parameters
and classifiers. However, there are many cases
when we decide based on subjective or sensor-
ial criteria (e.g. perceptions), but which prove to
function well. The domains in which decisions are
taken based on perceptions vary a lot: the quali-
tative evaluation of services, management, finan-
cial predictions, sociology, information/intelligent
systems, etc (Zadeh and Kacprzyk, 1999).
When people are asked to approximate the
height of some individual, they prefer to use terms
like: very tall, rather tall, tall enough, short, etc.
We can expect the same linguistic variable to have
a different metrical correspondence according to
the community to which the individual belongs
(i.e. an individual of 170 cm can be considered
short by the Australian soldiers and tall by the Es-
kimos). Similar situations also arise when people
are asked to hierarchically order a list of objects.
For example, we find it easy to make the top of
the best five novels that we read, since number one
is the novel that we like best and so on, rather than
to say that we liked in the proportion of 40% the
novel on the first position, 20 % the novel on the
second place and so on. The same thing is happen-
ing when we try to talk about the style of a certain
author: it is easier to say that the author x is closer
to y than z, then to quantify the distance between
their styles. In both cases we operate with a ?hid-
den variable? and a ?hidden metric?.
Especially when working with perceptions, but
not only, we face the situation to operate with
strings of objects where the essential information
is not given by the numerical value of some para-
meter of each object, but by the position the object
occupies in the strings (according to a natural hier-
archical order, in which on the first place we find
the most important element, on the second place
the next one and on the last position the least im-
portant element).
As in the case of perceptions calculus, in most
of the natural languages, the most important infor-
mation is also carried by the first part of the unit
(Marcus, 1974). Cf. M. Dinu (1997), it is advis-
able that the essential elements of a message to be
situated in the first part of the utterance, thus hav-
ing the best chances to be memorized1 (see Table
1).
Based on the remark that in most of the natural
1On the contrary, M. Dinu notices that at the other end, we
find the wooden language from the communist period, text
that was not meant to inform, but to confuse the receiver with
an incantation empty of content, and that used the reversed
process: to place the important information at the end of very
long phrases that started with irrelevant information
109
The length Memorized words (%)
of the phrase all first half second half
12 100 % 100 % 100 %
13 90 % 95 % 85 %
17 70 % 90% 50%
24 50 % 70 % 30 %
40 30 % 50 % 10 %
Table 1: The percentage of memorized words from
phrases
languages the most important information is car-
ried out by the first part of the unit, in this paper
we introduce two metrics: total rank distance and
scaled total rank distance.
Some preliminary and motivations are given in
Section 2. In Section 3 we introduce total rank dis-
tance; we prove that it is a metric (Section 3.1), we
investigate its max and expected values (Section
3.2) and its behavior regarding the median ranking
problem (Section 3.3). An extension for strings is
proposed in Section 4. Scaled total rank distance
is introduced in Section 4, where we prove that it
is a metric and we investigate its max and expected
values. In Section 6 a short application is pre-
sented: we investigate the similarity of Romance
languages by computing the scaled total rank dis-
tance between the digram rankings of each lan-
guage. Section 7 is reserved to conclusions, while
in Section 8 we give a mathematically addendum
where we present the proofs of the statements.
2 Rank distance
By analogy to computing with words, natural lan-
guage and genomics, we can say that if the differ-
ences between two strings are at the top (i.e., in
essential points), the distance has to have a bigger
value then when the differences are at the bottom
of the strings.
On the other hand, many of the similarity mea-
sures used today (edit distance, Hamming distance
etc.) do not take into account the natural tendency
of the objects to place the most important informa-
tion in the first part of the message.
This was the motivation we had in mind when
we proposed Rank distance (Dinu, 2003) as an al-
ternative similarity measure in computational lin-
guistics. This distance had already been suc-
cessfully used in computational linguistics, in
such problems as the similarity of Romance lan-
guages (Dinu and Dinu, 2005), or in bioinformat-
ics (in DNA sequence comparision problem, Dinu
and Sgarro).
2.1 Preliminaries and definitions
To measure the distance between two strings, we
use the following strategy: we scan (from left to
right) both strings and for each letter from the first
string we count the number of elements between
its position in first string and the position of its
first occurrence in the second string. We sum these
scores for all elements and obtain the rank dis-
tance. Clearly, the rank distance gives a score zero
only to letters which are in the same position in
both strings, as Hamming distance does (we recall
that Hamming distance is the number of positions
where two strings of the same length differ).
On the other hand, the reduced sensitivity of
the rank distance w.r.t. deletions and insertions
is of paramount importance, since it allows us to
make use of ad hoc extensions to arbitrary strings,
such as its low computational complexity is not
affected. This is not the case for the extensions
of the Hamming distance, mathematically optimal
but computationally heavy, which lead to the edit-
distance, or Levenshtein distance, and which are at
the base of the standard alignment principle. So,
rank distance sides with Hamming distance rather
than Levenshtein distance as far as computational
complexity is concerned: the fact that in the Ham-
ming and in the rank case the median string prob-
lem is tractable (Dinu and Manea), while in the
edit case it is is NP-hard (Higuera and Casacu-
berta, 2000), is a very significant indicator.
The rank distance is an ordinal distance tightly
related to the so-called Spearman?s footrule (Di-
aconis and Graham, 1977) 2, which has long been
used in non-parametric statistics. Unlike other or-
dinal distances, the Spearman?s footrule is linear
in n, and so very easy to compute. Its average
value is at two-thirds of the way to the maximum
value (both are quadratics in n); this is because,
in a way, the Spearman footrule becomes rather
?undiscriminating? for highly different orderings.
Rank distance has the same drawbacks and the
same advantages of Spearman?s foootrule. As for
?classical? ordinal distances for integers, with av-
erages values, maximal values, etc., the reader is
2Both Spearman?s footrules and binary Hamming dis-
tances are a special case of a well-known metric distance
called sometimes taxi distance, which is known to be equiv-
alent to the usual Euclidian distance. Computationally, taxi
distance is obviously linear.
110
referred to the basic work (Diaconis and Graham,
1977).
Let us go back to strings. Let us choose a fi-
nite alphabet, say {N,V,A,O} (Noun, Verb, Ad-
jective, Object) and two strings on that alphabet,
which for the moment will be constrained to be a
permutation of each other. E.g. take two strings
of length 6: NNV AOO and V OANON ; put
indexes for the occurrences of repeated letters in
increasing order to obtain N1N2V1A1O1O2 and
V1O1A1N1O2N2. Now, proceed as follows: in
the first sequence N1 is in position 1, while it is in
position 4 in the second sequence, and so the dif-
ference is 3; compute the difference in positions
for all letters and sum them. In this case the dif-
ferences are 3, 4, 2, 1, 3, 1 and so the distance is
14. Even if the computation of the rank distance
as based directly on its definition may appear to
be quadratic, in (Dinu and Sgarro) two algorithms
which take it back to linear complexity are exhibit.
In computational linguistics the rank distance
for strings without repetitions had been enough. In
a way, indexing converts a sequence with repeti-
tions into a sequence without repetitions, in which
the k occurrence of a letter a are replaced by sin-
gle occurrences of the k indexed letters a1, a2, . . .,
ak. Let u = x1x2 . . . xn and v = y1y2 . . . ym be
two strings of lengths n and m, respectively. For
an element xi ? u we define its order or rank by
ord(xi|u) = n+1?i: we stress that the rank of xi
is its position in the string, counted from the right
to the left, after indexing, so that for example the
second O in the string V OANON has rank 2.
Note that some (indexed) occurrences appear in
both strings, while some other are unmatched, i.e.
they appear only in one of the two strings. In de-
finition (1) the last two summations refer to these
unmatched occurrences. More precisely, the first
summation on x ? u ? v refers to occurrences x
which are common to both strings u and v, the sec-
ond summation on x ? u \ v refers to occurrences
x which appear in u but not in v, while the third
summation on x ? v \ u refers to occurrences x
which appear in v but not in u.
Definition 1 The rank distance between two
strings without repetitions u and v is given by:
?(u, v) = ?
x?u?v
|ord(x|u)? ord(x|v)|+
+ ?
x?u\v
ord(x|u) + ?
x?v\u
ord(x|v) (1)
Example 1 1. Let u = abcde and v = beaf be
two strings without repetitions. ?(u, v) =
|ord(a|u) ? ord(a|v)| + |ord(b|u) ?
ord(b|v)| + |ord(e|u) ? ord(e|v)| +
ord(c|u) + ord(d|u) + ord(f |v) =
3 + 0 + 2 + 3 + 2 + 1 = 11.
2. Let w1 = abbab and w2 = abbbac be two
strings with repetitions. Their corresponding
indexed strings will be: w1 = a1b1b2a2b3
and w2 = a1b1b2b3a2c1, respectively. So,
?(w1, w2) = ?(w1, w2) = 8.
Remark 1 The ad hoc nature of the rank distance
resides in the last two summations in (1), where
one compensates for unmatched letters, i.e. in-
dexed letters which appear only in one of the two
strings.
Deletions and insertions are less worrying in the
rank case rather than in the Hamming case: if one
incorrectly moves a symbol by, say, one position,
the Hamming distance loses any track of it, but
rank distance does not, and the mistake is quite
light. So, generalizations in the spirit of the edit
distance are unavoidable in the Hamming case,
even if they are computationally very demanding,
while in the rank case we may think of ad hoc
ways-out, which are computationally convenient.
3 Total Rank Distance
We remind that one of the goals of introducing
rank distance was to obtain a tool for measuring
the distance between two strings which is more
sensitive to the differences encountered in the be-
ginning of the strings than in the ending.
Rank distance satisfies in a good measure the
upper requirement (for example it penalizes more
heavily unmatched letters in the initial part of
strings), but some black points are yet remaining.
One of them is that rank distance is invariant to the
transpositions on a given length.
The following example is eloquent:
Example 2 1. Let a = (1, 2, 3, 4, 5), b =
(2, 1, 3, 4, 5), c = (1, 2, 4, 3, 5) and d =
(1, 2, 3, 5, 4) be four permutations. Rank dis-
tance between a and each of b, c or d is the
same, 2.
2. The same is happening with
a = (1, 2, 3, 4, 5, 6, 7, 8) and
b = (3, 2, 1, 4, 5, 6, 7, 8), c =
(1, 4, 3, 2, 5, 6, 7, 8), or d =
(1, 2, 3, 4, 5, 8, 7, 6) (here rank distance
is equal to 4).
111
In the following we will repair this inconve-
nient, by introducing the Total Rank Distance, a
measure which gives us a more comprehensive in-
formation (compared to rank distance) about the
two strings which we compare.
Since in many situations occurred in computa-
tional linguistics, the similarity for strings with-
out repetitions had been enough, in the following
we introduce first a metric between rankings3 and
then we generalize it to strings.
3.1 Total rank distance on permutations
Let A and B be two rankings over the same uni-
verse U , having the same length, n. Without loss
of generality, we suppose that U = {1, 2, . . . ,m}.
For each 1 ? i ? n we define the function ? by:
?(i) def= ?(Ai, Bi). (2)
where Ai and Bi are the partial rankings of length
i obtained from the initial rankings by deleting the
elements below position i (i.e. the top i rankings).
Definition 2 Let A and B be two rankings with
the same length over the same universe, U . The
Total Rank Distance between A and B is given by:
D(A,B) =
n?
i=1
?(i) =
n?
i=1
?(Ai, Bi).
Example 3 1. Let a, b, c and d be the four per-
mutations from Example 2, item 1. The total
rank distance between a and each of b, c, d
is: D(a, b) = 10, D(a, c) = 6, D(a, d) = 4.
2. The visible differences are also in the item 2
of the upper example if we apply total rank
distance: D(a, b) = 30, D(a, c) = 28,
D(a, d) = 10.
3A ranking is an ordered list of objects. Every ranking
can be considered as being produced by applying an order-
ing criterion to a given set of objects. More formally, let U
be a finite set of objects, called the universe of objects. We
assume, without loss of generality, that U = {1, 2, . . . , |U |}
(where by |U | we denote the cardinality of U ). A ranking
over U is an ordered list: ? = (x1 > x2 > . . . > xd),
where {x1, . . . , xd} ? U , and > is a strict ordering rela-
tion on {x1, . . . , xd}, (an ordering criterion. It is important
to point the fact that xi 6= xj if i 6= j. For a given object
i ? U present in ? , ?(i) represents the position (or rank) of i
in ? . If the ranking ? contains all the elements of U , than it is
called a full ranking. It is obvious that all full rankings repre-
sent all total orderings of U (the same as the permutations of
U ). However, there are situations when some objects cannot
be ranked by a given criterion: the ranking ? contains only
a subset of elements from the unverse U . Then, ? is called
partial ranking. We denote the set of elements in the list ?
with the same symbol as the list.
The following theorem states that our terminol-
ogy total rank distance is an adequate one:
Theorem 1 Total rank distance is a metric.
Proof:
It is easy to see that D(A,B) = D(B,A).
We prove that D(A,B) = 0 iff A = B. If
D(A,B) = 0, then ?(Ai, Bi) = 0 for each
1 ? i ? n (since ? is a metric, so a nonnega-
tive number), so ?(An, Bn) = ?(A,B) = 0, so
A = B.
For the triangle inequality we have: D(A,B)+
D(B,C) =
n?
i=1
?(Ai, Bi) +
n?
i=1
?(Bi, Ci)
=
n?
i=1
(?(Ai, Bi) + ?(Bi, Ci))
?
n?
i=1
?(Ai, Ci) = D(A,C). uunionsq
3.2 Expected and max values of the total
rank distance
Let Sn be the group of all permutations of length
n and let A, B be two permutations from Sn. We
investigate the max total rank distance between A
and B and the average total rank distance between
A and B.
Proposition 1 Under the upper hypothesis, the
expected value of the total rank distance between
A and B is:
E(D) = (n
2 ? 1)(n+ 2)
6 .
Proposition 2 Under the same hypothesis as in
the previous proposition, the max total rank dis-
tance between two permutations from Sn is:
max
A,B?Sn
D(A,B) = n
2(n+ 2)
4
and it is achieved when a permutation is the re-
verse of the other one.
3.3 On the aggregation problem via total
rank distance
Rank aggregation is the problem of combining
several ranked lists of objects in a robust way to
produce a single ranking of objects.
One of the most natural way to solve the aggre-
gation problem is to determine the median (some-
times called geometric median) of ranked lists via
a particular measure.
Given a multiset T of ranked lists, a median of
T is a list L such that
112
d(L, T ) = min
X
d(X,T ),
where d is a metric and X is a ranked list over
the universe of T .
Depending on the choice of measure d, the up-
per problem may contain many unpleasant sur-
prises. One of them is that computing the median
set is NP-complete for some usual measure (in-
cluding edit-distance or Kendal distance) even for
binary universe.
We will show in the following that the median
aggregation problem via Total rank distance can
be computed in polynomial time.
Theorem 2 Given a multiset T of full ranked lists
over the same universe, the median of T via total
rank distance can be computed in polynomial time,
namely proportional to the time to find a minimum
cost perfect matching in a bipartite graph.
Proof: Without loss of generality, we suppose
that the universe of lists is U = {1, 2, . . . , n}.
We define a weighted complete bipartite graph
G = (N,P,W ) as follows. The first set of nodes
N = {1, 2, . . . , n} denotes the set of elements to
be ranked in a full list. The second set of nodes
P = {1, 2, . . . , n} denotes the n available posi-
tions. The weight W (i, j) is the contribution, via
total rank distance, of node i to be ranked on place
j in a certain ranking.
We can give a close formula for computing the
weights W (i, j) and this ends the proof, because
we reduced the problem to the solving of the mini-
mum cost maximum matching problem on the up-
per bipartite graph ((Fukuda and Matsui, 1994),
(Fukuda and Matsui, 1992), (Dinu and Manea)).
uunionsq
4 An extension to strings of total rank
distance
We can extend total rank distance to strings.
Similar to the extensions of rank distance to
strings, we index each letter in a word with the
number of its previous occurrences.
First, we extent the total rank distance to rank-
ings with unequal lengths as it follows:
Definition 3 Let u and v be two rankings of length
|u| and |v|, respectively. We can assume that |u| <
|v|. The total rank distance between u and v is
defined by:
D(u, v) =
|u|?
i=1
?(vi, ui) +
|v|?
i=|u|+1
?(vi, u).
Theorem 3 The total rank distance between two
rankings with unequal lengths is a metric.
To extent the total rank distance to strings,
firstly we index both strings and than we apply
the upper definition to the newly obtained strings
(which are now rankings).
Example 4 Let u = aabca, v = aab and w =
bca be three strings. We obtained the following
results:
1. Rank distance: ?(u, v) =
?(a1a2b1c1a3, a1a2b1) = 9 and
?(u,w) = ?(a1a2b1c1a3, b1c2a1) = 9;
2. Total rank distance: D(u, v) =
D(a1a2b1c1a3, a1a2b1) = 13 and
D(u,w) = D(a1a2b1c1a3, b1c2a1) = 33.
What happens in item 1 is a consequence of a
general property of rank distance which states that
?(uv, u) = ?(uv, v), for any nonempty strings u
and v.
Total rank distance repairs this fact, as we can
see from item 2; we observe that the total rank
distance is more sensitive than rank distance to the
differences from the first part of strings.
5 Scaled Total Rank Distance
We use the same ideas from Total rank distance,
but we normalize each partial distance. To do this,
we divide each rank distance between two partial
rankings of length i by i(i+1), which is the max-
imal distance between two rankings of length i
(it corresponds to the case when the two rankings
have no common elements).
Definition 4 The Scaled Total Rank distance be-
tween two rankings A and B of length n is:
S(A,B) =
n?
i=1
?(Ai, Bi)
i(i+ 1) .
Theorem 4 Scaled total rank distance is a metric.
Proof: The proof is similar to the one from the
total rank distance. uunionsq
Remark 2 It is easy to see that S(A,B) ?
H(A,B), where H(A,B) is the Hamming dis-
tance.
113
Example 5 Let A = (a, b, c, d, e), B =
(b, a, c, d, e) and C = (a, b, d, e, c) be three per-
mutations. We have the following values for ?, D
and S, respectively:
1. Rank distance: ?(A,B) = 2, ?(A,C) = 4, so
?(A,B) < ?(A,C).
2. Total Rank Distance: D(A,B) = 2 + 2 + 2 +
2 + 2 = 10, D(A,C) = 0 + 0 + 2 + 4 + 4 = 10,
so D(A,B) = D(A,C).
3. Scaled Total Rank Distance: S(A,B) = 22+ 26+2
12 + 220 + 230 = 53 , S(A,C) = 02 + 06 + 212 + 420 +4
30 = 12 , so S(A,B) > S(A,C).
It is not hard to see that S(A,B) ? n, so we can
normalize scaled total rank distance by dividing it
to n.
We obtained the following two values for max
and average values of scaled total rank distance:
Proposition 3
1. If n ??, then max
A,B?Sn
1
nS(A,B) = 72 ? 4 ln 2.
2. The average value of scaled total rank distance
is: E(S) = 2(n?1)3 . When n ??, E(S)n ? 23 .
Remark 3 It is a nice exercise to show that 72 ?
4 ln 2 ? 1.
Proof: 72 ? 4 ln 2 ? 1 iff 1 ? 4(ln 4 ? 1).
But 4(ln 4 ? 1) > 4(ln 4 ? ln 3). From La-
grange Theorem, there is 3 < ? < 4 such that
ln 4 ? ln 3 = 1? , so 4(ln 4 ? ln 3) = 4? > 1, so
4(ln 4? 1) > 4(ln 4? ln 3) > 1. uunionsq
6 Application
We present here a short experiment regarding the
similarity of Romance languages. The work cor-
pus is formed by the representative vocabularies of
the following six Romance languages: Romanian,
Italian, Spanish, Catalan, French and Portuguese
languages (Sala, 1988). We extracted the digrams
from each vocabularies and then we constructed a
ranking of digrams for each language: on the first
position we put the most frequent digram of the
vocabulary, on the second position the next fre-
quent digram, and so on.
We apply the scaled total rank distance between
all pairs of such classifications and we obtain a se-
ries of results which are presented in Table 2.
Some remarks are immediate:
? If we analyze the Table 2, we observe
that every time Romanian finds itself at the
biggest distance from the other languages.
Table 2: Scaled total rank distances in Romance
languages
Ro It Sp Ca Po Fr
Ro 0 0.36 0.37 0.39 0.41 0.36
It 0.36 0 0.21 0.24 0.26 0.30
Sp 0.37 0.21 0 0.20 0.18 0.27
Ca 0.39 0.24 0.20 0 0.20 0.28
Po 0.41 0.26 0.18 0.20 0 0.30
Fr 0.36 0.30 0.27 0.28 0.30 0
This fact proves that the evolution of Ro-
manian in a distanced space from the Latin
nucleus has lead to bigger differences be-
tween Romanian and the rest of the Romance
languages, then the differences between any
other two Romance languages.
? The closest two languages are Portuguese
and Spanish.
? It is also remarkable that Catalan is equally
distanced from Portuguese and Spanish.
The upper remarks are in concordance with the
conclusions of (Dinu and Dinu, 2005) obtained
from the analise of the syllabic similarity of the
Romance languages, where the rank distance was
used to compare the rankings of syllables, based
on the frequency of syllables for each language.
During the time, different comparing methods
for natural languages were proposed. We mention
here the work of Hoppenbrouwers and Hoppen-
brouwers (2001). Their approach was the follow-
ing: using the letter frequency method for each
language variety the unigram frequencies of let-
ters are found on the basis of a corpus. The dis-
tance between two languages is equal to the sum
of the differences between the corresponding letter
frequencies. They verify that this approach cor-
rectly shows that the distance between Afrikaans
and Dutch is smaller than the distance between
Afrikaans and the Samoan language.
7 Conclusions
In this paper we provided some low-complexity
metrics to be used in various subfields of computa-
tional linguistics: total rank distance and scaled to-
tal rank distance. These metrics are inspired from
the natural tendency of objects to put the main in-
formation in the first part of the units. Our ana-
lyze was especially concentrated on the mathemat-
114
ical and computational properties of these metrics:
we showed that total rank distance and scaled to-
tal rank distance are metrics, computed their ex-
pected and max values on the permutations group
and showed that total rank distance can be used in
classification problem via a polynomial algorithm.
8 Mathematical addendum
This addendum may be skipped by readers who
are not interested in mathematical technicalities;
below some statements are sketched and other are
unproved, but then the proofs are quite straightfor-
ward.
Proposition 1:
Proof: It is not hard to see that D(A,Sn) =
D(B,Sn) for any two permutation A,B ? Sn.
So, the expected value can be computed by com-
puting first D(A,Sn) for a convenable permuta-
tion and then by dividing the upper sum to n!. If
we choose A = en (i.e. the identical permutation
of the group Sn), then the expected value is:
E(D) = 1n!
?
??Sn
D(en, ?).
The upper sum can be easily computed if we take
into account the fact that each number 1, 2, . . . , n
appears the same number of times (i.e. (n-1)!) on
the ranks 1, 2, . . . n. So, we obtain that the ex-
pected value is equal to:
E(D) = (n
2 ? 1)(n+ 2)
6 .
uunionsq
Proposition 2:
Proof: W.l.g. we can suppose that first permu-
tation is the identical one, i.e. en (otherwise we
will relabelled it). To compute the max value, the
following preliminary results must be proven (we
skipped the proofs).
We say that an integer from ? is low if its posi-
tion is ? n2 and it is high if its position is > n2 .
Let ? ? Sn be a permutation. We construct the
set ?? as following:
?? = {? ? Sn | ?x ? {1 . . . n}, x is low in ?
iff x is high in ? and viceversa}
Result 1 For each ? ? Sn and every two permu-
tation ?, pi in ?? we have: D(?, ?) = D(?, pi).
Result 2 For each ? ? Sn and every two permu-
tation ?, pi such that pi ? ?? and ? /? ??, we
have: D(?, ?) < D(?, pi).
To prove Result 2 we use the following Lemma:
Lemma 1 (Dinu, 2003) If a > b, then the func-
tion f(x) = |x ? b| ? |x ? a| is an increasing
one.
Result 3 Let ? ? Sn be a permutation. The max-
imum total rank distance is reached by the per-
mutation ? where ord(x|?) = n + 1 ? ord(x|?),
?x ? V (Pn). Under this conditions the maximum
total rank distance is:
max
A,B?Sn
D(A,B) = n
2(n+ 2)
4 (3)
In other words, we obtained a more general re-
sult:
Theorem 5 For a given permutation ?, the maxi-
mum rank distance is achieved by all permutations
from ?? and it is equal to (3).
uunionsq
Proposition 3:
Proof:
1. Similar to Proposition 2, given a permutation
? ? Sn, the max value is reached by its in-
vert. So, to give a close formula for the max
value it is enough to compute S(en, e?1n ). To
make easier our life, we can suppose that
n = 2k.
S(en, e?1n ) = k +
?k
i=1
2i2+(k?i)(k?i+1)
(k+i)(k+i+1) =
. . . = 4k ? 2k22k+1 ? 2(4k + 1)(
?k
i=1
1
k+i ?
k
2k+1);
When k ? ?, ?ki=1 1k+i ? ln 2, so
S(en,e?1n )
n = 72 ? 4 ln 2 uunionsq
2. To compute the expected value we use the
same motivation as in expected total rank dis-
tance. The rest is obvious.
Acknowledgements 1 We want to thank to re-
viewers for their comments and suggestions. Re-
search supported by CNR-NATO and MEdC-
ANCS.
References
P. Diaconis, R.L. Graham, 1977. Spearman footrule as
a Measure of Disarray, Journal of Royal Statistical
Society. Series B (Methodological), Vol. 39, No. 2,
262-268.
115
L. P. Dinu, 2003. On the classification and aggregation
of hierarchies with different constitutive elements,
Fundamenta Informaticae, 55(1), 39-50.
A. Dinu, L.P. Dinu, 2005. On the Syllabic Similari-
ties of Romance Languages. In Proc. CICLing 2005,
Lecture Notes in Computer Science, Volume 3406,
pp. 785-789.
L.P. Dinu, F. Manea. An efficient approach for the rank
aggregation problem. Theoretical Computer Science
(to appear).
L.P. Dinu, A. Sgarro. A low-complexity distance for
DNA strings, Fundamenta Informaticae (to appear).
M. Dinu, 1997. Comunicarea (in Romanian). Ed.
S?tiint?ifica?, Bucures?ti.
K. Fukuda, T. Matsui, 1992. Finding all minimum cost
perfect matchings in bipartite graphs, Networks, 22,
461-468.
K. Fukuda, T. Matsui, 1994. Finding all the perfect
matchings in bipartite graphs, Appl. Math. Lett.,
7(1), 15-18.
C. de la Higuera, F. Casacuberta, 2000. Topology of
strings: Median string is NP- complete, Theoretical
Computer Science, 230:39-48.
C. Hoppenbrouwers, G. Hoppenbrouwers, 2001. De
indeling van de Nederlandse streektalen. Dialecten
van 156 steden en dorpen geklasseerdvolgens de
FFM. Koninklijke Van Gorcum, Assen.
S. Marcus, 1974. Linguistic structures and generative
devices in molecular genetics. Cahiers Ling. Theor.
Appl., 11, 77-104.
M. Sala, (coord.) 1982. Vocabularul reprezentativ al
limbilor romanice, Bucures?ti.
L.A. Zadeh, J. Kacprzyk, 1999. Computing with words
in information/intelligent systems 1: Foundations, 2:
Application. Physica-Verlag, Heidelberg and New
York.
116
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1047?1058,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
An Etymological Approach to Cross-Language Orthographic Similarity.
Application on Romanian
Alina Maria Ciobanu, Liviu P. Dinu
Faculty of Mathematics and Computer Science, University of Bucharest
Center for Computational Linguistics, University of Bucharest
alina.ciobanu@my.fmi.unibuc.ro,ldinu@fmi.unibuc.ro
Abstract
In this paper we propose a computational
method for determining the orthographic
similarity between Romanian and related
languages. We account for etymons and
cognates and we investigate not only the
number of related words, but also their
forms, quantifying orthographic similari-
ties. The method we propose is adaptable
to any language, as far as resources are
available.
1 Introduction
Language relatedness and language change across
space and time are two of the main questions of the
historical and comparative linguistics (Rama and
Borin, 2014). Many comparative methods have
been used to establish relationships between lan-
guages, to determine language families and to re-
construct their proto-languages (Durie and Ross,
1996). If grouping of languages in linguistic fam-
ilies is generally accepted, the relationships be-
tween languages belonging to the same family are
periodically investigated. In spite of the fact that
linguistic literature abounds in claims of classifi-
cation of natural languages, the degrees of similar-
ity between languages are far from being certain.
In many situations, the similarity of natural lan-
guages is a fairly vague notion, both linguists and
non-linguists having intuitions about which lan-
guages are more similar to which others. McMa-
hon and McMahon (2003) and Rama and Borin
(2014) note that the computational historical lin-
guistics did not receive much attention until the
beginning of the 1990s, and argue for the necessity
of development of quantitative and computational
methods in this field.
1.1 Related Work
According to Campbell (2003), the methods based
on comparisons of cognate lists and sound corre-
spondences are the most popular approaches em-
ployed for establishing relationships between lan-
guages. Barbanc?on et al. (2013) emphasize the va-
riety of computational methods used in this field,
and state that the differences in datasets and ap-
proaches cause difficulties in the evaluation of the
results regarding the reconstruction of the phylo-
genetic tree of languages. Linguistic phylogeny
reconstruction proves especially useful in histor-
ical and comparative linguistics, as it enables the
analysis of language evolution. Ringe et al. (2002)
propose a computational method for evolutionary
tree reconstruction based on a ?perfect phylogeny?
algorithm; using a Bayesian phylogeographic ap-
proach, Alekseyenko et al. (2012), continuing the
work of Atkinson et al. (2005), model the expan-
sion of the Indo-European language family and
find support for the hypothesis which places its
homeland in Anatolia; Atkinson and Gray (2006)
analyze language divergence dates and argue for
the usage of computational phylogenetic meth-
ods in the question of Indo-European age and ori-
gins. Using modified versions of Swadesh?s lists
1
,
Dyen et al. (1992) investigate the classification of
Indo-European languages by applying a lexicosta-
tistical method.
The similarity of languages is interesting not
only for historical and comparative linguistics,
but for machine translation and language acqui-
sition as well. Scannell (2006) and Haji?c et al.
(2000) argue for the possibility of obtaining a bet-
ter translation quality using simple methods for
very closely related languages. Koppel and Ordan
(2011) study the impact of the distance between
languages on the translation product and conclude
that it is directly correlated with the ability to dis-
tinguish translations from a given source language
from non-translated text. Some genetically re-
lated languages are so similar to each other, that
1
http://www.wordgumbo.com/ie/cmp/iedata.txt
1047
speakers of such languages are able to communi-
cate without prior instruction (Gooskens, 2007).
Gooskens et al. (2008) analyze several phonetic
and lexical predictors and their conclusion is that
lexical similarity can be seen as a predictor of lan-
guage intelligibility. The impact of language sim-
ilarities in the process of second language acquisi-
tion is argued by the contrastive analysis hypoth-
esis, which claims that where similarities between
the first and the second language occur, the acqui-
sition would be easier compared with the situation
in which there were differences between the two
languages (Benati and VanPatten, 2011).
1.2 Our Approach
Although there are multiple aspects that are rele-
vant in the study of language relatedness, such as
the orthographic, phonetic, syntactic and semantic
differences, in this paper we focus only on the or-
thographic similarity. The orthographic approach
relies on the idea that sound changes leave traces
in the orthography, and alphabetic character cor-
respondences represent, to a fairly large extent,
sound correspondences (Delmestri and Cristianini,
2010).
In this paper we propose an orthographic simi-
larity method focused on etymons (direct sources
of the words in a foreign language) and cognates
(words in different languages having the same ety-
mology and a common ancestor). In a broadly ac-
cepted sense, the higher the similarity degree be-
tween two languages, the closer they are.
One of our motivations is that when people en-
counter a language for the first time in written
form, it is most likely that they can distinguish and
individualize words which resemble words from
their native language. These words are proba-
bly either inherited from their mother tongue (ety-
mons), or have a common ancestor with the words
in their language (cognates).
Our first goal is, given a corpus C, to automat-
ically detect etymons and cognates. In Section 2
we propose a dictionary-based approach to auto-
matically extract related words, and a method for
computing the orthographic similarity of natural
languages. Most of the traditional approaches in
this field focus either on etymology detection or
on cognate identification, most of them reporting
results only on small sets of cognate pairs (usually
manually determined lists of about 200 cognates,
for which the cognate judgments are made by hu-
man experts (Rama and Borin, 2014)). Our ap-
proach implies a detailed investigation which ac-
counts not only for the number of related words,
as it is usually done in lexicostatistics (where the
relationships between languages are determined
based on the percentage of related words), but also
for their forms, quantifying orthographic similari-
ties. We employ three string similarity metrics for
a finer-grained analysis, as related words in dif-
ferent languages do not have identical forms and
their partial similarity implies different degrees of
recognition and comprehensibility. For example,
the Romanian word lun?a (moon) is closer to its
Latin etymon luna than the word b?atr?an (old) to
its etymon veteranus, and the Romanian word v?ant
(wind) is closer to its French cognate pair vent than
the word castel (castle) to its cognate pair ch?ateau.
In this paper we investigate the orthographic
similarity between Romanian and related lan-
guages. Romanian is a Romance language, be-
longing to the Italic branch of the Indo-European
language family, and is of particular interest re-
garding its geographic setting. It is surrounded
by Slavic languages and its relationship with the
big Romance kernel was difficult. Besides gen-
eral typological comparisons that can be made
between any two or more languages, Romanian
can be studied based on comparisons of ge-
netic and geographical nature, participating in nu-
merous areally-based similarities that define the
Balkan convergence area. Joseph (1999) states
that, regarding the genetic relationships, Roma-
nian can be studied in the context of those lan-
guages most closely related to it and that the
well-studied Romance languages enable compar-
isons that might not be possible otherwise, within
less well-documented families of languages. The
position of Romanian within the Romance fam-
ily is controversial (McMahon and McMahon,
2003): either marginal or more integrated within
the group, depending on the versions of the cog-
nate lists that are used in the analysis.
In Section 3.1 we apply our method on Roma-
nian in different stages of its evolution, running
our experiments on high-volume corpora from
three historical periods: the period approximately
between 1642 and 1743, the second half of 19
th
century (1870 - 1889), and the present period. In
Section 3.2 we make use of a fourth corpus, Eu-
roparl, with a double goal: on the one hand, to
check if degrees of similarity between Romanian
1048
and other languages in the present period are con-
sistent across two different corpora, and on the
other hand, to investigate whether there are dif-
ferences between the overall degrees of similarity
obtained for the entire corpus and those obtained
in various experiments at sentence level. The con-
clusions of our paper are outlined in Section 4.
2 Methodology and Algorithm
In this section we introduce a technique for deter-
mining the orthographic similarity of languages.
In order to obtain accurate results, we investigate
both etymons and cognates. First, we automati-
cally identify etymons and cognates, then we mea-
sure the distances between related words, and fi-
nally we compute the overall degrees of similarity
between pairs of languages. We also applied this
method for investigating the mutual intelligibility
of the Romance languages, and preliminary results
are presented in (Ciobanu and Dinu, 2014b).
2.1 Similarity Method
Let C  tw
1
, w
2
, ..., w
N
words
u be a corpus in L
1
and letL
2
be a related language. We assume, with-
out any loss of generality, that the elements of C
are ordered such that C
L
 tw
1
, w
2
, ..., w
N
lingua
u
is the subset of C containing all the words that
have an etymon or a cognate pair in L
2
. We use
the following notations: N
words
is the number of
token words in C, N
lingua
is the number of token
words in C
L
, ? is the empty string and x
i
is the
etymon or cognate pair of w
i
in L
2
. Given a string
distance ?, we define the distance between L
1
and
L
2
(non-metric distance), with frequency support
from corpus C, as follows:
?pL
1
, L
2
q  1
N
lingua
N
words
 
?
N
lingua
i1
?pw
i
, x
i
q
N
words
(1)
Hence, the similarity between languages L
1
and
L
2
is defined as follows:
SimpL
1
, L
2
q  1?pL
1
, L
2
q
(2)
2.2 Algorithm
We present here the algorithm based on linguistic
relationships detection and string similarity meth-
ods for determining the orthographic similarity
between languages, with frequency support from
corpora in the source language. This algorithm,
?? ??
etymologyetymology
cognatescognates
LinguaC
Nlingua
Nwords - Nlingua
|C| = Nwords, |Lingua| = Nlingua
Figure 1: Schema for determining the ortho-
graphic similarity between related languages with
frequency support from corpus C.
Corpus
#words #stop words #lemmas
token type token type type
Parliament 22,469,290 162,399 14,451,178 214 40,065
Eminescu 870,828 65,742 565,396 212 21,456
Chronicles 253,786 28,936 170,582 193 8,189
RVR 2,464 2,464 124 124 2,252
Table 1: Statistics for the Romanian datasets.
represented in Figure 2, is applicable to any lan-
guage. After a preprocessing phase, which is de-
tailed in Subsection 2.2.1, we analyze words and
begin by identifying their etymologies.
2.2.1 Preprocessing
Given a corpus C, we start by preprocessing the
text.
Step 1. Data Cleaning. We perform basic
word segmentation, using whitespace and punc-
tuation marks as delimiters and we lower-case all
words. We remove from the datasets tokens that
are irrelevant for our investigation, such as dates,
numbers and non-textual annotations marked by
non-alphanumeric characters.
Step 2. Stop Words Removal. We focus on
analyzing word content and, in order to obtain
relevant results, we remove stop words from the
datasets. We use the lists of stop words for Roma-
nian provided by the Apache Lucene
2
text search
engine library. In Table 1 we list the total number
of stop words from each corpus.
2
http://lucene.apache.org
1049
Step 3. Lemmatization. We use lemmas for
identifying words? definitions in dictionaries and
for computing adequate distances between words
and their cognates or etymons. We use the Dex-
online
3
machine-readable dictionary to lemmatize
Romanian words.
Step 4. Diacritics Removal. Many words have
undergone transformations by the augmentation of
language-specific diacritics when entering a new
language. From an orthographic perspective, the
resemblance of words is higher between words
without diacritics than between words with dia-
critics. For example, the orthographic distance
is higher for the Romanian word amicit?ie (friend-
ship) and its French cognate pair amiti?e than for
their corresponding forms without diacritics, am-
icitie and amitie. For this reason, in this step
of our procedure we create two versions of each
dataset, with and without diacritics, in order to fur-
ther investigate the influence of the diacritics on
the cross-language orthographic similarity. In Ro-
manian, 5 diacritics are used today: ?a, ??, ?a, s?, t?.
2.2.2 Relationships Identification
Step 1. Etymology Detection. For most words,
etymological dictionaries offer a unique etymol-
ogy, but when more options are possible for ex-
plaining a word?s etymology (there are words
whose etymology was and remains difficult to
ascertain), dictionaries may provide multiple al-
ternatives. For example, the Romanian word
parlament (parliament) has a double etymology:
French (with the etymon parlement) and Italian
(with the etymon parlamento). We account for all
the given etymological hypotheses, enabling our
method to provide more accurate results.
For determining words? etymologies we use the
Dexonline machine-readable dictionary, which is
an aggregation of over 30 Romanian dictionaries.
By parsing its definitions, we are able to automat-
ically extract information regarding words? ety-
mologies and etymons. The most frequently used
pattern is shown below.
<abbr class="abbrev"
title="limba language_name">
language_abbreviation </abbr>
<b> origin_word </b>
As an example, we provide below an excerpt
from a Dexonline entry which uses this pattern to
3
http://dexonline.ro
specify the etymology of the Romanian word capi-
tol (chapter), which has double etymology: Latin
(with the etymon capitulum) and Italian (with the
etymon capitolo).
<b> CAP
?
ITOL </b>
<abbr class="abbrev"
title="limba italiana"> it. </abbr>
<b> capitolo </b>
<abbr class="abbrev"
title="limba latina"> lat. </abbr>
<b> capitulum </b>
Step 2. Cognate Identification. Cognates are
words in different languages having the same ety-
mology and a common ancestor. The methods for
cognate detection proposed so far are mostly based
on orthographic/phonetic and semantic similari-
ties (Kondrak, 2001; Frunza et al., 2005), but the
term ?cognates? is often used with a somewhat dif-
ferent meaning, denoting words with high ortho-
graphic/phonetic and cross-lingual meaning simi-
larity, the condition of common etymology being
left aside. We focus on etymology and we intro-
duce an automatic strategy for detecting pairs of
INPUT TEXT
etymolygcnogasy???
?ass?t??t?y?
GENETIC RELATIONSHIPSIDENTIFICATION ?t?sy?y??cata?t?y?
?y???tao?ca?t????t?y?
???? ? ??ao??ntag???
?gt?y?g?m???on?s???g?t??ysm?t?t?y?
SIMILARITY HIERARCHY
?ys?????c???y??g?an
LANGUAGE SIMILARITY COMPUTATION
?ga???o???? ?? ?anc??t?y??g?an
TEXT PROCESSING
?ygcnoc?nt???anosa?n?g???
Figure 2: Algorithm for determining the ortho-
graphic similarity between related languages.
1050
victoria (lat.)victorie (ro.) etymon etymoncognates vittoria (it.)
Figure 3: Word-etymon and cognate pairs.
cognates between two given languages, which en-
ables the identification of all cognate pairs for the
studied corpus.
Considering a set of words in a given language
L, to identify the cognate pairs between L and a re-
lated language L?, we first determine the etymolo-
gies of the given words. Then we translate in L?
all words without L? etymology. We consider cog-
nate candidates pairs formed of input words and
their translations. Using electronic dictionaries,
we extract etymology-related information for the
translated words. To identify cognates, we com-
pare, for each pair of candidates, the etymologies
and the etymons. If they match, we identify the
words as being cognates.
In our previous work (Ciobanu and Dinu,
2014a) we applied this method on a Romanian dic-
tionary, while here we extract cognates from Ro-
manian corpora. We identify cognate pairs be-
tween Romanian and six other languages: Ital-
ian, French, Spanish, Portuguese, Turkish and En-
glish. We use electronic dictionaries
4
to extract
etymology-related information and Google Trans-
late
5
to translate Romanian words. We are re-
stricted in our investigation by the available re-
sources, but we plan to extend our method to
other related languages as well. We selected
these six languages for the following reason: the
first four in our list are Romance languages, and
our intuition is that there are numerous words in
these languages which share a common ancestor
with Romanian words. We investigate the cog-
nate pairs for Turkish because many French words
were imported in both Romanian and Turkish in
the 19
th
century, and we believe that accounting
for Romanian-Turkish cognates would provide a
more accurate result for the similarity of these lan-
4
Italian: http://www.sapere.it/sapere/dizionari
French: http://www.cnrtl.fr
Spanish: http://lema.rae.es/drae
Portuguese: http://www.infopedia.pt/lingua-portuguesa
Turkish: http://www.nisanyansozluk.com
English: http://www.collinsdictionary.com
5
http://translate.google.com
guages. As for English, we decided to investigate
the cognate pairs for this language in order to ana-
lyze to what extent the influence of English on Ro-
manian increases across time. In Table 2 we report
the number of Romanian words having an etymon
or a cognate pair in the six related languages.
Step 3. Evaluation. In order to evaluate our au-
tomatic method for extracting etymology-related
information and for detecting related words, we
excerpt a sample of 500 words for each of the
considered languages (Romanian, French, Italian,
Spanish, Portuguese, Turkish and English). The
samples are drawn using a proportionate stratifi-
cation sampling method with regard to the length
of the lemmas in our datasets. We manually deter-
mine the etymologies of the words in the samples,
and we compare these results with the automati-
cally obtained etymologies. We compute the accu-
racy for etymology extraction for each language,
and we obtain the following results: 95.8% accu-
racy for Romanian, 97.8% for Italian, 96.8% for
French, 96.6% for Spanish, 97.0% for Portuguese,
96.0% for Turkish and, finally, 97.2% for English.
Language Relationship
Corpus
Parliament Eminescu Chronicles RVR
French
cognates 192,275 13,074 3,139 43
etymons 15,665,865 484,668 89,946 1,203
Italian
cognates 1,660,588 40,491 2,743 100
etymons 9,234,710 348,948 77,633 957
Spanish
cognates 4,616,528 119,627 9,942 355
etymons 4,411,707 212,106 65,336 482
Portuguese
cognates 4,378,354 115,309 15,755 324
etymons 3,477,285 156,908 55,991 435
Turkish
cognates 1,401,569 33,070 2,332 113
etymons 331,863 24,115 11,985 69
English
cognates 4,347,302 146,377 21,966 296
etymons 625,596 17,328 6,799 56
Table 2: Number of Romanian token words having
etymons or cognate pairs in related languages.
2.2.3 Linguistic Distances
Various approaches have been previously em-
ployed for assessing the orthographic distance
or similarity between related words. Their per-
formance has been investigated and compared
(Frunza et al., 2005; Rama and Borin, 2014), but
a clear conclusion cannot be drawn with respect to
which method is the most appropriate for a given
task. We employ three metrics to determine the
orthographic similarity between related words. In
Subsection 3.1.2 we investigate to what extent the
similarity scores computed with each of these met-
rics differ, and whether the differences are statisti-
cally significant. We use the following metrics:
1051
? LCSR: The longest common subsequence
ratio (Melamed, 1995) is the longest common
subsequence of two strings u and v divided
by the length of the longer string. We sub-
tract this value from 1, in order to obtain the
distance between two words.
? EDIT: The edit distance (Levenshtein, 1965)
counts the minimum number of operations
(insertion, deletion and substitution) required
to transform one string into another. We use
a normalized version of the edit distance, di-
viding it by the length of the longer string.
? RD: The rank distance (Dinu and Dinu,
2005) is used to measure the similarity be-
tween two ranked lists. A ranking of a set
of n objects can be represented as a permuta-
tion of the integers 1, 2, ..., n. Let S be a set
of ranking results, ? P S. ?piq represents the
rank of object i in the ranking result ?. The
rank distance is computed as: RDp?, ?q 
?
n
i1
|?piq  ?piq|. The ranks of the ele-
ments are assigned from bottom up, i.e. from
n to 1, using the Borda count method (de
Borda, 1781). The elements which do not oc-
cur in any of the rankings receive the rank
0. To extend the rank distance to strings,
we index each occurrence of a given letter
a with a
k
, where k is the number of its pre-
vious occurrences, and then apply the rank
distance on the new indexed strings, which
become rankings in this situation. In order
to normalize it, we divide the rank distance
by the maximum possible distance between
two strings u and v (Dinu and Sgarro, 2006):
?
max
pu, vq  |u|p|u| 1q{2 |v|p|v| 1q{2.
3 Experiments and Results
In this section we present the results obtained by
applying our method for determining orthographic
similarity on Romanian datasets.
To our knowledge, only basic lexicostatistical
methods (generally based on different dictionar-
ies or versions of the representative vocabulary
of Romanian) which compute the percentage of
words with a given etymology have been applied
for determining the relationships between Roma-
nian and related languages. Because of the diffi-
culty of setting the bounds between the basic lex-
icon and the remaining words, Graur (1968) uses
in his experiments three concentric versions of the
basic Romanian lexicon. Dinu (1996) reevaluates
the etymology detection for the three versions of
the basic Romanian lexicon and reclassifies the
lexical material. He argues against grouping to-
gether all the words with Slavic origins, without
differentiation between Old Slavic and languages
such as Bulgarian, Russian, Ukrainian and Polish.
Sala (1988) builds a version of the representative
vocabulary of Romanian comprising 2588 words,
which we use in our experiments as well.
3.1 Romanian Evolution
We apply our similarity method on high-volume
Romanian corpora from three distinct historical
periods of time, with different cultural, econom-
ical, political and social contexts. In Table 1 we
report statistics for these corpora and for the basic
Romanian lexicon.
3.1.1 Data
The first corpus consists of the transcription of the
parliamentary debates held in the Romanian Par-
liament from 1996 to 2007 (Grozea, 2012). The
second corpus consists of the publishing works of
Mihai Eminescu (Eminescu, 1980 1985), the lead-
ing Romanian poet. His works provide an insight-
ful description of the period between 1870 and
1889, with respect to its cultural, economical, so-
cial and political aspects, including some major
events in the Romanian history. Many researchers
consider that Eminescu had a crucial influence on
Romanian, his contribution to modern language
development being highly appreciated. The third
corpus dates back to the period approximately be-
tween 1642 and 1743, the beginning period of the
Romanian writing. Miron Costin, Grigore Ure-
che and Ion Neculce are Romanian chroniclers
whose main works follow one another in creat-
ing one of the most detailed and valuable descrip-
tions of Moldavia in that period, ?Letopiset?ul T??arii
Moldovei?. Along with them, Dimitrie Cantermir
contributed to the early development of the Roma-
nian writing, having written what is considered to
be the first attempt at a socio-political novel (?Isto-
ria Ieroglific?a?, 1703-1705). Their chronicles ac-
count for social, cultural, economical and political
events with the purpose of recreating historical pe-
riods of time. We also use the basic Romanian lex-
icon (Sala, 1988), abbreviated RVR, for our exper-
iments. The Dexonline machine-readable dictio-
nary, which we use for determining the etymolo-
gies for the Romanian words, aggregates defini-
1052
Language
Parliament Eminescu Chronicles RVR
%words D ND %words D ND %words D ND %words D ND
French 70.6 45.5 46.0 48.3 48.8 57.2 35.2 36.1 37.2 38.2 36.7 20.3 21.1 22.3 23.1 50.6 30.3 31.4 32.2 33.3
Latin 63.7 40.2 42.0 59.9 34.6 36.6 44.9 24.2 25.7 56.5 34.0 37.3
Italian 48.5 28.1 33.4 29.1 34.5 44.7 26.9 30.2 27.9 31.2 31.7 19.6 20.3 20.7 21.4 41.4 23.4 26.2 25.2 28.0
Spanish 40.2 9.2 24.9 10.7 27.0 38.1 10.9 21.2 12.9 23.7 29.7 11.9 15.1 13.9 17.2 32.5 9.0 19.5 9.9 21.0
Portuguese 35.0 8.3 22.1 9.5 24.0 31.3 9.6 18.5 11.3 21.0 28.3 12.2 16.3 13.9 18.4 29.3 8.6 17.4 9.4 18.9
English 22.1 2.2 14.0 2.2 14.2 18.8 1.1 9.9 1.2 10.1 11.3 1.3 5.9 1.3 6.2 14.3 1.6 10.3 1.6 10.4
Provencal 17.7 9.6 9.8 20.7 11.3 11.6 21.8 13.0 13.4 16.8 9.7 10.5
German 9.2 5.8 5.9 6.9 4.5 4.6 4.9 2.4 2.4 10.2 6.3 6.6
Turkish 7.7 0.9 5.4 0.9 5.6 6.6 1.7 4.5 1.7 4.7 5.6 2.9 3.7 3.1 3.9 7.4 1.6 5.0 1.8 5.3
Russian 5.9 3.7 4.0 6.5 4.0 4.4 7.5 4.3 4.9 9.0 5.4 6.2
Catalan 5.9 3.3 3.4 9.0 4.8 5.1 11.2 5.9 6.4 8.4 4.6 4.9
Greek 4.8 2.9 3.0 6.0 3.6 3.7 4.5 2.6 2.7 4.6 2.5 2.6
Albanian 4.8 2.6 3.0 6.7 3.7 4.0 9.1 4.9 5.3 8.4 4.2 4.8
Bulgarian 4.0 2.6 3.0 7.4 4.7 5.5 10.6 6.8 7.8 11.8 7.2 8.4
Slavic 4.9 2.3 2.5 6.6 3.4 3.8 12.1 6.5 7.7 9.8 5.0 5.7
Old Slavic 3.8 2.2 2.7 6.1 3.3 4.3 11.9 6.8 8.7 9.5 5.2 6.0
Hungarian 2.9 1.8 2.0 5.1 2.9 3.3 7.5 4.3 4.7 7.4 3.7 4.6
Ruthenian 2.4 1.6 2.0 4.7 3.0 3.7 6.0 3.7 4.4 4.5 2.4 3.0
Serbian 2.6 1.4 1.6 5.8 3.0 3.4 8.9 5.0 5.5 8.6 5.2 6.0
Sardinian 1.7 1.0 1.0 3.3 1.7 1.8 4.0 2.0 2.1 2.6 1.4 1.5
Table 3: Results for the Romanian datasets. In the D and ND columns we provide the average degrees
of similarity for the datasets with and without diacritics. For languages for which we determine cognate
pairs (besides etymons), we report both versions of the results, before and after cognate identification.
In the %words column we provide the percentage of words having an etymon or a cognate pair in each
language. The results are ordered according to the ranking of similarity for the corpus comprising the
parliamentary debates after identifying cognates and with diacritics included.
tions from over 30 dictionaries ranging from 1927
to the present time and contains archaisms and
obsolete words (which are marked accordingly);
therefore, we are able to identify etymologies for
words in all used corpora.
3.1.2 Results
In this subsection we present and analyze the main
results drawn from our research. In Table 3 we
list the output of our method for each corpus, with
and without diacritics
6
. We report the similarity
between Romanian and related languages, provid-
ing the average value of the three metrics used. In
the %words column we provide, for each corpus,
the percentage of words having an etymon or a
cognate pair in a given language (the typical mea-
sure used in lexicostatistical comparison, i.e., the
0 distance function). The results for the Romanian
datasets are plotted in Figure 4 and Figure 5.
Cognate Influence. Table 3 and Figure 4
present the gain obtained by cognate analysis. Ac-
counting for cognates leads to an increase of sim-
ilarity between Romanian and Spanish and Por-
tuguese with almost 300%, and between Roma-
nian and Italian with almost 20%. Another spec-
tacular increase of closeness is for Turkish, which
draws closer to Romanian with more than 500%
6
The complete ranking of similarity is available online at
http://nlp.unibuc.ro/resources/rosim.pdf.
by using the cognate gain. The degree of sim-
ilarity is not given by the contribution of words
inherited in Romanian from Turkish (about 1%),
but by the pairs of shared cognates. Both Ro-
manian and Turkish borrowed words from French
massively towards the end of the 19
th
century.
Thus, most pairs of Romanian-Turkish cognates
have common French ancestors, and words in Ro-
manian and Turkish which resemble are actually
loans from the same French words. We also notice
a significant increase in similarity between Roma-
nian and English in the modern period. This in-
crease is natural and probably arises for the simi-
larity between English and most of the other lan-
guages as well. We notice that this increase is
due preponderantly to the cognate pairs. Most of
the Romanian-English cognates have a Romance
common ancestor (78.4% Latin, 4.2% French,
3.4% Italian), and 11.8% have a Greek common
ancestor, counted at lemma level on the corpus
comprising the parliamentary debates.
Romanian Evolution. Some significant results
can be observed in the evolution of Romanian: the
degrees of similarity between Romanian and all
the Romance languages has increased significantly
from the Chronicles period until today. Besides
them, German is the only language to which Ro-
manian drew closer (a possible explanation might
be the fact that, after the establishment of Germans
1053
Fren
ch Lati
n
Itali
an
Spa
nish
Por
tug
ues
e
Eng
lish
Pro
ven
cal
Ger
man Tur
kish
Rus
sian
Cat
alan Gre
ek
Alba
nian
Bulg
aria
n
Slav
ic
Old
 Sla
vic
Hun
gar
ian
Rut
hen
ian
Ser
bian
Sar
dini
an
language
0
5
10
15
20
25
30
35
40
45
50
sim
ilari
ty
ParliamentEminescuChroniclesRVR
Figure 4: Degrees of similarity for the Romanian datasets. For French, Italian, Spanish, Portuguese,
Turkish and English, the values obtained after the cognate identification phase are also plotted.
in Banat and Transylvania, many German words
entered the basic Romanian lexicon). On the con-
trary, the similarity between Romanian and almost
all the Slavic languages decreased in the same pe-
riod. Russian is the only Slavic language which
preserved its degree of similarity with Romanian
(being, in the 2000s, the only Slavic language
among the top 10 most closely related languages
with Romanian, on the ninth position, with a de-
gree of similarity of less than 4%). In the 18
th
and
19
th
centuries, the transition to the Latin alpha-
bet and the desire to restore Romanian?s Latin ap-
Romance Germanic Slavic Altaic Finno-Ugriclanguage family
0
10
20
30
40
50
60
sim
ilari
ty
ParliamentEminescuChroniclesRVR
Figure 5: Degrees of similarity for the language
families. Iranian and Baltic have a degree of simi-
larity of less than 0.5.
pearance contributed to the decrease of the Slavic
influence (Ghet?ie, 1978). In fact, all Slavic in-
fluences in the 2000s sum up to 8.9%, in con-
trast with Latin influences, reaching 61.8%. Greek
is the only language which reaches its peak re-
garding the similarity with Romanian in the 19
th
century (due to the brief Phanariot dominance in
the 19
th
century). Therefore, Romanian preserved
its Latin character all along, and the influence of
the non-Latin languages on Romanian (overesti-
mated in some works) was in fact not so signif-
icant. This fact supports Darwin?s theory (Dar-
win, 1859), which states that the genealogy of lan-
guages is consistent with the genealogy of the na-
tions (analyzed based on DNA similarity).
Orthographic Metrics. In order to compare the
similarity scores computed with the three metrics
used, we conduct hypothesis tests (Sheskin, 2003)
to determine whether the differences between the
results obtained with each metric are statistically
significant. We extract a sample of 5,000 words
and we compute the pairwise differences between
the similarity scores. Using the R v3.1.0 software
environment for statistical computing (R Core
Team, 2014), we perform the one-way ANOVA
F-test, with the null hypothesis H
0
: ?
EDIT

?
LCSR
 ?
RD
(where ?
?
is the mean of the val-
1054
ues computed with the ? metric) and the alterna-
tive hypothesis H
a
: not all ?
EDIT
, ?
LCSR
, ?
RD
are equal. Since the p-value of 2.88e-05 is much
smaller than the 0.05 significance level, we have
very strong evidence to reject the null hypothesis
that the mean computed values for the three met-
rics are all equal. Further, we perform post-hoc
comparisons applying pairwise t-tests with Bon-
feronni correction for the p-value, in order to an-
alyze the differences between the metrics. For
each pair of metrics, p ! 0.05. The differences
are statistically significant, but we notice that they
are small. Applying a two-sampled t-test, we ob-
tain a [0.012, 0.015] confidence interval for the
mean difference between EDIT and LCSR, [0.015,
0.018] for EDIT and RD, and [0.001, 0.003] for
RD and LCSR, at 95% confidence level. More-
over, computing Spearman?s rank correlation co-
efficient for the rankings obtained by each metric
for each dataset, we observe a very high corre-
lation between them (? ? 0.98 for each pair of
variables). Thus, we conclude that reporting the
average of the three metrics is relevant for our ex-
periments, as differences are small and do not in-
fluence the ranking.
3.2 Europarl Experiments
We continue our investigation regarding the sim-
ilarity of natural languages with two additional
experiments. First, we want to see if degrees
of similarity between Romanian and other lan-
guages in the present period are consistent across
two different corpora. In the second experi-
ment we are interested to see if there are differ-
ences between the overall degrees of similarity
obtained for the entire corpus (the bag-of-words
model) and those obtained in various experiments
at sentence level. Our main corpus is Europarl
(Koehn, 2005). More specifically, we use the por-
tions larger than 2KB collected between 2007 and
2011 from the Romanian subcorpus of Europarl.
The corpus is tokenized and sentence-aligned in
21 languages. For preprocessing this corpus, we
discard all the transcribers? descriptions of the par-
liamentary sessions (such as ?The President in-
terrupted the speaker? or ?The session was sus-
pended at 19:30 and resumed at 21:00?).
Exp. #1. In a first step, we apply the methodol-
ogy described in Section 2 on the entire Europarl
corpus for Romanian, using a bag-of-words model
for the entire corpus, in which we account for the
overall frequencies of the words. In this experi-
ment, as in the previous ones, we cannot detect
outliers, i.e., sentences which are unbalanced re-
garding the etymologies of the comprised words.
For this reason, we conduct a second experiment
which addresses this potential issue.
Exp. #2. We determine sentence-level ortho-
graphic similarity and we aggregate the results to
compute the average values for the related lan-
guages. In this second experiment, we apply the
methodology described in Section 2 for each sen-
tence in the Europarl corpus for Romanian. For
each sentence we obtain a ranking of related lan-
guages and, in order to obtain a ranking of sim-
ilarity for the entire corpus, we compute the av-
erage degrees of similarity: for each related lan-
guage, we sum up the degrees of similarity for all
the sentences and divide this value by the number
of sentences in the corpus.
Exp. #3. Because the interpretation of statis-
tics derived from datasets that include outliers may
be misleading, we compute the standard quartiles
Q1, Q2 and Q3 (Sheskin, 2003) with regard to the
length of the sentences. We use the interquartile
range IQR  Q3  Q1 to find outliers in the
data. We consider outliers the observations that
fall below the lower fence LF = Q1  1.5pIQRq
or above the upper fence UF = Q3   1.5pIQRq.
We apply our methodology again only for the sen-
tences having the length in the rLF,UF s range.
Language Exp. #1 Exp. #2 Exp. #3 Exp. #4
French 53.1 52.1 52.1 52.8
Latin 44.1 43.6 43.6 44.0
Italian 40.6 39.9 39.9 40.2
Portuguese 33.6 32.9 32.8 33.2
Spanish 27.6 27.3 27.3 26.8
English 16.0 15.7 15.7 15.1
Provencal 10.0 10.1 10.1 9.3
Turkish 6.3 6.2 6.1 5.7
German 5.9 5.8 5.8 5.3
Greek 4.4 4.3 4.3 3.8
Russian 4.2 4.1 4.1 3.6
Catalan 4.1 4.2 4.2 3.5
Old Slavic 3.1 3.2 3.2 2.7
Albanian 3.0 3.1 3.1 2.5
Bulgarian 2.9 2.9 2.9 2.2
Slavic 2.6 2.5 2.5 1.9
Hungarian 2.5 2.4 2.4 1.7
Ruthenian 2.1 2.1 2.1 1.3
Serbian 1.6 1.5 1.5 0.7
Sardinian 1.2 1.2 1.3 0.1
Table 4: Results for Europarl on the entire corpus
(Exp. #1) and at sentence level (Exp. #2 - #4).
1055
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
20,000
40,000
60,000
80,000
100,000
120,000
140,000
# se
nten
ces
#sentences  385,968
French
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
#sentences  386,588
Latin
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
#sentences  383,668
Italian
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
#sentences  381,182
Portuguese
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
#sentences  379,439
Spanish
Figure 6: Distribution of the Romanian sentences in Europarl based on their similarity with the top 5
ranked languages. The OX axis represents the degree of similarity normalized to [0,1].
Exp. #4. As a last experiment, for each language
L we consider as observations the degrees of sim-
ilarity between Romanian and L, we discard out-
liers and we compute the average value of the ob-
servations inside the rLF,UF s range. For each
language, we determine the distribution of the sen-
tences according to their similarity with the given
language (the histograms for the top 5 languages
are presented in Figure 6).
In Figure 7 we report the top 20 languages in
the ranking of similarity for Europarl, emphasiz-
ing the gain obtained by identifying cognates. In
Table 4 we report the similarity scores for the top
20 languages in the rankings of similarity for all
the 4 experiments: overall similarity for the entire
Europarl corpus (Exp. #1), sentence-level similar-
ity (Exp. #2), similarity for the sentences having
the length in the rLF,UF s range (Exp. #3), and
similarity for the sentences having the similarity
between Romanian and each related language in
the rLF,UF s range (Exp. #4).
Some remarks are immediate. We observe
that the differences between the values obtained
Fren
ch Lati
n
Itali
an
Port
ugu
ese
Spa
nish Eng
lish
Prov
enc
al
Turk
ish
Ger
man Gre
ek
Rus
sian
Cat
alan
Old
 Sla
vic
Alba
nian
Bulg
aria
n
Slav
ic
Hun
gari
an
Rut
hen
ian
Serb
ian
Sard
inia
n
language
0
10
20
30
40
50
sim
ilari
ty
overallsentences
Figure 7: Degrees of similarity for Europarl.
for the entire corpus (Exp. #1) and those ob-
tained in various experiments at sentence level
(Exp. #2 - #4) are very small (an exception is
Exp. #4, for languages whose degrees of similarity
with Romanian are of less than 10%). We test the
bag-of-words model on two corpora from the same
period (the Parliament corpus and Europarl ? in
Exp. #1) and we notice that the results are consis-
tent across different corpora (0.98 Spearman?s ?).
The only significant difference is for Portuguese,
which is closer to Romanian as measured on Eu-
roparl than on the Parliament corpus.
4 Conclusions and Future Work
In this paper we proposed a computational method
for determining cross-language orthographic
similarity, with application on Romanian. We
investigated etymons and cognates and we con-
ducted a fine-grained analysis of the orthographic
similarity between Romanian and related lan-
guages. Our results provide a new insight into
the classification and evolution of Romanian. We
plan to apply our similarity method on a corpus
of spoken language, and to extend our analysis
to other languages as well, as we gain access to
available resources. We further intend to combine
our orthographic approach with syntactic and
semantic evidence for a wider perspective on
language similarity.
Acknowledgements
We thank the anonymous reviewers for their help-
ful and constructive comments. We thank Raluca
Vasilache for the help with evaluating the auto-
matic method for detecting related words. The
contribution of the authors to this paper is equal.
Research supported by CNCS UEFISCDI, project
number PNII-ID-PCE-2011-3-0959.
1056
References
Alexander V. Alekseyenko, Quentin D. Atkinson,
Remco Bouckaert, Alexei J. Drummond, Michael
Dunn, Russell D. Gray, Simon J. Greenhill, Philippe
Lemey, and Marc A. Suchard. 2012. Mapping the
Origins and Expansion of the Indo-European Lan-
guage Family. Science, 337(6097):957?960.
Quentin D. Atkinson and Russell D. Gray. 2006. How
Old is the Indo-European Language Family? Illumi-
nation or More Moths to the Flame? In Peter Forster
and Colin Renfrew, editors, Phylogenetic Methods
and the Prehistory of Languages, chapter 8, pages
91?109. McDonald Institute for Archaeological Re-
search.
Quentin D. Atkinson, Russell D. Gray, Geoff K.
Nicholls, and David J. Welch. 2005. From Words
to Dates: Water into Wine, Mathemagic or Phylo-
genetic Inference? Transactions of the Philological
Society, 103(2):193?219.
Francois Barbanc?on, Steven N. Evans, Luay Nakhleh,
Don Ringe, and Tandy Warnow. 2013. An Exper-
imental Study Comparing Linguistic Phylogenetic
Reconstruction Methods. Diachronica, 30(2):143 ?
170.
Alessandro G. Benati and Bill VanPatten. 2011. Key
Terms in Second Language Acquisition. Interna-
tional Journal of Applied Linguistics, 21(2):270?
273.
Lyle Campbell. 2003. How to Show Languages are
Related: Methods for Distant Genetic Relationship.
In Brian D. Joseph and Richard W. Janda, editors,
The Handbook of Historical Linguistics. Blackwell.
Alina Maria Ciobanu and Liviu P. Dinu. 2014a. Build-
ing a Dataset of Multilingual Cognates for the Ro-
manian Lexicon. In Proceedings of the 9th Interna-
tional Conference on Language Resources and Eval-
uation, LREC 2014, pages 1038?1043.
Alina Maria Ciobanu and Liviu P. Dinu. 2014b. On
the Romance Languages Mutual Intelligibility. In
Proceedings of the 9th International Conference on
Language Resources and Evaluation, LREC 2014,
pages 3313?3318.
Charles Robert Darwin. 1859. On the Origin of
Species by Means of Natural Selection, or the
Preservation of Favoured Races in the Struggle for
Life. John Murray.
Jean-Charles de Borda. 1781. M?emoire sur les
?
Elections au Scrutin. Histoire de l?Acad?emie
Royale des Sciences.
Antonella Delmestri and Nello Cristianini. 2010.
String Similarity Measures and PAM-like Matrices
for Cognate Identification. Bucharest Working Pa-
pers in Linguistics, 12(2):71?82.
Anca Dinu and Liviu P. Dinu. 2005. On the Syllabic
Similarities of Romance Languages. In Proceed-
ings of the 6th International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
CICLing 2005, pages 785?788.
Liviu P. Dinu and Andrea Sgarro. 2006. A Low-
Complexity Distance for DNA Strings. Fundam. In-
form., 73(3):361?372.
Mihai Dinu. 1996. Personalitatea Limbii Rom?ane.
Cartea Rom?aneasc?a.
Mark Durie and Malcolm Ross. 1996. The Compar-
ative Method Reviewed: Regularity and Irregularity
in Language Change. Oxford University Press.
Isidore Dyen, Joseph B. Kruskal, and Paul Black.
1992. An Indoeuropean Classification: a Lexico-
statistical Experiment. Transactions of the Americal
Philosophical Society, 82(5):1?132.
Mihai Eminescu. 1980-1985. Opere. Vol IX-XIII. Pub-
licistic?a. Editura Academiei Rom
?ane.
Oana Frunza, Diana Inkpen, and Grzegorz Kondrak.
2005. Automatic Identification of Cognates and
False Friends in French and English. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, RANLP
2005, pages 251?257.
Ion Ghet?ie. 1978. Istoria Limbii Rom?ane Literare.
Privire Sintetic?a. Editura S?tiint?ific
?a s?i Enciclope-
dic?a.
Charlotte Gooskens, Wilbert Heeringa, and Karin Bei-
jering. 2008. Phonetic and Lexical Predictors of
Intelligibility. International Journal of Humanities
and Arts Computing, 2(1-2):63?81.
Charlotte Gooskens. 2007. The Contribution of Lin-
guistic Factors to the Intelligibility of Closely Re-
lated Languages. Journal of Multilingual and Mul-
ticultural Development, 28(6):445.
Alexandru Graur. 1968. Tendint?ele Actuale ale Limbii
Rom?ane. Editura S?tiint?ific
?a.
Cristian Grozea. 2012. Experiments and Results with
Diacritics Restoration in Romanian. In Proceedings
of the 15th International Conference on Text, Speech
and Dialogue, TSD 2012, pages 199?206.
Jan Haji?c, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Conference on Applied Natural
Language Processing, ANLC 2000, pages 7?12.
Brian D. Joseph. 1999. Romanian and the Balkans:
Some Comparative Perspectives. In Sheila Emble-
ton, John E. Joseph, and Hans-Joseph Niederehe, ed-
itors, The Emergence of the Modern Language Sci-
ences. John Benjamins Publishing Company.
1057
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit, AAMT 2005,
pages 79?86.
Grzegorz Kondrak. 2001. Identifying Cognates by
Phonetic and Semantic Similarity. In Proceedings
of the 2nd Meeting of the North American Chapter
of the Association for Computational Linguistics on
Language Technologies, NAACL 2001, pages 1?8.
Moshe Koppel and Noam Ordan. 2011. Translationese
and Its Dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, HLT
2011, pages 1318?1326.
Vladimir I. Levenshtein. 1965. Binary Codes Capable
of Correcting Deletions, Insertions, and Reversals.
Soviet Physics Doklady, 10:707?710.
April McMahon and Robert McMahon. 2003. Finding
Families: Quantitative Methods in Language Clas-
sification. Transactions of the Philological Society,
101(1):7?55.
Dan Melamed. 1995. Automatic Evaluation and Uni-
form Filter Cascades for Inducing N-Best Transla-
tion Lexicons. In Proceedings of the 3rd Workshop
on Very Large Corpora, pages 184?198.
R Core Team, 2014. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Taraka Rama and Lars Borin. 2014. Comparative
Evaluation of String Similarity Measures for Auto-
matic Language Classification. In George K. Mikros
and J?an Macutek, editors, Sequences in Language
and Text. De Gruyter Mouton.
Don Ringe, Ann Taylor, and Tandy Warnow.
2002. Indo-European and Computational Cladistics.
Transactions of the Philological Society, 100(1):59?
129.
Marius Sala. 1988. Vocabularul Reprezentativ al Lim-
bilor Romanice. Editura Academiei.
Kevin Scannell. 2006. Machine Translation for
Closely Related Language Pairs. In Proceedings
of the Workshop on Strategies for Developing Ma-
chine Translation for Minority Languages, pages
103?107.
David J Sheskin. 2003. Handbook of Parametric
and Nonparametric Statistical Procedures. Chap-
man and Hall/CRC Press.
1058
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 524?528,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Learning How to Conjugate the Romanian Verb. Rules for Regular and
Partially Irregular Verbs
Liviu P. Dinu
Faculty of Mathematics
and Computer Science
University of Bucharest
ldinu@fmi.unibuc.ro
Vlad Niculae
Faculty of Mathematics
and Computer Science
University of Bucharest
vlad@vene.ro
Octavia-Maria S, ulea
Faculty of Foreign Languages
and Literatures
Faculty of Mathematics
and Computer Science
University of Bucharest
mary.octavia@gmail.com
Abstract
In this paper we extend our work described
in (Dinu et al 2011) by adding more con-
jugational rules to the labelling system in-
troduced there, in an attempt to capture
the entire dataset of Romanian verbs ex-
tracted from (Barbu, 2007), and we em-
ploy machine learning techniques to predict
a verb?s correct label (which says what con-
jugational pattern it follows) when only the
infinitive form is given.
1 Introduction
Using only a restricted group of verbs, in (Dinu
et al 2011) we validated the hypothesis that pat-
terns can be identified in the conjugation of the
Romanian (partially irregular) verb and that these
patterns can be learnt automatically so that, given
the infinitive of a verb, its correct conjugation
for the indicative present tense can be produced.
In this paper, we extend our investigation to the
whole dataset described in (Barbu, 2008) and at-
tempt to capture, beside the general ending pat-
terns during conjugation, as much of the phono-
logical alternations occuring in the stem of verbs
(apophony) from the dataset as we can.
Traditionally, Romanian has received a Latin-
inspired classification of verbs into 4 (or some-
times 5) conjugational classes based on the ending
of their infinitival form alone (Costanzo, 2011).
However, this infinitive-based classification has
proved itself inadequate due to its inability to ac-
count for the behavior of partially irregular verbs
(whose stems have a smaller number of allo-
morphs than the completely irregular) during their
conjugation.
There have been, thus, numerous attempts
throughout the history of Romanian Linguistics
to give other conjugational classifications based
on the way the verb actually conjugates. Lom-
bard (1955), looking at a corpus of 667 verbs,
combined the traditional 4 classes with the way in
which the biggest two subgroups conjugate (one
using the suffix ?ez?, the other ?esc?) and ar-
rived at 6 classes. Ciompec (Ciompec et. al.,
1985 in Costanzo, 2011) proposed 10 conjuga-
tional classes, while Felix (1964) proposed 12,
both of them looking at the inflection of the verbs
and number of allomorphs of the stem. Romalo
(1968, p. 5-203) produced a list of 38 verb types,
which she eventually reduced to 10.
For the purpose of machine translation, Moisil
(1960) proposed 5 regrouped classes of verbs,
with numerous subgroups, and introduced the
method of letters with variable values, while Pa-
pastergiou et al(2007) have recently developed
a classification from a (second) language acquisi-
tion point of view, dividing the 1st and 4th tradi-
tional classes into 3 and respectively 5 subclasses,
each with a different conjugational pattern, and
offering rules for alternations in the stem.
Of the more extensive classifications, Barbu
(2007) distinguished 41 conjugational classes for
all tenses and 30 for the indicative present alone,
covering a whole corpus of more that 7000 con-
temporary Romanian verbs, a corpus which was
also used in the present paper. However, her
classes were developed on the basis of the suf-
fixes each verb receives during conjugation, and
the classification system did not take into account
the alternations occuring in the stem of irregular
and partially irregular verbs. The system of rules
presented below took into account both the end-
ings pattern and the type of stem alternation for
each verb.
In what follows we describe our method for la-
beling the dataset and finding a model able to pre-
524
dict the labels.
2 Approach
The problem which we are aiming to solve is to
determine how to conjugate a verb, given its in-
finitive form. The traditional infinitive-based clas-
sification taught in school does not take one all the
way to solving this problem. Many conjugational
patterns exist within each of these four classes.
2.1 Labeling the dataset
Following our own observations, the alternations
identified in (Papastergiou et al 2007) and the
classes of suffix patterns given in (Barbu, 2007),
we developed a number of conjugational rules
which were narrowed down to the 30 most pro-
ductive in relation to the dataset. Each of these
30 rules (or patterns) contains 6 regular expres-
sions through which the rule models how a (dif-
ferent) type of Romanian verb conjugates in the
indicative present. They each consist of 6 reg-
ular expressions because there are three persons
(first, second, and third) times two numbers (sin-
gular and plural).
Rule 10, for example, models, as stated in
the list that follows, how verbs of the type
?a ca?nta? (to sing) conjugate in the indicative
present, by having the first regular expression
model the first person singular form ?(eu) ca?nt?
(in regular expression format: ?(.+)$), the sec-
ond, model the second person singular form ?(tu)
ca?nt?i? (?(.+)t?i$), the third, model the third per-
son singular form ?(ei) ca?nta?? (?(.+)a?$), and so
forth. Thus, rule 10 catches the alternation t?t?
for the 2nd person singular, while modelling a
particular type of verb class with a particular set
of suffixes. Note that the dot accepts any letter
in the Romanian alphabet and that, for each of
the six forms, the value of the capturing groups
(those between brackets) remains constant, in this
case ca?n. These groups correspond to all parts of
the stem that remain unchanged and ensure that,
given the infinitive and the regular expressions,
one can work backwards and produce the correct
conjugation.
For a clearer understanding of one such rule,
Table 1 shows an example of how the verb ?a
tresa?lta? is modeled by rule 14.
Below, we list all the rules used, with the stem
alternations they capture and an example of a verb
Person Regexp Example
1st singular ?(.+)a(.+)t$ tresalt
2nd singular ?(.+)a(.+)t?i$ tresalt?i
3rd singular ?(.+)a(.+)ta?$ tresalta?
1st plural ?(.+)a?(.+)ta?m$ tresa?lta?m
2nd plural ?(.+)a?(.+)tat?i$ tresa?ltat?i
3rd plural ?(.+)a(.+)ta?$ tresalta?
Table 1: Rule 14 modelling ?a tresa?lta?
that they model. Note that, when we say (no) al-
ternation, we mean (no) alternation in the stem.
So the difference between rules 1, 20, 22, and the
sort lies in the suffix that is added to the stem
for each verb form. They may share some suf-
fixes, but not all and/or not for the same person
and number.
1. no alternation; ?a spera? (to hope);
2. alternation: a??e for the 2nd person singular;
?a numa?ra? (to count);
3. no alternation; ?a intra? (to enter), stem ends
in ?tr?, ?pl?, ?bl? or ?fl? which determines
the addition of ?u? at the end of the 1st per-
son singular form;
4. alternation: it lacks t?t? for the 2nd person
singular, which otherwise normally occurs;
?a mis?ca? (to move), stem ends in ?s?ca?;
5. no alternation; ?a ta?ia? (to cut), ends in ?ia?
and has a vowel before;
6. no alternation; ?a speria? (to scare), ends in
?ia? and has a consonant before;
7. no alternation; ?a dansa? (to dance), conju-
gated with the suffix ?ez?;
8. no alternation; ?a copia? (to copy), conju-
gated with a modified ?ez? due to the stem
ending in ?ia?;
9. altenation c?ch(e) or g?gh(e); ?a parca?
(to park), conjugated with ?ez?, ending in
?ca? or ?ga?;
10. alternation: t?t? for the 2nd person singular;
?a ca?nta? (to sing);
11. alternation: s?s? which replaces the usual
t?t? for the 2nd person singular; ?a exista?
(to exist);
525
12. alternation: a?ea for the 3rd person singular
and plural, t?t? for the 2nd person singular;
?a des?tepta? (to awake/arouse);
13. alternation: e?ea for the 3rd person singular
and plural, t?t? for the 2nd person singular;
?a des?erta? (to empty);
14. alternation: a??a for all the forms except the
1st and 2nd person plural; ?a tresa?lta? (to
start, to take fright);
15. alternation: a??a in the 3rd person singular
and plural, a??e in the 2nd person singular;
?a desfa?ta? (to delight);
16. alternation: a??a for all the forms except for
the 1st and 2nd person plural; ?a pa?rea? (to
seem);
17. alternation: d?z for the 2nd person singu-
lar due to palatalization, along with a??e; ?a
vedea? (to see), stem ends in ?d?;
18. alternation: a??a for all forms except the 1st
and 2nd person plural, d?z for the 2nd per-
son singular due to palatalization; ?a ca?dea?
(to fall);
19. no alternation; ?a veghea? (to watch over),
conjugates with another type of ?ez? ending
pattern;
20. no alternations; ?a merge? (to walk), receives
the typical ending pattern for the third conju-
gational class;
21. alternation: t?t? for the 2nd person singular;
?a promite? (to promise);
22. no alternation; ?a scrie? (to write);
23. alternations: s?t?sc for the 1st person singu-
lar and 3rd person plural; ?a nas?te? (to give
birth), ends in ?s?te?;
24. alternation: ?n? is deleted from the stem in
the 2nd person singular; ?a pune? (to put),
ends in ?ne?;
25. alternation: d?z in the 2nd person singular
due to palatalization; ?a crede? (to believe),
stem ends in ?d?;
26. no alternation; ?a sui? (to climb), ends in
?ui?, ?a?i?, or ?a?i?;
27. no alternation; ?a citi? (to read), conjugates
with the suffix ?esc? ;
28. this type preserves the ?i? from the infinitive;
?a locui? (to reside), ends in ?a?i?, ?oi?, or ui?
and conjugates with ?esc?;
29. alternation: o?oa in the 3rd person singular
and plural; end in ????, ?a omor??? (to kill);
30. no alternation; ?a hota?r??? (to decide), ends in
???? and conjugates with ?a?sc?, a variant of
?esc?
2.2 Classifiers and features
Each infinitive in the dataset received a label cor-
responding to the first rule that correctly produces
a conjugation for it. This was implemented in
order to reduce the ambiguity of the data, which
was due to some verbs having alternate conjuga-
tion patterns. The unlabeled verbs were thrown
out, while the labeled ones were used to train and
evaluate a classifier.
The context sensitive nature of the alternations
leads to the idea that n-gram character windows
are useful. In the preprocessing step, the list of in-
finitives is transformed to a sparse matrix whose
lines correspond to samples, and whose features
are the occurence or the frequency of a specific n-
gram. This feature extraction step has three free
parameters: the maximum n-gram length, the op-
tional binarization of the features (taking only bi-
nary occurences instead of counts), and the op-
tional appending of a terminator character. The
terminator character allows the classifier to iden-
tify and assign a different weight to the n-grams
that overlap with the suffix of the string.
For example, consider the English infinitive to
walk. We will assume the following illustrative
values for the parameters: n-gram size of 3 and
appending the terminator character. Firstly, a ter-
minator is appended to the end, yielding the string
walk$. Subsequently, the string is broken into 1, 2
and 3-grams: w, a, l, k, $, wa, al, lk, k$, wal, alk,
lk$. Next, this list is turned into a vector using a
standard process. We have first built a dictionary
of all the n-grams from the whole dataset. These,
in order, encode the features. The verb (to) walk
is therefore encoded as a row vector with ones in
the columns corresponding to the features w, a,
etc. and zeros in the rest. In this particular case,
there is no difference between binary and count
526
rule no. verbs
1 547
2 8
3 18
4 5
5 8
6 16
7 3330
8 273
9 89
10 4
11 5
12 4
13 106
14 13
15 5
rule no. verbs
16 13
17 6
18 4
19 14
20 124
21 25
22 15
23 7
24 41
25 51
26 185
27 1554
28 486
29 5
30 27
Table 2: Number of verbs captured by each of our rules
features because all of the n-grams of this short
verb occur only once. But for a verb such as (to)
tantalize, the feature corresponding to the 2-gram
ta would get a value of 2 in a count reprezentation,
but only a value of 1 in a binary one.
The system was put together using the scikit-
learn machine learning library for Python (Pe-
dregosa et al 2011), which provides a fast, scal-
able implementation of linear support vector ma-
chines based on liblinear (Fan et al 2008), along
with n-gram extraction and grid search function-
ality.
3 Results
Tabel 2 shows how well the rules fitted the dataset.
Out of 7,295 verbs in the dataset, 349 were uncap-
tured by our rules. As expected, the rule capturing
the most verbs (3,330) is the one modelling those
from the 1st conjugational class (whose infinitives
end in ?a?) which conjugate with the ?ez? suffix
and are regular, namely rule 7, created for verbs
like ?a dansa?. The second largest class, also as
expected, is the one belonging to verbs from the
4th conjugational group (whose infinitives end in
?i?), which are regular, meaning no alternation in
the stem, and conjugate with the ?esc? suffix. This
class is modeled by rule number 27.
The support vector classifier was evaluated
using a 10-fold cross-validation. The multi-
class problem is treated using the one-versus-all
scheme. The parameters chosen by grid search are
a maximum n-gram length of 5, with appended
terminator and with non-binarized (count) fea-
tures. The estimated correct classification rate is
90.64%, with a weighted averaged precision of
80.90%, recall of 90.64% andF1 score of 89.89%.
Appending the artificial terminator character ?$?
consistently improves accuracy by around 0.7%.
Because each word was represented as a bag of
character n-grams instead of a continuous string,
and because, by its nature, a SVM yields sparse
solutions, combined with the evaluation using
cross-validation, we can safely say that the model
does not overfit and indeed learns useful decision
boundaries.
4 Conclusions and Future Works
Our results show that the labelling system based
on the verb conjugation model we developed can
be learned with reasonable accuracy. In the future,
we plan to develop a multiple tiered labelling sys-
tem that will allow for general alternations, such
as the ones occuring as a result of palatalization,
to be defined only once for all verbs that have
them, taking cues from the idea of letters with
multiple values. This, we feel, will highly im-
prove the acuracy of the classifier.
5 Acknowledgements
The authors would like to thank the anonymous
reviewers for their helpful comments. All authors
contributed equally to this work. The research of
Liviu P. Dinu was supported by the CNCS, IDEI
- PCE project 311/2011, ?The Structure and In-
terpretation of the Romanian Nominal Phrase in
Discourse Representation Theory: the Determin-
ers.?
References
Ana-Maria Barbu. Conjugarea verbelor roma?-
nes?ti. Dict?ionar: 7500 de verbe roma?nes?ti gru-
pate pe clase de conjugare. Bucharest: Coresi,
2007. 4th edition, revised. (In Romanian.) (263
pp.).
Ana-Maria Barbu. Romanian lexical databases:
Inflected and syllabic forms dictionaries. In
Sixth International Language Resources and
Evaluation (LREC?08), 2008.
Angelo Roth Costanzo. Romance Conjugational
Classes: Learning from the Peripheries. PhD
thesis, Ohio State University, 2011.
527
Figure 1: 10-fold cross validation scores for various combination of parameters. Only the values corresponding
to the best C regularization parameters are shown.
Liviu P. Dinu, Emil Ionescu, Vlad Niculae, and
Octavia-Maria S?ulea. Can alternations be
learned? a machine learning approach to verb
alternations. In Recent Advances in Natural
Language Processing 2011, September 2011.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. Liblinear:
A library for large linear classification. Journal
of Machine Learning Research, 9:1871?1874,
June 2008. ISSN 1532-4435.
Jir?i Felix. Classification des verbes roumains, vol-
ume VII. Philosophica Pragensia, 1964.
Alf Lombard. Le verbe roumain. Etude mor-
phologique, volume 1. Lund, C. W. K. Gleerup,
1955.
Grigore C. Moisil. Probleme puse de traduc-
erea automata?. conjugarea verbelor ??n limba
roma?na?. Studii si cerceta?ri lingvistice, XI(1):
7?29, 1960.
I. Papastergiou, N. Papastergiou, and L. Man-
deki. Verbul roma?nesc - reguli pentru ??nlesnirea
??nsus?irii indicativului prezent. In Romanian
National Symposium ?Directions in Roma-
nian Philological Research?, 7th Edition, May
2007.
F. Pedregosa, G. Varoquaux, A. Gramfort,
V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Jour-
nal of Machine Learning Research, 12:2825?
2830, Oct 2011.
Valeria Gut?u Romalo. Morfologie Structurala? a
limbii roma?ne. Editura Academiei Republicii
Socialiste Roma?nia, 1968.
528
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17?21,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Temporal Text Ranking and Automatic Dating of Texts
Vlad Niculae
1
, Marcos Zampieri
2
, Liviu P. Dinu
3
, Alina Maria Ciobanu
3
Max Planck Institute for Software Systems, Germany
1
Saarland University, Germany
2
Center for Computational Linguistics, University of Bucharest, Romania
3
vniculae@mpi-sws.org, marcos.zampieri@uni-saarland.de,
ldinu@fmi.unibuc.ro, alina.ciobanu@my.fmi.unibuc.ro
Abstract
This paper presents a novel approach to
the task of temporal text classification
combining text ranking and probability for
the automatic dating of historical texts.
The method was applied to three histor-
ical corpora: an English, a Portuguese
and a Romanian corpus. It obtained per-
formance ranging from 83% to 93% ac-
curacy, using a fully automated approach
with very basic features.
1 Introduction
Temporal text classification is an underexplored
problem in NLP, which has been tackled as a
multi-class problem, with classes defined as time
intervals such as months, years, decades or cen-
turies. This approach has the drawback of having
to arbitrarily delimit the intervals, and often leads
to a model that is not informative for texts written
within such a window. If the predefined window is
too large, the output is not useful for most systems;
if the window is too small, learning is impractical
because of the large number of classes. Particu-
larly for the problem of historical datasets (as the
one we propose here), learning a year-level classi-
fier would not work, because each class would be
represented by a single document.
Our paper explores a solution to this drawback
by using a ranking approach. Ranking amounts to
ordering a set of inputs with respect to some mea-
sure. For example, a search engine ranks returned
documents by relevance. We use a formalization
of ranking that comes from ordinal regression, the
class of problems where samples belong to inher-
ently ordered classes.
This study is of interest to scholars who deal
with text classification and NLP in general; his-
torical linguists and philologists who investigate
language change; and finally scholars in the dig-
ital humanities who often deal with historical
manuscripts and might take advantage of temporal
text classification applications in their research.
2 Related Work
Modelling temporal information in text is a rele-
vant task for a number of NLP tasks. For example,
in Information Retrieval (IR) research has been
concentrated on investigating time-sensitivity doc-
ument ranking (Dakka and Gravana, 2010). Even
so, as stated before, temporal text classification
methods were not substantially explored as other
text classification tasks.
One of the first studies to model temporal infor-
mation for the automatic dating of documents is
the work of de Jong et al. (2005). In these exper-
iments, authors used unigram language models to
classify Dutch texts spanning from January 1999
to February 2005 using normalised log-likelihood
ratio (NLLR) (Kraaij, 2004). As to the features
used, a number of approaches proposed to auto-
matic date take into account lexical features (Dalli
and Wilks, 2006; Abe and Tsumoto, 2010; Ku-
mar et al., 2011) and a few use external linguistic
knowledge (Kanhabua and N?rv?ag, 2009).
A couple of approaches try to classify texts not
only regarding the time span in which the texts
were written, but also their geographical location
such as (Mokhov, 2010) for French and, more re-
cently, (Trieschnigg et al., 2012) for Dutch. At the
word level, two studies aim to model and under-
stand how word usage and meaning change over
time (Wijaya and Yeniterzi, 2011), (Mihalcea and
Nastase, 2012).
The most recent studies in temporal text classifi-
cation to our knowledge are (Ciobanu et al., 2013)
for Romanian using lexical features and (
?
Stajner
and Zampieri, 2013) for Portuguese using stylistic
and readability features.
17
3 Methods
3.1 Corpora
To evaluate the method proposed here we used
three historical corpora. An English historical
corpus entitled Corpus of Late Modern English
Texts (CLMET)
1
(de Smet, 2005), a Portuguese
historical corpus entitled Colonia
2
(Zampieri and
Becker, 2013) and a Romanian historical corpus
(Ciobanu et al., 2013).
CLMET is a collection of English texts derived
from the Project Gutenberg and from the Oxford
Text Archive. It contains around 10 million to-
kens, divided over three sub-periods of 70 years.
The corpus is available for download as raw text
or annotated with POS annotation.
For Portuguese, the aforementioned Colonia
(Zampieri and Becker, 2013) is a diachronic col-
lection containing a total of 5.1 million tokens and
100 texts ranging from the 16
th
to the early 20
th
century. The texts in Colonia are balanced be-
tween European and Brazilian Portuguese (it con-
tains 52 Brazilian texts and 48 European texts) and
the corpus is annotated with lemma and POS in-
formation. According to the authors, some texts
presented edited orthography prior to their com-
pilation but systematic spelling normalisation was
not carried out.
The Romanian corpus was compiled to portrait
different stages in the evolution of the Romanian
language, from the 16
th
to the 20
th
century in a
total of 26 complete texts. The methodology be-
hind corpus compilation and the date assignment
are described in (Ciobanu et al., 2013).
3.2 Temporal classification as ranking
We propose a temporal model that learns a linear
function g(x) = w ? x to preserve the temporal or-
dering of the texts, i.e. if document
3
x
i
predates
document x
j
, which we will henceforth denote as
x
i
? x
j
, then g(x
i
) < g(x
j
). Such a problem is
often called ranking or learning to rank. When the
goal is to recover contiguous intervals that corre-
spond to ordered classes, the problem is known as
ordinal regression.
We use a pairwise approach to ranking that re-
duces the problem to binary classification using a
1
https://perswww.kuleuven.be/
?
u0044428/clmet
2
http://corporavm.uni-koeln.de/
colonia/
3
For brevity, we use x
i
to denote both the document itself
and its representation as a feature vector.
linear model. The method is to convert a dataset
of the form D = {(x, y) : x ? R
d
, y ? Y} into a
pairwise dataset:
D
p
= {((x
i
, x
j
), I[y
i
< y
j
]) :
(x
i
, y
i
), (x
j
, y
j
) ? D}
Since the ordinal classes only induce a partial or-
dering, as elements from the same class are not
comparable, D
p
will only consist of the compara-
ble pairs.
The problem can be turned into a linear classifi-
cation problem by noting that:
w ? x
i
< w ? x
j
?? w ? (x
i
? x
j
) < 0
In order to obtain probability values for the or-
dering, we use logistic regression as the linear
model. It therefore holds that:
P(x
i
? x
j
;w) =
1
1 + exp(?w ? (x
i
? x
j
))
While logistic regression usually fits an inter-
cept term, in our case, because the samples consist
of differences of points, the model operates in an
affine space and therefore gains an extra effective
degree of freedom. The intercept is therefore not
needed.
The relationship between pairwise ranking and
predicting the class from an ordered set {r
1
, ...r
k
}
is given by assigning to a document x the class r
i
such that
?(r
i?1
) ? g(x) < ?(r
i
) (1)
where ? is an increasing function that does not
need to be linear. (Pedregosa et al., 2012), who
used the pairwise approach to ordinal regression
on neuroimaging prediction tasks, showed using
artificial data that ? can be accurately recovered
using non-parametric regression. In this work, we
use a parametric estimation of ? that can be used
in a probabilistic interpretation to identify the most
likely period when a text was written, as described
in section 3.3.
3.3 Probabilistic dating of uncertain texts
The ranking model described in the previous sec-
tion learns a direction along which the temporal
order of texts is preserved as much as possible.
This direction is connected to the chronological
axis through the ? function. For the years t for
18
which we have an unique attested document x
t
,
we have that
x ? x
t
?? g(x) < g(x
t
) < ?(t)
This can be explained by seeing that equation 2
gives ?(t) as an upper bound for the projections of
all texts written in year t, and by transitivity for all
previous texts as well.
Assuming we can estimate the function ? with
another function
?
?, the cumulative densitiy func-
tion of the distribution of the time when an unseen
document was written can be expressed.
P (x ? t) ?
1
1 + exp(w ? x?
?
?(t))
(2)
Setting the probability to
1
2
provides a point es-
timate of the time when x was written, and confi-
dence intervals can be found by setting it to p and
1? p.
3.4 Features
Our ranking and estimation model can work with
any kind of numerical features. For simplicity
we used lexical and naive morphological features,
pruned using ?
2
feature selection with tunable
granularity.
The lexical features are occurrence counts of all
words that appear in at least p
lex
documents. The
morphological features are counts of character n-
grams of length up to w
mph
in final positions of
words, filtered to occur in at least n
mph
documents.
Subsequently, a non-linear transformation ? is
optionally applied to the numerical features. This
is one of ?
sqrt
(z) =
?
z, ?
log
(z) = log(z) or
?
id
(z) = z (no transformation).
The feature selection step is applied before gen-
erating the pairs for classification, in order for the
?
2
scoring to be applicable. The raw target val-
ues used are year labels, but to avoid separating
almost every document in its own class, we in-
troduce a granularity level that transforms the la-
bels into groups of n
gran
years. For example, if
n
gran
= 10 then the features will be scored ac-
cording to how well they predict the decade a doc-
ument was written in. The features in the top p
fsel
percentile are kept. Finally, C is the regulariza-
tion parameter of the logistic regression classifier,
as defined in liblinear (Fan et al., 2008).
0.2 0.4 0.6 0.8 1.00.72
0.74
0.76
0.78
0.80
0.82
0.84
RidgeRanking
0.6 0.7 0.8 0.9 1.00.78
0.79
0.80
0.81
0.82
0.83
RidgeRanking
Figure 1: Learning curves for English (top) and
Portuguese (bottom). Proportion of training set
used versus score.
4 Results
Each corpus is split randomly into training and test
sets with equal number of documents. The best
feature set is chosen by 3-fold cross-validated ran-
dom search over a large grid of possible configu-
rations. We use random search to allow for a more
efficient exploration of the parameter space, given
that some parameters have much less impact to the
final score than others.
The evaluation metric we used is the percentage
of non-inverted (correctly ordered) pairs, follow-
ing (Pedregosa et al., 2012).
We compare the pairwise logistic approach to
a ridge regression on the same feature set, and
two multiclass SVMs, at century and decade level.
While the results are comparable with a slight ad-
vantage in favour of ranking, the pairwise ranking
system has several advantages. On the one hand, it
provides the probabilistic interpretation described
in section 3.3. On the other hand, the model can
naturally handle noisy, uncertain or wide-range la-
bels, because annotating whether a text was writ-
ten before another can be done even when the texts
do not correspond to punctual moments in time.
While we do not exploit this advantage, it can lead
to more robust models of temporal evolution. The
learning curves in Figure 1 further show that the
pairwise approach can better exploit more data and
nonlinearity.
The implementation is based on the scikit-learn
machine learning library for Python (Pedregosa et
al., 2011) with logistic regression solver from (Fan
et al., 2008). The source code will be available.
4.1 Uncertain texts
We present an example of using the method from
Section 3.3 to estimate the date of uncertain, held-
out texts of historical interest. Figure 2 shows the
process used for estimating ? as a linear, and in
the case of Portuguese, quadratic function. The
19
size p
lex
n
mph
w
mph
? n
gran
p
fsel
C score ridge century decade MAE
en 293 0.9 0 3 ?
log
100 0.15 2
9
0.838 0.837 0.751 0.813 22.8
pt 87 0.9 25 4 ?
sqrt
5 0.25 2
?5
0.829 0.819 0.712 0.620 58.7
ro 42 0.8 0 4 ?
log
5 0.10 2
28
0.929 0.924 0.855 0.792 28.8
Table 1: Test results of the system on the three datasets. The score is the proportion of pairs of docu-
ments ranked correctly. The column ridge is a linear regression model used for ranking, while century
and decade are linear SVMs used to predict the century and the decade of each text, but scored as pair-
wise ranking, for comparability. Chance level is 0.5. MAE is the mean absolute error in years. The
hyperparameters are described in section 3.4.
1650 1700 1750 1800 1850 1900 1950Year
300
200
100
0
100
200
w?x
Linear (33.54)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
40
20
0
20
40
60
w?x
Linear (17.27)Quadratic (15.44)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
100
50
0
50
100
w?x
Linear (1.87)TrainTest
Figure 2: Estimating the function ? that defines the relationship between years and projections of docu-
ments to the direction of the model, for English, Portuguese and Romanian (left to right). In parantheses,
the normalized residual of the least squares fit is reported on the test set.
154
0
156
0
158
0
160
0
162
0
164
0
166
0
168
0
170
0
172
0
174
0
176
0
178
0
180
0
182
0
184
0
186
0
188
0
190
0
192
0
194
0
196
0
198
0
200
0
202
00.2
0.00.2
0.40.6
0.81.0
1.2
Figure 3: Visualisation of the probability esti-
mation for the dating of C. Cantacuzino?s Isto-
ria T
,
?arii Rum?anes
,
ti. The horizontal axis is the
time, the points are known texts with a height
equal to the probability predicted by the classifier.
The dashed line is the estimated probability from
Equation 2.
estimation is refit on all certain documents prior to
plugging into the probability estimation.
The document we use to demonstrate the pro-
cess is Romanian nobleman and historian Con-
stantin Cantacuzino?s Istoria T
,
?arii Rum?anes
,
ti.
The work is believed to be written in 1716, the
year of the author?s death, and published in sev-
eral editions over a century later (Stahl, 2001).
This is an example of the system being reasonably
close to the hypothesis, thus providing linguistic
support to it. Our system gives an estimated dat-
ing of 1744.7 with a 90% confidence interval of
1736.2 ? 1753.2. As publications were signifi-
cantly later, the lexical pull towards the end of 18
th
century that can be observed in Figure 3 could be
driven by possible editing of the original text.
5 Conclusion
We propose a ranking approach to temporal mod-
elling of historical texts. We show how the model
can be used to produce reasonable probabilistic
estimates of the linguistic age of a text, using a
very basic, fully-automatic feature extraction step
and no linguistic or historical knowledge injected,
apart from the labels, which are possibly noisy.
Label noise can be atenuated by replacing un-
certain dates with intervals that are more certain,
and only generating training pairs out of non-
overlapping intervals. This can lead to a more
robust model and can use more data than would
be possible with a regression or classification ap-
proach. The problem of potential edits that a text
has suffered still remains open.
Finally, better engineered and linguistically-
motivated features, such as syntactic, morphologi-
cal or phonetic patterns that are known or believed
to mark epochs in the evolution of a language, can
be plugged in with no change to the fundamental
method.
20
References
H. Abe and S. Tsumoto. 2010. Text categorization
with considering temporal patterns of term usages.
In Proceedings of ICDM Workshops, pages 800?
807. IEEE.
A. Ciobanu, A. Dinu, L. Dinu, V. Niculae, and
O. Sulea. 2013. Temporal text classification for
romanian novels set in the past. In Proceedings of
RANLP2013, Hissar, Bulgaria.
W. Dakka and C. Gravana. 2010. Answering gen-
eral time-sensitive queries. IEEE Transactions on
Knowledge and Data Engineering.
A. Dalli and Y. Wilks. 2006. Automatic dating of doc-
uments and temporal text classification. In Proceed-
ings of the Workshop on Annotating and Reasoning
about Time and Events, pages 17?22, Sidney, Aus-
tralia.
F. de Jong, H. Rode, and D. Hiemstra. 2005. Temporal
language models for the disclosure of historical text.
In Proceedings of AHC 2005 (History and Comput-
ing).
H. de Smet. 2005. A corpus of late modern english.
ICAME-Journal.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
N. Kanhabua and P. N?rv?ag. 2009. Using tem-
poral language models for document dating. In
ECML/PKDD, pages 738?741.
W. Kraaij. 2004. Variations on language modeling
for information retrieval. Ph.D. thesis, University
of Twente.
A. Kumar, M. Lease, and J. Baldridge. 2011. Super-
vised language modelling for temporal resolution of
texts. In Proceedings of CIKM11 of the 20th ACM
international conference on Information and knowl-
edge management, pages 2069?2072.
R. Mihalcea and V. Nastase. 2012. Word epoch dis-
ambiguation: Finding how words change over time.
In Proceedings of ACL, pages 259?263. Association
for Computational Linguistics.
S. Mokhov. 2010. A marf approach to deft2010. In
Proceedings of TALN2010, Montreal, Canada.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Fabian Pedregosa, Alexandre Gramfort, Ga?el Varo-
quaux, Elodie Cauvet, Christophe Pallier, and
Bertrand Thirion. 2012. Learning to rank from
medical imaging data. CoRR, abs/1207.3598.
H.H. Stahl. 2001. G?anditori s?i curente de istorie
social?a rom?aneasc?a. Biblioteca Institutului Social
Rom?an. Ed. Univ. din Bucures?ti.
S.
?
Stajner and M. Zampieri. 2013. Stylistic changes
for temporal text classification. In Proceedings of
the 16th International Conference on Text Speech
and Dialogue (TSD2013), Lecture Notes in Artificial
Intelligence (LNAI), pages 519?526, Pilsen, Czech
Republic. Springer.
D. Trieschnigg, D. Hiemstra, M. Theune, F. de Jong,
and T. Meder. 2012. An exploration of lan-
guage identification techniques for the dutch folktale
database. In Proceedings of LREC2012.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In Proc. of
the Workshop on Detecting and Exploiting Cultural
Diversity on the Social Web (DETECT).
M. Zampieri and M. Becker. 2013. Colonia: Corpus of
historical portuguese. ZSM Studien, Special Volume
on Non-Standard Data Sources in Corpus-Based Re-
search, 5.
21
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 64?68,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Predicting Romanian Stress Assignment
Alina Maria Ciobanu
1,2
, Anca Dinu
1,3
, Liviu P. Dinu
1,2
1
Center for Computational Linguistics, University of Bucharest
2
Faculty of Mathematics and Computer Science, University of Bucharest
3
Faculty of Foreign Languages and Literatures, University of Bucharest
alina.ciobanu@my.fmi.unibuc.ro, anca_d_dinu@yahoo.com, ldinu@fmi.unibuc.ro
Abstract
We train and evaluate two models for Ro-
manian stress prediction: a baseline model
which employs the consonant-vowel struc-
ture of the words and a cascaded model
with averaged perceptron training con-
sisting of two sequential models ? one for
predicting syllable boundaries and another
one for predicting stress placement. We
show in this paper that Romanian stress is
predictable, though not deterministic, by
using data-driven machine learning tech-
niques.
1 Introduction
Romanian is a highly inflected language with a
rich morphology. As dictionaries usually fail to
cover the pronunciation aspects for all word forms
in languages with such a rich and irregular mor-
phology (Sef et al., 2002), we believe that a
data-driven approach is very suitable for syllabi-
cation and stress prediction for Romanian words.
Moreover, such a system proves extremely useful
for inferring syllabication and stress placement for
out-of-vocabulary words, for instance neologisms
or words which recently entered the language.
Even if they are closely related, Romanian
stress and syllabication were unevenly studied in
the computational linguistic literature, i.e., the
Romanian syllable received much more attention
than the Romanian stress (Dinu and Dinu, 2005;
Dinu, 2003; Dinu et al., 2013; Toma et al., 2009).
One possible explanation for the fact that Roma-
nian syllabication was more intensively studied
than Romanian stress is the immediate application
of syllabication to text editors which need reliable
hyphenation. Another explanation could be that
most linguists (most recently Dindelegan (2013))
insisted that Romanian stress is not predictable,
thus discouraging attempts to investigate any sys-
tematic patterns.
Romanian is indeed a challenging case study,
because of the obvious complexities of the data
with respect to stress assignment. At first sight, no
obvious patterns emerge for learning stress place-
ment (Dindelegan, 2013), other than as part of in-
dividual lexical items. The first author who chal-
lenges this view is Chitoran (2002), who argues in
favor of the predictability of the Romanian stress
system. She states that stress placement strongly
depends on the morphology of the language, more
precisely on the distribution of the lexical items
based on their part of speech (Chitoran, 1996).
Thus, considering this type of information, lexical
items can be clustered in a limited number of re-
gular subpatterns and the unpredictability of stress
placement is significantly reduced. A rule-based
method for lexical stress prediction on Romanian
was introduced by Oancea and Badulescu (2002).
Dou et al. (2009) address lexical stress predic-
tion as a sequence tagging problem, which proves
to be an accurate approach for this task. The
effectiveness of using conditional random fields
for orthographic syllabication is investigated by
Trogkanis and Elkan (2010), who employ them
for determining syllable boundaries and show that
they outperform previous methods. Bartlett et
al. (2008) use a discriminative tagger for auto-
matic orthographic syllabication and present seve-
ral approaches for assigning labels, including the
language-independent Numbered NB tag scheme,
which labels each letter with a value equal to the
distance between the letter and the last syllable
boundary. According to Damper et al. (1999), syl-
lable structure and stress pattern are very useful in
text-to-speech synthesis, as they provide valuable
knowledge regarding the pronunciation modeling.
Besides converting the letters to the corresponding
phonemes, information about syllable boundaries
and stress placement is also needed for the correct
synthesizing of a word in grapheme-to-phoneme
conversion (Demberg et al., 2007).
64
In this paper, we rely on the assumption that the
stress system of Romanian is predictable. We pro-
pose a system for automatic prediction of stress
placement and we investigate its performance by
accounting for several fine-grained characteristics
of Romanian words: part of speech, number of
syllables and consecutive vowels. We investigate
the consonant-vowel structure of the words (C/V
structure) and we detect a high number of stress
patterns. This calls for the need of machine learn-
ing techniques, in order to automatically learn
such a wide range of variational patterns.
2 Approach
We address the task of stress prediction for Roma-
nian words (out-of-context) as a sequence tagging
problem. In this paper, we account only for the pri-
mary stress, but this approach allows further deve-
lopment in order to account for secondary stress
as well. We propose a cascaded model consist-
ing of two sequential models trained separately,
the output of the first being used as input for the
second. We use averaged perceptron for parame-
ter estimation and three types of features which are
described in detail further in this section: n-grams
of characters, n-grams marking the C/V structure
of the word and binary positional indicators of the
current character with respect to the syllable struc-
ture of the word. We use one sequential model
to predict syllable boundaries and another one to
predict stress placement. Previous work on or-
thographic syllabication for Romanian (Dinu et
al., 2013) shows that, although a rule-based algo-
rithm models complex interactions between fea-
tures, its practicality is limited. The authors re-
port experiments on a Romanian dataset, where
the rule-based algorithm is outperformed by an
SVM classifier and a CRF system with character
n-gram features.
We use a simple tagging structure for mar-
king primary stress. The stressed vowel re-
ceives the positive tag 1, while all previous cha-
racters are tagged 0 and all subsequent ones
2. This structure helps enforce the uniqueness
of the positive tag. The main features used
are character n-grams up to n = W in a win-
dow of radius W around the current position.
For example, if W = 2, the feature template
consists of c[-2], c[-1], c[0], c[1], c[2],
c[-2:-1], c[-1:0], c[0:1], c[1:2]. If the
current letter is the fourth of the word dinosaur,
o, the feature values would be i, n, o, s, a, in, no,
os, sa. We use two additional types of features:
? features regarding the C/V structure of the
word: n-grams using, instead of characters,
markers for consonants (C) and vowels (V);
? binary indicators of the following positional
statements about the current character, re-
lated to the statistics reported in Table 1:
? exactly before/after a split;
? in the first/second/third/fourth syllable
of the word, counting from left to right;
? in the first/second/third/fourth syllable
of the word, counting from right to left
The syllabication prediction is performed with
another sequential model of length n? 1, where
each node corresponds to a position between two
characters. Based on experimenting and previ-
ous work, we adopted the Numbered NB labeling.
Each position is labeled with an integer denoting
the distance from the previous boundary. For ex-
ample, for the word diamond, the syllable (above)
and stress annotations (below) are as follows:
d i a m o n d
1 0 0 1 2 3
0 1 2 2 2 2 2
The features used for syllabication are based on
the same principle, but because the positions are
in-between characters, the window of radius W
has length 2W instead of 2W + 1. For this model
we used only character n-grams as features.
3 Data
We run our experiments for Romanian using the
RoSyllabiDict (Barbu, 2008) dictionary, which is
a dataset of annotated words comprising 525,528
inflected forms for approximately 65,000 lemmas.
This is, to our best knowledge, the largest experi-
ment conducted and reported for Romanian so far.
For each entry, the syllabication and the stressed
vowel (and, in case of ambiguities, also grammat-
ical information or type of syllabication) are pro-
vided. For example, the word copii (children) has
the following representation:
<form w="copii" obs="s."> co-p?i</form>
We investigate stress placement with regard to
the syllable structure and we provide in Table 1
the percentages of words having the stress placed
on different positions, counting syllables from the
beginning and from the end of the words as well.
For our experiments, we discard words which
do not have the stressed vowel marked, compound
65
Syllable %words
1
st
5.59
2
nd
18.91
3
rd
39.23
4
th
23.68
5
th
8.52
(a) counting syllables from
the beginning of the word
Syllable %words
1
st
28.16
2
nd
43.93
3
rd
24.14
4
th
3.08
5
th
0.24
(b) counting syllables from
the end of the word
Table 1: Stress placement for RoSyllabiDict
words having more than one stressed vowel and
ambiguous words (either regarding their part of
speech or type of syllabication).
We investigate the C/V structure of the words in
RoSyllabiDict using raw data, i.e., a, a?, ?, e, i, ?, o,
u are always considered vowels and the rest of the
letters in the Romanian alphabet are considered
consonants. Thus, we identify a very large number
of C/V structures, most of which are not determin-
istic with regard to stress assignment, having more
then one choice for placing the stress
1
.
4 Experiments and Results
In this section we present the main results drawn
from our research on Romanian stress assignment.
4.1 Experiments
We train and evaluate a cascaded model consist-
ing of two sequential models trained separately,
the output of the first being used as input to the
second. We split the dataset in two subsets: train
set (on which we perform cross-validation to se-
lect optimal parameters for our model) and test
set (with unseen words, on which we evaluate the
performance of our system). We use the same
train/test sets for the two sequential models, but
they are trained independently. The output of the
first model (used for predicting syllabication) is
used for determining feature values for the second
one (used for predicting stress placement) for the
test set. The second model is trained using gold
syllabication (provided in the dataset) and we re-
port results on the test set in both versions: us-
ing gold syllabication to determine feature values
1
For example, for CCV-CVC structure (1,390 occurrences
in our dataset) there are 2 associated stress patterns: CCV-
CVC (1,017 occurrences) and CCV-CVC (373 occurrences).
Words with 6 syllables cover the highest number of distinct
C/V structures (5,749). There are 31 C/V structures (rang-
ing from 4 to 7 syllables) reaching the maximum number of
distinct associated stress patterns (6).
and using predicted syllabication to determine fea-
ture values. The results with gold syllabication
are reported only for providing an upper bound for
learning and for comparison.
We use averaged perceptron training (Collins,
2002) from CRFsuite (Okazaki, 2007). For the
stress prediction model we optimize hyperparam-
eters using grid search to maximize the 3-fold
cross-validation F
1
score of class 1, which marks
the stressed vowels. We searched over {2,3,4}
for W and over {1,5,10,25,50} for the maximum
number of iterations. The values which optimize
the system are 4 for W and 50 for the maximum
number of iterations. We investigate, during grid
search, whether employing C/V markers and bi-
nary positional indicators improve our system?s
performance. It turns out that in most cases they
do. For the syllabication model, the optimal hy-
perparameters are 4 for the window radius and 50
for the maximum number of iterations. We evalu-
ate the cross-validation F
1
score of class 0, which
marks the position of a hyphen. The system ob-
tains 0.995 instance accuracy for predicting sylla-
ble boundaries.
We use a "majority class" type of baseline
which employs the C/V structures described in
Section 3 and assigns, for a word in the test set,
the stress pattern which is most common in the
training set for the C/V structure of the word, or
places the stress randomly on a vowel if the C/V
structure is not found in the training set
2
. The per-
formance of both models on RoSyllabiDict dataset
is reported in Table 2. We report word-level ac-
curacy, that is, we account for words for which
the stress pattern was correctly assigned. As ex-
pected, the cascaded model performs significantly
better than the baseline.
Model Accuracy
Baseline 0.637
Cascaded model (gold) 0.975
Cascaded model (predicted) 0.973
Table 2: Accuracy for stress prediction
Further, we perform an in-depth analysis of the
sequential model?s performance by accounting for
2
For example, the word copii (meaning children) has the
following C/V structure: CV-CVV. In our training set, there
are 659 words with this structure and the three stress patterns
which occur in the training set are as follows: CV-CVV (309
occurrences), CV-CVV (283 occurrences) and CV-CVV (67
occurrences). Therefore, the most common stress pattern CV-
CVV is correctly assigned, in this case, for the word copii.
66
several fine-grained characteristics of the words
in RoSyllabiDict. We divide words in categories
based on the following criteria:
? part of speech: verbs, nouns, adjectives
? number of syllables: 2-8, 9+
? number of consecutive vowels: with at least
2 consecutive vowels, without consecutive
vowels
Category Subcategory ] words
Accuracy
G P
POS
Verbs 167,193 0.995 0.991
Nouns 266,987 0.979 0.979
Adjectives 97,169 0.992 0.992
Syllables
2 syllables 34,810 0.921 0.920
3 syllables 111,330 0.944 0.941
4 syllables 154,341 0.966 0.964
5 syllables 120,288 0.981 0.969
6 syllables 54,918 0.985 0.985
7 syllables 17,852 0.981 0.989
8 syllables 5,278 0.992 0.984
9+ syllables 1,468 0.979 0.980
Vowels
With VV 134,895 0.972 0.972
Without VV 365,412 0.976 0.974
Table 3: Accuracy for cascaded model with
gold (G) and predicted (P) syllabication
We train and test the cascaded model indepen-
dently for each subcategory in the same manner as
we did for the entire dataset. We decided to use
cross-validation for parameter selection instead of
splitting the data in train/dev/test subsets in or-
der to have consistency across all models, because
some of these word categories do not comprise
enough words for splitting in three subsets (words
with more than 8 syllables, for example, have only
1,468 instances). The evaluation of the system?s
performance and the number of words in each cat-
egory are presented in Table 3.
4.2 Results Analysis
The overall accuracy is 0.975 for the cascaded
model with gold syllabication and 0.973 for the
cascaded model with predicted syllabication. The
former system outperforms the latter by only very
little. With regard to the part of speech, the high-
est accuracy when gold syllabication is used was
obtained for verbs (0.995), followed by adjectives
(0.992) and by nouns (0.979). When dividing the
dataset with respect to the words? part of speech,
the cascaded model with predicted syllabication
is outperformed only for verbs. With only a few
exceptions, the accuracy steadily increases with
the number of syllables. The peak is reached for
words with 6 syllables when using the gold syllab-
ication and for words with 7 syllables when using
the predicted syllabication. Although, intuitively,
the accuracy should be inversely proportional to
the number of syllables, because the number of
potential positions for stress placement increases,
there are numerous stress patterns for words with
6, 7 or more syllables, which never occur in the
dataset
3
. It is interesting to notice that stress pre-
diction accuracy is almost equal for words con-
taining two or more consecutive vowels and for
words without consecutive vowels. As expected,
when words are divided in categories based on
their characteristics the system is able to predict
stress placement with higher accuracy.
5 Conclusion and Future Work
In this paper we showed that Romanian stress
is predictable, though not deterministic, by using
data-driven machine learning techniques. Syllable
structure is important and helps the task of stress
prediction. The cascaded sequential model using
gold syllabication outperforms systems with pre-
dicted syllabication by only very little.
In our future work we intend to experiment with
other features as well, such as syllable n-grams
instead of character n-grams, for the sequential
model. We plan to conduct a thorough error analy-
sis and to investigate the words for which the sys-
tems did not correctly predict the position of the
stressed vowels. We intend to further investigate
the C/V structures identified in this paper and to
analyze the possibility to reduce the number of
patterns by considering details of word structure
(for example, instead of using raw data, to aug-
ment the model with annotations about which let-
ters are actually vowels) and to adapt the learning
model to finer-grained linguistic analysis.
Acknowledgements
The authors thank the anonymous reviewers for
their helpful comments. The contribution of the
authors to this paper is equal. Research supported
by a grant of ANRCS, CNCS UEFISCDI, project
number PN-II-ID-PCE-2011-3-0959.
3
For example, for the stress pattern CV-CV-CV-CV-CV-
CVCV, which matches 777 words in our dataset, the stress is
never placed on the first three syllables.
67
References
Ana-Maria Barbu. 2008. Romanian Lexical Data
Bases: Inflected and Syllabic Forms Dictionaries. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation, LREC 2008,
pages 1937?1941.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic Syllabification with Structured
SVMs for Letter-to-Phoneme Conversion. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, ACL-HLT 2008, pages 568?
576.
Ioana Chitoran. 1996. Prominence vs. rhythm: The
predictability of stress in Romanian. In Grammat-
ical theory and Romance languages, pages 47?58.
Karen Zagona.
Ioana Chitoran. 2002. The phonology of Romanian. A
constraint-based approach. Mouton de Gruyter.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10, EMNLP
2002, pages 1?8.
Robert I. Damper, Yannick Marchand, M. J. Adam-
son, and K. Gustafson. 1999. Evaluating the
pronunciation component of text-to-speech systems
for English: a performance comparison of differ-
ent approaches. Computer Speech & Language,
13(2):155?176.
Vera Demberg, Helmut Schmid, and Gregor M?hler.
2007. Phonological Constraints and Morphologi-
cal Preprocessing for Grapheme-to-Phoneme Con-
version. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2007, pages 96?103.
Gabriela Pan
?
a Dindelegan. 2013. The Grammar of
Romanian. Oxford University Press.
Liviu P. Dinu and Anca Dinu. 2005. A Parallel Ap-
proach to Syllabification. In Proceedings of the
6th International Conference on Computational Lin-
guistics and Intelligent Text Processing, CICLing
2005, pages 83?87.
Liviu P. Dinu, Vlad Niculae, and Octavia-Maria S
,
ulea.
2013. Romanian Syllabication Using Machine
Learning. In Proceedings of the 16th International
Conference on Text, Speech and Dialogue, TSD
2013, pages 450?456.
Liviu Petrisor Dinu. 2003. An Approach to Syllables
via some Extensions of Marcus Contextual Gram-
mars. Grammars, 6(1):1?12.
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn,
and Grzegorz Kondrak. 2009. A Ranking Approach
to Stress Prediction for Letter-to-Phoneme Conver-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th IJCNLP
of the AFNLP, ACL 2009, pages 118?126.
Eugeniu Oancea and Adriana Badulescu. 2002.
Stressed Syllable Determination for Romanian
Words within Speech Synthesis Applications. Inter-
national Journal of Speech Technology, 5(3):237?
246.
Naoaki Okazaki. 2007. CRFsuite: a fast implementa-
tion of Conditional Random Fields (CRFs).
Tomaz Sef, Maja Skrjanc, and Matjaz Gams. 2002.
Automatic Lexical Stress Assignment of Unknown
Words for Highly Inflected Slovenian Language. In
Proceedings of the 5th International Conference on
Text, Speech and Dialogue, TSD 2002, pages 165?
172.
S.-A. Toma, E. Oancea, and D. Munteanu. 2009.
Automatic rule-based syllabication for Romanian.
In Proceedings of the 5th Conference on Speech
Technology and Human-Computer Dialogue, SPeD
2009, pages 1?6.
Nikolaos Trogkanis and Charles Elkan. 2010. Con-
ditional Random Fields for Word Hyphenation. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2010,
pages 366?374.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 99?105,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Automatic Detection of Cognates Using Orthographic Alignment
Alina Maria Ciobanu, Liviu P. Dinu
Faculty of Mathematics and Computer Science, University of Bucharest
Center for Computational Linguistics, University of Bucharest
alina.ciobanu@my.fmi.unibuc.ro,ldinu@fmi.unibuc.ro
Abstract
Words undergo various changes when en-
tering new languages. Based on the as-
sumption that these linguistic changes fol-
low certain rules, we propose a method
for automatically detecting pairs of cog-
nates employing an orthographic align-
ment method which proved relevant for se-
quence alignment in computational biol-
ogy. We use aligned subsequences as fea-
tures for machine learning algorithms in
order to infer rules for linguistic changes
undergone by words when entering new
languages and to discriminate between
cognates and non-cognates. Given a list
of known cognates, our approach does not
require any other linguistic information.
However, it can be customized to integrate
historical information regarding language
evolution.
1 Introduction
Cognates are words in different languages having
the same etymology and a common ancestor. In-
vestigating pairs of cognates is very useful in his-
torical and comparative linguistics, in the study
of language relatedness (Ng et al, 2010), phy-
logenetic inference (Atkinson et al, 2005) and
in identifying how and to what extent languages
change over time. In other several research ar-
eas, such as language acquisition, bilingual word
recognition (Dijkstra et al, 2012), corpus lin-
guistics (Simard et al, 1992), cross-lingual infor-
mation retrieval (Buckley et al, 1997) and ma-
chine translation (Kondrak et al, 2003), the con-
dition of common etymology is usually not essen-
tial and cognates are regarded as words with high
cross-lingual meaning and orthographic or pho-
netic similarity.
The wide range of applications in which cog-
nates prove useful attracted more and more at-
tention on methods for detecting such related
pairs of words. This task is most challenging
for resource-poor languages, for which etymologi-
cally related information is not accessible. There-
fore, the research (Inkpen et al, 2005; Mulloni and
Pekar, 2006; Hauer and Kondrak, 2011) focused
on automatic identification of cognate pairs, start-
ing from lists of known cognates.
In this paper, we propose a method for automat-
ically determining pairs of cognates across lan-
guages. The proposed method requires a list of
known cognates and, for languages for which ad-
ditional linguistic information is available, it can
be customized to integrate historical information
regarding the evolution of the language. The rest
of the paper is organized as follows: in Section
2 we present and analyze alternative methods and
related work in this area. In Section 3 we intro-
duce our approach for detection of cognates us-
ing orthographic alignment. In Section 4 we de-
scribe the experiments we conduct and we report
and analyze the results, together with a compari-
son with previous methods. Finally, in Section 5
we draw the conclusions of our study and describe
our plans for extending the method.
2 Related Work
There are three important aspects widely investi-
gated in the task of cognate identification: seman-
tic, phonetic and orthographic similarity. They
were employed both individually (Simard et al,
1992; Inkpen et al, 2005; Church, 1993) and com-
bined (Kondrak, 2004; Steiner et al, 2011) in or-
der to detect pairs of cognates across languages.
For determining semantic similarity, external lexi-
cal resources, such as WordNet (Fellbaum, 1998),
or large corpora, might be necessary. For measur-
ing phonetic and orthographic proximity of cog-
nate candidates, string similarity metrics can be
applied, using the phonetic or orthographic word
forms as input. Various measures were investi-
99
gated and compared (Inkpen et al, 2005; Hall and
Klein, 2010); Levenshtein distance (Levenshtein,
1965), XDice (Brew and McKelvie, 1996) and
the longest common subsequence ratio (Melamed,
1995) are among the most frequently used metrics
in this field. Gomes and Lopes (2011) proposed
SpSim, a more complex method for computing the
similarity of cognate pairs which tolerates learned
transitions between words.
Algorithms for string alignment were success-
fully used for identifying cognates based on both
their forms, orthographic and phonetic. Delmestri
and Cristianini (2010) used basic sequence align-
ment algorithms (Needleman and Wunsch, 1970;
Smith and Waterman, 1981; Gotoh, 1982) to ob-
tain orthographic alignment scores for cognate
candidates. Kondrak (2000) developed the ALINE
system, which aligns words? phonetic transcrip-
tions based on multiple phonetic features and com-
putes similarity scores using dynamic program-
ming. List (2012) proposed a framework for au-
tomatic detection of cognate pairs, LexStat, which
combines different approaches to sequence com-
parison and alignment derived from those used in
historical linguistics and evolutionary biology.
The changes undergone by words when enter-
ing from one language into another and the trans-
formation rules they follow have been successfully
employed in various approaches to cognate detec-
tion (Koehn and Knight, 2000; Mulloni and Pekar,
2006; Navlea and Todirascu, 2011). These ortho-
graphic changes have also been used in cognate
production, which is closely related to the task of
cognate detection, but has not yet been as inten-
sively studied. While the purpose of cognate de-
tection is to determine whether two given words
form a cognate pair, the aim of cognate produc-
tion is, given a word in a source language, to
automatically produce its cognate pair in a tar-
get language. Beinborn et al (2013) proposed a
method for cognate production relying on statis-
tical character-based machine translation, learn-
ing orthographic production patterns, and Mul-
loni (2007) introduced an algorithm for cognate
production based on edit distance alignment and
the identification of orthographic cues when words
enter a new language.
3 Our Approach
Although there are multiple aspects that are rel-
evant in the study of language relatedness, such
as orthographic, phonetic, syntactic and semantic
differences, in this paper we focus only on lexical
evidence. The orthographic approach relies on the
idea that sound changes leave traces in the orthog-
raphy and alphabetic character correspondences
represent, to a fairly large extent, sound correspon-
dences (Delmestri and Cristianini, 2010).
Words undergo various changes when entering
new languages. We assume that rules for adapting
foreign words to the orthographic system of the
target languages might not have been very well
defined in their period of early development, but
they may have since become complex and proba-
bly language-specific. Detecting pairs of cognates
based on etymology is useful and reliable, but, for
resource-poor languages, methods which require
less linguistic knowledge might be necessary. Ac-
cording to Gusfield (1997), an edit transcript (rep-
resenting the conversion of one string to another)
and an alignment are mathematically equivalent
ways of describing relationships between strings.
Therefore, because the edit distance was widely
used in this research area and produced good re-
sults, we are encouraged to employ orthographic
alignment for identifying pairs of cognates, not
only to compute similarity scores, as was previ-
ously done, but to use aligned subsequences as
features for machine learning algorithms. Our in-
tuition is that inferring language-specific rules for
aligning words will lead to better performance in
the task of cognate identification.
3.1 Orthographic Alignment
String alignment is closely related to the task
of sequence alignment in computational biology.
Therefore, to align pairs of words we employ the
Needleman-Wunsch global alignment algorithm
(Needleman and Wunsch, 1970), which is mainly
used for aligning sequences of proteins or nu-
cleotides. Global sequence alignment aims at de-
termining the best alignment over the entire length
of the input sequences. The algorithm uses dy-
namic programming and, thus, guarantees to find
the optimal alignment. Its main idea is that any
partial path of the alignment along the optimal
path should be the optimal path leading up to that
point. Therefore, the optimal path can be deter-
mined by incremental extension of the optimal
subpaths (Schuler, 2002). For orthographic align-
ment, we consider words as input sequences and
we use a very simple substitution matrix, which
100
gives equal scores to all substitutions, disregard-
ing diacritics (e.g., we ensure that e and `e are
matched).
3.2 Feature Extraction
Using aligned pairs of words as input, we extract
features around mismatches in the alignments.
There are three types of mismatches, correspond-
ing to the following operations: insertion, deletion
and substitution. For example, for the Romanian
word exhaustiv and its Italian cognate pair esaus-
tivo, the alignment is as follows:
e x h a u s t i v -
e s - a u s t i v o
The first mismatch (between x and s) is caused
by a substitution, the second mismatch (between
h and -) is caused by a deletion from source lan-
guage to target language, and the third mismatch
(between - and o) is caused by an insertion from
source language to target language. The features
we use are character n-grams around mismatches.
We experiment with two types of features:
i) n-grams around gaps, i.e., we account only
for insertions and deletions;
ii) n-grams around any type of mismatch, i.e.,
we account for all three types of mismatches.
The second alternative leads to better perfor-
mance, so we account for all mismatches. As
for the length of the grams, we experiment with
n ? {1, 2, 3}. We achieve slight improvements by
combining n-grams, i.e., for a given n, we use all
i-grams, where i ? {1, ..., n}. In order to provide
information regarding the position of the features,
we mark the beginning and the end of the word
with a $ symbol. Thus, for the above-mentioned
pair of cognates, (exhaustiv, esaustivo), we extract
the following features when n = 2:
x>s ex>es xh>s-
h>- xh>s- ha>-a
->o v->vo -$>o$
For identical features we account only once.
Therefore, because there is one feature (xh>s-)
which occurs twice in our example, we have 8 fea-
tures for the pair (exhaustiv, esaustivo).
3.3 Learning Algorithms
We use Naive Bayes as a baseline and we exper-
iment with Support Vector Machines (SVMs) to
learn orthographic changes and to discriminate be-
tween pairs of cognates and non-cognates. We
put our system together using the Weka work-
bench (Hall et al, 2009), a suite of machine learn-
ing algorithms and tools. For SVM, we use the
wrapper provided by Weka for LibSVM (Chang
and Lin, 2011). We use the radial basis function
kernel (RBF), which can handle the case when
the relation between class labels and attributes is
non-linear, as it maps samples non-linearly into a
higher dimensional space. Given two instances x
i
and x
j
, where x
i
? R
n
, the RBF kernel function
for x
i
and x
j
is defined as follows:
K(x
i
, x
j
) = exp(??||x
i
? x
j
||
2
), ? > 0,
where ? is a kernel parameter.
We split the data in two subsets, for training
and testing, with a 3:1 ratio, and we perform grid
search and 3-fold cross validation over the train-
ing set in order to optimize hyperparameters c
and ?. We search over {1, 2, ..., 10} for c and
over {10
?5
, 10
?4
, ..., 10
4
, 10
5
} for ?. The values
which optimize accuracy on the training set are re-
ported, for each pair of languages, in Table 3.
4 Experiments
4.1 Data
We apply our method on an automatically ex-
tracted dataset of cognates for four pairs of
languages: Romanian-French, Romanian-Italian,
Romanian-Spanish and Romanian-Portuguese. In
order to build the dataset, we apply the method-
ology proposed by Ciobanu and Dinu (2014) on
the DexOnline
1
machine-readable dictionary for
Romanian. We discard pairs of words for which
the forms across languages are identical (i.e., the
Romanian word matrice and its Italian cognate
pair matrice, having the same form), because these
pairs do not provide any orthographic changes to
be learned. For each pair of languages we de-
termine a number of non-cognate pairs equal to
the number of cognate pairs. Finally, we ob-
tain 445 pairs of cognates for Romanian-French
2
,
3,477 for Romanian-Italian, 5,113 for Romanian-
Spanish and 7,858 for Romanian-Portuguese. Be-
cause we need sets of approximately equal size for
1
http://dexonline.ro
2
The number of pairs of cognates is much lower for
French than for the other languages because there are numer-
ous Romanian words which have French etymology and, in
this paper, we do not consider these words to be cognate can-
didates.
101
1st
2
nd
3
rd
4
th
5
th
IT iu>io un>on l->le t$>-$ -$>e$
FR un>on ne>n- iu>io t?i>ti e$>-$
ES -$>o$ t?i>ci ?>?on ie>i?o at>ad
PT ie>?ao at?>ac? t?i>c??a i$>-$ ?a$>a$
Table 1: The most relevant orthographic cues for
each pair of languages determined on the entire
datasets using the ?
2
attribute evaluation method
implemented in Weka.
1
st
2
nd
3
rd
4
th
5
th
IT -$>e$ -$>o$ ?a$>a$ ?>re t?i>zi
FR e$>-$ un>on ne>n- iu>io t?i>ti
ES -$>o$ e$>-$ t?i>ci ?a$>a$ at>ad
PT -$>o$ ?a$>a$ e$>-$ -$>r$ -$>a$
Table 2: The most frequent orthographic cues for
each pair of languages determined on the cognate
lists using the raw frequencies.
comparison across languages, we keep 400 pairs
of cognates and 400 pairs of non-cognates for each
pair of languages. In Tables 1 and 2 we provide,
for each pair of languages, the five most relevant
2-gram orthographic changes, determined using
the ?
2
distribution implemented in Weka, and the
five most frequent 2-gram orthographic changes in
the cognate pairs from our dataset
3
. None of the
top ranked orthographic cues occurs at the begin-
ning of the word, while many of them occur at the
end of the word. The most frequent operation in
Tables 1 and 2 is substitution.
4.2 Results Analysis
We propose a method for automatic detection
of cognate pairs using orthographic alignment.
We experiment with two machine-learning ap-
proaches: Naive Bayes and SVM. In Table 3 we
report the results of our research. We report the
n-gram values for which the best results are ob-
tained and the hyperparameters for SVM, c and ?.
The best results are obtained for French and Span-
ish, while the lowest accuracy is obtained for Por-
tuguese. The SVM produces better results for all
languages except Portuguese, where the accuracy
is equal. For Portuguese, both Naive Bayes and
SVM misclassify more non-cognates as cognates
3
For brevity, we use in the tables the ISO 639-1 codes for
language abbreviation. We denote pairs of languages by the
target language, given the fact that Romanian is always the
source language in our experiments.
than viceversa. A possible explanation might be
the occurrence, in the dataset, of more remotely
related words, which are not labeled as cognates.
We plan to investigate this assumption and to ap-
ply the proposed method on other datasets in our
future work.
4.3 Comparison with Previous Methods
We investigate the performance of the method we
propose in comparison to previous approaches for
automatic detection of cognate pairs based on or-
thographic similarity. We employ several ortho-
graphic metrics widely used in this research area:
the edit distance (Levenshtein, 1965), the longest
common subsequence ratio (Melamed, 1995) and
the XDice metric (Brew and McKelvie, 1996)
4
.
In addition, we use SpSim (Gomes and Lopes,
2011), which outperformed the longest common
subsequence ratio and a similarity measure based
on the edit distance in previous experiments. To
evaluate these metrics on our dataset, we use the
same train/test sets as we did in our previous ex-
periments and we follow the strategy described in
(Inkpen et al, 2005). First, we compute the pair-
wise distances between pairs of words for each
orthographic metric individually, as a single fea-
ture
5
. In order to detect the best threshold for dis-
criminating between cognates and non-cognates,
we run a decision stump classifier (provided by
Weka) on the training set for each pair of lan-
guages and for each metric. A decision stump is a
decision tree classifier with only one internal node
and two leaves corresponding to our two class la-
bels. Using the best threshold value selected for
each metric and pair of languages, we further clas-
sify the pairs of words in our test sets as cognates
or non-cognates. In Table 4 we report the results
for each approach. Our method performs better
than the orthographic metrics considered as indi-
vidual features. Out of the four similarity met-
rics, SpSim obtains, overall, the best performance.
These results support the relevance of accounting
for orthographic cues in cognate identification.
4
We use normalized similarity metrics. For the edit dis-
tance, we subtract the normalized value from 1 in order to
obtain similarity.
5
SpSim cannot be computed directly, as the other metrics,
so we introduce an additional step in which we use 1/3 of the
training set (only cognates are needed) to learn orthographic
changes. In order to maintain a stratified dataset, we discard
an equal number of non-cognates in the training set and then
we compute the distances for the rest of the training set and
for the test set. We use the remaining of the initial training
set for the next step of the procedure.
102
Naive Bayes SVM
P R A n P R A n c ?
IT 0.72 0.93 79.0 1 0.76 0.92 81.5 1 1 0.10
FR 0.81 0.91 82.0 2 0.84 0.89 87.0 2 10 0.01
ES 0.79 0.92 84.0 1 0.85 0.88 86.5 2 4 0.01
PT 0.67 0.88 73.0 2 0.70 0.78 73.0 2 10 0.01
Table 3: Results for automatic detection of cognates using orthographic alignment. We report the preci-
sion (P), recall (R) and accuracy (A) obtained on the test sets and the optimal n-gram values. For SVM
we also report the optimal hyperparameters c and ? obtained during cross-validation on the training sets.
EDIT LCSR XDICE SPSIM
P R A t P R A t P R A t P R A t
IT 0.67 0.97 75.0 0.43 0.68 0.91 75.0 0.51 0.66 0.98 74.0 0.21 0.66 0.98 74.5 0.44
FR 0.76 0.93 82.0 0.30 0.76 0.90 81.5 0.42 0.77 0.79 78.0 0.26 0.86 0.83 85.0 0.59
ES 0.77 0.91 82.0 0.56 0.72 0.97 80.0 0.47 0.72 0.99 80.5 0.19 0.81 0.90 85.0 0.64
PT 0.62 0.99 69.5 0.34 0.59 0.99 65.5 0.34 0.57 0.99 63.5 0.10 0.62 0.97 69.0 0.39
Table 4: Comparison with previous methods for automatic detection of cognate pairs based on orthog-
raphy. We report the precision (P), recall (R) and accuracy (A) obtained on the test sets and the optimal
threshold t for discriminating between cognates and non-cognates.
5 Conclusions and Future Work
In this paper we proposed a method for automatic
detection of cognates based on orthographic align-
ment. We employed the Needleman-Wunsch al-
gorithm (Needleman and Wunsch, 1970) for se-
quence alignment widely-used in computational
biology and we used aligned pairs of words to
extract rules for lexical changes occurring when
words enter new languages. We applied our
method on an automatically extracted dataset of
cognates for four pairs of languages.
As future work, we plan to extend our method
on a few levels. In this paper we used a very
simple substitution matrix for the alignment algo-
rithm, but the method can be adapted to integrate
historical information regarding language evolu-
tion. The substitution matrix for the alignment al-
gorithm can be customized with language-specific
information, in order to reflect the probability of
a character to change into another. An important
achievement in this direction belongs to Delmestri
and Cristianini (2010), who introduced PAM-like
matrices, linguistic-inspired substitution matrices
which are based on information regarding ortho-
graphic changes. We plan to investigate the con-
tribution of using this type of substitution matrices
for our method.
We intend to investigate other approaches to
string alignment, such as local alignment (Smith
and Waterman, 1981), and other learning algo-
rithms for discriminating between cognates and
non-cognates. We plan to extend our analysis with
more language-specific features, where linguistic
knowledge is available. First, we intend to use the
part of speech as an additional feature. We assume
that some orthographic changes are dependent on
the part of speech of the words. Secondly, we want
to investigate whether accounting for the common
ancestor language influences the results. We are
interested to find out if the orthographic rules de-
pend on the source language, or if they are rather
specific to the target language. Finally, we plan to
make a performance comparison on cognate pairs
versus word-etymon pairs and to investigate false
friends (Nakov et al, 2007).
We further intend to adapt our method for cog-
nate detection to a closely related task, namely
cognate production, i.e., given an input word w,
a related language L and a set of learned rules for
orthographic changes, to produce the cognate pair
of w in L.
Acknowledgements
We thank the anonymous reviewers for their help-
ful and constructive comments. The contribution
of the authors to this paper is equal. Research sup-
ported by CNCS UEFISCDI, project number PN-
II-ID-PCE-2011-3-0959.
103
References
Quentin D. Atkinson, Russell D. Gray, Geoff K.
Nicholls, and David J. Welch. 2005. From Words
to Dates: Water into Wine, Mathemagic or Phylo-
genetic Inference? Transactions of the Philological
Society, 103:193?219.
Lisa Beinborn, Torsten Zesch, and Iryna Gurevych.
2013. Cognate Production using Character-based
Machine Translation. In Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing, IJCNLP 2013, pages 883?891.
Chris Brew and David McKelvie. 1996. Word-Pair
Extraction for Lexicography. In Proceeding of Text,
Speech and Dialogue, TSD 1996, pages 45?55.
Chris Buckley, Mandar Mitra, Janet A. Walz, and
Claire Cardie. 1997. Using Clustering and Super-
Concepts Within SMART: TREC 6. In Proceedings
of the 6th Text Retrieval Conference, TREC 1997,
pages 107?124.
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/
?
cjlin/libsvm.
Kenneth W. Church. 1993. Char align: A program
for aligning parallel texts at the character level. In
Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, ACL 1993,
pages 1?8.
Alina Maria Ciobanu and Liviu P. Dinu. 2014. Build-
ing a Dataset of Multilingual Cognates for the Ro-
manian Lexicon. In Proceedings of the 9th Interna-
tional Conference on Language Resources and Eval-
uation, LREC 2014.
Antonella Delmestri and Nello Cristianini. 2010.
String Similarity Measures and PAM-like Matrices
for Cognate Identification. Bucharest Working Pa-
pers in Linguistics, 12(2):71?82.
Ton Dijkstra, Franc Grootjen, and Job Schepens. 2012.
Distributions of Cognates in Europe as Based on
Levenshtein Distance. Bilingualism: Language and
Cognition, 15:157?166.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Lu??s Gomes and Jos?e Gabriel Pereira Lopes. 2011.
Measuring spelling similarity for cognate identifi-
cation. In Proceedings of the 15th Portugese Con-
ference on Progress in Artificial Intelligence, EPIA
2011, pages 624?633. Software available at http:
//research.variancia.com/spsim.
Osamu Gotoh. 1982. An improved algorithm for
matching biological sequences. Journal of Molec-
ular Biology, 162(3):705?708.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences: computer science and computational bi-
ology. Cambridge University Press New York, NY,
USA.
David Hall and Dan Klein. 2010. Finding Cognate
Groups Using Phylogenies. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2010, pages 1030?1039.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an up-
date. SIGKDD Explorations, 11(1):10?18. Soft-
ware available at http://www.cs.waikato.
ac.nz/ml/weka.
Bradley Hauer and Grzegorz Kondrak. 2011. Cluster-
ing semantically equivalent words into cognate sets
in multilingual lists. In 5th International Joint Con-
ference on Natural Language Processing, IJCNLP
2011, pages 865?873.
Diana Inkpen, Oana Frunza, and Grzegorz Kondrak.
2005. Automatic Identification of Cognates and
False Friends in French and English. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, RANLP
2005, pages 251?257.
Philipp Koehn and Kevin Knight. 2000. Estimat-
ing Word Translation Probabilities from Unrelated
Monolingual Corpora Using the EM Algorithm. In
Proceedings of the 17th National Conference on Ar-
tificial Intelligence and 12th Conference on Inno-
vative Applications of Artificial Intelligence, pages
711?715.
Grzegorz Kondrak, Daniel Marcu, and Keven Knight.
2003. Cognates Can Improve Statistical Translation
Models. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, HLT-NAACL 2003, pages 46?48.
Grzegorz Kondrak. 2000. A New Algorithm for the
Alignment of Phonetic Sequences. In Proceedings
of the 1st North American Chapter of the Asso-
ciation for Computational Linguistics Conference,
NAACL 2000, pages 288?295.
Grzegorz Kondrak. 2004. Combining Evidence in
Cognate Identification. In Proceedings of the 17th
Conference of the Canadian Society for Computa-
tional Studies of Intelligence on Advances in Artifi-
cial Intelligence, pages 44?59.
Vladimir I. Levenshtein. 1965. Binary Codes Capable
of Correcting Deletions, Insertions, and Reversals.
Soviet Physics Doklady, 10:707?710.
Johann-Mattis List. 2012. LexStat: Automatic De-
tection of Cognates in Multilingual Wordlists. In
Proceedings of the EACL 2012 Joint Workshop of
LINGVIS and UNCLH, pages 117?125.
104
Dan Melamed. 1995. Automatic Evaluation and Uni-
form Filter Cascades for Inducing N-Best Transla-
tion Lexicons. In Proceedings of the 3rd Workshop
on Very Large Corpora.
Andrea Mulloni and Viktor Pekar. 2006. Automatic
detection of orthographic cues for cognate recog-
nition. In In Proceedings of the 5th International
Conference on Language Resources and Evaluation,
LREC 2006, pages 2387?2390.
Andrea Mulloni. 2007. Automatic Prediction of Cog-
nate Orthography Using Support Vector Machines.
In Proceedings of the 45th Annual Meeting of the
ACL: Student Research Workshop, ACL 2007, pages
25?30.
Svetlin Nakov, Preslav Nakov, and Elena Paskaleva.
2007. Cognate or False Friend? Ask the Web! In
Proceedings of the RANLP 2007 Workshop ?Acqui-
sition and Management of Multilingual Lexicons?,
pages 55?62.
Mirabela Navlea and Amalia Todirascu. 2011. Using
Cognates in a French-Romanian Lexical Alignment
System: A Comparative Study. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing, RANLP 2011, pages
247?253.
Saul B. Needleman and Christian D. Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of Molecular Biology, 48(3):443 ? 453.
Ee-Lee Ng, Beatrice Chin, Alvin W. Yeo, and Bali
Ranaivo-Malanc?on. 2010. Identification of Closely-
Related Indigenous Languages: An Orthographic
Approach. Int. J. of Asian Lang. Proc., 20(2):43?
62.
Gregory D. Schuler. 2002. Sequence Alignment and
Database Searching. Bioinformatics: A Practical
Guide to the Analysis of Genes and Proteins, 43. A.
D. Baxevanis and B. F. F. Ouellette, John Wiley &
Sons, Inc., New York, USA.
Michel Simard, George F. Foster, and Pierre Isabelle.
1992. Using Cognates to Align Sentences in Bilin-
gual Corpora. In Proceedings of the 4th Interna-
tional Conference on Theoretical and Methodologi-
cal Issues in Machine Translation.
Temple F. Smith and Michael S. Waterman. 1981.
Identification of common molecular subsequences.
Journal of Molecular Biology, 147(1):195?197.
Lydia Steiner, Peter F. Stadler, and Michael Cysouw.
2011. A pipeline for computational historical lin-
guistics. Language Dynamics and Change, 1(1):89?
127.
105
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 72?77,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Pastiche detection based on stopword rankings. Exposing impersonators
of a Romanian writer
Liviu P. Dinu
Faculty of Mathematics
and Computer Science
University of Bucharest
ldinu@fmi.unibuc.ro
Vlad Niculae
Faculty of Mathematics
and Computer Science
University of Bucharest
vlad@vene.ro
Octavia-Maria S, ulea
Faculty of Foreign Languages
and Literatures
Faculty of Mathematics
and Computer Science
University of Bucharest
mary.octavia@gmail.com
Abstract
We applied hierarchical clustering using
Rank distance, previously used in compu-
tational stylometry, on literary texts written
by Mateiu Caragiale and a number of dif-
ferent authors who attempted to imperson-
ate Caragiale after his death, or simply to
mimic his style. Their pastiches were con-
sistently clustered opposite to the original
work, thereby confirming the performance
of the method and proposing an extension
of the method from simple authorship attri-
bution to the more complicated problem of
pastiche detection.
The novelty of our work is the use of fre-
quency rankings of stopwords as features,
showing that this idea yields good results
for pastiche detection.
1 Introduction
The postulated existence of the human stylome
has been thoroughly studied with encouraging re-
sults. The term stylome, which is currently not in
any English dictionaries, was recently defined as
a linguistic fingerprint which can be measured, is
largely unconscious, and is constant (van Halteren
et al, 2005).
Closely related to the problem of authorship
attribution lies the pastiche detection problem,
where the fundamental question is: Can the hu-
man stylome be faked in order to trick authorship
attribution methods? There are situations where
certain authors or journalists have tried to pass
their own work as written by someone else. A
similar application is in forensics, where an im-
personator is writing letters or messages and sign-
ing with someone else?s name, especially online.
It is important to note that sometimes pastiches
are not intended to deceive, but simply as an ex-
ercise in mimicking another?s style. Even in this
case, the best confirmation that the author of the
pastiche can get is if he manages to fool an au-
thorship attribution algorithm, even if the ground
truth is known and there is no real question about
it.
Marcus (1989) identifies the following four sit-
uation in which text authorship is disputed:
? A text attributed to one author seems non-
homogeneous, lacking unity, which raises
the suspicion that there may be more than
one author. If the text was originally at-
tributed to one author, one must establish
which fragments, if any, do not belong to
him, and who are their real authors.
? A text is anonymous. If the author of a text
is unknown, then based on the location, time
frame and cultural context, we can conjec-
ture who the author may be and test this hy-
pothesis.
? If based on certain circumstances, arising
from literature history, the paternity is dis-
puted between two possibilities, A and B, we
have to decide if A is preferred to B, or the
other way around.
? Based on literary history information, a text
seems to be the result of the collaboration of
two authors, an ulterior analysis should es-
tablish, for each of the two authors, their cor-
responding text fragments.
We situate ourselves in a case similar to the
third, but instead of having to choose between two
authors, we are asking whether a group of texts
were indeed written by the claimed author or by
someone else. Ideally, we would take samples au-
thored by every possible impersonator and run a
72
multi-class classifier in order to estimate the prob-
ability that the disputed work is written by them
or by the asserted author. Such a method can give
results if we know who the impersonator can be,
but most of the time that information is not avail-
able, or the number of possible impersonators is
intractabally large.
In the case of only one impersonator, the prob-
lem can simply be stated as authorship attribu-
tion with a positive or a negative answer. How-
ever, when there are a number of people sepa-
rately writing pastiches of one victim?s style, the
extra information can prove beneficial in an unsu-
pervised learning sense. In this paper we analyze
the structure induced by the Rank Distance metric
using frequencies of stopwords as features, previ-
ously applied for authorship attribution, on such
a sample space. The assumption is that trying
to fake someone else?s stylome will induce some
consistent bias so that new impersonators can be
caught using features from other pastiche authors.
2 The successors of Mateiu Caragiale
Mateiu Caragiale, one of the most important Ro-
manian novelists, died in 1936, at the age of 51,
leaving behind an unfinished novel, Sub pecetea
tainei. Some decades later, in the 70?s, a rumor
agitated the Romanian literary world: it seemed
that the ending of the novel had been found. A
few human experts agreed that the manuscript is
in concordance with Mateiu?s style, and in the
next months almost everybody talked about the
huge finding. However, it was suspicious that
the writer who claimed the discovery, Radu Al-
bala, was considered by the critics to be one of
the closest stylistic followers of Mateiu Caragiale.
When the discussions regarding the mysterious
finding reached a critical mass, Albala publically
put a stop to them, by admitting that he him-
self had written the ending as a challenge - he
wanted to see how well he could deceive the pub-
lic into thinking the text in question was written
by Mateiu himself.
Other authors attempted to write different end-
ings to the novel, but without claiming Caragiale?s
paternity, like Albala did. Around the same time,
Eugen Ba?lan also set to continue the unfinished
novel, as a stylistic exercise. He addressed a sep-
arate storyline than Albala?s. Later, Alexandru
George also attempted to finish the novel, claim-
ing that his ending is the best. Unfortunately
there is only one copy of George?s work, and we
couldn?t obtain it for this study.
In 2008, Ion Iovan published the so-called Last
Notes of Mateiu Caragiale, composed of sections
written from Iovan?s voice, and another section
in the style of a personal diary describing the life
of Mateiu Caragiale, suggesting that this is really
Caragiale?s diary. This was further strengthened
by the fact that a lot of phrases from the diary
were copied word for word from Mateiu Cara-
giale?s novels, therefore pushing the style towards
Caragiale?s. However, this was completely a work
of fiction, the diary having been admittedly imag-
ined and written by Iovan.
Another noteworthy case is the author S?tefan
Agopian. He never attempted to continue Mateiu
Caragiale?s novel, but critics consider him one of
his closest stylistic successors. Even though not
really a pastiche, we considered worth investigat-
ing how such a successor relates to the imperson-
ators.
3 Simple visual comparisons
The pioneering methods of Mendenhall (Menden-
hall, 1901) on the subject of authorship attribu-
tion, even though obsolete by today?s standards,
can be used to quickly examine at a glance the dif-
ferences between the authors, from certain points
of view. The Mendenhall plot, showing frequency
versus word length, does not give an objective cri-
terion to attribute authorship, but as an easy to cal-
culate statistic, it can motivate further research on
a specific attribution problem.
A further critique to Mendenhall?s method is
that different distributions of word length are not
necessary caused by individual stylome but rather
by the genre or the theme of the work. This can
further lead to noisy distributions in case of ver-
satile authors, whereas the stylome is supposed to
be stable.
Even so, the fact that Mateiu Caragiale?s
Mendenhall distribution has its modes consis-
tently in a different position than the others, sug-
gests that the styles are different, but it appears
that Caragiale?s successors have somewhat simi-
lar distributions. This can be seen in figure 3. In
order to evaluate the questions How different, how
similar?, and to make a more objective judgement
on authorship attribution, we resort to pairwise
distance-based methods.
73
(a) Mateiu Caragiale (b) S?tefan Agopian
(c) Radu Albala (d) Ion Iovan
Figure 1: Mendenhall plots: frequency distribution of word lengths, showing similarities between the other
authors, but differences between them and Mateiu Caragiale.
s?i ??n sa? se cu o la nu a ce mai din pe un ca? ca ma? fi care era lui fa?ra? ne pentru el ar dar
??l tot am mi ??nsa? ??ntr cum ca?nd toate al aa dupa? pa?na? deca?t ei nici numai daca? eu avea
fost le sau spre unde unei atunci mea prin ai ata?t au chiar cine iar noi sunt acum ale
are asta cel fie fiind peste aceasta? a cele face fiecare nimeni ??nca? ??ntre aceasta aceea
acest acesta acestei avut ceea ca?t da fa?cut noastra? poate acestui alte celor cineva ca?tre
lor unui alta? at?i dintre doar foarte unor va? aceste astfel avem avet?i cei ci deci este
suntem va vom vor de
Table 1: The 120 stopwords extracted as the most fre-
quent words in the corpus.
In order to speak of distances, we need to rep-
resent the samples (the novels) as points in a met-
ric space. Using the idea that stopword frequen-
cies are a significant component of the stylome,
and one that is difficult to fake (Chung and Pen-
nebaker, 2007), we first represented each work
as a vector of stopword frequencies, where the
stopwords are chosen to be the most frequent
words from all the concatenated documents. The
stopwords can be seen in table 1. Another use-
ful visualisation method is the Principal Compo-
nents Analysis, which gives us a projection from
a high-dimensional space into a low-dimensional
one, in this case in 2D. Using this stopword fre-
quency representation, the first principal compo-
nents plane looks like figure 3.
4 Distances and clustering
In (Popescu and Dinu, 2008), the use of rankings
instead of frequencies is proposed as a smoothing
method and it is shown to give good results for
computational stylometry. A ranking is simply an
ordering of items; in this case, the representation
of each document is the ranking of the stopwords
in that particular document. The fact that a spe-
cific function word has the rank 2 (is the second
most frequent word) in one text and has the rank 4
(is the fourth most frequent word) in another text
can be more directly relevant than the fact that the
respective word appears 349 times in the first text
and only 299 times in the second.
Rank distance (Dinu, 2003) is an ordinal metric
able to compare different rankings of a set of ob-
jects. In the general case, Rank distance works for
74
Figure 2: Principal components plot. Works are colour coded like in figure 3. The cluster on the left consists
only of novels by Mateiu Caragiale. Individual authors seem to form subclusters in the right cluster.
rankings where the support set is different (for ex-
ample, if a stopword would completely be missing
from a text). When this is not the case, we have
the following useful property:
A ranking of a set of n objects is a mapping
? : {1, 2, ..., n} ? {1, 2, ..., n} where ?(i) will
represent the place (rank) of the object indexed as
i such that if ?(q) < ?(p) word q is more frequent
than word p. The Rank distance in this case is
simply the distance induced by L1 norm on the
space of vector representations of permutations:
D(?1, ?2) =
n?
i=1
|?1(i)? ?2(i)| (1)
This is a distance between what is called full rank-
ings. However, in real situations, the problem of
tying arises, when two or more objects claim the
same rank (are ranked equally). For example, two
or more function words can have the same fre-
quency in a text and any ordering of them would
be arbitrary.
The Rank distance allocates to tied objects a
number which is the average of the ranks the tied
objects share. For instance, if two objects claim
the rank 2, then they will share the ranks 2 and 3
and both will receive the rank number (2+3)/2 =
2.5. In general, if k objects will claim the same
rank and the first x ranks are already used by other
objects, then they will share the ranks x + 1, x +
2, . . . , x + k and all of them will receive as rank
the number: (x+1)+(x+2)+...+(x+k)k = x +
k+1
2 .
In this case, a ranking will be no longer a permu-
tation (?(i) can be a non integer value), but the
formula (1) will remain a distance (Dinu, 2003).
Even though computationally the formula (1)
allows us to use the L1 distance we will continue
using the phrase Rank distance to refer to it, in or-
der to emphasize that we are measuring distances
between rankings of stopwords, not L1 distances
between frequency values or anything like that.
Hierarchical clustering (Duda et al, 2001) is a
bottom-up clustering method that starts with the
most specific cluster arrangement (one cluster for
each sample) and keeps joining the nearest clus-
ters, eventually stopping when reaching either a
stopping condition or the most general cluster ar-
rangement possible (one cluster containing all the
samples). When joining two clusters, there are
many possible ways to specify the distance be-
tween them. We used complete linkage: the dis-
tance between the most dissimilar points from the
two clusters. The resulting clustering path, visu-
alised a dendrogram, is shown in figure 4.
The use of clustering techniques in authorship
attribution problems has been shown useful by
Labbe? and Labbe? (2006); Luyckx et al (2006).
Hierarchical clustering with Euclidean distances
75
Figure 3: Dendrogram showing the results of hierarchical clustering using the L2 (euclidean) distance.
has been used for pastiche detection in (Somers
and Tweedie, 2003). The novelty of our work
is the use of rankings as features, and using the
L1 distance (equivalent to the Rank distance for
this particular case). (Somers and Tweedie, 2003)
shows how the Euclidean distance clusters mostly
works by the same author at the finest level, with a
few exceptions. On the data from our problem, we
observed a similar problem. The Euclidean dis-
tance behaves in a less than ideal fasion, joining
some of Agopian?s works with the cluster formed
by the other authors (see figure 3), whereas the
Rank distance always finds works by the same au-
thor the most similar at the leaves level (with the
obvious exception of Eugen Ba?lan?s text, because
it is his only available text).
Reading the dendrogram in the reverse order
(top to bottom), we see that for k = 2 clusters,
one corresponds to Mateiu Caragiale and the other
to all of his successors. In a little finer-grained
spot, there is a clear cluster of S?tefan Agopian?s
work, the (single) text by Eugen Ba?lan, and a joint
cluster with Radu Albala and Ion Iovan, which
also quickly breaks down into the separate au-
thors. The fact that there is no k for which all
authors are clearly separated in clusters can be
attributed to the large stylistic variance exhibited
by S?tefan Agopian and Mateiu Caragiale, whose
clusters break down more quickly.
These results confirm our intuition that rank-
ings of stopwords are more relevant than frequen-
cies, when an appropriate metric is used. Rank
distance is well-suited to this task. This leads us
to believe that if we go back and apply our meth-
ods to the texts studies in (Somers and Tweedie,
2003), an improvement will be seen, and we in-
tend to further look into this.
5 Conclusions
We reiterate that all of the authors used in
the study are considered stylistically similar to
Mateiu Caragiale by the critics. Some of their
works, highlighted on the graph, were either at-
tributed to Caragiale (by Albala and Iovan), or in-
tended as pastiche works continuing Caragiale?s
unfinished novel.
A key result is that with this models, all of these
successors prove to be closer to each other than to
Mateiu Caragiale. Therefore, when faced with a
new problem, we don?t have to seed the system
with many works from the possible authors (note
that we used a single text by Ba?lan): it suffices
to use as seeds texts by one or more authors who
are stylistically and culturally close to the claimed
author (in this case, Mateiu Caragiale). Cluster-
ing with an appropriate distance such as Rank dis-
76
Figure 4: Dendrogram showing the results of hierarchical clustering using L1 distance on stopword rankings
(equivalent to Rank distance).
tance will unmask the pastiche.
References
Cindy Chung and James Pennebaker. The psy-
chological functions of function words. Social
communication: Frontiers of social psychol-
ogy, pages 343?359, 2007.
Liviu Petrisor Dinu. On the classification and ag-
gregation of hierarchies with different consti-
tutive elements. Fundamenta Informaticae, 55
(1):39?50, 2003.
R. O. Duda, P. E. Hart, and D. G. Stork. Pattern
Classification (2nd ed.). Wiley-Interscience
Publication, 2001.
Cyril Labbe? and Dominique Labbe?. A tool for
literary studies: Intertextual distance and tree
classification. Literary and Linguistic Comput-
ing, 21(3):311?326, 2006.
Kim Luyckx, Walter Daelemans, and Edward
Vanhoutte. Stylogenetics: Clustering-based
stylistic analysis of literary corpora. In Pro-
ceedings of LREC-2006, the fifth International
Language Resources and Evaluation Confer-
ence, pages 30?35, 2006.
Solomon Marcus. Inventie si descoperire. Ed.
Cartea Romaneasca, Bucuresti, 1989.
T C Mendenhall. A mechanical solution of a liter-
ary problem. Popular Science Monthly, 60(2):
97?105, 1901.
Marius Popescu and Liviu Petrisor Dinu. Rank
distance as a stylistic similarity. In COLING
(Posters)?08, pages 91?94, 2008.
Harold Somers and Fiona Tweedie. Authorship
attribution and pastiche. Computers and the
Humanities, 37:407?429, 2003. ISSN 0010-
4817. 10.1023/A:1025786724466.
Hans van Halteren, R. Harald Baayen, Fiona J.
Tweedie, Marco Haverkort, and Anneke Neijt.
New machine learning methods demonstrate
the existence of a human stylome. Journal of
Quantitative Linguistics, pages 65?77, 2005.
77
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 102?106,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Temporal classification for historical Romanian texts
Alina Maria Ciobanu
Liviu P. Dinu
Octavia-Maria S, ulea
Faculty of Mathematics and Computer Science
Center for Computational Linguistics
University of Bucharest
alinamaria.ciobanu@yahoo.com
ldinu@fmi.unibuc.ro
mary.octavia@gmail.com
Anca Dinu
Faculty of Foreign Languages
University of Bucharest
anca d dinu@yahoo.com
Vlad Niculae
University of Wolverhampton
vlad@vene.ro
Abstract
In this paper we look at a task at border
of natural language processing, historical
linguistics and the study of language de-
velopment, namely that of identifying the
time when a text was written. We use
machine learning classification using lexi-
cal, word ending and dictionary-based fea-
tures, with linear support vector machines
and random forests. We find that lexical
features are the most helpful.
1 Introduction
Text dating, or determination of the time period
when it was written, proves to be a useful com-
ponent in NLP systems that can deal with such
diachronistically dynamic inputs (Moura?o et al,
2008). Besides this, the models that can perform
such classification can shine light on less than ob-
vious changes of certain features.
The knowledge captured in such systems can
prove useful in transferring modern language re-
sources and tools to historical domains (Meyer,
2011). Automatic translation systems between
and across language stages, as in the corpus in-
troduced by (Magaz, 2006), can benefit from the
identification of feature variation over time.
In this paper we study the problem of super-
vised temporal text classification across genres
and authors. The problem turns out to be solvable
to a very high degree of accuracy.
2 Related Work
The influence of the temporal effects in automatic
document classification is analyzed in (Moura?o et
al., 2008) and (Salles et al, 2010). The authors
state that a major challenge in building text clas-
sification models may be the change which occurs
in the characteristics of the documents and their
classes over time (Moura?o et al, 2008). There-
fore, in order to overcome the difficulties which
arise in automatic classification when dealing with
documents dating from different epochs, identify-
ing and accounting for document characteristics
changing over time (such as class frequency, rela-
tionships between terms and classes and the sim-
ilarity among classes over time (Moura?o et al,
2008)) is essential and can lead to a more accurate
discrimination between classes.
In (Dalli and Wilks, 2006) a method for clas-
sification of texts and documents based on their
predicted time of creation is successfully applied,
proving that accounting for word frequencies and
their variation over time is accurate. In (Kumar
et al, 2012) the authors argue as well for the ca-
pability of this method, of using words alone, to
determine the epoch in which a text was written or
the time period a document refers to.
The effectiveness of using models for individu-
als partitions in a timeline with the purpose of pre-
dicting probabilities over the timeline for new doc-
uments is investigated in (Kumar et al, 2011) and
(Kanhabua and N?rva?g, 2009). This approach,
based on the divergence between the language
model of the test document and those of the time-
line partitions, was successfully employed in pre-
dicting publication dates and in searching for web
pages and web documents.
In (de Jong et al, 2005) the authors raise the
problem of access to historical collections of doc-
uments, which may be difficult due to the differ-
ent historical and modern variants of the text, the
less standardized spelling, words ambiguities and
102
other language changes. Thus, the linking of cur-
rent word forms with their historical equivalents
and accurate dating of texts can help reduce the
temporal effects in this regard.
Recently, in (Mihalcea and Nastase, 2012), the
authors introduced the task of identifying changes
in word usage over time, disambiguating the epoch
at word-level.
3 Approach
3.1 Datasets used
In order to investigate the diachronic changes and
variations in the Romanian lexicon over time, we
used copora from five different stages in the evo-
lution of the Romanian language, from the 16th
to the 20th century. The 16th century represents
the beginning of the Romanian writing. In (Dim-
itrescu, 1994, p. 13) the author states that the mod-
ern Romanian vocabulary cannot be completely
understood without a thorough study of the texts
written in this period, which should be consid-
ered the source of the literary language used to-
day. In the 17th century, some of the most im-
portant cultural events which led to the develop-
ment of the Romanian language are the improve-
ment of the education system and the establish-
ing of several printing houses (Dimitrescu, 1994,
p. 75). According to (Lupu, 1999, p. 29), in
the 18th century a diversification of the philologi-
cal interests in Romania takes place, through writ-
ing the first Romanian-Latin bilingual lexicons,
the draft of the first monolingual dictionary, the
first Romanian grammar and the earliest transla-
tions from French. The transition to the Latin al-
phabet, which was a significant cultural achieve-
ment, is completed in the 19th century. The Cyril-
lic alphabet is maintained in Romanian writing
until around 1850, afterwards being gradually re-
placed with the Latin alphabet (Dimitrescu, 1994,
p. 270). The 19th century is marked by the conflict
(and eventually the compromise) between etymol-
ogism and phonetism in Romanian orthography.
In (Maiorescu, 1866) the author argues for apply-
ing the phonetic principle and several reforms are
enforced for this purpose. To represent this pe-
riod, we chose the journalism texts of the leading
Romanian poet Mihai Eminescu. He had a cru-
cial influence on the Romanian language and his
contribution to modern Romanian development is
highly appreciated. In the 20th century, some vari-
ations regarding the usage of diacritics in Roma-
nian orthography are noticed.
Century Corpus Nwordstype token
16
Codicele Todorescu 3,799 15,421
Codicele Martian 394 920
Coresi, Evanghelia cu ??nva?t?a?tura? 10,361 184,260
Coresi, Lucrul apostolesc 7,311 79,032
Coresi, Psaltirea slavo-roma?na? 4,897 36,172
Coresi, Ta?rgul evangheliilor 6,670 84,002
Coresi, Tetraevanghelul 3,876 36,988
Manuscrisul de la Ieud 1,414 4,362
Palia de la Ora?s?tie 6,596 62,162
Psaltirea Hurmuzaki 4,851 32,046
17
The Bible 15,437 179,639
Miron Costin, Letopiset?ul T?a?rii Moldovei 6,912 70,080
Miron Costin, De neamul moldovenilor 5,499 31,438
Grigore Ureche, Letopiset?ul T?a?rii Moldovei 5,958 55,128
Dosoftei, Viat?a si petreacerea sfint?ilor 23,111 331,363
Varlaam Motoc, Cazania 10,179 154,093
Varlaam Motoc, Ra?spunsul ??mpotriva 2,486 14,122
Catehismului calvinesc
18
Antim Ivireanul, Opere 11,519 123,221
Axinte Uricariul, Letopiset?ul T?a?rii 16,814 147,564
Roma?nesti s?i al T?a?rii Moldovei
Ioan Canta, Letopiset?ul T?a?rii Moldovei
Dimitrie Cantemir, Istoria ieroglifica? 13,972 130,310
Dimitrie Eustatievici Bras?oveanul, 5,859 45,621
Gramatica roma?neasca?
Ion Neculce, O sama? de cuvinte 9,665 137,151
19
Mihai Eminescu, Opere, v. IX 27,641 227,964
Mihai Eminescu, Opere, v. X 30,756 334,516
Mihai Eminescu, Opere, v. XI 27,316 304,526
Mihai Eminescu, Opere, v. XII 28,539 308,518
Mihai Eminescu, Opere, v. XIII 26,242 258,234
20
Eugen Barbu, Groapa 14,461 124,729
Mircea Cartarescu, Orbitor 35,486 306,541
Marin Preda, Cel mai iubit dintre pa?ma?nteni 28,503 388,278
Table 1: Romanian corpora: words
For preprocessing our corpora, we began by re-
moving words that are irrelevant for our investiga-
tion, such as numbers. We handled word bound-
aries and lower-cased all words. We computed,
for each text in our corpora, the number of words
(type and token). The results are listed in Table
1. For identifying words from our corpora in dic-
tionaries, we performed lemmatization. The in-
formation provided by the machine-readable dic-
tionary dexonline 1 regarding inflected forms al-
lowed us to identify lemmas (where no semantic
or part-of-speech ambiguities occurred) and to fur-
ther lookup the words in the dictionaries. In our
investigations based on dexonline we decided to
use the same approach as in (Mihalcea and Nas-
tase, 2012) and to account only for unambiguous
words. For example, the Romanian word ai is
morphologically ambiguous, as we identified two
corresponding lemmas: avea (verb, meaning to
have) and ai (noun, meaning garlic). The word
ama?nare is semantically ambiguous, having two
different associated lemmas, both nouns: ama?nar
(which means flint) and ama?na (which means to
postpone). We do not use the POS information di-
1http://dexonline.ro
103
rectly, but we use dictionary occurrence features
only for unambiguous words.
The database of dexonline aggregates informa-
tion from over 30 Romanian dictionaries from dif-
ferent periods, from 1929 to 2012, enabling us to
investigate the diachronic evolution of the Roma-
nian lexicon. We focused on four different sub-
features:
? words marked as obsolete in dexonline defi-
nitions (we searched for this tag in all dictio-
naries)
? words which occur in the dictionaries of ar-
chaisms (2 dictionaries)
? words which occur in the dictionaries pub-
lished before 1975 (7 dictionaries)
? words which occur in the dictionaries pub-
lished after 1975 (31 dictionaries)
As stated before, we used only unambiguous
words with respect to the part of speech, in order to
be able to uniquely identify lemmas and to extract
the relevant information. The aggregated counts
are presented in table 2.
Sub-feature 16 17 18 19 20
archaism type 1,590 2,539 2,114 1,907 2,140
token 5,652 84,804 56,807 120,257 62,035
obsolete type 5,652 8,087 7,876 9,201 8,465
token 172,367 259,367 199,899 466,489 279,654
< 1975 type 11,421 17,200 16,839 35,383 34,353
token 311,981 464,187 337,026 885,605 512,156
> 1975 type 12,028 18,948 18,945 42,855 41,643
token 323,114 480,857 356,869 943,708 541,258
Table 2: Romanian corpora: dexonline sub-
features
3.2 Classifiers and features
The texts in the corpus were split into chunks of
500 sentences in order to increase the number of
sample entries and have a more robust evaluation.
We evaluated all possible combinations of the four
feature sets available:
? lengths: average sentence length in words,
average word length in letters
? stopwords: frequency of the most common
50 words in all of the training set:
de s, i ??n a la cu au no o sa? ca? se pe
din s ca i lui am este fi l e dar pre ar
va? le al dupa? fost ??ntr ca?nd el daca?
ne n ei sau sunt
Century Precision Recall F1-score texts
16 1.00 1.00 1.00 16
17 1.00 0.88 0.94 17
18 0.88 1.00 0.93 14
19 1.00 1.00 1.00 23
20 1.00 1.00 1.00 21
average/ total 0.98 0.98 0.98 91
Table 4: Random Forest test scores using all fea-
tures and aggregating over 50 trees
? endings: frequency of all word suffixes of
length up to three, that occur at least 5 times
in the training set
? dictionary: proportion of words matching
the dexonline filters described above
The system was put together using the scikit-
learn machine learning library for Python (Pe-
dregosa et al, 2011), which provides an imple-
mentation of linear support vector machines based
on liblinear (Fan et al, 2008), an implementation
of random forests using an optimised version of
the CART algorithm.
4 Results
The hyperparameters (number of trees, in the ran-
dom forest case, and C, for the SVM) were op-
timized using 3 fold cross-validation for each of
the feature sets. For the best feature sets, denoted
with an asterisk in table 3, the test results and hy-
perparameter settings are presented in tables 4 and
5.
The results show that the nonlinear nature of
the random forest classifier is important when us-
ing feature sets so different in nature. However, a
linear SVM can perform comparably, using only
the most important features. The misclassifica-
tions that do occur are not between very distant
centuries.
5 Conclusions
We presented two classification systems, a linear
SVM one and a nonlinear random forest one, for
solving the temporal text classification problem on
Romanian texts. By far the most helpful features
turn out to be lexical, with dictionary-based histor-
ical information less helpful than expected. This is
probably due to inaccuracy and incompleteness of
104
lengths stopwords endings dictionary RF SVM
False False False False 25.38 25.38
False False False True 86.58 79.87
False False True False 98.51 95.16
False False True True 97.76 97.02
False True False False 98.51 96.27
False True False True 98.51 94.78
False True True False 98.88 *98.14
False True True True 98.51 97.77
True False False False 68.27 22.01
True False False True 92.92 23.13
True False True False 98.14 23.89
True False True True 98.50 23.14
True True False False 98.14 23.53
True True False True 98.51 25.00
True True True False 98.88 23.14
True True True True *99.25 22.75
Table 3: Cross-validation accuracies for different feature sets. The score presented is the best one over
all of the hyperparameter settings, averaged over the folds.
Century Precision Recall F1-score texts
16 1.00 1.00 1.00 16
17 1.00 1.00 1.00 17
18 1.00 0.93 0.96 14
19 1.00 1.00 1.00 23
20 0.95 1.00 0.98 21
average/ total 0.99 0.99 0.99 91
Table 5: Linear SVC test scores using only stop-
words and word endings for C = 104.
dictionary digitization, along with ambiguities that
might need to be dealt with better.
We plan to further investigate feature impor-
tances and feature selection for this task to ensure
that the classifiers do not actually fit authorship or
genre latent variables.
Acknowledgements
The authors thank the anonymous reviewers for
their helpful and constructive comments. The con-
tribution of the authors to this paper is equal. Re-
search supported by a grant of the Romanian Na-
tional Authority for Scientific Research, CNCS ?
UEFISCDI, project number PN-II-ID-PCE-2011-
3-0959.
References
Angelo Dalli and Yorick Wilks. 2006. Automatic dat-
ing of documents and temporal text classification.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, Sydney,, pages
17?-22.
Franciska de Jong, Henning Rode, and Djoerd Hiem-
stra. 2005. Temporal language models for the dis-
closure of historical text. In Humanities, computers
and cultural heritage: Proceedings of the XVIth In-
ternational Conference of the Association for His-
tory and Computing.
Florica Dimitrescu. 1994. Dinamica lexicului
roma?nesc - ieri s?i azi. Editura Logos. In Romanian.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874, June.
Nattiya Kanhabua and Kjetil N?rva?g. 2009. Using
temporal language models for document dating. In
ECML/PKDD (2), pages 738?741.
Abhimanu Kumar, Matthew Lease, and Jason
Baldridge. 2011. Supervised language modeling
for temporal resolution of texts. In CIKM, pages
2069?2072.
Abhimanu Kumar, Jason Baldridge, Matthew Lease,
and Joydeep Ghosh. 2012. Dating texts without ex-
plicit temporal cues. CoRR, abs/1211.2290.
Coman Lupu. 1999. Lexicografia roma?neasca? ??n pro-
cesul de occidentalizare latino-romanica? a limbii
roma?ne moderne. Editura Logos. In Romanian.
105
Judit Martinez Magaz. 2006. Tradi imt (xx-xxi):
Recent proposals for the alignment of a diachronic
parallel corpus. International Computer Archive of
Modern and Medieval English Journal, (30).
Titu Maiorescu. 1866. Despre scrierea limbei ruma?ne.
Edit?iunea s?i Imprimeria Societa?t?ei Junimea. In Ro-
manian.
Roland Meyer. 2011. New wine in old wineskins?
tagging old russian via annotation projection from
modern translations. Russian Linguistcs.
Rada Mihalcea and Vivi Nastase. 2012. Word epoch
disambiguation: Finding how words change over
time. In ACL (2), pages 259?263. The Association
for Computer Linguistics.
Fernando Moura?o, Leonardo Rocha, Renata Arau?jo,
Thierson Couto, Marcos Gonc?alves, and Wag-
ner Meira Jr. 2008. Understanding temporal aspects
in document classification. In WSDM ?08 Proceed-
ings of the 2008 International Conference on Web
Search and Data Mining, pages 159?170.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830, Oct.
Thiago Salles, Leonardo Rocha, Fernando Moura?o,
Gisele L. Pappa, Lucas Cunha, Marcos Gonc?alves,
and Wagner Meira Jr. 2010. Automatic document
classification temporally robust. Journal of Infor-
mation and Data Management, 1:199?211, June.
106
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 104?113,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Quantitative Insight into the Impact of Translation on Readability
Alina Maria Ciobanu, Liviu P. Dinu
Center for Computational Linguistics, University of Bucharest
Faculty of Mathematics and Computer Science, University of Bucharest
alina.ciobanu@my.fmi.unibuc.ro, ldinu@fmi.unibuc.ro
Abstract
In this paper we investigate the impact
of translation on readability. We propose
a quantitative analysis of several shallow,
lexical and morpho-syntactic features that
have been traditionally used for assessing
readability and have proven relevant for
this task. We conduct our experiments
on a parallel corpus of transcribed parlia-
mentary sessions and we investigate read-
ability metrics for the original segments of
text, written in the language of the speaker,
and their translations.
1 Introduction
Systems for automatic readability assessment have
been studied since the 1920s and have received an
increasing attention during the last decade. Early
research on readability assessment focused only
on shallow language properties, but nowadays na-
tural language processing technologies allow the
investigation of a wide range of factors which in-
fluence the ease which a text is read and under-
stood with. These factors correspond to differ-
ent levels of linguistic analysis, such as the le-
xical, morphological, semantic, syntactic or dis-
course levels. However, readability depends not
only on text properties, but also on characteristics
of the target readers. Aspects such as background
knowledge, age, level of literacy and motivation of
the expected audience should be considered when
developing a readability assessment system. Al-
though most readability metrics were initially de-
veloped for English, current research has shown a
growing interest in other languages, such as Ger-
man, French, Italian or Portuguese.
Readability assessment systems are relevant for
a wide variety of applications, both human- and
machine-oriented (Dell?Orletta et al., 2011). Se-
cond language learners and people with disabili-
ties or low literacy skills benefit from such sys-
tems, which provide assistance in selecting read-
ing material with an appropriate level of com-
plexity from a large collection of documents ?
for example, the documents available on the web
(Collins-Thompson, 2011). Within the medical
domain, the investigation of the readability level
of medical texts helps developing well-suited ma-
terials to increase the level of information for pre-
venting diseases (Richwald et al., 1989) and to au-
tomatically adapt technical documents to various
levels of medical expertise (Elhadad and Sutaria,
2007). For natural language processing tasks such
as machine translation (Stymne et al., 2013), text
simplification (Aluisio et al., 2010), speech recog-
nition (Jones et al., 2005) or document summa-
rization (Radev and Fan, 2000), readability ap-
proaches are employed to assist the process and
to evaluate and quantify its performance and ef-
fectiveness.
1.1 Related Work
Most of the traditional readability approaches in-
vestigate shallow text properties to determine the
complexity of a text. These readability metrics are
based on assumptions which correlate surface fea-
tures with the linguistic factors which influence
readability. For example, the average number of
characters or syllables per word, the average num-
ber of words per sentence and the percentage of
words not occurring among the most frequent n
words in a language are correlated with the lexi-
cal, syntactic and, respectively, the semantic com-
plexity of the text. The Flesch-Kincaid measure
(Kincaid et al., 1975) employs the average number
of syllables per word and the average number of
words per sentence to assess readability, while the
Automated Readability Index (Smith and Senter,
1967) and the Coleman-Liau metric (Coleman and
Liau, 1975) measure word length based on charac-
ter count rather than syllable count; they are func-
104
tions of both the average number of characters per
word and the average number of words per sen-
tence. Gunning Fog (Gunning, 1952) and SMOG
(McLaughlin, 1969) account also for the percent-
age of polysyllabic words and the Dale-Chall for-
mula (Dale and Chall, 1995) relies on word fre-
quency lists to assess readability. The traditional
readability approaches are not computationally ex-
pensive, but they are only a coarse approximation
of the linguistic factors which influence readabil-
ity (Pitler and Nenkova, 2008). According to Si
and Callan (2001), the shallow features employed
by standard readability indices are based on as-
sumptions about writing style that may not apply
in all situations.
Along with the development of natural lan-
guages processing tools and machine learning
techniques, factors of increasing complexity , cor-
responding to various levels of linguistic analy-
sis, have been taken into account in the study of
readability assessment. Si and Callan (2001) and
Collins-Thompson and Callan (2004) use statisti-
cal language modeling and Petersen and Ostendorf
(2009) combine features from statistical language
models, syntactic parse trees and traditional met-
rics to estimate reading difficulty. Feng (2009) ex-
plores discourse level attributes, along with lexical
and syntactic features, and emphasizes the value of
the global semantic properties of the text for pre-
dicting text readability. Pitler and Nenkova (2008)
propose and analyze two perspectives for the task
of readability assessment: prediction and ranking.
Using various features, they reach the conclusion
that only discourse level features exhibit robust-
ness across the two tasks. Vajjala and Meurers
(2012) show that combining lexical and syntac-
tic features with features derived from second lan-
guage acquisition research leads to performance
improvements.
Although most readability approaches deve-
loped so far deal with English, the development
of adequate corpora for experiments and the study
of readability features tailored for other languages
have received increasing attention. For Italian,
Franchina and Vacca (1986) propose the Flesch-
Vacca formula, which is an adaptation of the
Flesch index (Flesch, 1946). Another metric de-
veloped for Italian is Gulpease (Lucisano and
Piemontese, 1988), which uses characters instead
of syllables to measure word length and thus re-
quires less resources. Dell?Orletta et al. (2011)
combine traditional, morpho-syntactic, lexical and
syntactic features for building a readability model
for Italian, while Tonelli et al. (2012) propose a
system for readability assessment for Italian in-
spired by the principles of Coh-Metrix (Graesser
et al., 2004). For French, Kandel and Moles
(1958) propose an adaptation of the Flesch for-
mula and Franc?ois and Miltsakaki (2012) inves-
tigate a wide range of classic and non-classic fea-
tures to predict readability level using a dataset for
French as a foreign language. Readability assess-
ment was also studied for Spanish (Huerta, 1959)
and Portuguese (Aluisio et al., 2010) using fea-
tures derived from previous research on English.
1.2 Readability of Translation
According to Sun (2012), the reception of a trans-
lated text is related to cross-cultural readability.
Translators need to understand the particularities
of both the source and the target language in order
to transfer the meaning of the text from one lan-
guage to another. This process can be challenging,
especially for languages with significant structure
differences, such as English and Chinese. The
three-step system of translation (analysis, trans-
fer and restructuring) presented by Nida and Taber
(1969) summarizes the process and emphasizes
the importance of a proper understanding of the
source and the target languages. While rendering
the source language text into the target language, it
is also important to maintain the style of the docu-
ment. Various genres of text might be translated
for different purposes, which influence the choice
of the translation strategy. For example, for politi-
cal speeches the purpose is to report exactly what
is communicated in a given text (Trosborg, 1997).
Parallel corpora are very useful in studying
the properties of translation and the relationships
between source language and target language.
Therefore, the corpus-based research has become
more and more popular in translation research.
Using the Europarl (Koehn, 2005) parallel cor-
pus, van Halteren (2008) investigates the auto-
matic identification of the source language of Eu-
ropean Parliament speeches, based on frequency
counts of word n-grams. Islam and Mehler (2012)
draw attention to the absence of adequate corpora
for studies on translation and propose a resource
suited for this purpose.
105
2 Our Approach and Methodology
The problem that we address in this paper is
whether human translation has an impact on read-
ability. Given a text T
1
in a source language
L
1
and its translations in various target languages
L
2
, ..., L
n
, how does readability vary? Is the orig-
inal text in L
1
easier to read and understand than
its translation in a target language L
i
? Which lan-
guage is closest to the source language, in terms
of readability? We investigate several shallow,
lexical and morpho-syntactic features that have
been widely used and have proven relevant for as-
sessing readability. We are interested in observ-
ing the differences between the feature values ob-
tained for the original texts and those obtained for
their translations. Although some of the metrics
(such as average word length) might be language-
specific, most of them are language-independent
and a comparison between them across languages
is justified. The 10 readability metrics that we ac-
count for are described in Section 3.2.
We run our experiments on Europarl (Koehn,
2005), a multilingual parallel corpus which is de-
scribed in detail in Section 3.1. We investigate 5
Romance languages (Romanian, French, Italian,
Spanish and Portuguese) and, in order to excerpt
an adequate dataset of parallel texts, we adopt a
strategy similar to that of van Halteren (2008):
given n languagesL
1
, ..., L
n
, we apply the follow-
ing steps:
1. we select L
1
as the source language
2. we excerpt the collection of segments of text
T
1
for which L
1
is the source language
3. we identify the translations T
2
, ..., T
n
of T
1
in
the target languages L
2
, ..., L
n
4. we compute the readability metrics for
T
1
, ..., T
n
5. we repeat steps 1 ? 4 using each language
L
2
, ..., L
n
as the source language, one at a
time
We propose two approaches to quantify and
evaluate the variation in the readability feature val-
ues from the original texts to their translations: a
distance-based method and a multi-criteria tech-
nique based on rank aggregation.
3 Experimental Setup
3.1 Data
Europarl (Koehn, 2005) is a multilingual paral-
lel corpus extracted from the proceedings of the
European Parliament. Its main intended use is
as aid for statistical machine translation research
(Tiedemann, 2012). The corpus is tokenized and
aligned in 21 languages. The files contain annota-
tions for marking the document (<chapter>), the
speaker (<speaker>) and the paragraph (<p>).
Some documents have the attribute language for
the speaker tag, which indicates the language used
by the original speaker. Another way of annotating
the original language is by having the language ab-
breviation written between parentheses at the be-
ginning of each segment of text. However, there
are segments where the language is not marked in
either of the two ways. We account only for sen-
tences for which the original language could be
determined and we exclude all segments showing
inconsistent values.
We use the following strategy: because for the
Romance languages there are very few segments
of text for which the language attribute is consis-
tent across all versions, we take into account an at-
tribute L if all other Romance languages mention
it. For example, given a paragraph P in the Ro-
manian subcorpus, we assume that the source lan-
guage for this paragraph is Romanian if all other
four subcorpora (Italian, French, Spanish and Por-
tuguese) mark this paragraph P with the tag RO
for language. Thus, we obtain a collection of
segments of text for each subcorpus. We iden-
tify 4,988 paragraphs for which Romanian is the
source language, 13,093 for French, 7,485 for Ital-
ian, 5,959 for Spanish and 8,049 for Portuguese.
Because we need sets of approximately equal size
for comparison, we choose, for each language, a
subset equal with the size of the smallest subset,
i.e., we keep 4,988 paragraphs for each language.
Note that in this corpus paragraphs are aligned
across languages, but the number of sentences
may be different. For example, the sentence
?UE trebuie s?a fie ambit?ioas?a ??n combaterea
schimb?arilor climatice, iar rolul energiei nucle-
are s?i energiilor regenerabile nu poate fi negli-
jat.?
1
, for which Romanian is the source language,
1
Translation into English: ?The EU must be ambitious in
the battle against climate change, which means that the role
of nuclear power and renewable energy sources cannot be
discounted.?
106
is translated into French in two sentences: ?L?UE
doit se montrer ambitieuse dans sa lutte contre
les changements climatiques.? and ?L??energie
nucl?eaire et les sources d??energie renouvelables
ne peuvent donc pas ?etre ?ecart?ees.?. Therefore, we
match paragraphs, rather than sentences, across
languages.
As a preprocessing step, we discard the tran-
scribers? descriptions of the parliamentary ses-
sions (such as ?Applause?, ?The President in-
terrupted the speaker? or ?The session was sus-
pended at 19.30 and resumed at 21.00?).
According to van Halteren (2008), translations
in the European Parliament are generally made by
native speakers of the target language. Transla-
tion is an inherent part of the political activity
(Sch?affner and Bassnett, 2010) and has a high
influence on the way the political speeches are
perceived. The question posed by Sch?affner and
Bassnett (2010) ?What exactly happens in the
complex processes of recontextualisation across
linguistic, cultural and ideological boundaries??
summarizes the complexity of the process of trans-
lating political documents. Political texts might
contain complex technical terms and elaborated
sentences. Therefore, the results of our experi-
ments are probably domain-specific and cannot be
generalized to other types of texts. Although par-
liamentary documents probably have a low read-
ability level, our investigation is not negatively in-
fluenced by the choice of corpus because we are
consistent across all experiments in terms of text
gender and we report results obtained solely by
comparison between source and target languages.
3.2 Features
We investigate several shallow, lexical and
morpho-syntactic features that were traditionally
used for assessing readability and have proven
high discriminative power within readability met-
rics.
3.2.1 Shallow Features
Average number of words per sentence. The
average sentence length is one of the most widely
used metrics for determining readability level and
was employed in numerous readability formulas,
proving to be most meaningful in combined evi-
dence with average word frequency. Feng et al.
(2010) find the average sentence length to have
higher predictive power than all the other lexical
and syllable-based features they used.
Average number of characters per word. It
is generally considered that frequently occurring
words are usually short, so the average number
of characters per word was broadly used for mea-
suring readability in a robust manner. Many read-
ability formulas measure word length in syllables
rather than letters, but this requires additional re-
sources for syllabication.
3.2.2 Lexical Features
Percentage of words from the basic lexicon.
Based on the assumption that more common
words are easier to understand, the percentage of
words not occurring among the most frequent n
in the language is a commonly used metric to ap-
proximate readability. To determine the percent-
age of words from the basic lexicon, we employ
the representative vocabularies for Romance lan-
guages proposed by Sala (1988).
Type/Token Ratio. The proportion between the
number of lexical types and the number of to-
kens indicates the range of use of vocabulary. The
higher the value of this feature, the higher the vari-
ability of the vocabulary used in the text.
3.2.3 Morpho-Syntactic Features
Relative frequency of POS unigrams. The ra-
tio for 5 parts of speech (verbs, nouns, pronouns,
adjectives and adverbs), computed individually
on a per-token basis. This feature assumes that
the probability of a token is context-independent.
For lemmatization and part of speech tagging
we use the DexOnline
2
machine-readable dictio-
nary for Romanian and the FreeLing
3
(Padr?o and
Stanilovsky, 2012; Padr?o, 2011; Padr?o et al., 2010;
Atserias et al., 2006; Carreras et al., 2004) lan-
guage analysis tool suite for French, Italian, Span-
ish and Portuguese.
Lexical density. The proportion of content
words (verbs, nouns, adjectives and adverbs),
computed on a per-token basis. Grammatical fea-
tures were shown to be useful in readability pre-
diction (Heilman et al., 2007).
4 Results Analysis
Our main purpose is to investigate the variabil-
ity of the feature values from the original texts to
their translations. In Table 1 we report the values
2
http://dexonline.ro
3
http://nlp.lsi.upc.edu/freeling
107
obtained for 10 readability metrics computed for
the Europarl subcorpora for Romanian, French,
Italian, Spanish and Portuguese. The readability
metrics we computed lead to several immediate
remarks. We notice that, generally, when repre-
senting the values for a feature F on the real axis,
the values corresponding to the translations are not
placed on the same side of the value correspond-
ing to the original text. For example, considering
feature F3 (the percentage of words from the ba-
sic lexicon), and taking Romanian as the source
language, we observe that the value for the origi-
nal text is between Italian (on the left side) and the
other languages (on the right side).
In the absence of a widely-accepted readability
metric, such as the Flesch-Kincaid formula or the
Automated Readability Index, for all 5 Romance
languages, we choose two other ways to evalu-
ate the results obtained after applying the 10 read-
ability features: a distance-based evaluation and a
multi-criteria approach.
In order to compute distance measures reliably,
we normalize feature values using the following
formula:
f
?
i
=
f
i
? f
min
f
max
? f
min
,
where f
min
is the minimum value for feature F
and f
max
is the maximum value for feature F. For
example, if F = F1 and the source language is Ro-
manian, then f
min
= 26.2 and f
max
= 29.0.
4.1 Preliminaries
In this subsection we shortly describe the two tech-
niques used. The experimented reader can skip
this subsection.
4.1.1 Rank Aggregation
Rank distance (Dinu and Dinu, 2005) is a met-
ric used for measuring the similarity between two
ranked lists. A ranking of a set of n objects can
be represented as a permutation of the integers
1, 2, ..., n. S is a set of ranking results, ? ? S.
?(i) represents the rank of object i in the ranking
result ?. The rank distance is computed as:
?(?, ?) =
n
?
i=1
|?(i)? ?(i)|
The ranks of the elements are given from bot-
tom up, i.e., from n to 1, in a Borda order. The
elements which do not occur in any of the rank-
ings receive the rank 0.
In a selection process, rankings are issued for
a common decision problem, therefore a ranking
that ?combines? all the original (base) rankings is
required. One common-sense solution is finding a
ranking that is as close as possible to all the par-
ticular rankings.
Formally, given m partial rankings T =
?
1
, ?
2
, ..., ?
m
, over a universe U , the rank aggre-
gation problem requires a partial ranking that is
as close as possible to all these rankings to be de-
termined. In other words, it requires a means of
combining the rankings. There are many ways to
solve this problem, one of which is by trying to
find a ranking such that the sum of rank distances
between it and the given rankings is minimal. In
other words, find ? such that:
?(?, T ) =
?
??T
?(?, ?)
is minimal. The set of all rankings that minimize
?(?, T ) is called the aggregations set and is de-
noted by agr(T ).
Apart from many paradoxes of different aggre-
gation methods, this problem is NP-hard for most
non-trivial distances (e.g., for edit distance, see
(de la Higuera and Casacuberta, 2000)). Dinu
and Manea (2006) show that the rank aggregation
problem using rank distance, which minimizes the
sum ?(?, T ) of the rank distances between the ag-
gregation and each given ranking, can be reduced
to solving |U| assignment problems, where U is
the universe of objects. Let n = #U . The time
complexity to obtain one such aggregation (there
may be more than one) is O(n
4
).
We then transform the aggregation problem in
a categorization problem as follows (Dinu and
Popescu, 2008): for a multiset L of rankings, we
determine all the aggregations of L and then we
apply voting on the set of agr(L).
4.1.2 Cosine Distance
Cosine distance is a metric which computes the
angular cosine distance between two vectors of an
inner product space. Given two vectors of fea-
tures, A and B, the cosine distance is represented
as follows:
?(A,B) = 1?
?
n
i=1
A
i
?B
i
?
?
n
i=1
(A
i
)
2
?
?
?
n
i=1
(B
i
)
2
When used in positive space, the cosine distance
ranges from 0 to 1.
108
Source Target Features
Language Language F1 F2 F3 F4 F5 F6 F7 F8 F9 F10
RO
RO 26.2 5.61 0.67 0.06 0.66 0.15 0.29 0.16 0.05 0.11
FR 29.0 5.06 0.79 0.03 0.59 0.13 0.35 0.06 0.04 0.06
IT 27.4 5.57 0.63 0.04 0.61 0.16 0.30 0.10 0.04 0.06
ES 28.3 5.18 0.81 0.04 0.53 0.15 0.24 0.09 0.03 0.03
PT 26.8 5.31 0.78 0.04 0.58 0.14 0.30 0.08 0.04 0.02
FR
RO 24.6 5.35 0.70 0.06 0.64 0.17 0.26 0.14 0.06 0.13
FR 27.4 4.86 0.81 0.04 0.58 0.14 0.32 0.05 0.06 0.09
IT 25.7 5.46 0.65 0.05 0.61 0.17 0.28 0.09 0.05 0.07
ES 26.3 5.11 0.82 0.05 0.53 0.16 0.23 0.08 0.04 0.04
PT 25.1 5.21 0.80 0.05 0.58 0.16 0.29 0.07 0.05 0.02
IT
RO 29.7 5.46 0.69 0.06 0.62 0.16 0.27 0.15 0.05 0.12
FR 32.4 5.00 0.80 0.04 0.58 0.14 0.33 0.06 0.05 0.08
IT 30.9 5.48 0.64 0.05 0.61 0.16 0.28 0.10 0.05 0.07
ES 31.8 5.15 0.82 0.04 0.53 0.16 0.23 0.09 0.04 0.03
PT 30.5 5.28 0.79 0.04 0.58 0.15 0.29 0.07 0.05 0.02
ES
RO 27.6 5.33 0.70 0.06 0.64 0.17 0.26 0.14 0.06 0.13
FR 29.9 4.91 0.81 0.04 0.58 0.14 0.32 0.05 0.05 0.09
IT 27.9 5.45 0.66 0.05 0.60 0.17 0.28 0.09 0.05 0.08
ES 31.1 5.02 0.83 0.05 0.52 0.16 0.22 0.08 0.05 0.04
PT 28.2 5.17 0.81 0.05 0.57 0.16 0.28 0.07 0.05 0.02
PT
RO 29.3 5.58 0.67 0.05 0.65 0.15 0.28 0.16 0.05 0.12
FR 32.8 5.04 0.80 0.03 0.58 0.13 0.34 0.06 0.04 0.07
IT 30.9 5.56 0.62 0.04 0.60 0.15 0.29 0.10 0.04 0.06
ES 32.5 5.15 0.81 0.03 0.53 0.15 0.24 0.09 0.03 0.03
PT 30.9 5.28 0.79 0.04 0.57 0.14 0.30 0.08 0.04 0.02
Table 1: Values for readability metrics applied on Europarl. The first column represents the source
language (the language of the speaker). The second column represents the target language (the language
in which the text is written / translated). The features F1 - F10 are as follows:
? F1 - average number of words per sentence
? F2 - average number of characters per word
? F3 - percentage of words from the basic lexicon
? F4 - type / token ratio
? F5 - lexical density
? F6 - relative frequency of POS unigrams: verbs
? F7 - relative frequency of POS unigrams: nouns
? P8 - relative frequency of POS unigrams: adjectives
? F9 - relative frequency of POS unigrams: adverbs
? F10 - relative frequency of POS unigrams: pronouns
109
RO FR IT ES PT
RO ? 0.571 0.138 0.582 0.292
FR 0.513 ? 0.505 0.491 0.328
IT 0.075 0.416 ? 0.502 0.212
ES 0.531 0.423 0.545 ? 0.256
PT 0.300 0.227 0.252 0.275 ?
Table 2: Cosine distance between feature vectors.
The first column represents the source language
and the first line represents the target language.
4.2 Experiment Analysis: Original vs.
Translation
Our main goal is to determine a robust way to
evaluate the variation in readability from the origi-
nal texts to their translations, after applying the 10
readability features described in Section 3.2.
A natural approach is to use an evaluation
methodology based on a distance metric between
feature vectors to observe how close translations
are in various languages, with respect to readabil-
ity. The closer the distance is to 0, the more easily
can one language be translated into the other, in
terms of readability. Briefly, our first approach is
as follows: for each source language L in column
1 of Table 1, we consider the feature vector corre-
sponding to this language from column 2 and we
compute the cosine distance between this vector
and all the other 4 vectors remaining in column 2,
one for each target language. The obtained values
are reported in Table 2, on the line corresponding
to language L.
Table 2 provides not only information regard-
ing the closest language, but also the hierarchy of
languages in terms of readability. For example,
the closest language to Romanian is Italian, fol-
lowed by Portuguese, French and Spanish. Over-
all, the lowest distance between an original text
and its translation occurs when Italian is the source
language and Romanian the target language. The
highest distance is reported for translations from
Romanian into Spanish.
The second approach we use for investigating
the readability of translation is multi-criteria ag-
gregation: since the 10 monitored features can
be seen as individual classifiers for readability
(and in various papers they were used either in-
dividually or combined as representative features
for predicting readability), we experiment with a
multi-criteria aggregation of these metrics in order
to predict which language is closest to the source
language in terms of readability.
For segments of text having the source language
L, we consider each feature F
i
, one at a time, and
we compute the absolute value of the difference
between the F
i
value for the original text and the
F
i
values for its translations. Then, we sort the
values in ascending order, thus obtaining for each
language L and feature F
i
a ranking with 4 ele-
ments (one for each translation) determined as fol-
lows: the language having the lowest computed
absolute value is placed on the first position, the
language having the second to lowest computed
absolute value is placed on the second position,
and so on. Finally, we have, for each language L,
10 rankings (one for each feature) with 4 elements
(one for each translation), each ranking indicating
on the first position the target language which is
closest to the source language with regard to read-
ability measured by feature F
i
. In case of equal
values for the computed absolute distance, we con-
sider all possible rankings.
Given these rankings, the task we propose is to
determine which target language is closest to the
source language in terms of readability. To solve
this requirement, we apply multi-criteria aggrega-
tion based on rank distance. For each language, we
aggregate the 10 corresponding rankings and de-
termine the closest language with respect to read-
ability across translation. The results we obtain for
Romance languages after the rank aggregation are
as follows: the closest translation language for Ro-
manian is Italian (followed by Portuguese, Span-
ish and French). Conversely, for Italian the closest
language is Romanian (followed by Portuguese,
French and Spanish). For French, Portuguese oc-
cupies the first position in the ranking (followed
by Spanish, Italian and Romanian). For Spanish,
Portuguese ranks first (followed by Italian, French
and Romanian), while for Portuguese, Italian is
the closest language (followed by French, Spanish
and Romanian).
The obtained results are very similar to those
computed by the cosine distance and reported in
Table 2. The only difference regarding the closest
language in terms of readability is that rank ag-
gregation reports Italian as being closest to Por-
tuguese, while the cosine distance reports French
instead. However, the differences between the
first two ranked languages for Portuguese, namely
French and Italian, are insignificant.
110
1.5 1.0 0.5 0.0 0.5 1.0
1.0
0.5
0.0
0.5
1.0
RO_RO
RO_FR
RO_IT
RO_ES
RO_PT
FR_FR
FR_RO
FR_IT
FR_ES
FR_PT
IT_ITIT_RO
IT_FR
IT_ES
IT_PT
ES_ES
ES_RO
ES_FR
ES_IT ES_PT
PT_PT
PT_RO
PT_FR
PT_IT
PT_ES
originaltranslation
Figure 1: PCA. Languages are annotated in the figure as follows: L
1
L
2
, whereL
1
is the source language
and L
2
is the target language.
4.3 PCA: Original vs. Translation
In Figure 1 we employ Principal Component Anal-
ysis (PCA) to perform linear data reduction in or-
der to obtain a better representation of the read-
ability feature vectors without losing much infor-
mation. We use the Modular toolkit for Data Pro-
cessing (MDP), a Python data processing frame-
work (Zito et al., 2008). We observe that clusters
tend to be formed based on the target language.
rather than based on the source language. While
for Romanian and Italian the original texts are to
some extent isolated from their translations, for
French, Spanish and Portuguese the original texts
are more integrated within the groups of transla-
tions. The most compact cluster corresponds to
Romanian as a target language.
5 Conclusions
In this paper we investigate the behaviour of vari-
ous readability metrics across parallel translations
of texts from a source language to target lan-
guages. We focus on Romance languages and we
propose two methods for the analysis of the clos-
est translation, in terms of readability. Given a text
in a source language, we determine which of its
translations in various target languages is closest
to the original text with regard to readability. In
our future works, we plan to extend our analysis to
more languages, in order to cover a wider variety
of linguistic families. We are mainly interested in
the 21 languages covered by Europarl. Moreover,
we intend to enrich the variety of the texts, be-
ginning with an analysis of translations of literary
works. As far as resources are available, we plan
to investigate other readability metrics as well and
to combine our findings with the views of human
experts. We believe our method can provide valu-
able information regarding the difficulty of trans-
lation from one language into another in terms of
readability.
Acknowledgements
The authors thank the anonymous reviewers for
their helpful and constructive comments. The con-
tribution of the authors to this paper is equal. Re-
search supported by a grant of the Romanian Na-
tional Authority for Scientific Research, CNCS
UEFISCDI, project number PN-II-ID-PCE-2011-
3-0959.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability Assessment for
Text Simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, IUNLPBEA
2010, pages 1?9.
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonz?alez, Llu??s Padr?o, and Muntsa Padr?o.
2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In Proceedings of
111
the 5th International Conference on Language Re-
sources and Evaluation, LREC 2006, pages 2281?
2286.
Xavier Carreras, Isaac Chao, Llu??s Padr?o, and Muntsa
Padr?o. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, LREC 2004, pages 239?242.
Meri Coleman and T. L. Liau. 1975. A computer read-
ability formula designed for machine scoring. Jour-
nal of Applied Psychology, 60(2):283?284.
Kevyn Collins-Thompson and James P. Callan. 2004.
A Language Modeling Approach to Predicting
Reading Difficulty. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL 2004, pages 193?
200.
Kevyn Collins-Thompson. 2011. Enriching Informa-
tion Retrieval with Reading Level Prediction. In SI-
GIR 2011 Workshop on Enriching Information Re-
trieval.
Edgar Dale and Jeanne Chall. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline Books, Cambridge.
C. de la Higuera and F. Casacuberta. 2000. Topology
of Strings: Median String is NP-complete. Theoret-
ical Computer Science, 230(1-2):39?48.
Felice Dell?Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. READ?IT: Assessing Readabil-
ity of Italian Texts with a View to Text Simplifica-
tion. In Proceedings of the 2nd Workshop on Speech
and Language Processing for Assistive Technolo-
gies, SLPAT 2011, pages 73?83.
Anca Dinu and Liviu P. Dinu. 2005. On the Syllabic
Similarities of Romance Languages. In Proceed-
ings of the 6th International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
CICLing 2005, pages 785?788.
Liviu P. Dinu and Florin Manea. 2006. An Efficient
Approach for the Rank Aggregation Problem. The-
oretical Computer Science, 359(1):455?461.
Liviu P. Dinu and Marius Popescu. 2008. A Multi-
Criteria Decision Method Based on Rank Distance.
Fundamenta Informaticae, 86(1-2):79?91.
Noemie Elhadad and Komal Sutaria. 2007. Mining a
Lexicon of Technical Terms and Lay Equivalents. In
Proceedings of the Workshop on BioNLP 2007: Bi-
ological, Translational, and Clinical Language Pro-
cessing, BioNLP 2007, pages 49?56.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
No?emie Elhadad. 2010. A Comparison of Fea-
tures for Automatic Readability Assessment. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING 2010,
pages 276?284.
Lijun Feng. 2009. Automatic Readability Assessment
for People with Intellectual Disabilities. SIGAC-
CESS Access. Comput., (93):84?91.
Rudolf Flesch. 1946. The Art of plain talk. T. Harper.
Thomas Franc?ois and Eleni Miltsakaki. 2012. Do NLP
and Machine Learning Improve Traditional Read-
ability Formulas? In Proceedings of the First Work-
shop on Predicting and Improving Text Readability
for Target Reader Populations, PITR 2012, pages
49?57.
Valerio Franchina and Roberto Vacca. 1986. Adapta-
tion of Flesch readability index on a bilingual text
written by the same author both in Italian and En-
glish languages. Linguaggi, 3:47?49.
Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse, and Zhiqiang Cai. 2004. Coh-Metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments, and Computers,
36(2):193?202.
Robert Gunning. 1952. The technique of clear writing.
McGraw-Hill; Fouth Printing edition.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining
Lexical and Grammatical Features to Improve Read-
ability Measures for First and Second Language
Texts. In Proceedings of the Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL 2007, pages 460?467.
F. Huerta. 1959. Medida sencillas de lecturabilidad.
Consigna, 214:29?32.
Zahurul Islam and Alexander Mehler. 2012. Cus-
tomization of the Europarl Corpus for Translation
Studies. In Proceedings of the 8th International
Conference on Language Resources and Evaluation,
LREC 2012, pages 2505?2510.
Douglas Jones, Edward Gibson, Wade Shen, Neil Gra-
noien, Martha Herzog, Douglas Reynolds, and Clif-
ford Weinstein. 2005. Measuring Human Readabil-
ity of Machine Generated Text: Three Case Stud-
ies in Speech Recognition and Machine Translation.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
ICASSP 2005, pages 1009?1012.
L. Kandel and A. Moles. 1958. Application de l?indice
de Flesch a la langue franc?aise. Cahiers Etudes de
Radio-Television, 19:253?274.
J. Peter Kincaid, Lieutenant Robert P. Fishburne Jr.,
Richard L. Rogers, and Brad S. Chissom. 1975.
Derivation of new readability formulas (Automated
Readability Index, Fog Count and Flesch Reading
112
Ease formula) for Navy enlisted personnel. Re-
search Branch Report, Millington, TN: Chief of
Naval Training.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit, pages 79?86.
Pietro Lucisano and Maria Emanuela Piemontese.
1988. Gulpease. una formula per la predizione della
difficolt`a dei testi in lingua italiana. Scuola e Citt`a,
39:110?124.
G. Harry McLaughlin. 1969. Smog grading: A new
readability formula. Journal of Reading, 12(8):639?
646.
Eugene A. Nida and Charles R. Taber. 1969. The The-
ory and Practice of Translation. Leiden: E.J. Brill.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. FreeLing
3.0: Towards Wider Multilinguality. In Proceed-
ings of the 8th International Conference on Lan-
guage Resources and Evaluation, LREC 2012, pages
2473?2479.
Llu??s Padr?o, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castell?on. 2010. FreeLing
2.1: Five Years of Open-source Language Process-
ing Tools. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
LREC 2010, pages 931?936.
Llu??s Padr?o. 2011. Analizadores Multiling?ues en
FreeLing. Linguamatica, 3(2):13?20.
Sarah E. Petersen and Mari Ostendorf. 2009. A Ma-
chine Learning Approach to Reading Level Assess-
ment. Computer Speech and Language, 23(1):89?
106.
Emily Pitler and Ani Nenkova. 2008. Revisiting Read-
ability: A Unified Framework for Predicting Text
Quality. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2008, pages 186?195.
Dragomir R. Radev and Weiguo Fan. 2000. Auto-
matic Summarization of Search Engine Hit Lists. In
Proceedings of the ACL-2000 Workshop on Recent
Advances in Natural Language Processing and In-
formation Retrieval: Held in Conjunction with the
38th Annual Meeting of the Association for Compu-
tational Linguistics, RANLPIR 2000, pages 99?109.
Gary A. Richwald, Margarita Schneider-Mufnoz, and
R. Burciaga Valdez. 1989. Are Condom Instruc-
tions in Spanish Readable? Implications for AIDS
Prevention Activities for Hispanics. Hispanic Jour-
nal of Behavioral Sciences, 11(1):70?82.
Marius Sala. 1988. Vocabularul Reprezentativ al Lim-
bilor Romanice. Editura Academiei, Bucures?ti.
Christina Sch?affner and Susan Bassnett. 2010. Pol-
itics, Media and Translation - Exploring Syner-
gies. In Political Discourse, Media and Transla-
tion, pages 1?29. Newcastle upon Tyne: Cambridge
Scholars Publishing.
Luo Si and Jamie Callan. 2001. A Statistical Model
for Scientific Readability. In Proceedings of the
10th International Conference on Information and
Knowledge Management, CIKM 2001, pages 574?
576.
E.A. Smith and R.J. Senter. 1967. Automated read-
ability index. Wright-Patterson Air Force Base.
AMRL-TR-6620.
Sara Stymne, J?org Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical Machine Trans-
lation with Readability Constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics, NODALIDA 2013, pages 375?386.
Yifeng Sun. 2012. Translation and strategies for cross-
cultural communication. Chinese Translators Jour-
nal, 33(1):16?23.
J?org Tiedemann. 2012. Parallel Data, Tools and Inter-
faces in OPUS. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, LREC 2012, pages 2214?2218.
Sara Tonelli, Ke Tran Manh, and Emanuele Pianta.
2012. Making Readability Indices Readable. In
Proceedings of the 1st Workshop on Predicting and
Improving Text Readability for Target Reader Popu-
lations, PITR 2012, pages 40?48.
Anna Trosborg, editor. 1997. Text Typology and Trans-
lation. Benjamins Translation Library.
Sowmya Vajjala and Detmar Meurers. 2012. On Im-
proving the Accuracy of Readability Classification
Using Insights from Second Language Acquisition.
In Proceedings of the 7th Workshop on Building Ed-
ucational Applications Using NLP, pages 163?173.
Hans van Halteren. 2008. Source Language Mark-
ers in EUROPARL Translations. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, COLING 2008, pages 937?944.
Tiziano Zito, Niko Wilbert, Laurenz Wiskott, and
Pietro Berkes. 2008. Modular toolkit for Data
Processing (MDP): a Python data processing frame
work. Front. Neuroinform., 2(8).
113
