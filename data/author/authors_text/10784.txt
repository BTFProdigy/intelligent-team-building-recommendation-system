Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 591?599,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Seeded Discovery of Base Relations in Large Corpora
Nicholas Andrews
BBN Technologies?
noa@bbn.com
Naren Ramakrishnan
Virginia Tech
naren@cs.vt.edu
Abstract
Relationship discovery is the task of iden-
tifying salient relationships between named
entities in text. We propose novel approaches
for two sub-tasks of the problem: identifying
the entities of interest, and partitioning
and describing the relations based on their
semantics. In particular, we show that term
frequency patterns can be used effectively
instead of supervised NER, and that the p-
median clustering objective function naturally
uncovers relation exemplars appropriate for
describing the partitioning. Furthermore, we
introduce a novel application of relationship
discovery: the unsupervised identification of
protein-protein interaction phrases.
1 Introduction
Relationship extraction (RE) is the task of extracting
named relationships between entities in text given
some information about the relationships of interest.
Relationship discovery (RD), on the other hand, is
the task of finding which relations exist in a corpus
without any prior knowledge. The discovered rela-
tionships can then be used to bootstrap RE, which is
why RD has also been called unsupervised relation
extraction (Rosenfeld and Feldman, 2006). RD gen-
erally involves three sub-tasks: entities of interest
are either supplied or recognized in the corpus; sec-
ond, of all phrases in which entities co-occur, those
which express a relation are picked out; finally, these
relationship phrases are partitioned based on their
semantics and described. This work considers only
binary relations (those between exactly two entities).
Finding entities of interest has involved either
named entity recognition (NER) or general noun
?This work was conducted while author was at Virginia
Tech.
phrase (NP) chunking, to create the initial pool
of candidate entities. In Section 2, we describe a
corpus statistics approach, previously applied for
web mining (Davidov and Rappoport, 2006), which
we extend for relation discovery. Unlike supervised
machine learning methods, this algorithm does
not need training, is computationally efficient, and
requires as input only the raw corpus and a small set
of seed entities (as few as two). The result is a set
of entities likely to be related to the seeds.
An assumption commonly held in RD work is
that frequently co-occurring entity tuples are likely
to stand in some fixed relation (Hasegawa et al,
2004; Shinyama and Sekine, 2006; Rosenfeld and
Feldman, 2006; Rosenfeld and Feldman, 2007).
Tuples which share similar contexts (the exact
definition of context varies) are then grouped
together in clusters of relations using variants of hi-
erarchical agglomerate clustering (HAC). However,
to our knowledge, no prior work has satisfactorily
addressed the problem of describing the resulting
clusters. In Section 3, we propose an approach
which incorporates this requirement directly into
the clustering objective: to find relation clusters
which are well-described by a single exemplar.
In Section 4, we apply RD to recognize protein-
protein interaction (PPI) sentences, using proteins
as seeds for the entity discovery phase. We compare
our results against special-purpose methods in terms
of precision and recall on standard data sets.
The remainder of this paper is outlined below:
Section 2 describes how a small number of input
words (the entities of interest) are used as seeds
for unsupervised entity discovery. Section 3 de-
scribes how discovered entities are used to discover
relationships. Section 4 describes evaluation
methodology and results. Section 5 describes
related work. Section 6 concludes and discusses
591
directions for future work.
2 Entity discovery
For a corpus C, each sentence s ? C with words
s = (w1, w2, ..., wn), is mapped to the sequence
s
?
= f(s). The function f maps each word w ? s
to a symbol based on its frequency in C as follows:
f(w) =
?
?
?
S if w is a seed word
H otherwise if w is a frequent word
X otherwise
For example, the sentence:
A and B are usually mediated by an
overproduced C.
might be mapped to the sequence
(S,H,X,H,H,X,H,H,X,X), which we will
write as SHXHHXHHXX for brevity. In this
case, A is a seed term, while B and C are not. The
underlying assumption is that content words can
be distinguished from other words based on their
frequency in the corpus.
2.1 Pattern induction
In the example sentence, ?A and B are usually
mediated by an overproduced C?, ?and? is a good
indicator that A,B share some aspect of their
semantics; in this case, that they are both me-
diated by an overproduced C, and are therefore
also likely to belong to same family or type of
entities. The indicators ?and? and ?or? have together
been used to discover word categories in lexical
acquisition (Dorow et al, 2005). However, there
can be many other such indicators, many discourse
or corpus specific. To discover them, we use a
slightly modified version of the method presented
in (Davidov and Rappoport, 2006). In particular, in
this work we consider named entities of arbitrary
length (i.e., longer than a single token).
The corpus is searched for all instances of the
frequency pattern H1S1H2S2H3, for seed words
S1, S2, and pattern (H1, H2, H3). Of all these pat-
tern instances, we keep those which also appear as
H1S2H2S1H3. If seed words appear on either side
of the pattern, it is a good indication that the sym-
metric pattern expresses some sort of a conjunction,
often domain specific. This procedure is repeated for
variations of HSHSH with the goal of capturing
different forms of speech; for example, HSHSH
will capture ?; A , B and?, while HSHHSH will
capture ?; A but not B ,? and so on. We enforce that
frequent words appear before and after (i.e., sur-
round) the two seed words to ensure they are stand-
alone entities, and not part of a longer noun phrase.
For example, the phrase ?IFN-gamma mRNA and
IL-6 are? maps to XXHSH , and therefore ?mRNA?
would (correctly) not be added to the entity pool.
New entities are added to the initial set of seed
by matching symmetric patterns. If a seed word
S is found to occur with an infrequent word X in
any discovered symmetric pattern (as HSHXH or
HXHSH), then we add X to the pool of entities.
This process can be bootstrapped as needed.
2.2 Chunking
In Section 3.1, sentences in which entities co-occur
are clustered based on a measure of pairwise simi-
larity. The features used in this similarity calculation
are based on the surrounding or connecting words
in the sentence in which entities co-occur. To ensure
the context is not polluted with words which actually
belong the entity NP (such as ?IFN-gamma mRNA?)
rather than the context, we use frequency patterns
to search the corpus for common NP chunks.
In each sentence in which entities occur, we form
a candidate chunk by matching the regular expres-
sion HX?SX?H , which returns all content-words
X bracketing the entity S. Of all candidate chunks,
we keep those which occur frequently enough to
significantly affect the similarity calculations. The
remaining chunks are pruned based on the entropy
of the words appearing immediately before and after
the chunk in the corpus; if a given chunk appears
in a variety of contexts, it is more likely to express
a meaningful collocation (Shimohata et al, 1997).
Therefore, as an efficient filter on the candidate
chunks, we discard those which tend to occur in the
same contexts (where the context is H...H).
3 Identifying relation phrases
Once the pool of entities has been recognized in the
corpus, those which frequently co-occur are taken
as likely to stand in a relation. Order matters in that
S1..S2 is considered a different entity co-occurrence
(and therefore potential relation) than S2..S1.
The effect of the co-occurrence threshold on the
resulting relations is investigated in Section 4.
3.1 Clustering relation phrases
Partitioning the candidate relationships serves to
identify groups of differently expressed relation-
ships of similar semantics. The resulting clusters
should cover the most important relations in a cor-
pus between the entities of interest. The phrases in
592
each cluster are expected to capture most syntactic
variation in the expression of a given relationship.
Therefore, the largest clusters are well suited
as positive examples for training a relationship
extractor (Rosenfeld and Feldman, 2006).
We take the context of a co-occurring tuple to
be the terms connecting the two entities within
the sentence in which they appear, and call the
connecting terms a relation phrase (RP). Each RP is
treated separately in the similarity calculations and
the clustering. Relations are modeled using a vector
space model. Each relation is treated as a vector of
term frequencies (tf) weighted by tf ? idf. RPs are
preprocessed by filtering stopwords1. However, we
do not stem the remaining words, as suffixes can be
highly discriminative in determining the semantics
of a relation (e.g., ?production? vs ?produced?). Af-
ter normalizing vectors to unit length, we compute a
similarity matrix by computing the dot product be-
tween the vectors for each distinct RP pair. The sim-
ilarity matrix is then used as input for the clustering.
3.2 p-Median clustering
Prior approaches to relationship discovery have
used HAC to identify relation clusters. HAC is
attractive in unsupervised applications since the
number of clusters is not required a priori, but
can be determined from the resulting dendogram.
On the other hand, a typical HAC implementation
runs in ?(N2 log(N)), which can be prohibitive on
larger data sets2.
A further feature of HAC, and many other par-
titional clustering algorithms such as k-means and
spectral cuts, is that the resulting clusters are not
necessarily well-described by single instance. Re-
lations, however, typically have a base or root form
which would be desirable to uncover to describe the
relation clusters. For example, in the following RPs:
induced transient increases in
induced biphasic increases in
induced an increase in
induced an increase in both
induced a further increase in
the phrase ?induced an increase in? is well suited
as a base form of the relation and a descriptor for
the cluster. The p-median clustering objective is to
find p clusters which are well-described by a single
1We use the English stopword list from the Snowball
project, available at http://snowball.tartarus.
org/
2An optimization to ?(N2) is possible for single-linkage
HAC.
exemplar. Formally, given an N ? N similarity
matrix, the goal is to select p columns such that the
sum of the maximum values within each row of the
selected columns are maximized.
Note that an exemplar can also be chosen a
posteriori using some heuristic; for example, the
most frequently occurring instance in a cluster can
be taken as the exemplar. However, the p-median
clustering objective is robust, and ensures that only
those clusters which are well described by a single
exemplar appear in the resulting partition of the
relations. This means that the optimal number of
clusters for the p-median clustering objective in a
given data set will usually be quite different (usually
higher) than the optimal number of groups according
to the HAC, k-means, or normalized cut objectives.
Affinity propagation (AP) is the most efficient
approximation for the p-median problem that we are
aware of, which also has the property of not requir-
ing the number of clusters as an explicit input (Frey
and Dueck, 2007). Runtime is linear in the number
of similarities, which in the worst case is N2 (for
N relations), but in practice many relations share
no words in common, and therefore do not need to
have their similarity considered in the clustering.
AP is an iterative message-passing procedure
in which the objects being clustered compete to
serve as cluster exemplars by exchanging two types
of messages. The responsibility r(x,m), sent
from object x ? X (for set X of objects to be
clustered) to candidate exemplar m ? X , denotes
how well-suited m is of being the exemplar for x by
considering all other potential exemplars m? of x:
s(x,m)? max
m??X ,m? 6=m
a(x,m
?
) + s(x,m
?
)
where s(x,m) is the similarity between x,m. The
availability a(x,m) of each object x ? X is initially
set to zero. Availabilities, sent from candidate
exemplar m to object x, increase as evidence for m
to serve as the exemplar for x increases:
min
?
?
?
0, r(m,m) +
?
x??X ,x? 6?{x,m}
max{0, r(x?,m)}
?
?
?
Each object to be clustered is assigned an initial
preference of becoming a cluster exemplar. If there
are no a priori preferences for cluster exemplars, the
preferences are set to the median similarity (which
can be thought of as the ?knee? of the objective
function graph vs. number of clusters), and exem-
plars emerge from the message passing procedure.
However, shorter RP are more likely to contain base
593
forms of relations (because longer phrases likely
contain additional words specific to the sentence).
Therefore, we include a slight scaling factor in the
preferences, which assigns shorter RP higher initial
values (up to 1.5? the median similarity).
3.3 Pruning clusters
After clustering relation phrases with AP, we prune
the resulting partition by evaluating the number
of different relation instances appearing in each
cluster, as well as the entities involved. In our
experiments, we discard all clusters smaller than a
certain threshold, since we ultimately wish to use
the clustering to train RE, and small clusters do
not provide enough positive examples for training
(we investigate the effect of this threshold in Sec-
tion 4.2). We further assume that for a relationship
to be useful, a number of different entities should
stand in this relation. In particular, we inspect the
set of left and right arguments in the cluster, which
(in English) usually correspond to the subject and
object of the sentence. If a single entity constitutes
more than two thirds (23 ) of the left or right argu-
ments of a cluster, then this cluster is discarded from
the results. Our assumption is that these clusters
describe relations too specific to be useful.
4 Evaluation
RD systems are usually evaluated based on their re-
sults for a particular task such as RE (Rosenfeld and
Feldman, 2006), or by a manual inspection of their
results (Davidov et al, 2007; Rosenfeld and Feld-
man, 2007; Hasegawa et al, 2004), but we are not
aware of any which examines the effects of parame-
ters on performance exhaustively. In this section we
test several hypotheses of RD using data sets which
are already labeled for sentences which contain
entities of a particular type and in a fixed relation of
some kind. In particular, we adapt the output of the
discovery phase to identify phrases which express
PPIs. While this task is traditionally performed
using supervised algorithms such as support vector
machines (Erkan et al, 2007), we show that RD
is capable of achieving similar levels of precision
without any manually annotated training data.
4.1 Method
We construct a corpus of 87300 abstracts by query-
ing the PubMed database with the proteins shown in
Table 1. The 60 most frequent words are considered
definite non-entities; all remaining words are can-
didate entities. This corpus serves as input for the
Table 1: Proteins queried to create the evaluation corpus.
Seed entities (proteins)
c-cbl AmpC CD18 CD54 CD5
CD59 CK c-myc CNP DM
EBNA GSH IL-8 IL-1beta JNK1
p38 PABP PCNA PP1 PP2a
PPAR PSM TAT TNF-alpha TPO
relationship discovery. As seeds, we use the same
25 proteins used to query the database. Since all
seeds are proteins, we expect the entities discovered
to be proteins. The pattern induction found roughly
200 symmetric extraction patterns, which yield
4402 unique entities after 1 pass through the corpus.
Depending on the frequency of the seeds in the
corpus, more passes through the corpus might be
needed (bootstrapping with the discovered entities
after each pass). We retain all chunks that appear
at least 10 times in the corpus, yielding 3282
additional entities after entropy pruning.
A PPI denotes a broad class of bio-medical
relationships between two proteins. One example
of an interaction is where the two proteins bind
together to form a structural complex of cellular
machinery such as signal transduction machinery. A
second example is when one protein binds upstream
of the DNA sequence encoding a gene which en-
codes the second protein. A final example is when
proteins serve as enzymes catalyzing successive
steps of a biochemical reaction. More categories
of interactions are continually being catalogued
and hence unsupervised identification of PPIs is
important in biomedical text mining.
4.2 Experiment 1: PPI sentence identification
Method: To evaluate the performance of our sys-
tem, we measure how well the relationships discov-
ered compare with manually selected PPI sentences.
To do so, we follow the same procedure and data
sets used to evaluate semi-supervised classification
of PPI sentences (Erkan et al, 2007). The two data
sets are AIMED and CB, which have been marked
for protein entities and interaction phrases3.
For each sentence in which n proteins appear,
we build
(
n
2
)
phrases. Each phrase consists of
the words between each entity combination, and is
labeled as positive if it describes a PPI, or negative
otherwise. This results in 4026 phrases for the
3Available in preprocessed form at http://belabog.
si.umich.edu/biocreative
594
AIMED data set (951 positive, 3075 negative), and
4056 phrases for the CB data set (2202 positive,
1854 negative).
The output of the discovery phase is a clustering
of RPs. For purpose of this experiment, we ignore
the partition and treat the phrases in aggregate. A
phrase in the evaluation data set is classified as
positive (describing a PPI) if any substring of the
phrase matches an RP in our output. For example,
if the phrase is:
A significantly inhibited B
and the string ?inhibited? appears as a relation in
our output, then this phrase is marked positive.
Otherwise, the phrase is marked negative.
Performance is evaluated using standard metrics
of precision (P ), recall (R), and F-measure (F1),
defined as:
P =
TP
TP + FP
; R =
TP
TP + FN
where TP is the number of phrases correctly
identified as describing a PPI, FP is the number of
phrases incorrectly classified as describing a rela-
tion, and FN is the number of interaction phrases
(positives) marked negative. F1 is defined as:
F1 =
2PR
P + R
We calculate P , R, and F1 for three parameters
affecting which phrases are identified as expressing
a relation:
? the minimum co-occurrence threshold that con-
trols which entity tuples are kept as likely to stand
in some fixed relation
? the minimum cluster size that controls which
groups of relations are discarded
? the minimum RP length that controls the smallest
number of words appearing in relations
The threshold on the length of the relations can be
thought of as controlling the amount of contextual
information expressed. A single term relation
will be very general, while longer RPs express a
relation very specific to the context in which they
are written. The results are reported in Figures 1
through 6. Odd numbered figures use the AIMED
corpus; even numbered figures the CB corpus.
Results: Discarding clusters below a certain size
had no significant effect on precision. However, this
step is still necessary for bootstrapping RE, since
machine learning approaches require a sufficient
number of positive examples to train the extractor.
Table 2: Comparison with supervised methods?AIMED
corpus
Method P R F1
RD-F1 30.08 60.67 40.22
RD-P 55.17 5.04 9.25
(Yakushiji et al, 2005) 33.70 33.10 33.40
(Mitsumori et al, 2006) 54.20 42.60 47.70
(Erkan et al, 2007) 59.59 60.68 59.96
Table 3: Comparison with supervised methods?CB
corpus
Method P R F1
RD-F1 65.03 69.16 67.03
RD-P 86.27 2.00 3.91
(Erkan et al, 2007) 85.62 84.89 85.22
On the other hand, our results confirm the
observation that frequently co-occurring pairs of
entities are likely to stand in a fixed relation. On
the CB corpus, precision ranges from 0.63 to 0.86
for phrases between entities co-occurring at least
50 times. On the AIMED corpus, precision ranges
from 0.29 to 0.55 in the same threshold range.
The minimum phrase length had the most impact
on performance, which was particularly evident in
the CB corpus: this corpus reached perfect precision
discarding all RPs of fewer than 3 words. Lower
thresholds result in significantly more relations, at
the cost of precision.
The generally lower performance on the AIMED
corpus suggests that our training data (retrieved from
the seed proteins) provided less coverage for those
interactions than for the those in the CB corpus.
Table 2 and Table 3 compare our results at fixed
parameter settings with supervised approaches.
RD-F1 reports parameters which give highest recall
and RD-P highest precision. Specifically, both
RD-F1 and RD-P use a minimum RP length of
1, RD-F1 uses a co-occurrence threshold of 10,
and RD-P uses a co-occurrence threshold of 50.
As expected, RD alone does not match combined
precision and recall of state-of-the-art supervised
systems. However, we show better performance
than expected. RD-F1 outperforms the best results
of (Yakushiji et al, 2005). RD-P settings out-
perform or match the precision of top-performing
systems on both datasets.
595
AIMED corpus CB corpus
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
5 10 15 20 25
Cluster size threshold
Precision
Recall
F-Measure
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
5 10 15 20 25
Cluster size threshold
Precision
Recall
F-Measure
Figures 1 & 2: Performance as minimum cluster size is adjusted
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
10 20 30 40 50
Co-occurence threshold
Precision
Recall
F-Measure
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
10 20 30 40 50
Co-occurence threshold
Precision
Recall
F-Measure
Figures 3 & 4: Performance as co-occurrence threshold is adjusted
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
0 0.5 1 1.5 2 2.5 3
Minimum phrase length
Precision
Recall
F-Measure
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
0 0.5 1 1.5 2 2.5 3
Minimum phrase length
Precision
Recall
F-Measure
Figures 5 & 6: Performance as minimum phrase length is adjusted
596
4.3 Experiment 2: clustering relations
Method: We evaluate the appropriateness of the
p-median clustering as follows. For each cluster,
we take the cluster exemplar as defining the base
relation. If the base relation does not express
something meaningful, then we mark each mem-
ber of the cluster incorrect. Otherwise, we label
each member of the cluster either as semantically
similar to the exemplar (correct) or different than
the exemplar (incorrect). Thus, clusters with
inappropriate exemplars are heavily penalized.
These results are reported in Table 4. For purpose
of this experiment, we use the same parameters
as for RD-P , and evaluate the 20 largest clusters.
Results: In the 20 largest clusters, each cluster ex-
emplar expressed something meaningful. 3 of the
cluster exemplars were not representative of their
other members. We found that most error was due to
stopwords not being considered in our similarity cal-
culations. For example, ?detected by? and ?detected
in? express the same relationship in our similarity
calculations; however, they are clearly quite differ-
ent. Another source of error evident in Table 4 are
mistakes in the pattern and entropy based chunking.
The exemplar ?mrna expression in? includes the to-
ken ?mrna?, which belongs with the left protein NP
in the relation chosen as an exemplar.
5 Related work
RD is a relatively new area of research. Existing
methods differ primarily in the amount of super-
vision required and in how contextual features are
defined and used.
(Hasegawa et al, 2004) use NER to identify
frequently co-occurring entities as likely relation
phrases. As in this work, they use the vector model
and cosine similarity to define a measure of simi-
larity between relations, but build relation vectors
out of all instances of each frequently co-occurring
entity pair. Therefore, each mention of the same
co-occurring pair is assumed to express the same
relationship. These aggregate feature vectors are
clustered using complete-linkage HAC, and cluster
exemplars are determined by manual inspection
for evaluation purposes. (Shinyama and Sekine,
2006) rely further on supervised methods, defining
features over a full syntactic parse, and exploit
multiple descriptions of the same event in newswire
to identify useful relations.
(Rosenfeld and Feldman, 2006) consider the use
of RD for unsupervised relation extraction, and use
Table 4: Base relations identified using RP-P parameters
Exemplar Size P (%)
by activation of 33 87.9
was associated with 28 92.9
was induced by 24 83.3
was detected by 24 83.3
as compared with the 25 92.0
were measured with 23 87.0
mrna expression in 21 9.5
in response to 21 95.23
was determined by 21 90.4
with its effect in 19 10.5
was correlated with 18 100.0
by induction of 16 93.8
for binding to 16 75.0
is mediated by 16 93.8
was observed by 16 50.0
is an important 15 66.6
increased expression of 15 60.0
related to the 15 93.3
protein production as well as 15 33.3
dependent on 14 85.7
Median precision: 86.35
a more complex pattern-learning approach to define
feature vectors to cluster candidate relations, report-
ing gains in accuracy compared with the tf ? idf
weighed features used in (Hasegawa et al, 2004)
and in this work. They also use HAC, and do not
address the description of the relations. Arbitrary
noun phrases obtained through shallow parsing are
used as entities. (Rosenfeld and Feldman, 2007) use
a feature ranking scheme using separability-based
scores, and compare the performance of different
variants of HAC (finding single-linkage to perform
best). The complexity of the feature ranking-scheme
described can be greater than the clustering itself; in
contrast, while we use simple features, our approach
is much more efficient.
(Davidov et al, 2007) introduce the use of
term frequency patterns for relationship discovery.
However, they search for a specific type of relation-
ship; namely, attributes common to all entities of
a particular type (for example, all countries have
the attribute capital), and use a special purpose
set of filters rather than entity co-occurrence and
clustering. Our work can be seen as a generalization
of theirs to relationships of any kind, and we extend
the use of frequency patterns to finding general
n-gram entities rather than single word entities.
(Madkour et al, 2007) give an excellent overview
597
of biomedical NER and RE. They propose a statis-
tical system for RE, but rely on NER, POS tagging,
and the creation of a dictionary for each domain of
application. Also, they do not cluster relationships
into semantically related groups.
6 Conclusion
Our work makes a series of important improvements
to the state-of-the-art in relationship discovery.
First, by incorporating entity discovery into the rela-
tionship discovery pipeline, our method does not re-
quire distinct training phases to accommodate differ-
ent entity types, relations, or discourse types. Sec-
ond, p-median clustering effectively uncovers the
base form of relations present in the corpus, address-
ing an important limitation in usability. In terms of
specific hypotheses, we have tested and confirmed
that co-occurrence can be a good indicator of the
presence of a relationship but the size of a cluster
is not necessarily a good indicator of the importance
or strength of the discovered relationship. Further-
more, we have shown that longer RPs with more
context give higher precision (at the cost of reduced
coverage). Finally, the integration of ideas in our
approach?unsupervisedness, efficiency, flexibility
(in application), and specificity?is novel in itself.
In future work, we seek to expand upon our RD
methods in three directions. First, we would like
to generalize the scope of our discovery pipeline
beyond binary relations and with richer considera-
tions of context, even across sentences. Second, we
hope to achieve greater tunability of performance,
to account for additional discovery metrics besides
precision. Finally, we intend to induce entire con-
cept maps from text using the discovered relations
to bootstrap an RE phase, where the underlying
problem is not just of inferring multiple types of
relations, but to have sufficient co-ordination among
the discovered relations to ensure connectedness
among the resulting concepts.
While our method requires no supervision in the
form of manually annotated entities or relations,
the effectiveness of the system relies on the careful
tuning of a number of parameters. Nevertheless,
the results reported in Section 4.2 suggest that the
two parameters that most significantly affect perfor-
mance exhibit predictable precision/recall behavior.
Of the parameters not considered in Section 4.2,
we would like to further investigate the benefits of
chunking entities on the resulting base relations, ex-
perimenting with different measures of collocation.
Acknowledgements
We would like to thank our anonymous reviewers
for their thought-provoking questions. This work
was supported in part by the Institute for Critical
Technology and Applied Science (ICTAS), Virginia
Tech.
References
Dmitry Davidov and Ari Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In ACL
?06: Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the ACL, pages 297?304, Morristown, NJ,
USA. Association for Computational Linguistics.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 232?239, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, and Elisha Moses.
2005. Using curvature and markov clustering in
graphs for lexical acquisition and word sense discrim-
ination. In MEANING 05: 2nd workshop organized
by the MEANING Project, Trento, Italy, February.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extracting
protein interaction sentences using dependency pars-
ing. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL), pages 228?237.
Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:972?976.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL ?04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 415, Morristown, NJ, USA.
Association for Computational Linguistics.
Amgad Madkour, Kareem Darwish, Hany Hassan,
Ahmed Hassan, and Ossama Emam. 2007. Bionoc-
ulars: Extracting protein-protein interactions from
biomedical text. In Biological, translational, and
clinical language processing, pages 89?96, Prague,
Czech Republic, June. Association for Computational
Linguistics.
598
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with svm. IEICE Transac-
tions on Information and Systems, 89(8):2464?2466.
Benjamin Rosenfeld and Ronen Feldman. 2006. High-
performance unsupervised relation extraction from
large corpora. In ICDM ?06: Proceedings of the
Sixth International Conference on Data Mining, pages
1032?1037, Washington, DC, USA. IEEE Computer
Society.
Benjamin Rosenfeld and Ronen Feldman. 2007. Cluster-
ing for unsupervised relation identification. In CIKM
?07: Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 411?418, New York, NY, USA. ACM.
Sayori Shimohata, Toshiyuki Sugio, and Junji Nagata.
1997. Retrieving collocations by co-occurrences and
word order constraints. In In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 476?481.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation
discovery. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Con-
ference, pages 304?311, New York City, USA, June.
Association for Computational Linguistics.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In Proceedings of the
eleventh annual meeting of the association for natural
language processing, pages 93?96.
599
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 412?420,
Beijing, August 2010
Finding the Storyteller:
Automatic Spoiler Tagging using Linguistic Cues
Sheng Guo
Department of Computer Science
Virginia Tech
guos@cs.vt.edu
Naren Ramakrishnan
Department of Computer Science
Virginia Tech
naren@cs.vt.edu
Abstract
Given a movie comment, does it contain
a spoiler? A spoiler is a comment that,
when disclosed, would ruin a surprise or
reveal an important plot detail. We study
automatic methods to detect comments
and reviews that contain spoilers and ap-
ply them to reviews from the IMDB (Inter-
net Movie Database) website. We develop
topic models, based on Latent Dirichlet
Allocation (LDA), but using linguistic de-
pendency information in place of simple
features from bag of words (BOW) repre-
sentations. Experimental results demon-
strate the effectiveness of our technique
over four movie-comment datasets of dif-
ferent scales.
1 Introduction
In everyday parlance, the notion of ?spoilers?
refers to information, such as a movie plot, whose
advance revelation destroys the enjoyment of the
consumer. For instance, consider the movie De-
railed which features Clive Owen and Jennifer
Aniston. In the script, Owen is married and meets
Aniston on a train during his daily commute to
work. The two of them begin an affair. The adul-
tery is noticed by some inscrupulous people who
proceed to blackmail Owen and Aniston. To ex-
perience a spoiler, consider this comment from
imdb.com:
I can understand why Aniston wanted to do this
role, since she gets to play majorly against type
(as the supposedly ?nice? girl who?s really - oh
no! - part of the scam), but I?m at a loss to fig-
ure out what Clive Owen is doing in this sub-par,
unoriginal, ugly and overly violent excuse for a
thriller.
i.e., we learn that Aniston?s character is actually
a not-so-nice person who woos married men for
later blackmail, and thus a very suspenseful piece
of information is revealed. Automatic ways to de-
tect spoilers are crucial in large sites that host re-
views and opinions.
Arguably, what constitutes a spoiler is
inherently a subjective assessment and, for
movies/books with intricate storylines, some
comments are likely to contain more spoilers than
others. We therefore cast the spoiler detection
problem as a ranking problem so that comments
that are more likely to be spoilers are to be
ranked higher than others. In particular, we rank
user comments w.r.t. (i.e., given) the movie?s
synopsis which, according to imdb, is ?[a detailed
description of the movie, including spoilers, so
that users who haven?t seen a movie can read
anything about the title]?.
Our contributions are three fold. (i) We for-
mulate the novel task of spoiler detection in re-
views and cast it as ranking user comments against
a synopsis. We demonstrate how simple bag-
of-words (BOW) representations need to be aug-
mented with linguistic cues in order to satisfac-
torily detect spoilers. (ii) We showcase the abil-
ity of dependency parses to extract discrimina-
tory linguistic cues that can distinguish spoil-
ers from non-spoilers. We utilize an LDA-based
model (Wei and Croft, 2006) to probabilistically
rank spoilers. Our approach does not require man-
ual tagging of positive and negative examples ? an
advantage that is crucial to large scale implemen-
tation. (iii) We conduct a detailed experimental
evaluation with imdb to assess the effectiveness
of our framework. Using manually tagged com-
412
ments for four diverse movies and suitably con-
figured design choices, we evaluate a total of 12
ranking strategies.
2 LDA
Probabilistic topic modeling has attracted signifi-
cant attention with techniques such as probabilis-
tic latent semantic analysis (PLSA) (Hofmann,
1999) and LDA (Blei et al, 2003; Griffiths and
Steyvers, 2004; Heinrich, 2008; Steyvers and
Griffiths, 2007). We discuss LDA in detail due
to its centrality to our proposed techniques. As a
generative model, LDA describes how text could
be generated from a latent set of variables denot-
ing topics. Each document is modeled as a mix-
ture of topics, and topics are modeled as multino-
mial distributions on words.
An unlabeled training corpus can be used
to estimate an LDA model. Many infer-
ence methods have been proposed, e.g., vari-
ational methods (Blei et al, 2003), expecta-
tion propagation (Griffiths and Steyvers, 2004),
Gibbs sampling (Griffiths and Steyvers, 2004),
and a collapsed variational Bayesian inference
method (Teh et al, 2007). Gibbs sampling, as
a specific form of Markov chain Monte Carlo
(MCMC), is a popular method for estimating
LDA models. After an LDA model is estimated,
it can be used in a very versatile manner: to
analyze new documents, for inference tasks, or
for retrieval/comparison functions. For instance,
we can calculate the probability that a given
word appears in a document conditioned on other
words. Furthermore, two kinds of similarities
can be assessed: between documents and between
words (Steyvers and Griffiths, 2007). The sim-
ilarity between two documents can also be used
to retrieve documents relevant to a query docu-
ment (Heinrich, 2008). Yet another application is
to use LDA as a dimensionality reduction tool for
text classification (Blei et al, 2003).
To improve LDA?s expressiveness, we can re-
lax the bag-of-words assumption and plug in more
sophisticated topic models (Griffiths et al, 2005;
Griffiths et al, 2007; Wallach, 2006; Wallach,
2008; Wang and Mccallum, 2005; Wang et al,
2007). sLDA (supervised LDA), as a statisti-
cal model of labeled collections, focuses on the
prediction problem (Blei and Mcauliffe, 2007).
The correlated topic model (CTM) (Blei and Laf-
ferty, 2007) addresses plain LDA?s inability to
model topic correlation. The author-topic model
(AT) (Steyvers et al, 2004) considers not only
topics but also authors of the documents, and
models documents as if they were generated by
a two-stage stochastic process.
3 LDA-based spoiler ranking
3.1 Methods
Based on the fact that a spoiler should be topically
close to the synopsis, we propose three methods
to solve the spoiler ranking problem. The first
two use LDA as a preprocessing stage, whereas
the third requires positive training data.
Predictive perplexity: Our first method is moti-
vated by the use of LDA-based predictive per-
plexity (PP) for collaborative filtering (Blei et al,
2003). Here, the PP metric is evaluated over a
fixed test dataset in order to empirically compare
LDA with other models (pLSI, mixture of uni-
grams). In our work, we view documents as anal-
ogous to users, and words inside documents as
analogous to movies. Given a group of known
words, we predict the other group of unkown
words. We can either calculate the predictive per-
plexity value from each movie comment Com to
the unique synopsis (PP1), or from the synopsis
Syn to each comment (PP2).
PP1(Syn,wcom) = exp{?
PMsyn
d=1 log p(wd|wcom)
Msyn }
PP2(Com,wsyn) = exp{?
PMcom
d=1 log p(wd|wsyn)
Mcom }
In the equations above, p(wd|wcom) and
p(wd|wsyn) are the probabilities to generate the
word (wd) from a group of observed words wobs
(either a comment wcom or a synopsis wsyn).
p(w|wobs) =
? ?
z p(w|z)p(z|?)p(?|wobs)d?
Mcom or Msyn is the length of a comment or
a synopsis. Notice that p(?|wobs) can be easily
calculated after estimating LDA model by Gibbs
sampling. It is also discussed as ?predictive
likelihood ranking? in (Heinrich, 2008).
Symmetrized KL-divergence: Since docu-
ments are modeled as mixtures of topics in
LDA, we can calculate the similarity between
synopsis and comment by measuring their
413
topic distributions? similarity. We adopt the
widely-used symmetrized Kullback Leibler
(KL) divergence (Heinrich, 2008; Steyvers
and Griffiths, 2007) to measure the difference
between the two documents? topic distributions,
sKL(Syn,Com) = 12 [DKL(Syn?Com) + DKL(Com?Syn)]
where DKL(p?q) =
?T
j=1 pj log2
pj
qj
LPU: Viewing the spoiler ranking problem as a
retrieval task given the (long) query synopsis, we
also consider the LPU (Learning from Positive
and Unlabeled Data) method (Liu et al, 2003).
We apply LPU as if the comment collection was
the unlabeled dataset, and the synopsis together
with few obvious spoiler comments as the posi-
tive training data.
3.2 Dependency Parsing
LDA, as a topic model, is widely used as a clus-
tering method and dimensionality reduction tool.
It models text as a mixture of topics. However,
topics extracted by LDA are not necessarily the
same topics as judged by humans since the def-
inition of topic is very subjective. For instance,
when conducting sentimental polarity analysis,
we hope that topics are clusters concerning one
certain kind of subjective sentiment. But for other
purposes, we may desire topics focusing on broad
?plots.? Since LDA merely processes a collection
according to the statistical distribution of words,
its results might not fit either of these two cases
mentioned above.
In a basic topic model (section 3.1), neither the
order of a sequence of words nor the semantic
connections between two words affect the prob-
abilistic modeling. Documents are generated only
based on a BOW assumption. However, word or-
der information is very important for most text-
related tasks, and simply discarding the order in-
formation is inappropriate. Significant work has
gone in to address this problem. Griffiths et al
use order information by incorporating colloca-
tions (Griffiths et al, 2005; Griffiths et al, 2007).
They give an example of the collocation ?united
kingdom?, which is ideally treated as a single
chunk than two independent words. However,
this model can only be used to capture colloca-
tions involving sequential terms. Their extended
model (Griffiths et al, 2007) integrates topics and
syntax, and identifies syntactic classes of words
based on their distribution. More sophisticated
models exist (Wallach, 2006; Wang and Mccal-
lum, 2005; Wang et al, 2007; Wallach, 2008) but
all of them are focused on solving linguistic anal-
ysis tasks using topic models. In this paper, how-
ever, our focus is on utilizing dependency infor-
mation as a preprocessing step to help improve the
accuracy of LDA models.
In more detail, we utilize dependency parsing to
breakup sentences and treat parses as independent
?virtual words,? to be added to the original BOW-
based LDA model. In our experiments we employ
the Stanford typed dependency parser 1 (Marneffe
et al, 2006) as our parsing tool. We use collapsed
typed dependencies (a.k.a. grammatical relations)
to form the virtual words. However, we do not in-
corporate all the dependencies. We only retain de-
pendencies whose terms have the part-of-speech
tags such as ?NN", ?VB?, ?JJ?, ?PRP? and ?RB?2,
since these terms have strong plot meaning, and
are close to the movie topic. Fig. 2 shows a typi-
cal parsing result from one sample sentence. This
sentence is taken from a review of Unbreakable.
Figure 2: Dependency parse of ?David Dunn is
the sole survivor of this terrible disaster?.
Consider Fig. 1, which depicts five sample sen-
tences all containing two words: ?Dunn? and
?survivor?. Although these sentences appear dif-
ferent, these two words above refer to the same
individual. By treating dependencies as virtual
words, we can easily integrate these plot-related
relations into an LDA model. Notice that among
these five sentences, the grammatical relations be-
tween these two words are different: in the fourth
sentence, ?survivor? serves as an appositional
modifier of the term ?Dunn?(appos), whereas in
1http://nlp.stanford.edu/software, V1.6
2In the implementation, we actually considered all the
POS tags with these five tags as prefix, such as ?NNS?,
?VBN?, etc.
414
David Dunn is the sole survivor of this terrible disaster.
David Dunn (Bruce Willis) is the only survivor in a horrific train trash.
David Dunn, a man caught in what appears to be a loveless, deteriorating marriage, is the sole survivor of a Philadelphia train wreck.
In this Bruce Willis plays David Dunn, the sole survivor of a passenger train accident.
Then the story moves to security guard David Dunn (Bruce Willis) miraculously being the lone survivor of a mile-long train crash (that
you find out later was not accidental), and with no injuries what-so-ever.
nsubj
nsubj
nsubj
appos
nsubj
Figure 1: Four sentences with the same topical connection between ?Dunn? and ?survivor?. We inte-
grate this relation into LDA by treating it as a virtual word ?Dunn-survivor.?
other sentences, ?Dunn? serves as the nominal
subject of ?survivor?(nsubj). What is important
to note is that the surface distances between these
given words in different sentences vary a lot. By
utilizing dependency parsing, we can capture the
semantic connection which is physically sepa-
rated by even as much as 15 words, as in the third
sentence.
We evaluate topic drift among the results from
plain LDA. We mainly check whether plain LDA
will assign the same topic to those terms that have
specific linguistic dependency relations. We only
consider the following four types of dependencies
for evaluation3:
? Relations with two noun terms: <NN, NN>,
such as ?appos?, ?nn?, ?abbrev? etc.;
? Relations with one noun and one adjective:
<NN, JJ>, like ?amod?;
? Relations with one noun and one verb: <NN,
VB>, such as ?agent?, ?dobj?, etc.;
? Relations with only one noun: <NN, *>,
which is the relaxed version of <NN, NN>;
We experimented with different pre-set topic
numbers (500, 50, and 2) and conducted exper-
iments on four different movie comment collec-
tions with LDA analysis. Table 1 shows that
<NN, NN> dependency has the highest chance
3Here we use <NN, JJ> to express relations having NN
and JJ terms, but not necessarily in that order. Also, NN
represents all tags related with nouns in the Penn Treebank
Tagset, such as NNS. This applies to all the four expressions
here.
to be topic-matched4 than other relations. How-
ever, all dependencies have very low percentage
to be topic-matched, and with a topic number of 2,
there remained a significant amount of unmatched
<NN, NN> dependencies, demonstrating that sim-
ply doing plain LDA may not capture the plot
?topic? as we desire.
Observing the results above, each method from
section 3.1 (PP1, PP2, sKL and LPU) can be ex-
tended by: (1) using BOW-based words, (2) using
only dependency-based words, or (3) using a mix
of BOW and dependency (dependencies as virtual
words). This induces 12 different ranking strate-
gies.
Table 1: Topic match analysis for plain LDA
(Each entry is the ratio of topic-matched depen-
dencies to all dependencies)
topic number = 500
Movie Name <NN, NN> <NN, JJ> <NN, VB> <NN, *>
Unbreakable 772/3024 412/4411 870/19498 5672/61251
Blood Diamond 441/1775 83/553 80/1012 609/3496
Shooter 242/1846 42/1098 114/2150 1237/15793
Role Models 409/2978 60/1396 76/2529 559/7276
topic number = 50
Movie Name <NN, NN> <NN, JJ> <NN, VB> <NN, *>
Unbreakable 1326/3024 953/4411 3354/19498 14067/61251
Blood Diamond 806/1775 151/553 210/1012 1194/3496
Shooter 584/1846 204/1098 392/2150 3435/15793
Role Models 1156/2978 190/1396 309/2529 1702/7276
topic number = 2
Movie Name <NN, NN> <NN, JJ> <NN, VB> <NN, *>
Unbreakable 2379/3024 3106/4411 13606/19498 43876/61251
Blood Diamond 1391/1775 404/553 761/1012 2668/3496
Shooter 1403/1846 768/1098 1485/2150 11008/15793
Role Models 2185/2978 908/1396 1573/2529 4920/7276
4When both the left term and the right term of a depen-
dency share the same topic, the relation is topic-matched.
415
Table 2: Some examples of incorrect spoiler tagging in IMDb (italicized sentences are spoilers).
No. Tag by IMDb Comment in IMDb
1 Spoiler
The whole film is somewhat slow and it would?ve been possible to add more action scenes. Even though I liked it very much (6.8/10) I think it is less
impressive than "The Sixth Sense" (8.0/10). I would like to be more specific with each scene but it will turn this comment into a spoiler so I will leave
it there. I recommend you to see the movie if you come from the basic Sci-Fi generation, otherwise you may feel uncomfortable with it. Anyway once
upon a time you were a kid in wonderland and everything was possible. [tt0217869]
2
Spoiler
This is one of the rare masterpiece that never got the respect it deserved because people were expecting sixth sense part 2.Sixth sense was a great film
but this is M.N. Shyamalan?s best work till date. This is easily one of my top 10 films of all time. Excellent acting, direction, score, cinematography and
mood. This movie will hold you in awe from start to finish and any student of cinema would tell what a piece of art this film is. The cast is phenomenal,
right from bruce willis to sam jackson and penn , everyone is spectacular in their roles and they make u realise that you do not need loud dramatic moments
to create an impact, going slow and subtle is the trick here. This is not a thriller, it?s a realistic superhero film. [tt0217869]
3
Spoiler
I can?t believe this movie gets a higher rating than the village. OK, after thinking about it, i get the story of unbreakable and i understand what it?s trying
to say. I do think the plot and the idea is captivating and interesting. Having said that, i don?t think the director did anything to make this movie captivating
nor interesting. It seemed to try too hard to make this movie a riddle for the audience to solve. The pace was slow at the beginning and ended just as it
was getting faster. I remember going out of the cinema, feeling frustrated and confused. it?s not until i thoroughly thought about it that i understood the
plot. I believe a good movie should engaged the audience and be cleverly suspenseful without confusing the audience too much. Unbreakable tried to be
that but failed miserably. 2 out of 10, see the village instead. [tt0217869]
4
Spoiler
This movie touched me in ways I have trouble expressing, and brings forth a message one truly need to take seriously! I was moved, and the ending
brought a tear to my eye, as well as a constant two-minute shiver down my spine. It shows how our western way of life influence the lives of thousands of
innocents, in a not-so-positive way. Conflict diamonds, as theme this movie debates, are just one of them. Think of Nike, oil, and so on. We continually
exploit "lesser developed" nations for our own benefit, leaving a trail of destruction, sorrow, and broken backs in our trail. I, for one, will be more attentive
as to what products I purchase in the future, that?s for sure. [tt0450259]
5
Non-
spoiler
... But the movie takes a while to get to the point. "Mr. Glass" has caused lots of mass tragedies in order to find the UNBREAKABLE person. Thus,
he is both a mentor and a MONSTER. ... [tt0217869]
6
Non-
spoiler
... This film is about a sniper who loses his best friend while on a shooting mission. A few years later, he is now retired and living in a woodland with his
do. Then he is visited by the military to plan an assassination of the president. The shot is fired. Unfortunately he is set up to being the shooter and is
hunted by cops everywhere. He must find out why he has been set up and also try and stop the real killers. ... [tt0822854]
4 Experimental Results
4.1 Data preparation
IMDb boasts a collection of more than 203,000
movies (from 1999 to 2009), and the number of
comments and reviews for these movies num-
ber nearly 970,000. For those movies with syn-
opsis provided by IMDb, the average length of
their synopses is about 2422 characters5. Our
experimental setup, for evaluation purposes, re-
quires some amount of labeled data. We choose
four movies from IMDb, together with 2148 com-
ments. As we can see in Table 3, these four
movies have different sizes of comment sets: the
movie ?Unbreakable? (2000) has more than 1000
comments, whereas the movie ?Role Models?
(2008) has only 123 comments.
Table 3: Evaluation dataset about four movies
with different numbers of comments.
Movie Name IMDB ID #Comments #Spoilers
Unbreakable tt0217869 1219 205
Blood Diamond tt0450259 538 147
Shooter tt0822854 268 73
Role Models tt0430922 123 39
We labeled all the 2148 comments for these
four movies manually, and as Table 3 shows,
5Those movies without synopsis are not included.
about 20% of each movie?s comments are spoil-
ers. Our labeling result is a little different from the
current labeling in IMDb: among the 2148 com-
ments, although 1659 comments have the same la-
bels with IMDb, the other 489 are different (205
are treated as spoilers by IMDb but non-spoilers
by us; vice versa with 284) The current labeling
system in IMDb is very coarse: as shown in Ta-
ble 2, the first four rows of comments are labeled
as spoilers by IMDb, but actually they are not.
The last two rows of comments are ignored by
IMDb; however, they do expose the plots about
the twisting ends.
After crawling all the comments of these four
movies, we performed sentence chunking using
the LingPipe toolkit and obtained 356 sentences
for the four movies? synopses, and 26964 sen-
tences for all the comments of these four movies.
These sentences were parsed to extract depen-
dency information: we obtained 5655 dependen-
cies for all synopsis sentences and 448170 depen-
dencies for all comment sentences. From these,
we only retain those dependencies that have at
least one noun term in either left side or the right
side. For measures which require the dependency
information, the dependencies are re-organized
and treated as a new term planted in the text.
416
4.2 Experiments
4.2.1 Topic number analysis
One of the shortcomings of LDA-based meth-
ods is that they require setting a number of topics
in advance. Numerous ways have been proposed
to handle this problem (Blei et al, 2004; Blei et
al., 2003; Griffiths and Steyvers, 2004; Griffiths et
al., 2007; Heinrich, 2008; Steyvers and Griffiths,
2007; Teh et al, 2006). Perplexity, which is
widely used in the language modeling commu-
nity, is also used to predict the best number of
topics. It is a measure of how well the model
fits the unseen documents, and is calculated as
average per-word held-out likelihood. The lower
the perplexity is, the better the model is, and
therefore, the number of topic is specified as the
one leading to the best performance. Griffiths
and Steyvers (Griffiths and Steyvers, 2004) also
discuss the standard Bayesian method which
computes the posterior probability of different
models given the observed data. Another method
from non-parametric Bayesian statistics auto-
matically helps choose the appropriate number
of topics, with flexibility to still choose hyper-
parameters (Blei et al, 2004; Teh et al, 2006).
Although the debate of choosing an appropriate
number of topics continues (Boyd-Graber et
al., 2009), we utilized the classic perplexity
method in our work. Heinrich (Heinrich, 2008)
demonstrated that perplexity can be calculated by:
P (W?|M) = ?Mm=1 p( ~?wm|M)?
1
N = exp{?
PM
m=1 log p( ~?wm|M)
PM
m=1 Nm
}
We chose different topic numbers and calculated
the perplexity value for the 20% held-out com-
ments. A good number of topics was found to
be between 200 and 600 for both Bow-based
strategy and Bow+Dependency strategy, and
is also affected by the size of movie comment
collections. (We used 0.1 as the document topic
prior, and 0.01 as the topic word prior.)
4.2.2 LDA analysis process
As discussed earlier, our task is to rank all the
comments according to their possibilities of being
a spoiler. We primarily used four methods to do
the ranking: PP1, PP2, sKL, and the LPU method.
For each method, we tried the basic model using
?bag-of-words?, and the model using dependency
parse information (only), and also with both BOW
and dependency information mixed. We utilize
LingPipe LDA clustering component which uses
Gibbs sampling.
Among the four methods studied here, PP1,
PP2 and sKL are based on LDA preprocessing.
After obtaining the topic-word distribution and
the posterior distributions for topics in each doc-
ument, the PP1 and PP2 metrics can be easily
calculated. The symmetrized KL divergence be-
tween each pair of synopsis and comment is calcu-
lated by comparing their topic distributions. LPU
method, as a text classifier, requires a set of pos-
itive training data. We selected those comments
which contain terms or phrases as strong hint of
spoiler (using a list of 20 phrases as the filter, such
as ?spoiler alert?, ?spoiler ahead?, etc). These
spoiler comments together with the synopsis, are
treated as the positive training data. We then uti-
lized LPU to label each comment with a real num-
ber for ranking.
4.3 Evaluation
To evaluate the ranking effects of the 12 strate-
gies, we plot n-best precision and recall graphs,
which are widely used for assessing colloca-
tion measures (Evert and Krenn, 2001; Pecina
and Schlesinger, 2006). Fig. 3 visualizes the
precision-recall graphs from 12 different mea-
sures for the four movie comment collections.
The x-axis represents the proportion of the rank-
ing list, while the y-axis depicts the correspond-
ing precision or recall value. The upper part of
the figure is the result for the movie which con-
tains more than 1000 comments, while the bot-
tom part demonstrates the result for the relatively
small comment collection. The n-best evaluation
shows that for all the four movie comment col-
lections, PP1_mix and PP1 perform significantly
better than the other methods, and the dependency
information helps to increase the accuracy sig-
nificantly, especially for the larger size collec-
tion. The LPU method, though using part of the
positive training data, did not perform very well.
The reason could be that although some of the
users put the warning phrases (like ?spoiler alert?)
ahead of their comments, the comment might con-
tain only indirect plot-revealing information. This
also reflects that a spoiler tagging method by us-
417
0 50 100 150 200
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Pr
ec
isi
on
 (%
)
Precision
 
 
0 200 400 600 800 1000
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Re
ca
ll (%
)
Recall
PP1
PP1_mix
PP2
PP2_mix
LPU
LPU_mix
PP2_mix
 PP2
 PP2_mix
PP1_mix
 PP1
 PP1_mix
0 50 100 150 200
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Pr
ec
isi
on
 (%
)
Precision
 
 
0 100 200 300 400
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Re
ca
ll (%
)
Recall
PP1
PP1_mix
PP2
PP2_mix
LPU
LPU_mix
 PP2_mix
 PP2 PP1_mix
PP2_mix
 PP1
 PP1_mix
0 50 100 150 200
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Pr
ec
isi
on
 (%
)
Precision
 
 
0 50 100 150 200 250 300
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Re
ca
ll (%
)
Recall
PP1
PP1_mix
PP2
PP2_mix
LPU
LPU_mix
 PP1_mix
 PP1
 PP2
PP1_mix 
0 20 40 60 80 100
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Pr
ec
isi
on
 (%
)
Precision
 
 
0 20 40 60 80 100
0
10
20
30
40
50
60
70
80
90
100
N?best list (top n)
Re
ca
ll (%
)
Recall
PP1
PP1_mix
PP2
PP2_mix
LPU
LPU_mix
 PP1_mix
 PP1
PP1_mix
Figure 3: N-best(top nth) evaluation (Burnin period = 100): comparison of precision-recall for different
methods on four movie comment collections. The PP1 method with BOW and dependency information
mixed performs the best among all the measures. Other six methods such as dependency only and
KL-based which do not give good performance are ignored in this figure to make it readable. Full
comparison is available at: http://sites.google.com/site/ldaspoiler/
418
ing only keywords typically will not work. Fi-
nally, the approach to directly calculating the sym-
metrized KL divergence seems to be not suitable,
either.
4.4 LDA iteration analysis
We also compared the average precision val-
ues and normalized discounted cumulative gain
(nDCG) values (Croft et al, 2009; J?rvelin and
Kek?l?inen, 2002) of the ranking results with dif-
ferent parameters for Gibbs sampling, such as
burnin period and sample size. Average precision
is calculated by averaging the precision values
from the ranking positions where a valid spoiler
is found, and the nDCG value for the top-p list is
calculated as nDCGp = DCGpIDCG ?DCGp is defined as:
DCGp = rel1 +
?p
i=2
reli
log2 i where reli is 1 when
the i-th comment in the list is judged as a real
spoiler, and 0, otherwise. IDCG denotes the max-
imum possible DCG value when all the real spoil-
ers are ranked at the top (perfect ranking) (J?rvelin
and Kek?l?inen, 2002).
Table 4: Comparison of ranking by PP_mix us-
ing different parameters for Gibbs sampling (ana-
lyzed on the top 150 ranking lists, and the values
in the table are the mean of the accuracy from four
movie comment collections).
<S=100; Lag=2> <S=10; Lag=2> <S=1; Lag=2>
Burnin AvgP (%) nDCG AvgP (%) nDCG AvgP (%) nDCG
400 80.85 0.951 78.2 0.938 78.1 0.94
200 80.95 0.951 80.5 0.948 79.1 0.94
100 87.25 0.974 80.2 0.943 82.4 0.96
50 81.5 0.958 79.5 0.942 80.0 0.94
10 78.9 0.944 79.5 0.949 75.9 0.92
1 79.4 0.940 79.2 0.952 58.0 0.86
As we can see from Table 4, the accuracy is
not affected too much as long as the burin period
for the MCMC process is longer than 50 and the
sample size retained is larger than 10. In our ex-
periments, we use 100 as the burin parameter, and
beyond that, 100 samples were retained with sam-
ple lag of 2.
4.5 Representative results
As shown in Table 5, we find that the basic BOW
strategy prefers the longer comments whereas the
strategy that uses dependency information prefers
the shorter ones. Although it is reasonable that
a longer comment would have a higher probabil-
ity of revealing the plot, methods which prefers
the longer comments usually leave out the short
spoiler comments. By incorporating the depen-
dency information together with the basic BOW,
the new method reduces this shortcoming. For in-
stance, consider one short comment for the movie
?Unbreakable (2000)?:
This is the same formula as Sixth Sense ? from
the ability to see things other people don?t, to
the shocking ending. Only this movie is just not
plausible ? I mean Elijah goes around causing
disasters, trying to see if anyone is ?Unbreak-
able? ? it?s gonna take a lot of disasters because
its a big world.
whcih is ranked as the 27th result in the PP1_mix
method, whereas the BOW based PP1 method
places it at the 398th result in the list. Obviously,
this comment reveals the twisting end that it is Eli-
jah who caused the disasters.
Table 5: Comparison of average length of the top-
50 comments of 4 movies from 2 strategies.
Role Models Shooter Blood Diamond Unbreakable
BOW 2162.14 2259.36 2829.86 1389.18
Dependency 1596.14 1232.12 2435.58 1295.72
5 Conclusions and future work
We have introduced the spoiler detection problem
and proposed using topic models to rank movie
comments according to the extent they reveal the
movie?s plot. In particular, integrating linguistic
cues from dependency information into our topic
model significantly improves the ranking accu-
racy.
In future work, we seek to study schemes which
can segment comments to potentially identify the
relevant spoiler portion automatically. The auto-
matic labeling idea of (Mei et al, 2007) can also
be studied in our framework. Deeper linguistic
analysis, such as named entity recognition and se-
mantic role labeling, can also be conducted. In
addition, evaluating topic models or choosing the
right number of topics using dependency informa-
tion can be further studied. Finally, integrating
the dependency relationships more directly into
the probabilistic graphical model is also worthy
of study.
419
References
Blei, David M. and John D. Lafferty. 2007. A cor-
related topic model of science. Annals of Applied
Statistics, 1(1):17?35.
Blei, David M. and Jon D. Mcauliffe. 2007. Super-
vised topic models. In Proceedings of the 21st An-
nual Conference on Neural Information Processing
Systems.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of ma-
chine learning research, 3:993?1022.
Blei, David M., T. Gri, M. Jordan, and J. Tenenbaum.
2004. Hierarchical topic models and the nested chi-
nese restaurant process. In Proceedings of the 18th
Annual Conference on Neural Information Process-
ing Systems.
Boyd-Graber, Jordan, Jonathan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of the 23rd Annual Conference on Neural
Information Processing Systems.
Croft, Bruce, Donald Metzler, and Trevor Strohman.
2009. Search Engines: Information Retrieval in
Practice. Addison Wesley, 1 edition.
Evert, Stefan and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics.
Griffiths, Thomas L. and M. Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101 Suppl 1:5228?5235, April.
Griffiths, Thomas L., Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2005. Integrating topics
and syntax. In Proceedings of the 19th Annual Con-
ference on Neural Information Processing Systems.
Griffiths, Thomas L., Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211?244, April.
Heinrich, Gregor. 2008. Parameter estimation for text
analysis. Technical report, University of Leipzig.
Hofmann, Thomas. 1999. Probabilistic latent seman-
tic analysis. In Proceedings of 15th Conference on
Uncertainty in Artificial Intelligence.
J?rvelin, Kalervo and Jaana Kek?l?inen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422?
446.
Liu, Bing, Yang Dai, Xiaoli Li, Wee Lee, and Philip S.
Yu. 2003. Building text classifiers using positive
and unlabeled examples. In Proceedings of the 3rd
IEEE International Conference on Data Mining.
Marneffe, M., B. Maccartney, and C. Manning. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of the 5th Inter-
national Conference on Language Resources and
Evaluation.
Mei, Qiaozhu, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In Proceedings of the 13th ACM SIGKDD
conference.
Pecina, Pavel and Pavel Schlesinger. 2006. Com-
bining association measures for collocation extrac-
tion. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics.
Steyvers, Mark and Tom Griffiths, 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
Steyvers, Mark, Padhraic Smyth, Michal R. Zvi, and
Thomas Griffiths. 2004. Probabilistic author-topic
models for information discovery. In Proceedings
of the 10th ACM SIGKDD conference.
Teh, Yee Whye, Jordan, I. Michael, Beal, J. Matthew,
Blei, and M. David. 2006. Hierarchical dirichlet
processes. Journal of the American Statistical As-
sociation, 101(476):1566?1581, December.
Teh, Yee W., David Newman, and Max Welling. 2007.
A collapsed variational bayesian inference algo-
rithm for latent dirichlet alocation. In Proceedings
of the 21st Annual Conference on Neural Informa-
tion Processing Systems.
Wallach, Hanna M. 2006. Topic modeling: beyond
bag-of-words. In Proceedings of the 23rd Interna-
tional Conference on Machine Learning.
Wallach, Hanna M. 2008. Structured topic models for
language. Ph.D. thesis, University of Cambridge.
Wang, Xuerui and Andrew Mccallum. 2005. A note
on topical n-grams. Technical report, University of
Massachusetts Amherst.
Wang, Xuerui, Andrew McCallum, and Xing Wei.
2007. Topical n-grams: Phrase and topic discovery,
with an application to information retrieval. In Pro-
ceedings of the 7th IEEE International Conference
on Data Mining.
Wei, Xing and Bruce W. Croft. 2006. Lda-based doc-
ument models for ad-hoc retrieval. In Proceedings
of the 29th Annual International ACM SIGIR Con-
ference.
420
