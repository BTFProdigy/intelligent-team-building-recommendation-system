Probabilistic Sentence Reduction Using Support Vector Machines
Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi
Bao Tu Ho and Masaru Fukushi
Japan Advanced Institute of Science and Technology
1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN
{nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jp
Abstract
This paper investigates a novel application of sup-
port vector machines (SVMs) for sentence reduction.
We also propose a new probabilistic sentence reduc-
tion method based on support vector machine learn-
ing. Experimental results show that the proposed
methods outperform earlier methods in term of sen-
tence reduction performance.
1 Introduction
The most popular methods of sentence reduc-
tion for text summarization are corpus based
methods. Jing (Jing 00) developed a method
to remove extraneous phrases from sentences
by using multiple sources of knowledge to de-
cide which phrases could be removed. However,
while this method exploits a simple model for
sentence reduction by using statistics computed
from a corpus, a better model can be obtained
by using a learning approach.
Knight and Marcu (Knight and Marcu 02)
proposed a corpus based sentence reduction
method using machine learning techniques.
They discussed a noisy-channel based approach
and a decision tree based approach to sentence
reduction. Their algorithms provide the best
way to scale up the full problem of sentence re-
duction using available data. However, these al-
gorithms require that the word order of a given
sentence and its reduced sentence are the same.
Nguyen and Horiguchi (Nguyen and Horiguchi
03) presented a new sentence reduction tech-
nique based on a decision tree model without
that constraint. They also indicated that se-
mantic information is useful for sentence reduc-
tion tasks.
The major drawback of previous works on
sentence reduction is that those methods are
likely to output local optimal results, which may
have lower accuracy. This problem is caused by
the inherent sentence reduction model; that is,
only a single reduced sentence can be obtained.
As pointed out by Lin (Lin 03), the best sen-
tence reduction output for a single sentence is
not approximately best for text summarization.
This means that ?local optimal? refers to the
best reduced output for a single sentence, while
the best reduced output for the whole text is
?global optimal?. Thus, it would be very valu-
able if the sentence reduction task could gener-
ate multiple reduced outputs and select the best
one using the whole text document. However,
such a sentence reduction method has not yet
been proposed.
Support Vector Machines (Vapnik 95), on the
other hand, are strong learning methods in com-
parison with decision tree learning and other
learning methods (Sholkopf 97). The goal of
this paper is to illustrate the potential of SVMs
for enhancing the accuracy of sentence reduc-
tion in comparison with previous work. Accord-
ingly, we describe a novel deterministic method
for sentence reduction using SVMs and a two-
stage method using pairwise coupling (Hastie
98). To solve the problem of generating mul-
tiple best outputs, we propose a probabilistic
sentence reduction model, in which a variant of
probabilistic SVMs using a two-stage method
with pairwise coupling is discussed.
The rest of this paper will be organized as
follows: Section 2 introduces the Support Vec-
tor Machines learning. Section 3 describes the
previous work on sentence reduction and our
deterministic sentence reduction using SVMs.
We also discuss remaining problems of deter-
ministic sentence reduction. Section 4 presents
a probabilistic sentence reduction method using
support vector machines to solve this problem.
Section 5 discusses implementation and our ex-
perimental results; Section 6 gives our conclu-
sions and describes some problems that remain
to be solved in the future.
2 Support Vector Machine
Support vector machine (SVM)(Vapnik 95) is a
technique of machine learning based on statisti-
cal learning theory. Suppose that we are given
l training examples (xi, yi), (1 ? i ? l), where
xi is a feature vector in n dimensional feature
space, yi is the class label {-1, +1 } of xi. SVM
finds a hyperplane w.x + b = 0 which correctly
separates the training examples and has a max-
imum margin which is the distance between two
hyperplanes w.x+ b ? 1 and w.x+ b ? ?1. The
optimal hyperplane with maximum margin can
be obtained by solving the following quadratic
programming.
min 12 ?w?+ C0
l?
i
?i
s.t. yi(w.xi + b) ? 1? ?i
?i ? 0
(1)
where C0 is the constant and ?i is a slack vari-
able for the non-separable case. In SVM, the
optimal hyperplane is formulated as follows:
f(x) = sign
( l?
1
?iyiK(xi, x) + b
)
(2)
where ?i is the Lagrange multiple, and
K(x?, x??) is a kernel function, the SVM calcu-
lates similarity between two arguments x? and
x??. For instance, the Polynomial kernel func-
tion is formulated as follow:
K(x?, x??) = (x?.x??)p (3)
SVMs estimate the label of an unknown ex-
ample x whether the sign of f(x) is positive or
not.
3 Deterministic Sentence Reduction
Using SVMs
3.1 Problem Description
In the corpus-based decision tree approach, a
given input sentence is parsed into a syntax tree
and the syntax tree is then transformed into a
small tree to obtain a reduced sentence.
Let t and s be syntax trees of the original sen-
tence and a reduced sentence, respectively. The
process of transforming syntax tree t to small
tree s is called ?rewriting process? (Knight and
Marcu 02), (Nguyen and Horiguchi 03). To
transform the syntax tree t to the syntax tree
s, some terms and five rewriting actions are de-
fined.
An Input list consists of a sequence of words
subsumed by the tree t where each word in the
Input list is labelled with the name of all syntac-
tic constituents in t. Let CSTACK be a stack
that consists of sub trees in order to rewrite a
small tree. Let RSTACK be a stack that con-
sists of sub trees which are removed from the
Input list in the rewriting process.
? SHIFT action transfers the first word from the
Input list into CSTACK. It is written mathe-
matically and given the label SHIFT.
? REDUCE(lk,X) action pops the lk syntactic
trees located at the top of CSTACK and com-
bines them in a new tree, where lk is an integer
and X is a grammar symbol.
? DROP X action moves subsequences of words
that correspond to syntactic constituents from
the Input list to RSTACK.
? ASSIGN TYPE X action changes the label of
trees at the top of the CSTACK. These POS
tags might be different from the POS tags in
the original sentence.
? RESTORE X action takes the X element in
RSTACK and moves it into the Input list,
where X is a subtree.
For convenience, let configuration be a status
of Input list, CSTACK and RSTACK. Let cur-
rent context be the important information in a
configuration. The important information are
defined as a vector of features using heuristic
methods as in (Knight and Marcu 02), (Nguyen
and Horiguchi 03).
The main idea behind deterministic sentence
reduction is that it uses a rule in the current
context of the initial configuration to select a
distinct action in order to rewrite an input sen-
tence into a reduced sentence. After that, the
current context is changed to a new context and
the rewriting process is repeated for selecting
an action that corresponds to the new context.
The rewriting process is finished when it meets
a termination condition. Here, one rule corre-
sponds to the function that maps the current
context to a rewriting action. These rules are
learned automatically from the corpus of long
sentences and their reduced sentences (Knight
and Marcu 02), (Nguyen and Horiguchi 03).
3.2 Example
Figure 1 shows an example of applying a se-
quence of actions to rewrite the input sentence
(a, b, c, d, e), when each character is a word. It
illustrates the structure of the Input list, two
stacks, and the term of a rewriting process based
on the actions mentioned above. For example,
in the first row, DROP H deletes the sub-tree
with its root node H in the Input list and stores
it in the RSTACK. The reduced tree s can be
obtained after applying a sequence of actions
as follows: DROP H; SHIFT; ASSIGN TYPE K;
DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2
F; RESTORE H; SHIFT; ASSIGN TYPE D; RE-
DUCE 2G. In this example, the reduced sentence
is (b, e, a).
Figure 1: An Example of the Rewriting Process
3.3 Learning Reduction Rules Using
SVMs
As mentioned above, the action for each config-
uration can be decided by using a learning rule,
which maps a context to an action. To obtain
such rules, the configuration is represented by
a vector of features with a high dimension. Af-
ter that, we estimate the training examples by
using several support vector machines to deal
with the multiple classification problem in sen-
tence reduction.
3.3.1 Features
One important task in applying SVMs to text
summarization is to define features. Here, we
describe features used in our sentence reduction
models.
The features are extracted based on the cur-
rent context. As it can be seen in Figure 2, a
context includes the status of the Input list and
the status of CSTACK and RSTACK. We de-
fine a set of features for a current context as
described bellow.
Operation feature
The set of features as described in (Nguyen and
Horiguchi 03) are used in our sentence reduction
models.
Original tree features
These features denote the syntactic constituents
Figure 2: Example of Configuration
that start with the first unit in the Input list.
For example, in Figure 2 the syntactic con-
stituents are labels of the current element in the
Input list from ?VP? to the verb ?convince?.
Semantic features
The following features are used in our model as
semantic information.
? Semantic information about current words
within the Input list; these semantic types
are obtained by using the named entities such
as Location, Person, Organization and Time
within the input sentence. To define these
name entities, we use the method described in
(Borthwick 99).
? Semantic information about whether or not the
word in the Input list is a head word.
? Word relations, such as whether or not a word
has a relationship with other words in the sub-
categorization table. These relations and the
sub-categorization table are obtained using the
Commlex database (Macleod 95).
Using the semantic information, we are able to
avoid deleting important segments within the
given input sentence. For instance, the main
verb, the subject and the object are essential
and for the noun phrase, the head noun is essen-
tial, but an adjective modifier of the head noun
is not. For example, let us consider that the
verb ?convince? was extracted from the Com-
lex database as follows.
convince
NP-PP: PVAL (?of?)
NP-TO-INF-OC
This entry indicates that the verb ?convince?
can be followed by a noun phrase and a preposi-
tional phrase starting with the preposition ?of?.
It can be also followed by a noun phrase and a
to-infinite phrase. This information shows that
we cannot delete an ?of? prepositional phrase
or a to-infinitive that is the part of the verb
phrase.
3.3.2 Two-stage SVM Learning using
Pairwise Coupling
Using these features we can extract training
data for SVMs. Here, a sample in our training
data consists of pairs of a feature vector and
an action. The algorithm to extract training
data from the training corpus is modified using
the algorithm described in our pervious work
(Nguyen and Horiguchi 03).
Since the original support vector machine
(SVM) is a binary classification method, while
the sentence reduction problem is formulated as
multiple classification, we have to find a method
to adapt support vector machines to this prob-
lem. For multi-class SVMs, one can use strate-
gies such as one-vs all, pairwise comparison or
DAG graph (Hsu 02). In this paper, we use the
pairwise strategy, which constructs a rule for
discriminating pairs of classes and then selects
the class with the most winning among two class
decisions.
To boost the training time and the sentence
reduction performance, we propose a two-stage
SVM described below.
Suppose that the examples in training data
are divided into five groups m1,m2, ...,m5 ac-
cording to their actions. Let Svmc be multi-
class SVMs and let Svmc-i be multi-class SVMs
for a group mi. We use one Svmc classifier to
identify the group to which a given context e
should be belong. Assume that e belongs to
the group mi. The classifier Svmc-i is then used
to recognize a specific action for the context e.
The five classifiers Svmc-1, Svmc-2,..., Svmc-5
are trained by using those examples which have
actions belonging to SHIFT, REDUCE, DROP,
ASSIGN TYPE and RESTORE.
Table 1 shows the distribution of examples in
five data groups.
3.4 Disadvantage of Deterministic
Sentence Reductions
The idea of the deterministic algorithm is to
use the rule for each current context to select
the next action, and so on. The process termi-
nates when a stop condition is met. If the early
steps of this algorithm fail to select the best ac-
Table 1: Distribution of example data on five
data groups
Name Number of examples
SHIFT-GROUP 13,363
REDUCE-GROUP 11,406
DROP-GROUP 4,216
ASSIGN-GROUP 13,363
RESTORE-GROUP 2,004
TOTAL 44,352
tions, then the possibility of obtaining a wrong
reduced output becomes high.
One way to solve this problem is to select mul-
tiple actions that correspond to the context at
each step in the rewriting process. However,
the question that emerges here is how to deter-
mine which criteria to use in selecting multiple
actions for a context. If this problem can be
solved, then multiple best reduced outputs can
be obtained for each input sentence and the best
one will be selected by using the whole text doc-
ument.
In the next section propose a model for se-
lecting multiple actions for a context in sentence
reduction as a probabilistic sentence reduction
and present a variant of probabilistic sentence
reduction.
4 Probabilistic Sentence Reduction
Using SVM
4.1 The Probabilistic SVM Models
Let A be a set of k actions A =
{a1, a2...ai, ..., ak} and C be a set of n con-
texts C = {c1, c2...ci, ..., cn} . A probabilistic
model ? for sentence reduction will select an
action a ? A for the context c with probability
p?(a|c). The p?(a|c) can be used to score ac-
tion a among possible actions A depending the
context c that is available at the time of deci-
sion. There are several methods for estimating
such scores; we have called these ?probabilistic
sentence reduction methods?. The conditional
probability p?(a|c) is estimated using a variant
of probabilistic support vector machine, which
is described in the following sections.
4.1.1 Probabilistic SVMs using
Pairwise Coupling
For convenience, we denote uij = p(a = ai|a =
ai?aj , c). Given a context c and an action a, we
assume that the estimated pairwise class prob-
abilities rij of uij are available. Here rij can
be estimated by some binary classifiers. For
instance, we could estimate rij by using the
SVM binary posterior probabilities as described
in (Plat 2000). Then, the goal is to estimate
{pi}ki=1 , where pi = p(a = ai|c), i = 1, 2, ..., k.For this propose, a simple estimate of these
probabilities can be derived using the following
voting method:
pi = 2
?
j:j 6=i
I{rij>rji}/k(k ? 1)
where I is an indicator function and k(k? 1) is
the number of pairwise classes. However, this
model is too simple; we can obtain a better one
with the following method.
Assume that uij are pairwise probabilities of
the model subject to the condition that uij =
pi/(pi+pj). In (Hastie 98), the authors proposed
to minimize the Kullback-Leibler (KL) distance
between the rij and uij
l(p) =
?
i 6=j
nijrij log rijuij (4)
where rij and uij are the probabilities of a pair-
wise ai and aj in the estimated model and in
our model, respectively, and nij is the number
of training data in which their classes are ai or
aj . To find the minimizer of equation (6), they
first calculate
?l(p)
?pi =
?
i 6=j
nij(?rijpi +
1
pi + pj ).
Thus, letting ?l(p) = 0, they proposed to find
a point satisfying
?
j:j 6=i
nijuij =
?
j:j 6=i
nijrij ,
k?
i=1
pi = 1,
where i = 1, 2, ...k and pi > 0.
Such a point can be obtained by using an algo-
rithm described elsewhere in (Hastie 98). We
applied it to obtain a probabilistic SVM model
for sentence reduction using a simple method as
follows. Assume that our class labels belong to
l groups: M = {m1,m2...mi, ...,ml} , where l
is a number of groups and mi is a group e.g.,
SHIFT, REDUCE ,..., ASSIGN TYPE. Then
the probability p(a|c) of an action a for a given
context c can be estimated as follows.
p(a|c) = p(mi|c)? p(a|c,mi) (5)
where mi is a group and a ? mi. Here, p(mi|c)
and p(a|c,mi) are estimated by the method in
(Hastie 98).
4.2 Probabilistic sentence reduction
algorithm
After obtaining a probabilistic model p, we then
use this model to define function score, by which
the search procedure ranks the derivation of in-
complete and complete reduced sentences. Let
d(s) = {a1, a2, ...ad} be the derivation of a small
tree s, where each action ai belongs to a set of
possible actions. The score of s is the product
of the conditional probabilities of the individual
actions in its derivation.
Score(s) =
?
ai?d(s)
p(ai|ci) (6)
where ci is the context in which ai was decided.
The search heuristic tries to find the best re-
duced tree s? as follows:
s? = argmax? ?? ?
s?tree(t)
Score(s) (7)
where tree(t) are all the complete reduced trees
from the tree t of the given long sentence. As-
sume that for each configuration the actions
{a1, a2, ...an} are sorted in decreasing order ac-
cording to p(ai|ci), in which ci is the context
of that configuration. Algorithm 1 shows a
probabilistic sentence reduction using the top
K-BFS search algorithm. This algorithm uses
a breadth-first search which does not expand
the entire frontier, but instead expands at most
the top K scoring incomplete configurations in
the frontier; it is terminated when it finds M
completed reduced sentences (CL is a list of re-
duced trees), or when all hypotheses have been
exhausted. A configuration is completed if and
only if the Input list is empty and there is one
tree in the CSTACK. Note that the function
get-context(hi, j) obtains the current context of
the jth configuration in hi, where hi is a heap at
step i. The function Insert(s,h) ensures that the
heap h is sorted according to the score of each
element in h. Essentially, in implementation we
can use a dictionary of contexts and actions ob-
served from the training data in order to reduce
the number of actions to explore for a current
context.
5 Experiments and Discussion
We used the same corpus as described in
(Knight and Marcu 02), which includes 1,067
pairs of sentences and their reductions. To
evaluate sentence reduction algorithms, we ran-
domly selected 32 pairs of sentences from our
parallel corpus, which is refered to as the test
corpus. The training corpus of 1,035 sentences
extracted 44,352 examples, in which each train-
ing example corresponds to an action. The
SVM tool, LibSVM (Chang 01) is applied to
train our model. The training examples were
Algorithm 1 A probabilistic sentence reduction
algorithm
1: CL={Empty};
i = 0; h0={ Initial configuration}
2: while |CL| < M do
3: if hi is empty then4: break;5: end if6: u =min(|hi|, K)7: for j = 1 to u do8: c=get-context(hi, j)
9: Select m so that
m?
i=1
p(ai|c) < Q is maximal
10: for l=1 to m do11: parameter=get-parameter(al);12: Obtain a new configuration s by performing action al
with parameter
13: if Complete(s) then
14: Insert(s, CL)
15: else16: Insert(s, hi+1)17: end if18: end for19: end for20: i = i + 1
21: end while
divided into SHIFT, REDUCE, DROP, RE-
STORE, and ASSIGN groups. To train our
support vector model in each group, we used
the pairwise method with the polynomial ker-
nel function, in which the parameter p in (3)
and the constant C0 in equation (1) are 2 and
0.0001, respectively.
The algorithms (Knight and Marcu 02) and
(Nguyen and Horiguchi 03) served as the base-
line1 and the baseline2 for comparison with the
proposed algorithms. Deterministic sentence re-
duction using SVM and probabilistic sentence
reduction were named as SVM-D and SVMP, re-
spectively. For convenience, the ten top reduced
outputs using SVMP were called SVMP-10. We
used the same evaluation method as described
in (Knight and Marcu 02) to compare the pro-
posed methods with previous methods. For this
experiment, we presented each original sentence
in the test corpus to three judges who are spe-
cialists in English, together with three sentence
reductions: the human generated reduction sen-
tence, the outputs of the proposed algorithms,
and the output of the baseline algorithms.
The judges were told that all outputs were
generated automatically. The order of the out-
puts was scrambled randomly across test cases.
The judges participated in two experiments. In
the first, they were asked to determine on a scale
from 1 to 10 how well the systems did with re-
spect to selecting the most important words in
the original sentence. In the second, they were
asked to determine the grammatical criteria of
reduced sentences.
Table 2 shows the results of English language
sentence reduction using a support vector ma-
chine compared with the baseline methods and
with human reduction. Table 2 shows compres-
sion rates, and mean and standard deviation re-
sults across all judges, for each algorithm. The
results show that the length of the reduced sen-
tences using decision trees is shorter than using
SVMs, and indicate that our new methods out-
perform the baseline algorithms in grammatical
and importance criteria. Table 2 shows that the
Table 2: Experiment results with Test Corpus
Method Comp Gramma Impo
Baseline1 57.19% 8.60? 2.8 7.18? 1.92
Baseline2 57.15% 8.60? 2.1 7.42? 1.90
SVM-D 57.65% 8.76? 1.2 7.53? 1.53
SVMP-10 57.51% 8.80? 1.3 7.74? 1.39
Human 64.00% 9.05? 0.3 8.50? 0.80
first 10 reduced sentences produced by SVMP-
10 (the SVM probabilistic model) obtained the
highest performances. We also compared the
computation time of sentence reduction using
support vector machine with that in previous
works. Table 3 shows that the computational
times for SVM-D and SVMP-10 are slower than
baseline, but it is acceptable for SVM-D.
Table 3: Computational times of performing re-
ductions on test-set. Average sentence length
was 21 words.
Method Computational times (sec)
Baseline1 138.25
SVM-D 212.46
SVMP-10 1030.25
We also investigated how sensitive the pro-
posed algorithms are with respect to the train-
ing data by carrying out the same experi-
ment on sentences of different genres. We
created the test corpus by selecting sentences
from the web-site of the Benton Foundation
(http://www.benton.org). The leading sen-
tences in each news article were selected as the
most relevant sentences to the summary of the
news. We obtained 32 leading long sentences
and 32 headlines for each item. The 32 sen-
tences are used as a second test for our methods.
We use a simple ranking criterion: the more the
words in the reduced sentence overlap with the
words in the headline, the more important the
sentence is. A sentence satisfying this criterion
is called a relevant candidate.
For a given sentence, we used a simple
method, namely SVMP-R to obtain a re-
duced sentence by selecting a relevant candi-
date among the ten top reduced outputs using
SVMP-10.
Table 4 depicts the experiment results for
the baseline methods, SVM-D, SVMP-R, and
SVMP-10. The results shows that, when ap-
plied to sentence of a different genre, the per-
formance of SVMP-10 degrades smoothly, while
the performance of the deterministic sentence
reductions (the baselines and SVM determinis-
tic) drops sharply. This indicates that the prob-
abilistic sentence reduction using support vector
machine is more stable.
Table 4 shows that the performance of
SVMP-10 is also close to the human reduction
outputs and is better than previous works. In
addition, SVMP-R outperforms the determin-
istic sentence reduction algorithms and the dif-
ferences between SVMP-R?s results and SVMP-
10 are small. This indicates that we can ob-
tain reduced sentences which are relevant to the
headline, while ensuring the grammatical and
the importance criteria compared to the origi-
nal sentences.
Table 4: Experiment results with Benton Cor-
pus
Method Comp Gramma Impo
Baseline1 54.14% 7.61? 2.10 6.74? 1.92
Baseline2 53.13% 7.72? 1.60 7.02? 1.90
SVM-D 56.64% 7.86? 1.20 7.23? 1.53
SVMP-R 58.31% 8.25? 1.30 7.54? 1.39
SVMP-10 57.62% 8.60? 1.32 7.71? 1.41
Human 64.00% 9.01? 0.25 8.40? 0.60
6 Conclusions
We have presented a new probabilistic sentence
reduction approach that enables a long sentence
to be rewritten into reduced sentences based on
support vector models. Our methods achieves
better performance when compared with earlier
methods. The proposed reduction approach can
generate multiple best outputs. Experimental
results showed that the top 10 reduced sentences
returned by the reduction process might yield
accuracies higher than previous work. We be-
lieve that a good ranking method might improve
the sentence reduction performance further in a
text.
References
A. Borthwick, ?A Maximum Entropy Approach
to Named Entity Recognition?, Ph.D the-
sis, Computer Science Department, New York
University (1999).
C.-C. Chang and C.-J. Lin, ?LIB-
SVM: a library for support vec-
tor machines?, Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
H. Jing, ?Sentence reduction for automatic
text summarization?, In Proceedings of the
First Annual Meeting of the North Ameri-
can Chapter of the Association for Compu-
tational Linguistics NAACL-2000.
T.T. Hastie and R. Tibshirani, ?Classification
by pairwise coupling?, The Annals of Statis-
tics, 26(1): pp. 451-471, 1998.
C.-W. Hsu and C.-J. Lin, ?A comparison of
methods for multi-class support vector ma-
chines?, IEEE Transactions on Neural Net-
works, 13, pp. 415-425, 2002.
K. Knight and D. Marcu, ?Summarization be-
yond sentence extraction: A Probabilistic ap-
proach to sentence compression?, Artificial
Intelligence 139: pp. 91-107, 2002.
C.Y. Lin, ?Improving Summarization Perfor-
mance by Sentence Compression ? A Pi-
lot Study?, Proceedings of the Sixth Inter-
national Workshop on Information Retrieval
with Asian Languages, pp.1-8, 2003.
C. Macleod and R. Grishman, ?COMMLEX
syntax Reference Manual?; Proteus Project,
New York University (1995).
M.L. Nguyen and S. Horiguchi, ?A new sentence
reduction based on Decision tree model?,
Proceedings of 17th Pacific Asia Conference
on Language, Information and Computation,
pp. 290-297, 2003
V. Vapnik, ?The Natural of Statistical Learning
Theory?, New York: Springer-Verlag, 1995.
J. Platt,? Probabilistic outputs for support vec-
tor machines and comparison to regularized
likelihood methods,? in Advances in Large
Margin Classifiers, Cambridege, MA: MIT
Press, 2000.
B. Scholkopf et al ?Comparing Support Vec-
tor Machines with Gausian Kernels to Radius
Basis Function Classifers?, IEEE Trans. Sig-
nal Procesing, 45, pp. 2758-2765, 1997.
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1149?1155,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Multilingual Dependency Analysis System using Online
Passive-Aggressive Learning
Le-Minh Nguyen, Akira Shimazu, and Phuong-Thai Nguyen
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{nguyenml,shimazu,thai}@jaist.ac.jp
Xuan-Hieu Phan
Tohoku University
Aobayama 6-3-09, Sendai, 980-8579, Japan
hieuxuan@ecei.tohoku.ac.jp
Abstract
This paper presents an online algorithm for
dependency parsing problems. We propose
an adaptation of the passive and aggressive
online learning algorithm to the dependency
parsing domain. We evaluate the proposed
algorithms on the 2007 CONLL Shared
Task, and report errors analysis. Experimen-
tal results show that the system score is bet-
ter than the average score among the partici-
pating systems.
1 Introduction
Research on dependency parsing is mainly based
on machine learning methods, which can be called
history-based (Yamada and Matsumoto, 2003; Nivre
et al, 2006) and discriminative learning methods
(McDonald et al, 2005a; Corston-Oliver et al,
2006). The learning methods using in discrimina-
tive parsing are Perceptron (Collins, 2002) and on-
line large-margin learning (MIRA) (Crammer and
Singer, 2003).
The difference of MIRA-based parsing in com-
parison with history-based methods is that the
MIRA-based parser were trained to maximize the
accuracy of the overall tree. The MIRA based
parsing is close to maximum-margin parsing as in
Taskar et al (2004) and Tsochantaridis et al (2005)
for parsing. However, unlike maximum-margin
parsing, it is not limited to parsing sentences of 15
words or less due to computation time. The perfor-
mance of MIRA based parsing achieves the state-of-
the-art performance in English data (McDonald et
al., 2005a; McDonald et al, 2006).
In this paper, we propose a new adaptation of on-
line larger-margin learning to the problem of depen-
dency parsing. Unlike the MIRA parser, our method
does not need an optimization procedure in each
learning update, but users only an update equation.
This might lead to faster training time and easier im-
plementation.
The contributions of this paper are two-fold: First,
we present a training algorithm called PA learning
for dependency parsing, which is as easy to im-
plement as Perceptron, yet competitive with large
margin methods. This algorithm has implications
for anyone interested in implementing discrimina-
tive training methods for any application. Second,
we evaluate the proposed algorithm on the multilin-
gual data task as well as the domain adaptation task
(Nivre et al, 2007).
The remaining parts of the paper are organized as
follows: Section 2 proposes our dependency pars-
ing with Passive-Aggressive learning. Section 3
discusses some experimental results and Section 4
gives conclusions and plans for future work.
2 Dependency Parsing with
Passive-Aggressive Learning
This section presents the modification of Passive-
Aggressive Learning (PA) (Crammer et al, 2006)
for dependency parsing. We modify the PA algo-
rithm to deal with structured prediction, in which
our problem is to learn a discriminant function that
maps an input sentence x to a dependency tree y.
Figure 1 shows an example of dependency parsing
which depicts the relation of each word to another
word within a sentence. There are some algorithms
1149
Figure 1: This is an example of dependency tree
to determine these relations of each word to another
words, for instance, the modified CKY algorithm
(Eisner, 1996) is used to define these relations for
a given sentence.
2.1 Parsing Algorithm
Dependency-tree parsing as the search for the maxi-
mum spanning tree (MST) in a graph was proposed
by McDonald et al (2005b). In this subsection,
we briefly describe the parsing algorithms based on
the first-order MST parsing. Due to the limitation
of participation time, we only applied the first-order
decoding parsing algorithm in CONLL-2007. How-
ever, our algorithm can be used for the second order
parsing.
Let the generic sentence be denoted by x ; the
ith word of x is denoted by wi. The generic de-
pendency tree is denoted by y. If y is a dependency
tree for sentence x, we write (i, j) ? y to indicate
that there is a directed edge from word xwi to word
xwj in the tree, that is, xwi is the parent of xwj .
T = {(xt, yt)}nt=1 denotes the training data. We fol-
low the edge based factorization method of Eisner
(Eisner, 1996) and define the score of a dependency
tree as the sum of the score of all edges in the tree,
s(x, y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? ?(i, j) (1)
where ?(i, j) is a high-dimensional binary fea-
ture representation of the edge from xwi to xwj .
For example in Figure 1, we can present an example
?(i, j) as follows;
?(i, j) =
{
1 if xwi =? hit? andxwj =? ball?
0 otherwise
The basic question must be answered for models
of this form: how to find the dependency tree y with
the highest score for sentence x? The two algorithms
we employed in our dependency parsing model are
the Eisner parsing (Eisner, 1996) and Chu-Liu?s al-
gorithm (Chu and Liu, 1965). The algorithms are
commonly used in other online-learning dependency
parsing, such as in (McDonald et al, 2005a).
In the next subsection we will address the problem
of how to estimate the weight wi associated with a
feature ?i in the training data using an online PA
learning algorithm.
2.2 Online PA Learning
This section presents a modification of PA algo-
rithm for structured prediction, and its use in de-
pendency parsing. The Perceptron style for natural
language processing problems as initially proposed
by (Collins, 2002) can provide state of the art re-
sults on various domains including text chunking,
syntactic parsing, etc. The main drawback of the
Perceptron style algorithm is that it does not have a
mechanism for attaining the maximize margin of the
training data. It may be difficult to obtain high accu-
racy in dealing with hard learning data. The struc-
tured support vector machine (Tsochantaridis et al,
2005) and the maximize margin model (Taskar et al,
2004) can gain a maximize margin value for given
training data by solving an optimization problem (i.e
quadratic programming). It is obvious that using
such an optimization algorithm requires much com-
putational time. For dependency parsing domain,
McDonald et al(2005a) modified the MIRA learn-
ing algorithm (McDonald et al, 2005a) for struc-
tured domains in which the optimization problem
can be solved by using Hidreth?s algorithm (Censor
and Zenios, 1997), which is faster than the quadratic
programming technique. In contrast to the previous
method, this paper presents an online algorithm for
dependency parsing in which we can attain the max-
imize margin of the training data without using opti-
mization techniques. It is thus much faster and eas-
ier to implement. The details of PA algorithm for
dependency parsing are presented below.
Assume that we are given a set of sentences xi
and their dependency trees yi where i = 1, ..., n. Let
the feature mapping between a sentence x and a tree
y be: ?(x, y) = ?1(x, y),?2(x, y), ...,?d(x, y)
where each feature mapping ?j maps (x, y) to a real
value. We assume that each feature ?(x, y) is asso-
1150
ciated with a weight value. The goal of PA learning
for dependency parsing is to obtain a parameter w
that minimizes the hinge-loss function and the mar-
gin of learning data.
Input:S = {(xi; yi), i = 1, 2, ..., n} in which1
xi is the sentence and yi is a dependency tree
Aggressive parameter C2
Output: the PA learning model3
Initialize: w1 = (0, 0, ..., 0)4
for t=1, 2... do5
Receive an sentence xt6
Predict y?t = argmaxy?Y (wt ? ?(xt, yt))7
Suffer loss: lt =8
wt ??(xt, y?t )?wt ??(xt, yt) +
??(yt, y?t )
Set:9
PA: ?t = lt||?(xt,y?t )??(xt,yt)||2
PA-I: ?t = min{C, lt||?(xt,y?t )??(xt,yt)||2 }
PA-II: ?t = lt||?(xt,y?t )??(xt,yt)||2+ 12C
Update:
wt+1 = wt + ?t(?(xt, yt)? ?(xt, y?t ))
end10
Algorithm 1: The Passive-Aggressive algo-
rithm for dependency parsing.
Algorithm 1 shows the PA learning algorithm for
dependency parsing in which its three variants are
different only in the update formulas. In Algorithm
1, we employ two kinds of argmax algorithms: The
first is the decoding algorithm for projective lan-
guage data and the second one is for non-projective
language data. Algorithm 1 shows (line 8) p(y, yt)
is a real-valued loss for the tree yt relative to the
correct tree y. We define the loss of a dependency
tree as the number of words which have an incorrect
parent. Thus, the largest loss a dependency tree can
have is the length of the sentence. The similar loss
function is designed for the dependency tree with la-
beled. Algorithm 1 returns an averaged weight vec-
tor: an auxiliary weight vector v is maintained that
accumulates the values of w after each iteration, and
the returned weight vector is the average of all the
weight vectors throughout training. Averaging has
been shown to help reduce overfitting (McDonald et
al., 2005a; Collins, 2002). It is easy to see that the
main difference between the PA algorithms and the
Perceptron algorithm (PC) (Collins, 2002) as well as
the MIRA algorithm (McDonald et al, 2005a) is in
line 9. As we can see in the PC algorithm, we do
not need the value ?t and in the MIRA algorithm we
need an optimization algorithm to compute ?t. We
also have three updated formulations for obtaining
?t in Line 9. In the scope of this paper, we only
focus on using the second update formulation (PA-I
method) for training dependency parsing data.
2.3 Feature Set
We denote p-word: word of parent node in depen-
dency tree. c-word: word of child node. p-pos: POS
of parent node. c-pos: POS of child node. p-pos+1:
POS to the right of parent in sentence. p-pos-1: POS
to the left of parent. c-pos+1: POS to the right of
child. c-pos-1: POS to the left of child. b-pos: POS
of a word in between parent and child nodes. The
p-word,p-pos
p-word
p-pos
c-word, c-pos
c-word
c-pos
Table 1: Feature Set 1: Basic Unit-gram features
p-word, p-pos, c-word, c-pos
p-pos, c-word, c-pos
p-word, c-word, c-pos
p-word, p-pos, c-pos
p-word, p-pos, c-word
p-word, c-word
p-pos, c-pos
Table 2: Feature Set 2: Basic bi-gram features
p-pos, b-pos, c-pos
p-pos, p-pos+1, c-pos-1, c-pos
p-pos-1, p-pos, c-pos-1, c-pos
p-pos, p-pos+1, c-pos, c-pos+1
p-pos-1, p-pos, c-pos, c-pos+1
Table 3: Feature Set 3: In Between POS Features
and Surrounding Word POS Features
features used in our system are described below.
? Tables 1 and 2 show our basic features. These
1151
features are added for entire words as well as
for the 5-gram prefix if the word is longer than
5 characters.
? In addition to these features shown in Table 1,
the morphological information for each pair of
words p-word and c-word are represented. In
addition, we also add the conjunction morpho-
logical information of p-word and c-word. We
do not use the LEMMA and CPOSTAG infor-
mation in our set features. The morphological
information are obtained from FEAT informa-
tion.
? Table 3 shows our Feature set 3 which take the
form of a POS trigram: the POS of the par-
ent, of the child, and of a word in between,
for all words linearly between the parent and
the child. This feature was particularly helpful
for nouns identifying their parent (McDonald
et al, 2005a).
? Table 3 also depicts these features taken the
form of a POS 4-gram: The POS of the par-
ent, child, word before/after parent and word
before/after child. The system also used back-
off features with various trigrams where one of
the local context POS tags was removed.
? All features are also conjoined with the direc-
tion of attachment, as well as the distance be-
tween the two words being attached.
3 Experimental Results and Discussion
We test our parsing models on the CONLL-2007
(Hajic? et al, 2004; Aduriz et al, 2003; Mart?? et
al., 2007; Chen et al, 2003; Bo?hmova? et al, 2003;
Marcus et al, 1993; Johansson and Nugues, 2007;
Prokopidis et al, 2005; Csendes et al, 2005; Mon-
temagni et al, 2003; Oflazer et al, 2003) data set on
various languages including Arabic, Basque, Cata-
lan, Chinese, English, Italian, Hungarian, and Turk-
ish. Each word is attached by POS tags for each sen-
tence in both the training and the testing data. Table
4 shows the number of training and testing sentences
for these languages. The table shows that the sen-
tence length in Arabic data is largest and its size of
training data is smallest. These factors might be af-
fected to the accuracy of our proposed algorithm as
we will discuss later.
The training and testing were conducted on a pen-
tium IV at 4.3 GHz. The detailed information about
the data are shown in the CONLL-2007 shared task.
We applied non-projective and projective parsing
along with PA learning for the data in CONLL-2007.
Table 5 reports experimental results by using the
first order decoding method in which an MST pars-
ing algorithm (McDonald et al, 2005b) is applied
for non-projective parsing and the Eisner?s method
is used for projective language data. In fact, in our
method we applied non-projective parsing for the
Italian data, the Turkish data, and the Greek data.
This was because we did not have enough time to
train all training data using both projective and non-
projective parsing. This is the problem of discrimi-
native learning methods when performing on a large
set of training data. In addition, to save time in train-
ing we set the number of best trees k to 1 and the
parameter C is set to 0.05.
Table 5 shows the comparison of the proposed
method with the average, and three top systems on
the CONLL-2007. As a result, our method yields
results above the average score on the CONLL-2007
shared task (Nivre et al, 2007).
Table 5 also indicates that the Basque results ob-
tained a lower score than other data. We obtained
69.11 UA score and 58.16 LA score, respectively.
These are far from the results of the Top3 scores
(81.13 and 75.49). We checked the outputs of the
Basque data to understand the main reason for the
errors. We see that the errors in our methods are
usually mismatched with the gold data at the labels
?ncmod? and ?ncsubj?. The main reason might be
that the application of projective parsing for this data
in both training and testing is not suitable. This was
because the number of sentences with at least 1 non
projective relation in the data is large (26.1).
The Arabic score is lower than the scores of other
data because of some difficulties in our method as
follows. Morphological and sentence length prob-
lems are the main factors which affect the accuracy
of parsing Arabic data. In addition, the training size
in the Arabic is also a problem for obtaining a good
result. Furthermore, since our tasks was focused on
improving the accuracy of English data, it might be
unsuitable for other languages. This is an imbalance
1152
Languages Training size Tokens size tokens-per-sent % of NPR % of-sentence AL-1-NPR
Arabic 2,900 112,000 38.3 0.4 10.1
Basque 3,200 51,000 15.8 2.9 26.2
Catalan 15,000 431,000 28.8 0.1 2.9
Chinese 57,000 337,000 5.9 0.0 0.0
Czech 25,400 432,000 17.0 1.9 23.2
English 18,600 447,000 24.0 0.3 6.7
Greek 2,700 65,000 24.2 1.1 20.3
Hungarian 6,000 132,000 21.8 2.9 26.4
Italian 3,100 71,000 22.9 0.5 7.4
Turkish 5,600 65,000 11.6 0.5 33.3
Table 4: The data used in the multilingual track (Nivre et al, 2007). NPR means non-projective-relations.
AL-1-NPR means at-least-least 1 non-projective relation.
problem in our method. Table 5 also shows the com-
parison of our system to the average score and the
Top3 scores. It depicts that our system is accurate
in English data, while it has low accuracy in Basque
and Arabic data.
We also evaluate our models in the domain adap-
tation tasks. This task is to adapt our model trained
on PennBank data to the test data in the Biomedical
domain. The pchemtb-closed shared task (Marcus
et al, 1993; Johansson and Nugues, 2007; Kulick
et al, 2004) is used to illustrate our models. We do
not use any additional unlabeled data in the Biomed-
ical domain. Only the training data in the PennBank
is used to train our model. Afterward, we selected
carefully a suitable parameter using the development
test set. We set the parameter C to 0.01 and se-
lect the non projective parsing for testing to obtain
the highest result in the development data after per-
forming several experiments. After that, the trained
model was used to test the data in Biomedical do-
main. The score (UA=82.04; LA=79.50) shows that
our method yields results above the average score
(UA=76.42; LA=73.03). In addition, it is officially
coming in 4th place out of 12 teams and within 1.5%
of the top systems.
The good result of performing our model in an-
other domain suggested that the PA learning seems
sensitive to noise. We hope that this problem is
solved in future work.
4 Conclusions
This paper presents an online algorithm for depen-
dency parsing problem which have tested on various
language data in CONLL-2007 shared task. The per-
formance in English data is close to the Top3 score.
We also perform our algorithm on the domain adap-
tation task, in which we only focus on the training of
the source data and select a suitable parameter using
the development set. The result is very good as it
is close to the Top3 score of participating systems.
Future work will also be focused on extending our
method to a version of using semi-supervised learn-
ing that can efficiently be learnt by using labeled and
unlabeled data. We hope that the application of the
PA algorithm to other NLP problems such as seman-
tic parsing will be explored in future work.
Acknowledgments
We would like to thank D. Yuret for his helps in
checking errors of my parser?s outputs. We would
like to thank Vinh-Van Nguyen his helps during the
revision process and Mary Ann Mooradian for cor-
recting the paper.
We would like to thank to anonymous review-
ers for helpful discussions and comments on the
manuscript. Thank also to Sebastian Riedel for
checking the issues raised in the reviews.
The work on this paper was supported by a Mon-
bukagakusho 21st COE Program.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
1153
Languages Unlabled Accuracy Labeled Accuracy NTeams
PA-I Average Top3 Top2 Top1 PA-I Average Top3 Top2 Top1
Arabic 73.46 78.84 84.21 85.81 86.09 68.34 74.75 83.0 75.08 76.52 20
Basque 69.11 75.15 81.13 81.93 81.13 58.16 68.06 75.49 75.73 76.92 20
Catalan 88.12 87.98 93.12 93.34 93.40 83.23 79.85 87.90 88.16 88.70 20
Chinese 84.05 81.98 87.91 88.88 88.94 79.77 76.59 83.51 83.84 84.69 21
Czech 80.91 77.56 84.19 85.16 86.28 72.54 70.12 77.98 78.60 80.19 20
English 88.01 82.67 89.87 90.13 90.63 86.73 80.95 88.41 89.01 89.61 23
Greek 77.56 77.78 81.37 82.04 84.08 70.42 70.22 74.42 74.65 76.31 20
Hungarian 78.13 76.34 82.49 83.51 83.55 68.12 71.49 78.09 79.53 80.27 21
Italian 80.40 82.45 87.68 87.77 87.91 75.06 78.06 78.09 79.53 80.27 20
Turkish 80.19 73.19 85.77 85.77 86.22 67.63 73.19 79.24 79.79 79.81 20
Multilingual-average 79.99 71.13 85.62 85.71 86.55 72.52 65.77 79.90 80.28 80.32 23
pchemtb-closed 82.04 76.42 83.08 83.38 83.42 79.50 73.03 80.22 80.40 81.06 8
Table 5: Dependency accuracy in the CONLL-2007 shared task.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
Y. Censor and S.A. Zenios. 1997. Parallel optimization:
theory, algorithms, and applications. In Oxford Uni-
versity Press.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
S. Corston-Oliver, A. Aue, K. Duh, , and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of HLT/NAACL.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, J. Keshet, S.Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:581?585.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING 1996, pages 340?345.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald, K. Cramer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
R. McDonald, K. Crammer, and F. Pereira. 2006. Multi-
lingual dependency parsing with a two-stage discrim-
inative parser. In Conference on Natural Language
Learning.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
1154
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
B. Taskar, D. Klein, M. Collins, D. Koller, and C.D. Man-
ning. 2004. Max-margin parsing. In proceedings of
EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for interde-
pendent and structured output spaces. In proceedings
ICML 2004.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
1155
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 619?626,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic parsing with Structured SVM Ensemble Classification Models
Le-Minh Nguyen, Akira Shimazu, and Xuan-Hieu Phan
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{nguyenml,shimazu,hieuxuan}@jaist.ac.jp
Abstract
We present a learning framework for struc-
tured support vector models in which
boosting and bagging methods are used to
construct ensemble models. We also pro-
pose a selection method which is based on
a switching model among a set of outputs
of individual classifiers when dealing with
natural language parsing problems. The
switching model uses subtrees mined from
the corpus and a boosting-based algorithm
to select the most appropriate output. The
application of the proposed framework on
the domain of semantic parsing shows ad-
vantages in comparison with the original
large margin methods.
1 Introduction
Natural language semantic parsing is an interest-
ing problem in NLP (Manning and Schutze, 1999)
as it would very likely be part of any interesting
NLP applications (Allen, 1995). For example, the
necessary of semantic parsing for most of NLP ap-
plication and the ability to map natural language to
a formal query or command language is critical for
developing more user-friendly interfaces.
Recent approaches have focused on using struc-
tured prediction for dealing with syntactic parsing
(B. Taskar et. al., 2004) and text chunking prob-
lems (Lafferty et al 2001). For semantic pars-
ing, Zettlemoyer and Collins (2005) proposed a
method for mapping a NL sentence to its logical
form by structured classification using a log-linear
model that represents a distribution over syntac-
tic and semantic analyses conditioned on the in-
put sentence. Taskar et al(B. Taskar et. al.,
2004) present a discriminative approach to pars-
ing inspired by the large-margin criterion under-
lying support vector machines in which the loss
function is factorized analogous to the decoding
process. Tsochantaridis et al(Tsochantaridis et
al., 2004) propose a large-margin models based on
SVMs for structured prediction (SSVM) in gen-
eral and apply it for syntactic parsing problem so
that the models can adapt to overlap features, ker-
nels, and any loss functions.
Following the successes of the SSVM algorithm
to structured prediction, in this paper we exploit
the use of SSVM to the semantic parsing problem
by modifying the loss function, feature representa-
tion, maximization algorithm in the original algo-
rithm for structured outputs (Tsochantaridis et al,
2004).
Beside that, forming committees or ensembles
of learned systems is known to improve accuracy
and bagging and boosting are two popular ensem-
ble methods that typically achieve better accuracy
than a single classifier (Dietterich, 2000). This
leads to employing ensemble learning models for
SSVM is worth to investigate. The first problem of
forming an ensemble learning for semantic pars-
ing is how to obtain individual parsers with re-
spect to the fact that each individual parser per-
forms well enough as well as they make different
types of errors. The second one is that of com-
bining outputs from individual semantic parsers.
The natural way is to use the majority voting strat-
egy that the semantic tree with highest frequency
among the outputs obtained by individual parsers
is selected. However, it is not sure that the ma-
jority voting technique is effective for combining
complex outputs such as a logical form structure.
Thus, a better combination method for semantic
tree output should be investigated.
To deal with these problems, we proposed an
619
ensemble method which consists of learning and
averaging phases in which the learning phases are
either a boosting or a bagging model, and the av-
eraging phase is based on a switching method on
outputs obtained from all individual SSVMs. For
the averaging phase, the switching model is used
subtrees mined from the corpus and a boosting-
based algorithm to select the most appropriate out-
put.
Applications of SSVM ensemble in the seman-
tic parsing problem show that the proposed SSVM
ensemble is better than the SSVM in term of the F-
measure score and accuracy measurements.
The rest of this paper are organized as follows:
Section 2 gives some background about the struc-
tured support vector machine model for structured
predictions and related works. Section 3 proposes
our ensemble method for structured SVMs on the
semantic parsing problem. Section 4 shows exper-
imental results and Section 5 discusses the advan-
tage of our methods and describes future works.
2 Backgrounds
2.1 Related Works
Zelle and Mooney initially proposed the empir-
ically based method using a corpus of NL sen-
tences and their formal representation for learn-
ing by inductive logic programming (Zelle, 1996).
Several extensions for mapping a NL sentence to
its logical form have been addressed by (Tang,
2003). Transforming a natural language sentence
to a logical form was formulated as the task of de-
termining a sequence of actions that would trans-
form the given input sentence to a logical form
(Tang, 2003). The main problem is how to learn a
set of rules from the corpus using the ILP method.
The advantage of the ILP method is that we do not
need to design features for learning a set of rules
from corpus. The disadvantage is that it is quite
complex and slow to acquire parsers for mapping
sentences to logical forms. Kate et alpresented
a method (Kate et al, 2005) that used transfor-
mation rules to transform NL sentences to logi-
cal forms. Those transformation rules are learnt
using the corpus of sentences and their logical
forms. This method is much simpler than the ILP
method, while it can achieve comparable result on
the CLANG (Coach Language) and Query corpus.
The transformation based method has the condi-
tion that the formal language should be in the form
of LR grammar.
Ge and Mooney also presented a statistical
method (Ge and Mooney, 2005) by merging syn-
tactic and semantic information. Their method
relaxed the condition in (Kate et al, 2005) and
achieved a state-of the art performance on the
CLANG and query database corpus. However the
distinction of this method in comparison with the
method presented in (Kate et al, 2005) is that Ge
and Mooney require training data to have SAPTs,
while the transforation based method only needs
the LR grammar for the formal language.
The work proposed by (Zettlemoyer and
Collins, 2005) that maps a NL sentence to its log-
ical form by structured classification, using a log-
linear model that represents a distribution over
syntactic and semantic analyses conditioned on
the input sentence. This work is quite similar to
our work in considering the structured classifica-
tion problem. The difference is that we used the
kernel based method instead of a log-linear model
in order to utilize the advantage of handling a very
large number of features by maximizing the mar-
gin in the learning process.
2.2 Structured Support Vector Models
Structured classification is the problem of predict-
ing y from x in the case where y has a meaningful
internal structure. Elements y ? Y may be, for in-
stance, sequences, strings, labelled trees, lattices,
or graphs.
The approach we pursue is to learn a dis-
criminant function F : X ? Y ? R over <
input, output > pairs from which we can derive
a prediction by maximizing F over the response
variable for a specific given input x. Hence, the
general form of our hypotheses f is
f(x;w) = argmax
y?Y
F (x; y;w)
where w denotes a parameter vector.
As the principle of the maximum-margin pre-
sented in (Vapnik, 1998), in the structured clas-
sification problem, (Tsochantaridis et al, 2004)
proposed several maximum-margin optimization
problems.
For convenience, we define
??i(y) ? ?(xi, yi)? ?(xi, y)
where (xi,yi) is the training data.
The hard-margin optimization problem is:
SVM0 : minw
1
2
?w?2 (1)
620
?i,?y ? Y \yi : ?w, ??i(y)? > 0 (2)
where ?w, ??i(y)? is the linear combination of
feature representation for input and output.
The soft-margin criterion was proposed
(Tsochantaridis et al, 2004) in order to allow
errors in the training set, by introducing slack
variables.
SVM1 : min
1
2
?w?2 +
C
n
n?
i=1
?i,s.t.?i, ?i ? 0
(3)
?i, ?y ? Y \yi : ?w, ??i(y)? ? 1? ?i (4)
Alternatively, using a quadratic term C2n
?
i
?2i to
penalize margin violations, we obtained SVM2.
Here C > 0 is a constant that control the trade-
off between training error minimization and mar-
gin maximization.
To deal with problems in which |Y | is very
large, such as semantic parsing, (Tsochantaridis et
al., 2004) proposed two approaches that general-
ize the formulation SVM0 and SVM1 to the cases
of arbitrary loss function. The first approach is to
re-scale the slack variables according to the loss
incurred in each of the linear constraints.
SVM?s : min????
w,?
1
2
?w?2 +
C
n
n?
i=1
?i,s.t.?i, ?i ? 0
(5)
?i,?y ? Y \yi : ?w, ??i(y)? ?
1? ?i
?(yi, y)
(6)
The second approach to include loss function is to
re-scale the margin as a special case of the Ham-
ming loss. The margin constraints in this setting
take the following form:
?i,?y ? Y \yi : ?w, ??i(y)? ? ?(yi, y)? ?i (7)
This set of constraints yields an optimization prob-
lem, namely SVM?m1 .
2.3 Support Vector Machine Learning
The support vector learning algorithm aims to find
a small set of active constraints that ensures a suf-
ficiently accurate solution. The detailed algorithm,
as presented in (Tsochantaridis et al, 2004) can be
applied to all SVM formulations mentioned above.
The only difference between them is the cost func-
tion in the following optimization problems:
SVM?s1 : H(y) ? (1? ???i(y), w?)?(yi, y)
SVM?s2 : H(y) ? (1? ???i(y), w?)
?
?(yi, y)
SVM?m1 : H(y) ? (?(yi, y)? ???i(y), w?)
SVM?m2 : H(y) ? (
?
?(yi, y)? ???i(y), w?)
Typically, the way to apply structured SVM is to
implement feature mapping ?(x, y), the loss func-
tion ?(yi, y), as well as the maximization algo-
rithm. In the following section, we apply a struc-
tured support vector machine to the problem of se-
mantic parsing in which the mapping function, the
maximization algorithm, and the loss function are
introduced.
3 SSVM Ensemble for Semantic Parsing
Although the bagging and boosting techniques
have known to be effective for improving the
performance of syntactic parsing (Henderson and
Brill, 2000), in this section we focus on our en-
semble learning of SSVM for semantic parsing
and propose a new effective switching model for
either bagging or boosting model.
3.1 SSVM for Semantic Parsing
As discussed in (Tsochantaridis et al, 2004), the
major problem for using the SSVM is to imple-
ment the feature mapping ?(x, y), the loss func-
tion ?(yi, y), as well as the maximization algo-
rithm. For semantic parsing, we describe here
the method of structure representation, the feature
mapping, the loss function, and the maximization
algorithm.
3.1.1 Structure representation
A tree structure representation incorporated
with semantic and syntactic information is named
semantically augmented parse tree (SAPT) (Ge
and Mooney, 2005). As defined in (Ge and
Mooney, 2005), in an SAPT, each internal node in
the parse tree is annotated with a semantic label.
Figure 1 shows the SAPT for a simple sentence in
the CLANG domain. The semantic labels which
are shown after dashes are concepts in the domain.
Some concepts refer to predicates and take an or-
dered list of arguments. Concepts such as ?team?
and ?unum? might not have arguments. A special
semantic label, ?null?, is used for a node that does
not correspond to any concept in the domain.
3.1.2 Feature mapping
For semantic parsing, we can choose a mapping
function to get a model that is isomorphic to a
probabilistic grammar in which each rule within
the grammar consists of both a syntactic rule and
a semantic rule. Each node in a parse tree y for a
sentence x corresponds to a grammar rule gj with
a score wj .
621
Figure 1: An Example of tree representation in
SAPT
All valid parse trees y for a sentence x are
scored by the sum of the wj of their nodes, and the
feature mapping ?(x, y) is a history gram vector
counting how often each grammar rule gj occurs
in the tree y. Note that the grammar rules are lex-
icalized. The example shown in Figure 2 clearly
explains the way features are mapped from an in-
put sentence and a tree structure.
3.1.3 Loss function
Let z and zi be two semantic tree outputs and
|zi| and |zi| be the number of brackets in z and
zi, respectively. Let n be the number of common
brackets in the two trees. The loss function be-
tween zi and z is computed as bellow.
F ? loss(zi, z) = 1?
2? n
|zi|+ |z|
(8)
zero? one(zi, z) =
{
1 if zi 6= z
0 otherwise
(9)
3.1.4 Maximization algorithm
Note that the learning function can be efficiently
computed by finding a structure y ? Y that max-
imizes F (x, y;w)=?w, ??i(y)? via a maximiza-
tion algorithm. Typically we used a variant of
Figure 2: Example of feature mapping using tree
representation
CYK maximization algorithm which is similar to
the one for the syntactic parsing problem (John-
son,1999). There are two phases in our maximiza-
tion algorithm for semantic parsing. The first is
to use a variant of CYK algorithm to generate a
SAPT tree. The second phase then applies a deter-
ministic algorithm to output a logical form. The
score of the maximization algorithm is the same
with the obtained value of the CYK algorithm.
The procedure of generating a logical form us-
ing a SAPT structure originally proposed by (Ge
and Mooney, 2005) and it is expressed as Algo-
rithm 1. It generates a logical form based on a
knowledge database K for given input node N .
The predicate argument knowledge, K, specifies,
for each predicate, the semantic constraints on its
arguments. Constraints are specified in terms of
the concepts that can fill each argument, such as
player(team, unum) and bowner(player).
The GETsemanticHEAD determines which of
node?s children is its semantic head based on they
having matching semantic labels. For example, in
Figure 1N3 is determined to be the semantic head
of the sentence since it matches N8?s semantic la-
bel. ComposeMR assigns their meaning represen-
tation (MR) to fill the arguments in the head?s MR
to construct the complete MR for the node. Figure
1 shows an example of using BuildMR to generate
a semantic tree to a logical form.
622
Input: The root node N of a SAPT
Predicate knowledge K
Notation: XMR is the MR of node X
Output: NMR
Begin
Ci= the ith child node of N
Ch= GETsemanticHEAD(N )
ChMR =BuildMR(Ch,K)
for each other child Ci where i 6= h do
CiMR =BuildMR(Ci,K)
ComposeMR(ChMR ,CiMR ,K)
end
NMR=ChMR
End
Algorithm 1: BuildMR(N,K): Computing a logical
form form an SAPT(Ge and Mooney, 2005)
Input: S = (xi; yi; zi), i = 1, 2, ..., l in which xi is1
the sentence and yi, zi is the pair of tree structure and
its logical form
Output: SSVM model2
repeat3
for i = 1 to n do4
5
SVM?s1 : H(y, z) ? (1? ???i(y), w?)?(zi, z)
SVM?s2 : H(y, z) ? (1? ???i(y), w?)
?
?(zi, z)
SVM?m1 : H(y, z) ? (?(zi, z)? ???i(y), w?)
SVM?m2 : H(y, z) ? (
?
?(zi, z)? ???i(y), w?)
compute < y?, z? >= argmaxy,z?Y,Z H(Y,Z);6
compute ?i = max{0,maxy,z?Si H(y, z)};7
if H(y?, z?) > ?i + ? then8
Si ? Si ? y?, z?;9
solving optimization with SVM;10
end11
end12
until no Si has changed during iteration;13
Algorithm 2: Algorithm of SSVM learning for se-
mantic parsing. The algorithm is based on the original
algorithm(Tsochantaridis et al, 2004)
3.1.5 SSVM learning for semantic parsing
As mentioned above, the proposed maximiza-
tion algorithm includes two parts: the first is to
parse the given input sentence to the SAPT tree
and the second part (BuildMR) is to convert the
SAPT tree to a logical form. Here, the score
of maximization algorithm is the same with the
score to generate a SAPT tree and the loss function
should be the measurement based on two logical
form outputs. Algorithm 2 shows our generation
of SSVM learning for the semantic parsing prob-
lem which the loss function is based on the score
of two logical form outputs.
3.2 SSVM Ensemble for semantic parsing
The structured SVM ensemble consists of a train-
ing and a testing phase. In the training phase, each
individual SSVM is trained independently by its
own replicated training data set via a bootstrap
method. In the testing phase, a test example is ap-
plied to all SSVMs simultaneously and a collec-
tive decision is obtained based on an aggregation
strategy.
3.2.1 Bagging for semantic parsing
The bagging method (Breiman, 1996) is sim-
ply created K bootstrap with sampling m items
from the training data of sentences and their logi-
cal forms with replacement. We then applied the
SSVM learning in the K generated training data
to create K semantic parser. In the testing phase,
a given input sentence is parsed by K semantic
parsers and their outputs are applied a switching
model to obtain an output for the SSVM ensemble
parser.
3.2.2 Boosting for semantic parsing
The representative boosting algorithm is the
AdaBoost algorithm (Schapire, 1999). Each
SSVM is trained using a different training set.
Assuming that we have a training set TR =
(xi; yi)|i = 1, 2, ..., l consisting of l samples and
each sample in the TR is assigned to have the
same value of weight p0(xi) = 1/l. For training
the kth SSVM classifier, we build a set of training
samples
TRboostk = (xi; yi)|i = 1, 2, .., l
? that is ob-
tained by selecting l?(< l) samples among the
whole data set TR according to the weight value
pk?1(xi) at the (k-1)th iteration. This training
samples is used for training the kth SSVM clas-
sifier. Then, we obtained the updated weight val-
ues pk(xi) of the training samples as follows. The
weight values of the incorrectly classified sam-
ples are increased but the weight values of the
correctly classified samples are decreased. This
shows that the samples which are hard to clas-
sify are selected more frequently. These updated
weight values will be used for building the train-
ing samples TRboostk+1 = (xi; yi)|i = 1, 2, ..., l?
of the (k+1)th SSVM classifier. The sampling pro-
cedure will be repeated until K training samples
set has been built for the Kth SSVM classifier.
623
3.2.3 The proposed SSVM ensemble model
We construct a SSVM ensemble model by using
different parameters for each individual SSVM to-
gether with bagging and boosting models. The pa-
rameters we used here including the kernel func-
tion and the loss function as well as features used
in a SSVM. Let N and K be the number of dif-
ferent parameters and individual semantic parsers
in a SSVM ensemble, respectively. The motiva-
tion is to create individual parsers with respect to
the fact that each individual parser performs well
enough as well as they make different types of
errors. We firstly create N ensemble models us-
ing either boosting or bagging models to obtain
N?K individual parsers. We then select the top T
parsers so that their errors on the training data are
minimized and in different types. After forming
an ensemble model of SSVMs, we need a process
for aggregating outputs of individual SSVM clas-
sifiers. Intuitively, a simplest way is to use a vot-
ing method to select the output of a SSVM ensem-
ble. Instead, we propose a switching method using
subtrees mining from the set of trees as follows.
Let t1, t2, ..., tK be a set of candidate parse trees
produced by an ensemble of K parsers. From the
set of tree t1, t2, ..., tK we generated a set of train-
ing data that maps a tree to a label +1 or -1, where
the tree tj received the label +1 if it is an corrected
output. Otherwise tj received the label -1. We
need to define a learning function for classifying a
tree structure to two labels +1 and -1.
For this problem, we can apply a boosting tech-
nique presented in (Kudo and Matsumoto, 2004).
The method is based on a generation of Adaboost
(Schapire, 1999) in which subtrees mined from the
training data are severed as weak decision stump
functions.
The technique for mining these subtrees is pre-
sented in (Zaki, 2002) which is an efficient method
for mining a large corpus of trees. Table 1 shows
an example of mining subtrees on our corpus. One
Table 1: Subtrees mined from the corpus
Frequency Subtree
20 (and(bowner)(bpos))
4 (and(bowner)(bpos(right)))
4 (bpos(circle(pt(playerour11))))
15 (and(bpos)(not(bpos)))
8 (and(bpos(penalty-areaour)))
problem for using the boosting subtrees algorithm
(BT) in our switching models is that we might ob-
tain several outputs with label +1. To solve this,
we evaluate a score for each value +1 obtained by
the BT and select the output with the highest score.
In the case of there is no tree output received the
value +1, the output of the first individual semantic
parser will be the value of our switching model.
4 Experimental Results
For the purpose of testing our SSVM ensem-
bles on semantic parsing, we used the CLANG
corpus which is the RoboCup Coach Language
(www.robocup.org). In the Coach Competition,
teams of agents compete on a simulated soccer
field and receive advice from a team coach in
a formal language. The CLANG consists of 37
non-terminal and 133 productions; the corpus for
CLANG includes 300 sentences and their struc-
tured representation in SAPT (Kate et al, 2005),
then the logical form representations were built
from the trees. Table 2 shows the statistic on the
CLANG corpus.
Table 2: Statistics on CLANG corpus. The average length
of an NL sentence in the CLANG corpus is 22.52 words. This
indicates that CLANG is the hard corpus. The average length
of the MRs is also large in the CLANG corpus.
Statistic CLANG
No.of. Examples 300
Avg. NL sentence length 22.5
Avg. MR length (tokens) 13.42
No. of non-terminals 16
No. of productions 102
No. of unique NL tokens 337
Table 3: Training accuracy on CLANG corpus
Parameter Training Accuracy
linear+F-loss(?s) 83.9%
polynomial(d=2)+F-loss (?m) 90.1%
polynomial(d=2)+F-loss(?s) 98.8%
polynomial(d=2)+F-loss(?m) 90.2%
RBF+F-loss(?s) 86.3%
To create an ensemble learning with SSVM, we
used the following parameters with the linear ker-
nel, the polynomial kernel, and RBF kernel, re-
spectively. Table 3 shows that they obtained dif-
ferent accuracies on the training corpus, and their
accuracies are good enough to form a SSVM en-
semble. The parameters in Table 3 is used to form
our proposed SSVM model.
The following is the performance of the
SSVM1, the boosting model, the bagging model,
and the models with different parameters on the
1The SSVM is obtained via http://svmlight.joachims.org/
624
CLANG corpus2. Note that the numbers of in-
dividual SSVMs in our ensemble models are set
to 10 for boosting and bagging, and each individ-
ual SSVM can be used the zero-one and F1 loss
function. In addition, we also compare the per-
formance of the proposed ensemble SSVM mod-
els and the conventional ensemble models to as-
sert that our models are more effective in forming
SSVM ensemble learning.
We used the standard 10-fold cross validation
test for evaluating the methods. To train a BT
model for the switching phase in each fold test,
we separated the training data into 10-folds. We
keep 9/10 for forming a SSVM ensemble, and
1/10 for producing training data for the switch-
ing model. In addition, we mined a subset of
subtrees in which a frequency of each subtree is
greater than 2, and used them as weak functions
for the boosting tree model. Note that in testing
the whole training data in each fold is formed a
SSVM ensemble model to use the BT model esti-
mated above for selecting outputs obtained by the
SSVM ensemble.
To evaluate the proposed methods in parsing NL
sentences to logical form, we measured the num-
ber of test sentences that produced complete log-
ical forms, and the number of those logical forms
that were correct. For CLANG, a logical form is
correct if it exactly matches the correct representa-
tion, up to reordering of the arguments of commu-
tative operators. We used the evaluation method
presented in (Kate et al, 2005) as the formula be-
low.
precision = #correct?representation#completed?representation
recall = #correct?representation#sentences
Table 4 shows the results of SSVM, the SCSIS-
SOR system (Ge and Mooney, 2005), and the SILT
system (Kate et al, 2005) on the CLANG corpus,
respectively. It shows that SCSISSOR obtained
approximately 89% precision and 72.3% recall
while on the same corpus our best single SSVM
method 3 achieved a recall (74.3%) and lower pre-
cision (84.2%). The SILT system achieved ap-
proximately 83.9% precision and 51.3% recall 4
which is lower than the best single SSVM.
2We set N to 5 and K to 6 for the proposed SSVM.
3The parameter for SSVM is the polynomial(d=2)+(?s)
4Those figures for precision and recall described in
(Kate et al, 2005) showed approximately this precision and
recall of their method in this paper
Table 4: Experiment results with CLANG corpus. Each
SSVM ensemble consists of 10 individual SSVM. SSVM
bagging and SSVM boosting used the voting method. P-
SSVM boosting and P-SSVM bagging used the switching
method (BT) and voting method (VT).
System Methods Precision Recall
1 SSVM 84.2% 74.3%
1 SCSISSOR 89.0% 72.3%
1 SILT 83.9% 51.3%
10 SSVM Bagging 85.7% 72.4%
10 SSVM Boosting 85.7% 72.4%
10 P-SSVM Boosting(BT) 88.4% 79.3%
10 P-SSVM Bagging(BT) 86.5% 79.3%
10 P-SSVM Boosting(VT) 86.5% 75.8%
10 P-SSVM Bagging(VT) 84.6% 75.8%
Table 4 also shows the performance of Bagging,
Boosting, and the proposed SSVM ensemble mod-
els with bagging and boosting models. It is impor-
tant to note that the switching model using a boost-
ing tree method (BT) to learn the outputs of indi-
vidual SSVMs within the SSVM ensemble model.
It clearly indicates that our proposed ensem-
ble method can enhance the performance of the
SSVM model and the proposed methods are more
effective than the conventional ensemble method
for SSVM. This was because the output of each
SSVM is complex (i.e a logical form) so it is not
sure that the voting method can select a corrected
output. In other words, the boosting tree algo-
rithms can utilize subtrees mined from the corpus
to estimate the good weight values for subtrees,
and then combines them to determine whether or
not a tree is selected. In our opinion, with the
boosting tree algorithm we can have a chance to
obtain more accurate outputs. These results in Ta-
ble 4 effectively support for this evidence.
Moreover, Table 4 depicts that the proposed en-
semble method using different parameters for ei-
ther bagging and boosting models can effectively
improve the performance of bagging and boost-
ing in term of precision and recall. This was be-
cause the accuracy of each individual parser in the
model with different parameters is better than each
one in either the boosting or the bagging model.
In addition, when performing SSVM on the test
set, we might obtain some ?NULL? outputs since
the grammar generated by SSVM could not de-
rive this sentence. Forming a number of individual
SSVMs to an ensemble model is the way to handle
this case, but it could make the numbers of com-
pleted outputs and corrected outputs increase. Ta-
625
ble 4 indicates that the proposed SSVM ensemble
model obtained 88.4% precision and 79.3% recall.
Therefore it shows substantially a better F1 score
in comparison with previous work on the CLANG
corpus.
Summarily, our method achieved the best re-
call result and a high precision on CLANG corpus.
The proposed ensemble models outperformed the
original SSVM on CLANG corpus and its perfor-
mances also is better than that of the best pub-
lished result.
5 Conclusions
This paper presents a structured support vector
machine ensemble model for semantic parsing
problem by employing it on the corpus of sen-
tences and their representation in logical form.
We also draw a novel SSVM ensemble model in
which the forming ensemble strategy is based on a
selection method on various parameters of SSVM,
and the aggregation method is based on a switch-
ing model using subtrees mined from the outputs
of a SSVM ensemble model.
Experimental results show substantially that the
proposed ensemble model is better than the con-
ventional ensemble models for SSVM. It can also
effectively improve the performance in term of
precision and recall in comparison with previous
works.
Acknowledgments
The work on this paper was supported by a Mon-
bukagakusho 21st COE Program.
References
J. Allen. 1995. Natural Language Understand-
ing (2nd Edition). Mento Park, CA: Benjam-
ing/Cumming.
L. Breiman. 1996. Bagging predictors. Machine
Learning 24, 123-140.
T.G. Dietterich. 2000. An experimental compari-
son of three methods for constructing ensembles
of decision trees: Bagging, boosting, and ran-
domization. Machine Learning 40 (2) 139-158.
M. Johnson 1999. PCFG models of linguistic tree
representation. Computational Linguistics.
R. Ge and R.J. Mooney. 2005. A Statistical Se-
mantic Parser that Integrates Syntax and Seman-
ics. In proceedings of CONLL 2005.
J.C. Henderson and E. Brill 2000. Bagging and
Boosting a Treebank Parser. In proceedings
ANLP 2000: 34-41
R.J. Kate et al 2005. Learning to Transform Nat-
ural to Formal Languages. Proceedings of AAAI
2005, page 825-830.
T. Kudo, Y. Matsumoto. A Boosting Algorithm
for Classification of Semi-Structured Text. In
proceeding EMNLP 2004.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proc. of ICML 2001.
D.C. Manning and H. Schutze. 1999. Founda-
tion of Statistical Natural Language Processing.
Cambridge, MA: MIT Press.
L.S. Zettlemoyer and M. Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Catego-
rial Grammars. In Proceedings of UAI, pages
825?830.
I. Tsochantaridis, T. Hofmann, T. Joachims, and
Y. Altun. 2004. Support Vector Machine Learn-
ing for Interdependent and Structured Output
Spaces. In proceedings ICML 2004.
V. Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer, N.Y., 1995.
L.R. Tang. 2003. Integrating Top-down and
Bottom-up Approaches in Inductive Logic Pro-
gramming: Applications in Natural Language
Processing and Relation Data Mining. Ph.D.
Dissertation, University of Texas, Austin, TX,
2003.
B. Taskar, D. Klein, M. Collins, D. Koller, and
C.D. Manning. 2004. Max-Margin Parsing. In
proceedings of EMNLP, 2004.
R.E. Schapire. 1999. A brief introduction to boost-
ing. Proceedings of IJCAI 99
M.J. Zaki. 2002. Efficiently Mining Frequent
Trees in a Forest. In proceedings 8th ACM
SIGKDD 2002.
J.M. Zelle and R.J. Mooney. 1996. Learning
to parse database queries using inductive logic
programming. In Proceedings AAAI-96, 1050-
1055.
626
A Sentence Reduction Using Syntax Control
Nguyen Minh Le
The Graduate School of
Information Science JAIST
Ishikawa, 923-1292, Japan
nguyenml@jaist.ac.jp
Susumu Horiguchi
The Graduate School of
Information Science JAIST
Ishikawa, 923-1292, Japan
hori@jaist.ac.jp
Abstract
This paper present a method based on the
behavior of nonnative speaker for reduc-
tion sentence in foreign language. We
demonstrate an algorithm using seman-
tic information in order to produce two
reduced sentences in two difference lan-
guages and ensure both grammatical and
sentence meaning of the original sentence
in reduced sentences. In addition, the or-
ders of reduced sentences are able to be
different from original sentences.
1 Introduction
Most of the researches in automatic summarization
were focused on extraction or identifying the im-
portant clauses and sentences, paragraphs in texts
(Inderject Mani and Mark Maybury, 1999). How-
ever, when humans produce summaries of docu-
ments, they used to create new sentences that are
grammatical, that cohere with one another, and cap-
ture the most salient parts of information in the orig-
inal document. Sentence reduction is the problem to
remove some redundant words or some phrases from
the original sentence by creating a new sentence in
which the gist meaning of the original sentence was
unchanged.
Methods of sentence reduction have been used
in many applications. Grefenstette (G.Grefenstette,
1998) proposed removing phrases in sentences to
produce a telegraphic text that can be used to pro-
vide audio scanning services for the blind. Dolan
(S.H. Olivers and W.B.Dolan, 1999) proposed re-
moving clauses in sentences before indexing docu-
ment for information retrieval. Those methods re-
move phrases based on their syntactic categories but
not rely on the context of words, phrases and sen-
tences around. Without using that information can
be reduced the accuracy of sentence reduction prob-
lem. Mani and Maybury also present a process of
writing a reduced sentence by reversing the original
sentence with a set of revised rules to improve the
performance of summarization. (Inderject Mani and
Mark Maybury, 1999).
Jing and McKeown(H. Jing, 2000) studied a new
method to remove extraneous phrase from sentences
by using multiple source of knowledge to decide
which phrase in the sentences can be removed. The
multiple sources include syntactic knowledge, con-
text information and statistic computed from a cor-
pus that consists of examples written by human pro-
fessional. Their method prevented removing some
phrases that were relative to its context around and
produced a grammatical sentence.
Recently, Knight and Marcu(K.Knight and
D.Marcu, 2002) demonstrated two methods for sen-
tence compression problem, which are similar to
sentence reduction one. They devised both noisy-
channel and decision tree approach to the prob-
lem. The noisy-channel framework has been used
in many applications, including speech recognition,
machine translation, and information retrieval. The
decision tree approach has been used in parsing sen-
tence. (D. Magerman, 1995)(Ulf Hermijakob and
J.Mooney, 1997) to define the rhetorical of text doc-
uments (Daniel Marcu, 1999).
Most of the previous methods only produce a
short sentence whose word order is the same as that
of the original sentence, and in the same language,
e.g., English.
When nonnative speaker reduce a long sentence
in foreign language, they usually try to link the
meaning of words within the original sentence into
meanings in their language. In addition, in some
cases, the reduced sentence and the original sen-
tence had their word order are difference. Therefore,
two reduced sentences are performed by non-native
speaker, one is the reduced sentence in foreign lan-
guage and another is in their language.
Following the behavior of nonnative speaker, two
new requirements have been arisen for sentence re-
duction problem as follows:
1) The word order of the reduced sentence may dif-
ferent from the original sentence.
2) Two reduced sentences in two difference lan-
guages can be generated.
With the two new perspectives above, sentence re-
duction task are useful for many applications such
as: information retrieval, query text summarization
and especially cross-language information retrieval.
To satisfy these new requirements, we proposed a
new algorithm using semantic information to simu-
late the behavior of nonnative-speaker. The seman-
tic information obtained from the original sentence
will be integrated into the syntax tree through syntax
control. The remainder of this paper will be orga-
nized as follows: Section 2 demonstrated a method
using syntactic control to reduced sentences. Sec-
tion 3 shows implementation and experiments. Sec-
tion 4 gives some conclusions and remained prob-
lems to be solved in future.
2 Sentence reduction using syntax control
2.1 Formulation
Let E and V be two difference languages. Given a
long sentence e : e1, e2, ..., en in the language E.
The task of sentence reduction into two languages
E and V is to remove or replace some redundant
words in the sentence e to generate two new sen-
tences e?1, e?2, ..., e?m and v1, v2, ..., vk in language E
and V so that their gist meanings are unchanged.
In practice, we used English language as a source
language and the target language are in English and
Vietnamese. However, the reader should understand
that our method can apply for any pair of languages.
In the following part we present an algorithm of sen-
tence reduction using syntax control with rich se-
mantic information.
2.2 Sentence reduction algorithm
We present an algorithm based on a semantic parsing
in order to generate two short sentences into differ-
ence languages. There are three steps in a reduction
algorithm using syntax control. In the first step, the
input sentence e will be parsed into a syntax tree t
through a syntax parser.
In the second step, the syntax tree will be added
rich semantic information by using a semantic
parser, in which each node of the syntax tree is asso-
ciated with a specific syntax control. The final step is
a process of generating two deference sentences into
language E and V language from the syntax tree t
that has been annotated with rich semantic informa-
tion.
2.2.1 Syntax parsing
First, We parse a sentence into a syntax tree. Our
syntax parser locates the subject, object, and head
word within a sentence. It also recognizes phrase
verbs, cue phases or expressions in English sen-
tences. These are useful information to reduce sen-
tence. The Figure 2 explains the equivalent of our
grammar symbol with English grammar symbol.
Figure 1 shows an example of our syntax pars-
ing for the sentence ?Like FaceLift, much of ATM?s
screen performance depends on the underlying ap-
plication?.
To reduce the ambiguity, we design a syntactic pars-
ing base on grammar symbols, which classified in
detail. Part of speech of words was extended to cope
with the ambiguity problem. For example, in Figure
2, ?noun? was dived into ?private noun? and ?gen-
eral noun?.
The bilingual dictionary was built including about
200,000 words in English and its meaning in Viet-
namese. Each English word entry includes several
meanings in Vietnamese and each meaning was as-
sociated with a symbol meaning. The set of sym-
bol meanings in each word entry is defined by using
WordNet database.(C. Fellbaum, 1998) The dictio-
nary also contained several phrases, expressions in
Figure 1: An example of syntax tree of ?Like
FaceLift, much of ATM?s screen performance de-
pends on the underlying application?
English and its equivalent to Vietnamese.
2.2.2 Semantic parsing using syntax control
After producing a syntax tree with rich informa-
tion, we continue to apply a semantic parsing for that
syntax tree.
Let N be an internal node of the syntax tree t and N
has k children nodes: n1, n2, ...nk .
The node N based on semantic information from
its n children nodes to consider what the remained
part in the reducing sentence should be.
When parsing semantic for the syntax tree t, each
N must be used the information of children nodes
to define its information. We call that information is
semantic-information of the node N and define it as
N.sem . In addition, each semantic-information of
a given node N was mapped with a meaning in the
Figure 2: Example of symbol Equivalent
target language.
For convince, we define SI is a set of semantic-
information and assume that the jth semantic-
information of the node nj is nj [i].
To understand what the meaning of the node N
should be, we have to know the meaning of each
children node and know how to combine them into
meanings for the node N .
Figure 3: Syntax control
Figure 3 shows two choices for sequence mean-
ings of the node N in a reduction process .
It is easy for human to understand exactly which
meaning of ni should be and then decoding them as
objects to memorize. With this basic idea, we design
a control language to do this task.
The k children nodes n1, n2, ...nk are associated
with a set of a syntax control to conduct the reducing
sentence process. The node N and its children are
associated with a set of rules. To present the set of
rules we used a simple syntax of a control language
as follows:
1) Syntax to present the order of children nodes and
nodes to be removed.
2) Syntax to constraint each meaning of a children
node with meanings of other children nodes.
3) Syntax to combine sequence meanings into
one symbol meaning (this process called a inherit
process from the node N to its children).
A syntax rule control will be encoded as one-
generation rules and a set of condition rules so that
the generation rule has to satisfy. With a specifica-
tion condition rule, we can define its generation rule
directly.
Condition rule
A condition rule is formulated as follows: if
nj1 .sem = v1 ? nj2 .sem = v2... ? njm .sem = vm
then N.sem = v with v and vj ? SI
Generation rule
A generation rule is a sequence of symbols in order
to transfer the internal node N into the internal node
of a reduced sentence. We used two generation
rules, one for E and other one for V . Given a
sequence symbols g : g1g2...gm , in which gi is an
integer or a string. The equation gi = j means the
children node be remained at position j in the target
node. If gi = ?v1v2...vl?, we have that string will in
the children node ni of the target node.
Figure 1 shows a syntax tree of the input sentence:
?Much of ATM?s performance depends on the un-
derlying application.?. In this syntax tree, the syntax
rule:?S1=Bng-daucau Subj cdgt Bng-cuoicau? will
be used the syntax control bellow to reduce
< Con > default < /Con >
< Gen > 1 2 < /Gen >
The condition rule is ?default? mean the generation
rule is applied to any condition rule. The generation
rule be ?1 2? mean only the node (Subj) in the
index 1 and the node (cdgt) in the index 2 of the
rule ?S1=Bng-daucau Subj cdgt Bng-cuoicau? are
remained in the reduced sentence.
If the syntax control is changed to
< Con > Subj = HUMAN < /Con >
< Gen > 1 2 < /Gen >
This condition rule means that only the case the
semantic information in the children node ?Subj?
is ?HUMAN? the generation rule ?1 2? is applied
for reduction process. Using the default condition
rule the reduced sentences to be generated as
follows.
Original sentence: Like FaceLift, much of ATM?s
screen performance depends on the underlying
application.
Reduced sentence in English: Much of ATM?s per-
formance depends on the underlying application.
Reduced sentence in Vietnamese: Nhieu hieu suat
cua ATM phu thuoc vao nhung ung dung tiem an.
In order to generating reduced sentence in Viet-
namese language, the condition rule and generation
is also designed. This process is used the same way
as transfer translation method.
Because the gist meaning of a short sentence is un-
changed in comparing with the original sentence, the
gist meaning of a node after applying the syntax con-
trol will be unchanged. With this assumption, we
can reuse the syntax control for translating the origi-
nal sentence into other languages (English into Viet-
namese) for translating the reduced sentence. There-
fore, our sentence reduction program can produce
two reduced sentences in two difference languages.
Our semantic parsing used that set of rules to select
suitable rules for the current context. The problem
of selecting a set of suitable rules for the current con-
text of the current node N is to find the most likely
condition rule among the set of syntax control rules
that associated with it. Thus, semantic parsing using
syntax control problem can be described mathemat-
ically as follows:
Given a sequence of children nodes n1, n2, ..., nk
of a node N , each node ni consist of a list of mean-
ing, in which each meaning was associated with a
symbol meaning. The syntax rule for the node N
was associated with a set of condition rules. In ad-
dition, one condition rule is mapped with a specifi-
cation generation rule.
Find the most condition rules for that node se-
quences.
This problem can be solved by using a variant of the
Viterbi algorithm (A.J. Viterbi, 1967).
Firstly, we define each semantic-information of a
children node with all index condition rules. Sec-
ondly, we try to find all sequences that come from
the same condition rules.
Algorithm 1 A definition of condition rules algo-
rithm. FindRule(N )
Require: Input: N is a node
Ensure: A syntax control for a rule
{Initialization step:}
1: for i = 1 to k do
2: for j = 1 to Ki do
3: Set stack s[i]=all index rules in the set of
condition rules satisfy ni.sem = ni[j]
4: end for
5: for i = 1 to K1 do
6: Cost[0][i] = 1;
7: Back[0][i] = 0;
8: end for
9: end for
{Interaction step:}
10: for i = 1 to k do
11: for j=1 to Ki do
12: Cost[i][j] = maxCost[i ? 1][l] ?
V alue(s[i][j], s[i? 1][l]) with l = 1,Ki
Back[i][j]= all the index gave max
13: end for
14: end for
{Identification step:}
15: Set a list LS= all index rules gave max values
Cost[k][j] with j = 1,Kk.
16: Update all semantic-information of each condi-
tion rule in the list LS to node N .
17: Function Value (i, j)
Begin
If i==j return 2;
Else return 1;
End
After defining a set of semantic-information for
each internal node, we have a frame of semantic
parsing algorithm as shown in Algorithm 2. Our se-
mantic parsing using syntax control is fast because
of finding syntax control rule for each node tree is
applied dynamic programming.
Algorithm 2 Semantic parsing algorithm
Require: Given a syntax tree , a set of syntax con-
trol for each node of the syntax tree.
Ensure: a syntax tree with rich semantic informa-
tion
{SemanticParsingTree}
1: if N is leaf then
2: Update all symbol-meaning in word entry
3: else
4: FindRules(N);
5: end if{main procedure}
6: SemanticParsingNode(root);
2.2.3 Generation reduced sentences
The input of this process is a syntax tree which
associated with rich information after applying the
semantic parsing process. Browsing the syntax tree
following bottom-up process, in which, a node tree
can be generated a short sub-sentence by using the
corresponding generation rule. Because we have
two generation rules for each node tree, so we have
two reduced sentences in two difference languages.
3 Experiments and Discussion
3.1 Experiment Data
We used the same corpus(K.Knight and D.Marcu,
2002) with 1067 pair of sentence and its reduction.
We manually changed the order of some reduced
sentences in that corpus while keep their meaning.
We manually build a set of syntax control for that
corpus for our reduction algorithm using syntax con-
trol. The set of semantic symbols was described
such as, HUMAN, ANIMAL, THINGS, etc. We
make 100 pair of sentences with the order of a reduc-
tion sentence is different from its original sentence.
Afterward, those sentences are to be combined with
the corpus above in order to confirm that our method
can deal with the changeable word order problem.
3.2 Experiment Method
To evaluate our reduction algorithms, we randomly
selected 32 pair of sentences from our parallel
corpus, which will refer to as the Test corpus.
We used 1035 sentence pairs for training with
the reduction based decision tree algorithm. We
used test corpus to confirm that our methods us-
ing semantic-information will outperform than the
decision tree method without semantic-information
(K.Knight and D.Marcu, 2002). We presented each
original sentence in the test corpus to three judges
who are Vietnamese and specialize in English, to-
gether with three sentence reductions of it: The hu-
man generated reduction sentence, the outputs of the
sentence reduction based syntax control and the out-
put of the baseline algorithm. The judges were told
that all outputs were generated automatically. The
order of the outputs was scrambled randomly across
test cases. The judges participated in two experi-
ments. In the first experiment, they were asked to
determine on a scale from 1 to 10 how well the sys-
tems did with respect to selecting the most important
words in the original sentence. In the second exper-
iment, they were asked to determine on a scale from
1 to 10 how grammatical the outputs were. The out-
puts of our methods include both reduced sentences
in English and Vietnamese. In the third experiment,
we tested on the randomly of 32 sentences from 100
sentences whose had word order between input and
output are different.
3.3 Experiment Results
Using the first and the second experiment method,
we had two table results as follows.
Table 1: Experiment results with outputs in English
Method comp Grammatically Importance
Baseline 57.19 8.6? 2.8 7.18? 1.92
Syn.con 6.5 8.7? 1.2 7.3? 1.6
Human 53.33 9.05? 0.3 8.5? 0.8
Table 2: Experiment results with outputs in Viet-
namese
Method comp Grammatically Importance
Baseline x x x
Syn.con 67 6.5? 1.7 6? 1.3
Human 63 8.5? 0.3 8.7? 0.7
Using the third experiments method we achieved
a result to be shown in Table5.
Table 3: Experiment results with the changeable or-
der
Method comp Grammatically Importance
Baseline 56.2 7.4? 3.1 6.5? 1.3
Syn.con 66 8.4? 2.1 7.2? 1.7
Human 53.33 9.2? 0.3 8.5? 0.8
3.4 Discussion
Table 1 shows the compression of three reduc-
tion methods in comparing with human for English
language. The grammatically of semantic control
achieved a high results because we used the syntax
control from human expert. The sentence reduction
decision based is yielded a smallest result. We sus-
pect that the requirement of word order may affect
the grammatically. Table 1 and Table 3 also indi-
cates that our new method achieved the importance
of words are outperform than the baseline algorithm
due to semantic information. This was because our
method using semantic information to avoid deleting
important words. Following our point, the base line
method should integrate with semantic information
within the original sentence to enhance the accuracy.
Table 2 shows the outputs of our method into Viet-
namese language, the baseline method cannot gener-
ate the output into Vietnamese language. The syntax
control method achieved a good enough results in
both grammatically and importance aspects.
The comparison row in the Table 1 and the Table 2
also reported that the baseline yields a shorter output
than syntax control method.
Table 3 shows that when we selected randomly 32
sentence pairs from 100 pairs of sentences those had
words order between input and output are different,
we have the syntax method change a bit while the
baseline method achieved a low result. This is due
to the syntax control method using rule knowledge
based while the baseline was not able to learn with
that corpus that.
4 Conclusions
We have presented an algorithm that allows rewrit-
ing a long sentence into two reduced sentences in
two difference languages. We compared our meth-
ods with the other methods to show advantages as
well as limits of the method. We claim that the se-
mantic information of the original sentence through
using syntax control is very useful for sentence re-
duction problem.
We proposed a method for sentence reduction
using semantic information and syntactic parsing
so-called syntax control approach. Our method
achieved a higher accuracy and the outputted reduc-
tion sentences in two different languages e.g. En-
glish and Vietnamese. Thus, it is closed to the out-
puts of non-native speaker in reduction manner.
Investigate machine learning to generate syntax
control rules automatically from corpus available are
promising to enhance the accuracy of sentence re-
duction using syntax control .
Acknowledgements
We would like to thank to Dr. Daniel Marcus about
the data corpus for sentence reduction task. This re-
search was supported in part by the international re-
search project grant, JAIST.
References
Indeject Mani and Mark Maybury. 1999. Advances in
Automatic Text Summarization. The MIT press.
G.Grefenstette. 1998. Producing intelligent telegraphic
text reduction to provide an audio scanning service for
the blind. In Working notes of the AAAI Spring Sym-
posium on Intelligent Text summarization, pp.111-118.
S.H.Olivers and W.B.Dolan. 1999. Less is more; elimi-
nating index terms from subordinate clauses. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistic, pp.349-356.
H.Jing. 2000. Sentence reduction for automatic text
summarization. In Proceeding of the First Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics NAACL-2000.
Kevin Knight and Daniel Marcu. 2002. Summarization
beyond sentence extraction: A Probabilistic approach
to sentence compression. Artificial Intelligent, 139:
91?107.
D. Magerman. 1995. Statistical decision tree models for
parsing. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistic, pp.276-
283.
Ulf Hermijakob and Raymond J. Mooney. 1997. Learn-
ing parse and translation decision from examples with
rich context. In Proceeding of ACL/EACL?97, pp 482-
489.
Daniel Marcu. 1999. A decision- based approach to
Rhetorical parsing. In Proc. Of ACL?99, pp.365-372.
C.Fellbaum. 1998. WORDNET: An Electronic Lexical
Database. The Mit Press.
A.J.Viterbi. 1967. Error bounds for convolution codes
and an asymptotically optimal decoding algorithm.
IEEE Trans on Information Theory,13: 260?269.
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 9?16,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
An Empirical Study of Vietnamese Noun Phrase Chunking with
Discriminative Sequence Models
Le Minh Nguyen
School of Information Science, JAIST
nguyenml@jaist.ac.jp
Huong Thao Nguyen and Phuong Thai Nguyen
College of Technology, VNU
{thaonth, thainp}@vnu.edu.vn
Tu Bao Ho and Akira Shimazu
Japan Advanced Institute of Science and Technology
{bao,shimazu}@jaist.ac.jp
Abstract
This paper presents an empirical work
for Vietnamese NP chunking task. We
show how to build an annotation corpus of
NP chunking and how discriminative se-
quence models are trained using the cor-
pus. Experiment results using 5 fold cross
validation test show that discriminative se-
quence learning are well suitable for Viet-
namese chunking. In addition, by em-
pirical experiments we show that the part
of speech information contribute signifi-
cantly to the performance of there learning
models.
1 Introduction
Many Natural Language Processing applications
(i.e machine translation) require syntactic infor-
mation and tools for syntactic analysis. However,
these linguistic resources are only available for
some languages(i.e English, Japanese, Chines). In
the case of Vietnamese, currently most researchers
have focused on word segmentation and part of
speech tagging. For example, Nghiem et al
(Nghiem, Dinh, Nguyen, 2008) has developed a
Vietnamese POS tagging. Tu (Tu, Phan, Nguyen,
Ha, 2006) (Nguyen, Romary, Rossignol, Vu,
2006)(Dien, Thuy, 2006) have developed Viet-
namese word segmentation.
The processing of building tools and annotated
data for other fundamental tasks such as chunk-
ing and syntactic parsing are currently developed.
This can be viewed as a bottleneck for develop-
ing NLP applications that require a deeper under-
standing of the language. The requirement of de-
veloping such tools motives us to develop a Viet-
namese chunking tool. For this goal, we have
been looking for an annotation corpus for conduct-
ing a Vietnamese chunking using machine learn-
ing methods. Unfortunately, at the moment, there
is still no common standard annotated corpus for
evaluation and comparison regarding Vietnamese
chunking.
In this paper, we aim at discussing on how
we can build annotated data for Vietnamese text
chunking and how to apply discriminative se-
quence learning for Vietnamese text chunking. We
choose discriminative sequence models for Viet-
namese text chunking because they have shown
very suitable methods for several languages(i.e
English, Japanese, Chinese) (Sha and Pereira,
2005)(Chen, Zhang, and Ishihara, 2006) (Kudo
and Matsumoto, 2001). These presentative dis-
criminative models which we choose for conduct-
ing empirical experiments including: Conditional
Random Fields (Lafferty, McCallum, and Pereira,
2001), Support Vector Machine (Vapnik, 1995)
and Online Prediction (Crammer et al 2006). In
other words, because Noun Phrase chunks appear
most frequently in sentences. So, in this paper
we focus mainly on empirical experiments for the
tasks of Vietnamese NP chunking.
We plan to answer several major questions by
using empirical experiments as follows.
? Whether or not the discriminative learning
models are suitable for Vietnamese chunking
problem?
? We want to know the difference of SVM,
Online Learning, and Conditional Random
Fields for Vietnamese chunking task.
? Which features are suitable for discriminative
learning models and how they contribute to
the performance of Vietnamese text chunk-
ing?
The rest of this paper is organized as follows:
Section 2 describes Vietnamese text chunking with
discriminative sequence learning models. Section
3 shows experimental results and Section 4 dis-
9
cusses the advantage of our method and describes
future work.
2 Vietnamese NP Chunking with
Discriminative Sequence Learning
Noun Phrase chunking is considered as the task
of grouping a consecutive sequence of words into
a NP chunk lablel. For example: ?[NP Anh Ay
(He)] [VP thich(likes)] [NP mot chiec oto(a car)]
?
Before describing NP chunking tasks, we
summarize the characteristic of Vietnamese lan-
guage and the background of Conditional Ran-
dom Fields, Support Vector Machine, and Online
Learning. Then, we present how to build the an-
notated corpus for the NP chunking task.
2.1 The characteristic of Vietnamese Words
Vietnamese syllables are elementary units that
have one way of pronunciation. In documents,
they are usually delimited by white-space. Be-
ing the elementary units, Vietnamese syllables are
not undivided elements but a structure. Generally,
each Vietnamese syllable has all five parts: first
consonant, secondary vowel, main vowel, last con-
sonant and a tone mark. For instance, the sylla-
ble tu.n (week) has a tone mark (grave accent), a
first consonant (t), a secondary vowel (u), a main
vowel () and a last consonant (n). However, except
for main vowel that is required for all syllables,
the other parts may be not present in some cases.
For example, the syllable anh (brother) has no tone
mark, no secondary vowel and no first consonant.
In other case, the syllable hoa (flower) has a sec-
ondary vowel (o) but no last consonant.
Words in Vietnamese are made of one or more
syllables which are combined in different ways.
Based on the way of constructing words from syl-
lables, we can classify them into three categories:
single words, complex words and reduplicative
words (Mai,Vu, Hoang, 1997).
The past of speechs (Pos) of each word in Viet-
namese are mainly sketched as follows.
A Noun Phrase (NP) in Vietnamese consists of
three main parts as follows: the noun center, the
prefix part, and the post fix part. The prefix and
postfix are used to support the meaning of the NP.
For example in the NP ?ba sinh vien nay?, the
noun center is ?sinh vien?, and the prefix is ?ba
(three)?, the postfix is ?nay?.
Vietnamese Tag Equivalent to English Tag
CC Coordinating conjunction)
CD Cardinal number)
DT Determiner)
V Verb
P Preposition
A Adjective
LS List item marker
MD Modal
N Noun
Table 1: Part of Speeches in Vietnamese
2.2 The Corpus
We have collected more than 9,000 sentences from
several web-sites through the internet. After that,
we then applied the segmentation tool (Tu, Phan,
Nguyen, Ha, 2006) to segment each sentences
into a sequence of tokens. Each sequence of
tokens are then represented using the format of
CONLL 2000. The details are sketched as follows.
Each line in the annotated data consists of
three columns: the token (a word or a punc-
tuation mark), the part-of-speech tag of the to-
ken, and the phrase type label (label for short)
of the token. The label of each token indicates
whether the token is outside a phrase (O), starts
a phrase (B-?PhraseType?), or continues a phrase
(I-?PhraseType?).
In order to save time for building annotated
data, we made a set of simple rules for automat-
ically generating the chunking data as follows. If
a word is not a ?noun?, ?adjective?, or ?article? it
should be assigned the label ?O?. The consecu-
tive words are NP if they is one of type as follows:
?noun noun?; ?article noun?, ?article noun adjec-
tive?. After generating such as data, we ask an
expert about Vietnamese linguistic to correct the
data. Finally, we got more than 9,000 sentences
which are annotated with NP chunking labels.
Figure 1 shows an example of the Vietnamese
chunking corpus.
2.3 Discriminative Sequence Learning
In this section, we briefly introduce three dis-
criminative sequence learning models for chunk-
ing problems.
2.3.1 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty,
McCallum, and Pereira, 2001) are undirected
graphical models used to calculate the conditional
10
Figure 1: An Example of the Vietnamese chunk-
ing corpus
probability of values on designated output nodes,
given values assigned to other designated input
nodes for data sequences. CRFs make a first-order
Markov independence assumption among output
nodes, and thus correspond to finite state machine
(FSMs).
Let o = (o1, o2, . . . , oT ) be some observed in-
put data sequence, such as a sequence of words in
a text (values on T input nodes of the graphical
model). Let S be a finite set of FSM states, each is
associated with a label l such as a clause start po-
sition. Let s = (s1, s2, . . . , sT ) be some sequences
of states (values on T output nodes). CRFs de-
fine the conditional probability of a state sequence
given an input sequence to be
P?(s|o) = 1Zo exp
( T?
t=1
F (s, o, t)
)
(1)
where Zo =
?
s exp
(?T
t=1 F (s, o, t)
)
is a nor-
malization factor over all state sequences. We de-
note ? to be the Kronecker-?. Let F (s, o, t) be the
sum of CRFs features at time position t:
?
i
?ifi(st?1, st, t) +
?
j
?jgj(o, st, t) (2)
where fi(st?1, st, t) = ?(st?1, l?)?(st, l) is a
transition feature function which represents se-
quential dependencies by combining the label l?
of the previous state st?1 and the label l of the
current state st, such as the previous label l? =
AV (adverb) and the current label l = JJ (adjec-
tive). gj(o, st, t) = ?(st, l)xk(o, t) is a per-state
feature function which combines the label l of cur-
rent state st and a context predicate, i.e., the binary
function xk(o, t) that captures a particular prop-
erty of the observation sequence o at time position
t. For instance, the current label is JJ and the cur-
rent word is ?conditional?.
Training CRFs is commonly performed by max-
imizing the likelihood function with respect to
the training data using advanced convex optimiza-
tion techniques like L-BFGS. Recently, there are
several works apply Stochastic Gradient Descent
(SGD) for training CRFs models. SGD has been
historically associated with back-propagation al-
gorithms in multilayer neural networks.
And inference in CRFs, i.e., searching the most
likely output label sequence of an input observa-
tion sequence, can be done using Viterbi algo-
rithm.
2.3.2 Support Vector Machines
Support vector machine (SVM)(Vapnik, 1995)
is a technique of machine learning based on sta-
tistical learning theory. The main idea behind
this method can be summarized as follows. Sup-
pose that we are given l training examples (xi, yi),
(1 ? i ? l), where xi is a feature vector in n di-
mensional feature space, and yi is the class label
{-1, +1 } of xi.
SVM finds a hyperplane w.x+b = 0 which cor-
rectly separates training examples and has maxi-
mum margin which is the distance between two
hyperplanes w ? x + b ? 1 and w ? x + b ? ?1.
Finally, the optimal hyperplane is formulated as
follows:
f(x) = sign
( l?
1
?iyiK(xi, x) + b
)
(3)
where ?i is the Lagrange multiple, and K(x?, x??)
is called a kernel function, which calculates sim-
ilarity between two arguments x? and x??. For in-
stance, the Polynomial kernel function is formu-
lated as follows:
K(x?, x??) = (x? ? x??)p (4)
SVMs estimate the label of an unknown example
x whether the sign of f(x) is positive or not.
Basically, SVMs are binary classifier, thus we
must extend SVMs to multi-class classifier in or-
11
der to classify three or more classes. The pair-
wise classifier is one of the most popular meth-
ods to extend the binary classification task to that
of K classes. Though, we leave the details to
(Kudo and Matsumoto, 2001), the idea of pairwise
classification is to build K.(K-1)/2 classifiers con-
sidering all pairs of classes, and final decision is
given by their weighted voting. The implementa-
tion of Vietnamese text chunking is based on Yam-
cha (V0.33)1.
2.3.3 Online Passive-Aggressive Learning
Online Passive-Aggressive Learning (PA) was
proposed by Crammer (Crammer et al 2006) as
an alternative learning algorithm to the maximize
margin algorithm. The Perceptron style for nat-
ural language processing problems as initially pro-
posed by (Collins, 2002) can provide to state of
the art results on various domains including text
segmentation, syntactic parsing, and dependency
parsing. The main drawback of the Perceptron
style algorithm is that it does not have a mech-
anism for attaining the maximize margin of the
training data. It may be difficult to obtain high
accuracy in dealing with hard learning data. The
online algorithm for chunking parsing in which
we can attain the maximize margin of the training
data without using an optimization technique. It
is thus much faster and easier to implement. The
details of PA algorithm for chunking parsing are
presented as follows.
Assume that we are given a set of sentences
xi and their chunks yi where i = 1, ..., n. Let
the feature mapping between a sentence x and
a sequence of chunk labels y be: ?(x, y) =
?1(x, y),?2(x, y), ...,?d(x, y) where each fea-
ture mapping ?j maps (x, y) to a real value. We
assume that each feature ?(x, y) is associated with
a weight value. The goal of PA learning for chunk-
ing parsing is to obtain a parameter w that min-
imizes the hinge-loss function and the margin of
learning data.
Algorithm 1 shows briefly the Online Learning
for chunking problem. The detail about this al-
gorithm can be referred to the work of (Crammer
et al 2006). In Line 7, the argmax value is com-
puted by using the Viterbi algorithm which is sim-
ilar to the one described in (Collins, 2002). Algo-
rithm 1 is terminated after T round.
1Yamcha is available at
http://chasen.org/ taku/software/yamcha/
Input: S = (xi; yi), i = 1, 2, ..., n in which1
xi is the sentence and yi is a sequence of
chunks
Aggressive parameter C2
Output: the model3
Initialize: w1 = (0, 0, ..., 0)4
for t=1, 2... do5
Receive an sentence xt6
Predict y?t = argmaxy?Y (wt.?(xt, yt))7
Suffer loss: lt =
wt.?(xt, y?t )? wt.?(xt, yt) +
??(yt, y?t )
Set:?t = lt||?(xt,y?t )??(xt,yt)||28
Update:9
wt+1 = wt + ?t(?(xt, yt)? ?(xt, y?t ))
end10
Algorithm 1: The Passive-Aggressive algo-
rithm for NP chunking.
2.3.4 Feature Set
Feature set is designed through features template
which is shown in Table 2. All edge features obey
the first-order Markov dependency that the label
(l) of the current state depends on the label (l?)
of the previous state (e.g., ?l = I-NP? and ?l? =
B-NP?). Each observation feature expresses how
much influence a statistic (x(o, i)) observed sur-
rounding the current position i has on the label
(l) of the current state. A statistic captures a par-
ticular property of the observation sequence. For
instance, the observation feature ?l = I-NP? and
?word?1 is the? indicates that the label of the cur-
rent state should be I-NP (i.e., continue a noun
phrase) if the previous word is the. Table 2 de-
scribes both edge and observation feature tem-
plates. Statistics for observation features are iden-
tities of words, POS tags surrounding the current
position, such as words and POS tags at ?2, ?1,
1, 2.
We also employ 2-order conjunctions of the cur-
rent word with the previous (w?1w0) or the next
word (w0w1), and 2-order and 3-order conjunc-
tions of two or three consecutive POS tags within
the current window to make use of the mutual de-
pendencies among singleton properties. With the
feature templates shown in Table 2 and the feature
rare threshold of 1 (i.e., only features with occur-
rence frequency larger than 1 are included into the
discriminative models)
12
Edge feature templates
Current state: si Previous state: si?1
l l?
Observation feature templates
Current state: si Statistic (or context predicate) templates: x(o, i)
l w?2; w?1; w0; w1; w2; w?1w0; w0w1;
t?2; t?1; t0; t1; t2;
t?2t?1; t?1t0; t0t1; t1t2; t?2t?1t0;
t?1t0t1; t0t1t2
Table 2: Feature templates for phrase chunking
3 Experimental Results
We evaluate the performance of using several se-
quence learning models for the Vietnamese NP
chunking problem. The data of more than 9,000
sentences is evaluated using an empirical experi-
ment with 5 fold cross validation test. It means
we used 1,800 and 7,200 sentences for testing
and training the discriminative sequence learning
models, respectively. Note that the evaluation
method is used the same as CONLL2000 did. We
used Precision, Recall, and F-Measure in which
Precision measures how many chunks found by
the algorithm are correct and the recall is per-
centage of chunks defined in the corpus that were
found by the chunking program.
Precision = #correct?chunk#numberofchunks
Recall = #correct?chunks#numerofchunksinthecorpus
F?measure =2? Precision? RecallPrecision + Recall
To compute the scores in our experiments, we
utilized the evaluation tool (conlleval.pl) which is
available in CONLL 2000 (Sang and Buchholz,
2000, ).
Figure 2 shows the precision scores of three
methods using 5 Folds cross validation test. It
reports that the CRF-LBFGS attain the highest
score. The SVMs and CRF-SGD are comparable
to CRF-LBFGS. The Online Learning achieved
the lowest score.
Figure 3 shows the recall scores of three CRFs-
LBFGS, CRFs-SGD, SVM, and Online Learning.
The results show that CRFs-SGD achieved the
highest score while the Online Learning obtained
the lowest score in comparison with others.
Figure 4 and Figure 5 show the F-measure and
accuracy scores using 5 Folds Cross-validation
Figure 2: Precision results in 5 Fold cross valida-
tion test
Test. Similar to these results of Precision and Re-
call, CRFs-LBFGS was superior to the other ones
while the Online Learning method obtained the
lowest result.
Table 3 shows the comparison of three discrim-
inative learning methods for Vietnamese Noun
Phrase chunking. We compared the three se-
quence learning methods including: CRFs using
the LBFGS method, CRFs with SGD, and On-
line Learning. Experiment results show that the
CRFs-LBFGS is the best in comparison with oth-
ers. However, the computational times when train-
ing the data is slower than either SGD or Online
Learning. The SGD is faster than CRF-LBFS ap-
proximately 6 times. The SVM model obtained a
comparable results with CRFs models and it was
superior to Online Learning. It yields results that
were 0.712% than Online Learning. However, the
SVM?s training process take slower than CRFs
and Online Learning. According to our empirical
investigation, it takes approximately slower than
CRF-SGF, CRF-LBFGS as well as Online Learn-
ing.
13
Figure 3: Recall result in 5 Fold cross validation
test
Figure 4: The F-measure results of 5 Folds Cross-
validation Test
Note that we used FlexCRFs (Phan, Nguyen,
Tu , 2005) for Conditional Random Fields us-
ing LBFGS, and for Stochastic Gradient Descent
(SGD) we used SGD1.3 which is developed by
Leon Bottou 2.
Methods Precision Recall F1
CRF-LBGS 80.85 81.034 80.86
CRF-SGD 80.74 80.66 80.58
Online-PA 80.034 80.13 79.89
SVM 80.412 80.982 80.638
Table 3: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, SVM, Online-PA)
In order to investigate which features are ma-
jor effect on the discriminative learning models for
Vietnamese Chunking problems, we conduct three
experiments as follows.
2http://leon.bottou.org/projects/sgd
Figure 5: The accuracy scores of four methods
with 5 Folds Cross-validation Test
? Cross validation test for three modes without
considering the edge features
? Cross validation test for three models without
using POS features
? Cross validation test for three models without
using lexical features
? Cross validation test for three models without
using ?edge features template? features
Note that the computational time of training
SVMs model is slow, so we skip considering fea-
ture selection for SVMs. We only consider feature
selection for CRFs and Online Learning.
Feature Set LBFGS SGD Online
Full-Features 80.86 80.58 79.89
Without-Edge 80.91 78.66 80.13
Without-Pos 62.264 62.626 59.572
Without-Lex 77.204 77.712 75.576
Table 4: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, Online-PA)
Table 4 shows that the Edge features have an
impact to the CRF-SGD model while it do not
affect to the performance of CRFs-LBFGS and
Online-PA learning. Table 4 also indicates that
the POS features are severed as important features
regarding to the performance of all discrimina-
tive sequence learning models. As we can see,
if one do not use POS features the F1-score of
each model is decreased more than 20%. We also
remark that the lexical features contribute an im-
portant role to the performance of Vietnamese text
14
Figure 6: F-measures of three methods with different feature set
chunking. If we do not use lexical features the
F1-score of each model is decreased till approxi-
mately 3%. In conclusion, the POS features signif-
icantly effect on the performance of the discrimi-
native sequence models. This is similar to the note
of (Chen, Zhang, and Ishihara, 2006).
Figure 6 reports the F-Measures of using dif-
ferent feature set for each discriminative models.
Note that WPos, WLex, and WEdge mean without
using Pos features, without using lexical features,
and without using edge features, respectively. As
we can see, the CRF-LBFGs always achieved the
best scores in comparison with the other ones and
the Online Learning achieved the lowest scores.
4 Conclusions
In this paper, we report an investigation of devel-
oping a Vietnamese Chunking tool. We have con-
structed an annotation corpus of more than 9,000
sentences and exploiting discriminative learning
models for the NP chunking task. Experimen-
tal results using 5 Folds cross-validation test have
showed that the discriminative models are well
suitable for Vietnamese phrase chunking. Con-
ditional random fields show a better performance
in comparison with other methods. The part of
speech features are known as the most influence
features regarding to the performances of discrim-
inative models on Vietnamese phrases chunking.
What our contribution is expected to be useful
for the development of Vietnamese Natural Lan-
guage Processing. Our results and corpus can be
severed as a very good baseline for Natural Lan-
guage Processing community to develop the Viet-
namese chunking task.
There are still room for improving the perfor-
mance of Vietnamese chunking models. For ex-
ample, more attention on features selection is nec-
essary. We would like to solve this in future work.
Acknowledgments
The constructive comments and helpful sugges-
tions from three anonymous reviewers are greatly
appreciated. This paper is supported by JAIST
Grant for Research Associates and a part from a
national project named Building Basic Resources
and Tools for Vietnamese Language and Speech
Processing, KC01.01/06-10.
References
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002.
K. Crammer et al 2006. Online Passive-Aggressive
Algorithm. Journal of Machine Learning Research,
2006
W. Chen, Y. Zhang, and H. Ishihara 2006. An em-
pirical study of Chinese chunking. In Proceedings
COLING/ACL 2006
Dinh Dien, Vu Thuy 2006. A maximum entropy
approach for vietnamese word segmentation. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2006: 248-253
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In the proceed-
15
ings of International Conference on Machine Learn-
ing (ICML), pp.282-289, 2001
N.C. Mai, D.N. Vu, T.P. Hoang. 1997. Foundations
of linguistics and Vietnamese. Education Publisher
(1997) 142. 152
Thi Minh Huyen Nguyen, Laurent Romary, Mathias
Rossignol, Xuan Luong Vu. 2006. A lexicon
for Vietnamese language processing. Language Re-
seourse Evaluation (2006) 40:291-309.
Minh Nghiem, Dien Dinh, Mai Nguyen. 2008. Im-
proving Vietnamese POS tagging by integrating a
rich feature set and Support Vector Machines. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2008: 128?133.
X.H. Phan, M.L. Nguyen, C.T. Nguyen. Flex-
CRFs: Flexible Conditional Random Field Toolkit.
http://flexcrfs.sourceforge.net, 2005
T. Kudo and Y. Matsumoto. 2001. Chunking with
Support Vector Machines. The Second Meeting of
the North American Chapter of the Association for
Computational Linguistics (2001)
F. Sha and F. Pereira. 2005. Shallow Parsing with
Conditional Random Fields. Proceedings of HLT-
NAACL 2003 213-220 (2003)
C.T. Nguyen, T.K. Nguyen, X.H. Phan, L.M. Viet-
namese Word Segmentation with CRFs and SVMs:
An Investigation. 2006. The 20th Pacific Asia Con-
ference on Language, Information, and Computation
(PACLIC), 1-3 November, 2006, Wuhan, China
Tjong Kim Sang and Sabine Buchholz. 2000. Intro-
duction to the CoNLL-2000 Shared Task: Chunk-
ing. Proceedings of CoNLL-2000 , Lisbon, Portugal,
2000.
V. Vapnik. 1995. The Natural of Statistical Learning
Theory. New York: Springer-Verlag, 1995.
16
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 143?150
Manchester, August 2008
A Tree-to-String Phrase-based Model for Statistical Machine Translation
Thai Phuong Nguyen
College of Technology
Vietnam National University, Hanoi
thainp@vnu.edu.vn
Akira Shimazu1, Tu-Bao Ho2, Minh Le Nguyen1, and Vinh Van Nguyen1
1School of Information Science
2School of Knowledge Science
Japan Advanced Institute of Science and Technology
{shimazu,bao,nguyenml,vinhnv}@jaist.ac.jp
Abstract
Though phrase-based SMT has achieved high
translation quality, it still lacks of generaliza-
tion ability to capture word order differences
between languages. In this paper we describe
a general method for tree-to-string phrase-
based SMT. We study how syntactic trans-
formation is incorporated into phrase-based
SMT and its effectiveness. We design syntac-
tic transformation models using unlexicalized
form of synchronous context-free grammars.
These models can be learned from source-
parsed bitext. Our system can naturally make
use of both constituent and non-constituent
phrasal translations in the decoding phase. We
considered various levels of syntactic analy-
sis ranging from chunking to full parsing.
Our experimental results of English-Japanese
and English-Vietnamese translation showed
a significant improvement over two baseline
phrase-based SMT systems.
1 Introduction
Based on the kind of linguistic information which
is made use of, syntactic SMT can be divided into
four types: tree-to-string, string-to-tree, tree-to-tree,
and hierarchical phrase-based. The tree-to-string ap-
proach (Collins et al, 2005; Nguyen and Shimazu,
2006; Liu et al, 2006 and 2007) supposes that syn-
tax of the source language is known. This approach
can be applied when a source language parser is
available. The string-to-tree approach (Yamada and
Knight, 2001; Galley et al, 2006) focuses on syntactic
modelling of the target language in cases it has syn-
tactic resources such as treebanks and parsers. The
tree-to-tree approach models the syntax of both lan-
guages, therefore extra cost is required. The fourth
approach (Chiang, 2005) constraints phrases under
context-free grammar structure without any require-
ment of linguistic annotation.
In this paper, we present a tree-to-string phrase-
based method which is based on synchronous CFGs.
This method has two important properties: syntactic
transformation is used in the decoding phase includ-
ing a word-to-phrase tree transformation model and
a phrase reordering model; phrases are the basic unit
of translation. Since we design syntactic transforma-
tion models using un-lexicalized synchronous CFGs,
the number of rules is small1. Previous studies on
tree-to-string SMT are different from ours. Collins
et al Collins et al (2005) used hand crafted rules to
carry out word reordering in the preprocessing phase
but not decoding phase. Nguyen and Shimazu (2006)
presented a more general method in which lexicalized
syntactic reordering models based on PCFGs can be
learned from source-parsed bitext and then applied in
the preprocessing phase. Liu et al (2006) changed the
translation unit from phrases to tree-to-string align-
ment templates (TATs) while we do not. TATs was
represented as xRs rules while we use synchronous
CFG rules. In order to overcome the limitation that
TATs can not capture non-constituent phrasal transla-
tions, Liu et al (2007) proposed forest-to-string rules
while our system can naturally make use of such kind
of phrasal translation by word-to-phrase tree transfor-
mation.
We carried out experiments with two language
pairs English-Japanese and English-Vietnamese. Our
system achieved significant improvements over
Pharaoh, a state-of-the-art phrase-based SMT system.
We also analyzed the dependence of translation qual-
ity on the level of syntactic analysis (shallow or deep).
Figure 1 shows the architecture of our system. The
input of this system is a source-language tree and the
output is a target-language string. This system uses
all features of conventional phrase-based SMT as in
(Koehn et al, 2003). There are two new features in-
cluding a word-to-phrase tree transformation model
and a phrase reordering model. The decoding algo-
1See Section 6.2.
143
rithm is a tree-based search algorithm.
Figure 1: A syntax-directed phrase-based SMT archi-
tecture.
2 Translation Model
We use an example of English-Vietnamese translation
to demonstrate the translation process as in Figure 2.
Now we describe a tree-to-string SMT model based
on synchronous CFGs. The translation process is:
Figure 2: The translation process.
T
1
? T
2
? T
3
? T
4
(1)
where T
1
is a source tree, T
2
is a source phrase tree,
T
3
is a reordered source phrase tree, and T
4
is a target
phrase tree.
Using the first order chain rule, the join probability
over variables (trees) in graphical representation 1 is
approximately calculated by:
P (T
1
, T
2
, T
3
, T
4
) = P (T
1
)?P (T
2
|T
1
)?P (T
3
|T
2
)?P (T
4
|T
3
)
(2)
P (T
1
) can be omitted since only one syntactic tree
is used. P (T
2
|T
1
) is a word-to-phrase tree transfor-
mation model we describe later. P (T
3
|T
2
) is a re-
ordering model. P (T
4
|T
3
) can be calculated using a
phrase translation model and a language model. This
is the fundamental equation of our study represented
in this paper. In the next section, we will describe how
to transform a word-based CFG tree into a phrase-
based CFG tree.
3 Word-to-Phrase Tree Transformation
3.1 Penn Treebank?s Tree Structure
According to this formalism, a tree is represented by
phrase structure. If we extract a CFG from a tree or
set of trees, there will be two possible rule forms:
? A ? ? where ? is a sequence of nonterminals
(syntactic categories).
? B ? ? where ? is a terminal symbol (or a word
in this case).
We consider an example of a syntactic tree and a
simple CFG extracted from that tree.
Sentence: ?I am a student?
Syntactic tree: (S (NP (NN I)) (VP (VBP am) (NP (DT a) (NN
student))))
Rule set: S ? NP VP; VP ? VBP NP; NP ? NN | DT NN; NN
? I | student;
VBP ? am; DT ? a
However, we are considering phrase-based transla-
tion. Therefore the right hand side of the second rule
form must be a sequence of terminal symbols (or a
phrase) but not a single symbol (a word). Suppose
that the phrase table contains a phrase ?am a student?
which leads to the following possible tree structure:
Phrase segmentation: ?I | am a student?
Syntactic tree: (S (NP (NN I)) (VP (VBP am a student)))
Rule set: S ? NP VP; VP ? VBP; NP ? NN; NN ? I; VBP ?
am a student
We have to find out some way to transform a CFG
tree into a tree with phrases at leaves. In the next sub-
section we propose such an algorithm.
3.2 An Algorithm for Word-to-Phrase Tree
Transformation
Table 1 represents our algorithm to transform a CFG
tree to a phrase CFG tree. When designing this algo-
rithm, our criterion is to preserve the original struc-
ture as much as possible. This algorithm includes two
steps. There are a number of notions concerning this
algorithm:
? A CFG rule has a head symbol on the right hand
side. Using this information, head child of a
node on a syntactic tree can be determined.
144
+ Input: A CFG tree, a phrase segmentation
+ Output: A phrase CFG tree
+ Step 1: Allocate phrases to leaf nodes in a top-down manner: A phrase is allocated to head word of a node if the
phrase contains the head word. This head word is then considered as the phrase head.
+ Step 2: Transform the syntactic tree by replacing leaf nodes by their allocated phrase and removing all nodes whose
span is a substring of phrases.
Table 1: An algorithm to transform a CFG tree to a phrase CFG tree.
? If a node is a pre-terminal node (containing POS
tag), its head word is itself. If a node is an in-
ner node (containing syntactic constituent tag),
its head word is retrieved through the head child.
? Word span of a node is a string of its leaves. For
instance, word span of subtree (NP (PRP$ your)
(NN class)) is ?your class?.
Now we consider an example depicted in Figure 3
and 4. Head children are tagged with functional label
H. There are two phrases: ?is a? and ?in your class?.
After the Step 1, the phrase ?is a? is attached to (VBZ
is). The phrase ?in your class? is attached to (IN in).
In Step 2, the node (V is) is replaced by (V ?is a?) and
(DT a) is removed from its father NP. Similarly, (IN
in) is replaced by (IN ?in your class?) and the subtree
NP on the right is removed.
S
[is]
NP
[Fred]
VP-H
[is]
VBZ-H NP[student]NNP-H
is NP-H[student]
DT NN-H
PP
[in]
IN-H NP[class]
PRP$ NN-H
Fred
a student in
your class
{is a}
{in your class}
Figure 3: Tree transformation - step 1. Solid arrows
show the allocation process of ?is a?. Dotted arrows
demonstrate the allocation process of ?in your class?
The proposed algorithm has some properties. We
state these properties without presenting proof2.
? Uniqueness: Given a CFG tree and a phrase seg-
mentation, by applying Algorithm 1, one and
only one phrase tree is generated.
2Proofs are simple.
Figure 4: Tree transformation - step 2.
? Constituent subgraph: A phrase CFG tree is
a connected subgraph of input tree if leaves are
ignored.
? Flatness: A phrase CFG tree is flatter than input
tree.
? Outside head: The head of a phrase is always a
word whose head outside the phrase. If there is
more than one word satisfying this condition, the
word at the highest level is chosen.
? Dependency subgraph: Dependency graph of a
phrase CFG tree is a connected subgraph of in-
put tree?s dependency graph if there exist no de-
tached nodes.
The meaning of uniqueness property is that our al-
gorithm is a deterministic procedure. The constituent-
subgraph property will be employed in the next sec-
tion for an efficient decoding algorithm. When a syn-
tactic tree is transformed, a number of subtrees are
replaced by phrases. The head word of a phrase is the
contact point of that phrase with the remaining part
of a sentence. From the dependency point of view, a
head word should depend on an outer word rather than
an inner word. About dependency-subgraph property,
when there is a detached node, an indirect dependency
will become a direct one. In any cases, there is no
145
change in dependency direction. We can observe de-
pendency trees in Figure 5. The first two trees are
source dependency tree and phrase dependency tree
of the previous example. The last one corresponds to
the case in which a detached node exists.
Fred is
ROOT
student in your classa
Fred is a
ROOT
student in your class
Fred is a student
ROOT
in your class
Figure 5: Dependency trees. The third tree corre-
sponds with phrase segmentation: ?Fred | is a student
| in your class?
3.3 Probabilistic Word-to-Phrase Tree
Transformation
We have proposed an algorithm to create a phrase
CFG tree from a pair of CFG tree and phrase seg-
mentation. Two questions naturally arise: ?is there
a way to evaluate how good a phrase tree is?? and ?is
such an evaluation valuable?? Note that phrase trees
are the means to reorder the source sentence repre-
sented as phrase segmentations. Therefore a phrase
tree is surely not good if no right order can be gen-
erated. Now the answer to the second question is
clear. We need an evaluation method to prevent our
program from generating bad phrase trees. In other
words, good phrase trees should be given a higher pri-
ority.
We define the phrase tree probability as the product
of its rule probability given the original CFG rules:
P (T
?
) =
?
i
P (LHS
i
? RHS
?
i
|LHS
i
? RHS
i
)
(3)
where T ? is a phrase tree whose CFG rules are
LHS
i
? RHS
?
i
. LHS
i
? RHS
i
are origi-
nal CFG rules. RHS?
i
are subsequences of RHS
i
.
Since phrase tree rules should capture changes made
by the transformation from word to phrase, we use
?+? to represent an expansion and ?-? to show an
overlap. These symbol will be added to a nonter-
minal on the side having a change. In the previ-
ous example, since a head noun in the word tree
has been expanded on the right, the correspond-
ing symbol in phrase tree is NN-H+. A nonter-
minal X can become one of the following symbols
X,?X,+X,X?, X+,?X?,?X+,+X?,+X+.
Conditional probabilities are computed in a sepa-
rate training phase using a source-parsed and word-
aligned bitext. First, all phrase pairs consistent with
the word alignment are collected. Then using this
phrase segmentation and syntactic trees we can gener-
ate phrase trees by word-to-phrase tree transformation
and extract rules.
4 Phrase Reordering Model
Reordering rules are represented as SCFG rules
which can be un-lexicalized or source-side lexicalized
(Nguyen and Shimazu, 2006). In this paper, we used
un-lexicalized rules. We used a learning algorithm
as in (Nguyen and Shimazu, 2006) to learn weighted
SCFGs. The training requirements include a bilingual
corpus, a word alignment tool, and a broad coverage
parser of the source language. The parser is a con-
stituency analyzer which can produce parse tree in
Penn Tree-bank?s style. The model is applicable to
language pairs in which the target language is poor
in resources. We used phrase reorder rules whose ?+?
and ?-? symbols are removed.
5 Decoding
A source sentence can have many possible phrase seg-
mentations. Each segmentation in combination with a
source tree corresponds to a phrase tree. A phrase-tree
forest is a set of those trees. A naive decoding algo-
rithm is that for each segmentation, a phrase tree is
generated and then the sentence is translated. This al-
gorithm is very slow or even intractable. Based on
the constituent-subgraph property of the tree trans-
formation algorithm, the forest of phrase trees will
be packed into a tree-structure container whose back-
bone is the original CFG tree.
5.1 Translation Options
A translation option encodes a possibility to translate
a source phrase (at a leaf node of a phrase tree) to
another phrase in target language. Since our decoder
uses a log-linear translation model, it can exploit var-
ious features of translation options. We use the same
features as (Koehn et al, 2003). Basic information of
a translation option includes:
? source phrase
? target phrase
? phrase translation score (2)
146
? lexical translation score (2)
? word penalty
Translation options of an input sentence are col-
lected before any decoding takes place. This allows a
faster lookup than consulting the whole phrase trans-
lation table during decoding. Note that the entire
phrase translation table may be too big to fit into
memory.
5.2 Translation Hypotheses
A translation hypothesis represents a partial or full
translation of an input sentence. Initial hypotheses
correspond to translation options. Each translation
hypothesis is associated with a phrase-tree node. In
other words, a phrase-tree node has a collection of
translation hypotheses. Now we consider basic infor-
mation contained in a translation hypothesis:
? the cost so far
? list of child hypotheses
? left language model state and right language
model state
5.3 Decoding Algorithm
First we consider structure of a syntactic tree. A tree
node contains fields such as syntactic category, child
list, and head child index. A leaf node has an ad-
ditional field of word string. In order to extend this
structure to store translation hypotheses, a new field
of hypothesis collection is appended. A hypothe-
sis collection contains translation hypotheses whose
word spans are the same. Actually, it corresponds to
a phrase-tree node. A hypothesis collection whose
word span is [i
1
, i
2
] at a node whose tag is X ex-
presses that:
? There is a phrase-tree node (X, i
1
, i
2
).
? There exist a phrase [i
1
, i
2
] or
? There exist a subsequence of X?s child list:
(Y
1
, j
0
, j
1
), (Y
2
, j
1
+1, j
2
), ..., (Y
n
, j
n?1
+1, j
n
)
where j
0
= i
1
and j
n
= i
2
? Suppose that [i, j] is X?s span, then [i
1
, i
2
] is a
valid phrase node?s span if and only if: i
1
<= i
or i < i
1
<= j and there exist a phrase [i
0
, i
1
?
1] overlapping X?s span at [i, i
1
? 1]. A similar
condition is required of j.
Table 2 shows our decoding algorithm. Step 1 dis-
tributes translation options to leaf nodes using a pro-
cedure similar to Step 1 of algorithm in Table 1. Step
Corpus Size Training Development Testing
Conversation 16,809 15,734 403 672
Reuters 57,778 55,757 1,000 1,021
Table 3: Corpora and data sets.
English Vietnamese
Sentences 16,809
Average sent. len. 8.5 8.0
Words 143,373 130,043
Vocabulary 9,314 9,557
English Japanese
Sentences 57,778
Average sent. len. 26.7 33.5
Words 1,548,572 1,927,952
Vocabulary 31,702 29,406
Table 4: Corpus statistics of translation tasks.
2 helps check valid subsequences in Step 3 fast. Step
3 is a bottom-up procedure, a node is translated if all
of its child nodes have been translated. Step 3.1 calls
syntactic transformation models. After reordered in
Step 3.2, a subsequence will be translated in Step 3.3
using a simple monotonic decoding procedure result-
ing in new translation hypotheses. We used a beam
pruning technique to reduce the memory cost and to
accelerate the computation.
6 Experimental Results
6.1 Experimental Settings
We used Reuters3, an English-Japanese bilingual cor-
pus, and Conversation, an English-Vietnamese corpus
(Table 4). These corpora were split into data sets as
shown in Table 3. Japanese sentences were analyzed
by ChaSen4, a word-segmentation tool.
A number of tools were used in our experiments.
Vietnamese sentences were segmented using a word-
segmentation program (Nguyen et al, 2003). For
learning phrase translations and decoding, we used
Pharaoh (Koehn, 2004), a state-of-the-art phrase-
based SMT system which is available for research
purpose. For word alignment, we used the GIZA++
tool (Och and Ney, 2000). For learning language
models, we used SRILM toolkit (Stolcke, 2002). For
MT evaluation, we used BLEU measure (Papineni et
al., 2001) calculated by the NIST script version 11b.
For the parsing task, we used Charniak?s parser (Char-
niak, 2000). For experiments with chunking (or shal-
low parsing), we used a CRFs-based chunking tool 5
to split a source sentence into syntactic chunks. Then
a pseudo CFG rule over chunks is built to generate a
two-level syntactic tree. This tree can be used in the
3http://www2.nict.go.jp/x/x161/members/mutiyama/index.html
4http://chasen.aist-nara.ac.jp/chasen/distribution.html.en
5http://crfpp.sourceforge.net/
147
+ Input: A source CFG tree, a translation-option collection
+ Output: The best target sentence
+ Step 1: Allocate translation options to hypothesis collections at leaf nodes.
+ Step 2: Compute overlap vector for all nodes.
+ Step 3: For each node, if all of its children have been translated, then for each valid
sub-sequence of child list, carry out the following steps:
+ Step 3.1: Retrieve transformation rules
+ Step 3.2: Reorder the sub-sequence
+ Step 3.3: Translate the reordered sub-sequence and update corresponding
hypothesis collections
Table 2: A bottom-up dynamic-programming decoding algorithm.
Corpus CFG PhraseCFG W2PTT Reorder
Conversation 2,784 2,684 8,862 2,999
Reuters 7,668 5,479 13,458 7,855
Table 5: Rule induction statistics.
Corpus Pharaoh PB system SD system SD system
(chunking) (full-parsing)
Conversation 35.47 35.66 36.85 37.42
Reuters 24.41 24.20 20.60 25.53
Table 6: BLEU score comparison between phrase-
based SMT and syntax-directed SMT. PB=phrase-
based; SD=syntax-directed
same way as trees produced by Charniak?s parser.
We built a SMT system for phrase-based log-linear
translation models. This system has two decoders:
beam search and syntax-based. We implemented the
algorithm in Section 5 for the syntax-based decoder.
We also implemented a rule induction module and a
module for minimum error rate training. We used the
system for our experiments reported later.
6.2 Rule Induction
In Table 5, we report statistics of CFG rules,
phrase CFG rules, word-to-phrase tree transformation
(W2PTT) rules, and reordering rules. All counted
rules were in un-lexicalized form. Those numbers are
very small in comparison with the number of phrasal
translations (up to hundreds of thousands on our cor-
pora). There were a number of ?un-seen? CFG rules
which did not have a corresponding reordering rule.
A reason is that those rules appeared once or several
times in the training corpus; however, their hierarchi-
cal alignments did not satisfy the conditions for in-
ducing a reordering rule since word alignment is not
perfect (Nguyen and Shimazu, 2006). Another reason
is that there were CFG rules which required nonlocal
reordering. This may be an issue for future research:
a Markovization technique for SCFGs.
6.3 BLEU Scores
Table 6 shows a comparison of BLEU scores be-
tween Pharaoh, our phrase-based SMT system, and
our syntax-directed (SD) SMT system with chunking
and full parsing respectively. On both Conversation
corpus and Reuters corpus: The BLEU score of our
phrase-based SMT system is comparable to that of
Pharaoh; The BLEU score of our SD system with full
parsing is higher than that of our phrase-based sys-
tem. On Conversation corpus, our SD system with
chunking has a higher performance in terms of BLEU
score than our phrase-based system. Using sign test
(Lehmann, 1986), we verified the improvements are
statistically significant. However, on Reuters corpus,
performance of the SD system with chunking is much
lower than the phrase-based system?s. The reason is
that in English-Japanese translation, chunk is a too
shallow syntactic structure to capture word order in-
formation. For example, a prepositional chunk of-
ten includes only preposition and adverb, therefore
such information does not help reordering preposi-
tional phrases.
6.4 The Effectiveness of the W2PTT Model
Without this feature, BLEU scores decreased around
0.5 on both corpora. We now consider a linguistically
motivated example of English-Vietnamese translation
to show that phrase segmentation can be evaluated
through phrase tree scoring. This example was ex-
tracted from Conversation test set.
English sentence: for my wife ?s mother
Vietnamese word order: for mother ?s wife my
Phrase segmentation 1: for my wife | ?s | mother
P1=P(PP?IN+ -NP | PP?IN NP)xP(-NP?-NP NN | NP?NP
NN)xP(-NP?POS | NP?PRP$ NN
POS)=log(0.00001)+log(0.14)+log(0.048)=-5-0.85-1.32=-7.17
Phrase segmentation 2: for | my wife ?s | mother
P2=P(PP?IN NP | PP?IN NP)xP(NP?NP NN | NP?NP
NN) xP(NP?POS | NP?PRP$ NN POS)
=log(0.32)+log(0.57)+log(0.048)=-0.5-0.24-1.32=-2.06
The first phrase segmentation is bad (or even un-
acceptable) since the right word order can not be
achieved from this segmentation by phrase reorder-
ing and word reordering within phrases. The second
phrase segmentation is much better. Source syntax
tree and phrase trees are shown in Figure 6. The first
phrase tree has a much smaller probability (P1=-7.17)
than the second (P2=-2.06).
148
Figure 6: Two phrase trees.
Corpus Level-1 Level-2 Level-3 Level-4 Full
Conversation 36.85 36.91 37.11 37.23 37.42
Reuters 20.60 22.76 24.49 25.12 25.53
Table 7: BLEU score with different syntactic levels.
Level-i means syntactic transformation was applied to
tree nodes whose level smaller than or equal to i. The
level of a pre-terminal node (POS tag) is 0. The level
of an inner node is the maximum of its children?s lev-
els.
6.5 Levels of Syntactic Analysis
Since in practice, chunking and full parsing are often
used, in Table 6, we showed translation quality of the
two cases. It is interesting if we can find how syn-
tactic analysis can affect BLEU score at more inter-
mediate levels (Table 7). On the Conversation corpus,
using syntax trees of level-1 is effective in comparison
with baseline. The increase of syntactic level makes a
steady improvement in translation quality. Note that
when we carried out experiments with chunking (con-
sidered as level-1 syntax) the translation speed (in-
cluding chunking) of our tree-to-string system was
much faster than baseline systems?. This is an option
for developing applications which require high speed
such as web translation.
7 Related Works
7.1 A Comparison of Syntactic SMT Methods
To advance the state of the art, SMT system design-
ers have experimented with tree-structured transla-
tion models. The underlying computational models
were synchronous context-free grammars and finite-
state tree transducers which conceptually have a bet-
ter expressive power than finite-state transducers. We
create Tables 8 and 9 in order to compare syntac-
tic SMT methods including ours. The first row is a
baseline phrasal SMT approach. The second column
in Table 8 only describes input types because the out-
put is often string. Syntactic SMT methods are dif-
ferent in many aspects. Methods which make use of
phrases (in either explicit or implicit way) can beat
the baseline approach (Table 8) in terms of BLEU
metric. Two main problems these models aim to deal
with are word order and word choice. In order to ac-
complish this purpose, the underlying formal gram-
mars (including synchronous context-free grammars
and tree transducers) can be fully lexicalized or un-
lexicalized (Table 9).
7.2 Non-constituent Phrasal Translations
Liu et al (2007) proposed forest-to-string rules to
capture non-constituent phrasal translation while our
system can naturally make use of such kind of phrasal
translation by using word-to-phrase tree transforma-
tion. Liu et al (2007) also discussed about how
the phenomenon of non-syntactic bilingual phrases
is dealt with in other SMT methods. Galley et al
(2006) handled non-constituent phrasal translation by
traversing the tree upwards until reaches a node that
subsumes the phrase. Marcu et al (2006) reported
that approximately 28% of bilingual phrases are non-
syntactic on their English-Chinese corpus. They pro-
posed using a pseudo nonterminal symbol that sub-
sumes the phrase and corresponding multi-headed
syntactic structure. One new xRs rule is required to
explain how the new nonterminal symbol can be com-
bined with others. This technique brought a signif-
icant improvement in performance to their string-to-
tree noisy channel SMT system.
8 Conclusions
We have presented a general tree-to-string phrase-
based method. This method employs a syntax-based
reordering model in the decoding phase. By word-
to-phrase tree transformation, all possible phrases
are considered in translation. Our method does
not suppose a uniform distribution over all possible
phrase segmentations as (Koehn et al, 2003) since
each phrase tree has a probability. We believe that
other kinds of translation unit such as n-gram (Jos
et al, 2006), factored phrasal translation (Koehn and
Hoang, 2007), or treelet (Quirk et al, 2005) can be
used in this method. We would like to consider this
problem as a future study. Moreover we would like to
use n-best trees as the input of our system. A number
149
Method Input Theoretical Decoding style Linguistic Phrase Performance
model information usage
Koehn et al (2003) string FSTs beam search no yes baseline
Yamada and Knight (2001) string SCFGs parsing target no not better
Melamed (2003) string SCFGs parsing both sides no not better
Chiang (2005) string SCFGs parsing no yes better
Quirk et al (2005) dep. tree TTs parsing source yes better
Galley et al (2006) string TTs parsing target yes better
Liu et al (2006) tree TTs tree transf. source yes better
Our work tree SCFGs tree transf. source yes better
Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous
Context-Free Grammar; TT=Tree Transducer.
Method Rule form Rule function Rule lexicalization level
Koehn et al (2003) no no no
Yamada and Knight (2001) SCFG rule reorder and function-word ins./del. unlexicalized
Melamed (2003) SCFG rule reorder and word choice full
Chiang (2005) SCFG rule reorder and word choice full
Quirk et al (2005) Treelet pair word choice full
Galley et al (2006) xRs rule reorder and word choice full
Liu et al (2006) xRs rule reorder and word choice full
Our work SCFG rule reorder unlexicalized
Table 9: A comparison of syntactic SMT methods (part 2). xRs is a kind of rule which maps a syntactic pattern
to a string, for example VP(AUX(does), RB(not),x
0
:VB) ? ne, x
0
, pas. In the column Rule lexicalization
level: full=lexicalization using vocabularies of both source language and target language.
of non-local reordering phenomena such as adjunct
attachment should be handled in the future.
References
Charniak, E. 2000. A maximum entropy inspired parser.
In Proceedings of HLT-NAACL.
Galley, M., Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, Ignacio Thayer 2006. Scal-
able Inference and Training of Context-Rich Syntactic
Translation Models. In Proceedings of ACL.
Jos B. Mario, Rafael E. Banchs, Josep M. Crego, Adri de
Gispert, Patrik Lambert, Jos A. R. Fonollosa, Marta R.
Costa-juss. 2006. N-gram-based Machine Translation.
Computational Linguistics, 32(4): 527?549.
Koehn, P. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of AMTA.
Koehn, P. and Hieu Hoang. 2007. Factored Translation
Models. In Proceedings of EMNLP.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
Lehmann, E. L. 1986. Testing Statistical Hypotheses (Sec-
ond Edition). Springer-Verlag.
Liu, Y., Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL.
Liu, Y., Yun Huang, Qun Liu, and Shouxun Lin 2007.
Forest-to-String Statistical Translation Rules. In Pro-
ceedings of ACL.
Marcu, D., Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phrases. In Proceed-
ings of EMNLP.
Melamed, I. D. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
Nguyen, Thai Phuong and Akira Shimazu. 2006. Improv-
ing Phrase-Based Statistical Machine Translation with
Morphosyntactic Transformation. Machine Translation,
20(3): 147?166.
Nguyen, Thai Phuong, Nguyen Van Vinh and Le Anh
Cuong. 2003. Vietnamese Word Segmentation Using
Hidden Markov Model. In Proceedings of International
Workshop for Computer, Information, and Communica-
tion Technologies in Korea and Vietnam.
Och, F. J. and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL.
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Report.
Quirk, C., A. Menezes, and C. Cherry. 2005. Dependency
treelet translation: Syntactically informed phrasal SMT.
In Proceedings of ACL.
Stolcke, A. 2002. SRILM - An Extensible Language Mod-
eling Toolkit. In Proc. Intl. Conf. Spoken Language
Processing.
Yamada, K. and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proceedings of ACL.
150
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 160?168,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Reranking Model for Discourse Segmentation using Subtree Features
Ngo Xuan Bach, Nguyen Le Minh, Akira Shimazu
School of Information Science
Japan Advanced Institute of Science and Technology
1-1 Asahidai, Nomi, Ishikawa, 923-1292, Japan
{bachnx,nguyenml,shimazu}@jaist.ac.jp
Abstract
This paper presents a discriminative reranking
model for the discourse segmentation task, the
first step in a discourse parsing system. Our
model exploits subtree features to rerank N-
best outputs of a base segmenter, which uses
syntactic and lexical features in a CRF frame-
work. Experimental results on the RST Dis-
course Treebank corpus show that our model
outperforms existing discourse segmenters in
both settings that use gold standard Penn Tree-
bank parse trees and Stanford parse trees.
1 Introduction
Discourse structure has been shown to have an im-
portant role in many natural language applications,
such as text summarization (Marcu, 2000; Louis et
al., 2010), information presentation (Bateman et al,
2001), question answering (Sun and Chai, 2007),
and dialogue generation (Hernault et al, 2008). To
produce such kinds of discourse structure, several
attempts have been made to build discourse parsers
in the framework of Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988), one of the
most widely used theories of text structure.
In the RST framework, a text is first divided into
several elementary discourse units (EDUs). Each
EDU may be a simple sentence or a clause in a com-
plex sentence. Consecutive EDUs are then put in
relation with each other to build a discourse tree.
Figure 1 shows an example of a discourse tree with
three EDUs. The goal of the discourse segmentation
task is to divide the input text into such EDUs.
Figure 1: A discourse tree (Soricut and Marcu, 2003).
The quality of the discourse segmenter con-
tributes a significant part to the overall accuracy of
every discourse parsing system. If a text is wrongly
segmented, no discourse parsing algorithm can build
a correct discourse tree.
Existing discourse segmenters usually exploit lex-
ical and syntactic features to label each word in a
sentence with one of two labels, boundary or no-
boundary. The limitation of this approach is that it
only focuses on the boundaries of EDUs. It cannot
capture features that describe whole EDUs.
Recently, discriminative reranking has been used
successfully in some NLP tasks such as POS tag-
ging, chunking, and statistical parsing (Collins and
Koo, 2005; Kudo et al, 2005; Huang, 2008; Fraser
et al, 2009). The advantage of the reranking method
is that it can exploit the output of a base model to
learn. Based on that output, we can extract long-
distance non-local features to rerank.
In this paper, we present a reranking model for
the discourse segmentation task. We show how to
use subtree features, features extracted from whole
EDUs, to rerank outputs of a base model. Exper-
imental results on RST Discourse Treebank (RST-
DT) (Carlson et al, 2002) show that our model out-
160
performs existing systems.
The rest of this paper is organized as follows. Sec-
tion 2 summarizes related work. Section 3 presents
our method. Experimental results on RST-DT are
described in Section 4. Finally, Section 5 gives con-
clusions.
2 Related Work
Several methods have been proposed to deal with the
discourse segmentation task. Thanh et al (2004)
present a rule-based discourse segmenter with two
steps. In the first step, segmentation is done by us-
ing syntactic relations between words. The segmen-
tation algorithm is based on some principles, which
have been presented in Corston (1998) and Carlson
and Marcu (2001), as follows:
1. The clause that is attached to a noun phrase
can be recognised as an embedded unit. If the
clause is a subordinate clause, it must contain
more than one word.
2. Coordinate clauses and coordinate sentences
of a complex sentence are EDUs.
3. Coordinate clauses and coordinate elliptical
clauses of verb phrases (VPs) are EDUs. Co-
ordinate VPs that share a direct object with the
main VP are not considered as a separate dis-
course segment.
4. Clausal complements of reported verbs and
cognitive verbs are EDUs.
The segmenter then uses cue phrases to correct the
output of the first step.
Tofiloski et al (2009) describe another rule-based
discourse segmenter. The core of this segmenter
consists of 12 syntactic segmentation rules and some
rules concerning a list of stop phrases, discourse cue
phrases, and part-of-speech tags. They also use a
list of phrasal discourse cues to insert boundaries not
derivable from the parser?s output.
Soricut and Marcu (2003) introduce a statisti-
cal discourse segmenter, which is trained on RST-
DT to label words with boundary or no-boundary
labels. They use lexical and syntactic features to
determine the probabilities of discourse boundaries
P (bi|wi, t), where wi is the ith word of the input
sentence s, t is the syntactic parse tree of s, and bi ?
{boundary, no-boundary}. Given a syntactic parse
tree t, their algorithm inserts a discourse boundary
after each word w for which P (boundary|w, t) >
0.5.
Another statistical discourse segmenter using arti-
ficial neural networks is presented in Subba and Di
Eugenio (2007). Like Soricut and Marcu (2003),
they formulate the discourse segmentation task as
a binary classification problem of deciding whether
a word is the boundary or no-boundary of EDUs.
Their segmenter exploits a multilayer perceptron
model with back-propagation algorithm and is also
trained on RST-DT.
Hernault et al (2010) propose a sequential model
for the discourse segmentation task, which considers
the segmentation task as a sequence labeling prob-
lem rather than a classification problem. They ex-
ploit Conditional Random Fields (CRFs) (Lafferty
et al, 2001) as the learning method and get state-of-
the-art results on RST-DT.
In our work, like Hernault et al (2010), we also
consider the discourse segmentation task as a se-
quence labeling problem. The final segmentation
result is selected among N-best outputs of a CRF-
based model by using a reranking method with sub-
tree features.
3 Method
3.1 Discriminative Reranking
In the discriminative reranking method (Collins and
Koo, 2005), first, a set of candidates is generated us-
ing a base model (GEN). GEN can be any model for
the task. For example, in the part-of-speech (POS)
tagging problem, GEN may be a model that gener-
ates all possible POS tags for a word based on a dic-
tionary. Then, candidates are reranked using a linear
score function:
score(y) = ?(y) ?W
where y is a candidate, ?(y) is the feature vector of
candidate y, and W is a parameter vector. The final
output is the candidate with the highest score:
F (x) = argmaxy?GEN(x)score(y)
= argmaxy?GEN(x)?(y) ?W.
161
To learn the parameter W we use the average per-
ceptron algorithm, which is presented as Algorithm
1.
Algorithm 1 Average perceptron algorithm for
reranking (Collins and Koo, 2005)
1: Inputs: Training set {(xi, yi)|xi ? Rn, yi ?
C,?i = 1, 2, . . . ,m}
2: Initialize: W ? 0,Wavg ? 0
3: Define: F (x) = argmaxy?GEN(x)?(y) ?W
4: for t = 1, 2, . . . , T do
5: for i = 1, 2, . . . ,m do
6: zi ? F (xi)
7: if zi 6= yi then
8: W ?W + ?(yi)? ?(zi)
9: end if
10: Wavg ?Wavg + W
11: end for
12: end for
13: Wavg ?Wavg/(mT )
14: Output: Parameter vector Wavg.
In the next sections we will describe our base
model and features that we use to rerank candidates.
3.2 Base Model
Similar to the work of Hernault et al (2010), our
base model uses Conditional Random Fields1 to
learn a sequence labeling model. Each label is either
beginning of EDU (B) or continuation of EDU (C).
Soricut and Marcu (2003) and Subba and Di Euge-
nio (2007) use boundary labels, which are assigned
to words at the end of EDUs. Like Hernault et al
(2010), we use beginning labels, which are assigned
to words at the beginning of EDUs. However, we
can convert an output with boundary, no-boundary
labels to an output with beginning, continuation la-
bels and vice versa. Figure 2 shows two examples of
segmenting a sentence into EDUs and their correct
label sequences.
We use the following lexical and syntactic infor-
mation as features: words, POS tags, nodes in parse
trees and their lexical heads and their POS heads2.
When extracting features for word w, let r be the
1We use the implementation of Kudo (Kudo, CRF++).
2Lexical heads are extracted using Collins? rules (Collins,
1999).
Figure 2: Examples of segmenting sentences into EDUs.
word on the right-hand side of w and Np be the deep-
est node that belongs to both paths from the root to
w and r. Nw and Nr are child nodes of Np that
belong to two paths, respectively. Figure 3 shows
two partial lexicalized syntactic parse trees. In the
first tree, if w = says then r = it, Np = V P (says),
Nw = V BZ(says), and Nr = SBAR(will). We
also consider the parent and the right-sibling of Np
if any. The final feature set for w consists of not only
features extracted from w but also features extracted
from two words on the left-hand side and two words
on the right-hand side of w.
Our feature extraction method is different from
the method in previous work (Soricut and Marcu,
2003; Hernault et al, 2010). They define Nw as the
highest ancestor of w that has lexical head w and has
a right-sibling. Then Np and Nr are defined as the
parent and right-sibling of Nw. In the first example,
our method gives the same results as the previous
one. In the second example, however, there is no
node with lexical head ?done? and having a right-
sibling. The previous method cannot extract Nw,
Np, and Nr in such cases. We also use some new
features such as the head node and the right-sibling
node of Np.
3.3 Subtree Features for Reranking
We need to decide which kinds of subtrees are useful
to represent a candidate, a way to segment the input
sentence into EDUs. In our work, we consider two
kinds of subtrees: bound trees and splitting trees.
The bound tree of an EDU, which spans from
word u to word w, is a subtree which satisfies two
conditions:
162
Figure 3: Partial lexicalized syntactic parse trees.
1. its root is the deepest node in the parse tree
which belongs to both paths from the root of
the parse tree to u and w, and
2. it only contains nodes in two those paths.
The splitting tree between two consecutive EDUs,
from word u to word w and from word r to word
v, is a subtree which is similar to a bound tree, but
contains two paths from the root of the parse tree to
w and r. Hence, a splitting tree between two con-
secutive EDUs is a bound tree that only covers two
words: the last word of the first EDU and the first
word of the second EDU. Bound trees will cover the
whole EDUs, while splitting trees will concentrate
on the boundaries of EDUs.
From a bound tree (similar to a splitting tree), we
extract three kinds of subtrees: subtrees on the left
path (left tree), subtrees on the right path (right tree),
and subtrees consisting of a subtree on the left path
and a subtree on the right path (full tree). In the
third case, if both subtrees on the left and right paths
do not contain the root node, we add a pseudo root
node. Figure 4 shows the bound tree of EDU ?noth-
ing was done? of the second example in Figure 3,
and some examples of extracted subtrees.
Each subtree feature is then represented by a
string as follows:
? A left tree (or a right tree) is represented by
concatenating its nodes with hyphens between
nodes. For example, subtrees (b) and (e) in Fig-
ure 4 can be represented as follows:
S-NP-NN-nothing, and
S-VP-VP-VBN-done.
? A full tree is represented by concatenating its
left tree and right tree with string ### in the
middle. For example, subtrees (g) and (h) in
Figure 4 can be represented as follows:
S-NP-NN###S-VP-VP-VBN, and
NP-NN-nothing###VP-VP-VBN.
The feature set of a candidate is the set of all sub-
trees extracted from bound trees of all EDUs and
splitting trees between two consecutive EDUs.
Among two kinds of subtrees, splitting trees can
be computed between any two adjacent words and
therefore can be incorporated into the base model.
However, if we do so, the feature space will be very
large and contains a lot of noisy features. Because
many words are not a boundary of any EDU, many
subtrees extracted by this method will never be-
come a real splitting tree (tree that splits two EDUs).
Splitting trees extracted in the reranking model will
focus on a small but compact and useful set of sub-
trees.
4 Experiments
4.1 Data and Evaluation Methods
We tested our model on the RST Discourse Treebank
corpus. This corpus consists of 385 articles from the
Penn Treebank, which are divided into a Training
set and a Test set. The Training set consists of 347
articles (6132 sentences), and the Test set consists of
38 articles (991 sentences).
There are two evaluation methods that have been
used in previous work. The first method measures
only beginning labels (B labels) (Soricut and Marcu,
2003; Subba and Di Eugenio, 2007). The second
163
Figure 4: Subtree features.
method (Hernault et al, 2010) measures both be-
ginning and continuation labels (B and C labels)3.
This method first calculates scores on B labels and
scores on C labels, and then produces the average of
them. Due to the number of C labels being much
higher than the number of B labels, the second eval-
uation method yields much higher results. In Her-
nault et al (2010), the authors compare their systems
with previous work despite using different evalua-
tion methods. Such comparisons are not valid. In
our work, we measure the performance of the pro-
posed model using both methods.
4.2 Experimental Results
We learned the base model on the Training set and
tested on the Test set to get N-best outputs to rerank.
To learn parameters of the reranking model, we con-
ducted 5-fold cross-validation tests on the Training
set. In all experiments, we set N to 20. To choose
the number of iterations, we used a development set,
which is about 20 percent of the Training set.
Table 1 shows experimental results when evaluat-
ing only beginning (B) labels, in which SPADE is
the work of Soricut and Marcu (2003), NNDS is a
segmenter that uses neural networks (Subba and Di
Eugenio, 2007), and CRFSeg is a CRF-based seg-
menter (Hernault et al, 2010). When using gold
parse trees, our base model got 92.5% in the F1
score, which improves 1.3% compared to the state-
of-the-art segmenter (CRFSeg). When using Stan-
ford parse trees (Klein and Manning, 2003), our
base model improved 1.7% compared to CRFSeg.
It demonstrates the effectiveness of our feature ex-
3Neither evaluation method counts sentence boundaries.
Table 1: Performance when evaluating on B labels
Model Trees Pre(%) Re(%) F1(%)
SPADE Penn 84.1 85.4 84.7
NNDS Penn 85.5 86.6 86.0
CRFSeg Penn 92.7 89.7 91.2
Base Penn 92.5 92.5 92.5
Reranking Penn 93.1 94.2 93.7
CRFSeg Stanford 91.0 87.2 89.0
Base Stanford 91.4 90.1 90.7
Reranking Stanford 91.5 90.4 91.0
Human - 98.5 98.2 98.3
traction method in the base model. As expected,
our reranking model got higher results compared
to the base model in both settings. The rerank-
ing model got 93.7% and 91.0% in two settings,
which improves 2.5% and 2.0% compared to CRF-
Seg. Also note that, when using Stanford parse trees,
our reranking model got competitive results with
CRFSeg when using gold parse trees (91.0% com-
pared to 91.2%).
Table 2 shows experimental results when evaluat-
ing on both beginning and continuation labels. Our
models also outperformed CRFSeg in both settings,
using gold parse trees and using Stanford parse trees
(96.6% compared to 95.3% in the first setting, and
95.1% compared to 94.1% in the second setting).
Both evaluation methods have a weak point in
that they do not measure the ability to find EDUs
exactly. We suggest that the discourse segmenta-
tion task should be measured on EDUs rather than
boundaries of EDUs. Under this evaluation scheme,
our model achieved 90.0% and 86.2% when using
164
Table 2: Performance when evaluating on B and C labels
Model Trees Pre(%) Re(%) F1(%)
CRFSeg Penn 96.0 94.6 95.3
Base Penn 96.0 96.0 96.0
Reranking Penn 96.3 96.9 96.6
CRFSeg Stanford 95.0 93.2 94.1
Base Stanford 95.3 94.7 95.0
Reranking Stanford 95.4 94.9 95.1
gold parse trees and Stanford parse trees, respec-
tively.
We do not compare our segmenter to systems de-
scribed in Thanh et al (2004) and Tofiloski et al
(2009). Thanh et al (2004) evaluated their system
on only 8 texts of RST-DT with gold standard parse
trees. They achieved 81.4% and 79.2% in the preci-
sion and recall scores, respectively. Tofiloski et al
(2009) tested their system on only 3 texts of RST-DT
and used different segmentation guidelines. They
reported a precision of 82.0% and recall of 86.0%
when using Stanford parse trees.
An important question is which subtree features
were useful for the reranking model. This question
can be answered by looking at the weights of sub-
tree features (the parameter vector learned by the
average perceptron algorithm). Table 3 shows 30
subtree features with the highest weights in absolute
value. These features are thus useful for reranking
candidates in the reranking model. We can see that
most subtree features at the top are splitting trees,
so splitting trees have a more important role than
bound trees in our model. Among three types of
subtrees (left tree, right tree, and full tree), full tree
is the most important type. It is understandable be-
cause subtrees in this type convey much informa-
tion; and therefore describe splitting trees and bound
trees more precise than subtrees in other types.
4.3 Error Analysis
This section discusses the cases in which our model
fails to segment discourses. Note that all errors be-
long to one of two types, over-segmentation type
(i.e., words that are not EDU boundaries are mis-
taken for boundaries) and miss-segmentation type
(i.e., words that are EDU boundaries are mistaken
for not boundaries).
Table 4: Top error words
Word Percentage among all errors (%)
to 14.5
and 5.8
that 4.6
the 4.6
? 3.5
he 2.3
it 2.3
of 2.3
without 2.3
? 1.7
as 1.7
if 1.7
they 1.7
when 1.7
a 1.2
Tabel 4 shows 15 most frequent words for which
our model usually makes a mistake and their per-
centage among all segmentation errors. Most errors
are related to coordinating conjunctions and subor-
dinators (and, that, as, if, when), personal pronouns
(he, it, they), determiners (the, a), prepositions (of,
without), punctuations (quotes and hyphens), and
the word to.
Figure 5 shows some errors made by our model.
In these examples, gold (correct) EDU boundaries
are marked by bracket squares ([]), while predicted
boundaries made by our model are indicated by ar-
rows (? or ?). A down arrow (?) shows a boundary
which is predicted correctly, while an up arrow (?)
indicates an over-segmentation error. A boundary
with no arrow means a miss-segmentation error. For
example, in Sentence 1, we have a correct boundary
and an over-segmentation error. Sentences 2 and 3
show two over-segmentation errors, and sentences 4
and 6 show two miss-segmentation errors.
We also note that many errors occur right after
punctuations (commas, quotes, hyphens, brackets,
and so on). We analyzed statistics on words that
appear before error words. Table 5 shows 10 most
frequent words and their percentage among all er-
rors. Overall, more than 35% errors occur right after
punctuations.
165
Table 3: Top 30 subtree features with the highest weights
Type of tree Type of subtree Subtree feature Weight
Splitting tree Full tree NP###NP-VP 23.0125
Splitting tree Full tree VP###S-VP 19.3044
Splitting tree Full tree NP###VBN 18.3862
Splitting tree Right tree VP -18.3723
Splitting tree Full tree NP###SBAR 17.7119
Splitting tree Full tree NP###NP-SBAR 17.0678
Splitting tree Full tree NP###, -16.6763
Splitting tree Full tree NP###VP 15.9934
Splitting tree Left tree NP-VP 15.2849
Splitting tree Full tree NP###NP 15.1657
Splitting tree Right tree SBAR 14.6778
Splitting tree Full tree NP###S-NP 14.4962
Splitting tree Full tree NP###S 13.1656
Bound tree Full tree S-PP###, 12.7428
Splitting tree Full tree NP###NP-VP-VBN 12.5210
Bound tree Full tree NP###NP -12.4723
Bound tree Full tree VP###VP -12.1918
Splitting tree Full tree NP-VP###S 12.1367
Splitting tree Right tree NP-VP 12.0929
Splitting tree Full tree NP-SBAR###VP 12.0858
Splitting tree Full tree NP-SBAR-S###VP 12.0858
Splitting tree Full tree VP###VP-VP -12.0338
Bound tree Full tree VBG###. 11.9067
Bound tree Right tree : 11.8833
Bound tree Full tree VP###S -11.7624
Bound tree Full tree S###VP -11.7596
Bound tree Full tree ?###? 11.5524
Bound tree Full tree S###, 11.5274
Splitting tree Full tree NP###VP-VBN 11.3342
Bound tree Left tree 0 11.2878
Figure 5: Some errors made by our model.
166
Table 5: Most frequent words that appear before error
words
Word Percentage among all errors (%)
, 24.9
? 5.2
? 2.3
time 1.7
) 1.2
assets 1.2
investors 1.2
month 1.2
plan 1.2
was 1.2
5 Conclusion
This paper presented a reranking model for the dis-
course segmentation task. Our model exploits sub-
tree features to rerank N-best outputs of a base
model, which uses CRFs to learn. Compared with
the state-of-the-art system, our model reduces 2.5%
among 8.8% errors (28.4% in the term of error rate)
when using gold parse trees, and reduces 2% among
11% errors (18.2% in the term of error rate) when
using Stanford parse trees. In the future, we will
build a discourse parser that uses the described dis-
course segmenter.
Acknowledgments
This work was partially supported by the 21st Cen-
tury COE program ?Verifiable and Evolvable e-
Society?, and Grant-in-Aid for Scientific Research,
Education and Research Center for Trustworthy e-
Society.
The authors would like to thank the three anony-
mous reviewers for the time they spent reading and
reviewing this paper and Michael Strube for his
comments during the revision process of the paper.
References
J. Bateman, J. Kleinz, T. Kamps, and K. Reichenberger.
2001. Towards Constructive Text, Diagram, and Lay-
out Generation for Information Presentation. Compu-
tational Linguistics, 27(3), pp. 409-449.
L. Carlson and D. Marcu. 2001. Discourse Tagging
Manual. ISI Technical Report, ISI-TR-545.
L. Carlson, D. Marcu, and M.E. Okurowski. 2002.
RST Discourse Treebank. Linguistic Data Consor-
tium (LDC).
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. Thesis, University
of Pennsylvania.
S. Corston-Oliver. 1998. Computing Representations of
the Structure of Written Discourse. Ph.D. Thesis, Uni-
versity of California, Santa Barbara.
M. Collins and T. Koo. 2005. Discriminative Rerank-
ing for Natural Language Parsing. Computational Lin-
guistics, 31(1), pp. 25-70.
A. Fraser, R. Wang, and H. Schu?tze. 2009. Rich Bitext
Projection Features for Parse Reranking. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL), pp. 282-290.
H. Hernault, P. Piwek, H. Prendinger, and M. Ishizuka.
2008. Generating Dialogues for Virtual Agents Using
Nested Textual Coherence Relations. In Proceedings
of IVA, pp. 139-145.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A Se-
quential Model for Discourse Segmentation. In Pro-
ceedings of the 11th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing), pp. 315-326.
L. Huang. 2008. Forest Reranking: Discriminative Pars-
ing with Non-Local Features. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pp. 586-594
D. Klein and C. Manning. 2003. Accurate Unlexicalized
Parsing. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics (ACL),
pp. 423-430.
T. Kudo. CRF++: Yet Another CRF toolkit. Available at
http://crfpp.sourceforge.net/
T. Kudo, J. Suzuki, and H. Isozaki. 2005. Boosting-
based parse reranking with subtree features. In Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pp. 189-
196.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing (ICML), pp.282-289.
A. Louis, A. Joshi, and A. Nenkova 2010. Discourse
indicators for content selection in summarization. In
Proceedings of the 11th annual SIGdial Meeting on
Discourse and Dialogue (SIGDIAL) , pp.147-156.
W.C. Mann and S.A. Thompson. 1988. Rhetorical Struc-
ture Theory. Toward a Functional Theory of Text Or-
ganization. Text 8, pp. 243-281.
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press, Cambridge.
167
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), pp. 149-156.
R. Subba and B. Di Eugenio. 2007. Automatic Discourse
Segmentation using Neural Networks. In Proceedings
of the Workshop on the Semantics and Pragmatics of
Dialogue (SemDial), pp. 189-190.
M. Sun and J.Y. Chai. 2007. Discourse Processing
for Context Question Answering Based on Linguis-
tic Knowledge. Knowledge-Based Systems. 20(6), pp.
511-526.
H.L. Thanh, G. Abeysinghe, and C. Huyck. 2004. Auto-
mated Discourse Segmentation by Syntactic Informa-
tion and Cue Phrases. In Proceedings of IASTED.
M. Tofiloski, J. Brooke, and M. Taboada. 2009. A
Syntactic and Lexical-Based Discourse Segmenter. In
Proceedings of the Joint conference of the 47th Annual
Meeting of the Association for Computational Linguis-
tics and the 4th International Joint Conference on Nat-
ural Language Processing (ACL-IJCNLP), pp. 77-80.
168
