Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 113?116,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Kernels on Linguistic Structures for Answer Extraction
Alessandro Moschitti and Silvia Quarteroni
DISI, University of Trento
Via Sommarive 14
38100 POVO (TN) - Italy
{moschitti,silviaq}@disi.unitn.it
Abstract
Natural Language Processing (NLP) for Infor-
mation Retrieval has always been an interest-
ing and challenging research area. Despite the
high expectations, most of the results indicate
that successfully using NLP is very complex.
In this paper, we show how Support Vector
Machines along with kernel functions can ef-
fectively represent syntax and semantics. Our
experiments on question/answer classification
show that the above models highly improve on
bag-of-words on a TREC dataset.
1 Introduction
Question Answering (QA) is an IR task where the
major complexity resides in question processing
and answer extraction (Chen et al, 2006; Collins-
Thompson et al, 2004) rather than document re-
trieval (a step usually carried out by off-the shelf IR
engines). In question processing, useful information
is gathered from the question and a query is created.
This is submitted to an IR module, which provides
a ranked list of relevant documents. From these, the
QA system extracts one or more candidate answers,
which can then be re-ranked following various crite-
ria. Although typical methods are based exclusively
on word similarity between query and answer, recent
work, e.g. (Shen and Lapata, 2007) has shown that
shallow semantic information in the form of predi-
cate argument structures (PASs) improves the auto-
matic detection of correct answers to a target ques-
tion. In (Moschitti et al, 2007), we proposed the
Shallow Semantic Tree Kernel (SSTK) designed to
encode PASs1 in SVMs.
1in PropBank format, (www.cis.upenn.edu/
?
ace).
In this paper, similarly to our previous approach,
we design an SVM-based answer extractor, that se-
lects the correct answers from those provided by a
basic QA system by applying tree kernel technol-
ogy. However, we also provide: (i) a new kernel
to process PASs based on the partial tree kernel al-
gorithm (PAS-PTK), which is highly more efficient
and more accurate than the SSTK and (ii) a new ker-
nel called Part of Speech sequence kernel (POSSK),
which proves very accurate to represent shallow syn-
tactic information in the learning algorithm.
To experiment with our models, we built two
different corpora, WEB-QA and TREC-QA by us-
ing the description questions from TREC 2001
(Voorhees, 2001) and annotating the answers re-
trieved from Web resp. TREC data (available at
disi.unitn.it/
?
silviaq). Comparative exper-
iments with re-ranking models of increasing com-
plexity show that: (a) PAS-PTK is far more efficient
and effective than SSTK, (b) POSSK provides a re-
markable further improvement on previous models.
Finally, our experiments on the TREC-QA dataset,
un-biased by the presence of typical Web phrasings,
show that BOW is inadequate to learn relations be-
tween questions and answers. This is the reason
why our kernels on linguistic structures improve it
by 63%, which is a remarkable result for an IR task
(Allan, 2000).
2 Kernels for Q/A Classification
The design of an answer extractor basically depends
on the design of a classifier that decides if an an-
swer correctly responds to the target question. We
design a classifier based on SVMs and different ker-
nels applied to several forms of question and answer
113
PAS
A1
autism
rel
characterize
A0
spectrum
PAS
A0
behavior
R-A0
that
rel
characterize
A1
inattention
(a)
PAS
A1
disorder
rel
characterize
A0
anxiety
(b)
PAS
rel
characterize
PAS
A1 rel A0
PAS
A1 rel
characterize
PAS
rel
characterize
A0
rel
characterize
(c)
Figure 1: Compact PAS-PTK structures of s1 (a) and s2 (b) and some fragments they have in common as produced by
the PTK (c). Arguments are replaced with their most important word (or semantic head) to reduce data sparseness.
representations:
(1) linear kernels on the bag-of-words (BOW) or
bag-of-POS-tags (POS) features,
(2) the String Kernel (SK) (Shawe-Taylor and Cris-
tianini, 2004) on word sequences (WSK) and POS-
tag sequences (POSSK),
(3) the Syntactic Tree Kernel (STK) (Collins and
Duffy, 2002) on syntactic parse trees (PTs),
(4) the Shallow Semantic Tree Kernel (SSTK) (Mos-
chitti et al, 2007) and the Partial Tree Kernel (PTK)
(Moschitti, 2006) on PASs.
In particular, POS-tag sequences and PAS trees
used with SK and PTK yield to two innovative ker-
nels, i.e. POSSK and PAS-PTK2. In the next sec-
tions, we describe in more detail the data structures
on which we applied the above kernels.
2.1 Syntactic Structures
The POSSK is obtained by applying the String Ker-
nel on the sequence of POS-tags of a question or
a answer. For example, given sentence s0: What
is autism?, the associated POS sequence is WP
AUX NN ? and some of the substrings extracted by
POSSK are WP NN or WP AUX. A more complete
structure is the full parse tree (PT) of the sentence,
that constitutes the input of the STK. For instance,
the STK accepts the syntactic parse: (SBARQ (WHNP
(WP What))(SQ (VP (AUX is)(NP (NN autism))))(. ?)).
2.2 Semantic Structures
The intuition behind our semantic representation is
the idea that when we ignore the answer to a def-
inition question we check whether such answer is
formulated as a ?typical? definition and whether an-
swers defining similar concepts are expressed in a
2For example, let PTK(t1, t2) = ?(t1) ? ?(t2), where t1
and t2 are two syntactic parse trees. If we map t1 and t2
into two new shallow semantic trees s1 and s2 with a map-
ping ?M (?), we obtain: PTK(s1, s2) = ?(s1) ? ?(s2) =
?(?M (t1)) ? ?(?M (t2)) = ??(t1) ? ??(t2)=PAS-PTK(t1, t2),
which is a noticeably different kernel induced by the mapping
?? = ? ? ?M .
similar way.
To take advantage of semantic representations, we
work with two types of semantic structures; first,
the Word Sequence Kernel applied to both ques-
tion and answer; given s0, sample substrings are:
What is autism, What is, What autism, is autism,
etc. Then, two PAS-based trees: Shallow Seman-
tic Trees for SSTK and Shallow Semantic Trees for
PTK, both based on PropBank structures (Kings-
bury and Palmer, 2002) are automatically generated
by our SRL system (Moschitti et al, 2005). As an
example, let us consider an automatically annotated
sentence from our TREC-QA corpus:
s1: [A1 Autism] is [rel characterized] [A0 by a broad
spectrum of behavior] [R?A0 that] [relincludes] [A1 ex-
treme inattention to surroundings and hypersensitivity to
sound and other stimuli].
Such annotation can be used to design a shallow se-
mantic representation that can be matched against
other semantically similar sentences, e.g.
s2: [A1 Panic disorder] is [rel characterized] [A0 by un-
realistic or excessive anxiety].
It can be observed here that, although autism is a
different disease from panic disorder, the structure
of both definitions and the latent semantics they con-
tain (inherent to behavior, disorder, anxiety) are sim-
ilar. So for instance, s2 appears as a definition even
to someone who only knows what the definition of
autism looks like.
The above annotation can be compactly repre-
sented by predicate argument structure trees (PASs)
such as those in Figure 1. Here, we can notice that
the semantic similarity between sentences is explic-
itly visible in terms of common fragments extracted
by the PTK from their respective PASs. Instead,
the similar PAS-SSTK representation in (Moschitti
et al, 2007) does not take argument order into ac-
count, thus it fails to capture the linguistic ratio-
nale expressed above. Moreover, it is much heavier,
causing large memory occupancy and, as shown by
our experiments, much longer processing time.
114
3 Experiments
In our experiments we show that (a) the PAS-PTK
shallow semantic tree kernel is more efficient and ef-
fective than the SSTK proposed in (Moschitti et al,
2007), and (b) our POSSK jointly used with PAS-
PTK and STK greatly improves on BOW.
3.1 Experimental Setup
In our experiments, we implemented the BOW and
POS kernels, WSK, POSSK, STK (on syntactic
PTs derived automatically with Charniak?s parser),
SSTK and PTK (on PASs derived automatically with
our SRL system) as well as their combinations in
SVM-light-TK3. Since answers often contain more
than one PAS (see Figure 1), we sum PTK (or SSTK)
applied to all pairs P1?P2, P1 and P2 being the sets
of PASs of the first two answers.
The experimental datasets were created by sub-
mitting the 138 TREC 2001 test questions labeled as
?description? in (Li and Roth, 2002) to our basic QA
system, YourQA (Quarteroni and Manandhar, 2008)
and by gathering the top 20 answer paragraphs.
YourQA was run on two sources: Web docu-
ments by exploiting Google (code.google.com/
apis/) and the AQUAINT data used for TREC?07
(trec.nist.gov/data/qa) by exploiting Lucene
(lucene.apache.org), yielding two different cor-
pora: WEB-QA and TREC-QA. Each sentence of
the returned paragraphs was manually evaluated
based on whether it contained a correct answer to
the corresponding question. To simplify our task,
we isolated for each paragraph the sentence with the
maximal judgment (such as s1 and s2 in Sec. 2.2)
and labeled it as positive if it answered the question
either concisely or with noise, negative otherwise.
The resulting WEB-QA corpus contains 1309 sen-
tences, 416 of which positive; the TREC-QA corpus
contains 2256 sentences, 261 of which positive.
3.2 Results
In a first experiment, we compared the learning and
classification efficiency of SVMs on PASs by apply-
ing either solely PAS-SSTK or solely PAS-PTK on
the WEB-QA and TREC-QA sets. We divided the
training data in 9 bins of increasing size (with a step
3Toolkit available at dit.unitn.it/moschitti/, based
on SVM-light (Joachims, 1999)
0
20
40
60
80
100
120
140
160
180
200
220
240
200 400 600 800 1000 1200 1400 1600 1800
Training Set Size
Ti
m
e 
in
 S
ec
on
ds
PTK (training) PTK (test)
SSTK (test) SSTK (training)
Figure 2: Efficiency of PTK and SSTK
60
61
62
63
64
65
66
67
68
69
1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0
cost-factor
F1
-m
ea
su
re
PT+WSK+PAS(PTK) PT
PT+BOW PT+POS
PT+WSK WSK
BOW PT+WSK+PAS(SSTK)
Figure 3: Impact of different kernels on WEB-QA
20
22
24
26
28
30
32
34
36
38
40
4 6 8 10 12 14 16 18 20
cost-factor
F1
-m
ea
su
re
PT POS+PT
POSSK+PT POSSK+PT+PAS-PTK
BOW+PT BOW+POS+PT
BOW POSSK+PT+PAS-SSTK
Figure 4: Impact of different kernels on TREC-QA
of 200) and measured the training and test time4 for
each bin. Figure 2 shows that in both the test and
training phases, PTK is much faster than SSTK. In
training, PTK is 40 times faster, enabling the exper-
imentation of SVMs with large datasets. This differ-
ence is due to the combination of our lighter seman-
tic structures and the PTK?s ability to extract from
these at least the same information that SSTK de-
rives from much larger structures.
Further interesting experiments regard the accu-
4Processing time in seconds of a Mac-Book Pro 2.4 Ghz.
115
racy tests of different kernels and some of their most
promising combinations. As a kernel operator, we
applied the sum between kernels5 that yields the
joint feature space of the individual kernels (Shawe-
Taylor and Cristianini, 2004).
Figure 3 shows the F1-plots of several kernels ac-
cording to different cost-factor values (i.e. different
Precision/Recall rates). Each F1 value is the average
of 5 fold cross-validation. We note that (a) BOW
achieves very high accuracy, comparable to the one
produced by PT; (b) the BOW+PT combination im-
proves on both single models; (c) WSK improves on
BOW and it is enhanced by WSK+PT, demonstrat-
ing that word sequences and PTs are very relevant
for this task; (d) both PAS-SSTK and PAS-PTK im-
prove on previous models yielding the highest result.
The high accuracy of BOW is surprising as sup-
port vectors are compared with test examples which
are in general different (there are no questions
shared between training and test set). The explana-
tion resides in the fact that WEB-QA contains com-
mon BOW patterns due to typical Web phrasings,
e.g. Learn more about X, that facilitate the de-
tection of incorrect answers.
Hence, to have un-biased results, we experi-
mented with the TREC corpus which is cleaner from
a linguistic viewpoint and also more complex from
a QA perspective. A comparative analysis of Fig-
ure 4 suggests that: (a) the F1 of all models is much
lower than for the WEB-QA dataset; (b) BOW de-
notes the lowest accuracy; (c) POS combined with
PT improves on PT; (d) POSSK+PT improves on
POS+PT; (f) finally, PAS adds further information
as the best model is POSSK+PT+PAS-PTK (or PAS-
SSTK).
4 Conclusions
With respect to our previous findings, experimenting
with TREC-QA allowed us to show that BOW is not
relevant to learn re-ranking functions from exam-
ples; indeed, while it is useful to establish an initial
ranking by measuring the similarity between ques-
tion and answer, BOW is almost irrelevant to grasp
typical rules that suggest if a description is valid or
not. Moreover, using the new POSSK and PAS-PTK
5All adding kernels are normalized to have a similarity score
between 0 and 1, i.e. K?(X1,X2) = K(X1,X2)?K(X1,X1)?K(X2,X2) .
kernels provides an improvement of 5 absolute per-
cent points wrt our previous work.
Finally, error analysis revealed that PAS-PTK can
provide patterns like A1(X) R-A1(that) rel(result)
A1(Y) and A1(X) rel(characterize) A0(Y), where X
and Y need not necessarily be matched.
Acknowledgments
This work was partly supported by the FP6 IST LUNA
project (contract No. 33549) and by the European
Commission Marie Curie Excellence Grant for the
ADAMACH project (contract No. 022593).
References
J. Allan. 2000. Natural language processing for informa-
tion retrieval. In Proceedings of NAACL/ANLP (tuto-
rial notes).
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
ACL?06.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL?02.
K. Collins-Thompson, J. Callan, E. Terra, and C. L.A.
Clarke. 2004. The effect of document retrieval quality
on factoid QA performance. In SIGIR?04.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In LREC?02.
X. Li and D. Roth. 2002. Learning question classifiers.
In ACL?02.
A. Moschitti, B. Coppola, A. Giuglea, and R. Basili.
2005. Hierarchical semantic role labeling. In CoNLL
2005 shared task.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting syntactic and shallow semantic
kernels for question/answer classification. In ACL?07.
A. Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In
ECML?06.
S. Quarteroni and S. Manandhar. 2008. Designing an
interactive open domain question answering system.
Journ. of Nat. Lang. Eng. (in press).
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In EMNLP-CoNLL.
E. M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. In TREC?01.
116
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 41?44,
Suntec, Singapore, 3 August 2009.
c
?2009 ACL and AFNLP
Combining POMDPs trained with User Simulations and
Rule-based Dialogue Management in a Spoken Dialogue System
Sebastian Varges, Silvia Quarteroni, Giuseppe Riccardi, Alexei V. Ivanov, Pierluigi Roberti
Department of Information Engineering and Computer Science
University of Trento
38050 Povo di Trento, Italy
{varges|silviaq|riccardi|ivanov|roberti}@disi.unitn.it
Abstract
Over several years, we have developed an
approach to spoken dialogue systems that
includes rule-based and trainable dialogue
managers, spoken language understanding
and generation modules, and a compre-
hensive dialogue system architecture. We
present a Reinforcement Learning-based
dialogue system that goes beyond standard
rule-based models and computes on-line
decisions of the best dialogue moves. The
key concept of this work is that we bridge
the gap between manually written dia-
log models (e.g. rule-based) and adaptive
computational models such as Partially
Observable Markov Decision Processes
(POMDP) based dialogue managers.
1 Reinforcement Learning-based
Dialogue Management
In recent years, Machine Learning techniques,
in particular Reinforcement Learning (RL), have
been applied to the task of dialogue management
(DM) (Levin et al, 2000; Williams and Young,
2006). A major motivation is to improve robust-
ness in the face of uncertainty, for example due
to speech recognition errors. A further motivation
is to improve adaptivity w.r.t. different user be-
haviour and application/recognition environments.
The Reinforcement Learning framework is attrac-
tive because it offers a statistical model represent-
ing the dynamics of the interaction between sys-
tem and user. This is in contrast to the super-
vised learning approach of learning system be-
haviour based on a fixed corpus (Higashinaka et
al., 2003). To explore the range of dialogue man-
agement strategies, a simulation environment is
required that includes a simulated user (Schatz-
mann et al, 2006) if one wants to avoid the pro-
hibitive cost of using human subjects.
We demonstrate the various parameters that in-
fluence the learnt dialogue management policy by
using pre-trained policies (section 4). The appli-
cation domain is a tourist information system for
accommodation and events in the local area. The
domain of the trained DMs is identical to that of a
rule-based DM that was used by human users (sec-
tion 2), allowing us to compare the two directly.
The state of the POMDP keeps track of the SLU
hypotheses in the form of domain concepts (10 in
the application domain, e.g. main activity, star rat-
ing of hotels, dates etc.) and their values. These
values may be abstracted into ?known/unknown,?
for example, increasing the likelihood that the sys-
tem re-visits a dialogue state which it can exploit.
Representing the verification status of the con-
cepts in the state, influences ? in combination with
the user model (section 1.2) and N best hypotheses
? if the system learns to use clarification questions.
1.1 The exploration/exploitation trade-off in
reinforcement learning
The RL-DM maintains a policy, an internal data
structure that keeps track of the values (accumu-
lated rewards) of past state-action pairs. The goal
of the learner is to optimize the long-term reward
by maximizing the ?Q-Value?Q
pi
(s
t
, a) of a policy
pi for taking action a at time t. The expected cu-
mulative value V of a state s is defined recursively
as V
pi
(s
t
) =
?
a
pi(s
t
, a)
?
s
t+1
P
a
s
t
,s
t+1
[R
a
s
t
,s
t+1
+ ?V
pi
(s
t+1
)].
Since an analytic solution to finding an optimal
value function is not possible for realistic dialogue
scenarios, V (s) is estimated by dialogue simula-
tions.
To optimize Q and populate the policy with ex-
pected values, the learner needs to explore un-
tried actions (system moves) to gain more expe-
riences, and combine this with exploitation of the
41
??
?
?
?
?
?
?
??
??
??
??????
?
??
??
??
?
?
?
??
?
?
?
??
?
?
???
???
??
?
??
?
?
?
?
??
??
?
???????
?
????
?
??
?
?
?
?
?????
?
?
?
?
??
???
?
?????
?
???
??
?
?
?
?
?
?
??
??
??
??
?
?
??
?
?
?
?
??
??
????
?
??
????
??
????
??
?
??
???
???
??
?
?
???
??
????
????
?
??
?
?
?
?
?
??
?
????
???
?
??
?
???
???
???
??
?
????
?
??
?
??
?
?
?
?
?
?
????
?
??
??
??
?
???
??????
??
??
?
??????
??
?
?
??
?????????
?
?
?????
???
?
?
?
??
???
??
?
??????
??
?
?
?
????
?
???
?
?
???????
?
??
?
??
?
?
??
?
??
???
?
?
?????
??????
?
???
?
?
?
?????
??
????
????
?
?
??
?
????
?
??
?
?
?????
??
?
??????????
??
?
?
???
??
?
??????????
?
?
???
??
????
?????????
?
?
???
????
?
?????
??
??
??
???
?????
?
???
???
??
????
?
??
??
?????
????
?????
?
??????
?
????
?????
?
??
???
???
??????????
???
?
??
?
??????
??
????
?????????
?
?????????
????
????
????????
?????
?
???????
?
???????
???????
?????
?????
??????
???????
???????
?
???
?
??????????
?
???????
???????????
???
???????????????????
????????
?
??????
??????
?
???????????
?????????
???????
?
?????????
????????
?
???????
?????
????????????
????
???????????
??????
????????
?????
????????????
?
?????
?
????????
?
?????????
?
????
?
????
?
?????
?
????????
???????
?????????????
?????????
?
?????
?????
?
????????
??????????
?
?????
????????
????
??????
??????
??????????
????????
????????
?
????????
????
?????
?
????
?
?????
?
????
?
???????????
?????
????????
?
??????
?
??????????
??????
??????
????????
??????
??????
?
????????????????
???????
?
?????
?
????
?
?????
?
?????????
?
?????
?
????
?
????
???????
?
???????????
?
?????????
???????
??????
?
???????
???????????
?
?????
?????
?
???????????
??????
?
?????
????????
?
???????
????????????????????
?
??????????
??????
????????
???????
????????
???????????
??????
?
??????
?????????
?
??????
???????????????????
?????????
?
?????????
?
???????
?
???????
?
????
?
????????
???????
??????????
?
???????
?
???
?
?????
?
???????
??????????
?
????
??????
???????????
?????
??????
??????
??????????
???????
??????
????????
?????
???????????
??????
?????????
?????
?
??????
?
???????
?
?????
?
???????
?????
???????????????
??????????
?
?
0 2000 4000 6000 8000 10000
?8
?6
?4
?2
0
x8
gree
dy0.0
_fixe
d_er
ror_s
essio
ns10
000_
maxs
essio
nleng
th4_r
uns1
0
rewa
rd 
# sessions 
(a) 0% exploration, 100% exploitation: learner does not find
optimal dialogue strategy
????
?
?
?
?
?
?
?
?
?
?
??
?
???
?
??
?
?
?
??
?
?
?
?
??
???
?
?
?
???
??
???
??
?
?
?
??
???
???
?
??
?
?
?
??
?
?
???
?
????????
???
?
?
???
?
??
???
????
?
?
??
?
??
??
?
?
?
??
?
??
??
?
?
?
?
?
??
?
??
??
?
??
?
?
????
??
?
?
??
?
?
??
?
??
?
??
???
??
?
?
??
???
?
?
?
?
?
?
?
??
?
???
?
???
?
?
???
??
??
???
??
???
??
???
??
??
?
?
?
?
?
??
??
???
?
?
?
??
?
???
??
?
??
?
??
?
??????
??
?
?
??
????
?????
??
?
?
???
?
?
?
?
?????
?
???
?
???
??
?
???
??
?
??
??
?
?
??
??
?
?
?
?
??
?
?
?
??
?
???
?
?
?
??
?
?
??
??????
??
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?????
?
?
?
?
??
?
???
?
?
?
?
?
?
??????
?
?
?
??
?????
?
?
?
?
??
?
??
??
?????
???
??
?
??
??
?
?
??
??
????
?
??
?
?
?
?
?
???
?
??
?
?
??
?
??????
????
?
??
?
?
?
?
??
?
?
?
??
?
??
???
????
?
?
???
?
??
?
?
?
?
?
?
?
??
?
?
????
???
?
?
?
???
?
????
?????
???
?
???
?
???
??
?
?
?
??
???
?
?
???
???
?
?
???
?
?
??
?
?
??
?
??
?
??
?
?
?
??
?
???
??
?
?
?
???
?
?
???
?
?
?
??
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
???
??
?
?
???
?
?
?
?
??
?
?
?
?
?
?
?
?
????
?
?
????
????
?
?
?
??
??
???
?
?
?
?
?
?
???
?
?
?
?
?
?
?
?
???
?
?
?
??
??
??
??
??
?????
?
??
?
???
?
?
?
?
??
?
?
?
????
??
?
?
?
????
?
???
????
?
?
??
??
?
?
?
?
??
?
??
????
?
?
??
?
???
??
?
??
???
?
??
?
?
?
?
?
?
??
?
?
????
??
??
??
??
????
?
?
??
?
??
?
?
?
?
?
?
?
??
?
?
???
?
?
?
?
?
?
???
?
?
?
?
?
?
??
?
?
??
?
?
????
?
?
?
??
?
??
?
?
?
??
?
?
???
?
?
?
?
??
?
?
?
?
??
?
??????
?
??
?
?
??
?
?
?
???
??
?
?
?
?
?
?
???
?
?
?
?
?
?
?
??
??
?
?
?
??
???
???
?
?
?
?
??
?
?
?
?
?
??
?
?
?
?
?
?
??
???
?
?
?
?
??
??
?
?
??
?
?
??
??
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
????
?
?
??
?
?
?
?
????
?????
???
?
??
?
?
??
??
?
?
?
?
???
?
??
?
?
?
??
?
????
?
??
?
?
?
?
??
??
?
??
?
???
?
?
?
?
?
??
?
?
??
?
???
?
?
?
?
?
??
?
?
?
?
?
???
?
?
?
?
?
????
?
?
?
?
??
??
?
???
??
?
???
?
?
?
?
???
?
??
??
?
?
?
??
?
??
?
?
?
?
?
?
???
??
?
?????
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
?
??
?
?
?
??
?
?
?
?
?
???
?
?
?
?
?
??
????
??
?
?
??
?
?
?
??
?
?
?
????
??
?
?
?
?
???
?
?
???
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????
?
?????
????
??
?
?
???
?
?
?
?
?
?
?
?
?
??
?
?
??
????
?
????
?
????
?
?
??
?
?
?
??
?
???
?
?
??
?
?
?
?
?
???
??????
?
?
?
?
??
?
?
?
???
?
??
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
??
??
?
?
?
?
?
?
?
??
?
?
????
?
?
?
?
??
???
??
?????
?
?
?
??
?
?
?
?
?
?
?
??
?????
?
?
?
??
?
?
?
?
?
??
??
?
?
??
?
?
??
??
?
?
?
?
?
?
?
??
?
?
?
??
??
?
?
?
?
?
??
?
???
?
??
??
?
?
???
??
?
?
???
?
?
?
???
?
?
???
??
??
?
?
??
?
??
???
??
?
?
?
?
?
?
?
?????
?
?
??
???
??
?
?
?
?
?
???
??
??
????
???
?
?
?
?
??
?
?
?
?
?
??
?
?
???
?
??
??
?
?
?
??
??
?
??
??
?
??
??
???????
??
?
?
?
?
??
??
??
?
?
??
???
??
?
???
?
??
??
??
?
?
??????
??
?
???
????
?
????
??
?
??
??
?
?
??
?
?
???
?
?
??
??
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????
?
?
?
??
??
??
??
?
?
??
???
??
????????
??
??
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
???????
??
???
?
?
?
?
?
??????
?
?
?
?
?
?
??
??
?
?
??
?
?
??
?
?
?
?
?
?
?
??
??
???
?
?
?
?
??
?
?
??
?
??
?
??
?
?
????
?
??
?
????
?
?
???
?
?
?
?
?
?
?
????
?
?
???
??
?
?
??
?
?
?
?
??
?
?
?
????
?
?
????
?
?
?
?
?
???
??
?
?
????
??
??
???
?
?
?
??
?
????
???
?
?
?
?
?
??
??
?
?
????
?
?
?
??
?
?
??
??
?
?????
???
?
?
?
?
?
????
?
??
?
??
??
?
?????
????
?
?????
?
?
?
???
??
?
????
???
?
??
?
??
?
???
?
?
?
?
?
?????
?
?
??
??
??
?
?
???
?
?
??
??
?
?
?
?????
?
????
?
??
?
?
?
?
??
?
???
??
??
??
?
?
?
?
???
?
?
?????????
?
?
??
?
?
?
?
???
?
?
?
??
?
?
?
??
??
???
???
?
??
?
?
?
???
?
?
?
?
??
?
?
?
?
?
??
?
??
??
?
?
?
?
?
?
??
?
?
???
??
??
??
?
?
?
?
???????
??
?
??
?
?
?
??
??
??
?
?
?
?
??
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
??
?
?
??
?
?
?
?
?
?
??
??
??
?
?
?
?
??
?
?
??
??
??
?
??
??
??
?
?
?
??
????
?
?
?
?
???
?
?
?
?
?
?
?
??
?
?
?
?
??
?
?
??
?
??
??
???
?
??
??
????
?
?
?
?
?
??
???
?
?
?
??
?
???
???
?
?
?
?
?
?
?
??
?
?
?
?
?
????
??
?
??
?
?
?
?
??
??
?
??
??
?
?
?
????
??????
??
???
?
???
?
??
?
?
?
?
?
?
??
??
???
?
?
?
?
??
?
?
?
????
?
??
?
?
?
??
??
???
?
?
?
??
???
?
???
?
???
????
??
??
?
??
?
??
?
?
??
?
??
?
??
?
?
?
?
??
???
?????
???
?
?
?
?
?
???
?
?
?
?
?
?
?
?
?
?
???
?
?
??
????
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
??
????
??
??
?
?
?
???
???
?
?
?
?
?
?
?
?
???
?
??
?
?
?
??
?
?
?
??
?
???
??
?
?
?
????
?
??
??
?????
?
???
?
????
?
???
?
?
?
???
?
?
?
??
?
?
?
?
?
???
??
??
?
????
?
?
?
?
?
???
?
?
?
?
?
??
?
??
?
?
?
?
?
??
??
?
?
?
?
??
?
??
???
?
?
?
??
??
??
?
?
??
?
??
?
??
?
?
?
?
??
?
?
???
????
?
?
?????
??
?
?
?
???
?
????
???
?
??
?
???
?
?
?
?
??
?
??
??
?
?
???
?
??
?
??
???
?
??
?
?
?
?
??
?
?
??
?
?
?
?
????
?
?
?
???
?
?
??
??
?
??
?
?
?
?
?
?
?
?
???
?
?
?
?
?
?
?
??
?
?
???
?
???
?
?
?
?
?
??
?
???
?
?
?
?
??
?
?
?
?
???
?
?
??
?
???
???
??
?
?
?
?
?
?
?
?
??
?
??
?
????
??
?
?
?
?
?
?
???
?
?
?
??
??
??
?
???
?
???
???
??
?
??
??
??????
?
?????
??
?
??????
?
??
?
?
???
???
??
?
??
?
?
?
?
?
??
?
?
?
?
?
???
???
?
??
?
?
?
?
?????
???
???
?
??
?
???
???
?
??
?
?
?
?
??
????
??
?
??
?
??
?
???
???
???
?????
??
??
?
?
??
?
?
???
??
?
??
?
?
???
?
?
?
?
????
?
?
?
?
???
????
?
?
??
??
??
??
?
?
??
?
?
?
?
?
?
?
?
?
?
???
?
?
?
?
?
?
????
?????
??
???
?
??
???
???
??
?
??
?
?
??
?
???????
?
???
?
?
??
?
??
??
??
??
?
???
?
?
?
???
?
?
?
?
?
??
?
?
??
?
?
??
?
?
??
?
?
?
??
??
?
?
?
?
???
?
?
?
??
?
?
????
??
?
?
?
?
?
??
??
??
?
???
??
????
?
???????
?
?
?
?
????
?
?
?
???
?
?
?
?
??
?
??
?
??
?
?
?
???
?
?
?
?
???????
?
?
?
?
?
?
?
??
??
?
?
?
?
???
?
???
?
??
???
?
??
?
?
?
????
?
?
?
?
???
?
?
?
?
?
?
?
??
????
?
????
??
?
?
????
?
?
?
?
?
?
??
?
??
???
?
?
?
?
??
?
?
??
??
??
???
???
????
?
?
?????
??
??
??
?
?
?
?
?
??
?
?????
?
?
?
?
?
?
?
?
???
?
??
?
?
?
?
?
??
?
??
?
?
???
??
??
?????
?
??
?
?
?
?
?
??
?
?
?
??
???
?
??
?
?
?
??
?
??
?
?
?
?
?
??
?
?
?
??
??
?
?
?
?
???
??
?
?
?
?
?
?
??
?
?
?
?
?
???
?
??
?
????
?
?
?
??
?????
?
?
???
??
?
?
??
?
?
?
?
??
?
?
?
?
??
??
?
?
???
?
?
??
?
?
???
???
?
?
??
??
?
?
??
?
?
?
?
???
?
?
?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
???
??
?
?
??
?
?
?
??
?
?
?
?
???
?
?
??
????
?
???
???
???
?
?
?
??
?
?
???
?
?
??
?
?
???
??
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?????
?????
?
?
??
?
??
?
??
?????
?
?
?
??
???
?
?
??
??
?
?
??
?
?
??
?
?
?
?
?
?
?
?
?
??
?
??
?
?
??
??
???
?
?
?
?
?
?
????
???
?
?
?
?
???
?
?
?
????
????
?
??
???
?
??
??
?
?
??
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
???
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?????
?
?
?
?
?
?
?
?
?
?
??
??
?
?
?
?
?
?
???
??
?
?
?
???
?
?
?
??????
?
?
??
?
?
?
?
????
?
?
???
??
??
??
???
?
?
??
?
???
?
?
?
?
??
??
?
??
?
?
?
?
??
?
?
??
?
?
??
???
?
?
??
?
?
???
??
?
??
?
?
?
?
?
??
?
?
????
??
?
?
??
?
0 2000 4000 6000 8000 10000
?8
?6
?4
?2
0
x8
gree
dy0.2
_fixe
d_er
ror_s
essio
ns10
000_
maxs
essio
nleng
th4_r
uns1
0
# sessions 
rewa
rd 
(b) 20% exploration, 80% exploitation: noticeable increase in
reward, hitting upper bound
Figure 1: Exploration/exploitation trade-off
already known successful actions to also ensure
high reward. In principle there is no distinction
between training and testing. Learning in the RL-
based dialogue manager is strongly dependent on
the chosen exploration/exploitation trade-off. This
is determined by the action selection policy, which
for each system turn decides probabilistically (-
greedy, softmax) if to exploit the currently known
best action of the policy for the believed dialogue
state, or to explore an untried action. Figure 1(a)
shows, for a subdomain of the application domain,
how the reward (expressed as minimizing costs)
reaches an upper bound early during 10,000 sim-
ulated dialogue sessions (each dot represents the
average of 10 rewards at a particular session num-
ber). Note that if the policy provides no matching
state, the system can only explore, and thus a cer-
tain amount of exploration always takes place. In
contrast, with exploration the system is able to find
lower cost solutions (figure 1(b)).
1.2 User Simulation
In order to conduct thousands of simulated dia-
logues, the DM needs to deal with heterogeneous
but plausible user input. For this purpose, we have
designed a User Simulator (US) which bootstraps
likely user behaviors starting from a small corpus
of 74 in-domain dialogs, acquired using the rule-
based version of the SDS (section 2). The task of
the US is to simulate the output of the SLU mod-
ule to the DM, hence providing it with a ranked
list of SLU hypotheses.
A list of possible user goals is stored in a
database table (section 3) using a frame/slot rep-
resentation. For each simulated dialogue, one or
more user goals are randomly selected. The User
Simulator?s task is to mimic a user wanting to per-
form such task(s). At each turn, the US mines the
previous system dialog act to obtain the concepts
required by the DM and obtains the corresponding
values (if any) from the current user goal.
The output of the user model proper is passed
to an error model that simulates the ?noisy chan-
nel? recognition errors based on statistics from the
dialogue corpus. These concern concept values as
well as other dialogue phenomena such as noIn-
put, noMatch and hangUp. If the latter phenomena
occur, they are propagated to the DM directly; oth-
erwise, the following US step is to attach plausible
confidences to concept-value pairs, also based on
the dialogue corpus. Finally, concept-value pairs
are combined in an SLU hypothesis and, as in the
regular SLU module, a cumulative utterance-level
confidence is computed, determining the rank of
each of the n hypotheses. The probability of a
given concept-value observation at time t+1 given
the system act at time t, named a
s,t
, and the ses-
sion user goal g
u
, P (o
t+1
|a
s,t
, g
u
), is obtained by
combining the error model and the user model:
P (o
t+1
|a
u,t+1
) ? P (a
u,t+1
|a
s,t
, g
u
)
where a
u,t+1
is the true user action.
2 Rule-based Dialogue Management
A rule-based dialogue manager was developed as a
meaningful comparison to the trained DM, to ob-
tain training data from human-system interaction
for the user simulator, and to understand the prop-
erties of the domain (Varges et al, 2008). Rule-
based dialog management works in two stages:
retrieving and preprocessing facts (tuples) taken
from a dialogue state database (section 3), and
inferencing over those facts to generate a system
response. We distinguish between the ?context
model? of the first phase ? essentially allowing
42
more recent values for a concept to override less
recent ones ? and the ?dialog move engine? (DME)
of the second phase. In the second stage, accep-
tor rules match SLU results to dialogue context,
for example perceived user concepts to open ques-
tions. This may result in the decision to verify the
application parameter in question, and the action
is verbalized by language generation rules. If the
parameter is accepted, application dependent task
rules determine the next parameter to be acquired,
resulting in the generation of an appropriate re-
quest.
3 Data-centric System Architecture
All data is continuously stored in a database which
web-service based processing modules (such as
SLU, DM and language generation) access. This
architecture also allows us to access the database
for immediate visualization. The system presents
an example of a ?thick? inter-module informa-
tion pipeline architecture. Individual components
exchange data by means of sets of hypotheses
complemented by the detailed conversational con-
text. The database concentrates heterogeneous
types of information at various levels of descrip-
tion in a uniform way. This facilitates dialog eval-
uation, data mining and online learning because
data is available for querying as soon as it has
been stored. There is no need for separate logging
mechanisms. Multiple systems/applications are
available on the same infrastructure due to a clean
separation of its processing modules (SLU, DM,
NLG etc.) from data storage (DBMS), and moni-
toring/analysis/visualization and annotation tools.
4 Visualization Tool
We developed a live web-based dialogue visual-
ization tool that displays ongoing and past di-
alogue utterances, semantic interpretation confi-
dences and distributions of confidences for incom-
ing user acts, the dialogue manager state, and
policy-based decisions and updating. An exam-
ple of the visualization tool is given in figures 3
(dialogue logs) and 4 (annotation view). We are
currently extending the visualization tool to dis-
play the POMDP-related information that is al-
ready present in the dialogue database.
The visualization tool shows how our dedicated
SLU module produces a number of candidate se-
mantic parses using the semantics of a domain on-
tology and the output of ASR.
The visualization of the internal representation
of the POMDP-DM includes the N best dialogue
states after each user utterance and the reranking
of the action set. At the end of each dialogue ses-
sion, the reward and the policy updates are shown,
i.e. new or updated state entries and action val-
ues. Another plot relates the current dialogue?s
reward to the reward of previous dialogues (as in
plots 1(b) and 1(a)).
Users are able to talk with several systems
(via SIP phone connection to the dialogue system
server) and see their dialogues in the visualization
tool. They are able to compare the rule-based
system, a randomly exploring learner that has
not been trained yet, and several systems that
use various pre-trained policies. These policies
are obtained by dialogue simulations with user
models based on data obtained from human-
machine dialogues with the original rule-based
dialogue manager. The web tool is available
at http://cicerone.dit.unitn.it/
DialogStatistics/.
Acknowledgments
This work was partially supported by the Euro-
pean Commission Marie Curie Excellence Grant
for the ADAMACH project (contract No. 022593)
and by LUNA STREP project (contract No.
33549).
References
R. Higashinaka, M. Nakano, and K. Aikawa. 2003.
Corpus-based discourse understanding in spoken di-
alogue systems. In ACL-03, Sapporo, Japan.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21(2):97?126.
S. Varges, G. Riccardi, and S. Quarteroni. 2008. Per-
sistent Information State in a Data-Centric Architec-
ture. In SIGDIAL-08, Columbus, Ohio.
J. D. Williams and S. Young. 2006. Partially Ob-
servable Markov Decision Processes for Spoken Di-
alog Systems. Computer Speech and Language,
21(2):393?422.
43
ASR
TTS
Turn 
Setup
DB
SLU
DM
NLG
http-req
http-req
http-req
http-req
Ids
VXML
   page
ASR results
        HTTP request 
SLU results
DM context/results
VXMLgen
http-req
NLG context/results
(a) Turn-level information flow in the data-centric SDS ar-
chitecture
DB
Simulation
Environment
DM
NLG
User Model
Error Model
User Goals
Corpus
(b) User simulator interface with the dialogue manager
Figure 2: Architecture for interacting with human user (left) and simulated user (right)
Figure 3: Left pane: overview of all dialogues. Right pane: visualization of a system opening prompt fol-
lowed by the user?s activity request. All distinct SLU hypotheses (concept-value combinations) deriving
from ASR are ranked based on concept-level confidence (2 in this turn).
Figure 4: Turn annotation of task success based on previously filled dialog transcriptions (left box).
44
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 68?71,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Persistent Information State in a Data-Centric Architecture?
Sebastian Varges, Giuseppe Riccardi, Silvia Quarteroni
Department of Information Engineering and Computer Science
University of Trento
38050 Povo di Trento, Italy
{varges|riccardi|silviaq}@disi.unitn.it
Abstract
We present the ADAMACH data centric dia-
log system, that allows to perform on- and off-
line mining of dialog context, speech recog-
nition results and other system-generated rep-
resentations, both within and across dialogs.
The architecture implements a ?fat pipeline?
for speech and language processing. We detail
how the approach integrates domain knowl-
edge and evolving empirical data, based on a
user study in the University Helpdesk domain.
1 Introduction
In this paper, we argue that the ability to store and
query large amounts of data is a key requirement
for data-driven dialog systems, in which the data is
generated by the spoken dialog system (SDS) com-
ponents (spoken language understanding (SLU), di-
alog management (DM), natural language genera-
tion (NLG) etc.) and the world it is interacting
with (news streams, ambient sensors etc.). We
describe an SDS that is built around a database
management system (DBMS), uses the web ser-
vice paradigm (in contrast to the architecture de-
scribed in (Varges and Riccardi, 2007)), and em-
ploys a Voice XML (VXML) server for interfac-
ing with Automatic Speech Recognition (ASR) and
Text-to-Speech (TTS) components. We would like
to emphasize upfront that this does not mean that
we follow a VXML dialog model.
?This work was partially supported by the European Com-
mission Marie Curie Excellence Grant for the ADAMACH
project (contract No. 022593) and by LUNA STREP project
(contract no33549).
The data centric architecture we adopt has sev-
eral advantages: first, the database concentrates het-
erogeneous types of information allowing to uni-
formly query the evolving data at any time, e.g. by
performing queries across various types of infor-
mation. Second, the architecture facilitates dialog
evaluation, data mining and online learning because
data is available for querying as soon as it has been
stored. Third, multiple systems/applications can be
made available on the same infrastructure due to a
clean separation of its processing modules (SLU,
DM, NLG etc.) from data storage and persistency
(DBMS), and monitoring/analysis/visualization and
annotation tools. Fourth, there is no need for sep-
arate ?logging? mechanisms: the state of the SDS
is contained in the database, and is therefore persis-
tently available for analysis after the dialog ends.
As opposed to the presented architecture, the
Open Agent Architecture (OAA) (Martin et al,
1999) and DARPA Communicator (Seneff et al,
1998) treat data as peripheral: they were not specif-
ically designed to handle large volumes of data, and
data is not automatically persistent. In contrast to
the CSLI-DM (Mirkovic and Cavedon, 2005), and
TrindiKit (Larsson and Traum, 2000), but similar
to Communicator, the ADAMACH architecture is
server-based, thus enabling continuous operation.
To prove our concept, we test it on a University
helpdesk application (section 4).
2 Dialog System Architecture
Figure 1 shows our vision for the architecture of the
ADAMACH system. We implemented and evalu-
ated the speech modality based core of this system
68
Figure 1: Architecture vision
(figure 2). A typical interaction is initiated by a
phone call that arrives at an telephony server which
routes it to a VXML platform. A VXML page is
continuously rewritten by the dialog manager, con-
taining the system utterance and other TTS param-
eters, and the ASR recognition parameters for the
next user utterance. Thus, VXML is used as a low-
level interface to the ASR and TTS engines, but not
for representing dialog strategies. Once a user utter-
ance is recognized, a web service request is issued
to a dialog management server.
All communication between the above-mentioned
components is stored in the DBMS: ASR recogni-
tion results, TTS parameters and ASR recognition
parameters reside in separate tables. The dialog
manager uses the basic tables as its communication
protocol with ASR and TTS engines, and addition-
ally stores its Information State (IS) in the database.
This means that the IS is automatically persistent,
and that dialog management becomes a function that
maps ASR results and old IS to the TTS and ASR
parameters and a new IS. The tables of the database
are organized into turns, several of which belong to a
call (dialog), thus resulting in a tree structure that is
enforced by foreign key constraints in the relational
database.
The VXML standard is based on the web infras-
tructure. In particular, a VXML platform can issue
HTTP requests that can be served by a web server
just like any (HTML) page. The VXML server only
sees the generated VXML page, the ?return value?
of the HTTP request. This allows us to organize the
processing modules of the dialog system (SLU, DM,
VXML generator) as web services that are invoked
by the HTTP request. As a consequence, each sys-
tem turn of a dialog is a separate, stateless request.
The state of the dialog is stored in the database.
Furthermore, by threading the VXML session ID
through the processing loop (including the VXML
pages generated on-the-fly) and distinguishing en-
tries in the DB by sessions, the SDS is inherently
parallelizable, just as a conventional web server can
serve many users in parallel. Figure 2 shows how
information is processed for each turn. The HTTP
requests that invoke the processing modules pass on
various IDs and parameters, but the actual data is
stored in the DB and retrieved only if a processing
module requires it. This effectively implements a
?fat pipeline?: each speech, language and DM mod-
ule has access to the database for rescoring and mod-
eling (i.e. data within and across dialogs). At the im-
plementation level, this balances a lightweight com-
munication protocol downstream with data flowing
laterally towards the database.
3 Dialog Management
Dialog management works in two stages: retriev-
ing and preprocessing facts (tuples) taken from the
database, and inferencing over those facts to gen-
erate a system response. We distinguish between
the ?context model? of the first phase and the ?dialog
move engine? (DME) of the second phase (Larsson
and Traum, 2000).
The first stage entails retrieving from the persis-
tent Information State the following information:
all open questions for the current dialog from the
database, any application information already pro-
vided by the user (including their grounding status),
the ASR recognition results of last user turn, and
confidence and other thresholds. The context model
that is applied when retrieving the relevant dialog
history from the database can be characterized as a
?linear default model?: application parameters pro-
vided by the user, such as student ID, are overrid-
den if the user provides a new value, for example to
correct a previous misunderstanding. Task bound-
aries are detected and prevent an application param-
eter from carrying over directly to the new task.
The second stage employs an inference engine
to determine the system action and response: SLU
rules match the user utterance to open questions.
69
Figure 2: Turn-level information flow
This may result in the decision to verify the applica-
tion parameter in question, and the action is verbal-
ized by language generation rules. If the parameter
is accepted, application dependent task rules deter-
mine the next parameter to be acquired, resulting in
the generation of an appropriate request. For reasons
of space, we cannot provide more details here.
4 Experiments
Our current application is a University helpdesk
in Italian which students call to perform 5 tasks:
receive information about exams (times, rooms
. . . ), subscribe/cancel subscriptions to exams, obtain
exammark, or request to talk to an operator. Follow-
ing experimentations, we annotated the dialogs and
conducted performance statistics using the system?s
built-in annotation tool.
Two Italian mothertongues were in charge of
manually annotating a total of 423 interactions.
Each annotator independently annotated each dialog
turn according to whether one of the five available
tasks was being requested or completed in it. To
compute inter-annotator agreement, 24 dialogs were
processed by both annotators; the remaining ones
were partitioned equally among them.
We computed agreement at both turn and dialog
level. Turn level agreement is concerned with which
tasks are requested and completed at a given dia-
log turn according to each annotator. An agree-
ment matrix is compiled where rows and columns
correspond to the five task types in our application.
Cohen?s ? (Cohen, 1960), computed over the turn
matrix, gave a turn agreement of 0.72 resp. 0.77
for requests resp. completions, exceeding the rec-
ommended 0.7 threshold. While turn-level agree-
ment refers to which tasks occurred and at what
turn, dialog level agreement refers to how many task
requests/completions occurred. Also at the dialog
level, the ? statistic gave good results (0.71 for re-
quests and 0.9 for completions).
General dialog statistics The average duration of
the 423 annotated dialogs is 63.1 seconds, with an
average of 7.43 turn (i.e. adjacency) pairs. 356 of
the dialogs contained at least one task; the majority
(338) contained exactly one, 17 dialogs contained 2
tasks, and one dialog contained 3. In the remain-
ing 67 dialogs, no tasks were detected: from the
audio files, it seems that these generally happened
by accident or in noisy environments, hence noin-
put/hangup events occurred shortly after the initial
system prompt.
Furthermore, relative frequencies of task requests
and task completions are reported in Table 1. In to-
tal, according to the two annotators, there were 375
task requests and 234 task completions. Among the
requested tasks, the vast majority was composed by
?Get exam mark? ?a striking 96%? while ?Exam
withdrawal? never occurred and the three others
were barely performed. Indeed, it seems that stu-
dents preferred to use the system to carry on ?in-
formative? tasks such as obtaining exam marks and
general information rather than ?active? tasks such
as exam subscription and withdrawal.
Table 1: Task request and completion frequencies (%)
Task Request Completion
Get exam mark 96 (360) 96.6 (226)
Info on exam 1.9 (7) 1.7 (4)
Exam subscription 1.1 (4) 0.4 (1)
Exam withdrawal 0.0 (0) 0.0 (0)
Talk to operator 1.1 (4) 1.3 (3)
Total 100 (375) 100 (234)
Task and dialog success Based on the annotation
of task requests and completions, we defined task
success as a binary measure of whether the request
of a given task type is eventually followed by a task
completion of the same type. Table 2 reports the av-
erage success of each task type according to the an-
70
notators1. Our results show that the most frequently
requested type, ?Get exammark?, has a 64.64% suc-
cess rate (it seems that failure was mostly due to the
system?s inability to recognize student IDs).
Table 2: Top: annotator (srM ) and automatic (srA) task
success rates. Mean ? binomial proportion confidence
interval on the average task success (?= 95%) is reported.
Bottom: mean annotator (dsrM ) and automatic (dsrA)
dialog success rates ? normal law c.i. (?= 95%).
Task srM (%) srA(%)
Get exam mark 64.64 77.97
Info on exam 57.14 71.43
Exam subscription 25 100
Exam withdrawal - -
Talk to operator 75 75
Average 64.17?4.96 78.06?4.28
Dialog dsrM (%) dsrA(%)
Average 64.47?4.95 88.31?9.2
In fact, while it is straightforward to obtain task
success information using the manual annotation of
dialogs, when the dialog system cannot rely on hu-
man judgments, unsupervised approaches must be
defined for a rapid (on-line or off-line) evaluation.
For this purpose, an automatic approximation of the
?manual? task success estimation has been defined
using a set of database queries associated to each
task type. For instance, the task success query as-
sociated to ?Info on exam? checks that two condi-
tions are met in the current dialog: 1) it includes
a turn where an action is requested the interpreta-
tion of which contains ?information?; 2) it contains
a turn where the concept Exam Name is in focus.
Automatic task success rates have been computed
on the same dialogs for which manual task success
rates were available and are reported in Table 2, col.
2. The comparison shows that the automatic metric
srA is more ?optimistic? than the manual one srM .
Indeed, automatic estimators rely on ?punctual? in-
dicators (such as the occurrence of confirmations of
a given value) in the whole dialog, regardless of the
task they appear in (this information is only avail-
able from human annotation) and also of the order
with which such indicators appear in the dialog.
1As several task types occur seldom, we only report the con-
fidence intervals on the means relating to the overall (?Aver-
age?) task success, computed according to the normal law.
As a by-product of task success evaluation, we de-
fined dialog success rate (dsr) as the average success
rate of the tasks in a dialog: dsr =
?
ti?T
sr(ti)
|T | ,
T being the set of requested tasks. Depending on
whether srM or srA is used, we obtain two metrics,
dsrM resp. dsrA.
Our dialog success results (last row of Table 2) are
comparable to the task success ones; also, the differ-
ence between the automatic and manual estimators
of dialog success is similar to their difference at the
the task level. This is not surprising when consider-
ing that most of the dialogs contained only one task.
5 Conclusions
We have presented a data-centric Spoken Dialog
System whose novel aspect is the storage and re-
trieval of dialog management state, ASR results and
other information in a database. As a consequence,
dialog management can be lightweight and operate
on a turn-by-turn basis, and dialog system evaluation
and logging are facilitated.
Acknowledgments
We would like to thank Pierluigi Roberti for helping
with the speech platform and annotation tools, and
LOQUENDO for providing the VXML platform.
References
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
S. Larsson and D. Traum. 2000. Information State and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3?
4):323?340.
D. L. Martin, A. J. Cheyer, and D. B. Moran. 1999. The
Open Agent Architecture: A framework for building
distributed software systems. Applied Artificial Intel-
ligence: An International Journal, 13(1-2):91?128.
D. Mirkovic and L. Cavedon. 2005. Practical Plug-and-
Play Dialogue Management. In Proceedings of PA-
CLING, Tokyo, Japan.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. of
ICSLP 1998, Sydney, Australia.
S. Varges and G. Riccardi. 2007. A data-centric archi-
tecture for data-driven spoken dialogue systems. In
Proceedings of ASRU, Kyoto, Japan.
71
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 34?41,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Annotating Spoken Dialogs: from Speech Segments to Dialog Acts and
Frame Semantics
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli, Alessandro Moschitti, Giuseppe Riccardi?
University of Trento
38050 Povo - Trento, Italy
{dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.eu
Abstract
We are interested in extracting semantic
structures from spoken utterances gener-
ated within conversational systems. Cur-
rent Spoken Language Understanding sys-
tems rely either on hand-written seman-
tic grammars or on flat attribute-value se-
quence labeling. While the former ap-
proach is known to be limited in coverage
and robustness, the latter lacks detailed re-
lations amongst attribute-value pairs. In
this paper, we describe and analyze the hu-
man annotation process of rich semantic
structures in order to train semantic statis-
tical parsers. We have annotated spoken
conversations from both a human-machine
and a human-human spoken dialog cor-
pus. Given a sentence of the transcribed
corpora, domain concepts and other lin-
guistic features are annotated, ranging
from e.g. part-of-speech tagging and con-
stituent chunking, to more advanced anno-
tations, such as syntactic, dialog act and
predicate argument structure. In particu-
lar, the two latter annotation layers appear
to be promising for the design of complex
dialog systems. Statistics and mutual in-
formation estimates amongst such features
are reported and compared across corpora.
1 Introduction
Spoken language understanding (SLU) addresses
the problem of extracting and annotating the
meaning structure from spoken utterances in the
context of human dialogs (De Mori et al, 2008).
In spoken dialog systems (SDS) most used models
of SLU are based on the identification of slots (en-
?This work was partially funded by the European Com-
mission projects LUNA (contract 33549) and ADAMACH
(contract 022593).
tities) within one or more frames (frame-slot se-
mantics) that is defined by the application. While
this model is simple and clearly insufficient to
cope with interpretation and reasoning, it has sup-
ported the first generation of spoken dialog sys-
tems. Such dialog systems are thus limited by the
ability to parse semantic features such as predi-
cates and to perform logical computation in the
context of a specific dialog act (Bechet et al,
2004). This limitation is reflected in the type of
human-machine interactions which are mostly di-
rected at querying the user for specific slots (e.g.
?What is the departure city??) or implementing
simple dialog acts (e.g. confirmation). We believe
that an important step in overcoming such limita-
tion relies on the study of models of human-human
dialogs at different levels of representation: lexi-
cal, syntactic, semantic and discourse.
In this paper, we present our results in address-
ing the above issues in the context of the LUNA
research project for next-generation spoken dialog
interfaces (De Mori et al, 2008). We propose
models for different levels of annotation of the
LUNA spoken dialog corpus, including attribute-
value, predicate argument structures and dialog
acts. We describe the tools and the adaptation of
off-the-shelf resources to carry out annotation of
the predicate argument structures (PAS) of spoken
utterances. We present a quantitative analysis of
such semantic structures for both human-machine
and human-human conversations.
To the best of our knowledge this is the first
(human-machine and human-human) SDS corpus
denoting a multilayer approach to the annotation
of lexical, semantic and dialog features, which al-
lows us to investigate statistical relations between
the layers such as shallow semantic and discourse
features used by humans or machines. In the fol-
lowing sections we describe the corpus, as well as
a quantitative analysis and statistical correlations
between annotation layers.
34
2 Annotation model
Our corpus is planned to contain 1000 equally
partitioned Human-Human (HH) and Human-
Machine (HM) dialogs. These are recorded by
the customer care and technical support center of
an Italian company. While HH dialogs refer to
real conversations of users engaged in a problem
solving task in the domain of software/hardware
troubleshooting, HM dialogs are acquired with a
Wizard of Oz approach (WOZ). The human agent
(wizard) reacts to user?s spontaneous spoken re-
quests following one of ten possible dialog scenar-
ios inspired by the services provided by the com-
pany.
The above data is organized in transcrip-
tions and annotations of speech based on a new
multi-level protocol studied specifically within the
project, i.e. the annotation levels of words, turns1,
attribute-value pairs, dialog acts, predicate argu-
ment structures. The annotation at word level
is made with part-of-speech and morphosyntac-
tic information following the recommendations of
EAGLES corpora annotation (Leech and Wilson,
2006). The attribute-value annotation uses a pre-
defined domain ontology to specify concepts and
their relations. Dialog acts are used to annotate in-
tention in an utterance and can be useful to find
relations between different utterances as the next
section will show. For predicate structure annota-
tion, we followed the FrameNet model (Baker et
al., 1998) (see Section 2.2).
2.1 Dialog Act annotation
Dialog act annotation is the task of identifying
the function or goal of a given utterance (Sinclair
and Coulthard, 1975): thus, it provides a comple-
mentary information to the identification of do-
main concepts in the utterance, and a domain-
independent dialog act scheme can be applied.
For our corpus, we used a dialog act taxonomy
which follows initiatives such as DAMSL (Core
and Allen, 1997), TRAINS (Traum, 1996) and
DIT++ (Bunt, 2005). Although the level of granu-
larity and coverage varies across such taxonomies,
a careful analysis leads to identifying three main
groups of dialog acts:
1. Core acts, which represent the fundamen-
tal actions performed in the dialog, e.g. re-
1A turn is defined as the interval when a speaker is active,
between two pauses in his/her speech flow.
questing and providing information, or exe-
cuting a task. These include initiatives (often
called forward-looking acts) and responses
(backward-looking acts);
2. Conventional/Discourse management acts,
which maintain dialog cohesion and delimit
specific phases, such as opening, continua-
tion, closing, and apologizing;
3. Feedback/Grounding acts,used to elicit and
provide feedback in order to establish or re-
store a common ground in the conversation.
Our taxonomy, following the same three-fold
partition, is summarized in Table 1.
Table 1: Dialog act taxonomy
Core dialog acts
Info-request Speaker wants information from ad-
dressee
Action-request Speaker wants addressee to perform
an action
Yes-answer Affirmative answer
No-answer Negative answer
Answer Other kinds of answer
Offer Speaker offers or commits to perform
an action
ReportOnAction Speaker notifies an action is being/has
been performed
Inform Speaker provides addressee with in-
formation not explicitly required (via
an Info-request)
Conventional dialog acts
Greet Conversation opening
Quit Conversation closing
Apology Apology
Thank Thanking (and down-playing)
Feedback/turn management dialog acts
Clarif-request Speaker asks addressee for confirma-
tion/repetition of previous utterance
for clarification.
Ack Speaker expresses agreement with
previous utterance, or provides feed-
back to signal understanding of what
the addressee said
Filler Utterance whose main goal is to man-
age conversational time (i.e. dpeaker
taking time while keeping the turn)
Non-interpretable/non-classifiable dialog acts
Other Default tag for non-interpretable and
non-classifiable utterances
It can be noted that we have decided to retain
only the most frequent dialog act types from the
schemes that inspired our work. Rather than as-
piring to the full discriminative power of possible
conversational situations, we have opted for a sim-
ple taxonomy that would cover the vast majority
35
of utterances and at the same time would be able
to generalize them. Its small number of classes is
meant to allow a supervised classification method
to achieve reasonable performance with limited
data. The taxonomy is currently used by the sta-
tistical Dialogue Manager in the ADAMACH EU
project (Varges et al, 2008); the limited number
of classes allows to reduce the number of hypoth-
esized current dialogue acts, thus reducing the di-
alogue state space.
Dialog act annotation was performed manually
by a linguist on speech transcriptions previously
segmented into turns as mentioned above. The an-
notation unit for dialog acts, is the utterance; how-
ever, utterances are complex semantic entities that
do not necessarily correspond to turns. Hence, a
segmentation of the dialog transcription into ut-
terances was performed by the annotator before
dialog act labeling. Both utterance segmentation
and dialog act labeling were performed through
the MMAX tool (Mu?ller and Strube, 2003).
The annotator proceeded according to the fol-
lowing guidelines:
1. by default, a turn is also an utterance;
2. if more than one tag is applicable to an ut-
terance, choose the tag corresponding to its
main function;
3. in case of doubt among several tags, give pri-
ority to tags in core dialog acts group;
4. when needed, split the turn into several utter-
ances or merge several turns into one utter-
ance.
Utterance segmentation provides the basis not
only for dialog act labeling but also for the other
semantic annotations. See Fig. 1 for a dialog sam-
ple where each line represents an utterance anno-
tated according to the three levels.
2.2 Predicate Argument annotation
We carried out predicate argument structure an-
notation applying the FrameNet paradigm as de-
scribed in (Baker et al, 1998). This model
comprises a set of prototypical situations called
frames, the frame-evoking words or expressions
called lexical units and the roles or participants in-
volved in these situations, called frame elements.
The latter are typically the syntactic dependents of
the lexical units. All lexical units belonging to
the same frame have similar semantics and show
                                              PERSON-NAME 
Info: Buongiorno, sono   Paola.  
  
          GREETING    B._NAMED Name 
Good morning, this is Paola. 
 
Info-req: Come la posso aiutare? 
                      
                    Benefitted_party   ASSISTANCE 
How may I help you? 
 
                                                       CONCEPT         HARDWARE-COMPONENT 
Info: Buongiorno. Ho un problema con la stampante.  
 
          GREETING            PR._DESCRIPTION     Affected_device 
Good morning. I have a problem with the printer. 
 
           PART-OF-DAY   NEGAT. ACTION                ACTION 
Info: Da stamattina non   riesco pi? a  stampare 
                                       
                                    Problem 
Since this morning I can?t print. 
 
Info-req:   Mi  pu?  dire   nome e cognome per favore? 
 
              Addressee      TELLING               Message 
Can you tell me your name and surname, please? 
 
                                       PERSON-NAME  PERSON-SURNAME 
Answer: Mi chiamo  Alessandro  Manzoni. 
 
               Entity B._NAMED                   Name 
My name is Alessandro Manzoni. 
Figure 1: Annotated dialog extract. Each utterance
is preceded by dialog act annotation. Attribute-
value annotation appears above the text, PAS an-
notation below the text.
the same valence. A particular feature of the
FrameNet project both for English and for other
languages is its corpus-based nature, i.e. every el-
ement described in the resource has to be instanti-
ated in a corpus. To annotate our SDS corpus, we
adopted where possible the already existing frame
and frame element descriptions defined for the En-
glish FrameNet project, and introduced new def-
initions only in case of missing elements in the
original model.
Figure 1 shows a dialog sample with PAS an-
notation reported below the utterance. All lexi-
cal units are underlined and the frame is written in
capitals, while the other labels refer to frame el-
ements. In particular, ASSISTANCE is evoked by
the lexical unit aiutare and has one attested frame
element (Benefitted party), GREETING has no
frame element, and PROBLEM DESCRIPTION
and TELLING have two frame elements each.
Figure 2 gives a comprehensive view of the an-
notation process, from audio file transcription to
the annotation of three semantic layers. Whereas
36
Figure 2: The annotation process
Audio file 
Turn segmentation & 
Transcription 
Utterance segmentation 
POS tagging Domain attribute 
annotation 
PAS annotation 
Dialog Act 
annotation 
Syntactic parsing 
attribute-value and DA annotation are carried
out on the segmented dialogs at utterance level,
PAS annotation requires POS-tagging and syntac-
tic parsing (via Bikel?s parser trained for Italian
(Corazza et al, 2007)). Finally, a shallow manual
correction is carried out to make sure that the tree
nodes that may carry semantic information have
correct constituent boundaries. For the annotation
of frame information, we used the Salto tool (Bur-
chardt et al, 2006), that stores the dialog file in
TIGER-XML format and allows to easily intro-
duce word tags and frame flags. Frame informa-
tion is recorded on top of parse trees, with target
information pointing to terminal words and frame
elements pointing to tree nodes.
3 Quantitative comparison of the
Annotation
We evaluated the outcome of dialog act and
PAS annotation levels on both the human-human
(henceforth HH) and human-machine (HM) cor-
pora by not only analyzing frequencies and occur-
rences in the separate levels, but also their interac-
tion, as discussed in the following sections.
3.1 Dialog Act annotation
Analyzing the annotation of 50 HM and 50 HH
dialogs at the dialog act level, we note that an
HH dialog is composed in average by 48.9?17.4
(standard deviation) dialog acts, whereas a HM
dialog is composed of 18.9?4.4. The difference
between average lengths shows how HH sponta-
neous speech can be redundant, while HM dialogs
are more limited to an exchange of essential infor-
mation. The standard deviation of a conversation
in terms of dialog acts is considerably higher in
the HH corpus than in the HM one. This can be ex-
plained by the fact that the WOZ follows a unique,
previously defined task-solving strategy that does
not allow for digressions. Utterance segmentation
was also performed differently on the two corpora.
In HH we performed 167 turn mergings and 225
turn splittings; in HM dialogs, only turn splittings
(158) but no turn mergings were performed.
Tables 2 and 3 report the dialog acts occurring
in the HM and HH corpora, respectively, ranked
by their frequencies.
Table 2: Dialog acts ranked by frequency in the
human-machine (HM) corpus
human-machine (HM)
DA count rel. freq.
Info-request 249 26.3%
Answer 171 18.1%
Inform 163 17.2%
Yes-answer 70 7.4%
Quit 60 6.3%
Thank 56 5.9%
Greet 50 5.3%
Offer 49 5.2%
Clarification-request 26 2.7%
Action-request 25 2.6%
Ack 12 1.3%
Filler 6 0.6%
No-answer 5 0.5%
Other, ReportOnAction 2 0.2%
Apology 1 0.1%
TOTAL 947
From a comparative analysis, we note that:
1. info-request is by far the most common dia-
log act in HM, whereas in HH ack and info
share the top ranking position;
2. the most frequently occurring dialog act in
HH, i.e. ack, is only ranked 11th in HM;
3. the relative frequency of clarification-request
(4,7%) is considerably higher in HH than in
HM.
We also analyzed the ranking of the most fre-
quent dialog act bigrams in the two corpora. We
can summarize our comparative analysis, reported
in Table 4, to the following: in both corpora,
most bigram types contain info and info-request,
37
Table 3: Dialog acts ranked by frequency in the
human-human (HH) corpus
human-human (HH)
DA count rel. freq.
Ack 582 23.8%
Inform 562 23.0%
Info-request 303 12.4%
Answer 192 7.8%
Clarification-request 116 4.7%
Offer 114 4.7%
Yes-answer 112 4.6%
Quit 101 4.1%
ReportOnAction 91 3.7%
Other 70 2.9%
Action-request 69 2.8%
Filler 61 2.5%
Thank 33 1.3%
No-answer 26 1.1%
Greet, Apology 7 0.3%
TOTAL 2446
as expected in a troubleshooting system. How-
ever, the bigram info-request answer, which we
expected to form the core of a task-solving dia-
log, is only ranked 5th in the HH corpus, while 5
out of the top 10 bigram types contain ack. We
believe that this is because HH dialogs primarily
contain spontaneous information-providing turns
(e.g. several info info by the same speaker) and
acknowledgements for the purpose of backchan-
nel. Instead, HM dialogs, structured as sequences
of info-request answers pairs, are more minimal
and brittle, showing how users tend to avoid re-
dundancy when addressing a machine.
Table 4: The 10 most frequent dialog act bigrams
human-machine (HM) human-human (HH)
info-req answer ack info
answer info-req info ack
info info-req info info
info-req y-answer ack ack
sentence beginning greet info-req answer
greet info info info-req
info quit info-req y-answer
offer info ack info-req
thank info answer ack
y-answer thank quit sentence end
3.2 Predicate Argument annotation
We annotated 50 HM and 50 HH dialogs with
frame information. Differently from the English
FrameNet database, we didn?t annotate one frame
per sentence. On the contrary, we identified all
lexical units corresponding to ?semantically rele-
vant? verbs, nouns and adjectives with a syntac-
tic subcategorization pattern, eventually skipping
the utterances with empty semantics (e.g. dis-
fluencies). In particular, we annotated all lexical
units that imply an action, introduce the speaker?s
opinion or describe the office environment. We
introduced 20 new frames out of the 174 iden-
tified in the corpus because the original defini-
tion of frames related to hardware/software, data-
handling and customer assistance was sometimes
too coarse-grained. Few new frame elements were
introduced as well, mostly expressing syntactic re-
alizations that are typical of spoken Italian.
Table 5 shows some statistics about the cor-
pus dimension and the results of our annotation.
The human-human dialogs contain less frame in-
stances in average than the human-machine group,
meaning that speech disfluencies, not present in
turns uttered by the WOZ, negatively affect the se-
mantic density of a turn. For the same reason, the
percentage of turns in HH dialogs that were manu-
ally corrected in the pre-processing step (see Sec-
tion 2.2) is lower than for HM turns, since HH di-
alogs have more turns that are semantically empty
and that were skipped in the correction phase. Be-
sides, HH dialogs show a higher frame variabil-
ity than HM, which can be explained by the fact
that spontaneous conversation may concern mi-
nor topics, whereas HM dialogs follow a previ-
ously defined structure, designed to solve soft-
ware/hardware problems.
Tables 6 and 7 report the 10 most frequent
frames occurring in the human-machine resp.
human-human dialogs. The relative frame fre-
quency in HH dialogs is more sparse than in HM
dialogs, meaning that the task-solving strategy fol-
lowed by the WOZ limits the number of digres-
sions, whereas the semantics of HH dialogs is
richer and more variable.
As mentioned above, we had to introduce and
define new frames which were not present in the
original FrameNet database for English in order to
capture all relevant situations described in the di-
alogs. A number of these frames appear in both
tables, suggesting that the latter are indeed rel-
38
Table 5: Dialog turn and frame statistics for the
human-machine (HM) resp. human-human (HH)
corpus
HM HH
Total number of turns 662 1,997
Mean dialog length (turns) 13.2 39.9
Mean turn length (tokens) 11.4 10.8
Corrected turns (%) 50 39
Total number of annotations 923 1951
Mean number of frame annota-
tions per dialog
18.5 39.0
Mean number of frame elements
per frame annotation
1.6 1.7
evant to model the general semantics of the di-
alogs we are approaching. The most frequent
frame group comprises frames relating to infor-
mation exchange that is typical of the help-desk
activity, including Telling, Greeting, Contacting,
Statement, Recording, Communication. Another
relevant group encompasses frames related to the
operational state of a device, for example Be-
ing operational, Change operational state, Oper-
ational testing, Being in operation.
The two groups also show high variability of
lexical units. Telling, Change operational state
and Greeting have the richest lexical unit set,
with 11 verbs/nouns/adjectives each. Arriving
and Awareness are expressed by 10 different lexi-
cal units, while Statement, Being operational, Re-
moving and Undergo change of operational state
have 9 different lexical units each. The informal
nature of the spoken dialogs influences the com-
position of the lexical unit sets. In fact, they are
rich in verbs and multiwords used only in collo-
quial contexts, for which there are generally few
attestations in the English FrameNet database.
Similarly to the dialog act statistics, we also
analyzed the most frequent frame bigrams and
trigrams in HM and HH dialogs. Results are
reported in Tables 8 and 9. Both HH bigrams
and trigrams show a more sparse distribution and
lower relative frequency than HM ones, implying
that HH dialogs follow a more flexible structure
with a richer set of topics, thus the sequence of
themes is less predictable. In particular, 79%
of HH bigrams and 97% of HH trigrams occur
only once (vs. 68% HM bigrams and 82% HM
trigrams). On the contrary, HM dialogs deal with
Table 6: The 10 most frequent frames in the HM
corpus (* =newly introduced)
HM corpus
Frame count freq-%
Greeting* 146 15.8
Telling 134 14.5
Recording 83 8.9
Being named 74 8.0
Contacting 52 5.6
Usefulness 50 5.4
Being operational 28 3.0
Problem description* 24 2.6
Inspecting 24 2.6
Perception experience 21 2.3
Table 7: The 10 most frequent frames in the HH
corpus (* =newly introduced)
HH corpus
Frame count freq-%
Telling 143 7.3
Greeting* 124 6.3
Awareness 74 3.8
Contacting 63 3.2
Giving 62 3.2
Navigation* 61 3.1
Change operational state 51 2.6
Perception experience 46 2.3
Insert data* 46 2.3
Come to sight* 38 1.9
a fix sequence of topics driven by the turns uttered
by the WOZ. For instance, the most frequent
HM bigram and trigram both correspond to the
opening utterance of the WOZ:
Help desk buongiornoGREETING, sonoBEING NAMED
Paola, in cosa posso esserti utileUSEFULNESS?
(Good morning, help-desk service, Paola speaking, how can
I help you?)
3.3 Mutual information between PAS and
dialog acts
A unique feature of our corpus is the availabil-
ity of both a semantic and a dialog act annota-
tion level: it is intuitive to seek relationships in
the purpose of improving the recognition and un-
derstanding of each level by using features from
the other. We considered a subset of 20 HH and
50 HM dialogs and computed an initial analysis
39
Table 8: The 5 most frequent frame bigrams
human-machine (HM) freq-%
Greeting Being named 17.1
Being named Usefulness 15.3
Telling Recording 12.9
Recording Contacting 10.9
Contacting Greeting 10.6
human-human (HH) freq-%
Greeting Greeting 4.7
Navigation Navigation 1.2
Telling Telling 1.0
Change op. state Change op. state 0.9
Telling Problem description 0.8
Table 9: The 5 most frequent frame trigrams
human-machine (HM) freq-%
Greeting Being named Usefulness 9.5
Recording Contacting Greeting 5.7
Being named Usefulness Greeting 3.7
Telling Recording Contacting 3.5
Telling Recording Recording 2.2
human-human (HH) freq-%
Greeting Greeting Greeting 1.6
Greeting Being named Greeting 0.5
Contacting Greeting Greeting 0.3
Navigation Navigation Navigation 0.2
Working on Greeting Greeting 0.2
of the co-occurrences of dialog acts and PAS. We
noted that each PAS tended to co-occur only with a
limited subset of the available dialog act tags, and
moreover in most cases the co-occurrence hap-
pened with only one dialog act. For a more thor-
ough analysis, we computed the weighted condi-
tional entropy between PAS and dialog acts, which
yields a direct estimate of the mutual information
between the two levels of annotation2.
2Let H(yj |xi) be the weighted conditional entropy of ob-
servation yj of variable Y given observation xi of variable
X:
H(yj |xi) = ?p(xi; yj)log
p(xi; yj)
p(xi)
,
where p(xi; yj) is the probability of co-occurrence of xi and
yj , and p(xi) and p(yj) are the marginal probabilities of oc-
currence of xi resp. yj in the corpus. There is an obvious re-
lation with the weighted mutual information between xi and
yj , defined following e.g. (Bechet et al, 2004) as:
wMI(xi; yj) = p(xi; yj)log
p(xi; yj)
p(xi)p(yj)
.
(a) human-machine dialogs (filtering co-occurrences below 3)
(b) human-human dialogs (filtering co-occurrences below 5)
Figure 3: Weighted conditional entropy between
PAS and dialog acts in the HM (a) and HH corpus
(b). To lower entropies correspond higher values
of mutual information (darker color in the scale)
Our results are illustrated in Figure 3. In the
HM corpus (Fig. 3(a)), we noted some interesting
associations between dialog acts and PAS. First,
info-req has the maximal MI with PAS like Be-
ing in operation and Being attached, as requests
are typically used by the operator to get informa-
tion about the status of device. Several PAS de-
note a high MI with the info dialog act, includ-
ing Activity resume, Information, Being named,
Contacting, and Resolve problem. Contacting
refers to the description of the situation and of the
speaker?s point of view (usually the caller). Be-
ing named is primarily employed when the caller
introduces himself, while Activity resume usually
refers to the operator?s description of the sched-
Indeed, the higher is H(yj |xi), the lower is wMI(xi; yj).
We approximate all probabilities using frequency of occur-
rence.
40
uled interventions.
As for the remaining acts, clarif has the high-
est MI with Perception experience and Statement,
used to warn the addressee about understanding
problems and asking him to repeat/rephrase an ut-
terance, respectively. The two strategies can be
combined in the same utterance, as in the utter-
ance: Non ho sentito bene: per favore ripeti cer-
cando di parlare piu` forte. (I haven?t quite heard
that, please repeat trying to speak up.).
The answer tag is highly informative with Suc-
cessful action, Change operational state, Becom-
ing nonfunctional, Being detached, Read data.
These PAS refer to the exchange of infor-
mation (Read data) or to actions performed
by the user after a suggestion of the system
(Change operational state). Action requests (act-
req) seem to be correlated to Replacing as it usu-
ally occurs when the operator requests the caller
to carry out an action to solve a problem, typically
to replace a component with another. Another fre-
quent request may refer to some device that the
operator has to test.
In the HH corpus (Fig. 3(b)), most of the PAS
are highly mutually informative with info: in-
deed, as shown in Table 3, this is the most fre-
quently occurring act in HH except for ack, which
rarely contain verbs that can be annotated by a
frame. As for the remaining acts, there is an easily
explainable high MI between quit and Greeting;
moreover, info-req denote its highest MI with
Giving, as in requests to give information, while
rep-action denotes a strong co-occurrence with
Inchoative attaching: indeed, interlocutors often
report on the action of connecting a device.
These results corroborate our initial observation
that for most PAS, the mutual information tends
to be very high in correspondence of one dialog
act type: this suggests the beneficial effect of in-
cluding shallow semantic information as features
for dialog act classification. The converse is less
clear as the same dialog act can relate to a span
of words covered by multiple PAS and generally,
several PAS co-occur with the same dialog act.
4 Conclusions
In this paper we have proposed an approach to
the annotation of spoken dialogs using seman-
tic and discourse features. Such effort is crucial
to investigate the complex dependencies between
the layers of semantic processing. We have de-
signed the annotation model to incorporate fea-
tures and models developed both in the speech
and language research community and bridging
the gap between the two communities. Our multi-
layer annotation corpus allows the investigation
of cross-layer dependencies and across human-
machine and human-human dialogs as well as
training of semantic models which accounts for
predicate interpretation.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
ACL/Coling?98, pages 86?90.
F. Bechet, G. Riccardi, and D. Hakkani-Tur. 2004.
Mining spoken dialogue corpora for system evalu-
ation and modeling. In Proceedings of EMNLP?04,
pages 134?141.
H. Bunt. 2005. A framework for dialogue act specica-
tion. In Proceedings of SIGSEM WG on Represen-
tation of Multimodal Semantic Information.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. Salto - a versatile multi-
level annotation tool. In Proceedings of LREC 2006,
pages 517?520, Genoa, Italy.
A. Corazza, A. Lavelli, and G. Satta. 2007. Anal-
isi sintattica-statistica basata su costituenti. Intelli-
genza Artificiale, 4(2):38?39.
M. G. Core and J. F. Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communicative
Actions in Humans and Machines.
R. De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,
G. Riccardi, and G. Tur. 2008. Spoken language
understanding: A survey. IEEE Signal Processing
magazine, 25(3):50?58.
G. Leech and A. Wilson. 2006. EAGLES recommen-
dations for the morphosyntactic annotation of cor-
pora. Technical report, ILC-CNR.
C. Mu?ller and M. Strube. 2003. Multi-level annotation
in MMAX. In Proceedings of SIGDIAL?03.
J. M. Sinclair and R. M. Coulthard. 1975. Towards an
Analysis of Discourse: The English Used by Teach-
ers and Pupils. Oxford University Press, Oxford.
D. Traum. 1996. Conversational agency: The
TRAINS-93 dialogue manager. In Proceedings of
TWLT 11: Dialogue Management in Natural Lan-
guage Systems, pages 1?11, June.
S. Varges, G. Riccardi, and S. Quarteroni. 2008. Per-
sistent information state in a data-centric architec-
ture. In Proceedings of SIGDIAL?08.
41
Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 19?23,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Addressing How-to Questions using a Spoken Dialogue System:
a Viable Approach?
Silvia Quarteroni
University of Trento
38050 Povo (Trento), Italy
silviaq@disi.unitn.it
Patrick Saint-Dizier
IRIT
Toulouse, France
stdizier@irit.fr
Abstract
In this document, we illustrate how com-
plex questions such as procedural (how-to)
ones can be addressed in an interactive for-
mat by means of a spoken dialogue sys-
tem. The advantages of interactivity and
in particular of spoken dialogue with re-
spect to standard Question Answering set-
tings are numerous. First, addressing user
needs that do not necessarily arise in front
of a computer; moreover, a spoken or mul-
timodal answer format can often be bet-
ter suited to the user?s need. Finally, the
procedural nature of the information itself
makes iterative question formulation and
answer production particularly appealing.
1 Introduction
Question answering (QA) is nowadays an estab-
lished technology, advancing information retrieval
to the point of allowing queries to be formulated
in natural language and to return actual answers
(in the form of sentences/phrases).
While the first QA systems (Simmons, 1965)
mainly dealt with factoid questions, i.e. ques-
tions about names, dates and all that can be re-
duced to a fact, a number of systems in the last
decade have appeared with the aim of addressing
non-factoid questions (Voorhees, 2003). In par-
ticular, the problem of addressing definition ques-
tions has received great attention from the research
community (Chen et al, 2006; Moschitti et al,
2007), while less research has been conducted so
far on other types of non-factoid QA, such as why-
questions (Verberne et al, 2007; Pechsiri et al,
2008) and procedural (also called how-to) ques-
tions (Yin, 2006; Delpech and Saint-Dizier, 2008).
Another recent trend in QA is interactivity, i.e.
the use of a dialogue interface to better support
the user, e.g. by resolving anaphoric and elliptic
expressions in his/her queries (Webb and Strza-
lkowski, 2006). Indeed, the dialogue community
has been addressing the problem of information
seeking for decades, often with very satisfying
commercial products able to interact not only in
text but especially via spoken interfaces (Gupta et
al., 2006; Traum, 1996). However, also in this
field the information retrieval task has mainly fo-
cused on a limited domain (travel planning, tele-
com rates) and on returning database values rather
than cooperatively solving problems or providing
complex information.
In this paper, we focus on handling procedural
questions, not as commonly researched as defini-
tional QA but for which a number of resources
are available on the Web. Indeed, although por-
tals dedicated to how-to questions exist (eHow.
com), where stereotyped questions are presented
together with a few responses, QA would allow a
broader approach to intelligently respond to how-
to questions.
Our main claim is that joining the existing QA
technology for complex procedural questions with
the potentials of spoken conversation would pro-
vide an excellent testbed for the integration of
these two technologies. Indeed, understanding and
answering procedural questions requires a high
level of cooperation between the user and the sys-
tem: a procedure is a complex answer to return
and would better be provided and received step by
step than ?dumped? in a text-to-speech generator
or a text file.
In the rest of this document, we outline the main
features of procedural QA and the approach we
propose to address it via dialogue. We illustrate
the potentials of our approach with two use cases
of different complexity.
2 Procedural Question Answering
Procedural text contains not only step-by-step in-
structions, but also additional content such as
19
warnings, recommendations and advice. Due to
the argumentative nature of such text, procedural
QA is a complex task. Indeed, the main challeges
offered by procedural QA can be summarized as:
1. Acquiring procedural data:
? (automatically) obtaining the data, filter-
ing out text with little procedural con-
tent;
? tagging relevant structures in procedures
(such as warnings, advice, step-wise in-
structions);
? efficiently indexing texts based on their
title and content;
2. Answering procedural questions:
? recognizing and interpreting procedu-
ral questions (question classification and
analysis);
? pinpointing answer passages (answer re-
trieval);
? generating answers to procedural ques-
tions and supporting interaction span-
ning over more than one Q/A pair, such
as step-by-step procedural descriptions.
To our knowledge, little extensive work exists
in this field; an example is the TextCoop project
(Delpech and Saint-Dizier, 2008) that produced a
procedural tagger able to recognize and segment
the main units found in French procedural text (ti-
tles, instructions, prerequisites, warnings and ad-
vice) via an ad hoc markup convention (see Table
1). In addition, QA technology was used for the
resolution of elliptic titles and their indexing for
answer matching (titles often express goals).
Although automatic procedure tagging and
analysis appears as a necessary step towards an ef-
ficient treatment of procedural questions, we argue
that an accurate choice of the format and modality
in which their answers are returned would be a vi-
tal advantage. In particular, we propose to return
the response to a procedural QA under the form
of oral instructions rather than text to read. In-
deed, besides the advantages of oral communica-
tion in terms of expressiveness, the latter solution
may be inappropriate in some situations such as
when walking around or driving.
In Section 3, we discuss our dialogue-based ap-
proach to procedural QA.
3 Dialogue-based Procedural QA
We believe that the integration of QA research
with a Spoken Dialogue System (SDS) is a
promising approach to procedural Question An-
swering. Indeed, work on procedural QA so far
accounts for the textual structures of written docu-
ments; since procedural texts are in general highly
interactive, it is clear that the pairing with a spo-
ken dialogue system is of much interest. In addi-
tion, a spoken interface enables to go far beyond a
mere enumeration of instructions (as found in Web
pages), achieving cooperation between the service
provider (SDS) and the user.
A first step towards this is the (automatic or
semi-automatic) annotation of procedural texts via
an ad hoc markup in order to distinguish sub-
texts that can yield to dialogues, such as conditions
(texts containing ?if you are under 20 . . . ?, ?if you
are aged between . . . ? may be translated in the
question: ?how old are you??). Similarly, in warn-
ings, terms bearing the illocutionary force (?Re-
member?, ?Caution?, ?Notice?) can be marked in
order to be stressed.
During system execution, instructions can be ut-
tered one after the other, waiting for an acknowl-
edgement from the user, but the system can also
provide more information about the task at hand
upon request. Moreover, the system can provide
alternative solutions when users have trouble car-
rying out an instruction (?I cannot pay by credit
card?), or make an instruction more explicit by
splitting it into simpler ones (automatically gen-
erating another how-to question for the subgoal at
hand).
Finally, in addition to speech and dialogue, mul-
timodal aspects of interactivity can be consid-
ered, such as displaying a map when providing an
itinerary, or a 3D picture related to an instruction.
Translating a procedural text into speech is a
challenge that requires intensive NLP processing,
a strong and accurate domain model and an ability
for reasoning. In order to address this challenge,
we propose the following approach:
1. Obtain domain-related data from the Web;
2. Represent domain knowledge and reasoning.
While most of the factual knowledge of a do-
main can be captured by means of an en-
riched ontology, other types of knowledge
(know-how, domain constraints, etc.) and
20
reasoning procedures need to be defined on
other grounds, optionally manually;
3. Devise a Dialogue Manager able to interact
about procedural information using markup
in the procedural data representation;
4. Define how the procedural data representa-
tion can be rendered by a Natural Language
Generator;
5. Use existing technology for Automatic
Speech Recognition and Text-To-Speech.
Evidently, the difficulty of answering procedu-
ral questions via dialogue varies depending on the
availability and format of answers. We distinguish
between two types of questions:
Type 1: a procedural text corresponding to the
question is already available on the Web; in
this case, the user?s query can be answered
by tagging such text using a tagger such as
TextCoop and enriching it with dialogic and
prosodic markers to be rendered by an off-
the-shelf TTS module;
Type 2: there is no direct answer to the user?s
query on the Web; for instance, the answer
may be dependent on information which the
user has not yet provided. In this case, the
query must first be formulated, and procedu-
ral tagging/TTS intervene later.
In Sections 4 and 5, we report two case stud-
ies reflecting type 1 and type 2 situations, respec-
tively: the first relates to the University helpdesk
domain, the second to the tourist advice domain.
4 Text-to-Speech from a Web page
To illustrate type 1 questions, we study a well-
known domain, universities, where helpdesks
must provide various kinds of procedural informa-
tion (dealing with e.g. paperwork, student life and
infrastructure).Let us consider the question: ?How
to get a student card in Wolverhampton??. In Fig.
1, we report an extract of the top Web page ob-
tained by typing such question into a search en-
gine. It can be noted that in this case, the top
search engine result contains the procedural an-
swer sought by the question, hence procedural tag-
ging can be performed on the text.
A possible procedural annotation has been
(manually) applied to the same text in Figure 2,
Figure 1: Extract from the top Web hit for: ?How
to get a student card in Wolverhampton?? (source:
wlv.ac.uk)
following the conventions used in the TexCoop
tagger (see Tab. 1) to denote the abilities of a pro-
cedural tagger. While some of the HTML objects
in the Webpage, such as title, headers and enumer-
ations, are directly converted in their equivalent
tags (item, subtitle), additional markup ap-
pears, such as warnings and prerequisites.
Table 1: TextCoop procedural markup (extract)
Label Example
title ?Get your student ID card?
subtitle ?What you?ll need?
cond ?if you are a UK student?
objective ?in order to get your ID?
instr ?Head to the Uni info service.?
prerequisite ?You?ll need 3 passport photos?
warning ?Format MUST be passport!?
aim ?to get good photos?
advice ?try the photobooth next to . . . ?
At this point, using a dialogue system to sim-
ply ?read out? the above passage (even if split
into their main components) would result in inef-
fective, close-to intonation free speech. Indeed,
in order to provide instructions to the Natural
Language Generator and Text-to-Speech modules
of a dialogue system for verbalizing such text,
dialogue-level markup must be added to the above
procedural annotation.
In some cases, direct mapping rules can be de-
vised to directly translate procedural markup into
dialogue patterns. For instance, step-by-step in-
structions (item) contained in the itemize en-
vironment can be rendered as a sequence of inform
21
<subtitle> Applying for an ID card </subtitle>
<inst-compound>
When you receive a firm offer from the University,you can upload your photo
for your student ID card, <warning> and you should do this as soon as
you can. </warning>
</inst-compound>
<prerequisite> What you?ll need
<itemize>
<item:1>Your student number, a seven-digit number which will be on your
offer letter < /item:1>
<item:2> A digital photo that meets the requirements outlined below
< /item:2>
<item:3> Access to a computer with Internet access to send your
photo to the University using the Photo Upload facility.< /item:3>
</itemize></prerequisite>
<inst-compound><cond> If you don?t have a digital photo or a com-
puter with internet access, </cond> . . .
Figure 2: Procedural annotation of a Web page
dialogue acts, expecting acknowledgments (ack)
from the user. In addition, conditions can be ren-
dered as yes-no questions (?If you don?t have a
digital photo? becomes ask(digital photo));
In other cases, such as verbalizing warnings and
advice, specific salient words should be marked
with prosodic information as to how to pronounce
them. Specific lexical patters can be matched by
rules to provide such annotations, such as ?Re-
member? or ?as soon as possible?. Finally, part of
the procedural annotation could be excluded from
the dialog when redundant or implicit. For in-
stance, titles (title) could be skipped or men-
tioned separately by the dialogue system (e.g.
?Would you like to hear about how to Get your
student ID card??).
Figure 3 illustrates the dialog act and prosodic
annotation enriching the procedural one of Figure
2. Such generic markup can then me converted
in a specific commercial voice markup languages,
e.g. VXML or SALT, via simple rules.
5 Integrating Scenarios and QA
Besides improving access to procedures via direct
interactions by spoken dialogue, it is often neces-
sary to interact with the user to get more precise
information about his query, so that the response
can be accurate enough. Furthermore, a number
of procedural questions do not get any direct re-
sponse via Web queries. This is the case of type
2 questions, as introduced in Section 3. There are
several reasons to this situation. First, a number
of these questions are both complex and very spe-
cific. Next, most of them involve various forms
of reasoning and of elaboration. Other questions
require the integration of several simpler proce-
dures, e.g. via concatenation. Finally, others re-
<subtitle> Applying for an ID card </subtitle>
<inst-compound> When you receive a firm offer from the University,
you can upload your photo for your student ID card, <warning> and
you should do this < /prosody:emphasize> as soon as you can.
< /prosody:emphasize></warning></inst-compound>
<prerequisite> What you?ll need
<itemize>
<item:1> <dialog:inform-ack> Your student number, a seven-
digit number which will be on your offer letter< /dialog:inform-ack>
< /item:1>
<item:2> <dialog:inform-ack> A digital photo that meets the re-
quirements outlined below < /dialog:inform-ack>< /item:2>
<item:3> <dialog:inform-ack> Access to a computer with Inter-
net access to send your photo to the University using the Photo Upload facility.
< /dialog:inform-ack>< /item:3>
</itemize></prerequisite>
<inst-compound><dialog:ask><cond> If you don?t have a dig-
ital photo or a computer with internet access,</cond>< /dialog:ask>
. . .
Figure 3: Dialog act and prosodic annotation of a
Web page
quire a substantial adaptation of existing proce-
dures: adaptation to a different context, general-
izations (e.g. knowing how to register in a uni-
versity may lead to a generalization so that it is
globally acceptable for other universities).
This is in particular the case for non-trivial
itineraries. For example, looking on the Web for
ways to go from Toulouse to Trento does not lead
to any solution. Search engines return partial and
often local information, e.g. description of Verona
airport, train schedules going via Trento, etc. We
need in this case to define a very generic scenario,
which is a procedure, of type ?travel? and, for a
given trip, to construct the details from simpler
procedures or factual data available on the Web.
To overcome these limitations and to be able to
offer a real QA service, we propose the following
approach:
? Creating a general scenario, in our case for
itinerary construction, involving dialogue to
get necessary (departure/arrival location and
dates, etc.) and optional (budget, comfort,
etc.) information from the user.
? Including reasoning procedures and prefer-
ences related to transportation: e.g. it is
preferable to fly above a certain distance or if
there are obstacles (sea, mountains), or elab-
orate compromise between cost and trans-
portation length. Itinerary construction also
involves a planner, that operates over any
kind of transportation means, paired with an
optimizer. The planner should be flexible so
that it can propose alternatives (e.g. train or
22
renting a car, stops at different places) while
the optimizer should take user preferences
into account.
? Submitting queries to a search engine to get
detailed information on precise points: flight
schedules, airport transportation, bus routes,
etc. Such queries are triggered by the differ-
ent functions of the scenario to fill in infor-
mation slots. From search engine results, it
is necessary to process the text segments so
that the correct information is found. This in-
cludes either getting precise data or selecting
a text portion (e.g. that describes services,
schedules, etc.).
? Summarizing the information and generat-
ing a response in natural language under the
form of a procedure, possibly with schemas,
maps, etc. and then producing a vocal output.
As shown above, parts of this scenario may
be vocal or multimedia. As in most natural
language generation systems, this involves a
planner that operates of various types of input
data (text, words, structured sequences of the
scenarion) and a language generation compo-
nent which, in this type of application, can
be based on predefined word sequences and
gaps to be filled in for the query at stake.
This approach has its roots in the frames and
scripts of cognitive science and AI in the 70s
(Schank and Abelson, 1977). However, in our
case we include a QA component to get informa-
tion and a planner to construct the itinerary based
on the results of the queries which also outputs
a procedure in natural language. In addition, the
proposed approach supports cooperative dialogues
and provides explanations to the user when there
is no direct answer to his request.
6 Perspectives
We have proposed a model of procedural QA sys-
tem conducting cooperative spoken dialogue with
the user. Indeed, we argue that the advantages of
spoken communication channel to address proce-
dural QA are mainly twofold. On the one hand,
procedural information can be returned to the user
in a more efficient way compared to the textual
format. On the other hand, cooperative dialogue
allows the system to understand and refine the
user?s information needs and to account for the
cases when information is not directly available on
the Web.
Our proposed approach has currently only been
validated through case studies and a long process
is required in order to achieve spoken procedural
QA. However, we believe that using existing re-
sources to address procedural information, such as
procedural taggers, as well as state-of-the art QA
and spoken dialogue technology, fulfilling our ob-
jectives is a feasible task.
References
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models.
In Proc. ACL.
E. Delpech and P. Saint-Dizier. 2008. Investigating the
structure of procedural texts for answering how-to
questions. In Proc. LREC.
N. Gupta, G. Tur, D. Hakkani-tur, G. Riccardi, S. Ban-
galore, M. Rahim, and M Gilbert. 2006. The AT&T
spoken language understanding system. IEEE trans-
actions on speech and audio, 14:213?222.
A. Moschitti, S. Quarteroni, R. Basili, and S. Man-
andhar. 2007. Exploiting syntactic and shallow se-
mantic kernels for question/answer classification. In
Proc. ACL.
C. Pechsiri, P. Sroison, and U. Janviriyasopa. 2008.
Know-why extraction from textual data. In Proc.
KRAQ.
R. C. Schank and R. P. Abelson. 1977. Scripts, plans,
goals, and understanding: An inquiry into human
knowledge structures. Erlbaum.
R. F. Simmons. 1965. Answering english questions by
computer: a survey. Comm. ACM, 8(1):53?70.
D. Traum. 1996. Dialogue management in conversa-
tional agency: The TRAINS-93 dialogue manager.
In Proc. TWLT, pages 1?11.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating discourse-based answer extraction
for why-question answering. In Proc. SIGIR, pages
735?737.
E. M. Voorhees. 2003. Overview of TREC 2003. In
Proc. TREC.
N. Webb and T. Strzalkowski, editors. 2006. Proc.
HLT-NAACL Workshop on Interactive Question An-
swering.
L. Yin. 2006. A two-stage approach to retrieving an-
swers for how-to questions. In Proc. EACL (Student
Session).
23
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 156?159,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Leveraging POMDPs trained with User Simulations and
Rule-based Dialogue Management in a Spoken Dialogue System
Sebastian Varges, Silvia Quarteroni, Giuseppe Riccardi, Alexei V. Ivanov, Pierluigi Roberti
Department of Information Engineering and Computer Science
University of Trento
38050 Povo di Trento, Italy
{varges|silviaq|riccardi|ivanov|roberti}@disi.unitn.it
Abstract
We have developed a complete spoken di-
alogue framework that includes rule-based
and trainable dialogue managers, speech
recognition, spoken language understand-
ing and generation modules, and a com-
prehensive web visualization interface.
We present a spoken dialogue system
based on Reinforcement Learning that
goes beyond standard rule based models
and computes on-line decisions of the best
dialogue moves. Bridging the gap between
handcrafted (e.g. rule-based) and adap-
tive (e.g. based on Partially Observable
Markov Decision Processes - POMDP) di-
alogue models, this prototype is able to
learn high rewarding policies in a number
of dialogue situations.
1 Reinforcement Learning in Dialogue
Machine Learning techniques, and particularly
Reinforcement Learning (RL), have recently re-
ceived great interest in research on dialogue man-
agement (DM) (Levin et al, 2000; Williams and
Young, 2006). A major motivation for this choice
is to improve robustness in the face of uncertainty
due for example to speech recognition errors. A
second important motivation is to improve adap-
tivity w.r.t. different user behaviour and applica-
tion/recognition environments.
The RL approach is attractive because it offers a
statistical model representing the dynamics of the
interaction between system and user. This con-
trasts with the supervised learning approach where
system behaviour is learnt based on a fixed cor-
pus. However, exploration of the range of dialogue
management strategies requires a simulation en-
vironment that includes a simulated user (Schatz-
mann et al, 2006) in order to avoid the prohibitive
cost of using human subjects.
We demonstrate various parameters that influ-
ence the learnt dialogue management policy by
using pre-trained policies (section 5). The appli-
cation domain is a tourist information system for
accommodation and events in the local area. The
domain of the trained DMs is identical to that of a
rule-based DM that was used by human users (sec-
tion 4), allowing us to compare the two directly.
2 POMDP demonstration system
The POMDP DM implemented in this work is
shown in figure 1: at each turn at time t, the incom-
ingN user act hypotheses an,u split the state space
St to represent the complete set of interpretations
from the start state (N=2). A belief update is per-
formed resulting in a probability assigned to each
state. The resulting ranked state space is used as
a basis for action selection. In our current imple-
mentation, belief update is based on probabilistic
user responses that include SLU confidences. Ac-
tion selection to determine system action am,s is
based on the best state (m is a counter for actions
in action set A). In each turn, the system uses an
-greedy action selection strategy to decide prob-
abilistically if to exploit the policy or explore any
other action at random. (An alternative would be
softmax, for example.) At the end of each dia-
logue/session a reward is assigned and policy en-
tries are added or updated for each state-action
pair involved. These pairs are stored in tabular
form. We perform Monte Carlo updating similar
to (Levin et al, 2000):
Qt(s, a) = R(s, a)/n+Qt?1 ? (n ? 1)/n (1)
where n is the number of sessions, R the reward
and Q the estimate of the state-action value.
At the beginning of each dialogue, a user goal
UG (a set of concept-value pairs) is generated ran-
domly and passed to a user simulator. The user
simulator takes UG and the current dialogue con-
text to produce plausible SLU hypotheses. These
156
Turn?t1?
POLICY:?? Q(s1,a1)?s1,a1?s1,a2?s2,a1?s3,a4?
St1?
a1,s?a2,s?a3,s?a4,s?
policy?lookup?
policy??update?
Ut1?
a1,u?
final?state?
reward?computation?
Turn?t2? Turn?tn?
state?space?
Ut2?
state?space? St2?
?s1,1? ?s1,2??s2,2?a2,u?
a1,s? a2,s?a3,s?
a4,s?
an,s? an,s?an,s?
an,s?
state?space? Stn?user?goal?
start?state?
s1,n?
Q(s1,a2)?Q(s2,a1)?Q(s3,a4)?
UG?
s3,n?
s2,n?
s4,n?s5,n?
s6,n?
Figure 1: POMDP Dialogue Manager
are a subset of the concept-value pairs in UG along
with a confidence estimate bootstrapped from a
small corpus of 74 in-domain dialogs. We assume
that the user ?runs out of patience? after 15 turns
and ends the call.
The system visualizes POMDP-related infor-
mation live for the ongoing dialogue (figure 2).
The visualization tool shows the internal represen-
tation of the dialogue manager including the the
N best dialogue states after each user utterance
and the reranking of the action set. At the end
of each dialogue session, the reward and the pol-
icy updates are shown, i.e. new or updated state
entries and action values. Moreover, the system
generates a plot that relates the current dialogue?s
reward to the reward of previous dialogues.
3 User Simulation
To conduct thousands of simulated dialogues, the
DM needs to deal with heterogeneous but plau-
sible user input. We designed a User Simulator
(US) which bootstraps likely user behaviors start-
ing from a small corpus of 74 in-domain dialogs,
acquired using a rule-based version of the system
(section 4). The role of the US is to simulate
the output of the SLU module to the DM during
the whole interaction, fully replacing the ASR and
SLU modules. This differs from other user sim-
ulation approaches where n-gram models of user
dialog acts are represented.
For each simulated dialogue, one or more user
goals are randomly selected from a list of possible
user goals stored in a database table. A goal is rep-
resented as the set of concept-value pairs defining
a task. Simulation of the user?s behaviour happens
in two stages. First, a user model, i.e. a model
of the user?s intentions at the current stage of the
dialogue, is created. This is done by mining the
previous system move to obtain the concepts re-
quired by the DM and their corresponding values
(if any) from the current user goal. Then, the out-
put of the user model is passed to an error model
that simulates the ?noisy channel? recognition er-
rors based on statistics from the dialogue corpus.
Errors produce perturbations of concept values as
well as phenomena such as noInput, noMatch and
hangUp. If the latter phenomena occur, they are
directly propagated to the DM; otherwise, plau-
sible confidences (based on the dialogue corpus)
are attached to concept-value pairs. The probabil-
ity of a given concept-value observation at time
t + 1 given the system move at time t, as,t, and
the session user goal gu, called P (ot+1|as,t, gu),
is obtained by combining the outputs of the error
model and the user model:
P (ot+1|au,t+1) ? P (au,t+1|as,t, gu)
where au,t+1 is the true user action. Finally,
concept-value pairs are combined in an SLU hy-
pothesis and, as in the regular SLU module, a cu-
mulative utterance-level confidence is computed,
determining the rank of each of the N hypotheses
output to the DM.
4 Rule-based Dialogue Management
A rule-based DM was developed as a meaning-
ful comparison to the trained DM, to obtain train-
ing data from human-system interaction for the
US, and to understand the properties of the do-
main. Rule-based dialog management works in
two stages: retrieving and preprocessing facts (tu-
ples) taken from a dialogue state database, and
inferencing over those facts to generate a system
response. We distinguish between the ?context
model? of the first phase ? essentially allowing
more recent values for a concept to override less
recent ones ? and the ?dialog move engine? of the
second phase. In the second stage, acceptor rules
match SLU results to dialogue context, for ex-
ample perceived user concepts to open questions.
This may result in the decision to verify the ap-
plication parameter in question, and the action is
verbalized by language generation rules. If the
parameter is accepted, application dependent task
157
Figure 2: A screenshot of the online visualization tool. Left: user goal (top), evolving ranked state space
(bottom). Center: per state action distribution at turn ti. Right: consequent reward computation (top) and
policy updates (bottom). See video at http://www.youtube.com/watch?v=69QR0tKKhCw.
158
Figure 3: Left Pane: overview of a selection of dialogues in our visualization tool. Right Pane: visual-
ization of a system opening prompt followed by the user?s activity request. All distinct SLU hypotheses
(concept-value combinations) deriving from ASR are ranked based on concept-level confidence (2 in this
turn).
rules determine the next parameter to be acquired,
resulting in the generation of an appropriate re-
quest. See (Varges et al, 2008) for more details.
5 Visualization Tool
In addition to the POMDP-related visualization
tool (figure 2), we developed another web-based
dialogue tool for both rule-based and POMDP sys-
tem that displays ongoing and past dialogue ut-
terances, semantic interpretation confidences and
distributions of confidences for incoming user acts
(see dialogue logs in figure 3).
Users are able to talk with several systems
(via SIP phone connection to the dialogue system
server) and see their dialogues in the visualization
tool. They are able to compare the rule-based
system, a randomly exploring learner that has not
been trained yet, and several systems that use vari-
ous pre-trained policies. The web tool is available
at http://cicerone.dit.unitn.it/
DialogStatistics/.
Acknowledgments
This work was partially supported by the Euro-
pean Commission Marie Curie Excellence Grant
for the ADAMACH project (contract No. 022593)
and by LUNA STREP project (contract No.
33549).
References
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21(2):97?126.
Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-
teroni. 2008. Persistent information state in a data-
centric architecture. In Proc. 9th SIGdial Workhop
on Discourse and Dialogue, Columbus, Ohio.
J. D. Williams and S. Young. 2006. Partially Ob-
servable Markov Decision Processes for Spoken Di-
alog Systems. Computer Speech and Language,
21(2):393?422.
159
Adaptivity in Question Answering
with User Modelling and a Dialogue Interface
Silvia Quarteroni and Suresh Manandhar
Department of Computer Science
University of York
York YO10 5DD
UK
{silvia,suresh}@cs.york.ac.uk
Abstract
Most question answering (QA) and infor-
mation retrieval (IR) systems are insensi-
tive to different users? needs and prefer-
ences, and also to the existence of multi-
ple, complex or controversial answers. We
introduce adaptivity in QA and IR by cre-
ating a hybrid system based on a dialogue
interface and a user model. Keywords:
question answering, information retrieval,
user modelling, dialogue interfaces.
1 Introduction
While standard information retrieval (IR) systems
present the results of a query in the form of a
ranked list of relevant documents, question an-
swering (QA) systems attempt to return them in
the form of sentences (or paragraphs, or phrases),
responding more precisely to the user?s request.
However, in most state-of-the-art QA systems
the output remains independent of the questioner?s
characteristics, goals and needs. In other words,
there is a lack of user modelling: a 10-year-old and
a University History student would get the same
answer to the question: ?When did the Middle
Ages begin??. Secondly, most of the effort of cur-
rent QA is on factoid questions, i.e. questions con-
cerning people, dates, etc., which can generally be
answered by a short sentence or phrase (Kwok et
al., 2001). The main QA evaluation campaign,
TREC-QA 1, has long focused on this type of
questions, for which the simplifying assumption is
that there exists only one correct answer. Even re-
cent TREC campaigns (Voorhees, 2003; Voorhees,
2004) do not move sufficiently beyond the factoid
approach. They account for two types of non-
factoid questions ?list and definitional? but not for
non-factoid answers. In fact, a) TREC defines list
questions as questions requiring multiple factoid
1http://trec.nist.gov
answers, b) it is clear that a definition question
may be answered by spotting definitional passages
(what is not clear is how to spot them). However,
accounting for the fact that some simple questions
may have complex or controversial answers (e.g.
?What were the causes of World War II??) remains
an unsolved problem. We argue that in such situa-
tions returning a short paragraph or text snippet is
more appropriate than exact answer spotting. Fi-
nally, QA systems rarely interact with the user:
the typical session involves the user submitting a
query and the system returning a result; the session
is then concluded.
To respond to these deficiencies of existing QA
systems, we propose an adaptive system where a
QA module interacts with a user model and a di-
alogue interface (see Figure 1). The dialogue in-
terface provides the query terms to the QA mod-
ule, and the user model (UM) provides criteria
to adapt query results to the user?s needs. Given
such information, the goal of the QA module is to
be able to discriminate between simple/factoid an-
swers and more complex answers, presenting them
in a TREC-style manner in the first case and more
appropriately in the second.
DIALOGUE
INTERFACE
QUESTION
PROCESSING
DOCUMENT
RETRIEVAL
ANSWER
EXTRACTION
USER
MODEL
Question
Answer
QA MODULE
Figure 1: High level system architecture
Related work To our knowledge, our system is
among the first to address the need for a different
approach to non-factoid (complex/controversial)
199
answers. Although the three-tiered structure of
our QA module reflects that of a typical web-
based QA system, e.g. MULDER (Kwok et al,
2001), a significant aspect of novelty in our archi-
tecture is that the QA component is supported by
the user model. Additionally, we drastically re-
duce the amount of linguistic processing applied
during question processing and answer generation,
while giving more relief to the post-retrieval phase
and to the role of the UM.
2 User model
Depending on the application of interest, the UM
can be designed to suit the information needs of
the QA module in different ways. As our current
application, YourQA2, is a learning-oriented, web-
based system, our UM consists of the user?s:
1) age range, a ? {7 ? 11, 11 ? 16, adult};
2) reading level, r ? {poor,medium, good};
3) webpages of interest/bookmarks, w.
Analogies can be found with the SeAn (Ardissono
et al, 2001) and SiteIF (Magnini and Strapparava,
2001) news recommender systems where age and
browsing history, respectively, are part of the UM.
In this paper we focus on how to filter and adapt
search results using the reading level parameter.
3 Dialogue interface
The dialogue component will interact with both
the UM and the QA module. From a UM point of
view, the dialogue history will store previous con-
versations useful to construct and update a model
of the user?s interests, goals and level of under-
standing. From a QA point of view, the main goal
of the dialogue component is to provide users with
a friendly interface to build their requests. A typi-
cal scenario would start this way:
? System: Hi, how can I help you?
? User: I would like to know what books Roald Dahl wrote.
The query sentence ?what books Roald Dahl wrote?, is
thus extracted and handed to the QA module. In a
second phase, the dialogue module is responsible
for providing the answer to the user once the QA
module has generated it. The dialogue manager
consults the UM to decide on the most suitable
formulation of the answer (e.g. short sentences)
and produce the final answer accordingly, e.g.:
? System: Roald Dahl wrote many books for kids and adults,
including: ?The Witches?, ?Charlie and the Chocolate Fac-
tory?, and ?James and the Giant Peach".
2http://www.cs.york.ac.uk/aig/aqua
4 Question Answering Module
The flow between the three QA phases ? question
processing, document retrieval and answer gener-
ation ? is described below (see Fig. 2).
4.1 Question processing
We perform query expansion, which consists in
creating additional queries using question word
synonyms in the purpose of increasing the recall
of the search engine. Synonyms are obtained via
the WordNet 2.0 3 lexical database.
Question QUERY
EXPANSION
DOCUMENT
RETRIEVAL
KEYPHRASE
EXTRACTION
ESTIMATION
OF READING
LEVELS
CLUSTERING
Language
Models
UM-BASED
FILTERING
SEMANTIC
SIMILARITY
RANKING
User Model
Reading
Level
Ranked
Answer
Candidates
Figure 2: Diagram of the QA module
4.2 Retrieval
Document retrieval We retrieve the top 20 doc-
uments returned by Google4 for each query pro-
duced via query expansion. These are processed
in the following steps, which progressively narrow
the part of the text containing relevant informa-
tion.
Keyphrase extraction Once the documents are
retrieved, we perform keyphrase extraction to de-
termine their three most relevant topics using Kea
(Witten et al, 1999), an extractor based on Na?ve
Bayes classification.
Estimation of reading levels To adapt the read-
ability of the results to the user, we estimate
the reading difficulty of the retrieved documents
using the Smoothed Unigram Model (Collins-
Thompson and Callan, 2004), which proceeds in
3http://wordnet.princeton.edu
4http://www.google.com
200
two phases. 1) In the training phase, sets of repre-
sentative documents are collected for a given num-
ber of reading levels. Then, a unigram language
model is created for each set, i.e. a list of (word
stem, probability) entries for the words appearing
in its documents. Our models account for the fol-
lowing reading levels: poor (suitable for ages 7?
11), medium (ages 11?16) and good (adults). 2)
In the test phase, given an unclassified document
D, its estimated reading level is the model lmi
maximizing the likelihood that D ? lmi5.
Clustering We use the extracted topics and es-
timated reading levels as features to apply hierar-
chical clustering on the documents. We use the
WEKA (Witten and Frank, 2000) implementation
of the Cobweb algorithm. This produces a tree
where each leaf corresponds to one document, and
sibling leaves denote documents with similar top-
ics and reading difficulty.
4.3 Answer extraction
In this phase, the clustered documents are filtered
based on the user model and answer sentences are
located and formatted for presentation.
UM-based filtering The documents in the clus-
ter tree are filtered according to their reading diffi-
culty: only those compatible with the UM?s read-
ing level are retained for further analysis6.
Semantic similarity Within each of the retained
documents, we seek the sentences which are se-
mantically most relevant to the query by applying
the metric in (Alfonseca et al, 2001): we rep-
resent each document sentence p and the query
q as word sets P = {pw1, . . . , pwm} and Q =
{qw1, . . . , qwn}. The distance from p to q is then
distq(p) =
?
1?i?m minj [d(pwi, qwj)], where
d(pwi, qwj) is the word-level distance between
pwi and qwj based on (Jiang and Conrath, 1997).
Ranking Given the query q, we thus locate
in each document D the sentence p? such that
p? = argminp?D[distq(p)]; then, distq(p?) be-
comes the document score. Moreover, each clus-
5The likelihood is estimated using the formula:
Li,D =
?
w?D C(w, D) ? log(P (w|lmi)), where w is a
word in the document, C(w, d) is the number of occurrences
of w in D and P (w|lmi) is the probability with which w
occurs in lmi
6However, if their number does not exceed a given thresh-
old, we accept in our candidate set part of the documents hav-
ing the next lowest readability ? or a medium readability if the
user?s reading level is low
ter is assigned a score consisting in the maximal
score of the documents composing it. This allows
to rank not only documents, but also clusters, and
present results grouped by cluster in decreasing or-
der of document score.
Answer presentation We present our answers
in an HTML page, where results are listed follow-
ing the ranking described above. Each result con-
sists of the title and clickable URL of the originat-
ing document, and the passage where the sentence
which best answers the query is located and high-
lighted. Question keywords and potentially useful
information such as named entities are in colour.
5 Sample result
We have been running our system on a range
of queries, including factoid/simple, complex and
controversial ones. As an example of the latter, we
report the query ?Who wrote the Iliad??, which is
a subject of debate. These are some top results:
? UMgood: ?Most Classicists would agree that, whether
there was ever such a composer as "Homer" or not, the
Homeric poems are the product of an oral tradition [. . . ]
Could the Iliad and Odyssey have been oral-formulaic po-
ems, composed on the spot by the poet using a collection of
memorized traditional verses and phases??
? UMmed: ?No reliable ancient evidence for Homer ?
[. . . ] General ancient assumption that same poet wrote Il-
iad and Odyssey (and possibly other poems) questioned by
many modern scholars: differences explained biographi-
cally in ancient world (e g wrote Od. in old age); but simi-
larities could be due to imitation.?
? UMpoor: ?Homer wrote The Iliad and The Odyssey
(at least, supposedly a blind bard named "Homer" did).?
In the three results, the problem of attribution of
the Iliad is made clearly visible: document pas-
sages provide a context which helps to explain the
controversy at different levels of difficulty.
6 Evaluation
Since YourQA does not single out one correct an-
swer phrase, TREC evaluation metrics are not suit-
able for it. A user-centred methodology to assess
how individual information needs are met is more
appropriate. We base our evaluation on (Su, 2003),
which proposes a comprehensive search engine
evaluation model, defining the following metrics:
1. Relevance: we define strict precision (P1) as
the ratio between the number of results rated as
relevant and all the returned results, and loose pre-
201
cision (P2) as the ratio between the number of re-
sults rated as relevant or partially relevant and all
the returned results.
2. User satisfaction: a 7-point Likert scale7 is used
to assess the user?s satisfaction with loose preci-
sion of results (S1) and query success (S2).
3. Reading level accuracy: given the set R of re-
sults returned for a reading level r, Ar is the ratio
between the number of results ? R rated by the
users as suitable for r and |R|.
4. Overall utility (U ): the search session as a
whole is assessed via a 7-point Likert scale.
We performed our evaluation by running 24
queries (some of which in Tab. 2) on Google and
YourQA and submitting the results ?i.e. Google
result page snippets and YourQA passages? of
both to 20 evaluators, along with a questionnaire.
The relevance results (P1 and P2) in Tab. 1 show a
P1 P2 S1 S2 U
Google 0,39 0,63 4,70 4,61 4,59
YourQA 0,51 0,79 5,39 5,39 5,57
Table 1: Evaluation results
10-15% difference in favour of YourQA for both
strict and loose precision. The coarse seman-
tic processing applied and context visualisation
thus contribute to creating more relevant passages.
Both user satisfaction results (S1 and S2) in Tab.
1 also denote a higher level of satisfaction tributed
to YourQA. Tab. 2 shows that evaluators found our
Query Ag Am Ap
When did the Middle Ages begin? 0,91 0,82 0,68
Who painted the Sistine Chapel? 0,85 0,72 0,79
When did the Romans invade Britain? 0,87 0,74 0,82
Who was a famous cubist? 0,90 0,75 0,85
Who was the first American in space? 0,94 0,80 0,72
Definition of metaphor 0,95 0,81 0,38
average 0,94 0,85 0,72
Table 2: Sample queries and accuracy values
results appropriate for the reading levels to which
they were assigned. The accuracy tended to de-
crease (from 94% to 72%) with the level: it is
indeed more constraining to conform to a lower
reading level than to a higher one. Finally, the
7This measure ? ranging from 1= ?extremely unsatisfac-
tory? to 7=?extremely satisfactory? ? is particularly suitable
to assess how well a system meets user?s search needs.
general satisfaction values for U in Tab. 1 show
an improved preference for YourQA.
7 Conclusion
A user-tailored QA system is proposed where a
user model contributes to adapting answers to the
user?s needs and presenting them appropriately.
A preliminary evaluation of our core QA module
shows a positive feedback from human assessors.
Our short term goals involve performing a more
extensive evaluation and implementing a dialogue
interface to improve the system?s interactivity.
References
E. Alfonseca, M. DeBoni, J.-L. Jara-Valencia, and
S. Manandhar. 2001. A prototype question answer-
ing system using syntactic and semantic information
for answer retrieval. In Text REtrieval Conference.
L. Ardissono, L. Console, and I. Torre. 2001. An adap-
tive system for the personalized access to news. AI
Commun., 14(3):129?147.
K. Collins-Thompson and J. P. Callan. 2004. A lan-
guage modeling approach to predicting reading dif-
ficulty. In Proceedings of HLT/NAACL.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference Re-
search on Computational Linguistics (ROCLING X).
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In World Wide
Web, pages 150?161.
Bernardo Magnini and Carlo Strapparava. 2001. Im-
proving user modelling with content-based tech-
niques. In UM: Proceedings of the 8th Int. Confer-
ence, volume 2109 of LNCS. Springer.
L. T. Su. 2003. A comprehensive and systematic
model of user evaluation of web search engines: Ii.
an evaluation by undergraduates. J. Am. Soc. Inf.
Sci. Technol., 54(13):1193?1223.
E. M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Text REtrieval Confer-
ence.
E. M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Text REtrieval Confer-
ence.
H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementation. Morgan Kaufmann.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In ACM DL, pages
254?255.
202
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776?783,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Exploiting Syntactic and Shallow Semantic Kernels
for Question/Answer Classification
Alessandro Moschitti
University of Trento
38050 Povo di Trento
Italy
moschitti@dit.unitn.it
Silvia Quarteroni
The University of York
York YO10 5DD
United Kingdom
silvia@cs.york.ac.uk
Roberto Basili
?Tor Vergata? University
Via del Politecnico 1
00133 Rome, Italy
basili@info.uniroma2.it
Suresh Manandhar
The University of York
York YO10 5DD
United Kingdom
suresh@cs.york.ac.uk
Abstract
We study the impact of syntactic and shallow
semantic information in automatic classifi-
cation of questions and answers and answer
re-ranking. We define (a) new tree struc-
tures based on shallow semantics encoded
in Predicate Argument Structures (PASs)
and (b) new kernel functions to exploit the
representational power of such structures
with Support Vector Machines. Our ex-
periments suggest that syntactic information
helps tasks such as question/answer classifi-
cation and that shallow semantics gives re-
markable contribution when a reliable set of
PASs can be extracted, e.g. from answers.
1 Introduction
Question answering (QA) is as a form of informa-
tion retrieval where one or more answers are re-
turned to a question in natural language in the form
of sentences or phrases. The typical QA system ar-
chitecture consists of three phases: question pro-
cessing, document retrieval and answer extraction
(Kwok et al, 2001).
Question processing is often centered on question
classification, which selects one of k expected an-
swer classes. Most accurate models apply super-
vised machine learning techniques, e.g. SNoW (Li
and Roth, 2005), where questions are encoded us-
ing various lexical, syntactic and semantic features.
The retrieval and answer extraction phases consist in
retrieving relevant documents (Collins-Thompson et
al., 2004) and selecting candidate answer passages
from them. A further answer re-ranking phase is op-
tionally applied. Here, too, the syntactic structure
of a sentence appears to provide more useful infor-
mation than a bag of words (Chen et al, 2006), al-
though the correct way to exploit it is still an open
problem.
An effective way to integrate syntactic structures
in machine learning algorithms is the use of tree ker-
nel (TK) functions (Collins and Duffy, 2002), which
have been successfully applied to question classifi-
cation (Zhang and Lee, 2003; Moschitti, 2006) and
other tasks, e.g. relation extraction (Zelenko et al,
2003; Moschitti, 2006). In more complex tasks such
as computing the relatedness between questions and
answers in answer re-ranking, to our knowledge no
study uses kernel functions to encode syntactic in-
formation. Moreover, the study of shallow semantic
information such as predicate argument structures
annotated in the PropBank (PB) project (Kingsbury
and Palmer, 2002) (www.cis.upenn.edu/?ace) is a
promising research direction. We argue that seman-
tic structures can be used to characterize the relation
between a question and a candidate answer.
In this paper, we extensively study new structural
representations, encoding parse trees, bag-of-words,
POS tags and predicate argument structures (PASs)
for question classification and answer re-ranking.
We define new tree representations for both simple
and nested PASs, i.e. PASs whose arguments are
other predicates (Section 2). Moreover, we define
new kernel functions to exploit PASs, which we au-
tomatically derive with our SRL system (Moschitti
et al, 2005) (Section 3).
Our experiments using SVMs and the above ker-
776
nels and data (Section 4) shows the following: (a)
our approach reaches state-of-the-art accuracy on
question classification. (b) PB predicative structures
are not effective for question classification but show
promising results for answer classification on a cor-
pus of answers to TREC-QA 2001 description ques-
tions. We created such dataset by using YourQA
(Quarteroni and Manandhar, 2006), our basic Web-
based QA system1. (c) The answer classifier in-
creases the ranking accuracy of our QA system by
about 25%.
Our results show that PAS and syntactic parsing
are promising methods to address tasks affected by
data sparseness like question/answer categorization.
2 Encoding Shallow Semantic Structures
Traditionally, information retrieval techniques are
based on the bag-of-words (BOW) approach aug-
mented by language modeling (Allan et al, 2002).
When the task requires the use of more complex se-
mantics, the above approaches are often inadequate
to perform fine-level textual analysis.
An improvement on BOW is given by the use of
syntactic parse trees, e.g. for question classification
(Zhang and Lee, 2003), but these, too are inadequate
when dealing with definitional answers expressed by
long and articulated sentences or even paragraphs.
On the contrary, shallow semantic representations,
bearing a more ?compact? information, could pre-
vent the sparseness of deep structural approaches
and the weakness of BOW models.
Initiatives such as PropBank (PB) (Kingsbury
and Palmer, 2002) have made possible the design of
accurate automatic Semantic Role Labeling (SRL)
systems (Carreras and Ma`rquez, 2005). Attempting
an application of SRL to QA hence seems natural,
as pinpointing the answer to a question relies on a
deep understanding of the semantics of both.
Let us consider the PB annotation: [ARG1
Antigens] were [AM?TMP originally] [rel
defined] [ARG2 as non-self molecules].
Such annotation can be used to design a shallow
semantic representation that can be matched against
other semantically similar sentences, e.g. [ARG0
Researchers] [rel describe] [ARG1 antigens]
[ARG2 as foreign molecules] [ARGM?LOC in
1Demo at: http://cs.york.ac.uk/aig/aqua.
PAS
rel
define
ARG1
antigens
ARG2
molecules
ARGM-TMP
originally
PAS
rel
describe
ARG0
researchers
ARG1
antigens
ARG2
molecules
ARGM-LOC
body
Figure 1: Compact predicate argument structures of
two different sentences.
the body].
For this purpose, we can represent the above anno-
tated sentences using the tree structures described in
Figure 1. In this compact representation, hereafter
Predicate-Argument Structures (PAS), arguments
are replaced with their most important word ? often
referred to as the semantic head. This reduces
data sparseness with respect to a typical BOW
representation.
However, sentences rarely contain a single pred-
icate; it happens more generally that propositions
contain one or more subordinate clauses. For
instance let us consider a slight modification of the
first sentence: ?Antigens were originally defined
as non-self molecules which bound specifically to
antibodies2 .? Here, the main predicate is ?defined?,
followed by a subordinate predicate ?bound?. Our
SRL system outputs the following two annotations:
(1) [ARG1 Antigens] were [ARGM?TMP
originally] [rel defined] [ARG2 as non-self
molecules which bound specifically to
antibodies].
(2) Antigens were originally defined as
[ARG1 non-self molecules] [R?A1 which] [rel
bound] [ARGM?MNR specifically] [ARG2 to
antibodies].
giving the PASs in Figure 2.(a) resp. 2.(b).
As visible in Figure 2.(a), when an argument node
corresponds to an entire subordinate clause, we label
its leaf with PAS, e.g. the leaf of ARG2. Such PAS
node is actually the root of the subordinate clause
in Figure 2.(b). Taken as standalone, such PASs do
not express the whole meaning of the sentence; it
is more accurate to define a single structure encod-
ing the dependency between the two predicates as in
2This is an actual answer to ?What are antibodies?? from
our question answering system, YourQA.
777
PAS
rel
define
ARG1
antigens
ARG2
PAS
AM-TMP
originally
(a)
PAS
rel
bound
ARG1
molecules
R-ARG1
which
AM-ADV
specifically
ARG2
antibodies
(b)
PAS
rel
define
ARG1
antigens
ARG2
PAS
rel
bound
ARG1
molecules
R-ARG1
which
AM-ADV
specifically
ARG2
antibodies
AM-TMP
originally
(c)
Figure 2: Two PASs composing a PASN
Figure 2.(c). We refer to nested PASs as PASNs.
It is worth to note that semantically equivalent
sentences syntactically expressed in different ways
share the same PB arguments and the same PASs,
whereas semantically different sentences result in
different PASs. For example, the sentence: ?Anti-
gens were originally defined as antibodies which
bound specifically to non-self molecules?, uses the
same words as (2) but has different meaning. Its PB
annotation:
(3) Antigens were originally defined
as [ARG1 antibodies] [R?A1 which] [rel
bound] [ARGM?MNR specifically] [ARG2 to
non-self molecules],
clearly differs from (2), as ARG2 is now non-
self molecules; consequently, the PASs are also
different.
Once we have assumed that parse trees and PASs
can improve on the simple BOW representation, we
face the problem of representing tree structures in
learning machines. Section 3 introduces a viable ap-
proach based on tree kernels.
3 Syntactic and Semantic Kernels for Text
As mentioned above, encoding syntactic/semantic
information represented by means of tree structures
in the learning algorithm is problematic. A first so-
lution is to use all its possible substructures as fea-
tures. Given the combinatorial explosion of consid-
ering subparts, the resulting feature space is usually
very large. A tree kernel (TK) function which com-
putes the number of common subtrees between two
syntactic parse trees has been given in (Collins and
Duffy, 2002). Unfortunately, such subtrees are sub-
ject to the constraint that their nodes are taken with
all or none of the children they have in the original
tree. This makes the TK function not well suited for
the PAS trees defined above. For instance, although
the two PASs of Figure 1 share most of the subtrees
rooted in the PAS node, Collins and Duffy?s kernel
would compute no match.
In the next section we describe a new kernel de-
rived from the above tree kernel, able to evaluate the
meaningful substructures for PAS trees. Moreover,
as a single PAS may not be sufficient for text rep-
resentation, we propose a new kernel that combines
the contributions of different PASs.
3.1 Tree kernels
Given two trees T1 and T2, let {f1, f2, ..} = F be
the set of substructures (fragments) and Ii(n) be
equal to 1 if fi is rooted at node n, 0 otherwise.
Collins and Duffy?s kernel is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2 ?(n1, n2), (1)
where NT1 and NT2 are the sets of nodes
in T1 and T2, respectively and ?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the number
of common fragments rooted in nodes n1 and n2. ?
can be computed as follows:
(1) if the productions (i.e. the nodes with their
direct children) at n1 and n2 are different then
?(n1, n2) = 0;
(2) if the productions at n1 and n2 are the same, and
n1 and n2 only have leaf children (i.e. they are pre-
terminal symbols) then ?(n1, n2) = 1;
(3) if the productions at n1 and n2 are the same, and
n1 and n2 are not pre-terminals then ?(n1, n2) =
?nc(n1)
j=1 (1+?(cjn1 , cjn2)), where nc(n1) is the num-
ber of children of n1 and cjn is the j-th child of n.
Such tree kernel can be normalized and a ? factor
can be added to reduce the weight of large structures
(refer to (Collins and Duffy, 2002) for a complete
description). The critical aspect of steps (1), (2) and
(3) is that the productions of two evaluated nodes
have to be identical to allow the match of further de-
scendants. This means that common substructures
cannot be composed by a node with only some of its
778
PAS
SLOT
rel
define
SLOT
ARG1
antigens
*
SLOT
ARG2
PAS
*
SLOT
ARGM-TMP
originally
*
(a)
PAS
SLOT
rel
define
SLOT
ARG1
antigens
*
SLOT
null
SLOT
null
(b)
PAS
SLOT
rel
define
SLOT
null
SLOT
ARG2
PAS
*
SLOT
null
(c)
Figure 3: A PAS with some of its fragments.
children as an effective PAS representation would
require. We solve this problem by designing the
Shallow Semantic Tree Kernel (SSTK) which allows
to match portions of a PAS.
3.2 The Shallow Semantic Tree Kernel (SSTK)
The SSTK is based on two ideas: first, we change
the PAS, as shown in Figure 3.(a) by adding SLOT
nodes. These accommodate argument labels in a
specific order, i.e. we provide a fixed number of
slots, possibly filled with null arguments, that en-
code all possible predicate arguments. For simplic-
ity, the figure shows a structure of just 4 arguments,
but more can be added to accommodate the max-
imum number of arguments a predicate can have.
Leaf nodes are filled with the wildcard character *
but they may alternatively accommodate additional
information.
The slot nodes are used in such a way that the
adopted TK function can generate fragments con-
taining one or more children like for example those
shown in frames (b) and (c) of Figure 3. As pre-
viously pointed out, if the arguments were directly
attached to the root node, the kernel function would
only generate the structure with all children (or the
structure with no children, i.e. empty).
Second, as the original tree kernel would generate
many matches with slots filled with the null label,
we have set a new step 0:
(0) if n1 (or n2) is a pre-terminal node and its child
label is null, ?(n1, n2) = 0;
and subtract one unit to ?(n1, n2), in step 3:
(3) ?(n1, n2) =
?nc(n1)
j=1 (1 + ?(cjn1 , cjn2))? 1,
The above changes generate a new ? which,
when substituted (in place of the original ?) in Eq.
1, gives the new Shallow Semantic Tree Kernel. To
show that SSTK is effective in counting the number
of relations shared by two PASs, we propose the fol-
lowing:
Proposition 1 The new ? function applied to the
modified PAS counts the number of all possible k-
ary relations derivable from a set of k arguments,
i.e.
?k
i=1
(k
i
)
relations of arity from 1 to k (the pred-
icate being considered as a special argument).
Proof We observe that a kernel applied to a tree and
itself computes all its substructures, thus if we eval-
uate SSTK between a PAS and itself we must obtain
the number of generated k-ary relations. We prove
by induction the above claim.
For the base case (k = 0): we use a PAS with no
arguments, i.e. all its slots are filled with null la-
bels. Let r be the PAS root; since r is not a pre-
terminal, step 3 is selected and ? is recursively ap-
plied to all r?s children, i.e. the slot nodes. For the
latter, step 0 assigns ?(cjr, cjr) = 0. As a result,
?(r, r) = ?nc(r)j=1 (1 + 0)? 1 = 0 and the base case
holds.
For the general case, r is the root of a PAS with k+1
arguments. ?(r, r) = ?nc(r)j=1 (1 + ?(cjr, cjr)) ? 1
=
?k
j=1(1+?(cjr , cjr))?(1+?(ck+1r , ck+1r ))?1. For
k arguments, we assume by induction that?kj=1(1+
?(cjr, cjr))? 1 =
?k
i=1
(k
i
)
, i.e. the number of k-ary
relations. Moreover, (1 + ?(ck+1r , ck+1r )) = 2, thus
?(r, r) = ?ki=1
(k
i
)
? 2 = 2k ? 2 = 2k+1 = ?k+1i=1
(k+1
i
)
, i.e. all the relations until arity k + 1 2
TK functions can be applied to sentence parse
trees, therefore their usefulness for text processing
applications, e.g. question classification, is evident.
On the contrary, the SSTK applied to one PAS ex-
tracted from a text fragment may not be meaningful
since its representation needs to take into account all
the PASs that it contains. We address such problem
779
by defining a kernel on multiple PASs.
Let Pt and Pt? be the sets of PASs extracted from
the text fragment t and t?. We define:
Kall(Pt, Pt?) =
?
p?Pt
?
p??Pt?
SSTK(p, p?), (2)
While during the experiments (Sect. 4) the Kall
kernel is used to handle predicate argument struc-
tures, TK (Eq. 1) is used to process parse trees and
the linear kernel to handle POS and BOW features.
4 Experiments
The purpose of our experiments is to study the im-
pact of the new representations introduced earlier for
QA tasks. In particular, we focus on question clas-
sification and answer re-ranking for Web-based QA
systems.
In the question classification task, we extend pre-
vious studies, e.g. (Zhang and Lee, 2003; Moschitti,
2006), by testing a set of previously designed ker-
nels and their combination with our new Shallow Se-
mantic Tree Kernel. In the answer re-ranking task,
we approach the problem of detecting description
answers, among the most complex in the literature
(Cui et al, 2005; Kazawa et al, 2001).
The representations that we adopt are: bag-of-
words (BOW), bag-of-POS tags (POS), parse tree
(PT), predicate argument structure (PAS) and nested
PAS (PASN). BOW and POS are processed by
means of a linear kernel, PT is processed with TK,
PAS and PASN are processed by SSTK. We imple-
mented the proposed kernels in the SVM-light-TK
software available at ai-nlp.info.uniroma2.it/
moschitti/ which encodes tree kernel functions in
SVM-light (Joachims, 1999).
4.1 Question classification
As a first experiment, we focus on question classi-
fication, for which benchmarks and baseline results
are available (Zhang and Lee, 2003; Li and Roth,
2005). We design a question multi-classifier by
combining n binary SVMs3 according to the ONE-
vs-ALL scheme, where the final output class is the
one associated with the most probable prediction.
The PASs were automatically derived by our SRL
3We adopted the default regularization parameter (i.e., the
average of 1/||~x||) and tried a few cost-factor values to adjust
the rate between Precision and Recall on the development set.
system which achieves a 76% F1-measure (Mos-
chitti et al, 2005).
As benchmark data, we use the question train-
ing and test set available at: l2r.cs.uiuc.edu/
?cogcomp/Data/QA/QC/, where the test set are the
500 TREC 2001 test questions (Voorhees, 2001).
We refer to this split as UIUC. The performance of
the multi-classifier and the individual binary classi-
fiers is measured with accuracy resp. F1-measure.
To collect statistically significant information, we
run 10-fold cross validation on the 6,000 questions.
Features Accuracy (UIUC) Accuracy (c.v.)
PT 90.4 84.8?1.2
BOW 90.6 84.7?1.2
PAS 34.2 43.0?1.9
POS 26.4 32.4?2.1
PT+BOW 91.8 86.1?1.1
PT+BOW+POS 91.8 84.7?1.5
PAS+BOW 90.0 82.1?1.3
PAS+BOW+POS 88.8 81.0?1.5
Table 1: Accuracy of the question classifier with dif-
ferent feature combinations
Question classification results Table 1 shows the
accuracy of different question representations on the
UIUC split (Column 1) and the average accuracy ?
the corresponding confidence limit (at 90% signifi-
cance) on the cross validation splits (Column 2).(i)
The TK on PT and the linear kernel on BOW pro-
duce a very high result, i.e. about 90.5%. This is
higher than the best outcome derived in (Zhang and
Lee, 2003), i.e. 90%, obtained with a kernel combin-
ing BOW and PT on the same data. Combined with
PT, BOW reaches 91.8%, very close to the 92.5%
accuracy reached in (Li and Roth, 2005) using com-
plex semantic information from external resources.
(ii) The PAS feature provides no improvement. This
is mainly because at least half of the training and
test questions only contain the predicate ?to be?, for
which a PAS cannot be derived by a PB-based shal-
low semantic parser.
(iii) The 10-fold cross-validation experiments con-
firm the trends observed in the UIUC split. The
best model (according to statistical significance) is
PT+BOW, achieving an 86.1% average accuracy4.
4This value is lower than the UIUC split one as the UIUC
test set is not consistent with the training set (it contains the
780
4.2 Answer classification
Question classification does not allow to fully ex-
ploit the PAS potential since questions tend to be
short and with few verbal predicates (i.e. the only
ones that our SRL system can extract). A differ-
ent scenario is answer classification, i.e. deciding
if a passage/sentence correctly answers a question.
Here, the semantics to be generated by the classi-
fier are not constrained to a small taxonomy and an-
swer length may make the PT-based representation
too sparse.
We learn answer classification with a binary SVM
which determines if an answer is correct for the tar-
get question: here, the classification instances are
?question, answer? pairs. Each pair component can
be encoded with PT, BOW, PAS and PASN repre-
sentations (processed by previous kernels).
As test data, we collected the 138 TREC 2001 test
questions labeled as ?description? and for each, we
obtained a list of answer paragraphs extracted from
Web documents using YourQA. Each paragraph sen-
tence was manually evaluated based on whether it
contained an answer to the corresponding question.
Moreover, to simplify the classification problem, we
isolated for each paragraph the sentence which ob-
tained the maximal judgment (in case more than one
sentence in the paragraph had the same judgment,
we chose the first one). We collected a corpus con-
taining 1309 sentences, 416 of which ? labeled ?+1?
? answered the question either concisely or with
noise; the rest ? labeled ?-1?? were either irrele-
vant to the question or contained hints relating to the
question but could not be judged as valid answers5.
Answer classification results To test the impact
of our models on answer classification, we ran 5-fold
cross-validation, with the constraint that two pairs
?q, a1? and ?q, a2? associated with the same ques-
tion q could not be split between training and test-
ing. Hence, each reported value is the average over 5
different outcomes. The standard deviations ranged
TREC 2001 questions) and includes a larger percentage of eas-
ily classified question types, e.g. the numeric (22.6%) and de-
scription classes (27.6%) whose percentage in training is 16.4%
resp. 16.2%.
5For instance, given the question ?What are invertebrates??,
the sentence ?At least 99% of all animal species are inverte-
brates, comprising . . . ? was labeled ?-1? , while ?Invertebrates
are animals without backbones.? was labeled ?+1?.
  
  
   
   
    
    
   
   
  
  
      	  	                 

         







Incorporating User Models in Question Answering to Improve Readability
Silvia Quarteroni and Suresh Manandhar
Department of Computer Science
University of York
York YO10 5DD
United Kingdom
{silvia,suresh}@cs.york.ac.uk
Abstract
Most question answering and information
retrieval systems are insensitive to differ-
ent users? needs and preferences, as well
as their reading level. In (Quarteroni and
Manandhar, 2006), we introduce a hybrid
QA-IR system based on a a user model.
In this paper we focus on how the system
filters and re-ranks the search engine re-
sults for a query according to their reading
difficulty, providing user-tailored answers.
Keywords: question answering, informa-
tion retrieval, user modelling, readability.
1 Introduction
Question answering (QA) systems are information
retrieval systems accepting queries in natural lan-
guage and returning the results in the form of sen-
tences (or paragraphs, or phrases). They move
beyond standard information retrieval (IR) where
results are presented in the form of a ranked list
of query-relevant documents. Such a finer answer
presentation is possible thanks to the application
of computational linguistics techniques in order
to filter irrelevant documents, and of a consistent
amount of question pre-processing and result post-
processing.
However, in most state-of-the-art QA systems
the output remains independent of the questioner?s
characteristics, goals and needs; in other words,
there is a lack of user modelling. For instance, an
elementary school child and a University history
student would get the same answer to the question:
?When did the Middle Ages begin??.
Secondly, most QA systems focus on factoid
questions, i.e. questions concerning people, dates,
numerical quantities etc., which can generally be
answered by a short sentence or phrase (Kwok et
al., 2001). The mainstream approach to QA evalu-
ation, represented by TREC-QA campaigns1, has
long fostered the criterion that a ?good? system
is one that returns the ?correct? answer in the
shortest possible formulation. Although recent ef-
forts in TREC 2003 and 2004 (Voorhees, 2003;
Voorhees, 2004) denoted an interest towards list
questions and definitional (or ?other?) questions,
we believe that there has not been enough inter-
est towards non-factoid answers. The real issue is
?realizing? that the answer to a question is some-
times too complex to be formulated and evaluated
as a factoid: some queries have multiple, com-
plex or controversial answers (take e.g. ?What
were the causes of World War II??). In such sit-
uations, returning a short paragraph or text snip-
pet is more appropriate than exact answer spot-
ting. For instance, the answer to ?What is a
metaphor?? may be better understood with the in-
clusion of examples. This viewpoint is supported
by recent user behaviour studies which showed
that even in the case of factoid-based QA systems,
the most eligible result format consisted in a para-
graph where the sentence containing the answer
was highlighted (Lin et al, 2003).
The issue of non-factoids is related to the user
modelling problem: while factoid answers do not
necessarily require to be contextualized within the
user?s knowledge and viewpoint, the need is much
stronger in the case of definitions, explanations
and descriptions. This is mentioned in the TREC
2003 report (Voorhees, 2003) when discussing the
evaluation of definitional questions: however, the
issue is expeditiously solved by assuming a fixed
user profile (the ?average news reader?).
We are currently developing an adaptive sys-
tem which adjusts its output with respect to a user
model. The system can be seen as an enhanced IR
system which adapts both the content and presen-
tation of the final results, improving their quality.
1http://trec.nist.gov
50 KRAQ06
In this paper, we show that QA systems can benefit
from the contribution of user models, and explain
how these can be used to filter the information pre-
sented as an answer based on readability. Eventu-
ally, we describe preliminary results obtained via
an evaluation framework inspired by user-centered
search engine evaluation.
2 System Architecture
The high-level architecture as represented in Fig-
ure 1 shows the basic components of the system,
the QA module and the user model.
QUESTION 
PROCESSING
DOCUMENT
RETRIEVAL
ANSWER 
EXTRACTION
Question
Answer
QA MODULE
USER MODEL
Webpage
Webpages
Age Range
Reading 
Level
Figure 1: High level system architecture
The QA module, described in the following sec-
tion, is organized according to the three-tier parti-
tion underlying most state-of-the-art systems: 1)
question processing, 2) document retrieval, 3) an-
swer generation. The module makes use of a web
search engine for document retrieval and consults
the user model to obtain the criteria to filter and
re-rank the search engine results and to eventually
present them appropriately to the user.
2.1 User model
Depending on the application of interest, the user
model (UM) can be designed to suit the informa-
tion needs of the QA module in different ways.
Our current application, YourQA2, is a learning-
oriented system to help students find information
on the Web for their assignments. Our UM con-
sists of the user?s:
? age range, a ? {7? 11, 11? 16, adult}
? reading level, r ? {poor,medium, good}
? webpages of interest/bookmarks, w
The age range parameter has been chosen to
match the partition between primary school, con-
temporary school and higher education age in
2http://www.cs.york.ac.uk/aig/aqua
Britain; our reading level parameter takes three
values which ideally (but not necessarily) corre-
spond to the three age ranges and may be further
refined in the future for more fine-grained mod-
elling.
Analogies can be found with the SeAn (Ardis-
sono et al, 2001), and SiteIF (Magnini and Strap-
parava, 2001) news recommender systems, where
information such as age and browsing history,
resp. are part of the UM. More generally, our
approach is similar to that of personalized search
systems (Teevan et al, 2005; Pitkow et al, 2002),
which construct UMs based on the user?s docu-
ments and webpages.
In our system, UM information is explicitly col-
lected from the user; while age and reading level
are self-assessed, the user?s interests are extracted
from the document set w using a keyphrase ex-
tractor (see further for details). Eventually, a di-
alogue framework with a history component will
contribute to the construction and update of the
user model in a less intruding and thus more user-
friendly way. In this paper we focus on how to
adapt search result presentation using the reading
level parameter: age and webpages will not be dis-
cussed.
2.2 Related work
Non-factoids and user modelling As men-
tioned above, the TREC-QA evaluation campaign,
to which the vast majority of current QA systems
abide, mainly approaches factoid-based answers.
To our knowledge, our system is among the first to
address the need for a different approach to non-
factoid answers. The structure of our QA compo-
nent reflects the typical structure of a web-based
QA system in its three-tier composition. Analo-
gies in this can be found for instance in MUL-
DER (Kwok et al, 2001), which is organized ac-
cording to a question processing/answer extrac-
tion/passage ranking pipeline. However, a signifi-
cant aspect of novelty in our architecture is that the
QA component is supported by the user model.
Additionally, we have changed the relative im-
portance of the different tiers: while we drastically
reduce linguistic processing during question pro-
cessing and answer generation, we give more re-
lief to the post-retrieval phase and to the role of
the UM. Having removed the need for fine-grained
answer spotting, the emphasis is shifted towards
finding closely connected sentences that are highly
51 KRAQ06
relevant to answer the query.
Readability Within computational linguistics,
several applications have been designed to address
the needs of users with low reading skills. The
computational approach to textual adaptation is
commonly based on natural language generation:
the process ?translate? a difficult text into a syntac-
tically and lexically simpler version. In the case of
PSET (Carroll et al, 1999) for instance, a tagger, a
morphological analyzer and generator and a parser
are used to reformulate newspaper text for users
affected by aphasia. Another interesting research
is Inui et al?s lexical and syntactical paraphrasing
system for deaf students (Inui et al, 2003). In this
system, the judgment of experts (teachers) is used
to learn selection rules for paraphrases acquired
using various methods (statistical, manual, etc.).
In the SKILLSUM project (Williams and Reiter,
2005), used to generate literacy test reports, a set
of choices regarding output (cue phrases, order-
ing and punctuation) are taken by a micro-planner
based on a set of rules.
Our approach is conceptually different from the
above: exploiting the wealth of information avail-
able in the context of a Web-based QA system, we
can afford to choose among the documents avail-
able on a given subject those which best suit our
readability requirements. This is possible thanks
to the versatility of language modelling, which al-
lows us to tailor the readability estimation of doc-
uments to any kind of user profile in a dynamic
manner, as explained in section 3.2.3.
3 QA Module
In this section we discuss the information flow
among the subcomponents of the QA module (see
Figure 2 for a representative diagram) and focus
on reading level estimation and document filter-
ing. For further details on the implementation of
the QA module, see (Quarteroni and Manandhar,
2006).
3.1 Question Processing
The first step performed by YourQA is query ex-
pansion: additional queries are created replacing
question terms with synonyms using WordNet3.
3http://wordnet.princeton.edu
Question
QUERY 
EXPANSION
DOCUMENT
RETRIEVAL
KEYPHRASE
EXTRACTION
ESTIMATION
OF READING
LEVELS
CLUSTERING
Language
Models
UM-BASED
FILTERING
SEMANTIC 
SIMILARITY
RANKING
User Model
Reading 
Level
Ranked
Answer
Candidates
Figure 2: Diagram of the QA module
3.2 Retrieval and Result Processing
3.2.1 Document retrieval
We use Google4 to retrieve the top 20 docu-
ments returned for each of the queries issued from
the query expansion phase. The subsequent steps
will progressively narrow the parts of these docu-
ments where relevant information is located.
3.2.2 Keyphrase extraction
Keyphrase extraction is useful in two ways:
first, it produces features to group the retrieved
documents thematically during the clustering
phase, and thus enables to present results by
groups. Secondly, when the document parame-
ter (w) of the UM is active, matches are sought
between the keyphrases extracted from the docu-
ments and those extracted from the user?s set of
interesting documents; thus it is possible to pri-
oritize results which are more compatible with
his/her interests.
Hence, once the documents are retrieved, we
extract their keyphrases using Kea (Witten et al,
1999), an extractor based on Na?ve Bayes classifi-
cation. Kea first splits each document into phrases
and then takes short subsequences of these initial
phrases as candidate keyphrases. Two attributes
are used to classify a phrase p as a keyphrase or
a non-keyphrase: its TF? IDF score within the
set of retrieved documents and the index of p?s
first appearance in the document. Kea outputs a
ranked list of phrases, among which we select the
top three as keyphrases for each of our documents.
4http://www.google.com
52 KRAQ06
3.2.3 Estimation of reading levels
In order to adjust search result presentation to
the user?s reading ability, we estimate the read-
ing difficulty of each retrieved document using the
Smoothed Unigram Model, a variation of a Multi-
nomial Bayes classifier (Collins-Thompson and
Callan, 2004). Whereas other popular approaches
such as Flesch-Kincaid (Kincaid et al, 1975) are
based on sentence length, the language modelling
approach accounts especially for lexical informa-
tion. The latter has been found to be more effective
as the former when approaching the reading level
of subjects in primary and secondary school age
(Collins-Thompson and Callan, 2004). Moreover,
it is more applicable than length-based approach
for Web documents, where sentences are typically
short regardless of the complexity of the text.
The language modelling approach proceeds in
two phases: in the training phase, given a range of
reading levels, a set of representative documents
is collected for each reading level. A unigram lan-
guage model lms is then built for each set s; the
model consists of a list of the word stems appear-
ing in the training documents with their individual
probabilities. Textual readability is not modelled
at a conceptual level: thus complex concepts ex-
plained in simple words might be classified as suit-
able even for a poor reading level; However we
have observed that in most Web documents lexi-
cal, syntactic and conceptual complexity are usu-
ally consistent within documents, hence it makes
sense to apply a reasoning-free technique with-
out impairing readability estimation. Our unigram
language models account for the following read-
ing levels:
1) poor, i.e. suitable for ages 7 ? 11;
2) medium, suitable for ages 11?16;
3) good, suitable for adults.
This partition in three groups has been chosen to
suit the training data available for our school appli-
cation, which consists of about 180 HTML pages
(mostly from the ?BBC schools?5, ?Think En-
ergy?6, ?Cassini Huygens resource for schools?7
and ?Magic Keys storybooks?8 websites), explic-
itly annotated by the publishers according to the
reading levels above.
In the test phase, given an unclassified docu-
5http://bbc.co.uk/schools
6http://www.think-energy.com
7http://www.pparc.ac.uk/Ed/ch/Home.htm
8http://www.magickeys.com/books/
ment D, the estimated reading level of D is the
language model lmi maximizing the likelihood
L(lmi|D) that D has been generated by lmi. Such
likelihood is estimated using the formula:
L(lmi|D) =
?
w?D
C(w,D) ? log(P (w|lmi))
where w is a word in the document, C(w, d) rep-
resents the number of occurrences of w in D and
P (w|lmi) is the probability that w occurs in lmi
(approached by its frequency).
An advantage of language modelling is its
portability, since it is quite quick to create word
stem/frequency histograms on the fly. This implies
that models can be produced to represent more
fine-grained reading levels as well as the specific
requirements of a single user: the only necessary
information are sets of training documents repre-
senting each level to be modelled.
3.2.4 Clustering
As an indicator of inter-document relatedness,
we use document clustering (Steinbach et al,
2000) to group them using both their estimated
reading difficulty and their topic (i.e. their
keyphrases). In particular we use a hierarchi-
cal algorithm, Cobweb (implemented using the
WEKA suite of tools (Witten and Frank, 2000) as
it produces a cluster tree which is visually sim-
ple to analyse: each leaf corresponds to one doc-
ument, and sibling leaves denote documents that
are strongly related both in topic and in reading
difficulty. Figure 3 illustrates an example clus-
ter tree for the the query: ?Who painted the Sis-
tine chapel??. Leaf labels represent document
keyphrases extracted by Kea for the corresponding
documents and ovals represent non-terminal nodes
in the cluster tree (these are labelled using the most
common keyphrases in their underlying leaves).
3.3 Answer Extraction
The purpose of answer extraction is to present the
most interesting excerpts of the retrieved docu-
ments according to both the user?s query topics
and reading level. This process, presented in sec-
tions 3.3.1 ? 3.3.4, follows the diagram in Figure
2: we use the UM to filter the clustered documents,
then compute the similarity between the question
and the filtered document passages in order to re-
turn the best ones in a ranked list.
53 KRAQ06
0 chapel
1 ceiling 4 michelangelo art
7 chapel
 ceiling 
paint 
pope 2
 painted 
ceiling 
frescoes 3
 art 
michelangelo 
paint 5
 art 
michelangelo 
download 6
8 chapel michelangelo
11 chapel
 michelangelo 
paintings 
chapel 9
 chapel 
michelangelo 
christ 10
12 chapel
 red_ball 
chapel 
vaticano 15
 chapel 
sistine_chapel 
walls 13
 fresco 
cappella_sistina 
chapel 14
Figure 3: Cluster tree for ?Who painted the Sistine
chapel??. Leaf 3 and the leaves grouped under
nodes 8 and 12 represent documents with an esti-
mated good reading level; leaf 15 and the leaves
underlying node 4 have a medium reading level;
leaf 2 represents a poor reading level document.
3.3.1 UM-based filtering
The documents in the cluster tree are filtered ac-
cording to the UM reading level, r: only those
compatible with the user?s reading ability are re-
tained for further analysis. However, if the num-
ber of retained documents does not exceed a given
threshold, we accept in our candidate set part of
the documents having the next lowest readability
in case r ? {good,medium} or a medium read-
ability in case r = poor.
3.3.2 Semantic similarity
Within each of the documents retained, we seek
for the sentences which are semantically most rel-
evant to the query. Given a sentence p and the
query q, we represent them as two sets of words
P = {pw1, . . . , pwm} and Q = {qw1, . . . , qwn}.
The semantic distance from p to q is then:
distq(p) =
?
1?i?m minj [d(pwi, qwj)]
where d(pwi, qwj) represents the Jiang-Conrath
word-level distance between pwi and qwj (Jiang
and Conrath, 1997), based on WordNet 2.0. The
intuition is that for each question word, we find the
word in the candidate answer sentence which min-
imizes the word-level distance and then we com-
pute the sum of such minima.
3.3.3 Passage and cluster ranking
For a given document, we can thus isolate a sen-
tence s minimizing the distance to the query. The
passage P , i.e. a window of up to 5 sentences cen-
tered on s, will be a candidate result. We assign
to such passage a score equal to the similarity of s
to the query; in turn, the score of P is used as the
score of the document containing it. We also de-
fine a ranking function for clusters, which allows
to order them according to the maximal score of
their component documents. Passages from the
highest ranking cluster will be presented first to
the user, in decreasing order of score, followed by
the passages from lower ranking clusters.
3.3.4 Answer presentation
To present our answers, we fix a threshold for
the number of results to be returned following the
ranking exposed above. Each result consists of
a title and document passage where the sentence
which best answers the query is highlighted; the
URL of the original document is also available for
loading if the user finds the passage interesting and
wants to read more.
4 Results
We report the results of running our system on
a range of queries, which include factoid/simple,
complex and controversial questions9.
4.1 Simple answer
As an example of a simple query, we present the
results for: ?Who painted the Sistine Chapel??,
the system returned the following passages:
?UMgood: ?Sistine Chapel (sis-teen). A chapel adjoin-
ing Saint Peter?s Basilica, noted for the frescoes of biblical
subject painted by Michelangelo on its walls and ceilings.?
?UMmed: ?In all Michelangelo painted more than 300
different figures on the Sistine Chapel ceiling.?
?UMpoor: ?My name is Jacopo L?Indaco and I was
an assistant to Michelangelo when he painted the Sistine
Chapel.?
To obtain the above answers the system was run 3
times with different values for the reading level pa-
rameter in the UM, as defined in 3.2.3. As we can
see, in all cases the correct information is present
although not always explicitly, as in the first two
cases. This is because our current semantic sim-
ilarity metric only operates at word level. In this
example, all sentences containing ?painted?, ?Sis-
tine? and ?Chapel? obtain a distance of 0 to the
query, regardless of their formulation. Also no-
tice how the difference in language complexity is
clearly discernible in the different answers.
4.2 Complex answer
We illustrate the results of our system with the
query ?Definition of metaphor?, which relates to
a difficult concept. Our top results, highlighted in
9Notice that this partition is not to be interpreted as a
methodological division, as we currently approach complex
and controversial answers the same way.
54 KRAQ06
boldface within their containing passages, are:
?UMgood: ?Definition of Metaphor. Metaphor is a pro-
cess by which we conceive "one thing in terms of another,
and its primary function is understanding" (Lakoff and
Johnson, Metaphors We Live By 36). That is, we often do
(and must) talk about things that do not physically exist, that
we cannot reach out and touch or take apart with our hands
to see what is inside and how it works: love, time, anger,
life. [. . . ] We make them tangible by, in a process of thought,
applying models of physical things to them: love is falling;
argument is war. ?
?UMmed: ?Definition: A metaphor is a figure of speech
that compares unlike things by saying that one thing is the
other. Often metaphors are simple comparisons, but they can
be extended so that different aspects of the things compared
are treated separately. ?
?UMpoor: we did not find any document exactly match-
ing the reading level, we thus returned the results forUMmed
(see above).
As before, there is a clear difference in the lan-
guage complexity of the two answers.
4.3 Controversial answer
We illustrate this case using the query ?Who wrote
the Iliad??, which is a subject of debate: the exis-
tence of a single author, as well as the historicity
of the poet Homer are under discussion. These are
among our top results:
?UMgood: ?Most Classicists would agree that, whether
there was ever such a composer as "Homer" or not, the
Homeric poems are the product of an oral tradition, a
generations-old technique that was the collective inheritance
of many singer-poets, aoidoi. [. . . ] Could the Iliad and
Odyssey have been oral-formulaic poems, composed on the
spot by the poet using a collection of memorized traditional
verses and phases??
?UMmed: ?No reliable ancient evidence for Homer ?
earliest traditions involve conjecture (e.g. conflicting claims
to be his place of origin) and legend (e.g. Homer as son
of river-god). General ancient assumption that same poet
wrote Iliad and Odyssey (and possibly other poems) ques-
tioned by many modern scholars: differences explained bi-
ographically in ancient world (e g wrote Od. in old age);
but similarities could be due to imitation.?
?UMpoor: ?Homer wrote The Iliad and The Odyssey (at
least, supposedly a blind bard named "Homer" did).?
In this case we can see how the problem of attri-
bution of the Iliad is made clearly visible: in the
three results, document passages provide a context
which helps to explain such controversy at differ-
ent levels of difficulty.
5 Evaluation
5.1 Methodology
Our system is not a QA system in the strict
sense, as it does not single out one correct an-
swer phrase. The key objective is an improved sat-
isfaction of the user towards its adaptive results,
which are hopefully more suitable to his read-
ing level. A user-centred evaluation methodology
that assesses how the system meets individual in-
formation needs is therefore more appropriate for
YourQA than TREC-QA metrics.
We draw our evaluation guidelines from (Su,
2003), which proposes a comprehensive search
engine evaluation model. We define the following
metrics (see Table 1):
1. Relevance:
? strict precision (P1): the ratio between
the number of results rated as relevant
and all the returned results,
? loose precision (P2): the ratio between
the number of results rated as relevant
or partially relevant and all the returned
results.
2. User satisfaction: a 7-point Likert scale10 is
used to assess satisfaction with:
? loose precision of results (S1),
? query success (S2).
3. Reading level accuracy (Ar). This metric
was not present in (Su, 2003) and has been
introduced to assess the reading level estima-
tion. Given the set R of results returned by
the system for a reading level r, it is the ratio
between the number of documents ? R rated
by the users as suitable for r and |R|. We
compute Ar for each reading level.
4. Overall utility (U ): the search session as a
whole is assessed via a 7-point Likert scale.
We have discarded some of the metrics proposed
by (Su, 2003) when they appeared as linked to
technical aspects of search engines (e.g. connec-
tivity), and when response time was concerned as
at the present stage this has not been considered
10This measure ? ranging from 1= ?extremely unsatisfac-
tory? to 7=?extremely satisfactory? ? is particularly suitable
to assess the degree to which the system meets the user?s
search needs. It was reported in (Su, 1991) as the best sin-
gle measure for information retrieval among 20 tested.
55 KRAQ06
an issue. Also, we exclude metrics relating to the
user interface which are not relevant for this study.
Metric field description
Relevance P1 strict precision
P2 loose precision
Satisfaction S1 with loose precision
S2 with query success
Accuracy Ag good reading level
Am medium reading level
Ap poor reading level
Utility U overall session
Table 1: Summary of evaluation metrics
5.2 Evaluation results
We performed our evaluation by running 24
queries (partly reported in Table 3) on both Google
and YourQA11. The results ? i.e. snippets from
the Google result page and passages returned by
YourQA ? were given to 20 evaluators. These
were aged between 16 and 52, all having a self-
assessed good or medium English reading level.
They came from various backgrounds (University
students/graduates, professionals, high school)
and mother-tongues. Evaluators filled in a ques-
tionnaire assessing the relevance of each passage,
the success and result readability of the single
queries, and the overall utility of the system; val-
ues were thus computed for the metrics in Table 1.
P1 P2 S1 S2 U
Google 0,39 0,63 4,70 4,61 4,59
YourQA 0,51 0,79 5,39 5,39 5,57
Table 2: Evaluation results
5.2.1 Relevance
The precision results (see Table 2) for the whole
search session were computed by averaging the
values obtained for the 20 queries. Although quite
close, they show a 10-15% difference in favour of
the YourQA system for both strict precision (P1)
and loose precision (P2). This suggests that the
coarse semantic processing applied and the visu-
alisation of the context contribute to the creation
of more relevant passages.
11To make the two systems more comparable, we turned
off query expansion and only submitted the original question
sentence
5.2.2 User satisfaction
After each query, we asked evaluators the fol-
lowing questions: ?How would you rate the ratio
of relevant/partly relevant results returned?? (as-
sessing S1) and ?How would you rate the success
of this search?? (assessing S2). Table 2 denotes a
higher level of satisfaction tributed to the YourQA
system in both cases.
5.2.3 Reading level accuracy
Adaptivity to the users? reading level is the dis-
tinguishing feature of the YourQA system: we
were thus particularly interested in its perfor-
mance in this respect. Table 3 shows that alto-
gether, evaluators found our results appropriate for
the reading levels to which they were assigned.
The accuracy tended to decrease (from 94% to
72%) with the level: this was predictable as it is
more constraining to conform to a lower reading
level than to a higher one. However this also sug-
gests that our estimation of document difficulty
was perhaps too ?optimisitic?: we are currently
working with better quality training data which al-
lows to obtain more accurate language models.
Query Ag Am Ap
Who painted the Sistine Chapel? 0,85 0,72 0,79
Who was the first American in space? 0,94 0,80 0,72
Who was Achilles? best friend? 1,00 0,98 0,79
When did the Romans invade Britain? 0,87 0,74 0,82
Definition of metaphor 0,95 0,81 0,38
What is chickenpox? 1,00 0,97 0,68
Define german measles 1,00 0,87 0,80
Types of rhyme 1,00 1,00 0,79
Who was a famous cubist? 0,90 0,75 0,85
When did the Middle Ages begin? 0,91 0,82 0,68
Was there a Trojan war? 0,97 1,00 0,83
Shakespeare?s most famous play? 0,90 0,97 0,83
average 0,94 0,85 0,72
Table 3: Queries and reading level accuracy
5.2.4 Overall utility
At the end of the whole search session, users
answered the question: ?Overall, how was this
search session?? relating to their search experi-
ence with Google and the YourQA system. The
values obtained for U in Table 2 show a clear pref-
erence (a difference of ' 1 on the 7-point scale) of
the users for YourQA, which is very positive con-
56 KRAQ06
sidering that it represents their general judgement
on the system.
5.3 Future work
We plan to run a larger evaluation by including
more metrics, such as user vs system ranking of
results and the contribution of cluster by cluster
presentation. We intend to conduct an evaluation
also involving users with a poor reading level, so
that each evaluator will only examine answers tar-
geted to his/her reading level. We will analyse our
results with respect to the individual reading levels
and the different types of questions proposed.
6 Conclusion
A user-tailored open domain QA system is out-
lined where a user model contributes to elaborat-
ing answers corresponding to the user?s needs and
presenting them efficiently. In this paper we have
focused on how the user?s reading level (a param-
eter in the UM) can be used to filter and re-order
the candidate answer passages. Our preliminary
results show a positive feedback from human as-
sessors on the utility of the system in an informa-
tion seeking domain. Our short term goals involve
performing a more extensive evaluation, exploit-
ing more UM parameters in answer selection and
implementing a dialogue interface to improve the
system?s interactivity.
References
L. Ardissono, L. Console, and I. Torre. 2001. An adap-
tive system for the personalized access to news. AI
Commun., 14(3):129?147.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings of
EACL?99, pages 269?270.
K. Collins-Thompson and J. P. Callan. 2004. A lan-
guage modeling approach to predicting reading dif-
ficulty. In Proceedings of HLT/NAACL.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In ACL Workshop on Paraphrasing:
Paraphrase Acquisition and Applications, pages 9?
16.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference Re-
search on Computational Linguistics (ROCLING X).
J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.
1975. Derivation of new readability formulas for
navy enlisted personnel. Technical Report Branch
Report 8-75, Chief of Naval Training.
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In World Wide
Web, pages 150?161.
J. Lin, D. Quan, V. Sinha, and K Bakshi. 2003. What
makes a good answer? the role of context in question
answering. In Proceedings of INTERACT 2003.
Bernardo Magnini and Carlo Strapparava. 2001. Im-
proving user modelling with content-based tech-
niques. In UM: Proceedings of the 8th Int. Confer-
ence, volume 2109 of LNCS. Springer.
James Pitkow, Hinrich Schuetze, Todd Cass, Rob Coo-
ley, Don Turnbull, Andy Edmonds, Eytan Adar, and
Thomas Breuel. 2002. Personalized search. Com-
mun. ACM, 45(9):50?55.
S. Quarteroni and S. Manandhar. 2006. User mod-
elling for adaptive question answering and informa-
tion retrieval. In Proceedings of FLAIRS?06.
M. Steinbach, G. Karypid, and V. Kumar. 2000. A
comparison of document clustering techniques.
L. T. Su. 1991. An investigation to find appropriate
measures for evaluating interactive information re-
trieval. Ph.D. thesis, New Brunswick, NJ, USA.
L. T. Su. 2003. A comprehensive and systematic
model of user evaluation of web search engines: Ii.
an evaluation by undergraduates. J. Am. Soc. Inf.
Sci. Technol., 54(13):1193?1223.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz.
2005. Personalizing search via automated analysis
of interests and activities. In Proceedings of SIGIR
?05, pages 449?456, New York, NY, USA. ACM
Press.
E. M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Text REtrieval Confer-
ence.
E. M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Text REtrieval Confer-
ence.
S. Williams and E. Reiter. 2005. Generating readable
texts for readers with low basic skills. In Proceed-
ings of ENLG-2005, pages 140?147.
H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementation. Morgan Kaufmann.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In ACM DL, pages
254?255.
57 KRAQ06
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 33?40
Manchester, August 2008
Personalized, Interactive Question Answering on the Web
Silvia Quarteroni
University of Trento
Via Sommarive 14
38100 Povo (TN), Italy
silviaq@disi.unitn.it
Abstract
Two of the current issues of Question Answer-
ing (QA) systems are the lack of personaliza-
tion to the individual users? needs, and the lack
of interactivity by which at the end of each Q/A
session the context of interaction is lost.
We address these issues by designing and im-
plementing a model of personalized, interac-
tive QA based on a User Modelling component
and on a conversational interface. Our eval-
uation with respect to a baseline QA system
yields encouraging results in both personaliza-
tion and interactivity.
1 Introduction
Information overload, i.e. the presence of an exces-
sive amount of data from which to search for relevant
information, is a common problem to Information Re-
trieval (IR) and its subdiscipline of Question Answering
(QA), that aims at finding concise answers to questions
in natural language. In Web-based QA in particular, this
problem affects the relevance of results with respect to
the users? needs, as queries can be ambiguous and even
answers extracted from documents with relevant con-
tent but expressed in a difficult language may be ill-
received by users.
While the need for user personalization has been ad-
dressed by the IR community for a long time (Belkin
and Croft, 1992), very little effort has been carried out
up to now in the QA community in this direction. In-
deed, personalized Question Answering has been ad-
vocated in TREC-QA starting from 2003 (Voorhees,
2003); however, the issue was solved rather expedi-
tiously by designing a scenario where an ?average news
reader? was imagined to submit the 2003 task?s defini-
tion questions.
Moreover, a commonly observed behavior in users
of IR systems is that they often issue queries not as
standalone questions but in the context of a wider in-
formation need, for instance when researching a spe-
cific topic. Recently, a new research direction has
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
been proposed, which involves the integration of QA
systems with dialogue interfaces in order to encour-
age and accommodate the submission of multiple re-
lated questions and handle the user?s requests for clar-
ification in a less artificial setting (Maybury, 2002);
however, Interactive QA (IQA) systems are still at an
early stage or applied to closed domains (Small et al,
2003; Kato et al, 2006). Also, the ?complex, inter-
active QA? TREC track (www.umiacs.umd.edu/
?
jimmylin/ciqa/) has been organized, but here
the interactive aspect refers to the evaluators being en-
abled to interact with the systems rather than to dia-
logue per se.
In this paper, we first present an adaptation of User
Modelling (Kobsa, 2001) to the design of personalized
QA, and secondly we design and implement an inter-
active open-domain QA system, YourQA. Section 2
briefly introduces the baseline architecture of YourQA.
In Section 3, we show how a model of the user?s read-
ing abilities and personal interests can be used to effi-
ciently improve the quality of the information returned
by a QA system. We provide an extensive evaluation
methodology to assess such efficiency by improving on
our previous work in this area (Quarteroni and Manand-
har, 2007b).
Moreover, we discuss our design of interactive QA
in Section 4 and conduct a more rigorous evaluation of
the interactive version of YourQA by comparing it to
the baseline version on a set of TREC-QA questions,
obtaining encouraging results. Finally, a unified model
of personalized, interactive QA is described in Section
5.
2 Baseline System Architecture
The baseline version of our system, YourQA, is able to
extract answers to both factoid and non-factoid ques-
tions from the Web. As most QA systems (Kwok et al,
2001), it is organized according to three phases:
? Question Processing: The query is classified and
the two top expected answer types are estimated; it
is then submitted to the underlying search engine;
? Document Retrieval: The top n documents are
retrieved from the search engine (Google, www.
google.com) and split into sentences;
33
? Answer Extraction:
1. A sentence-level similarity metric combining
lexical, syntactic and semantic criteria is ap-
plied to the query and to each retrieved doc-
ument sentence to identify candidate answer
sentences;
2. Candidate answers are ordered by relevance
to the query; the Google rank of the answer
source document is used as a tie-breaking cri-
terion.
3. The list of top ranked answers is then re-
turned to the user in an HTML page.
Note that our answers are in the form of sentences with
relevant words or phrases highlighted (as visible in Fig-
ure 2) and surrounded by their original passage. This
is for two reasons: we believe that providing a con-
text to the exact answer is important and we have been
mostly focusing on non-factoids, such as definitions,
which it makes sense to provide in the form of a sen-
tence. A thorough evaluation of YourQA is reported in
e.g. (Moschitti et al, 2007); it shows an F1 of 48?.7
for non-factoids on Web data, further improved by a
SVM-based re-ranker.
In the following sections, we describe how the base-
line architecture is enhanced to accommodate personal-
ization and interactivity.
3 User Modelling for Personalization
Our model of personalization is centered on a User
Model which represents students searching for informa-
tion on the Web according to three attributes:
1. age range a ? {7? 10, 11? 16, adult},
2. reading level r ? {basic,medium, advanced};
3. profile p, a set of textual documents, bookmarks
and Web pages of interest.
Users? age
1
and browsing history are typical UM
components in news recommender systems (Magnini
and Strapparava, 2001); personalized search systems
such as (Teevan et al, 2005) also construct UMs based
on the user?s documents and Web pages of interest.
3.1 Reading Level Estimation
We approach reading level estimation as a supervised
learning task, where representative documents for each
of the three UM reading levels are collected to be la-
belled training instances and used to classify previously
unseen documents.
Our training instances consist of about 180 HTML
documents from a collection of Web portals
2
where
1
Although the reading level can be modelled separately
from the age range, for simplicity we here assume that these
are paired in a reading level component.
2
Such Web portals include: bbc.co.uk/schools,
www.think-energy.com, kids.msfc.nasa.gov.
pages are explicitly annotated by the publishers ac-
cording to the three reading levels above. As a learn-
ing model, we use unigram language modelling in-
troduced in (Collins-Thompson and Callan, 2004) to
model the reading level of subjects in primary and sec-
ondary school.
Given a set of documents, a unigram language model
represents such a set as the vector of all the words ap-
pearing in the component documents associated with
their corresponding probabilities of occurrence within
the set.
In the test phase of the learning process, for each un-
classified document D, a unigram language model is
built (as done for the training documents). The esti-
mated reading level of D is the language model lm
i
maximizing the likelihood that D has been generated
by lm
i
(In our case, three language models lm
i
are de-
fined, where i ? {basic,medium, advanced}.). Such
likelihood is estimated using the function:
L(lm
i
|D) =
?
w?D
C(w,D) ? log[P (w|lm
i
)], (1)
where w is a word in the document, C(w, d) represents
the number of occurrences of w in D and P (w|lm
i
) is
the probability that w occurs in lm
i
(approximated by
its frequency).
3.2 Profile Estimation
Information extraction from the user?s documents as a
means of representation of the user?s interests, such as
his/her desktop files, is a well-established technique for
personalized IR (Teevan et al, 2005).
Profile estimation in YourQA is based on key-phrase
extraction, a technique previously employed in several
natural language tasks (Frank et al, 1999).
For this purpose, we use Kea (Witten et al, 1999),
which splits documents into phrases and chooses some
of the phrases as be key-phrases based on two criteria:
the first index of their occurrence in the source doc-
uments and their TF ? IDF score
3
with respect to
the current document collection. Kea outputs for each
document in the set a ranked list where the candidate
key-phrases are in decreasing order; after experiment-
ing with several values, we chose to use the top 6 as
key-phrases for each document.
The profile resulting from the extracted key-phrases
is the base for all the subsequent QA activity: any ques-
tion the user submits to the QA system is answered by
taking such profile into account, as illustrated below.
3.3 Personalized QA Algorithm
The interaction between the UM component and the
core QA component modifies the standard QA process
at the Answer Extraction phase, which is modified as
follows:
3
The TF ? IDF of a term t in document D within a col-
lection S is: TF?IDF (t,D, S) = P (t ? D)??logP (t ?
[S/D]).
34
1. The retrieved documents? reading levels are esti-
mated;
2. Documents having a different reading level from
the user are discarded; if the remaining documents
are insufficient, part of the incompatible docu-
ments having a close reading level are kept;
3. From the documents remaining from step 2, key-
phrases are extracted using Kea;
4. The remaining documents are split into sentences;
5. Document topics are matched with the topics in
the UM that represent the user?s interests;
6. Candidate answers are extracted from the docu-
ments and ordered by relevance to the query;
7. As an additional answer relevance criterion, the
degree of match between the candidate answer
document topics and the user?s topics of interest is
used and a new ranking is computed on the initial
list of candidate answers.
Step 7 deserves some deeper explanation. For each
document composing the UM profile and the retrieved
document set, a ranked list of key-phrases is available
from the previous steps. Both key-phrase sets are rep-
resented by YourQA as arrays, where each row corre-
sponds to one document and each column corresponds
to the rank within such document of the key-phrase in
the corresponding cell.
As an illustrative example, a basic user profile, cre-
ated from two documents about Italian cuisine and the
movie ?Ginger and Fred?, respectively, might result in
the following array:
[
pizza lasagne tiramisu recipe chef egg
fred ginger film music movie review
]
The arrays of UM profile and retrieved document
key-phrases are named P and Retr, respectively. We
call Retr
i
the document represented in the i-th row in
Retr, and P
n
the one represented in the n-th row of P
4
. Given k
ij
, i.e. the j-th key-phrase extracted from
Retr
i
, and P
n
, i.e. the n-th document in P , we call
w(k
ij
, P
n
) the relevance of k
ij
with respect to P
n
. We
define
w(k
ij
, P
n
) =
{
|Retr
i
|?j
|Retr
i
|
, k
ij
? P
n
0, otherwise
(2)
where |Retr
i
| is the number of key-phrases of Retr
i
.
The total relevance of document Retr
i
with respect to
P , w
P
(Retr
i
), is defined as the maximal sum of the
relevance of its key-phrases, obtained for all the rows
in P :
w
P
(Retr
i
) = max
n?P
?
k
ij
?Retr
i
w(k
ij
, P
n
). (3)
4
Note that, while column index reflects a ranking based
on the relevance of a key-phrase to its source document, row
index only depends on the name of such document.
The personalized answer ranking takes w
P
into ac-
count as a secondary ranking criterion with respect
to the baseline system?s similarity score; as before,
Google rank of the source document is used as further
a tie-breaking criterion.
Notice that our approach to User Modelling can be
seen as a form of implicit (or quasi-implicit) relevance
feedback, i.e. feedback not explicitly obtained from the
user but inferred from latent information in the user?s
documents. Indeed, we take inspiration from (Teevan
et al, 2005)?s approach to personalized search, comput-
ing the relevance of unseen documents (such as those
retrieved for a query) as a function of the presence and
frequency of the same terms in a second set of docu-
ments on whose relevance the user has provided feed-
back.
Our approaches to personalization are evaluated in
Section 3.4.
3.4 Evaluating Personalization
The evaluation of our personalized QA algorithms as-
sessed the contributions of the reading level attribute
and of the profile attribute of the User Model.
3.4.1 Reading Level Evaluation
Reading level estimation was evaluated by first as-
sessing the robustness of the unigram language models
by running 10-fold cross-validation on the set of doc-
uments used to create such models, and averaging the
ratio of correctly classified documents with respect to
the total number of documents for each fold. Our re-
sults gave a very high accuracy, i.e. 94.23% ? 1.98
standard deviation.
However, this does not prove a direct effect on the
user?s perception of such levels. For this purpose, we
defined Reading level agreement (A
r
) as the percentage
of documents rated by the users as suitable to the read-
ing level to which they were assigned. We performed
a second experiment with 20 subjects aged between 16
and 52 and with a self-assessed good or medium En-
glish reading level. They evaluated the answers re-
turned by the system to 24 questions into 3 groups (ba-
sic, medium and advanced reading levels), by assessing
whether they agreed that the given answer was assigned
to the correct reading level.
Our results show that altogether, evaluators found an-
swers appropriate for the reading levels to which they
were assigned. The agreement decreased from 94% for
A
adv
to 85% for A
med
to 72% for A
bas
; this was pre-
dictable as it is more constraining to conform to a lower
reading level than to a higher one.
3.4.2 Profile Evaluation
The impact of the UM profile was tested by us-
ing as a baseline the standard version of YourQA,
where the UM component is inactive. Ten adult par-
ticipants from various backgrounds took part in the
experiment; they were invited to form an individual
profile by brainstorming key-phrases for 2-3 topics of
35
their interest chosen from the Yahoo! directory (dir.
yahoo.com): examples were ?ballet?, ?RPGs? and
?dog health?.
For each user, we created the following 3 questions
so that he/she would submit them to the QA system:
Q
per
, related to the user?s profile, for answering which
the personalized version of YourQA would be used;
Q
bas
, related to the user?s profile, for which the base-
line version of the system would be used; and Q
unr
,
unrelated to the user?s profile, hence not affected by
personalization. The reason why we handcrafted ques-
tions rather than letting users spontaneously interact
with YourQA?s two versions is that we wanted the re-
sults of the two versions to be different in order to mea-
sure a preference. After examining the top 5 results to
each question, users had to answer the following ques-
tionnaire
5
:
? For each of the five results separately:
TEST1: This result is useful to me:
5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)
Not at all
TEST2: This result is related to my profile:
5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)
Not at all
? For the five results taken as a whole:
TEST3: Finding the info I wanted in the result page
took:
1) Too long, 2) Quite long, 3) Not too long, 4)
Quite little, 5) Very little
TEST4: For this query, the system results were sensi-
tive to my profile:
5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)
Not at all
The experiment results are summarized in Table 1. The
Table 1: Profile evaluation results (avg ? st. dev.)
Measurement Q
rel
Q
bas
Q
unr
TEST1 3.6?0.4 2.3?0.3 3.3?0.3
TEST2 4.0?0.5 2.2?0.3 1.7?0.1
TEST3 3.1?1.1 2.7?1.3 3.4?1.4
TEST4 3.9?0.7 2.5?1.1 1.8?1.2
first row reports a remarkable difference between the
perceived usefulness for question Q
rel
with respect to
question Q
bas
(answers to TEST1).
The results were compared by carrying out a one-
way analysis of variance (ANOVA) and performing the
Fischer test using the usefulness as factor (with the
5
The adoption of a Likert scale made it possible to com-
pute the average and standard deviations of the user comments
with respect to each answer among the top five returned by the
system. It was therefore possible to replace the binary mea-
surement of perceived usefulness, relatedness and sensitivity
used in (Quarteroni and Manandhar, 2007b) in terms of to-
tal number of users with a more fine-grained one in terms of
average computed over the users.
three queries as levels) at a 95% level of confidence.
The test revealed an overall significant difference be-
tween factors, confirming that users are positively bi-
ased towards questions related to their own profile when
it comes to perceived utility.
To analyze the answers to TEST2 (Table 1, row 2),
which measured the perceived relatedness of each an-
swer to the current profile, we used ANOVA again and
and obtained an overall significant difference. Hence,
answers obtained without using the users? profile were
perceived as significantly less related to those obtained
using their own profile, i.e. there is a significant differ-
ence between Q
rel
and Q
bas
. As expected, the differ-
ence between Q
rel
and Q
unr
is even more significant.
Thirdly, the ANOVA table computed using average
perceived time (TEST3) as variable and the three ques-
tions as factors did not give any significance, nor did
any of the paired t-tests computed over each result pair.
We concluded that apparently, the time spent browsing
results is not directly correlated to the personalization
of results.
Finally, the average sensitivity of the five answers al-
together (TEST4) computed over the ten participants
for each query shows an overall significant difference in
perceived sensitivity between the answers to question
Q
rel
(3.9?0.7) and those to question Q
bas
(2.5?1.1)
and Q
unr
(1.8?1.2).
To conclude, our experience with profile evaluation
shows that personalized QA techniques yield answers
that are indeed perceived as more satisfying to users in
terms of usefulness and relatedness to their own profile.
4 Interactivity
Making a QA system interactive implies maintaining
and efficiently using the current dialogue context and
the ability to converse with the user in a natural manner.
Our implementation of IQA is guided by the following
conversation scenario:
1. An optional reciprocal greeting, followed by a
question q from the user;
2. q is analyzed to detect whether it is related to pre-
vious questions or not;
3. (a) If q is unrelated to the preceding questions, it
is submitted to the QA component;
(b) If q is related to the preceding questions
(follow-up question), it is interpreted by the
system in the context of previous queries;
a revised version of q, q?, is either directly
submitted to the QA component or a request
for confirmation (grounding) is issued to the
user; if he/she does not agree, the system asks
the user to reformulate the question until it
can be interpreted by the QA component;
4. As soon as the QA component results are avail-
able, an answer a is provided;
36
5. The system enquires whether the user is interested
in submitting new queries;
6. Whenever the user wants to terminate the interac-
tion, a final greeting is exchanged.
4.1 Choosing a Dialogue Manager
Among traditional methods for implementing
information-seeking dialogue management, Finite-
State (FS) approaches are the simplest. Here, the
dialogue manager is represented as a Finite-State
machine, where each state models a separate phase
of the conversation, and each dialogue move encodes
a transition to a subsequent state (Sutton, 1998).
However, an issue with FS models is that they allow
very limited freedom in the range of user utterances:
since each dialogue move must be pre-encoded in the
models, there is a scalability issue when addressing
open domain dialogue.
On the other hand, we believe that other dialogue ap-
proaches such as the Information State (IS) (Larsson et
al., 2000) are primarily suited to applications requiring
a planning component such as closed-domain dialogue
systems and to a lesser extent to open-domain QA.
As an alternative approach, we studied conversa-
tional agents (?chatbots?) based on AIML (Artificial
Intelligence Markup Language), such as ALICE
6
. Chat-
bots are based on the pattern matching technique, which
consists in matching the last user utterance against a
range of dialogue patterns known to the system. A co-
herent answer is created by following a range of ?tem-
plate? responses associated with such patterns.
As its primary application is small-talk, chatbot di-
alogue appears more natural than in FS and IS sys-
tems. Moreover, since chatbots support a limited no-
tion of context, they can handle follow-up recognition
and other dialogue phenomena not easily covered using
standard FS models.
4.2 A Wizard-of-Oz Experiment
To assess the utility of a chatbot-based dialogue man-
ager in an open-domain QA application, we conducted
an exploratory Wizard of Oz experiment.
Wizard-of-Oz (WOz) experiments are usually de-
ployed for natural language systems to obtain initial
data when a full-fledged prototype is not yet available
(Dahlbaeck et al, 1993) and consist in ?hiding? a hu-
man operator behind a computer interface to simulate a
conversation with the user, who believes to be interact-
ing with a fully automated prototype.
We designed six tasks reflecting the intended typical
usage of the system (e.g.: ?Find out who painted Guer-
nica and ask the system for more information about the
artist?) to be carried out by 7 users by interacting with
an instant messaging platform, which they were told to
be the system interface.
6
www.alicebot.org/
The role of the Wizard was to simulate a limited
range of utterances and conversational situations han-
dled by a chatbot.
User feedback was collected mainly by using a
post-hoc questionnaire inspired by the experiment in
(Munteanu and Boldea, 2000), which consists of ques-
tions Q
1
to Q
6
in Table 2, col. 1, to be answered using
a scale from 1=?Not at all? to 5=?Yes, absolutely?.
From the WOz results, reported in Table 2, col.
?WOz?, users appear to be generally very satisfied with
the system?s performances: Q
6
obtained an average of
4.5?.5. None of the users had difficulties in reformu-
lating their questions when this was requested: Q
4
ob-
tained 3.8?.5. For the remaining questions, satisfaction
levels were high: users generally thought that the sys-
tem understood their information needs (Q
2
obtained 4)
and were able to obtain such information (Q
1
obtained
4.3?.5).
The dialogue manager and interface of YourQA were
implemented based on the dialogue scenario and the
successful outcome of the WOz experiment.
4.3 Dialogue Management Algorithms
As chatbot dialogue follows a pattern-matching ap-
proach, it is not constrained by a notion of ?state?:
when a user utterance is issued, the chatbot?s strategy is
to look for a pattern matching it and fire the correspond-
ing template response. Our main focus of attention
in terms of dialogue manager design was therefore di-
rected to the dialogue tasks invoking external resources,
such as handling follow-up questions, and tasks involv-
ing the QA component.
4.3.1 Handling follow-up questions
For the detection of follow-up questions, the algo-
rithm in (De Boni and Manandhar, 2005) is used, which
uses features such as the presence of pronouns and
the absence of verbs in the current question and word
repetitions with the n previous questions to determine
whether q
i
is a follow-up question with respect to the
current context. If the question q is not identified as a
follow-up question, it is submitted to the QA compo-
nent. Otherwise, the reference resolution strategy be-
low is applied on q, drawing on the stack S of previous
user questions:
1. If q is elliptic (i.e. contains no verbs), its keywords
are completed with the keywords extracted by the
QA component from the previous question in S
for which there exists an answer. The completed
query is submitted to the QA component;
2. If q contains pronoun/adjective anaphora, a chun-
ker is used to find the most recent compatible an-
tecedent in S. This must be a NP compatible in
number with the referent.
3. If q contains NP anaphora, the first NP in S con-
taining all the words in the referent is used to re-
place the latter in q. When no antecedent can be
37
found, a clarification request is issued by the sys-
tem until a resolved query can be submitted to the
QA component.
When the QA process is terminated, a message direct-
ing the user to the HTML answer frame (see Figure 1) is
returned and a follow-up proposal or an enquiry about
user satisfaction is optionally issued.
4.4 Implementation
To implement the dialogue manager and allow a seam-
less integration with our Java-based QA system, we ex-
tended the Java-based AIML interpreter Chatterbean
7
.
We started by augmenting the default AIML tag set
(including tags such as <srai> and <that>) with
two tags: <query>, to seamlessly invoke the core QA
module, and <clarify>, to support follow-up detec-
tion and resolution.
Moreover, the interpreter allows to instantiate and
update a set of variables, represented as context prop-
erties. Among others, we defined:
a) userID, which is matched against a list of known
user IDs to select a UM profile for answer extraction
(see Section 5);
b) the current query, which is used to dynamically up-
date the stack of recent user questions used by the clar-
ification request detection module to perform reference
resolution;
c) the topic of conversation, i.e. the keywords of the
last question issued by the user which received an an-
swer. The latter is used to clarify elliptic questions, by
augmenting the current query keywords with those in
the topic when ellipsis is detected.
Figure 1 illustrates YourQA?s interactive version,
which is accessible from the Web. As in a normal chat
application, users write in a text field and the current
session history as well as the interlocutor replies are vi-
sualized in a text area.
4.5 Interactive QA evaluation
For the evaluation of interactivity, we built on our pre-
vious results from a Wizard-of-Oz experiment and an
initial evaluation conducted on a limited set of hand-
crafted questions (Quarteroni and Manandhar, 2007a).
We chose 9 question series from the TREC-QA 2007
campaign
8
. Three questions were retained per series to
make each evaluation balanced. For instance, the three
following questions were used to form one task: 266.1:
?When was Rafik Hariri born??, 266.2: ?To what reli-
gion did he belong (including sect)?? and 266.4: ?At
what time in the day was he assassinated??.
Twelve users were invited to find answers to the
questions to one of them by using the standard version
of the system and to the second by using the interactive
version. Each series was evaluated at least once using
both versions of the system. At the end of the exper-
iment, users had to give feedback about both versions
7
chatterbean.bitoflife.cjb.net.
8
trec.nist.gov
Table 2: Interactive QA evaluation results obtained for
the WOz, Standard and Interactive versions of YourQA.
Average ? st. dev. are reported.
Question WOz Stand Interact
Q
1
Did you get al the in-
formation you wanted
using YourQA?
4.3?.5 4.1?1 4.3?.7
Q
2
Do you think YourQA
understood what you
asked?
4.0 3.4?1.3 3.8?1.1
Q
3
How easy was it to
obtain the information
you wanted?
4.0?.8 3.9?1.1 3.7?1
Q
4
Was it difficult to re-
formulate your ques-
tions when requested?
3.8?.5 - 3.9?.6
Q
5
Do you think you
would use YourQA
again?
4.1?.6 3.3?1.6 3.1?1.4
Q
6
Overall, are you satis-
fied with YourQA?
4.5?.5 3.7?1.2 3.8?1.2
Q
7
Was the pace of inter-
action with YourQA
appropriate?
- 3.2?1.2 3.3?1.2
Q
8
How often was
YourQA sluggish in
replying to you?
- 2.7?1.1 2.5?1.2
Q
9
Which interface did
you prefer?
- 41.7% 58.3%
of the system by filling in the satisfaction questionnaire
reported in Table 2.
Although the paired t-test conducted to compare
questionnaire replies to the standard and interactive ver-
sions did not register statistical significance, we believe
that the evidence we collected suggests a few interest-
ing interpretations.
First, a good overall satisfaction appears with both
versions of the system (Q
6
), with a slight difference in
favor of the interactive version. The two versions of
the system seem to offer different advantages: while
the ease of use of the standard version was rated higher
(Q
3
), probably because the system?s reformulation re-
quests added a challenge to users used to search engine
interaction, users felt they obtained more information
using the interactive version (Q
1
).
Concerning interaction comfort, users seemed to feel
that the interactive version understood better their re-
quests than the standard one (Q
2
); they also found it
easy to reformulate questions when the former asked
to (Q
6
). However, while the pace of interaction was
judged slightly more appropriate in the interactive case
(Q
7
), interaction was considered faster when using the
standard version (Q
4
). This partly explains the fact that
users seemed more ready to use again the standard ver-
sion of the system (Q
5
).
7 out of 12 users (58.3%) answered the ?preference?
question Q
9
by saying that they preferred the inter-
active version. The reasons given by users in their
38
Figure 1: YourQA?s interactive interface
comments were mixed: while some of them were en-
thusiastic about the chatbot?s small-talk features, oth-
ers clearly said that they felt more comfortable with a
search engine-like interface. Most of the critical aspects
emerging from our overall satisfactory evaluation de-
pend on the specific system we have tested rather than
on the nature of interactive QA, to which none of such
results appear to be detrimental.
We believe that the search-engine-style use and in-
terpretation of QA systems are due to the fact that QA
is still a very little known technology. It is a challenge
for both developers and the larger public to cooperate
in designing and discovering applications that take ad-
vantage of the potentials of interactivity.
5 A Unified Model
Our research so far has demonstrated the utility of per-
sonalization and interactivity in a QA system. It is
thus inevitable to regard the formulation of a unified
model of personalized, interactive QA as a valuable by-
product of these two technologies. In this perspective,
we propose the following dialogue scenario:
1. The user interacts with the dialogue interface for-
mulating an utterance q;
2. If q is recognized as a question, it is analyzed by
the dialogue manager (DM) to detect and resolve
multiple and follow-up questions;
3. As soon as a resolved version q
?
of q is available,
the DM passes q
?
to the QA module; the latter pro-
cesses q
?
and retrieves a set Retr(q
?
) of relevant
documents;
4. The QA module exchanges information with the
UM component which is responsible of maintain-
ing and updating the User Model of the current
user, u; Based on u, the QA module extracts a list
L(q
?
, u) of personalized results from Retr(q
?
);
5. The DM produces a reply r, which is returned
along with L(q
?
, u) to the user via the dialogue in-
terface;
6. Once terminated, the current QA session is logged
into the dialogue history H(u), that will be used
to update u;
Concerning step 4, an efficient strategy for eliciting
the User Model from the user is yet to be specified at
this stage: the current one relies on the definition of
a context variable userID in the dialogue manager,
which at the moment corresponds to the user?s name. A
number of AIML categories are created are created for
YourQA to explicitly ask for the user?s name, whihc is
then assigned to the userID variable.
Figure 2 illustrates an example of a personalized, QA
session in YourQA where the user?s name is associated
with a basic reading level UM. This affects the docu-
ment retrieval phase, where only documents with sim-
ple words are retained for answer extraction.
6 Conclusions and Future Work
In this paper, we present an efficient and light-weight
method to personalize the results of a Web-based QA
system based on a User Model representing individual
users? reading level, age range and interests. Our results
show the efficiency of reading level estimation, and a
39
Figure 2: Screenshot from a personalized, interactive QA session. Here, the user?s name (?Kid?) is associated with
a UM requiring a basic reading level, hence the candidate answer documents are filtered accordingly.
significant improvement in satisfaction when filtering
answers based on the users? profile with respect to the
baseline version of our system. Moreover, we introduce
a dialogue management model for interactive QA based
on a chat interface and evaluate it with optimistic con-
clusions.
In the future, we plan to study efficient strategies for
bootstrapping User Models based on current and past
conversations with the present user. Another problem
to be solved is updating user interests and reading lev-
els based on the dialogue history, in order to make the
system fully adaptive.
Acknowledgements
The research reported here was mainly conducted at the Com-
puter Science Department of the University of York, UK, un-
der the supervision of Suresh Manandhar.
References
Belkin, N. J. and W.B. Croft. 1992. Information filter-
ing and information retrieval: Two sides of the same
coin? Comm. ACM, 35(12):29?38.
Collins-Thompson, K. and J. P. Callan. 2004. A lan-
guage modeling approach to predicting reading diffi-
culty. In HLT/NAACL?04.
Dahlbaeck, N., A. Jonsson, and L. Ahrenberg. 1993.
Wizard of Oz studies: why and how. In IUI ?93.
De Boni, M. and S. Manandhar. 2005. Implement-
ing clarification dialogue in open-domain question
answering. JNLE, 11.
Frank, E., G. W. Paynter, I. H. Witten, C. Gutwin,
and C. G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In IJCAI ?99.
Kato, T., J. Fukumoto, F.Masui, and N. Kando. 2006.
Woz simulation of interactive question answering. In
IQA?06.
Kobsa, A. 2001. Generic user modeling systems.
UMUAI, 11:49?63.
Kwok, C. T., O. Etzioni, and D. S. Weld. 2001. Scaling
question answering to the web. In WWW?01.
Larsson, S., P. Ljungl?of, R. Cooper, E. Engdahl, and
S. Ericsson. 2000. GoDiS?an accommodating di-
alogue system. In ANLP/NAACL?00 WS on Conver-
sational Systems.
Magnini, B. and C. Strapparava. 2001. Improving user
modelling with content-based techniques. In UM?01.
Maybury, M. T. 2002. Towards a question answering
roadmap. Technical report, MITRE Corporation.
Moschitti, A., S. Quarteroni, R. Basili, and S. Man-
andhar. 2007. Exploiting syntactic and shallow se-
mantic kernels for question/answer classification. In
ACL?07.
Munteanu, C. and M. Boldea. 2000. Mdwoz: A wizard
of oz environment for dialog systems development.
In LREC?00.
Quarteroni, S. and S. Manandhar. 2007a. A chatbot-
based interactive question answering system. In
DECALOG?07, Rovereto, Italy.
Quarteroni, S. and S. Manandhar. 2007b. User
modelling for personalized question answering. In
AI*IA?07, Rome, Italy.
Small, S., T. Liu, N. Shimizu, and T. Strzalkowski.
2003. HITIQA: an interactive question answering
system- a preliminary report. In ACL?03 WS on Mul-
tilingual summarization and QA.
Sutton, S. 1998. Universal speech tools: the CSLU
toolkit. In ICSLP?98.
Teevan, J., S. T. Dumais, and E. Horvitz. 2005. Per-
sonalizing search via automated analysis of interests
and activities. In SIGIR ?05.
Voorhees, E. M. 2003. Overview of the TREC 2003
Question Answering Track. In TREC?03.
Witten, I. H., G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. KEA: Practical auto-
matic keyphrase extraction. In ACM DL.
40
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 213?216,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Investigating Clarification Strategies in a
Hybrid POMDP Dialog Manager
Sebastian Varges and Silvia Quarteroni and Giuseppe Riccardi and Alexei V. Ivanov
Department of Information Engineering and Computer Science
University of Trento, 38050 Povo di Trento, Italy
{varges|silviaq|riccardi|ivanov}@disi.unitn.it
Abstract
We investigate the clarification strategies
exhibited by a hybrid POMDP dialog
manager based on data obtained from a
phone-based user study. The dialog man-
ager combines task structures with a num-
ber of POMDP policies each optimized for
obtaining an individual concept. We in-
vestigate the relationship between dialog
length and task completion. In order to
measure the effectiveness of the clarifica-
tion strategies, we compute concept pre-
cisions for two different mentions of the
concept in the dialog: first mentions and
final values after clarifications and simi-
lar strategies, and compare this to a rule-
based system on the same task. We ob-
serve an improvement in concept precision
of 12.1% for the hybrid POMDP com-
pared to 5.2% for the rule-based system.
1 Introduction
In recent years, probabilistic models of dialog
have been introduced into dialog management, the
part of the spoken dialog system that takes the ac-
tion decision. A major motivation is to improve
robustness in the face of uncertainty, in particu-
lar due to speech recognition errors. The inter-
action is characterized as a dynamic system that
manipulates its environment by performing dialog
actions and perceives feedback from the environ-
ment through its sensors. The original sensory in-
formation is obtained from the speech recognition
(ASR) results which are typically processed by a
spoken language understanding module (SLU) be-
fore being passed on to the dialog manager (DM).
The seminal work of (Levin et al, 2000) mod-
eled dialog management as a Markov Decision
Process (MDP). Using reinforcement learning as
the general learning paradigm, an MDP-based di-
alog manager incrementally acquires a policy by
obtaining rewards about actions it performed in
specific dialog states. As we found in earlier ex-
periments, an MDP can learn to gradually drop the
use of clarification questions if there is no noise.
This is due to the fact that clarifications do not
improve the outcome of the dialog, i.e. the re-
ward. However, with extremely high levels of
noise, the learner prefers to end the dialog imme-
diately (Varges et al, 2009). In contrast to deliber-
ate decision making in the pragmatist tradition of
dialog processing, reinforcement learning can be
regarded as low-level decision making.
MDPs do not account for the observational un-
certainty of the speech recognition results, a key
challenge in spoken dialog systems. Partially Ob-
servable Markov Decision Process (POMDPs) ad-
dress this issue by explicitly modeling how the dis-
tribution of observations is governed by states and
actions.
In this work, we describe the evaluation of a
divide-and-conquer approach to dialog manage-
ment with POMDPs that optimizes policies for
acquiring individual concepts separately. This
makes optimization much easier and allows us to
model the confusability of concrete concept values
explicitly. This also means that different clarifica-
tion strategies are learned for individual concepts
and even individual concept values. The use of the
POMDP policies is orchestrated by an explicit task
structure, resulting in a hybrid approach to dialog
management. The evaluation involved a user study
of 20 subjects in a tourist information domain. The
system is compared against a rule-based baseline
system in the same domain that was also evaluated
with 20 subjects.
2 Hybrid POMDP dialog management
In this section we introduce the hybrid POMDP di-
alog manager that was used in the data collection.
213
2.1 Concept-level POMDPs
The domain is a tourist information system that
uses 5 different policies that can be used in 8
different task roles (see below). For each con-
cept we optimized an individual policy. The
number of states of the POMDP can be lim-
ited to the concept values, for example a loca-
tion name such as trento. The set of ac-
tions consists of a question to obtain the concept
(e.g. question-location), a set of clari-
fication actions (e.g. verify-trento) and a
set of submit actions (e.g. submit-trento).
POMDP modeling including a heuristically set re-
ward structure follows the (simpler) ?tiger prob-
lem? that is well-known in the AI community
(Kaelbling et al, 1998): the system has a num-
ber of actions to obtain further information which
it can try and repeat in any order until it is ready
to commit to a concept value. For optimization we
used the APPL solver (Kurniawati et al, 2008).
2.2 Task structure and dialog management
The use of individual policies is orchestrated by
an explicit task structure that activates and de-
activates them. The task structure is essentially
a directed AND-OR graph with a common root
node. The dialog manager maintains a separate be-
lief distribution for each concept. Figure 1 shows
the general system architecture with a schematic
view of the task structure, and additionally a more
detailed view of an active location node. In the
example, the root node has already finished and
the system is currently obtaining the location for a
lodging task. The term ?role? refers to a concept?s
part in the task, for example a month may be the
check-in or check-out month for accommodation
booking.
At the beginning of a dialog, the task structure is
initialized by activating the root node. A top level
function activates nodes of the task structure and
passes control to that node. Each node maintains
a belief bc for a concept c, which is used to rank
the available actions by computing the inner prod-
uct of policy vectors and belief. The top-ranked
action am is selected by the system, i.e. it is ex-
ploiting the policy, and passed to the natural lan-
guage generator (NLG). Next, the top-ranked SLU
results for the active node and concept are used as
observation zu,c to update the belief to b?c, which
User	 ?
ASR	 ?
TTS	 ?
SLU	 ?
NLG	 ?
PASSIVE	 ?
BLOCKED	 ?
ACTIVE	 ?
BLOCKED	 ?
BLOCKED	 ? BLOCKED	 ?
OPEN	 ? OPEN	 ?
am	 ?
zu,c	 ?
Condi?n :	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?(ac?vity=	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?lodging-??enquiry	 ??	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?lodging-??reserva?n),	 ?ConceptName	 ?	 ?	 ?	 ?	 ?	 ?loca?n,	 ?ConceptRole:	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?loca?n-??lodging,	 ?Status:	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?ACTIVE,	 ?Belief:	 ?
Ac?on:	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?ques?n-??loca?n.	 ?
zu,d	 ?
DM	 ?
Figure 1: System architecture with Task Structure
(task node example in detailed view)
follows the standard method for POMDPs:
b?c(s
?) =
?
s?S
bc(s) T (s, am, s
?) O(a, s?, zu,c)/pzu,c
(1)
where probability b?c(s
?) is the updated belief of
being in state s?, which is computed as the sum of
the probabilities of transitioning from all previous
belief points s to s? by taking machine action am
with probability T (s, am, s?) and observing zu,c
with (smoothed) probability O(am, s?, zu,c). Nor-
malization to obtain a valid probability distribu-
tion is performed by dividing by the probability of
the observation pzu,c .
A concept remains active until a submit action
is selected. At that point, the next active node is
retrieved from the task structure and immediately
used for action selection with an initially uniform
belief. Submit actions are not communicated to
the user but collected and used for the database
query at the end of the dialog.
Overanswering, i.e. the user providing more in-
formation than directly asked for, is handled by de-
layed belief updating: the SLU results are stored
until the first concept of a matching type becomes
active. This is a heuristic rule designed to ensure
that a concept is interpreted in its correct role. Op-
erationally, unused SLU results zu,d (where con-
cept d 6= c) are passed on to the next activated
task node (see also figure 1).
3 Experiments and data analysis
We conducted user studies with two systems in-
volving 20 subjects and 8 tasks in each study.
The systems use a Voice XML platform to drive
ASR and TTS components. Speech recognition is
214
Lodging Task Event Enquiry
TCR #turns TCR #turns
Rule-based DM 75.5% 13.7 66.7% 8.7
(40/53) (?=4.8) (28/42) (?=3.3)
POMDP-DM 78.1% 23.0 84.3% 14.4
(50/64) (?=8.8) (27/32) (?=4.5)
Table 1: Task completion and length metrics
based on statistical language models for the open-
ing prompt, and is grammar-based otherwise. One
system used the hybrid POMDP-DM, the other
is a rule-based dialog manager that uses explicit,
heuristically set confidence thresholds to trigger
the use of clarification questions (Varges et al,
2008).
Dialog length and task completion Table 1
shows task completion rates (?TCR?) and dura-
tions (?#turns?) for the POMDP and rule-based
systems. Task completion in this metric is defined
as the number of tasks of a certain type that were
successfully concluded. Duration is measured in
the number of turn pairs consisting of a system
action followed by a user action. We combine
the counts for two closely related lodging tasks.
The number of tasks is shown in brackets. Table
1 shows that the POMDP-DM successfully con-
cludes more and longer lodging tasks and almost
as many event tasks. In general, the POMDP poli-
cies can be described as more cautious although
obviously the dialog length of the rule system de-
pends on the chosen thresholds.
Concept precision at the value level In order
to measure the effect of the clarification strategies
in both systems, we computed concept precisions
for two different mentions of a concept in a dialog
(table 2): first mentions and final values after clar-
ifications and similar strategies. The rationale for
this metric is that the last mentioned concept value
is the value that the system ultimately obtains from
the user, which is used in the database query:
? if the system decides not to use clarifications,
the only mentioned value is the accepted one,
? if the system verifies and obtains a positive
answer, the last mentioned value is the ac-
cepted one,
? if the system verifies and obtains a negative
answer, the user will mention a new value
(which may or may not be accepted).
Thus, this metric is a uniform way of capturing
the obtained values from systems that internally
Rule-based DM POMDP-DM
first final ?% first final ?%
a) activity 0.78 0.74 -4.1 0.83 0.88 5.0
b) location 0.64 0.74 15.8 0.69 0.73 6.3
c) starrating 0.67 0.70 3.4 0.90 0.97 7.7
d) month 0.85 0.89 4.3 0.76 0.86 12.7
e) day 0.70 0.76 8.3 0.61 0.76 25.3
ALL (a-e) 0.74 0.78 5.2 0.74 0.83 12.1
Clarifications 0.84 0.85 1.5 0.96 0.87 -8.8
Table 2: Concept precision of first vs final value
use very different dialog managers and representa-
tions. The actual precision of a conceptC is calcu-
lated by comparing SLU results to annotations and
counting true positives (matchesM ) and false pos-
itives (separated into mismatches N and entirely
un-annotated concepts U ): Prec(C) = MM+N+U .
Unrecognized concepts, on the other hand, are re-
call related and not counted since they cannot be
part of any system belief.
As table 2 clearly shows, the use of clarification
strategies has a positive effect on concept preci-
sion in both systems. The exception is the preci-
sion of concept activity in the rule-based system
for which the system reprompted rather than ver-
ified.1 In table 2, row ?All? refers to the average
weighted precision of the five concepts. Both sys-
tems start from a similar level of overall precision.
The relative improvement of the POMDP-DM for
all concepts is 12.1%, compared to 5.2% of the
rule-based DM.
We conducted a statistical significance test by
computing the delta in the form of three values for
individual data points, i.e. dialogs, and assigned
+1 for all changes from non-match to match, -1
for a change in the opposite direction and 0 for ev-
erything else (e.g. from mismatch to mismatch).
We found that, although there is a tendency for
the POMDP-DM to perform better, the difference
is not statistically significant at p=0.05 (a possi-
ble explanation is the data size since we are using
human subjects).
We furthermore measured the precision of rec-
ognizing ?yes/no? answers to clarification ques-
tions. In contrast to actual concepts, there is no be-
lief distribution for these in the DM since clarifica-
tion actions are part of the concept POMDP mod-
els. We are thus dealing with individual one-off
recognition results that should be entirely indepen-
dent of each other. However, as table 2 (bottom)
1The second value obtained may be incorrect but above
the confidence threshold; note that the rule system does not
maintain a belief distribution over values.
215
shows, the precision of verifications decreases for
the hybrid POMDP system. A plausible expla-
nation for this is the increasing impatience of the
users due to the longer dialog duration.
Characterization of dialog strategies For
some concepts, the best policy is to ask the
concept question once and then verify once before
committing to the value (assuming the answer is
positive). Other policies verify the same value
twice. Another learned strategy is to ask the orig-
inal concept question twice and then only verify
the value once (assuming that the understood
value was the same in both concept questions). In
other words, the individual concept policies show
different types of strategies regarding uncertainty
handling. This is in marked contrast to the
manually programmed DM that always asks the
concept question once and verifies it if needed
(concept activity being the exception).
HCI and language generation The domain is
sufficiently simple to use template-based genera-
tion techniques to produce the surface forms of
the responses. However, the experiments with the
POMDP-DM highlight some new challenges re-
garding HCI aspects of spoken dialog systems: the
choice of actions may not be ?natural? from the
user?s perspective, for example if the system asks
for a concept twice. However, it should be possi-
ble to better communicate the (change in the) be-
lief to the user.
4 Related work
The pragmatist tradition of dialog processing uses
explicit representations of dialog structure to take
decisions about clarification actions. These mod-
els are more fine-grained and often deal with writ-
ten text, e.g. (Purver, 2006), whereas in spo-
ken dialog systems a major challenge is managing
the uncertainty of the recognition. Reinforcement
learning approaches to dialog management learn
decisions from (often simulated) dialog data in a
less deliberative way. For example, the Hidden In-
formation State model (Young et al, 2010) uses a
reduced summary space that abstracts away many
of the details of observations and dialog state, and
mainly looks at the confidence scores of the hy-
potheses. This seems to imply that clarification
strategies are not tailored toward individual con-
cepts and their values. (Bui et al, 2009) uses fac-
tored POMDP representations that seem closest to
our approach. However, the effect of clarifications
does not seem to have been investigated.
5 Conclusions
We presented evaluation results for a hybrid
POMDP system and compared it to a rule-based
one. The POMDP system achieves higher con-
cept precision albeit at the cost of longer dialogs,
i.e. there is an empirically measurable trade-off
between concept precision and dialog length.
Acknowledgments
This work was partially supported by the Eu-
ropean Commission Marie Curie Excellence
Grant for the ADAMACH project (contract No.
022593).
References
T.H. Bui, M. Poel, A. Nijholt, and J. Zwiers. 2009.
A tractable hybrid DDN-POMDP approach to affec-
tive dialogue modeling for probabilistic frame-based
dialogue systems. Natural Language Engineering,
15(2):273?307.
Leslie Pack Kaelbling, Michael L. Littman, and An-
thony R. Cassandra. 1998. Planning and acting in
partially observable stochastic domains. Artificial
Intelligence, 101:99?134.
H. Kurniawati, D. Hsu, andW.S. Lee. 2008. SARSOP:
Efficient point-based POMDP planning by approxi-
mating optimally reachable belief spaces. In Proc.
Robotics: Science and Systems.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
Matthew Purver. 2006. CLARIE: Handling clarifica-
tion requests in a dialogue system. Research on Lan-
guage and Computation, 4(2-3):259?288, October.
Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-
teroni. 2008. Persistent information state in a data-
centric architecture. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, Colum-
bus, Ohio.
Sebastian Varges, Giuseppe Riccardi, Silvia Quar-
teroni, and Alexei V. Ivanov. 2009. The explo-
ration/exploitation trade-off in reinforcement learn-
ing for dialogue management. In Proceedings of
IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU).
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The Hid-
den Information State Model: a practical framework
for POMDP-based spoken dialogue management.
Computer Speech and Language, 24:150?174.
216
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217?220,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Cooperative User Models in Statistical Dialog Simulators
Meritxell Gonza?lez1,2, Silvia Quarteroni1, Giuseppe Riccardi1, Sebastian Varges1
1 DISI - University of Trento, Povo (Trento), Italy
2 TALP Center - Technical University of Catalonia, Barcelona, Spain
mgonzalez@lsi.upc.edu, name.lastname@disi.unitn.it
Abstract
Statistical user simulation is a promis-
ing methodology to train and evaluate the
performance of (spoken) dialog systems.
We work with a modular architecture for
data-driven simulation where the ?inten-
tional? component of user simulation in-
cludes a User Model representing user-
specific features. We train a dialog sim-
ulator that combines traits of human be-
havior such as cooperativeness and con-
text with domain-related aspects via the
Expectation-Maximization algorithm. We
show that cooperativeness provides a finer
representation of the dialog context which
directly affects task completion rate.
1 Introduction
Data-driven techniques are a promising approach
to the development of robust (spoken) dialog sys-
tems, particularly when training statistical dialog
managers (Varges et al, 2009). User simulators
have been introduced to cope with the scarcity of
real user conversations and optimize a number of
SDS components (Schatzmann et al, 2006).
In this work, we investigate the combination of
aspects of human behavior with contextual aspects
of conversation in a joint yet modular data-driven
simulation model. For this, we integrate conversa-
tional context representation, centered on a Dialog
Act and a Concept Model, with a User Model rep-
resenting persistent individual features. Our aim
is to evaluate different simulation regimes against
real dialogs to identify any impact of user-specific
features on dialog performance.
In this paper, Section 2 presents our simulator
architecture and Section 3 focuses on our model of
cooperativeness. Our experiments are illustrated
Work partly funded by EU project ADAMACH (022593)
and Spanish project OPENMT-2 (TIN2009-14675-C03).
in Section 4 and conclusions are summarized in
Section 5.
2 Simulator Architecture
Data-driven simulation takes place within the rule-
based version of the ADASearch system (Varges
et al, 2009), which uses a taxonomy of 16 dialog
acts and a dozen concepts to deal with three tasks
related to tourism in Trentino (Italy): Lodging En-
quiry, Lodging Reservation and Event Enquiry.
Simulation in our framework occurs at the in-
tention level, where the simulator and the Dia-
log Manager (DM) exchange actions, i.e. or-
dered sequences of dialog acts and a number of
concept-value pairs. In other words, we repre-
sent the DM action as as = {da0, .., dan}, (s
is for ?System?) where daj is short for a dialog
act defined over zero or more concept-value pairs,
daj(c0(v0), .., cm(vm)).
In response to the DM action as, the different
modules that compose the User Simulator gener-
ate an N -best list of simulated actions Au(as) =
{a0u, .., a
N
u }. The probability of each possible ac-
tion being generated after the DM action as is es-
timated based on the conversation context. Such a
context is represented by a User Model, a Dialog
Act Model, a Concept Model and an Error Model
(Quarteroni et al, 2010). The User Model simu-
lates the behavior of an individual user in terms of
goals and other behavioral features such as coop-
erativeness and tendency to hang up. The Dialog
Act Model generates a distribution of M actions
Au = {a0u, .., a
M
u }. Then, one action a?u is chosen
out of Au. In order to vary the simulation behav-
ior, the choice of the user action a?u is a random
sampling according to the distribution of proba-
bilities therein; making the simulation more realis-
tic. Finally, the Concept Model generates concept
values for a?u; and the Error Model simulates the
noisy ASR-SLU channel by ?distorting? a?u.
These models are derived from the ADASearch
217
dataset, containing 74 spoken human-computer
conversations.
2.1 User Model
The User Model represents user-specific fea-
tures, both transient and persistent. The
transient feature we focus on in this work is
the user?s goal in the dialog (UG), represented
as a task name and the list of concepts and
values required to fulfill it: an example of
UG is {Activity(EventEnquiry), Time day(2),
Time month(may), Event type(fair), Loca-
tion name(Povo)}.
Persistent features included in our model so far
are: patience, silence (no input) and cooperative-
ness. Patience pat is defined as the tendency
to abandon the conversation (hang up event), i.e.
pat = P (HangUp|as). Similarly, NoInput prob-
ability noi is used to account for user behavior in
noisy environments: noi = P (NoInput|as). Fi-
nally, cooperativeness coop is a real value repre-
senting the ratio of concepts mentioned in as that
also appear in a?u (see Section 3).
2.2 Dialog Act Model
We define three Dialog Act (DA) Models: Obedi-
ent (OB), Bigram (BI) and Task-based (TB).
In the Obedient model, total patience and coop-
erativeness are assumed of the user, who will al-
ways respond to each query requiring values for a
set of concepts with an answer concerning exactly
such concepts. Formally, the model responds to a
DM action as with a single user action a?u obtained
by consulting a rule table, having probability 1. In
case a request for clarification is issued by the DM,
this model returns a clarifying answer. Any offer
from the DM to continue the conversation will be
either readily met with a new task request or de-
nied at a fixed probability: Au(as) = {(a?u, 1)}.
In the Bigram model, first defined in (Eckert et
al., 1997), a transition matrix records the frequen-
cies of transition from DM actions to user actions,
including hang up and no input/no match. Given
a DM action as, the model responds with a list of
M user actions and their probabilities estimated
according to action distribution in the real data:
Au(as) = {(a0u, P (a
0
u|as)), .., (a
M
u , P (a
M
u |as))}.
The Task-based model, similarly to the ?goal?
model in (Pietquin, 2004), produces an action dis-
tribution containing only the actions observed in
the dataset of dialogs in the context of a spe-
cific task Tk. The TB model divides the dataset
into one partition for each Tk, then creates a
task-specific bigram model, by computing ? k:
Au(as) = {(a0u, P (a
0
u|as, Tk)), .., (a
M
u , P (a
M
u |as, Tk))}.
As the partition of the dataset reduces the number
of observations, the TB model includes a mech-
anism to back off to the simpler bigram and uni-
gram models.
2.3 Concept & Error Model
The Concept Model takes the action a?u selected
by the DA Model and attaches values and sam-
pled interpretation confidences to its concepts. In
this work, we adopt a Concept Model which as-
signs the corresponding User Goal values for the
required concepts, which makes the user simulated
responses consistent with the user goal.
The Error Model is responsible of simulating
the noisy communication channel between user
and system; as we simulate the error at SLU level,
errors consist of incorrect concept values. We ex-
periment with a data-driven model where the pre-
cision Prc obtained by a concept c in the refer-
ence dataset is used to estimate the frequency with
which an error in the true value v? of c will be in-
troduced: P (c(v)|c(v?)) = 1? Prc (Quarteroni et
al., 2010).
3 Modelling Cooperativeness
As in e.g. (Jung et al, 2009), we define coop-
erativeness at the turn level (coopt) as a function
of the number of dialog acts in the DM action as
sharing concepts with the dialog acts in the user
action au; at the dialog level, coop is the average
of turn-level cooperativeness.
We discretize coop into a binary variable reflect-
ing high vs low cooperativeness based on whether
or not the dialog cooperativeness exceeds the me-
dian value of coop found in a reference corpus; in
our ADASearch dataset, the median value found
for coop is 0.28; hence, we annotate dialogs as co-
operative if they exceed such a threshold, and as
uncooperative otherwise. Using a corpus thresh-
old allows domain- and population-driven tuning
of cooperativeness rather than a ?hard? definition
(as in (Jung et al, 2009)).
We then model cooperativeness as two bigram
models, reflecting the high vs low value of coop.
In practice, given a DM action as and the coop
value (? = high/low) we obtain a list of user ac-
tions and their probabilities:
Au(as, ?) = {(a0u, P (a
0
u|as, ?)), .., (a
M
u , P (a
M
u |as, ?))}.
218
3.1 Combining cooperativeness and context
At this point, the distribution Au(as, ?) is lin-
early interpolated with the distribution of actions
Au(as, ?) obtained using the DA model ? (in the
Task-based DA model; ? can have three values,
one for each task as explained in Section 2.2):
Au(as) = ?? ?Au(as, ?) + ?? ?Au(as, ?),
where ?? and ?? are the weights of each fea-
ture/model and ?? + ?? = 1.
For each user action aiu, ?? and ?? are
estimated using the Baum-Welch Expectation-
Maximization algorithm as proposed by (Jelinek
and Mercer, 1980). We use the distributions of ac-
tions obtained from our dataset and we align the
set of actions of the two models. Since we only
have two models, we only need to calculate ex-
pectation for one of the distributions:
P (?|as, a
i
u) =
P (aiu|as, ?)
P (aiu|as, ?) + P (aiu|as, ?)
?Mi=0a
i
u
where M is the number of actions. Then, the
weights ?? and ?? that maximize the data like-
lihood are calculated as follows:
?? =
?M
j=0 P (?|as, a
j
u)
M
;?? = 1? ??.
The resulting combined distribution Au(as) is
obtained by factoring the probabilities of each ac-
tion with the weight estimated for the particular
distribution:
Au(as) = {(a
0
u, ???P (a
0
u|as, ?)), .., (a
M
u , ???P (a
M
u |as, ?)),
(a0u, ?? ? P (a
0
u|as, ?)), .., (a
M
u , ?? ? P (a
M
u |as, ?))}
3.2 Effects of cooperativeness
To assess the effect of the cooperativeness feature
in the final distribution of actions, we set a 5-fold
cross-validation experiment with the ADASearch
dataset where we average the ?? estimated at each
turn of the dialog. We investigated in which con-
text cooperativeness provides more contribution
by comparing the ?? weights attributed by high
vs. low coop models to user action distributions in
response to Dialog Manager actions.
Figure 1 shows the values achieved by ?? for
several DM actions for high vs low coop regimes.
We can see that ?? achieves high values in case
of uncooperative users in response to DM dialog
acts as [ClarificationRequest] and [Info-request].
In contrast, forward-looking actions, such as the
ones including [Offer], seem to discard the con-
tribution of the low coop model, but to favor the
contribution provided by high coop.
!"
!#$"
!#%"
!#&"
'()*+,-*./012345" '-*./012345" '67189/34:3;<5" '=/33<,>?3/5" '>?3/5" '6718/@,>?3/5"
A0+A" *8B"
Figure 1: Estimated ?? weights in response to se-
lected DM actions in case of high/low coop
4 Experiments
We evaluate our simulator models using two meth-
ods: first, ?offline? statistics are used to assess
how realistic the action estimations by DA Models
are with respect to a dataset of real conversations
(Sec. 4.1); then, ?online? statistics (Sec. 4.2) eval-
uate end-to-end simulator performance in terms of
dialog act distributions, error robustness and task
duration and completion rates by comparing real
dialogs with fresh simulated dialogs using action
sampling in the different simulation models.
4.1 ?Offline? statistics
In order to compare simulated and real user ac-
tions, we evaluate dialog act Precision (PDA)
and Recall (RDA) following the methodology in
(Schatzmann et al, 2005).
For each DM action as the simulator picks a
user action a?u from Au(as) and we compare it
with the real user choice a?u. A simulated dialog
act is correct when it appears in the real action
a?u. The measurements were obtained using 5-fold
cross-validation on the ADASearch dataset.
Table 1: Dialog Act Precision and Recall
Simulation (a?u) Most frequent (a
?
u)
DA Model PDA RDA PDA RDA
OB 33.8 33.4 33.9 33.5
BI (+coop) 35.6 (35.7) 35.5 (35.8) 49.3 (47.9) 48.8 (47.4)
TB (+coop) 38.2 (39.7) 38.1 (39.4) 51.1 (50.6) 50.6 (50.2)
Table 1 shows PDA/RDA obtained for the OB,
BI and TB models alone and with cooperative-
ness models (+coop). First, we see that TB is
much better than BI and OB at reproducing real
action selection. This is also visible in both PDA
and RDA obtained by selecting a?u, the most fre-
quent user action from the As generated by each
model. By definition, a?u maximizes the expected
PDA and RDA, providing an upper bound for our
models; however, to reproduce any possible user
behavior, we need to sample au rather than choos-
ing it by frequency. By now inspecting (+coop)
219
values in Table 1, we see that explicit cooperative-
ness models match real dialogs more closely. It
points out that partitioning the reference dataset in
high vs low coop sets allows better data represen-
tation. There is however no improvement in the
a?u case: we explain this by the fact that by ?slic-
ing? the reference dataset, the cooperative model
augments data sparsity, affecting robustness.
4.2 ?Online? statistics
We now discuss online deployment of our sim-
ulation models with different user behaviors and
?fresh? user goals and data. To align with the
ADASearch dataset, we ran 60 simulated dialogs
between the ADASearch DM and each combina-
tion of the Task-based and Bigram models and
high and low values of coop. For each set of simu-
lated dialogs, we measured task duration, defined
as the average number of turns needed to complete
each task, and task completion rate, defined as:
TCR = number of times a task has been completedtotal number of task requests .
Table 2 reports such figures in comparison
to the ones obtained for real dialogs from the
ADASearch dataset. In general, we see that task
duration is closer to real dialogs in the Bigram and
Task-based models when compared to the Obedi-
ent model. Moreover, it can easily be observed
in both BI and TB models that under high-coop
regime (in boldface), the number of turns taken
to complete tasks is lower than under low-coop.
Furthermore, in both TB and BI models, TCR
is higher when cooperativeness is higher, indicat-
ing that cooperative users make dialogs not only
shorter but also more efficient.
Table 2: Task duration and TCR in simulated di-
alogs with different regimes vs real dialogs.
Lodging Enquiry Lodging Reserv Event Enquiry All
Model #turns TCR #turns TCR #turns TCR TCR
OB 9.2?0.0 78.1 9.7?1.4 82.4 8.1?2.9 66.7 76.6
BI+low 15.1?4.1 71.4 14.2?3.9 69.4 9.3?1.8 52.2 66.7
BI+high 12.1?2.5 74.6 12.9?3.1 82.9 7.8?1.8 75.0 77.4
TB+low 13.6?4.1 75.8 13.4?3.7 83.3 8.4?3.3 64.7 77.2
TB+high 11.6?2.8 80.0 12.6?3.6 83.7 6.5?1.9 57.1 78.4
Real dialogs 11.1?3.0 71.4 12.7?4.7 69.6 9.3?4.0 85.0 73.4
5 Conclusion
In this work, we address data-driven dialog sim-
ulation for the training of statistical dialog man-
agers. Our simulator supports a modular combina-
tion of user-specific features with different models
of dialog act and concept-value estimation, in ad-
dition to ASR/SLU error simulation.
We investigate the effect of joining a model of
user intentions (Dialog Act Model) with a model
of individual user traits (User Model). In partic-
ular, we represent the user?s cooperativeness as
a real-valued feature of the User Model and cre-
ate two separate simulator behaviors, reproducing
high and low cooperativeness. We explore the im-
pact of combining our cooperativeness model with
the Dialog Act model in terms of dialog act accu-
racy and task success.
We find that 1) an explicit modelling of user
cooperativeness contributes to an improved accu-
racy of dialog act estimation when compared to
real conversations; 2) simulated dialogs with high
cooperativeness result in higher task completion
rates than low-cooperativeness dialogs. In future
work, we will study yet more fine-grained and re-
alistic User Model features.
References
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. IEEE ASRU.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Workshop on Pattern Recognition in Practice.
S. Jung, C. Lee, K. Kim, and G. G. Lee. 2009. Hy-
brid approach to user intention modeling for dialog
simulation. In Proc. ACL-IJCNLP.
O. Pietquin. 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Fac-
ulte? Polytechnique de Mons, TCTS Lab (Belgique).
S. Quarteroni, M. Gonza?lez, G. Riccardi, and
S. Varges. 2010. Combining user intention and error
modeling for statistical dialog simulators. In Proc.
INTERSPEECH.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative evaluation of user simulation tech-
niques for spoken dialogue systems. In Proc. SIG-
DIAL.
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of di-
alogue management strategies. Knowl. Eng. Rev.,
21(2):97?126.
S. Varges, S. Quarteroni, G. Riccardi, A. V. Ivanov, and
P. Roberti. 2009. Leveraging POMDPs trained with
user simulations and rule-based dialogue manage-
ment in a spoken dialogue system. In Proc. SIG-
DIAL.
220
