Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 466?475,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative state tracking for spoken dialog systems
Angeliki Metallinou1?, Dan Bohus2, and Jason D. Williams2
1University of Southern California, Los Angeles, CA, USA
2Microsoft Research, Redmond, WA, USA
metallin@usc.edu dbohus@microsoft.com jason.williams@microsoft.com
Abstract
In spoken dialog systems, statistical state
tracking aims to improve robustness to
speech recognition errors by tracking a
posterior distribution over hidden dialog
states. Current approaches based on gener-
ative or discriminative models have differ-
ent but important shortcomings that limit
their accuracy. In this paper we discuss
these limitations and introduce a new ap-
proach for discriminative state tracking
that overcomes them by leveraging the
problem structure. An offline evaluation
with dialog data collected from real users
shows improvements in both state track-
ing accuracy and the quality of the pos-
terior probabilities. Features that encode
speech recognition error patterns are par-
ticularly helpful, and training requires rel-
atively few dialogs.
1 Introduction
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog
in a process called dialog state tracking. For ex-
ample, in a bus schedule information system, the
dialog state might indicate the user?s desired bus
route, origin, and destination. Dialog state track-
ing is difficult because automatic speech recog-
nition (ASR) and spoken language understand-
ing (SLU) errors are common, and can cause the
system to misunderstand the user?s needs. At
the same time, state tracking is crucial because
the system relies on the estimated dialog state to
choose actions ? for example, which bus schedule
information to present to the user.
The dialog state tracking problem can be for-
malized as follows (Figure 1). Each system turn
in the dialog is one datapoint. For each datapoint,
the input consists of three items: a set of K fea-
tures that describes the current dialog context, G
dialog state hypotheses, and for each dialog state
hypothesis, M features that describe that dialog
state hypothesis. The task is to assign a probabil-
ity distribution over the G dialog state hypotheses,
plus a meta-hypothesis which indicates that none
of the G hypotheses is correct.
Note that G varies across turns (datapoints) ?
for example, in the first turn of Figure 1, G = 3,
and in the second and third turns G = 5. Also
note that the dialog state tracker is not predicting
the contents of the dialog state hypotheses; the di-
alog state hypotheses contents are given by some
external process, and the task is to predict a proba-
bility distribution over them, where the probability
assigned to a hypothesis indicates the probability
that it is correct. It is a requirement that the G
hypotheses are disjoint; with the special ?every-
thing else? meta-hypothesis, exactly one hypoth-
esis is correct by construction. After the dialog
state tracker has output its distribution, this distri-
bution is passed to a separate, downstream process
that chooses what action to take next (e.g., how to
respond to the user).
Dialog state tracking can be seen an analogous
to assigning a probability distribution over items
on an ASR N-best list given speech input and the
recognition output, including the contents of the
N-best list. In this task, the general features de-
scribe the recognition overall (such as length of
utterance), and the hypothesis-specific features de-
scribe each N-best entry (such as decoder cost).
? Work done while at Microsoft Research
466
Another analogous task is assigning a probabil-
ity distribution over a set of URLs given a search
query and the URLs. Here, general features de-
scribe the whole set of results, e.g., number of
words in the query, and hypothesis-specific fea-
tures describe each URL, e.g., the fraction of
query words contained in page.
For dialog state tracking, most commercial sys-
tems use hand-crafted heuristics, selecting the
SLU result with the highest confidence score,
and discarding alternatives. In contrast, statisti-
cal approaches compute a posterior distribution
over many hypotheses for the dialog state. The
key insight is that dialog is a temporal process in
which correlations between turns can be harnessed
to overcome SLU errors. Statistical state track-
ing has been shown to improve task completion
in end-to-end spoken dialog systems (Bohus and
Rudnicky (2006); Young et al (2010); Thomson
and Young (2010)).
Two types of statistical state tracking ap-
proaches have been proposed. Generative ap-
proaches (Horvitz and Paek (1999); Williams and
Young (2007); Young et al (2010); Thomson and
Young (2010)) use generative models that capture
how the SLU results are generated from hidden
dialog states. These models can be used to track
an arbitrary number of state hypotheses, but can-
not easily incorporate large sets of potentially in-
formative features (e.g. from ASR, SLU, dialog
history), resulting in poor probability estimates.
As an illustration, in Figure 1, a generative model
might fail to assign the highest score to the correct
hypothesis (61C) after the second turn. In contrast,
discriminative approaches use conditional mod-
els, trained in a discriminative fashion (Bohus and
Rudnicky (2006)) to directly estimate the distribu-
tion over a set of state hypotheses based on a large
set of informative features. They generally pro-
duce more accurate distributions, but in their cur-
rent form they can only track a handful of state hy-
potheses. As a result, the correct hypothesis may
be discarded: for instance, in Figure 1, a discrim-
inative model might consider only the top 2 SLU
results, and thus fail to consider the correct 61C
hypothesis at all.
The main contribution of this paper is to de-
velop a new discriminative model for dialog state
tracking that can operate over an arbitrary number
of hypotheses and still compute accurate proba-
bility estimates. We also explore the relative im-
portance of different feature sets for this task, and
measure the amount of data required to reliably
train our model.
2 Data and experimental design
We use data from the public deployment of two
systems in the Spoken Dialog Challenge (Black
et al (2010)) which provide bus schedule infor-
mation for Pittsburgh, USA. The systems, DS1
and DS2, were fielded by AT&T, and are de-
scribed in Williams et al (2010) and Williams
(2012). Both systems followed a highly directed
flow, separately collecting 5 slots. All users were
asked for their bus route, origin, and destination;
then, they were optionally prompted for a date and
time. Each slot was explicitly or implicitly con-
firmed before collecting the next. At the end, bus
times were presented. The two systems differed in
acoustic models, confidence scoring model, state
tracking method and parameters, number of sup-
ported routes (8 vs 40, for DS1 and DS2 respec-
tively), presence of minor bugs, and user popu-
lation. These differences yield distinctions in the
distributions in the two corpora (Williams (2012)).
In both systems, a dialog state hypothesis con-
sists of a value of the user?s goal for a certain
slot: for example, a state hypothesis for the origin
slot might be ?carnegie mellon university?. The
number G of state hypotheses (e.g. slot values)
observed so far depends on the dialog, and turn
within that dialog. For instance, in Fig. 1, G pro-
gressively takes values 3, 5 and 5. Dialog state
hypotheses with identical contents (e.g., the same
bus route) are merged. The correctness of the SLU
results was manually labeled by professional an-
notators.
2.1 Experimental setup
To perform a comparative analysis of various state
tracking algorithms, we test them offline, i.e., by
re-running state tracking against the SLU results
from deployment. However, care must be taken:
when the improved state-tracker is installed into a
dialog system and used to drive action selection,
the distribution of the resulting dialog data (which
is an input for the state tracker) will change. In
other words, it is known a priori that the train
and test distributions will be mismatched. Hence,
when conducting offline experiments, if train and
test data were drawn from the same matched dis-
tribution, this may overstate performance.
467
Figure 1: Overview of dialog state tracking. In this example, the dialog state contains the user?s desired
bus route. At each turn, the system produces a spoken output. The user?s spoken response is processed
to extract a set of spoken language understanding (SLU) results, each with a local confidence score. A
set of G dialog state hypotheses is formed by considering all SLU results observed so far, including
the current turn and all previous turns. For each state hypothesis, a feature extractor produces a set of
M hypothesis-specific features, plus a single set of K general features that describes the current dialog
context. The dialog state tracker uses these features to produce a distribution over theG state hypotheses,
plus a meta-hypothesis rest which accounts for the possibility that none of the G hypotheses are correct.
dataset train set test set
MATCH1 half calls from DS2 remaining calls in DS2
MATCH2 half calls from DS1,
half from DS2
remaining calls from
DS1 and DS2
MISMATCH all calls from DS1 all calls from DS2
Table 1: Train-test data splits
To account for this effect, we explicitly study
train/test mismatch through three partitions of data
from DS1 and DS2 (see Table 1): MATCH1 con-
tains matched train/test data from the DS2 dataset;
MATCH2 contains matched train/test data from
both datasets; finally, MISMATCH contains mis-
matched train/test data. While the MISMATCH
condition may not identically replicate the mis-
match observed from deploying a new state tracker
online (since online characteristics depend on user
behavior) training on DS1 and testing on DS2 at
least ensures the presence of some real-world mis-
match.
We assess performance via two metrics: accu-
racy and L2 norm. Accuracy indicates whether the
state hypothesis with the highest assigned proba-
bility is correct, where rest is correct iff none of
the SLU results prior to the current turn include the
user?s goal. High accuracy is important as a dialog
system must ultimately commit to a single inter-
pretation of the user?s needs ? e.g., it must commit
to a route in order to provide bus timetable infor-
mation. In addition, the L2 norm (or Brier score,
Murphy (1973)) also captures how well calibrated
the output probabilities are, which is crucial to de-
cision theoretic methods for action selection. The
L2 norm is computed between the output poste-
rior and the ground-truth vector, which has 1 in
the position of the correct item and 0 elsewhere.
Both metrics are computed for each slot in each
turn, and reported by averaging across all turns
and slots.
468
2.2 Hand-crafted baseline state tracker
As a baseline, we construct a hand-crafted state
tracking rule that follows a strategy common in
commercial systems: it returns the SLU result
with the maximum confidence score, ignoring all
other hypotheses. Although this is very a simple
rule, it is very often effective. For example, if the
user says ?no? to an explicit confirmation or ?go
back? to an implicit confirmation, they are asked
the same question again, which gives an opportu-
nity for a higher confidence score. Of the G pos-
sible hypotheses for a slot, we denote the number
actually assigned a score by a model as G?, so in
this heuristic baseline G? = 1.
The performance of this baseline (BASELINE
in Table 3) is relatively strong because the top
SLU result is by far most likely to be correct, and
because the confidence score was already trained
with slot-specific speech data (Williams and Bal-
akrishnan (2009), Williams (2012)). However,
this simple rule can?t make use of SLU results on
the N-best list, or statistical priors; these limita-
tions motivate the use of statistical state trackers,
introduced next.
3 Generative state tracking
Generative state tracking approaches leverage
models that describe how SLU results are gener-
ated from a hidden dialog state, denoted g. The
user?s true (unobserved) action u is conditioned on
g and the system action a via a user action model
P (u|g, a), and also on the observed SLU result
u? via a model of how SLU results are generated
P (u?|u). Given a prior distribution b(g) and a re-
sult u?, an updated distribution b?(g) can be com-
puted by summing over all hidden user actions u:
b?(g) = ?
?
u
P (u?|u) ? P (u|g, a)b(g) (1)
where ? is a normalizing constant (Williams and
Young (2007)). Generative approaches model the
posterior over all possible dialog state hypotheses,
including those not observed in the SLU N-best
lists. In general this is computationally intractable
because the number of states is too large. One ap-
proach to scaling up is to group g into a few par-
titions, and to track only states suggested by ob-
served SLU results (Young et al (2010); Williams
(2010); Gas?ic? and Young (2011)). Another ap-
proach is to factor the components of a dialog
state, make assumptions about conditional inde-
pendence between the components, and apply ap-
proximate inference techniques such as loopy be-
lief propagation (Thomson and Young (2010)).
In deployment, DS1 and DS2 used the AT&T
Statistical Dialog Toolkit (ASDT) for dialog state
tracking (Williams (2010); AT&T Statistical Dia-
log Toolkit). ASDT implements a generative up-
date of the form of Eq 1, and uses partitions to
maintain tractability. Component models were
learned from dialog data from a different dia-
log system. A maximum of G? = 20 state hy-
potheses were tracked for each slot. The per-
formance (GENONLINE in Table 3), was worse
than BASELINE: an in-depth analysis attributed
this to the mismatch between train and test data
in the component models, and to the underlying
flawed assumption of eq. 1 that observations at
different turns are independent conditioned on the
dialog state ? in practice, confusions made by
speech recognition are highly correlated (Williams
(2012)).
For all datasets, we re-estimated the models on
the train set and re-ran generative tracking with
an unlimited number of partitions (i.e., G? = G);
see GENOFFLINE in Table 3. The re-estimated
tracker improved accuracy in MATCH conditions,
but degraded accuracy in the MISMATCH condi-
tion. This can be partly attributed to the difficulty
in estimating accurate initial priors b(g) for MIS-
MATCH, where the bus route, origin, and destina-
tion slot values in train and test systems differed
significantly.
4 Discriminative State Tracking:
Preliminaries and existing work
In contrast to generative models, discriminative
approaches to dialog state tracking directly predict
the correct state hypothesis by leveraging discrim-
inatively trained conditional models of the form
b(g) = P (g|f), where f are features extracted
from various sources, e.g. ASR, SLU, dialog his-
tory, etc. In this work we will use maximum en-
tropy models. We begin by briefly introducing
these models in the next subsection. We then de-
scribe the features used, and finally review exist-
ing discriminative approaches for state tracking
which serve as a starting point for the new ap-
proach we introduce in Section 5.
469
4.1 Maximum entropy models
The maximum entropy framework (Berger et al
(1996)) models the conditional probability distri-
bution of the label y given features x, p(y|x) via
an exponential model of the form:
P (y|x, ?) = exp(
?
i?I ?i?i(x, y))?
y?Y exp(
?
i?I ?i?i(x, y))
(2)
where ?i(x, y) are feature functions jointly de-
fined on features and labels, and ?i are the model
parameters. The training procedure optimizes the
parameters ?i to maximize the likelihood over the
data instances subject to regularization penalties.
In this work, we optimize the L1 penalty using a
cross-validation process on the train set, and we
use a fixed L2 penalty based on heuristic based on
the dataset size. The same optimization is used for
all models.
4.2 Features
Discriminative approaches for state tracking rely
on informative features to predict the correct di-
alog state. In this work we designed a set of
hypothesis-specific features that convey informa-
tion about the correctness of a particular state hy-
pothesis, and a set of general features that convey
information about the correctness of the rest meta-
hypothesis.
Hypothesis-specific features can be grouped
into 3 categories: base, history and confusion fea-
tures. Base features consider information about
the current turn, including rank of the current SLU
result (current hypothesis), the SLU result confi-
dence score(s) in the current N-best list, the differ-
ence between the current hypothesis score and the
best hypothesis score in the current N-best list, etc.
History features contain additional useful informa-
tion about past turns. Those include the number of
times an SLU result has been observed before, the
number of times an SLU result has been observed
before at a specific rank such as rank 1, the sum
and average of confidence scores of SLU results
across all past recognitions, the number of possi-
ble past user negations or confirmations of the cur-
rent SLU result etc.
Confusion features provide information about
likely ASR errors and confusability. Some recog-
nition results are more likely to be incorrect than
others ? background noise tends to trigger certain
results, especially short bus routes like ?p?. More-
over, similar sounding phrases are more likely to
be confused. The confusion features were com-
puted on a subset of the training data. For each
SLU result we computed the fraction of the time
that the result was correct, and the binomial 95%
confidence interval for that estimate. Those two
statistics were pre-computed for all SLU results
in the training data subset, and were stored in a
lookup table. At runtime, when an SLU hypoth-
esis is recognized, its statistics from this lookup
table are used as features. Similar statistics were
computed for prior probability of an SLU result
appearing on an N-best list, and prior probability
of SLU result appearance at specific rank positions
of an N-best list, prior probability of confusion be-
tween pairs of SLU results, and others.
General features provide aggregate information
about dialog history and SLU results, and are
shared across different SLU results of an N-best
list. For example, from the current turn, we use
the number of distinct SLU results, the entropy
of the confidence scores, the best path score of
the word confusion network, etc. We also include
features that contain aggregate information about
the sequence of all N-best lists up to the current
turn, such as the mean and variance of N-best list
lengths, the number of distinct SLU results ob-
served so far, the entropy of their corresponding
confidence scores, and others.
We denote the number of hypothesis-specific
features as M , and the number of general features
asK. K andM are each in the range of 100?200,
although M varies depending on whether history
and confusion features are included. For a given
dialog turn with G state hypotheses, there are a to-
tal of G ?M +K distinct features.
4.3 Fixed-length discriminative state
tracking
In past work, Bohus and Rudnicky (2006) intro-
duced discriminative state tracking, casting the
problem as standard multiclass classification. In
this setup, each turn constitutes one data instance.
Since in dialog state tracking the number of state
hypotheses varies across turns, Bohus and Rud-
nicky (2006) chose a subset of G? state hypothe-
ses to score. In this work we used a similar
setup, where we considered the top G1 SLU re-
sults from the current N-best list at turn t, and the
top G2 and G3 SLU results from the previous N-
best lists at turns t ? 1 and t ? 2. The problem
can then be formulated as multiclass classification
470
over G?+1 = G1+G2+G3+1 classes, where the
correct class indicates which of these hypotheses
(or rest) is correct. We experimented with differ-
ent values and found that G1 = 3, G2 = 2, and
G3 = 1 (G? = 6) yielded the best performance.
Feature functions are defined in the standard
way, with one feature function ? and weight ? for
each (feature,class) pair. Formally, ? of eq. 2 is
defined as ?i,j(x, y) = xi?(y, j), where ?(y, j) =
1 if y = j and 0 otherwise. i indexes over the
G?M +K features and j over the G? + 1 classes.1
The two-dimensional subscript i, j if used for clar-
ity of notation, but is otherwise identical in role to
the one-dimension subscript i in Eq 2. Figure 2 il-
lustrates the relationship between hypotheses and
weights.
Results are reported as DISCFIXED in Table 3.
In the MATCH conditions, performance is gener-
ally higher than the other baselines, particularly
when confusion features are included. In the MIS-
MATCH condition, performance is worse that the
BASELINE.
A strength of this approach is that it enables
features from every hypothesis to independently
affect every class. However, the total number
of feature functions (hence weights to learn) is
(G? + 1) ? (G?M +K), which increases quadrat-
ically with the number of hypotheses considered
G?. Although regularization can help avoid over-
fitting per se, it becomes a more challenging task
with more features. Learning weights for each
(feature,class) pair has the drawback that the ef-
fect of hypothesis-specific features such as confi-
dence have to be learned separately for every hy-
pothesis. Also, although we know in advance that
posteriors for a dialog state hypothesis are most
dependent on the features corresponding to that
hypothesis, in this approach the features from all
hypotheses are pooled together and the model is
left to discover these correspondences via learn-
ing. Furthermore, items lower down on the SLU
N-best list are much less likely to be correct: an
item at a very deep position (say 19) might never
be correct in the training data ? when this occurs,
it is unreasonable to expect posteriors to be esti-
mated accurately.
As a result of these issues, in practice G? is lim-
ited to being a small number ? here we found that
increasing G? > 6 degraded performance. Yet with
1Although in practice, maximum entropy model con-
straints render weights for one class redundant.
G? = 6, we found that in 10% of turns, the correct
state hypothesis was present but was being dis-
carded by the model, which substantially reduces
the upper-bound on tracker performance. In the
next section, we introduce a novel discriminative
state tracking approach that addresses the above
limitations, and enables jointly considering an ar-
bitrary number of state hypotheses, by exploiting
the structure inherent in the dialog state tracking
problem.
5 Dynamic discriminative state tracking
The key idea in the proposed approach is to use
feature functions that link hypothesis-specific fea-
tures to their corresponding dialog state hypoth-
esis. This approach makes it straightforward to
model relationships such as ?higher confidence for
an SLU result increases the probability of its cor-
responding state hypothesis being correct?. This
formulation also decouples the number of models
parameters (i.e. weights to learn) from the number
of hypotheses considered, allowing an arbitrary
number of dialog states hypotheses to be scored.
Figure 2: The DISCFIXED model is a traditional
maximum entropy model for classification. Every
feature in every hypothesis is linked to every hy-
pothesis, requiring (G?+ 1)(G?M +K) weights.
We begin by re-stating how features are in-
dexed. Recall each dialog state hypothesis has M
hypothesis-specific features; for each hypothesis,
we concatenate these M features with the K gen-
eral features, which are identical for all hypothe-
ses. For the meta-hypothesis rest, we again con-
catenateM+K features, where theM hypothesis-
specific features take special undefined values. We
write xgi to refer to the ith feature of hypothesis g,
where i ranges from 1 to M +K and g from 1 to
G+ 1.
471
Figure 3: The DISCDYN model presented in this
paper exploits the structure of the state tracking
problem. Features are linked to only their own
hypothesis, and weights are shared across all hy-
potheses, requiring M +K weights.
algorithm description
BASELINE simple hand-crafted rule
GENONLINE generative update, in deployed system
GENOFFLINE generative update, re-trained and run offline
DISCFIXED discr. fixed size multiclass (7 classes)
DISCDYN1 discr. joint dynamic estimation
DISCDYN2 discr. joint dynamic estimation, using indicator
encoding of ordinal features
DISCDYN3 discr. joint dynamic estimation, using indicator
encoding and ordinal-ordinal conjunctions
DISCIND discr. separate estimation
Table 2: Description of the various implemented
state tracking algorithms
The model is based on M + K feature func-
tions. However, unlike in traditional maximum
entropy models such as the fixed-position model
above, these features functions are dynamically
defined when presented with each turn. Specif-
ically, for a turn with G hypotheses, we define
?i(x, y = g) = xgi , where y ranges over the
set of possible dialog states G + 1 (and as above
i ? 1 . . .M +K). The feature function ?i is dy-
namic in that the domain of y ? i.e., the number of
dialog state hypotheses to score ? varies from turn
to turn. With feature functions defined this way,
standard maximum entropy optimization is then
applied to learn the corresponding set of M + K
weights, denoted ?i. Fig. 3 shows the relationship
of hypotheses and weights.
In practice, this formulation ? in which general
features are duplicated across every dialog state
hypothesis ? may require some additional feature
engineering: for every hypothesis g and general
feature i, the value of that general feature xgi will
be multiplied by the same weight ?i. The result
is that any setting of ?i affects all scores identi-
cally, with no net change to the resulting poste-
rior. Nonetheless, general features do contain use-
ful information for state tracking; to make use of
them, we add conjunctions (combinations) of gen-
eral and hypothesis-specific features.
We use 3 different feature variants. In DIS-
CDYN1, we use the original feature set, ignor-
ing the problem described above (so that the gen-
eral features contribute no information), result-
ing in M + K weights. DISCDYN2 adds indi-
cator encodings of the ordinal-valued hypothesis-
specific features. For example, rank is encoded
as a vector of boolean indicators, where the first
indicator is nonzero if rank = 1, the second is
nonzero if rank = 2, and the third if rank ?
3. This provides a more detailed encoding of
the ordinal-valued hypothesis-specific features, al-
though it still ignores information from the gen-
eral features. This encoding increases the number
of weights to learn to about 2(M +K).
Finally, DISCDYN3 extends DISCDYN2 by in-
cluding conjunctions of the ordinal-valued general
features with ordinal-valued hypothesis-specific
features. For example, if the 3-way hypothesis-
specific indicator feature for rank described above
were conjoined with a 4-way general indicator
feature for dialog state, the result would be an in-
dicator of dimension 3 ? 4 = 12. This expansion
results in approximately 10(M + K) weights to
learn in DISCDYN3.2
For comparison, we also estimated a simpler
alternative model, called DISCIND. This model
consists of 2 binary classifiers: the first one
scores each hypothesis in isolation, using the M
hypothesis-specific features for that hypothesis +
the K general features for that turn, and outputs a
(single) probability that the hypothesis is correct.
For this classifier, each hypothesis (not each turn)
defines a data instance. The second binary clas-
sifier takes the K general features, and outputs a
probability that the rest meta-hypothesis is correct.
For this second classifier, each turn defines one
data instance. The output of these two models is
then calibrated with isotonic regression (Zadrozny
and Elkan (2002)) and normalized to generate the
posterior over all hypotheses.
2We explored adding all possible conjunctions, including
real-valued features, but this increased memory and computa-
tional requirements dramatically without performance gains.
472
Metric Accuracy (larger numbers better) L2 (smaller numbers better)
Dataset MATCH1 MATCH2 MISMATCH MATCH1 MATCH2 MISMATCH
Features b bc bch b bc bch b bc bch b bc bch b bc bch b bc bch
BASELINE 61.5 61.5 61.5 63.4 63.4 63.4 62.5 62.5 62.5 27.1 27.1 27.1 25.5 25.5 25.5 27.3 27.3 27.3
GENONLINE 54.4 54.4 54.4 55.8 55.8 55.8 54.8 54.8 54.8 34.8 34.8 34.8 32.0 32.0 32.0 34.8 34.8 34.8
GENOFFLINE 57.1 57.1 57.1 60.1 60.1 60.1 51.8 51.8 51.8 37.6 37.6 37.6 33.4 33.4 33.4 42.0 42.0 42.0
DISCFIXED 61.9 66.7 65.3 63.6 69.7 68.8 59.1 61.9 59.3 27.2 23.6 24.4 25.8 21.9 22.4 28.9 27.8 27.8
DISCDYN1 62.0 70.9 71.1 64.4 72.4 72.9 59.4 61.8 62.3 26.3 21.3 20.9 25.0 20.4 20.1 27.7 26.3 25.9
DISCDYN2 62.6 71.3 71.5 65.7 72.1 72.2 61.9 63.2 63.1 26.3 21.4 21.2 24.4 20.5 20.4 26.9 25.8 25.4
DISCDYN3 63.6 70.1 70.9 65.9 72.1 70.7 60.7 62.1 62.9 26.2 21.5 21.4 24.3 20.6 20.7 27.1 25.9 26.1
DISCIND 62.4 69.8 70.5 63.4 71.5 71.8 59.9 63.3 62.2 26.7 23.3 22.5 25.7 21.8 20.7 28.4 27.3 28.8
Table 3: Performance of the different algorithms on each dataset using three feature combinations. Base
features are denoted as b, ASR/SLU confusion features as c and history features as h. Performance for
the feature combinations bh is omitted for space; it is between b and bc.
6 Results and discussion
The implemented state tracking methods are sum-
marized in Table 2, and our results are presented in
Table 3. These results suggest several conclusions.
First, discriminative approaches for state track-
ing broadly outperform generative methods. Since
discriminative methods incorporate many features
and are trained directly to optimize performance,
this is perhaps unsurprising for the MATCH con-
ditions. It is interesting that discriminative meth-
ods are also superior in the more realistic MIS-
MATCH setting, albeit with smaller gains. This
result suggests that discriminative methods have
good promise when deployed into real systems,
where mismatch between training and test distri-
butions is expected.
Second, the dynamic discriminative DISCDYN
models also outperformed the fixed-length dis-
criminative methods. This shows the benefit of
a model which can score every dialog state hy-
potheses, rather than a fixed subset. Third, the
three variants of the DISCDYN model, which pro-
gressively contain more detailed feature encoding
and conjunctions, perform similarly. This suggests
that a relatively simple encoding is sufficient to
achieve good performance, as the feature indica-
tors and conjunctions present in DISCDYN2 and
DISCDYN3 give only a small additional increase.
Among the discriminative models, the jointly-
optimized DISCDYN versions also slightly out-
perform the simpler, independently-optimized DI-
SCIND version. This is to be expected, for two rea-
sons: first, DISCIND is trained on a per-hypothesis
basis, while the DISCDYN models are trained on a
per-turn basis, which is the true performance met-
ric. For example, some turns have 1 hypothesis
and others have 100, but DISCIND training counts
all hypotheses equally. Second, model parameters
in DISCIND are trained independently of compet-
ing hypotheses. However, they should rather be
adjusted specifically so that the correct item re-
ceives a larger score than incorrect items ? not
merely to increase scores for correct items and de-
crease scores for incorrect items in isolation ? and
this is what is done in the DISCDYN models.
The analysis of various feature sets indicates
that the ASR/SLU error correlation (confusion)
features yield the largest improvement ? c.f. fea-
ture set bc compared to b in Table 3. The im-
provement is smallest for MISMATCH, which un-
derscores the challenges of mismatched train and
test conditions during a realistic runtime scenario.
Note, however, that we have constructed a highly
mismatched case where we train on DS1 (that sup-
ports just 8 routes) and test on DS2 (that supports
40 routes). Therefore, many route, origin and des-
tination slot values in the test data do not appear
in the training data. Hence, it is unsurprising that
the positive effect of confusion features would de-
crease.
While Table 3 shows performance measures av-
eraged across all turns, Table 4 breaks down per-
formance measures by slot, using the full feature
set bch and the realistic MISMATCH dataset. Re-
sults here show a large variation in performance
across the different slots. For the date and time
slots, there is an order of magnitude less data than
for the other slots; however performance for dates
is quite good, whereas times is rather poor. We
believe this is because the SLU confusion features
can be estimated well for slots with small cardinal-
ities (there are 7 possible values for the day), and
less well for slots with large cardinalities (there are
24 ? 60 = 1440 possible time values). This sug-
473
Accuracy (larger numbers better)
algorithms rout origin dest. date time
BASELINE 53.81 66.49 67.78 71.88 52.32
GENONLINE 50.02 54.11 59.05 75.78 35.02
GENOFFLINE 48.12 58.82 58.98 72.66 20.25
DISCFIXED 52.83 67.81 70.67 71.88 33.34
DISCDYN1 54.28 68.24 68.53 79.69 40.51
DISCDYN2 56.18 68.42 70.10 80.47 40.51
DISCDYN3 54.52 66.24 67.96 82.81 43.04
DISCIND 54.25 68.84 70.79 78.13 38.82
L2 metric (smaller numbers better)
algorithms route origin dest. date time
BASELINE 33.15 24.67 24.68 21.61 32.35
GENONLINE 35.50 35.10 31.13 19.86 52.58
GENOFFLINE 46.42 35.73 37.76 19.97 70.30
DISCFIXED 34.09 23.92 23.35 17.59 40.15
DISCDYN1 31.30 23.01 23.07 15.29 37.02
DISCDYN2 30.53 22.40 22.74 13.58 37.59
DISCDYN3 31.58 23.86 23.68 13.93 37.52
DISCIND 36.50 23.45 23.41 15.20 45.43
Table 4: Performance per slot on dataset MIS-
MATCH using the full feature set bch.
(a) MISMATCH dataset (b) MATCH2 dataset
Figure 4: Accuracy vs. amount of training data
gests that the amount of data required to estimate a
good model may depend on the cardinality of slot
values.
Finally, in Figure 4 we show how performance
varies with different amounts of training data for
the MATCH2 and MISMATCH datasets, where the
full training set size is approximately 5600 and
4400 turns, respectively. In both cases asymptotic
performance is reached after about 2000 turns, or
about 150 dialogs. This is particularly encour-
aging, as it suggests models could be learned or
adapted online with relatively little data, or could
even be individually tailored to particular users.
7 Conclusion and Future Work
Dialog state tracking is crucial to the successful
operation of spoken dialog systems. Recently de-
veloped statistical approaches are promising as
they fully utilize the dialog history, and can in-
corporate priors from past usage data. However,
existing methodologies are either limited in their
accuracy or their coverage, both of which hamper
performance.
In this paper, we have introduced a new model
for discriminative state tracking. The key idea is to
exploit the structure of the problem, in which each
dialog state hypothesis has features drawn from
the same set. In contrast to past approaches to dis-
criminative state tracking which required a num-
ber of parameters quadratic in the number of state
hypotheses, our approach uses a constant number
of parameters, invariant to the number of state hy-
potheses. This is a crucial property that enables
generalization and dealing with an unlimited num-
ber of hypotheses, overcoming a key limitation in
previous models.
We evaluated the proposed method and com-
pared it to existing generative and discrimina-
tive approaches on a corpus of real-world human-
computer dialogs chosen to include a mismatch
between training and test, as this will be found
in deployments. Results show that the proposed
model exceeds both the accuracy and probabil-
ity quality of all baselines when using the rich-
est feature set, which includes information about
common ASR confusions and dialog history. The
model can be trained efficiently, i.e. only about
150 training dialogs are necessary.
The next step is to incorporate this approach
into a deployed dialog system, and use the esti-
mated posterior over dialog states as input to the
action selection process. In future, we also hope
to explore unsupervised online adaptation, where
the trained model can be updated as test data is
processed.
Acknowledgments
We thank Patrick Nguyen for helpful discussions
regarding maximum entropy modeling and fea-
ture functions for handling structured and dynamic
output classification problems.
References
AT&T Statistical Dialog Toolkit. AT&T Statistical
Dialog Toolkit. http://www2.research.
att.com/sw/tools/asdt/, 2013.
Adam Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. A maximum entropy approach
to natural language processing. Computational
Linguistics, 22:39?71, 1996.
474
Alan W. Black, S. Burger, B. Langner, G. Par-
ent, and M. Eskenazi. Spoken dialog challenge
2010. In Proc. of Workshop on Spoken Lan-
guage Technologies (SLT), 2010.
Dan Bohus and Alex Rudnicky. A k hypothe-
ses + other belief updating model. In Proc.
of AAAI Workshop on Statistical and Empirical
Approaches to Spoken Dialog Systems, 2006.
Milica Gas?ic? and Steve Young. Effective handling
of dialogue state in the hidden information state
pomdp dialogue manager. ACM Transactions
on Speech and Language Processing, 7, 2011.
Eric Horvitz and Tim Paek. A computational ar-
chitecture for conversation. In Proc. of the 7th
Intl. Conf. on User Modeling, 1999.
Allan H Murphy. A new vector partition of the
probability score. Journal of Applied Meteorol-
ogy, 12:595?600, 1973.
Blaise Thomson and Steve Young. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588, 2010.
Jason D. Williams. Incremental partition recombi-
nation for efficient tracking of multiple dialogue
states. In Proc. of ICASSP, 2010.
Jason D. Williams. Challenges and opportuni-
ties for state tracking in statistical spoken dialog
systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Pro-
cessing, Special Issue on Advances in Spoken
Dialogue Systems and Mobile Interface, 6(8):
959?970, 2012.
Jason D. Williams and Suhrid Balakrishnan. Esti-
mating probability of correctness for asr n-best
lists. In Proc. SigDial Conference, 2009.
Jason D. Williams and Steve Young. Partially ob-
servable markov decision processes for spoken
dialog systems. Computer Speech and Lan-
guage, 21:393?422, 2007.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. Demonstration of AT&T Let?s Go: A
production-grade statistical spoken dialog sys-
tem. In Proc of Workshop on Spoken Language
Technologies (SLT), 2010.
Steve Young, Milica Gas?ic?, Simon Keizer,
Franc?ois Mairesse, Jost Schatzmann, Blaise
Thomson, and Kai Yu. The hidden informa-
tion state model: a practical framework for
POMDP-based spoken dialogue management.
Computer Speech and Language, 24(2):150?
174, 2010.
Bianca Zadrozny and Charles Elkan. Transform-
ing classifier scores into accurate multiclass
probability estimates. In Proc. of the eighth
ACM SIGKDD Intl. Conf on Knowledge Dis-
covery and Data mining, pages 694?699, 2002.
475
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 89?98,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Syllable and language model based features for detecting non-scorable
tests in spoken language proficiency assessment applications
Angeliki Metallinou, Jian Cheng
Knowledge Technologies, Pearson
4040 Campbell Ave., Menlo Park, California 94025, USA
angeliki.metallinou@pearson.com jian.cheng@pearson.com
Abstract
This work introduces new methods for de-
tecting non-scorable tests, i.e., tests that
cannot be accurately scored automatically,
in educational applications of spoken lan-
guage proficiency assessment. Those in-
clude cases of unreliable automatic speech
recognition (ASR), often because of noisy,
off-topic, foreign or unintelligible speech.
We examine features that estimate signal-
derived syllable information and compare
it with ASR results in order to detect
responses with problematic recognition.
Further, we explore the usefulness of lan-
guage model based features, both for lan-
guage models that are highly constrained
to the spoken task, and for task inde-
pendent phoneme language models. We
validate our methods on a challenging
dataset of young English language learn-
ers (ELLs) interacting with an automatic
spoken assessment system. Our proposed
methods achieve comparable performance
compared to existing non-scorable detec-
tion approaches, and lead to a 21% rela-
tive performance increase when combined
with existing approaches.
1 Introduction
Automatic language assessment systems are be-
coming a valuable tool in education, and provide
efficient and consistent student assessment that
can complement teacher assessment. Recently,
there has been a great increase of English Lan-
guage Learners (ELLs) in US education (Pear-
son, 2006). ELLs are students coming from non-
English speaking backgrounds, and often require
additional teacher attention. Thus, assessing ELL
student language proficiency is a key issue.
Pearson has developed an automatic spoken as-
sessment system for K-12 students and collected
a large dataset of ELL students interacting with
the system. This is a challenging dataset, con-
taining accented speech and speech from young
students. Thus, for a small percentage of tests, it
is technically challenging to compute an accurate
automatic score, often because of background/line
noise, off-topic or non-English responses or un-
intelligible speech. Such tests as referred to as
non-scorable. Here, our goal is to propose new
methods for better classifying non-scorable tests
and describe a system for non-scorable detection.
We propose two new sets of features: sylla-
ble based and language model (LM) based. The
intuition is to contrast information from differ-
ent sources when processing a test, in order to
detect inconsistencies in automatic speech recog-
nition (ASR), that often appear in non-scorable
tests. Syllable features measure similarity be-
tween different estimates of syllable locations, one
extracted from ASR and the second from the raw
signal. LM features measure similarity between
two ASR results, one using a standard item spe-
cific word LM, and the second using a item inde-
pendent phoneme LM. Finally, an additional set
of ASR confidence scores and log-likelihoods is
computed using the proposed phoneme LM.
Compared to existing work, our new methods
achieve comparable performance, although they
approach the problem from a different perspective.
Furthermore, our proposed features carry comple-
mentary information to existing ones, and lead to
a 21% relative performance increase when com-
bined with existing work.
2 Related Work
A review of spoken language technologies for ed-
ucation can be found in Eskanazi (2009). There
is a considerable amount of previous work on
automatic speech assessment. Pearson?s auto-
mated speech scoring technologies that measure
the candidates? speaking skill (pronunciation, flu-
89
ency, content) have been used in the Versant series
tests: English, Aviation English, Junior English,
Spanish, Arabic, French, Dutch, Chinese (Bern-
stein et al., 2000; Bernstein and Cheng, 2007;
Cheng et al., 2009; Bernstein et al., 2010; Xu et
al., 2012), and Pearson Test of English Academic
(Pearson, 2011). A non-scorable detection com-
ponent (Cheng and Shen, 2011) is usually required
for such systems. Educational Testing Service de-
scribed a three-stage system on spoken language
proficiency scoring, that rates open-ended speech
and includes a non-scorable detection component
(Higgins et al., 2011).
The system described here evaluates spoken En-
glish skills of ELL students in manner and con-
tent. Past work on children?s automatic assess-
ment of oral reading fluency includes systems that
score performance at the passage-level (Cheng and
Shen, 2010; Downey et al., 2011) or word-level
(Tepperman et al., 2007).
Regarding detecting problematic responses in
speech assessment applications, related work in-
cludes off-topic and non-scorable detection. Non-
scorable detection is a more general problem
which includes not only off-topic responses, but
also noisy, poor quality, foreign or unintelligible
responses, etc. Higgins et al. (2011) describe a
system that uses linear regression and four infor-
mative features (number of distinct words, average
ASR confidence, average and standard deviation
of speech energy) for filtering out non-scorable re-
sponses. Yoon et al. (2011) use a set of 42 signal-
derived and ASR features along with a decision
tree classifier for non-scorable response detection.
Many of their features are also extracted here for
comparison purposes (see Section 7).
Chen and Mostow (2011) focus on off-topic
detection for a reading tutor application. They
use signal features (energy, spectrum, cepstrum
and voice quality features) and ASR features (per-
centage of off-topic words) with a Support Vector
Machine (SVM) classifier. In our previous work
(Cheng and Shen, 2011), we described an off-topic
detection system, where we computed three vari-
ations for ASR confidence scores, along with fea-
tures derived from acoustic likelihood, language
model likelihood, and garbage modeling. Linear
regression was used for classification.
Here, we focus on non-scorable test detection,
using aggregate information from multiple test re-
sponses. We propose new similarity features that
are derived from syllable location estimation and
the use of a item independent phoneme LM.
3 The ELL student dataset
3.1 The asessment system
Pearson has developed an English proficiency as-
sessment test, which has been administered in a
large number of K-12 ELL students in a U.S. state.
The speaking component of the test is delivered
via speakerphone, and the student performance is
automatically scored. Each tests consists of a se-
ries of spoken tasks which are developed by pro-
fessional educators to elicit various displays of
speaking ability. There are repeat tasks, where
students repeat a short sentence, and open ended
tasks, where students are required to answer ques-
tions about an image or a topic, give instructions,
ask a question about an image, etc. Each test
contains multiple test prompts (also referred to as
items), some of which may belong to the same
task. For example, for the ?question about image?
task, there may be items refering to different im-
ages. Each test contains student responses to the
items. Responses which are typically two or three
sentences long.
Figure 1 summarizes the components of Pear-
son?s automatic proficiency assessment system.
Assessment is done through combination of ASR,
speech and text processing, and machine learn-
ing to capture the linguistic content, pronuncia-
tion and fluency of the student?s responses. In this
work, we focus on the lower block of Figure 1 that
illustrates the non-scorable detection component,
whose purpose is to detect the tests that cannot
be reliably scored. It exploits signal related and
ASR information to extract features that are later
used by a binary classifier to decide whether a test
is scorable or not. Our goal is to filter out non-
scorable tests, to be graded by humans. The pro-
ficiency assessment system (upper part of Figure
1) is described elsewhere (Cheng and Shen, 2010;
Downey et al., 2011). The word error rate (WER)
over the test set using the final acoustic models is
around 35%.
3.2 The non-scorable tests
This research focuses on data obtained from four
stages; elementary, primary, middle school and
high school. Those consist of 6000 spoken tests
(1500 per stage), of which 4800 were used for
training (1200 per stage) and the remaining 1200
90
Figure 1: Outline of the assessment system. The
lower block is the non-scorable test detection
module, that is the focus of this work.
were used for testing. Professional human graders
were recruited to provide a grade for each test
response, following pre-defined rubrics per item.
The grades per test are then summed up to com-
pute an overall human grade in the range 0-14.
Each test was double graded and the final human
grade was computed by averaging. Our automatic
scoring system was also used to estimate an over-
all machine grade in the range 0-14 for each test,
after considering all student responses.
We define a test as non-scorable when the over-
all machine and human grades differ by more than
3 points. For our dataset of 6000 tests, only 308
(or approx. 5.1%) are non-scorable, according to
this definition. Inspecting a subset of those tests,
revealed various reasons that may cause a test to
be non-scorable. Those include poor audio qual-
ity (recording or background noise, volume too
loud or too soft), excessive mouth noises and vo-
calizations, foreign language, off-topic responses
and unintelligible speech (extremely disfluent and
mispronounced). As expected, the above issues
are more common among younger test takers. Al-
though the cases above can be very different, a
commonality is that their ASR results are unreli-
able, therefore making subsequent automatic scor-
ing inaccurate. In the following sections, we pro-
pose new methods for detecting problematic ASR
outputs and filtering out non-scorable tests.
4 Syllable based features
The intuition behind the syllable based features
is to compare information coming from the ASR
component with information that is derived di-
rectly from the speech signal. If these two sources
are inconsistent, this may indicate problems in the
recognition output, which often results in non-
scorable tests. Here, we focus on syllable loca-
tions as the type of information to compare. Sylla-
ble locations can be approximated as the vowel lo-
Figure 2: Two examples of mapping between
ASR-derived and signal-derived syllable loca-
tions.
cations of the speech recognition output. Alterna-
tively, they can be approximated using the speech
pitch and intensity signals. By examining inten-
sity, we may find intensity peaks that are preceded
by intensity dips, and by examining pitch, we may
select voiced intensity peaks as estimates of sylla-
ble locations. This method for identifying sylla-
bles was described by Jong and Wempe (2009),
and the number of syllables has been used as a
feature for non-scorable detection in Yoon et al.
(2011). In this work, we propose to use the sylla-
ble information in order to compute features that
measure similarities between signal-derived and
ASR-derived syllable locations.
Assume that we have a sequence of n ASR-
derived syllable locations: X = {x
1
, x
2
, . . . , x
n
}
and a sequence of m signal-derived locations:
Y = {y
1
, y
2
, . . . , y
m
}. The first step in com-
puting similarity features is finding a mapping be-
tween the two sequences. Specifically, we want
to find an appropriate mapping that pairs points
(x
i
, y
j
), x
i
? X, y
j
? Y such that the smallest
possible distances d(x
i
, y
j
) are preferred. Poten-
tially inconsistent points can be discarded. Two
examples are presented in Figure 2. In the up-
per example n > m, therefore some syllable lo-
cations of the longer sequence will be discarded
(here location x
3
). In the lower example, although
n = m, the mapping that produces location pairs
with the smallest distances is (x
1
, y
2
) and (x
2
, y
3
),
while locations y
1
, x
3
will be discarded. A map-
ping (x
3
, y
1
) would be invalid as it violates time
constraints, given the existing mappings. We use a
greedy algorithm for finding the mapping, which
iteratively searches all available valid paired loca-
tions and finds the pair (x
i
, y
j
) with the smallest
91
distance. A mapping (x
i
, y
j
) is valid if no time
constraints are violated, e.g., there is no previously
selected mapping (x
k
, y
l
), where k < i, l > j or
k > i, l < j.
The algorithm is described in Algorithm 1. Our
implementation is recursive: after finding the loca-
tions that define the best available mapping at each
step, the algorithm is recursively called to search
for mappings between points that are both either at
the right subsequences, or at the left subsequences,
with respect to the recent mapping. The right sub-
sequences contain points on the right of the se-
lected mapping (similarly for left subsequences).
That way we avoid searching for mappings that
would violate the time constraints.
Data: Syllable locations X = {x
1
, x
2
, . . . , x
n
} and
Y = {y
1
, y
2
, . . . , y
m
}
Result: Mapping between X and Y. Some locations in X or Y
may be discarded
Compute pairwise distances: d(x
i
, y
j
), x
i
? X, y
j
? Y ;
Set of pairs: E = mapping(1, n, 1,m);
function mapping(i, j, k, l) returns set of pairs ;
if i > j or k > l then
return empty set
end
Find min(d(x
u
, y
v
)), u ? [i, j], v ? [k, l];
E
now
= (u, v);
//check left subsequences
E
left
= mapping(i, u? 1, k, v ? 1);
//check right subsequences
E
right
= mapping(u+ 1, j, v + 1, l);
return union(E
left
, E
now
, E
right
);
Algorithm 1: Compute mapping between ASR-
based and signal-based syllable locations
Based on the mapping of Algorithm 1, we es-
timate a set similarity features including number
of pairs found, number of syllables that were not
paired, the absolute length difference between the
two sequences, as well as normalized versions of
these features (we normalize the features by divid-
ing with the maximum sequence length). For ex-
ample, in the lower part of Figure 2, there are two
pairs and the longest sequence has length three,
so the normalized number of pairs found is 2/3.
Other features include average, min, max and stan-
dard deviation of the distances of the pairs found,
as well as the lengths of the two sequences. These
features are a set of descriptions of the quality of
the mapping or, in other words, of the similarity
between the two syllable sequences.
Algorithm 1 follows a greedy approach, how-
ever, one could derive a similar mapping using dy-
namic programming (DP) to minimize the average
distance over all selected pairs. In practice, we do
(a) Number of syllable pairs found.
(b) Number of pairs over length of largest sequence.
Figure 3: Visualization of feature values across
tasks during a test, for sampled tests. Scorable
tests are in black, non-scorable in dashed red lines.
For tasks that contain multiple responses, we aver-
age the feature values of the responses of a task.
not expect the choice of greedy or DP approach
to greatly affect the final computed similarity fea-
tures, and we chose the greedy approach for sim-
plicity (although DP implementations could be ex-
plored in the future).
To visualize the feature information, we plot the
feature values across tasks of a test, for randomly
sampled tests. For tasks that contain multiple re-
sponses (multiple items), we average the feature
values of the responses of a task. Figure 3(a) visu-
alizes the number of pairs found. Each test is rep-
resented by a set of feature values (one per task)
connected by lines between tasks. Values of some
tasks may be missing if they are undefined, e.g.,
the student did not reply. Scorable tests are repre-
sented in black, and non-scorable tests in dashed
red lines. We notice that the number of pairs found
for non-scorable tests is consistently low through-
out the tasks of the test. This agrees with our
intuition that for non-scorable tests there will be
less similarity between the ASR-based and signal-
based syllable locations, thus there will be fewer
pairs between these two location sequences, com-
pared to scorable tests. Similarly, Figure 3(b) vi-
92
sualizes the normalized pairs found, and again this
percentage is lower for non-scorable tests, indicat-
ing that fewer pairs were found for those tests.
In our implementation, we computed the ASR-
based syllable sequences by performing phoneme-
level forced alignment of the ASR, and approxi-
mating the syllable location as the center of each
vowel segment of the force aligned result. We
computed the signal-based syllable sequence by
augmenting the open source Praat script developed
by Jong and Wempe (2009) to output syllable lo-
cations. The syllable locations are approximate:
computing the syllable detection accuracy would
require human annotation of syllables in our cor-
pus, which is out of the scope of this work. Our
focus is to estimate syllables well enough, so as
to compute useful features. Based on Figures 3(a)
and (b) and the results of Section 9, our syllable
detection works sufficiently well for our purpose.
5 Language model based features
Language models (LMs) are used to model word
transition probabilities in ASR systems, and are
learnt using large text corpora. For cases where
the input speech belongs to a specific topic, it is
common to use constrained LMs, e.g., learn the
word transitions from corpora related to the topic
in question. Here we explore the idea of using dif-
ferent LMs for our ASR system, either highly con-
strained or unconstrained ones, and comparing the
corresponding recognition results. If the ASR re-
sults of the two LMs are very different, then it is
likely that the ASR result is problematic, which
may be indicative of a non-scorable test. To de-
tect those cases, we introduce a set of features that
measure the similarity between ASR results ob-
tained using different language models.
In our system, each item requires the user to talk
about a specific known topic. The default LM used
by our ASR component is item dependent and is
constrained on the topic of the item. In general,
this is beneficial to our system as it allows the ASR
to focus on words that have a high enough likeli-
hood of appearing given the item topic. However,
for some non-scorable tests, we noticed that this
approach may result in misrecognizing phrases
that are off-topic or non-English as valid on-topic
phrases. Therefore, we introduce an unconstrained
LM to detect cases where the constrained LM
causes our system to misrecognize topic specific
words that were not actually spoken. We create the
(a) Edit distance over longest sequence length.
(b) Length difference over longest sequence length.
Figure 4: Visualization of feature values across
tasks during a test, for sampled tests. Scorable
tests are in black, non-scorable in dashed red lines.
unconstrained LM independent of the vocabulary
used, by training a phoneme bigram LM that mod-
els phoneme transition probabilities. Hence, our
LM can handle out of vocabulary or non-English
words that often appear in non-scorable tests.
We use item specific training data to build a
standard bigram word LM for each item. For
the unconstrained LM, we perform phoneme-level
forced alignment of all training data, and build
a item independent bigram phoneme LM. We
perform recognition using both LMs and com-
pare the resulting phoneme-level recognition re-
sults. Comparison is performed by computing
the edit distance between the two phoneme se-
quences, obtained from the two LMs. Edit dis-
tance is a common metric for measuring similar-
ity between sequences and estimates the minimum
number of insertions, deletions or substitutions re-
quired to change one sequence to the other. We
compute a number of similarity features includ-
ing edit distance, length difference between the se-
quences, number of insertions, deletions and sub-
stitutions, as well as normalized versions of those
features (by dividing with the maximum sequence
length). We also include the two phoneme se-
93
quence lengths as features.
Similarly to Section 4, we visualize feature in-
formation by plotting feature values across tasks,
for randomly sampled tests. The resulting plots
for edit distance and length difference between se-
quences, both normalized, are presented in Figures
4 (a) and (b) respectively. Scorable tests are in
black and non-scorable in red dashed lines. Intu-
itively, the more dissimilar the sequences from the
two LMs are, the larger the features values will be
for these two features. Looking at the plots, we
notice that, as expected, non-scorable tests tend
to have larger feature values compared to scorable
ones. This indicates that the proposed phoneme
LM can help detect cases of non-scorable tests.
6 Confidence features
The ASR component of the Pearson assessment
system assigns confidence scores to the recog-
nized words. Three variants of confidence scores
are computed: mconf (based on normalized acous-
tic scores), aconf (based on force alignment and
phoneme recognition) and lconf (lattice-based).
They are described in our previous work (Cheng
and Shen, 2011), where they were used for off-
topic detection. Here, we use them for non-
scorable detection, and compute them separately
using the ASR result obtained from either the
item specific word LM or the item independent
phoneme LM. For each confidence score, our fea-
ture set includes the average score value over
words of a response, and the maximum, mini-
mum and standard deviation. We also compute the
word-level recognition log-likelihood using each
of the two LMs, and include as features the aver-
age, minimum, maximum and standard deviation
of these log-likelihoods over words of a response.
Although the confidence scores are described in
Cheng and Shen (2011), here we compute them
using the proposed phoneme LM (in addition to
the standard word LM), thus they are significantly
different from prior work. Indeed, scores com-
puted by the proposed phoneme LM prove to be
highly informative (see Section 9, Table 3).
7 Signal derived and ASR features
A variety of signal-derived and ASR-based fea-
tures have been used in the literature for non-
scorable detection (Cheng and Shen, 2011; Yoon
et al., 2011; Chen and Mostow, 2011), as well as
related work on pronunciation and fluency assess-
ment (Bernstein et al., 2010; Higgins et al., 2011).
In this study, we extract and include a set of com-
mon features.
Signal-derived features typically describe prop-
erties of the pitch and energy of the speech sig-
nal. Our feature set includes maximum and mini-
mum energy, number of nonzero pitch frames and
average pitch. We also extract features that esti-
mate noise level, specifically Signal to Noise Ra-
tio (SNR). For SNR estimation we used the NIST
Speech Quality Assurance package (NIST, 2009)
Furthermore, we use features extracted from the
ASR result, including utterance duration, number
of words spoken, number of interword pauses, av-
erage interword pause duration, average pause du-
ration before the first spoken word (response la-
tency), and number of hesitations. Pauses, hesi-
tations and response latency have been found in-
formative of speaking fluency (Bernstein et al.,
2010), and could be indicative of problematic,
non-scorable tests. We also compute two varia-
tions of speech rate: words over total response
duration and words over duration of speech (ex-
cluding pauses). Other ASR features we use in-
clude recognition log-likelihood, average LM like-
lihood, number of phonemes pruned during recog-
nition, and average word lattice confidence. We
include some additional confidence-related fea-
tures, like percentage of low confidence words or
phonemes in the response (low confidence is de-
fined based on an experimental threshold).
We compute ASR features that are specific to
the task: either repeat or non-repeat. For the re-
peat tasks, where the student is asked to repeat a
prompt sentence, we compute the number of inser-
tions, deletions and substitutions of the recognized
response compared to the prompt, as well as the
number and percentage of the recognized prompt
words. For the open question (non-repeat) tasks,
where the student gives an open ended response
on a topic, we estimate the number of key words
recognized in the response, from a set of prede-
fined, topic key words.
Finally, we also include some features that are
not used in previous work, and were devised to
enhance earlier versions of our non-scorable de-
tection system. Specifically, we compute the num-
ber of clipped energy frames, where clipping hap-
pens when energy exceeds a max value (often be-
cause the student is speaking too close to the mi-
crophone). Also, we include an indicator feature
94
that indicates when the number of non zero pitch
frames exceeds a certain threshold but the ASR
recognizes only silence. This is a rough way to
detect inconsistencies between the ASR and the
pitch signal, where pitch indicates the presence
of voiced speech, but the ASR recognizes silence.
Although these features are new, for simplicity, we
merge them in our baseline feature set.
Overall, we have extracted a diverse and pow-
erful set of representative features, which will be
referred as ?base? feature set, and is summarized
in Table 1.
Table 1: Summary of features included in the
?Base? feature set
description
signal max and min energy, nonzero pitch frames, avg. pitch, num-
ber of clipped frames, SNR
ASR
number of words spoken, pauses and hesitations, utterance
duration, speech rate (2 variations), avg. interword pause du-
ration, leading pause duration.
ASR log-likelihood, average LM likelihood, number of
phonemes pruned, average word lattice confidence, percent-
age of low confidence words and phonemes
Repeat types: number of insertions, deletions, substitutions,
number of recognized prompt words, percentage of recog-
nized prompt words.
Non repeat types: number of recognized key words
indicator indicator when number of zero pitch frames exceeds a thresh-
old while ASR recognizes silence
8 Random forest classification
We use a binary random forest classifier to decide
if a test is scorable or not. A random forest is
an ensemble of decision trees where each tree de-
cides using a subset of the features and the final
decision is computed by combining the tree deci-
sions (Breiman, 2001). Random forests can take
advantage of feature combinations to construct a
complex, non-linear decision region in the feature
space. In addition, they can be trained fast, have
good generalization properties and do not require
much parameter tuning, which makes them popu-
lar classifiers in the machine learning literature. In
our work, a variety of diverse reasons may cause
a test to be non-scorable, including background or
line/static noise, off-topic responses, non-English
or unintelligible speech. Random forests combine
a number of decision trees that could correspond
to the different sub-cases of our problem, there-
fore they seem well suited for non-scorable test
detection. According to our experiments, random
forests outperform decision trees and maximum
entropy classifiers. Therefore, all results of Sec-
tion 9 are based on random forest classification.
Up to now, we have described feature extraction
for each test response. The non-scorable detection
system needs to aggregate multiple response in-
formation to make an overall decision at the test
level. We can combine response-level features in
a straightforward manner by taking their average
over a test. However, responses may belong to
different types of tasks, either repeat or non repeat
ones, and some of the features are task specific.
Also, repeat task responses often resemble recited
speech, while non-repeat ones tend to be more
spontaneous. To preserve this information, we
separately average features that belong to repeat
responses and non-repeat responses of a test (two
averaged features are extracted per test and per
feature). There are cases where a feature cannot be
extracted for a response, because it is undefined,
i.e., for a response that is recognized as silence
the average interword pause duration is undefined.
Therefore, we also include the percentage of re-
peat or non-repeat responses used to compute the
average, i.e., two percentage features (for repeat
and non-repeat cases) are extracted per test and per
response. More statistics could be extracted when
combining response features, e.g., variance, max
and min values, and others. However, our pre-
liminary experiments indicated that including just
averages and corresponding percentages is suffi-
cient, and adding more statistics greatly increases
the feature vector size without significant perfor-
mance gains. Therefore, our final feature set in-
cludes only averages and percentages.
9 Experiments and results
9.1 Experimental setup
Our experiments are organized in 5-fold cross val-
idation: we randomly split the 6000 tests into five
sets, and each time we use three sets for training
the random forest classifier, one set as a develop-
ment for optimizing the number of trees, and one
set for testing non-scorable classification perfor-
mance. Performance is computed after merging
all test set results. Because the percentage of non-
scorable tests in our dataset is small (approx. 5%)
and random forests are trained with a degree of
randomness, different runs of an experiment can
cause small variations in performance. To mini-
mize this effect, we repeat each 5-fold cross vali-
dation experiment 10 times, and report the average
and standard deviation over the 10 runs.
Performance is estimated using the ROC curve
of false acceptance rate (FAR) versus false rejec-
95
tion rate (FRR) for the binary (scorable vs non-
scorable) classification task. Our goal is to mini-
mize the area under the curve (AUC), e.g., achieve
low values for both FAR and FRR. Our exper-
iments were performed using the Python Scikit-
Learn toolbox (Scikit-Learn, 2014).
9.2 Results
Table 2 presents the average AUC performance
of non-scorable test detection over 10 experi-
ment runs, using different feature sets and ran-
dom forests. ?Base? denotes the set of standard
ASR-based and signal-based features described in
Section 7. Syllable based and LM based denote
the similarity features introduced in Sections 4
and 5 respectively. Finally, ?confidence? denotes
the confidence and log-likelihood features derived
from the standard and the proposed phoneme LM,
as described in Section 6. According to our results,
?base? features are the best performing. However,
it is encouraging that our proposed comparison-
based syllable and LM approaches, that approach
the problem from a different perspective and only
use similarity features, still achieve comparable
performance.
Table 2: Average and standard deviation of AUC
over 10 experiment runs for the different feature
sets, and combinations of feature sets.
features AUC (Avg ? Std.dev)
Base 0.102 ? 0.007
Syllable based 0.122 ? 0.011
LM based 0.123 ? 0.008
Confidence 0.106 ? 0.011
Feature Combination
Base+Syllable 0.091 ? 0.007
Base+LM 0.091 ? 0.011
Base+Confidence 0.094 ? 0.011
All 0.097 ? 0.011
Feature Combination (select top 300 features)
Base+Syllable 0.092 ? 0.008
Base+LM 0.088 ? 0.012
Base+Confidence 0.097 ? 0.010
All 0.092 ? 0.008
Classifier Decision Combination
Base+Syllable 0.087 ? 0.008
Base+LM 0.085 ? 0.007
Base+Confidence 0.084 ? 0.007
All 0.081 ? 0.006
Table 2 also presents the AUC performance af-
ter concatenating the feature vectors of different
feature sets, under ?Feature Combination?. We no-
tice that adding separately each of our proposed
syllable based, LM based and confidence features
to the base features improves performance by de-
creasing AUC. This further indicates that the pro-
Figure 5: Test set ROC curves for different feature
sets, and their combination using decision fusion
(averaging), for one run of the experiment.
posed features carry useful information, which is
complementary to the ?base? feature set. Combin-
ing all features together leads to a relatively small
performance increase, possibly because the large
number of features may cause overfitting.
We also perform feature selection by selecting
the top 300 features from each feature set. Fea-
tures are ranked based on their positions in the
trees of the random forest: features closer to the
root of a tree contribute to the decision of a larger
number of input samples, thus, the expected frac-
tion of the samples that each feature contributes
to, can be used as an estimate of feature impor-
tance. We use Scikit-Learn to compute the fea-
ture importance for each feature, and rank features
based on their average importance over the 10 ex-
periment runs. The results, presented in Table 2,
show that feature selection helps for cases of large
feature sets, i.e., when combining all features to-
gether. However, for cases when fewer features
are used, the performance does not change much
compared to no feature selection.
Finally, instead of concatenating features, we
perform decision combination by averaging the
decisions of classifiers trained on different feature
sets. For simplicity, we perform simple averag-
ing (in future when a larger train set will be avail-
able, we can explore learning appropriate classifier
weights, and performing weighted average). From
the results of Table 2, we notice that this approach
is advantageous and leads to a significant perfor-
mance increase, especially when we combine all
four classifiers: one using existing ?base? features,
and the rest using our new features. Overall, we
96
Table 3: Top-10 ranked features from each feature
set. ?Av? and ?prc? denote that the feature is an av-
erage or percentage respectively, while ?r? and ?nr?
denote that the feature is computed over repeat or
non-repeat responses, respectively. For the confi-
dence features, ?wLM? denotes the feature is com-
puted using regular bigram word LM and ?pLM?
denotes proposed bigram phoneme LM.
feature set description
signal and ASR
n hesitations (av, r) indicator pitch asr (av,r)
min energy (av,r) n pitch frames (av, nr)
n pitch frames (av,r) asr loglik (av, nr)
asr loglik (av,r) min energy (av, nr)
avg pitch( av,nr) snr (av, nr)
syllable based
diff lengths norm (av,r) diff lengths norm (av,nr)
min pair distances(av,nr) diff lengths (av,r)
n pairs norm (av,nr) diff lengths(av,nr)
avg pair distances (av,r) min pair distances (av,r)
n pairs norm (av,r) max pair distances (av,nr)
LM based
edit dist norm (av,r) diff lengths norm (av,r)
n insert norm (av,r) edit dist norm (av,nr)
diff lengths norm (av, nr) n insert norm (av,nr)
n substitute norm (av,nr) min length (av,nr)
min length (av,r) n substitute (av, nr)
Confidence
avg aconf pLM (av,nr) min loglik pLM (av,r)
min loglik pLM (av,nr) max lconf pLM (av,r)
min aconf pLM (av,nr) stddev loglik pLM (av,nr)
min loglik wLM (av,r) min aconf pLM (av,r)
std loglik pLM (av,r) avg loglik pLM (av,r)
achieved a decrease in AUC from 0.102 to 0.081,
a 21% relative performance improvement.
Figure 5 presents the ROC curves for one run
of the experiment, for the four feature sets, and
their combination using averaging of the classifier
decisions. Combining all feature sets leads to a
lower AUC (thick black line). We notice improve-
ment especially in reducing false positives, e.g.,
misclassifying scorable test as non-scorable.
In Table 3, we present the top 10 selected fea-
tures from each feature set, based on their aver-
aged feature importance. Overall, we notice that
both repeat and non-repeat features are among the
top ranked, indicating that both types are infor-
mative. Only average features are among the top
ranked, which suggests that averages carry more
information than percentage features. For the syl-
lable and LM features, we can see many intuitive
similarity features being at the top, such as differ-
ence of sequence lengths, edit distance and num-
ber of insertions (LM based feature set), and aver-
age, min and max of the distances of paired sylla-
bles (syllable based feature set). For confidence,
we note that many log-likelihood features are at
the top (here log-likelihood statistics are computed
over words of a response). Also, note that the
great majority of top-ranked confidence features
are computed using our proposed item indepen-
dent phoneme LM, instead of the regular item de-
pendent word LM, indicating the usefulness of this
approach.
10 Conclusion and future work
In this work, we have proposed new methods for
detecting non-scorable tests in spoken language
proficiency assessment applications. Our meth-
ods compare information extracted from differ-
ent sources when processing a test, and compute
similarity features. Inconsistencies suggest prob-
lematic ASR, which is often indicative of non-
scorable tests. We extract two sets of features:
syllable based, which compare syllable location
information, and LM based, which compare ASR
obtained using item specific and item independent
LMs. Our proposed item independent LM is a
bigram phoneme LM, which can handle out-of-
vocabulary or non-English words, that often ap-
pear in non-scorable tests. By visualizing the pro-
posed similarity features, we verify that they can
highlight inconsistencies that are common in non-
scorable tests. We experimentally validate our
methods in a large, challenging dataset of young
ELLs interacting with the Pearson spoken assess-
ment system. Our features carry complementary
information to existing features, and when com-
bined with existing work, they achieve a 21% rel-
ative performance improvement. Our final, non-
scorable detection system combines the decisions
of four random forest classifiers: one using base-
line features, and the rest using proposed features.
We are currently collecting human annotations
for non-scorable tests in our dataset, which contain
additional annotation of the different non-scorable
subcases in these tests, e.g., noise, off-topic, non-
English, unintelligible speech etc. In the future,
we plan to use these annotations to further validate
our methods, as well as perform detailed evalua-
tion of the usefulness of our proposed feature sets
for each of the non-scorable test subcases.
References
J. Bernstein and J. Cheng. 2007. Logic and valida-
tion of a fully automatic spoken English test. In
V. M. Holland and F. P. Fisher, editors, The Path
of Speech Technologies in Computer Assisted Lan-
guage Learning, pages 174?194. Routledge, New
York.
J. Bernstein, J. De Jong, D. Pisoni, and B. Townshend.
2000. Two experiments on automatic scoring of
spoken language proficiency. In Proc. of STIL (Inte-
grating Speech Technology in Learning).
97
J. Bernstein, A. Van Moere, and J. Cheng. 2010. Vali-
dating automated speaking tests. Language Testing,
27.
L. Breiman. 2001. Random forests. Machine Learn-
ing, 45.
W. Chen and J. Mostow. 2011. A tale of two tasks: De-
tecting children?s off-task speech in a reading tutor.
In Proc. of Interspeech.
J. Cheng and J. Shen. 2010. Towards accurate recog-
nition for children?s oral reading fluency. In Proc. of
IEEE-SLT, pages 91?96.
J. Cheng and J. Shen. 2011. Off-topic detection in
automated speech assessment applications. In Proc.
of Interspeech.
J. Cheng, J. Bernstein, U. Pado, and M. Suzuki. 2009.
Automated assessment of spoken modern standard
arabic. In Proc. of the Fourth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions.
R. Downey, D. Rubin, J. Cheng, and J. Bernstein.
2011. Performance of automated scoring for chil-
dren?s oral reading. In Proc. of the 6th Workshop
on Innovative Use of NLP for Building Educational
Applications.
M. Eskanazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51.
D. Higgins, X. Xi, K. Zechner, and D. Williamson.
2011. A three-stage approach to the automated scor-
ing of spontaneous spoken responses. Computer
Speech and Language, 25.
N. H. De Jong and T. Wempe. 2009. Praat script to
detect syllable nuclei and measure speech rate auto-
matically. Behavior research methods, 41:385?390.
NIST. 2009. The NIST SPeech Quality Assurance
(SPQA) Package. http://www.nist.gov/
speech/tools/index.htm.
G. Pearson. 2006. Ask NCELA No.1: How many
school-aged English-language learners (ELLs) are
there in the U.S.? Washington, D.C: Na-
tional Clearing House for English-Language Ac-
quisition and Language Instruction Educational
Programs 2006, Retrieved Online February 2007
at http://www.ncela.gwu.edu/expert/
faq/01leps.htm.
Pearson. 2011. Skills and scoring in PTE Aca-
demic. http://www.pearsonpte.com/
SiteCollectionDocuments/US_Skills_
Scoring_PTEA_V3.pdf.
Scikit-Learn. 2014. The Scikit-Learn Machine Learn-
ing Python Toolbox. http://scikit-learn.
org/.
J. Tepperman, M. Black, P. Price, S. Lee,
A. Kazemzadeh, M. Gerosa, M. Heritage, A. Al-
wan, and S. Narayanan. 2007. A Bayesian network
classifier for word-level reading assessment. In
Proc. of Interspeech.
X. Xu, M. Suzuki, and J. Cheng. 2012. An automated
assessment of spoken Chinese: Technical definition
of hanyu standards for content and scoring develop-
ment. In Proc. of the Seventh International Confer-
ence & Workshops on Technology & Chinese Lan-
guage Teaching.
S.-Y. Yoon, K. Evanini, and K. Zechner. 2011. Non-
scorable response detection for automated speaking
proficiency assessment. In Proc. of the Sixth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications.
98
