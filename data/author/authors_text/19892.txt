Proceedings of the 12th Conference of the European Chapter of the ACL, pages 10?15,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
NLP and the humanities: the revival of an old liaison
Franciska de Jong
University of Twente
Enschede, The Netherlands
fdejong@ewi.utwente.nl
Abstract
This paper present an overview of some
emerging trends in the application of NLP
in the domain of the so-called Digital Hu-
manities and discusses the role and nature
of metadata, the annotation layer that is so
characteristic of documents that play a role
in the scholarly practises of the humani-
ties. It is explained how metadata are the
key to the added value of techniques such
as text and link mining, and an outline is
given of what measures could be taken to
increase the chances for a bright future for
the old ties between NLP and the humani-
ties. There is no data like metadata!
1 Introduction
The humanities and the field of natural language
processing (NLP) have always had common play-
grounds. The liaison was never constrained to lin-
guistics; also philosophical, philological and lit-
erary studies have had their impact on NLP , and
there have always been dedicated conferences and
journals for the humanities and the NLP com-
munity of which the journal Computers and the
Humanities (1966-2004) is probably known best.
Among the early ideas on how to use machines to
do things with text that had been done manually
for ages is the plan to build a concordance for an-
cient literature, such as the works of St Thomas
Aquinas (Schreibman et al, 2004). which was ex-
pressed already in the late 1940s. Later on hu-
manities researchers started thinking about novel
tasks for machines, things that were not feasible
without the power of computers, such as author-
ship discovery. For NLP the units of process-
ing gradually became more complex and shifted
from the character level to units for which string
processing is an insufficient basis. At some stage
syntactic parsers and generators were seen as a
method to prove the correctness of linguistic the-
ories. Nowadays semantic layers can be analysed
at much more complex levels of granularity. Not
just phrases and sentences are processed, but also
entire documents or even document collections in-
cluding those involving multimodal features. And
in addition to NLP for information carriers, also
language-based interaction has grown into a ma-
tured field, and applications in other domains than
the humanities now seem more dominant. The
impact of the wide range of functionalities that
involve NLP in all kinds of information process-
ing tasks is beyond what could be imagined 60
years ago and has given rise to the outreach of
NLP in many domains, but during a long period
the humanities were one of the few valuable play-
grounds.
Even though the humanities have been able
to conduct NLP-empowered research that would
have been impossible without the the early tools
and resources already for many decades, the more
recent introduction of statistical methods in lan-
gauge is affecting research practises in the human-
ities at yet another scale. An important explana-
tion for this development is of course the wide
scale digitisation that is taken up in the humani-
ties. All kinds of initiatives for converting ana-
logue resources into data sets that can be stored
in digital repositories have been initiated. It is
widely known that ?There is no data like more
data? (Mercer, 1985), and indeed the volumes of
digital humanities resources have reached the level
required for adequate performance of all kinds of
tasks that require the training of statistical mod-
els. In addition, ICT-enabled methodologies and
types of collaboration are being developed and
have given rise to new epistemic cultures. Digital
Humanities (sometimes also referred to as Com-
putational Humanities) are a trend, and digital
scholarship seems a prerequisite for a successful
research career. But in itself the growth of digi-
10
tal resources is not the main factor that makes the
humanities again a good testbed for NLP. A key
aspect is the nature and role of metadata in the hu-
manities. In the next section the role of metadata
in the humanities and the the ways in which they
can facilitate and enhance the application of text
and data mining tools will be described in more
detail. The paper takes the position that for the hu-
manities a variant of Mercer?s saying is even more
true. There is no data like metadata!
The relation between NLP and the humanities
is worth reviewing, as a closer look into the way
in which techniques such as text and link mining
can demonstrate that the potential for mutual im-
pact has gained in strength and diversity, and that
important lessons can be learned for other appli-
cation areas than the humanities. This renewed
liaison with the now digital humanities can help
NLP to set up an innovative research agenda which
covers a wide range of topics including semantic
analysis, integration of multimodal information,
language-based interaction, performance evalua-
tion, service models, and usability studies. The
further and combined exploration of these topics
will help to develop an infrastructure that will also
allow content and data-driven research domains in
the humanities to renew their field and to exploit
the additional potential coming from the ongoing
and future digitisation efforts, as well as the rich-
ness in terms of available metadata. To name a
few fields of scholarly research: art history, media
studies, oral history, archeology, archiving stud-
ies, they all have needs that can be served in novel
ways by the mature branches that NLP offers to-
day. After a sketch in section 2 of the role of
metadata, so crucial for the interaction between
the humanities and NLP, a rough overview of rel-
evant initiatives will be given. Inspired by some
telling examples, it will be outlined what could be
done to increase the chances for a bright future for
the old ties, and how other domains can benefit as
well from the reinvention of the old common play-
ground between NLP and the humanities.
2 Metadata in the Humanities
Digital text, but also multimedia content, can be
mined for the occurrence of patterns at all kinds
of layers, and based on techniques for information
extraction and classification, documents can be an-
notated automatically with a variety of labels, in-
cluding indications of topic, event types, author-
ship, stylistics, etc. Automatically generated an-
notations can be exploited to support to what is
often called the semantic access to content, which
is typically seen as more powerful than plain full
text search, but in principle also includes concep-
tual search and navigation.
The data used in research in the domain of
the humanities comes from a variety of sources:
archives, musea (or in general cultural heritage
collections), libraries, etc. As a testbed for NLP
these collections are particularly challenging be-
cause of the combination of complexity increas-
ing features, such as language and spelling change
over time, diversity in orthography, noisy content
(due to errors introduced during data conversion,
e.g., OCR or transcription of spoken word ma-
terial), wider than average stylistic variation and
cross-lingual and cross-media links. They are
also particularly attractive because of the avail-
able metadata or annotation records, which are the
reflection of analytical and comparative scholarly
processes. In addition, there is a wide diversity
of annotation types to be found in the domain (cf.
the annotation dimensions distinguished by (Mar-
shall, 1998)), and the field has developed mod-
elling procedures to exploit this diversity (Mc-
Carty, 2005) and visualisation tools (Unsworth,
2005).
2.1 Metadata for Text
For many types of textual data automatically gen-
erated annotations are the sole basis for seman-
tic search, navigation and mining. For human-
ities and cultural heritage collections, automati-
cally generated annotation is often an addition to
the catalogue information traditionally produced
by experts in the field. The latter kind of manu-
ally produced metadataa is often specified in ac-
cordance to controlled key word lists and meta-
data schemata agreed for the domain. NLP tag-
ging is then an add on to a semantic layer that in
itself can already be very rich and of high qual-
ity. More recently initiatives and support tools for
so-called social tagging have been proposed that
can in principle circumvent the costly annotation
by experts, and that could be either based on free
text annotation or on the application of so-called
folksonomies as a replacement for the traditional
taxonomies. Digital librarians have initiated the
development of platforms aiming at the integration
of the various annotation processes and at sharing
11
tools that can help to realise an infrastructure for
distributed annotation. But whatever the genesis is
of annotations capturing the semantics of an entire
document, they are a very valuable source for the
training of automatic classifiers. And traditionally,
textual resources in the humanities have lots of it,
partly because the mere art of annotating texts has
been invented in this domain.
2.2 Metadata for Multimedia
Part of the resources used as basis for scholarly
research is non-textual. Apart from numeric data
resources, which are typically strongly structured
in database-like environments, there is a growing
amount of audiovisual material that is of interest
to humanities researchers. Various kinds of multi-
media collections can be a primary source of infor-
mation for humanities researchers, in particular if
there is a substantial amount of spoken word con-
tent, e.g., broadcast news archives, and even more
prominently: oral history collections.
It is commonly agreed that accessibility of het-
erogeneous audiovisual archives can be boosted
by indexing not just via the classical metadata,
but by enhancing indexing mechanisms through
the exploitation of the spoken audio. For sev-
eral types of audiovisual data, transcription of the
speech segments can be a good basis for a time-
coded index. Research has shown that the quality
of the automatically generated speech transcrip-
tions, and as a consequence also the index quality,
can increase if the language models applied have
been optimised to both the available metadata (in
particular on the named entities in the annotations)
and the collateral sources available (Huijbregts et
al., 2007). ?Collateral data is the term used for
secondary information objects that relate to the
primary documents, e.g., reviews, program guide
summaries, biographies, all kinds of textual pub-
lications, etc. This requires that primary sources
have been annotated with links to these secondary
materials. These links can be pointers to source
locations within the collection, but also links to re-
lated documents from external sources. In labora-
tory settings the amount of collateral data is typi-
cally scarce, but in real life spoken word archives,
experts are available to identify and collect related
(textual) content that can help to turn generic lan-
guage models into domain specific models with
higher accuracy.
2.3 Metadata for Surprise Data
The quality of automatically generated content an-
notations in real life settings is lagging behind in
comparison to experimental settings. This is of
course an obstacle for the uptake of technology,
but a number of pilot projects with collections
from the humanities domain show us what can be
done to overcome the obstacles. This can be illus-
trated again with the situation in the field of spo-
ken document retrieval.
For many A/V collections with a spoken au-
dio track, metadata is not or only sparsely avail-
able, which is why this type of collection is often
only searchable by linear exploration. Although
there is common agreement that speech-based, au-
tomatically generated annotation of audiovisual
archives may boost the semantic access to frag-
ments of spoken word archives enormously (Gold-
man et al, 2005; Garofolo et al, 2000; Smeaton
et al, 2006), success stories for real life archives
are scarce. (Exceptions can be found in research
projects in the broadcast news and cultural her-
itage domains, such as MALACH (Byrne et al,
2004), and systems such as SpeechFind (Hansen
et al, 2005).) In lab conditions the focus is usu-
ally on data that (i) have well-known characteris-
tics (e.g, news content), often learned along with
annual benchmark evaluations,1 (ii) form a rela-
tively homogeneous collection, (iii) are based on
tasks that hardly match the needs of real users, and
(iv) are annotated in large quantities for training
purposes. In real life however, the exact character-
istics of archival data are often unknown, and are
far more heterogeneous in nature than those found
in laboratory settings. Language models for real-
istic audio sets, sometimes referred to as surprise
data (Huijbregts, 2008), can benefit from a clever
use of this contextual information.
Surprise data sets are increasingly being taken
into account in research agendas in the field focus-
ing on multimedia indexing and search (de Jong
et al, 2008). In addition to the fact that they are
less homogenous, and may come with links to re-
lated documents, real user needs may be available
from query logs, and as a consequence they are
an interesting challenge for cross-media indexing
strategies targeting aggregated collections. Sur-
1E.g., evaluation activities such as those organised by
NIST, the National Institute of Standards, e.g., TREC for
search tasks involving text, TRECVID for video search, Rich
Transcription for the analysis of speech data, etc. http:
//www.nist.gov/
12
prise data are therefore an ideal source for the de-
velopment of best practises for the application of
tools for exploiting collateral content and meta-
data. The exploitation of available contextual in-
formation for surprise content and the organisation
of this dual annotation process can be improved,
but in principle joining forces between NLP tech-
nologies and the capacity of human annotators is
attractive. On the one hand for the improved ac-
cess to the content, on the other hand for an inno-
vation of the NLP research agenda.
3 Ingredients for a Novel
Knowledge-driven Workflow
A crucial condition for the revival of the com-
mon playground for NLP and the humanities is
the availability of representatives of communities
that could use the outcome, either in the devel-
opment of services to their users or as end users.
These representatives may be as diverse and in-
clude e.g., archivists, scholars with a research in-
terest in a collection, collection keepers in libraries
and musea, developers of educational materials,
but in spite of the divergence that can be attributed
to such groups, they have a few important charac-
teristics in common: they have a deep understand-
ing of the structure, semantic layers and content
of collections, and in developing new road maps
and novel ways of working, the pressure they en-
counter to be cost-effective is modest. They are
the first to understand that the technical solutions
and business models of the popular web search en-
gines are not directly applicable to their domain
in which the workflow is typically knowledge-
driven and labour-intensive. Though with the in-
troduction of new technologies the traditional role
of documentalists as the primary source of high
quality annotations may change, the availability of
their expertise is likely to remain one of the major
success factors in the realisation of a digital in-
frastructure that is as rich source as the reposito-
ries from the analogue era used to be.
All kinds of coordination bodies and action
plans exist to further the field of Digital Hu-
manities, among which The Alliance of Dig-
ital Humanities Organizations http://www.
digitalhumanities.org/ and HASTAC
(https://www.hastac.org/) and Digital
Arts an Humanities www.arts-humanities.
net, and dedicated journals and events have
emerged, such as the LaTeCH workshop series. In
part they can build on results of initiatives for col-
laboration and harmonisation that were started ear-
lier, e.g., as Digital Libraries support actions or as
coordinated actions for the international commu-
nity of cultural heritage institutions. But in order
to reinforce the liaison between NLP and the hu-
manities continued attention, support and funding
is needed for the following:
Coordination of coherent platforms (both lo-
cal and international) for the interaction be-
tween the communities involved that stim-
ulate the exchange of expertise, tools, ex-
perience and guidelines. Good examples
hereof exist already in several domains,
e.g., the field of broadcast archiving (IST
project PrestoSpace; www.prestospace.
org/), the research area of Oral History, all
kinds of communities and platforms targeting
the accessibility of cultural heritage collec-
tions (e.g., CATCH; http://www.nwo.
nl/catch), but the long-term sustainability
of accessible interoperable institutional net-
works remains a concern.
Infrastructural facilities for the support of re-
searchers and developers of NLP tools; such
facilities should support them in finetuning
the instruments they develop to the needs
of scholarly research. CLARIN (http://
www.clarin.eu/) is a promising initia-
tive in the EU context that is aiming to cover
exactly this (and more) for the social sciences
and the humanities.
Open access, source and standards to increase
the chances for inter-institutional collabora-
tion and exchange of content and tools in
accordance with the policies of the de facto
leading bodies, such as TEI (http://www.
tei-c.org/) and OAI (http://www.
openarchives.org/).
Metadata schemata that can accommodate
NLP-specific features:
? automatically generated labels and sum-
maries
? reliability scores
? indications of the suitability of items for
training purposes
Exchange mechanisms for best practices e.g.,
of building and updating training data, the
13
use of annotation tools and the analysis of
query logs.
Protocols and tools for the mark-up of content,
the specification of links between collections,
the handling of IPR and privacy issues, etc.
Service centers that can offer heavy processing
facilities (e.g. named entity extraction or
speech transcription) for collections kept in
technically modestly equipped environments
hereof.
User Interfaces that can flexibly meet the needs
of scholarly users for expressing their infor-
mation needs, and for visualising relation-
ships between interactive information ele-
ments (e.g., timelines and maps).
Pilot projects in which researchers from vari-
ous backgrounds collaborate in analysing
a specific digital resource as a central
object in order to learn to understand
how the interfaces between their fields
can be opened up. An interesting ex-
ample is the the project Veteran Tapes
(http://www.surffoundation.nl/
smartsite.dws?id=14040). This
initiative is linked to the interview collection
which is emerging as a result for the Dutch
Veterans Interview-project, which aims at
collecting 1000 interviews with a represen-
tative group of veterans of all conflicts and
peace-missions in which The Netherlands
were involved. The research results will be
integrated in a web-based fashion to form
what is called an enriched publication.
Evaluation frameworks that will trigger contri-
butions to the enhancement en tuning of what
NLP has to offer to the needs of the hu-
manities. These frameworks should include
benchmarks addressing tasks and user needs
that are more realistic than most of the ex-
isting performance evaluation frameworks.
This will require close collaboration between
NLP developers and scholars.
4 Conclusion
The assumption behind presenting these issues as
priorities is that NLP-empowered use of digital
content by humanities scholars will be beneficial
to both communities. NLP can use the testbed
of the Digital Humanities for the further shaping
of that part of the research agenda that covers the
role of NLP in information handling, and in par-
ticular those avenues that fall under the concept of
mining. By focussing on the integration of meta-
data in the models underlying the mining tools and
searching for ways to increase the involvement of
metadata generators, both experts and ?amateurs?,
important insights are likely to emerge that could
help to shape agendas for the role of NLP in other
disciplines. Examples are the role of NLP in the
study of recorded meeting content, in the field of
social studies, or the organisation and support of
tagging communities in the biomedical domain,
both areas where manual annotation by experts
used to be common practise, and both areas where
mining could be done with aggregated collections.
Equally important are the benefits for the hu-
manities. The added value of metadata-based min-
ing technology for enhanced indexing is not so
much in the cost-reduction as in the wider usabil-
ity of the materials, and in the impulse this may
bring for sharing collections that otherwise would
too easily be considered as of no general impor-
tance. Furthermore the evolution of digital texts
from ?book surrogates? towards the rich semantic
layers and networks generated by text and/or me-
dia mining tools that take all available metadata
into account should help the fields involved in not
just answering their research questions more effi-
ciently, but also in opening up grey literature for
research purposes and in scheduling entirely new
questions for which the availability of such net-
works are a conditio sine qua non.
Acknowledgments
Part of what is presented in this paper has been
inspired by collaborative work with colleagues. In
particular I would like to thank Willemijn Heeren,
Roeland Ordelman and Stef Scagliola for their role
in the genesis of ideas and insights.
References
W. Byrne, D.Doermann, M. Franz, S. Gustman, J. Ha-
jic, D. Oard, M. Picheny, J. Psutka, B. Ramabhad-
ran, D. Soergel, T.Ward, andW-J. Zhu. 2004. Auto-
matic recognition of spontaneous speech for access
to multilingual oral history archives. IEEE Transac-
tions on Speech and Audio Processing, 12(4).
F. M. G. de Jong, D. W. Oard, W. F. L. Heeren, and
R. J. F. Ordelman. 2008. Access to recorded inter-
14
views: A research agenda. ACM Journal on Com-
puting and Cultural Heritage (JOCCH), 1(1):3:1?
3:27, June.
J.S. Garofolo, C.G.P. Auzanne, and E.M Voorhees.
2000. The TREC SDR Track: A Success Story.
In 8th Text Retrieval Conference, pages 107?129,
Washington.
J. Goldman, S. Renals, S. Bird, F. M. G. de Jong,
M. Federico, C. Fleischhauer, M. Kornbluh,
L. Lamel, D. W. Oard, C. Stewart, and R. Wright.
2005. Accessing the spoken word. International
Journal on Digital Libraries, 5(4):287?298.
J.H.L. Hansen, R. Huang, B. Zhou, M. Deadle, J.R.
Deller, A. R. Gurijala, M. Kurimo, and P. Angk-
ititrakul. 2005. Speechfind: Advances in spoken
document retrieval for a national gallery of the spo-
ken word. IEEE Transactions on Speech and Audio
Processing, 13(5):712?730.
M.A.H. Huijbregts, R.J.F. Ordelman, and F.M.G.
de Jong. 2007. Annotation of heterogeneous multi-
media content using automatic speech recognition.
In Proceedings of SAMT 2007, volume 4816 of
Lecture Notes in Computer Science, pages 78?90,
Berlin. Springer Verlag.
M.A.H. Huijbregts. 2008. Segmentation, Diarization
and Speech Transcription: Surprise Data Unrav-
eled. Phd thesis, University of Twente.
C. Marshall. 1998. Toward an ecology of hypertext
annotation. In Proceedings of the ninth ACM con-
ference on Hypertext and hypermedia : links, ob-
jects, time and space?structure in hypermedia sys-
tems (HYPERTEXT ?98), pages 40?49, Pittsburgh,
Pennsylvania.
W. McCarty. 2005. Humanities Computing. Bas-
ingstoke, Palgrave Macmillan.
S. Schreibman, R. Siemens, and J. Unsworth (eds.).
2004. A Companion to Digital Humanities. Black-
well.
A.F. Smeaton, P. Over, and W. Kraaij. 2006. Evalu-
ation campaigns and trecvid. In 8th ACM SIGMM
International Workshop on Multimedia Information
Retrieval (MIR2006).
J. Unsworth. 2005. New Methods for Humanities Re-
search. The 2005 Lyman Award Lecture. National
Humanities Center, NC.
15
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1950?1961, Dublin, Ireland, August 23-29 2014.
Why Gender and Age Prediction from Tweets is Hard:
Lessons from a Crowdsourcing Experiment
Dong Nguyen
14?
Dolf Trieschnigg
14
A. Seza Do
?
gru
?
oz
23
Rilana Gravel
4
Mari
?
et Theune
1
Theo Meder
4
Franciska de Jong
1
(1) Human Media Interaction, University of Twente, Enschede, The Netherlands
(2) Netherlands Institute for Advanced Studies, Wassenaar, NL
(3) Tilburg School of Humanities, Tilburg University, Tilburg, NL
(4) Meertens Institute, Amsterdam, The Netherlands
?
Corresponding author: d.nguyen@utwente.nl
Abstract
There is a growing interest in automatically predicting the gender and age of authors from texts.
However, most research so far ignores that language use is related to the social identity of speak-
ers, which may be different from their biological identity. In this paper, we combine insights
from sociolinguistics with data collected through an online game, to underline the importance
of approaching age and gender as social variables rather than static biological variables. In our
game, thousands of players guessed the gender and age of Twitter users based on tweets alone.
We show that more than 10% of the Twitter users do not employ language that the crowd as-
sociates with their biological sex. It is also shown that older Twitter users are often perceived
to be younger. Our findings highlight the limitations of current approaches to gender and age
prediction from texts.
1 Introduction
A major thrust of research in sociolinguistics aims to uncover the relationship between social variables
such as age and gender, and language use (Holmes and Meyerhoff, 2003; Eckert and McConnell-Ginet,
2013; Eckert, 1997; Wagner, 2012). In line with scholars from a variety of disciplines, including the so-
cial sciences and philosophy, sociolinguists consider age and gender as social and fluid variables (Eckert,
2012). Gender and age are shaped depending on the societal context, the culture of the speakers involved
in a conversation, the individual experiences and the multitude of social roles: a female teenager might
also be a high school student, a piano player, a swimmer, etc. (Eckert, 2008).
Speakers use language as a resource to construct their identity (Bucholtz and Hall, 2005). For example,
a person?s gender identity is constructed through language by using linguistic features associated with
male or female speech. These features gain social meaning in a cultural and societal context. On Twitter,
users construct their identity through interacting with other users (Marwick and boyd, 2011). Depending
on the context, they may emphasize specific aspects of their identity, which leads to linguistic variation
both within and between speakers. We illustrate this with the following three tweets:
Tweet 1: I?m walking on sunshine <3 #and don?t you feel good
Tweet 2: lalaloveya <3
Tweet 3: @USER loveyou ;D
In these tweets, we find linguistic markers usually associated with females (e.g. a heart represented
as <3). Indeed, 77% of the 181 players guessed that a female wrote these tweets in our online game.
However, this is a 16-year old biological male, whose Twitter account reveals that he mostly engages
with female friends. Therefore, he may have accommodated his style to them (Danescu-Niculescu-Mizil
et al., 2011) and as a result he employs linguistic markers associated with the opposite biological sex.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1950
Most of the NLP research focusing on predicting gender and age has approached these variables as bi-
ological and static, rather than social and fluid. For example, current approaches use supervised machine
learning models trained on tweets from males and females. However, the resulting stereotypical models
are ineffective for Twitter users who tweet differently from what is to be expected from their biological
sex.
As explained above, language use is based on social gender and age identity, and not on biological sex
and chronological age. In other words, treating gender and age as fixed biological variables in analyzing
language use is too simplistic. By comparing the biological sex and chronological age of Twitter users
with how they are perceived by the crowd (as an indication of socially constructed identities), we shed
light on the difficulty of predicting gender and age from language use and draw attention to the inherent
limitations of current approaches.
As has been demonstrated in several studies, the crowd can be used for experimentation (e.g., Munro
et al. (2010)). Our study illustrates the value of the crowd for the study of human behavior, in particular
for the experimental study of the social dimension of language use. To collect data, we created an online
game (an example of gamification (Deterding et al., 2011)) in which thousands of players (the crowd)
guessed the biological sex and age of Twitter users based on only the users? tweets. While variance
between annotators has traditionally been treated as noise, more recently variation is being treated as a
signal rather than noise (Aroyo and Welty, 2013). For example, Makatchev and Simmons (2011) analyze
how English utterances are perceived differently across language communities.
This paper follows this trend, treating variation as meaningful information. We assume that the crowd?s
perception (based on the distribution of the players? guesses) is an indication of to what extent Twitter
users emphasize their gender and age identity in their tweets. For example, when a large proportion of
the players guess the same gender for a particular user, the user is assumed to employ linguistic markers
that the crowd associates with gender-specific speech (e.g. iconic hearts used by females).
Our contributions are as follows:
? We demonstrate the use of gamification to study sociolinguistic research problems (Section 3).
? We study the difficulty of predicting an author?s gender (Section 4) and age (Section 5) from text
alone by analyzing prediction performance by the crowd. We relate our results to sociolinguistic
theories and show that approaching gender and age as fixed biological variables is too simplistic.
? Based on our findings, we reflect on current approaches to predicting age and gender from text, and
draw attention to the limitations of these approaches (Section 6).
2 Related Work
Gender Within sociolinguistics, studies on gender and language have a long history (Eckert and
McConnell-Ginet, 2013). More recently, the NLP community has become increasingly interested in
this topic. Most of the work aims at predicting the gender of authors based on their text, thereby focusing
more on prediction performance than sociolinguistic insights.
A variety of datasets have been used, including Twitter (Rao et al., 2010; Bamman et al., 2014; Fink et
al., 2012; Bergsma and Van Durme, 2013; Burger et al., 2011), blogs (Mukherjee and Liu, 2010; Schler et
al., 2005), telephone conversations (Garera and Yarowsky, 2009), YouTube (Filippova, 2012) and chats
in social networks (Peersman et al., 2011). Females tend to use more pronouns, emoticons, emotion
words, and blog words (lol, omg, etc.), while males tend to use more numbers, technology words, and
links (Rao et al., 2010; Bamman et al., 2014; Nguyen et al., 2013). These differences have also been
exploited to improve sentiment classification (Volkova et al., 2013) and cyberbullying detection (Dadvar
et al., 2012).
To the best of our knowledge, the study by Bamman et al. (2014) is the only computational study that
approaches gender as a social variable. By clustering Twitter users based on their tweets, they show that
multiple gendered styles exist. Unlike their study, we use the crowd and focus on implications for gender
and age prediction.
1951
Figure 1: Screenshot of the game. Text is translated into English (originally in Dutch). Left shows the
interface when the user needs to make a guess. Right shows the feedback interface.
Age Eckert (1997) makes a distinction between chronological (number of years since birth), biological
(physical maturity), and social age (based on life events). Most of the studies on language and age
focus on chronological age. However, speakers with the same chronological age can have very different
positions in society, resulting in variation in language use. Computational studies on language use and
age usually focus on automatic (chronological) age prediction. This has typically been modeled as
a classification problem, although this approach often suffers from ad hoc and dataset dependent age
boundaries (Rosenthal and McKeown, 2011). In contrast, recent works also explored predicting age as a
continuous variable and predicting lifestages (Nguyen et al., 2013; Nguyen et al., 2011) .
Similar to studies on gender prediction, a variety of resources have been used for age prediction, in-
cluding Twitter (Rao et al., 2010; Nguyen et al., 2013), blogs (Rosenthal and McKeown, 2011; Goswami
et al., 2009), chats in social networks (Peersman et al., 2011) and telephone conversations (Garera and
Yarowsky, 2009). Younger people use more alphabetical lengthening, more capitalization of words,
shorter words and sentences, more self-references, more slang words, and more Internet acronyms
(Rosenthal and McKeown, 2011; Nguyen et al., 2013; Rao et al., 2010; Goswami et al., 2009; Pen-
nebaker and Stone, 2003; Barbieri, 2008).
3 Data
To study how people perceive the gender and age identity of Twitter users based on their tweets, we
created an online game. Players were asked to guess the gender and age of Twitter users from tweets.
The game was part of a website (TweetGenie, www.tweetgenie.nl) that also hosted an automatic system
that predicts the gender and age of Twitter users based on their tweets (Nguyen et al., 2014). To attract
players, a link to the game was displayed on the page with the results of the automatic prediction, and
visitors were challenged to test if they were better than the automatic system (TweetGenie).
3.1 Twitter Data
We sampled Dutch Twitter users in the fall of 2012. We employed external annotators to annotate the
biological sex and chronological age (in years) using all information available through tweets, the Twitter
profile and external social media profiles such as Facebook and Linkedin. In total over 3000 Twitter users
were annotated. For more details regarding the collection of the dataset we refer to Nguyen et al. (2013).
We divided the data into train and test sets. 200 Twitter users were randomly selected from the test
set to be included in the online game (statistics are shown in Table 1). Named entities were manually
anonymized to conceal the user?s identity. Names in tweets were replaced by ?similar? names (e.g. a
first name common in a certain region in the Netherlands was replaced with another common name in
that region). This was done without knowing the actual gender and age of the Twitter users. Links were
replaced with a general [LINK] token and user mentions with @USER.
1952
Gender and age F, <20 M, <20 F, [20-40) M, [20-40) F, ?40 M, ?40
Frequency 61 60 24 23 17 15
Table 1: Statistics Twitter users in our game
3.2 Online Game
Game Setup The interface of the game is shown in Figure 1. Players guessed the biological sex (male
or female) and age (years) of a Twitter user based on only the tweets. For each user, {20, 25, 30, 35,
40} tweets were randomly selected. For a particular Twitter user, the same tweets were displayed to all
players. Twitter users were randomly selected to be displayed to the players.
To include an entertainment element, players received feedback after each guess. They were shown
the correct age and gender, the age and gender guessed by the computer, and the average guessed age
and gender distribution by the other players. In addition, a score was shown of the player versus the
computer.
Collection In May 2013, the game was launched. Media attention resulted in a large number of visitors
(Nguyen et al., 2014). We use the data collected from May 13, 2013 to August 21, 2013, resulting in
a total of 46,903 manual guesses. Players tweeted positively about the game, such as ?@USER Do you
know what is really addictive? ?Are you better than Tweetgenie? ...? and ?@USER Their game is quite
fun!? (tweets translated to English).
We filter sessions that do not seem to contain genuine guesses: when the entered age is 80 years
or above, or 8 or below. These thresholds were based on manual inspection, and chosen because it is
unlikely that the shown tweets are from users of such ages. For each guess, we registered a session ID
and an IP address. A new session started after 2 hours of inactivity. To study player performance more
robustly, we excluded multiple sessions of the same player. After three or more guesses had been made
in a session, all next sessions from the same IP address were discarded.
Statistics Statistics of the data are shown in Table 2. Figure 2 shows the distribution of the number of
guesses per session. The longest sessions consisted of 18 guesses. Some of our analyses require multiple
guesses per player. In that case, we only include players having made at least 7 guesses.
1 2 3 4 5 6 7 8 9 10 >10
Number of guesses
Freq
uenc
y
01
000
3000
5000
Figure 2: Number of guesses per session
# guesses 41,989
# sessions 15,724
Avg. time (sec) per guess 46
Avg. # guesses / session 2.67
Table 2: Statistics online game (after
cleaning)
We calculate the time taken for a guess by taking the time difference between two guesses (therefore,
no time for the first guess in each session could be measured). For each Twitter user, we calculate the
average time that was taken to guess the gender and age of the user. (Figure 3a). There is a significant
correlation (Pearson?s r = 0.291, p < 0.001) between the average time the players took to evaluate the
tweets of a Twitter user and the number of displayed tweets.
There is also a significant correlation between the average time taken for a user and the entropy over
gender guesses (Pearson?s r = 0.410, p < 0.001), and the average time taken for a user and the standard
deviation of the age guesses (Pearson?s r = 0.408, p < 0.001). Thus, on average, players spent more time
on Twitter users for whom it was more difficult to estimate gender and age.
1953
Avg time taken for Twitter user (sec)
Freq
uen
cy
0
20
40
60
80
30 35 40 45 50 55 60 65
(a) Average time taken for Twitter users
Turn
Av
era
ge 
tim
e ta
ken
30
32
34
36
2 3 4 5 6 7
(b) Average time taken per turn
Figure 3: Time taken in game
We observe that as the game progresses, players tend to take less time to make a guess. This is shown
in Figure 3b, which shows the average time taken for a turn (restricted to players with at least 7 guesses).
There was no significant correlation between time spent on a guess and the performance of players and
we did not find trends of performance increase or decrease as players progressed in the game.
3.3 Automatic Prediction
Besides studying human performance, we also compare the predictions of humans with those of an
automatic system. We split the data into train and test sets using the same splits as used by Nguyen et al.
(2013). We train a logistic regression model to predict gender (male or female), and a linear regression
model to predict the age (in years) of a person.
More specifically, given an input vector x ? R
m
, x
1
, . . . , x
m
represent features. In the case of gender
classification (e.g. y ? {?1, 1}), the model estimates a conditional distribution P (y|x, ?) = 1/(1 +
exp(?y(?
0
+ x
>
?))), where ?
0
and ? are the parameters to estimate. Age is treated as a regression
problem, and we find a prediction y? ? R for the exact age of a person y ? R using a linear regression
model: y? = ?
0
+ x
>
?. We use Ridge (also called L
2
) regularization to prevent overfitting.
We make use of the liblinear (Fan et al., 2008) and scikit-learn (Pedregosa et al., 2011) libraries.
We only use unigram features, since they have proven to be very effective for gender (Bamman et al.,
2014; Peersman et al., 2011) and age (Nguyen et al., 2013) prediction. Parameters were tuned using
cross-validation on the training set.
4 Gender
Most of the computational work on language and gender focuses on gender classification, treating gender
as fixed and classifying speakers into females and males. However, this assumes that gender is fixed and
is something people have, instead of something people do (Butler, 1990).
In this section, we first analyze the task difficulty by studying crowd performance on inferring gender
from tweets. We observe a relatively large group of Twitter users who employ language that the crowd
associates with the opposite biological sex. This, then, raises questions about the upper bound that a
prediction system based on only text can achieve.
Next, we place Twitter users on a gender continuum based on the guesses of the players and show that
treating gender as a binary variable is too simplistic. While historically gender has been treated as binary,
researchers in fields such as sociology (Lorber, 1996) and sociolinguistics (Holmes and Meyerhoff, 2003;
Bergvall et al., 1996) find this view too limited. Instead, we assume the simplest extension beyond a
binary variable: a one-dimensional gender continuum (or scale) (Bergvall et al., 1996). For example,
Bergvall (1999) talks about a ?continuum of humans? gendered practices?. While these previous studies
were based on qualitative analyses, we take a quantitative approach using the crowd.
1954
4.1 Task Difficulty
Majority vote We study crowd performance using a system based on the majority of the players?
guesses. Majority voting has proven to be a strong baseline to aggregate votes (e.g. in crowdsourcing
systems (Snow et al., 2008; Le et al., 2010)). On average, we have 210 guesses per Twitter user, providing
substantial evidence per Twitter user. A system based on majority votes achieves an accuracy of 84%
(Table 3a shows a confusion matrix). Table 3b shows a confusion matrix of the majority predictions
versus the automatic system. We find that the biological sex was predicted incorrectly by both the
majority vote system and the automatic system for 21 out of the 200 Twitter users (10.5%, not in Table).
Automatic classification systems on English tweets achieve similar performances as our majority vote
system (e.g. Bergsma and Van Durme (2013) report an accuracy of 87%, Bamman et al. (2014) 88%).
More significantly, the results suggest that 10.5% (automatic + majority) to 16% (majority) of the Dutch
Twitter users do not employ language that the crowd associates with their biological sex. As said, this
raises the question of whether we can expect much higher performances by computational systems based
on only language use.
Biological sex
Male Female
Crowd
Male 82 16
Female 16 86
(a) Crowd (majority)
Crowd
Male Female
Automatic
Male 68 22
Female 30 80
(b) Automatic vs crowd
Table 3: Confusion matrices crowd prediction
Individual players versus an automatic system When considering players with 7 or more guesses,
the average accuracy for a player is 0.71. Our automatic system achieves an accuracy of 0.69. The small
number of tweets per Twitter user in our data (20-40) makes it more difficult to automatically predict
gender.
Entropy We characterize the difficulty of inferring a user?s gender by calculating the entropy for each
Twitter user based on the gender guesses (Figure 4a). We find that the difficulty varies widely across
users, and that there are no distinct groups of ?easy? and ?difficult? users. However, we do observe an
interaction effect between the entropy of the gender guesses and the ages of the Twitter users. At an
aggregate level, we find no significant trend. Analyzing females and males separately, we observe a
significant trend with females (Pearson?s r = 0.270, p < 0.01), suggesting that older female Twitter users
tend to emphasize other aspects than their gender in tweets (as perceived by the crowd).
Persons
Ent
rop
y
0.2
0.6
1.0
0 50 100 150 200
(a) Entropy over gender guesses
0
5
10
15
20
25
0.0 0.5 1.0
Proportion of people that guessed male
Fre
que
ncy Biological 
sex
Male
Female
(b) A histogram of all Twitter users and the proportion of
players who guessed the users were male. For example,
there are 25 female users for which 10 - 20% of the players
guessed they were male.
Figure 4: Gender prediction
1955
4.2 Binarizing Gender, a Good Approach?
Using data collected through the online game we quantitatively put speakers on a gender continuum
based on how their tweets are perceived by the crowd. For each Twitter user, we calculate the proportion
of players who guessed the users were male and female. A plot is displayed in Figure 4b. We can make
the following observations:
First, the guesses by the players are based on their expectations about what kind of behaviour and
language is used by males and females. The plot shows that for some users, almost all players guessed
the same gender, indicating that these expectations are quite strong and that there are stylistic markers
and topics that the crowd strongly associates with males or females.
Second, if treating gender as a binary variable is reasonable, we would expect to see two distinct
groups. However, we observe quite an overlap between the biological males and females. There are 1)
users who conform to what is expected based on their biological sex, 2) users who deviate from what is
expected, 3) users whose tweets do not emphasize a gender identity or whose tweets have large variation
using language associated with both genders. We investigated whether this is related to their use of
Twitter (professional, personal, or both), but the number of Twitter users in our dataset who used Twitter
professionally was small and not sufficient to draw conclusions.
We now illustrate our findings using examples. The first example is a 15-year old biological female
for who the crowd guessed most strongly that she is female (96% of n=220). Three tweets from her are
shown below. She uses language typically associated with females, talking about spending time with
her girlfriends and the use of stylistic markers such as hearts and alphabetical lengthening. Thus, she
conforms strongly to what the crowd expects from her biological sex.
Tweet 4: Gezellig bij Emily en Charlotte.
Translation: Having fun with Emily and Charlotte.
Tweet 5: Hiiiiii schatjesss!
Translation: Hiiiiii cutiesss!
Tweet 6: ? @USER
Below are two tweets from a 40 year old biological female who does not employ linguistic markers
strongly associated with males or females. Therefore, only 46% of the crowd (n=200) was able to guess
that she is female.
Tweet 7: Ik viel op mijn bek. En het kabinet ook. Geinig toch? #Catshuis
Translation: I went flat on my face. And the cabinet as well. Funny right? #Catshuis
Tweet 8: Jeemig. Ik kan het bijna niet volgen allemaal.
Translation: Jeez. I almost can?t follow it all.
Twitter users vary in how much they emphasize their gender in their tweets. As a result, the difficulty
of inferring gender from tweets varies across persons, and treating gender as a binary variable ignores
much of the interesting variation within and between persons.
Automatic system We now analyze whether an automatic system is capable of capturing the position
of Twitter users on the gender continuum (as perceived by the crowd). We calculate the correlation
between the proportion of male guesses (i.e. the position on the gender continuum) and the scores of
the logistic regression classifier: ?
0
+ x
>
?. While the training data was binary (users were labeled as
male or female), a reasonable Spearman correlation of ? = 0.584 (p < 0.001) was obtained between the
classifier score and the score based on the crowd?s perception. We did not observe a significant relation
between the score of the classifier (corresponding to the confidence of the gender prediction) and age.
1956
5 Age
We start with an analysis of task difficulty, by studying crowd performance on inferring age from tweets.
Next, we show that it is particularly hard to accurately infer the chronological age of older Twitter users
from tweets.
5.1 Task Difficulty
The crowd?s average guesses As with a system based on majority vote for gender prediction, we test
the performance of a system that predicts the ages of Twitter users based on the average of all guesses.
We find that such a system achieves a Mean Absolute Error (MAE) of 4.844 years and a Pearson?s
correlation of 0.866. Although the correlation is high, the absolute errors are quite large. We find that the
crowd has difficulty predicting the ages of older Twitter users. There is a positive correlation (Pearson?s
? = 0.789) between the absolute errors and the actual age of Twitter users. There is a negative correlation
between the errors (predicted - actual age) and the actual age of Twitter users (Pearson?s ? = -0.872).
We calculate the standard deviation over all the age guesses for a user (Figure 5a) to measure the
difficulty of inferring a user?s age. There is a positive correlation between age and standard deviation of
the guesses (? = 0.691), which indicates that players have more difficulty in guessing the ages of older
Twitter users.
Individual players versus an Automatic System To estimate the performance of individual players,
we restrict our attention to players with at least 7 guesses. We find that individual players are, on average,
5.754 years off. A linear regression system achieves a MAE of 6.149 years and a Pearson correlation of
0.812. The small number of tweets in our data (20-40) increases the difficulty of the task for automatic
systems.
Age
Sta
nda
rd d
evia
tion
2
4
6
8
10
10 20 30 40 50 60
(a) Standard deviation and actual age
Actual age
Pre
dict
ed a
ge
10
20
30
40
50
60
10 20 30 40 50 60
Correct line
Human prediction
(b) Average age prediction by humans.
Figure 5: Age prediction
5.2 Inferring the Age of Older Twitter Users
Figure 5b shows the average player predictions with the actual age of the Twitter users. The red line is
the ?perfect? line, i.e. the line when the predictions would match the exact age. Black represents a fitted
LOESS curve (Cleveland et al., 1992) based on the human predictions. We find that the players tend
to overpredict the age of younger Twitter users, but even more strikingly, on average they consistently
underpredict the age of older Twitter users. The prediction errors already start at the end of the 20s, and
the gap between actual and predicted age increases with age.
This could be explained by sociolinguistic studies that have found that people between 30 and 55 years
use standard forms the most, because they experience the maximum societal pressure in the workplace to
conform (Holmes, 2013). On Twitter, this has been observed as well: Nguyen et al. (2013) found fewer
linguistic differences between older age groups than between younger age groups. This makes it difficult
for the crowd to accurately estimate the ages of older Twitter users. Younger people and retired people
use more non-standard forms (Holmes, 2013). Unfortunately, our dataset does not contain enough retired
users to analyze whether this trend is also present on Twitter.
1957
6 Discussion
We now discuss the implications of our findings for research on automatically predicting the gender and
age of authors from their texts.
Age and gender as social variables Most computational research has treated gender and age as fixed,
biological variables. The dominant approach is to use supervised machine learning methods to generalize
across a large number of examples (e.g. texts written by females and males). While the learned models
so far are effective at predicting age and gender of most people, they learn stereotypical behaviour and
therefore provide a simplistic view.
First, by using the crowd we have shown that Twitter users emphasize their gender and age in varying
degrees and in different ways, so that for example, treating gender as a binary variable is too simplistic
(Butler, 1990; Eckert and McConnell-Ginet, 2013). Many users do not employ the stereotypical language
associated with their biological sex, making models that take a static view of gender ineffective for such
users. More detailed error analyses of the prediction systems will increase understanding of the reasons
for incorrect predictions, and shed light on the relation between language use and social variables.
Second, models that assume static variables will not be able to model the interesting variation (Eisen-
stein, 2013). Models that build on recent developments in sociolinguistics will be more meaningful and
will also have the potential to contribute to new sociolinguistic insights. For example, modeling what
influences speakers to show more or less of their identity through language, or jointly modeling varia-
tion between and within speakers, are in our opinion interesting research directions. The ever increasing
amounts of social media data offer opportunities to explore these research directions.
Sampling We have shown that the difficulty of tasks such as gender and age prediction varies across
persons. Therefore, creating datasets for such tasks requires maximum attention. For example, when
a dataset is biased towards people who show a strong gender identity (e.g. by sampling followers of
accounts highly associated with males or females, such as sororities (Rao et al., 2010)), the results
obtained on such a set may not be representative of a more random set (as observed when classifying
political affiliation (Cohen and Ruths, 2013)).
Task difficulty Our study also raises the question of what level of performance can be obtained for
tasks such as predicting gender and age from only language use. Since we often form an impression
based on someone?s writing, crowd performance is a good indicator of the task difficulty. While the
crowd performance does not need to be the upper bound, it does indicate that it is difficult to predict
gender and age of a large number of Twitter users.
When taking the majority label, only 84% of the users were correctly classified according to their
biological sex. This suggests that about 16% of the Dutch Twitter users do not use language that the
crowd associates with their biological sex.
We also found that it is hard to accurately estimate the ages of older Twitter users, and we related
this to sociolinguistics studies who found less linguistic differences in older age groups due to societal
pressure in the workplace.
Limitations A limitation of our work is that we focused on language variation between persons, and not
on variation within persons. However, speakers vary their language depending on the context and their
conversation partners (e.g. accommodation effects were found in social media (Danescu-Niculescu-Mizil
et al., 2011)). For example, we assigned Twitter users an overall ?score? by placing them on a gender
continuum, ignoring the variation we find within users.
Crowdsourcing as a tool to understand NLP tasks Most research on crowdsourcing within the NLP
community has focused on how the crowd can be used to obtain fast and large amounts of annotations.
This study is an example of how the crowd can be used to obtain a deeper understanding of an NLP task.
We expect that other tasks where disagreement between annotators is meaningful (i.e. it is not only due
to noise), could potentially benefit from crowdsourcing experiments as well.
1958
7 Conclusion
In this paper, we demonstrated the successful use of the crowd to study the relation between language
use and social variables. In particular, we took a closer look at inferring gender and age from language
using data collected through an online game. We showed that treating gender and age as fixed variables
ignores the variety of ways people construct their identity through language.
Approaching age and gender as social variables will allow for richer analyses and more robust systems.
It has implications ranging from how datasets are created to how results are interpreted. We expect that
our findings also apply to other social variables, such as ethnicity and status. Instead of only focusing on
performance improvement, we encourage NLP researchers to also focus on what we can learn about the
relation between language use and social variables using computational methods.
Acknowledgements
This research was supported by the Royal Netherlands Academy of Arts and Sciences (KNAW)
and the Netherlands Organization for Scientific Research (NWO), grants IB/MP/2955 (TINPOT) and
640.005.002 (FACT). The third author is supported through the Digital Humanities research grant by
Tilburg University and a NIAS research fellowship. The authors would like to thank the players of the
TweetGenie game.
References
Lora Aroyo and Chris Welty. 2013. Crowd truth: Harnessing disagreement in crowdsourcing a relation extraction
gold standard. In Proceedings of WebSci?13.
David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2014. Gender identity and lexical variation in social
media. Journal of Sociolinguistics, 18(2):135?160.
Federica Barbieri. 2008. Patterns of age-based linguistic variation in American English. Journal of Sociolinguis-
tics, 12(1):58?88.
Shane Bergsma and Benjamin Van Durme. 2013. Using conceptual class attributes to characterize social media
users. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages
710?720.
Victoria L. Bergvall, Janet M. Bing, and Alice F. Freed. 1996. Rethinking Language and Gender Research: Theory
and Practice. Routledge.
Victoria L. Bergvall. 1999. Toward a comprehensive theory of language and gender. Language in society,
28(02):273?293.
Mary Bucholtz and Kira Hall. 2005. Identity and interaction: A sociocultural linguistic approach. Discourse
studies, 7(4-5):585?614.
John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on Twitter. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301?1309.
Judith Butler. 1990. Gender Trouble: Feminism and the Subversion of Identity. Routledge.
William S. Cleveland, Eric Grosse, and William M. Shyu. 1992. Local regression models. Statistical models in S,
pages 309?376.
Raviv Cohen and Derek Ruths. 2013. Classifying political orientation on Twitter: It?s not easy! In Proceedings of
the Seventh International AAAI Conference on Weblogs and Social Media, pages 91?99.
Maral Dadvar, Franciska de Jong, Roeland Ordelman, and Dolf Trieschnigg. 2012. Improved cyberbullying de-
tection using gender information. In Proceedings of the Twelfth Dutch-Belgian Information Retrieval Workshop
(DIR 2012), pages 23?25.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words!: linguistic style
accommodation in social media. In Proceedings of the 20th international conference on World Wide Web, pages
745?754.
1959
Sebastian Deterding, Dan Dixon, Rilla Khaled, and Lennart Nacke. 2011. From game design elements to game-
fulness: Defining ?gamification?. In Proceedings of the 15th International Academic MindTrek Conference:
Envisioning Future Media Environments, pages 9?15.
Penelope Eckert and Sally McConnell-Ginet. 2013. Language and gender. Cambridge University Press.
Penelope Eckert. 1997. Age as a sociolinguistic variable. The handbook of sociolinguistics, pages 151?167.
Penelope Eckert. 2008. Variation and the indexical field. Journal of Sociolinguistics, 12(4):453?476.
Penelope Eckert. 2012. Three waves of variation study: the emergence of meaning in the study of sociolinguistic
variation. Annual Review of Anthropology, 41:87?100.
Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of the Annual Conference
of the North American Chapter of the Association for Computational Linguistics, pages 359?369.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning Research, 9:1871?1874.
Katja Filippova. 2012. User demographics and language in an implicit social network. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language
Learning, pages 1478?1488.
Clayton Fink, Jonathon Kopecky, and Maksym Morawski. 2012. Inferring gender from the content of tweets:
A region specific example. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social
Media.
Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP, pages 710?718.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi. 2009. Stylometric analysis of bloggers? age and gender.
In Proceedings of the Third International ICWSM Conference, pages 214?217.
Janet Holmes and Miriam Meyerhoff. 2003. The handbook of language and gender. Wiley-Blackwell.
Janet Holmes. 2013. An introduction to sociolinguistics. Routledge.
John Le, Andy Edmonds, Vaughn Hester, and Lukas Biewald. 2010. Ensuring quality in crowdsourced search
relevance evaluation: The effects of training question distribution. In Proceedings of the SIGIR 2010 Workshop
on Crowdsourcing for Search Evaluation (CSE 2010), pages 21?26.
Judith Lorber. 1996. Beyond the binaries: Depolarizing the categories of sex, sexuality, and gender*. Sociological
Inquiry, 66(2):143?160.
Maxim Makatchev and Reid Simmons. 2011. Perception of personality and naturalness through dialogues by
native speakers of American English and Arabic. In Proceedings of the SIGDIAL 2011: the 12th Annual
Meeting of the Special Interest Group on Discourse and Dialogue, pages 286?293.
Alice E. Marwick and danah boyd. 2011. I tweet honestly, I tweet passionately: Twitter users, context collapse,
and the imagined audience. New Media & Society, 13(1):114?133.
Arjun Mukherjee and Bing Liu. 2010. Improving gender classification of blog authors. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing, pages 207?217.
Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler
Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic
data. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 122?130.
Dong Nguyen, Noah A Smith, and Carolyn P. Ros?e. 2011. Author age prediction from text using linear regression.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences,
and Humanities, pages 115?123.
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and Theo Meder. 2013. ?How old do you think I am??: A study
of language and age in Twitter. In Proceedings of the Seventh International AAAI Conference on Weblogs and
Social Media, pages 439?448.
1960
Dong Nguyen, Dolf Trieschnigg, and Theo Meder. 2014. Tweetgenie: Development, evaluation, and lessons
learned. In Proceedings of COLING 2014.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, Matthieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
Claudia Peersman, Walter Daelemans, and Leona Van Vaerenbergh. 2011. Predicting age and gender in online so-
cial networks. In Proceedings of the 3rd international workshop on Search and mining user-generated contents,
pages 37?44.
James W. Pennebaker and Lori D. Stone. 2003. Words of wisdom: Language use over the life span. Journal of
personality and social psychology, 85(2):291?301.
Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes
in Twitter. In Proceedings of the 2nd international workshop on Search and mining user-generated contents,
pages 37?44.
Sara Rosenthal and Kathleen McKeown. 2011. Age prediction in blogs: a study of style, content, and online be-
havior in pre- and post-social media generations. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 763?772.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker. 2005. Effects of age and gender
on blogging. In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs,
pages 199?205.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast?but is it good?:
Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 254?263.
Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring demographic language variations to
improve multilingual sentiment analysis in social media. In Proceedings of the 2013 Conference on Empirical
Methods on Natural Language Processing, pages 1815?1827.
Suzanne E. Wagner. 2012. Age grading in sociolinguistic theory. Language and Linguistics Compass, 6(6):371?
382.
1961
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 203?207,
Dublin, Ireland, August 23-24, 2014.
COMMIT-P1WP3: A Co-occurrence Based Approach
to Aspect-Level Sentiment Analysis
Kim Schouten
1,2
schouten@ese.eur.nl
Flavius Frasincar
1
frasincar@ese.eur.nl
1
Econometric Institute, Erasmus University Rotterdam, The Netherlands
2
Erasmus Studio, Erasmus University Rotterdam, The Netherlands
Franciska de Jong
2
fdejong@ese.eur.nl
Abstract
In this paper, the crucial ingredients for
our submission to SemEval-2014 Task 4
?Aspect Level Sentiment Analysis? are
discussed. We present a simple aspect de-
tection algorithm, a co-occurrence based
method for category detection and a dic-
tionary based sentiment classification al-
gorithm. The dictionary for the latter is
based on co-occurrences as well. The fail-
ure analysis and related work section focus
mainly on the category detection method
as it is most distinctive for our work.
1 Introduction
In recent years, sentiment analysis has taken flight
and is now actively used, on the Web and be-
yond (Liu, 2012). To provide users of sentiment
tools with more detailed and useful information, a
number of innovations have been introduced, and
among others a switch from document-level sen-
timent analysis towards fine-grained, aspect-level
sentiment analysis can be seen (Feldman, 2013).
In line with the many challenges associated with
this, SemEval-2014 Task 4 ?Aspect Level Senti-
ment Analysis? (Pontiki et al., 2014) is split into
four sub-tasks: Aspect Detection, Aspect Senti-
ment Classification, Category Detection, and Cat-
egory Sentiment Classification.
The main focus of this paper is on the category
detection algorithm we developed, but a method
for aspect detection and a sentiment classifica-
tion algorithm (both for aspects and categories) are
also included. The aspect detection algorithm will
be presented first, followed by the category de-
tection algorithm and the sentiment classification
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
method. Next, the benchmark results for all algo-
rithms are presented, plus a discussion and failure
analysis of the category detection method. Finally,
conclusions are drawn and some suggestions for
future work are presented.
2 Related Work
Because the focus of this paper lies on the cat-
egory detection method, only for that method a
short snippet of related work is given. That algo-
rithm, being an adapted version of Schouten and
Frasincar (2014), is inspired by the work of Zhang
and Zhu (2013) and Hai et al (2011). In these
works, a co-occurrence matrix is created between
words in the sentence and aspects in order to find
implicit aspects (i.e., aspects that are not literally
mentioned, as opposed to the explicit aspects used
in this task).
While implicit aspects are similar to aspect cat-
egories to some extent, these methods do not work
when a fixed, limited set of possible aspect cat-
egories is used that is, most importantly, not a
subset of the set of aspects. The above meth-
ods could never, for instance, identify the ?anec-
dotes/miscellaneous? category, as this word never
appears as an aspect in the data set. This is the
main reason why we have chosen to count the co-
occurrences between words and the annotated as-
pect categories.
3 Aspect Detection Method
In the work reported here, the aspect detection
method plays the role of a baseline method rather
than a full-fledged algorithm. In its most basic
form, it annotates all noun phrases as aspects.
However, by using the training set to count how
often each word appears within an aspect, a sim-
ple probability can be computed representing the
chance that this word is an aspect word or not.
This probability is used to filter the set of noun
phrases, such that only noun phrases remain that
203
have at least one word for which the aspect proba-
bility ? 0.05 and for those noun phrases, all lead-
ing words in the noun phrase with a probability
below 0.05 are removed. This will remove words
like determiners from the initial noun phrase, as
those are not included in the aspect term. Because
this filtering is strict, the result is a typical high
precision, low recall algorithm for aspect detec-
tion.
4 Category Detection Method
To find the aspect categories, the co-occurrence
based algorithm from Schouten and Frasin-
car (2014) is used and improved upon. The cen-
tral construct in this algorithm is a co-occurrence
matrix that captures the frequency of the co-
occurrences between words (i.e., the lemmas of
the words) in the sentence and the annotated as-
pect category. This gives a mapping from words to
aspect categories. When processing an unlabelled
sentence, a score is computed for each aspect cat-
egory as shown in Eq. 1.
score
a
i
=
1
v
v
?
j=1
c
i,j
o
j
, (1)
where v is the number of words in the sentence,
a
i
is the ith aspect category in the list of possible
aspect categories for which the score is computed,
j represents the jth word in the sentence, c
i,j
is the
co-occurrence frequency of aspect category i and
lemma j in the data set, and o
j
is the frequency of
lemma j in the data set.
Whereas in Schouten and Frasincar (2014), the
highest scoring category was chosen on the con-
dition that its score exceeded a threshold, our
method is now able to choose more than one as-
pect category per sentence. This is done by train-
ing a separate threshold for each of the five aspect
categories using all training data. When the score
for some aspect category is higher than its associ-
ated threshold (i.e., score
a
i
> threshold
a
i
), the
sentence is annotated as having that aspect cate-
gory.
Since we assume the five threshold values to be
independent of each other, a simple linear search
is performed separately for all five of them to find
the optimal threshold value by optimizing F
1
(cf.
Sec. 6). As a default option, the fifth category
(?anecdotes/miscellaneous?) is associated to any
sentence for which none of the five categories ex-
ceeded their threshold. The trained threshold val-
ues for the five categories are:
ambience price food service misc
0.042 0.024 0.211 0.071 0.143
The pseudocode for the creation of the co-
occurrence matrix can be found in Algorithm 1,
and Algorithm 2 describes the process of annotat-
ing a sentence with aspect categories.
Algorithm 1 Aspect category detection training.
Initialize set of word lemmas with frequencies
O
Initialize set of aspect categories A
Initialize co-occurrence matrix C
for sentence s ? training data do
for word w ? s do
O(w) = O(w) + 1
end for
for aspect category a ? s do
add a to A
for word w ? s do
C(w, a) = C(w, a) + 1
end for
end for
end for
for aspect category a in A do
threshold
a
=0
bestF
1
= 0
for t = 0 to 1 step 0.001 do
Execute Algorithm 2 on training data
Compute F
1
if F
1
> bestF
1
then threshold
a
= t
end if
end for
end for
5 Sentiment Classification Method
For sentiment classification, a method is devel-
oped that first creates a sentiment lexicon based
on the aspect sentiment annotation. That lexicon
is then consequently used to determine the senti-
ment of both aspects and categories that have no
sentiment annotation. The intuition behind this
method is that a lexicon should cover domain-
specific words and expressions in order to be ef-
fective. To avoid creating such a sentiment lexi-
con manually, the aspect sentiment annotations are
leveraged to create one automatically. The idea is
that words that often appear close to positive or
204
Algorithm 2 Aspect category detection execution.
for sentence s ? test data do
for aspect category a ? A do
score = 0
for word w ? s do
if O(w) > 0 then
score = score+C(w, a)/O(w)
end if
end for
score = score/ length(s)
if score > threshold
a
then
Assign aspect category a to s
end if
end for
if s has no assigned aspect categories then
Assign ?anecdotes/miscellaneous? to s
end if
end for
negative aspects are likely to have the same polar-
ity. Since sentiment is also carried by expressions,
rather than single words only, the constructed sen-
timent lexicon has entries for encountered uni-
grams, bigrams, and trigrams. In each sentence,
the distance between each n-gram and each aspect
is computed and the sentiment of the aspect, dis-
counted by the computed distance, is added to the
sentiment value of the n-gram, as shown in Eq. 2.
sentiment
g
=
1
freq
g
?
?
s?S
g
p ? t
order(g)
?
?
a?A
s
polarity
a
(distance
g,a
)
m
,
(2)
where g is the n-gram (i.e., word unigram, bigram,
or trigram), freq
g
is the frequency of n-gram g
in the data set, s is a sentence in S
g
, which is
the set of sentences that contain n-gram g, p is a
parameter to correct for the overall positivity of
the data set, t is a parameter that corrects for the
relative influence of the type of n-gram (i.e., dif-
ferent values are used for t
1
, t
2
, and t
3
), a is an
aspect in A
s
, which is the set of aspects in sen-
tence s, polarity
a
is 1 when aspect a is positive
and ?1 when a is negative, and m is a parame-
ter that determines how strong the discounting by
the distance should be. The distance computed
is the shortest word distance between the aspect
and the n-gram (i.e., both an n-gram and an as-
pect can consist of multiple words, in which case
the closest two are used to compute the distance).
Note that essentially, dictionary creation is based
on how often an n-gram co-occurs with positive
or negative aspects. In our submitted run on the
restaurant data, we set t
order(g)
to 1, 5, and 4 for
unigrams, bigrams, and trigrams, respectively, and
p = 2 and for the laptop data we set t
order(g)
to 1,
0, and 3 for the n-grams and p = 1. In both cases,
m was kept at 1. These values were determined by
manual experimentation.
To compute the sentiment of an aspect, the sen-
timent value of each n-gram is divided by the dis-
tance between that n-gram and the aspect, com-
puted in a similar fashion as in the above formula)
and summed up, as shown in Eq. 3.
sentiment
a,s
a
=
?
g?s
a
sentiment
g
(min distance
g,a
)
m
, (3)
where, in addition to the definitions in the previ-
ous equation, g is an n-gram in s
a
, which is the
sentence in which aspect a occurs. Note that for
each occurrence of a term, its sentiment value is
added to the total score. If the result is above zero,
the class will be ?positive?, and if the result is be-
low zero, the class will be ?negative?. In the rare
event of the sentiment score being exactly zero,
the ?neutral? class is assigned.
For category sentiment classification, the for-
mula of Eq. 3 remains the same, except that the
distance term min distance
m
g,a
is set to 1, since
aspect categories pertain to the whole sentence in-
stead of having specific offsets.
6 Evaluation
All three algorithms presented above were evalu-
ated through a submission in the SemEval-2014
Task 4 ?Aspect Level Sentiment Analysis?. Two
data sets have been used, one consisting of sen-
tences from restaurant reviews, the other consist-
ing of sentences from laptop reviews. Both sets
have been annotated with aspects and aspect senti-
ment, but only the restaurant set is also annotated
with aspect categories and their associated senti-
ment class. Both data sets are split into a training
set of roughly 3000 sentences and a test set of 800
sentences.
All sentences in the data set have been pre-
processed by a tokenizer, a Part-of-Speech tagger,
and a lemmatizer. These tasks were performed by
205
Table 1: Official results for both algorithms.
aspect detection (subtask 1)
precision recall F
1
laptop 0.836 0.148 0.252
restaurant 0.909 0.388 0.544
category detection (subtask 3)
precision recall F
1
restaurant 0.633 0.558 0.593
aspect sentiment classification (subtask 2)
laptop accuracy 0.570
restaurant accuracy 0.660
category sentiment classification (subtask 4)
restaurant accuracy 0.677
the Stanford CoreNLP framework
1
. Furthermore,
the OpenNLP
2
chunker was used to provide basic
phrase chunking in order to retrieve noun phrases
for instance.
The official scores, as computed by the task or-
ganizers are shown in Table 1. Note that the senti-
ment classification algorithm is used for subtasks 2
and 4, so two scores are reported, and that subtasks
3 and 4 can only be performed with the restaurant
data set.
As the performance of the category detection
method was lower than anticipated, a failure anal-
ysis has been performed. This led to the observa-
tion that overfitting is one of major factors in ex-
plaining the lower performance . This is shown in
Figure 1, in which one can easily notice the great
difference in in-sample performance, and the per-
formance on unseen data. Notice that by using 10-
fold cross-validation, better results are achieved
than on the official test set. This indicates that
there are factors other than overfitting that influ-
ence the performance.
Interestingly, especially recall is influenced by
the overfitting problem: precision is almost the
same for the 10-fold cross-validation and even
with the in-sample performance it increases only
a little bit. To gain more insight into the difference
in recall, a graph showing the relative contribution
to false negatives of the five categories is shown in
Figure 2. For completeness, the same graph but for
false positives is also shown, together with the fre-
quency distribution of the categories in both train-
ing and test set.
Immediately visible is the effect of defaulting to
1
http://nlp.stanford.edu/software/corenlp.shtml
2
https://opennlp.apache.org/
0.70.750.80.850.9
precisio
n
recall
F1-mea
sure
0.50.550.60.650.7
official
 results
10-fold
 
crossva
lidation
 on 
trainin
g data
in-sam
ple res
ult: 
test on
 trainin
g 
data
trainin
g data
data
Figure 1: Performance measure of category detec-
tion on different parts of data.
the ?anecdotes/miscellaneous? when no category is
assigned to that sentence: many false positives are
generated by this rule, but there are almost no false
negatives for this category. Note that without this
default, F
1
-measure would drop by roughly 3 per-
centage points.
Also notable is the difference between the in-
sample bar and the official results bar: two cat-
egories, namely ?anecdotes/miscellaneous? and
?food? show large differences in contribution to
false positives and false negatives. The algo-
rithm finds fewer ?food? categories in the test
set, than in the training set, while for ?anec-
dotes/miscellaneous?, the reverse is the case. This
can at least be partly explained by the change in
data statistics: in the training set, 33% of the an-
notated categories are ?food? and 30% are ?anec-
dotes/miscellaneous?, whereas in the test set, these
numbers are 40% and 22%, respectively. With
much more sentences having the ?food? category,
false positives will be lower but false negatives
will be higher. For ?anecdotes/miscellaneous?, the
reverse is true: with less sentences in the test set
having this category, false positives will by higher,
but false negatives will be lower, a change rein-
forced by ?anecdotes/miscellaneous? being the de-
fault.
Two factors remain that might have negatively
impacted the performance of the algorithm. The
first is that in the restaurant set, many words ap-
pear only once (e.g., dishes, ingredients), and
when words do not appear in the training set, no
co-occurrence with any category can be recorded.
This primarily affects recall. The second is that
the category thresholds, while working well on the
training set, do not seem to generalize well to the
206
0%20%40%60%80%100
%
train
ing
test
Category Distribution
food
pric
e
serv
ice
amb
ienc
e
ane
cdo
tes/
mis
c
0%20%40%60%80%100
%
offic
ial
10-f
old
in-s
amp
le
Contribution to False 
Negatives
0%20%40%60%80%100
%
offic
ial
10-f
old
in-s
amp
le
Contribution to False Positives
train
ing
test
food
pric
e
serv
ice
amb
ienc
e
ane
cdo
tes/
mis
c
offic
ial
10-f
old
in-s
amp
le
offic
ial
10-f
old
in-s
amp
le
Figure 2: The frequency distribution of each category and its relative contribution to the total number of
false negatives (left graph) and false positives (right graph). The middle graph shows the change in the
distribution of categories in the training and test set.
test set. Testing the algorithm with one threshold
for all five categories, while showing a sharply de-
creased in-sample performance, yields an out-of-
sample F
1
-measure that is only slightly lower than
F
1
-measure with different thresholds.
7 Conclusion
In this paper the main ingredients for our submis-
sion to SemEval-2014 Task 4 ?Aspect Level Sen-
timent Analysis? are discussed: a simple aspect
detection method, a co-occurrence based method
for category detection, and a dictionary based sen-
timent classification algorithm. Since the category
detection algorithm did not perform as expected, a
failure analysis has been performed, while for the
others this was less necessary as they performed
roughly as expected.
The failure analysis provides several starting
points for future research. First, it would be in-
teresting to determine the exact nature of the de-
pendency between category performance and cat-
egory frequency, as discussed above, and to re-
move this dependency, since it is not guaranteed in
real-life scenarios that the frequency distribution
of the training set is the same as the set of instances
an algorithm will encounter when in use. Fur-
thermore, training five separate category thresh-
old, while good for performance in general, also
aggravates the problem of overfitting. Hence, im-
proving the generalization of the algorithm, and
the thresholds in particular, is important. Last,
a method to deal with very low frequency words
could prove useful as well.
Acknowledgment
The authors are partially supported by the Dutch
national program COMMIT.
References
Ronen Feldman. 2013. Techniques and Applications
for Sentiment Analysis. Communications of the
ACM, 56(4):82?89.
Zhen Hai, Kuiyu Chang, and J. Kim. 2011. Implicit
Feature Identification via Co-occurrence Associa-
tion Rule Mining. In Proceedings of the 12th In-
ternational Conference on Computational Linguis-
tics and Intelligent Text processing (CICLing 2011),
volume 6608, pages 393?404. Springer.
Bing Liu. 2012. Sentiment Analysis and Opinion
Mining, volume 16 of Synthesis Lectures on Human
Language Technologies. Morgan & Claypool.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. In Proceedings
of the International Workshop on Semantic Evalua-
tion (SemEval 2014).
Kim Schouten and Flavius Frasincar. 2014. Find-
ing Implicit Features in Consumer Reviews for Sen-
timent Analysis. In Proceedings of the 14th In-
ternational Conference on Web Engineering (ICWE
2014), pages 130?144. Springer.
Yu Zhang and Weixiang Zhu. 2013. Extracting
Implicit Features in Online Customer Reviews for
Opinion Mining. In Proceedings of the 22nd Inter-
national Conference onWorldWideWeb Companion
(WWW 2013 Companion), pages 103?104. Interna-
tional World Wide Web Conferences Steering Com-
mittee.
207
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 61?68,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
Applying prosodic speech features in mental health care: 
An exploratory study in a life-review intervention for depression 
  
Sanne M.A. Lamers 
University of Twente 
Psychology, Health, & 
Technology 
the Netherlands 
s.m.a.lamers@ 
utwente.nl 
 
Khiet P. Truong 
University of Twente 
Human Media Interaction 
the Netherlands 
k.p.truong@ 
utwente.nl 
Bas Steunenberg 
UMC Utrecht 
the Netherlands 
 b.steunenberg@ 
umcutrecht.nl 
Franciska de Jong  
University of Twente 
Human Media Interaction 
the Netherlands 
f.m.g.dejong@ 
utwente.nl  
 
Gerben J. Westerhof 
University of Twente 
Psychology, Health, & Technology 
the Netherlands 
g.j.westerhof@ 
utwente.nl 
 
Abstract 
The present study aims to investigate the   
application of prosodic speech features in a 
psychological intervention based on life-
review. Several studies have shown that 
speech features can be used as indicators of 
depression severity, but these studies are 
mainly based on controlled speech recording 
tasks instead of natural conversations. The 
present exploratory study investigated speech 
features as indicators of depression in con-
versations of a therapeutic intervention. The 
changes in the prosodic speech features pitch, 
duration of pauses, and total duration of the 
participant?s speaking time were studied over 
four sessions of a life-review intervention for 
three older participants. The ecological valid-
ity of the dynamics observed for prosodic 
speech features could not be established in 
the present study. The changes in speech fea-
tures differed from what can be expected in 
an intervention that is effective in decreasing 
depression and were inconsistent with each 
other for each of the participants. We suggest 
future research to investigate changes within 
the intervention sessions, to relate the chang-
es in feature values to the topical content of 
the speech, and to relate the speech features 
directly to depression scores. 
1 Introduction 
Depression is a mood disorder that is mainly 
characterized by a sad mood or the loss of in 
terest and pleasure in nearly all activities in a 
period of at least two weeks (American Psychia- 
tric Association, 2000). Depression disorders are 
the leading cause of disability and contribute 
largely to the burden of disease in middle- and 
high-income countries worldwide (?stun et al., 
2004). In 2012, more than 350 million people 
around the world suffered from depression symp-
toms (World Health Organization, 2012). To de-
crease the onset of depression disorders, early 
psychological interventions, i.e., psychological 
methods targeting behavioral change to reduce 
limitations or problems (Vingerhoets, Kop, & 
Soons, 2002), aiming at adults with depression 
symptoms or mild depression disorders are    
necessary. Meta-analytic findings show that psy-
chological interventions reduce the incidence of 
depression disorders by 22%, indicating that pre-
vention of new cases of depression disorders is 
indeed possible (Cuijpers et al., 2008).  
To evaluate the effectiveness of interventions 
for depression and changes during the interven-
tions, reliable and valid measures of depression 
severity are necessary. Depression severity is 
mostly measured by self-report questionnaires 
such as the Center for Epidemiologic Studies 
Depression scale (CES-D; Radloff, 1977), the 
Hamilton Depression Rating Scale (HAM-D; 
Hamilton, 1960), and the Beck Depression In-
ventory (Beck, Steer, & Brown, 1996). These 
self-report questionnaires often include items on 
mood and feelings. Moreover, questionnaire 
items may cover physical depression symptoms 
such as sleep disturbances, changes in weight 
61
and appetite, and loss in energy. However, in 
some target groups such as older adults these 
items can confound with health problems and 
physical diseases, which increase in old age. For 
these reasons, there is a need for valid and objec-
tive measures of depression severity. Not only to 
assess depression severity before and after thera-
py, but also to detect the dynamics during the 
therapy (Elliot, 2010). 
1.1 Computational linguistics, speech ana-
lysis, and mental health care 
It is commonly assumed  and confirmed in    
several studies that emotions and mood can in-
fluence the speaking behavior of a person and the 
characteristics of the sound in speech (Kuny & 
Stassen, 1993; Scherer, Johnstone, & Klasmeyer, 
2003). Already in 1954, Moses concluded that 
the voice and speech patterns of psychiatric pa-
tients differed from those of people without a 
psychiatric diagnosis. Clinicians observe the 
speech of depressed patients frequently as uni-
form, monotonous, slow, and with a low voice 
(Kuny & Stassen, 1993). A review by Sobin and 
Sackeim (1997) showed that depressed people 
differ from normal and other psychiatric groups 
on psychomotor symptoms such as speech. The 
speech of depressed patients is characterized by a 
longer pause duration, that is, an increased 
amount of time between speech utterances as 
well as by a reduced variability in mean vocal 
pitch.  
More recently these insights have led to      
collaborative and multidisciplinary work be-
tween researchers from the fields of computa-
tional linguistics and mental health care. With 
the growing availability of models and algo-
rithms for automated natural language processing 
that can be put to use in clinical scenarios, de-
pression can now increasingly be measured 
based on the characteristics of the language used 
by patients, such as the frequency of verbal ele-
ments in a narrative that express a certain mood 
or sentiment (Pennebaker & Chung, 2011), and  
acoustic speech features.  Because vocal acoustic 
features such as pause durations and pitch are 
biologically based, it has even been argued that 
they can serve as biomarkers of depression se-
verity (Mundt et al., 2012). As a consequence, 
speech features such as pitch and pause durations 
can be used to estimate the severity of a depres-
sion. 
To date, several studies investigated the validi-
ty of several speech features as indicators  of de-
pression. Indeed, the speech features pitch and 
speech loudness correlate significantly with 
global depression scores during recovery (Kuny 
& Stassen, 1993; Stassen, Kuny, & Hell, 1998). 
After recovery from depression, the speech pause 
time of depressed adults was no longer elongated 
(Hardy et al., 1984). These results indicate that  
prosodic speech features are valid measures of 
depression.  
However, these studies have the limitation that 
the speech analyses are based on the recording of 
controlled speech based on tasks such as count-
ing and reading out loud. Such speech recording 
tasks take place under ideal voice recording con-
ditions (Cannizzaro, Harel, Reilly, Chappell, & 
Snyder, 2004), while speech analysis is more 
difficult when conducted outside a controlled 
setting, because of so-called noisy channel     
effects (Janssen, Tacke, de Vries, van den Broek, 
Westerink, Haselager, & IJsselsteijn, 2013). 
Moreover, controlled speech tasks are cognitive-
ly less demanding than free speech tasks (Alpert 
et al., 2011). This evokes the question whether 
speech features are also ecological valid, i.e., 
whether they can be used as indicators of depres-
sion severity, when measured during natural 
conversations instead of during the recording of 
controlled speech tasks (Bronfenbrenner, 1977).  
A study on speech samples from video recor- 
dings of structured interviews revealed promis-
ing results: speaking rate and pitch variation, but 
not the percentage of pauses, showed a large cor-
relation with depression rating scores (Canni-
zaro, Harel, Reilly, Chappell, & Snyder, 2004). 
Additional studies on the ecological validity of 
using prosodic speech features as indicator for  
depression are necessary. 
1.2 Speech features as mood markers in a 
life-review intervention 
In the present study the speech of older adults 
will be measured in four sessions of a psycholo- 
gical intervention, combining knowledge in the 
fields of computational linguistics and psycho-
logical interventions in mental health care. Be-
cause psychological interventions of depression 
have shown to be effective (e.g., Cuijpers, van 
Straten, & Smit, 2006) and are broadly imple-
mented in mental health care, the measurement 
of speech features in psychological interventions 
is a promising application for the field of compu-
tational linguistics. For example, speech features 
can be used to provide direct feedback to both 
the therapist and patient on the severity and 
changes in severity of depression during the psy-
chological intervention. Clinicians do not have 
62
the ability to differentiate precisely the duration 
of for example the patient?s utterances and paus-
es (Alpert et al. 2001). There is also ample evi-
dence that text mining techniques based on the 
frequency of certain terms can be applied to nar-
ratives from patients in order to monitor changes 
in mood (Pennebaker & Chung, 2011), and a re-
cent study has shown that machines can better 
recognize certain emotions than lay people 
(Janssen et al., 2013), underlining once again the 
added value  of automated speech analysis. To 
pave the way for future applications that would 
enable the use of  speech features as a direct 
feedback mechanism, the first step is to gain 
more knowledge on the patterns in speech fea-
tures and on how changes in these features can 
be considered as meaningful signals of patterns 
in   psychological interventions.  
The psychological intervention in the present 
study is based on life-review: the structured re-
collection of autobiographical memories. De-
pressed people have difficulties in retrieving spe-
cific, positive memories. Their autobiographical 
memory is characterized by more negative and 
general memories (e.g., Williams et al., 2007), 
for example memories that reflect a period or 
recurrent event (e.g., the period of a marriage) 
rather than a specific event (e.g., the ceremony 
on the wedding day). The present life-review 
course targets the recollection of specific, posi-
tive memories in older adults with depression 
symptoms. In four weekly sessions, the inter-
viewer stimulates the recollection by asking 
questions on the depressed person?s childhood, 
adolescence, adulthood and life in general. An 
advantage of life-review in comparison to other 
therapies such as Cognitive Behavioral Therapy, 
is that it fits in with a natural activity of older 
adults to recollect memories and tell stories 
about their lives (Bluck & Levine, 1998). Life-
review has shown to be an effective method to 
decrease depression symptoms (Korte, Bohl-
meijer, Cappeliez, Smit, & Westerhof, 2012; 
Pinquart & Forstmeier, 2012) and is considered 
an evidence-based intervention for depression in 
older adults (Scogin, Welsh, Hanson, Stump, & 
Coates, 2005).  
Our study is one of the first to investigate pro-
sodic speech features during a psychological in-
tervention. The study is exploratory and aims to 
gain insight into the ecological validity of pro-
sodic speech features in a psychological life-
review intervention. The life-review intervention 
offers the opportunity to investigate the prosodic 
speech features over time. Life-review is highly 
suitable to investigate speech features during an 
intervention, since the speech from the recall of 
autobiographical memories provides strong pro-
sodic speech changes (Cohen, Hong, & Guevara, 
2010) and the expression of emotions charac-
terized by speech characteristics is stronger after 
open and meaning-questions as compared to 
closed and fact-questions (Truong, Westerhof, 
Lamers, & de Jong, under review). Our paper is a 
first step to gain insight into the methods that are 
necessary to evaluate the application of prosodic 
speech features in mental health care. In the pre-
sent study into the role of prosodic speech fea-
tures,  vocal pitch and pause duration will be in-
vestigated in three participants across all four 
weekly sessions. Because the life-review inter-
vention is effective in decreasing depression 
symptoms (Korte et al., 2011; Serrano, Latorre, 
Gatz, & Montanes, 2004), we expect that the 
prosodic features change accordingly. Therefore, 
we hypothesize (a) an increase in average vocal 
pitch, (b) an increase in the variation in vocal 
pitch, (c) a decrease in average pause duration, 
(d) a decrease in the ratio between the total pause 
time and total speech time (pause speech ratio), 
and (e) an increase in the ratio between the par-
ticipant?s speech and total duration of the session 
(speech total duration ratio) during the interven-
tion.  
2 Method 
In this section we will describe the methodology 
applied in the design of the psychological inter-
ventions during which the research data sets 
were generated, the procedure for selecting the 
participants and the corresponding data sets, the 
data preparations steps and the analyses per-
formed.  
2.1 Intervention ?Precious memories? 
The life-review intervention ?Precious memories? 
(Bohlmeijer, Serrano, Cuijpers, & Steunenberg, 
2007) targets the recollection of specific, positive 
memories. The intervention is developed for  
older adults with depression symptoms living in 
a nursing home. Each of the four weekly sessions 
focuses on a different theme: childhood, ado-
lescence, adulthood, and life in general. The ses-
sions are individual and guided by a trained in-
terviewer. The sessions take place at the partici-
pant?s home and last approximately 45 minutes. 
Each of the sessions is structured by fourteen 
main questions that stimulate the participant to 
recollect and tell specific positive memories 
63
about his or her life. The interviewers are in-
structed to ask for lively details about each of the 
positive memories of the participants, for        
example the colors, smells and people that were 
involved in the memory. Table 1 shows an ex-
ample question for each of the four sessions. 
 
Session  Example question 
1: Childhood Can you remember an 
event in which your father 
or mother did something 
when you were a child that 
made you very happy? 
2: Adolescence Do you remember a special 
moment of getting your 
first kiss or falling in love 
with someone? 
3: Adulthood What has been a very im-
portant positive experience 
in your life between the 
ages of 20 and 60? 
4: Life in general What is the largest gift you 
ever received in your life? 
Tabel 1. Example questions for the four sessions 
of the life-review intervention ?Precious  
memories? 
2.2 Procedure and participants 
Participants with depression symptoms were re-
cruited in nursing homes in the area of Amster-
dam, the Netherlands. Participation in the life-
review intervention was voluntary. Three partici-
pants were selected for whom audio recordings 
of the four sessions were available, which resul-
ted in a dataset of twelve life-review sessions. 
The three participants (below labeled as P1, P3 
and P5) were females with an age between 83 
and 90 years. The educational background varied 
from low to high and the marital status from 
married to never married. The participants signed 
an informed consent form for the use of the au-
dio-tapes  for scientific purposes.  
2.3 Data preparation and analysis 
All acoustic features were automatically extract-
ed with Praat (Boersma, 2001). Because the 
speech of both the interviewer and the partici-
pants were recorded on one mixed audio channel, 
some manual interventions had to be applied in 
order to determine the segments in which the 
participant is talking. First, for each session, the 
segments in which the participant is the main 
speaker were selected. These so-called ?turns? 
were then labeled in more detail; utterances pro-
duced by the interviewer were marked and dis-
carded in the speech analysis. For each turn, 
mean pitch, standard deviation pitch, pause dura-
tion, the ratio between total pause time and total 
speech time, and the ratio between total speech 
time and total duration of the session were ex-
tracted. Pause durations were automatically ex-
tracted by applying silence detection where the 
minimal silence duration was set at 500 ms. All 
features were normalized per speaker by trans-
forming the raw feature values to z-scores (mean 
and standard deviation were calculated over all 4 
sessions, z = ((x-m)/sd)). The ratio between total 
speech time and total duration time was not nor-
malized because this feature was calculated over 
a whole session instead of a turn. Subsequently, 
averages over all turns per session were taken in 
order to obtain one value per session. 
3 Results 
The results of the prosodic speech features over 
the four sessions of the life-review intervention 
are graphically presented separately for each fea-
ture, in the Figures 1 to 5. We hypothesized an 
increase in the average pitch during the interven-
tion. As shown in Figure 1, the patterns in ave-
rage pitch during the intervention differs across 
the three participants. Only in Participant 3, the 
pattern is in line with our expectations, showing 
an increase in the sessions 3 and 4. In both Par-
ticipant 1 and 5, there was a decrease in average 
pitch in the sessions 3 and 4. 
 
 
 
Figure 1. Average pitch of the participants 
(P1,P3,P5) during the four sessions. 
 
We expected the variation in pitch to increase 
during the intervention. Figure 2 shows the par-
ticipants? patterns of the standard deviation of 
pitch during the intervention. The changes in 
standard deviation do not confirm our hypothe-
sis. Although the speech of Participant 3 shows 
64
an increase in session 4, the standard deviation is 
lower in session 4 than in session 1 of the inter-
vention. The standard deviation of Participant 5 
is relatively stable during the intervention. Par-
ticipant 1 mainly shows a large variation in pitch 
in session 2. 
 
 
 
Figure 2. Standard deviation in pitch of the par-
ticipants (P1,P3,P5) during the four sessions. 
 
It was hypothesized that the average pause du-
ration would decrease during the four sessions of 
the intervention. Figure 3 shows that the average 
pause duration was relatively stable over the first 
three sessions in all three participants. Only in 
Participant 1 the average pause duration de-
creased in session 4, in line with our expec-
tations.  
 
 
 
Figure 3. Pause duration of the participants 
(P1,P3,P5) during the four sessions. 
 
In agreement with our hypothesis on average 
pause duration, we also expected a decrease dur-
ing the intervention in the ratio between the total 
pause time and total speech time. Although there 
was a large decrease in the pause speech ratio of 
Participant 1 between the sessions 2 and 3, the 
ratio in session 4 was similar to the pause speech 
ratio in the first session (see Figure 4). In both 
Participant 2 and 3, the ratio was relatively stable 
in the sessions 1 to 3, but in session 4 the pause 
speech ratio showed an increase in Participant 3 
and a slight decrease in Participant 2. 
 
 
 
Figure 4. Pause speech ratio of the participants 
(P1,P3,P5) during the four sessions. 
 
Last, we investigated the ratio between the 
participant?s speech and total duration of the ses-
sion. We hypothesized an increase in the speech 
total duration ration during the intervention. Fig-
ure 5 shows the differences between the partici-
pants in the speech total duration ratio over the 
four sessions. The ratio is relatively stable, and 
high, in Participant 5. The ratio in both Partici-
pant 1 and 3 in general decreases during the in-
tervention, with a lower speech total duration 
ratio in session 4 as compared to session 1.  
 
 
 
Figure 5. Speech total duration ratio of the par-
ticipants (P1,P3,P5) during the four sessions. 
 
4 Conclusion 
The aim of the present study was to investigate 
the suitability of applying  an analysis of proso-
dic speech features in the speech recordings    
65
collected in psychological intervention based on 
life-review. Because several studies have shown 
that speech features can be used as indicators of 
depression severity (e.g., Kuny & Stassen, 1993; 
Stassen, Kuny, & Hell, 1998), the application of 
speech analyses in mental health care is promi- 
sing. However, the measurement of speech fea-
tures is often based on speech recording tasks 
and the ecological validity within psychological 
interventions is not yet established. The study is 
a first exploratory step to gain insight into the 
ecological validity of prosodic speech features in 
a psychological life-review intervention.  
We expected to measure a change during the 
intervention in the prosodic speech features that 
could be  related to depression symptoms, and 
hypothesized an increase in average pitch and 
pitch variation, a decrease in average pause dura-
tion, and an increase in the amount of speech by 
the participant during the intervention. However, 
we could not establish  the ecological validity of 
these speech indicators in the present study. In 
general, the patterns of the prosodic speech indi-
cators differ from our expectations. The dyna-
mics  in the speech indicators was different from 
what can be expected in an intervention that is 
effective in decreasing depression (Korte et al., 
2011; Serrano et al., 2004). Moreover, the speech 
indicators were inconsistent with each other for  
the participants in the pool. For example, Partici-
pant 3 showed an increase in pitch during the 
intervention, which indicates a decrease in de-
pression, and an increase in average pause dura-
tion and pause speech ratio, which indicates an 
increase in depression.  
Taken together, the findings from the present 
study indicate that the prosodic speech features 
that have been validated for controlled settings, 
are not directly applicable for the spontaneous 
type of conversation that is typical for a mental 
health care setting. More research is needed to 
establish the ecological validity of prosodic 
speech features such as pitch, pauses, and speech 
duration as indicators of depression severity. A 
few suggestions can be made. First, each of the 
four sessions in the life-review intervention in 
the present study focused on a different theme. 
Although we aimed to evaluate the development 
of the speech features during the intervention, the 
differences across the session may be the conse-
quence of differences in session theme. More-
over, not all parts of the session consisted of  
life-review, and participants were talking about a 
variety of subjects, for example about  their 
caregivers. The goal of the life-review interven-
tion is to stimulate the retrieval of specific posi-
tive memories. In a next step, we aim to select 
the parts in which the participant is recollecting 
such memories and to evaluate the patterns in 
prosodic speech features only for  these parts. 
Second, the prosodic speech indicators were 
averaged per session to provide a clear overview 
of the changes over the four sessions. However, 
changes can also occur within the session. For 
example, vocal pitch may increase during the 
session, which would indicate a decrease in de-
pression symptoms. Furthermore, within each 
session, the interaction between the interviewer 
and participant may play a role. For instance, 
when the interviewer speaks with a higher pitch 
and more variation in pitch, the participant may 
unconsciously take over some of this speaking 
behavior. We suggest future studies to investi-
gate not only the average session, but to include 
changes during the session the interviewer?s 
speech features. 
Third, the present research was conducted in 
line with the assumption that life-review is effec-
tive as an intervention for mood disorder, as is 
shown in several studies (Korte et al., 2011; Ser-
rano et al., 2004). However, we due to lack of 
data on depression severity we do not know 
whether the life-review intervention was fully 
effective for the participants in the present study. 
To validate the patterns prosodic speech features 
as a reliable indicator for depressions that can be 
used in mental health care, it is necessary to 
demonstrate  that  the dynamics in speech fea-
tures can be related directly to changes in depres-
sion scores. As argued in earlier studies, in order 
to  conclude that speech features correlate signi-
ficantly with global depression scores during re-
covery (Kuny & Stassen, 1993; Stassen, Kuny, & 
Hell, 1998), these correlations need to be inves-
tigated in psychological interventions.  
In sum, the study  of how prosodic speech fea-
tures such as pitch and pauses relate to the kind 
of spoken narratives that play a role in mental 
health care settings is a promising field. How-
ever, the ecological validity of prosodic speech 
features could not be established in the present 
study. More research based on  larger data sam-
ples the establishment of a direct relation to de-
pression scores is  necessary before the tech-
niques from the field of computational linguistics 
can be applied as a basis for the collection of in-
dicators that can be used  in psychological inter-
ventions in a meaningful and effective way. 
 
66
References 
Alpert, M., Pouget, E. R., & Silva, R. R. (2001). Re-
flections of depression in acoustic measures of the 
patient?s speech. Journal of Affective Disorders, 
66, 59-69. 
 
American Psychiatric Association (2000). Diagnostic 
and statistical manual of mental disorders (4th ed., 
text rev.). Washington, DC: Author.  
 
Beck, A. T., Steer, R. A., & Brown, G. K. (1996). 
Manual for the Beck Depression Inventory-II. San 
Antoinio, TX: Psychological Corporation. 
 
Bluck, S., & Levine, L. J. (1998). Reminiscence as 
autobiographical memory: A catalyst for reminis-
cence theory development. Ageing and Society, 
18, 185-208. 
 
Boersma, P. (2001). Praat, a sysem for doing pho- 
netics by computer. Glot International, 5(9/10), 
341-345. 
 
Bohlmeijer, E. T., Serrano, J., Cuijpers, P., & Steu-
nenberg, B. (2007). Dierbare herinneringen. Pro-
tocol voor life-review bij ouderen met depressieve 
klachten in verzorgings- en verpleeghuizen [Pre-
cious memories. Protocol for life-review in older 
people with depressive symptoms in nursing 
homes]. 
 
Bronfenbrenner, U. (1977). Toward an experimental 
ecology of human development. American Psy-
chologist, 32, 513-531. 
Cannizzaro, M., Harel, B., Reilly, N., Chappell, P., & 
Snyder, P. J. (2004). Voice acoustical measure-
ment of the severity of major depression. Brain 
and Cognition, 56, 30-35. 
 
Chien, J.-T., & Chueh, C-H. (2010). Joint acoustic 
and language modeling for speech recognition. 
Speech Communication, 52, 223-235. 
 
Cohen, A. S., Hong, S. L., & Guevara, A. (2010). 
Understanding emotional expression using prosod-
ic analysis of natural speech: refining the method-
ology. Journal of Behavioral Therapy & Experi-
mental Psychiatry, 41, 150-157. 
 
Cuijpers, P., van Straten, A., Smit, F. (2006). Psycho-
logical treatment of late-life depression: A meta-
analysis of randomized controlled trials. Interna-
tional Journal of Geriatric Psychiatry, 21, 1139-
1149. 
 
Cuijpers, P., van Straten, A., Smit, F., Mihalopoulos, 
C., & Beekman, M. D. (2008). Preventing the on-
set of depressive disorders: a meta-analytic review 
of psychological interventions. American Journal 
of Psychiatry, 165, 1272-1280. 
 
Elliot, R. (2010). Psychotherapy change process re-
search: Realizing the promise. Psychotherapy Re-
search, 20, 123-135. 
 
Hamilton, M. (1960). A rating scale for depression. 
Journal of Neurology, Neurosurgery, & Psychia-
try, 23, 56-62. 
 
Hardy, P., Jouvant, R., & Widl?cher, D. (1984). 
Speech pause time and the retardation rating scale 
for depresstion (ERD): towards a reciprocal vali-
dation. Journal of Affective Disorders, 6, 123-127. 
 
Janssen, J. H., Tacke, P., de Vries, J. J. G., van den 
Broek, E. L., Westerink, J. H. D. M., Haselager, 
P., & IJsselsteijn, W. A. (2013). Machines outper-
form laypersons in recognizing emotions elicited 
by autobiographical recollection. Human-
Computer Interaction, 28, 479-517. 
 
Koolagudi, S. G., & Sreenivasa, K. S. (2012). Emo-
tion recognition from speech: a review. Interna-
tional Journal of Speech Technology, 15, 99-117. 
 
Korte, J., Bohlmeijer, E. T., Cappeliez, P., Smit, F., & 
Westerhof G. J. (2012). Life-review therapy for 
older adults with moderate depressive sympto-
matology: A pragmatic randomized controlled tri-
al. Psychological Medicine, 42, 1163-1172.  
 
Kuny, S., & Stassen, H. H. (1993). Speaking behavior 
and voice sound characteristics in depressive pa-
tients during recovery. Journal of Psychiatric Re-
search, 27, 289-307. 
 
Moses, J. P. (1954). The voice of neurosis. Oxford, 
UK: Grune and Stratton. 
 
Mundt, J. C., Vogel, A. P., Feltner, D. E., & Lender-
king, W. R. (2012). Vocal acoustic biomarkers of 
depression severity and treatment response. Bio-
logical Psychiatry, 72, 580-587. 
 
Pennebaker, J. W. and Chung, C. K. 
(2011). Expressive Writing and its Links to Men-
tal and Physical Health. In H. S. Friedman 
(Ed.), Oxford Handbook of Health Psycholo-
gy. New York, NY: Oxford University Press, 417-
437. 
 
Pinquart, M., & Forstmeier, S. (2012). Effects of rem-
iniscence interventions on psychosocial outcomes: 
A meta-analysis. Aging & Mental Health, 16, 514-
558. 
 
Radloff, L. S. (1977). The CES-D Scale: A Self-
Report Depression Scale for Research in the Gen-
eral Population. Applied Psychological Measure-
ment, 1, 385-401. 
67
 
Scherer, K. R., Johnstone, T., & Klasmeyer, G. 
(2003). Vocal expression of emotion. In R. J. Da-
vidson, K. R. Scherer, & H. Goldsmith (Eds.), 
Handbook of the Affective Sciences (pp. 433?456). 
New York and Oxford: Oxford University Press. 
 
Scogin, F., Welsh, D., Hanson, A., Stump, J., & 
Coates, A. (2005). Evidence-based psychothera-
pies for depression in older adults. Clinical Psy-
chology: Science and Practice, 12, 222-237. 
 
Serrano, J., Latorre, J., Gatz, M., & Montanes, J. 
(2004). Life review therapy using autobiograph-
ical retrieval practice for older adults with depres-
sive symptomatology. Psychology & Aging, 19, 
272-277. 
 
Sobin, C., & Alpert, M. (1999). Emotion in speech: 
the acoustic attributes of fear, anger, sadness, and 
joy. Journal of Psycholinguistic Research, 28, 
347-365. 
 
Sobin, C., & Sackeim, H. A. (1997). Psychomotor 
symptoms of depression. American Journal of 
Psychiatry, 154, 4-17. 
 
Stassen, H. H., Kuny, S., & Hell, D. (1998). The 
speech analysis approach to determining onset of 
improvement under antidepressants. European 
Neuropsychopharmacology, 8, 303-310. 
 
Truong, K., Westerhof, G. J., Lamers, S. M. A., & de 
Jong, F. (under review). Towards modeling ex-
pressed emotions in oral history interviews: using 
verbal and non-verbal signals to track personal 
narratives. Literary and Linguistic Computing. 
 
?st?n, T. B., Ayuso-Mateos, J. L., Chatterji, S., 
Mathers, C., & Murray, C. J. L. (2004). Global 
burden of depressive disorders in the year 2000. 
British Journal of Psychiatry, 184, 386-392. 
 
Vervedis, D., & Kotropoulos, C. (2006). Emotional 
speech recognition: Resources, features, and 
methods. Speech Communication, 48, 1162-1181. 
 
Vingerhoets, A. J. J. M., Kop, P. F. M., & Soons, P. 
H. G. M. (2002). Psychologie in de gezondheids-
zorg: een praktijkori?ntatie [Psychology in health 
care: a practical orientation]. Houten, the Nether-
lands: Bohn Stafleu van Loghum. 
 
Williams, J. M., Barnhofer, T., Crane, C., Herman, D, 
Raes, F., Watkins, E., & Dalgleish, T. (2007). Au-
tobiographical memory specificity and emotional 
disorder. Psychological Bulletin, 133, 122-148. 
 
World Health Organization. (2012). Depression Fact 
Sheet. Retrieved at March, 5, 2014:  
www.who.int/mediacentre/factsheets/fs369/en/ 
 
68
