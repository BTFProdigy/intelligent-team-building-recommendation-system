An Empirical Investigation of the Relation Between Discourse Structure and 
Co-Reference 
Dan Cristea 
Department of Computer Science 
University "A.I. Cuza" 
Ia~i, Romania 
dcristea @ infoiasi, m 
Daniel Marcu 
In fo rmat ion  Sc iences  Inst itute and 
Depar tment  of  Computer  Sc ience 
Univers i ty  of  Southern Cal i forn ia  
Los  Angeles ,  CA, USA 
ma twig @ isi. edu 
Nancy Ide 
Department ofComputer Science 
Vassar College 
Poughkeepsie, NY, USA 
ide @ cs. vassal: edu 
Valentin Tablan* 
Department of Computer Science 
University of Sheffield 
United Kingdom 
v. tablan @ sheJ.'field, ac. uk 
Abstract 
We compare the potential of two classes el' linear and hi- 
erarchical models of discourse to determine co-reference 
links and resolve anaphors. The comparison uses a co l  
pus of thirty texts, which were manually annotated for 
co-reference and discourse structure. 
1 Introduction 
Most current anaphora resolution systems implelnent a
pipeline architecture with three modules (Lappin and Le- 
ass, 1994; Mitkov, 1997; Kameyama, 1997). 
1. A COLLF.CT module determines a list of potential 
antecedents (LPA) for each anaphor (l~ronourl, deli- 
nile noun, proper name, etc.) that have the potential 
to resolve it. 
2. A FILTI~,P, module eliminates referees incompatible 
with the anaphor fi'om the LPA. 
3. A PP, EFERENCE module determines the most likely 
antecedent on the basis of an ordering policy. 
In most cases, the COLLECT module determines an LPA 
by enumerating all antecedents in a window o1' text that 
precedes the anaphor under scrutiny (Hobbs, 1978; Lap- 
pin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; 
Ge et al, 1998). This window can be as small as two 
or three sentences or as large as the entire preceding 
text. The FILTEP, module usually imposes emantic on- 
straints by requiring that the anaphor and potential an- 
tecedents have the same number and gendm; that selec- 
tional restrictions are obeyed, etc. The PREFERENCE 
module imposes preferences on potential antecedents 
on the basis of their grammatical roles, parallelism, 
fi'equency, proximity, etc. In some cases, anaphora 
resolution systems implement hese modules explic- 
itly (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 
* On leave fi'om lhe Faculty of Computer Science, University "AI. I. 
Cuza" of lasi. 
1997; Kameyama, 1997). In other cases, these modules 
are integrated by means of statistical (Ge et al, 1998) or 
uncertainty reasoning teclmiques (Mitkov, 1997). 
The fact that current anaphora resolution systems rely 
exclusively on the linear nature of texts in order to de- 
termine the LPA of an anaphor seems odd, given flint 
several studies have claimed that there is a strong rela- 
tion between discourse structure and reference (Sidner, 
1981 ; Grosz and Sidner, 1986; Grosz et al, 1995; Fox, 
1987; Vonk ct al., 1992; Azzam el al., 1998; Hitzcman 
and Pocsio, 1998). These studies claim, on the one hand, 
that the use of referents in naturally occurring texts im- 
poses constraints on the interpretation f discourse; and, 
on the other, that the structure of discourse constrains the 
LPAs to which anaphors can be resolved. The oddness 
of the situation can be explained by lho fac! that both 
groups seem primafilcie m be right. Empirical exper- 
iments studies that employ linear techniques for deter- 
mining the LPAs o1' almphol's report recall and precision 
anaphora resolution results in the range of 80% (Lappin 
and Leass, 1994; Ge ct al., 1998). Empirical experiments 
that investigated the relation between discourse structure 
and reference also claim that by exploiting the structure 
of discourse one has the potential of determining correct 
co-referential links for more than 80% of the referential 
expressions (Fox, 1987; Cristca et al, 1998) although to 
date, no discourse-based anaphora resolution system has 
been implemented. Since no direct comparison of these 
two classes of approaches has been made, it is difficult to 
determine which group is right, and what method is the 
best. 
In this paper, we attempt o Iill this gap by empiri- 
cally comparing the potential of linear and hierarchical 
models el' discourse to correctly establish co-referential 
links in texts, and hence, their potential to correctly re- 
solve anaphors. Since it is likely that both linear- and 
discourse-based anaphora resolution systems can imple- 
ment similar FILTER and PI~,I~FERENCE strategies, we fo- 
cus here only on the strategies that can be used to COL- 
208 
LECT lists of potential antecedents. Specilically, we fo- 
ct, s on deterlnining whether discourse llteories can help 
an anaphora resolution system detemfine LPAs that are 
"better" than the LPAs that can be contputed from a lin- 
ear interpretation f texts. Section 2 outlines the theoreti- 
cal assumptions of otu" empirical investigation. Section 3 
describes our experiment. We conclude with a discussion 
of tile results. 
2 Background 
2.1 Assumptions 
Our approach is based on the following assmnptions: 
I. For each anaphor in a text, an anaphora resolution 
system must produce an LPA that contains a refer- 
ent to which 111e anaphor can be resolvcd. The size 
of this LPA wuies fronl system to system, depend- 
ing on tile theory a system implements. 
2. The smaller the I,PA (while retaining a correct an- 
tecedent), the less likely that errors ill the \]7tI,TH{ 
:(lid PI',V,I;F, RI;,NCI ~, modules wil l  affect the ability of 
a system to select the appropriate referent. 
3. Theory A is better than lheory B for the task of rel- 
erence resolution if theory A produces I J 'As that 
contain more antecedents o which amtphors can be 
corrcclly resolved than theory B, and if the l,l~As 
produced by theory A arc smaller than those pro- 
duccd by theory B. l;or cxaml)lc, if for a given 
anaphor, theory A produces an I,PA thai contains a 
referee to which the anaphor can be resolved, while 
theory B produces an IJ~A that does not contain 
such a re\[eree, theory A is better than theory B. 
Moreover, if for a given anaphor, theory A produces 
an Lt)A wilh two referees and theory B produces an 
LPA with seven rel'crees (each LPA containing a ref- 
eree to which tile anal)her can be resolved), lheory 
A is considered better than theory 11 because it has a 
higher probability of solving that anaphor correctly. 
We consider two classes of models for determining the 
LPAs of anaphors ill a text: 
Linear-k models. This is at class of linear models in 
which the LPAs include all the references foulad in the 
discourse unit under scrutiny and the k discourse units 
that immediately precede it. Linear-0 models an ap- 
proach that assumes that :tll anaphors can be resolved 
intra-unit; Linear- 1 models an approach that cor,'esponds 
roughly to centering (Grosz et al, 1995). Linear-k is con- 
sistent with the asslunl)tions that underlie most current 
anaphora resohltion systems, which look back h units in 
order to resolve an anaphor. 
l) iscourse-V1:k models. In |his class ()1'models, LPAs 
include all lhe refcrentM expressions fotmd in the dis- 
course unit under scrutiny and the k discourse units that 
hierarchically precede it. The units that hierarchically 
precede a given unit are determined according to Veins 
Theory (VT) (Cristea et al, 1998), which is described 
brielly below. 
2.2 Veins Theory 
VT extends and formalizes the relation between dis- 
course  s t ruc ture  and reference proposed by Fox (1987). 
It identilies "veins", i.e., chains of elementary discourse 
units, over discourse structure trees that are built accord- 
ing to the requirements put forth in Rhetorical Structure 
Theo,y (RST) (Mann and Thompson, 1988). 
One of the conjectures ()1' VT is that the vein expres- 
sion of an elementary discourse unit provides a coher- 
ent "abstract" of the discourse fi'agmcnt hat contains 
that unit. As an internally coherent discottrse fragment, 
most ()1' the anaphors and referentM expressions (REst 
in a unit must be resolved to referees that oceul" in the 
text subs:used by the units in tile vein. This conjec- 
ture is consistent with Fox's view (1987) that the units 
that contain referees to which anaphors can be resolved 
are determined by the nuclearity of the discourse units 
thal precede the anaphors and the overall structure of dis- 
course. According to V'I; REs of both satellites and nu- 
clei can access referees of hierarchically preceding nt,- 
cleus nodes. REs of nuclei can mainly access referees of 
preceding nuclei nodes and of directly subordinated, pre- 
ceding satellite nodes. And the interposition ()1' a nucleus 
after a satellite blocks tim accessibility of the satellite for 
all nodes that att'e lovcer in the corresponding discourse 
structure (see (Cristea et al, 1998) for a full delinition). 
Hence, the fundamental intuition unde,lying VT is 
that the RST-spceilie distinction between nuclei and 
satellites constrains the range of referents to which 
anaphors can 19e resolved; in other words, the nucleus- 
satellite distinction induces for each anaphor (and each 
referential expression) a Do,naita of Refcrenlial Acces- 
sibility (DRA). For each anaphor a in a discourse unit 
~z, VT hypothesizes that a can be resolved by examin- 
ing referential expressions that were used in a subset o1' 
the discourse units that precede it; this subset is called 
the DRA of u. For any elcntentary unit u in a text, the 
corresponding DRA is computed autonmtically from the 
rhetorical representation f that text in two steps: 
1. lteads for each node are computed bottom-up over 
the rhetorical representation tree. Heads ()1" elemen- 
tary discottrse traits are the traits themselves. Heads 
of internal nodes, i.e., discourse spans, are con> 
pt, ted by taking the union of the heads of the im- 
mediate child nodes that :ire nuclei. For example, 
for the text in Figure I, whose rhetorical structure is 
shown in Figure 2, the head ()1' span 115,711 is unit 5 
because the head ()t' the inmmdiate nucleus, the ele- 
mentary unit 5, is 5. However, the head of span 116,7\] 
is the list (6,7) because both immediate children are 
nuclei of a multinuclear relation. 
2. Using the results of step 1, Vein expressions are 
eOmlmted top-down lbr each node in the tree. The 
vein of the root is its head. Veins of child nodes 
209 
i. \[Michael D. Casey,\[a top Johnson&Johnson 
...... get, moved teCGe~ Therapy In~,  
a small biotechnology concern here, 
2. to become_~_t>president and ch ieC  
operating officer. \[ 
3. \[Mr. Casey, 46 years old,\] was\[ president of 
J&J's McNeil Pharmaceutical subsidiary,\] 
*t. which was merged with another J&J unit, 
Ortho Pharmaceutical Corp., this year in 
a cost-cutting move. 
5. I,Ir. Casev\[ succeeds M. James Barrett, 50, 
as\[president of ~,~netic Ther-ap~. 
6. Mr. Barrett remains chief executive officer 
7. and becomes chainaan. 
8. \[Mr-\] r. Casey\] said 
9. ~made the move te {\]{e smaller compan~ 
i0. because~saw health care moving toward 
technologies like 
products. 
ll. Ube l ieve  that the field is emerging and i~ 
prepared to break loose, 
12.\[he\[said. 
Figure 1: An example of text and its elementary units. 
The |'eferential expressions surrounded by boxes and el- 
lipses correspond to two distinct co-referential equiv- 
alence classes. Referential expressions urrounded by 
boxes refer to Mr: Casey; those surrounded by ellipses 
refer to Genetic Thercq~y Inc. 
are computed recursively according to tile rules de- 
scribed by Cristea et al(1998). The DRA ot" a unit u 
is given by the units that precede u in 1t~e vein. 
For example, for the text and RST tree in Figures 1 
and 2, the vein expression of unit 3, which contains 
units 1 and 3, suggests that anaphors from unit 3 
should be resolved only to referential expressions 
in units 1 and 3. Because unit 2 is a satellite to 
unit 1, it is considered to be "blocked" to referen- 
tial links fi'om trait 3. In contrast, tile DRA of unit 
9, consisting o1' units 1, 8, and 9, reflects the intu- 
ition that anaphors l?om unit 9 can be resolved only 
to referential ext)ressions fi'om unit 1, which is the 
most important trait in span \[1,7\], and to unit 8, a 
satellite that immediately precedes unit 9. Figure 2 
shows the heads and veins of all internal nodes in 
the rhetorical representation. 
2.3 Comparing models 
The premise underlying out" experiment is that there are 
potentially significant differences in the size of the search 
space rcquired to resolve referential cxpressions when 
using Linear models vs. Discou|'se-VT models. For ex- 
ample, for text and tile RST tree in Figures 1 and 2, the 
Discourse-VT model narrows tlle search space required 
to resolve the a|mphor the smaller company in unit 9. 
According to VT, we look lbr potential antecede|Us for 
the smaller company in the DRA of unit 9, which lists 
units I, 8, and 9. The antecedent Genetic Therapy Inc. 
appears in unit 1 ; therefore, using VT we search back 2 
units (units 8 and 1) to lind a correct antecedent. In con- 
trast, to resolve the same reference using a linear model, 
four units (units 8, 7, 6, and 5) must he examined be- 
fore Genetic The;zq?y is found. Assuming that referen- 
tial links are established as tile text is processed, Genetic 
Therapy would be linked back to pronottn its in unit 2, 
which would in turn be linked to the first occurrence of 
the antecedent,Genetic Therapy Inc., in unit 1, tile an- 
tecedent determined irectly by using V'E 
In general, when hierarchical adjacency is considered, 
an anaphor may be resolved to a referent hat is not the 
closest in a linear interpretation of a text. Simihu'ly, aref- 
erential expression can be linked to a referee that is not 
the closest in a linear interpretation of a text. However, 
this does not create problems because we are focusing 
here only on co-referential relations of identity (see sec- 
tion 3). Since these relations induce equivalence classes 
over tile set of referential expressions in a text, it is suf\[i- 
cient that an anaphor or referential expression is resolved 
to any of the members of the relevant equiw|lence class. 
For example, according to V'I, the referential expression 
MI: Casey in unit 5 in Figm'e 1 can be linked directly 
only to the referee Mr Casey in unit 1, because the DRA 
o1' unit 5 is { 1,5}. By considering the co-referential links 
of the REs in the other units, tile full equivalence class 
can be determined. This is consistent with tile distinction 
between "direct" and "indirect" references discussed by 
Cristea, et al(1998). 
3 The Experiment 
3.1 Materials 
We used thirty newspaper texts whose lengths varied 
widely; the mean o- is 408 words and tile standard e- 
viation/t is 376. Tile texts were annotated manually for 
co-reference r lations of identity (Hirschman and Chin- 
chef, 1997). Tile co-reference relations define equiv- 
alence classes oil the set of all marked referents in a 
text. Tile texts were also manually annotated by Marcu 
et al (1999) with disconrse structures built in the style 
of Mann and Thompson (1988). Each discourse analy- 
sis yielded an average of 52 elementary discourse units. 
See (Hirschman and Chinchor, 1997) and (Marcu et al, 
1999) for details of tile annotation processes. 
210 
H = 1 9 * 
V=lg*  
t I= l  
"?r ... = 1 9 * . . . . . . .  
H=I  L -  - - -  V=lg*  _ . . . . . . . . . .  
H= i |  ~{:-.,, - 
V 1 9 :+:~f- . . . . .  ~'}'-~%l. 3 5 9 * 
1 2 3 4 
H=3 
V=1359 
DF~,= 1 3 
___ - - - - -  - - ~  
_ _ - - -  - - - ___  
I - _  I 
H=5 
- - - - - __  _ 
';,~ = 1 59* 
q _}L___= 6 7 
._~-"-"- \; =xt,,5 67 9 * 
5 //%,,\ 
6 7 
H=9 
"?" = 1 9 * 
I H=9 i . . . .  i la---- . . . . . . . . . . . .  
m 
V= 1 9"  ~ ~---- . . . .  -- i 21-25 
\ [ -  
13-213 
9 191011*  
\" = 1 (g)9  
DRA = 1 11 12 
Figure 2: The I),ST analysis of the text in ligure I. The trcc is rcprescnted using the conventions proposed by Mann 
and Thompson (1988). 
3.2 Compar ing  potent ia l  to es tab l i sh  co - re ferent ia l  
l inks  
3.2.1 Method  
The annotations for co-reference rchttions and rhetorical 
struclure trues for the thirty texts were fused, yielding 
representations that rcllect not  only tile discourse strut- 
lure, but also the co-reference quivalencc lasses spe- 
citic to each tcxl. Based on this information, we cval- 
ualed the potential of each of the two classes (51" mod- 
els discussed in secdon 2 (Linear-k and Discourse-VT-k) 
to correctly establish co-referential links as follows: For 
each model, each k, and each marked referential expres- 
sion o., we determined whether or not tlle corresponding 
LPA (delined over k elementary units) contained a ref- 
eree from the same equiwdence class. For example, for 
the Linear-2 model and referential expression lhe .vmaller 
company in t, nit 9, we estimated whether a co-refercntial 
link could be established between the smaller company 
and another referential expression in units 7, 8, or 9. 
For the Discourse-VT-2 model and the same referential 
expression, we estimated whether a co-referential link 
could bE established between the smaller company and 
another eferential expression in units 1, 8, or 9, which 
correspond to the DRA of unit 9. 
qb enable a fair comparison of the two models, when k 
is la,'ger than the size of the DRA of a given unit, WE ex- 
tend thatDRA using the closest units that precede the unit 
under scrutiny and are not ah'eady in the DRA. Hence, 
for the Linear-3 model and the referential expression the 
smaller conq~any in trait 9, we estimate whether a co- 
referential link can be established between the xmaller 
company and another eferential expression in units 9, 8, 
7, or 6. For tile Discourse-VT-3 model and tile same rcf- 
ermltial expression, we estimate whclher a co-referential 
link can be established between the smaller company and 
another eferential expression in units 9, 8, 1, or 7, which 
correspond I(5 the DRA of mill 9 (unfls 9, 8, and 1) and to 
unit 7, the closest unit preceding unit 9 that is not  ill ils 
I)RA. 
For the l)iscottrse-VT-k models, we assume Ihat the 
Fxtended DRA (EDRA) of size \]c of a unit ~t. (EDRAz: ('~)) 
is given by the lh'st 1 _< k units of a sequence that 
lists, in reverse order, the units of the DRA of '~z plus 
the /c - l units that precede tt but arc not in its DRA. 
For example, \['or the text in Figure 1, the follow- 
ing relations hold: EDRAc,(!)) = 9; EDRAI(C.)) = 
9, 8; EDRA.,(9) = .q,8, I; EI)RA3(9) := 9 ,8 ,1 ,7 ;  
I'~DRA.,I(!)) = !),8, 1,7,6. For Linear-k inodels, the 
EDRAz:(u) is given by u and the k units that immedi- 
ately precede ~t. 
The potential p(M,  a, EDRA,~) (5t' a model M to de- 
termine correct co-referential links with respect o a ref- 
Erential expression a in unit u, given a corresponding 
EDRA of size k (EDRAt.(u)), is assigned the value 1 if 
the EDRA contains a co-referent from the same equiwt- 
lence class as a. Otherwise, p(M, ,, EDRAt~) is assigned 
the value O. The potential p(k4, 6', k) of a model M 
to determine correct co-rEferential links for all referen- 
tial expressions in a corpus of texts C, using EDRAs 
of size k, is computed as the SUlll oF the potentials 
p(M, a.,EI)RA#) of  all referential expressions ct in C'. 
This potential is normalized to a wdue bEtweEn 0 and 
1 by dividing p(k/l, C, k) by the number ot' referential 
211 
expressions in the corpus that have an antecedent. 
By examining the potential of each model to correctly 
determine co-referential expressions for each k, it is pos- 
sible to determine the degree to which an implementa- 
tion of a given approach can contribute to the overall 
efficiency of anaphora resolution systems. That is, if a 
given model has the potential to correctly determine a
significant percentage of co-referential expressions with 
small DRAs, an anaphora resolution system implement- 
ing that model will have to consider fewer options over- 
all. Hence, the probability of error is reduced. 
3.2.2 Results 
The graph in Figure 3 shows the potentials of the Linear- 
k and Discourse-VT-k models to correctly determine co- 
referential links for each k from 1 to 20. The graph in 
Figure 4 represents he same potefftials but focuses only 
on ks in the interval \[2,9\]. As these two graphs how, the 
potentials increase monotonically with k, the VT-k mod- 
els always doing better than the Linear-k models. Even- 
tually, for large ks, the potential performance of the two 
models converges to 100%. 
The graphs in Figures 3 and 4 also suggest resolution 
strategies for implemented systems. For example, the 
graphs suggests that by choosing to work with EDRAs 
of size 7, a discourse-based system has the potential of 
resolving more than 90% of the co-referential links in 
a text correctly. To achieve the same potential, a linear- 
based system needs to look back 8 units. I fa system does 
not look back at all and attempts to resolve co-referential 
links only within the unit under scrutiny (k = 0), it has 
the potential to correctly resolve about 40% of the co- 
referential links. 
To provide a clearer idea of how the two models differ, 
Figure 5 shows, for each k, the value of the Discourse- 
VT-k potentials divided by the value of the Linear-k po- 
tentials. For k = 0, the potentials of both models are 
equal because both use only the unit in focus in order to 
determine co-referential links. For k = 1, the Discourse- 
VT-1 model is about 7% better than the Linear-! model. 
As the value of k increases, the value Discourse-VT- 
k/Linear-k converges to 1. 
In Figures 6 and 7, we display the number of excep- 
tions, i.e., co-referential links that Discourse-VT-k and 
Linear-k models cannot determine correctly. As one 
can see, over the whole corpus, for each k _< 3, the 
Discourse-VT-k models have the potential to determine 
correctly about 100 more co-referential links than the 
Linear-k models. As k increases, the performance of the 
two models converges. 
3,2,3 Statistical significance 
In order to assess the statistical significance of the differ- 
ence between the potentials of the two models to estab- 
lish correct co-referential links, we carried out a Paired- 
Samples T Test for each k. In general, a Paired-Samples 
T Test checks whether the mean of casewise differences 
between two variables differs from 0. For each text in 
I O0 00% 
__~____x.~-.,:..-~'.. . . . . . .  
a0~? oo~??'~ ?- .~:..- ...... 
70.00% 
60 00% ~' 
5000% . 
40O0% ? 
o 
EDRA s i ze  
- - - -  VT-k  . . . . . . .  tmeaf .k  
Figure 3: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (0 < 
k < 20). 
9"5.00% 
90.00"/,, 
85.00% 
800O% 
75.(X~% 
70.007~ 
~ ~ . - ' " ' "  -"'"" --"" .... .. 
J 
1 2 3 4 5 6 7 9 
EDI1A Bt=~ 
? ---13---  VT-k - - . I f , , .  Linear-k 
Figure 4: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (2 < 
k _< 9). 
the corpus and each k, we determined the potentials of 
both VT-k and Linear-k models to establish correct co- 
referential links in that text. For ks smaller than 4, the 
difference in potentials was statistically significant. For 
example, for k = 3, t = 3.345, df = 29, P = 0.002. For 
values of k larger than or equal to 4, the difference was no 
longer significant. These results are consistent with the 
graphs shown in Figure 3 to 7, which all show that the 
potentials of Discourse-VT-k and Linear-k models con- 
verges to the same value as the value of k increases. 
3.3 Comparing the effort required to establish 
co-referential links 
3.3.1 Method 
The method escribed in section 3.2.1 estimates the po- 
tential of Linear-k and Discourse-VT-k models to deter- 
mine correct co-referential links by treating EDRAs as 
sets. However, from a computational perspective (and 
212 
L'07 / Q 
t .a6  '" 
. . . .  \[ / ", 
t l ' . ,  u 
0.9~ 
o.97 I 
~,oo I "' 
700 c , " ? 
"', o 
'500 " - . -  "~-~ 
400 \ ' - - ,  "'*x ? "--- 12~:~: ...... 
2OO 
~00 
2 3 4 5 6 7 t3 10 
L 
Figure 5: A direct comparison of Discourse-VT-k 
and Linear-VT-k potentials to correctly determine co- 
referential links (0 < k < 20). 
Figure 7: The number of co-referential links that caunot 
be correctly determined by Discourse-VT-k and Linear-k 
models (1 < k < 10). 
14130 
1200 
0 lOOO 
600 
401) 
200 
\ 
- - -~u ,~:  ?~:  :g~: \[t..:~g<_ .~, ~ g :~. . . t~_~.~_~_ , .=~r ,~ ~ 
EORA ~lz~ 
Figure 6: The number of co-referential links that cannot 
be correctly determined by Discourse-VT-k and Linear-k 
models (0 < /~' < 20). 
presumably, from a psycholinguistic perspective as well) 
it also makes sense to compare the effort required by the 
two classes of models to establish correct co-referential 
links. We estimate this effort using a very simple metric 
that assumes that the closer an antecedent is to a cor- 
responding referential expression in the EDRA, the bet- 
ter. Hence, in estimating the effort to establish a co- 
referential link, we treat EDRAs as ordered lists. For ex- 
ample, using the Linear-9 model, to determine the correct 
antecedent of the referential expression the smaller com- 
pany in unit 9 of Figure 1, it is necessary to search back 
through 4 units (to unit 5, which contains the referent Ge- 
netic Therapy). Had unit 5 been Ml: Cassey succeeds M. 
James Barrett, 50, we would have had to go back 8 units 
(to unit 1) in order to correctly resolve the RE the smaller 
company. In contrast, in the Discourse-VT-9 model, we 
go back only 2 units because unit 1 is two units away 
fi'om unit 9 (EDRA:~ (9) = 9, 8, 1,7, 6, 5,4, 3, 2). 
We consider that the effort e(AJ, a, EDRAa.) of a 
model M to determine correct co-referential links with 
respect o one referential, in unit u, given a correspond- 
ing EDRA of size L" (EDRA~.(,)) is given by the number 
of units between u and the first unit in EDRAk(u) that 
contains a co.-referential expression of a. 
The effort e(M, C, k) of a model M to determine cor- 
rect co-referential links for all referential expressions in 
a corpus of texts C using EDRAs of size k was computed 
as the sum of the efforts e(M, a, EDRAk) of all referen- 
tia ! expressions a in C. 
3.3.2 Results 
Figure 8 shows the Discourse-VT-k and Linear-k efforts 
computed over all referential expressions in the corpus 
and all ks. It is possible, for a given referent a and a 
given k, that no co-referential link exists in the units of 
the corresponding EDRAa.. In this case, we consider that 
the effort is equal to k. As a consequence, for small ks 
the effort required to establish co-referential links is sim- 
ilar for both theories, because both can establish only a 
limited number of links. However, as k increases, the 
effort computed over the entire corpus diverges dramat- 
ically: using the Discourse-VT model, the search space 
for co-referential links is reduced by about 800 units for a 
corpus containing roughly 1200 referential expressions. 
3.3.3 Statistical significance 
A Paired-Samples T Test was performed for each k. For 
each text in the corpus and each k, we determined the 
effort of both VT-k and Linear& models to establish cor- 
rect co-referential links in that text. For all ks the dif- 
ference in effort was statistically significant. For exam- 
ple, for k = T, we obtained the values t = 3.5l, df = 
29, P = 0.001. These results are intuitive: because 
EDRAs are treated as ordered lists and not as sets, the 
effect of the discourse structure on establishing correct 
co-referential links is not diminished as/,' increases. 
4 Conclusion 
We analyzed empirically the potentials of discourse and 
linear models of text to determine co-referential links. 
Our analysis suggests that by exploiting the hierarchi- 
cal structure of texts, one can increase the potential 
213 
7000 . . . . . . . . . .  i - ; ; ;~ : ' : - ' ; ;  . . . .  ~ . . . . .  
~ 0 0 -  
k . . . . . . . . . . . . . . . . . . . . . . . . . .  
I000 
tORA ~ z Q  
- - V T ~ e s s  . . . . . . .  ~ o s s  
Figure 8: The effort required by Linear-k and Discourse- 
VT-k models to determine correct co-referential links 
(0< h< 100). 
q 
of natural anguage systems to correctly determine co- 
referential links, which is a requirement for correctly re- 
solving anaphors. If one treats all discourse units in the 
preceding discourse qually, the increase is statistically 
significant only when a discourse-based coreference sys- 
tem looks back at most four discourse units in order to 
establish co-referential links. However, if one assumes 
that proximity plays an important role in establishing co- 
referential links and that referential expressions are more 
likely to be linked to referees that were used recently in 
discourse, the increase is statistically significant no mat- 
ter how many units a discourse-based co-reference sys~ 
tern looks back in order to establish co-referential links. 
Acknowledgements. We are grateful to Lynette 
Hirschman and Nancy Chinchor for making available 
their corpora of co-reference annotations. We are also 
grateful to Graeme Hirst for comments and feedback on 
a previous draft of this paper. 
References 
Saliha Azzam, Kevin Humphreys, and Robert 
Gaizauskas. 1998. Evaluating a focus-based ap- 
proach to anaphora resolution. In Proceedings of 
the 361h Ammal Meeting of the Associatiot~ for 
Computational Linguistics and of the 17th Inter- 
national Conference on Computational Linguistics 
(COLlNG/ACL'98), pages 74-78, Montreal, Canada, 
August 10-14. 
Dan Cristea, Nancy Ide, and Laurent Romary. 1998. 
Veins theory: A model of global discourse cohesion 
and coherence. In Proceedings of the 36th Ammal 
Meeting of the Association Jot" Computational Lin- 
guistics attd of the 17th lntertmtional Conference on 
Computational Linguistics (COLING/ACL'98), pages 
281-285, Montreal, Canada, August. 
Barbara Fox. 1987. Discourse Structure and Anaphora. 
Cambridge Studies in Linguistics; 48. Cambridge Uni- 
versity Press. 
Niyu Oe, John Hale, and Eugene Charniak. 1998. A sta- 
tistical approach to anaphora resolution. In Proceed- 
ings of the Sixth Worksho t) on Vet 3, Large Corpora, 
pages 161-170, Montreal, Canada, August 15-16. 
Barbara J. Grosz and Candace L. Sidner. 1986. At- 
tention, intentions, and the structure of discourse. 
Computational Linguistics, 12(3): 175-204, July- 
September. 
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 
1995. Centering: A framework tbr modeling the lo- 
cal coherence of discourse. Computational Linguis- 
tics, 21 (2):203-226, June. 
Lynette Hirschman and Nancy Chinchor, 1997. MUC-7 
Coreference Task Definition, July 13. 
Janet Hitzeman and Massimo Poesio. 1998. Long dis- 
tance pronominalization a d global focus. In Ptv- 
ceedings of the 36th Ammal Meeting of the Associ- 
ation for Computational Linguistics attd of the 17th 
hzternational Conference on Computational Linguis- 
tics (COLING/ACL'98), pages 550-556, Montreal, 
Canada, August. 
Jerry H. Hobbs. 1978. Resolving pronoun references. 
Lingua, 44:311-338. 
Megumi Kameyama. 1997. Recognizing referential 
links: An information extraction perspective. In Pro- 
ceedings of the ACL/EACL'97 Workshop on Opera- 
tional Factors in Practical, Robust Anaphora Resoht- 
tion, pages 46-53. 
Shalom Lappin and Herbert J. Leass. 1994, An algo- 
rithm for pronominal anaphora resolution. Computa- 
tional Linguistics, 20(4):535-561. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional the- 
ory of text organization. Text, 8(3):243-28 i. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments in constructing a cor- 
pus of discourse trees. In Ptvceedings of the ACL'99 
Worksho t)on Standards and 7bols for Discout:re Tag- 
ging, pages 48-57, University of Maryland, June 22. 
Ruslan Mitkov. 1997. Factors in anaphora resolution: 
They are not the only things that matter, a case study 
based on two different approaches. In Proceedings of 
the ACL/EACL'97 Workshop on Operational Factors 
in Practical, Robust Anaphora Resolution, pages 14- 
21. 
Candace L. Sidner. 1981. Focusing for interpretation f
pronouns. Computational Linguistics, 7(4):217-231, 
October-December. 
Wietske Vonk, Lettica G.M.M. Hustinx, and Wire tt.G. 
Simons. 1992. The use of referential expressions in 
structuring discourse, l~mguage and Cognitive Pro- 
cesses, 7(3,4):301-333. 
214 
A Hierarchical Account of Referential Accessibility
Nancy IDE
Department of Computer Science
Vassar College
Poughkeepsie, New York 12604-0520 USA
ide@cs.vassar.edu
Dan CRISTEA
Department of Computer Science
University ?Al. I. Cuza?
Iasi, Romania
dcristea@infoiasi.ro
Abstract
In this paper, we outline a theory of
referential accessibility called Veins
Theory (VT). We show how VT addresses
the problem of "left satellites", currently a
problem for stack-based models, and show
that VT can be used to significantly reduce
the search space for antecedents. We also
show that VT provides a better model for
determining domains of referential
accessibility, and discuss how VT can be
used to address various issues of structural
ambiguity.
Introduction
In this paper, we outline a theory of referential
accessibility called Veins Theory (VT). We
compare VT to stack-based models based on
Grosz and Sidner's (1986) focus spaces, and
show how VT addresses the problem of "left
satellites", i.e., subordinate discourse segments
that appear prior to their nuclei (dominating
segments) in the linear text. Left-satellites pose
a problem for stack-based models, which
remove subordinate segments from the stack
before pushing a nuclear or dominating
segment, thus rendering them inaccessible. The
percentage of such cases is typically small,
which may account for the fact that their
treatment has been largely overlooked in the
literature, but the phenomenon nonetheless
persists in most texts. We also show how VT
can be used to address various issues of
structural ambiguity.
1 Veins Theory
Veins Theory (VT) extends and formalizes the
relation between discourse structure and
reference proposed by Fox (1987). VT
identifies ?veins? over discourse structure trees
that are built according to the requirements put
forth in Rhetorical Structure Theory (RST)
(Mann and Thompson, 1987). RST structures
are represented as binary trees, with no loss of
information. Veins are computed based on the
RST-specific distinction between nuclei and
satellites; therefore, RST relations labeling
nodes in the tree are ignored. Terminal nodes
in the tree represent discourse units and non-
terminal nodes represent discourse relations.
The fundamental intuition underlying VT is
that the distinction between nuclei and
satellites constrains the range of referents to
which anaphors can be resolved; in other
words, the nucleus-satellite distinction induces
a domain of referential accessibility (DRA) for
each referential expression. More precisely, for
each anaphor a in a discourse unit u , VT
hypothesizes that a can be resolved by
examining referential expressions that were
used in a subset of the discourse units that
precede u; this subset is called the DRA of u.
For any elementary unit u in a text, the
corresponding DRA is computed automatically
from the text's RST tree  in two steps:
1. Heads for each node are computed bottom-
up over the rhetorical representation tree.
Heads of elementary discourse units are
the units themselves. Heads of internal
nodes, i.e., discourse spans, are computed
by taking the union of the heads of the
immediate child nodes that are nuclei. For
example, for the text in Figure 1,1 with the
rhetorical structure shown in Figure 2,2 the
head of span [5,7] is unit 5. Note that the
head of span [6,7] is the list <6,7> because
both immediate children are nuclei.
2 .  Using the results of step 1, Vein
expressions are computed top-down for
each node in the tree, using the following
functions:
-  mark (x), which returns each symbol in a
string of symbols x marked with
parentheses.
-  seq(x,y), which concatenates the labels in
x with the labels in y, left-to-right.
-  simpl(x), which eliminates all marked
symbols from x, if they exist.
The vein of the root is its head. Veins of
child nodes are computed recursively, as
follows:
?  for each nuclear node whose parent
has vein v, if the node has a left non-
nuclear sibling with head h, then the
vein expression is seq(mark(h), v);
otherwise v.
? for each non-nuclear node with head h
whose parent node has vein v, if the
node is the left child of its parent, then
seq(h,v); otherwise, seq(h, simpl(v)).
                                                       
1
 Figure 1 highlights two co-referential equivalence
classes: referential expressions surrounded by
boxes refer to ?Mr. Casey?; those surrounded by
ellipses refer to ?Genetic Therapy Inc.?.
2
 The rhetorical structure is represented using the
conventions proposed by Mann and Thompson
(1988).
One of the conjectures of VT is that the vein
expression of a unit (terminal node), which
includes a chain of discourse units that contain
that unit itself, provides an ?abstract? or
summary of the discourse fragment that
contains that unit. Because it is an internally
coherent piece of discourse, all referential
expressions (REs) in the unit preferentially
find their referees within that sub-text.
Referees that do not appear in the DRA are
possible, but are more difficult to process, both
computationally and cognitively (see Section
2.2). This conjecture expresses the intuition
that potential referees of the REs of a unit
depend on the nuclearity of previous units:
both a satellite and a nucleus can access a
previous nuclear node, a nucleus can only
access another left nuclear node or its own left
satellite, and the interposition of a nucleus
after a satellite blocks the accessibility of the
satellite for any nodes lower in the hierarchy.
1. Michael D. Casey, a top Johnson & Johnson
manager, moved to Genetic Therapy Inc., a
small biotechnology concern here,
2. to become its president and chief operating
officer
3. Mr. Casey, 46, years old, was president of
J&J?s McNeil Pharmaceutical subsidiary,
4. which was merged with another J&J unit,
Ortho Pharmaceutical Corp., this year in a
cost-cutting move.
5. Mr. Casey succeeds M. James Barrett, 50, as
president of Genetic Therapy.
6. Mr. Barrett remains chief executive officer
7. and becomes chairman.
8. Mr. Casey said
9. he made the move to the smaller company
10. because he saw health care moving toward
technologies like the company?s gene
therapy products.
11. I believe that the field is emerging and is
prepared to break loose,
12. he said.
Figure 1: MUC corpus text fragment
The DRA of a unit u is given by the units in
the vein that precede u. For example, for the
text and RST tree in Figures 1 and 2, the vein
expression of unit 3, which contains units 1
and 3, suggests that anaphors from unit 3
should be resolved only to referential
expressions in units 1 and 3. Because unit 2 is
a satellite to unit 1, it is considered to be
?blocked? to referential links from unit 3. In
contrast, the DRA of unit 9, consisting of units
1, 8, and 9, reflects the intuition that anaphors
from unit 9 can be resolved only to referential
expressions from unit 1, which is the most
important unit in span [1,7] and to unit 8, a
satellite that immediately precedes unit 9.
Figure 2 shows the heads and veins of all
internal nodes in the rhetorical representation.
In general, co-referential relations (such as the
identity relation) induce equivalence classes
over the set of referential expressions in a text.
When hierarchical adjacency is considered, an
anaphor may be resolved to a referent that is
not the closest in a linear interpretation of a
text. However, because referential expressions
are organized in equivalence classes, it is
sufficient that an anaphor is resolved to some
member of the set. This is consistent with the
distinction between "direct" and "indirect"
references discussed in (Cristea, et al, 1998).
1 2 3 4
5
6 7 8
9
10
11 12
13-?
??-?
H = 1 9 *
V = 1 9 *
H = 1
V = 1 9 *
H = 9
V = 1 9 *
H = 1
V = 1 9 *
H = 5
V = 1 5 9 *
H = 1
V = 1 9 *
H = 3
V = 1 3 5 9 *
H = 6 7
V = 1 5 6 7 9 *
H = 9
V = 1 9 * 
H = 9
V = 1 9 *
H = 9
V = 1 (8) 9 *
H = 10
V = 1 9 10 *
H = 11
V = 1 9 10 11 *H = 3
V = 1 3 5 9
DRA  = 1 3 H = 9
V = 1 (8) 9
DRA  = 1 8 9 
Figure 2: RST analysis of the text in Figure 1
2 VT and Stack-based Models
Veins Theory claims that references from a
given unit are possible only in its DRA, i.e., that
discourse structure constrains the areas of the
text over which references can be resolved. In
previous work, we compared the potential of
hierarchical and linear models of discourse--i.e.,
approaches that enumerate potential antecedents
in an undifferentiated window of text linearly
preceding the anaphor under scrutiny--to
correctly establish co-referential links in texts,
and hence, their  potential to correctly resolve
anaphors (Cristea, et al, 2000). Our results
showed that by exploiting the hierarchical
discourse structure of texts, one can increase the
potential of natural language systems to correctly
determine co-referential links, which is a
requirement for correctly resolving anaphors. In
general, the potential to correctly determine co-
referential links was greater for VT than for
linear models when one looks back 4 elementary
discourse units. When looking back more than
four units, the linear model was equally
effective.
Here, we compare VT to stack-based models of
discourse structure based on Grosz and Sidner's
(1986) (G&S) focus spaces (e.g., Hahn and
Str?be, 1997; Azzam, et al, 1998). In these
approaches, discourse segments are pushed on
the stack as they are encountered in a linear
traversal of the text. Before a dominating
segment is pushed, subordinate segments that
precede it are popped from the stack.
Antecedents for REs appearing in the segment
on the top of the stack are sought in discourse
segments in the stack below it. Therefore, in
cases where a subordinate segment a precedes a
dominating segment b, a reference to an entity in
a by an RE in b is not resolvable. Special
provision could be made in order to handle such
cases?e.g., subsequently pushing a on top of
b?but this would violate the overall strategy of
resolving REs appearing in segments currently
on the top of the stack.
The special status given to left satellites in VT
addresses this problem. For example, one RST
analysis of (1) proposed by Moser and Moore
(1996) is given in Figure 3. Moser and Moore
note that the relation of an RST nucleus to its
satellite is analogous to the dominates relation
proposed by G&S (see also Marcu, 2000). As a
subordinate segment preceding the segment that
dominates it, the satellite is popped from the
stack before the dominant segment (the nucleus)
is pushed in the stack-based model, and therefore
it is not included among the discourse segments
that are searched to resolve co-references.3
Similarly, the text in (2), taken from the MUC
annotated corpus (Marcu, et al, 1999), was
assigned the RST structure in Figure 4, which
presents the same problem for the stack-based
approach: the referent for this  in C2 is to the
Clinton program in A2, but because it is a
subordinate segment, it is no longer on the stack
when C2 is processed.
(1) A1. George Bush supports big business.
B1. He's sure to veto House Bill 1711.
Figure 3: RST analysis of (1)
                                                       
3
 Note that Moser and Moore (1996) also propose an
informational RST structure for the same text, in
which a ??volitional-cause?? relation holds between
the nucleus a  and the satellite b, thus providing for a
to be on the stack when b is processed.
 (2) A2. Some of the executives also signed letters on
behalf of the Clinton program.
B2. Nearly all of them praised the president for
his efforts to pare the deficit.
C2. This is not necessarily the package I would
design,
D2. said Martin Marietta's Mr. Augustine.
E2. But we have to attack the deficit.
Figure 4: RST analysis of (2)
2.1 Validation
To validate our claim, we examined 23
newspaper texts with widely varying lengths
(mean length = 408 words, standard deviation
376). The texts were annotated manually for co-
reference relations of identity (Hirschman and
Chinchor, 1997). The co-reference relations
define equivalence relations on the set of all
marked references in a text. The texts were also
annotated manually with discourse structures
built in the style of Mann and Thompson (1988).
Each analysis yielded an average of 52
elementary discourse units. Details of the
annotation process are given in (Marcu et al,
1999).
Six percent of all co-references in the corpus are
to left satellites. If only co-references pointing
outside the unit in which they appear (inter-unit
references) are considered, the rate increases to
7.76%. Among these cases, two possibilities
exist: either the reference is unresolvable using
the stack-based method because the unit in
which the referent appears has been popped from
the stack, or the stack-based algorithm finds a
correct referent in an earlier unit that is still on
the stack. Twenty-two percent (2.38% of all co-
referring expressions in the corpus) of the
referents that VT finds in left satellites fall into
B1A1
evidence
A2-B2
background
elaboration-addition
A2 B2
C2-D2-E2
antithesis
C2-D2
attribution
C2 D2
E2
the first category. For example, in text fragment
(3), taken from the MUC corpus, the co-
referential equivalence class for the pronoun he
in C3 includes Saloman Brothers analyst Jeff
Canin in B3 and he in A3. The RST analysis of
this fragment in Figure 5 shows that both A3 and
B3 are left satellites. A stack-based approach
would not find either antecedent for he in C3,
since both A3 and B3 are popped from the stack
before C3 is processed.
(3) A3. Although the results were a little lighter than
the 49 cents a share he hoped for,
B3. Salomon Brothers analyst Jeff Canin said
C3. he was pleased with Sun's gross margins for
the quarter.
Figure 5: RST analysis of (3)
In cases where stack-based approaches find a co-
referent (although not the most recent
antecedent) elsewhere in the stack, it makes
sense to compare the effort required by the two
models to establish correct co-referential links.
That is, we assume that from a computational
perspective (and, presumably a psycholinguistic
one as well), the closer an antecedent is to the
referential expression to be resolved, the better.
We have shown elsewhere (Cristea et al, 2000)
that VT, compared to linear models, requires
significantly less effort for DRAs of any size.
We use a similar strategy here to compute the
effort required by VT and stack-based models.
DRAs for both models are treated as ordered
lists. For example, text fragment (4) reflects the
set of units on the stack at a given point in
processing one of the MUC texts; units D4 and
E4, in brackets, are left satellites and therefore
not available using the stack-based model, but
visible using VT. To determine the correct
antecedent of Mr. Clinton in F4 using the stack-
based model, it is necessary to search back
through 3 units (C4, B4, A4) to find the referent
President Clinton. In contrast, using VT, we
search back only 1 unit to D4.
 (4) A4. A group of top corporate executives urged
Congress to pass President Clinton's deficit-
reduction plan,
B4. declaring that it is superior to the only
apparent alternative: more gridlock.
C4. Some of the executives who attended
yesterday's session weren't a surprise.
  [ D4. Tenneco Inc. Chairman Michael Walsh, for
instance, is a staunch Democrat who
provided an early endorsement for Mr.
Clinton during the presidential campaign.
E4. Xerox Corp.'s Chairman Paul Allaire was
one of the few top corporate chief executive
officers who contributed money to the
Clinton campaign.   ]
F4. And others, such as Atlantic Richfield Co.
Chairman Lodwrick M. Cook and Zenith
Electronics Corp. Chairman Jerry Pearlman,
have also previously voiced their approval of
Mr. Clinton's economic strategy.
We compute the effort e(M,a,DRAk) of a model
M to determine correct co-referential links with
respect to a referential expression a in unit u,
given a DRA of size k (DRAk(u)) is given by the
number of units between u and the first unit in
DRAk that contains a co-referential expression of
a. The effort e(M,C,k) of a model M to determine
correct co-referential links for all referential
expressions in a corpus of texts C using DRAs of
size k is computed as the sum of the efforts
e(M,a,DRAk) of all referential expressions a
where VT finds the co-reference of a in a left
satellite. Since co-referents found in units that
are not left satellites will be identical for both
VT and stack-based models, the difference in
effort between the two models depends only on
co-referents found in left satellites.
Figure 6 shows the VT and stack-based efforts
computed over referential expressions resolved
by VT in left satellites and k = 1 to 12.
Obviously, for a given k and a given referent a,
that no co-reference exists in the units of the
corresponding DRAk  In these cases, we consider
B3-C3
attribution
concession
A3
B3 C3
the effort to be equal to k. As a result, for small k
the effort required to establish co-referential
links is similar for both models, because both
can establish only a limited number of links.
However, as k increases, the effort computed
over the entire corpus diverges, with VT
performing consistently better than the stack-
based model.
Figure 6: Effort required by VT and stack-based
models
Note that in some cases, the stack-based model
performs better than VT, in particular for small
k. This occurs when VT searches back through n
adjacent left satellites, where n > 1, to find a co-
reference, but a co-referent is found using the
stack-based method by searching back m non-
left satellite units, where m < n. This would be
the case, if for instance, VT first found a co-
referent for Mr. Clinton In text (4) in D4 (2 units
away), but the stack-based model found a co-
referent in C4 (1 unit away since the left
satellites are not on the stack).
In our corpus, 15% of the co-references found in
left satellites by VT required less effort using the
stack-based method, whereas VT out-performed
the stack-based method 23% of the time. In the
majority of cases (62%), the two models
required the same level of effort. However, all of
the cases in which the stack-based model
performed better are for small k (k<4), and the
average difference in distance (in units) is 1.25.
In contrast, VT out-performs the stack-based
model for cases ranging over all values of k in
our experiment (1 to 12), and the average
difference in distance is 3.8 units. At k=4, VT
can determine all the co-referents in our corpus,
whereas the stack-based model requires DRAs of
up to 12 units to resolve them all. This accounts
for the marked divergence in effort shown in
Figure 6 as k  increases. So, despite the minor
difference in the percentage of cases where VT
out-performs the stack-based model, VT has the
potential to significantly reduce the search space
for co-referential links.
2.2 Exceptions
We have also examined the exceptions, i.e., co-
referential links that VT and stack-based models
cannot determine correctly. Because of the
equivalence of the stack contents for left-
balanced discourse trees, there is no case in
which the stack-based model finds a referent
where VT does not. There is, however, a number
of referring expressions for which neither VT
nor the stack-based model finds a co-referent. In
the corpus of MUC texts we consider, 12.3% of
inter-unit references fall into this category, or
9.3% of the references in the corpus if we
include intra-unit references.
Table 1 provides a summary of the types of
referring expressions for which co-referents are
not found in our corpus?i.e., no antecedent
exists, or the antecedent appears outside the
DRA.4 We show the percentage of REs in our
corpus for which VT (and the stack-based model
as well, since all units in the DRA computed
according to VT are in the DRA computed using
the stack-based model) fails to find an
antecedent, and the percentage of REs for which
VT finds a co-referent (in a left satellite) but the
stack-based model does not.
                                                       
4
 Our calculations are made based on the RST
analysis of the MUC data, in which we detected a
small number of structural errors. Therefore, the
values given here are not absolute but rather provide
an indication of the relative distribution of RE types.
0
2 0
4 0
6 0
8 0
100
120
1 2 3 4 5 6 7 8 9 1 0 1 1 1 2
DRA length (k)
Nu
m
be
r 
of
 c
o-
re
fs
Stack
VT
We consider four types of REs:
(1) Pragmatic references, which refer to entities
that can be assumed part of general
knowledge, such as the Senate, the key in the
phrase lock them up and throw away the key,
or our in the phrase our streets.
(2) Proper nouns, such as Mr. Gerstner or
Senator Biden.
(3) Common nouns, such as the steelmaker, the
proceeds, or the top job.
(4) Pronouns
Following (Gundel, et al, 1993), we consider
that the evoking power of each of these types of
REs decreases as we move down the list. That is,
pragmatic references are easily understood
without an antecedent; proper nouns and noun
phrases less so, and are typically resolved by
inference over the context. On the other hand,
pronouns have very poor evoking power, and
therefore a message emitter employs them only
when s/he is certain that the structure of the
discourse allows for easy recuperation of the
antecedent in the message receiver's memory.5
Except for the cases where a pronoun can be
understood without an antecedent (e.g., our in
our streets), it is virtually impossible to use a
pronoun to refer to an antecedent that is outside
the DRA.
Type of RE VT Stack-based
pragmatic 56.3% 0.0%
proper nouns 22.7% 26.1%
common nouns 16.0% 39.1%
pronouns 5.0% 34.8%
Table 1: Exceptions for VT and stack-based models
The alignment of the evoking power of
referential expressions with the percentage of
exceptions for both models shows that the
predictions made by VT relative to DRAs are
fundamentally correct--that is, their prevalence
corresponds directly to their respective evoking
                                                       
5
 Ideally, a psycho-linguistic study of reading times to
verify the claim that referees outside the DRA are
more difficult to process would be in order.
powers. On the other hand, the almost equal
distribution of exceptions over RE types for the
stack-based model shows that it is less reliable
for determining DRAs.
Note that in all VT exceptions for pronouns, the
RST attribution relation is involved. Text
fragment (5) and the corresponding RST tree
(Figure 7) shows the typical case:
(5) A5. A spokesman for the company said,
B5. Mr. Bartlett?s promotion reflects the current
emphasis at Mary Kay on international
expansion.
C5. Mr. Bartlett will be involved in developing
the international expansion strategy,
D5. he said
The antecedent for he in D5 is a spokesman for
the company in A5, which, due to the nuclear-
satellite relations, is inaccessible on the vein.
Our results suggest that annotation of attributive
relations needs to be refined, possibly by treating
X said and the attributed quotation as a single
unit. If this were done, the vein expression
would allow appropriate access.
Figure 7: RST analysis of (5)
2.3 Summary
In sum, VT provides a more natural account of
referential accessibility than the stack-based
model. In cases where the discourse structure is
not left-polarized, at least one satellite precedes
its nucleus in the discourse and is therefore its
left sibling in the binary discourse tree. The vein
definition formalizes the intuition that in a
sequence of units a b c, where a and c  are
satellites of b, b can refer to entities in a (its left
satellite), but the subsequent right satellite, c,
cannot refer to a due to the interposition of
nuclear unit b--or, if such a reference exists, it is
A5-B5
elaboration
attribution
A5 B5
C5-D5
attribution
D5C5
harder to process. In stack-based approaches to
referentiality, such configurations pose
problems: because b dominates a, in order to
resolve potential references from b to a, b must
appear below a on the stack even though it is
processed after a. Even if the processing
difficulties are overcome, this situation leads to
the postulation of cataphoric references when a
satellite precedes its nucleus, which is counter-
intuitive.
3 VT and Structural Ambiguity
The fact that VT considers only the nuclear-
satellite distinction and ignores rhetorical
labeling has practical ramifications for anaphora
resolution systems that rely on discourse
structure to determine the DRA for a given RE.
(Marcu, et al, 1999) show that over a corpus of
texts drawn from MUC newspaper texts, the
Wall Street Journal corpus, and the Brown
Corpus, reliable agreement among annotators is
consistently obtained for discourse segmentation
and assignment of nuclear-satellite status, while
agreement on rhetorical labeling was less
reliable (statistically significant for only the
MUC texts). This means that even when there
exist differences in rhetorical labeling, vein
expressions can be computed and used to
determine DRAs.
VT also has ramifications for evaluating the
viability of different structural representations
for a given text, at least for the purposes of
reference resolution. Like syntactic parsing,
discourse parsing typically yields several
interpretations, and one of the a priori tasks for
further analysis of the parsed texts is to choose
one from among potentially several alternative
structures. Marcu (1996) showed that using only
rhetorical relations, as many as five different
structures can be identified for some texts.
Considering intention-based relations can yield
even more alternatives. For anaphora resolution,
the choice of one structure over another may
have significant impact. For example, an RST
tree for (6) using rhetorical relations is given in
Figure 8; Figure 9 shows another RST tree for
the same text, using intention-based relations. If
we compute the vein expressions for both
representations, we see that the vein for segment
C6 in the intentional representation is <A6 B6
C6>, whereas in the rhetorical representation, the
vein is <(B6), C6>. That is, under the constraints
imposed by VT, John  is not available as a
referent for he in C6 in the rhetorical version,
although J o h n  is clearly the appropriate
antecedent. Interestingly, the intention-based
analysis is skewed to the right and thus is a
"better" representation according to the criteria
outlined in (Marcu, 1996); it also eliminates the
left-satellite that was shown to pose problems for
stack-based approaches. It is therefore likely that
the intention-based analysis is "better" for the
purposes of anaphora resolution.
(6) A6.  Tell John to bring the car home by 5.
B6. That way I can get to the store before it
closes.
C6. Then he can finish the bookshelves tonight.
Figure 8: RST tree for text (6), using rhetorical
relations
Figure 9: RST tree for text (6), using intention-based
relations
Conclusion
Veins Theory is based on established notions of
discourse structure: hierarchical organization, as
in the stack-based model and RST's tree
structures, and dominance or nuclear/satellite
motivation
B6-C6
motivation
B6 C6
A6
A6-B6
condition
condition
A6 B6
C6
relations between discourse segments. As such,
VT captures and formalizes intuitions about
discourse structure that run through the current
literature. VT also explicitly recognizes the
special status of the left satellite for discourse
structure, which has not been adequately
addressed in previous work.
In this paper we have shown how VT addresses
the left satellite problem, and how VT can be
used to address various issues of structural
ambiguity. VT predicts that references not
resolved in the DRA of the unit in which it
appears are more difficult to process, both
computationally and cognitively; by looking at
cases where VT fails we determine that this
claim is justified. By comparing the types of
referring expressions for which VT and the
stack-based model fail, we also show that VT
provides a better model for determining DRAs.
Acknowledgements
We thank Daniel Marcu for providing us with
the RST annotated MUC corpus, and Valentin
Tablan for developing part of the software that
enabled us to process the data.
References
Azzam S., Humphreys K. and Gaizauskas R.
(1998). Evaluating a Focus-Based Approach to
Anaphora Resolution. Proceedings of
COLING-ACL?98, 74-78.
Cristea D., Ide N., and Romary L. (1998). Veins
Theory: A Model of Global Discourse
Cohesion and Coherence. Proceedings of
COLING-ACL?98, 281-285.
Cristea D., Ide N., Marc, D., and Tablan V.
(2000).  An Empirical Investigation of the
Relation Between Discourse Structure and Co-
Reference. Proceedings of COLING 2000,
208-214.
Fox B. (1987). Discourse Structure and
Anaphora. Written and Conversational
English. No 48 in Cambridge Studies in
Linguistics, Cambridge University Press.
Grosz B. and Sidner C. (1986). Attention,
Intention and the Structure of Discourse.
Computational Linguistics, 12, 175-204.
Gundel J., Hedberg N. and Zacharski R.  (1993).
Cognitive Status and the Form of Referring
Expressions. Language, 69:274-307.
Hahn U. and Str?be M. (1997). Centering in-the-
large: Computing referential discourse
segments. Proceedings of ACL-EACL?97, 104-
111.
Hirschman L. and Chinchor N. (1997). MUC-7
Co-reference Task Definition.
Mann, W.C. and Thompson S.A. (1988).
Rhetorical structure theory: A theory of text
organization, Text, 8:3, 243-281.
Marcu D., Amorrortu E. and Romera M. (1999).
Experiments in Constructing a Corpus of
Discourse Trees. Proceedings of the ACL?99
Workshop on Standards and Tools for
Discourse Tagging.
Marcu D. (2000). Extending a Formal and
Computational Model of Rhetorical Structure
Theory with Intentional Structures ? la Grosz
and Sidner. Proceedings of COLING 2000,
523-29.
Marcu D. (1999). A Formal and Computational
Synthesis of Grosz and Sidner's and Mann and
Thompson's theories. Workshop on Levels of
Representation in Discourse, 101-108.
Marcu D. (1996). Building Up Rhetorical
Structure Trees. Proceedings of the Thirteenth
National Conference on Artificial Intelligence,
vol. 2, 1069-1074.
Moser M. and Moore J. (1996). Towards a
Synthesis of Two Accounts of Discourse
Structure. Computational Linguistics, 18(4):
537-544.
Sidner C. (1981). Focusing and the Interpretation
of Pronouns. Computational Linguistics,
7:217-231.
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 6?10,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
 
 
Harnessing NLP Techniques in the Processes of  
Multilingual Content Management 
 
 
Anelia Belogay Diman Karagyozov 
Tetracom IS Ltd. Tetracom IS Ltd. 
anelia@tetracom.com diman@tetracom.com 
Svetla Koeva Cristina Vertan 
Institute for Bulgarian Language Universitaet Hamburg 
svetla@dcl.bass.bg cristina.vertan@uni-hamburg.de 
Adam Przepi?rkowski Polivios Raxis 
Instytut Podstaw Informatyki Polskiej 
Akademii Nauk 
Atlantis Consulting SA 
adamp@ipipan.waw.pl raxis@atlantisresearch.gr 
Dan Cristea  
Universitatea Alexandru Ioan Cuza  
dcristea@info.uaic.ro  
 
 
Abstract 
The emergence of the WWW as the main 
source of distributing content opened the 
floodgates of information. The sheer 
volume and diversity of this content 
necessitate an approach that will reinvent 
the way it is analysed. The quantitative 
route to processing information which 
relies on content management tools 
provides structural analysis. The 
challenge we address is to evolve from 
the process of streamlining data to a level 
of understanding that assigns value to 
content. 
We present an open-source multilingual 
platform ATALS that incorporates 
human language technologies in the 
process of multilingual web content 
management. It complements a content 
management software-as-a-service 
component i-Publisher, used for creating, 
running and managing dynamic content-
driven websites with a linguistic 
platform. The platform enriches the 
content of these websites with revealing 
details and reduces the manual work of 
classification editors by automatically 
categorising content. The platform 
ASSET supports six European languages. 
We expect ASSET to serve as a basis for 
future development of deep analysis tools 
capable of generating abstractive 
summaries and training models for 
decision making systems. 
Introduction 
The advent of the Web revolutionized the way in 
which content is manipulated and delivered. As a 
result, digital content in various languages has 
become widely available on the Internet and its 
sheer volume and language diversity have 
presented an opportunity for embracing new 
methods and tools for content creation and 
distribution. Although significant improvements 
have been made in the field of web content 
management lately, there is still a growing 
demand for online content services that 
incorporate language-based technology. 
Existing software solutions and services such 
as Google Docs, Slingshot and Amazon 
implement some of the linguistic mechanisms 
addressed in the platform. The most used open-
source multilingual web content management 
6
  
systems (Joomla, Joom!Fish, TYPO3, Drupal)1 
offer low level of multilingual content 
management,   providing abilities for building 
multilingual sites. However, the available 
services are narrowly focused on meeting the 
needs of very specific target groups, thus leaving 
unmet the rising demand for a comprehensive 
solution for multilingual content management 
addressing the issues posed by the growing 
family of languages spoken within the EU. 
We are going to demonstrate the open-source 
content management platform ATLAS and as 
proof of concept, a multilingual library i-
librarian, driven by the platform. The 
demonstration aims to prove that people reading 
websites powered by ATLAS can easily find 
documents, kept in order via the automatic 
classification, find context-sensitive content, find 
similar documents in a massive multilingual data 
collection, and get short summaries in different 
languages that help the users to discern essential 
information with unparalleled clarity. 
The ?Technologies behind the system? chapter 
describes the implementation and the integration 
approach of the core linguistic processing 
framework and its key sub-components ? the 
categorisation, summarisation and machine-
translation engines. The chapter ?i-Librarian ? a 
case study? outlines the functionalities of an 
intelligent web application built with our system 
and the benefits of using it. The chapter 
?Evaluation? briefly discusses the user 
evaluation of the new system. The last chapter 
?Conclusion and Future Work? summarises the 
main achievements of the system and suggests 
improvements and extensions. 
Technologies behind the system 
The linguistic framework ASSET employs 
diverse natural language processing (NLP) tools 
technologically and linguistically in a platform, 
based on UIMA 2 . The UIMA pluggable 
component architecture and software framework 
are designed to analyse content and to structure 
it. The ATLAS core annotation schema, as a 
uniform representation model, normalizes and 
harmonizes the heterogeneous nature of the NLP 
tools3. 
                                                          
1 http://www.joomla.org/, http://www.joomfish.net/, 
http://typo3.org/, http://drupal.org/ 
2 http://uima.apache.org/ 
3 The system exploits heterogeneous NLP tools, for 
the supported natural languages, implemented in Java, 
C++ and Perl. Examples are: 
The processing of text in the system is split 
into three sequentially executed tasks. 
Firstly, the text is extracted from the input 
source (text or binary documents) in the ?pre-
processing? phase.  
Secondly, the text is annotated by several NLP 
tools, chained in a sequence in the ?processing? 
phase. The language processing tools are 
integrated in a language processing chain (LPC), 
so that the output of a given NLP tool is used as 
an input for the next tool in the chain. The 
baseline LPC for each of the supported languages 
includes a sentence and paragraph splitter, 
tokenizer, part of speech tagger, lemmatizer, 
word sense disambiguation, noun phrase chunker 
and named entity extractor (Cristea and Pistiol, 
2008). The annotations produced by each LPC 
along with additional statistical methods are 
subsequently used for detection of keywords and 
concepts, generation of summary of text, multi-
label text categorisation and machine translation.  
Finally, the annotations are stored in a fusion 
data store, comprising of relational database and 
high-performance Lucene4 indexes. 
The architecture of the language processing 
framework is depicted in Figure 1. 
 
 
 
Figure 1. Architecture and communication channels in 
our language processing framework. 
 
The system architecture, shown in Figure 2, is 
based on asynchronous message processing 
                                                                                        
OpenNLP (http://incubator.apache.org/opennlp/), 
RASP (http://ilexir.co.uk/applications/rasp/), 
Morfeusz (http://sgjp.pl/morfeusz/),  Panterra 
(http://code.google.com/p/pantera-tagger/), ParsEst 
(http://dcl.bas.bg/), TnT Tagger (http://www.coli.uni-
saarland.de/~thorsten/tnt/). 
4 http://lucene.apache.org/ 
7
  
patterns (Hohpe and Woolf, 2004) and thus 
allows the processing framework to be easily 
scaled horizontally. 
 
 
 
Figure 2. Top-level architecture of our CMS and its 
major components. 
Text Categorisation 
We implemented a language independent text 
categorisation tool, which works for user-defined 
and controlled classification hierarchies. The 
NLP framework converts the texts to a series of 
natural numbers, prior sending the texts to the 
categorisation engine. This conversion allows 
high level compression of the feature space. The 
categorisation engine employs different 
algorithms, such as Na?ve Bayesian, relative 
entropy, Class-Feature Centroid (CFC) (Guan et. 
al., 2009), and SVM. New algorithms can be 
easily integrated because of the chosen OSGi-
based architecture (OSGi Alliance, 2009). A 
tailored voting system for multi-label multi-class 
tasks consolidates the results of each of the 
categorisation algorithms. 
Summarisation (prototype phase) 
The chosen implementation approach for 
coherent text summarisation combines the well-
known LexRank algorithm (Erkan and Radev, 
2004) and semantic graphs and word-sense 
disambiguation techniques (Plaza and Diaz, 
2011). Furthermore, we have automatically built 
thesauri for the top-level domains in order to 
produce domain-focused extractive summaries. 
Finally, we apply clause-boundaries splitting in 
order to truncate the irrelevant or subordinating 
clauses in the sentences in the summary.  
Machine Translation (prototype phase) 
The machine translation (MT) sub-component 
implements the hybrid MT paradigm, combining 
an example-based (EBMT) component and a 
Moses-based statistical approach (SMT). Firstly, 
the input is processed by the example-based MT 
engine and if the whole or important chunks of it 
are found in the translation database, then the 
translation equivalents are used and if necessary 
combined (Gavrila, 2011). In all other cases the 
input is processed by the categorisation sub-
component in order to select the top-level 
domain and respectively, the most appropriate 
SMT domain- and POS-translation model 
(Niehues and Waibel, 2010). 
The translation engine in the system, based on 
MT Server Land (Federmann and Eisele, 2010),  
is able to accommodate and use different third 
party translation engines, such as the Google, 
Bing, Lusy or Yahoo translators. 
Case Study: Multilingual Library  
i-Librarian5  is a free online library that assists 
authors, students, young researchers, scholars, 
librarians and executives to easily create, 
organise and publish various types of documents 
in English, Bulgarian, German, Greek, Polish 
and Romanian. Currently, a sample of the 
publicly available library contains over 20 000 
books in English. 
On uploading a new document to i-Librarian, 
the system automatically provides the user with 
an extraction of the most relevant information 
(concepts and named entities, keywords). Later 
on, the retrieved information is used to generate 
suggestions for classification in the library 
catalogue, containing 86 categories, as well as a 
list of similar documents. Finally, the system 
compiles a summary and translates it in all 
supported languages. Among the supported 
formats are Microsoft Office documents, PDF, 
OpenOffice documents, books in various 
electronic formats, HTML pages and XML 
documents. Users have exclusive rights to 
manage content in the library at their discretion.   
The current version of the system supports 
English and Bulgarian. In early 2012 the Polish, 
Greek, German and Romanian languages will be 
in use. 
                                                          
5 i-Librarian web site is available at http://www.i-
librarian.eu/. One can access the i-Librarian demo content 
using ?demo@i-librarian.eu? for username and ?sandbox? 
for password. 
8
  
Evaluation 
The technical quality and performance of the 
system is being evaluated as well as its appraisal 
by prospective users. The technical evaluation 
uses indicators that assess the following key 
technical elements: 
? overall quality and performance 
attributes (MTBF6, uptime, response 
time); 
? performance of specific functional 
elements (content management, machine 
translation, cross-lingual content 
retrieval, summarisation, text 
categorisation).  
The user evaluation assesses the level of 
satisfaction with the system. We measure non 
functional elements such as: 
? User friendliness and satisfaction, clarity 
in responses and ease of use; 
? Adequacy and completeness of the 
provided data and functionality; 
? Impact on certain user activities and the 
degree of fulfilment of common tasks. 
We have planned for three rounds of user 
evaluation; all users are encouraged to try online 
the system, freely, or by following the provided 
base-line scenarios and accompanying exercises. 
The main instrument for collecting user feedback 
is an online interactive electronic questionnaire7. 
The second round of user evaluation is 
scheduled for Feb-March 2012, while the first 
round took place in Q1 2011, with the 
participation of 33 users. The overall user 
impression was positive and the Mean value of 
each indicator (in a 5-point Likert scale) was 
measured on AVERAGE or ABOVE 
AVERAGE.  
 
 
Figure 3. User evaluation ? UI friendliness and ease 
of use. 
                                                          
6 Mean Time Between Failures 
7 The electronic questionnaire is available at 
http://ue.atlasproject.eu 
 
Figure 4. User evaluation ? user satisfaction with the 
available functionalities in the system. 
 
 
Figure 5. User evaluation ? users productivity 
incensement. 
Acknowledgments 
ATLAS (Applied Technology for Language-
Aided CMS) is a European project funded under 
the CIP ICT Policy Support Programme, Grant 
Agreement 250467. 
Conclusion and Future Work 
The abundance of knowledge allows us to widen 
the application of NLP tools, developed in a 
research environment. The tailor made voting 
system maximizes the use of the different 
categorisation algorithms. The novel summary 
approach adopts state of the art techniques and 
the automatic translation is provided by a cutting 
edge hybrid machine translation system. 
The content management platform and the 
linguistic framework will be released as open-
source software. The language processing chains 
for Greek, Romanian, Polish and German will be 
fully implemented by the end of 2011. The 
summarisation engine and machine translation 
tools will be fully integrated in mid 2012. 
We expect this platform to serve as a basis for 
future development of tools that directly support 
decision making and situation awareness. We 
will use categorical and statistical analysis in 
order to recognise events and patterns, to detect 
opinions and predictions while processing 
The user interface is friendly and 
easy to use 
Excellent
28%
Good 
35%
Average
28%
Below 
Average
9%
Poor
Below Average
Average
Good 
Excellent
I am satisfied with the functionalities 
Below 
Average
3%
Average
38%
Excellent
31%
Good 
28%
Poor
Below
Average
Average
Good 
Excellent
The system increases y ur 
productivity 
Excellent
13%
Below 
Averag
9%
Average
31%
Good 
47%
Poor
Below
Average
Average
Good 
Excellent
9
  
extremely large volumes of disparate data 
resources. 
Demonstration websites 
The multilingual content management platform is 
available for testing at http://i-
publisher.atlasproject.eu/atlas/i-publisher/demo . 
One can access the CMS demo content using 
?demo? for username and ?sandbox2? for 
password. 
The multilingual library web site is available 
at http://www.i-librarian.eu/. One can access the 
i-Librarian demo content using ?demo@i-
librarian.eu? for username and ?sandbox? for 
password. 
References  
Dan Cristea and Ionut C. Pistol, 2008. Managing 
Language Resources and Tools using a Hierarchy 
of Annotation Schemas. In the proceedings of 
workshop 'Sustainability of Language Resources 
and Tools for Natural Language Processing', 
LREC, 2008 
Gregor Hohpe and Bobby Woolf. 2004. Enterprise 
Integration Patterns: Designing, Building, and 
Deploying Messaging Solutions. Addison-Wesley 
Professional. 
Hu Guan, Jingyu Zhou and Minyi Guo. A Class-
Feature-Centroid Classifier for Text 
Categorization. 2009. WWW 2009 Madrid, Track: 
Data Mining / Session: Learning, p201-210. 
OSGi Alliance. 2009. OSGi Service Platform, Core 
Specification, Release 4, Version 4.2. 
Gunes Erkan and Dragomir R. Radev. 2004. 
LexRank: Graph-based Centrality as Salience in 
Text Summarization. Journal of Artificial 
Intelligence Research 22 (2004), p457?479. 
Laura Plaza and Alberto Diaz. 2011. Using Semantic 
Graphs and Word Sense Disambiguation 
Techniques to Improve Text Summarization. 
Procesamiento del Lenguaje Natural, Revista n? 47 
septiembre de 2011 (SEPLN 2011), pp 97-105. 
Monica Gavrila. 2011. Constrained Recombination in 
an Example-based Machine Translation System, In 
the Proceedings of the EAMT-2011: the 15th 
Annual Conference of the European Association 
for Machine Translation, 30-31 May 2011, Leuven, 
Belgium, p. 193-200 
 Jan Niehues and Alex Waibel. 2010. Domain 
adaptation in statistical machine translation using 
factored translation models. EAMT 2010: 
Proceedings of the 14th Annual conference of the 
European Association for Machine Translation, 27-
28 May 2010, Saint-Rapha?l, France. 
Christian Federmann and Andreas Eisele. 2010. MT 
Server Land: An Open-Source MT Architecture. 
The Prague Bulletin of Mathematical Linguistics. 
NUMBER 94, 2010, p57?66 
10
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 189?195,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Sentimatrix ? Multilingual Sentiment Analysis Service 
 
Alexandru-Lucian G?nsc?1, Emanuela Boro?1, Adrian Iftene1, Diana Trandab??1,  
Mihai Toader2, Marius Cor?ci2, Cenel-Augusto Perez1, Dan Cristea1, 3 
1
?Al. I. Cuza? University, Faculty of Computer Science, Iasi, Romania 
2Intelligentics, Cluj-Napoca, Romania 
3Institute of Computer Science, Romanian Academy, Iasi, Romania 
{lucian.ginsca, emanuela.boros, adiftene, dtrandabat, augusto.perez, 
dcristea}@info.uaic.ro, {mtoader, marius}@intelligentics.ro 
 
 
Abstract 
This paper describes the preliminary results 
of a system for extracting sentiments 
opinioned with regard with named entities. 
It also combines rule-based classification, 
statistics and machine learning in a new 
method. The accuracy and speed of 
extraction and classification are crucial. 
The service oriented architecture permits 
the end-user to work with a flexible 
interface in order to produce applications 
that range from aggregating consumer 
feedback on commercial products to 
measuring public opinion on political 
issues from blog and forums. The 
experiment has two versions available for 
testing, one with concrete extraction results 
and sentiment calculus and the other with 
internal metrics validation results. 
1 Motivation 
Nowadays, big companies and organizations spend 
time and money in order to find users? opinions 
about their products, the impact of their marketing 
decisions, or the overall feeling about their support 
and maintenance services. This analysis helps in 
the process of establishing new trends and policies 
and determines in which areas investments must be 
made. One of the focuses of our work is helping 
companies build such analysis in the context of 
users? sentiment identification. Therefore, the 
corpus we work on consists of articles of 
newspapers, blogs, various entries of forums, and 
posts in social networks. 
Sentiment analysis, i.e. the analysis and 
classification of the opinion expressed by a text on 
its subject matter, is a form of information 
extraction from text, which recently focused a lot 
of research and growing commercial interest. 
This paper describes Sentimatrix, a sentiment 
analysis service, doing sentiment extraction and 
associating these analyses with named entities, in 
different languages. We seek to explore how 
sentiment analysis methods perform across 
languages, especially Romanian. The main 
applications that this system experiments with are 
monitoring the Internet before, during and after a 
campaign/message release and obtaining consumer 
feedback on different topics/products. 
In Section 2 we briefly discuss a state of the art 
in sentiment analysis, the system?s architecture is 
described in Section 3 and in Section 4 we focus 
on identifying opinions on Romanian. 
Subsequently, we present the experiment results, 
analysis and discussion in Sections 5 and 6. Future 
work and conclusions are briefly described in 
Section 7. 
2 Sentimatrix compared with state-of-
the-art 
A comprehensive state of the art in the field of 
sentiment analysis, together with potential 
applications of such opinion identification tools, is 
presented in (Pang and Lee, 2008). 
Starting from the early 1990s, the research on 
sentiment-analysis and point of views generally 
assumed the existence of sub-systems for rather 
sophisticated NLP tasks, ranging from parsing to 
the resolution of pragmatic ambiguities (Hearst, 
1992; Wiebe 1990 and 1994). In Sentimatrix, in 
order to identify the sentiment a user expresses 
about a specific product or company, the company 
name must be first identified in the text. Named 
189
entity recognition (NER) systems typically use 
linguistic grammar-based techniques or statistical 
models (an overview is presented in (Nadeau and 
Satoshi Sekine. 2007)). Hand-crafted grammar-
based systems typically obtain better precision, but 
at the cost of lower recall and months of work by 
experienced computational linguists. Besides, the 
task is hard to adapt to new domains. Various 
sentiment types and levels have been considered, 
starting from the ?universal? six level of emotions 
considered in (Ovesdotter Alm, 2005; Liu et al, 
2003; Subasic and Huettner, 2001): anger, disgust, 
fear, happiness, sadness, and surprise. For 
Sentimatrix, we adapted this approach to five 
levels of sentiments: strong positive, positive, 
neutral, negative and strong negative.  
The first known systems relied on relatively 
shallow analysis based on manually built 
discriminative word lexicons (Tong 2001), used to 
classify a text unit by trigger terms or phrases 
contained in a lexicon. The lack of sufficient 
amounts of sentiment annotated corpora led the 
researchers to incorporate learning components 
into their sentiment analysis tools, usually 
supervised classification modules, (e.g., 
categorization according to affect), as initiated in 
(Wiebe and Bruce 1995). 
Much of the literature on sentiment analysis has 
focused on text written in English. Sentimatrix is 
designed to be, as much as possible, language 
independent, the resources used being easily 
adaptable for any language. 
Some of the most known tools available 
nowadays for NER and Opinion Mining are: 
Clarabridge (www.clarabridge.com), RavenPack 
(ravenpack.com), Lexalytics (www.lexalytics.com) 
OpenAmplify (openamplify.com), Radian6 
(www.radian6.com), Limbix (lymbix.com), but 
companies like Google, Microsoft, Oracle, SAS, 
are also deeply involved in this task. 
3 System components 
In Figure 1, the architecture and the main modules 
of our system are presented: preprocessing, named 
entity extraction and opinion identification 
(sentiment extraction per fragment).  
The final production system is based on service 
oriented architecture in order to allow users 
flexible customization and to enable an easier way 
for marketing technology. Each module of the 
system (Segmenter, Tokenizer, Language Detector, 
Entity Extractor, and Sentiment Extractor) can be 
exposed in a user-friendly interface.  
 
Figure 1. System architecture 
3.1 Preprocessing 
The preprocessing phase is made out of a text 
segmentator and a tokenizer. Given a text, we 
divide it into paragraphs, every paragraph is split 
into sentences, and every phrase is tokenized. Each 
token is annotated with two pieces of information: 
its lemma (for Romanian it is obtained from our 
resource with 76,760 word lemmas corresponding 
to 633,444 derived forms) and the normalized form 
(translated into the proper diacritics1). 
3.2 Language Detection 
Language detection is a preprocessing step 
problem of classifying a sample of characters 
based on its features (language-specific models). 
Currently, the system supports English, Romanian 
and Romanian without Diacritics. This step is 
needed in order to correctly identify a sentiment or 
a sentiment modifier, as the named entity detection 
depends on this. We combined three methods for 
                                                          
1
 In Romanian online texts, two diacritics are commonly used, 
but only one is accepted by the official grammar. 
190
identifying the language: N-grams detection, 
strictly 3-grams detection and lemma correction.  
The 3-grams classification method uses corpus 
from Apache Tika for several languages. The 
Romanian 3-gram profile for this method was 
developed from scratch, using our articles archive. 
The language detection in this case performs 
simple distance measurement between every 
language profile that we have and the test 
document profile. The N-grams classification 
method implies, along with computing frequencies, 
a posterior Naive Bayes implementation. The third 
method solves the problematic issue of short 
phrases language detection and it implies looking 
through the lemmas of several words to obtain the 
specificity of the test document. 
3.3 Named Entity Recognition 
The Named Entity Recognition component for 
Romanian language is created using linguistic 
grammar-based techniques and a set of resources. 
Our component is based on two modules, the 
named entity identification module and the named 
entity classification module. After the named entity 
candidates are marked for each input text, each 
candidate is classified into one of the considered 
categories, such as Person, Organization, Place, 
Country, etc. 
 
Named Entity Extraction: After the pre-
processing step, every token written with a capital 
letter is considered to be a named entity candidate.  
For tokens with capital letters which are the first 
tokens in phrases, we consider two situations:  
1. this first token of a phrase is in our stop word 
list (in this case we eliminate it from the 
named entities candidate list),  
2. the first token of a phrase is in our common 
word list. In the second situation there are 
considered two cases:  
a. this common word is followed by lowercase 
words (then we check if the common word 
can be found in the list of trigger words, like 
university, city, doctor, etc.),  
b. this common word is followed by uppercase 
words (in this case the first word of the 
sentence is kept in the NEs candidate list, 
and in a further step it will be decided if it 
will be combined with the following word in 
order to create a composed named entity).  
Named Entities Classification: In the 
classification process we use some of rules utilized 
in the unification of NEs candidates along with the 
resource of NEs and several rules specifically 
tailored for classification. Thus, after all NEs in the 
input text are identified and, if possible, compound 
NEs have been created, we apply the following 
classification rules: contextual rules (using 
contextual information, we are able to classify 
candidate NEs in one of the categories 
Organization, Company, Person, City and Country 
by considering a mix between regular expressions 
and trigger words) and resource-based rules (if no 
triggers were found to indicate what type of entity 
we have, we start searching our databases for the 
candidate entity). 
 
Evaluation: The system?s Upper Bound and its 
performance in real context are evaluated for each 
of the two modules (identification and 
classification) and for each named entity type. The 
first part of the evaluation shows an upper bound 
of 95.76% for F-measure at named entity 
extraction and 95.71% for named entity 
classification. In real context the evaluation shows 
a value of 90.72% for F-measure at named entity 
extraction and a value of 66.73% for named entity 
classification. The results are very promising, and 
they are being comparable with the existing 
systems for Romanian, and even better for Person 
recognition. 
4 Identify users opinions on Romanian 
4.1 Resources 
In such a task as sentiment identification, linguistic 
resources play a very important role. The core 
resource is a manually built list of words and 
groups of words that semantically signal a positive 
or a negative sentiment. From now on, we will 
refer to such a word or group of words as 
?sentiment trigger?. Certain weights have been 
assigned to these words after multiple revisions. 
The weights vary from -3, meaning strong negative 
to +3, which translates to a strong positive. There 
are a total of 3,741 sentiment triggers distributed to 
weight groups as can be observed in Figure 2. The 
triggers are lemmas, so the real number of words 
that can be identified as having a sentiment value 
is much higher. 
191
This list is not closed and it suffers modifications, 
especially by adding new triggers, but in certain 
cases, if a bad behavior is observed, the weights 
may also be altered.  
 
 
Figure 2. Number of sentiment words by weight groups 
 
We define a modifier as a word or a group of 
words that can increase or diminish the intensity of 
a sentiment trigger. We have a manually built list 
of modifiers. We consider negation words a special 
case of modifiers that usually have a greater impact 
on sentiment triggers. So, we also built a small list 
of negation words.   
4.2 Formalism 
General definitions: We define a sentiment 
segment as follows: 
  = (	
, , 		) 
 
sSG is a tuple in which the first two elements are 
optional.  
Let NL be the set of negation words that we use, 
ML the set of modifiers and TL the set of sentiment 
triggers.  We define two partially ordered sets: 
  = (, ?), 