Semiautomatic labelling of semantic features 
Arantza D?az de Ilarraza, Aingeru Mayor and Kepa Sarasola 
IXA Group. Computer Science Faculty. University of the Basque Country 
Donostia/San Sebastian. The Basque Country 
jipdisaa/jibmamaa/jipsagak@si.ehu.es 
 
Abstract 
This paper presents the strategy and design 
of a highly efficient semiautomatic method for 
labelling the semantic features of common 
nouns, using semantic relationships between 
words, and based on the information extracted 
from an electronic monolingual dictionary. The 
method, that uses genus data, specific relators 
and synonymy information, obtains an accuracy 
of over 99% and a scope of 68,2% with regard to 
all the common nouns contained in a real corpus 
of over 1 million words, after the manual 
labelling of only 100 nouns. 
1 Introduction 
Semantic information is essential in a lot of 
NLP applications. In our case, the feature 
[?animate] is necessary to disambiguate between 
the possible Basque translations for the English 
preposition "of" and the Spanish preposition 
"de", when referring to location or possession. 
This ambiguity appears very often when 
translating to Basque [D?az de Ilarraza et al, 
2000]. A complete manual labelling of semantic 
information would prove extremely expensive.  
This study aims to outline the strategy and 
design of a semiautomatic method for labelling 
semantic features of common nouns in Basque, 
expanding and improving the idea outlined in 
[D?az de Ilarraza et al 2000]. Due to the poor 
results obtained, this study dismissed the 
possibility of an initial approach aimed at 
extracting the information corresponding to the 
(?animate) feature automatically from corpus. 
Instead, an alternative idea was proposed, i.e. 
that of using semantic relationships between 
words extracted from the Basque monolingual 
dictionary Euskal Hiztegia (Sarasola 1996). In 
this context, we used genus data and specific 
relators, together with a few words manually 
labelled, to extract the information 
corresponding to the (?animate) feature. The 
results obtained were very promising: 8,439 
common nouns were labelled automatically after 
the manual labelling of just 100.  
This paper describes the work carried out with 
the aim of expanding this idea this idea through 
the inclusion of information about synonymy, 
repeating the automatic process iteratively in 
order to obtain better results  and, monitoring the 
reliability of the labelling of each individual 
noun. After studying the ideal relationship 
between the manual part of the operation and the 
scope of the automatic process, we generalised 
the process in order to adapt it to other semantic 
features. We obtained very satisfactory results 
considering the labelling of common nouns 
contained in the dictionary: for the [?animate] 
feature, we labelled 12,308 nouns with an 
accuracy of 99.2%, after the manual labelling of 
only 100.  
 This paper is organised as follows: section 2 
presents the semantic relationships between 
words extracted from the Basque monolingual 
dictionary, and used by our semiautomatic 
labelling method. The method itself is described 
in section 3. The experiments carried out with 
the aim of optimising the efficiency of the 
method are described in section 4, and section 5 
outlines the accuracy and scope of the labelling 
process for the [?animate] semantic feature. 
Finally, section 6 describes how the method was 
generalised to cover other semantic features. The 
study finishes by underlining the results obtained 
and suggesting future research. 
2 Superficial semantic relationships 
between words in dictionaries  
According to Smith and Maxwell, there are 
three basic methods for defining a lexical entry 
[Smith and Maxwell., 1980]: 
? By means of a synonym: a word with the 
same sense as the lexical entry. 
finish. conclude(sin), terminate(sin) 
? By means of a classical definition: ?genus + 
differentia?. The genus is the generic term or 
 Figure 1. Implementation of the automatic process using genus and relater information 
procedure Labelling_of_the_dictionary { 
foreach (common Noun of the dictionary) { 
(Label, Reliability) = Find_its_label (Noun)  }   
} 
procedure Find_its_label (Noun) { 
foreach (Sense with Noun Genus/Relator) { 
if (Genus/Relator labelled){ Sense.Label  = Genus/Relator.Label 
 Sense.Reliability = Genus/Relator.Reliability 
} 
   else {(  Sense.Label, 
 Sense.Reliability) = Find_its_label(Genus) } #recursion 
if (Noun.Label != Sense.Label) { Noun.Label = [?] } 
   else  { Noun.Label =  Sense.Label } 
} # end foreach 
Noun.Reliability = ? Reliability labelled senses / number of senses 
return (Noun.Label, Noun.Reliability) 
}  
hyperonym, and the lexical entry a more 
specific term or hyponym.  
aeroplane. vehicle (genus) that can fly 
(differentia) 
? By means of specific relators, that will often 
determine the semantic relationship between 
the lexical entry and the core of the 
definition. 
horsefly. Name given to (relator) certain 
insects (related term) of the Tabanidae family  
One method for identifying the semantic 
relationship that exists between different words 
is to extract the information from monolingual 
dictionaries.  
Agirre et al (2000) applied it for Basque, 
using the definitions contained in the 
monolingual dictionary Euskal Hiztegia. We use 
for our research the information about genus, 
specific relators and synonymy extracted by 
them. 
3 Semiautomatic labelling using genus, 
specific relators and synonymy  
In order to label the common nouns that 
appear in the dictionary, we used the definitions 
of the 26,461 senses of the 16,380 common 
nouns defined by means of genus/relators 
(14,569) or synonyms (11,892).  
The experiment was carried out as follows: 
firstly, we used the information relative to genus 
and specific relators to extract the information 
regarding the [?animate] feature (3.1). 
Subsequently, we also incorporated the 
information relative to synonymy (3.2). Finally, 
we repeated the automatic process iteratively in 
order to obtain better results (3.3). An example 
of the whole process is given in section 3.4. 
3.1 Labelling using information relative to 
genus and specific relators 
Our strategy consisted of manually labelling 
the semantic feature for a small number of words 
that appear most frequently in the dictionary as 
genus/relators. We used these words to infer the 
value of this feature for as many other words as 
possible. 
This inference is possible because in the 
hyperonymy/hyponymy relationship, that 
characterises the genus, semantic attributes are 
inherited. For example, if ?langile? (worker) has 
the [+animate] feature, all its hyponyms (or in 
other words, all the words whose hyperonym is 
?langile?) will have the same [+animate] feature. 
Certain genus are ambiguous, since they 
contain senses with opposing semantic features. 
For example ?buru? (head/boss) has the [-
animate] feature when it means ?head? and the 
[+animate] feature when it means ?boss?. The 
semantic feature of the sense defined can also be 
deduced from some specific relators. In this way, 
the semantic feature of words whose relator is 
?nolakotasuna? (quality) would be [-animate], 
such as in the case of ?aitatasuna? (paternity), for 
example. There are also certain relators that offer 
no information, such as ?mota? (type), ?izena? 
(name), and ?banako? (unit, individual). 
We used four types of labels during the 
manual operation: [+], [-], [?] and [x]. [?] for 
ambiguous cases; and [x] for relators that do not 
offer information regarding this semantic feature. 
 In order to establish the reliability of the 
automatic labelling process for a particular noun, 
we considered the number of senses labelled, 
taking into account the reliability of the labels of 
the genus (or relator) that provided the 
information. The result was calculated as 
follows:   
 Rel_noun = ? Rel_genus_per_sense / n_senses 
During manual labelling, we assigned 
reliability value 1 to all labels, since all the 
senses of these nouns are taken into account.  
Figure 1 shows the algorithm used. For each 
common noun defined in the dictionary, we take, 
one by one, all their senses containing genus or 
relator, assigning in each case the first label 
associated to a genus or relator in the hierarchy 
of hyperonyms. When the sign of all the labels 
are coincident we use it to label the entry, in 
other case, we use the label [?]. In all cases, their 
reliability is calculated.  
When we detect a cycle, the search is 
interrupted and the sense to be tagged remains 
unlabelled.  
3.2 Labelling using synonymy information  
Labelling using genus and relators can be 
expanded by using synonymy. Since the 
synonymy relationship shares semantic features, 
we can deduce the semantic label of a sense if 
we know the label of its synonymes.  
Therefore, the information obtained during the 
previous phase can now be used to label new 
nouns. It also serves to increase the reliability of 
nouns already been labelled thanks to the genus 
information of some of their senses. If the 
synonymy information provided corroborates the 
genus information, the noun?s reliability rating 
increases. If, on the other hand, the new label 
does not coincide with the previous one, a 
special label: [?] is assigned to the noun 
indicating this ambiguity.  
The automatic process using synonymy was 
implemented in the same way as in the previous 
process. 
3.3 Iterative repetition of the automatic 
process  
Our next idea was to repeat the process; since 
the information gathered so far using synonymy 
may also be applied hereditarily through the 
genus? hyperonymy relationship. 
We therefore repeated the process from the 
beginning, trying to label all the senses of the 
nouns that had not been fully labelled during the 
initial operations, by using the information 
contained in the senses of the nouns that had 
been fully labelled (reliability 1). 
As with the initial operation, we first used 
information about genus and relators, and then, 
synonymy.  
This process can be repeated any number of 
times, thereby labelling more and more words 
while increasing the reliability of the labelling 
itself. However, repetition of the process also 
increases the number of words labelled as 
ambiguous [?], since more senses are labelled 
during each iteration, thereby increasing the 
chances of inconsistencies. As we shall see, this 
iterative process improves the results 
logarithmically up to a certain number of 
repetitions, after which it has no further 
advantageous effects.  
3.4 Example of semiautomatic labelling for 
the [?animate] feature 
The 100 words that are most frequently used 
as genus (g) or relators (r) were labelled 
manually for the [?animate] feature, as shown in 
table 2 (tables 3, 4 and 5 contain the Basque 
words processed during the explained operation, 
along with their English translation in italics). 
Noun  ?anim Freq Gen/rel 
nolakotasun (quality) - 531   Relator 
pertsona (person) + 377   Genus   
multzo (collection)  - 362   Relator 
txikigarri (collection)  x  213 Relator 
zati (part) - 230   Relator 
gai (material)  - 202   Genus 
tresna (instrument)  - 188   Genus 
...     
buru (head) ? 54 Genus 
Table 2. Manual labelling 
We shall now trace the implementation of the 
automatic labelling process for certain nouns.  
Table 3 shows the results of the first labelling 
process using information about genus and 
relators. The words printed in bold in the results 
column are nouns that were labelled during the 
manual labelling process. We can see how the 
noun ?babesgarri? (protector) is labelled as [-] 
thanks to the information provided by the relator 
of its only sense, which was manually labelled. 
Th
In
(
r
n
t
i
h
r
(
w
g
a
N
b
(
a
(
  
a
(
i
(
  
g
(
  
g
(
e
(
  
a
(
a
(
  
a
(
  
f
(
i
(
j
(
z
(oun N. sense N. genus Result of process using genus and relators  Lab Rel. 
abesgarri 
protector)   
1 1 (zer[-]1) 
(thing)   
[-] 1 
rmadura 
armour) 
3 3 (multzo[-]1) (babesgarri[-]1)(soineko[]) 
(collection) (protector)     (garment)   
[-] 0.66 
          
ma 
mother) 
5 3 (emakume[+]1)(animalia[+]1)(eme[]) 
(woman)      (animal)      (female) 
[+] 0.4 
turburu 
spring)   
3 1 (aterabide[]) 
(outlet)   
[] 0 
          
ertaera 
event)   
1 1 (gauza[-]1) 
(thing) 
[-] 1 
          
iltzape 
prison)   
2 1 (toki[-]1) 
(place)   
[-] 0.5 
spetxe 
jail) 
2 2 (eraikuntza[-]1)(leku[-]1) 
(construction)  (place) 
[-] 1 
          
diskide 1 1 (pertsona[+]1) [+] 1 e noun therefore has a reliability rating of 1. 
 the same way, 2 of the 3 senses of ?armadura? 
1. The reliability rating obtained for ?zinismo? 
was therefore 0.87 (f=(1+0.75)/2=0.87). 
friend)   (person) 
diskidetzako 
friend)   
1 1 (lagun[]) 
(companion) 
[] 0 
          
pio 
celery) 
2 2 (jateko[])  (landare[-]1) 
(food)      (plant) 
[-] 0.5 
          
ilosofia 
philosophy) 
2 2 (jakintza[-]1)(multzo[-]1) 
(knowledge)   (collection)   
[-] 1 
kusgune 
viewpoint)   
2 1 (gune[-]1) 
(point)   
[-] 0.5 
arrera 
attitude) 
2 2 (era[-]1)(ikusgune[-]0.5) 
(way)    (viewpoint)   
[-] 0.75 
inismo 
cynicism) 
2 2 (filosofia[-]1)(jarrera[-]0.75 ) 
(philosophy)   (attitude) 
[-] 0.87 
Table 3. Result of automatic labelling using genus and relator information armour) had coincident labels, thereby giving a 
eliability rating of 0.66 (f=(1+1)/3=0.66). The 
oun ?ama? (mother) was labelled as [+], thanks 
o the information about genus and relator of 2 of 
ts 3 senses, out of a total of 5 (the remaining two 
ave synonymy information). The reliability 
ating was therefore calculated as 0.4 
f=(1+1)/5=0.4). The word ?zinismo? (cynicism) 
as labelled as [-] thanks to the fact that the 
enus of its 2 senses were both labelled as such, 
lthough one did not have a reliability rating of 
Table 4 shows some examples of the process 
using synonym information.  
As we can see, ?iturburu? (spring), which the 
previous process had not managed to tag, is now 
labelled as [-] thanks to the synonymy 
information associated to one of the two senses. 
The resulting reliability rating is 0.06 
(f=0.2/3=0.06). If we look at the term ?ama?, 
which had previously been labelled as [+] on the 
basis of genus information, we see that the 
synonyms of the two senses that use synonymy 
Noun Genus lab. N. sens N. syn Results of the process using synonymy Lab. Relia. 
iturburu 
(spring)   
[] 3 2 (etorki[])  (hasiera[-]0.20) 
(origin)    (start)   
[-] 0.06 
ama 
(mother) 
[+] 5 2 (iturburu[-])(jatorri[-]) 
(spring)     (origin)   
[?] 1 
            
gertakuntza 
(event)   
1 1 (gertaera[-]1) 
(happening)   
[-] 1 
lagun 
(companion) 
1 1 (adiskide[+]1) 
(friend) 
[+] 1 
jateko 
(food)   
1 1 (janari[-]1) 
(food) 
[-] 1 
            
giltzape 
(prison)   
[-] 2 1 (espetxe[-]1) 
(jail) 
[-] 1 
ikusgune 
(viewpoint) 
[-] 2 1 (ikuspen[-]0.33) 
(view)   
[-] 0.66 
Table 4. Results of automatic labelling using synonymy information  
 Noun 
armadur
(armour
adiskid
(friend
apio 
(celery
  
ikusgun
(viewpo
jarrera
(attitu
zinismo
(cynici
informa
inconsi
The 
(compa
previou
thanks to synonym information. The words 
?giltzape? (prison) and ?ikusgune? (viewpoint), 
which had had one sense labelled on the basis of 
genus, now have both senses labelled. The 
reliability rating for ?ikusgune? is calculated as 
f=(1+0.33)/2=0.66. 
We then repeated the process using first the 
genus/relator information (table 4) followed by 
the synonymy information (table 5).   
The aim of this repetition was to label only 
those words that had not been fully labelled, 
using the information provided by the terms that 
had been and that had a reliability rating of 1, 
such as  ?babesgarri?, ?gertaera?, ?espetxe?, 
?adiskide?, ?filosofia?, ?ama?, ?gertakuntza?, 
?lagun?, ?jateko? and ?giltzape? (tables 4 and 5).  
This process succeeded in labelling the senses 
information. On the other hand, ?ikusgune? 
(viewpoint), ?jarrera? (attitude) and ?zinismo? 
(cynicism), did not benefit from this repetition.  
Following this process, we applied the 
synonymy information, thus completing the 
second iteration. The process may be repeated as 
many times as you wish.  
4 Experiments for optimising the 
efficiency of the method  
We carried out a number of different tests for 
the [?animate] semantic feature labelling the 2, 
5, 10, 50, 100, 125 and 150 words most 
frequently used as genus/relators, and repeating 
the whole process (using both genus and relator 
and synonymy information) 1, 2 and 3 times.   
The first 5 terms that appear most frequently 
0
2000
4000
6000
8000
10000
12000
14000
0 20 40 60 80 100 120 140
Manual labelling
Au
to
ma
tic
 la
be
llin
g
0
400
800
1200
1600
2000
Re
lat
ive
 in
cr
ea
se
  
Fig. 2. Automatic labelling and relative increase N. sense N. genus Result of process using genus and relators  Lab. Relia. 
a 
) 
3 3 (multzo[-]1)(babesgarri[-]1)(soineko[-]1) 
(collection)  (protector)     (garment)        
[-] 1 
etzako 
) 
1 1 (lagun[+]1) 
(companion) 
[+] 1 
) 
2 2 (jateko[-]1)(landare[-]1) 
(food)    (plant) 
[-] 1 
          
e 
int) 
2 2 (gune[-]1) 
(point)   
[-] 0.5 
 
de) 
2 2 (era[-]1)(ikusgune[-]0.5) 
(way)   (viewpoint) 
[-] 0.75 
 
sm) 
2 2 (filosofia[-]1)(jarrera[-]0.75 ) 
(philosophy)    (attitude)        
[-] 0.87 
Table 5. Results of the 2nd iteration of automatic labelling using genus and relator information 
tion are labelled as [-]. Due to this 
stency, the word is now labelled as [?]. 
terms ?gertakuntza? (event), ?lagun? 
nion) and ?jateko? (food), which 
sly only had one sense, are now labelled 
of ?armadura? (protector), ?adiskidetzako? 
(friend) and ?apio? (celery), previously left 
unlabelled, since their genus ?soineko? 
(garment), ?lagun? (friend) and ?jateko? (food) 
had been fully labelled using the synonym 
 as genus/relators are also the most productive 
during the automatic labelling process. From 
here on, the rate of increase gradually falls, until 
only 7 terms are labelled automatically for every 
noun labelled manually.  
On average, the first 2 nouns each enabled 
1840 terms to be labelled, the next 3 enabled 
1112 while the next 5 enabled only 250. After 
the hundredth noun, this average dropped to just 
7 new terms labelled automatically for every 
term labelled manually. These results are 
illustrated in figure 2. 
For efficiency reasons, we decided that when 
labelling other semantic features, we will label 
manually the 100 nouns most frequently used as 
genus/relators.  
In order to decide the number of iterations 
required for optimum results, we compared the 
results obtained after 1 to 10 iterations after 
manually labelling 100 nouns (Figure 3). 
Although no increase was recorded for the 
number of nouns with reliability rating 1 (i.e. 
with all senses labelled) after the 3rd iteration, the 
results for other reliability ratings continued to 
increase up until the 8th iteration, since as more 
and more information is gathered, new 
contradictions are generated and the number of 
ambiguous labels increases. When the results 
stabilise, we can affirm that all the available 
information has been used and the most accurate 
results possible with this manual labelling 
operation have been obtained. It is important to 
check that the process does indeed stabilise, and 
that it does so after a fairly low number of 
iterations (in this case, after 8). 
The repetition of the process does not 
significantly increase execution time. 10 
iterations of the automatic labelling process for 
the [?animate] feature takes just 11 minutes 33 
seconds using the total capacity of the CPU of a 
Sun Sparc 10 machine with 512 Megabytes of 
memory running at 360 MHz.  
We can therefore conclude that the method is 
viable and that, in the automatic process for 
other semantic features, the necessary iterations 
should be carried out until the results are totally 
stabilised. 
5 Accuracy and scope of the labelling 
process for the [?animate] feature   
In order to calculate the accuracy of the 
automatic labelling process, we took 1% of the 
labelled words as a sample and checked them 
manually. The results are shown in table 6. 
Reliability  
f=1 1>f>0.5 0.5>f>0 Total 
Accuracy 100% 100% 94% 99.2% 
Table 6. Accuracy of automatic labelling 
Although we initially planned to use only the 
labels with a reliability rating of 1, after seeing 
the accuracy of the others, we decided to use all 
the labels obtained during the process, thereby 
achieving an overall accuracy rating of 99.2%. 
We can affirm that the semiautomatic process 
designed and implemented here is very efficient.  
The scope for the automatic labelling of the 
[?animate] feature (table 7) was 75.14% of all 
the nouns contained in the dictionary (12,308 of 
16,380), having manually labelled 100 nouns and 
0
2000
4000
6000
8000
10000
12000
14000
0 2 4 6 8 10 12
Number of iterations
Au
to
ma
tic
 la
be
llin
g
Automatic labelling
f=1
1>f>0.5
0.5>f>0
?
 
Fig. 3. Automatic labelling according to number of iterations 
 carried out 8 iterations.  
Labelling  
f=1 1>f>0.5 0.5>f>0 
 
? 
6132 4513 1663 Auto 
lab. 12308 (75.14%) 
 
1301 
Table 7. Scope of the dictionary 
We also calculated the scope of this labelling 
in a real context, using the corpus gathered from 
the newspaper Euskaldunon Egunkaria, which 
contains 1,267,453 words and 311,901 common 
nouns, of which 7,219 are different nouns. Table 
8 shows the results ? a scope of 69.2% with 
regard to the nouns that appear in the text (47.6% 
of the total number of different common nouns 
contained in the corpus). In other words, after 
carrying out a very minor manual operation, we 
managed to label two out of every three nouns 
that appear in the corpus. Similarly, we noted 
that of the 500 nouns that appear most frequently 
in the corpus, 348 (69.6%) were labelled.  
 Appearances in 
the corpus 
Different 
nouns 
Total 311,901 7,219 
Labelled (68.2%) 212,887 (47.6%) 3,434 
[+] 17,408 356 
[-] 195,479 3,078 
Table 8. Scope of labelling within the corpus 
6 Generalisation for use with other 
semantic features  
Given the process?s efficiency, it can be 
generalised for use with other semantic features. 
To this end, we have adapted its implementation 
to enable the automatic process to be carried out 
on the basis of the manual labelling of any 
semantic feature.  
So far, we have carried out the labelling 
process for the [?animate], [?human] and 
[?concrete] semantic features. Table 12 shows 
the corresponding results.  
Label ?animate ?human ?concrete 
[+] 1,643 1,118 7,611 
[-] 10,665 10,684 1,143 
Total 12,308 11,802 8,754 
Table 12. Labelling data for different semantic 
features 
Conclusions 
We have presented a highly efficient 
semiautomatic method for labelling the semantic 
features of common nouns, using the study of 
genus, relators and synonymy as contained in the 
Euskal Hiztegia dictionary. The results obtained 
have been excellent, with an accuracy of over 
99% and a scope of 68,2% with regard to all the 
common nouns contained in a real corpus of over 
1 million words, after the manual labelling of 
only 100 nouns.   
As far as we know, no so method of semantic 
feature labelling has been described in the 
literature, although many authors [Pustejovsky, 
2000; Sheremetyeva & Nirenburg, 2000] claim 
the significance of semantic features in general, 
and [animacy] in particular, for NLP systems. 
One of the possible applications of these 
experiments is to enrich the Basque Lexical 
Database, EDBL, using the semantic information 
obtained.  
Acknowledgements 
The Basque Government Department of 
Education, Universities and Research sponsored 
this study.  
Bibliography 
Agirre E., Ansa O., Arregi X., Artola X., D?az de 
Ilarraza A., Lersundi M., Martinez D., Sarasola K., 
Urizak R., 2000, ?Extraction of semantic relations 
from a Basque monolingual dictionary using 
Constraint Grammar?, EURALEX?2000. 
Diaz de Ilarraza A., Lersundi M., Mayor A., Sarasola 
K., 2000. Etiquetado semiautom?tico del rasgo 
sem?ntico de animicidad para su uso en un sistema 
de traducci?n autom?tica. SEPLN?2000. Vigo.. 
Diaz de Ilarraza A., Mayor A., Sarasola K., 2000. 
?Reusability of Wide-Coverage Linguistic 
Resources in the Construction of a Multilingual MT 
System?.MT 2000. Exeter. UK. 
Pustejovsky J., 2000. ?Syntagmatic Processes?. 
Handbook of Lexicology and Lexicography. de 
Gruyter, 2000. 
Sheremetyeva S. and Nirenburg S., 2000. "Towards A 
Universal Tool for NLP Resource Acquisition". 
LREC2000. Greece.  
Smith, R.N., Maxwell, E., 1980, ?An English 
dictionary for computerised syntactic and semantic 
processing systems?, Proceedings of the 
International Conference on Computational 
Linguistics. 1980. 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 31?34
Manchester, August 2008
Detecting Erroneous Uses of Complex Postpositions in an 
Agglutinative Language 
Arantza D?az de Ilarraza Koldo Gojenola Maite Oronoz  
IXA NLP group. University of the Basque Country 
jipdisaa@si.ehu.es koldo.gojenola@ehu.es maite.oronoz@ehu.es  
 
Abstract 
This work presents the development of a 
system that detects incorrect uses of com-
plex postpositions in Basque, an aggluti-
native language. Error detection in com-
plex postpositions is interesting because: 
1) the context of detection is limited to a 
few words; 2) it implies the interaction of 
multiple levels of linguistic processing 
(morphology, syntax and semantics). So, 
the system must deal with problems rang-
ing from tokenization and ambiguity to 
syntactic agreement and examination of 
local contexts. The evaluation was per-
formed in order to test both incorrect uses 
of postpositions and also false alarms.1 
1 Structure of complex postpositions  
Basque postpositions play a role similar to 
English prepositions, with the difference that 
they appear at the end of noun phrases or 
postpositional phrases. They are defined as 
?forms that represent grammatical relations 
among phrases appearing in a sentence? 
(Euskaltzaindia, 1994). There are two main types 
of postpositions in Basque: (1) a suffix appended 
to a lemma and, (2) a suffix followed by a lemma 
(main element) that can also be inflected. 
(1) etxe-tik 
house-(from the)  
from the house 
 (2) etxe-aren gain-etik  
house-(of the)  top-(from the) 
from the top of the house 
The last type of elements has been termed as 
complex postposition. We will use this term to 
name the whole sequence of two words involved, 
and not just to refer to the second element. Com-
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 
Unported license (http://creativecommons.org/ 
licenses/by-nc-sa/3.0/). Some rights reserved. 
plex postpositions can be described as: 
(3) lemma1 + (suffix1 + lemma2 + suffix2) 
In these constructions, the second lemma is fixed 
for each postposition, while the first lemma al-
lows for much more variation, ranging from 
every noun to some specific semantic classes. 
The above description (3) is intended to stress  
(with parentheses) the fact that the combination 
of both suffixes with the second lemma acts as a 
complex case-suffix that is ?appended? to the 
first lemma. Both suffixes present different com-
binations of number and case, which can agree in 
several ways, depending on the lemma, case or 
contextual factors. Table 1 shows the different 
variants of two complex postpositions, derived 
from the lemmas bitarte and aurre. For example, 
the lemma bitarte is polysemous (?means, by 
means of, instrument, while (temporal), be-
tween?). Multiple factors affect the correctness 
of a postposition, including morphological and 
syntactic constraints. We also discovered a num-
ber of relevant contextual factors, which are not 
explicitly accounted for in standard grammars. 
2 The corpus 
The detection of erroneous uses of complex 
postpositions needs first a corpus that can serve 
for both development and evaluation of the sys-
tem. To obtain such a corpus is a labor-intensive 
task, to which it must be added the examination 
and markup of incorrect examples. The use of a 
big ?correct? corpus will allow us to test our sys-
tem negatively, thoroughly testing the system?s 
behavior in respect to false alarms. We used an 
automatic system for detecting complex postpo-
sitions in order to get development and test data. 
There are two text types: Newspaper corpora 
(henceforth NC, 8,207,919 word-forms) that is 
subject to an edition process and style guides, 
and Learner corpora (LC, 994,658 word-forms), 
which come from texts written by learners of 
Basque and University students. These texts are 
more ?susceptible? of containing errors. 
31
We decided to study those types of postpositions 
that appear most frequently in texts, those con-
taining the following lemmas as their second ele-
ment: arte, aurre, bitarte, buruz, and zehar2. We 
selected these postpositions given that they are 
well documented in grammar books, with de-
tailed descriptions of their correct and incorrect 
uses (e.g. see Table 1 for bitarte), and also that 
they are very frequent in both types of texts. 
Each kind of syntactic error occurs with very 
low frequency and, therefore, big corpora are 
needed for evaluation and testing3. Even if such 
corpora are available, to obtain naturally occur-
ring test data, hundreds of texts should be manu-
ally examined and marked. As a result, we de-
cided to only manually mark errors in Learners? 
Corpora (LC), because NC, an order of magni-
tude bigger than LC, is presumed to contain less 
errors. This implies that we will be able to meas-
ure precision4 in both corpora, while recall5 will 
only be evaluated in LC. Table 2 shows the 
number of sentences used for development (60% 
of each corpus) and test (40%). We treated LC 
and NC separately, as they presumably differ in 
the number of errors. 
3 Linguistic Processing Tools 
The corpus was automatically analyzed by means 
of several linguistic processors: a morphosyntac-
tic analyzer (Aduriz et al, 2000), EUSTAGGER, 
the lemmatizer/tagger for Basque, and the Con-
straint Grammar parser (CG, Tapanainen, 1996) 
for morphological disambiguation.  
                                                 
2
 As each lemma has several meanings depending on each 
variant, we will not give their translation equivalence. 
3
 We made an estimate of more than 1% of elements in 
general corpora being complex postpositions. 
4
 Number of errors correctly identified by the system / total 
number of elements identified as erroneous. 
5
 Number of errors correctly identified by the system / total 
number of real errors. 
Added to these, we also used other resources: 
? Grammar books which describe errors in 
postpositions (Zubiri & Zubiri, 1995). 
? Place names. Two of the selected postposi-
tions (arte, aurre) are used in expressions 
that denote temporal and spatial coordinates, 
but their variants impose different restric-
tions and agreement (case, number). In order 
to recognize common nouns that refer to a 
spatial context, we made use of a new lexical 
resource: electronic versions of dictionaries 
(Sarasola, 2007; Elhuyar, 2000). 168 and 242 
words were automatically acquired from 
each dictionary. To this, we added proper 
names corresponding to places. 
? Animate/inanimate distinction. Regarding 
postpositions formed with aurre, Zubiri et al 
(1995) point out that ?typically the previous 
word takes the genitive case, although it can 
also be used without a case mark with inani-
mate nouns?. For this reason, we used a dic-
tionary enriched with semantic features, such 
as animate/inanimate, time or instrument. 
We selected 1,642 animate words. We also 
added person names and pronouns. 
4 Rule design 
The system will assign an error-tag to those 
word-forms that show the presence of an incor-
rect use of a postposition. We use the CG formal-
ism (Tapanainen, 1996) for this task. CG allows 
 
NC LC  
Dev Test Dev Test 
arte 7769 5179 1209 806 
aurre 8129 5420 1157 771 
bitarte 3846 2564 772 514 
buruz 5435 3623 560 373 
zehar 1500 1000 186 126 
Total 26679 17786 3884 2590 
Errors   60 29 
Table 2. Number of sentences in development 
and test sets, including the errors in LC.  
lemma2 suffix1 suffix 2 Examples 
-en (genitive) -z (instrumental) etxearen bitartez  (by means of the house) 
-ra (alative) -n (inessive, sg.) etxera bitartean  (while going to the house) 
-a (absolutive, sg.) -n (inessive, sg.) ordubata bitartean (around one o?clock) 
-? (no case) -n (inessive, sg.) meza bitartean (while attending mass) 
-en (genitive) -n (inessive, sg.) mendeen bitartean (between those centuries) 
-? (no case) -? /ko (no case/genitive) Lau hektarea bitarte  (in a range of four hectares) 
-ak (absolutive, pl.) 
-? /ko (no case/genitive) seiak bitarte (around six o?clock) 
bitarte 
(noun) 
-ra (alative) 
-? /ko (no case/genitive) etxera bitarte (in the way home) 
-? /-en (no 
case/genitive) 
-n/-ra/-tik/-ko (inessive/ ala-
tive/ ablative/ genitive) 
eliza aurrean (in front of the church) aurre 
(noun) 
-tik (ablative) -ra (alative) hemendik aurrera (from here onwards) 
Table 1. Complex postpositions for bitarte and aurre. 
32
the definition of complex contextual rules in or-
der to detect error patterns by means of mapping 
rules and a notation akin to regular expressions. 
Fig. 1 shows a general overview of the system. 
Syntactic constraints are encoded by means of 
CG rules using morphosyntactic categories (part 
of speech, case, number, ?). Semantic restric-
tions are enforced by lists of words belonging to 
a semantic group. All of the five postpositions 
have clear requirements about the combinations 
of case and number in the surrounding context.  
Overall, the CG grammar contains 30 rules for 
the set of 5 postpositions. We found that 
although the study of authoritative grammatical 
descriptions was exhaustive, the grammarians? 
descriptions of correct and incorrect uses refer 
mainly to morphology and syntax. Nevertheless, 
we discovered empirically that most of the rules 
needed to be extended with several classes of 
semantic restrictions. Among others, distinctions 
were needed for animate nouns, place names, or 
several classes of time expressions, depending on 
each different variant of each postposition.  
5 Evaluation 
The rules were applied both to the (presumably) 
correct newspapers texts (NC) and to the learn-
ers? texts (LC). The actual errors in LC were 
marked in advance but not in NC, which means 
that recall can only be evaluated in LC. Table 3 
shows the main results including all the selected 
five postpositions. The LC corpus contains 60 
and 29 error instances in development and test 
corpus, respectively. If we concentrate on preci-
sion, Table 4 shows the overall precision results 
for the total of errors detected in the test corpora. 
When we consider the whole set of postpositions 
precision is 50.5%, giving 42 false alarms out of 
85 detected elements. We performed an analysis 
of false alarms which showed several causes: 
? Morphological ambiguity (43% of alarms). 
? Semantic ambiguity (28%). We included sets 
of context words to identify the correct 
senses, but it still causes many false alarms. 
? Syntactic ambiguity (22%). The false alarms 
are mostly concerned with coordination. 
? Tokenization errors (7%). 
As most of the false alarms came from postpo-
sitions formed with arte, the most ambiguous 
one, we counted the errors when dealing only 
with the other four postpositions, giving a better 
precision (70.4%, second row in Table 4), al-
though detecting less true errors. If the system 
only deals with three postpositions (third row in 
Table 4), then precision reaches 78.3%. Johan-
nessen et al (2002) note that the acceptable num-
ber of false alarms in a grammar checker should 
not exceed 30%, that is, at least 70% of all 
alarms had to report true errors. Our experiments 
show that our system performs within that limit, 
albeit restricting its application to the most ?prof-
itable? postpositions. Although the number of 
rules varies widely (from 15 rules for arte to 2 
rules in the case of zehar) their effectiveness 
greatly depends on the complexity and ambiguity 
of the contextual factors. For that reason, arte 
presents the worst precision results even when it 
contains by far the biggest set of detection rules. 
On the other hand, zehar, with 2 rules, presents 
the best precision, due to its limited ambiguity. 
So, to deal with the full set postpositions (several 
works estimate more than 150), it will be more 
profitable to make a preliminary study on ambi-
guities and variants for each postposition. 
6 Related work 
Kukich (1992) surveys the state of the art in syn-
tactic error detection. She estimates that a pro-
portion of all the errors varying between 25% 
and over 50% are valid words. Atwell and Elliott 
Postpositions Precision 
arte, aurre, bitarte, buruz, zehar 50.5% (43/85)  
aurre, bitarte, buruz, zehar 70.4% (31/44) 
bitarte, buruz, zehar 78.3% (29/37) 
Table 4. Precision for the test sets (NC + LC). 
 NC LC 
 Dev Test Dev Test 
Sentences 26679 17786 3884 2590 
Errors - - 60 29 
Undetected - - 10 10 
Detected 30 24 50 19 
False alarms 45 33 2 9 
Recall - - 83% 65% 
Precision 40% 42% 96% 67% 
Table 3. Evaluation results. 
Sentences 
Morphological  
analysis 
Constraint Grammar 
parser 
No Error / Error Type 
Figure 1. General architecture. 
Error detection  
grammar 
Place 
nouns 
Animate 
nouns 
?
33
(1987) concluded that 55% of them are local syn-
tactic errors (detectable by an examination of the 
local syntactic context), 18% are due to global 
syntactic errors (which need a full parse of the 
sentence), and 27% are semantic errors. Regard-
ing their treatment, there have been proposals 
ranging from error patterns (Kukich 1992; Gold-
ing and Schabes 1996), in the form of hand-
coded rules or automatically learned ones, to sys-
tems that integrate syntactic analysis. 
(Chodorow et al, 2007) present a system for 
detecting errors in English prepositions using 
machine learning. Although both English prepo-
sitions and Basque postpositions have in some 
part relation with semantic features, Basque 
postpositions are, in our opinion, qualitatively 
more complex, as they are distributed across two 
words, and they also show different kinds of syn-
tactic agreement in case and number, together 
with a high number of variants. This is the main 
reason why we chose a knowledge-based method. 
7 Conclusions 
We have presented a system for the detection of 
errors in complex postpositions in Basque. Al-
though at first glance it could seem that postposi-
tions imply the examination of two consecutive 
words, a posterior analysis showed that they of-
fer rich and varied contexts of application, re-
quiring the inspection of several context words, 
albeit not enough to need a full syntactic or se-
mantic analysis of sentences. The system uses a 
varied set of linguistic resources, ranging from 
morphological analysis to specialized lexical re-
sources. As the detection of these errors implies a 
detailed and expert linguistic knowledge, the sys-
tem uses a purely knowledge-based approach. 
A considerable effort has been invested in the 
compilation of a corpus that provides a testbed 
for the system, which should be representative 
enough as to predict the behaviour of the system 
in an environment of a grammar checker. For 
that reason, we have tried to put a real emphasis 
on avoiding false alarms, that is, treating also lots 
of correct instances. The results show that good 
precision can be obtained. Regarding recall, our 
experiments do not allow to make an estimation, 
as the NC test corpora is too big to perform a 
detailed examination. However, the LC corpora 
can give us an upper bound of 65% (see Table 3). 
This work also shows that the use of purely 
morphosyntactic information is not enough for 
the detection of errors in postpositions. For that 
reason we were forced to also include several 
types of semantic features into the system. On 
the other hand, the process of automatic error 
detection has also helped us to explore new sets 
of semantic distinctions. So, the process of error 
detection has helped us to organize concepts into 
sets of semantically related elements, and can 
serve to make explicit types of knowledge that 
can be used to enrich other linguistic resources. 
We can conclude saying that descriptive lin-
guistics could benefit from error diagnosis and 
detection, as this could help to deeply understand 
the linguistic descriptions of postpositions, which 
are done at the moment mainly by means of 
morphosyntactic information, insufficient to give 
an account of the involved phenomena.  
Acknowledgements 
This research is supported by the University of 
the Basque Country (GIU05/52) and the Basque 
Government (ANHITZ project, IE06-185). 
References 
Aduriz I., Agirre E., Aldezabal I., Alegria I., Arregi 
X., Arriola J., Artola X., Gojenola K., Sarasola 
K.  2000. A Word-grammar based morphological 
analyzer for agglutinative languages. COLING-00. 
Atwell E., Elliott S. (1987) Dealing with Ill-Formed 
English Text. In The Computational Analysis of 
English: a Corpus-Based Approach. Longman. 
Chodorow M., Tetreault J. and Han N. 2007. Detec-
tion of Grammatical Errors Involving Prepositions. 
4th ACL-SIGSEM Workshop on Prepositions. 
D?az de Ilarraza A., Gojenola K., Oronoz M.  2008. 
Detecting Erroneous Uses of Complex Postposi-
tions in an Agglutinative Language. Internal report 
(extended version). (https://ixa.si.ehu.es/Ixa/Argitalpenak) 
Elhuyar. 2000. Modern Basque Dictionary. Elkar.  
Euskaltzaindia. 1994. Basque Grammar: First Steps 
(in Basque). Euskaltzaindia. 
Golding A. and Schabes. Y. (1996) Combining tri-
gram-based and feature-based methods for context-
sensitive spelling correction. ACL 1996. 
Johannessen J.B., Hagen K., and Lane P. 2002. The 
performance of a grammar checker with deviant 
language input. Proceedings of COLING, Taiwan. 
Kukich K. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys. 
Tapanainen P. 1996. The Constraint Grammar parser 
CG-2. Publications of the Univ. of Helsinki, 27. 
Sarasola, Ibon. 2007. Basque Dictionary (in Basque). 
Donostia : Elkar, L.G. ISBN 978-84-9783-258-8. 
Zubiri I. and  Zubiri E. 1995. E. Euskal Gramatika 
Osoa (in Basque). Didaktiker, Bilbo. 
34
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 59?64,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Strategies for sustainable MT for Basque:  
incremental design, reusability, standardization and open-source 
 I. Alegria, X. Arregi, X. Artola, A. Diaz de Ilarraza, G. Labaka,  
M. Lersundi, A. Mayor, K. Sarasola 
Ixa taldea.  
University of the Basque Country. 
i.alegria@ehu.es 
 
 
 
Abstract 
We present some Language Technology 
applications that have proven to be effec-
tive tools to promote the use of Basque, a 
European less privileged language. We also 
present the strategy we have followed for 
almost twenty years to develop those appli-
cations as the top of an integrated environ-
ment of language resources, language 
foundations, language tools and other ap-
plications. When we have faced a difficult 
task such as Machine Translation to 
Basque, our strategy has worked well. We 
have had good results in a short time just 
reusing previous works for Basque, reusing 
other open-source tools, and developing 
just a few new modules in collaboration 
with other groups. In addition, new reus-
able tools and formats have been produced.  
1 Introduction and Basque Language 
Basque is a highly inflected minority language 
with free order of sentence constituents. Machine 
Translation for Basque is thus both, a real need and 
a test bed for our strategy to develop NLP tools for 
Basque.          
Basque is an isolate language, and little is 
known of its origins. It is likely that an early form 
of the Basque language was already present in 
Western Europe before the arrival of the Indo-
European languages. 
Basque is an agglutinative language, with a rich 
flexional morphology. In fact for nouns, for 
example, at least 360 word forms are possible for 
each lemma. Each of the declension cases such as 
absolutive, dative, associative? has four different 
suffixes to be added to the last word of the noun 
phrase. These four suffix variants correspond to 
undetermined, determined singular, determined 
plural and ?close? determined plural.  
Basque is also an ergative-absolutive language. 
The subject of an intransitive verb is in the 
absolutive case (which is unmarked), and the same 
case is used for the direct object of a transitive 
verb. The subject of the transitive verb (that is, the 
agent) is marked differently, with the ergative case 
(shown by the suffix -k). This also triggers main 
and auxiliary verbal agreement. 
The auxiliary verb, which accompanies most 
main verbs, agrees not only with the subject, but 
with the direct object and the indirect object, if 
present. Among European languages, this 
polypersonal system (multiple verb agreement) is 
only found in Basque, some Caucasian languages, 
and Hungarian. The ergative-absolutive alignment 
is rare among European languages, but not 
worldwide. 
Although in last centuries Basque suffered 
continuous regression it still remains alive. The 
region in which Basque is spoken is smaller than 
what is known as the Basque Country, and the 
distribution of Basque speakers is not 
homogeneous there. The main reasons of this 
regression (Amorrortu, 2002) are that Basque was 
not an official language, and that it was out of 
educational system, out of media and out of 
industrial environments. Besides, the fact of being 
six different dialects made the wide development 
of written Basque difficult.  
However, after 1980, some of those features 
changed and many citizens and some local 
59
governments promote recovering of Basque 
Language.  
Today, Basque holds co-official language status 
in the Basque regions of Spain: the whole 
autonomous community of the Basque Country 
and some parts of Navarre. Basque has no official 
standing in the Northern Basque Country.   
In the past, Basque was associated with lack of 
education, stigmatized as uneducated, rural, or 
holding low economic and power resources. There 
is not such an association today; Basque speakers 
do not differ from Spanish or French monolinguals 
in any of these characteristics.  
Standard Basque, called Batua (unified) in 
Basque, was defined by the Academy of Basque 
Language (Euskaltzaindia) in 1968. At present, its 
morphology is completely standardized, but the 
lexical standardization process is still underway. 
Now this is the language model taught in most 
schools and used on some media and official 
papers published in Basque.  
Basque speakers are about 700,000, about 25% 
of the total population of the Basque Country, but 
they are not evenly distributed. Still the use of 
Basque in industry and specially in Information 
and Communication Technology is not 
widespread. A language that seeks to survive in the 
modern information society has to be present also 
in such field and this requires language technology 
products. Basque, as other minority languages, has 
to make a great effort to face this challenge (Petek, 
2000; Williams et al, 2001).  
2 Strategy to develop Human Language 
Technology (HLT) in Basque 
IXA group is a research Group created in 1986 by 
5 university lecturers in the computer science fac-
ulty of the University of the Basque Country with 
the aim of laying foundations for research and de-
velopment of NLP software mainly for Basque. 
We wanted to face the challenge of adapting 
Basque to language technology. 
Twenty one years later, now IXA is a group 
composed of 28 computer scientists, 13 linguists 
and 2 research assistants. It works in cooperation 
with more than 7 companies from Basque Country 
and 5 from abroad; it has been involved in the birth 
of two new spin-off companies; and it has devel-
oped more than seven language technology prod-
ucts. 
In recent years, several private companies and 
technology centers in the Basque Country have 
begun to get interested and to invest in this area. At 
the same time, more agents have come to be aware 
of the fact that collaboration is essential to the de-
velopment of language technologies for minority 
languages. One of the fruits of this collaboration 
are HIZKING21 (2002-2005) and ANHITZ (2006-
2008) projects. Both projects were accepted by the 
Government of the Basque Country in a new 
strategical research line called ?Language Infoen-
gineering?. 
At the very beginning, twenty years ago, our 
first goal was just to create a Spanish-Basque 
translation system, but after some preliminary 
work we realized that instead of wasting our time 
in creating an ad hoc MT system with small accu-
racy, we had to invest our effort in creating basic 
tools such as a morphological analyzer/generator 
for Basque, that could later be used to build not 
only a more robust MT system but also other ap-
plications. 
This thought was the seed to design our strategy 
to make progress in the adaptation of Basque to 
Language Technology. Basque language had to 
face up scarcity of resources and tools that could 
make possible its development in Language Tech-
nology at a reasonable and competitive rate. 
We presented an open proposal for making pro-
gress in Human Language Technology (Aduriz et 
al., 1998). Anyway, the steps proposed did not cor-
respond exactly with those observed in the history 
of the processing of English, because the high ca-
pacity and computational power of new computers 
allowed facing problems in a different way.  
Our strategy may be described in two points: 
1) The need for standardization of resources to 
be useful in different researches, tools and applica-
tions 
2) The need for incremental design and devel-
opment of language foundations, tools, and appli-
cations in a parallel and coordinated way in order 
to get the best benefit from them. Language foun-
dations and research are essential to create any tool 
or application; but in the same way tools and ap-
plications will be very helpful in the research and 
improvement of language foundations. 
Following this strategy, our steps on standardi-
zation of resources led us to adopt TEI and XML 
standards and also to define a methodology for 
60
stand-off corpus tagging based on TEI, feature 
structures and XML (Artola et al, 2005). 
In the same way, taking as reference our experi-
ence in incremental design and development we 
proposed four phases as a general strategy for lan-
guage processing. These are the phases defined 
with the products to be developed in each of them. 
1. Initial phase: Foundations. Corpus I (collection 
of raw text with no tagging mark). Lexical da-
tabase I (the first version could be a list of 
lemmas and affixes). Machine-readable dic-
tionaries. Morphological description.  
2. Second phase: Basic tools and applications. 
Statistical tools for the treatment of corpora. 
Morphological analyzer/generator. Lemma-
tizer/tagger. Spelling checker and corrector (al-
though in morphologically simple languages a 
word list could be enough). Speech processing 
at word level. Corpus II (word-forms are 
tagged with their part of speech and lemma). 
Lexical database II (lexical support for the con-
struction of general applications, including part 
of speech and morphological information). 
3. Third phase: Advanced tools and applications. 
An environment for tool integration. Web 
search engine.  A traditional search machine 
that integrates lemmatization and language 
identification. Surface syntax. Corpus III (syn-
tactically tagged text). Grammar and style 
checkers. Structured versions of dictionaries 
(they allow enhanced functionality not avail-
able for printed or raw electronic versions). 
Lexical database III (the previous version is en-
riched with multiword lexical units. Integration 
of dictionaries in text editors). Lexical-
semantic knowledge base. Creation of a con-
cept taxonomy (e.g.: Wordnet). Word-sense 
disambiguation. Speech processing at sentence 
level. Basic Computer Aided Language Learn-
ing (CALL) systems 
4. Fourth phase: Multilingualism and general 
applications. Information extraction. Transla-
tion aids (integrated use of multiple on-line 
dictionaries, translation of noun phrases and 
simple sentences). Corpus IV (semantically 
tagged text after word-sense disambiguation). 
Dialog systems. Knowledge base on multilin-
gual lexico-semantic relations and its applica-
tions.  
We will complete this strategy with some sug-
gestions about what shouldn?t be done when work-
ing on the treatment of minority languages. a) Do 
not start developing applications if linguistic foun-
dations are not defined previously; we recommend 
following the above given sequence: foundations, 
tools and applications. b) When a new system has 
to be planned, do not create ad hoc lexical or syn-
tactic resources; you should design those resources 
in a way that they could be easily extended to full 
coverage and reusable by any other tool or applica-
tion. c) If you complete a new resource or tool, do 
not keep it to yourself; there are many researchers 
working on English, but only a few on each minor-
ity language; thus, the few results should be public 
and shared for research purposes, for it is desirable 
to avoid needless and costly repetition of work. 
3 Machine Translation for Basque 
After years working on basic resources and tools 
we decided it was time to face  the MT task (Hut-
chins and Somers, 1992). Our general strategy was 
more specifically for Machine Translation defined 
bearing in mind the following concepts:  
? reusability of previous resources, specially 
lexical resources and morphology of Basque 
? standardization and collaboration: using a 
more general framework in collaboration 
with other groups working in NLP 
? open-source: this means that anyone having 
the necessary computational and linguistic 
skills will be able to adapt or enhance it to 
produce a new MT system,  
Due to the real necessity for translation in our 
environment the involved languages would be 
Basque, Spanish and English. 
From the beginning we wanted to combine the 
two basic approaches for MT (rule-based and cor-
pus-based) in order to build a hybrid system, be-
cause it is generally agreed that there are not 
enough corpora for a good corpus-based system in 
minority languages like Basque.  
Data-driven Machine Translation (example-
based or statistical) is nowadays the most prevalent 
trend in Machine Translation research. Translation 
results obtained with this approach have already 
reached a high level of accuracy, especially when 
the target language is English. But these Data-
driven MT systems base their knowledge on 
aligned bilingual corpora, and the accuracy of their 
61
output depends heavily on the quality and the size 
of these corpora. Large and reliable bilingual cor-
pora are unavailable for many language pairs. 
3.1 The rule-based approach 
First, we present the main architecture and the pro-
posed standards of an open source MT engine, the 
first implementation of which translates from 
Spanish into Basque using the traditional transfer 
model and based on shallow and dependency pars-
ing. 
The design and the programs are independent 
from the languages, so the software can be used for 
other projects in MT. Depending on the languages 
included in the adaptation, it will be necessary to 
add, reorder and change some modules, but this 
will not be difficult because a unique XML format 
is used for the communication among all the mod-
ules. 
The project has been integrated in the OpenTrad 
initiative (www.opentrad.com), a government-
funded project shared among different universities 
and small companies, which also include MT en-
gines for translation among the main languages in 
Spain. The main objective of this initiative is the 
construction of an open, reusable and interoperable 
framework. 
In the OpenTrad project, two different but coor-
dinated designs have been carried out: 
? A shallow-transfer machine translation en-
gine for similar languages (Spanish, Catalan 
and Galician by the the time being). The 
MT architecture uses finite-state transducers 
for lexical processing, hidden Markov mod-
els for part-of-speech tagging, and chunking 
based on finite-state for structural transfer. 
It is named Apertium and it can be 
downloaded from apertium.sourceforge.net. 
(Armentano-Oller et al, 2004) 
? A deeper-transfer engine for the Spanish-
Basque pair. It is named Matxin (Alegria et 
al., 2007) and it is stored in 
matxin.sourceforge.net. It is an extension of 
previous work in our group. In order to re-
use resources in this Spanish-Basque system 
the analysis module for similar languages 
was not included in Matxin; another open 
source engine, FreeLing (Carreras et al, 
2004), was used here, of course, and its out-
put had to be converted to the proposed in-
terchange format. 
Some of the components (modules, data formats 
and compilers) from the first architecture in Open-
Trad were used in the second one. Indeed, an im-
portant additional goal of this work was testing 
which modules from the first architecture could be 
integrated in deeper-transfer architectures for more 
difficult language pairs. 
The transfer module is also based on three main 
objects in the translation process: words or nodes, 
chunks or phrases, and sentences.  
? First, lexical transfer is carried out using a 
bilingual dictionary compiled into a finite-
state transducer. We use the XML specifica-
tion of Apertium engine.  
? Then, structural transfer at the sentence 
level is applied, and some information is 
transferred from some chunks to others, and 
some chunks may disappear. Grammars 
based on regular expressions are used to 
specify these changes. For example, in the 
Spanish-Basque transfer, the person and 
number information of the object and the 
type of subordination are imported from 
other chunks to the chunk corresponding to 
the verb chain. 
? Finally the structural transfer at the chunk 
level is carried out. This process can be 
quite simple (e.g. noun chains between 
Spanish and Basque) or more complex (e.g. 
verb chains between these same languages). 
The XML file coming from the transfer module 
is passed on the generation module. 
? In the first step, syntactic generation is per-
formed in order to decide the order of 
chunks in the sentence and the order of 
words in the chunks. Several grammars are 
used for this purpose.  
? Morphological generation is carried out in 
the last step. In the generation of Basque, 
the main inflection is added to the last word 
in the phrase (in Basque: the declension 
case, the article and other features are added 
to the whole noun phrase at the end of the 
last word), but in verb chains other words 
need morphological generation. A previous 
morphological analyzer/generator for 
Basque (Alegria et al, 1996) has been 
adapted and transformed to the format used 
in Apertium. 
The results for the Spanish/Basque system using 
FreeLing and Matxin are promising. The quantita-
62
tive evaluation uses the open source evaluation 
tool IQMT and figures are given using Bleu and 
NIST measures (Gim?nez et al, 2005). An user 
based evaluation has been carried out too. 
3.2 The corpus-based approach 
The corpus-based approach has been carried out in 
collaboration with the National Center for Lan-
guage Technology in Dublin.  
The system exploits both EBMT and SMT tech-
niques to extract a dataset of aligned chunks. We 
conducted Basque to English and Spanish to 
Basque translation experiments, evaluated on a 
large corpus (270, 000 sentence pairs).  
Some tools have been reused for this purpose: 
? GIZA++: for word/morpheme alignment we 
used the GIZA++ statistical word alignment 
toolkit, and following the ?refined? method 
of (Och and Ney, 2003), extracted a set of 
high-quality word/ morpheme alignments 
from the original unidirectional alignment 
sets. These along with the extracted chunk 
alignments were passed to the translation 
decoder.                                         
? Pharaoh/Moses decoder: the decoder is also 
a hybrid system which integrates EBMT 
and SMT. It is capable of retrieving already 
translated sentences and also provides a 
wrapper around the PHARAOH SMT de-
coder (Koehn, 2004). 
? MaTrEx: the MATREX (Machine Transla-
tion using Examples) system used in our 
experiments is a data-driven MT engine, 
built following an extremely modular de-
sign. It consists of a number of extensible 
and re-implementable modules (Way and 
Gough, 2005). 
   For this engine, we reuse a toolkit to chunk the 
Basque sentences. After this processing stage, a 
sentence is treated as a sequence of morphemes, in 
which chunk boundaries are clearly visible. Mor-
phemes denoting morphosyntactic features are re-
placed by conventional symbolic strings. After 
some adaptation, the chunks obtained in this man-
ner are actually very comparable to the English 
chunks obtained with the marker-based chunker. 
The experimental results have shown that our 
system significantly outperforms state-of-the-art 
approaches according to several common auto-
matic evaluation metrics: WER, Bleu and PER 
(Stroppa et al, 2006; Labaka et al, 2007). 
4 Conclusions 
A language that seeks to survive in the modern 
information society requires language technology 
products. "Minority" languages have to do a great 
effort to face this challenge. The Ixa group has 
been working since 1986 on adapting Basque to 
language technology, having developed several 
applications that are effective tools to promote the 
use of Basque. Now we are planning to define the 
BLARK for Basque (Krauwer, 2003).  
From our experience, we defend that research 
and development for a minority language should to 
be faced following these points: high standardiza-
tion,  reusing language foundations, tools, and ap-
plications, and their incremental design and devel-
opment. We know that any HLT project related to 
a less privileged language should follow those 
guidelines, but from our experience we know that 
in most cases they do not. We think that if Basque 
is now in an good position in HLT is because those 
guidelines have been applied even  when it was 
easier to define "toy" resources and tools useful to 
get good short term academic results, but not reus-
able in future developments.  
This strategy has been completely useful when 
we have created MT systems for Basque. Reusing 
previous works for Basque (that were defined fol-
lowing XML and TEI standards) and reusing other 
open-source tools have been the key to get satisfac-
tory results in a short time.  
Two results produced in the MT track are pub-
licly available:  
? matxin.sourceforge.net for the free code for 
the Spanish-Basque RBMT system 
? www.opentrad.org for the on-line demo  
Acknowledgments 
This work has been partially funded by the Spanish 
Ministry of Education and Science (OpenMT: 
Open Source Machine Translation using hybrid 
methods,TIN2006-15307-C03-01) and the Local 
Government of the Basque Country (AnHITZ 
2006: Language Technologies for Multingual In-
teraction in Intelligent Environments., IE06-185). 
Andy Way, Declan Groves and Nicolas Stroppa 
from National Centre for Language Technology in 
Dublin are kindly acknowledged for providing 
their expertise on the Matrex system and the 
evaluation of the output. 
63
References 
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, O. Ansa, 
X. Arregi, J. Arriola, X. Artola, A. D?az de Ilarraza, 
N. Ezeiza, K.Gojenola, M. Maritxalar, M. Oronoz, K. 
Sarasola, A. Soroa, R. Urizar. 1998. A framework for 
the automatic processing of Basque. Proceedings of 
Workshop on Lexical Resources for Minority Lan-
guages.  
I. Alegria, X. Artola, K. Sarasola. 1996.Automatic mor-
phological analysis of Basque. Literary & Linguistic 
Computing Vol. 11, No. 4, 193-203. Oxford Univer-
sity Press. Oxford. 1996. 
I. Alegria, A. D?az de Ilarraza, G. Labaka, M Lersundi, 
A. Mayor, K. Sarasola.  2007. Transfer-based MT 
from Spanish into Basque: reusability, standardiza-
tion and open source. LNCS 4394. 374-384. Cicling 
2007.  
E. Amorrortu. 2002. Bilingual Education in the Basque 
Country: Achievements and Challenges after Four 
Decades of Acquisition Planning. Journal of Iberian 
and Latin American Literary and Cultural Stud-
ies.Volume 2 Number 2 (2002) 
C. Armentano-Oller, A. Corb?-Bellot, M. L. Forcada, 
M. Ginest?-Rosell, B. Bonev, S. Ortiz-Rojas, J. A. 
P?rez-Ortiz, G. Ram?rez-S?nchez, F. S?nchez-
Mart?nez, 2005. An open-source shallow-transfer 
machine translation toolbox: consequences of its re-
lease and availability. Proceedings of OSMaTran: 
Open-Source Machine Translation workshop, MT 
Summit X. 
X. Artola, A. D?az de Ilarraza, N. Ezeiza, K. Gojenola, 
G. Labaka, A. Sologaistoa, A. Soroa.  2005. A 
framework for representing and managing linguistic 
annotations based on typed feature structures. Proc. 
of RANLP 2005. 
X. Carreras,, I. Chao, L. Padr? and M. Padr?. 2004. 
FreeLing: An open source Suite of Language Ana-
lyzers, in  Proceedings of the 4th International Con-
ference on Language Resources and Evaluation 
(LREC'04).  
J. Gim?nez, E. Amig?, C. Hori. 2005. Machine 
Translation Evaluation Inside QARLA. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Technology (IWSLT'05) 
W. Hutchins and H. Somers. 1992. An Introduction to 
Machine Translation. Academic Press. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els.  In Proceedings of AMTA-04, pages 115?124, 
Washington, District of Columbia. 
S. Krauwer. 2003. The Basic Language Resource Kit 
(BLARK) as the First Milestone for the Language 
Resources Roadmap. Proc. of the International 
Workshop  Speech and Computer. Moscow, Russia. 
G. Labaka, N. Stroppa, A. Way, K. Sarasola  2007 
Comparing Rule-Based and Data-Driven Approaches 
to Spanish-to-Basque Machine Translation Proc. of 
MT-Summit XI, Copenhagen 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1): 19?51. 
B. Petek. 2000. Funding for research into human lan-
guage technologies for less prevalent languages, Sec-
ond International Conference on Language Re-
sources and Evaluation (LREC 2000). Athens, 
Greece. 
N. Stroppa, D. Groves, A. Way, K. Sarasola K. 2006. 
Example-Based Machine Translation of the Basque 
Language. AMTA. 7th conference of the Association 
for Machine Translation in the Americas.. 
A. Way and N. Gough. 2005. Comparing Example-
Based and Statistical Machine Translation. Natural 
Language Engineering, 11(3):295?309. 
B. Williams, K. Sarasola, D. ??Cr?inin, B. Petek. 2001. 
Speech and Language Technology for Minority Lan-
guages. Proceedings of Eurospeech 2001 
 
 
64
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine Learning Techniques to Build a Comma Checker for 
Basque
I?aki Alegria Bertol Arrieta Arantza Diaz de Ilarraza Eli Izagirre Montse Maritxalar
Computer Engineering Faculty. University of the Basque Country.
Manuel de Lardizabal Pasealekua, 1
20018 Donostia, Basque Country, Spain.
{acpalloi,bertol,jipdisaa,jibizole,jipmaanm}@ehu.es
Abstract
In this paper, we describe the research 
using  machine  learning  techniques  to 
build a comma checker to be integrated 
in a grammar checker for Basque. After 
several experiments, and trained with a 
little corpus of 100,000 words, the sys?
tem guesses correctly not placing com?
mas with a precision of 96% and a re?
call of 98%. It also gets a precision of 
70% and a recall of 49% in the task of 
placing  commas.  Finally,  we  have 
shown  that  these  results  can  be  im?
proved using a bigger and a more ho?
mogeneous  corpus  to  train,  that  is,  a 
bigger corpus written by one unique au?
thor. 
1 Introduction
In the last years, there have been many studies 
aimed  at  building  a  grammar  checker  for  the 
Basque language (Ansa et al, 2004; Diaz De Il?
arraza et al, 2005). These works have been fo?
cused, mainly, on building rule sets ??taking into 
account syntactic information extracted from the 
corpus  automatically??  that  detect  some  erro?
neous grammar forms. The research here presen?
ted wants to complement the earlier work by fo?
cusing on  the  style  and the  punctuation of  the 
texts. To be precise, we have experimented using 
machine learning techniques for the special case 
of the comma, to evaluate their performance and 
to analyse the possibility of applying it in other 
tasks of the grammar checker.  
However,  developing  a  punctuation  checker 
encounters  one  problem  in  particular:  the  fact 
that the punctuation rules are not totally estab?
lished. In general, there is no problem when us?
ing the  full  stop,  the  question mark or  the ex?
clamation mark.  Santos (1998) highlights these 
marks are reliable punctuation marks, while all 
the rest are unreliable. Errors related to the reli?
able ones (putting or not the initial  question or 
exclamation mark depending on the language, for 
instance) are not so hard to treat. A rule set to 
correct some of these has already been defined 
for the Basque language (Ansa et al, 2004). In 
contrast, the comma is the most polyvalent and, 
thus, the least defined punctuation mark (Bayrak?
tar et al, 1998; Hill and Murray, 1998). The am?
biguity of the comma, in fact,  has been shown 
often (Bayraktar et  al.,  1998; Beeferman et al, 
1998;  Van  Delden  S.  and  Gomez  F.,  2002). 
These works have shown the lack of fixed rules 
about the comma. There are only some intuitive 
and  generally  accepted  rules,  but  they  are  not 
used in a standard way. In Basque, this problem 
gets even more evident, since the standardisation 
and  normalisation  of  the  language  began  only 
about twenty?five years ago and it  has not fin?
ished yet. Morphology is mostly defined, but, on 
the contrary, as far as syntax is concerned, there 
is  quite  work  to  do.  In  punctuation  and  style, 
some basic rules have been defined and accepted 
by the Basque Language Academy (Zubimendi, 
2004).  However,  there  are  not  final  decisions 
about the case of the comma. 
Nevertheless,  since  Nunberg?s  monograph 
(Nunberg, 1990), the importance of the comma 
has  been  undeniable,  mainly  in  these  two  as?
pects: i) as a due to the syntax of the sentence 
(Nunberg, 1990; Bayraktar et al, 1998; Garzia, 
1997), and ii) as a basis to improve some natural 
language  processing  tools  (syntactic  analysers, 
error  detection  tools?),  as  well  as  to  develop 
some  new  ones  (Briscoe  and  Carroll,  1995; 
Jones, 1996). The relevance of the comma for the 
syntax of the sentence may be easily proved with 
some clarifying examples where the sentence is 
understood in  one or  other  way,  depending on 
whether  a  comma  is  placed  or  not  (Nunberg, 
1990): 
a. Those students who can, contribute to the 
United Fund. 
b. Those students who can contribute to the 
United Fund. 
1
In the same sense,  it  is  obvious  that  a  well 
punctuated  text,  or  more  concretely,  a  correct 
placement of the commas, would help consider?
ably  in  the  automatic  syntactic  analysis  of  the 
sentence,  and, therefore,  in the development of 
more and better tools in the NLP field. Say and 
Akman (1997) summarise the research efforts in 
this direction.
As an important background for our work, we 
note  where  the  linguistic  information  on  the 
comma for the Basque language was formalised. 
This  information  was  extracted  after  analysing 
the  theories  of  some experts  in  Basque  syntax 
and punctuation (Aldezabal et al, 2003). In fact, 
although no final decisions have been taken by 
the Basque Language Academy yet,  the theory 
formalised in the above mentioned work has suc?
ceeded in unifying the main points of view about 
the  punctuation in  Basque.  Obviously,  this  has 
been the basis for our work. 
2 Learning commas
We have designed two different but combinable 
ways to get the comma checker:
? based on clause boundaries
? based directly on corpus
Bearing  in  mind  the formalised  theory  of 
Aldezabal et  al.  (2003)1,  we realised that if  we 
got to split the sentence into clauses, it would be 
quite easy to develop rules for detecting the exact 
places where commas would have to go. Thus, 
the best way to build a comma checker would be 
to get, first, a clause identification tool. 
Recent papers in this area report quite good 
results using machine learning techniques. Car?
reras and M?rquez (2003) get one of the best per?
formances in this  task (84.36% in test).  There?
fore, we decided to adopt this as a basis in order 
to  get  an  automatic  clause  splitting  tool  for 
Basque.  But  as  it  is  known,  machine  learning 
techniques cannot be applied if no training cor?
pus is available, and one year ago, when we star?
ted this  process,  Basque texts  with this  tagged 
clause splits were not available.
Therefore, we decided to use the second al?
ternative.  We  had  available  some  corpora  of 
Basque, and we decided to try learning commas 
from raw text, since a previous tagging was not 
needed. The problem with the raw text is that its 
commas are not the result of applying consistent 
rules.
1 From now on, we will speak about this as ?the accepted theory of Basque 
punctuation?. 
Related work
Machine learning techniques have been applied 
in many fields and for  many purposes,  but  we 
have found only one reference in the literature 
related to the use of machine learning techniques 
to assign commas automatically. 
Hardt (2001) describes research in using the 
Brill tagger (Brill 1994; Brill, 1995) to learn to 
identify incorrect commas in Danish. The system 
was developed by randomly inserting commas in 
a text, which were tagged as incorrect, while the 
original  commas  were  tagged  as  correct.  This 
system identifies incorrect commas with a preci?
sion  of  91%  and  a  recall  of  77%,  but  Hardt 
(2001) does not mention anything about identify?
ing correct commas. 
In  our  proposal,  we have tried  to  carry out 
both aspects, taking as a basis other works that 
also use machine learning techniques in similar 
problems  such  as  clause  splitting  (Tjong  Kim 
Sang E.F. and D?jean H., 2001) or detection of 
chunks (Tjong Kim Sang E.F. and Buchholz S., 
2000).
3 Experimental setup
Corpora
As we have mentioned before, some corpora 
in Basque are available. Therefore, our first task 
was to select the training corpora, taking into ac?
count that well punctuated corpora were needed 
to train the machine correctly. For that purpose, 
we looked for corpora that satisfied as much as 
possible our ?accepted theory of Basque punctu?
ation?.  The  corpora  of  the  unique  newspaper 
written in Basque, called  Egunkaria (nowadays 
Berria), were chosen, since they are supposed to 
use the ?accepted theory of Basque punctuation?. 
Nevertheless,  after  some brief  verifications, we 
realised that the texts of the corpora do not fully 
match with our theory. This can be understood 
considering that a lot of people work in a news?
paper. That is, every journalist can use his own 
interpretation of  the  ?accepted theory?,  even if 
all of them were instructed to use it in the same 
way. Therefore, doing this  research, we had in 
mind that the results we would get were not go?
ing to be perfect.
To counteract this problem, we also collected 
more  homogeneous  corpora  from  prestigious 
writers: a translation of a book of philosophy and 
a novel. Details about these corpora are shown in 
Table 1.
2
Size of the corpora
Corpora from the newspaper Egunkaria 420,000 words
Philosophy texts written by one unique author 25,000 words
Literature texts written by one unique author 25,000 words
Table 1. Dimensions of the used corpora
A short version of the first corpus was used in 
different experiments in order to tune the system 
(see section 4). The differences between the re?
sults  depending on the type of  the corpora are 
shown in section 5.
Evaluation
Results are shown using the standard measures in 
this area: precision, recall and f?measure2, which 
are calculated based on the test corpus. The res?
ults are shown in two colums ("0" and "1") that 
correspond to the result categories used. The res?
ults for the column ?0? are the ones for the in?
stances that are not followed by a comma. On the 
contrary, the results for the column ?1? are the 
results for the instances that should be followed 
by a comma. 
Since  our  final  goal  is  to  build  a  comma 
checker,  the precision in the column ?1? is the 
most  important  data  for  us,  although the recall 
for the same column is also relevant. In this kind 
of tools, the most important thing is to first ob?
tain all the comma proposals right (precision in 
columns ?1?), and then to obtain all the possible 
commas (recall in columns ?1?).
Baselines
In  the  beginning,  we  calculated  two  possible 
baselines based on a big part of the newspaper 
corpora in order to choose the best one. 
The  first  one  was  based  on  the  number  of 
commas  that  appeared  in  these  texts.  In  other 
words,  we  calculated  how  many  commas  ap?
peared in the corpora (8% out of all words), and 
then we put commas randomly in this proportion 
in the test corpus. The results obtained were not 
very good (see Table 2, baseline1), especially for 
the  instances  ?followed by  a  comma? (column 
?1?).
The second baseline was developed using the 
list  of  words appearing before a comma in the 
training corpora. In the test corpus, a word was 
tagged as ?followed by a comma? if it was one of 
the words of the mentioned list. The results (see 
baseline 2, in Table 2) were better, in this case, 
for the instances followed by a comma (column 
named  ?1?).  But,  on  the  contrary,  baseline  1 
provided us with better results for the instances 
not followed by a comma (column named ?0?). 
That is why we decided to take, as our baseline, 
2 f?measure = 2*precision*recall / (precision+recall)
the best data offered by each baseline (the ones 
in bold in table 2). 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
baseline 1 0.927 0.924 0.926 0.076 0.079 0.078
baseline 2 0.946 0.556 0.700 0,096 0.596 0.165
Table 2: The baselines
Methods and attributes
We  use  the  WEKA3 implementation  of  these 
classifiers: the Naive Bayes based classifier (Na?
iveBayes),  the  support  vector  machine  based 
classifier  (SMO)  and  the  decision?tree  (C4.5) 
based one (j48).
It  has  to  be  pointed  out  that  commas  were 
taken  away  from  the  original  corpora.  At  the 
same time, for each token, we stored whether it 
was followed by a  comma or not.  That  is,  for 
each  word  (token),  it  was  stored  whether  a 
comma was placed next to it or not. Therefore, 
each token in the corpus is equivalent to an ex?
ample (an instance). The attributes of each token 
are based on the token itself and some surround?
ing ones. The application window describes the 
number of tokens considered as information for 
each token.
Our initial application window was [?5, +5]; 
that means we took into account the previous and 
following 5 words (with their corresponding at?
tributes)  as  valid  information  for  each  word. 
However, we tuned the system with different ap?
plication windows (see section 4). 
Nevertheless, the attributes managed for each 
word can be as complex as we want. We could 
only use words, but we thought some morpho?
syntactic information would be beneficial for the 
machine to learn. Hence, we decided to include 
as much information as we could extract using 
the shallow syntactic parser of Basque (Aduriz et 
al.,  2004).  This  parser  uses  the  tokeniser,  the 
lemmatiser, the chunker and the morphosyntactic 
disambiguator  developed by  the  IXA4 research 
group. 
The attributes we chose to use for each token 
were the following:
? word?form
? lemma
? category 
? subcategory
? declension case
? subordinate?clause type
3 WEKA is a collection of machine learning algorithms for data mining tasks 
(http://www.cs.waikato.ac.nz/ml/weka/).
4 http://ixa.si.ehu.es
3
? beginning of chunk (verb, nominal, enti?
ty, postposition)
? end of chunk (verb, nominal, entity, post?
position)
? part of an apposition
? other  binary  features:  multiple  word  to?
ken,  full  stop,  suspension  points,  colon, 
semicolon,  exclamation  mark  and  ques?
tion mark 
We also included some additional  attributes 
which were automatically calculated: 
? number of verb chunks to the beginning 
and to the end of the sentence 
? number of nominal chunks to the begin?
ning and to the end of the sentence
? number  of  subordinate?clause  marks  to 
the beginning and to the end of the sen?
tence
? distance (in tokens) to the beginning and 
to the end of the sentence 
We also did other experiments using binary 
attributes that correspond to most used colloca?
tions (see section 4).
Besides, we used the result attribute ?comma? 
to store whether a comma was placed after each 
token. 
4 Experiments
Dimension of the corpus
In  this  test,  we  employed the  attributes  de?
scribed in section 3 and an initial window of [?5, 
+5], which means we took into account the pre?
vious 5 tokens and the following 5. We also used 
the C4.5 algorithm initially, since this algorithm 
gets very good results in other similar machine 
learning  tasks  related  to  the  surface  syntax 
(Alegria et al, 2004).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
100,000 train / 30,000 test 0,955 0,981 0,968 0,635 0,417 0,503
160,000 train / 45,000 test 0,947 0,981 0,964 0,687 0,43 0,529
330,000 train / 90,000 test 0,96 0,982 0,971 0,701 0,504 0,587
Table 3. Results depending on the size of corpora 
(C4.5 algorithm; [?5,+5] window).
As it  can be seen in table 3, the bigger the 
corpus,  the  better  the results,  but  logically,  the 
time expended to obtain the results also increases 
considerably. That is why we chose the smallest 
corpus  for  doing  the  remaining  tests  (100,000 
words  to  train  and  30,000  words  to  test).  We 
thought that the size of this corpus was enough to 
get good comparative results. This test, anyway, 
suggested that the best  results  we could obtain 
would  be  always  improvable  using  more  and 
more corpora. 
Selecting the window
Using the corpus and the attributes described be?
fore, we did some tests to decide the best applic?
ation window. As we have already mentioned, in 
some problems of this type, the information of 
the  surrounding  words  may  contain  important 
data to decide the result of the current word. 
In this test, we wanted to decide the best ap?
plication window for our problem. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
-5+5 0,955 0,981 0,968 0,635 0,417 0,503
-2+5 0,956 0,982 0,969 0,648 0,431 0,518
-3+5 0,957 0,979 0,968 0,627 0,441 0,518
-4+5 0,957 0,98 0,968 0,634 0,446 0,52
-5+2 0,956 0,982 0,969 0,65 0,424 0,514
-5+3 0,956 0,981 0,969 0,643 0,432 0,517
-5+4 0,955 0,982 0,968 0,64 0,417 0,505
-6+2 0,956 0,982 0,969 0,645 0,421 0,509
-6+3 0,956 0,982 0,969 0,646 0,426 0,514
-8+2 0,956 0,982 0,969 0,645 0,425 0,513
-8+3 0,956 0,979 0,967 0,615 0,431 0,507
-8+8 0,956 0,978 0,967 0,604 0,422 0,497
Table  4.  Results  depending  on  the  application 
window (C4.5 algorithm; 100,000 train / 30,000 
test)
As it can be seen, the best f?measure for the 
instances followed by a comma was obtained us?
ing the application window [?4,+5]. However, as 
we have said before, we are more interested in 
the precision. Thus, the application window [?5
,+2] gets the best precision, and, besides, its f?
measure is almost the same as the best one. This 
is the reason why we decided to choose the [?5
,+2] application window. 
Selecting the classifier
With  the  selected  attributes,  the  corpus  of 
130,000 words and the application window of [?5
, +2], the next step was to select the best classifi?
er for our problem. We tried the WEKA imple?
mentation of these classifiers:  the Naive Bayes 
based classifier (NaiveBayes), the support vector 
machine based classifier (SMO) and the decision 
tree based one (j48).  Table 5 shows the results 
obtained:
4
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
NB 0,948 0,956 0,952 0,376 0,335 0,355
SMO 0,936 0,994 0,965 0,672 0,143 0,236
J48 0,956 0,982 0,969 0,652 0,424 0,514
Table 5. Results depending on the classifier 
(100,000 train / 30,000 test; [?5, +2] window).
As we can see, the f?measure for the instances 
not followed by a comma (column ?0?) is almost 
the same for the three classifiers, but, on the con?
trary, there is a considerable difference when we 
refer  to  the  instances  followed  by  a  comma 
(column ?1?). The best f?measure gives the C4.5 
based classifier (J48) due to the better recall, al?
though the best precision is for the support vector 
machine  based  classifier  (SMO).  Definitively, 
the Na?ve Bayes (NB) based classifier was dis?
carded, but we had to think about the final goal 
of our research to choose between the other two 
classifiers.  Since our  final  goal  was to  build  a 
comma checker, we would have to have chosen 
the classifier that gave us the best precision, that 
is, the support vector machine based one. But the 
recall of the support vector machine based classi?
fier was not as good as expected to be selected. 
Consequently,  we  decided  to  choose  the  C4.5 
based classifier. 
Selecting examples
At this  moment,  the results  we get  seem to be 
quite good for the instances not  followed by a 
comma, but  not  so good for  the  instances  that 
should follow a comma. This could be explained 
by the fact that we have no balanced training cor?
pus. In other words, in a normal text, there are a 
lot  of  instances not  followed by a  comma, but 
there are not so many followed by it. Thus, our 
training  corpus,  logically,  has  very  different 
amounts of instances followed by a comma and 
not followed by a comma. That is the reason why 
the system will learn more easily to avoid the un?
necessary  commas  than  placing  the  necessary 
ones. 
Therefore,  we  resolved  to  train  the  system 
with a corpus where the number of instances fol?
lowed by a comma and not followed by a comma 
was the same. For that purpose, we prepared a 
perl program that changed the initial corpus, and 
saved only x words for each word followed by a 
comma. 
In  table  6,  we can see  the  obtained results. 
One to one means that in that case, the training 
corpus  had  one  instance  not  followed  by  a 
comma, for each instance followed by a comma. 
On the  other  hand,  one to  two means that  the 
training corpus had two instances not  followed 
by  a  comma  for  each  word  followed  by  a 
comma, and so on. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
normal 0,955 0,981 0,968 0,635 0,417 0,503
one to one 0,989 0,633 0,772 0,164 0,912 0,277
one to two 0,977 0,902 0,938 0,367 0,725 0,487
one to three 0,969 0,934 0,951 0,427 0,621 0,506
one to four 0,966 0,952 0,959 0,484 0,575 0,526
one to five 0,966 0,961 0,963 0,534 0,568 0,55
one to six 0,963 0,966 0,964 0,55 0,524 0,537
Table  6.  Results  depending  on  the  number  of 
words  kept  for  each  comma  (C4.5  algorithm; 
100,000 train / 30,000 test; [?5, +2] window). 
As  observed  in  the  previous  table,  the  best 
precision in the case of the instances followed by 
a comma is the original one: the training corpus 
where  no  instances  were  removed.  Note  that 
these results are referred as normal in table 6.
The corpus where a unique instance not fol?
lowed by a comma is kept for each instance fol?
lowed by a comma gets the best  recall  results, 
but the precision decreases notably. 
The  best  f?measure  for  the  instances  that 
should be followed by a comma is obtained by 
the one to five scheme, but as mentioned before, 
a comma checker must take care of offering cor?
rect comma proposals. In other words, as the pre?
cision of the original corpus is quite better (ten 
points better), we decided to continue our work 
with  the  first  choice:  the  corpus  where  no  in?
stances were removed. 
Adding new attributes
Keeping the best results obtained in the tests de?
scribed above (C4.5 with the [?5,  +2] window, 
and not removing any ?not comma? instances), 
we thought that giving importance to the words 
that appear normally before the comma would in?
crease our results. Therefore, we did the follow?
ing tests: 
1) To search a big corpus in order to extract 
the most  frequent  one hundred words  that  pre?
cede a  comma,  the  most  frequent  one hundred 
pairs of words (bigrams) that precede a comma, 
and the most frequent one hundred sets of three 
words (trigrams) that precede a comma, and use 
them as attributes in the learning process. 
2) To use only three attributes instead of the 
mentioned three hundred to encode the informa?
tion  about  preceding  words.  The  first  attribute 
would indicate whether a word is or not one of 
5
the  most  frequent  one  hundred  words.  The 
second attribute would mean whether a word is 
or not the last part of one of the most frequent 
one hundred pairs of words. And the third attrib?
ute would mean whether a word is or not the last 
part of one of the most frequent one hundred sets 
of three words. 
3) The case (1), but with a little difference: 
removing the attributes ?word? and ?lemma? of 
each instance. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
(0): normal 0,956 0,982 0,969 0,652 0,424 0,514
(1): 300 attributes + 0,96 0,983 0,972 0,696 0,486 0,572
(2): 3 attributes + 0,96 0,981 0,97 0,665 0,481 0,558
(3): 300 attributes +,  
no lemma, no word 0,955 0,987 0,971 0,71 0,406 0,517
Table 7. Results depending on the new attributes 
used (C4.5 algorithm; 100,000 train / 30,000 test; 
[?5, +2] window; not removed instances).
Table 7 shows that case number 1 (putting the 
300 data as attributes) improves the precision of 
putting  commas  (column  ?1?)  in  more  than  4 
points. Besides, it also improves the recall, and, 
thus, we improve almost 6 points its f?measure. 
The third test gives the best precision, but the 
recall decreases considerably. Hence, we decided 
to choose the case number 1, in table 7.
5 Effect of the corpus type
As we have said before (see section 3), depend?
ing on the quality of the texts, the results could 
be different.
In table 8, we can see the results using the dif?
ferent types of corpus described in table 1. Obvi?
ously,  to  give  a  correct  comparison,  we  have 
used the same size for all the corpora (20,000 in?
stances to train and 5,000 instances to test, which 
is the maximum size we have been able to ac?
quire for the three mentioned corpora).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
Newspaper 0.923 0.977 0.949 0.445 0.188 0.264
Philosophy 0.932 0.961 0.946 0.583 0.44 0.501
Literature 0.925 0.976 0.95 0.53 0.259 0.348
Table 8. Results depending on the type of corpo?
ra (20,000 train / 5,000 test).
The first line shows the results obtained using 
the short version of the newspaper. The second 
line  describes  the  results  obtained  using  the 
translation of a book of philosophy, written com?
pletely by one author. And the third one presents 
the  results  obtained  using  a  novel  written  in 
Basque. 
In any case, the results prove that our hypo?
thesis  was  correct.  Using  texts  written  by  a 
unique author improves the results. The book of 
philosophy has the best precision and the best re?
call.  It  could be  because it  has  very long sen?
tences  and  because  philosophical  texts  use  a 
stricter syntax comparing with the free style of a 
literature writer.  
As it was impossible for us to collect the ne?
cessary  amount  of  unique  author  corpora,  we 
could not go further in our tests.
6 Conclusions and future work
We have used machine learning techniques for 
the  task  of  placing  commas  automatically  in 
texts. As far as we know, it is quite a novel ap?
plication field. Hardt (2001) described a system 
which identified incorrect commas with a preci?
sion of 91% and a recall of 77% (using 600,000 
words  to  train).  These  results  are  comparable 
with the ones we obtain for the task of guessing 
correctly when not to place commas (see column 
?0? in the tables). Using 100,000 words to train, 
we obtain 96% of precision and 98.3% of recall. 
The main reason could be that we use more in?
formation to learn.
However, we have not obtained as good res?
ults as we hoped in the task of placing commas 
(we  get  a  precision  of  69.6%  and  a  recall  of 
48.6%). Nevertheless, in this particular task, we 
have  improved  considerably  with  the  designed 
tests, and more improvements could be obtained 
using more corpora and more specific corpora as 
texts written by a unique author or by using sci?
entific texts. 
Moreover,  we have detected some possible 
problems that could have brought these regular 
results in the mentioned task:
? No fixed rules for commas in the Basque 
language
? Negative influence when training using 
corpora from different writers
In this sense, we have carried out a little ex?
periment with some English corpora. Our hypo?
thesis was that a completely settled language like 
English,  where  comma  rules  are  more  or  less 
fixed, would obtain better results. Taking a com?
parative English corpus5 and similar learning at?
tributes6 to  Basque?s  one,  we  got,  for  the  in?
stances  followed  by  a  comma  (column  ?1?  in 
tables), a better precision (%83.3) than the best 
5 A newspaper corpus, from Reuters
6 Linguistic information obtained using Freeling (http://garraf.ep?
sevg.upc.es/freeling/)
6
one obtained for the Basque language. However, 
the recall was worse than ours: %38.7. We have 
to take into account that we used less learning at?
tributes with the English corpus and that we did 
not  change  the  application  window chosen  for 
the Basque experiment. Another application win?
dow would have been probably more suitable for 
English.  Therefore, we believe that with a few 
tests  we  easily  would  achieve  a  better  recall. 
These  results,  anyway,  confirm our  hypothesis 
and our diagnosis of the detected problems. 
Nevertheless,  we think the presented results 
for the Basque language could be improved. One 
way would  be  to  use  ?information  gain? tech?
niques in order to carry out the feature selection. 
On the other hand, we think that more syntactic 
information, concretely clause splits tags, would 
be especially beneficial to detect those commas 
named delimiters by Nunberg (1990).
In fact, our main future research will consist 
on clause identification. Based on the ?accepted 
theory of the comma?, we can assure that a good 
identification of clauses (together with some sig?
nificant linguistic information we already have) 
would enable us to put commas correctly in any 
text,  just  implementing some simple rules.  Be?
sides, a combination of both methods ??learning 
commas  and  putting  commas  after  identifying 
clauses??  would  probably  improve  the  results 
even more. 
Finally,  we contemplate building an ICALL 
(Intelligent Computer Assisted Language Learn?
ing) system to help learners to put commas cor?
rectly.
Acknowledgements
We would like to thank all the people who have 
collaborated in this research: Juan Garzia,  Joxe 
Ramon  Etxeberria,  Igone  Zabala,  Juan  Carlos 
Odriozola, Agurtzane Elorduy, Ainara Ondarra, 
Larraitz Uria and Elisabete Pociello. 
This research is supported by the University 
of  the  Basque  Country  (9/UPV00141.226?
14601/2002) and the Ministry of Industry of the 
Basque  Government  (XUXENG  project, 
OD02UN52).
References
Aduriz  I., Aranzabe  M., Arriola  J., D?az  de  Ilarraza 
A., Gojenola  K., Oronoz  M., Uria  L.   2004.
A  Cascaded  Syntactic  Analyser  for  Basque  
Computational  Linguistics  and  Intelligent  Text  
Processing. 2945  LNCS  Series.pg.  124?135. 
Springer Verlag. Berlin (Germany).
Aldezabal I., Aranzabe M., Arrieta B., Maritxalar M., 
Oronoz M. 2003.  Toward a punctuation checker 
for Basque. Atala Workshop on Punctuation. Paris 
(France).
Alegria I., Arregi  O., Ezeiza N., Fernandez I., Urizar 
R. 2004. Design and Development of a Named En?
tity  Recognizer  for  an  Agglutinative  Language. 
First International Joint Conference on NLP (IJC?
NLP?04). Workshop on Named Entity Recognition. 
Ansa O., Arregi X., Arrieta B., Ezeiza N., Fernandez 
I.,  Garmendia  A.,  Gojenola  K.,  Laskurain  B., 
Mart?nez  E.,  Oronoz  M.,  Otegi  A.,  Sarasola  K., 
Uria L. 2004. Integrating NLP Tools for Basque in  
Text Editors. Workshop on International Proofing 
Tools  and Language Technologies.  University  of 
Patras (Greece).
Aranzabe M., Arriola J.M., D?az de Ilarraza A.  2004.
Towards  a  Dependency  Parser  of  Basque.
Proceedings of the Coling 2004 Workshop on Re?
cent Advances in Dependency Grammar. Geneva 
(Switzerland).
Bayraktar M., Say B., Akman V. 1998. An Analysis of  
English Punctuation:  the special  case of  comma. 
International  Journal  of  Corpus  Linguistics 
3(1):pp. 33?57.  John  Benjamins  Publishing  Com?
pany. Amsterdam (The Netherlands).
Beeferman D.,  Berger  A.,  Lafferty  J.  1998.  Cyber?
punc: a lightweight punctuation annotation system 
for speech. Proceedings of the IEEE International 
Conference on Acoustics, Speech and Signal Pro?
cessing, pages 689?692, Seattle (WA).
Brill, E. 1994.  Some Advances in rule?based part of  
speech tagging. In Proceedings of the Twelfth Na?
tional Conference on Artificial Intelligence. Seattle 
(WA). 
Brill,  E.  1995.  Transformation?based  error?driven 
learning and natural language processing: a case 
study  in  part  of  speech  tagging. Computational 
Linguistics 21(4). MIT Press. Cambridge (MA).
Briscoe T., Carroll J. 1995.  Developing and evaluat?
ing a probabilistic lr parser of part?of?speech and  
punctuation  labels.  ACL/SIGPARSE 4th  interna?
tional Workshop on Parsing Technologies, Prague / 
Karlovy Vary (Czech Republic). 
Carreras X., M?rquez L. 2003. Phrase Recognition by 
Filtering and Ranking with Perceptrons. Proceed?
ings of the 4th RANLP Conference. Borovets (Bul?
garia).
D?az de  Ilarraza A., Gojenola K., Oronoz M.   2005.
Design and Development of a System for the De?
tection of Agreement Errors in Basque. CICLing?
2005, Sixth International Conference on Intelligent 
Text  Processing  and  Computational  Linguistics. 
Mexico City (Mexico).
Garzia  J.  1997.  Joskera  Lantegi. Herri  Arduralar?
itzaren Euskal Erakundea. Gasteiz, Basque Country 
(Spain).
7
Hardt D. 2001.  Comma checking in Danish.  Corpus 
linguistics. Lancaster (England). 
Hill R.L., Murray W.S. 1998.  Commas and Spaces: 
the Point of Punctuation. 11th Annual CUNY Con?
ference  on  Human  Sentence  Processing.  New 
Brunswick, New Jersey (USA). 
Jones B. 1996. Towards a Syntactic Account of Punc?
tuation. Proceedings of the 16th International Con?
ference on Computational Linguistics. Copenhagen 
(Denmark). 
Nunberg,  G.  1990.  The  linguistics  of  punctuation. 
Center for the Study of Language and Information. 
Leland Stanford Junior University (USA).
Say B., Akman V. 1996.  Information?Based Aspects 
of  Punctuation.  Proceedings  ACL/SIGPARSE In?
ternational  Meeting  on  Punctuation  in  Computa?
tional  Linguistics,  pages  pp. 49?56,  Santa  Cruz, 
California (USA). 
Tjong Kim Sang E.F. and Buchholz S. 2000.  Intro?
duction to the CoNLL?2000 shared task: chunking. 
In  proceedings  of  CoNLL?2000  and  LLL?2000. 
Lisbon (Portugal).
Tjong Kim Sang E.F. and D?jean H. 2001. Introduc?
tion to the CoNLL?2001 shared task: clause identi?
fication. In proceedings of CoNLL?2001. Tolouse 
(France).
Van Delden  S.,  Gomez  F.  2002.  Combining  Finite 
State Automata and a Greedy Learning Algorithm 
to Determine the Syntactic Roles of Commas. 14th 
IEEE International Conference on Tools with Arti?
ficial Intelligence. Washington, D.C. (USA)
Zubimendi,  J.R. 2004.  Ortotipografia.  Estilo liburu?
aren lehen atala. Eusko Jaurlaritzaren Argitalpen 
Zerbitzu  Nagusia.  Gasteiz,  Basque  Country 
(Spain).
8
  Towards a Dependency Parser for Basque 
M. J. Aranzabe, J.M. Arriola and A. Diaz de Ilarraza, 
Ixa Group. (http://ixa.si.ehu.es) 
Department of Computer Languages and Systems 
University of the Basque Country 
P.O. box 649, E-20080 Donostia  
jibarurm@si.ehu.es 
 
 
Abstract 
We present the Dependency Parser, 
called Maxuxta, for the linguistic 
processing of Basque, which can serve 
as a representative of agglutinative 
languages that are also characterized by 
the free order of its constituents. The 
Dependency syntactic model is applied 
to establish the dependency-based 
grammatical relations between the 
components within the clause. Such a 
deep analysis is used to improve the 
output of the shallow parsing where 
syntactic structure ambiguity is not fully 
and explicitly resolved. Previous to the 
completion of the grammar for the 
dependency parsing, the design of the 
Dependency Structure-based Scheme 
had to be accomplished; we concentrated 
on issues that must be resolved by any 
practical system that uses such models. 
This scheme was used both to the 
manual tagging of the corpus and to 
develop the parser. The manually tagged 
corpus has been used to evaluate the 
accuracy of the parser. We have 
evaluated the application of the grammar 
to corpus, measuring the linking of the 
verb with its dependents, with 
satisfactory results. 
1 Introduction 
This article describes the steps given for the 
construction of a dependency syntactic parser 
for Basque (Maxuxta ). Our dependency 
analyser follows the constraint-based approach 
advocated by Karlsson (Karlsson, 1995). It 
takes as input the information obtained in the 
shallow parsing process (Abney, 1997). The 
shallow syntax refers to POS tagging and the 
chunking rules which group sequences of 
categories into structures (chunks) to facilitate 
the dependency analysis. The dependency 
parser is considered as the module involved in 
deep parsing (see Fig. 1). In this approach, 
incomplete syntactic structures are produced 
and, thus, the process goes beyond shallow 
parsing to a deeper language analysis in an 
incremental fashion (Aduriz et al, 2004). This 
allows us to tackle unrestricted text parsing 
through descriptions that are organized in 
ordered modules, depending on the depth level 
of the analysis (see Fig. 1).  
In agglutinative languages like Basque, it is 
difficult to separate morphology from syntax. 
That is why we consider morphosyntactic 
parsing for the first phase of the shallow 
syntactic analyser. 
CG
M
or
ph
os
yn
ta
ct
ic
pa
rs
in
g
Sy
nt
ac
tic
 
ta
gg
in
g
C
hu
nk
er
D
ep
en
de
nc
ie
s
EUSLEM 
Morpheus
Disambiguation using linguistic 
information
Disambiguation using statistical 
information
Shallow syntactic parsing
Named Entities
%
CG
PostpositionsCG
xfst
Noun and verb chainsCG
Tagging of syntactic dependenciesCG
Sh
al
lo
w
 
pa
rs
in
g
D
ee
p 
pa
rs
in
g
Raw data
Analysed text
 
Fig. 1. Syntactic processing for Basque. 
The dependency parser has been performed 
in order to improve the syntactic analysis 
 achieved so far, in the sense that, apart from 
the surface structural properties, we have 
added information about deeper structures by 
expressing the relation between the head and 
the dependent in an explicit manner. 
Additionally, we have adopted solutions to 
overcome problems that have emerged in 
doing this analysis (such as discontinuous 
constituents, subordinate clauses, etc. This 
approach has been used in several projects 
(J?rvinen & Tapanainen, 1998; Oflazer, 2003).  
Before carrying out the definition of the 
grammar for the parser, we established the 
syntactic tagging system in linguistic terms. 
We simultaneously have applied it to build the 
treebank for Basque (Eus3LB1) (Aduriz et al, 
2003) as well as to define the Dependency 
Grammar. The treebank would serve to 
evaluate and improve the dependency parser. 
This will enable us to check how robust our 
grammar is.  
The dependency syntactic tagging system is 
based on the framework presented in Carroll et 
al., (1998, 1999): each sentence in the corpus 
is marked up with a set of grammatical 
relations (GRs), specifying the syntactic 
dependency which holds between each head 
and its dependent(s). However, there are 
certain differences: in our system, arguments 
that are not lexicalised may appear in 
grammatical relations  (for example, the 
phonetically empty pro argument, which 
appears in the so-called pro-drop languages). 
The scheme is superficially similar to a 
syntactic dependency analysis in the style of 
Lin (1998). We annotate syntactically the 
Eus3LB corpus following the dependency-
based formalism. The dependencies we have 
defined constitute a hierarchy (see Fig. 2) that 
describes the theoretically and empirically 
relevant dependency tags employed in the 
analysis of the basic syntactic structures of 
Basque.  
                                                
1This work is part of a general project 
(http://www.dlsi.ua.es/projectes/3lb) which objective is to build 
three linguistically annotated corpora with linguistic annotation 
at syntactic, semantic and pragmatic levels: Cat3LB (for 
Catalan), Cast3LB (for Spanish) (Civit & Mart?, 2002) and 
Eus3LB (for Basque). The Catalan and the Spanish corpora 
include 100.000 words each, and the Basque Corpus 50.000 
words. 
This formalism is also used in the Prague 
Dependency Treebank for Czech (Hajic, 1998) 
and in NEGRA corpora for German (Brants et 
al., 2003) among others.  
 
dependant
structurally case
marked
complements
negation
linking-words
modifiers
auxiliary
others
semantics
non clausal
clausal
clausal
non
clausal
determiner
non clausal
clausal
predicative
finite
non finite
clausal
non
clausal
connector
apposition
graduator
particle
interjec.
ncsubj
nczobj
ncobj
ncmod
finite
non finite
detmod
xcomp_obj
xmod
xcomp_subj
cmod
ccomp_obj
ccomp_subj
ncmod
lot
auxmod
ncpred
non finite xpred
finite
non
finite
aponcmod
apocmod
apoxmod
gradmod
prtmod
itj_out
arg_mod
meta
galdemod
ccomp_zobj
xcomp_zobj
 
Fig. 2. Dependency relations hierarchy. 
Section 2 examines the main features of the 
language involved in the analysis in terms of 
dependency relations. Taking into account 
these features, we will explain the reasons for 
choosing the dependency-based formalism. In 
section 3 we briefly describe the general 
parsing system. Section 4 explains the 
dependency relations, the implementation of 
the dependency rules and a preliminary 
evaluation. Finally, some conclusions and 
objectives for future work are presented. 
 
2 A brief description of Basque in order 
to illustrate the adequacy of the adopted 
formalism 
Basque is an agglutinative language, that is, 
for the formation of words the dictionary entry 
independently takes each of the elements 
necessary for the different functions (syntactic 
case included). More specifically, the affixes 
corresponding to the determinant, number and 
declension case are taken in this order and 
independently of each other. These elements 
appear only after the last element in the noun 
phrase. One of the main characteristics of 
 Basque is its declension system with numerous 
cases, which differentiates it from languages 
spoken in the surrounding countries.  
At sentence level, the verb appears as the 
last element in a neutral order. That is, given 
the language typology proposed by Greenberg, 
Basque is a Subject-Object-Verb (SOV) type 
language (Laka, 1998) or a final head type 
language. However, this corresponds to the 
neutral order, but in real sentences any order of 
the sentence elements (NPs, PPs) around the 
verb is possible, that is, Basque can also be 
considered a language with free order of 
sentence constituents.  
These are the principal features that 
characterize the Basque language and, 
obviously, they have influenced us critically in 
our decision:  
 
1. The dependency-based formalism is the one 
that could best deal with the free word order 
displayed by Basque syntax (Skut et al, 
1997). 
2. We consider that the computational tools 
developed so far in our group facilitate 
either achieving dependency relations or 
transforming from dependency-trees to other 
modes of representation.  
3. From our viewpoint, it is less messy to 
evaluate the relation between the elements 
that compose a sentence rather than the 
relation of elements included in parenthesis. 
4. Dependency-based formalism provides a 
way of expressing semantic relations. 
3 Overview of the Syntactic Processing 
of Basque: from shallow parsing to deep 
parsing  
We face the creation of a robust syntactic 
analyser by implementing it in sequential rule 
layers. In most of the cases, these layers are 
realized in grammars defined by the Constraint 
Grammar formalism (Karlsson et al , 1995; 
Tapanainen & Voutilainen, 1994). Each 
analysis layer uses the output of the previous 
layer as its input and enriches it with further 
information. Rule layers are grouped into 
modules depending on the level of depth of 
their analysis. Modularity helps to maintain 
linguistic data and makes the system easily 
customisable or reusable.  
Figure 1 shows the architecture of the 
system, for more details, see Aduriz et al, 
2004. The shallow parsing of the text begins 
with the morphosyntactic analysis and ends 
delimiting noun and verb chains. Finally, the 
deep analysis phase establishes the 
dependency-based grammatical relations 
between the components within the clause.  
The parsing system is based on finite state 
grammars. The Constraint Grammar (CG) 
formalism has been chosen in most cases 
because, on the one hand, it is suitable for 
treating unrestricted texts and, on the other 
hand, it provides a useful methodology and the 
tools to tackle morphosyntax as well as free 
order phrase components in a direct way.  
A series of grammars are implemented 
within the module of the shallow parsing 
which aim:  
1. To be useful for the disambiguation of 
grammatical categories, removing incorrect 
tags based on the context. 
2. To assign and disambiguate partial syntactic 
functions. 
3. To assign the corresponding tags to delimit 
verb and noun chains. 
3.1 Shallow Syntactic Analyser 
The shallow or partial parsing analyser 
produces minimal and incomplete syntactic 
structures. The output of the shallow parser, as 
stated earlier, is the main base for the 
dependency parser. The shallow syntactic 
analyser includes the following modules: 
1. The morphosyntactic analyser MORFEUS. 
The parsing process starts with the outcome 
of the morphosyntactic analyser MORFEUS 
(Alegria et al, 1996), which was created 
following a two-level morphology 
(Koskenniemi, 1983). It deals with the 
parsing of all the lexical units of a text, both 
simple words and multiword units as a 
Complex Lexical Unit (CLU).  
2. The morphosyntactic disambiguation 
module EUSLEM. From the obtained 
results, grammatical categories and lemmas 
are disambiguated. Once morphosyntactic 
disambiguation has been performed, this  
module assigns a single syntactic function to 
each word.  
 3. The ckunk analysis module ZATIAK. This 
module identifies verb and noun chains 
based on the information about syntactic 
functions provided by each word-form. 
Entity names and postpositional phrases are 
also determined.  
We will focus on the last step of the shallow 
analysis because it contains the more 
appropriate information to make explicit the 
dependency relations. Basically, we use the 
syntactic functions and the chunks that are 
determined in the partial analysis. 
Shallow syntactic functions 
The syntactic functions that are determined 
in the partial analysis are based on those given 
in Aduriz et al, 2000. The syntactic functions 
employed basically follow the same approach 
to syntactic tags found in ENGCG 
(Voutilainen et al, 1992), although some 
decisions and a few changes were necessary. 
There are three types of syntactic functions:  
1. Those that represent the dependencies 
within noun chains (@CM>, @NC> etc.). 
2. Non-dependent or main syntactic functions 
(@SUBJ, @OBJ, etc.). 
3. Syntactic functions of the components of 
verb chains (@-FMAINV, @+FMAINV, 
etc.). 
The distinction of these three groups is 
essential when designing the rules that assign 
the function tags for verb and noun chains 
detection. 
Chunker: verb chain and noun chains 
After the morphological analysis and the 
disambiguation are performed (see Figure 1), 
we have the corpus syntactically analysed 
following the CG syntax. In this syntactic 
representation there are not phrase units. But 
on the basis of this representation, the 
identification of various kinds of phrase units 
such as verb chains and noun chains is 
reasonably straightforward.   
Verb chains  
The identification of verb chains is based on 
both the verb function tags (@+FAUXV, @-
FAUXV, @-FMAINV, @+FMAINV, etc.) and 
some particles (the negative particle, modal 
particles, etc.).  
There are two types of verb chains: 
continuous and dispersed verb chains (the 
latter consisting of three components at most). 
The following function tags have been defined: 
? %VCH: this tag is attached to a verb chain 
consisting of a single element. 
? %INIT_VCH: this tag is attached to the 
initial element of a complex verb chain. 
? %FIN_VCH: this tag is attached to the final 
element of a complex verb chain. 
The tags used to mark-up dispersed verb 
chains are: 
? %INIT_NCVCH: this tag is attached to the 
initial element of a non-continuous verb 
chain. 
? %SEC_NCVCH: this tag is attached to the 
second element of a non-continuous verb 
chain. 
? %FIN_NCVCH: this tag is attached to the 
fina l element of a non-continuous verb 
chain. 
Noun chains 
This module is based on the following 
assumption: any word having a modifier 
function tag has to be linked to some word or 
words with a main syntactic function tag. 
Moreover, a word with a main syntactic 
function tag can, by itself, constitute a phrase 
unit (for instance, noun phrases, adverbials and 
prepositional phrases). Taking into account this 
assumption, we recognise simple and 
coordinated noun chains, for which these three 
function tags have been established:  
? %NCH: this tag is attached to words with 
main syntactic function tags that constitute a 
phrase unit by themselves 
? %INIT_NCH: this tag is attached to the 
initial element of a phrase unit.  
? %FIN_NCH: this tag is attached to the final 
element of a phrase unit.  
Figure 3 shows part of the information 
obtained in the process of parsing the sentence 
Defentsako abokatuak desobedientzia 
zibilerako eskubidea aldarrikatu du epaiketan 
(The defense lawyer has claimed the right to 
civil disobedience in the  trial) with its 
corresponding chains tags.  
Let us know the some syntactic tags used in 
fig. 3: @NC>: noun complement; @CM>: 
modifier of the word carrying case in the noun 
 chain; @-FMAINV: non finite main verb; 
@+FAUXV: finite auxiliary verb and 
@ADVL: adverbial. 
"<Defentsako>" <INIT_CAP>"   defense  
     "defentsa" N @NC>  %INIT_NCH 
"<abokatuak>"  the lawyer  
      "abokatu" N @SUBJ  %FIN_NCH 
"<desobedientzia>"                       disobedience  
   "desobedientzia" N @CM> %INIT_NCH 
"<zibilerako>"                                to civil  
       "zibil" ADJ @<NC 
"<eskubidea>"                                the right  
       "eskubide" N @OBJ %FIN_NCH 
"<aldarrikatu>"                              claimed  
   "aldarrikatu" V @-FMAINV %INIT_VCH 
"<du>"                                            has  
   "*edun" AUXV @+FAUXV %FIN_VCH   
"<epaiketan>"                                 in the trial 
        "epaiketa" N @ADVL  %NCH  
"<$.>" <PUNCT_PUNCT>" 
Fig. 3. Analysis of chains. English translation on the 
right 
3.3 Deep Syntactic Analysis  
The aim of the deep syntactic analysis is to 
make explicit the dependency relations 
between words or chunks. For this reason, we 
have designed a Dependency Grammar based 
on the Constraint Grammar Formalism. 
4 The Dependency Grammar for the 
Parser  
In this section we describe in more detail the 
dependency relations defined (see fig. 2), the 
design of the rules and the results obtained. 
The results obtained in the deep parsing of 
sample sentence will help in providing a better 
understanding of the mentioned parsing 
process. This parsing process takes as basis the 
output of the shallow parser (see fig. 3). The 
rules are implemented by means of the CG-2 
parser (www.conexor.com). 
4.1 The dependency relations 
As Lin (2003) says a dependency 
relationship (Hays, 1964; Hudson, 1984; 
Mel?cuk, 1987; B?mov? et al, 2003) is an 
asymmetric binary relationship between a 
word called head (or governor, parent), and 
another word called modifier (or dependent, 
daughter). Dependency grammars represent 
sentence structures as a set of dependency 
relationships. Normally the dependency 
relationships form a tree that connects all the 
words in a sentence. A word in the sentence 
may have several modifiers, but each word 
may modify at most one word. The root of the 
dependency tree does not modify any word. It 
is also called the head of the sentence. 
For example, figure 4 describes the 
dependency structure of the example sentence. 
We use a list of tuples to represent a 
dependency tree. Each tuple represents one 
relation in the dependency tree. For example, a 
structurally case-marked complement when 
complements are nc (non-clausal, Noun 
Phrases, henceforth NP) has the following 
format: 
case : the case-mark by means of what the 
relation is established among the head and the 
modifier. 
head: the modified word head of 
NP/dependent: the modifier. In this case, the 
head of the NP. 
case-marked element within 
NP/dependent: the component of the 
dependent NP that carries the case. 
subj relationship: the label assigned to the 
dependency relationship. 
The syntactic dependencies between the 
components within the sentence are 
represented by tags starting with ?&?. The 
symbols ?>? and ?<? attached to each 
dependency-tag represent the direction in 
which we find the sentence component whose 
dependant is the target word.  
In the example we can see that the noun 
phrase defentsako abokatuak  ?the defense 
lawyer? depends on the verb aldarrikatu ?to 
claim?, which is on its right side. A post-
process will make this link explicit. 
The dependency tree in fig 4 is represented 
by the following tuples: 
 
Modifier Cat Head Type 
Defentsako 
abokatuak 
desobedientzia 
zibilerako 
eskubidea 
aldarrikatu 
du 
epaiketan 
N 
N 
N 
ADJ 
N 
V 
Aux 
N 
abokatuak  
aldarrikatu  
eskubidea  
desobedientzia 
aldarrikatu 
 
aldarrikatu 
aldarrikatu 
&NCMOD> 
&NCSUBJ> 
&NCMOD> 
&<NCMOD 
&NCOBJ> 
 
&<AUXMOD 
&<NCMOD 
 4.2 The dependency grammar rules  
The grammar consists of 255 rules that have 
been defined and distributed in the following 
way: 
 
complements modifiers 
nc2 cc3 det nc cm4 
others 
62 11 19 124 20 19 
 
These rules were formulated, implemented, 
and tested using a part of the manually 
disambiguated corpus (24.000 words). For the 
moment, part of the rest of the corpus was used 
for testing.  
For more details of the rules, we describe 
some examples that illustrate how dependency 
rules can be written to define different types of 
linguistic relations. 
 
1. Verb-subject dependency 
The following rule defines a verb-subject 
dependency relation between 2 words 
aldarrikatu (claimed) and abokatuak   (lawyer) 
of the sentence in the previous example:  
  
 MAP (&NCSUBJ>) TARGET (NOUN)  
   IF (0 (ERG) + (@SUBJ) +(%FIN_NCH)) 
      (*1(@-FMAINV) + (%INIT_VCH)  
       BARRIER (PUNCT_PUNCT)); 
 
The rule assigned the ncsubj tag to the noun 
abokatuak (lawyer) if the following conditions 
are satisfied: a) the noun is declined in ergative 
case; besides, it has assigned the @SUBJ 
syntactic function and, it is the last word of a 
noun chain; b) it has a non-finite main verb 
everywhere on its right before the punctuation 
mark. 
                                                
2 nc: non-clausal complement or modifier 
3 cc:clausal complement 
4 cm: clausal modifier 
 
2. Subordinate clause dependency 
The following rule defines a complement 
subordinate clause dependency relation 
between a subordinate verb and a main verb. 
We illustrate this rule by means of an example 
in which the word egoten (usually stayed) is 
the verb of the complement subordinate clause 
linked to esan (told): 
 
Example: Lehenago aitona egoten zela ni 
EGOTEN naizen tokian esan dit amonak5. 
 
 MAP(&CCOMP>>)TARGET (V)  
 IF(0(@-FMAINV)+ (%INIT_VCH)) 
(1(@+FAUXV_SUB)+ (%FIN_VCH)); 
 
The rule assigned the CCOMP tag to the 
verb egoten  (usually stayed) if the following 
conditions are satisfied: a) the verb is a non-
finite main verb and, it?s the first word-form of 
a verb chain; b) it has an auxiliary verb on its 
immediate right-side which has assigned the 
complement tag and appears as the last part of 
the verb chain.  
 
3. Infinitive control 
The following rule defines that in the 
sentence Jonek Miren etortzea nahi du. (John 
wants to come Mary), etortzea (infinitive 
subordinate clause with object function, "to 
come") is controled by the main verb nahi  ("to 
want"). Taking into account, that etortzea  is 
the controlled object of nahi, if there is another 
non-infinitive object Miren; then we will 
assign to it the subject dependency relation to 
the infinitive verb ("to come").   
  
                                               
5 My grandmother told me my grandfather 
usually stayed  where I am now 
epaiketan Defentsako abokatuak desobedientzia  zibilerako eskubidea aldarrikatu du 
Fig.4. Dependency tree 
 MAP (&NCSUBJ>) TARGET (NOUN)  
IF (0 (ABS) + (@SUBJ) OR (@OBJ)  + (%NCH))  
    (1(@-FMAINV_SUB_@OBJ) ) (2 VTRANS_ -FV )); 
  
4.3 Evaluation 
The system has been manually tested on a 
corpus of newspaper articles (included in 
Eus3LB), containing 302 sentences (3266 
words).  
We have evaluated the precision (correctly 
selected dependent / number of dependant 
returned) and the recall (correctly selected 
dependent / actual dependent in the sentence) 
of the subject (including coordinated subjects), 
and modifier dependency of verbs. For subject, 
precision and recall were respectively 67% and  
69 %, while the figures for verb modifiers were 
73 % and   95%. 
We have detected two main  reasons for 
explaining these figures: 1) the analysis 
strategy is limited because we cannot make use 
of semantic or contextual information for 
resolving uncertainties at an early level; 2) 
errors in previous steps. These errors can be a) 
due either to an incorrect assignment of POS to 
word-forms or to the syncretism of case marks 
(@SUBJ, @OBJ); b) the presence of non-
known word-forms that increases the number 
of possible analysis. At this moment, the head 
and dependent slot fillers are, in all cases, the 
base forms of single head words, so for 
example, ?multi-component? heads, such as 
names, are reduced to a single word; thus the 
slot filler corresponding to Xabier Arzallus 
would be Arzallus.  
5 Conclusions 
We have presented the application of the 
dependency grammar parser for the processing 
of Basque, which can serve as a representative 
of agglutinative languages with free order of 
constituents.  
We have shown how dependency grammar 
approach provides a good solution for deeper 
syntactic analysis, being at this moment the 
best alternative for morphologically complex 
languages.  
We have also evaluated the application of 
the grammar to corpus, measuring the linking 
of the verb with its dependents, with 
satisfactory results. However, the development 
of a full dependency syntactic analyser is still a 
matter of research.  For instance, all kinds of 
constructions without a clear syntactic head are 
difficult to analyse: ellipses, sentences without 
a verb (e.g., copula -less predicative), and 
coordination. All these aspects have been 
treated in our manually annotated Corpus; our 
efforts now are oriented to deal with them 
automatically. 
 
6 Acnowledgments  
This research is supported by the University 
of the Basque Country (9/UPV00141.226-
14601/2002), the Ministry of Industry of the 
Basque Government (project XUXENG, 
OD02UN52). 
References  
Abney S. P. 1997. Part-of-speech tagging and 
partial parsing. S. Young and G. Bloothooft, 
editors,  Corpus -Based Methods in Language 
and Speech Processing, Kluwer, Dordrecht. 
Aduriz I., Aranzabe M.J., Arriola J.M.,  D?az 
de Ilarraza A., Gojenola K., Oronoz M., Ur?a 
L. 2004. A Cascaded Syntactic Analyser for 
Basque. In Gelbukh, A (ed.) Computational 
Linguistics and Intelligent Text Processing. 
SpringerLNCS 2945.  
Aduriz I., Aranzabe M.J., Arriola J.M., Atutxa 
A., D?az de Ilarraza A., Garmendia A., 
Oronoz M. 2003. Construction of a Basque 
Dependency Treebank. Proceedings of the 
Second Workshop on Treebanks and 
Linguistic Theories "TLT 2003", (J. Nivre 
and E. Hinrichs eds.), V?xj? University 
Press. V ?xj?, Suecia   
Aduriz I., Arriola J.M., Artola X., Diaz de 
Illarraza A., Gojenola K., Maritxalar M. 
2000. Euskararako Murriztapen Gramatika: 
mapaketak, erregela morfosintaktikoak eta 
sintaktikoak. UPV/EHU/LSI/TR 12-2000.  
Alegria I., Artola X., Sarasola K., Urkia M. 
1996. Automatic morphological analysis of 
Basque. Literary & Linguistic Computing 
Vol. 11, No. 4, 193-203. Oxford University 
Press. Oxford. 
 B?mov? , A., Haji?c, J., Hajicov?a, E., 
Hladk?a, B. 2003. The Prague 
DependencyTreebank: A Three level 
Annotation Scenario. In Abeill? (ed.) 
Treebanks Building and Using Parsed 
Corpora, Book Series: TEXT, SPEECH 
AND LANGUAGE TECHNOLOGY : 
Volume 20 Kluwer Academic Publisher, 
Dordrecht. 
Brants T., Skut W. & Uszkoreit H. 2003 
"Syntactic Annotation of a German Newspa-  
per Corpus?. In Abeill? (ed.) Treebanks 
Building and Using Parsed Corpora, Book 
Series: TEXT, SPEECH AND LANGUAGE 
TECHNOLOGY : Volume 20 Kluwer 
Academic Publisher, Dordrecht. 
Carroll J., Briscoe E., Sanfilippo A. 1998. 
Parser evaluation: a survey and a new 
proposal. Proceedings of the 1st 
International Conference on Language 
Resources and Evaluation, 447-454. 
Granada, Spain.  
Carroll J., Minnen G., Briscoe T. 1999. Corpus 
Annotation for Parser Evaluation. 
Proceedings of Workshop on Linguistically 
Interpretated Corpora, EACL?99. Bergen. 
Civit M. & Mart? M. 2002. Design Principles 
for a Spanish Treebank. Proceedings of The 
Treebank and Linguistic Theories 
(TLT2002). Sozopol, Bulgaria. 
Hays, D. 1964. Dependency theory: a 
formalism and some observations. 
Language40, p. 511?525. 
Hajic J. 1998. Building a Syntactically 
Annotated Corpus: The Prague Dependency 
Treebank. In  Issues of Valency and 
Meaning, 106-132. Karolinum, Praha. 
Hudson, R. 1984. Word Grammar. Oxford, 
England: Basil Blackwell PublishersLimited. 
J?rvinen T. and Tapanainen P, 1998. Towards 
an implementable dependency grammar. In 
Proceedings of the Workshop "Processing of 
Dependency-Based Grammars", (eds.) 
Sylvain Kahane and Alain Polgu?re, 
Universit? de Montr?al, Quebec, Canada, 
15th August 1998, pp. 1-10. 
Karlsson F., Voutilainen A., Heikkila J., 
Anttila A. 1995. Constraint Grammar: a 
Language-Independent System for Parsing 
Unrestricted Text. Mouton de Gruyter. 
Koskenniemi K 1983. Two-level Morphology: 
A general Computational Model for Word-
Form Recognition and Production. 
University of Helsinki, Department of 
General Linguistics. Publications 11.  
Laka, I. 1998. A Brief Grammar of Euskara, 
the Basque Language. HTML document. 
http://www.ehu.es/grammar. Office of the 
Vice-Dean for the Basque Language. 
University of the Basque Country. 
Lin D. 1998. A Dependency-based Method for 
Evaluating Broad-Coverage Parsers. Natural 
Language Engineering.  
Lin D. 2003. "Dependency-based evaluation of 
MINIPAR" in Building and Using 
syntactically annotated corpora, Abeill?, A. 
Ed. Kluwer, Dordrecht 
Mel?cuk, I. A. 1987. Dependency syntax: 
theory and practice. Albany: StateUniversity 
of New York Press. 
Oflazer K. 2003. Dependency Parsing with an 
Extended Finite-State Approach. ACL 
Journal of Computational Linguistics, Vol. 
29, n?4. 
Skut W., Krenn B., Brants T., Uszkoreit H. 
1997. An Annotation Scheme for Free Word 
Order Languages. In Proceedings of the 
Fifth Conference on Applied Natural 
Language Processing (ANLP-97). 
Washington, DC, USA. 
Tapanainen P. and Voutilainen A. 1994 
Tagging Accurately-Don?t guess if you know. 
In Proceedings of the 4th Conference on  
Applied Natural Language Processing, 
Washington. 
Voutilainen A., Heikkil? J. and Anttila A. 
1992. Constraint Grammar of English. A 
Performance-Oriented Introduction. 
Publications of Department of General 
Linguistics, University of Helsinki, No. 21, 
Helsinki. 
 
Proceedings of BioNLP Shared Task 2011 Workshop, pages 138?142,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Using Kybots for Extracting Events in Biomedical Texts
Arantza Casillas (*)
arantza.casillas@ehu.es
Arantza D??az de Ilarraza (?)
a.diazdeilarraza@ehu.es
Koldo Gojenola (?)
koldo.gojenola@ehu.es
Maite Oronoz (?)
maite.oronoz@ehu.es
German Rigau (?)
german.rigau@ehu.es
IXA Taldea UPV/EHU
(*) Department of Electricity and Electronics
(?) Department of Computer Languages and Systems
Abstract
In this paper we describe a rule-based sys-
tem developed for the BioNLP 2011 GENIA
event detection task. The system applies Ky-
bots (Knowledge Yielding Robots) on anno-
tated texts to extract bio-events involving pro-
teins or genes. The main goal of this work is to
verify the usefulness and portability of the Ky-
bot technology to the domain of biomedicine.
1 Introduction
The aim of the BioNLP?11 Genia Shared Task (Kim
et al, 2011b) concerns the detection of molecular
biology events in biomedical texts using NLP tools
and methods. It requires the identification of events
together with their gene or protein arguments. Nine
event types are considered: localization, binding,
gene expression, transcription, protein catabolism,
phosphorylation, regulation, positive regulation and
negative regulation.
When identifying the events related to the given
proteins, it is mandatory to detect also the event
triggers, together with its associated event-type, and
recognize their primary arguments. There are ?sim-
ple? events, concerning an event together with its
arguments (Theme, Site, ...) and also ?complex?
events, or events that have other events as secundary
arguments. Our system did not participate in the op-
tional tasks of recognizing negation and speculation.
The training dataset contained 909 texts together
with a development dataset of 259 texts. 347 texts
were used for testing the system.
The main objective of the present work was to ver-
ify the applicability of a new Information Extraction
(IE) technology developed in the KYOTO project1
(Vossen et al, 2008), to a new specific domain. The
KYOTO system comprises a general and extensible
multilingual architecture for the extraction of con-
ceptual and factual knowledge from texts, which has
already been applied to the environmental domain.
Currently, our system follows a rule-based ap-
proach (i.e. (Kim et al, 2009), (Kim et al, 2011a),
(Cohen et al, 2011) or (Vlachos, 2009)), using a set
of manually developed rules.
2 System Description
Our system proceeds in two phases. Firstly, text doc-
uments are tokenized and structured using an XML
layered structure called KYOTO Annotation Format
(KAF) (Bosma et al, 2009). Secondly, a set of Ky-
bots (Knowledge Yielding Robots) are applied to de-
tect the biological events of interest occurring in the
KAF documents. Kybots form a collection of gen-
eral morpho-syntactic and semantic patterns on se-
quences of KAF terms. These patterns are defined
in a declarative format using Kybot profiles.
2.1 KAF
Firstly, basic linguistic processors apply segmenta-
tion and tokenization to the text. Additionally, the
offset positions of the proteins given by the task or-
ganizers are also considered. The output of this ba-
sic processing is stored in KAF, where words, terms,
syntactic and semantic information can be stored in
separate layers with references across them.
Currently, our system only considers a minimal
amount of linguistic information. We are only using
1http://www.kyoto-project.eu/
138
the word form and term layers. Figure 1 shows an
example of a KAF document where proteins have
been annotated using a special POS tag (PRT). Note
that our approach did not use any external resource
apart of the basic linguistic processing.
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<KAF xml:lang="en">
<text>
...
<wf wid="w210" sent="10">phosphorylation</wf>
<wf wid="w211" sent="10">of</wf>
<wf wid="w212" sent="10">I</wf>
<wf wid="w213" sent="10">kappaB</wf>
<wf wid="w214" sent="10">alpha<...
</text>
<term tid="t210" type="open" lemma="phosphorylation"
start="1195" end="1210" pos="W">
<span><target id="w210"/></span>
</term>
<term tid="t211" type="open" lemma="of"
start="1211" end="1213" pos="W">
<span><target id="w211"/></span>
</term>
<term tid="T5" type="open" lemma="I kappaB alpha"
start="1214" end="1228" pos="PRT">
<span><target id="w212"/></span>
<target id="w213"/>
<target id="w214"/></span>
</term>...
</terms>
</KAF>
Figure 1: Example of a document in KAF format.
2.2 Kybots
Kybots (Knowledge Yielding Robots) are abstract
patterns that detect actual concept instances and re-
lations in KAF. The extraction of factual knowledge
by the mining module is done by processing these
abstract patterns on the KAF documents. These pat-
terns are defined in a declarative format using Kybot
profiles, which describe general morpho-syntactic
and semantic conditions on sequences of terms. Ky-
bot profiles are compiled to XQueries to efficiently
scan over KAF documents uploaded into an XML
database. These patterns extract and rank the rele-
vant information from each match.
Kybot profiles are described using XML syn-
tax and each one consists of three main declarative
parts:
? Variables: In this part, the entities and its prop-
erties are defined
? Relations: This part specifies the positional re-
lations among the previously defined variables
? Events: describes the output to be produced for
every matching
Variables (see the Kybot section variables in fig-
ure 2) describe the term variables used by the Kybot.
They have been designed with the aim of being flex-
ible enough to deal with many different information
associated with the KAF terms including semantic
and ontological statements.
Relations (see the Kybot section relations in fig-
ure 2) define the sequence of variables the Kybot
is looking for. For example, in the Kybot in fig-
ure 2, the variable named Phosphorylation
is the main pivot, the variable Of must follow
Phosphorylation (immediate is true indi-
cating that it must be the next term in the sequence),
and a variable representing a Proteinmust follow
Of. Proteins and genes are identified with the PRT
tag.
Events (expressions marked as events in figure 2)
describes the output template of the Kybot. For ev-
ery matched pattern, the kybot produces a new event
filling the template structure with the selected pieces
of information. For example, the Kybot in figure 2
selects some features of the event represented with
the variable called Phosphorylation: its term-
identification (@tid), its lemma, part of speech and
offset. The expression also describes that the vari-
able Protein plays the role of being the ?Theme?
of the event. The output obtained when aplying the
Kybot in figure 2 is shown in figure 3. Comparing
the examples in table 1 and in figure 3 we observe
that all the features needed for generating the files
for describing the results are also produced by the
Kybot.
<doc shortname="PMID-9032271.kaf">
<event eid="e1" target="t210" kybot="phosphorylation of P"
type="Phosphorylation"
lemma="phosphorylation" start="1195" end="1210" />
<role target="T5" rtype="Theme"
lemma="I kappaB alpha" start="1214" end="1228" />
</doc>
Figure 3: Output obtained after the application of the Ky-
bot in figure 2.
3 GENIA Event Extraction Task and
Results
We developed a set of basic auxiliary pro-
grams to extract event patterns from the train-
ing corpus. These programs obtain the struc-
139
<?xml version="1.0" encoding="utf-8"?>
<!-- Sentence: phosphorylation of Protein
Event1: phosphorylation
Role: Theme Protein -->
<Kybot id="bionlp">
<variables>
<var name="Phosphorylation" type="term" lemma="phosphorylat*>
<var name="Of" type="term" lemma="of"/>
<var name="Protein" type="term" pos="PRT"/>
</variables>
<relations>
<root span="Phosphorylation"/>
<rel span="Of" pivot="Phosphorylation" direction="following" immediate="true"/>
<rel span="Protein" pivot="Of" direction="following" immediate="true"/>
</relations>
<events>
<event eid="" target="$Phosphorylation/@tid" kybot="phosphorylation of P"
type="Phosphorylation" lemma="$Phosphorylation/@lemma"
pos="$Phosphorylation/@pos" start="$Phosphorylation/@start" end="$Phosphorylation/@end"/>
<role target="$Protein/@tid" rtype="Theme" lemma="$Protein/@lemma" start="$Protein/@start"
end="$Protein/@end"/>
</events>
</Kybot>
Figure 2: Example of a Kybot for the pattern Event of Protein.
.a1 file
T5 Protein 1214 1228 I kappaB alpha
.a2 file
T20 Phosphorylation 1195 1210 phosphorylation
E7 Phosphorylation:T20 Theme:T5
Table 1: Results in the format required in the GENIA
shared task.
ture of the events, their associated trigger words
and their frequency. For example, in the
training corpus, a pattern of the type Event
of Protein appears 35 times, where the
Event is further described as phosporylation,
phosphorylated.... Taking the most fre-
quently occurring patterns in the training data into
account, we manually developed the set of Kybots
used to extract the events from the development and
test corpora. For example, in this way we wrote the
Kybot in figure 2 that fulfils the conditions of the
pattern of interest.
The two phases mentioned in section 2, corre-
sponding to the generation of the KAF documents
and the application of Kybots, have different input
files depending on the type of event we want to
detect: simple or complex events. When extract-
ing simple events (see figure 4), we used the in-
put text and the files containing protein annotations
(?.a1? files in the task) to generate the KAF docu-
ments. These KAF documents and Kybots for sim-
ple events are provided to the mining module. In
the case of complex events (events that have other
KAF generator
.txt .a1
.kaf
Kybot processor
Kybots
(Simple)
.a2
Figure 4: Application of Kybots. Simple events.
events as arguments), the identifiers of the detected
simple events are added to the KAF document in the
first phase. A new set of Kybots describing complex
events and KAF (now with annotations of the simple
events) are used to obtain the final result (see figure
5).
For the evaluation, we also developed some pro-
grams for adapting the output of the Kybots (see fig-
ure 3) to the required format (see table 1).
We used the development corpus to improve the
Kybot performance. We developed 65 Kybots for
detecting simple events. Table 2 shows the number
of Kybots for each event type. Complex events rela-
tive to regulation (also including negative and posi-
tive regulations) were detected using a set of 24 Ky-
bots.
The evaluation of the task was based on the output
140
KAF generator
.a2 .kaf
.kaf
(with simple events)
Kybot processor
Kybots
(Complex)
.a2
Figure 5: Application of Kybots. Complex events.
Event Class Simple Kyb. Complex Kyb.
Transcription 10
Protein Catabolism 5
Binding 5
Regulation 3
Negative Regulation 5 4
Positive Regulation 3 17
Localization 7
Phosphorylation 18
Gene Exrpesion 12
Total 65 24
Table 2: Number of Kybots generated for each event.
of the system when applied to the test dataset of 347
previously unseen texts. Table 3 shows in the Gold
column the number of instances for each event-type
in the test corpus. R, P and F-score columns stand
for the recall, precision and f-score the system ob-
tained for each type of event. As a consequence of
the characteristics of our system, precision is primed
over recall. For example, the system obtains 95%
and 97% precision on Phosphorylation an Localiza-
tion events, respectively, although its recall is con-
siderably lower (41% and 19%).
4 Conclusions and Future work
This work presents the first results of the applica-
tion of the KYOTO text mining system for extracting
events when ported to the biomedical domain. The
KYOTO technology and data formats have shown to
be flexible enough to be easily adapted to a new task
and domain. Although the results are far from satis-
factory, we must take into account the limited effort
we dedicated to adapting the system and designing
the kybots, which can be roughly estimated in two
Event Class Gold R P F-score
Localization 191 19.90 97.44 33.04
Binding 491 5.30 50.00 9.58
Gene Expression 1002 54.19 42.22 47.47
Transcription 174 13.22 62.16 21.80
Protein catabolism 15 26.67 44.44 33.33
Phosphorylation 185 41.62 95.06 57.89
Non-reg total 2058 34.55 47.27 39.92
Regulation 385 7.53 9.63 8.45
Positive regulation 1443 6.38 62.16 11.57
Negative regulation 571 3.15 26.87 5.64
Regulatory total 2399 5.79 26.94 9.54
All total 4457 19.07 42.08 26.25
Table 3: Performance analysis on the test dataset.
person/months.
After the final evaluation, our system obtained the
thirteenth position out of 15 participating systems
in the main task (processing PubMed abstracts and
full paper articles), obtaining 19.07%, 42.08% and
26.25 recall, precision an f-score, respectively, far
from the best competing system (49.41%, 64.75%
and 56.04%). Although they are far from satisfac-
tory, we must take into account the limited time we
dedicated to adapting the system and designing the
kybots. Apart from that, due to time restrictions,
our system did not make use of the ample set of
resources available, such as named entities, corefer-
ence resolution or syntactic parsing of the sentences.
On the other hand, the system, based on manually
developed rules, obtains reasonable accuracy in the
task of processing full paper articles, obtaining 45%
precision and 21% recall, compared to 59% and 47%
for the best system, which means that the rule-based
approach performs more robustly when dealing with
long texts (5 full texts correspond to approximately
150 abstracts). As we have said before, our main
objective was to evaluate the capabilities of the KY-
OTO technology without adding any additional in-
formation. The use of more linguistic information
will probably facilitate our work and will benefit the
system results. In the near future we will study the
application of machine learning techniques for the
automatic generation of Kybots from the training
data. We also plan to include additional linguistic
and semantic processing in the event extraction pro-
cess to exploit the current semantic and ontological
capabilities of the KYOTO technology.
141
Acknowledgments
This research was supported by the the KYOTO
project (STREP European Community ICT-2007-
211423) and the Basque Government (IT344-10).
References
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini and Carlo Aliprandi. KAF: a generic semantic
annotation format Proceedings of the 5th International
Conference on Generative Approaches to the Lexicon
GL 2009 Pisa, Italy, September 17-19, 2009
Kevin Bretonnel Cohen, Karin Verspoor, Helen L. John-
son, Chris Roeder, Philip V. Ogren, Willian A. Baum-
gartner, Elizabeth White, Hannah Tipney, and Lawer-
ence Hunter. High-precision biological event extrac-
tion: Effects of system and data. Computational Intel-
ligence, to appear, 2011.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano and Jun?ichi Tsujii. Overview of
BioNLP?09 Shared Task on Event Extraction. Pro-
ceedings of the BioNLP 2009 Workshop. Association
for Computational Linguistics. Boulder, Colorado, pp.
89?96., 2011
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011a. Overview of
BioNLP Shared Task 2011. Proceedings of the
BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Andreas Vlachos. Two Strong Baselines for the BioNLP
2009 Event Extraction Task. Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing. Association for Computational Linguistics Upp-
sala, Sweden, pp. 1?9., 2010
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Chris-
tiane Fellbaum, Shu-kai Hsieh, Chu-Ren Huang, Hi-
toshi Isahara, Kyoko Kanzaki, Andrea Marchetti,
Monica Monachini, Federico Neri, Remo Raffaelli,
German Rigau, Maurizio Tescon, Joop VanGent. KY-
OTO: A System for Mining, Structuring, and Dis-
tributing Knowledge Across Languages and Cultures.
Proceedings of LREC 2008. Marrakech, Morocco,
2008.
142
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 48?54,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Combining Rule-Based and Statistical Syntactic Analyzers 
 
 
Mar?a Jes?s Aranzabe*, Arantza D?az de Ilarraza, Nerea Ezeiza, Kepa Bengoetxea, 
Iakes Goenaga, Koldo Gojenola,  
Department of Computer Languages and Systems / * Department of Basque Philology 
University of the Basque Country UPV/EHU 
{maxux.aranzabe@ehu.es, kepa.bengoetxea, jipdisaa@si.ehu.es, 
n.ezeiza@ehu.es, koldo.gojenola@ehu.es, iakes@gmail.com} 
 
 
 
Abstract 
This paper presents the results of a set of 
preliminary experiments combining two 
knowledge-based partial dependency 
analyzers with two statistical parsers, 
applied to the Basque Dependency 
Treebank. The general idea will be to apply 
a stacked scheme where the output of the 
rule-based partial parsers will be given as 
input to MaltParser and MST, two state of 
the art statistical parsers. The results show 
a modest improvement over the baseline, 
although they also present interesting lines 
for further research. 
1. Introduction 
In this paper we present a set of preliminary 
experiments on the combination of two 
knowledge-based partial syntactic analyzers with 
two state of the art data-driven statistical parsers. 
The experiments have been performed on the 
Basque Dependency Treebank (Aduriz et al, 
2003). 
In the last years, many attempts have been 
performed trying to combine different parsers 
(Surdeanu and Manning, 2010), with significant 
improvements over the best individual parser?s 
baseline. The two most successful approaches have 
been stacking (Martins et al, 2008) and voting 
(Sagae and Lavie, 2006, Nivre and McDonald, 
2008, McDonald and Nivre, 2011). In this paper 
we will experiment the use of the stacking 
technique, giving the tags obtained by the rule-
based syntactic partial parsers as input to the 
statistical parsers. 
Morphologically rich languages present new 
challenges, as the use of state of the art parsers for 
more configurational and non-inflected languages 
like English does not reach similar performance 
levels in languages like Basque, Greek or Turkish 
(Nivre et al, 2007a). As it was successfully done 
on part of speech (POS) tagging, where the use of 
rule-based POS taggers (Tapanainen and 
Voutilainen, 1994) or a combination of a rule-
based POS tagger with a statistical one (Aduriz et 
al., 1997, Ezeiza et al, 1998) outperformed purely 
statistical taggers, we think that exploring the 
combination of knowledge-based and data-driven 
systems in syntactic processing can be an 
interesting line of research. 
Most of the experiments on combined parsers 
have relied on different types of statistical parsers 
(Sagae and Lavie, 2006, Martins et al, 2008, 
McDonald and Nivre, 2011), trained on an 
automatically annotated treebank. Yeh (2000) used 
the output of several baseline diverse parsers to 
increase the performance of a second 
transformation-based parser. In our work we will 
study the use of two partial rule-based syntactic 
analyzers together with two data-driven parsers: 
? A rule-based chunker (Aduriz et al, 2004) 
that marks the beginning and end of noun 
phrases, postpositional phrases and verb 
chains, in the IOB (Inside/ 
Outside/Beginning of a chunk) style. 
? A shallow dependency relation annotator 
(Aranzabe et al, 2004), which tries to 
detect dependency relations by assigning a 
48
set of predefined tags to each word, where 
each tag gives both the name of a 
dependency relation (e.g. subject) together 
with the direction of its head (left or right). 
? We will use two statistical dependency 
parsers, MaltParser (Nivre et al, 2007b) 
and MST (McDonald et al 2005). 
In the rest of this paper, section 2 will first 
present the corpus and the different parsers we will 
combine, followed by the experimental results in 
section 3, and the main conclusions of the work. 
2. Resources 
This section will describe the main resources that 
have been used in the experiments. First, 
subsection 2.1 will describe the Basque 
Dependency Treebank, and then subsection 2.2 
will explain the main details of the analyzers that 
have been employed. The analyzers are a rule-
based chunker, a rule-based shallow dependency 
parser and two state of the art data-driven 
dependency parsers, MaltParser and MST.  
2.1 Corpora 
Our work will make use the second version of the 
Basque dependency Treebank (BDT II, Aduriz et 
al., 2003), containing 150,000 tokens (11,225 
sentences). Figure 1 presents an example of a 
syntactically annotated sentence. Each word 
contains its form, lemma, category or coarse part 
of speech (CPOS), POS, morphosyntactic features 
such as case, number of subordinate relations, and 
the dependency relation (headword + dependency). 
The information in figure 1 has been simplified 
due to space reasons, as typically each word 
contains many morphosyntactic features (case, 
number, type of subordinated sentence, ...), which 
are relevant for parsing. The last two lines of the 
sentence in figure 1 do not properly correspond to 
the treebank, but are the result of the rule-based 
partial syntactic analyzers (see subsection 2.2). For 
evaluation, we divided the treebank in three sets, 
corresponding to training, development, and test 
(80%, 10%, and 10%, respectively). The 
experiments were performed on the development 
set, leaving the best system for the final test. 
2.2 Analyzers 
This subsection will present the four types of 
analyzers that have been used. The rule-based 
analyzers are based on the Contraint Grammar 
(CG) formalism (Karlsson et al, 1995), based on 
the assignment of morphosyntactic tags to words 
using a formalism that has the capabilities of finite 
state automata or regular expressions, by means of 
a set of rules that examine mainly local contexts of 
words to determine the correct tag assignment. 
The rule-based chunker (RBC henceforth, 
Aranzabe et al, 2009) uses 560 rules, where 479 of 
the rules deal with noun phrases and the rest with 
verb phrases. The chunker delimits the chunks with 
three tags, using a standard IOB marking style (see 
figure 1). The first one is to mark the beginning of 
the phrase (B-VP if it is a verb phrase and B-NP 
whether it's a noun phrase) and the other one to 
mark the continuation of the phrase (I-NP or I-VP, 
meaning that the word is inside an NP or VP). The 
last tag marks words that are outside a chunk. The 
evaluation of the chunker on the BDT gave a result 
of 87% precision and 85% recall over all chunks. 
We must take into account that this evaluation was 
auxmod 
ccomp_obj 
 auxmod 
Gizonak    mutil    handia   etorri     dela        esan      du . 
The-man       boy        tall-the    come         has+he+that     tell      he+did+it   
N-ERG-S       N          ADJ-ABS-S   V            AUXV+S+COMPL    V         AUXV 
B-NP          B-NP       I-NP        B-VP         I-NP            B-VP      I-VP 
&NCSUBJ>      &NCSUBJ>   $<NCMOD     $CCOMP_OBJ>  &<AUXMOD        &MAINV    &<AUXMOD 
ncsubj 
ncmod 
ncsubj 
Figure 1. Dependency tree for the sentence Gizonak mutil handia etorri dela esan du (the man told that the tall 
boy has come). The two last lines show the tags assigned by the rule-based chunker and the rule-based 
dependency analyzer, respectively. 
(V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG = 
ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, 
&MAINV = main verb, &<AUXMOD = verbal auxiliary modifier). 
 
49
performed on the gold POS tags, rather than on 
automatically assigned POS tasks, as in the present 
experiment. For that reason, the results can serve 
as an upper bound on the real results. 
The rule-based dependency analyzer (RBDA, 
Aranzabe et al, 2004) uses a set of 505 CG rules 
that try to assign dependency relations to 
wordforms. As the CG formalism only allows the 
assignment of tags, the rules only aim at marking 
the name of the dependency relation together with 
the direction of the head (left or right). For 
example, this analyzer assigns tags of the form 
&NCSUBJ> (see figure 1), meaning that the 
corresponding wordform is a non-clausal syntactic 
subject and that its head is situated to its right (the 
?>? or ?<? symbols mark the direction of the 
head). This means that the result of this analysis is 
on the one hand a partial analysis and, on the other 
hand, it does not define a dependency tree, and can 
also be seen as a set of constraints on the shape of 
the tree. The system was evaluated on the BDT, 
obtaining f-scores between 90% for the auxmod 
dependency relation between the auxiliary and the 
main verb and 52% for the subject dependency 
relation, giving a (macro) average of 65%. 
Regarding the data-driven parsers, we have 
made use of MaltParser (Nivre et al, 2007b) and 
MST Parser (McDonald et al, 2006), two state of 
the art dependency parsers representing two 
dominant approaches in data-driven dependency 
parsing, and that have been successfully applied to 
typologically different languages and treebanks 
(McDonald and Nivre, 2007).  
MaltParser (Nivre, 2006) is a representative of 
local, greedy, transition-based dependency parsing 
models, where the parser obtains deterministically 
a dependency tree in a single pass over the input 
using two data structures: a stack of partially 
analyzed items and the remaining input sequence. 
To determine the best action at each step, the 
parser uses history-based feature models and 
discriminative machine learning. The learning 
configuration can include any kind of information 
(such as word-form, lemma, category, subcategory 
or morphological features). Several variants of the 
parser have been implemented, and we will use 
one of its standard versions (MaltParser version 
1.4). In our experiments, we will use the Stack-
Lazy algorithm with the liblinear classifier.  
The MST Parser can be considered a 
representative of global, exhaustive graph-based 
parsing (McDonald et al, 2005, 2006). This 
algorithm finds the highest scoring directed 
spanning tree in a dependency graph forming a 
valid dependency tree. To learn arc scores, it uses 
large-margin structured learning algorithms, which 
optimize the parameters of the model to maximize 
the score margin between the correct dependency 
graph and all incorrect dependency graphs for 
every sentence in a training set. The learning 
procedure is global since model parameters are set 
relative to classifying the entire dependency graph, 
and not just over single arc attachments. This is in 
contrast to the local but richer contexts used by 
transition-based parsers. We use the freely 
available version of MSTParser1. In the following 
experiments we will make use of the second order 
non-projective algorithm.  
3. Experiments  
We will experiment the effect of using the output 
of the knowledge-based analyzers as input to the 
data-driven parsers in a stacked learning scheme. 
Figure 1 shows how the two last lines of the 
example sentence contain the tags assigned by the 
rule-based chunker (B-NP, I-NP, B-VP and I-VP) 
and the rule-based partial dependency analyzer 
(&NCSUBJ, &<NCMOD, &<AUXMOD, 
&CCOMP_OBJ and &MAINV) . 
The first step consisted in applying the complete 
set of text processing tools for Basque, including: 
? Morphological analysis. In Basque, each 
word can receive multiple affixes, as each 
lemma can generate thousands of word-
forms by means of morphological 
properties, such as case, number, tense, or 
different types of subordination for verbs. 
Consequently, the  morphological analyzer 
for Basque (Aduriz et al 2000) gives a 
high ambiguity. If only categorial (POS) 
ambiguity is taken into account, there is an 
average of 1.55 interpretations per word-
form, which rises to 2.65 when the full 
morphosyntactic information is taken into 
account, giving an overall 64% of 
ambiguous word-forms. 
? Morphological disambiguation. 
Disambiguating the output of 
morphological analysis, in order to obtain 
a single interpretation for each word-form, 
                                                           
1 http://mstparser.sourceforge.net 
50
can pose an important problem, as 
determining the correct interpretation for 
each word-form requires in many cases the 
inspection of local contexts, and in some 
others, as the agreement of verbs with 
subject, object or indirect object, it could 
also suppose the examination of elements 
which can be far from each other, added to 
the free constituent order of the main 
sentence elements in Basque. The 
erroneous assignment of incorrect part of 
speech or morphological features can 
difficult the work of the parser. 
? Chunker 
? Partial dependency analyzer 
When performing this task, we found the 
problem of matching the treebank tokens with 
those obtained from the analyzers, as there were 
divergences on the treatment of multiword units, 
mostly coming from Named Entities, verb 
compounds and complex postpositions (formed 
with morphemes appearing at two different words). 
For that reason, we performed a matching process 
trying to link the multiword units given by the 
morphological analysis module and the treebank, 
obtaining a correct match for 99% of the sentences.  
Regarding the data-driven parsers, they are 
trained using two kinds of tags as input: 
? POS and morphosyntactic tags coming 
from the automatic morphological 
processing of the dependency treebank. 
Disambiguation errors, such as an 
incorrect POS category or morphological 
analyses (e.g. the assignment of an 
incorrect case) can harm the parser, as 
tested in Bengoetxea et al (2011). 
? The output of the rule-based partial 
syntactic analyzers (two last lines of the 
example in figure 1). These tags contain 
errors of the CG-based syntactic taggers. 
As the analyzers are applied after 
morphological processing, the errors can 
be propagated and augmented. 
Table 1 shows the results of using the output of 
the knowledge-based analyzers as input to the 
statistical parsers. We have performed three 
experiments for each statistical parser, trying with 
the chunks provided by the chunker, the partial 
dependency parser, and both. The table shows 
modest gains, suggesting that the rule-based 
analyzers help the statistical ones, giving slight 
increases over the baseline, which are statistically 
significant when applying MaltParser to the output 
of the rule-based dependency parser and a 
combination of the chunker and rule-based parsers. 
As table 1 shows, the parser type is relevant, as 
MaltParser seems to be sensitive when using the 
stacked features, while the partial parsers do not 
seem to give any significant improvement to MST. 
3.1 Error analysis 
Looking with more detail at the errors made by the 
different versions of the parsers, we observe 
significant differences in the results for different 
dependency relations, seeing that the statistical 
parsers behave in a different manner regarding to 
each relation, as shown in table 2. The table shows 
the differences in f-score2  corresponding to five 
local dependency relations, (determination of 
verbal modifiers, such as subject, object and 
indirect object).  
McDonald and Nivre (2007) examined the types 
of errors made by the two data-driven parsers used 
in this work, showing how the greedy algorithm of 
MaltParser performed better with local dependency 
relations, while the graph-based algorithm of MST 
was more accurate for global relations. As both the 
chunker and the partial dependency analyzer are 
based on a set of local rules in the CG formalism, 
we could expect that the stacked parsers could 
benefit mostly on the local dependency relations. 
                                                           
2 f-score = 2 * precision * recall / (precision + recall) 
 MaltParser MST Parser 
 LAS UAS LAS UAS 
Baseline 76.77% 82.09%  77.96% 84.04% 
+ RBC 77.10% (+0.33) 82.29% (+0.20)  77.99% (+0.03) 83.99% (-0.05) 
+ RBDA *77.15% (+0.38) 82.27% (+0.18)  78.03% (+0.07) 83.76% (-0.28) 
+ RBC + RBDA  *77.25% (+0.48) 82.18% (+0.09)  78.00% (+0.04) 83.34% (-0.70) 
Table 1. Evaluation results  
(RBC = rule-based chunker, RBDA = rule-based dependency analyzer, LAS: Labeled Attachment Score,  
UAS: Unlabeled Attachment Score, *: statistically significant in McNemar's test, p < 0.05) 
 
51
Table 2 shows how the addition of the rule-based 
parsers? tags performs in accord with this behavior, 
as MaltParser gets f-score improvements for the 
local relations. Although not shown in Table 2, we 
also inspected the results on the long distance 
relations, where we did not observe noticeable 
improvements with respect to the baseline on any 
parser. For that reason, MaltParser, seems to 
mostly benefit of the local nature of the stacked 
features, while MST does not get a significant 
improvement, except for some local dependency 
relations, such as ncobj and ncsubj. 
We performed an additional test using the partial 
dependency analyzer?s gold dependency relations 
as input to MaltParser. As could be expected, the 
gold tags gave a noticeable improvement to the 
parser?s results, reaching 95% LAS. However, 
when examining the scores for the output 
dependency relations, we noticed that the gold 
partial dependency tags are beneficial for some 
relations, although negative for some others. For 
example the non-clausal modifier (ncmod) 
relation?s f-score increases 3.25 points, while the 
dependency relation for clausal subordinate 
sentences functioning as indirect object decreases 
0.46 points, which is surprising in principle. 
For all those reasons, the relation between the 
input dependency tags and the obtained results 
seems to be intricate, and we think that it deserves 
new experiments in order to determine their nature. 
As each type of syntactic information can have an 
important influence on the results on specific 
relations, their study can shed light on novel 
schemes of parser combination. 
4. Conclusions  
We have presented a preliminary effort to integrate 
different syntactic analyzers, with the objective of 
getting the best from each system. Although the 
potential gain is in theory high, the experiments 
have shown very modest improvements, which 
seem to happen in the set of local dependency 
relations. We can point out some avenues for 
further research: 
? Development of the rule-based 
dependency parser using the dependencies 
that give better improvements on the gold 
dependency tags, as this can measure the 
impact of each kind of shallow 
dependency tag on the data-driven parsers. 
? Development of rules that deal with the 
phenomena where the statistical parsers 
perform worse. This requires a careful 
error analysis followed by a redesign of 
the manually developed CG tagging rules. 
? Application of other types of combining 
schemes, such as voting, trying to get the 
best from each type of parser. 
Finally, we must also take into account that the 
rule-based analyzers were developed mainly 
having linguistic principles in mind, such as 
coverage of diverse linguistic phenomena or the 
treatment of specific syntactic constructions 
(Aranzabe et al, 2004), instead of performance-
oriented measures, such as precision and recall. 
This means that there is room for improvement in 
the first-stage knowledge-based parsers, which will 
have, at least in theory, a positive effect on the 
second-phase statistical parsers, allowing us to test 
whether knowledge-based and machine learning-
based systems can be successfully combined. 
Acknowledgements 
This research was supported by the Department of 
Industry of the Basque Government (IT344-10, S-
PE11UN114), the University of the Basque 
Country (GIU09/19) and the Spanish Ministry of 
 MaltParser MST Parser 
Dependency 
relation 
Baseline + RBC + RBDA + RBC  
+ RBDA 
Baseline + RBC + RBDA + RBC  
+ RBDA 
ncmod 75,29 75,90 76,08 76,40 77,15 77,44 76,39 76,92 
ncobj 67,34 68,49 69,67 69,54 64,85 64,86 65,56 66,18 
ncpred 61,37 61,92 61,26 63,50 60,37 57,55 58,44 59,27 
ncsubj 61,92 61,90 63,96 63,91 59,19 59,26 62,23 61,61 
nciobj 75,76 76,53 77,16 76,29 74,23 74,47 72,16 69,08 
Table 2. Comparison of the different parsers? f-score with regard to specific dependency relations 
(ncmod = non-clausal modifier, ncobj = non-clausal object, ncpred = non-clausal predicate, ncsubj = non-clausal subject, 
nciobj = non-clausal indirect object) 
52
Science and Innovation (MICINN, TIN2010- 
20218). 
References  
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, 
Arantza D?az de Ilarraza, Koldo Gojenola and 
Montse Maritxalar. 1997. Morphosyntactic 
disambiguation for Basque based on the Constraint 
Grammar Formalism. Conference on Recent 
Advances in Natural Language Processing 
(RANLP), Bulgaria. 
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki 
Alegria, Xabier Arregi, Jose Mari Arriola, Xabier 
Artola, Koldo Gojenola, Montserrat Maritxalar, Kepa 
Sarasola, and Miriam Urkia. 2000. A word-grammar 
based morphological analyzer for agglutinative 
languages. Coling 2000, Saarbrucken. 
Itziar Aduriz, Jos? Mar?a Arriola, Arantza D?az de 
Ilarraza, Koldo Gojenola, Maite Oronoz and Larraitz 
Uria. 2004. A cascaded syntactic analyser for 
Basque. In Computational Linguistics and Intelligent 
Text Processing, pages 124-135. LNCS Series. 
Springer Verlag. Berlin. 2004 
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria 
Arriola, Aitziber Atutxa, Arantza Diaz de Ilarraza, 
Aitzpea Garmendia and Maite Oronoz. 2003. 
Construction of a Basque dependency treebank. 
Treebanks and Linguistic Theories. 
Mar?a Jes?s Aranzabe, Jos? Mar?a Arriola and Arantza 
D?az de Ilarraza. 2004. Towards a Dependency 
Parser for Basque. In Proceedings of the Workshop 
on Recent Advances in Dependency Grammar, 
Geneva, Switzerland. 
Maria Jesus Aranzabe, Jose Maria Arriola and Arantza 
D?az de Ilarraza. 2009. Theoretical and 
Methodological issues of tagging Noun Phrase 
Structures following Dependency Grammar 
Formalism. In Artiagoitia, X. and Lakarra J.A. (eds) 
Gramatika Jaietan. Patxi Goenagaren omenez. 
Donostia: Gipuzkoako Foru Aldundia-UPV/EHU. 
Kepa Bengoetxea and Koldo Gojenola. 2010. 
Application of Different Techniques to Dependency 
Parsing of Basque. Proceedings of the 1st Workshop 
on Statistical Parsing of Morphologically Rich 
Languages (SPMRL), NAACL-HLT Workshop. 
Kepa Bengoetxea, Arantza Casillas and Koldo 
Gojenola. 2011. Testing the Effect of Morphological 
Disambiguation in Dependency Parsing of Basque. 
Proceedings of the International Conference on 
Parsing Technologies (IWPT). 2nd Workshop on 
Statistical Parsing Morphologically Rich Languages 
(SPMRL), Dublin, Ireland. 
G?lsen Eryi?it, Joakim Nivre and Kemal Oflazer. 2008. 
Dependency Parsing of Turkish. Computational 
Linguistics, Vol. 34 (3).  
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n 
Urizar and Itziar Aduriz. 1998. Combining 
Stochastic and Rule-Based Methods for 
Disambiguation in Agglutinative Languages. 
COLING-ACL?98, Montreal. 
Fred Karlsson, Atro Voutilainen, Juka Heikkila and 
Arto Anttila. 1995. Constraint Grammar: A 
Language-independent System for Parsing 
Unrestricted Text. Mouton de Gruyter. 
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith and 
Eric P. Xing. 2008. Stacking Dependency Parsing. 
Proceedings of EMNLP-2008. 
Ryan McDonald, Kevin Lerman and Fernando Pereira. 
2005. Online large-margin training of dependency 
parsers. In Proceedings of ACL. 
Ryan McDonald, Kevin Lerman and Fernando Pereira. 
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. CoNLL. 
Ryan McDonald and Joakim Nivre. 2007. 
Characterizing the Errors of Data-Driven 
Dependency Parsing Models. Proceedings of the 
2007 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning, EMNLP/CoNLL. 
Ryan McDonald and Joakim Nivre. 2011. Analyzing 
and Integrating Dependency Parsers. Computational 
Linguistics, Vol. 37(1), 197-230. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Springer. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson, Sebastian Riedel and Deniz 
Yuret. 2007a. The CoNLL 2007 Shared Task on 
Dependency Parsing. Proceedings of EMNLP-
CoNLL. 
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, 
G?lsen Eryi?it, Sandra K?bler, S. Marinov and 
Edwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency 
parsing. Natural Language Engineering.  
Joakim Nivre and Ryan McDonald. 2008. Integrating 
graph-based and transition-based dependency 
parsers. Proceedings of ACL-2008. 
Kenji Sagae and Alon Lavie. 2006. Parser Combination 
by Reparsing. Proceedings of the Human Language 
53
Technology Conference of the North American 
Chapter of the ACL, pages 129?132, New York. 
Mihai Surdeanu and Christopher D. Manning. 2010. 
Ensemble Models for Dependency Parsing: Cheap 
and Good? Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the ACL. 
Pasi Tapanainen and Atro Voutilainen. 1994. Tagging 
Accurately-Don?t guess if you know. Proceedings 
of the Conference on Applied Natural Language 
Processing, ANLP?94. 
Alexander Yeh. 2000. Using existing systems to 
supplement small amounts of annotated 
grammatical relations training data. Proceedings of 
ACL 2000. 
54
