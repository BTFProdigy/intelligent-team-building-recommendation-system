Automatic Detection of Grammar Elements that Decrease Readability
Masatoshi Tsuchiya and Satoshi Sato
Department of Intelligence Science and Technology,
Graduate School of Informatics, Kyoto University
tsuchiya@pine.kuee.kyoto-u.ac.jp, sato@i.kyoto-u.ac.jp
Abstract
This paper proposes an automatic method
of detecting grammar elements that de-
crease readability in a Japanese sentence.
The method consists of two components:
(1) the check list of the grammar elements
that should be detected; and (2) the de-
tector, which is a search program of the
grammar elements from a sentence. By
defining a readability level for every gram-
mar element, we can find which part of the
sentence is difficult to read.
1 Introduction
We always prefer readable texts to unreadable texts.
The texts that transmit crucial information, such as
instructions of strong medicines, must be completely
readable. When texts are unreadable, we should
rewrite them to improve readability.
In English, measuring readability as reading age
is well studied (Johnson, 1978). The reading age
is the chronological age of a reader who could just
understand the text. The value is usually calculated
from the sentence length and the number of sylla-
bles. From this value, we find whether a text is read-
able or not for readers of a specific age; however, we
do not find which part we should rewrite to improve
readability when the text is unreadable.
The goal of our study is to present tools that help
rewriting work of improving readability in Japanese.
The first tool is to help detect the sentence frag-
ments (words and phrases) that should be rewrit-
ten; in other words, it is a checker of ?hard-to-read?
words and phrases in a sentence. Such a checker can
be realized with two components: the check list and
its detector. The check list provides check items and
their readability levels. The detector is a program
that searches the check items in a sentence. From
the detected items and their readability levels, we
can identify which part of the sentence is difficult to
read.
We are currently working on three aspects con-
cerned with readability of Japanese: kanji charac-
ters, vocabulary, and grammar. In this paper, we re-
ports the readability checker for the grammar aspect.
2 The check list of grammar elements
The first component of the readability checker is
the check list; in this list, we should define every
Japanese grammar element and its readability level.
A grammar element is a grammatical phenomenon
concerned with readability, and its readability level
indicates the familiarity of the grammar element.
In Japanese, grammar elements are classified into
four categories.
1. Conjugation: the form of a verb or an adjective
changes appropriately to the proceed word.
2. Functional word: postpositional particles work
as case makers; auxiliary verbs represent tense
and modality.
3. Sentential pattern: negation, passive form, and
question are represented as special sentence
patterns.
4. Functional phrase: there are idiomatic phrases
works functionally, like ?not only ... but also
...? in English.
A grammar section exists in a part of the Japanese
Language Proficiency Test, which is used to measure
and certify the Japanese language ability of a person
who is a non-Japanese. There are four levels in this
test; Level 4 is the elementary level, and Level 1 is
the advanced level.
Test Content Specifications (TCS) (Foundation
and Association of International Education, 1994) is
intended to serve as a reference guide in question
compilation of the Japanese Language Proficiency
Test. This book describes the list of grammar ele-
ments, which can be tested at each level. These lists
fit our purpose: they can be used as the check list for
the readability checker.
TCS describes grammar elements in two ways. In
the first way, a grammar element is described as a
3-tuple: its name, its patterns, and its example sen-
tences. The following 3-tuple is an example of the
grammar element that belongs to Level 4.
Name
daimeishi
??? (Pronoun)
Patterns
kore
?? (this), sore?? (that)
Examples
kore
??
ha
?
hon
?
desu.
???(This is a book.),
sore
??
ha
?
no?to
???
desu.
???(That is a note.)
Grammar elements of Level 3 and Level 4 are con-
jugations, functional words and sentential patterns
that are defined in this first way. In the second way,
a grammar element is described as a pair of its pat-
terns and its examples. The following pair is an ex-
ample of the grammar element that belongs to Level
2.
Patterns ?
ta
?
tokoro
??? (when ...)
Examples
sensei
??
no
?
otaku
??
he
?
ukagatta
???
tokoro
???
(When visiting the teacher?s home)
Grammar elements of Level 1 and Level 2 are func-
tional phrases that are defined in this second way.
We decided to use this example-based definition
for the check list, because the check list should be in-
dependent from the implementation of the detector.
If the check list depends on detector?s implementa-
tion, the change of implementation requires change
of the check list.
Each item of the check list is defined as a 3-tuple:
(1) readability level, (2) name, and (3) a list of exam-
ple pairs. There are four readability levels according
Table 1: The size of the check list
Level # of rules
1 134
2 322
3 97
4 95
Total 648
to the Japanese Language Proficiency Test. An ex-
ample pair consists of an example sentence and an
instance of the grammar element. It is an implicit
description of the pattern detecting the grammar el-
ement. For example, the check item for ?Adjective
(predicative, negative, polite)? is shown as follows,
Level 4
Name Adjective (predicative, negative, polite)
Test Pairs
Sentence1
kono
??
heya
??
ha
?
hiroku
??
nai
??
desu.
???
(This room is not large.)
Instance1
hiroku
??
nai
??
desu
??
(is not large)
The instance??????/hirokunaidesu/ consists
of three morphemes: (1)??/hiroku/, the adjective
means ?large? in renyo form, (2)??/nai/, the ad-
jective means ?not? in root form, and (3)??/desu/,
the auxiliary verb ends a sentence politely. So, this
test pair represents implicitly that the grammar el-
ement can be detected by a pattern ?Adjective(in
renyo form) + nai + desu?.
All example sentences are originated from TCS.
Some check items have several test pairs. Table 1
shows the size of the check list.
3 The grammar elements detector
The check list must be converted into an explicit
rule set, because each item of the check list shows
no explicit description of its grammar element, only
shows one or more pairs of an example sentence and
an instance.
3.1 The explicit rule set
Four categories of grammar elements leads that each
rule of the explicit rule set may take three different
types.
? Type M: A rule detecting a sequence of mor-
phemes
? Type B: A rule detecting a bunsetsu.
? Type R: A rule detecting a modifier-modifee re-
lationship.
Type M is the basic type of them, because almost of
grammar elements can be detected by morphologi-
cal sequential patterns.
Conversion from a check item to a Type M rule
is almost automatic. This conversion process con-
sists of three steps. First, an example sentence of
the check item is analyzed morphologically and syn-
tactically. Second, a sentence fragment covered by
the target grammar element is extracted based on
signs and fixed strings included in the name of the
check item. Third, a part of a generated rule is re-
laxed based on part-of-speech tags. For example,
the check item of the grammar element whose name
is ?Adjective (predicative, negative, polite)? is con-
verted to the following rule.
np( 4, ?Adjective
(predicative,negative,polite)?,
Dm({ H1=>?Adjective?,
K2=>?Basic Renyou Form? },
{ G=>???/nai/?,
H1=>?Postfix?, K2=>?Root Form? },
{ G=>???/desu/?,
H1=>?Auxiliary Verb? }) );
The function np() makes the declaration of the
rule, and the function Dm() describes a morphologi-
cal sequential pattern which matches the target. This
example means that this grammar element belongs
to Level 4, and can be detected by the pattern which
consists of three morphemes.
Type B rules are used to describe grammar ele-
ments such as conjugations including no functional
words. They are not generated automatically; they
are converted by hand from type M rules that are
generated automatically. For example, the rule de-
tecting the grammar element whose name is ?Adjec-
tive in Root Form? is defined as follows.
np( 4, ?Adjective in Root Form?,
Db( { H1=>?Adjective?,
K2=>?Root Form? } ) );
The function Db() describes a pattern which
matches a bunsetsu which consists of specified mor-
phemes. This example means that this grammar el-
ement belongs to Level 3, and shows the detection
pattern of this grammar element.
Converted Automatically
     + Modified by Hand
KNP
Juman
Detection
Converted
Automatically
Loaded
Sentence
Morphological
Analysis
Syntactic Analysis
+Detection against
  morphmes and 
  bunsetsues
Detection against
modifier-modifee
relationships
+ Lanking
KNP RuleRule Set
Check List
Sentence + Grammar Elements
Figure 1: System structure
Type R rules are used to describe grammar ele-
ments that include modifier-modifee relationships.
In the case of the grammar element whose name is
?Verb Modified by Adjective?, it includes a structure
that an adjective modifies a verb. It is impossible
to detect this grammar element by a morphological
continuous pattern, because any bunsetsus can be in-
serted between the adjective and the verb. For such a
grammar element, we introduce the function Dk()
that takes two arguments: the former is a modifier
and the latter is its modifee.
np( 4, ?Verb Modified by Adjective?,
Dk( Db({ H1=>?Adjective?,
K2=>?Basic Renyou Form? }),
Dm({ H1=>?Verb? }) ) );
3.2 The architecture of the detector
The architecture of the detector is shown in Figure 1.
The detector uses a morphological analyzer, Juman,
and a syntactic analyzer, KNP (Kurohashi and Na-
gao, 1994). The rule set is converted into the format
that KNP can read and it is added to the standard rule
set of KNP. This addition enables KNP to detect can-
didates of grammar elements. The ?Detection? part
selects final results from these candidates based on
preference information given by the rule set.
Figure 2 shows grammar elements detected by our
detector from the sentence ?
chizu
??
ha
?
oroka,
????
ryakuzu
??
sae
??
mo
?
kubarare
???
nakatta.
?????? which means ?Neither a
map nor a rough map was not distributed.?
4 Experiment
We conducted two experiments, in order to check
the performance of our detector.
Fragment Name Level
chizu
?? (a map) - -
ha
?
oroka
??? (neither) ?ha? oroka??? (neither ...) 1
? (,) ?? (comma) 4
ryakuzu
? ? (a rough map) - -
sae
?? (even) ? sae?? (even ...) 2
mo
? (nor) ?!? (huku postpositional particle means ?nor?) 4
kubarare
??? (distributed) ? reru?? (passive verb phrase) 3
nakatta
???? (was not) ? nai?? (predicative adjective means ?not?) 4
? (.) ?? (period) 4
Figure 2: Automatically detected grammar elements
The first test is a closed test, where we examine
whether grammar elements in example sentences of
TCS are detected correctly. TCS gives 840 example
sentences, and there are 802 sentences from which
their grammar elements are detected correctly. From
the rest 38 sentences, our detector failed to detect
the right grammar element. This result shows that
our program achieves the sufficient recall 95% in the
closed test. Almost of these errors are caused failure
of morphological analysis.
The second test is an open test, where we examine
whether grammar elements in example sentences of
the textbook, which is written for learners preparing
for the Japanese Language Proficiency Test (Tomo-
matsu et al, 1996), are detected correctly. The text-
book gives 1110 example sentences, and there are
680 sentences from which their grammar elements
are detected correctly. Wrong grammar elements
are detected from 71 sentences, and no grammar el-
ements are detected from the rest 359 sentences. So,
the recall of automatic detection of grammar ele-
ments is 61%, and the precision is 90%. The ma-
jor reason of these failures is strictness of several
rules; several rules that are generated from example
pairs automatically are overfitting to example pairs
so that they cannot detect variations in the textbook.
We think that relaxation of such rules will eliminate
these failures.
References
The Japan Foundation and Japan Association of Interna-
tional Education. 1994. Japanese Language Profi-
ciency Test: Test content Specifications (Revised Edi-
tion). Bonjin-sha Co.
Keith Johnson. 1978. Readability. http://www.
timetabler.com/readable.pdf.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
Etsuko Tomomatsu, Jun Miyamoto, and Masako Waguri.
1996. Donna-toki Dou-tsukau Nihongo Hyougen
Bunkei 500. ALC Co.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 197?200,
Prague, June 2007. c?2007 Association for Computational Linguistics
Expanding Indonesian-Japanese Small Translation Dictionary
Using a Pivot Language
Masatoshi Tsuchiya? Ayu Purwarianti? Toshiyuki Wakita? Seiichi Nakagawa?
?Information and Media Center / ?Department of Information and Computer Sciences,
Toyohashi University of Technology
tsuchiya@imc.tut.ac.jp, {wakita,ayu,nakagawa}@slp.ics.tut.ac.jp
Abstract
We propose a novel method to expand a
small existing translation dictionary to a
large translation dictionary using a pivot lan-
guage. Our method depends on the assump-
tion that it is possible to find a pivot lan-
guage for a given language pair on con-
dition that there are both a large transla-
tion dictionary from the source language
to the pivot language, and a large transla-
tion dictionary from the pivot language to
the destination language. Experiments that
expands the Indonesian-Japanese dictionary
using the English language as a pivot lan-
guage shows that the proposed method can
improve performance of a real CLIR system.
1 Introduction
Rich cross lingual resources including large trans-
lation dictionaries are necessary in order to realize
working cross-lingual NLP applications. However,
it is infeasible to build such resources for all lan-
guage pairs, because there are many languages in the
world. Actually, while rich resources are available
for several popular language pairs like the English
language and the Japanese language, poor resources
are only available for rest unfamiliar language pairs.
In order to resolve this situation, automatic con-
struction of translation dictionary is effective, but it
is quite difficult as widely known. We, therefore,
concentrate on the task of expanding a small existing
translation dictionary instead of it. Let us consider
three dictionaries: a small seed dictionary which
consists of headwords in the source language and
their translations in the destination language, a large
source-pivot dictionarywhich consists of headwords
in the source language and their translations in the
pivot language, and a large pivot-destination dictio-
nary which consists of headwords in the pivot lan-
guage and their translations in the destination lan-
guage. When these three dictionaries are given, ex-
panding the seed dictionary is to translate words in
the source language that meets two conditions: (1)
they are not contained in the seed dictionary, and (2)
they can be translated to the destination language
transitively referring both the source-pivot dictio-
nary and the pivot-destination dictionary.
Obviously, this task depends on two assumptions:
(a) the existence of the small seed dictionary, and
(b) the existence of the pivot language which meets
the condition that there are both a large source-
pivot dictionary and a large pivot-destination dic-
tionary. Because of the first assumption, it is true
that this task cannot be applied to a brand-new lan-
guage pair. However, the number of such brand-
new language pairs are decreasing while machine-
readable language resources are increasing. More-
over, The second assumption is valid for many lan-
guage pairs, when supposing the English language
as a pivot. From these point of view, we think that
the expansion task is more promising, although it de-
pends more assumptions than the construction task.
There are two different points among the expan-
sion task and the construction task. Previous re-
searches of the construction task can be classified
into two groups. The first group consists of re-
searches to construct a new translation dictionary for
a fresh language pair from existing translation dic-
tionaries or other language resources (Tanaka and
Umemura, 1994). In the first group, information of
the seed dictionary are not counted in them unlike
the expansion task, because it is assumed that there
is no seed dictionary for such fresh language pairs.
The second group consists of researches to translate
197
xs
v(xs) vt(xs)
ys zs u(zs)
Corpus in
the source
Source-Pivot
Dictionary
Pivot-
Destination
Dictionary
Corpus in
the destination
Seed
Dictionary
Select
output
words
Figure 1: Translation Procedure
novel words using both a large existing translation
dictionary and other linguistic resources like huge
parallel corpora (Tonoike et al, 2005). Because al-
most of novel words are nouns, these researches fo-
cus into the task of translating nouns. In the expan-
sion task, however, it is necessary to translate verbs
and adjectives as well as nouns, because a seed dic-
tionary will be so small that only basic words will be
contained in it if the target language pair is unfamil-
iar. We will discuss about this topic in Section 3.2.
The remainder of this paper is organised as fol-
lows: Section 2 describes the method to expand a
small seed dictionary. The experiments presented in
Section 3 shows that the proposed method can im-
prove performance of a real CLIR system. This pa-
per ends with concluding remarks in Section 4.
2 Method of Expanding Seed Dictionary
The proposed method roughly consists of two steps
shown in Figure 1. The first step is to generate a co-
occurrence vector on the destination language cor-
responding to an input word, using both the seed
dictionary and a monolingual corpus in the source
language. The second step is to list translation can-
didates up, referring both the source-pivot dictionary
and the pivot-destination dictionary, and to calculate
their co-occurrence vectors based on a monolingual
corpus in the destination.
The seed dictionary is used to convert a co-
occurrence vector in the source language into a
vector in the destination language. In this paper,
f(wi, wj) represents a co-occurrence frequency of
a word wi and a word wj for all languages. A co-
occurrence vector v(xs) of a word xs in the source
is:
v(xs) = (f(xs, x1), . . . , f(xs, xn)), (1)
where xi(i = 1, 2, . . . , n) is a headword of the
seed dictionary D. A co-occurrence vector v(xs),
whose each element is corresponding to a word in
the source, is converted into a vector vt(xs), whose
each element is corresponding to a word in the des-
tination, referring the dictionary D:
vt(xs) = (ft(xs, z1), . . . , ft(xs, zm)), (2)
where zj(j = 1, 2, . . . ,m) is a translation word
which appears in the dictionary D. The function
ft(xs, zk), which assigns a co-occurrence degree be-
tween a word xs and a word zj in the destination
based on a co-occurrence vector of a word xs in the
source, is defined as follows:
ft(xs, zj) =
n
?
i=1
f(xs, xi) ? ?(xi, zj). (3)
where ?(xi, zj) is equal to one when a word zj is in-
cluded in a translation word set D(xi), which con-
sists of translation words of a word xi, and zero oth-
erwise.
A set of description sentences Ys in the pivot
are obtained referring the source-pivot dictionary
for a word xs. After that, a description sentence
ys ? Ys in the pivot is converted to a set of de-
scription sentences Zs in the destination referring
the pivot-destination dictionary. A co-occurrence
vector against a candidate description sentence zs =
z1sz2s ? ? ? zls, which is an instance of Zs, is calculated
by this equation:
u(zs) =
( l
?
k=1
f(zks , z1) , . . . ,
l
?
k=1
f(zks , zm)
)
(4)
Finally, the candidate zs which meets a certain
condition is selected as an output. Two conditions
are examined in this paper: (1) selecting top-n can-
didates from sorted ones according to each similarity
score, and (2) selecting candidates whose similarity
scores are greater than a certain threshold. In this pa-
per, cosine distance s(vt(xs),u(zs)) between a vec-
tor based on an input word xs and a vector based on
198
a candidate zs is used as the similarity score between
them.
3 Experiments
In this section, we present the experiments of the
proposed method that the Indonesian language, the
English language and the Japanese language are
adopted as the source language, the pivot language
and the destination language respectively.
3.1 Experimental Data
The proposed method depends on three translation
dictionaries and two monolingual corpora as de-
scribed in Section 2.
Mainichi Newspaper Corpus (1993?1995), which
contains 3.5M sentences consist of 140M words, is
used as the Japanese corpus. When measuring simi-
larity between words using co-occurrence vectors, it
is common that a corpus in the source language for
the similar domain to one of the corpus in the source
language is more suitable than one for a different do-
main. Unfortunately, because we could not find such
corpus, the articles which were downloaded from
the Indonesian Newspaper WEB sites1 are used as
the Indonesian corpus. It contains 1.3M sentences,
which are tokenized into 10M words.
An online Indonesian-Japanese dictionary2 con-
tains 10,172 headwords, however, only 6,577 head-
words of them appear in the Indonesian corpus. We
divide them into two sets: the first set which con-
sists of 6,077 entries is used as the seed dictionary,
and the second set which consists of 500 entries is
used to evaluate translation performance. Moreover,
an online Indonesian-English dictionary3, and an
English-Japanese dictionary(Michibata, 2002) are
also used as the source-pivot dictionary and the
pivot-destination dictionary.
3.2 Evaluation of Translation Performance
As described in Section 2, two conditions of select-
ing output words among candidates are examined.
Table 1 shows their performances and the baseline,
1http://www.kompas.com/,
http://www.tempointeraktif.com/
2http://m1.ryu.titech.ac.jp/?indonesia/
todai/dokumen/kamusjpina.pdf
3http://nlp.aia.bppt.go.id/kebi
that is the translation performance when all candi-
dates are selected as output words. It is revealed that
the condition of selecting top-n candidates outper-
forms the another condition and the baseline. The
maximum F?=1 value of 52.5% is achieved when
selecting top-3 candidates as output words.
Table 2 shows that the lexical distribution of head-
words contained in the seed dictionary are quite sim-
ilar to the lexical distribution of headwords con-
tained in the source-pivot dictionary. This obser-
vation means that it is necessary to translate verbs
and adjectives as well as nouns, when expanding this
seed dictionary. Table 3 shows translation perfor-
mances against nouns, verbs and adjectives, when
selecting top-3 candidates as output words. The pro-
posed method can be regarded likely because it is
effective to verbs and adjectives as well as to nouns,
whereas the baseline precision of verbs is consider-
ably lower than the others.
3.3 CLIR Performance Improved by
Expanded Dictionary
In this section, performance impact is presented
when the dictionary expanded by the proposed
method is adopted to the real CLIR system proposed
in (Purwarianti et al, 2007).
NTCIR3 Web Retrieval Task(Eguchi et al, 2003)
provides the evaluation dataset and defines the eval-
uation metric. The evaluation metric consists of four
MAP values: PC, PL, RC and RL. They are cor-
responding to assessment types respectively. The
dataset consists 100GB Japanese WEB documents
and 47 queries of Japanese topics. The Indonesian
queries, which are manually translated from them,
are used as inputs of the experiment systems. The
number of unique words which occur in the queries
is 301, and the number of unique words which are
not contained in the Indonesian-Japanese dictionary
is 106 (35%). It is reduced to 78 (26%), while the
existing dictionary that contains 10,172 entries is ex-
panded to the dictionary containing 20,457 entries
with the proposed method.
Table 4 shows the MAP values achieved by both
the baseline systems using the existing dictionary
and ones using the expanded dictionary. The for-
mer three systems use existing dictionaries, and the
latter three systems use the expanded one. The 3rd
system translates keywords transitively using both
199
Table 1: Comparison between Conditions of Selecting Output Words
Selecting top-n candidates Selecting plausible candidates Baseline
n = 1 n = 2 n = 3 n = 5 n = 10 x = 0.1 x = 0.16 x = 0.2 x = 0.3
Prec. 55.4% 49.9% 46.2% 40.0% 32.2% 20.8% 23.6% 25.8% 33.0% 18.9%
Rec. 40.9% 52.6% 60.7% 67.4% 74.8% 65.3% 50.1% 40.0% 16.9% 82.5%
F?=1 47.1% 51.2% 52.5% 50.2% 45.0% 31.6% 32.1% 31.4% 22.4% 30.8%
Table 2: Lexical Classification of Headwords
Indonesian- Indonesian-
Japanese English
# of nouns 4085 (57.4%) 15718 (53.5%)
# of verbs 1910 (26.8%) 9600 (32.7%)
# of adjectives 795 (11.2%) 3390 (11.5%)
# of other words 330 (4.6%) 682 (2.3%)
Total 7120 (100%) 29390 (100%)
Table 3: Performance for Nouns, Verbs and Adjectives
Noun Verb Adjective
n = 3 Baseline n = 3 Baseline n = 3 Baseline
Prec. 49.1% 21.8% 41.0% 14.7% 46.9% 26.7%
Rec. 65.6% 80.6% 52.3% 84.1% 59.4% 88.4%
F?=1 56.2% 34.3% 46.0% 25.0% 52.4% 41.0%
Table 4: CLIR Performance
PC PL RC RL
(1) Existing Indonesian-Japanese dictionary 0.044 0.044 0.037 0.037
(2) Existing Indonesian-Japanese dictionary and Japanese proper name dictionary 0.054 0.052 0.047 0.045
(3) Indonesian-English-Japanese transitive translation with statistic filtering 0.078 0.072 0.055 0.053
(4) Expanded Indonesian-Japanese dictionary 0.061 0.059 0.046 0.046
(5) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary 0.066 0.063 0.049 0.049
(6) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary and
statistic filtering
0.074 0.072 0.059 0.058
the source-pivot dictionary and the pivot-destination
dictionary, and the others translate keywords using
either the existing source-destination dictionary or
the expanded one. The 3rd system and the 6th sys-
tem try to eliminate unnecessary translations based
statistic measures calculated from retrieved docu-
ments. These measures are effective as shown in
(Purwarianti et al, 2007), but, consume a high run-
time computational cost to reduce enormous transla-
tion candidates statistically. It is revealed that CLIR
systems using the expanded dictionary outperform
ones using the existing dictionary without statistic
filtering. And more, it shows that ones using the ex-
panded dictionary without statistic filtering achieve
near performance to the 3rd system without paying
a high run-time computational cost. Once it is paid,
the 6th system achieves almost same score of the 3rd
system. These observation leads that we can con-
clude that our proposed method to expand dictionary
is valuable to a real CLIR system.
4 Concluding Remarks
In this paper, a novel method of expanding a small
existing translation dictionary to a large translation
dictionary using a pivot language is proposed. Our
method uses information obtained from a small ex-
isting translation dictionary from the source lan-
guage to the destination language effectively. Exper-
iments that expands the Indonesian-Japanese dictio-
nary using the English language as a pivot language
shows that the proposed method can improve perfor-
mance of a real CLIR system.
References
Koji Eguchi, Keizo Oyama, Emi Ishida, Noriko Kando, , and
Kazuko Kuriyama. 2003. Overview of the web retrieval task
at the third NTCIR workshop. In Proceedings of the Third
NTCIR Workshop on research in Information Retrieval, Au-
tomatic Text Summarization and Question Answering.
Hideki Michibata, editor. 2002. Eijiro. ALC, 3. (in Japanese).
Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nakagawa.
2007. Indonesian-Japanese transitive translation using En-
glish for CLIR. Journal of Natural Language Processing,
14(2), Apr.
Kumiko Tanaka and Kyoji Umemura. 1994. Construction of
a bilingual dictionary intermediated by a third language. In
Proceedings of the 15th International Conference on Com-
putational Linguistics.
Masatugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro
Sasaki, Takehito Utsuro, and Satoshi Sato. 2005. Trans-
lation estimation for technical terms using corpus collected
from the web. In Proceedings of the Pacific Association for
Computational Linguistics, pages 325?331, August.
200
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 125?128,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Robust Extraction of Named Entity Including Unfamiliar Word
Masatoshi Tsuchiya? Shinya Hida? Seiichi Nakagawa?
?Information and Media Center / ?Department of Information and Computer Sciences,
Toyohashi University of Technology
tsuchiya@imc.tut.ac.jp, {hida,nakagawa}@slp.ics.tut.ac.jp
Abstract
This paper proposes a novel method to extract
named entities including unfamiliar words
which do not occur or occur few times in a
training corpus using a large unannotated cor-
pus. The proposed method consists of two
steps. The first step is to assign the most simi-
lar and familiar word to each unfamiliar word
based on their context vectors calculated from
a large unannotated corpus. After that, tra-
ditional machine learning approaches are em-
ployed as the second step. The experiments of
extracting Japanese named entities from IREX
corpus and NHK corpus show the effective-
ness of the proposed method.
1 Introduction
It is widely agreed that extraction of named entity
(henceforth, denoted as NE) is an important sub-
task for various NLP applications. Various ma-
chine learning approaches such as maximum en-
tropy(Uchimoto et al, 2000), decision list(Sassano
and Utsuro, 2000; Isozaki, 2001), and Support
Vector Machine(Yamada et al, 2002; Isozaki and
Kazawa, 2002) were investigated for extracting NEs.
All of them require a corpus whose NEs are an-
notated properly as training data. However, it is dif-
ficult to obtain an enough corpus in the real world,
because there are increasing the number of NEs like
personal names and company names. For example,
a large database of organization names(Nichigai As-
sociates, 2007) already contains 171,708 entries and
is still increasing. Therefore, a robust method to ex-
tract NEs including unfamiliar words which do not
occur or occur few times in a training corpus is nec-
essary.
This paper proposes a novel method of extract-
ing NEs which contain unfamiliar morphemes us-
ing a large unannotated corpus, in order to resolve
the above problem. The proposed method consists
Table 1: Statistics of NE Types of IREX Corpus
NE Type Frequency (%)
ARTIFACT 747 (4.0)
DATE 3567 (19.1)
LOCATION 5463 (29.2)
MONEY 390 (2.1)
ORGANIZATION 3676 (19.7)
PERCENT 492 (2.6)
PERSON 3840 (20.6)
TIME 502 (2.7)
Total 18677
of two steps. The first step is to assign the most
similar and familiar morpheme to each unfamiliar
morpheme based on their context vectors calculated
from a large unannotated corpus. The second step is
to employ traditional machine learning approaches
using both features of original morphemes and fea-
tures of similar morphemes. The experiments of
extracting Japanese NEs from IREX corpus and
NHK corpus show the effectiveness of the proposed
method.
2 Extraction of Japanese Named Entity
2.1 Task of the IREX Workshop
The task of NE extraction of the IREX workshop
(Sekine and Eriguchi, 2000) is to recognize eight
NE types in Table 1. The organizer of the IREX
workshop provided a training corpus, which consists
of 1,174 newspaper articles published from January
1st 1995 to 10th which include 18,677 NEs. In the
Japanese language, no other corpus whose NEs are
annotated is publicly available as far as we know.1
2.2 Chunking of Named Entities
It is quite common that the task of extracting
Japanese NEs from a sentence is formalized as
a chunking problem against a sequence of mor-
1The organizer of the IREX workshop also provides the test-
ing data to its participants, however, we cannot see it because
we did not join it.
125
phemes. For representing proper chunks, we em-
ploy IOB2 representation, one of those which have
been studied well in various chunking tasks of
NLP (Tjong Kim Sang, 1999). This representation
uses the following three labels.
B Current token is the beginning of a chunk.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
Actually, we prepare the 16 derived labels from the
label B and the label I for eight NE types, in order
to distinguish them.
When the task of extracting Japanese NEs from
a sentence is formalized as a chunking problem of a
sequence of morphemes, the segmentation boundary
problem arises as widely known. For example, the
NE definition of IREX tells that a Chinese character
? (bei)? must be extracted as an NE means Amer-
ica from a morpheme ? (hou-bei)? which means
visiting America. A naive chunker using a mor-
pheme as a chunking unit cannot extract such kind of
NEs. In order to cope this problem, (Uchimoto et al,
2000) proposed employing translation rules to mod-
ify problematic morphemes, and (Asahara and Mat-
sumoto, 2003; Nakano and Hirai, 2004) formalized
the task of extracting NEs as a chunking problem
of a sequence of characters instead of a sequence of
morphemes. In this paper, we keep the naive formal-
ization, because it is still enough to compare perfor-
mances of proposed methods and baseline methods.
3 Robust Extraction of Named Entities
Including Unfamiliar Words
The proposed method of extracting NEs consists
of two steps. Its first step is to assign the most
similar and familiar morpheme to each unfamiliar
morpheme based on their context vectors calculated
from a large unannotated corpus. The second step is
to employ traditional machine learning approaches
using both features of original morphemes and fea-
tures of similar morphemes. The following sub-
sections describe these steps respectively.
3.1 Assignment of Similar Morpheme
A context vector Vm of a morpheme m is a vector
consisting of frequencies of all possible unigrams
and bigrams,
Vm =
?
?
?
?
?
f(m,m0), ? ? ? f(m,mN ),
f(m,m0,m0), ? ? ? f(m,mN ,mN ),
f(m0,m), ? ? ? f(mN ,m),
f(m0,m0,m), ? ? ? f(mN ,mN ,m)
?
?
?
?
?
,
where M ? {m0,m1, . . . ,mN} is a set of all mor-
phemes of the unannotated corpus, f(mi,mj) is a
frequency that a sequence of a morpheme mi and
a morpheme mj occurs in the unannotated corpus,
and f(mi,mj ,mk) is a frequency that a sequence
of morphemes mi,mj and mk occurs in the unan-
notated corpus.
Suppose an unfamiliar morpheme mu ? M?MF ,
where MF is a set of familiar morphemes that occur
frequently in the annotated corpus. The most sim-
ilar morpheme m?u to the morpheme mu measured
with their context vectors is given by the following
equation,
m?u = argmax
m?MF
sim(Vmu , Vm), (1)
where sim(Vi, Vj) is a similarity function between
context vectors. In this paper, the cosine function is
employed as it.
3.2 Features
The feature set Fi at i-th position is defined as a tuple
of the morpheme feature MF (mi) of the i-th mor-
pheme mi, the similar morpheme feature SF (mi),
and the character type feature CF (mi).
Fi = ? MF (mi), SF (mi), CF (mi) ?
The morpheme feature MF (mi) is a pair of the sur-
face string and the part-of-speech of mi. The similar
morpheme feature SF (mi) is defined as
SF (mi) =
{
MF (m?i) if mi ? M ? MF
MF (mi) otherwise
,
where m?i is the most similar and familiar morpheme
to mi given by Equation (1). The character type fea-
ture CF (mi) is a set of four binary flags to indi-
cate that the surface string of mi contains a Chinese
character, a hiragana character, a katakana charac-
ter, and an English alphabet respectively.
When we identify the chunk label ci for the i-
th morpheme mi, the surrounding five feature sets
Fi?2, Fi?1, Fi, Fi+1, Fi+2 and the preceding two
chunk labels ci?2, ci?1 are refered.
126
Morpheme Feature Similar Morpheme Feature Character
(English POS (English POS Type Chunk Label
translation) translation) Feature
(kyou) (today) Noun?Adverbial (kyou) (today) Noun?Adverbial ?1, 0, 0, 0? O
(no) gen Particle (no) gen Particle ?0, 1, 0, 0? O
(Ishikari) (Ishikari) Noun?Proper (Kantou) (Kantou) Noun?Proper ?1, 0, 0, 0? B-LOCATION
(heiya) (plain) Noun?Generic (heiya) (plain) Noun?Generic ?1, 0, 0, 0? I-LOCATION
(no) gen Particle (no) gen Particle ?0, 1, 0, 0? O
(tenki) (weather) Noun?Generic (tenki) (weather) Noun?Generic ?1, 0, 0, 0? O
(ha) top Particle (ha) top Particle ?0, 1, 0, 0? O
(hare) (fine) Noun?Generic (hare) (fine) Noun?Generic ?1, 1, 0, 0? O
Figure 1: Example of Training Instance for Proposed Method
?? Parsing Direction ??
Feature set Fi?2 Fi?1 Fi Fi+1 Fi+2
Chunk label ci?2 ci?1 ci
Figure 1 shows an example of training instance of
the proposed method for the sentence ? (kyou)
(no) (Ishikari) (heiya) (no)
(tenki) (ha) (hare)? which means ?It is fine at
Ishikari-plain, today?. ? (Kantou)? is assigned
as the most similar and familiar morpheme to ?
(Ishikari)? which is unfamiliar in the training corpus.
4 Experimental Evaluation
4.1 Experimental Setup
IREX Corpus is used as the annotated corpus to train
statistical NE chunkers, and MF is defined experi-
mentally as a set of all morphemes which occur five
or more times in IREX corpus. Mainichi News-
paper Corpus (1993?1995), which contains 3.5M
sentences consisting of 140M words, is used as
the unannotated corpus to calculate context vectors.
MeCab2(Kudo et al, 2004) is used as a preprocess-
ing morphological analyzer through experiments.
In this paper, either Conditional Random
Fields(CRF)3(Lafferty et al, 2001) or Support Vec-
tor Machine(SVM)4(Cristianini and Shawe-Taylor,
2000) is employed to train a statistical NE chunker.
4.2 Experiment of IREX Corpus
Table 2 shows the results of extracting NEs of IREX
corpus, which are measured with F-measure through
5-fold cross validation. The columns of ?Proposed?
show the results with SF , and the ones of ?Base-
line? show the results without SF . The column of
?NExT? shows the result of using NExT(Masui et
2http://mecab.sourceforge.net/
3http://chasen.org/?taku/software/CRF++/
4http://chasen.org/?taku/software/
yamcha/
Table 2: NE Extraction Performance of IREX Corpus
Proposed Baseline NExT
CRF SVM CRF SVM
ARTIFACT 0.487 0.518 0.458 0.457 -
DATE 0.921 0.909 0.916 0.916 0.682
LOCATION 0.866 0.863 0.847 0.846 0.696
MONEY 0.951 0.610 0.937 0.937 0.895
ORGANIZATION 0.774 0.766 0.744 0.742 0.506
PERCENT 0.936 0.863 0.928 0.928 0.821
PERSON 0.825 0.842 0.788 0.787 0.672
TIME 0.901 0.903 0.902 0.901 0.800
Total 0.842 0.834 0.821 0.820 0.732
Table 3: Statistics of NE Types of NHK Corpus
NE Type Frequency (%)
DATE 755 (19%)
LOCATION 1465 (36%)
MONEY 124 (3%)
ORGANIZATION 1056 (26%)
PERCENT 55 (1%)
PERSON 516 (13%)
TIME 101 (2%)
Total 4072
al., 2002), an NE chunker based on hand-crafted
rules, without 5-fold cross validation.
As shown in Table 2, machine learning ap-
proaches with SF outperform ones without SF .
Please note that the result of SVM without SF and
the result of (Yamada et al, 2002) are comparable,
because our using feature set without SF is quite
similar to their feature set. This fact suggests that
SF is effective to achieve better performances than
the previous research. CRF with SF achieves better
performance than SVM with SF , although CRF and
SVM are comparable in the case without SF . NExT
achieves poorer performance than CRF and SVM.
4.3 Experiment of NHK Corpus
Nippon Housou Kyoukai (NHK) corpus is a set of
transcriptions of 30 broadcast news programs which
were broadcasted from June 1st 1996 to 12th. Ta-
ble 3 shows the statistics of NEs of NHK corpus
which were annotated by a graduate student except
127
Table 4: NE Extraction Performance of NHK Corpus
Proposed Baseline NExT
CRF SVM CRF SVM
DATE 0.630 0.595 0.571 0.569 0.523
LOCATION 0.837 0.825 0.797 0.811 0.741
MONEY 0.988 0.660 0.971 0.623 0.996
ORGANIZATION 0.662 0.636 0.601 0.598 0.612
PERCENT 0.538 0.430 0.539 0.435 0.254
PERSON 0.794 0.813 0.752 0.787 0.622
TIME 0.250 0.224 0.200 0.247 0.260
Total 0.746 0.719 0.702 0.697 0.615
Table 5: Extraction of Familiar/Unfamiliar NEs
Familiar Unfamiliar Other
CRF (Proposed) 0.789 0.654 0.621
CRF (Baseline) 0.757 0.556 0.614
for ARTIFACT in accordance with the NE definition
of IREX. Because all articles of IREX corpus had
been published earlier than broadcasting programs
of NHK corpus, we can suppose that NHK corpus
contains unfamiliar NEs like real input texts.
Table 4 shows the results of chunkers trained from
whole IREX corpus against NHK corpus. The meth-
ods with SF outperform the ones without SF . Fur-
thermore, performance improvements between the
ones with SF and the ones without SF are greater
than Table 2.
The performance of CRF with SF and one of
CRF without SF are compared in Table 5. The col-
umn ?Familiar? shows the results of extracting NEs
which consist of familiar morphemes, as well as the
column ?Unfamiliar? shows the results of extracting
NEs which consist of unfamiliar morphemes. The
column ?Other? shows the results of extracting NEs
which contain both familiar morpheme and unfa-
miliar one. These results indicate that SF is espe-
cially effective to extract NEs consisting of unfamil-
iar morphemes.
5 Concluding Remarks
This paper proposes a novel method to extract NEs
including unfamiliar morphemes which do not occur
or occur few times in a training corpus using a large
unannotated corpus. The experimental results show
that SF is effective for robust extracting NEs which
consist of unfamiliar morphemes. There are other
effective features of extracting NEs like N -best mor-
pheme sequences described in (Asahara and Mat-
sumoto, 2003) and features of surrounding phrases
described in (Nakano and Hirai, 2004). We will in-
vestigate incorporating SF and these features in the
near future.
References
Masayuki Asahara and Yuji Matsumoto. 2003. Japanese
named entity extraction with redundant morphological
analysis. In Proc. of HLT?NAACL ?03, pages 8?15.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proc. of the 19th COLING, pages 1?7.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision tree
learning. In Proc. of ACL ?01, pages 314?321.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to japanese
morphological analysis. In Proc. of EMNLP2004,
pages 230?237.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of ICML, pages 282?289.
Fumito Masui, Shinya Suzuki, and Junichi Fukumoto.
2002. Development of named entity extraction tool
NExT for text processing. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 176?179. (in Japanese).
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tions of Information Processing Society of Japan,
45(3):934?941, Mar. (in Japanese).
Nichigai Associates, editor. 2007. DCS Kikan-mei Jisho.
Nichigai Associates. (in Japanese).
Manabu Sassano and Takehito Utsuro. 2000. Named
entity chunking techniques in supervised learning for
japanese named entity recognition. In Proc. of the 18th
COLING, pages 705?711.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation: analysis of results.
In Proc. of the 18th COLING, pages 1106?1110.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. of the 9th EACL, pages 173?179.
Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi
Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000.
Named entity extraction based on a maximum entropy
model and transformation rules. Journal of Natural
Language Processing, 7(2):63?90, Apr. (in Japanese).
Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto.
2002. Japanese named entity extraction using support
vector machine. Transactions of Information Process-
ing Society of Japan, 43(1):44?53, Jan. (in Japanese).
128
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Indonesian-Japanese CLIR Using Only Limited Resource 
Ayu Purwarianti Masatoshi Tsuchiya Seiichi Nakagawa 
Department of Information and Computer Science, Toyohashi University of Technology 
ayu@slp.ics.tut.ac.jp tsuchiya@imc.tut.ac.jp nakagawa@slp.ics.tut.ac.jp
 
Abstract 
Our research aim here is to build a CLIR 
system that works for a language pair 
with poor resources where the source 
language (e.g. Indonesian) has limited 
language resources. Our Indonesian-
Japanese CLIR system employs the 
existing Japanese IR system, and we 
focus our research on the Indonesian-
Japanese query translation. There are two 
problems in our limited resource query 
translation: the OOV problem and the 
translation ambiguity. The OOV problem 
is handled using target language?s 
resources (English-Japanese dictionary 
and Japanese proper name dictionary). 
The translation ambiguity is handled 
using a Japanese monolingual corpus in 
our translation filtering. We select the 
final translation set using the mutual 
information score and the TF?IDF score. 
The result on NTCIR 3 (NII-NACSIS 
Test Collection for IR Systems) Web 
Retrieval Task shows that the translation 
method achieved a higher IR score than 
the transitive machine translation (using 
Kataku (Indonesian-English) and 
Babelfish/ Excite (English-Japanese) 
engine) result. The best result achieved 
about 49% of the monolingual retrieval. 
1 Introductions 
Due to the various languages used by different 
nations in the world, the CLIR has been an 
interesting research topic. For language pair with 
a rich language resource, the translation in the 
CLIR can be done with a bilingual dictionary - 
based direct translation, machine translation - or 
a parallel corpus - based translation. For a rare 
language pair, there is an attempt to use a pivot 
language (usually English), known as transitive 
translation, because there is no ample bilingual 
dictionary or machine translation system 
available. Some studies have been done in the 
field of transitive translation using bilingual 
dictionaries in the CLIR system such as 
[Ballesteros 2000; Gollins and Sanderson 2001]. 
Ballesteros [2000] translated Spanish queries 
into French with English as the interlingua. 
Ballesteros used Collins Spanish-English and 
English-French dictionaries. Gollins and 
Sanderson [2001] translated German queries into 
English using two pivot languages (Spanish and 
Dutch). Gollins used the Euro Wordnet as a data 
resource. To our knowledge, no CLIR is 
available with transitive translation for a source 
language with poor data resources such as 
Indonesian. 
Translation using a bilingual dictionary 
usually provides many translation alternatives 
only a few of which are appropriate. A transitive 
translation gives more translation alternatives 
than a direct translation. In order to select the 
most appropriate translation, a monolingual 
corpus can be used to select the best translation. 
Ballesteros and Croft [1998] used an English 
corpus to select some English translation based 
on Spanish-English translation and analyzed the 
co-occurrence frequencies to disambiguate 
phrase translations. The occurrence score is 
called the em score. Each set is ranked by em 
score, and the highest ranking set is taken as the 
final translation. Gao et al [2001] used a Chinese 
corpus to select the best English-Chinese 
translation set. It modified the EMMI weighting 
measure to calculate the term coherence score. 
Qu et al [2002] selected the best Spanish-
English and Chinese-English translation using an 
English corpus. The coherence score calculation 
was based on 1) web page count; 2) retrieval 
score; and 3) mutual information score. Mirna 
[2001] translated Indonesian into English and 
used an English monolingual corpus to select the 
best translation, employing a term similarity 
score based on the Dice similarity coefficient. 
Federico and Bertoldi [2002] combined the N-
best translation based on an HMM model of a 
query translation pair and relevant document 
probability of the input word to rank Italian 
documents retrieved by English query. Kishida 
and Kando [2004], used all terms to retrieve a 
document in order to obtain the best term 
combination and chose the most frequent term in 
1
each term translation set that appears in the top 
ranked document.  
In our poor resource language ? Japanese 
CLIR where we select Indonesian as the source 
language with limited resource, we calculate the 
mutual information score for each Japanese 
translation combination, using a Japanese 
monolingual corpus. After that, we select one 
translation combination with the highest TF?IDF 
score obtained from the Japanese IR engine. 
By our experiments on Indonesian-Japanese 
CLIR, we would like to show how easy it is to 
build a CLIR for a restricted language resource. 
By using only an Indonesian (as the source 
language) ? English dictionary we are able to 
retrieve Japanese documents with 41% of the 
performance achieved by the monolingual 
Japanese IR system.  
The rest of the paper is organized as follows: 
Section 2 presents an overview of an Indonesian 
query sentence; Section 3 discusses the method 
used for our Indonesian-Japanese CLIR; Section 
4 describes the comparison methods, and Section 
5 presents our experimental data and the results. 
2 Indonesian Query Sentence 
Indonesian is the official language in Indonesia. 
The language is understood by people in 
Indonesia, Malaysia, and Brunei. The Indonesian 
language family is Malayo-Polynesian 
(Austronesian), which extends across the islands 
of Southeast Asia and the Pacific [Wikipedia]. 
Indonesian is not related to either English or 
Japanese.  
Unlike other languages used in Indonesia 
such as Javanese, Sundanese and Balinese that 
use their own scripts, Indonesian uses the 
familiar Roman script. It uses only 26 letters as 
in the English alphabet. A transliteration module 
is not needed to translate an Indonesian sentence.  
Indonesian language does not have 
declensions or conjugations. The basic sentence 
order is Subject-Verb-Object. Verbs are not 
inflected for person or number. There are no 
tenses. Tense is denoted by the time adverb or 
some other tense indicators. The time adverb can 
be placed at the front or end of the sentence.  
A rather complex characteristic of the 
Indonesian language is that it is an agglutinave 
language. Words in Indonesian, usually verbs, 
can be attached by many prefixes or suffixes. 
Affixes used in the Indonesian language include 
[Kosasih 2003] me(n)-, ber-, di-, ter-, pe(n)-, per-, 
se-, ke-, -el-, -em-, -er-, -kan, -i, -nya, -an, me(n)-
kan, di-kan, memper-i, diper-i, ke-an, pe(n)-an, 
per-an, ber-an, ber-kan, se-nya. Words with 
different affixes might have uniform or different 
translation. Examples of different word 
translation are ?membaca? and ?pembaca?, 
which are translated into ?read? and ?reader?, 
respectively. Examples of same word translation 
are the words ?baca? and ?bacakan?, which are 
both translated into ?read? in English. Other 
examples are the words ?membaca? and ?dibaca?, 
which are translated into ?read? and ?being read?, 
respectively. By using a stop word elimination, 
the translation result of ?membaca? and ?dibaca? 
will give the same English translation, ?read?.  
An Indonesian dictionary usually contains 
words with affixes (that have different 
translations) and base words. For example, ?se-
nya? affix declares a ?most possible? pattern, 
such as ?sebanyak-banyaknya? (as much as 
possible), ?sesedikit-sedikitnya? (less possible), 
?sehitam-sehitamnya? (as black as possible). 
This affix can be attached to many adjectives 
with the same meaning pattern. Therefore, words 
with ?se-nya? affix are usually not included in an 
Indonesian dictionary.  
Query 1 
Saya ingin mengetahui siapa yang telah menjadi peraih 
Academy Awards beberapa generasi secara berturut-turut 
(I want to know who have been the recipients of successive 
generations of Academy Awards) 
Query 2 
Temukan buku-buku yang mengulas tentang novel yang 
ditulis oleh Miyabe Miyuki 
(Find book reviews of novels written by Miyabe Miyuki) 
Figure 1. Indonesian Query Examples 
Indonesian sentences usually consist of 
native (Indonesian) words and borrowed words. 
The two query examples in Figure 1 contain 
borrowed words. The first query contains 
?Academy Awards?, which is borrowed from the 
English language. The second query contains 
?Miyabe Miyuki?, which is transliterated from 
Japanese. To obtain a good translation, the query 
translation in our system must be able to translate 
those words, the Indonesian (native) words and 
the borrowed words. Problems that occur in a 
query translation here include OOV words and 
translation ambiguity. 
3 Indonesian - Japanese Query 
Translation System 
Indonesian-Japanese query translation is a 
component of the Indonesian-Japanese CLIR. 
The query translation system aims to translate an 
2
Indonesian sentence query 
Indonesian sentence query 
Indonesian query sentence(s) into a Japanese 
keyword list. The Japanese keyword list is then 
executed in the Japanese IR system to retrieve 
the relevant document. The schema of the 
Indonesian-Japanese query translation system 
can be seen in Figure 2.  
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Indonesian-Japanese Query 
Translation Schema 
The query translation system consists of 2 
subsystems: the keyword translation and 
translation candidate filtering. The keyword 
translation system seeks to obtain Japanese 
translation candidates for an Indonesian query 
sentence. The translation candidate filtering aims 
to select the most appropriate translation among 
all Japanese translation alternatives. The filtering 
result is used as the input for the Japanese IR 
system. The keyword translation and translation 
filtering process is described in the next section.  
3.1 Indonesian ? Japanese Key Word 
Translation Process  
The keyword translation system is a process used 
to translate Indonesian keywords into Japanese 
keywords. In this research, we do transitive 
translation using bilingual dictionaries as the 
proposed method. Other approaches such as 
direct translation or machine translation are 
employed for the comparison method. The 
schema of our keyword transitive translation 
using bilingual dictionaries is shown in Figure 3.  
The keyword translation process consists of 
native (Indonesian) word translation and 
borrowed word translation. The native words are 
translated using Indonesian-English and English-
Japanese dictionaries. Because the Indonesian 
tag parser is not available, we do the translation 
on a single word and consecutive pair of words 
that exist as a single term in the Indonesian-
English dictionary. As mentioned in the previous 
section dealing with affix combination in 
Indonesian language, not all words with the affix 
combination are recorded in an Indonesian 
dictionary. Therefore, if a search does not reveal 
the exact word, it will search for other words that 
are the basic term of the query word or have the 
same basic term. For example, the Indonesian 
word, ?munculnya? (come out), has a basic term 
?muncul? with the postfix ?-nya?.  Here, the term 
?munculnya? is not available in the dictionary. 
Therefore, the searching will take ?muncul? as 
the matching word with ?munculnya? and give 
the English translation for ?muncul? such as 
?come out? as its translation result. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Indonesian-Japanese Keyword 
Translation Schema 
In Indonesian, a noun phrase has the reverse 
word position of that in English. For example, 
?ozone hole? is translated as ?lubang ozon? 
(ozone=ozon, hole=lubang) in Indonesian. 
Therefore, in English translation, besides word-
by-word translation, we also search for the 
reversed English word pair as a single term in an 
English-Japanese dictionary. This strategy 
reduces the number of translation alternatives.  
The borrowed words are translated using an 
English-Japanese dictionary. The English-
Japanese dictionary is used because most of the 
borrowed words in our query translation system 
come from English. Examples of borrowed 
words in our query are ?Academy Awards?, 
?Aurora?, ?Tang?, ?baseball?, ?Plum?, ?taping?, 
and ?Kubrick?.  
Even though using an English-Japanese 
dictionary may help with accurate translation of 
words, but there are some proper names which 
can not be translated by this dictionary, such as 
?Miyabe Miyuki?, ?Miyazaki Hayao?, ?Honjo 
Manami?, etc. These proper names come from 
Japanese words which are romanized. In the 
Japanese language, these proper names might be 
written in one of the following scripts: kanji 
(Chinese character), hiragana, katakana and 
romaji (roman alphabet). One alphabet word can 
Indonesian ? Japanese 
Keyword Translation 
Candidates for Japanese Translation 
Translation Candidate Filtering 
Japanese Translation 
Indonesian ? English 
Bilingual Dictionary 
Japanese Keyword List 
English ? Japanese 
Bilingual Dictionary 
Translation
Candidates for Japanese Translation 
Japanese Morphological Analyzer (Chasen) 
Japanese Stop Word Elimination 
Indonesian words borrowed words 
? English ? Japanese Bilingual 
Dictionary Translation  
? Japanese Proper Name 
Dictionary Translation 
? Hiragana/Katakana 
Transliteration 
3
be transliterated into more than one Japanese 
words. For example, ?Miyabe? can be 
transliterated into ??, ??, ??? or ???. 
?? and ?? are written in kanji, ??? is 
written in hiragana, and ???  is written in 
katakana. For hiragana and katakana script, the 
borrowed word is translated by using a pair list 
between hiragana or katakana and its roman 
alphabet. These systems have a one-to-one 
correspondence for pronunciation (syllables or 
phonemes), something that can not be done for 
kanji. Therefore, to find the Japanese word in 
kanji corresponding to borrowed words, we use a 
Japanese proper name dictionary. Each term in 
the original proper name dictionary usually 
consists of two words, the first and last names. 
For a wider selection of translation candidates, 
we separate each term with two words into two 
terms. Even though the input word can not be 
found in the original proper name dictionary 
(family name and first name), a match may still 
be possible with the new proper name dictionary. 
Each of the above translation processes also 
involves the stop word elimination process, 
which aims to delete stop words or words that do 
not have significant meaning in the documents 
retrieved. The stop word elimination is done at 
every language step. First, Indonesian stop word 
elimination is applied to a Indonesian query 
sentence to obtain Indonesian keywords. Second, 
English stop word elimination is applied before 
English keywords are translated into Japanese 
keywords. Finally, Japanese stop word 
elimination is done after the Japanese keywords 
are morphologically analyzed by Chasen 
(http://chasen.naist.jp/hiki/ChaSen). 
The keyword transitive translation is used in 
2 systems: 1) transitive translation to translate all 
words in the query, and 2) transitive translation 
to translate OOV (Indonesian) words from direct 
translation using an Indonesian-Japanese 
dictionary. We label the first method as the 
transitive translation using bilingual dictionary 
and the second method as the combined 
translation (direct-transitive).  
3.2 Candidate Filtering Process 
The keyword transitive translation results in 
many more translation candidates than the direct 
translation result. The candidates have a 
translation ambiguity problem which will be 
handled by our Japanese translation candidate 
filtering process, which seeks to select the most 
appropriate translation among the Japanese 
translation candidates. In order to select the best 
Japanese translation, rather than choosing only 
the highest TF?IDF score or only the highest 
mutual information score among all sets, we 
combine both scores. The procedure is as 
follows: 
1. Calculate the mutual information score for 
all term sets. To avoid calculation of all term 
sets, we calculate the mutual information 
score iteratively. First we calculate it for 2 
translation candidate sets. Then we select 
100 sets with the highest mutual information 
score. These sets are joined with the 3rd 
translation candidate sets and the mutual 
information score is recalculated. This step is 
repeated until all translation candidate sets 
are covered.  
For a word set, the mutual information score 
is shown in Equation 1.  
I(t1?tn) =???
= +=
1
1 1
n
i
n
ij
I(ti;tj) 
=???
= +=
1
1 1 )(tlog).(tlog
)t,(tlogn
i
n
ij ji
ji
PP
P
 
 
 
 
 
(1)
I(t1?tn) means the mutual information for a 
set of words t1, t2,?tn. I(ti,tj) means the 
mutual information between two words (ti,tj). 
Here, for a zero frequency word, it will have 
no impact on the mutual information score of 
a word set.  
2. Select 5 sets with highest mutual information 
score and execute them into the IR engine in 
order to obtain the TF?IDF scores. The TF
?IDF score used here is the relevance score 
between the document and the query 
(Equation (2) from Fujii and Ishikawa 
[2003]). 
?
t  
??
??
?
?
??
??
?
?
+ tt,ii
t,i
DF
N.log
TF
avglen
DL
TF
 
 
 
(2)
 
TFt,i denotes the frequency of term t 
appearing in document i. DFt denotes the 
number of documents containing term t. N 
indicates the total number of documents in 
the collection. DLi denotes the length of 
document i (i.e., the number of characters 
contained in i), and avglen the average 
length of documents in the collection. 
3. Select the term set with the highest mutual 
information score among 3 top TF? IDF 
scores 
Figure 4 shows an example of the keyword 
selection process after completion of the 
4
keyword translation process. The translation 
combination and set rankings are for all words (4 
translation sets) in the query. Actually, the 
translation combinations and sets for the query 
example are also ranked for 2 and 3 translation 
sets. All resulting sets (ranked by its mutual 
information score) are executed in the IR system 
in order to obtain the TF?IDF score. The final 
query chosen is the one with the highest TF?
IDF score. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Illustration of Translation Filtering 
Method 
4 Compared Methods 
In the experiment, we compare our proposed 
method with other translation methods. Methods 
for comparing Indonesian-Japanese query 
translation include transitive translation using 
MT (machine translation), direct translation 
using existing Indonesian-Japanese dictionary, 
direct translation using a built-in Indonesian-
Japanese dictionary, transitive translation with 
English keyword selection based on mutual 
information taken from English corpus, and 
transitive translation with Japanese keyword 
selection based on mutual information only.  
4.1 Transitive Translation using Machine 
Translation 
The first method compared is a transitive 
translation using MT (machine translation). The 
Indonesian- Japanese transitive translation using 
MT has a schema similar to Indonesian-Japanese 
transitive translation using a bilingual dictionary.  
However, machine transitive translation does not 
use Indonesian-English and English-Japanese 
dictionaries. Indonesian queries are translated 
into English queries using an online Indonesian-
English MT (Kataku engine, 
http://www.toggletext.com). The English 
translation results are then translated into 
Japanese using 2 online MTs (Babelfish engine, 
http://www.altavista.com/babelfish and Excite 
engine, http://www.excite.co.jp/world). 
4.2 Direct Translation using Existing 
Indonesian-Japanese Bilingual 
Dictionary 
The second method compared is a direct 
translation using an Indonesian-Japanese 
dictionary. This direct translation also has a 
schema similar to the transitive translation using 
bilingual dictionary (Figure 2). The difference is 
that in translation of an Indonesian keyword, 
only 1 dictionary is used, rather than using 2 
dictionaries; in this case, an Indonesian-Japanese 
bilingual dictionary with a fewer words than the 
Indonesian-English and English-Japanese 
dictionaries.  
4.3 Direct Translation using Built-in 
Indonesian-Japanese Dictionary 
We also compare the transitive translation results 
with the direct translation using a built-in 
Indonesian-Japanese dictionary. The Indonesian-
Japanese dictionary is built from Indonesian-
English, English-Japanese and Japanese-English 
dictionaries using ?one-time inverse 
consultation? such as in Tanaka and Umemura 
[1998]. The matching process is similar with that 
in query translation. A Japanese translation is 
searched for an English translation (from every 
Indonesian term in Indonesian-English 
dictionary) as a term in the Japanese-English 
dictionary. If no match can be found, the English 
terms will be normalized by eliminating certain 
stop words (?to?, ?a?, ?an?, ?the?, ?to be?, ?kind 
of?). These normalized English terms will be 
checked again in the Japanese-English dictionary. 
For every Japanese translation, a ?one-time 
inverse consultation? is calculated. If the score is 
Query: 
Saya ingin mengetahui metode untuk belajar 
bagaimana menari salsa (= I wanted to know the 
method of studying how to dance the salsa)  
 
Keyword Selection: 
Metode (method), belajar (to learn, to study, to take 
up), menari (dance), salsa 
 
Japanese Keyword: 
Metode: ????,??,??,?? 
Belajar: ???,??,??,??,??,??,??,
??,???,??,???,??,??,?????
Menari: ??,???,?????,???,??,?
?,?? 
Salsa: ??? 
 
Translation Combination: 
(????,???,??,???) 
(??,???,??,???) 
(??,???,??,???), etc 
 
Rank sets based on Mutual Information Score: 
1. (??, ??,   ??,   ???) 
2. (??, ??,   ??,   ???) 
3. (??, ???, ???, ???) 
4. (??, ???, ???, ???) 
5. (??, ???, ??,   ???) 
 
Select query with highest TF?IDF score 
??, ???, ???, ???
5
more than one (for more than one English term), 
then it is accepted as an Indonesian-Japanese pair. 
If not, the WordNet is used to find its synonym 
and recalculate the ?one-time inverse 
consultation? score so as to compensate for the 
poor quality of Indonesian-English dictionary 
(29054 words). 
5 Experiments 
5.1 Experimental Data  
We measure our query translation performance 
by the IR score achieved by a CLIR system 
because CLIR is a real application and includes 
the performance of key word expansion. For this, 
we do not use word translation accuracy, as for 
the CLIR, since a one-to-one translation rate is 
not suitable, given there are so many 
semantically equivalent words.  
Our CLIR experiments are conducted on 
NTCIR-3 Web Retrieval Task data (100 Gb 
Japanese documents), in which the Japanese 
queries and translated English queries were 
prepared. The Indonesian queries (47 queries) 
are manually translated from English queries. 
The 47 queries contain 528 Indonesian words 
(225 are not stop words), 35 English borrowed 
words, and 16 transliterated Japanese words 
(proper nouns). The IR system (Fujii and 
Ishikawa [2003]) is borrowed from Atsushi Fujii 
(Tsukuba University). External resources used in 
the query translation are listed in Table 1.  
Table 1. External Resource List 
Resource Description 
KEBI Indonesian-English 
dictionary, 29,054 words  
Eijirou English-Japanese dictionary, 
556,237 words 
Kmsmini2000 Indonesian-Japanese 
dictionary, 14,823 words 
ToggleText Kataku Indonesian-English machine 
translation 
Excite  English-Japanese machine 
translation 
Babelfish English-Japanese machine 
translation 
[Fox, 1989] and 
[Zu et al, 2004] 
English stop words (are also 
translated into Indonesian 
stop words) 
Chasen Japanese morphological 
analyzer 
Jinmei Jisho Japanese proper name 
dictionary, 61,629 words 
Mainichi Shinbun 
& Online Yomiuri 
Shinbun 
Japanese newspaper corpus 
5.2 Experimental Result 
In the experiments, we compare the IR score of 
each translation method. The IR scores shown in 
this section are in Mean Average Precision 
(MAP) scores. The evaluation metrics is referred 
to [Fujii and Ishikawa 2003b]. Each query group 
has 4 MAP scores: RL (highly relevant 
document as correct answer with hyperlink 
information used), RC (highly relevant document 
as correct answer), PL (partially relevant 
document as correct answer with hyperlink 
information used), and PC (partially relevant 
document as correct answer). The documents 
hyperlinked from retrieved documents are used 
for relevance assessment. 
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
jp iej-mx iej-mb ijn ij iej ij-iej
PC PL RC RL
Figure 5. Baseline Indonesian-Japanese CLIR 
Figure 5 shows the IR scores of queries 
translated using basic translation methods such 
as the bilingual dictionary or machine translation, 
without any enhanced process. The labels used in 
Figure 5 are:  
? jp (monolingual translation), where ?jp? 
denotes Japanese query 
? iej (transitive translation using bilingual 
dictionary), where ?i?, ?e?, ?j? denote 
Indonesian, English and Japanese, respectively,  
? iej-mx (transitive machine translation using 
Kataku and Excite engines), where ?m? 
denotes machine translation,  
? iej-mb (transitive machine translation using 
Kataku and Babelfish engines), 
? ijn (direct translation using the built in 
Indonesian-Japanese dictionary), 
? ij (direct translation using Indonesian-Japanese 
dictionary), 
? ij-iej (combination of direct (ij) and transitive 
(iej) translation using bilingual dictionary). 
The highest CLIR score in the baseline 
translation (without the enhancement process) 
achieves 30% of the performance achieved by 
the monolingual IR (jp).  
IR results in Figure 6 shows that OOV 
translation does improve the retrieval result. 
Here, our proposed methods (iej and ij-iej) 
achieve lower score than the comparison 
methods. 
6
0
0.01
0.02
0.03
0.04
0.05
iej-mx iej-mb ijn ij iej ij-iej
PC PL RC RL
 
Figure 6. Indonesian-Japanese CLIR with OOV 
Translation 
0 0.02 0.04 0.06 0.08
ij-iej-IR-60
ij-iej-IR-50
ij-iej-IR-40
ij-iej-IR-30
ij-iej-IR-20
ij-iej-IR-10
ij-iej-IR-5
ij-iej-I-10
ij-iej-I-5
ij-iej-I-3
ij-iej-I5
ij-iej-I4
ij-iej-I3
ij-iej-I2
ij-iej-I1
iej-IR-30
iej-IR-20
iej-IR-10
iej-IR-5
iej-I-10
iej-I-5
iej-I-3
iej-I5
iej-I4
iej-I3
iej-I2
iej-I1
ij-IR-30
ij-IR-20
ij-IR-10
ij-IR-5
ij-I-10
ij-I-5
ij-I-3
ij-I5
ij-I4
ij-I3
ij-I2
ij-I1
ijn-IR
ijn-I-10
ijn-I-5
ijn-I-3
ijn-I5
ijn-I4
ijn-I3
ijn-I2
ijn-I1
iej-mx-IR
iej-mx-I-
iej-mx-I-5
iej-mx-I-3
iej-mx-I5
iej-mx-I4
iej-mx-I3
iej-mx-I2
iej-mx-I1
iej-mb-IR
iej-mb-I-
iej-mb-I-5
iej-mb-I-3
iej-mb-I5
iej-mb-I4
iej-mb-I3
iej-mb-I2
iej-mb-I1
RL
RC
PL
PC
 
Figure 7. Indonesian-Japanese CLIR with OOV 
Translation and Keyword Filtering 
Figure 7 shows the MAP score on the 
proposed Indonesian-Japanese CLIR. The 
keyword selection description of each query 
label follows: 
? In (n = 1 .. 5): one query candidate based on 
mutual information score; example: I2 means 
the 2nd ranked query by its mutual information 
score. 
? I-n (n = 3,5,10): combination of the n-best 
query candidates based on mutual information 
score; example: iej-3 (disjuncture of the 3-best 
mutual information score candidates).  
? IR: the 1-best query candidate based on 
combination of mutual information score and 
TF? IDF engine score. X in IR-X shows 
number of combinations. For example, IR-5 
means the highest TF? IDF score among 5 
highest mutual information score sets.  
Figure 7 shows that the proposed filtering 
method yields higher IR score on the transitive 
translation. We achieve 41% of the performance 
achieved by the monolingual IR. The proposed 
transitive translation (iej-IR-10) improves the IR 
score of the baseline method of transitive 
translation (iej) from 0.0156 to 0.0512. The t-test 
shows that iej-IR-10 significantly increases the 
baseline method (iej) with a 97% confidence 
level, T(68) = 1.91, p<0.03. t-test also shows that, 
compared to other baseline systems, the 
proposed transitive translation (iej-IR-10) can 
significantly increase the IR score at 85% (T(84) 
= 1.04, p<0.15), 69% (T(86) = 0.49, p<0.31), 
91% (T(83) = 1.35, p<0.09), and 93% (T(70) = 
1.49, p<0.07) confidence level for iej-mb, iej-mx, 
ij and ij-iej, respectively. Another proposed 
method, a combination of direct and transitive 
translation (ij-iej), achieved the best IR score 
among all the translation methods. The proposed 
combination translation method (ijiej-IR-30) 
improves the  IR score of the baseline 
combination translation (ij-iej) from 0.025 to 
0.0629. The t-test showed that the proposed 
combination translation improves IR score of the 
baseline ij-iej with a 98% confidence level, T(69) 
= 2.09, p<0.02. Compared to other baseline 
systems, t-test shows that the proposed 
combination translation method (ijiej-IR-30) 
improves the IR score at 95% (T(83) = 1.66, 
p<0.05), 86% (T(85) = 1.087, p<0.14), 97%, 
(T(82) = 1.91, p<0.03) and 99% (T(67) = 2.38, 
p<0.005) confidence level for iej-mb, iej-mx, ij 
and iej, respectively. 
6 Conclusions 
We present a translation method on CLIR that 
is suitable for language pair with poor resources, 
where the source language has a limited data 
resource. Compared to other translation methods 
7
such as transitive translation using machine 
translation and direct translation using bilingual 
dictionary (the source-target dictionary is a poor 
bilingual dictionary), our transitive translation 
and the combined translation (direct translation 
and transitive translation) achieve higher IR 
scores. The transitive translation achieves a 41% 
performance of the monolingual IR and the 
combined translation achieves a 49% 
performance of the monolingual IR.  
The two important methods in our transitive 
translation are the borrowed word translation and 
the keyword selection method. The borrowed 
word approach can reduce the number of OOV 
from 50 words to 5 words using a pivot-target 
(English-Japanese) bilingual dictionary and 
target (Japanese) proper name dictionary. The 
keyword selection using the combination of 
mutual information score and TF?IDF score has 
improved the baseline transitive translation. The 
other important method, the combination method 
between transitive and direct translation using 
bilingual dictionaries also improves the CLIR 
performance.  
Acknowledgements 
We would like to give our appreciation to Dr. 
Atsushi Fujii (Tsukuba University) to allow us to 
use the IR Engine in our research. This work was 
partially supported by The 21st Century COE 
Program ?Intelligent Human Sensing? 
References 
Adriani, Mirna. 2000. Using statistical term similarity 
for sense disambiguation in cross language 
information retrieval. Information Retrieval: 67-78. 
Agency for The Assessment and Application of 
Technology: KEBI (Kamus Elektronik Bahasa 
Indonesia). http://nlp.aia.bppt.go.id/kebi/. Last 
access: February 2004. 
Babelfish English-Japanese Online Machine 
Translation. http://www.altavista.com/babelfish/. 
Last access:  April 2004. 
Ballesteros, Lisa A. and W. Bruce Croft. 1998. 
Resolving ambiguity for cross-language retrieval. 
ACM Sigir. 
Ballesteros, Lisa A. 2000. Cross Language Retrieval 
via Transitive Translation. Advances in 
Information Retrieval: 203-230. Kluwer Academic 
Publisher. 
Chasen. http://chasen.naist.jp/hiki/ChaSen/. Last 
access: February 2004. 
Chen, Kuang-hua, et,al. 2003. Overview of CLIR Task 
at the Third NTCIR Workshop. Proceedings of the 
Third NTCIR Workshop. 
Excite English-Japanese Online Machine Translation. 
http://www.excite.co.jp/world/. Last access: April 
2004. 
Federico, M. and N. Bertoldi. 2002. Statistical cross 
language information retrieval using n-best query 
translations. Proc. Of 25th International ACM 
Sigir. 
Fox, Christopher. 1989. A stop list for general text. 
ACM Sigir, Vol 24:19-21, Issue 2 Fall 89/Winter 
90. 
Fujii, Atsushi and Tetsuya Ishikawa. 2003. NTCIR-3 
cross-language IR experiments at ULIS. Proc. Of 
the Third NTCIR Workshop. 
Fujii, Atsushi and Katunobu Itou. 2003. Building a 
test collection for speech driven web retrieval. 
Proceedings of the 8th European Conference on 
Speech Communication and Technology. 
Gao, Jianfeng,  et, al. 2001. Improving query 
translation for cross-language information 
retrieval using statistical model. Proc. Sigir.  
Gollins, Tim and Mark Sanderson. 2001. Improving 
cross language information retrieval with 
triangulated translation. Proc. Sigir.  
ToggleText, Kataku Automatic Translation System. 
http://www.toggletext.com/kataku_trial.php. Last 
access: May 2004. 
Information Retrieval Resources for Bahasia 
Indonesia. Informatics Institute, University of 
Amsterdam. http://ilps.science.uva.nl/Resources/. 
Last access: Jan 2005. 
Kishida, Kazuaki and Noriko Kando. 2004. Two-stage 
refinement of query translation in a pivot language 
approach to cross-lingual information retrieval: 
An experiment at CLEF 2003. CLEF 2003, LNCS 
3237: 253-262.  
Kosasih, E. 2003. Kompetensi Ketatabahasaan dan 
Kesusastraan, Cermat Berbahasa Indonesia. 
Yrama Widya. 
Mainichi Shinbun CD-Rom data sets 1993-1995, 
Nichigai Associates Co., 1994-1996. 
Michibata, H., ed.: Eijirou, Alc. Last access:2002. 
Qu, Yan and G. Grefenstette, D. A. Evans. 2002. 
Resolving translation ambiguity using monolingual 
corpora. Advanced in Cross-Language Information 
Retrieval, vol. 2785 of LNCS: 223-241. Springer 
Verlag. 
Sanggar Bahasa Indonesia Proyek: Kmsmini2000. 
http://ml.ryu.titech.ac.jp/~indonesia/tokodai/dokum
en/ kamusjpina.pdf. Last access: May 2004. 
Tanaka, Kumiko and Kyoji Umemura. Construction 
of a bilingual dictionary intermediated by a third 
language. COLING 1994, pages 297-303, Kyoto. 
Wikipedia on Indonesian Language. 
http://en.wikipedia.org/wiki/ Indonesian_language. 
Last access: May 2005. 
WordNet. http://wordnet.princeton.edu/. Last access: 
February 2004. 
Zu, Guowei, et, al. 2004. Automatic Text 
Classification Techniques. IEEJ Trans EIS, Vol. 
124, No. 3. 
8
Chunking Japanese Compound Functional Expressions
by Machine Learning
Masatoshi Tsuchiya? and Takao Shime? and Toshihiro Takagi?
Takehito Utsuro?? and Kiyotaka Uchimoto?? and Suguru Matsuyoshi?
Satoshi Sato?? and Seiichi Nakagawa??
?Computer Center / ??Department of Information and Computer Sciences,
Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN
??Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
??National Institute of Information and Communications Technology,
3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
The Japanese language has various types
of compound functional expressions,
which are very important for recogniz-
ing the syntactic structures of Japanese
sentences and for understanding their
semantic contents. In this paper, we
formalize the task of identifying Japanese
compound functional expressions in a
text as a chunking problem. We apply a
machine learning technique to this task,
where we employ that of Support Vector
Machines (SVMs). We show that the pro-
posed method significantly outperforms
existing Japanese text processing tools.
1 Introduction
As in the case of other languages, the Japanese
language has various types of functional words
such as post-positional particles and auxiliary
verbs. In addition to those functional words,
the Japanese language has much more compound
functional expressions which consist of more than
one words including both content words and func-
tional words. Those single functional words as
well as compound functional expressions are very
important for recognizing the syntactic structures
of Japanese sentences and for understanding their
semantic contents. Recognition and understanding
of them are also very important for various kinds
of NLP applications such as dialogue systems, ma-
chine translation, and question answering. How-
ever, recognition and semantic interpretation of
compound functional expressions are especially
difficult because it often happens that one com-
pound expression may have both a literal (in other
words, compositional) content word usage and
a non-literal (in other words, non-compositional)
functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni)???
(tsuite)?, which consists of a post-positional par-
ticle ?? (ni)?, and a conjugated form ????
(tsuite)? of a verb ??? (tsuku)?. In the sentence
(A), the compound expression functions as a case-
marking particle and has a non-compositional
functional meaning ?about?. On the other hand,
in the sentence (B), the expression simply corre-
sponds to a literal concatenation of the usages of
the constituents: the post-positional particle ??
(ni)? and the verb ???? (tsuite)?, and has a
content word meaning ?follow?. Therefore, when
considering machine translation of those Japanese
sentences into English, it is necessary to precisely
judge the usage of the compound expression ??
(ni)??? (tsuite)?, as shown in the English trans-
lation of the two sentences in Table 1.
There exist widely-used Japanese text process-
ing tools, i.e., pairs of a morphological analysis
tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. How-
ever, they process those compound expressions
only partially, in that their morphological analy-
sis dictionaries list only limited number of com-
pound expressions. Furthermore, even if certain
expressions are listed in a morphological analysis
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman-e.html
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
25
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 2: Classification of Functional Expressions based on Grammatical Function
# of major # of
Grammatical Function Type expressions variants Example
subsequent to predicate 36 67 ????
post-positional / modifying predicate (to-naru-to)
particle subsequent to nominal 45 121 ?????
type / modifying predicate (ni-kakete-ha)
subsequent to predicate, nominal 2 3 ???
/ modifying nominal (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
dictionary, those existing tools often fail in resolv-
ing the ambiguities of their usages, such as those
in Table 1. This is mainly because the frame-
work of those existing tools is not designed so as
to resolve such ambiguities of compound (possi-
bly functional) expressions by carefully consider-
ing the context of those expressions.
Considering such a situation, it is necessary
to develop a tool which properly recognizes and
semantically interprets Japanese compound func-
tional expressions. In this paper, we apply a ma-
chine learning technique to the task of identify-
ing Japanese compound functional expressions in
a text. We formalize this identification task as a
chunking problem. We employ the technique of
Support Vector Machines (SVMs) (Vapnik, 1998)
as the machine learning technique, which has been
successfully applied to various natural language
processing tasks including chunking tasks such
as phrase chunking (Kudo and Matsumoto, 2001)
and named entity chunking (Mayfield et al, 2003).
In the preliminary experimental evaluation, we fo-
cus on 52 expressions that have balanced distribu-
tion of their usages in the newspaper text corpus
and are among the most difficult ones in terms of
their identification in a text. We show that the pro-
posed method significantly outperforms existing
Japanese text processing tools as well as another
tool based on hand-crafted rules. We further show
that, in the proposed SVMs based framework, it is
sufficient to collect and manually annotate about
50 training examples per expression.
2 Japanese Compound Functional
Expressions and their Example
Database
2.1 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) examine
450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their exam-
ple sentences. Compared with those two collec-
tions, Gendaigo Hukugouji Youreishu (National
Language Research Institute, 2001) (henceforth,
denoted as GHY) concentrates on 125 major func-
tional expressions which have non-compositional
usages, as well as their variants5 (337 expressions
in total), and collects example sentences of those
expressions. As a first step of developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants. In this paper, we take
an approach of regarding each of those variants as
a fixed expression, rather than a semi-fixed expres-
sion or a syntactically-flexible expression (Sag et
al., 2002). Then, we focus on evaluating the ef-
fectiveness of straightforwardly applying a stan-
5For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) in-
sertion/deletion/alternation of certain particles, ii) alternation
of synonymous words, iii) normal/honorific/conversational
forms, iv) base/adnominal/negative forms.
26
Table 3: Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ???? ????????????? ????
??????
functional
(to-naru-to) (The situation is serious if it is not effec-
tive against this disease.)
(???? (to-naru-to) = if)
(2) ???? ???????????????
???? ????????
content
(to-naru-to) (They think that it will become a require-
ment for him to be the president.)
(????? (to-naru-to)
= that (something) becomes ?)
(3) ????? ???????? ????? ????
??????????
functional
(ni-kakete-ha) (He has a great talent for earning money.) (?????? (ni-kakete-ha)
= for ?)
(4) ????? ???? ????? ???? content
(ni-kakete-ha) (I do not worry about it.)
( (??)??????
((?)-wo-ki-ni-kakete-ha)
= worry about ?)
(5) ??? ??????? ??? ??????
??
functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ????????????? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this
discussion.)
(???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2, according to their grammat-
ical functions, those 337 expressions in total
are roughly classified into post-positional particle
type, and auxiliary verb type. Functional expres-
sions of post-positional particle type are further
classified into three subtypes: i) those subsequent
to a predicate and modifying a predicate, which
mainly function as conjunctive particles and are
used for constructing subordinate clauses, ii) those
subsequent to a nominal, and modifying a predi-
cate, which mainly function as case-marking parti-
cles, iii) those subsequent to a nominal, and modi-
fying a nominal, which mainly function as adnom-
inal particles and are used for constructing adnom-
inal clauses. For each of those types, Table 2 also
shows the number of major expressions as well as
that of their variants listed in GHY, and an exam-
ple expression. Furthermore, Table 3 gives exam-
ple sentences of those example expressions as well
as the description of their usages.
2.2 Issues on Identifying Compound
Functional Expressions in a Text
The task of identifying Japanese compound func-
tional expressions roughly consists of detecting
candidates of compound functional expressions in
a text and of judging the usages of those can-
didate expressions. The class of Japanese com-
pound functional expressions can be regarded as
closed and their number is at most a few thousand.
27
Table 4: Examples of Detecting more than one Candidate Expression
Expression Example sentence (English translation) Usage
(9) ??? ????? ??? ???????? functional
(to-iu) (That?s why a match is not so easy.) (NP1??? (to-iu)NP2
= NP
2
called as NP
1
)
(10) ?????? ??? ?????? ???????? functional
(to-iu-mono-no) (Although he won, the score is bad.)
(???????
(to-iu-mono-no)
= although ?)
Therefore, it is easy to enumerate all the com-
pound functional expressions and their morpheme
sequences. Then, in the process of detecting can-
didates of compound functional expressions in a
text, the text are matched against the morpheme
sequences of the compound functional expressions
considered.
Here, most of the 125 major functional expres-
sions we consider in this paper are compound ex-
pressions which consist of one or more content
words as well as functional words. As we intro-
duced with the examples of Table 1, it is often
the case that they have both a compositional con-
tent word usage as well as a non-compositional
functional usage. For example, in Table 3, the
expression ????? (to-naru-to)? in the sen-
tence (2) has the meaning ? that (something) be-
comes ??, which corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ???, the verb ????, and the
post-positional particle ???, and can be regarded
as a content word usage. On the other hand, in
the case of the sentence (1), the expression ???
?? (to-naru-to)? has a non-compositional func-
tional meaning ?if?. Based on this discussion, we
classify the usages of those expressions into two
classes: functional and content. Here, functional
usages include both non-compositional and com-
positional functional usages, although most of the
functional usages of those 125 major expressions
can be regarded as non-compositional. On the
other hand, content usages include compositional
content word usages only.
More practically, in the process of detecting
candidates of compound functional expressions in
a text, it can happen that more than one can-
didate expression is detected. For example, in
Table 4, both of the candidate compound func-
tional expressions ???? (to-iu)? and ????
??? (to-iu-mono-no)? are detected in the sen-
tence (9). This is because the sequence of the two
morphemes ?? (to)? and ??? (iu)? constituting
the candidate expression ???? (to-iu)? is a sub-
sequence of the four morphemes constituting the
candidate expression ??????? (to-iu-mono-
no)? as below:
Morpheme sequence
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression??? (to-iu)
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression?????? (to-iu-mono-no)
? (to) ?? (iu) ?? (mono) ? (no)
This is also the case with the sentence (10).
Here, however, as indicated in Table 4, the sen-
tence (9) is an example of the functional usage of
the compound functional expression ???? (to-
iu)?, where the sequence of the two morphemes ?
? (to)? and ??? (iu)? should be identified and
chunked into a compound functional expression.
On the other hand, the sentence (10) is an ex-
ample of the functional usage of the compound
functional expression ??????? (to-iu-mono-
no)?, where the sequence of the four morphemes ?
? (to)?, ??? (iu)?, ??? (mono)?, and ?? (no)?
should be identified and chunked into a compound
functional expression. Actually, in the result of
our preliminary corpus study, at least in about 20%
of the occurrences of Japanese compound func-
tional expressions, more than one candidate ex-
pression can be detected. This result indicates that
it is necessary to consider more than one candidate
expression in the task of identifying a Japanese
compound functional expression, and also in the
task of classifying the functional/content usage of
a candidate expression. Thus, in this paper, based
on this observation, we formalize the task of iden-
tifying Japanese compound functional expressions
as a chunking problem, rather than a classification
problem.
28
Table 5: Number of Sentences collected from
1995 Mainichi Newspaper Texts (for 337 Expres-
sions)
# of expressions
50 ? # of sentences 187 (55%)
0 < # of sentences < 50 117 (35%)
# of sentences = 0 33 (10%)
2.3 Developing an Example Database
We developed an example database of Japanese
compound functional expressions, which is used
for training/testing a chunker of Japanese com-
pound functional expressions (Tsuchiya et al,
2005). The corpus from which we collect example
sentences is 1995 Mainichi newspaper text corpus
(1,294,794 sentences, 47,355,330 bytes). For each
of the 337 expressions, 50 sentences are collected
and chunk labels are annotated according to the
following procedure.
1. The expression is morphologically analyzed
by ChaSen, and its morpheme sequence6 is
obtained.
2. The corpus is morphologically analyzed by
ChaSen, and 50 sentences which include the
morpheme sequence of the expression are
collected.
3. For each sentence, every occurrence of the
337 expressions is annotated with one of the
usages functional/content by an annotator7.
Table 5 classifies the 337 expressions accord-
ing to the number of sentences collected from the
1995 Mainichi newspaper text corpus. For more
than half of the 337 expressions, more than 50 sen-
tences are collected, although about 10% of the
377 expressions do not appear in the whole cor-
pus. Out of those 187 expressions with more than
50 sentences, 52 are those with balanced distribu-
tion of the functional/content usages in the news-
paper text corpus. Those 52 expressions can be re-
garded as among the most difficult ones in the task
of identifying and classifying functional/content
6For those expressions whose constituent has conjugation
and the conjugated form also has the same usage as the ex-
pression with the original form, the morpheme sequence is
expanded so that the expanded morpheme sequences include
those with conjugated forms.
7For the most frequent 184 expressions, on the average,
the agreement rate between two human annotators is 0.93 and
the Kappa value is 0.73, which means allowing tentative con-
clusions to be drawn (Carletta, 1996; Ng et al, 1999). For
65% of the 184 expressions, the Kappa value is above 0.8,
which means good reliability.
usages. Thus, this paper focuses on those 52 ex-
pressions in the training/testing of chunking com-
pound functional expressions. We extract 2,600
sentences (= 52 expressions ? 50 sentences) from
the whole example database and use them for
training/testing the chunker. The number of the
morphemes for the 2,600 sentences is 92,899. We
ignore the chunk labels for the expressions other
than the 52 expressions, resulting in 2,482/701
chunk labels for the functional/content usages, re-
spectively.
3 Chunking Japanese Compound
Functional Expressions with SVMs
3.1 Support Vector Machines
The principle idea of SVMs is to find a separate
hyperplane that maximizes the margin between
two classes (Vapnik, 1998). If the classes are not
separated by a hyperplane in the original input
space, the samples are transformed in a higher di-
mensional features space.
Giving x is the context (a set of features) of
an input example; xi and yi(i = 1, ..., l, xi ?
Rn, yi?{1,?1}) indicate the context of the train-
ing data and its category, respectively; The deci-
sion function f in SVM framework is defined as:
f(x) = sgn
( l
?
i=1
?iyiK(xi,x) + b
)
(1)
where K is a kernel function, b ? R is a thresh-
old, and ?i are weights. Besides, the weights ?i
satisfy the following constraints:
0 ? ?i ? C (i = 1, ..., l) (2)
?l
i=1 ?iyi = 0 (3)
where C is a misclassification cost. The xi with
non-zero ?i are called support vectors. To train
an SVM is to find the ?i and the b by solving the
optimization problem; maximizing the following
under the constraints of (2) and (3):
L(?) =
l
?
i=1
?i?
1
2
l
?
i,j=1
?i?jyiyjK(x
i
,x
j
) (4)
The kernel function K is used to transform the
samples in a higher dimensional features space.
Among many kinds of kernel functions available,
we focus on the d-th polynomial kernel:
K(x,y) = (x ? y + 1)d (5)
29
Through experimental evaluation on chunking
Japanese compound functional expressions, we
compared polynomial kernels with d = 1, 2, and
3. Kernels with d = 2 and 3 perform best, while
the kernel with d = 3 requires much more compu-
tational cost than that with d = 2. Thus, through-
out the paper, we show results with the quadratic
kernel (d = 2).
3.2 Chunking with SVMs
This section describes details of formalizing the
chunking task using SVMs. In this paper, we use
an SVMs-based chunking tool YamCha8 (Kudo
and Matsumoto, 2001). In the SVMs-based
chunking framework, SVMs are used as classi-
fiers for assigning labels for representing chunks
to each token. In our task of chunking Japanese
compound functional expressions, each sentence
is represented as a sequence of morphemes, where
a morpheme is regarded as a token.
3.2.1 Chunk Representation
For representing proper chunks, we employ
IOB2 representation, one of those which have
been studied well in various chunking tasks of nat-
ural language processing (Tjong Kim Sang, 1999;
Kudo and Matsumoto, 2001). This method uses
the following set of three labels for representing
proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
As we described in section 2.2, given a candi-
date expression, we classify the usages of the ex-
pression into two classes: functional and content.
Accordingly, we distinguish the chunks of the two
types: the functional type chunk and the content
type chunk. In total, we have the following five la-
bels for representing those chunks: B-functional,
I-functional, B-content, I-content, and O. Ta-
ble 6 gives examples of those chunk labels rep-
resenting chunks.
Finally, as for exending SVMs to multi-class
classifiers, we experimentally compare the pair-
wise method and the one vs. rest method, where
the pairwise method slightly outperformed the one
vs. rest method. Throughout the paper, we show
results with the pairwise method.
8http://chasen.org/?taku/software/
yamcha/
3.2.2 Features
For the feature sets for training/testing of
SVMs, we use the information available in the sur-
rounding context, such as the morphemes, their
parts-of-speech tags, as well as the chunk labels.
More precisely, suppose that we identify the chunk
label ci for the i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, mi is the morpheme appearing at i-th po-
sition, Fi is the feature set at i-th position, and ci
is the chunk label for i-th morpheme. Roughly
speaking, when identifying the chunk label ci for
the i-th morpheme, we use the feature sets Fi?2,
Fi?1, Fi, Fi+1, Fi+2 at the positions i ? 2, i ? 1,
i, i + 1, i + 2, as well as the preceding two chunk
labels ci?2 and ci?1.
The detailed definition of the feature set Fi at i-
th position is given below. The feature set Fi is de-
fined as a tuple of the morpheme feature MF (mi)
of the i-th morpheme mi, the chunk candidate fea-
ture CF (i) at i-th position, and the chunk context
feature OF (i) at i-th position.
Fi = ? MF (mi), CF (i), OF (i) ?
The morpheme feature MF (mi) consists of the
lexical form, part-of-speech, conjugation type and
form, base form, and pronunciation of mi.
The chunk candidate feature CF (i) and the
chunk context feature OF (i) are defined consid-
ering the candidate compound functional expres-
sion, which is a sequence of morphemes includ-
ing the morpheme mi at the current position i. As
we described in section 2, the class of Japanese
compound functional expressions can be regarded
as closed and their number is at most a few thou-
sand. Therefore, it is easy to enumerate all the
compound functional expressions and their mor-
pheme sequences. Chunk labels other than O
should be assigned to a morpheme only when it
constitutes at least one of those enumerated com-
pound functional expressions. Suppose that a se-
quence of morphemes mj . . . mi . . . mk including
mi at the current position i constitutes a candidate
functional expression E as below:
m
j?2
m
j?1
m
j
. . . m
i
. . . m
k
m
k+1
m
k+2
candidate E of
a compound
functional expression
where the morphemes mj?2, mj?1, mk+1, and
mk+2 are at immediate left/right contexts of E.
Then, the chunk candidate feature CF (i) at i-th
position is defined as a tuple of the number of mor-
phemes constituting E and the position of mi in
E. The chunk context feature OF (i) at i-th posi-
tion is defined as a tuple of the morpheme features
30
Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features
(a) Sentence (7) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
?? (giron) (discussion) O ? ?
? (ga) (NOM) O ? ?
???(owatt) (finish) O ? ?
?? (tara) (after) O ? ?
?? (kyuukei) (break) O ? ?
? (shi) (have) O ? ?
? (te) (may) B-functional ?2, 1? ? MF (?? (kyuukei)), ?, MF (? (shi)), ?,
?? (ii) I-functional ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
(b) Sentence (8) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
??? (bag) (discussion) O ? ?
? (ha) (TOP) O ? ?
??? (ookiku) (big) O ? ?
? (te) (because) B-content ?2, 1? ? MF (? (ha)), ?, MF (??? (ookiku)), ?,
?? (ii) (nice) I-content ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
as well as the chunk candidate features at immedi-
ate left/right contexts of E.
CF (i) = ? length of E, position of m
i
in E ?
OF (i) = ? MF (m
j?2
), CF (j ? 2),
MF (m
j?1
), CF (j ? 1),
MF (m
k+1
), CF (k + 1),
MF (m
k+2
), CF (k + 2) ?
Table 6 gives examples of chunk candidate fea-
tures and chunk context features
It can happen that the morpheme at the cur-
rent position i constitutes more than one candidate
compound functional expression. For example,
in the example below, the morpheme sequences
mi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-
tute candidate expressions E
1
, E
2
, and E
3
, respec-
tively.
Morpheme sequence m
i?1
m
i
m
i+1
m
i+2
Candidate E
1
m
i?1
m
i
m
i+1
Candidate E
2
m
i?1
m
i
Candidate E
3
m
i
m
i+1
m
i+2
In such cases, we prefer the one starting with the
leftmost morpheme. If more than one candidate
expression starts with the leftmost morpheme, we
prefer the longest one. In the example above, we
prefer the candidate E
1
and construct the chunk
candidate features and chunk context features con-
sidering E
1
only.
4 Experimental Evaluation
The detail of the data set we use in the experimen-
tal evaluation was presented in section 2.3. As we
show in Table 7, performance of our SVMs-based
chunkers as well as several baselines including ex-
isting Japanese text processing tools is evaluated
in terms of precision/recall/F?=1 of identifying
functional chunks. Performance is evaluated also
in terms of accuracy of classifying detected can-
didate expressions into functional/content chunks.
Among those baselines, ?majority ( = functional)?
always assigns functional usage to the detected
candidate expressions. ?Hand-crafted rules? are
manually created 145 rules each of which has con-
ditions on morphemes constituting a compound
functional expression as well as those at immedi-
ate left/right contexts. Performance of our SVMs-
based chunkers is measured through 10-fold cross
validation.
As shown in Table 7, our SVMs-based chunkers
significantly outperform those baselines both in
F?=1 and classification accuracy9. We also evalu-
ate the effectiveness of each feature set, i.e., the
morpheme feature, the chunk candidate feature,
and the chunk context feature. The results in the
table show that the chunker with the chunk candi-
date feature performs almost best even without the
chunk context feature10.
9Recall of existing Japanese text processing tools is low,
because those tools can process only 50?60% of the whole
52 compound functional expressions, and for the remaining
40?50% expressions, they fail in identifying all of the occur-
rences of functional usages.
10It is also worthwhile to note that training the SVMs-
based chunker with the full set of features requires computa-
tional cost three times as much as training without the chunk
31
Table 7: Evaluation Results (%)
Identifying Acc. of classifying
functional chunks functional/content
Prec. Rec. F?=1 chunks
majority ( = functional) 78.0 100 87.6 78.0
Baselines Juman/KNP 89.2 49.3 63.5 55.8
ChaSen/CaboCha 89.0 45.6 60.3 53.2
hand-crafted rules 90.7 81.6 85.9 79.1
SVM morpheme 88.0 91.0 89.4 86.5
(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0
set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2
Figure 1: Change of F?=1 with Different Number
of Training Instances
For the SVMs-based chunker with the chunk
candidate feature with/without the chunk context
feature, Figure 1 plots the change of F?=1 when
training with different number of labeled chunks
as training instances. With this result, the increase
in F?=1 seems to stop with the maximum num-
ber of training instances, which supports the claim
that it is sufficient to collect and manually annotate
about 50 training examples per expression.
5 Concluding Remarks
The Japanese language has various types of com-
pound functional expressions, which are very im-
portant for recognizing the syntactic structures of
Japanese sentences and for understanding their se-
mantic contents. In this paper, we formalized
the task of identifying Japanese compound func-
tional expressions in a text as a chunking prob-
lem. We applied a machine learning technique
to this task, where we employed that of Sup-
port Vector Machines (SVMs). We showed that
the proposed method significantly outperforms ex-
isting Japanese text processing tools. The pro-
context feature.
posed framework has advantages over an approach
based on manually created rules such as the one in
(Shudo et al, 2004), in that it requires human cost
to manually create and maintain those rules. On
the other hand, in our framework based on the ma-
chine learning technique, it is sufficient to collect
and manually annotate about 50 training examples
per expression.
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistic. Computational Linguistics,
22(2):249?254.
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten.
Kuroshio Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support
vector machines. In Proc. 2nd NAACL, pages 192?199.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named entity
recognition using hundreds of thousands of features. In
Proc. 7th CoNLL, pages 184?187.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo
Hukugouji Youreishu. (in Japanese).
H. T. Ng, C. Y. Lim, and S. K. Foo. 1999. A case study on
inter-annotator agreement for word sense disambiguation.
In Proc. ACL SIGLEXWorkshop on Standardizing Lexical
Resources, pages 9?13.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc.
2nd ACL Workshop on Multiword Expressions: Integrat-
ing Processing, pages 32?39.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. 9th EACL, pages 173?179.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2005. A corpus for classifying usages of Japanese
compound functional expressions. In Proc. PACLING,
pages 345?350.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
32
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 65?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Dependency Relations of
Japanese Compound Functional Expressions
Takehito Utsuro? and Takao Shime? and Masatoshi Tsuchiya??
Suguru Matsuyoshi?? and Satoshi Sato??
?Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
?NEC Corporation
??Computer Center, Toyohashi University of Technology,
Tenpaku-cho, Toyohashi, 441?8580, JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
This paper proposes an approach of process-
ing Japanese compound functional expressions
by identifying them and analyzing their depen-
dency relations through a machine learning tech-
nique. First, we formalize the task of identify-
ing Japanese compound functional expressions
in a text as a machine learning based chunking
problem. Next, against the results of identify-
ing compound functional expressions, we apply
the method of dependency analysis based on the
cascaded chunking model. The results of ex-
perimental evaluation show that, the dependency
analysis model achieves improvements when ap-
plied after identifying compound functional ex-
pressions, compared with the case where it is ap-
plied without identifying compound functional
expressions.
1 Introduction
In addition to single functional words, the Japanese
language has many more compound functional ex-
pressions which consist of more than one word in-
cluding both content words and functional words.
They are very important for recognizing syntactic
structures of Japanese sentences and for understand-
ing their semantic content. Recognition and under-
standing of them are also very important for vari-
ous kinds of NLP applications such as dialogue sys-
tems, machine translation, and question answering.
However, recognition and semantic interpretation of
compound functional expressions are especially dif-
ficult because it often happens that one compound
expression may have both a literal (i.e. compo-
sitional) content word usage and a non-literal (i.e.
non-compositional) functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni) ???
(tsuite)?, which consists of a post-positional particle
?? (ni)?, and a conjugated form ???? (tsuite)? of
a verb ??? (tsuku)?. In the sentence (A), the com-
pound expression functions as a case-marking parti-
cle and has a non-compositional functional meaning
?about?. On the other hand, in the sentence (B), the
expression simply corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ?? (ni)? and the verb ????
(tsuite)?, and has a content word meaning ?follow?.
Therefore, when considering machine translation of
these Japanese sentences into English, it is neces-
sary to judge precisely the usage of the compound
expression ?? (ni)??? (tsuite)?, as shown in the
English translation of the two sentences in Table 1.
There exist widely-used Japanese text processing
tools, i.e. combinations of a morphological analy-
sis tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. However,
they process those compound expressions only par-
tially, in that their morphological analysis dictionar-
ies list only a limited number of compound expres-
sions. Furthermore, even if certain expressions are
listed in a morphological analysis dictionary, those
existing tools often fail in resolving the ambigui-
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
65
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
Correct English Translation:
( As a means of solving the problem, USA recommended the activity of OSCE in which Russia participates.)
(1) Correct Dependency Relation by Identifying Compound Functional Expression: ??????
with a Case Marking Particle Usage.
(2)  Incorrect Dependency Relation without Identifying Compound Functional Expression: ??????,
which Literally Consists of a Post-positional Particle ??? (with) and a Conjugation Form ????
of a Verb ???? (do).
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP as a means for solution       Russia-NOM also             participate in                                of  OSCE activity-ACC          recommended
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP with a means for Russia-NOM also             participate in                            of  OSCE activity-ACC       recommended
solution
Figure 1: Example of Improving Dependency Analysis of Compound Functional Expressions by Identifying
them before Dependency Analysis
ties of their usages, such as those in Table 1. This
is mainly because the framework of these existing
tools is not designed so as to resolve such ambigu-
ities of compound (possibly functional) expressions
by carefully considering the context of those expres-
sions.
Actually, as we introduce in the next section, as a
first step towards studying computational processing
of compound functional expressions, we start with
125 major functional expressions which have non-
compositional usages, as well as their variants (337
expressions in total). Out of those 337 expressions,
111 have both a content word usage and a functional
usage. However, the combination of JUMAN+KNP
is capable of distinguishing the two usages only for
43 of the 111 expressions, and the combination of
ChaSen+CaboCha only for 40 of those 111 expres-
sions. Furthermore, the failure in distinguishing the
two usages may cause errors of syntactic analysis.
For example, (1) of Figure 1 gives an example of
identifying a correct modifiee of the second bunsetsu
segment 5 ???????? (as a means for solu-
tion)? including a Japanese compound functional ex-
pression ???? (as)?, by appropriately detecting
the compound functional expression before depen-
dency analysis. On the other hand, (2) of Figure 1
gives an example of incorrectly indicating an erro-
neous modifiee of the third bunsetsu ????, which
actually happens if we do not identify the compound
functional expression ???? (as)? before depen-
dency analysis of this sentence.
Considering such a situation, it is necessary to
develop a tool which properly recognizes and se-
mantically interprets Japanese compound functional
expressions. This paper proposes an approach of
processing Japanese compound functional expres-
sions by identifying them and analyzing their de-
pendency relations through a machine learning tech-
nique. The overall flow of processing compound
functional expressions in a Japanese sentence is il-
5A Japanese bunsetsu segment is a phrasal unit which con-
sits of at least one content word and zero or more functional
words.
66
( As a means of solving the 
problem, USA recommended the 
activity of OSCE in which Russia 
participates.)
???????????
????????????
??????????
?????
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
morphological 
analysis
by ChaSen
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
compound
functional 
expression
Identifying
compound
functional
expression
chunking
bunsetsu
segmentation
&
dependency
analysis
bunsetsu
segment
dependency
relation
Figure 2: Overall Flow of Processing Compound Functional Expressions in a Japanese Sentence
lustrated in Figure 2. First of all, we assume a
sequence of morphemes obtained by a variant of
ChaSen with all the compound functional expres-
sions removed from its outputs, as an input to our
procedure of identifying compound functional ex-
pressions and analyzing their dependency relations.
We formalize the task of identifying Japanese com-
pound functional expressions in a text as a machine
learning based chunking problem (Tsuchiya et al,
2006). We employ the technique of Support Vec-
tor Machines (SVMs) (Vapnik, 1998) as the ma-
chine learning technique, which has been success-
fully applied to various natural language process-
ing tasks including chunking tasks such as phrase
chunking and named entity chunking. Next, against
the results of identifying compound functional ex-
pressions, we apply the method of dependency anal-
ysis based on the cascaded chunking model (Kudo
and Matsumoto, 2002), which is simple and efficient
because it parses a sentence deterministically only
deciding whether the current bunsetsu segment mod-
ifies the one on its immediate right hand side. As
we showed in Figure 1, identifying compound func-
tional expressions before analyzing dependencies in
a sentence does actually help deciding dependency
relations of compound functional expressions.
In the experimental evaluation, we focus on 59
expressions having balanced distribution of their us-
ages in the newspaper text corpus and are among the
most difficult ones in terms of their identification in
a text. We first show that the proposed method of
chunking compound functional expressions signifi-
cantly outperforms existing Japanese text processing
tools. Next, we further show that the dependency
analysis model of (Kudo and Matsumoto, 2002) ap-
plied to the results of identifying compound func-
tional expressions significantly outperforms the one
applied to the results without identifying compound
functional expressions.
2 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) exam-
ine 450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their example
sentences. Compared with those two collections,
Gendaigo Hukugouji Youreishu (National Language
Research Institute, 2001) (henceforth, denoted as
GHY) concentrates on 125 major functional expres-
sions which have non-compositional usages, as well
as their variants6, and collects example sentences of
those expressions. As we mentioned in the previous
section, as a first step towards developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants (337 expressions in to-
6For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) inser-
tion/deletion/alternation of certain particles, ii) alternation of
synonymous words, iii) normal/honorific/conversational forms,
iv) base/adnominal/negative forms.
67
(a) Classification of Compound Functional Expressions based on Grammatical Function
Grammatical Function Type # of major expressions # of variants Example
post-positional conjunctive particle 36 67 ??? (kuse-ni)
particle type case-marking particle 45 121 ??? (to-shite)
adnominal particle 2 3 ??? (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
(b) Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ??? ??????? ??? ???????????????? functional
(kuse-ni) (To my brother, (someone) gave money, while (he/she) did noth-
ing to me but just sent a letter.)
(??? (kuse-ni) = while)
(2) ??? ???? ??? ??????? content
(kuse-ni) (They all were surprised by his habit.) (???? (kuse-ni)
= by one?s habit
(3) ??? ?????????? ??? ??????? functional
(to-shite) (He is known as an expert of the problem.) (???? (to-shite)
= as ?)
(4) ??? ?????????????? ??? ???? content
(to-shite) (Please make it clear whether this is true or not.) (?? ???? (to-shite)
= make ? ?
(5) ??? ??????? ??? ???????? functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ?????????? ??? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this discussion.) (???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
Table 2: Classification and Example Usages of Compound Functional Expressions
tal). In this paper, following (Sag et al, 2002), we
regard each variant as a fixed expression, rather than
a semi-fixed expression or a syntactically-flexible
expression 7. Then, we focus on evaluating the
effectiveness of straightforwardly applying a stan-
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2 (a), according to their grammat-
ical functions, those 337 expressions in total are
roughly classified into post-positional particle type,
and auxiliary verb type. Functional expressions of
post-positional particle type are further classified
into three subtypes: i) conjunctive particle types,
which are used for constructing subordinate clauses,
ii) case-marking particle types, iii) adnominal parti-
cle types, which are used for constructing adnominal
7Compound functional expressions of auxiliary verb types
can be regarded as syntactically-flexible expressions.
clauses. Furthermore, for examples of compound
functional expressions listed in Table 2 (a), Table 2
(b) gives their example sentences as well as the de-
scription of their usages.
3 Identifying Compound Functional
Expressions by Chunking with SVMs
This section describes summaries of formalizing the
chunking task using SVMs (Tsuchiya et al, 2006).
In this paper, we use an SVMs-based chunking tool
YamCha8 (Kudo and Matsumoto, 2001). In the
SVMs-based chunking framework, SVMs are used
as classifiers for assigning labels for representing
chunks to each token. In our task of chunking
Japanese compound functional expressions, each
8http://chasen.org/?taku/software/
yamcha/
68
sentence is represented as a sequence of morphemes,
where a morpheme is regarded as a token.
3.1 Chunk Representation
For representing proper chunks, we employ IOB2
representation, which has been studied well in var-
ious chunking tasks of natural language processing.
This method uses the following set of three labels
for representing proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
Given a candidate expression, we classify the us-
ages of the expression into two classes: functional
and content. Accordingly, we distinguish the chunks
of the two types: the functional type chunk and the
content type chunk. In total, we have the follow-
ing five labels for representing those chunks: B-
functional, I-functional, B-content, I-content, and
O. Finally, as for extending SVMs to multi-class
classifiers, we experimentally compare the pairwise
method and the one vs. rest method, where the pair-
wise method slightly outperformed the one vs. rest
method. Throughout the paper, we show results with
the pairwise method.
3.2 Features
For the feature sets for training/testing of SVMs, we
use the information available in the surrounding con-
text, such as the morphemes, their parts-of-speech
tags, as well as the chunk labels. More precisely,
suppose that we identify the chunk label c
i
for the
i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, m
i
is the morpheme appearing at i-th posi-
tion, F
i
is the feature set at i-th position, and c
i
is
the chunk label for i-th morpheme. Roughly speak-
ing, when identifying the chunk label c
i
for the i-th
morpheme, we use the feature sets F
i?2
, F
i?1
, F
i
,
F
i+1
, F
i+2
at the positions i ? 2, i ? 1, i, i + 1,
i+2, as well as the preceding two chunk labels c
i?2
and c
i?1
. The detailed definition of the feature set
F
i
at i-th position is given in (Tsuchiya et al, 2006),
which mainly consists of morphemes as well as in-
formation on the candidate compound functional ex-
pression at i-th position.
4 Learning Dependency Relations of
Japanese Compound Functional
Expressions
4.1 Japanese Dependency Analysis using
Cascaded Chunking
4.1.1 Cascaded Chunking Model
First of all, we define a Japanese sen-
tence as a sequence of bunsetsu segments
B = ?b
1
, b
2
, . . . , b
m
? and its syntactic struc-
ture as a sequence of dependency patterns
D = ?Dep(1), Dep(2), . . . , Dep(m ? 1)?, where
Dep(i) = j means that the bunsetsu segment b
i
depends on (modifies) bunsetsu segment b
j
. In
this framework, we assume that the dependency
sequence D satisfies the following two constraints:
1. Japanese is a head-final language. Thus, except
for the rightmost one, each bunsetsu segment
modifies exactly one bunsetsu segment among
those appearing to its right.
2. Dependencies do not cross one another.
Unlike probabilistic dependency analysis models
of Japanese, the cascaded chunking model of Kudo
and Matsumoto (2002) does not require the proba-
bilities of dependencies and parses a sentence de-
terministically. Since Japanese is a head-final lan-
guage, and the chunking can be regarded as the cre-
ation of a dependency between two bunsetsu seg-
ments, this model simplifies the process of Japanese
dependency analysis as follows: 9
1. Put an O tag on all bunsetsu segments. The O
tag indicates that the dependency relation of the
current segment is undecided.
2. For each bunsetsu segment with an O tag, de-
cide whether it modifies the bunsetsu segment
on its immediate right hand side. If so, the O
tag is replaced with a D tag.
3. Delete all bunsetsu segments with a D tag that
immediately follows a bunsetsu segment with
an O tag.
9The O and D tags used in this section have no relation to
those chunk reppresentation tags introduced in section 3.1.
69
Initialization
?? ??? ??? ??? ?????
( He was moved by her warm heart. )
He her warm heart be moved
Input:
Tag:
?? ??? ??? ??? ?????
O O O O O
Input:
Tag:
?? ??? ??? ??? ?????
O O D D O
Deleted
Input:
Tag:
?? ??? ??? ?????
O D D O
Deleted
Input:
Tag:
?? ??? ?????
O D O
Input:
Tag:
?? ?????
O
Deleted
Input:
Tag:
?????
O
Finish
D
Deleted
Figure 3: Example of the Parsing Process with Cas-
caded Chunking Model
4. Terminate the algorithm if a single bunsetsu
segment remains, otherwise return to the step
2 and repeat.
Figure 3 shows an example of the parsing process
with the cascaded chunking model.
4.1.2 Features
As a Japanese dependency analyzer based on the
cascaded chunking model, we use the publicly avail-
able version of CaboCha (Kudo and Matsumoto,
2002), which is trained with the manually parsed
sentences of Kyoto text corpus (Kurohashi and Na-
gao, 1998), that are 38,400 sentences selected from
the 1995 Mainichi newspaper text.
The standard feature set used by CaboCha con-
sists of static features and dynamic features. Static
features are those solely defined once the pair
of modifier/modifiee bunsetsu segments is speci-
fied. For the pair of modifier/modifiee bunsetsu
segments, the following are used as static fea-
tures: head words and their parts-of-speech tags,
inflection-types/forms, functional words and their
parts-of-speech tags, inflection-types/forms, inflec-
tion forms of the words that appear at the end
of bunsetsu segments. As for features between
modifier/modifiee bunsetsu segments, the distance
of modifier/modifiee bunsetsu segments, existence
of case-particles, brackets, quotation-marks, and
punctuation-marks are used as static features. On the
other hand, dynamic features are created during the
parsing process, so that, when a certain dependency
relation is determined, it can have some influence
on other dependency relations. Dynamic features in-
clude bunsetsu segments modifying the current can-
didate modifiee (see Kudo and Matsumoto (2002)
for the details).
4.2 Coping with Compound Functional
Expressions
As we show in Figure 2, a compound functional ex-
pression is identified as a sequence of several mor-
phemes and then chunked into one morpheme. The
result of this identification process is then trans-
formed into the sequence of bunsetsu segments. Fi-
nally, to this modified sequence of bunsetsu seg-
ments, the method of dependency analysis based on
the cascaded chunking model is applied.
Here, when chunking a sequence of several mor-
phemes constituting a compound functional expres-
sion, the following two cases may exist:
(A) As in the case of the example (A) in Table 1, the
two morphemes constituting a compound func-
tional expression ?? (ni)??? (tsuite)? over-
laps the boundary of two bunsetsu segments.
In such a case, when chunking the two mor-
phemes into one morpheme corresponding to
a compound functional expression, those two
bunsetsu segments are concatenated into one
bunsetsu segment.
? ?
kare ni
(he)
???
tsuite
=?
? ????
kare ni-tsuite
(he) (about)
(B) As we show below, a compound functional ex-
pression ??? (koto)? (ga)?? (aru)? over-
laps the boundary of two bunsetsu segments,
though the two bunsetsu segments concatenat-
ing into one bunsetsu segment does include no
content words. In such a case, its immedi-
ate left bunsetsu segment (???(itt)? (ta)? in
the example below), which corresponds to the
content word part of ??? (koto)? (ga)??
(aru)?, has to be concatenated into the bunsetsu
segment ??? (koto)? (ga)?? (aru)?.
70
?? ?
itt ta
(went)
?? ?
koto ga
??
aru
=?
?? ? ?????
itt ta koto-ga-aru
(have been ?)
Next, to the compound functional expression, we
assign one of the four grammatical function types
listed in Table 2 as its POS tag. For example,
the compound functional expression ?? (ni)???
(tsuite)? in (A) above is assigned the grammatical
function type ?case-marking particle type?, while ?
?? (koto) ? (ga) ?? (aru)? in (B) is assigned
?auxiliary verb type?.
These modifications cause differences in the final
feature representations. For example, let us compare
the feature representations of the modifier bunsetsu
segments in (1) and (2) of Figure 1. In (1), the mod-
ifier bunsetsu segment is ????????? which
has the compound functional expression ?????
in its functional word part. On the other hand, in
(2), the modifier bunsetsu segment is ????, which
corresponds to the literal verb usage of a part of the
compound functional expression ?????. In the
final feature representations below, this causes the
following differences in head words and functional
words / POS of the modifier bunsetsu segments:
(1) of Figure 1 (2) of Figure 1
head word ?? (means) ?? (do)
functional word ??? (as) ? (and)
POS subsequent to nominal conjunctive
/ modifying predicate particle
5 Experimental Evaluation
5.1 Training/Test Data Sets
For the training of chunking compound functional
expressions, we collected 2,429 example sentences
from the 1995 Mainichi newspaper text corpus. For
each of the 59 compound functional expressions for
evaluation mentioned in section 1, at least 50 ex-
amples are included in this training set. For the
testing of chunking compound functional expres-
sions, as well as training/testing of learning depen-
dencies of compound functional expressions, we
used manually-parsed sentences of Kyoto text cor-
pus (Kurohashi and Nagao, 1998), that are 38,400
sentences selected from the 1995 Mainichi newspa-
per text (the 2,429 sentences above are selected so
that they are exclusive of the 37,400 sentences of
Kyoto text corpus.). To those data sets, we manually
annotate usage labels of the 59 compound functional
expressions (details in Table 3).
Usages # of
functional content total sentences
for chunker
training 1918 1165 3083 2429
Kyoto text corpus 5744 1959 7703 38400
Table 3: Statistics of Data Sets
Identifying
functional chunks
Acc. of
classifying
functional /
content
Prec. Rec. F
?=1
chunks
majority ( = functional) 74.6 100 85.5 74.6
Juman/KNP 85.8 40.5 55.0 58.4
ChaSen/CaboCha 85.2 26.7 40.6 51.1
SVM 91.4 94.6 92.9 89.3
Table 4: Evaluation Results of Chunking (%)
5.2 Chunking
As we show in Table 4, performance of our SVMs-
based chunkers as well as several baselines includ-
ing existing Japanese text processing tools is evalu-
ated in terms of precision/recall/F
?=1
of identifying
all the 5,744 functional chunks included in the test
data (Kyoto text corpus in Table 3). Performance is
evaluated also in terms of accuracy of classifying de-
tected candidate expressions into functional/content
chunks. Among those baselines, ?majority ( = func-
tional)? always assigns functional usage to the de-
tected candidate expressions. Performance of our
SVMs-based chunkers is measured through 10-fold
cross validation. Our SVMs-based chunker signif-
icantly outperforms those baselines both in F
?=1
and classification accuracy. As we mentioned in
section 1, existing Japanese text processing tools
process compound functional expressions only par-
tially, which causes damage in recall in Table 4.
5.3 Analyzing Dependency Relations
We evaluate the accuracies of judging dependency
relations of compound functional expressions by the
variant of CaboCha trained with Kyoto text cor-
pus annotated with usage labels of compound func-
tional expressions. This performance is measured
through 10-fold cross validation with the modified
version of the Kyoto text corpus. In the evaluation
phase, according to the flow of Figure 2, first we ap-
ply the chunker of compound functional expressions
trained with all the 2,429 sentences in Table 3 and
obtain the results of chunked compound functional
expressions with about 90% correct rate. Then, bun-
setsu segmentation and dependency analysis are per-
71
modifier modifiee
baselines CaboCha (w/o FE) 72.5 88.0
CaboCha (public) 73.9 87.6
chunker + CaboCha (proposed) 74.0 88.0
reference + CaboCha (proposed) 74.4 88.1
Table 5: Accuracies of Identifying Modi-
fier(s)/Modifiee (%)
formed by our variant of CaboCha, where accu-
racies of identifying modifier(s)/modifiee of com-
pound functional expressions are measured as in Ta-
ble 5 (?chunker + CaboCha (proposed)? denotes that
inputs to CaboCha (proposed) are with 90% correct
rate, while ?reference + CaboCha (proposed)? de-
notes that they are with 100% correct rate). Here,
?CaboCha (w/o FE)? denotes a baseline variant of
CaboCha, with all the compound functional expres-
sions removed from its inputs (which are outputs
from ChaSen), while ?CaoboCha (public)? denotes
the publicly available version of CaboCha, which
have some portion of the compound functional ex-
pressions included in its inputs.
For the modifier accuracy, the difference of
?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is statistically significant at a level of
0.05. Identifying compound functional expressions
typically contributes to improvements when the lit-
eral constituents of a compound functional expres-
sion include a verb. In such a case, for bunsetsu
segments which usually modifies a verb, an incor-
rect modifee candidate is removed, which results in
improvements in the modifier accuracy. The dif-
ference between ?CaoboCha (public)? and ?chunker
+ CaboCha (proposed)? is slight because the pub-
licly available version of CaboCha seems to include
compound functional expressions which are dam-
aged in identifying their modifiers with ?CaboCha
(w/o FE)?. For the modifiee accuracy, the difference
of ?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is zero. Here, more than 100 instances of
improvements like the one in Figure 1 are observed,
while almost the same number of additional fail-
ures are also observed mainly because of the sparse-
ness problem. Furthermore, in the case of the modi-
fiee accuracy, it is somehow difficult to expect im-
provement because identifying modifiees of func-
tional/content bunsetsu segments mostly depends on
features other than functional/content distinction.
6 Concluding Remarks
We proposed an approach of processing Japanese
compound functional expressions by identifying
them and analyzing their dependency relations
through a machine learning technique. This ap-
proach is novel in that it has never been applied
to any language so far. Experimental evaluation
showed that the dependency analysis model applied
to the results of identifying compound functional ex-
pressions significantly outperforms the one applied
to the results without identifying compound func-
tional expressions. The proposed framework has ad-
vantages over an approach based on manually cre-
ated rules such as the one in (Shudo et al, 2004), in
that it requires human cost to create manually and
maintain those rules. Related works include Nivre
and Nilsson (2004), which reports improvement of
Swedish parsing when multi word units are manu-
ally annotated.
References
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten. Kuroshio
Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support vec-
tor machines. In Proc. 2nd NAACL, pages 192?199.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency ana-
lyisis using cascaded chunking. In Proc. 6th CoNLL, pages
63?69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese parsed
corpus while improving the parsing system. In Proc. 1st
LREC, pages 719?724.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo Huku-
gouji Youreishu. (in Japanese).
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic
parsing. In Proc. LRECWorkshop, Methodologies and Eval-
uation of Multiword Units in Real-World Applications, pages
39?46.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc. 2nd
ACL Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 32?39.
M. Tsuchiya, T. Shime, T. Takagi, T. Utsuro, K. Uchimoto,
S. Matsuyoshi, S. Sato, and S. Nakagawa. 2006. Chunk-
ing Japanese compound functional expressions by machine
learning. In Proc. Workshop on Multi-Word-Expressions in
a Multilingual Context, pages 25?32.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
72
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 161?167,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Analysis and Robust Extraction of Changing Named Entities
Masatoshi Tsuchiya? Shoko Endo? Seiichi Nakagawa?
?Information and Media Center / ?Department of Information and Computer Sciences,
Toyohashi University of Technology
tsuchiya@imc.tut.ac.jp, {shoko,nakagawa}@slp.ics.tut.ac.jp
Abstract
This paper focuses on the change of named
entities over time and its influence on the
performance of the named entity tagger.
First, we analyze Japanese named enti-
ties which appear in Mainichi Newspaper
articles published in 1995, 1996, 1997,
1998 and 2005. This analysis reveals that
the number of named entity types and
the number of named entity tokens are
almost steady over time and that 70 ?
80% of named entity types in a certain
year occur in the articles published either
in its succeeding year or in its preceding
year. These facts lead that 20 ? 30%
of named entity types are replaced with
new ones every year. The experiment
against these texts shows that our propos-
ing semi-supervised method which com-
bines a small annotated corpus and a large
unannotated corpus for training works ro-
bustly although the traditional supervised
method is fragile against the change of
name entity distribution.
1 Introduction
It is widely agreed that extraction of named entity
(henceforth, denoted as NE) is an important sub-
task for various NLP applications, such as infor-
mation retrieval, machine translation, information
extraction and natural language understanding.
Several conferences like Message Understanding
Conference(Grishman and Sundheim, 1996) and
the IREX workshop (Sekine and Eriguchi, 2000)
were conducted to encourage researchers of NE
extraction and to provide its common evaluation
basis.
In Japanese NE extraction, it is quite common
to apply morphological analysis as preprocessing
stage which segments a sentence into a sequence
of morphemes. After that, either a pattern matcher
based on hand-crafted rules or a statistical chun-
ker is employed to extract NEs from a sequence of
morphemes. Various machine learning approaches
such as maximum entropy(Uchimoto et al, 2000),
decision list(Sassano and Utsuro, 2000; Isozaki,
2001), and Support Vector Machine(Yamada et
al., 2002; Isozaki and Kazawa, 2002) were in-
vestigated for extracting NEs. These researches
show that machine learning approaches are more
promising than approaches based on hand-crafted
rules if a large corpus whose NEs are properly an-
notated is available as training data.
However, it is difficult to obtain an enough cor-
pus in the real world because of the increasing
number of NE types and the increasing time gap
between the training corpus and the test corpus.
There is the increasing number of NE types like
personal names and company names in the real
world. For example, a large database of organi-
zation names(Nichigai Associates, 2007) already
contains 171,708 types and is still increasing. Be-
cause annotation work is quite expensive, the an-
notated corpus may become obsolete in a short
period of time. Both of two factors expands the
difference of NE distribution between the training
corpus and the test corpus, and it may decrease the
performance of the NE tagger as shown in (Mota
and Grishman, 2008). Therefore, a robust method
to extract NEs which do not occur or occur few
times in a training corpus is necessary.
This paper focuses on the change of NEs over
time and its influence on the performance of the
NE tagger. First, we annotate NEs in Mainichi
Newspaper articles published in 1996, 1997, 1998
and 2005, and analyze NEs which appear in
these texts and an existing corpus. It consists of
Mainichi Newspaper articles published in 1995,
thus, we get an annotated corpus that spans 10
years. This analysis reveals that the number of
NE types and the number of NE tokens are almost
161
Table 1: Statistics of NE categories of IREX cor-
pus
NE Categories Frequency (%)
ARTIFACT 747 (4.0)
DATE 3567 (19.1)
LOCATION 5463 (29.2)
MONEY 390 (2.1)
ORGANIZATION 3676 (19.7)
PERCENT 492 (2.6)
PERSON 3840 (20.6)
TIME 502 (2.7)
Total 18677
steady over time and that that 70 ? 80% of NE
types in a certain year occur in the articles pub-
lished either in its succeeding year or in its preced-
ing year. These facts lead that 20 ? 30% of named
entity types are replaced with new ones every year.
The experiment against these corpora shows that
the traditional supervised method is fragile against
the change of NE types and that our proposing
semi-supervised method which combines a small
annotated corpus and a large unannotated corpus
for training is robust against the change of NE
types.
2 Analysis of Changing Named Entities
2.1 Task of the IREX Workshop
The task of NE extraction of the IREX work-
shop (Sekine and Eriguchi, 2000) is to recognize
eight NE categories in Table 1. The organizer
of the IREX workshop provided a training corpus
(henceforth, denoted as IREX corpus), which con-
sists of 1,174 Mainichi Newspaper articles pub-
lished from January 1st 1995 to 10th which in-
clude 18,677 NEs. In the Japanese language, no
other corpora whose NEs are annotated are pub-
licly available as far as we know.1 Thus, IREX
corpus is referred as a golden sample of NE distri-
bution in this paper.
2.2 Data Description
The most homogeneous texts which are written in
different days are desirable, to explore the influ-
ence of the text time frame on NE distribution. Be-
cause IREX corpus is referred as a golden sample
1The organizer of the IREX workshop also provides the
testing data to its participants, however, we cannot see it be-
cause we did not join it.
??
???
???
???
???
???
???
???
???
???
????
?????
?????
???
?????
?????
????
?????
?????
????
?????
?????
???
?????
?????
???
?????
?????
????
?????
?????
????
?????
?????
????
??????????????????????????????????????????????????????
Figure 1: Distribution of NE categories
??
???
???
???
???
????
?? ?? ?? ? ? ? ? ????????????????????????????????????????????????????????
?????
????
????
?
????????????????
Figure 2: Overlap ratio of NEs over years
in this paper, Mainichi Newspaper articles writ-
ten in different years than IREX corpus is suit-
able. Thus, ordinal days of June and October in
1996, 1997, 1998 and 2005 are randomly selected
as sampling days.
Because annotating work is too expensive for
us to annotate all articles published in sampling
days, thirty percent of them are only annotated.
Each article of Mainichi Newspaper belongs into
16 categories like front page articles, international
stories, economical stories, political stories, edito-
rial columns, and human interest stories. Because
these categories may influence to NE distribution,
it is important to keep the proportion of categories
in the sampled texts to the proportion in the whole
newspaper, in order to investigate NE distribution
over the whole newspaper. Therefore, thirty per-
cent articles of each category published at sam-
pling days are randomly selected and annotated in
accordance with the IREX regulation.
2.3 Analysis of Annotated Samples
Table 2 shows the statistics of our annotated cor-
pus. The leftmost column of Table 2 (whose pub-
162
Table 2: Statistics of sampling texts
Published date 1995 1996 1997 1998 2005Jan. 1?10 Jun. 5 Oct. 15 Jun. 10 Oct. 7 Jun. 8 Oct. 21 Jun. 23 Oct. 12
# of articles 1174 120 133 106 117 96 126 90 99
# of characters 407881 60790 53625 46653 50362 51006 67744 49038 44344
# of NE types 6979 1446 1656 1276 1350 1190 1226 1230 1113
# of NE tokens 18677 2519 2652 2145 2403 2126 2052 1902 2007
# of NE types / # of characters 0.0171 0.0238 0.0309 0.0274 0.0268 0.0233 0.0181 0.0251 0.0251
# of NE tokens / # of characters 0.0458 0.0414 0.0495 0.0460 0.0477 0.0417 0.0303 0.0388 0.0453
Table 3: Overlap of NE types between texts published in different years
Published date of Published year of unannotated corpus U
annotated corpus A 1993 1994 1995 1996 1997 1998 1999
Jan. 1?10 (1995) 73.2% 78.6% ? 74.4% 65.0% 64.4% 63.3%
Jun. 6, Oct. 15 (1996) 67.2% 71.7% 72.2% ? 77.3% 76.0% 75.1%
Jun. 6, Oct. 7 (1997) 71.2% 73.4% 74.4% 78.6% ? 80.8% 78.6%
Jun. 8, Oct. 21 (1998) 72.5% 74.6% 76.2% 79.7% 82.7% ? 84.0%
Jun. 23, Oct. 12 (2005) 62.3% 64.1% 66.8% 68.7% 71.2% 72.9% 73.8%
lish date is January 1st to 10th in 1995) is corre-
sponding to IREX corpus, and other columns are
corresponding to articles annotated by ourselves.
Table 2 illustrates that the normalized number of
NE types and the normalized number of NE tokens
are almost steady over time. Figure 1 shows the
distributions of NE categories for sampling texts
and that there is no significant difference between
them.
We also investigate the relation of the time gap
between texts and NE types which appear in these
texts. The overlap ratio of NE types between the
annotated corpus A published in the year YA and
the annotated corpus B published in the year YB
was defined in (Mota and Grishman, 2008) as fol-
lows
type overlap(A,B) = |TA ? TB|
|TA|+ |TB| ? |TA ? TB|
,
where TA and TB are lists of NE types which ap-
pear in A and B respectively. However, it is im-
possible to compute reliable type overlap in our
research because enough annotated texts are un-
available. As an alternative of type overlap, the
overlap ratio of NE types between the annotated
corpus A and the unannotated corpus U published
in the year YU is defined as follows
string overlap(A,U) =
?
s?TA ?(s, U)
|TA|
,
where ?(s, U) is the binary function to indicate
whether the string s occurs in the string U or not.
Table 3 shows string ratio values of annotated
texts. It shows that 70 ? 80% of TA appear in the
preceding year of YA, and that 70 ? 80% of TA
appear in the succeeding year of YA.
Figure 2 shows the relation between the time
gap YU ? YA and string ratio(A,U). Sup-
pose that all NEs are independent and equiv-
alent on their occurrence probability and that
string ratio(A,U) is equal to 0.8 when the time
gap YU ? YA is equal to one. When the time gap
YU ? ? YA is equal to two years, although this as-
sumption leads that string ratio(A,U ?) will be
equal to 0.64, string ratio(A,U ?) in Figure 2 is
greater than 0.7. This suggests that NEs are not
equivalent on their occurrence probability. And
more, Table 4 shows that the longer time span
of the annotated text increases the number of NE
types. These facts lead that some NEs are short-
lived and superseded by other new NEs.
3 Robust Extraction of Changing Named
Entities
It is infeasible to prepare a large annotated cor-
pus which covers all increasing NEs. A semi-
supervised learning approach which combines a
small annotated corpus and a large unannotated
corpus for training is promising to cope this prob-
lem. (Miller et al, 2004) proposed the method
using classes which are assigned to words based
on the class language model built from a large
unannotated corpus. (Ando and Zhang, 2005) pro-
163
Table 4: Number of NE types and Time Span of Annotated Text
1995 1995?1996 1995?1997 1995?1998 1995?2005
ARTIFACT 541 (1.00) 743 (1.37) 862 (1.59) 1025 (1.89) 1169 (2.16)
DATE 950 (1.00) 1147 (1.21) 1326 (1.40) 1461 (1.54) 1583 (1.67)
LOCATION 1403 (1.00) 1914 (1.36) 2214 (1.58) 2495 (1.78) 2692 (1.92)
MONEY 301 (1.00) 492 (1.63) 570 (1.89) 656 (2.18) 749 (2.49)
ORGANIZATION 1487 (1.00) 1890 (1.27) 2280 (1.53) 2566 (1.73) 2893 (1.95)
PERCENT 249 (1.00) 319 (1.28) 353 (1.42) 401 (1.61) 443 (1.78)
PERSON 1842 (1.00) 2540 (1.38) 3175 (1.72) 3683 (2.00) 4243 (2.30)
TIME 206 (1.00) 257 (1.25) 291 (1.41) 314 (1.52) 332 (1.61)
Total 6979 (1.00) 9302 (1.33) 11071 (1.59) 12601 (1.81) 14104 (2.02)
(Values in brackets are rates of increase comparing to 1995.)
Morpheme Feature Similar Morpheme Feature Character
(English POS (English POS Type Chunk Label
translation) translation) Feature
?? (kyou) (today) Noun?Adverbial ?? (kyou) (today) Noun?Adverbial ?1, 0, 0, 0, 0, 0? O
? (no) gen Particle ? (no) gen Particle ?0, 1, 0, 0, 0, 0? O
?? (Ishikari) (Ishikari) Noun?Proper ?? (Kantou) (Kantou) Noun?Proper ?1, 0, 0, 0, 0, 0? B-LOCATION
?? (heiya) (plain) Noun?Generic ?? (heiya) (plain) Noun?Generic ?1, 0, 0, 0, 0, 0? I-LOCATION
? (no) gen Particle ? (no) gen Particle ?0, 1, 0, 0, 0, 0? O
?? (tenki) (weather) Noun?Generic ?? (tenki) (weather) Noun?Generic ?1, 0, 0, 0, 0, 0? O
? (ha) top Particle ? (ha) top Particle ?0, 1, 0, 0, 0, 0? O
?? (hare) (fine) Noun?Generic ?? (hare) (fine) Noun?Generic ?1, 1, 0, 0, 0, 0? O
Figure 3: Example of Training Instance for Proposed Method
posed the method using thousands of automati-
cally generated auxiliary classification problems
on an unannotated corpus. (?) proposed the semi-
supervised discriminative model whose potential
function can treat both an annotated corpus and an
unannotated corpus.
In this paper, the method proposed by (Tsuchiya
et al, 2008) is employed, because its implementa-
tion is quite easy. It consists of two steps. The
first step is to assign the most similar and famil-
iar morpheme to each unfamiliar morpheme based
on their context vectors calculated from a large
unannotated corpus. The second step is to employ
Conditional Random Fields(CRF)2(Lafferty et al,
2001) using both features of original morphemes
and features of similar morphemes.
This section gives the detail of this method.
3.1 Chunking of Named Entities
It is quite common that the task of extracting
Japanese NEs from a sentence is formalized as
a chunking problem against a sequence of mor-
phemes. For representing proper chunks, we em-
ploy IOB2 representation, one of representations
which have been studied well in various chunking
2http://chasen.org/?taku/software/CRF+
+/
tasks of NLP (Tjong Kim Sang, 1999). This rep-
resentation uses the following three labels.
B Current token is the beginning of a chunk.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
Actually, we prepare the 16 derived labels from
the label B and the label I for eight NE categories,
in order to distinguish them.
When the task of extracting Japanese NEs from
a sentence is formalized as a chunking problem
of a sequence of morphemes, the segmentation
boundary problem arises as widely known. For
example, the NE definition of IREX tells that a
Chinese character ?? (bei)? must be extracted as
an NE means America from a morpheme ???
(hou-bei)? which means visiting America. A naive
chunker using a morpheme as a chunking unit can-
not extract such a kind of NEs. In order to cope
this problem, (Uchimoto et al, 2000) proposed
employing translation rules to modify problematic
morphemes, and (Asahara and Matsumoto, 2003;
Nakano and Hirai, 2004) formalized the task of ex-
tracting NEs as a chunking problem of a sequence
of characters instead of a sequence of morphemes.
In this paper, we keep the naive formalization, be-
cause it is still enough to analyze the influence of
164
the text time frame.
3.2 Assignment of Similar Morpheme
A context vector Vm of a morpheme m is a vector
consisting of frequencies of all possible unigrams
and bigrams,
Vm =
?
?
?
?
?
f(m,m0), ? ? ? f(m,mN ),
f(m,m0,m0), ? ? ? f(m,mN ,mN ),
f(m0,m), ? ? ? f(mN ,m),
f(m0,m0,m), ? ? ? f(mN ,mN ,m)
?
?
?
?
?
,
where M ? {m0,m1, . . . ,mN} is a set of all
morphemes of the unannotated corpus, f(mi,mj)
is a frequency that a sequence of a morpheme mi
and a morpheme mj occurs in the unannotated
corpus, and f(mi,mj ,mk) is a frequency that a
sequence of morphemes mi,mj and mk occurs in
the unannotated corpus.
Suppose an unfamiliar morpheme mu ? M ?
MF , where MF is a set of familiar morphemes
that occur frequently in the annotated corpus. The
most similar morpheme m?u to the morpheme mu
measured with their context vectors is given by the
following equation,
m?u = argmax
m?MF
sim(Vmu , Vm), (1)
where sim(Vi, Vj) is a similarity function between
context vectors. In this paper, the cosine function
is employed as it.
3.3 Features
The feature set Fi at i-th position is defined as
a tuple of the morpheme feature MF (mi) of the
i-th morpheme mi, the similar morpheme feature
SF (mi), and the character type feature CF (mi).
Fi = ? MF (mi), SF (mi), CF (mi) ?
The morpheme feature MF (mi) is a pair of the
surface string and the part-of-speech of mi. The
similar morpheme feature SF (mi) is defined as
SF (mi) =
{
MF (m?i) if mi ? M ?MF
MF (mi) otherwise
,
where m?i is the most similar and familiar mor-
pheme to mi given by Eqn. 1. The character type
feature CF (mi) is a set of six binary flags to in-
dicate that the surface string of mi contains a Chi-
nese character, a hiragana character, a katakana
?? Chunking Direction ??
Feature set Fi?2 Fi?1 Fi Fi+1 Fi+2
Chunk label ci?2 ci?1 ci
Figure 4: Chunking Direction
character, an English alphabet, a number and an
other character respectively.
When we identify the chunk label ci for the i-
th morpheme mi, the surrounding five feature sets
Fi?2, Fi?1, Fi, Fi+1, Fi+2 and the preceding two
chunk labels ci?2, ci?1 are referred as shown in
Figure 4.
Figure 3 shows an example of training instance
of the proposed method for the sentence ???
(kyou)? (no)?? (Ishikari)?? (heiya)? (no)
?? (tenki)? (ha)?? (hare)? which means ?It
is fine at Ishikari-plain, today?. ??? (Kantou)?
is assigned as the most similar and familiar mor-
pheme to ??? (Ishikari)? which is unfamiliar in
the training corpus.
3.4 Experimental Result
Figure 5 compares performances of the proposed
method and the baseline method over the test texts
which were published in 1996, 1997, 1998 and
2005. The proposed method combines a small an-
notated corpus and a large unannotated corpus as
already described. This experiment refers IREX
corpus as a small annotated corpus, and refers
Mainichi Newspaper articles published from 1993
to the preceding year of the test text published
year as a large unannotated corpus. For example,
when the test text was published in 1998, Mainichi
Newspaper articles published from 1993 to 1997
are used. The baseline method is trained from
IREX corpus with CRF. But, it uses only MF and
CF as features, and does not use SF . Figure 5 il-
lustrates two points: (1) the proposed method out-
performs the baseline method consistently, (2) the
baseline method is fragile to changing of test texts.
Figure 6 shows the relation between the per-
formance of the proposed method and the size of
unannotated corpus against the test corpus pub-
lished in 2005. It reveals that that increasing unan-
notated corpus size improves the performance of
the proposed method.
4 Conclusion
In this paper, we explored the change of NE dis-
tribution over time and its influence on the per-
165
??????????
??????????
??????????
??????????
??????????
??????????
???? ???? ???? ????????????????????????????????
???
????
??
????????????????
Figure 5: Comparison between proposed method
and baseline method
?????
????
?????
????
?????
????
?????
????? ????
?????
????
?????
????
?????
????
?????
????
?????
????
?????
????
??????????????????????????????????
???
???
???
Figure 6: Relation of performance and unanno-
tated corpus size
formance of the NE tagger. First, we annotated
Mainichi Newspaper articles published in 1996,
1997, 1998 and 2005, and analyzed NEs which
appear in these texts and IREX corpus which con-
sists of Mainichi Newspaper articles published in
1995. This analysis illustrated that the number of
NE types and the number of NE tokens are al-
most steady over time, and that 70 ? 80% of NE
types seen in a certain year occur in the texts pub-
lished either in its succeeding year or in its pre-
ceding year. The experiment against these texts
showed that our proposing semi-supervised NE
tagger works robustly although the traditional su-
pervised NE tagger is fragile against the change of
NE types. Based on the results described in this
paper, we will investigate the relation between the
performance of NE tagger and the similarity of its
training corpus and its test corpus.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proc. of ACL ?05, pages 1?9, June.
Masayuki Asahara and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proc. of HLT?NAACL
?03, pages 8?15.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: a brief history. In
Proc. of the 16th COLING, pages 466?471.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. of the 19th COLING, pages 1?7.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision
tree learning. In Proc. of ACL ?01, pages 314?321.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282?
289.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proc. of HLT-NAACL 2004,
pages 337?342, May.
Cristina Mota and Ralph Grishman. 2008. Is this NE
tagger getting old? In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), May.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tions of Information Processing Society of Japan,
45(3):934?941, Mar. (in Japanese).
Nichigai Associates, editor. 2007. DCS Kikan-mei
Jisho. Nichigai Associates. (in Japanese).
Manabu Sassano and Takehito Utsuro. 2000. Named
entity chunking techniques in supervised learning
for japanese named entity recognition. In Proc. of
the 18th COLING, pages 705?711.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation: analysis of re-
sults. In Proc. of the 18th COLING, pages 1106?
1110.
E. Tjong Kim Sang. 1999. Representing text chunks.
In Proc. of the 9th EACL, pages 173?179.
Masatoshi Tsuchiya, Shinya Hida, and Seiichi Naka-
gawa. 2008. Robust extraction of named entity in-
cluding unfamiliar word. In Proceedings of ACL-
08: HLT, Short Papers, pages 125?128, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
166
Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi
Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000.
Named entity extraction based on a maximum en-
tropy model and transformation rules. Journal of
Natural Language Processing, 7(2):63?90, Apr. (in
Japanese).
Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto.
2002. Japanese named entity extraction using sup-
port vector machine. Transactions of Information
Processing Society of Japan, 43(1):44?53, Jan. (in
Japanese).
167
