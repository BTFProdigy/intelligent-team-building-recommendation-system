Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 610?619, Prague, June 2007. c?2007 Association for Computational Linguistics
A Discriminative Learning Model for Coordinate Conjunctions
Masashi Shimbo?
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
shimbo@is.naist.jp
Kazuo Hara?
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
kazuo-h@is.naist.jp
Abstract
We propose a sequence-alignment based
method for detecting and disambiguating co-
ordinate conjunctions. In this method, av-
eraged perceptron learning is used to adapt
the substitution matrix to the training data
drawn from the target language and domain.
To reduce the cost of training data con-
struction, our method accepts training exam-
ples in which complete word-by-word align-
ment labels are missing, but instead only
the boundaries of coordinated conjuncts are
marked. We report promising empirical re-
sults in detecting and disambiguating coor-
dinated noun phrases in the GENIA corpus,
despite a relatively small number of train-
ing examples and minimal features are em-
ployed.
1 Introduction
Coordination, along with prepositional phrase at-
tachment, is a major source of syntactic ambiguity
in natural language. Although only a small number
of previous studies in natural language processing
have dealt with coordinations, this does not mean
disambiguating coordinations is easy and negligible;
it still remains one of the difficulties for state-of-the-
art parsers. in Charniak and Johnson?s recent work
(Charniak and Johnson, 2005), for instance, two of
the features incorporated in their parse reranker are
aimed specifically at resolving coordination ambi-
guities.
Previous work on coordinations includes (Agar-
wal and Boggess, 1992; Chantree et al, 2005; Kuro-
?Equal contribution.
hashi and Nagao, 1994; Nakov and Hearst, 2005;
Okumura and Muraki, 1994; Resnik, 1999). Ear-
lier studies (Agarwal and Boggess, 1992; Okumura
and Muraki, 1994) attempted to find heuristic rules
to disambiguate coordinations. More recent re-
search are concerned with capturing structural sim-
ilarity between conjuncts using thesauri and cor-
pora (Chantree et al, 2005), or web-based statistics
(Nakov and Hearst, 2005).
We identify three problems associated with the
previous work.
1. Most of these studies evaluate the proposed
heuristics against restricted forms of conjunc-
tions. In some cases, they only deal with co-
ordinations with exactly two conjuncts, leaving
the generality of these heuristics unclear.
2. Most of these studies assume that the bound-
aries of coordinations are known in advance,
which, in our opinion, is impractical.
3. The proposed heuristics and statistics capture
many different aspects of coordination. How-
ever, it is not clear how they interact and how
they can be combined.
To address these problems, we propose a new
framework for detecting and disambiguating coor-
dinate conjunctions. Being a discriminative learning
model, it can incorporate a large number of overlap-
ping features encoding various heuristics for coordi-
nation disambiguation. It thus provides a test bed for
examining combined use of the proposed heuristics
as well as new ones. As the weight on each feature
is automatically tuned on the training data, assessing
these weights allows us to evaluate the relative merit
of individual features.
610
w
r
i
t
e
r
n nt e riv
Figure 1: An alignment between ?writer? and ?vint-
ner,? represented as a path in an edit graph
Our learning model is also designed to admit ex-
amples in which only the boundaries of coordinated
conjuncts are marked, to reduce the cost of training
data annotation.
The state space of our model resembles that of
Kurohashi and Nagao?s Japanese coordination de-
tection method (Kurohashi and Nagao, 1994). How-
ever, they considered only the decoding of coordi-
nated phrases and did not address automatic param-
eter tuning.
2 Coordination disambiguation as
sequence alignment
It is widely acknowledged that coordinate conjunc-
tions often consist of two or more conjuncts having
similar syntactic constructs. Our coordination detec-
tion model also follows this observation. To detect
such similar constructs, we use the sequence align-
ment technique (Gusfield, 1997).
2.1 Sequence alignment
Sequence alignment is defined in terms of transfor-
mation of one sequence (string) into another through
an alignment, or a series of edit operations. Each of
the edit operations has an associated cost, and the
cost of an alignment is defined as the total cost of
edit operations involved in the alignment. The min-
imum cost alignment can be computed by dynamic
programming in a state space called an edit graph,
such as illustrated in Figure 1. In this graph, a com-
plete path starting from the upper-left initial vertex
and arriving at the lower-right terminal vertex con-
stitutes a global alignment. Likewise, a partial path
corresponds to a local alignment.
Sequence alignment can also be formulated with
the scores of edit operations instead of their costs. In
this case, the sequence alignment problem is that of
finding a series of edit operations with the maximum
score.
2.2 Edit graph for coordinate conjunctions
A fundamental difference between biological local
sequence alignment and coordination detection is
that the former deals with finding local homologies
between two (or more) distinct sequences, whereas
coordination detection is concerned with local simi-
larities within a single sentence.
The maximal local alignment between two iden-
tical sequences is a trivial (global) alignment of
identity transformation (the diagonal path in an edit
graph). Coordination detection thus reduces to find-
ing off-diagonal partial paths with the highest sim-
ilarity score. Such paths never cross the diagonal,
and we can limit our search space to the upper trian-
gular part of the edit graph, as illustrated in Figure 2.
3 Automatic parameter tuning
Given a suitable substitution matrix, i.e., function
from edit operations to scores, it is straightforward
to find optimal alignments, or coordinate conjunc-
tions in our task, by running the Viterbi algorithm in
an edit graph.
In computational biology, there exist established
substitution matrices (e.g., PAM and BLOSUM)
built on a generative model of mutations and their
associated probabilities.
Such convenient substitution matrices do not ex-
ist for coordination detection. Moreover, optimal
score functions are likely to vary from one domain
(or language) to another. Instead of designing a
specific function for a single domain, we propose a
general discriminative learning model in which the
score function is a linear function of the features as-
signed to vertices and edges in the state space, and
the weight of the features are automatically tuned for
given gold standard data (training examples) drawn
from the application domain. Designing heuristic
rules for coordination detection, such as those pro-
posed in previous studies, translates to the design of
suitable features in our model.
Our learning method is an extension of Collins?s
perceptron-based method for sequence labeling
(Collins, 2002). However, a few incompatibilities
exists between Collins? sequence labeling method
and edit graphs used for sequence alignment.
611
median
dose
intensity
was
99%
for
standard
arm
182%
the
dose
arm
and
for
the
dense
.
m
ed
ia
n
do
se
in
te
ns
ity
w
as
99
%
fo
r
sta
nd
ar
d
ar
m
18
2%
th
e
do
se
ar
m
an
d
fo
r
th
e
de
ns
e
.
Figure 2: An edit graph for coordinate detection
1. Collins?s method, like the linear-chain condi-
tional random fields (CRFs) (Lafferty et al,
2001; Sha and Pereira, 2003), seeks for a com-
plete path from the initial vertex to the terminal
using the Viterbi algorithm. In an edit graph, on
the other hand, coordinations are represented
by partial paths. And we somehow need to
complement the partial path to make a com-
plete path.
2. A substitution matrix, which defines the score
of edit operations, can be represented as a func-
tion of features defined on edges. But to deal
with complex coordinations, a more expressive
score function is sometimes desirable, so that
scores can be computed not only on the basis of
a single edit operation, but also on consecutive
edit operations. Edit graphs are not designed to
accommodate features for such a higher-order
interaction of edit operations.
To reconcile these incompatibilities, we derive
a more finer-grained model from the original edit
graph. In presenting the description of our model be-
low, we reserve the terminology ?vertex? and ?edge?
for the original edit graph, and use ?node? and ?arc?
for our new model, to avoid confusion.
3.1 State space for learning coordinate
conjunctions
The new model is also based on the edit graph. In
this model, we create a node for each triple (v, p,e),
(a) (b) (c) (d) (e)
Figure 3: Five node types created for a vertex in an
edit graph: (a) Inside Delete, (b) Inside Insert, (c) In-
side Substitute, (d) Outside Delete, and (e) Outside
Insert.
(a) (b)
Figure 4: Series of edit operations with an equiv-
alent net effect. (a) (Insert,Delete), and (b)
(Delete, Insert). (b) is prohibited in our model.
where v is a vertex in the original edit graph, e ?
{Delete, Insert,Substitute} is an admissible1 edit op-
eration at v, and p ? {Inside,Outside} is a polarity
denoting whether or not the edit operation e is in-
volved in an alignment.
For a node (v, p,e), we call the pair (p,e) its type.
All five possible node types for a single vertex of an
edit graph are shown in Figure 3. We disallow type
(Outside,Substitute), as it is difficult to attribute an
intuitive meaning to substitution when two words
are not aligned (i.e., Outside).
Arcs between nodes are built according to the
transitions allowed in the original edit graph. To be
precise, an arc between node (v1, p1,e1) and node
(v2, p2,e2) is created if and only if the following
three conditions are met. (i) Edit operations e1 and
e2 are admissible at v1 and v2, respectively; (ii) the
sink of the edge for e1 at v1 is v2; and (iii) it is not
the case with p1 = p2 and (e1,e2) = (Delete, Insert).
Condition (iii) is introduced so as to disallow tran-
sition (Delete, Insert) depicted in Figure 4(b). In
contrast, the sequence (Insert,Delete) (Figure 4(a))
is allowed. The net effects of these edit operation
sequences are identical, in that they both skip one
word each from the two sequences to be aligned. As
a result, there is no use in discriminating between
these two, and one of them, namely (Delete, Insert),
is prohibited.
1For a vertex v at the border of an edit graph, some edit op-
erations are not applicable (e.g., Insert and Substitute at vertices
on the right border in Figure 2); we say such operations are in-
admissible at v. Otherwise, an edit operation is admissible.
612
A
,
B
,
C
and
D
an
d
A B ,, C D
A
,
B
,
C
and
D
an
d
A B ,, C D
(a) chainable (b) non-chainable
Figure 5: A coordination with four conjuncts repre-
sented as (a) chainable, and (b) non-chainable partial
paths. We take (a) as the canonical representation.
3.2 Learning task
By the restriction of condition (iii) introduced above
and the omission of (Outside, Substitute) from the
node types, we can uniquely determine the com-
plete path (from the initial node to the terminal node)
that conjoins all the local alignments by Outside
nodes (which corresponds to edges in the original
edit graph). In Figure 2, the augmented Outside
edges in this unique path are plotted as dotted lines
for illustration.
Thus we obtain a complete path which is compat-
ible with Collins?s perceptron-based sequence learn-
ing method. The objective of the learning algo-
rithms, which we will describe in Section 4, is to
optimize the weight of features so that running the
Viterbi algorithm will yield the same path as the gold
standard.
Because a node in our state space corresponds to
an edge in the original edit graph (see Figure 3), an
arc in our state space is actually a pair of consec-
utive edges (or equivalently, edit operations) in the
original graph. Hence our model is more expressive
than the original edit graph in that the score function
can have a term (feature) defined on a pair of edit
operations instead of one.
3.3 More complex coordinations
Even if a coordination comprises three or more con-
juncts, our model can handle them, as it can be rep-
resented as a set of pairwise local alignments that
are chainable (Gusfield, 1997, Section 13.3). If pair-
wise local alignments are chainable, a unique com-
plete path that conjoins all these alignments can be
determined, allowing the same treatment as the case
with two conjuncts.
For instance, a coordination with four conjuncts
(A, B, C and D) can be decomposed into a set of pair-
wise alignments {(A,B),(B,C),(C,D)} as depicted
in Figure 5(a). This set of alignments are chain-
able and thus constitute the canonical encoding for
this coordination; any other pairwise decomposition
for these four conjuncts, like {(A,B),(B,C),(A,D)}
(Figure 5(b)), is not chainable.
Our model can handle multiple non-nested coor-
dinations in a single sentence as well, as they can
also be decomposed into chainable pairwise align-
ments. It cannot encode nested coordinations like
(A, B, and (C and D)), however.
4 Algorithms
4.1 Reducing the cost of training data
construction
Our learning method is supervised, meaning that it
requires training data annotated with correct labels.
Since a label in our problem is local alignments
(or paths in an edit graph) representing coordina-
tions, the training sentences have to be annotated
with word-by-word alignments.
There are two reasons relaxing this requirement
is desirable. First, it is expensive to construct such
data. Second, there are coordinate conjunctions
in which word-by-word correspondence is unclear
even for humans. In Figure 2, for example, a word-
by-word alignment of ?standard? with ?dense? is de-
picted, but it might be more natural to regard a word
?standard? as being aligned with two words ?dose
dense? combined together.
Even if word-by-word alignment is uncertain, the
boundaries of conjuncts are often obvious, and it is
also much easier for human annotators to mark only
the beginning and end of each conjunct. Thus we
would like to allow for training examples in which
only alignment boundaries are specified, instead of
a full word-by-word alignment.
For these examples, conjunct boundaries corre-
sponds to a rectangular region rather than a sin-
gle path in an edit graph. The shaded box in Fig-
ure 2 illustrates the rectangular region determined by
the boundaries of an alignment between the phrases
?182% for the dose dense arm? and ?99% for the
standard arm.? There are many possible alignment
paths in this box, among which we do not know
which one is correct (or even likely). To deal with
613
input: Set of examples S = {(xi,Yi)}
Iteration cutoff T
output: Averaged weight vector w?
1: w? ? 0; w ? 0
2: for t ? 1 . . .T do
3: ?w ? 0
4: for each (xi,Yi) ? S do
5: y ? argmaxy?Yi w ? f (xi,y)
6: y? ? argmaxy?A(xi) w ? f (xi,y)
7: ? f ? f (xi,y)? f (xi,y?)
8: ?w ? ?w+? f
9: end for
10: if ?w = 0 then
11: return w?
12: end if
13: w ? w+?w
14: w? ? [(t ?1)w?+w]/t
15: end for
16: return w?
Figure 6: Path-based algorithm
this difficulty, we propose two simple heuristics we
call the (i) path-based and (ii) box-based methods.
As mentioned earlier, both of these methods are
based on Collins?s averaged-perceptron algorithm
for sequence labeling (Collins, 2002).
4.2 Path-based method
Our first method, which we call the ?path-based?
algorithm, is shown in Figure 6. We denote by A(x)
all possible alignments (paths) over x. The algorithm
receives T , the maximum number of iterations, and
a set of examples S = {(xi,Yi)} as input, where xi is a
sentence (a sequence of words with their attributes,
e.g., part-of-speech, lemma, prefixes, and suffixes)
and Yi ? A(xi) is the set of admissible alignments
(paths) for xi. When a sentence is fully annotated
with a word-by-word alignment y, Yi = {y} is a sin-
gleton set. In general boundary-only examples we
described in Section 4.1, Yi holds all possible align-
ments compatible with the marked range, or equiv-
alently, paths that pass through the upper-left and
lower-right corners of a rectangular region. Note
that it is not necessary to explicitly enumerate all the
member paths of Yi; the set notation here is only for
the sake of presentation.
The external function f (x,y) returns a vector
(called the global feature vector in (Sha and Pereira,
2003)) of the number of feature occurrences along
the alignment path y. In the beginning (line 5 in the
figure) of the inner loop, the target path (alignment)
input: Set of examples S = {(xi,Yi)}
Iteration cutoff T
output: Averaged weight vector w?
1: w? ? 0; w ? 0
2: for each (xi,Yi) ? S do
3: gi ? (1/|Yi|)?y?Yi f (xi,y)
4: end for
5: for t ? 1 . . .T do
6: ?w ? 0
7: for each (xi,Yi) ? S do
8: y? ? argmaxy?A(xi) w ? f (xi,y)
9: Convert y? into its box representation Y ?
10: g? ? (1/|Y ?i |)?y?Y ?i f (xi,y)
11: ? f ? gi ?g?
12: ?w ? ?w+? f
13: end for
14: if ?w = 0 then
15: return w?
16: end if
17: w ? w+?w
18: w? ? [(t ?1)w?+w]/t
19: end for
20: return w?
Figure 7: Box-based algorithm
is recomputed with the current weight vector w. The
argmax in lines 5 and 6 can be computed efficiently
(O(n2), where n is the number of words in x) by run-
ning a pass of the Viterbi algorithm in the edit graph
for x. The weight vector w varies between iterations,
and so does the most likely alignment with respect
to w. Hence the recomputation in line 5 is needed.
4.3 Box-based method
Our next method, called ?box-based,? is designed
on the following heuristic. Given a rectangle region
representing a local alignment (hence all nodes in
the region are of polarity Inside) in an edit graph,
we distribute feature weights in proportion to the
probability of a node (or an arc) being passed by a
path from the initial (upper left) node to the termi-
nal (lower right) node of the rectangle. We assume
paths are uniformly distributed.
Figure 8 displays an 8? 8 sub-grid of an edit
graph. The figure under each vertex shows the num-
ber of paths passing through the vertex. Vertices
near the upper-left and the lower-right corner have
a large frequency, and the frequency drops exponen-
tially towards the top right corner and the bottom
left corner, hence placing a strong bias on the paths
near diagonals. This distribution fits our preference
614
Figure 8: Number of paths passing through the ver-
tices of an 8?8 grid.
towards alignments with a larger number of substi-
tutions.
The pseudo-code for the box-based algorithm is
shown in Figure 7. For each example xi and its pos-
sible target labels (alignments)Yi, this algorithm first
(line 3) computes and stores in the vector gi the aver-
age number of feature occurrences in all possible tar-
get paths in Yi. This quantity can be computed sim-
ply by summing over all nodes and edges feature oc-
currences multiplied by the pre-computed frequency
of each nodes and arcs at which these features occur.
analogously to the forward-backward algorithm. In
each iteration, the algorithm scans every example
(lines 7?13), computing the Viterbi path y? (line 8)
according to the current weight vector w. Line 9
then converts y? to its box representation Y ?, by se-
quentially collapsing consecutive Inside nodes in y?
as a box. For instance, let y? be the local alignment
depicted as the bold line in Figure 2. The box Y ?
computed in line 9 for this y? is the shaded area in the
figure. In parallel to the initialization step in line 3,
we store in g? the average feature occurrences in Y ?
and update the current weight vector w by the differ-
ence between the target gi and g?. These steps can
be interpreted as a Viterbi approximation for com-
puting the optimal set Y ? of alignments directly.
5 Related work
5.1 Discriminative learning of edit distance
In our model, the state space of sequence alignment,
or edit graph, is two-dimensional (which is actu-
ally three-dimensional if the dimension for labels is
taken into account). This is contrastive to the one
dimensional models used by Collins?s perceptron-
based sequence method (Collins, 2002) which our
algorithms are based upon, and by the linear-chain
CRFs.
McCallum et al (McCallum et al, 2005) pro-
posed a CRF tailored to learning string edit distance
for the identity uncertainty problem. The state space
in their work is two dimensional just like our model,
but it is composed of two decoupled subspaces, each
corresponding to ?match? and ?mismatch,? thus shar-
ing only the initial state. It is not possible to make
a transition from a state in the ?match? state space to
the ?mismatch? space (and vice versa). As we can
see from the decoupled state space, this method is
based on global alignment rather than local align-
ment; it is not clear whether their method can iden-
tify local homologies in sequences. Our method uses
a single state space in which both ?match (inside)?
and ?mismatch (outside)? nodes co-exist and transi-
tion between them is permitted.
5.2 Inverse sequence alignment in
computational biology
In computational biology, the estimation of a sub-
stitution matrix from data is called the inverse se-
quence alignment problem. Until recently, there
have been a relatively small number of papers in
this field despite a large body of literature in se-
quence alignment. Theoretical studies in the inverse
sequence alignment include (Pachter and Sturmfels,
2004; Sun et al, 2004). Recently, CRFs have been
applied for optimizing the substitution matrix in the
context of global protein sequence alignment (Do et
al., 2006).
6 Empirical evaluation
6.1 Dataset and Task
We used the GENIA Treebank beta corpus (Kim et
al., 2003)2 for evaluation of our methods. The cor-
2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA
615
pus consists of 500 parsed abstracts in Medline with
a total of 4529 sentences.
Although the Penn Treebank Wall Street Journal
(WSJ) is the de facto standard corpus for evaluating
chunking and parsing performance, it lacks adequate
structural information on coordinate conjunctions,
and therefore does not serve our purpose. Many
coordinations in the Penn Treebank are given a flat
bracketing like (A, B, and C D), and thus we cannot
tell which of ((A, B, and C) D) and ((A), (B), and
(C D)) gives a correct alignment. The GENIA cor-
pus, in contrast, distinguishes ((A, B, and C) D) and
((A), (B), and (C D)) explicitly, by providing more
detailed bracketing. In addition, the corpus contains
an explicit tag ?COOD? for marking coordinations.
To avoid nested coordinations, which admittedly
require techniques other than the one proposed in
this paper, we selected from the GENIA corpus sen-
tences in which the conjunction ?and? occurs just
once. After this operation, the number of sentences
reduced to 1668, from which we further removed 32
that are not associated with the ?COOD? tag, and
3 more whose annotated tree structures contained
obvious errors. Of the remaining 1633 sentences,
1061 were coordinated noun phrases annotated with
NP-COOD tags, 226 coordinated verb phrases (VP-
COOD), 142 coordinated adjective phrases (ADJP-
COOD), and so on. Because the number of VP-
COOD, ADJP-COOD, and other types of coordi-
nated phrases are too small to make a meaningful
benchmark, we focus on coordinated noun phrases
in this experiment.
The task hence amounts to identifying coordi-
nated NPs and their constituent conjuncts in the
1633 sentences, all of which contain a coordination
marker ?and? but only 1061 of which are actually
coordinated NPs.
6.2 Baselines
We used several publicly available full parsers
as baselines: (i) the Bikel parser (Bikel,
2005) version 0.9.9c with configuration file
bikel.properties (denoted as Bikel/Bikel),
(ii) the Bikel parser in the Collins parser emula-
tion mode (using collins.properties file)
(Bikel/Collins), and (iii) Charniak and Johnson?s
reranking parser (Charniak-Johnson) (Charniak and
Johnson, 2005). We trained Bikel?s parser and its
Collins emulator with the GENIA corpus, WSJ, and
the combination of the two. Charniak and Johnson?s
parser was used as distributed at Charniak?s home
page (and is WSJ trained).
Another baseline we used is chunkers based
on linear-chain CRFs and the standard BIO la-
bels. We trained two types of CRF-based chun-
kers by using different BIO sequences, one for
the conjunct bracketing and the other for coor-
dination bracketing. The chunkers were imple-
mented with T. Kudo?s CRF++ package version
0.45. We varied its regularization parameters C
among C ? {0.01,0.1,1,10,100,1000}, and the best
results among these are reported below.
6.3 Features
Let x = (x1, . . . ,xn) be a sentence, with its member
xk a vector of attributes for the kth word. The at-
tributes include word surface, part-of-speech (POS),
and suffixes, among others.
Table 1 summarizes (i) the features assigned to a
node whose corresponding edge in the original edit
graph for x is emanating from row i and column j,
and (ii) the features assigned to the arcs (consisting
of two edges in the original edit graph) whose joint
(the vertex between the two edges) is a vertex at row
i and column j.
We also tested the path-based and box-based
methods, and the CRF chunkers both with and with-
out the word and suffix features.
Although this is not a requirement of our model or
algorithms, every feature we use in this experiment
is binary; if the condition associated with a feature
is satisfied, the feature takes a value of 1; otherwise,
it is 0. A condition typically asks whether or not
specific attributes match those at a current node, arc,
or their neighbors.
We used the POS tags from the GENIA corpus
as the POS attribute. The morphological features
include 3- and 4-gram suffixes and indicators of
whether a word includes capital letters, hyphens, and
digits.
For the baseline CRF-based chunkers, we assign
the word, POS (from GENIA), and the morphologi-
cal features to nodes, and the POS features to edges.
The feature set is identical to those used for our pro-
posed methods, except for features defined on row-
column combination (i.e., those defined over both i
616
Table 1: Features for the proposed methods
Substitute (diagonal) nodes
(?,Substitute,?)
Indicators of the word, POS, and morphological attributes of xi, x j , (xi?1,xi),
(xi,xi+1), (x j?1,x j), (x j,x j+1), and (xi, x j), respectively combined with the
type of the node.
For each of the word, POS, and morphological attributes, an indicator of
whether the respective attribute is identical in xi and x j , combined with the
type of the node.
Delete (vertical) nodes
(?,Delete,?)
Indicators of the word, POS, and morphological attributes of xi, x j , x j?1,
(xi?1,xi), (xi,xi+1), and (x j?1,x j), combined with the type of the node.
Insert (horizontal) nodes
(?, Insert,?)
Indicators of the word, POS, and morphological attributes of xi, xi?1, x j,
(xi?1,xi), (x j?1,x j), and (x j, x j+1), combined with the type of the node.
Any arcs
(?,?,?)? (?,?,?)
Indicators of the POS attribute of xi, xi?1, x j, x j?1, (xi?2,xi?1), (xi?1,xi),
(xi,xi+1), (x j?2,x j?1), (x j?1,x j), (x j,x j+1), (xi?1,x j?1), (xi?1,x j), (xi, x j?1)
and (xi,x j), combined with the type pair of the arc.
Arcs between nodes of different polarity
(?, Inside,?)? (?,Outside,?) and
(?,Outside,?)? (?, Inside,?)
Indicator of the distance j? i between two words xi and x j, combined with the
type pair of the arc.
and j in Table 1. The latter cannot be incorporated
as a local features in chunkers based on linear chain.
For the Bikel (and its Collins emulation) parsers
which accepts POS tags output by external taggers
upon testing, we gave them the POS tags from the
GENIA corpus, for fair comparison with the pro-
posed methods and CRF-based chunkers.
6.4 Evaluation criteria
We employed two evaluation criteria: (i) correctness
of the conjuncts output by the algorithm, and (ii) cor-
rectness of the range of coordinations as a whole.
For the correctness of conjuncts, we further use
two evaluation criteria. The first evaluation method
(?pairwise evaluation?) is based on the decomposi-
tion of coordinations into the canonical set of pair-
wise alignments, as described in Section 3.3. After
the set of pairwise alignments is obtained, each pair-
wise alignment is transformed into a box surrounded
by their boundaries. Using these boxes, we evaluate
precision, recall and F rates through the following
definition. The precision measures how many of the
boxes output by the algorithm exactly match those
in the gold standard, and the recall rate is the per-
centage of boxes found by the algorithm. The F rate
is the harmonic mean of the precision and the recall.
The second evaluation method (?chunk-based
evaluation?) for conjuncts is based on whether the
algorithm correctly outputs the beginning and end of
each conjunct, in the same manner as the chunking
tasks. Here, we adopt the evaluation criteria for the
CoNLL 99 NP bracketing task3; the precision equals
how many of the NP conjuncts output by the algo-
rithm are correct, and the recall is the percentage of
NP conjuncts found by the algorithm.
Of these two evaluation methods for conjuncts, it
is harder to obtain a higher pairwise evaluation score
than the chunk-based evaluation. To be counted as a
true positive in the pairwise evaluation, two consec-
utive chunks must be output correctly by the algo-
rithm.
For the correctness of the coordination range, we
check if both the start of the first coordinated con-
junct and the end of the last conjunct in the gold
match those output by the algorithm The reason we
evaluate coordination range is to compare our pro-
posed method with the full parsers trained on WSJ
(but applied to GENIA). Although WSJ and GE-
NIA differ in the way conjuncts are annotated, they
are mostly identical on how the range of coordina-
tions are annotated, and hence comparison is feasi-
ble in terms of coordination range. For the baseline
parsers, we regard the bracketing directly surround-
ing the coordination marker ?and? as their output.
In (Clegg and Shepherd, 2007), an F score of 75.5
is reported for the Bikel parser on coordination de-
tection. Their evaluation is based on dependencies,
which is different from our evaluation criteria which
are all based on boundaries. Generally speaking, our
evaluation criterion seems stricter, as exemplified in
Figures 7 and 8 of Clegg and Shepherd?s paper; in
these figures, our evaluation criterion would result
3http://www.cnts.ua.ac.be/conll99/npb/
617
Table 2: Performance on conjunct bracketing. P: precision (%), R: recall (%), F: F rate.
Pairwise evaluation Chunk-based evaluation
Method P R F P R F
Path-based method 61.4 56.2 58.7 70.9 66.9 68.9
Path-based method without word and suffix features 61.7 58.8 60.2 71.2 69.7 70.5
Box-based method 60.6 58.3 59.4 70.5 69.1 69.8
Box-based method without word and suffix features 59.5 58.3 58.9 69.7 69.5 69.6
Linear-chain CRF chunker (conjunct bracketing) 62.6 51.4 56.4 71.0 66.1 68.5
Bikel/Collins, trained with GENIA 50.0 48.6 49.3 65.0 64.2 64.6
Bikel/Bikel, trained with GENIA 50.1 47.8 49.0 63.9 61.3 62.6
Table 3: Performance on coordination bracketing. P: precision (%), R: recall (%), F: F rate.
Method P R F
Path-based method 58.2 55.3 56.7
Path-based method without words and suffix features 57.7 56.6 57.2
Box-based method 55.6 54.4 55.0
Box-based method without words and suffix features 54.8 54.6 54.7
Linear-chain CRF chunker, trained with conjunct bracketing 43.9 46.7 45.3
Linear-chain CRF chunker, trained with coordination bracketing 58.4 51.0 54.5
Bikel/Collins, trained with GENIA 44.0 45.4 44.7
Bikel/Collins, trained with WSJ 42.3 43.2 42.7
Bikel/Collins, trained with GENIA+WSJ 43.3 45.1 44.1
Bikel/Bikel, trained with GENIA 44.8 45.4 45.1
Bikel/Bikel, trained with WSJ 40.7 41.5 41.1
Bikel/Bikel, trained with GENIA+WSJ 43.9 45.8 44.9
Charniak-Johnson reranking parser 48.3 45.2 46.7
in zero true positive, whereas their evaluation counts
the dependency arc from ?genes? to ?human? as one
true positive.
6.5 Results
The results of conjunct and coordination bracketing
are shown in Tables 2 and 3, respectively. These
are the results of a five-fold cross validation. We
ran the proposed methods until convergence or the
cutoff iteration of T = 10000, whichever comes first.
The path-based method (without words and suf-
fixes) and box-based method (with full features)
each achieved 2.0 and 1.3 point improvements over
the CRF chunker in terms of the F score in conjunct
identification (chunk-based evaluation), 3.8 and 3.0
point improvement in terms of pairwise evaluation,
and 2.7 and 0.5 points in coordinate identification,
respectively. Our methods also showed a perfor-
mance considerably higher than the baseline parsers.
The performance of the path-based method was
better when the word and suffix features were re-
moved, while the box-based method and CRF chun-
kers performed better with these features.
7 Conclusions
We have proposed a new coordination learning and
disambiguation method that can incorporate many
different features, and automatically optimize their
weights on training data.
In the experiment of Section 6, the proposed
method obtained a performance superior to a linear-
chain chunker and to the state-of-art full parsers.
We used only syntactic and morphological fea-
tures, and did not use external similarity measures
like thesauri and corpora, although they are reported
to be effective for disambiguating coordinations. We
note that it is easy to incorporate such external sim-
ilarity measures as a feature in our model, thanks to
its two-dimensional state space. The similarity of
two words derived from an external knowledge base
can be assigned to a Substitute node at a correspond-
ing location in the state space in a straightforward
manner. This is a topic we are currently working on.
We are also planning to reimplement our algo-
rithms using CRFs instead of the averaged percep-
tron algorithm.
618
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computing Linguistics (ACL?92), pages 15?21.
Daniel M. Bikel. 2005. Multilingual statistical pars-
ing engine version 0.9.9c. http://www.cis.upenn.edu/
?dbikel/software.html.
Francis Chantree, Adam Kilgarriff, Anne de Roeck, and
Alistair Willis. 2005. Disambiguating coordina-
tions using word distribution information. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP
2005), Borovets, Bulgaria.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL-2005).
Andrew B Clegg and Adrian J Shepherd. 2007. Bench-
marking natural-language parsers for biological appli-
cations using dependency graphs. BMC Bioinformat-
ics, 8(24).
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002).
C. B. Do, S. S. Gross, and S. Batzoglou. 2006. CON-
TRAlign: discriminative training for protein sequence
alignment. In Proceedings of the Tenth Annual Inter-
national Conference on Computational Molecular Bi-
ology (RECOMB 2006).
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus: a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl. 1):i180?i182.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20:507?534.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning (ICML-2001), pages 282?289. Morgan
Kaufmann.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the 21st Conference on Uncertainty in Artificial In-
telligence (UAI-2005).
Preslav Nakov and Marti Hearst. 2005. Using the web as
an implicit training set: application to structural ambi-
guity resolution. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language (HLT/EMNLP), pages
835?842, Vancouver.
Akitoshi Okumura and Kazunori Muraki. 1994. Sym-
metric pattern matching analysis for English coordi-
nate structures. In Proceedings of the Fourth Confer-
ence on Applied Natural Language Processing, pages
41?46.
Lior Pachter and Bernd Sturmfels. 2004. Parametric
inference for biological sequence analysis. Proceed-
ings of the National Academy of Sciences of the USA,
101(46):16138?16143.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
an information-based measure and its application to
problems of ambiguity in natural language. Journal
of Artificial Intelligence Research, 11:95?130.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the Human Language Technology Conference North
American Chapter of Association for Computational
Linguistics (HLT-NAACL 2003), pages 213?220, Ed-
monton, Alberta, Canada. Association for Computa-
tional Linguistics.
Fangting Sun, David Ferna?ndez-Baca, and Wei Yu. 2004.
Inverse parametric sequence alignment. Journal of Al-
gorithms, 53:36?54.
619
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011?1020,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Graph-based Analysis of Semantic Drift in Espresso-like
Bootstrapping Algorithms
Mamoru Komachi
NAIST, Japan
mamoru-k@is.naist.jp
Taku Kudo
Google Inc.
taku@google.com
Masashi Shimbo
NAIST, Japan
shimbo@is.naist.jp
Yuji Matsumoto
NAIST, Japan
matsu@is.naist.jp
Abstract
Bootstrapping has a tendency, called seman-
tic drift, to select instances unrelated to the
seed instances as the iteration proceeds. We
demonstrate the semantic drift of bootstrap-
ping has the same root as the topic drift of
Kleinberg?s HITS, using a simplified graph-
based reformulation of bootstrapping. We
confirm that two graph-based algorithms, the
von Neumann kernels and the regularized
Laplacian, can reduce semantic drift in the
task of word sense disambiguation (WSD)
on Senseval-3 English Lexical Sample Task.
Proposed algorithms achieve superior perfor-
mance to Espresso and previous graph-based
WSD methods, even though the proposed al-
gorithms have less parameters and are easy to
calibrate.
1 Introduction
In recent years machine learning techniques be-
come widely used in natural language processing
(NLP). These techniques offer various ways to ex-
ploit large corpora and are known to perform well
in many tasks. However, these techniques often re-
quire tagged corpora, which are not readily available
to many languages. So far, reducing the cost of hu-
man annotation is one of the important problems for
building NLP systems.
To mitigate the problem of hand-tagging re-
sources, semi(or minimally)-supervised and unsu-
pervised techniques have been actively studied.
Hearst (1992) first presented a bootstrapping method
which requires only a small amount of instances
(seed instances) to start with, but can easily mul-
tiply the number of tagged instances with mini-
mal human annotation cost, by iteratively apply-
ing the following phases: pattern induction, pattern
ranking/selection, and instance extraction. Boot-
strapping has been widely adopted in NLP applica-
tions such as word sense disambiguation (Yarowsky,
1995), named entity recognition (Collins and Singer,
1999) and relation extraction (Riloff and Jones,
1999; Pantel and Pennacchiotti, 2006).
However, it is known that bootstrapping often ac-
quires instances not related to seed instances. For
example, consider the task of collecting the names
of common tourist sites from web corpora. Given
words like ?Geneva? and ?Bali? as seed instances,
bootstrapping would eventually learn generic pat-
terns such as ?pictures? and ?photos,? which also
co-occur with many other unrelated instances. The
subsequent iterations would likely acquire frequent
words that co-occur with these generic patterns,
such as ?Britney Spears.? This phenomenon is
called semantic drift (Curran et al, 2007).
A straightforward approach to avoid semantic
drift is to terminate iterations before hitting generic
patterns, but the optimal number of iterations is task
dependent and is hard to come by. The recently pro-
posed Espresso (Pantel and Pennacchiotti, 2006) al-
gorithm incorporates sophisticated scoring functions
to cope with generic patterns, but as Komachi and
Suzuki (2008) pointed out, Espresso still shows se-
mantic drift unless iterations are terminated appro-
priately.
Another deficiency in bootstrapping is its sensi-
tivity to many parameters such as the number of
1011
seed instances, the stopping criterion of iteration, the
number of instances and patterns selected on each it-
eration, and so forth. These parameters also need to
be calibrated for each task.
In this paper, we present a graph-theoretic anal-
ysis of Espresso-like bootstrapping algorithms. We
argue that semantic drift is inherent in these algo-
rithms, and propose to use two graph-based algo-
rithms that are theoretically less prone to semantic
drift, as an alternative to bootstrapping.
After a brief review of related work in Section 2,
we analyze in Section 3 a bootstrapping algorithm
(Simplified Espresso) which can be thought of as a
degenerate version of Espresso. Simplified Espresso
is simple enough to allow an algebraic treatment,
and its equivalence to Kleinberg?s HITS algorithm
(Kleinberg, 1999) is shown. An implication of this
equivalence is that semantic drift in this bootstrap-
ping algorithm is essentially the same phenomenon
as topic drift observed in link analysis. Another im-
plication is that semantic drift is inevitable in Sim-
plified Espresso as it converges to the same score
vector regardless of seed instances.
The original Espresso also suffers from the same
problem as its simplified version does. It incorpo-
rates heuristics not present in Simplified Espresso to
reduce semantic drift, but these heuristics have lim-
ited effect as we demonstrate in Section 3.3.
In Section 4, we propose two graph-based algo-
rithms to reduce semantic drift. These algorithms
are used in link analysis community to reduce the
effect of topic drift. In Section 5 we apply them to
the task of word sense disambiguation on Senseval-3
Lexical Sample Task and verify that they indeed re-
duce semantic drift. Finally, we conclude our work
in Section 6.
2 Related Work
2.1 Overview of Bootstrapping
Bootstrapping (or self-training) is a general frame-
work for reducing the requirement of manual an-
notation. Hearst (1992) described a bootstrapping
procedure for extracting words in hyponym (is-a)
relation, starting with three manually given lexico-
syntactic patterns.
The idea of learning with a bootstrapping method
was adopted for many tasks. Yarowsky (1995) pre-
sented an unsupervised WSD system which rivals
supervised techniques. Abney (2004) presented a
thorough discussion on the Yarowsky algorithm. He
extended the original Yarowsky algorithm to a new
family of bootstrapping algorithms that are mathe-
matically well understood.
Li and Li (2004) proposed a method called Bilin-
gual Bootstrapping. It makes use of a translation
dictionary and a comparable corpus to help disam-
biguate word senses in the source language, by ex-
ploiting the asymmetric many-to-many sense map-
ping relationship between words in two languages.
Curran et al (2007) presented an algorithm called
Mutual Exclusion Bootstrapping, which minimizes
semantic drift using mutual exclusion between se-
mantic classes of learned instances. They prepared
a list of so-called stop classes similar to a stop word
list used in information retrieval to help bound the
semantic classes. Stop classes are sets of terms
known to cause semantic drift in particular seman-
tic classes. However, stop classes vary from task to
task and domain to domain, and human intervention
is essential to create an effective list of stop classes.
A major drawback of bootstrapping is the lack
of principled method for selecting optimal param-
eter values (Ng and Cardie, 2003; Banko and Brill,
2001). Also, there is an issue of generic patterns
which deteriorates the quality of acquired instances.
Previously proposed bootstrapping algorithms differ
in how they deal with the problem of semantic drift.
We will take recently proposed Espresso algorithm
as the example to explain common configuration for
bootstrapping in detail.
2.2 The Espresso Algorithm
Pantel and Pennachiotti (2006) proposed a boot-
strapping algorithm called Espresso to learn binary
semantic relations such as is-a and part-of from
a corpus. What distinguishes Espresso from other
bootstrapping algorithms is that it benefits from
generic patterns by using a principled measure of
instance and pattern reliability. The key idea of
Espresso is recursive definition of pattern-instance
scoring metrics. The reliability scores of pattern p
and instance i, denoted respectively as rpi(p) and
1012
r?(i), are given as follows:
rpi(p) =
?
i?I
pmi(i,p)
max pmir?(i)
|I|
(1)
r?(i) =
?
p?P
pmi(i,p)
max pmirpi(p)
|P |
(2)
where
pmi(i, p) = log2
|i, p|
|i, ?||?, p|
(3)
is pointwise mutual information between i and p, P
and I are sets of patterns and instances, and |P | and
|I| are the numbers of patterns and instances, respec-
tively. |i, ?| and |?, p| are the frequencies of pattern
p and instance i in a given corpus, respectively, and
|i, p| is the frequency of pattern p which co-occurs
with instance i. max pmi is a maximum value of
the pointwise mutual information over all instances
and patterns. The intuition behind these definitions
is that a reliable pattern co-occurs with many reli-
able instances, and a reliable instance co-occurs with
many reliable patterns.
Espresso and other bootstrapping methods iterate
the following three phases: pattern induction, pat-
tern ranking/selection, and instance extraction.
We describe these phases below, along with the
parameters that controls each phase.
Phase 1. Pattern Induction Induce patterns from
a corpus given seed instances. Patterns may be sur-
face text patterns, lexico-syntactic patterns, and/or
just features.
Phase 2. Pattern Ranking/Selection Create a
pattern ranker from a corpus using instances as fea-
tures and select patterns which co-occur with seed
instances for the next instance extraction phase. The
main issue here is to avoid ranking generic patterns
high and to choose patterns with high relatedness to
the seed instances. Parameters and configurations:
(a) a pattern scoring metrics and (b) the number of
patterns to use for extraction of instances.
Phase 3. Instance Extraction Select high-
confidence instances to the seed instance set. It is
desirable to keep only high-confidence instances at
this phase, as they are used as seed instances for the
input:
seed vector i0
pattern-instance co-occurrence matrix M
output:
instance and pattern score vectors i and p
1: i = i0
2: loop
3: p ? M i
4: Normalize p
5: i ? MTp
6: Normalize i
7: if i and p have both converged then
8: return i and p
9: end if
10: end loop
Figure 1: A simple bootstrapping algorithm
next iteration. Optionally, instances can be cumula-
tively obtained on each iteration to retain highly rel-
evant instances learned in early iterations. Parame-
ters and configurations: (c) instance scoring metrics,
(d) whether to retain extracted instances on each it-
eration or not, and (e) the number of instances to
pass to the next iteration.
Bootstrapping iterates the above three phases sev-
eral times until stopping criteria are met. Acquired
instances tend to become noisy as the iteration pro-
ceeds, so it is important to terminate before semantic
drift occurs. Thus, we have another configuration:
(f) stopping criterion.
Espresso uses Equations (1) for (a) and (2) for (c)
respectively, whereas other parameters rely on the
tasks and need calibration. Even though Espresso
greatly improves recall while keeping high precision
by using these pattern and instance scoring metrics,
Komachi and Suzuki (2008) observed that extracted
instances matched against generic patterns may be-
come erroneous after tens of iterations, showing the
difficulty of applying bootstrapping methods to dif-
ferent domains.
3 Analysis of an Espresso-like
Bootstrapping Algorithm
3.1 Simplified Espresso
Let us consider a simple bootstrapping algorithm
illustrated in Figure 1, in order to elucidate the cause
1013
of semantic drift.
As before, let |I| and |P | be the numbers of
instances and patterns, respectively. The algo-
rithm takes a seed vector i0, and a pattern-instance
co-occurrence matrix M as input. i0 is a |I|-
dimensional vector with 1 at the position of seed in-
stances, and 0 elsewhere. M is a |P | ? |I|-matrix
whose (p, i)-element [M ]pi holds the (possibly re-
weighted) number of co-occurrence of pattern p and
instance i in the corpus. If both i and p have con-
verged, the algorithm returns the pair of i and p as
output.
This algorithm, though simple, can encode
Espresso?s update formulae (1) and (2) as Steps 3
through 6 if we pose
[M ]pi =
pmi(i, p)
max pmi
, (4)
and normalize p and i in Steps 4 and 6 by
p ? p/|I| and i ? i/|P |, (5)
respectively.
This specific instance of the algorithm of Fig-
ure 1, obtained by specialization through Equations
(4) and (5), will be henceforth referred to as Simpli-
fied Espresso. Indeed, it is an instance of the origi-
nal Espresso in which the iteration is not terminated
until convergence, all instances are carried over to
the next iteration, and instances are not cumulatively
learned.
3.2 Simplified Espresso as Link Analysis
Let n denote the number of times Steps 2?10 are
iterated. Plugging (4) and (5) into Steps 3?6, we
see that the score vector of instances after the nth
iteration is
in = Ani0 (6)
where
A = 1
|I||P |
MTM. (7)
Suppose matrix A is irreducible; i.e., the graph
induced by taking A as the adjacency matrix is con-
nected. If n is increased and in is normalized on
each iteration, in tends to the principal eigenvec-
tor of A. This implies that no matter what seed in-
stances are input, the algorithm will end up with the
same ranking of instances, if it is run until conver-
gence. Because A = MTM|I||P | , the principal eigen-
vector of A is identical to the authority vector of
HITS (Kleinberg, 1999) algorithm run on the graph
induced by M . 1 This similarity of Equations (1),
(2) and HITS is not discussed in (Pantel and Pen-
nacchiotti, 2006).
As a consequence of the above discussion, se-
mantic drift in simplified Espresso seems to be in-
evitable as the iteration proceeds, since the principal
eigenvector of A need not resemble seed vector i0.
A similar phenomenon is reported for HITS and is
known as topic drift, in which pages of the dominant
topic are ranked high regardless of the given query.
(Bharat and Henzinger, 1998)
Unlike HITS and Simplified Espresso, how-
ever, Espresso and other bootstrapping algo-
rithms (Yarowsky, 1995; Riloff and Jones, 1999),
incorporate heuristics so that only patterns and in-
stances with high confidence score are carried over
to the next iteration.
3.3 Convergence Process of Espresso
To investigate the effect of semantic drift on
Espresso with and without the heuristics of selecting
the most confident instances on each iteration (i.e.,
the original Espresso and Simplified Espresso of
Section 3.2), we apply them to the task of word sense
disambiguation of word ?bank? in the Senseval-3
Lexical Sample (S3LS) Task data.2 There are 394
instances of word ?bank? and their occurring con-
text in this dataset, and each of them is annotated
with its true sense. Of the ten senses of bank, the
most frequent is the bank as in ?bank of the river.?
We use the standard training-test split provided with
the data set.
We henceforth denote Espresso with the follow-
ing filtering strategy as Filtered Espresso to stress
the distinction from Simplified Espresso. For Fil-
tered Espresso, we cleared all but the 100 top-
scoring instances in the instance vector on each iter-
ation, and the number of non-zeroed instance scores
1As long as the relative magnitude of the components of vec-
tor in is preserved, the vector can be normalized in any way on
each iteration. Hence HITS and Simplified Espresso use differ-
ent normalization but both converge to the principal eigenvector
of A.
2http://www.senseval.org/senseval3/data.html
1014
grows by 100 on each iteration. On the other hand,
we cleared all but the 20 top-scoring patterns in the
pattern vector on each iteration, and the number of
non-zeroed pattern scores grows by 1 on each iter-
ation following (Pantel and Pennacchiotti, 2006).3
The values of other parameters (b), (d), (e) and (f)
remains the same as those for simplified Espresso in
Section 3.1.
The task of WSD is to correctly predict the senses
of test instances whose true sense is hidden from the
system, using training data and their true senses. To
predict the sense of a given instance i, we apply k-
nearest neighbor algorithm.
Given a test instance i, its sense is predicted with
the following procedure:
1. Compute the instance-pattern matrix M from
the entire set of instances. We defer the details
of this step to Section 5.2.
2. Run Simplified- and Filtered Espresso using
the given instance i as the only seed instance.
3. After the termination of the algorithm, select k
training instances with the highest scores in the
score vector i output by the algorithm.
4. Since the selected k instances are training
instances, their true senses are accessible.
Choose the majority sense s from these k in-
stances, and output s as the prediction for the
given instance i. When there is a tie, output the
sense of the instance with the highest score in
i. Note that only Step 4 uses sense information.
Figure 2 shows the convergence process of
Simplified- and Filtered Espresso. X-axis indicates
the number of bootstrapping iterations and Y-axis
indicates the recall, which in this case equals pre-
cision, as the coverage is 100% in all cases.
3We conducted preliminary experiment to find these param-
eters to maximize the performance of Filtered Espresso. (These
numbers are different from the original Espresso (Pantel and
Pennacchiotti, 2006).) The number of initial patterns is rel-
atively large because of a data sparseness problem in WSD,
unlike relation extraction and named entity recognition. Also,
WSD basically uses more features than relation extraction and
thus it is hard to determine the stopping criterion based on the
number and scores of patterns, as (Pantel and Pennacchiotti,
2006) does.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20  25  30
re
ca
ll o
f "b
ank
"
iteration
Simplified EspressoFiltered Espresso
most frequent sense (baseline)
Figure 2: Recall of Simplified- and Filtered Espresso
Simplified Espresso tends to select the most fre-
quent sense as the iteration proceeds, and after nine
iterations it selects the most frequent sense (?the
bank of the river?) regardless of the seed instances.
As expected from the discussion in Section 3.2,
generic patterns gradually got more weight and se-
mantic drift occurred in later iterations. Indeed, the
ranking of the instances after convergence was iden-
tical to the HITS authority ranking computed from
instance-pattern matrix M (i.e., the ranking induced
by the dominant eigenvector of MTM ).
On the other hand, Filtered Espresso suffers less
from semantic drift. The final recall achieved
was 0.773 after convergence on the 20th iteration,
outperforming the most-frequent sense baseline by
0.10. However, a closer look reveals that the filter-
ing heuristics is limited in effectiveness.
Figure 3 plots the learning curve of Filtered
Espresso on the set of test instances. We show re-
call ( |correct instances||total true instances| ) of each sense to see how
Filtered Espresso tends to select the most frequent
sense. If semantic drift takes place, the number
of instances predicted as the most frequent sense
should increase as the iteration proceeds, resulting
in increased recall on the most frequent sense and
decreased recall on other senses. Figure 3 exactly
exhibit this trend, meaning that Filtered Espresso is
not completely free from semantic drift. Figure 2
also shows that the recall of Filtered Espresso starts
to decay after the seventh iteration.
1015
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20  25  30
re
ca
ll
iteration
most frequent sense
other senses
Figure 3: Recall of Filtered Espresso on the instances
having ?bank of the river? and other senses
4 Two Graph-based Algorithms for
Exploiting Generic Patterns
We explore two graph-based methods which have
the advantage of Espresso to harness the property of
generic patterns by the mutual recursive definition
of instance and pattern scores. They also have less
parameters than bootstrapping, and are less prone to
semantic drift.
4.1 Von Neumann Kernel
Kandola et al (2002) proposed the von Neumann
kernels for measuring similarity of documents us-
ing words. If we apply the von Neumann kernels to
the pattern-instance co-occurrence matrix instead of
the document-word matrix, the relative importance
of an instance to seed instances can be estimated.
Let A = MTM be the instance similarity matrix
obtained from pattern-instance matrix M , and ? be
the principal eigenvalue of A. The von Neumann
kernel matrix K? with diffusion factor ? (0 ? ? <
??1) is defined as follows:
K? = A
?
?
n=0
?nAn = A(I ? ?A)?1. (8)
The similarity between two instances i, j is given by
the (i, j) element of K? . Hence, the i-th column
vector can be used as the score vector for seed in-
stance i.
Ito et al (2005) showed that the von Neumann
kernels represent a mixture of the co-citation re-
latedness and Kleinberg?s HITS importance. They
compute the weighted sum of all paths between two
nodes in the co-citation graph induced by A =
MTM . The (MTM)n term of smaller n corre-
sponds to the relatedness to the seed instances, and
the (MTM)n term of larger n corresponds to HITS
importance. The von Neumann kernels calculate the
weighted sum of (MTM)n from n = 1 to ?, and
therefore smaller diffusion factor ? results in rank-
ing by relatedness, and larger ? returns ranking by
HITS importance.
In NLP literature, Schu?tze (1998) introduced the
notion of first- and second-order co-occurrence.
First-order co-occurrence is a context which directly
co-occurs with a word, whereas second-order co-
occurrence is a context which occurs with the (con-
textual) words that co-occur with a word. Higher-
order co-occurrence information is less sparse and
more robust than lower-order co-occurrence, and
thus is useful for a proximity measure.
Given these definitions, we see that the (MTM)n
term of smaller n corresponds to lower-order co-
occurrence, which is accurate but sparse, and the
(MTM)n term of larger n corresponds to higher-
order co-occurrence, which is dense but possibly
giving too much weight on unrelated instances ex-
tracted by generic patterns.
As a result, it is expected that setting diffusion
factor ? to a small value prevents semantic drift and
also takes higher order pattern vectors into account.
We verify this claim in Section 5.3.
4.2 Regularized Laplacian Kernel
The von Neumann kernels can be regarded as a mix-
ture of relatedness and importance, and diffusion
factor ? controls the trade-off between relatedness
and importance. In practice, however, setting the
right parameter value becomes an issue. We solve
this problem by the regularized Laplacian (Smola
and Kondor, 2003; Chebotarev and Shamis, 1998),
which are stable across diffusion factors and can
safely benefit from generic patterns.
LetG be a weighted undirected graph whose adja-
cency (weight) matrix is a symmetric matrix A. The
(combinatorial) graph Laplacian L of a graph G is
defined as follows:
L = D ?A (9)
where D is a diagonal matrix, and the ith diagonal
1016
Table 1: Recall of predicted labels of bank
algorithm MFS others
Simplified Espresso 100.0 0.0
Filtered Espresso 100.0 30.2
Filtered Espresso (optimal stopping) 94.4 67.4
von Neumann kernels 92.1 65.1
regularized Laplacian 92.1 62.8
element [D]ii is given by
[D]ii =
?
j
[A]ij . (10)
Here, [A]ij stands for the (i, j) element of A. By re-
placing A with ?L in Equation (8) and deleting the
first A, we obtain a regularized Laplacian kernel 4.
R? =
?
?
n=0
?n(?L)n = (I + ?L)?1 (11)
Again, ?(? 0) is called the diffusion factor.
Both the regularized Laplacian and the von Neu-
mann kernels compute all the possible paths in a
graph, and consequently they can calculate influence
between nodes in a long distance in the graph. Also,
Equations (9) and (10) show that the negative Lapla-
cian ?L can be regarded as a modification to the
graph G with the weight of self-loops re-weighted
to negative values. In this modified graph, if an in-
stance co-occurs with a pattern which also co-occurs
with a large number of other instances, a self-loop
of a node in the instance similarity graph induced
by MTM will receive a higher negative weight.
In other words, instances co-occurring with generic
patterns will get less weight in the regularized Lapla-
cian than in the von Neumann kernels.
5 Experiments and Results
5.1 Experiment 1: Reducing Semantic Drift
We test the von Neumann kernels and the regular-
ized Laplacian on the same task as we used in Sec-
tion 3.3; i.e., word sense disambiguation of word
4It has been reported that normalization of A improves per-
formance in application (Johnson and Zhang, 2007), so we nor-
malize L by L = I ?D?
1
2AD?
1
2 .
?bank.? During the training phase, a pattern-instance
matrix M was constructed using the training and
testing data from Senseval-3 Lexical Sample (S3LS)
Task. The (i, j) element of M of both kernels is set
to pointwise mutual information of a pattern i and
an instance j, just the same as in Espresso. Recall is
used in evaluation.5 The diffusion parameter ? is set
to 10?5 and 10?2 for the von Neumann kernels and
the regularized Laplacian, respectively.
Table 1 illustrates how well the proposed meth-
ods reduce semantic drift, just the same as the ex-
periment of Figure 3 in Section 3.3. We evalu-
ate the recall on predicting the most frequent sense
(MFS) and the recall on predicting other less fre-
quent senses (others). For Filtered Espresso, two
results are shown: the result on the seventh iter-
ation, which maximizes the performance (Filtered
Espresso (optimal stopping)), and the one after con-
vergence. As in Section 3.3, if semantic drift oc-
curs, recall of prediction on the most frequent sense
increases while recall of prediction on other senses
declines. Even Filtered Espresso was affected by se-
mantic drift, which is again a consequence of the
inherent graphical nature of Espresso-like bootstrap-
ping algorithms. On the other hand, both proposed
methods succeeded to balance the most frequent
sense and other senses. Filtered Espresso at the op-
timal number of iterations achieved the best perfor-
mance. Nevertheless, the number of iterations has to
be estimated separately.
5.2 Experiment 2: WSD Benchmark Data
We conducted experiments on the task of word sense
disambiguation of S3LS data, this time not just on
the word ?bank? but on all target nouns in the data,
following (Agirre et al, 2006). We used two types
of patterns.
Unordered single words (bag-of-words) We
used all single words (unigrams) in the provided
context from S3LS data sets. Each word in the con-
text constructs one pattern. The pattern correspond-
ing to a word w is set to 1 if it appears in the con-
text of instance i. Words were lowercased and pre-
processed with the Porter Stemmer6.
5Again, recall equals precision in this case as the coverage
is 100% in all cases.
6http://tartarus.org/?martin/PorterStemmer/def.txt
1017
Table 2: Comparison of WSD algorithms
algorithm Recall
most frequent sense 54.5
HyperLex (Ve?ronis, 2004) 64.6
PageRank (Agirre et al, 2006) 64.5
Simplified Espresso 44.1
Filtered Espresso 46.9
Filtered Espresso (optimal stopping) 66.5
von Neumann kernels (? = 10?5) 67.2
regularized Laplacian (? = 10?2) 67.1
Local collocations A local collocation refers to
the ordered sequence of tokens in the local, narrow
context of the target word. We allowed a pattern to
have wildcard expressions like ?sale of * interest in
* *? for the target word interest. We set the window
size to ?3 by a preliminary experiment.
We report the results of Filtered Espresso both af-
ter convergence, and with its optimal number of iter-
ations to show the upper bound of its performance.
Table 2 compares proposed methods with
Espresso with various configurations. The proposed
methods outperform by a large margin the most fre-
quent sense baseline and both Simplified- and Fil-
tered Espresso. This means that the proposed meth-
ods effectively prevent semantic drift.
Also, Filtered Espresso without early stopping
shows more or less identical performance to Sim-
plified Espresso. It is implied that the heuristics of
filtering and early stopping is a crucial step not to
select generic patterns in Espresso, and the result is
consistent with the experiment of convergence pro-
cess of Espresso in Section 3.3.
Filtered Espresso halted after the seventh itera-
tion (Filtered Espresso (optimal stopping)) is com-
parable to the proposed methods. However, in boot-
strapping, not only the number of iterations but also
a large number of parameters must be adjusted for
each task and domain. This shortcoming makes it
hard to adapt bootstrapping in practical cases. One
of the main advantages of the proposed methods is
that they have only one parameter ? and are much
easier to tune.
It is suggested in Sections 3.3 and 4.1 that
Espresso and the von Neumann kernel with large ?
 40
 45
 50
 55
 60
 65
 70
 75
 1e-07  1e-06  1e-05  0.0001  0.001
re
ca
ll
diffusion factor  
von Neumann kernelSimplified Espresso
most frequent sense
Figure 4: Recall of the von Neumann kernels with a dif-
ferent diffusion factor ? on S3LS WSD task
converge to the principal eigenvector of A, though
the result does not seem to support this claim (both
Simplified- and Filtered Espresso are 10 points
lower than the most frequent sense baseline). The
reason seems to be because Espresso and the von
Neumann kernels use pointwise mutual information
as a weighting factor so that the principal eigenvec-
tor of A may not always represent the most frequent
sense.7
We also show the results of previous graph-based
methods (Agirre et al, 2006), based on Hyper-
Lex (Ve?ronis, 2004) and PageRank (Brin and Page,
1998). The experimental set-up is the same as ours
in that they do not use the sense tags of training cor-
pus to construct a co-occurrence graph, and they use
the sense tags of all the S3LS training corpus for
mapping senses to clusters. However, these meth-
ods have seven parameters to tune in order to achieve
the best performance, and hence are difficult to opti-
mize.
5.3 Experiment 3: Sensitivity to a Different
Diffusion Factor
Figure 4 shows the performance of the von Neu-
mann kernels with a diffusion factor ?. As ex-
pected, smaller ? leads to relatedness to seed in-
stances, and larger ? asymptotically converges to the
HITS authority ranking (or equivalently, Simplified
7A similar but more extreme case is described in (Ito et al,
2005) in which the use of a normalized weight matrixM results
in an unintuitive principal eigenvector.
1018
 40
 45
 50
 55
 60
 65
 70
 75
 0.001  0.01  0.1  1  10  100  1000
re
ca
ll
diffusion factor  
regularized Laplacian
most frequent sense
Figure 5: Recall of the regularized Laplacian with a dif-
ferent diffusion factor ? on S3LS WSD task
Espresso).
One of the disadvantages of the von Neumann
kernels over the regularized Laplacian is their sen-
sitivity to parameter ?. Figure 5 illustrates the per-
formance of the regularized Laplacian with a diffu-
sion factor ?. The regularized Laplacian is stable for
various values of ?, while the von Neumann kernels
change their behavior drastically depending on the
value of ?. However, ? in the von Neumann kernels
is upper-bounded by the reciprocal 1/? of the prin-
cipal eigenvalue of A, and the derivatives of kernel
matrices with respect to ? can be used to guide sys-
tematic calibration of ? (see (Ito et al, 2005) for
detail).
6 Conclusion and Future Work
This paper gives a graph-based analysis of seman-
tic drift in Espresso-like bootstrapping algorithms.
We indicate that semantic drift in bootstrapping is a
parallel to topic drift in HITS. We confirm that the
von Neumann kernels and the regularized Laplacian
reduce semantic drift in the Senseval-3 Lexical Sam-
ple task. Our proposed methods have only one pa-
rameters and are easy to calibrate.
Beside the regularized Laplacian, many other ker-
nels based on the eigenvalue regularization of the
Laplacian matrix have been proposed in machine
learning community (Kondor and Lafferty, 2002;
Nadler et al, 2006; Saerens et al, 2004). One such
kernel is the commute-time kernel (Saerens et al,
2004) defined as the pseudo-inverse of Laplacian.
Despite having no parameters at all, it has been re-
ported to perform well in many collaborative filter-
ing tasks (Fouss et al, 2007). We plan to test these
kernels in our task as well.
Another research topic is to investigate other
semi-supervised learning techniques such as co-
training (Blum and Mitchell, 1998). As we have
described in this paper, self-training can be thought
of a graph-based algorithm. It is also interesting to
analyze how co-training is related to the proposed
algorithm.
Bootstrapping algorithms have been used in many
NLP applications. Two major tasks of bootstrap-
ping are word sense disambiguation and named en-
tity recognition. In named entity recognition task,
instances are usually retained on each iteration and
added to seed instance set. This seems to be be-
cause named entity recognition suffers from seman-
tic drift more severely than word sense disambigua-
tion. Even though this problem setting is different
from ours, it needs to be verified that the graph-
based approaches presented in this paper are also ef-
fective in named entity recognition.
Acknowledgements
We thank anonymous reviewers for helpful com-
ments and for making us aware of Abney?s work.
The first author is partially supported by the Japan
Society for Promotion of Science (JSPS), Grant-in-
Aid for JSPS Fellows.
References
Steven Abney. 2004. Understanding the Yarowsky Al-
gorithm. Computational Linguistics, 30(3):365?395.
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 585?593.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
26?33.
Krishna Bharat and Monika R. Henzinger. 1998. Im-
proved algorithms for topic distillation in a hyper-
linked environment. In Proceedings of the 21st ACM
SIGIR Conference.
1019
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the Workshop on Computational Learning
Theory (COLT), pages 92?100. Morgan Kaufmann.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Pavel Yu Chebotarev and Elena V. Shamis. 1998. On
proximity measures for graph vertices. Automation
and Remote Control, 59(10):1443?1459.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora, pages 100?110.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual Exclu-
sion Bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172?180.
Franc?ois Fouss, Luh Yen, Pierr Dupont, and Marco
Saerens. 2007. Random-walk computation of simi-
larities between nodes of a graph with application to
collaborative recommendation. IEEE Transactions on
Knowledge and Data Engineering, 19(3):355?369.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, pages 539?545.
Takahiko Ito, Masashi Shimbo, Taku Kudo, and Yuji
Matsumoto. 2005. Application of Kernels to
Link Analysis. In Proceedings of the Eleventh
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 586?592.
Rie Johnson and Tong Zhang. 2007. On the Effec-
tiveness of Laplacian Normalization for Graph Semi-
supervised Learning. Journal of Machine Learning
Research, 8:1489?1517.
Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.
2002. Learning Semantic Similarity. In Advances
in Neural Information Processing Systems 15, pages
657?664.
Jon Kleinberg. 1999. Authoritative Sources in a Hyper-
linked Environment. Journal of the ACM, 46(5):604?
632.
Mamoru Komachi and Hisami Suzuki. 2008. Mini-
mally Supervised Learning of Semantic Knowledge
from Query Logs. In Proceedings of the 3rd Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 358?365.
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
Proceedings of the 19th International Conference on
Machine Learning (ICML-2002).
Hang Li and Cong Li. 2004. Word Translation Disam-
biguation Using Bilingual Bootstrapping. Computa-
tional Linguistics, 30(1):1?22.
Boaz Nadler, Stephane Lafon, Ronald Coifman, and
Ioannis Kevrekidis. 2006. Diffusion maps, spectral
clustering and eigenfunctions of fokker-planck opera-
tors. Advances in Neural Information Processing Sys-
tems 18, pages 955?962.
Vincent Ng and Claire Cardie. 2003. Weakly Su-
pervised Natural Language Learning Without Redun-
dant Views. In Proceedings of the HLT-NAACL 2003,
pages 94?101.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL, pages
113?120.
Ellen Riloff and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level
Bootstrapping. In Proceedings of the Sixteenth Na-
tional Conference on Artificial Intellligence (AAAI-
99), pages 474?479.
Marco Saerens, Franc?ois Fouss, Luh Yen, and Pierre
Dupont. 2004. The principal component analysis
of a graph, and its relationship to spectral clustering.
In Proceedings of European Conference on Machine
Learning (ECML 2004), pages 371?383. Springer.
Heinrich Schu?tze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1):97?
123.
Alex J. Smola and Risi Imre Kondor. 2003. Kernels and
Regularization of Graphs. In Proceedings of the 16th
Annual Conference on Learning Theory, pages 144?
158.
Jean Ve?ronis. 2004. HyperLex: Lexical Cartography for
Information Retrieval. Computer Speech & Language,
18(3):223?252.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189?196.
1020
Generic Text Summarization Using  
Probabilistic Latent Semantic Indexing 
Harendra Bhandari 
Graduate School of Information Science 
     Nara Institute of Science and Technology 
Nara 630-0192, Japan 
harendra-b@is.naist.jp 
 
Takahiko Ito 
Graduate School of Information Science 
Nara Institute of Science and Technology 
Nara 630-0192, Japan 
takahahi-i@is.naist.jp 
 
Masashi Shimbo 
Graduate School of Information Science 
Nara Institute of Science and Technology 
Nara  630-0192, Japan 
shimbo@is.naist.jp 
 
Yuji Matsumoto 
Graduate School of Information Science 
Nara Institute of Science and Technology 
Nara 630-0192, Japan 
matsu@is.naist.jp 
 
 
 
Abstract 
This paper presents a strategy to generate ge-
neric summary of documents using Probabilistic 
Latent Semantic Indexing. Generally a docu-
ment contains several topics rather than a single 
one. Summaries created by human beings tend 
to cover several topics to give the readers an 
overall idea about the original document. Hence 
we can expect that a summary containing sen-
tences from better part of the topic spectrum 
should make a better summary. PLSI has 
proven to be an effective method in topic detec-
tion. In this paper we present a method for cre-
ating extractive summary of the document by 
using PLSI to analyze the features of document 
such as term frequency and graph structure. We 
also show our results, which was evaluated us-
ing ROUGE, and compare the results with other 
techniques, proposed in the past.  
1 Introduction 
The advent of the Internet has made a wealth of 
textual data available to everyone. Finding a spe-
cific piece of information in this mass of data can 
be compared with "finding a small needle in a large 
heap of straw." Search engines do a remarkable job 
in providing a subset of the original data set which 
is generally a lot smaller than the original pile of 
data. However the subset provided by the search 
engines is still substantial in size. Users need to 
manually scan through all the information con-
tained in the list of results provided by the search 
engines until the desired information is found. This 
makes automatic summarization the task of great 
importance as the users can then just read the 
summaries and obtain an overview of the document, 
hence saving a lot of time during the process. 
Several methods have been proposed in the field 
of automatic text summarization. In general two 
approaches have been taken, extract-based summa-
rization and abstract-based summarization. While 
extract-based summarization focuses in finding 
relevant sentences from the original document and 
using the exact sentences as a summary, abstract-
based summaries may contain the words or phrases 
not present in the original document (Mani, 1999). 
The summarization task can also be classified as 
query-oriented or generic. The query-oriented 
summary presents text that contains information 
relevant to the given query, and the generic sum-
marization method presents the summary that gives 
overall sense of the document (Goldstein et al 
1998). In this paper, we will focus on extract-based 
generic single-document summarization. 
In the recent years graph based techinques have 
become very popular in automatic text summariza-
133
tion (Erkan and Radev, 2004), (Mihalcea, 2005). 
These techniques view each sentence as a node of a 
graph and the similarities between each sentences 
as the links between those sentences. Generally the 
links are retained only if the similarity values be-
tween the sentences exceed a pre-determined 
threshold value; the links are discarded otherwise. 
The sentences are then ranked using some graph 
ranking algorithms such as HITS (Kleinberg, 1998) 
or PageRank (Brin and Page, 1998) etc. However 
the graph ranking algorithms tend to give the high-
est ranking to the sentences related to one central 
topic in the document. So if a document contains 
several topics, these algorithms will only choose 
one central topic and rank the sentences related to 
those topic higher than any other topics, ignoring 
the importance of other topics present. This will 
create summaries that may not cover the overall 
topics of the document and hence cannot be con-
sidered generic enough. We will focus on that 
problem and present a way to create better generic 
summary of the document using PLSI (Hofmann 
1999) which covers several topics in the document 
and is closer to the summaries created by human 
beings. The benchmarking done using DUC2 2002 
data set showed that our technique improves over 
other proposed methods in terms of ROUGE1 
evaluation score. 
 
2 Related Work 
 
2.1 Maximal Marginal Relevance(MMR) 
MMR is a summarization procedure based on vec-
tor-space model and is suited to generic summari-
zation (Goldstein et al 1999). In MMR the sen-
tence are chosen according to the weighed combi-
nation of their general relevance in the document 
and their redundancy with the sentences already 
chosen. Both the relevance and redundancy are 
measured using cosine similarity. Relevance is the 
cosine similarity of a sentence with rest of the sen-
tence in the document whereas  redundancy is 
measured using cosine similarity between the sen-
tence and the sentences already chosen for the 
summary. 
 
2.2 Graph Based Summarization 
The graph-based summarization procedure are be 
_________________________________________ 
1 ROUGE:http://openrouge.com/default.aspx 
2 http://duc.nist.gov 
coming increasingly popular in recent years. Lex 
PageRank (Erkan and Radev, 2004) is one of such 
methods. LexPageRank constructs a graph where 
each sentence is a node and links are the similari-
ties between the sentences. Similarity is measured 
using cosine similarity of the word vectors, and if 
the similarity value is more than certain threshold 
value the link is kept otherwise the links are re-
moved. PageRank is an algorithm which has been 
successfully applied by Google search engine to 
rank the search results. Similarly PageRank is ap-
plied in LexPageRank to rank the nodes (or, sen-
tences) of the resultant graph. A similar summari-
zation method has been proposed by Mihalcea 
(2005).  
    Algorithms like HITS and PageRank calculate 
the principal eigenvector (hence find the principal 
community) of the matrix representing the graph. 
But as illustrated in Figure 1, another eigenvector 
which is slightly smaller than the principal eigen-
vector may exist. In documents, each community 
represented by the eigenvectors can be considered 
as a topic present in the document. As these algo-
rithms tend to ignore the influence of eigenvectors 
other than largest one, the sentences related to top-
ics other than a central one can be ignored, and 
creating the possibility for the inclusion of redun-
dant sentences as well. This kind of summary can-
not be considered as a generic one. 
 
 
Figure 1. In algorithms like HITS and PageRank 
only the principal eigenvectors are considered. In 
the figure the vector EV1 is slightly larger than 
vector EV2, but the score commanded by members 
of EV2 communities are ignored. 
 
As we mentioned in section 1, we take into consid-
eration the sentences from all the topics generated 
by PLSI in the summary, hence getting a more ge-
neric summary.  
 
2.3 Latent Semantic Analysis 
Latent Semantic Analysis (LSA) (Deerwester et al, 
EV1 
EV2 
134
1990) takes the high dimensional vector space rep-
resentation of the document based on term fre-
quency and projects it to lesser dimension space. It 
is thought that the similarities between the docu-
ments can be more reliably estimated in the re-
duced latent space representation than original rep-
resentation. LSA has been applied in areas of text 
retrieval (Deerwester et al, 1990) and automatic 
text summarization (Gong and Liu, 2001). LSA is 
based on Singular Value Decomposition (SVD) of 
m?n term-document matrix A. Each entry in A, Aij, 
represents the frequency of term i in document j. 
Using SVD, the matrix A is decomposed into 
U,S,V as, 
A=USVT              
U=Matrix of n left singular vectors 
S=diag(?i)=Diagonal matrix of singular values 
where with ?i? ?i+1 for all i. 
VT=Matrix of right singular vectors. Each  
       row represents a topic and the values in each 
           row represent the score of  documents,  
           represented by each columns, for the topic  
           represented by the row. 
 Gong and Liu (2001) have proposed a scheme for 
automatic text summarization using LSA. Their 
algorithm can be stated below. 
a. Choose the highest ranked sentence from 
kth right singular vector in matrix VT and 
use the sentence in summary. 
b. If k reaches the predefined number, termi-
nate the process; otherwise, go to step a 
again. 
LSA categorizes sentences on the basis of the top-
ics they belong to. Gong and Liu?s method picks 
sentences from various topics hence producing the 
summaries that are generic in nature. 
In section 3 we explain how PLSI is more ad-
vanced form of LSA. In section 5, we compare our 
summarization results with that of LSA. 
 
3 Probabilistic Latent Semantic Indexing  
Probabilistic Latent Semantic Indexing (PLSI) 
(Hofmann, 1999) is a new approach to automated 
document indexing, and is based on a statistical 
latent class model for factor analysis of count data. 
PLSI is considered to be a probabilistic analogue of 
Latent Semantic Indexing (LSI), which is a docu-
ment indexing technique based on LSA. Despite 
the success of LSI, it is not devoid of deficits. The 
main argument against LSI is pointed to its unsatis-
factory statistical foundations. In contrast, PLSI has 
solid statistical foundations, as it is based on the 
maximum likelihood principle and defines a proper 
generative model of data.  Hofmann (1999) has 
shown that PLSI indeed performs better                         
than LSI in several text retrieval experiments. The 
factor representation obtained in PLSI allows us to 
classify sentences according to the topics they be-
long to. We will use this ability of PLSI to generate 
summary of document that are more generic in na-
ture by picking sentences from different topics. 
 
4 Summarization with PLSI  
4.1 The Latent Variable Model for Document 
Our document model is similar to Aspect Model 
(Hofmann et al 1999, Saul and Pereira, 1997) used                         
by Hoffman (1999). The model attempts to associ-
ate an unobserved class variable z?Z={z1, ..., zk} 
(in our case the topics contained in the document), 
with two sets of observables, documents (d?
D={d1,?..dm}, sentences in our case) and words (w
?W={w1,?,wn}) contained in documents. In terms 
of generative model it can be defined as follows: 
    -A document d is selected with probability P(d) 
    -A latent class z is selected with probability 
P(z|d) 
    -A word w is selected with probability P(w|z) 
For each document-word pair (d,w), the likelihood 
for each pair can be represented as 
P(d,w)=P(d)P(w|d)=P(d) ?
z
P(w|z)P(z|d).                 
Following the maximum likelihood principle P(d), 
P(z|d), P(w|z) are determined by the maximization 
of of log-likelihood function, 
    L= ?
d
?
w
n(d,w)logP(d,w)                                    
where n(d,w) denotes the term frequency, i.e., the 
number of time w occurred in d.  
 
4.2 Maximizing Model Likelihood 
 
Expectation Maximization (EM) is the standard 
procedure for maximizing  likelihood estimation in 
the presence of latent variables. EM is an iterative 
procedure and each of the iteration contains two 
steps. (a) An Expectation (E) step, where the poste-
rior probabilities for latent variable z are computed 
and (b) Maximization (M) step, where parameters 
for given posterior probabilities are computed. 
  The aspect model can be re-parameterized using 
the Bayes? rule as follows: 
135
  P(d,w)= ?
z
P(z) P(d|z) P(w|z) .                           
Then using the re-parameterized equation the E-
step calculates the posterior for z by 
    
'
( ) ( | ) ( | )
( ') ( | ') ( | ')
( | , )
z
P z P d z P w z
P z P d z P w zP z d w = ?                        
This step calculates the probability that word w 
present in document d can be described by the fac-
tor corresponding to z. Subsequently, the M-step 
re-evaluates the parameters using following equa-
tions. 
  
, '
( , ) ( | , )
( , ') ( | , ')
( | ) ,d
d w
n d w P z d w
n d w P z d wP w z =
?
?                               (1) 
',
( , ) ( | , )
( ', ) ( | ', )
( | ) ,w
d w
n d w P z d w
n d w P z d wP d z =
?
?                               (2) 
,
,
( , ) ( | , )
( , )
( ) d w
d w
n d w P z d w
n d wP z =
?
?                                       (3) 
                
Alternating the E- and M- steps one approaches a 
converging point which describes local maximum 
of the log-likelihood. 
   We used the tempered EM (TEM) as described 
by Hofmann (1999). TEM basically introduces a 
control parameter B, upon which the E-step is 
modified as,  
'
( )[ ( | ) ( | )]
( ')[ ( | ') ( | ')]
( | , )
B
B
z
P z P d z P w z
P z P d z P w z
P z d w = ?             (4) 
The TEM reduces to original EM if B=1. 
 
4.3 Summarization procedure 
 
We applied PLSI in 4 different ways during the 
summarization process. We will denote each of the 
4 ways as PROC1, PROC2, PROC3, PROC4. 
Each of the four summarization procedure is dis-
cussed below. 
 
PROC1 (Dominant topic only): PROC1 consists of 
the following steps: 
a. Each document is represented as term-
frequency matrix. 
b. P(w|z), P(d|z), and P(z) (as in (1), (2), (3)) are 
calculated until the convergence criteria for 
EM-algorithm is met. P(d|z) represents the im-
portance of document d in given topic repre-
sented by z and P(z) represents the importance 
of the topic z itself in the document d. 
c. z with highest probability P(z) is picked as the 
central topic of the document and then the sen-
tences with highest P(d|z) score contained in 
selected topic are picked. 
d. The top scoring sentences are used in the 
summary. 
PROC2 (Dominant topic only): PROC2 is the graph 
based method. PROC2 is similar to PROC1 except 
for the fact that instead of using term-frequency 
matrix we use sentence-similarity matrix. Sen-
tence-similarity matrix A is n?n matrix where n is 
the number of sentences present in the document. 
Cosine similarity of each sentence present in the 
document with respect to all the sentences is calcu-
lated. The cosine-similarity values calculated are 
used instead of term-frequency values as in PROC1. 
Each entry Aij in matrix A is 0 if the cosine similar-
ity value between sentence i and sentence j is less 
than threshold value and 1 if greater. We used 0.2 
as the threshold value in our experiments after 
normalizing cosine similarity value. Steps b, c, d 
from PROC1 are followed after the initial proce-
dure is complete. 
    This method is analogous to PHITS (Cohn and 
Chang (2001)) method where the authors utilized 
PLSI to find communities in hyperlinked environ-
ment. 
PROC3 (Multiple topics): In both PROC1 and 
PROC2 we did not take the advantage of the fact 
that PLSI divides a document into several topics. 
We only used the sentences from highest ranked 
topic. In PROC3 we attempt to combine the sen-
tences from different topics while forming the 
summary. PROC3 can be explained in the follow-
ing steps. 
a. Steps a and b from PROC1 are taken as normal. 
b. We mentioned that P(d|z) represents the score 
of the sentence d in topic z. In this procedure 
we will create new score R for each sentence 
using following relation. 
          R=?
z
P(d|z)P(z)=P(d)    
 
 
                           
136
This will essentially score the sentences with ge-
neric values or the sentences which have good in-
fluence ranging over several topics better. 
c. We pick the sentences that score highest score 
R as the summary. 
PROC3 will pick sentences from several topics 
resulting in better generic summary of the docu-
ment. 
 
 
PROC4 (Multiple Topics): PROC4 is essentially 
PROC3 except for the first few steps. PROC4 does 
not use the matrix created in PROC1 instead it uses 
the similarity-matrix produced in PROC2. Once the 
similarity matrix is created P(z) and P(d|z) are cal-
culated as in step b of PROC1. Then steps b and c 
of PROC3 are taken to produce the summary of the 
document. 
 
5 Experiments and Results 
We produced summaries for all the procedures 
mentioned in section 4.3. We used DUC 2002 data 
set for summarization. DUC 2002 contains test data 
for both multiple document and single document 
summarization. It also contains summaries created 
by human beings for both single document and 
multiple document summarization. Our focus in 
this paper is single document summarization.  
After creating summaries we evaluated summa-
ries using ROUGE. ROUGE has been the standard 
benchmarking technique for summarization tasks 
adopted by Document Understanding Conference 
(DUC). We also compared our results with other 
summarization methods such as LexPageRank (Er-
kan and Radev, 2004) and Gong and Liu?s (2001) 
LSA-based method. We also compared the results 
with HITS based method which is similar to Lex-
PageRank but instead of PageRank, HITS is used 
as ranking algorithm (Klienberg 1998).The results 
are listed in Table 1. 
  We used five measures for evaluation, Rouge-L 
Rouge1, Rouge2, Rouge-SU4 and F1. These meth-
ods are standard methods used in DUC evaluation  
 
Table 1: Evaluation of summaries  
The table shows the score of summaries generated using methods described in section 4.3. On the table 
n means number of topics into which the document has been divided into. Control parameter B from (4) 
was fixed to 0.75 in this case. 
 
Method Used n 
ROUGE-L 
(recall) Rouge1 Rouge-2 Rouge-SU4 
PROC1 2 0.499 0.557 0.242 0.272 
PROC2 2 0.465 0.515 0.227 0.253 
2 0.571 0.634 0.291 0.321 
3 0.571 0.628 0.288 0.318 
4 0.571 0.62 0.28 0.31 
5 0.571 0.613 0.274 0.305 
PROC3 6 0.5 0.612 0.27 0.302 
2 0.473 0.508 0.225 0.25 
3 0.472 0.504 0.22 0.245 
4 0.472 0.5 0.219 0.244 
5 0.472 0.492 0.213 0.238 
PROC4 6 0.471 0.483 0.207 0.231 
Compared Methods 
*LexPageRank   0.522 0.577 0.265 0.291 
*LSA   0.414 0.463 0.186 0.215 
*HITS   0.504            0.562                0.251                    0.282 
137
tests and these schemes are known to be very effect 
tive to calculate the correlation between the sum-
maries. All of the scores can be calculated using 
Rouge package. Rouge is based on N-gram statis-
tics (Lin and Hovy, 2003). Rouge has been known 
to highly correlate with human evaluations. Ac-
cording to (Lin and Hovy, 2003), among the meth-
ods implemented in ROUGE, ROUGE-N (N=1,2), 
ROUGE-L, ROUGE-S are relatively simple and 
work very well even when the length of summary 
is quite short, which is mostly the case in single 
document summarization. ROUGE-N, ROUGE-L 
and ROUGE-S are all basically the recall scores. 
As DUC keeps the length of the summaries con-
stant recall is the main evaluation criterion. F-
measure is also shown in the table as a reference 
parameter, but since we kept the length of our 
summaries constant, too, the ROUGE-L, ROUGE-
N and ROUGE-S scores carry the highest weight.  
 As seen on Table 1, the scores gained by PROC1 
and PROC2 are less than others. This is mainly 
because the sentences chosen by these methods 
were simply chosen from one topic. As PROC3 
and PROC4 use sentences from several topics the 
score of PROC3 and PROC4 were better than 
PROC1 and PROC2. For methods PROC3 and 
PROC4 we took the summaries for topics 2 
through 6 and found that the method performed 
well when the number of topics was kept between 
2 to 4. But the difference was very small, and in 
general the performance was quite stable. 
 We also compared our results to other methods 
such as LexPageRank and LSA and found that 
PROC3 performed quite well when compared to 
those methods. LexPageRank was marginally bet-
ter in F-measure (F1) but PROC3 got best recall 
scores. PROC3 also outperformed LSA by 0.16 in 
recall (ROUGE-L) scores. Comparison to HITS 
also shows PROC3 more advantageous. 
 
6 Discussion 
  In this paper we have argued that choosing sen-
tences from multiple topics makes a better generic 
summary. It is especially true if we compare our 
method to graph based ranking methods like HITS 
and PageRank. Richardson and Domingos (2002) 
have mentioned that both HITS and PageRank suf-
fer from the topic drift. This not only makes these 
algorithms susceptible for exclusion of important 
sentences outside the main topic but miss the sen-
tences from main topic as well. Cohn and Chang 
(2001) also have shown similar results for HITS. 
They (Cohn and Chang) have shown that the cen-
tral topic identified by HITS (principal eigenvec-
tor) may not always correspond to the most au-
thoritative topic. The main topic in fact may be 
represented by smaller eigenvectors rather than the 
principal one. They also show that the topic segre-
 
Effect of parameter B on scores
0.567
0.5675
0.568
0.5685
0.569
0.5695
0.57
0.5705
0.571
0.5715
0.572
0.5725
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Temperature(B)
R
ou
ge
-L
 S
co
re
n=2
n=3
n=4
 
Figure 2: Effect of tempering factor B in the ROUGE-L score for PROC3. 
138
gation in HITS is quite extreme so if we just use 
principal eigenvector, first there is a chance of be-
ing drifted away from the main topic hence produc-
ing low quality summary and there is also a chance 
of missing out other important topics due to the 
extreme segregation of communities. In PLSI the 
segregation of topics is not as extreme. If a sen-
tence is related to several topics the sentence can 
attain high rank in many topics.   
  We can see from the scores that the performance 
of graph based algorithms like LexPageRank and 
HITS are not as good as our method. This can be 
attributed to the fact that the graph based summar-
izers take only a central topic under consideration.  
The method that proved most successful in our 
summarization was the one where we extracted the 
sentences that had the most influence in the docu-
ment. 
 We used the tempered version of EM-algorithm 
(4) in our summarization task. We evaluated the 
effect of tempering factor B in performance of 
summarization for PROC3. We found that that the 
tempering factor did not influence the results by a 
big margin. We conducted our experiment using 
values of B from 0.1 through 1.0 incrementing each 
step by 0.1. The results are shown in Figure 2. In 
the results shown in Table 1 the value for temper-
ing factor was set to 0.75. 
 
 
7 Conclusion and Future Work 
In this paper we presented a method for creating 
generic summaries of the documents using PLSI. 
PLSI allowed us classify the sentences present in 
the document into several topics. Our summary 
included sentences from all the topics, which made 
the generation of generic summary possible. Our 
experiments showed that the results we obtained in 
summarization tasks were better than some other 
methods we compared with. LSA can also be used 
to summarize documents in similar manner by ex-
tracting sentences from several topics, but our ex-
periments showed that PLSI performs better than 
LSA. In the future we plan to investigate how more 
recent methods such as LDA (Blei et al perform in 
document summarization tasks. We also plan to 
apply our methods to multiple document summari-
zation.  
 
8 Acknowledgement 
We pay our special gratitude to the reviewers who 
have taken their time to give very useful comments 
on our work. The comments were very useful for 
us to as we were able to provide wider perspective 
on our work with the help of those comments. 
 
References: 
Blei D, Ng A, and Jordan M.2003. Journal of Ma-
chine Learning Research 3 993-1022. 
 
Brin S and Page L.1998. The Anatomy of a Large-
Scale Hypertextual Web Search Engine. Computer 
Networks 30(1-7): 107-117. 
 
Carbonell J and Goldstein J.1998. The Use of 
MMR, Diversity-Based Reranking for Reordering 
Documents and Producing Summaries. Proc. ACM 
SIGIR 
 
Cohn D, Chang H.2001. Learning to probabilisti-
vally identify authoritative documents. Proceedings 
of 18th International Conference of Machine Learn-
ing. 
 
Deerwester S, Dumais ST, Furnas GT, Landauer 
TK, and Harshman R.1990. Indexing by Latent 
Semantic Analysis. Journal of the American Soci-
ety of Information Science. 
 
Erkan G and Radev DR.2004. LexPageRank: Pres-
tige in Multi-Document Text Summariza-
tion.EMNLP. 
 
Gong Y and Liu X.2001.Generic text Summariza-
tion using relevance measure and latent semantic 
analysis.Proc ACM SIGIR. 
 
Hofmann, T.1999.Probabilistic Latent Semantic 
Indexing. Twenty Second International ACM-
SIGIR Conference on Informatioln Retrieval. 
 
Hofmann, et al1998. Unsupervised Learning from 
Dyadic Data. Technical Report TR-98-042, Inter-
national Computer Science Insitute, Berkeley, CA. 
 
Kleinberg J.1998. Authoritative sources in a hyper-
linked environment. Proc. 9th ACM-SIAM Sym-
posium on Discrete Algorithms. 
 
 
 
139
Mani I. 1999. Advances in Automatic Text Sum-
marization. MIT Press, Cambridge, MA, USA. 
 
Mihalcea R.2005.Language Independent Extractive 
Summarization. AAAI 
 
 Richardson M, Domingos P.2002. The Intelligent 
Surfer: Probabilistic Combination of Link and Con-
tent Information in PageRank. Advances in Neural 
Information Processing Systems 14 
 
Saul L and Pereria F.1997.Aggregate and mixed-
order Markov models for statistical language proc-
essing.Proc 21st ACM-SIGIR International Confer-
ence on Research and Development in Information 
Retrieval. 
 
 
 
 
140
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 967?975,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Coordinate Structure Analysis with Global Structural Constraints and
Alignment-Based Local Features
Kazuo Hara Masashi Shimbo Hideharu Okuma Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
{kazuo-h,shimbo,hideharu-o,matsu}@is.naist.jp
Abstract
We propose a hybrid approach to coor-
dinate structure analysis that combines
a simple grammar to ensure consistent
global structure of coordinations in a sen-
tence, and features based on sequence
alignment to capture local symmetry of
conjuncts. The weight of the alignment-
based features, which in turn determines
the score of coordinate structures, is op-
timized by perceptron training on a given
corpus. A bottom-up chart parsing al-
gorithm efficiently finds the best scor-
ing structure, taking both nested or non-
overlapping flat coordinations into ac-
count. We demonstrate that our approach
outperforms existing parsers in coordina-
tion scope detection on the Genia corpus.
1 Introduction
Coordinate structures are common in life sci-
ence literature. In Genia Treebank Beta (Kim et
al., 2003), the number of coordinate structures is
nearly equal to that of sentences. In clinical pa-
pers, the outcome of clinical trials is typically de-
scribed with coordination, as in
Median times to progression and median
survival times were 6.1 months and 8.9
months in arm A and 7.2 months and 9.5
months in arm B. (Schuette et al, 2006)
Despite the frequency and implied importance
of coordinate structures, coordination disambigua-
tion remains a difficult problem even for state-of-
the-art parsers. Figure 1(a) shows the coordinate
structure extracted from the output of Charniak
and Johnson?s (2005) parser on the above exam-
ple. This is somewhat surprising, given that the
symmetry of conjuncts in the sentence is obvious
to human eyes, and its correct coordinate structure
shown in Figure 1(b) can be readily observed.
6.1 
months and
8.9 
months
in 
arm A
7.2 
months
and 9.5 
months
in
arm Band
6.1 
months
and 8.9 
months
in 
arm A
7.2 
months
and 9.5 
months
in 
arm Band
(b)
(a)
Figure 1: (a) Output from the Charniak-Johnson
parser and (b) the correct coordinate structure.
Structural and semantic symmetry of conjuncts
is one of the frequently observed features of coor-
dination. This feature has been explored by previ-
ous studies on coordination, but these studies often
dealt with a restricted form of coordination with
apparently too much information provided from
outside. Sometimes it was assumed that the co-
ordinate structure contained two conjuncts each
solely composed of a few nouns; and in many
cases, the longest span of coordination (e.g., outer
noun phrase scopes) was given a priori. Such rich
information might be given by parsers, but this is
still an unfounded assumption.
In this paper, we approach coordination by tak-
ing an extreme stance, and assume that the input is
a whole sentence with no subsidiary information
except for the parts-of-speech of words.
As it assumes minimal information about syn-
tactic constructs, our method provides a baseline
for future work exploiting deeper syntactic infor-
mation for coordinate structure analysis. More-
over, this stand-alone approach has its own merits
as well:
1. Even apart from parsing, the output coordi-
nate structure alone may provide valuable in-
formation for higher-level applications, in the
same vein as the recent success of named
entity recognition and other shallow parsing
967
technologies. One such potential application
is extracting the outcome of clinical tests as
illustrated above.
2. As the system is designed independently
from parsers, it can be combined with any
types of parsers (e.g., phrase structure or de-
pendency parsers), if necessary.
3. Because coordination bracketing is some-
times inconsistent with phrase structure
bracketing, processing coordinations apart
from phrase structures might be beneficial.
Consider, for example,
John likes, and Bill adores, Sue.
(Carston and Blakemore, 2005)
This kind of structure might be treated by as-
suming the presence of null elements, but the
current parsers have limited ability to detect
them. On the other hand, the symmetry of
conjuncts, John likes and Bill adores, is rather
obvious and should be easy to detect.
The method proposed in this paper builds a
tree-like coordinate structure from the input sen-
tence annotated with parts-of-speech. Each tree
is associated with a score, which is defined in
terms of features based on sequence alignment be-
tween conjuncts occurring in the tree. The feature
weights are optimized with a perceptron algorithm
on a training corpus annotated with the scopes of
conjuncts.
The reason we build a tree of coordinations is to
cope with nested coordinations, which are in fact
quite common. In Genia Treebank Beta, for ex-
ample, about 1/3 of the whole coordinations are
nested. The method proposed in this paper im-
proves upon our previous work (Shimbo and Hara,
2007) which also takes a sentence as input but is
restricted to flat coordinations. Our new method,
on the other hand, can successfully output the cor-
rect nested structure of Figure 1(b).
2 Related work
Resnik (1999) disambiguated coordinations of the
form [n1 and n2 n3], where ni are all nouns. This
type of phrase has two possible readings: [(n1)
and (n2 n3)] and [((n1) and (n2)) n3]. He demon-
strated the effectiveness of semantic similarity cal-
culated from a large text collection, and agreement
of numbers between n1 and n2 and between n1 and
n3. Nakov and Hearst (2005) collected web-based
statistics with search engines and applied them to
a task similar to Resnik?s.
Hogan (2007) improved the parsing accuracy
of sentences in which coordinated noun phrases
are known to exist. She presented a generative
model incorporating symmetry in conjunct struc-
tures and dependencies between coordinated head
words. The model was then used to rerank the n-
best outputs of the Bikel parser (2005).
Recently, Buyko et al (2007; 2008) and
Shimbo and Hara (2007) applied discriminative
learning methods to coordinate structure analysis.
Buyko et al used a linear-chain CRF, whereas
Shimbo and Hara proposed an approach based on
perceptron learning of edit distance between con-
juncts.
Shimbo and Hara?s approach has its root in
Kurohashi and Nagao?s (1994) rule-based method
for Japanese coordinations. Other studies on co-
ordination include (Agarwal and Boggess, 1992;
Chantree et al, 2005; Goldberg, 1999; Okumura
and Muraki, 1994).
3 Proposed method
We propose a method for learning and detecting
the scopes of coordinations. It makes no assump-
tion about the number of coordinations in a sen-
tence, and the sentence can contain either nested
coordinations, multiple flat coordinations, or both.
The method consists of (i) a simple gram-
mar tailored for coordinate structure, and (ii) a
perceptron-based algorithm for learning feature
weights. The features are defined in terms of se-
quence alignment between conjuncts.
We thus use the grammar to filter out incon-
sistent nested coordinations and non-valid (over-
lapping) conjunct scopes, and the alignment-based
features to evaluate the similarity of conjuncts.
3.1 Grammar for coordinations
The sole objective of the grammar we present be-
low is to ensure the consistency of two or more
coordinations in a sentence; i.e., for any two co-
ordinations, either (i) they must be totally non-
overlapping (non-nested coordinations), or (ii) one
coordination must be embedded within the scope
of a conjunct of the other coordination (nested co-
ordinations).
Below, we call a parse tree built from the gram-
mar a coordination tree.
968
Table 1: Non-terminals
COORD Complete coordination.
COORD? Partially-built coordination.
CJT Conjunct.
N Non-coordination.
CC Coordinate conjunction like ?and,?
?or,? and ?but?.
SEP Connector of conjuncts other than CC:
e.g., punctuations like ?,? and ?;?.
W Any word.
Table 2: Production rules for coordination trees.
(. . . | . . . | . . .) denotes a disjunction (matches any
one of the elements). A ?*? matches any word.
Rules for coordinations:
(i) COORDi,m ? CJTi, j CC j+1,k?1 CJTk,m
(ii) COORDi,n ? CJTi, j SEP j+1,k?1 COORD?k,n[m]
(iii) COORD?i,m[ j]? CJTi, j CC j+1,k?1 CJTk,m
(iv) COORD?i,n[ j]? CJTi, j SEP j+1,k?1 COORD?k,n[m]
Rules for conjuncts:
(v) CJTi, j ? (COORD | N)i, j
Rules for non-coordinations:
(vi) Ni,k ? COORDi, j N j+1,k
(vii) Ni, j ? Wi,i (COORD|N)i+1, j
(viii) Ni,i ? Wi,i
Rules for pre-terminals:
(ix) CCi,i ? (and | or | but )i
(x) CCi,i+1 ? ( , | ; )i (and | or | but )i+1
(xi) SEPi,i ? ( , | ; )i
(xii) Wi,i ? ?i
3.1.1 Non-terminals
The grammar is composed of non-terminal sym-
bols listed in Table 1. The distinction between
COORD and COORD? is made to cope with three or
more conjuncts in a coordination. For example
?a , b and c? is treated as a tree of the form (a ,
(b and c))), and the inner tree (b and c) is not a
complete coordination, until it is conjoined with
the first conjunct a. We represent this inner tree
by a COORD? (partial coordination), to distinguish it
from a complete coordination represented by CO-
ORD. Compare Figures 2(a) and (b), which respec-
tively depict the coordination tree for this exam-
ple, and a tree for nested coordination with a sim-
ilar structure.
3.1.2 Production rules
Table 2 lists the production rules. Rules are shown
with explicit subscripts indicating the span of their
production. The subscript to a terminal word
(shown in a box) specifies its position within a sen-
tence (word index). Non-terminals have two sub-
script indices denoting the span of the production.
COORD? in rules (iii) and (iv) has an extra in-
dex j shown in brackets. This bracketed index
maintains the end of the first conjunct (CJT) on
the right-hand side. After a COORD? is produced
by these rules, it may later constitute a larger CO-
ORD or COORD? through the application of produc-
tions (ii) or (iv). At this point, the bracketed in-
dex of the constituent COORD? allows us to identify
the scope of the first conjunct immediately under-
neath. As we describe in Section 3.2.4, the scope
of this conjunct is necessary to compute the score
of coordination trees.
These grammar rules are admittedly minimal
and need further elaboration to cover all real use
cases of coordination (e.g., conjunctive phrases
like ?as well as?, etc.). Yet they are sufficient to
generate the basic trees illustrated in Figure 2. The
experiments of Section 5 will apply this grammar
on a real biomedical corpus.
Note that although non-conjunction cue expres-
sions, such as ?both? and ?either,? are not the
part of this grammar, such cues can be learned
(through perceptron training) from training exam-
ples if appropriate features are introduced. Indeed,
in Section 5 we use features indicating which
words precede coordinations.
3.2 Score of a coordination tree
Given a sentence, our system outputs the coordina-
tion tree with the highest score among all possible
trees for the sentence. The score of a coordination
tree is simply the sum of the scores of all its nodes,
and the node scores are computed independently
from each other. Hence a bottom-up chart parsing
algorithm can be designed to efficiently compute
the highest scoring tree.
While scores can be assigned to any nodes, we
have chosen to assign a non-zero score only to two
types of coordination nodes, namely COORD and
COORD?, in the experiment of Section 5; all other
nodes are ignored in score computation. The score
of a coordination node is defined via sequence
alignment (Gusfield, 1997) between conjuncts be-
low the node, to capture the symmetry of these
969
(a) a , b and c
W W W
COORD
COORD?
N SEP N CC N
(b) a or b and c
W
CC
W
CC
W
N N N
COORD
COORD
(c) a
W
b
W
c
W
N
N
N
Figure 2: Coordination trees for (a) a coordination with three conjuncts, (b) nested coordinations, and
(c) a non-coordination. The CJT nodes in (a) and (b) are omitted for brevity.
W W CC W W W W W CC W W CC W W W W W
N
N
N
N
N
N
N
N
N
N
N
N
COORD
N
N
COORD
N
N
COORD
6.1 
months 
8.
9 
m
on
th
s 
9.
5 
m
on
th
s 
7.2 
months 
6.1 
months 
and
8.9
months
in
arm
A
7.
2 
m
on
th
s 
an
d
9.
5
m
on
th
s
in ar
m
B
M
ed
ia
n 
tim
es
 
to
 
pr
og
re
ss
io
n 
an
d
m
ed
ia
n 
su
rv
iv
al
 
tim
es
 
w
er
e 
6.
1 
m
on
th
s 
an
d 
8.
9 
m
on
th
s in
ar
m A
an
d
7.
2
m
on
th
s 
an
d
9.
5
m
on
th
s in
ar
m B
W W W W CC W W W
N
N
NN
N
N
N
COORD
W
N
N
Median
times
to
progression
m
ed
ia
n
su
rv
iv
al
tim
es
Figure 3: A coordination tree for the example sen-
tence presented in Section 1, with the edit graphs
attached to COORD nodes.
m
ed
ia
n
su
rv
iv
al
tim
es
Median
times
to
progression
initial vertex
terminal vertex
Figure 4: An edit graph and an alignment path
(bold line).
conjuncts.
Figure 3 schematically illustrates the relation
between a coordination tree and alignment-based
computation of the coordination nodes. The score
of this tree is given by the sum of the scores of the
four COORD nodes, and the score of a COORD node
is computed with the edit graph shown above the
node.
3.2.1 Edit graph
The edit graph is a basic data structure for comput-
ing sequence alignment. An example edit graph is
depicted in Figure 4 for word sequences ?Median
times to progression? and ?median survival times.?
A diagonal edge represents alignment (or sub-
stitution) between the word at the top of the edge
and the one on the left, while horizontal and ver-
tical edges represent skipping (or deletion) of re-
spective word. With this representation, a path
starting from the top-left corner (initial vertex) and
arriving at the bottom-right corner (terminal ver-
tex) corresponds one-to-one to a sequence of edit
operations transforming one word sequence to the
other.
In standard sequence alignment, each edge of an
edit graph is associated with a score representing
the merit of the corresponding edit operation. By
defining the score of a path as the total score of its
component edges, we can assess the similarity of
a pair of sequences as the maximum score over all
paths in its edit graph.
3.2.2 Features
In our model, instead of assigning a score inde-
pendently to edges of an edit graph, we assign a
vector of features to edges. The score of an edge
is the inner product of this feature vector and an-
other vector w, called global weight vector. Fea-
ture vectors may differ from one edge to another,
but the vector w is unique in the entire system and
consistently determines the relative importance of
individual features.
In parallel to the definition of a path score, the
feature vector of a path can be defined as the sum
of the feature vectors assigned to its component
edges. Then the score of a path is equal to the
inner product ?w, f? of w and the feature vector f
of the path.
A feature assigned to an edge can be an arbi-
trary indicator of edge directions (horizontal, ver-
tical, or diagonal), edge coordinates in the edit
graph, attributes (such as the surface form, part-
of-speech, and the location in the sentence) of the
current or surrounding words, or their combina-
tion. Section 5.3 will describe the exact features
used in our experiments.
970
3.2.3 Averaged path score as the score of a
coordination node
Finally, we define the score of a COORD (or COORD?)
node in a coordination tree as the average score
of all paths in its associated edit graph. This is an-
other deviation from standard sequence alignment,
in that we do not take the maximum scoring paths
as representing the similarity of conjuncts, but in-
stead use the average over all paths.
Notice that the average is taken over paths, and
not edges. In this way, a natural bias is incurred
towards features occurring near the diagonal con-
necting the initial vertex and the terminal vertex.
For instance, in an edit graph of size 8? 8, there
is only one path that goes through the vertex at the
top-right corner, while more than 3,600 paths pass
through the vertex at the center of the graph. In
other words, the features associated with the cen-
ter vertex receives 3,600 times more weights than
those at the top-right corner after averaging.
The major benefit of this averaging is the re-
duced computation during training. During the
perceptron training, the global weight vector w
changes and the score of individual paths changes
accordingly. On the other hand, the average fea-
ture vector f (as opposed to the average score
?w, f?) over all paths in the edit graph remains
constant. This means that f can be pre-computed
once before the training starts, and the score com-
putation during training reduces to simply taking
the inner product of the current w and the pre-
computed f.
Alternatively, the alignment score could be de-
fined as that of the best scoring path with respect
to the current w, following the standard sequence
alignment computation. However, it would require
running the Viterbi algorithm in each iteration of
the perceptron training, for all possible spans of
conjuncts. While we first pursued this direction,
it was abandoned as the training was intolerably
slow.
3.2.4 Coordination with three or more
conjuncts
For a coordination with three or more conjuncts,
we define its score as the sum of the similarity
scores of all pairwise consecutive conjuncts; i.e.,
for a coordination ?a, b, c, and d? with four con-
juncts, the score is the sum of the similarity scores
for conjunct pairs (a, b), (b, c), and (c, d). Ide-
ally, we should take all combinations of conjuncts
into account, but it would lead to a combinatorial
a , b , c and d
W W W W
COORD
COORD?
COORD?
N SEP N SEP N CC N
Figure 5: A coordination tree with four conjuncts.
All CJT nodes are omitted.
explosion and is impractical.
Recall that in the grammar introduced in Sec-
tion 3.1, we attached a bracketed index to COORD?.
This bracketed index was introduced for the com-
putation of this pairwise similarity.
Figure 5 shows the coordination tree for ?a, b,
c, and d.? The pairwise similarity scores for (a,
b), (b, c), and (c, d) are respectively computed at
the top COORD, left COORD?, and right COORD? nodes,
using the scheme described in Section 3.2.3. To
compute the similarity of a and b, we need to lift
the information about the end position of b upward
to the COORD node. The same applies to computing
the similarity of b and c; the end position of c is
needed at the left COORD?. The bracketed index of
COORD? exactly maintains this information, i.e., the
end of the first conjunct below the COORD?. See
production rules (iii) and (iv) in Table 2.
3.3 Perceptron learning of feature weights
As we saw above, our model is a linear model with
the global weight vector w acting as the coefficient
vector, and hence various existing techniques can
be exploited to optimize w.
In this paper, we use the averaged perceptron
learning (Collins, 2002; Freund and Schapire,
1999) to optimize w on a training corpus, so that
the system assigns the highest score to the correct
coordination tree among all possible trees for each
training sentence.
4 Discussion
4.1 Computational complexity
Given an input sentence of N words, finding its
maximum scoring coordination tree by a bottom-
up chart parsing algorithm incurs a time complex-
ity of O(N3).
While the right-hand side of rules (i)?(iv) in-
volves more than three variables and thus appears
to increase complexity, this is not the case since
971
some of the variables ( j and k in rules (i) and (iii),
and j, k, and m in rules (ii) and (iv)) are con-
strained by the location of conjunct connectors (CC
and SEP), whose number in a sentence is negligi-
ble compared to the sentence length N. As a result,
these rules can be processed in O(N2) time. Hence
the run-time complexity is dominated by rule (vi),
which has three variables and leads to O(N3).
Each iteration of the perceptron algorithm for
a sentence of length N also incurs O(N3) for the
same reason.
Our method also requires pre-processing in the
beginning of perceptron training, to compute the
average feature vectors f for all possible spans
(i, j) and (k,m) of conjuncts in a sentence. With a
reasoning similar to the complexity analysis of the
chart parsing algorithm above, we can show that
the pre-processing takes O(N4) time.
4.2 Difference from Shimbo and Hara?s
method
The method proposed in this paper extends the
work of Shimbo and Hara (2007). Both take a
whole sentence as input and use perceptron learn-
ing, and the difference lies in how hypothesis co-
ordination(s) are encoded as a feature vector.
Unlike our new method which constructs a tree
of coordinations, Shimbo and Hara used a chain-
able partial paths (representing non-overlapping
series of local alignments; see (Shimbo and Hara,
2007, Figure 5)) in a global triangular edit graph.
In our method, we compute many edit graphs of
smaller size, one for each possible conjunct pair in
a sentence. We use global alignment (a complete
path) in these smaller graphs, as opposed to chain-
able local alignment (partial paths) in a global edit
graph used by Shimbo and Hara.
Since nested coordinations cannot be encoded
as chainable partial paths (Shimbo and Hara,
2007), their method cannot cope with nested coor-
dinations such as those illustrated in Figure 2(b).
4.3 Integration with parsers
Charniak and Johnson (2005) reported an im-
proved parsing accuracy by reranking n-best parse
trees, using features based on similarity of coor-
dinated phrases, among others. It should be inter-
esting to investigate whether alignment-based fea-
tures like ours can be built into their reranker, or
more generally, whether the coordination scopes
output by our method help improving parsing ac-
curacy.
The combinatory categorial grammar (CCG)
(Steedman, 2000) provides an account for vari-
ous coordination constructs in an elegant manner,
and incorporating alignment-based features into
the CCG parser (Clark and Curran, 2007) is also
a viable possibility.
5 Evaluation
We evaluated the performance of our method1 on
the Genia corpus (Kim et al, 2003).
5.1 Dataset
Genia Treebank Beta is a collection of Penn
Treebank-like phrase structure trees for 4529 sen-
tences from Medline abstracts.
In this corpus, each scope of coordinate struc-
tures is annotated with an explicit tag, and the
conjuncts are always placed inside brackets. Not
many treebanks explicitly mark the scope of con-
juncts; for example, the Penn Treebank frequently
omits bracketing of coordination and conjunct
scopes, leaving them as a flat structure.
Genia contains a total of 4129 occurrences of
COOD tags indicating coordination. These tags are
further subcategorized into phrase types such as
NP-COOD and VP-COOD. Among coordinations anno-
tated with COOD tags, we selected those surround-
ing ?and,? ?or,? and ?but.? This yielded 3598 co-
ordinations (2997, 355, and 246 for ?and,? ?or,?
and ?but,? respectively) in 2508 sentences. These
coordinations constitute nearly 90% of all coordi-
nations in Genia, and we used them as the evalua-
tion dataset. The length of these sentences is 30.0
words on average.
5.2 Evaluation method
We tested the proposed method in two tasks:
(i) identify the scope of coordinations regardless
of phrase types, and
(ii) detect noun phrase (NP) coordinations and
identify their scopes.
While the goal of task (i) is to determine the scopes
of 3598 coordinations, task (ii) demands both to
judge whether each of the coordinations constructs
an NP, and if it does, to determine its scope.
1A C++ implementation of our method can be found
at http://cl.naist.jp/project/coordination/, along with supple-
mentary materials including the preliminary experimental re-
sults of the CCG parser on the same dataset.
972
Table 3: Features in the edit graph for conjuncts wkwk+1 ? ? ?wm and wlwl+1 ? ? ?wn.
edge/vertex type vertical edge horizontal edge diagonal edge initial vertex terminal vertex
? ? ?
w j?1 w j w j+1
? ? ?
.
.
.
wi?1
wi
wi+1
.
.
.
? ? ?
w j?1 w j w j+1
? ? ?
.
.
.
wi?1
wi
wi+1
.
.
.
? ? ?
w j?1 w j w j+1
? ? ?
.
.
.
wi?1
wi
wi+1
.
.
.
wl wl+1 ? ? ?
wk
wk+1
.
.
.
? ? ?
wn?1 wn
.
.
.
wm?1
wm
vertical bigrams wi?1wi
wiwi+1
wi?1wi wi?1wi
wiwi+1
wk?2wk?1
wk?1wk
wkwk+1
wm?2wm?1
wm?1wm
wmwm+1
horizontal bigrams wj?1wj w j?1wj
w jw j+1
wj?1wj
w jw j+1
wl?2wl?1
wl?1wl
wlwl+1
wn?2wn?1
wn?1wn
wnwn+1
orthogonal bigrams wiw j wk?1wl?1
wk?1wl
wkwl?1
wkwl
wm?1wn?1
wm?1wn
wmwn?1
wmwn
For comparison, two parsers, the Bikel-Collins
parser (Bikel, 2005)2 and Charniak-Johnson
reranking parser3, were applied in both tasks.
Task (ii) imitates the evaluation reported by
Shimbo and Hara (2007), and to compare our
method with their coordination analysis method.
Because their method can only process flat coordi-
nations, in task (ii) we only used 1613 sentences in
which ?and? occurs just once, following (Shimbo
and Hara, 2007). Note however that the split of
data is different from their experiments.
We evaluate the performance of the tested meth-
ods by the accuracy of coordination-level brack-
eting (Shimbo and Hara, 2007); i.e., we count
each of the coordination (as opposed to conjunct)
scopes as one output of the system, and the system
output is deemed correct if the beginning of the
first output conjunct and the end of the last con-
junct both match annotations in the Genia Tree-
bank.
In both tasks, we report the micro-averaged re-
sults of five-fold cross validation.
The Bikel-Collins and Charniak-Johnson
parsers were trained on Genia, using all the phrase
structure trees in the corpus except the test set;
i.e., the training set alo contains (in addition to
the four folds) 2021(= 4129 ? 2508) sentences
which are not in the five folds. Since the two
parsers were also trained on Genia, we interpret
the bracketing above each conjunction in the
parse tree output by them as the coordination
scope output by the parsers, in accordance with
how coordinations are annotated in Genia. In
2http://www.cis.upenn.edu/?dbikel/software.html
3ftp://ftp.cs.brown.edu/pub/nlparser/
reranking-parserAug06.tar.gz
testing, the Bikel-Collins parser and Shimbo-Hara
method were given the gold parts-of-speech
(POS) of the test sentences in Genia. We trained
the proposed method twice, once with the gold
POS tags and once with the POS tags output by
the Charniak-Johnson parser. This is because the
Charniak-Johnson parser does not accept POS
tags of the test sentences.
5.3 Features
To compute features for our method, each word
in a sentence was represented as a list of at-
tributes. The attributes include the surface word,
part-of-speech, suffix, prefix, and the indicators
of whether the word is capitalized, whether it is
composed of all uppercase letters or digits, and
whether it contains digits or hyphens. All fea-
tures are defined as an indicator of an attribute in
two words coming from either a single conjunct
(either horizontal or vertical word sequences asso-
ciated with the edit graph) or two conjuncts (one
from the horizontal word sequence and one from
the vertical sequence). We call the first type hori-
zontal/vertical bigrams and the second orthogonal
bigrams.
Table 3 summarizes the features in an edit
graph for two conjuncts (wkwk+1 ? ? ?wm) and
(wlwl+1 ? ? ?wn), where wi denotes the ith word in
the sentence.
As seen from the table, features are assigned
to the initial and terminal vertices as well as to
edges. A wiwj in the table indicates that for each
attribute (e.g., part-of-speech, etc.), an indicator
function for the combination of the attribute val-
ues in wi and wj is assigned to the vertex or edge
shown in the figure above. Note that the features
973
Table 4: Results of Task (i). The number of coor-
dinations of each type (#), and the recall (%) for
the proposed method, Bikel-Collins parser (BC),
and Charniak-Johnson parser (CJ).
gold POS CJ POS
COOD # Proposed BC Proposed CJ
Overall 3598 61.5 52.1 57.5 52.9
NP 2317 64.2 45.5 62.5 50.1
VP 465 54.2 67.7 42.6 61.9
ADJP 321 80.4 66.4 76.3 48.6
S 188 22.9 67.0 15.4 63.3
PP 167 59.9 53.3 53.9 58.1
UCP 60 36.7 18.3 38.3 26.7
SBAR 56 51.8 85.7 33.9 83.9
ADVP 21 85.7 90.5 85.7 90.5
Others 3 66.7 33.3 33.3 0.0
assigned to different types of vertex or edge are
treated as distinct even if the word indices i and j
are identical; i.e., all features are conditioned on
edge/vertex types to which they are assigned.
5.4 Results
Task (i) Table 4 shows the results of task (i). We
only list the recall score in the table, as precision
(and hence F1-measure, too) was equal to recall
for all methods in this task; this is not surpris-
ing given that in this data set, conjunctions ?and?,
?or?, and ?but? always indicate the existence of a
coordination, and all methods successfully learned
this trend from the training data.
The proposed method outperformed parsers on
the coordination scope identification overall. The
table also indicates that our method considerably
outperformed two parsers on NP-COOD, ADJP-COOD,
and UCP-COOD categories, but it did not work well
on VP-COOD, S-COOD, and SBAR-COOD. In contrast,
the parsers performed quite well in the latter cate-
gories.
Task (ii) Table 5 lists the results of task (ii).
The proposed method outperformed Shimbo-Hara
method in this task, although the setting of this
task is mostly identical to (Shimbo and Hara,
2007) and does not include nested coordinations.
Note also that both methods use roughly equiva-
lent features.
One reason should be that our grammar rules
can strictly enforce the scope consistency of con-
juncts in coordinations with three or more con-
juncts. Because the Shimbo-Hara method repre-
sents such coordinations as a series of sub-paths
in an edit graph which are output independently
of each other without enforcing consistency, their
Table 5: Results of Task (ii). Proposed method,
BC: Bikel-Collins, CJ: Charniak-Johnson, SH:
Shimbo-Hara.
gold POS CJ POS
Proposed BC SH Proposed CJ
Precision 61.7 45.6 55.9 60.2 49.0
Recall 57.9 46.1 53.7 55.6 46.8
F1 59.7 45.8 54.8 57.8 47.9
method can produce inconsistent scopes of con-
juncts in the middle.
In fact, the advantage of the proposed method in
task (ii) is noticeable especially in coordinations
with three or more conjuncts; if we restrict the test
set only to coordinations with three or more con-
juncts, the F-measures in the proposed method and
Shimbo-Hara become 53.0 and 42.3, respectively;
i.e., the margin increases to 10.7 from 4.9 points.
6 Conclusion and outlook
We have proposed a method for learning and
analyzing generic coordinate structures including
nested coordinations. It consists of a simple gram-
mar for coordination and perceptron learning of
alignment-based features.
The method performed well overall and on co-
ordinated noun and adjective phrases, but not on
coordinated verb phrases and sentences. The lat-
ter coordination types are in fact easy for parsers,
as the experimental results show.
The proposed method failing in verbal and sen-
tential coordinations is as expected, since con-
juncts in these coordinations are not necessarily
similar, if they are viewed as a sequence of words.
We will investigate similarity measures different
from sequence alignment, to better capture the
symmetry of these conjuncts.
We will also pursue integration of our method
with parsers. Because they have advantages in dif-
ferent coordination phrase types, their integration
looks promising.
Acknowledgments
We thank anonymous reviewers for helpful com-
ments and the pointer to the combinatory catego-
rial grammar.
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th Annual Meeting of the Associa-
974
tion for Computational Linguistics (ACL?92), pages
15?21.
Daniel M. Bikel. 2005. Multilingual statistical pars-
ing engine version 0.9.9c. http://www.cis.upenn.
edu/?dbikel/software.html.
Ekaterina Buyko and Udo Hahn. 2008. Are morpho-
syntactic features more predicative for the resolution
of noun phrase coordination ambiguity than lexico-
semantic similarity scores. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING 2008), pages 89?96, Manch-
ester, UK.
Ekaterina Buyko, Katrin Tomanek, and Udo Hahn.
2007. Resolution of coordination ellipses in bi-
ological named entities using conditional random
fields. In Proceedings of the Pacific Association
for Computational Linguistics (PACLIC?07), pages
163?171.
Robyn Carston and Diane Blakemore. 2005. Editorial:
Introduction to coordination: syntax, semantics and
pragmatics. Lingua, 115:353?358.
Francis Chantree, Adam Kilgarriff, Anne de Roeck,
and Alistair Willis. 2005. Disambiguating coor-
dinations using word distribution information. In
Proceedings of the Int?l Conference on Recent Ad-
vances in Natural Language Processing, Borovets,
Bulgaria.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), pages 173?180, Ann Arbor, Michigan,
USA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2002), pages 1?
8, Philadelphia, PA, USA.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Miriam Goldberg. 1999. An unsupervised model
for statistically determining coordinate phrase at-
tachment. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL 1999), pages 610?614, College Park, Mary-
land, USA.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL 2007), pages 680?
687, Prague, Czech Republic.
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
GENIA corpus: a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl. 1):i180?
i182.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20:507?534.
Preslav Nakov and Marti Hearst. 2005. Using the web
as an implicit training set: application to structural
ambiguity resolution. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language (HLT-
EMNLP 2005), pages 835?842, Vancouver, Canada.
Akitoshi Okumura and Kazunori Muraki. 1994. Sym-
metric pattern matching analysis for English coordi-
nate structures. In Proceedings of the Fourth Con-
ference on Applied Natural Language Processing,
pages 41?46.
Philip Resnik. 1999. Semantic similarity in a tax-
onomy. Journal of Artificial Intelligence Research,
11:95?130.
Wolfgang Schuette, Thomas Blankenburg, Wolf
Guschall, Ina Dittrich, Michael Schroeder, Hans
Schweisfurth, Assaad Chemaissani, Christian Schu-
mann, Nikolas Dickgreber, Tabea Appel, and Di-
eter Ukena. 2006. Multicenter randomized trial for
stage iiib/iv non-small-cell lung cancer using every-
3-week versus weekly paclitaxel/carboplatin. Clini-
cal Lung Cancer, 7:338?343.
Masashi Shimbo and Kazuo Hara. 2007. A discrimi-
native learning model for coordinate conjunctions.
In Proceedings of Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL 2007), pages 610?619, Prague, Czech Re-
public.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
975
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 5?8,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Bypassed Alignment Graph for Learning Coordination in Japanese
Sentences
Hideharu Okuma Kazuo Hara Masashi Shimbo Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
{okuma.hideharu01,kazuo-h,shimbo,matsu}@is.naist.jp
Abstract
Past work on English coordination has fo-
cused on coordination scope disambigua-
tion. In Japanese, detecting whether coor-
dination exists in a sentence is also a prob-
lem, and the state-of-the-art alignment-
based method specialized for scope dis-
ambiguation does not perform well on
Japanese sentences. To take the detection
of coordination into account, this paper in-
troduces a ?bypass? to the alignment graph
used by this method, so as to explicitly
represent the non-existence of coordinate
structures in a sentence. We also present
an effective feature decomposition scheme
based on the distance between words in
conjuncts.
1 Introduction
Coordination remains one of the challenging prob-
lems in natural language processing. One key
characteristic of coordination explored in the past
is the structural and semantic symmetry of con-
juncts (Chantree et al, 2005; Hogan, 2007;
Resnik, 1999). Recently, Shimbo and Hara (2007)
proposed to use a large number of features to
model this symmetry, and optimize the feature
weights with perceptron training. These features
are assigned to the arcs of the alignment graph (or
edit graph) originally developed for biological se-
quence alignment.
Coordinate structure analysis involves two re-
lated but different tasks:
1. Detect the presence of coordinate structure in
a sentence (or a phrase).
2. Disambiguate the scope of coordinations in
the sentences/phrases detected in Task 1.
The studies on English coordination listed
above are concerned mainly with scope disam-
biguation, reflecting the fact that detecting the
presence of coordinations in a sentence (Task 1)
is straightforward in English. Indeed, nearly 100%
precision and recall can be achieved in Task 1 sim-
ply by pattern matching with a small number of
coordination markers such as ?and,? ?or,? and ?as
well as?.
In Japanese, on the other hand, detecting coor-
dination is non-trivial. Many of the coordination
markers in Japanese are ambiguous and do not al-
ways indicate the presence of coordinations. Com-
pare sentences (1) and (2) below:
rondon to pari ni itta
(London) (and) (Paris) (to) (went)
(I went to London and Paris)
(1)
kanojo to pari ni itta
(her) (with) (Paris) (to) (went)
(I went to Paris with her)
(2)
These sentences differ only in the first word. Both
contain a particle to, which is one of the most fre-
quent coordination markers in Japanese?but only
the first sentence contains a coordinate structure.
Pattern matching with particle to thus fails to filter
out sentence (2).
Shimbo and Hara?s model allows a sentence
without coordinations to be represented as a nor-
mal path in the alignment graph, and in theory it
can cope with Task 1 (detection). In practice, the
representation is inadequate when a large number
of training sentences do not contain coordinations,
as demonstrated in the experiments of Section 4.
This paper presents simple yet effective modi-
fications to the Shimbo-Hara model to take coor-
dination detection into account, and solve Tasks 1
and 2 simultaneously.
5
a 
policeman 
and 
warehouse
guard
a po
lic
em
an
 
an
d 
w
ar
eh
ou
se
gu
ar
d
a 
policeman 
and 
warehouse
guard
a po
lic
em
an
 
an
d 
w
ar
eh
ou
se
gu
ar
d
(a) Alignment graph (b) Path 1
a 
policeman 
and 
warehouse
guard
a po
lic
em
an
 
an
d 
w
ar
eh
ou
se
gu
ar
d
a 
policeman 
and 
warehouse
guard
a po
lic
em
an
 
an
d 
w
ar
eh
ou
se
gu
ar
d
(c) Path 2 (d) Path 3 (no coordination)
Figure 1: Alignment graph for ?a policeman and
warehouse guard? ((a)), and example paths repre-
senting different coordinate structure ((b)?(d)).
2 Alignment-based coordinate structure
analysis
We first describe Shimbo and Hara?s method upon
which our improvements are made.
2.1 Triangular alignment graph
The basis of their method is a triangular align-
ment graph, illustrated in Figure 1(a). Kurohashi
and Nagao (1994) used a similar data structure in
their rule-based method. Given an input sentence,
the rows and columns of its alignment graph are
associated with the words in the sentence. Un-
like the alignment graph used in biological se-
quence alignment, the graph is triangular because
the same sentence is associated with rows and
columns. Three types of arcs are present in the
graph. A diagonal arc denotes coordination be-
tween the word above the arc and the one on the
right; the horizontal and vertical arcs represent
skipping of respective words.
Coordinate structure in a sentence is repre-
sented by a complete path starting from the top-
left (initial) node and arriving at the bottom-right
(terminal) node in its alignment graph. Each arc
in this path is labeled either Inside or Outside de-
pending on whether its span is part of coordina-
tion or not; i.e., the horizontal and vertical spans
of an Inside segment determine the scope of two
conjuncts. Figure 1(b)?(d) depicts example paths.
Inside and Outside arcs are depicted by solid and
dotted lines, respectively. Figure 1(b) shows a
path for coordination between ?policeman? (ver-
tical span of the Inside segment) and ?warehouse
guard? (horizontal span). Figure 1(c) is for ?po-
liceman? and ?warehouse.? Non-existence of co-
ordinations in a sentence is represented by the
Outside-only path along the top and the rightmost
borders of the graph (Figure 1(d)).
With this encoding of coordinations as paths,
coordinate structure analysis can be reduced to
finding the highest scoring path in the graph,
where the score of an arc is given by a measure
of how much two words are likely to be coordi-
nated. The goal is to build a measure that assigns
the highest score to paths denoting the correct co-
ordinate structure. Shimbo and Hara defined this
measure as a linear function of many features as-
sociated to arcs, and used perceptron training to
optimize the weight coefficients for these features
from corpora.
2.2 Features
For the description of features used in our adap-
tation of the Shimbo-Hara model to Japanese, see
(Okuma et al, 2009). In this model, all features
are defined as indicator functions asking whether
one or more attributes (e.g., surface form, part-of-
speech) take specific values at the neighbor of an
arc. One example of a feature assigned to a diag-
onal arc at row i and column j of the alignment
graph is
f =
?
?
?
1 if POS[i] = Noun, POS[ j] = Adjective,
and the label of the arc is Inside,
0 otherwise.
where POS[i] denotes the part-of-speech of the ith
word in a sentence.
3 Improvements
We introduce two modifications to improve the
performance of Shimbo and Hara?s model in
Japanese coordinate structure analysis.
3.1 Bypassed alignment graphs
In their model, a path for a sentence with no coor-
dination is represented as a series of Outside arcs
as we saw in Figure 1(d). However, Outside arcs
also appear in partial paths between two coordina-
tions, as illustrated in Figure 2. Thus, two differ-
6
Aand
B
are
X
and
Y
A an
d
B ar
e 
X an
d
Y
Figure 2: Original alignment graph for sentence
with two coordinations. Notice that Outside (dot-
ted) arcs connect two coordinations
Figure 3: alignment graph with a ?bypass?
ent roles are given to Outside arcs in the original
Shimbo-Hara model.
We identify this to be a cause of their model not
performing well for Japanese, and propose to aug-
ment the original alignment graph with a ?bypass?
devoted to explicitly indicate that no coordination
exists in a sentence; i.e., we add a special path di-
rectly connecting the initial node and the terminal
node of an alignment graph. See Figure 3 for il-
lustration of a bypass.
In the new model, if the score of the path
through the bypass is higher than that of any paths
in the original alignment graph, the input sentence
is deemed not containing coordinations.
We assign to the bypass two types of features
capturing the characteristics of a whole sentence;
i.e., indicator functions of sentence length, and of
the existence of individual particles in a sentence.
The weight of these features, which eventually de-
termines the score of the bypass, is tuned by per-
ceptron just like the weights of other features.
3.2 Making features dependent on the
distance between conjuncts
Coordinations of different type (e.g., nominal and
verbal) have different relevant features, as well as
different average conjunct length (e.g., nominal
coordinations are shorter).
This observation leads us to our second modi-
fication: to make all features dependent on their
occurring positions in the alignment graph. To be
precise, for each individual feature in the original
model, a new feature is introduced which depends
on whether the Manhattan distance d in the align-
ment graph between the position of the feature oc-
currence and the nearest diagonal exceeds a fixed
threshold1 ? . For instance, if a feature f is an in-
dicator function of condition X , a new feature f ? is
introduced such that
f ? =
{
1, if d ? ? and condition X holds,
0, otherwise.
Accordingly, different weights are learned and as-
sociated to two features f and f ?. Notice that the
Manhattan distance to the nearest diagonal is equal
to the distance between word pairs to which the
feature is assigned, which in turn is a rough esti-
mate of the length of conjuncts.
This distance-based decomposition of features
allows different feature weights to be learned for
coordinations with conjuncts shorter than or equal
to ? , and those which are longer.
4 Experimental setup
We applied our improved model and Shimbo and
Hara?s original model to the EDR corpus (EDR,
1995). We also ran the Kurohashi-Nagao parser
(KNP) 2.02, a widely-used Japanese dependency
parser to which Kurohashi and Nagao?s (1994)
rule-based coordination analysis method is built
in. For comparison with KNP, we focus on bun-
setsu-level coordinations. A bunsetsu is a chunk
formed by a content word followed by zero or
more non-content words like particles.
4.1 Dataset
The Encyclopedia section of the EDR corpus was
used for evaluation. In this corpus, each sentence
is segmented into words and is accompanied by a
syntactic dependency tree, and a semantic frame
representing semantic relations among words.
A coordination is indicated by a specific relation
of type ?and? in the semantic frame. The scope of
conjuncts (where a conjunct may be a word, or a
series of words) can be obtained by combining this
information with that of the syntactic tree. The
detail of this procedure can be found in (Okuma et
al., 2009).
1We use ? = 5 in the experiments of Section 4.
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
7
Table 1: Accuracy of coordination scopes and end of conjuncts, averaged over five-fold cross validation.
The numbers in brackets are the improvements (in points) relative to the Shimbo-Hara (SH) method.
Scope of coordinations End of conjuncts
Method Precision Recall F1 measure Precision Recall F1 measure
KNP n/a n/a n/a 58.8 65.3 61.9 (?2.6)
Shimbo and Hara?s method (SH; baseline) 53.7 49.8 51.6 (?0.0) 67.0 62.1 64.5 (?0.0)
SH + distance-based feature decomposition 55.3 52.1 53.6 (+2.0) 68.3 64.3 66.2 (+1.7)
SH + distance-based feature decomposition + bypass 55.0 57.6 56.3 (+4.7) 66.8 69.9 68.3 (+3.8)
Of 10,072 sentences in the Encyclopedia sec-
tion, 5,880 sentences contain coordinations. We
excluded 1,791 sentences in which nested coordi-
nations occur, as these cannot be processed with
Shimbo and Hara?s method (with or without our
improvements).
We then applied Japanese morphological ana-
lyzer JUMAN 5.1 to segment each sentence into
words and annotate them with parts-of-speech,
and KNP with option ?-bnst? to transform the se-
ries of words into a bunsetsu series. With this
processing, each word-level coordination pair is
also translated into a bunsetsu pair, unless the
word-level pair is concatenated into a single bun-
setsu (sub-bunsetsu coordination). Removing sub-
bunsetsu coordinations and obvious annotation er-
rors left us with 3,257 sentences with bunsetsu-
level coordinations. Combined with the 4,192 sen-
tences not containing coordinations, this amounts
to 7,449 sentences used for our evaluation.
4.2 Evaluation metrics
KNP outputs dependency structures in Kyoto Cor-
pus format (Kurohashi et al, 2000) which spec-
ifies the end of coordinating conjuncts (bunsetsu
sequences) but not their beginning.
Hence two evaluation criteria were employed:
(i) correctness of coordination scopes3 (for com-
parison with Shimbo-Hara), and (ii) correctness of
the end of conjuncts (for comparison with KNP).
We report precision, recall and F1 measure, with
the main performance index being F1 measure.
5 Results
Table 1 summarizes the experimental results.
Even Shimbo and Hara?s original method (SH)
outperformed KNP. KNP tends to output too many
coordinations, yielding a high recall but low pre-
cision. By contrast, SH outputs a smaller number
3A coordination scope is deemed correct only if the brack-
eting of constituent conjuncts are all correct.
of coordinations; this yields a high precision but a
low recall.
The distance-based feature decomposition of
Section 3.2 gave +2.0 points improvement over the
original SH in terms of F1 measure in coordination
scope detection. Adding bypasses to alignment
graphs further improved the performance, making
a total of +4.7 points in F1 over SH; recall signifi-
cantly improved, with precision remaining mostly
intact. Finally, the improved model (SH + decom-
position + bypass) achieved an F1 measure +6.4
points higher than that of KNP in terms of end-of-
conjunct identification.
References
F. Chantree, A. Kilgarriff, A. de Roeck, and A. Willis.
2005. Disambiguating coordinations using word
distribution information. In Proc. 5th RANLP.
EDR, 1995. The EDR dictionary. NICT. http://www2.
nict.go.jp/r/r312/EDR/index.html.
D. Hogan. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In Proc. 45th
ACL, pages 680?687.
S. Kurohashi and M. Nagao. 1994. A syntactic analy-
sis method of long Japanese sentences based on the
detection of conjunctive structures. Comput. Lin-
guist., 20:507?534.
S. Kurohashi, Y. Igura, and M. Sakaguchi, 2000. An-
notation manual for a morphologically and sytac-
tically tagged corpus, Ver. 1.8. Kyoto Univ. In
Japanese. http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
corpus/KyotoCorpus4.0/doc/syn guideline.pdf.
H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto.
2009. Bypassed alignment graph for learning coor-
dination in Japanese sentences: supplementary ma-
terials. Tech. report, Grad. School of Information
Science, Nara Inst. Science and Technology. http://
isw3.naist.jp/IS/TechReport/report-list.html#2009.
P. Resnik. 1999. Semantic similarity in a taxonomy. J.
Artif. Intel. Res., 11:95?130.
M. Shimbo and K. Hara. 2007. A discriminative learn-
ing model for coordinate conjunctions. In Proc.
2007 EMNLP/CoNLL, pages 610?619.
8
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130?140,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Modeling and Learning Semantic Co-Compositionality
through Prototype Projections and Neural Networks
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{masashi-t,kevinduh,shimbo,matsu}@is.naist.jp
Abstract
We present a novel vector space model for se-
mantic co-compositionality. Inspired by Gen-
erative Lexicon Theory (Pustejovsky, 1995),
our goal is a compositional model where
both predicate and argument are allowed to
modify each others? meaning representations
while generating the overall semantics. This
readily addresses some major challenges with
current vector space models, notably the pol-
ysemy issue and the use of one represen-
tation per word type. We implement co-
compositionality using prototype projections
on predicates/arguments and show that this
is effective in adapting their word represen-
tations. We further cast the model as a
neural network and propose an unsupervised
algorithm to jointly train word representations
with co-compositionality. The model achieves
the best result to date (? = 0.47) on the
semantic similarity task of transitive verbs
(Grefenstette and Sadrzadeh, 2011).
1 Introduction
Vector space models of words have been very
successful in capturing the semantic and syntactic
characteristics of individual lexical items (Turney
and Pantel, 2010). Much research has addressed
the question of how to construct individual word
representations, for example distributional models
(Mitchell and Lapata, 2010) and neural models
(Collobert and Weston, 2008). These word repre-
sentations are used in various natural language pro-
cessing (NLP) tasks such as part-of-speech tagging,
chunking, named entity recognition, and semantic
Vk	
Pcompany=VkTVk	
Co-Compositionality with Prototype Projections	Co-compositional vector	of verb and object	
start	build	
buy	
operate =	
company	run	
VERB	
runcompany	
Prototype	 Projection	
companyrun	
OBJ	
???	 ???	 Ok	
firm	bank	
hotel	
???	 ???	
Prun=OkTOk	? ?m	?	 ?	
Figure 1: Here, we capture the semantics of run in run
company by projecting the original word representation
of run to the prototype space of company (and vice versa).
role labeling (Turian et al, 2010; Collobert et al,
2011).
Recently, modeling of semantic compositionality
(Frege, 1892) in vector space has emerged as another
important line of research (Mitchell and Lapata,
2008; Mitchell and Lapata, 2010; Baroni and Zam-
parelli, 2010; Socher et al, 2012; Grefenstette and
Sadrzadeh, 2011; Van de Cruys et al, 2013). The
goal is to formulate how individual word represen-
tations ought to be combined to achieve phrasal or
sentential semantics.
The main questions for semantic compositionality
that we are concerned with are: (1) how can poly-
semy be handled by a single vector representation
per word type, learned by either a distributional or
neural model, and (2) how does composition resolve
130
these ambiguities. To this end, we are inspired by
the idea of type coercion and co-compositionality
in Generative Lexicon Theory (Pustejovsky, 1995).
Co-compositionality advocates that instead of a
predicate-argument view of composition, both pred-
icate and argument influence/coerce each other to
generate the overall meaning. For example, consider
a polysemous word like run:
? (a) He runs the company.
? (b) He runs the marathon.
Run may have several senses, but the prototypical
verbs that select for company differ from those
that select for marathon, and thus the ambiguity
at the word level is resolved at the sentence level.
The same is true for the other direction, where the
predicate also coerces meaning to the argument to
fit expectation.
We believe that models for semantic com-
position ought to incorporate elements of co-
compositionality. We propose such a model here,
using what we call prototype projections. For each
predicate, we transform its vector representation by
projecting it into a latent space that is prototypical
of its argument. This projection is performed anal-
ogously for each argument as well, and the final
meaning is computed by composition of these trans-
formed vectors (Figure 1). In addition, the model is
cast as a neural network where word representations
could be re-trained or fine-tuned.1
Our contributions are two-fold:
1. We propose a novel model for semantic co-
compositionality. This model, based on
prototype projections, is easy to implement
and achieves state-of-the-art performance in
the sentence similarity dataset developed by
Grefenstette and Sadrzadeh (2011).
2. Our results empirically confirm that existing
word representations (eg., SDS and NLM in
Section 2) are sufficiently effective at capturing
1While we are inspired by co-compositionality, it is impor-
tant to note that our model does not implement qualia structure
and other important components of Generative Lexicon Theory.
We operate within the vector space model of distributional
semantics, so these ideas are implemented with matrix algebra,
which is a natural fit with neural networks.
polysemy, as long as we have the proper mech-
anism to tease out the proper sense during com-
position. We further propose an unsupervised
neural network training algorithm that jointly
fine-tunes the word representations within the
co-composition model, resulting in even better
performance on the sentence similarity task.
We would like to emphasize the second contribu-
tion especially. Semantics research is divided in two
strands, one focusing on learning word represen-
tations without consideration for compositionality,
and the other focusing on compositional semantics
using the representations only as an input. But issues
are actually related from the linguistics perspective,
and even more so if we adopt a Generative Lexicon
perspective. Our neural network model bridges
these two strands of research by modeling co-
compositionality and learning word representations
simultaneously. We note that methods using context
effects have been explored by Erk and Pado? (2008;
2009) and Thater et al (2010; 2011), but to the
best of our knowledge, ours is the first model to
perform co-compositionality and learning of word
representations jointly.
In the following, we first provide background to
the word representations employed here (Section 2).
We describe the model for co-compositionality in
Section 3 and the corresponding neural network in
Section 4. Evaluation and experiments are presented
in Sections 5 and 6. Finally, we end with related
work (Section 7) and conclusions (Section 8).
2 Word Vector Representations
2.1 Simple Distributional Semantic space
(SDS) word vectors
Word meaning is often represented in a high di-
mensional space, where each element corresponds to
some contextual element in which the word is found.
Mitchell and Lapata (2010) present a co-occurrence-
based semantic space called Simple Distributional
Semantic space (SDS). Their SDS model uses a con-
text window of five words on either side of the target
word and 2,000 vector components, representing the
most frequent context words (excluding a list of stop
words). These components vi(t) were set to the
ratio of the probability of the context word given the
131
target word to the probability of the context word
overall:
vi(t) =
p(ci|t)
p(ci)
= freqci,t ? freqtotalfreqt ? freqci
(1)
where freqci,t, freqtotal, freqt and freqci are the
frequencies of the context word ci with the target
word t, the total count of all word tokens, the
frequency of the target word t, and the frequency
of the context word ci, respectively.
2.2 Neural Language Model (NLM) word
embeddings
Another popular way to learn word representations
is based on the Neural Language Model (NLM)
(Bengio et al, 2003). In comparison with SDS,
NLM tend to be low-dimensional (e.g. 50 dimen-
sions) but employ dense features. These dense
feature vectors are usually called word embeddings,
and it has been shown that such vectors can cap-
ture interesting linear relationships, such as king ?
man + woman ? queen (Mikolov et al, 2013).
In this work, we adopt the model by Collobert
and Weston (2008). The idea is to construct a
neural network based on word sequences, where
one outputs high scores for n-grams that occur in a
large unlabeled corpus and low scores for nonsense
n-grams where one word is replaced by a random
word. This word representation with NLM has been
used to good effect, for example in (Turian et al,
2010; Collobert et al, 2011; Huang et al, 2012)
where induced word representations are used with
sophisticated features to improve performance in
various NLP tasks.
Specifically, we first represent the word sequence
as a vector x = [d(w1);d(w2); . . . ;d(wm)], where
wi is ith word in the sequence, m is the win-
dow size, d(w) is the vector representation of
word w (an n-dimensional column vector) and
[d(w1);d(w2); . . . ;d(wm)] is the concatenation of
word vectors as an input of neural network. Second,
we compute the score of the sequence,
score(x) = sT(tanh(Wx+ b)) (2)
where W ? Rh?(mn) and s ? Rh are the first
and second layer weights of the neural network,
and b ? Rh is the bias unit of hidden layer. The
superscript T represents transposition, and tanh is
applied element-wise. We also create a corrupted
sequence xc = [d(w1);d(w2); . . . ;d(wm?)] where
wm? is chosen randomly from the vocabulary. We
compute the score of this implicit negative sequence
xc with the same neural network, score(xc) =
sT(tanh(Wxc + b)). Finally, we get the cost
function of this training algorithm as follow.
J = max(0, 1 ? score(x) + score(xc)) (3)
In order to minimize this cost function, we optimize
the parameters ? = (s,W,b,x) via backpropagation
with stochastic gradient descent (SGD).
3 The Model
3.1 Prototype Projection
Generative Lexicon Theory (Pustejovsky, 1995)
makes a distinction between accidental polysemy
(homonyms, e.g. bank as financial institution vs.
as river side) and logical polysemy (e.g. figure and
ground meanings of door). Our model handles both
cases using the concept of projection to latent proto-
type space. The fundamental idea is that for each
word w and a syntactic/semantic (binary) relation
R (such as verb-object relation), w has a set of
prototype words with which it frequently occurs in
relation R. For example, if w is a word company,
and R is the object-verb relation, prototype words
should include start, build, and buy (Figure 1).
For each word-relation pair, we pre-compute the
latent semantic subspace spanned by these prototype
words.
Later, when we encounter a phrase expressing a
relationR between two wordsw1 andw2, each word
is first projected onto a latent subspace determined
by the other word and relation R. The projection
operation shifts the meaning of individual words in
accordance with context, and through this operation
we realize coercion/co-composition. And finally, the
meaning of the phrase is computed from the two
projected points in the semantic space.
Let us describe how to compute the latent sub-
space associated with a word w0 and a relation R.
First, we collect from a corpus a set of prototype
words that occur frequently in relation R with target
word w0. So for example in Figure 1, if w0 =
132
verb object landmark similarity(verb, landmark) similarity(projected verb, landmark)
run company operate 0.40 0.70
meet criterion satisfy 0.49 0.71
spell name write 0.04 0.50
Table 1: Examples of verb-object pairs. Original verb and landmark verb similarity, prototype projected verb and
landmark verb similarity, as measure by cosine using Collobert and Weston?s word embeddings. Meet has a abstract
meaning itself, but after prototype projection with matrix constructed by word vectors of W (VerbOf, criterion), meet
is more close to meaning of satisfy.
company, and R = VerbOf is the object-verb
relation,
W (VerbOf, company) = {start, build, . . . , buy}.
Now let W (R,w0) = {w1, w2, ? ? ? , wm} be
the m prototype words we collected, and let d(w)
denote the n-dimensional (column) vector represen-
tation of word w (either by SDS or NLM representa-
tion). We make anm?nmatrixC(R,w0) by stacking
the prototype word vectors, i.e.,
C(R,w0) = [d(w1),d(w2), ? ? ? ,d(wm)]
T (4)
and then apply Singular Value Decomposition
(SVD) to extract the latent space from this matrix:
C(R,w0) ? Uk?kV
T
k . (5)
word1	
word vector dimension n	
word2	
wordm	
m	
? ? ?	
?????	
k	
k	
n	
? ? ?	
? ? ?	
?????	
? ? ?	
? ? ?	
k	
k	?k VkTUkC ?
Figure 2: Graphical representation of SVD in our model.
Figure 2 shows the graphical representation of
this matrix factorization. In NLP tasks, SVD is
often applied to a term-document matrix, but in our
model, we apply SVD to the matrix consisting of
word vectors.
Intuitively, ?kVTk represents the latent sub-
space formed by prototypical words W (R,w0) =
{w1, w2, ? ? ? , wm}. We call this matrix the proto-
type space of word w0 with respect to relation R.
Note that the matrix of orthogonal projection
onto this prototype space is given by P(R,w0) =
(?kVTk)T(?kVTk). Hence, when we observe a rela-
tion R(w0, w), the projected representation of word
w in this context is computed by prpj(R,w0)(w)
defined as follows:
prpj(R,w0)(w) = P(R,w0)d(w). (6)
Table 1 shows several examples of how meanings
change after prototype projection using word em-
beddings of Collobert and Weston (2008).2
3.2 Co-Compositionality
In order to model co-compositionality, we apply
prototype projection to both the verb and the object.
In particular, suppose verb is wv and object is wo,
C(VerbOf,wo) is used to project wv and C(ObjOf,wv)
is used to project wo. The vector that represents
the overall meaning of verb-object with prototype
projection is computed by:
cocomp(wv, wo) =
f(prpj(VerbOf,wo)(wv),prpj(ObjOf,wv)(wo)) (7)
Function f can be a compositional computation like
simple addition or element-wise multiplication of
two vectors. This is graphically shown in Figure 1.
4 Unsupervised Learning of
Co-Compositionality
In this section, we propose a new neural language
model that learns word representations while jointly
accounting for compositional semantics. One cen-
tral assumption of our work (and many other works
in compositional semantics) is that a single vector
2ronan.collobert.com/senna/
133
v	 o	
z = f(v, o)	s	
score = sTz	
Compositional	Neural Language Model (C-NLM)	
verb	 obj	
Figure 3: Compositional Neural Language Model (C-
NLM).
per word type sufficiently represents the multiple
meanings and usage patterns of a word.3 That
means that for a polysemous word, its word vector
actually represents an aggregation of the distinctly
different contexts it occurs in. We will show that
such an assumption is quite reasonable under our
model, since the prototype projections successfully
tease out the proper semantics from these aggregate
representations.
However, it is natural to wonder whether one
can do better if one incorporates the compositional
model into the training of the word representations
in the first place. To do so, we formulate a nov-
el model called Compositional Neural Language
Model (Section 4.1). This model is a combination
of an unsupervised training algorithm with basic
compositionality (addition/multiplications). Then,
we extend this model with the projection idea in
section 3.2 to formulate a Co-Compositional Neural
Language Model (Section 4.2).
4.1 Compositional Neural Language Model
(C-NLM)
Compositional Neural Language Model (C-NLM)
is a combination of a word representation learning
method and compositional rule. In contrast to other
compositional models based on machine learning,
our model has no complex parameters for model-
ing composition. Composition is modeled using
straightforward vector addition/multiplications; in-
stead, what is learned is the word representation.
Figure 3 shows the C-NLM. The learning al-
gorithm is unsupervised, and works by artificially
3There are works on multiple representations, e.g.,
(Reisinger and Mooney, 2010); we focus on single represen-
tation here.
z = f(v, o)	s	
score = sTz	
Pobj	 Pverb	
Co-Compositional	Neural Language Model (CoC-NLM)	
v	 o	
y	x	
verb	 obj	
Figure 4: Co-Compositional Neural Language Model
(CoC-NLM) is C-NLM with prototype projection.
generating negative examples in a fashion analogous
to the NLM learning algorithm of (Collobert and
Weston, 2008) and contrastive estimation (Smith
and Eisner, 2005). First, given some initial word
representations and raw sentences, we compute the
compositional vector with function f (in this sec-
tion, we will assume that we will be using the
addition operator). Second, in order to obtain the
score of compositional vector, we compute the dot
product with vector s ? Rn (n is the dimension of
the word vector space): verb vector v = d(wv) and
object vector o = d(wo).
score(v,o) = sTf(v,o) = sT(v + o) (8)
We also create a corrupted pair by substituting a ran-
dom verb wverb?. The cost function J = max(0, 1?
score(v,o) + score(vc,o)), where vc is the word
vector of wverb?, encourages that the score of correct
pair is higher than the score of the corrupt pair. Let
z = v + o, our model parameters are ? = (s, z,v).
The optimization is divided into two steps:
1. Optimize s and z via SGD.
2. Let znew be the updated z via step 1. The new
verb vector vnew trained within additive composi-
tionality is just vnew = znew ? o. Note that if we
also want to optimize o, we may want to also corrupt
the object and run SGD in step 2 as well.
4.2 Co-Compositional Neural Language Model
(CoC-NLM)
We now add prototype projection into C-NLM,
making our final model: Co-Compositional Neural
134
Language Model (CoC-NLM). We define the score
function as dot product of s and additional vector of
prototype projected vectors (Figure 4). Let Pobj =
P(VerbOf,wo) and Pverb = P(ObjOf,wv),
score(v,o) = sT(Pobjv +Pverbo). (9)
Let x = Pobjv, y = Pverbo and z = x + y.
Our model parameters are ? = (s, z,v). The
optimization algorithm of CoC-NLM is divided into
three steps like C-NLM. First, we optimize s and
z. Second, the projected verb vector is updated
as xnew = znew ? y. Finally we optimize v to
minimize the Euclidean distance between xnew and
Pobjv, where ? is a regularization hyper-parameter:
J(v) = 12 ||xnew ?Pobjv||
2 + ?2v
Tv (10)
5 Evaluation
5.1 Dataset
In order to evaluate the performance of our new
co-compositional model with prototype projection
and word representation learning algorithm, we
make use of the disambiguation task of transitive
sentences developed by Grefenstette and Sadrzadeh
(2011). This is an extension of the two words
phrase similarity task defined in Mitchell and Lapata
(2008), and constructed according to similar guide-
lines. The dataset consists of similarity judgments
between a landmark verb and a triple consisting of
a transitive target verb, subject and object extracted
from the BNC corpus. Human judges give scores
between 1 to 7, with higher scores implying higher
semantic similarity. For example, Table 2 shows
some examples from the data: we see that the verb
meet with subject system and object criterion is
judged similar to the landmark verb satisfy but not
visit. The dataset contains a total of 2500 similarity
judgements, provided by 25 participants.4 The
task is to have the model produce a score for each
pair of landmark verb and verb-subject-object triple.
Models are evaluated by computing the Spearman?s
? correlation between its similarity scores and that
of the human judgments.
4http://www.cs.ox.ac.uk/
people/edward.grefenstette/
verb subj obj landmark sim
meet system criterion satisfy 6
meet system criterion visit 1
write student name spell 7
write student paper spell 2
Table 2: Examples from the disambiguation task de-
veloped by Grefenstette and Sadrzadeh (2011). Human
judges give scores between 1 to 7, with higher scores
implying higher semantic similarity. Verb meet with
subject system and object criterion is judged similar to
the landmark verb satisfy but not visit.
5.2 Baselines
We compare our model against multiple baselines
for semantic compositionality:
1. Mitchell and Lapata?s (2008) additive and
element-wise multiplicative model as simplest
baselines.
2. Grefenstette and Sadrzadeh?s (2011) model
based on the abstract categorical framework
(Coecke et al, 2010). This model computes
the outer product of the subject and object
vector, the outer product of the verb vector
with itself, and then the element-wise product
of both results.
3. Erk and Pado??s (2008) model, which adapts the
word vectors based on context and is the most
similar in terms of motivation to ours.
4. Van de Cruy et al (2013) multi-way interaction
model based on matrix factorization. This
achieves the best result for this task to date.
A detailed explanation of these models will be
provided in Section 7. For the underlying word rep-
resentations, we experiment with sparse 2000-dim
SDS and dense 50-dim NLM. These are provided
by Blacoe and Lapata (2012)5 and trained on the
British National Corpus (BNC). We are interested
in knowing how sensitive each model is to the
underlying word representation. In general, this is
a challenging task: the upper-bound of ? = 0.62 is
the inter-annotator agreement.
5http://homepages.inf.ed.ac.uk/
s1066731/index.php?page=resources
135
5.3 Implementation details
In terms of implementation detail, our model and our
re-implementation of Erk and Pado?s model make
use of the ukWaC corpus (Baroni et al, 2009).6 This
corpus is a two billion word corpus automatically
harvested from the web and parsed by the Malt-
Parser (Nivre et al, 2006). We use ukWaC corpus
to collect W (VerbOf, wo) and W (ObjOf, wv) for
prototype projections. We also extract about 5000
verb-object pairs that relevant for testdata from this
corpus to train our neural network learning algorith-
m. In our co-compositional model, the contribution
ratio of SVD is set to 80% (i.e. automatically
fixing k in SVD to include 80% of the top singular
values). We set the number of prototype vectors
to be m = 20, where W (VerbOf, wo) is filtered
with high frequency words and W (ObjOf, wv) is
filtered with both high frequency and high similarity
words. In our model, we output the scores for SVO
triple sentence dataset as (subject=ws, verb=wv,
object=wo, f = Addition/Multiplication):
cocomp(ws, wv, wo) =
f(d(ws), cocomp(wv, wo)) (11)
6 Results and Discussion
6.1 Main Results: The Correlation
Table 3 shows the correlation scores of various
models. Our observations are as follows:
1. The best reported result for this task (Van de
Cruys et al, 2013) is ? = 0.37. Our
model (with NLM as word representation and
f=Addition as operator) achieves ? = 0.44,
outperforming it by a large margin. To the best
of our knowledge, this is now state-of-the-art
result for this task.
2. Our model is not very sensitive to the underly-
ing word representation. With f=Addition, we
have ? = 0.41 for SDS vs ? = 0.44 for NLM.
With f=Multiply, we have ? = 0.37 for SDS
vs. ? = 0.35 for NLM. This implies that the
prototype projection is robust to the underlying
word representation, which is a desired charac-
teristic of compositional models.
6http://wacky.sslmit.unibo.it/
doku.php?id=corpora
Model ?
Grefenstette and Sadrzadeh (2011) ? 0.21
Add (SDS) ? 0.31
Add (NLM) ? 0.31
Multiply (SDS) ? 0.35
Multiply (NLM) ? 0.30
Van de Cruys et al (2013) 0.37
Erk and Pado? (SDS) 0.39
Erk and Pado? (NLM) 0.03
Co-Comp with f=Add (SDS) 0.41
Co-Comp with f=Add (NLM) ? 0.44
Co-Comp with f=Multiply (SDS) 0.37
Co-Comp with f=Multiply (NLM) 0.35
Upper bound ? 0.62
Table 3: Results of the different compositionality models
on the similarity task. The number of prototype words
m = 20 in all our models. Our model (f=Addition and
NLM) achieves the new state-of-the-art performance for
this task (? = 0.44).
3. The contextual model of Erk and Pado? (SDS)
also performed relatively well (? = 0.39),
in fact outperforming the Van de Cruy et al
(2013) result as well. This means that the
general idea of adapting word representations
based on context is a very powerful one. How-
ever, Erk and Pado??s model using the NLM rep-
resentation is extremely poor (? = 0.03). The
reason is that it uses a product operation under-
the-hood to adapt the vectors, which inherently
assumes a sparse representation. In this sense,
our projection approach is more robust.
The state-of-the-art result for our model in Table
3 does not yet make use of the training algorithm
described in Section 4. It is simply implementing
the co-compositionality idea using prototype projec-
tions (Section 3.2). Next in Section 6.2 we will show
additional gains using unsupervised learning.
6.2 Improvements from unsupervised learning
In this experiment, we examine how much gain is
possible by re-training the word representation of
verbs using the unsupervised algorithm described
in Section 4. We focus on the additive model
of Compositional NLM, both basic and prototype
projection. The initial word representation is from
136
model original representation re-trained
C-NLM 0.31 0.38
CoC-NLM 0.44 ? 0.47
Table 4: Results of re-training the word representation
for C-NLM and CoC-NLM. Learning rate ? = 0.01,
regularization ? = 10?4 and iteration = 20. One iteration
is one run through the dataset of 5000 verb-object pairs
which we made from the ukWaC corpus.
NLM. Table 4 shows the gains in correlation score.
This result shows that our learning model suc-
cessfully captures good representation within co-
compositionality of additive model. In contrast to
other previous compositional models, our model
does not require estimating a large number of pa-
rameters for computation of compositional vectors
and word representation itself is more suitable for
it. Furthermore, learning is very fast, taking about
10 minutes for C-NLM on a standard machine with
Intel Core i7 2.93Ghz CPU and 8GB of RAM.
6.3 The number of prototype words
The number of prototype words (m in Figure 1) we
use to generate the prototype space is one hyper-
parameter that our model has. Here, we analyze the
effect of the choice of m. Figure 5 shows the rela-
tion of m and the performance of co-compositional
model with prototype projections using either SDS
or NLM representations. In general, both NLM
and SDS show relatively smooth and flat curves
across m, indicating the relative robustness of the
approach. Nevertheless, results do degrade for large
m, due to increase in noise from non-prototype
words. Further, it does appear that NLM has a slow-
er drop in correlation with increasing m compared
with SDS. This suggests that NLM is more robust,
which is possibly attributable to the dense and low-
dimensional distributed features.
6.4 Variations in model configuration
We have presented a compositional model of the
form d(ws) + cocomp(wv, wo), where prototype
projections are performed on both wv and wo and
ws is composed as is without projection. In general,
we have the freedom to choose what to project
and what not to project under this co-compositional
framework. Here in Table 5 we show the results of
Figure 5: The relation between the number of prototype
words and correlation of SDS or NLM. In general, NLM
has higher correlation than SDS and is more robust across
the m.
Subj Verb Obj NLM ? SDS ?
prpj prpj prpj 0.39 0.37
+ prpj prpj 0.44 0.41
prpj prpj 0.45 0.41
+ prpj + 0.43 0.38
prpj + 0.43 0.38
+ + + 0.31 0.31
Table 5: Variants of the full co-compositional model,
based on how subject, verb, and object vector repre-
sentations are included. prpj indicates that prototype
projection is used. + indicates that the vector is added
without projection first. Blank indicates that the vector is
not used in the final compositional score.
these variants, using f =Addition and SDS/NLM
representations without re-training. We note that
our positive results mainly come from the verb
projections. Subject information actually does not
help. We believe this best configuration is task-
dependent; in this test collection, the subjects appear
to have little contribution to the landmark verb.
7 Related work
In recent years, several sophisticated vector space
models have been proposed for computing compo-
sitional semantics. Mitchell and Lapata (2010), Erk
(2012) and Baroni et al (2013) are recommended
survey papers.
137
One of the first approaches is the vector ad-
dition/multiplication idea of Mitchell and Lapata
(2008). The appeal of this kind of simple approach
is its intuitive geometric interpretation and its ro-
bustness to various datasets. However, it may not
be sufficiently expressive to represent the various
factors involved in compositional semantics, such
as syntax and context. To this end, Baroni and
Zamparelli (2010) present a compositional model
for adjectives and nouns. In their model, an adjective
is a matrix operator that modifies the noun vector
into an adjective-noun vector. Zanzotto et al (2010)
and Guevara (2010) also proposed linear transfor-
mation models for composition and address the issue
of estimating large matrices with least squares or
regression techniques. Socher et al (2012) extend
this linear transformation approach with the more
powerful model of Matrix-Vector Recursive Neural
Networks (MV-RNN). Each node in a parse tree is
assigned both a vector and a matrix. The vector
captures the actual meaning of the word itself, while
the matrix is modeled as a operator that modify the
meaning of neighboring words and phrases. This
model captures semantic change phenomenon like
not bad is similar to good due to a composition
of the bad vector with a meaning-flipping not ma-
trix. But this MV-RNN also need to optimize all
matrices of words from initial value (identity plus
a small amount of Gaussian noise) with supervised
dataset like movie reviews. Our prototype projection
model is similar to these models as a matrix-vector
operation, except that the matrix is not learned and
computed from prototype words. In future work,
we can imagine integrating the two models, using
these prototype projection matrices as initial values
for MV-RNN training (Socher et al, 2012).
Another approach is exemplified by Coecke et
al. (2010). In their mathematical framework u-
nifying categorical logic and vector space models,
the sentence vector is modeled as a function of the
Kronecker product of its word vectors. Grefenstette
and Sadrzadeh (2011) implement this based on un-
supervised learning of matrices for relational words
and apply them to the vectors of their arguments.
Their idea is that words with relational types, such as
verbs, adjectives, and adverbs are matrices that act
as a filter on their arguments. They also developed
a new semantic similarity task based on transitive
Composition Operator Parameter
Add: w1u + w2v w1, w2 ? R
Multiply: uw1 ? vw2 w1, w2 ? R
FullAdd: W1u + W2v W1,W2 ? Rn?n
LexFunc: Auv Au ? Rn?n
FullLex: ?([W1Auv,W2Avu])
?
Au, Av ? Rn?n
W1,W2 ? Rn?n
Ours (Add): P(R,v)u + P(R,u)v SVD?s (m, k)
Ours (Mult): P(R,v)u? P(R,u)v SVD?s (m, k)
Table 6: Comparison of composition operators that com-
bine two word vector representations, u, v ? Rn and
their learning parameters. Our model only needs two
hyper-parameters: the number of prototype words m and
dimensional reduction k in SVD
verbs, which is the dataset we used here. The pre-
vious state-of-the-art result for this task comes from
the model of Van de Cruys et al (2013). They model
compositionality as a multi-way interaction between
latent factors, which are automatically constructed
from corpus data via matrix factorization.
Comprehensive evaluation of various existing
models are reported in (Blacoe and Lapata, 2012; D-
inu et al, 2013). Blacoe and Lapata (2012) highlight
the importance of jointly examining word represen-
tations and compositionality operators. However,
two out of three composition methods they evaluate
are parameter-free, so that they can side-step the
issue of parameter estimation. Dinu et al (2013) de-
scribe the relation between word vector and compo-
sitionality in more detail with free parameters. Table
6 summarizes some ways to compose the meaning
of two word vectors (u, v), following (Dinu et al,
2013). These range from simple operators (e.g. Add
and Multiply) to expressive models with many free
parameters (e.g. LexFunc, FullLex). Many of these
models need to optimize n ? n parameters, which
may be large. On the other hand, our model only
needs two hyper-parameters: the number of proto-
type words m and dimensional reduction k in SVD
(Table 6). Furthermore, our model performance with
neural language model word embeddings is robust to
variations in m.
Most closely related to our work is the work by
Erk and Pado? (2008; 2009) and Thater et al (2010;
2011), which falls under the research theme of
computing word meaning in context. Both methods
are characterized by the use of selectional prefer-
138
ence information for subjects, verbs, and objects in
context; our prototype word vectors are essentially
equivalent to this idea. The main difference is in
how we modify the target word representation v
using this information: whereas we project v onto
a latent subspace formed by collection of prototype
vectors, Erk and Pado? (2008; 2009) and Thater
et al (2010; 2011) use the prototype vectors to
directly modify the elements of v, i.e. by element-
wise product with the centroid prototype vector.
Intuitively, both our method and theirs essentially
delete part of a word vector representation to adapt
the meaning in context. We believe the projection
is more robust to the underlying word representation
(and this is shown in the results for SDS vs. NLM
representations), but we note that we may be able
to borrow some of more sophisticated ways to find
prototype vectors from Erk and Pado? (2008; 2009)
and Thater et al (2010; 2011).
8 Conclusion and Future Work
We began this work by asking how it is possible to
handle polysemy issues in compositional semantics,
especially when adopting distributional semantics
methods that construct only one representation per
word type. After all, the different senses of the
same word are all conflated into a single vector
representation. We found our inspiration in Gen-
erative Lexicon Theory (Pustejovsky, 1995), where
ambiguity is resolved due to co-compositionality of
the words in the sentence, i.e., the meaning of an
ambiguous verb is generated by the properties the
object it takes, and vice versa. We implement this
idea in a novel neural network model using proto-
type projections. The advantages of this model is
that it is robust to the underlying word representation
used and that it enables an effective joint learning
of word representations. The model achieves the
current state-of-the-art performance (? = 0.47)
on the semantic similarity task of transitive verbs
(Grefenstette and Sadrzadeh, 2011).
Directions for future research include:
? Experiments on other semantics tasks, such
as paraphrase detection, word sense induction,
and word meaning in context.
? Extension to more holistic sentence-level com-
position using a matrix-vector recursive frame-
work like (Socher et al, 2012).
? Explore further the potential synergy between
Distributional Semantics and the Generative
Lexicon.
Acknowledgments
This work was partially supported by JSPS KAK-
ENHI Grant Number 24800041, JSPS KAKENHI
2430057 and Microsoft Research CORE Project.
We would like to thank Hiroyuki Shindo and anony-
mous reviewers for their helpful comments.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language resources and evaluation,
43(3):209?226.
Marco Baroni, Raffaella Bernardi, and Roberto Zampar-
elli. 2013. Frege in space: A program for compo-
sitional distributional semantics. Linguistic Issues in
Language Technologies.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
William Blacoe andMirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a composi-
tional distributional model of meaning. CoRR, ab-
s/1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
139
Journal of Machine Learning Research, 12:2493?
2537.
Georgiana Dinu, Nghia The Pham, and Marco Barori.
2013. General estimation and evaluation of composi-
tional distributional semantic models. In Proceedings
of the Workshop on Continuous Vector Space Models
and their Compositionality.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language Semantics.
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635?653.
G Frege. 1892. U?ber sinn und bedeutung. In Zeitschfrift
fu?r Philosophie und philosophische Kritik, 100.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the Workshop on GEomet-
rical Models of Natural Language Semantics.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In Human Language Technologies:
The Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT).
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1439.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the International
Conference on Language Resources and Evaluation
(LREC).
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, MA.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Human Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT).
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and
effective vector model. In Asian Federation of Natural
Language Processing (IJCNLP).
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of artificial intelligence research, 37(1):141?
188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.
2013. A tensor-based factorization model of semantic
compositionality. In Human Language Technologies:
The Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT).
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar.
2010. Estimating linear models for compositional
distributional semantics. In Proceedings of
the International Conference on Computational
Linguistics (COLING).
140
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 613?623,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Centering Similarity Measures to Reduce Hubs
Ikumi Suzuki
National Institute of Genetics
Mishima, Shizuoka, Japan
suzuki.ikumi@gmail.com
Kazuo Hara
National Institute of Genetics
Mishima, Shizuoka, Japan
kazuo.hara@gmail.com
Masashi Shimbo
Nara Institute of Science and Technology
Ikoma, Nara, Japan
shimbo@is.naist.jp
Marco Saerens
Universite? catholique de Louvain
Louvain-la-Neuve, Belgium
marco.saerens@uclouvain.be
Kenji Fukumizu
The Institute of Statistical Mathematics
Tachikawa, Tokyo, Japan
fukumizu@ism.ac.jp
Abstract
The performance of nearest neighbor methods
is degraded by the presence of hubs, i.e., ob-
jects in the dataset that are similar to many
other objects. In this paper, we show that the
classical method of centering, the transforma-
tion that shifts the origin of the space to the
data centroid, provides an effective way to re-
duce hubs. We show analytically why hubs
emerge and why they are suppressed by cen-
tering, under a simple probabilistic model of
data. To further reduce hubs, we also move
the origin more aggressively towards hubs,
through weighted centering. Our experimental
results show that (weighted) centering is effec-
tive for natural language data; it improves the
performance of the k-nearest neighbor classi-
fiers considerably in word sense disambigua-
tion and document classification tasks.
1 Introduction
1.1 Background
The k-nearest neighbor (kNN) algorithm is a sim-
ple nonparametric method of classification. It has
been applied to various natural language process-
ing (NLP) tasks such as document classification
(Masand et al, 1992; Yang and Liu, 1999), part-
of-speech tagging (S?gaard, 2011), and word sense
disambiguation (Navigli, 2009).
To apply the kNN algorithm, data is typically rep-
resented as a vector object in a feature space, and
(dis)similarity between data is measured by the dis-
tance between the vectors, their inner product, or co-
sine of the angle between them (Jurafsky and Mar-
tin, 2008). With such a (dis)similarity measure, the
unknown class label of a test object is predicted by
a majority vote of the classes of its k most similar
objects in the labeled training set.
Recent studies (Radovanovic? et al, 2010a;
Radovanovic? et al, 2010b) have shown that if the
feature space is high-dimensional, some objects in
the dataset emerge as hubs; i.e., these objects fre-
quently appear in the k nearest neighbors of other
objects.
The emergence of hubs may deteriorate the per-
formance of kNN classification and nearest neighbor
search in general:
? If hub objects exist in the training set, they have
a strong chance to be a kNN of many test ob-
jects. Because the class of a test object is pre-
dicted by a majority vote from its k nearest
neighbors, prediction is biased toward the la-
bels of the hubs.
? In information retrieval, nearest neighbor
search finds objects in the database that are
most relevant, or similar, to user-provided
queries. If particular objects, such as hubs, are
nearly always returned for any query, the re-
trieved results are probably not very useful.
These drawbacks may hinder application of near-
est neighbor methods in NLP, as typical natural lan-
guage data are extremely high-dimensional (Juraf-
sky and Martin, 2008) and thus prone to produce
hubs.
1.2 Contributions
Centering (Mardia et al, 1979; Fisher and Lenz,
1996; Eriksson et al, 2006) is a standard technique
613
for removing observation bias in the data. It is a
transformation of feature space in a way that the ori-
gin of the space is moved to the data centroid (sam-
ple mean). The distance between data objects is not
changed by centering, but their inner product and co-
sine are affected; see Section 3 for detail.
In this paper, we advocate the use of centering as a
means of reducing hubs. Specifically, we propose to
measure the similarity of objects by the inner prod-
uct (not distance or cosine) in the centered feature
space.
Our approach is motivated by the observation that
the objects similar to the data centroid tend to be-
come hubs (Radovanovic? et al, 2010a). This ob-
servation suggests that the number of hubs may be
reduced if we can define a similarity measure that
makes all objects in a dataset equally similar to the
centroid (Suzuki et al, 2012). The inner product in
the centered space indeed enjoys this property.
In Section 4, we analyze why hubs emerge under
a simple probabilistic model of data, and also give
an account of why they are suppressed by centering.
Using both synthetic and real datasets, we show
that objects similar to the centroid also emerge as
hubs in multi-cluster data (Section 5), so the applica-
tion of centering is wider than expected. To further
reduce hubs, we also propose to move the origin of
the space more aggressively towards hubs, through
weighted centering (Section 6).
In Section 7, we show that centering and weighted
centering are effective for natural language data.
these methods markedly improve the performance
of kNN classifiers in word sense disambiguation and
document classification tasks.
2 Related work
Centering is a classical technique widely used in
many fields of science. For instance, centering
forms a preprocessing step in principal component
analysis and Fisher linear discriminant analysis.
In NLP, however, centering is seldom used; the
use of cosine and inner product similarities is quite
common, but they are nearly always used uncen-
tered. Non-centered cosine is used, for instance, in
word sense disambiguation (Schu?tze, 1998; Navigli,
2009), paraphrasing (Erk and Pado?, 2008; Thater
et al, 2010), and compositional semantics (Mitchell
and Lapata, 2008), to name a few.
There have been several approaches to improv-
ing kNN classification: learning similarity/distance
measures from training data (metric learning)
(Weinberger and Saul, 2009; Qamar et al, 2008),
weighting nearest neighbors for similarity-based
classification (Chen et al, 2009), and neighbor-
hood size selection (Wang et al, 2006; Guo and
Chakraborty, 2010). However, none of these have
addressed the reduction of hubs.
More recently, Schnitzer et al (2012) proposed
the Mutual Proximity transformation that rescales
distance measures to decrease hubs in a dataset.
Suzuki et al (2012) showed that kernels based on
graph Laplacian, such as the commute-time kernels
(Saerens et al, 2004) and the regularized Laplacian
(Chebotarev and Shamis, 1997; Smola and Kondor,
2003), make all objects equally similar to the data
centroid, which in turn reduce hubs.
In Section 7, we evaluate centering, Mutual Prox-
imity, and Laplacian kernels in NLP tasks, and
demonstrate that centering is equally or even more
effective. Section 4 presents a theoretical justifica-
tion for using centering to reduce hubs, but this kind
of analysis is missing for the Laplacian kernels.
Centering is easier to compute as well. For a
dataset of n objects, it takes O(n2) time to com-
pute, whereas computing a Laplacian-based kernel
requires O(n3) time for matrix inversion. Mutual
Proximity also has a time complexity of O(n2).
3 Centering
Consider a dataset of n objects in an m-dimensional
feature space, x1, ? ? ? , xn ? Rm. Throughout this
paper, we use the inner product ?xi, x j? as a measure
of similarity between xi and x j. Let K be the Gram
matrix of the n feature vectors, i.e., the n ? n matrix
whose (i, j) element holds ?xi, x j?. Using m? n data
matrix X = [x1, ? ? ? , xn], we can write K as
K = XTX,
where XT represents the matrix transpose of X.
Centering is a transformation in which the origin
of the feature space is shifted to the data centroid
x? =
1
n
n?
i=1
xi, (1)
614
and object x is mapped to the centered feature vector
xcent = x ? x?. (2)
The similarity between two objects x and x? is now
measured by ?xcent, x?cent? = ?x ? x?, x? ? x??.
After centering, the inner product between any
object and the data centroid (which is a zero vector
because x?cent = x? ? x? = 0) is uniformly 0; in other
words, all objects in the dataset have an equal simi-
larity to the centroid. According to the observation
that the objects similar to the centroid become hubs
(Radovanovic? et al, 2010a), we can expect hubs to
be reduced after centering.
Intuitively, centering reduces hubs because it
makes the length of the feature vector xcent short
for (hub) objects x that lie close to the data centroid
x?; see Eq. (2). And since we measure object simi-
larity by inner product, shorter vectors tend to pro-
duce smaller similarity scores. Hence objects close
to the data centroid become less similar to other ob-
jects after centering, and no longer be hubs. In Sec-
tion 4, we analyze the effect of centering on hubness
in more detail.
3.1 Centered Gram matrix
Let I be an n ? n identity matrix and 1 be an n-
dimensional all-ones vector. The symmetric matrix
H = I?(1/n)11T is called centering matrix, because
the centered data matrix Xcent = [xcent1 , ? ? ? , x
cent
n ]
can be computed by Xcent = XH (Mardia et al,
1979).
The Gram matrix Kcent of the centered feature
vectors, whose (i, j) element holds the inner prod-
uct ?xcenti , x
cent
j ?, can be calculated from the original
Gram matrix K by
Kcent =
(
Xcent
)T (
Xcent
)
= HXTXH = HKH. (3)
Eq. (3) implies that the original data matrix X is
not needed to compute the centered Gram matrix
Kcent, provided that K is given. It is hence possi-
ble to use the so-called kernel trick; i.e., centering
can be applied even if data matrix X is not available
but the similarity of objects can be measured by a
kernel function in an implicit feature space.
4 Theoretical analysis of the effect of
centering on hubness
We now analyze why objects most similar to the
centroid tend to be hubs in the dataset, and give an
explanation as to why centering may suppress the
emergence of hubs.
4.1 Before centering
Consider a dataset of m-dimensional feature vectors,
with each vector x ? Rm generated independently
from a distribution with a finite mean vector ?. In
other words, objects x in this dataset are drawn from
a distribution P(x), i.e.,
x ? P(x),
and
? = E[x] =
?
x dP(x) (4)
where E[?] denotes the expectation of a random vari-
able.
We will use the following elementary lemma on
the distributions of inner product subsequently.
Lemma 1. Let a ? Rm be a fixed vector, and x ? Rm
be an object sampled according to distribution P(x).
Then the inner product ?a, x? follows a distribution
with mean ?a,??.
Proof. From the linearity of the inner product and
Eq. (4), we obtain
E[?a, x?] =
?
?a, x? dP(x)
= ?a,
?
x dP(x)? = ?a,??. 
Now, imagine that we have an object x sam-
pled from P(x), and we want to compute its nearest
neighbor in a dataset. Let h and ` be two fixed ob-
jects in the dataset, such that the inner product to the
true mean ? is higher for h than for `, i.e.,
?h,?? ? ?`,?? > 0. (5)
We are interested in which of h and ` is more similar
to x (in terms of inner product), or in other words,
the difference of two inner products
z = ?h, x? ? ?`, x? = ?h ? `, x?. (6)
615
Because x is a random variable, so is z. Let Q(z) be
the distribution of z; i.e., z ? Q(z).
Using Lemma 1 with a = h ? `, together with
Eq. (5), we have
E[z] = ?h ? `,?? = ?h,?? ? ?`,?? > 0. (7)
Note that the above statement is only concerned
about the mean, so it does not in general assure that
?h, x? > ?`, x? (8)
holds with high probability; there is a chance that
a small number of outliers are inflating the mean.
To assure that inequality (8) holds with probability
greater than 1/2 for instance, the median rather than
the mean of the distribution Q(z) must be greater
than 0.
If the distribution Q(z) is symmetric, the median
occurs at the same point as the mean, and the above
claim holds. Indeed, if the components of x are gen-
erated independently from (possibly non-identical)
normal distributions, we can show that Q(z) also
obeys a normal distribution. Because it is a symmet-
ric distribution, we can safely say that in this case,
Eq. (8) holds with probability greater than 1/2.
For a general non-symmetric distribution with a
finite variance, the median is known to be within the
standard deviation of the mean (Mallows, 1991), so
we could still say that Eq. (8) is likely to hold if ?h?
`,?? is sufficiently large compared to the standard
deviation.
Now, if we let h be the object in a given dataset
with the highest similarity (inner product) to the
mean ?, and let ` be any other object in the set, then
we see from the above discussion that h is likely to
have higher similarity to x, a test sample drawn from
distribution P(x). Because this holds for any ` in
the dataset, the conclusion is that the objects in the
dataset most similar to ? are likely to become hubs.
4.2 After centering
Next let us investigate what happens if the dataset
is centered. Let x? be the sample (empirical) mean
given by Eq. (1). After centering, the similarity of x
with each of the two fixed objects h and ` are evalu-
ated by ?h? x?, x? x?? and ?`? x?, x? x??, respectively.
Their difference zcent is given by
zcent = ?h ? x?, x ? x?? ? ?` ? x?, x ? x??
= ?h ? `, x ? x??
= ?h ? `, x? ? ?h ? `, x??
= z ? ?h ? `, x??.
The last equality follows from Eq. (6). By definition
we have z ? Q(z), and since ?h ? `, x?? is a constant,
zcent = z ? ?h ? `, x?? ? Q(z + ?h ? `, x??).
In other words, the shape of the distribution does not
change, but the mean is shifted to
E[zcent] = E[z] ? ?h ? `, x??
= ?h ? `,?? ? ?h ? `, x??
= ?h ? `,? ? x??,
where E[z] is given by Eq. (7). If the sample mean
x? is close enough to the true mean ?, i.e., x? ? ?, we
have an approximation
E[zcent] = ?h ? `,? ? x?? ? 0. (9)
Thus, if the median and the mean of distribution
Q(z) are again not far apart, Eq. (9) suggests that
h ? x? and ` ? x? are about equally likely to be more
similar to x ? x?; i.e., neither has a greater chance to
become a hub.
5 Hubs in multi-cluster data
In this section, we discuss emergence of hubs when
the data consists of multiple clusters. In fact, the
analysis of Section 4 is distribution-free, and thus
also applies to the case of multi-modal P(x). How-
ever, one might still argue that objects similar to the
data centroid should hardly occur in that case. Us-
ing both synthetic and real datasets, we demonstrate
below that even in multi-cluster data, objects that
are only slightly more similar to the data mean (cen-
troid) may emerge as hubs.
5.1 Synthetic data
5.1.1 Data generation
We generated a high-dimensional multi-cluster
dataset by modeling it as a mixture of ten von Mises-
Fisher distributions (Mardia and Jupp, 2000) in
616
0 50 100 1500.45
0.5
0.55
0.6
N10
Similar
ity with
 centro
id
(a) Before centering: N10 vs. inner
product similarity to the data cen-
troid
200 400 600 800 1000
200
400
600
800
1000
Object ID
Object I
D
 
 
510
1520
2530
3540
4550
(b) Before centering: kNN matrix
200 400 600 800 1000150
100
50
0
50
100
150
Object ID
Freque
ncy
(c) Before centering: Breakdown of
N10 by cluster match/mismatch
between objects and neighbors
0 50 100 150?0.1
?0.05
0
0.05
0.1
N10
Similar
ity with
 centro
id
(d) After centering: N10 vs. inner
product similarity to the data cen-
troid
200 400 600 800 1000
200
400
600
800
1000
Object ID
Object I
D
 
 
510
1520
2530
3540
4550
(e) After centering: kNN matrix
200 400 600 800 1000150
100
50
0
50
100
150
Object ID
Freque
ncy
(f) After centering: Breakdown of
N10 by cluster match/mismatch
between objects and neighbors
Figure 1: 300-dimensional synthetic data. (a), (d): scatter plot of the N10 value of objects and their similarity to
centroid. (b), (e): kNN matrices. The points are colored according to the N10 value of object x; warmer colors indicate
higher N10 values. (c), (f): the number of times (y-axis) an object (whose ID is on the x-axis) appears in the 10 nearest
neighbors of objects of the same cluster (black bars), and those of different clusters (magenta).
R300. The von Mises-Fisher distribution is a distri-
bution of unit vectors (it can roughly be thought of
as a normal distribution on a unit hypersphere), so
for objects (feature vectors) sampled from this dis-
tribution, inner product reduces to cosine similarity.
We sampled1 100 objects from each of the ten dis-
tributions (clusters), and made a dataset of 1,000 ob-
jects in total.
The von Mises-Fisher distribution has two param-
eters, the mean direction vector ?, and the concen-
tration parameter ? characterizing how strongly the
population is concentrated around the direction ?.
We set ? = 500 for all ten distributions, but the mean
directions ? were made distinct; all mean direction
1We used the random sampling code available at http:
//people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html
(Banerjee et al, 2005).
vectors had 30 components set to 0.5 while the re-
maining 270 components were set to 1, but the 30
components with value 0.5 were chosen to be dis-
tinct among the ten clusters. This configuration as-
sures that all ten mean directions have the same an-
gle from the all-ones vector [1, . . . , 1]T, which is the
direction of the mean of the entire data distribution.
Note that even though all sampled objects reside
on the surface of the unit hypersphere, the data cen-
troid lies not on the surface but inside the hyper-
sphere. And after centering, the length of the fea-
ture vectors may vary from one another, but we do
not normalize these vectors; i.e., object similarity is
measured by raw inner product, not by cosine.
617
5.1.2 Correlation between hubness and
centroid similarity
The scatter plot in Figure 1(a) shows the correla-
tion between the degree of hubness (N10) of an ob-
ject and its inner product similarity to the data cen-
troid. The N10 value of an object is defined as the
number of times the object appears in the 10 nearest
neighbors of other objects in the dataset. It was used
in (Radovanovic? et al, 2010a) to measure the degree
of hubness of individual objects.
The plot clearly shows that the hub objects (i.e.,
those with high N10) consist of objects that are simi-
lar to the centroid. Figure 1(d) shows the scatter plot
after the data is centered, created in the same way
as Figure 1(a). The similarity to the centroid is uni-
formly 0 as a result of centering, and no objects have
an N10 value greater than 33.
5.1.3 Influence of hubs on objects in different
clusters
The kNN matrix of Figure 1(b) depicts the kNN
relations with k = 10 among objects before center-
ing. In this matrix, both the x- and y- axes represent
the ID of the objects. If object x is in the 10 nearest
neighbors of object y, a point is plotted at coordi-
nates (x, y). As a result, there are exactly k = 10
points in each row. The color of points indicates the
degree of hubness of object x; warmer color repre-
sents higher N10 value of the object.
In this matrix, object IDs are sorted by the clus-
ter the objects belong to. Hence in the ideal case in
which the k nearest neighbors of every object consist
genuinely of objects from the same cluster, only the
diagonal blocks would be colored, and off-diagonal
areas would be left blank.
As Figure 1(b) shows, the actual situation is far
from ideal, even though ten diagonal blocks are still
identifiable. The presence of many warm colored
vertical lines suggests that many hub objects appear
in the 10 nearest neighbors of other objects that are
not in the same cluster as the hubs. Thus these hubs
may have a strong influence on the kNN prediction
of other objects.
Figure 1(e) shows the kNN matrix after centering.
The warm colored lines have disappeared, and the
diagonal blocks are now more visible.
The bar graphs of Figures 1(c) and (f) plot the N10
value of each object (whose ID is on the x-axis). Re-
call that N10 is the number of times an object appears
in the 10 nearest neighbors of other objects. The
bar for each object is broken down by whether the
object and its neighbors belong to the same cluster
(black bar) or in different clusters (magenta bar). In
terms of kNN classification, having a large number
of nearest neighbors with the same class improves
the classification performance, so longer black bars
and shorter magenta bars are more desirable.
Before centering (Figure 1(c)), hub objects with
large N10 values are similar not only to objects be-
longing to the same cluster (as indicated by black
bars), but also to objects belonging to different clus-
ters (magenta bars). After centering (Figure 1(f)),
the number of tall magenta bars decreases.
Before centering, 22.7% of the 10 nearest neigh-
bors of an object have the same class label as the
object (as indicated by the ratio of the total height of
black bars relative to that of all bars in Figure 1(c)).
After centering, the percentage increases to 31.6%.
5.2 Real dataset
We did the same analysis as Sections 5.1.2?5.1.3
to a real dataset with multiple-cluster structure: the
Reuters Transcribed dataset. This multi-class docu-
ment classification dataset has ten classes, and each
class roughly forms a cluster. We will also use this
dataset in an experiment in Section 7.2.
The results are shown in Figure 2. We can ob-
serve the same trends as we saw in Figure 1 for the
synthetic data: positive correlation between hubness
(N10) and inner product with the data centroid be-
fore centering; hubs appearing in the nearest neigh-
bors of many objects of different classes; and both
are reduced after centering.
The ratio of the height of black bars to that of
all bars in Figure 2(c) is 38.4% before centering,
whereas it improves to 41.0% after centering (Fig-
ure 2(f)).
6 Hubness weighted centering
Centering shifts the origin of the space to the data
centroid, and objects similar to the centroid tend to
become hubs. Thus in a sense, centering can be
interpreted as an operation that shifts the origin to-
wards hubs.
In this section, we extrapolate this interpretation,
618
0 10 20 30 40 500
0.01
0.02
0.03
0.04
0.05
0.06
N10
Simila
rity w
ith ce
ntroid
(a) Before centering: N10 vs. inner
product similarity to the data cen-
troid
50 100 150 200
50
100
150
200
Object ID
Object
 ID
 
 
5
10
15
20
25
30
(b) Before centering: kNN matrix
50 100 150 20040
20
0
20
40
Object ID
Frequ
ency
(c) Before centering: Breakdown of
N10 by class match/mismatch be-
tween objects and neighbors
0 10 20 30 40 50?0.03
?0.02
?0.01
0
0.01
0.02
0.03
N10
Simila
rity w
ith ce
ntroid
(d) After centering: N10 vs. inner
product similarity to the data cen-
troid
50 100 150 200
50
100
150
200
Object ID
Object
 ID
 
 
5
10
15
20
25
30
(e) After centering: kNN matrix
50 100 150 20040
20
0
20
40
Object ID
Frequ
ency
(f) After centering: Breakdown of N10
by class match/mismatch between
objects and neighbors
Figure 2: Reuters Transcribed data.
and move the origin more actively towards hub ob-
jects in the dataset, rather than towards the data cen-
troid. To this end, we consider weighted centering,
a variation of centering in which each object is asso-
ciated with a weight, and the origin is shifted to the
weighted mean of the data. Specifically, we define
the weight of an object as the sum of the similarities
(inner products) between the object and all objects,
regarding this sum as the index of how likely the ob-
ject can be a hub.
6.1 Weighted centering
In weighted centering, we associate weight wi to
each object i in the dataset, and move the origin to
the weighted centroid
x?weighted =
n?
i=1
wixi
where
?n
i=1 wi = 1 and 0 ? wi ? 1 for i = 1, . . . , n.
Thus, object x is mapped to a new feature vector
xweighted = x ? x?weighted = x ?
n?
i=1
wixi.
Notice that the original centering formula (2) is re-
covered by letting wi = 1/n for all i = 1, . . . , n.
Weighted centering can also be kernelized by us-
ing the weighted centering matrix H(w) = I ? 1wT
in place of H in Eq. (3). The resulting Gram matrix
is
Kweighted = H(w)KH(w)T. (10)
6.2 Similarity-dependent weighting
To move the origin towards hubs more aggressively,
we place more weights on objects that are more
likely to become hubs. This likelihood is estimated
by the similarity of individual objects to all objects
in the data set.
619
Let di be the sum of the similarity between object
xi and all objects in the dataset. So,
di =
n?
j=1
?xi, x j? = n ?xi,
1
n
n?
j=1
x j?.
As seen from the last equation, di is proportional to
the similarity (inner product) between object xi and
the data centroid.
Now we define {wi}ni=1 from {di}
n
i=1 by
wi =
d?i
?n
j=1 d
?
j
,
where ? is a parameter controlling how much we
emphasize the effect of di. Setting ? = 0 results in
wi = 1 for every i, and hence is equivalent to normal
centering. When ? > 0, weighted centering moves
the origin closer to the objects with a large di than
normal centering would.
7 Experiments
We evaluated the effect of centering in two natural
language tasks: word sense disambiguation (WSD)
and document classification. We are interested in
whether hubs are actually reduced after centering,
and whether the performance of kNN classification
is improved.
Throughout this section, K denotes cosine simi-
larity matrix; i.e., inner product of feature vectors
normalized to unit length; Kcent denotes the cen-
tered similarity matrix computed by Eq. (3) from K;
Kweighted denotes its hubness weighted variant given
by Eq. (10). Depending on context, these symbols
are also used to denote kNN classifiers using respec-
tive similarity measures.
For comparison, we also tested two recently pro-
posed approaches to hub reduction: transformation
of the base similarity measure (in our case, K) by
Mutual Proximity (Schnitzer et al, 2012)2, and the
one (Suzuki et al, 2012) based on graph Laplacian
kernels. Since the Laplacian kernels are defined for
graph nodes, we computed them by taking the co-
sine similarity matrix K as the weighted adjacency
(affinity) matrix of a graph. For Laplacian kernels,
2We used the Matlab script downloaded from http://www.
ofai.at/?dominik.schnitzer/mp/.
we computed both the regularized Laplacian ker-
nel (Chebotarev and Shamis, 1997; Smola and Kon-
dor, 2003) with several parameter values, as well as
the commute-time kernel (Saerens et al, 2004), but
present only the best results among these kernels.
7.1 Word sense disambiguation
7.1.1 Task and dataset
In the WSD experiment, we used the dataset for
the Senseval-3 English Lexical Sample (ELS) task
(Mihalcea et al, 2004). It is a collection of sen-
tences containing 57 polysemous words, and each
of these sentences is annotated with a gold standard
sense of the target word. The goal of the ELS task
is to build a classifier for each target word, which,
given a context around the word, predicts a sense
from the known set of senses.
We used a basic bag-of-words representation for
the context surrounding a target word (Mihalcea,
2004; Navigli, 2009). A context is thus represented
as a high-dimensional feature vector holding the tf-
idf weighted frequency of words3 in context.
7.1.2 Compared methods
We applied kNN classification using cosine sim-
ilarity K, and its four transformed similarity mea-
sures: centered similarity Kcent, its weighted vari-
ant Kweighted, Mutual Proximity and graph Laplacian
kernels. The sense of a test object was predicted by
voting from the k training objects most similar to the
test object, as measured by the respective similarity
measures.
We used leave-one-out cross validation within the
training data to tune neighborhood size k for the
kNN classification and the voting scheme, i.e., ei-
ther (unweighted) majority vote, or weighted vote in
which votes from individual objects are weighted by
their similarity score to the test objects. We also se-
lected parameter ? in Kweighted and the best graph
Laplacian kernel among the regularized Laplacian
and commute time kernels using the training data.
7.1.3 Evaluation
We computed two indices for each similarity mea-
sure: (i) skewness of the N10 distribution to evaluate
3We removed stop words listed in the on-line appendix of
(Lewis et al, 2004).
620
Method F1 score Skewness
K 60.3 4.55
Kcent 64.0 1.19
Kweighted 64.8 1.02
Mutual Proximity 63.0 1.00
Graph Laplacian 61.2 4.51
GAMBL (Decadt et al, 2004) 64.5 ?
Table 1: WSD results: Macro-averaged F1 score (points)
of the compared methods (larger is better) and empirical
skewness of the N10 distribution for each similarity mea-
sure (smaller is better).
the emergence of hubs, and (ii) macro-averaged F1
score to evaluate the classification performance.
Skewness To evaluate the degree of hub emer-
gence for each similarity measure, we followed
(Radovanovic? et al, 2010a) and counted Nk(x), the
number of times object x occurs in the kNN lists
of other objects in the dataset (we fix k = 10 be-
low). The emergence of hubs in a dataset can then
be quantified with skewness, defined as follows:
S Nk =
E
[(
Nk ? ?Nk
)3
]
?3Nk
.
In this equation, E[ ? ] denotes expectation, and ?Nk
and ?Nk are the mean and the standard deviation of
the Nk distribution, respectively.
When hubs exist in a dataset, the distribution of
Nk is expected to skew to the right, and yields a large
S Nk (Radovanovic? et al, 2010a). In other words,
similarity measures that yield smaller S Nk are more
desirable in terms of hub reduction.
Skewness can only be computed for each dataset,
and in the WSD task, each target word has its own
dataset. Hence we computed the skewness S N10 for
each word and then took average.
Macro-averaged F1 score Classification perfor-
mance was measured by the F1 score macro-
averaged over all the 57 target words in the Senseval-
3 ELS dataset. The standard Senseval-3 ELS scor-
ing method is based on micro average, but we used
macro average to make the evaluation consistent
with skewness computation, which, as mentioned
above, can only be computed for each dataset (i.e.,
word).
Dataset #classes #objects #features
Reuters Transcribed 10 201 2730
Mini Newsgroups 20 2000 8811
Table 2: Document classification datasets: Number of
classes, data size, and number of features.
7.1.4 Result
Table 1 shows the F1 scores and the skewness of
the N10 distributions, macro averaged over the 57
target words. The table also includes the macro-
averaged F1 score4 of the GAMBL system, the best
memory-based system participated in the Senseval-
3 ELS task. Note however that GAMBL uses more
elaborate features (e.g., part-of-speech of words)
than just a plain bag-of-words used by other methods
in this comparison. GAMBL also employs complex
post-processing of the kNN outputs.
After centering (Kcent and Kweighted) skewness
became markedly smaller than that of the non-
centered cosine K. F1 score also improved with the
decrease in skewness. In particular, weighted cen-
tering (Kweighted) slightly outperformed GAMBL,
though the difference was small. Recall however
that Kcent and Kweighted only use naive bag-of-words
features, unlike GAMBL.
7.2 Document classification
7.2.1 Task and dataset
Two multiclass document classification datasets
were used: Reuters Transcribed and Mini News-
groups, distributed at http://archive.ics.uci.edu/ml/.
The properties of the datasets are summarized in Ta-
ble 2.
7.2.2 Evaluation
The performance was evaluated by the F1 score
(equivalent to accuracy in this task) of prediction us-
ing leave-one-out cross validation, due to the limited
number of documents.
7.2.3 Compared methods
We used the cosine similarity as the base sim-
ilarity matrix (K). The centered similarity matrix
(Kcent) and its weighted variant (Kweighted), Mutual
4The macro-averaged F1 of GAMBL was calculated from
the per-word F1 scores listed in Table 1 of (Decadt et al, 2004).
621
Method F1 score Skewness
K 56.7 1.61
Kcent 61.2 0.11
Kweighted 60.2 0.04
Mutual Proximity 60.2 ?0.10
Graph Laplacian 57.2 0.37
(a) Reuters Transcribed
Method F1 score Skewness
K 76.5 4.37
Kcent 79.0 1.56
Kweighted 79.4 1.68
Mutual Proximity 79.0 0.49
Graph Laplacian 77.6 2.13
(b) Mini Newsgroups
Table 3: Document classification results: F1 score (%)
(larger is better) and skewness of the N10 distribution for
each similarity measure (smaller is better).
Proximity, and graph Laplacian based kernels were
computed from K.
kNN classification was done in a standard way:
The class of object x is predicted by the majority
vote from k = 10 objects most similar to x, mea-
sured by a specified similarity measure. The param-
eter k for the kNN classification, the voting scheme
(i.e., either unweighted or weighted majority vote),
? in Kweighted, and the best graph Laplacian kernel
were selected by leave-one-out cross validation.
7.2.4 Result
Table 3 shows the F1 score and the skewness of
the N10 distribution of the respective methods in
document classification. Centered cosine (Kcent)
outperformed uncentered cosine similarity K, and
achieved an F1 score comparable to Mutual Proxim-
ity. Weighted centering (Kweighted) further improved
F1 on the Mini Newsgroups data.
8 Conclusion
We have shown that centering similarity matrices re-
duces the emergence of hubs in the data, and conse-
quently improves the accuracy of nearest neighbor
classification. We have theoretically analyzed why
objects most similar to the mean tend to make hubs,
and also proved that centering cancels the bias in the
distribution of inner products, and thus is expected
to reduce hubs.
In WSD and document classification tasks, kNN
classifiers showed much better performance with
centered similarity measures than non-centered
ones. Weighted centering shifts the origin towards
hubs more aggressively, and further improved the
classification performance in some cases.
In future work, we plan to exploit the class distri-
bution in the dataset to make more effective similar-
ity measures; notice that the hubness weighted cen-
tering of Section 6 is an unsupervised method, in the
sense that class information was not used for deter-
mining weights. We will investigate if more effec-
tive weighting can be done using this information.
Acknowledgments
We thank anonymous reviewers for helpful com-
ments.
References
Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh,
and Suvrit Sra. 2005. Clustering on the unit hyper-
sphere using von Mises-Fisher distributions. Journal
of Machine Learning Research, 6:1345?1382.
P. Yu. Chebotarev and E. V. Shamis. 1997. The matrix-
forest theorem and measuring relations in small social
groups. Automation and Remote Control, 58(9):1505?
1514.
Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi,
and Luca Cazzanti. 2009. Similarity-based classifi-
cation: Concepts and algorithms. Journal of Machine
Learning Research, 10:747?776.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and
Antal Van den Bosch. 2004. GAMBL, genetic algo-
rithm optimization of memory-based WSD. In Rada
Mihalcea and Phil Edmonds, editors, Proceedings of
the 3rd International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text (Senseval-
3), pages 108?112.
L. Eriksson, E. Johansson, N. Kettaneh-Wold, J. Trygg,
C. Wikstro?m, and S. Wold. 2006. Multi- and
Megavariate Data Analysis, Part 1, Basic Principles
and Applications. Umetrics, Inc.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?08),
pages 897?906, Honolulu, Hawaii, USA.
Douglas H. Fisher and Hans-Joachim Lenz, editors.
1996. Learning from Data: Artificial Intelligence and
622
Statistics V: Workshop on Artificial Intelligence and
Statistics. Lecture Notes in Statistics 112. Springer.
Ruixin Guo and Sounak Chakraborty. 2010. Bayesian
adaptive nearest neighbor. Statistical Analysis and
Data Mining, 3(2):92?105.
Daniel Jurafsky and James H. Martin. 2008. Speech and
Language Processing. Prentice Hall, 2nd edition.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: a new benchmark collection for text
categorization research. Journal of Machine Learning
Research, 5:361?397.
Colin Mallows. 1991. Another comment on O?Cinneide.
The American Statistician, 45(3):257.
K. V. Mardia and P. Jupp. 2000. Directional Statistics.
John Wiley and Sons, 2nd edition.
K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multi-
variate Analysis. Academic Press.
Brij M. Masand, Gordon Linoff, and David L. Waltz.
1992. Classifying news stories using memory based
reasoning. In Proceedings of the 15th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR ?92), pages
59?65.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Rada Mihalcea and Phil Edmonds, editors,
Proceedings of the 3rd International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text (Senseval-3), pages 25?28, Barcelona, Spain.
Rada Mihalcea. 2004. Co-training and self-training for
word sense disambiguation. In Hwee Tou Ng and
Ellen Riloff, editors, Proceedings of the 8th Confer-
ence on Computational Natural Language Learning
(CoNLL ?04), pages 33?40, Boston, Massachusetts,
USA.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association of Compu-
tational Linguistics: Human Language Technologies
(ACL ?08), pages 236?244, Columbus, Ohio, USA.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:10:1?10:69.
Ali Mustafa Qamar, E?ric Gaussier, Jean-Pierre Cheval-
let, and Joo-Hwee Lim. 2008. Similarity learning for
nearest neighbor classification. In Proceedings of the
8th International Conference on Data Mining (ICDM
?08), pages 983?988, Pisa, Italy.
Milos? Radovanovic?, Alexandros Nanopoulos, and Mir-
jana Ivanovic?. 2010a. Hubs in space: Popular nearest
neighbors in high-dimensional data. Journal of Ma-
chine Learning Research, 11:2487?2531.
Milos? Radovanovic?, Alexandros Nanopoulos, and Mir-
jana Ivanovic?. 2010b. On the existence of obstinate
results in vector space models. In Proceedings of the
33rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR ?10), pages 186?193, Geneva, Switzerland.
Marco Saerens, Franc?ois Fouss, Luh Yen, and Pierr
Dupont. 2004. The principal components analysis
of graph, and its relationships to spectral clustering.
In Proceedings of the 15th European Conference on
Machine Learning (ECML ?04), Lecture Notes in Ar-
tificial Intelligence 3201, pages 371?383, Pisa, Italy.
Springer.
Dominik Schnitzer, Arthur Flexer, Markus Schedl, and
Gerhard Widmer. 2012. Local and global scaling re-
duce hubs in space. Journal of Machine Learning Re-
search, 13:2871?2902.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24:97?123.
Alexander J. Smola and Risi Kondor. 2003. Kernels and
regularization on graphs. In Learning Theory and Ker-
nel Machines: 16th Annual Conference on Learning
Theory and 7th Kernel Workshop, Proceedings, Lec-
ture Notes in Artificial Intelligence 2777, pages 144?
158. Springer.
Anders S?gaard. 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL ?11), pages 48?
52, Portland, Oregon, USA.
Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Mat-
sumoto, and Marco Saerens. 2012. Investigating the
effectiveness of Laplacian-based kernels in hub reduc-
tion. In Proceedings of the 26th AAAI Conference on
Artificial Intelligence (AAAI-12), pages 1112?1118,
Toronto, Ontario, Canada.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL ?10), pages 948?957,
Uppsala, Sweden.
Jigang Wang, Predrag Neskovic, and Leon N. Cooper.
2006. Neighborhood size selection in the k-nearest-
neighbor rule using statistical confidence. Pattern
Recognition, 39(3):417?423.
Kilian Q. Weinberger and Lawrence K. Saul. 2009. Dis-
tance metric learning for large margin nearest neighbor
classification. Journal of Machine Learning Research,
10:207?244.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR ?99), pages 42?49, Berkeley, California, USA.
623
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30?36,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
HITS-based Seed Selection and Stop List Construction for Bootstrapping
Tetsuo Kiso Masashi Shimbo Mamoru Komachi Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
{tetsuo-s,shimbo,komachi,matsu}@is.naist.jp
Abstract
In bootstrapping (seed set expansion), select-
ing good seeds and creating stop lists are two
effective ways to reduce semantic drift, but
these methods generally need human super-
vision. In this paper, we propose a graph-
based approach to helping editors choose ef-
fective seeds and stop list instances, appli-
cable to Pantel and Pennacchiotti?s Espresso
bootstrapping algorithm. The idea is to select
seeds and create a stop list using the rankings
of instances and patterns computed by Klein-
berg?s HITS algorithm. Experimental results
on a variation of the lexical sample task show
the effectiveness of our method.
1 Introduction
Bootstrapping (Yarowsky, 1995; Abney, 2004) is a
technique frequently used in natural language pro-
cessing to expand limited resources with minimal
supervision. Given a small amount of sample data
(seeds) representing a particular semantic class of
interest, bootstrapping first trains a classifier (which
often is a weighted list of surface patterns character-
izing the seeds) using the seeds, and then apply it on
the remaining data to select instances most likely to
be of the same class as the seeds. These selected in-
stances are added to the seed set, and the process is
iterated until sufficient labeled data are acquired.
Many bootstrapping algorithms have been pro-
posed for a variety of tasks: word sense disambigua-
tion (Yarowsky, 1995; Abney, 2004), information
extraction (Hearst, 1992; Riloff and Jones, 1999;
Thelen and Riloff, 2002; Pantel and Pennacchiotti,
2006), named entity recognition (Collins and Singer,
1999), part-of-speech tagging (Clark et al, 2003),
and statistical parsing (Steedman et al, 2003; Mc-
Closky et al, 2006).
Bootstrapping algorithms, however, are known to
suffer from the problem called semantic drift: as the
iteration proceeds, the algorithms tend to select in-
stances increasingly irrelevant to the seed instances
(Curran et al, 2007). For example, suppose we want
to collect the names of common tourist sites from a
web corpus. Given seed instances {New York City,
Maldives Islands}, bootstrapping might learn, at one
point of the iteration, patterns like ?pictures of X?
and ?photos of X,? which also co-occur with many
irrelevant instances. In this case, a later iteration
would likely acquire frequent words co-occurring
with these generic patterns, such as Michael Jack-
son.
Previous work has tried to reduce the effect of se-
mantic drift by making the stop list of instances that
must not be extracted (Curran et al, 2007; McIntosh
and Curran, 2009). Drift can also be reduced with
carefully selected seeds. However, both of these ap-
proaches require expert knowledge.
In this paper, we propose a graph-based approach
to seed selection and stop list creation for the state-
of-the-art bootstrapping algorithm Espresso (Pantel
and Pennacchiotti, 2006). An advantage of this ap-
proach is that it requires zero or minimal super-
vision. The idea is to use the hubness score of
instances and patterns computed from the point-
wise mutual information matrix with the HITS al-
gorithm (Kleinberg, 1999). Komachi et al (2008)
pointed out that semantic drift in Espresso has the
same root as topic drift (Bharat and Henzinger,
1998) observed with HITS, noting the algorithmic
similarity between them. While Komachi et al pro-
posed to use algorithms different from Espresso to
30
avoid semantic drift, in this paper we take advantage
of this similarity to make better use of Espresso.
We demonstrate the effectiveness of our approach
on a word sense disambiguation task.
2 Background
In this section, we review related work on seed se-
lection and stop list construction. We also briefly in-
troduce the Espresso bootstrapping algorithm (Pan-
tel and Pennacchiotti, 2006) for which we build our
seed selection and stop list construction methods.
2.1 Seed Selection
The performance of bootstrapping can be greatly in-
fluenced by a number of factors such as the size of
the seed set, the composition of the seed set and the
coherence of the concept being expanded (Vyas et
al., 2009). Vyas et al (2009) studied the impact of
the composition of the seed sets on the expansion
performance, confirming that seed set composition
has a significant impact on the quality of expansions.
They also found that the seeds chosen by non-expert
editors are often worse than randomly chosen ones.
A similar observation was made by McIntosh and
Curran (2009), who reported that randomly chosen
seeds from the gold-standard set often outperformed
seeds chosen by domain experts. These results sug-
gest that even for humans, selecting good seeds is a
non-trivial task.
2.2 Stop Lists
Yangarber et al (2002) proposed to run multiple
bootstrapping sessions in parallel, with each session
trying to extract one of several mutually exclusive
semantic classes. Thus, the instances harvested in
one bootstrapping session can be used as the stop
list of the other sessions. Curran et al (2007) pur-
sued a similar idea in their Mutual Exclusion Boot-
strapping, which uses multiple semantic classes in
addition to hand-crafted stop lists. While multi-class
bootstrapping is a clever way to reduce human su-
pervision in stop list construction, it is not generally
applicable to bootstrapping for a single class. To ap-
ply the idea of multi-class bootstrapping to single-
class bootstrapping, one has to first find appropri-
ate competing semantic classes and good seeds for
them, which is in itself a difficult problem. Along
this line of research, McIntosh (2010) recently used
Algorithm 1 Espresso algorithm
1: Input: Seed vector i0
2: Instance-pattern co-occurrence matrix A
3: Instance cutoff parameter k
4: Pattern cutoff parameter m
5: Number of iterations ?
6: Output: Instance score vector i
7: Pattern score vector p
8: function ESPRESSO(i0,A,k,m,?)
9: i? i0
10: for t = 1,2, ...,? do
11: p? AT i
12: Scale p so that the components sum to one.
13: p? SELECTKBEST(p,k)
14: i? Ap
15: Scale i so that the components sum to one.
16: i? SELECTKBEST(i,m)
17: return i and p
18: function SELECTKBEST(v,k)
19: Retain only the k largest components of v, resetting the
remaining components to 0.
20: return v
clustering to find competing semantic classes (nega-
tive categories).
2.3 Espresso
Espresso (Pantel and Pennacchiotti, 2006) is one of
the state-of-the-art bootstrapping algorithms used in
many natural language tasks (Komachi and Suzuki,
2008; Abe et al, 2008; Ittoo and Bouma, 2010;
Yoshida et al, 2010). Espresso takes advantage of
pointwise mutual information (pmi) (Manning and
Schu?tze, 1999) between instances and patterns to
evaluate their reliability. Let n be the number of all
instances in the corpus, and p the number of all pos-
sible patterns. We denote all pmi values as an n? p
instance-pattern matrix A, with the (i, j) element of
A holding the value of pmi between the ith instance
and the jth pattern. Let AT denote the matrix trans-
pose of A.
Algorithm 1 shows the pseudocode of Espresso.
The input vector i0 (called seed vector) is an n-
dimensional binary vector with 1 at the ith com-
ponent for every seed instance i, and 0 elsewhere.
The algorithm outputs an n-dimensional vector i and
an p-dimensional vector p, respectively representing
the final scores of instances and patterns. Note that
for brevity, the pseudocode assumes fixed numbers
(k and m) of components in i and p are carried over
to the subsequent iteration, but the original Espresso
31
allows them to gradually increase with the number
of iterations.
3 HITS-based Approach to Seed Selection
and Stop List Construction
3.1 Espresso and HITS
Komachi et al (2008) pointed out the similarity
between Espresso and Kleinberg?s HITS web page
ranking algorithm (Kleinberg, 1999). Indeed, if we
remove the pattern/instance selection steps of Algo-
rithm 1 (lines 13 and 16), the algorithm essentially
reduces to HITS. In this case, the outputs i and p
match respectively the hubness and authority score
vectors of HITS, computed on the bipartite graph of
instances and patterns induced by matrix A.
An implication of this algorithmic similarity is
that the outputs of Espresso are inherently biased
towards the HITS vectors, which is likely to be
the cause of semantic drift. Even though the pat-
tern/instance selection steps in Espresso reduce such
a bias to some extent, the bias still persists, as em-
pirically verified by Komachi et al (2008). In other
words, the expansion process does not drift in ran-
dom directions, but tend towards the set of instances
and patterns with the highest HITS scores, regard-
less of the target semantic class. We exploit this ob-
servation in seed selection and stop list construction
for Espresso, in order to reduce semantic drift.
3.2 The Procedure
Our strategy is extremely simple, and can be sum-
marized as follows.
1. First, compute the HITS ranking of instances
in the graph induced by the pmi matrix A. This
can be done by calling Algorithm 1 with k =
m = ? and a sufficiently large ? .
2. Next, check the top instances in the HITS rank-
ing list manually, and see if these belong to the
target class.
3. The third step depends on the outcome of the
second step.
(a) If the top instances are of the target class,
use them as the seeds. We do not use a
stop list in this case.
(b) If not, these instances are likely to make a
vector for which semantic drift is directed;
hence, use them as the stop list. In this
case, the seed set must be prepared manu-
ally, just like the usual bootstrapping pro-
cedure.
4. Run Espresso with the seeds or stop list found
in the last step.
4 Experimental Setup
We evaluate our methods on a variant of the lexi-
cal sample word sense disambiguation task. In the
lexical sample task, a small pre-selected set of a tar-
get word is given, along with an inventory of senses
for each word (Jurafsky and Martin, 2008). Each
word comes with a number of instances (context
sentences) in which the target word occur, and some
of these sentences are manually labeled with the cor-
rect sense of the target word in each context. The
goal of the task is to classify unlabeled context sen-
tences by the sense of the target word in each con-
text, using the set of labeled sentences.
To apply Espresso for this task, we reformulate
the task to be that of seed set expansion, and not
classification. That is, the hand-labeled sentences
having the same sense label are used as the seed set,
and it is expanded over all the remaining (unlabeled)
sentences.
The reason we use the lexical sample task is that
every sentence (instance) belongs to one of the pre-
defined senses (classes), and we can expect the most
frequent sense in the corpus to form the highest
HITS ranking instances. This allows us to com-
pletely automate our experiments, without the need
to manually check the HITS ranking in Step 2 of
Section 3.2. That is, for the most frequent sense
(majority sense), we take Step 3a and use the highest
ranked instances as seeds; for the rest of the senses
(minority senses), we take Step 3b and use them as
the stop list.
4.1 Datasets
We used the seven most frequent polysemous nouns
(arm, bank, degree, difference, paper, party and
shelter) in the SENSEVAL-3 dataset, and line (Lea-
cock et al, 1993) and interest (Bruce and Wiebe,
32
Task Method MAP AUC R-Precision P@30 P@50 P@100
arm Random 84.3 ?4.1 59.6 ?8.1 80.9 ?2.2 89.5 ?10.8 87.7 ?9.6 85.4 ?7.2
HITS 85.9 59.7 79.3 100 98.0 89.0
bank Random 74.8 ?6.5 61.6 ?9.6 72.6 ?4.5 82.9 ?14.8 80.1 ?13.5 76.6 ?10.9
HITS 84.8 77.6 78.0 100 100 94.0
degree Random 69.4 ?3.0 54.3 ?4.2 66.7 ?2.3 76.8 ?9.5 73.8 ?7.5 70.5 ?5.3
HITS 62.4 49.3 63.2 56.7 64.0 66.0
difference Random 48.3 ?3.8 54.5 ?5.0 47.0 ?4.4 53.9 ?10.7 50.7 ?8.8 47.9 ?6.1
HITS 50.2 60.1 51.1 60.0 60.0 48.0
paper Random 75.2 ?4.1 56.4 ?7.1 71.6 ?3.3 82.3 ?9.8 79.6 ?8.8 76.9 ?6.1
HITS 75.2 61.0 75.2 73.3 80.0 78.0
party Random 79.1 ?5.0 57.0 ?9.7 76.6 ?3.1 84.5 ?10.7 82.7 ?9.2 80.2 ?7.5
HITS 85.2 68.2 78.5 100 96.0 87.0
shelter Random 74.9 ?2.3 51.5 ?3.3 73.2 ?1.3 77.3 ?7.8 76.0 ?5.6 74.5 ?3.5
HITS 77.0 54.6 72.0 76.7 84.0 79.0
line Random 44.5 ?15.1 36.3 ?16.9 40.1 ?14.6 75.0 ?21.0 69.8 ?24.1 62.3 ?27.9
HITS 72.2 68.6 68.5 100 100 100
interest Random 64.9 ?8.3 64.9 ?12.0 63.7 ?10.2 87.6 ?13.2 85.3 ?13.7 81.2 ?13.9
HITS 75.3 83.0 80.1 100 94.0 77.0
Avg. Random 68.4 55.1 65.8 78.9 76.2 72.8
HITS 74.2 64.7 71.8 85.2 86.2 79.8
Table 1: Comparison of seed selection for Espresso (? = 5, nseed = 7). For Random, results are reported as (mean ?
standard deviation). All figures are expressed in percentage terms. The row labeled ?Avg.? lists the values macro-
averaged over the nine tasks.
1994) datasets1 for our experiments. We lowercased
words in the sentence and pre-processed them with
the Porter stemmer (Porter, 1980) to get the stems of
words.
Following (Komachi et al, 2008), we used two
types of features extracted from neighboring con-
texts: collocational features and bag-of-words fea-
tures. For collocational features, we set a window of
three words to the right and left of the target word.
4.2 Evaluation methodology
We run Espresso on the above datasets using differ-
ent seed selection methods (for majority sense of tar-
get words), and with or without stop lists created by
our method (for minority senses of target words).
We evaluate the performance of the systems ac-
cording to the following evaluation metrics: mean
average precision (MAP), area under the ROC curve
(AUC), R-precision, and precision@n (P@n) (Man-
ning et al, 2008). The output of Espresso may con-
tain seed instances input to the system, but seeds are
excluded from the evaluation.
1http://www.d.umn.edu/?tpederse/data.html
5 Results and Discussion
5.1 Effect of Seed Selection
We first evaluate the performance of our seed se-
lection method for the majority sense of the nine
polysemous nouns. Table 1 shows the performance
of Espresso with the seeds chosen by the proposed
HITS-based seed selection method (HITS), and with
the seed sets randomly chosen from the gold stan-
dard sets (Random; baseline). The results for Ran-
dom were averaged over 1000 runs. We set the num-
ber of seeds nseed = 7 and number of iterations ? = 5
in this experiment.
As shown in the table, HITS outperforms the
baseline systems except degree. Especially, the
MAP reported in Table 1 shows that our approach
achieved improvements of 10 percentage points on
bank, 6.1 points on party, 27.7 points on line, and
10.4 points on interest over the baseline, respec-
tively. AUC and R-precision mostly exhibit a trend
similar to MAP, except R-precision in arm and shel-
ter, for which the baseline is better. It can be seen
from the P@n (P@30, P@50 and P@100) reported
in Table 1 that our approach performed considerably
better than baseline, e.g., around 17?20 points above
33
Task Method MAP AUC R-Precision P@10 P@20 P@30
arm NoStop 12.7 ?4.3 51.8 ?10.8 13.9 ?9.8 21.4 ?19.1 15.1 ?12.0 14.1 ?10.4
HITS 13.4 ?4.1 53.7 ?10.5 15.0 ?9.5 23.8 ?17.7 17.5 ?12.0 15.5 ?10.2
bank NoStop 32.5 ?5.1 73.0 ?8.5 45.1 ?10.3 80.4 ?21.8 70.3 ?21.2 62.6 ?18.1
HITS 33.7 ?3.7 75.4 ?5.7 47.6 ?8.1 82.6 ?18.1 72.7 ?18.5 65.3 ?15.5
degree NoStop 34.7 ?4.2 69.7 ?5.6 43.0 ?7.1 70.0 ?18.7 62.8 ?15.7 55.8 ?14.3
HITS 35.7 ?4.3 71.7 ?5.6 44.3 ?7.6 72.4 ?16.4 64.4 ?15.9 58.3 ?16.2
difference NoStop 20.2 ?3.9 57.1 ?6.7 22.3 ?8.3 35.8 ?18.7 27.7 ?14.0 25.5 ?11.9
HITS 21.2 ?3.8 59.1 ?6.3 24.2 ?8.4 38.2 ?20.5 30.2 ?14.0 28.0 ?11.9
paper NoStop 25.9 ?6.6 53.1 ?10.0 27.7 ?9.8 55.2 ?34.7 42.4 ?25.4 36.0 ?17.8
HITS 27.2 ?6.3 56.3 ?9.1 29.4 ?9.5 57.4 ?35.3 45.6 ?25.3 38.7 ?17.5
party NoStop 23.0 ?5.3 59.4 ?10.8 30.5 ?9.1 59.6 ?25.8 46.8 ?17.4 38.7 ?12.7
HITS 24.1 ?5.0 62.5 ?9.8 32.1 ?9.4 61.6 ?26.4 47.9 ?16.6 40.8 ?12.7
shelter NoStop 24.3 ?2.4 50.6 ?3.2 25.1 ?4.6 25.4 ?11.7 26.9 ?10.3 25.9 ?8.7
HITS 25.6 ?2.3 53.4 ?3.0 26.5 ?4.8 28.8 ?12.9 29.0 ?10.4 28.1 ?8.2
line NoStop 6.5 ?1.8 38.3 ?5.3 2.1 ?4.1 0.8 ?4.4 1.8 ?8.9 2.3 ?11.0
HITS 6.7 ?1.9 38.8 ?5.8 2.4 ?4.4 1.0 ?4.6 2.0 ?8.9 2.5 ?11.1
interest NoStop 29.4 ?7.6 61.0 ?12.1 33.7 ?13.2 69.6 ?40.3 67.0 ?39.1 65.7 ?37.8
HITS 31.2 ?5.6 63.6 ?9.1 36.1 ?10.5 81.0 ?29.4 78.1 ?27.0 77.4 ?24.3
Avg. NoStop 23.2 57.1 27.0 46.5 40.1 36.3
HITS 24.3 59.4 28.6 49.6 43.0 39.4
Table 2: Effect of stop lists for Espresso (nstop = 10, nseed = 10, ? = 20). Results are reported as (mean ? standard
deviation). All figures are expressed in percentage. The row labeled ?Avg.? shows the values macro-averaged over all
nine tasks.
the baseline on bank and 25?37 points on line.
5.2 Effect of Stop List
Table 2 shows the performance of Espresso using
the stop list built with our proposed method (HITS),
compared with the vanilla Espresso not using any
stop list (NoStop).
In this case, the size of the stop list is set to nstop =
10, and the number of seeds nseed = 10 and iterations
? = 20. For both HITS and NoStop, the seeds are
selected at random from the gold standard data, and
the reported results were averaged over 50 runs of
each system. Due to lack of space, only the results
for the second most frequent sense for each word are
reported; i.e., the results for more minor senses are
not in the table. However, they also showed a similar
trend.
As shown in the table, our method (HITS) outper-
forms the baseline not using a stop list (NoStop), in
all evaluation metrics. In particular, the P@n listed
in Table 2 shows that our method provides about
11 percentage points absolute improvement over the
baseline on interest, for all n = 10, 20, and 30.
6 Conclusions
We have proposed a HITS-based method for allevi-
ating semantic drift in the bootstrapping algorithm
Espresso. Our idea is built around the concept of
hubs in the sense of Kleinberg?s HITS algorithm, as
well as the algorithmic similarity between Espresso
and HITS. Hub instances are influential and hence
make good seeds if they are of the target seman-
tic class, but otherwise, they may trigger semantic
drift. We have demonstrated that our method works
effectively on lexical sample tasks. We are currently
evaluating our method on other bootstrapping tasks,
including named entity extraction.
Acknowledgements
We thank Masayuki Asahara and Kazuo Hara for
helpful discussions and the anonymous reviewers
for valuable comments. MS was partially supported
by Kakenhi Grant-in-Aid for Scientific Research C
21500141.
34
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning cooc-
currence patterns and fertilizing cooccurrence samples
with verbal nouns. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP ?08), pages 497?504.
Steven Abney. 2004. Understanding the Yarowsky algo-
rithm. Computational Linguistics, 30:365?395.
Krishna Bharat and Monika R. Henzinger. 1998. Im-
proved algorithms for topic distillation environment in
a hyperlinked. In Proceedings of the 21st Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ?98),
pages 104?111.
Rebecca Bruce and Janyce Wiebe. 1994. Word-sense
disambiguation using decomposable models. In Pro-
ceedings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ?94), pages
139?146.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS taggers using unlabelled
data. In Proceedings of the 7th Conference on Natural
Language Learning (CoNLL ?03), pages 49?55.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP-VLC ?99), pages 189?196.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING ?07), pages 172?180.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th Conference on Computational Linguistics (COL-
ING ?92), pages 539?545.
Ashwin Ittoo and Gosse Bouma. 2010. On learning
subtypes of the part-whole relation: do not mix your
seeds. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
?10), pages 1328?1336.
Daniel Jurafsky and James H. Martin. 2008. Speech and
Language Processing. Prentice Hall, 2nd edition.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
Mamoru Komachi and Hisami Suzuki. 2008. Minimally
supervised learning of semantic knowledge from query
logs. In Proceedings of the 3rd International Joint
Conference on Natural Language Processing (IJCNLP
?08), pages 358?365.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?08),
pages 1011?1020.
Claudia Leacock, Geoffrey Towell, and Ellen Voorhees.
1993. Corpus-based statistical sense resolution. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology (HLT ?93), pages 260?265.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics (HLT-NAACL ?06), pages
152?159.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP ?09), volume 1, pages 396?
404.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP ?10), pages
356?365.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL ?06), pages
113?120.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 16th National Confer-
ence on Artificial Intelligence and the 11th Innovative
Applications of Artificial Intelligence (AAAI/IAAI ?99),
pages 474?479.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-
borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Example
35
selection for bootstrapping statistical parsers. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL ?03), volume 1, pages 157?164.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the ACL-02 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ?02), pages 214?221.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management (CIKM
?09), pages 225?234.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING ?02).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting on Association for
Computational Linguistics (ACL ?95), pages 189?196.
Minoru Yoshida, Masaki Ikeda, Shingo Ono, Issei Sato,
and Hiroshi Nakagawa. 2010. Person name dis-
ambiguation by bootstrapping. In Proceeding of the
33rd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR ?10), pages 10?17.
36
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 154?162,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using the Mutual k-Nearest Neighbor Graphs
for Semi-supervised Classification of Natural Language Data
Kohei Ozaki and Masashi Shimbo and Mamoru Komachi and Yuji Matsumoto
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{kohei-o,shimbo,komachi,matsu}@is.naist.jp
Abstract
The first step in graph-based semi-supervised
classification is to construct a graph from in-
put data. While the k-nearest neighbor graphs
have been the de facto standard method of
graph construction, this paper advocates using
the less well-known mutual k-nearest neigh-
bor graphs for high-dimensional natural lan-
guage data. To compare the performance
of these two graph construction methods, we
run semi-supervised classification methods on
both graphs in word sense disambiguation and
document classification tasks. The experi-
mental results show that the mutual k-nearest
neighbor graphs, if combined with maximum
spanning trees, consistently outperform the k-
nearest neighbor graphs. We attribute better
performance of the mutual k-nearest neigh-
bor graph to its being more resistive to mak-
ing hub vertices. The mutual k-nearest neigh-
bor graphs also perform equally well or even
better in comparison to the state-of-the-art
b-matching graph construction, despite their
lower computational complexity.
1 Introduction
Semi-supervised classification try to take advan-
tage of a large amount of unlabeled data in addi-
tion to a small amount of labeled data, in order to
achieve good classification accuracy while reducing
the cost of manually annotating data. In particular,
graph-based techniques for semi-supervised classi-
fication (Zhou et al, 2004; Zhu et al, 2003; Cal-
lut et al, 2008; Wang et al, 2008) are recognized
as a promising approach. Some of these techniques
have been successfully applied for NLP tasks: word
sense disambiguation (Alexandrescu and Kirchhoff,
2007; Niu et al, 2005), sentiment analysis (Gold-
berg and Zhu, 2006), and statistical machine trans-
lation (Alexandrescu and Kirchhoff, 2009), to name
but a few.
However, the focus of these studies is how to as-
sign accurate labels to vertices in a given graph. By
contrast, there has not been much work on how such
a graph should be built, and graph construction re-
mains ?more of an art than a science? (Zhu, 2005).
Yet, it is an essential step for graph-based semi-
supervised classification and (unsupervised) cluster-
ing, and the input graph affects the quality of final
classification/clustering results.
Both for semi-supervised classification and for
clustering, the k-nearest neighbor (k-NN) graph
construction has been used almost exclusively in the
literature. However, k-NN graphs often produce
hubs, or vertices with extremely high degree (i.e.,
the number of edges incident to a vertex). This ten-
dency is obvious especially if the original data is
high-dimensional?a characteristic typical of natu-
ral language data. In a later section, we demonstrate
that such hub vertices indeed deteriorate the accu-
racy of semi-supervised classification.
While not in the context of graph construction,
Radovanovic? et al (2010) made an insightful obser-
vation into the nature of hubs in high-dimensional
space; in their context, a hub is a sample close to
many other samples in the (high-dimensional) sam-
ple space. They state that such hubs inherently
emerge in high-dimensional data as a side effect of
the ?curse of dimensionality,? and argue that this is a
154
reason nearest neighbor classification does not work
well in high-dimensional space.
Their observation is insightful for graph con-
struction as well. Most of the graph-based semi-
supervised classification methods work by gradu-
ally propagating label information of a vertex to-
wards neighboring vertices in a graph, but the neigh-
borhood structure in the graph is basically deter-
mined by the proximity of data in the original high-
dimensional sample space. Hence, it is very likely
that a hub in the sample space also makes a hub
in the k-NN graph, since k-NN graph construction
greedily connects a pair of vertices if the sample cor-
responding to one vertex is among the k closest sam-
ples of the other sample in the original space. It is
therefore desirable to have an efficient graph con-
struction method for high-dimensional data that can
produce a graph with reduced hub effects.
To this end, we propose to use the mutual k-
nearest neighbor graphs (mutual k-NN graphs),
a less well-known variant of the standard k-NN
graphs. All vertices in a mutual k-NN graph have
a degree upper-bounded by k, which is not usually
the case with standard k-NN graphs. This property
helps not to produce vertices with extremely high
degree (hub vertices) in the graph. A mutual k-NN
graph is easy to build, at a time complexity identical
to that of the k-NN graph construction.
We first evaluated the quality of the graphs apart
from specific classification algorithms using the ?-
edge ratio of graphs. Our experimental results show
that the mutual k-NN graphs have a smaller num-
ber of edges connecting vertices with different la-
bels than the k-NN graphs, thus reducing the possi-
bility of wrong label information to be propagated.
We also compare the classification accuracy of two
standard semi-supervised classification algorithms
on the mutual k-NN graphs and the k-NN graphs.
The results show that the mutual k-NN graphs con-
sistently outperorm the k-NN graphs. Moreover, the
mutual k-NN graphs achieve equally well or bet-
ter classification accuracy than the state-of-the-art
graph construction method called b-matching (Je-
bara et al, 2009), while taking much less time to
construct.
2 Problem Statement
2.1 Semi-supervised Classification
The problem of semi-supervised classification can
be stated as follows. We are given a set of n ex-
amples, X = {x1, . . . ,xn}, but only the labels
of the first l examples are at hand; the remaining
u = n ? l examples are unlabeled examples. Let
S = {1, . . . , c} be the set of possible labels, and
yi ? S the label of xi, for i = 1, . . . , n. Since
we only know the labels of the first l examples, we
do not have access to yl+1, . . . , yn. For later conve-
nience, further let y = (y1, . . . , yn).
The goal of a semi-supervised classification al-
gorithm is to predict the hidden labels yl+1, . . . , yn
of u unlabeled examples xl+1, . . . ,xn, given
these unlabeled examples and l labeled data
(x1, y1), . . . , (xl, yl). A measure of similarity be-
tween examples is also provided to the algorithm.
Stated differently, the classifier has access to an all-
pair similarity matrix W ? of size n ? n, with its
(i, j)-element W ?ij holding the similarity of exam-
ples xi and xj . It is assumed that W ? is a symmetric
matrix, and the more similar two examples are (with
respect to the similarity measure), more likely they
are to have the same label. This last assumption is
the premise of many semi-supervised classification
algorithms and is often called the cluster assumption
(Zhou et al, 2004).
2.2 Graph-based Semi-supervised
Classification
Graph-based approaches to semi-supervised classi-
fication are applicable if examples X are graph ver-
tices. Otherwise, X must first be converted into a
graph. This latter case is the focus of this paper.
That is, we are interested in how to construct a graph
from the examples, so that the subsequent classifica-
tion works well.
Let G denote the graph constructed from the ex-
amples. Naturally, G has n vertices, since vertices
are identified with examples. Instead of graph G it-
self, let us consider its real-valued (weighted) adja-
cency matrix W , of size n ? n. The task of graph
construction then reduces to computing W from all-
pairs similarity matrix W ?.
The simplest way to compute W from W ? is to
let W = W ?, which boils down to using a dense,
155
complete graph G with the unmodified all-pairs sim-
ilarity as its edge weights. However, it has been ob-
served that a sparseW not only save time needed for
classification, but also results in better classification
accuracy1 than the full similarity matrix W ? (Zhu,
2008). Thus, we are concerned with how to sparsify
W ? to obtain a sparseW ; i.e., the strategy of zeroing
out some elements of W ?.
Let the set of binary values be B = {0, 1}. A spar-
sification strategy can be represented by a binary-
valued matrix P ? Bn?n, where Pij = 1 if W ?ij
must be retained as Wij , and Pij = 0 if Wij = 0.
Then, the weighted adjacency matrix W of G is
given by Wij = PijW ?ij . The n ? n matrices W
and P are symmetric, reflecting the fact that most
graph-based algorithms require the input graph to be
undirected.
3 k-Nearest Neighbor Graphs and the
Effect of Hubs
The standard approach to making a sparse graph G
(or equivalently, matrix W ) is to construct a k-NN
graph from the data (Szummer and Jaakkola, 2002;
Niu et al, 2005; Goldberg and Zhu, 2006).
3.1 The k-Nearest Neighbor Graphs
The k-NN graph is a weighted undirected graph con-
necting each vertex to its k-nearest neighbors in the
original sample space. Building a k-NN graph is a
two step process. First we solve the following opti-
mization problem.
max
P??Bn?n
?
i,j
P?ijW
?
ij (1)
s.t.
?
j
P?ij = k, P?ii = 0, ?i, j ? {1, . . . , n}
Note that we are trying to find P? , and not P . This
is an easy problem and we can solve it by greedily
assigning P?ij = 1 only if W ?ij is among the top k
elements in the ith row of W ? (in terms of the mag-
nitude of the elements). After P? is determined, we
let Pij = max(P?ij , P?ji). Thus P is a symmetric
matrix, i.e., Pij = Pji for all i and j, while P? may
1See also the experimental results of Section 6.3.2 in which
the full similarity matrix W ? is used as the baseline.
d 1 2 ? 3 total
# of vertices 1610 1947 164 3721
original 65.9 65.7 69.8 66.0
hub-removed 66.6 66.0 69.8 66.4
Table 1: Classification accuracy of vertices around hubs
in a k-NN graph, before (?original?) and after (?hub-
removed?) hubs are removed. The value d represents the
shortest distance (number of hops) from a vertex to its
nearest hub vertex in the graph.
not. Finally, weighted adjacency matrix W is deter-
mined byWij = PijW ?ij . MatrixW is also symmet-
ric since P and W ? are symmetric.
This process is equivalent to retaining all edges
from each vertex to its k-nearest neighbor vertices,
and then making all edges undirected.
Note the above symmetrization step is necessary
because the k-nearest neighbor relation is not sym-
metric; even if a vertex vi is a k-nearest neighbor of
another vertex vj , vj may or may not be a k-nearest
neighbor of vi. Thus, symmetrizing P and W as
above makes the graph irregular; i.e., the degree of
some vertices may be larger than k, which opens the
possibility of hubs to emerge.
3.2 Effect of Hubs on Classification
In this section, we demonstrate that hubs in k-NN
graphs are indeed harmful to semi-supervised clas-
sification as we claimed earlier. To this end, we
eliminate such high degree vertices from the graph,
and compare the classification accuracy of other ver-
tices before and after the elimination. For this pre-
liminary experiment, we used the ?line? dataset of
a word sense disambiguation task (Leacock et al,
1993). For details of the dataset and the task, see
Section 6.
In this experiment, we randomly selected 10 per-
cent of examples as labeled examples. The remain-
ing 90 percent makes the set of unlabeled examples,
and the goal is to predict the label (word sense) of
these unlabeled examples.
We first built a k-NN graph (with k = 3)
from the dataset, and ran Gaussian Random Fields
(GRF) (Zhu et al, 2003), one of the most widely-
used graph-based semi-supervised classification al-
gorithms. Then we removed vertices with degree
156
greater than or equal to 30 from the k-NN graph,
and ran GRF again on this ?hub-removed? graph.
Table 1 shows the classification accuracy of GRF
on the two graphs. The table shows both the over-
all classification accuracy, and the classification ac-
curacy on the subsets of vertices, stratified by their
distance d from the nearest hub vertices (which were
eliminated in the ?hub-removed? graph). Obvi-
ously, overall classification accuracy has improved
after hub removal. Also notice that the increase in
the classification accuracy on the vertices nearest to
hubs (d = 1, 2). These results suggest that the pres-
ence of hubs in the graph is deteriorating classifica-
tion accuracy.
4 Mutual k-Nearest Neighbor Graphs for
Semi-supervised Classification
As demonstrated in Section 3.2, removing hub ver-
tices in k-NN graphs is an easy way of improv-
ing the accuracy of semi-supervised classification.
However, this method adds another parameter to the
graph construction method, namely, the threshold on
the degree of vertices to be removed. The method
also does not tell us how to assign labels to the re-
moved (hub) vertices. Hence, it is more desirable
to have a graph construction method which has only
one parameter just like the k-NN graphs, but is at the
same time less prone to produce hub vertices.
In this section, we propose to use mutual k-NN
graphs for this purpose.
4.1 Mutual k-Nearest Neighbor Graphs
The mutual k-NN graph is not a new concept and
it has been used sometimes in clustering. Even in
clustering, however, they are not at all as popular as
the ordinary k-NN graphs. A mutual k-NN graph
is defined as a graph in which there is an edge be-
tween vertices vi and vj if each of them belongs to
the k-nearest neighbors (in terms of the original sim-
ilarity metric W ) of the other vertex. By contrast, a
k-NN graph has an edge between vertices vi and vj
if one of them belongs to the k-nearest neighbors of
the other. Hence, the mutual k-NN graph is a sub-
graph of the k-NN graph computed from the same
data with the same value of k. The mutual k-NN
graph first optimizes the same formula as (1), but in
mutual k-NN graphs, the binary-valued symmetric
matrix P is defined as Pij = min(P?ij , P?ji). Since
mutual k-NN graph construction guarantees that all
vertices in the resulting graph have degree at most
k, it is less likely to produce extremely high degree
vertices in comparison with k-NN graphs, provided
that the value of k is kept adequately small.
4.2 Fixing Weak Connectivity
Because the mutual k-NN graph construction is
more selective of edges than the standard k-NN
graphs, the resulting graphs often contain many
small disconnected components. Disconnected
components are not much of a problem for clus-
tering (since its objective is to divide a graph into
discrete components eventually), but can be a prob-
lem for semi-supervised classification algorithms; if
a connected component does not contain a labeled
node, the algorithms cannot reliably predict the la-
bels of the vertices in the component; recall that
these algorithms infer labels by propagating label in-
formation along edges in the graph.
As a simple method for overcoming this problem,
we combine the mutual k-NN graph and the max-
imum spanning tree. To be precise, the minimum
number of edges from the maximum spanning tree
are added to the mutual k-NN graph to ensure that
only one connected component exists in a graph.
4.3 Computational Efficiency
Using a Fibonacci heap-based implementation
(Fredman and Tarjan, 1987), one can construct
the standard k-NN graph in (amortized) O(n2 +
kn log n) time. A mutual k-NN graph can also be
constructed in the same time complexity as the k-
NN graphs. The procedure below transforms a stan-
dard k-NN graph into a mutual k-NN graph. It uses
Fibonacci heaps once again and assumes that the in-
put k-NN graph is represented as an adjacency ma-
trix in sparse matrix representation.
1. Each vertex is associated with its own heap.
For each edge e connecting vertices u and v,
insert e to the heaps associated with u and v.
2. Fetch maximum weighted edges from each
heap k times, keeping globally the record of
the number of times each edge is fetched. No-
tice that an edge can be fetched at most twice,
157
once at an end vertex of the edge and once at
the other end.
3. A mutual k-NN graph can be constructed by
only keeping edges fetched twice in the previ-
ous step.
The complexity of this procedure is O(kn). Hence
the overall complexity of building a mutual k-NN
graph is dominated by the time needed to build
the standard k-NN graph input to the system; i.e.,
O(n2 + kn log n).
If we call the above procedure on an approximate
k-NN graph which can be computed more efficiently
(Beygelzimer et al, 2006; Chen et al, 2009; Ram
et al, 2010; Tabei et al, 2010), it yields an ap-
proximate mutual k-NN graphs. In this case, the
overall complexity is identical to that of the ap-
proximate k-NN graph construction algorithm, since
these approximate algorithms have a complexity at
least O(kn).
5 Related Work
5.1 b-Matching Graphs
Recently, Jebara et al (2009) proposed a new
graph construction method called b-matching. A b-
matching graph is a b-regular graph, meaning that
every vertex has the degree b uniformly. It can be ob-
tained by solving the following optimization prob-
lem.
max
P?Bn?n
?
ij
PijW
?
ij
s.t.
?
j
Pij = b, ?i ? {1, . . . , n} (2)
Pii = 0, ?i ? {1, . . . , n} (3)
Pij = Pji, ?i, j ? {1, . . . , n} (4)
After P is computed, the weighted adjacency matrix
W is determined by Wij = PijW ?ij The constraint
(4) makes the binary matrix P symmetric, and (3) is
to ignore self-similarity (loops). Also, the constraint
(2) ensures that the graph is regular. Note that k-NN
graphs are in general not regular. The regularity re-
quirement of the b-matching graphs can be regarded
as an effort to avoid the hubness phenomenon dis-
cussed by Radovanovic? et al (2010).
Figure 1: Two extreme cases of ?-edge ratio. Vertex
shapes (and colors) denote the class labels. The ?-edge
ratio of the graph on the left is 1, meaning that all edges
connect vertices with different labels. The ?-edge ratio
of the one on the right is 0, because all edges connect
vertices of the same class.
Jebara et al (2009) reported that b-matching
graphs achieve semi-supervised classification accu-
racy higher than k-NN graphs. However, with-
out approximation, building a b-matching graph
is prohibitive in terms of computational complex-
ity. Huang and Jebara (2007) developed a fast im-
plementation based on belief propagation, but the
guaranteed running time of the implementation is
O(bn3), which is still not practical for large scale
graphs. Notice that the k-NN graphs and mutual k-
NN graphs can be constructed with much smaller
time complexity, as we mentioned in Section 4.3.
In Section exp, we empirically compare the per-
formance of mutual k-NN graphs with that of b-
matching graphs.
5.2 Mutual Nearest Neighbor in Clustering
In the clustering context, mutual k-NN graphs have
been theoretically analyzed by Maier et al (2009)
with Random Geometric Graph Theory. Their study
suggests that if one is interested in identifying the
most significant clusters only, the mutual k-NN
graphs give a better clustering result. However, it is
not clear what their results imply in semi-supervised
classification settings.
6 Experiments
We compare the k-NN, mutual k-NN, and b-
matching graphs in word sense disambiguation and
document classification tasks. All of these tasks are
multi-class classification problems.
6.1 Datasets
We used two word sense disambiguation datasets in
our experiment: ?interest? and ?line.? The ?inter-
est? data is originally taken from the POS-tagged
158
interest dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.10
0.15
0.20
0.25
ll
llll
llll
lllll
llllll
lllllll
l
0 5 10 15 20 25 30 35
l bMG
kNNG
MkNNG
line dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.15
0.20
0.25
0.30
0.35
lll
llll
llll
lllllll
llll
0 10 20 30 40
l bMG
kNNG
MkNNG
Reuters dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.10
0.12
0.14
0.16
0.18
0.20
0.22
0.24
ll
lll
lll
llll
llllllll
llllllll
0 10 20 30 40 50
l bMG
kNNG
MkNNG
20 newsgroups dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.15
0.20
0.25
0.30
0.35
0.40
0 50 100 150 200 250
kNNG
MkNNG
Figure 2: ?-edge ratios of the k-NN graph, mutual k-NN graph, and b-matching graphs. The ?-edge ratio of a graph
is a measure of how much the cluster assumption is violated; hence, smaller the ?-edge ratio, the better. The plot for
b-matching graph is missing for the 20 newsgroups dataset, because its construction did not finish after one week for
this dataset.
dataset examples features labels
interest 2,368 3,689 6
line 4,146 8,009 6
Reuters 4,028 17,143 4
20 newsgroups 19,928 62,061 20
Table 2: Datasets used in experiments.
portion of the Wall Street Journal Corpus. Each in-
stance of the polysemous word ?interest? has been
tagged with one of the six senses in Longman Dic-
tionary of Contemporary English. The details of the
dataset are described in Bruce and Wiebe (1994).
The ?line? data is originally used in numerous com-
parative studies of word sense disambiguation. Each
instance of the word ?line? has been tagged with one
of the six senses on the WordNet thesaurus. Further
details can be found in the Leacock et al (1993).
Following Niu et al (2005), we used the following
context features in the word sense disambiguation
tasks: part-of-speech of neighboring words, single
words in the surrounding context, and local colloca-
tion. Details of these context features can be found
in Lee and Ng (2002).
The Reuters dataset is extracted from RCV1-
v2/LYRL2004, a text categorization test collection
(Lewis et al, 2004). In the same manner as Cram-
mer et al (2009), we produced the classification
dataset by selecting approximately 4,000 documents
from 4 general topics (corporate, economic, gov-
ernment and markets) at random. The features de-
scribed in Lewis et al (2004) are used with this
dataset.
The 20 newsgroups dataset is a popular dataset
frequently used for document classification and
clustering. The dataset consists of approximately
20,000 messages on newsgroups and is originally
distributed by Lang (1995). Each message is as-
signed one of the 20 possible labels indicating which
newsgroup it has been posted to, and represented as
binary bag-of-words features as described in Rennie
(2001).
Table 2 summarizes the characteristics of the
datasets used in our experiments.
6.2 Experimental Setup
Our focus in this paper is a semi-supervised classi-
fication setting in which the dataset contains a small
amount of labeled examples and a large amount of
unlabeled examples. To simulate such settings, we
create 10 sets of labeled examples, with each set
consisting of randomly selected l examples from the
original dataset, where l is 10 percent of the total
number of examples. For each set, the remaining
90 percent constitute the unlabeled examples whose
labels must be inferred.
After we build a graph from the data using one
of the graph construction methods discussed earlier,
a graph-based semi-supervised classification algo-
rithm must be run on the resulting graph to infer la-
bels to the unlabeled examples (vertices). We use
two most frequently used classification algorithms:
Gaussian Random Fields (GRF) (Zhu et al, 2003)
and the Local/Global Consistency algorithm (LGC)
(Zhou et al, 2004). Averaged classification accuracy
is used as the evaluation metric. For all datasets, co-
159
interest dataset (GRF)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.79
0.80
0.81
0.82
0.83
ll
llllllllllllllllll
l
ll
lll
0 5 10 15 20 25 30 35
l bMG
kNNG
MkNNG
interest dataset (LGC)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.79
0.80
0.81
0.82
0.83
ll
llllllll
lllllllllll
l
ll
lll
0 5 10 15 20 25 30 35
l bMG
kNNG
MkNNG
line dataset (GRF)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.62
0.64
0.66
0.68
0.70
lllllllllllll
l
llllll
0 10 20 30 40
l bMG
kNNG
MkNNG
line dataset (LGC)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.62
0.64
0.66
0.68
0.70
lllllllllllll
l
llllll
0 10 20 30 40
l bMG
kNNG
MkNNG
Figure 3: Averaged classification accuracies for k-NN graphs, b-matching graphs and mutual k-NN graphs (+ maxi-
mum spanning trees) in the interest and line datasets.
sine similarity is used as the similarity measure be-
tween examples.
In ?interest? and ?line? datasets, we compare the
performance of the graph construction methods over
the broad range of their parameters; i.e., b in b-
matching graphs and k in (mutual) k-NN graphs.
In Reuters and the 20 newsgroups datasets, 2-fold
cross validation is used to determine the hyperpa-
rameters (k and b) of the graph construction meth-
ods; i.e., we split the labeled data into two folds, and
used one fold for training and the other for develop-
ment, and then switch the folds in order to find the
optimal hyperparameter among k, b ? {2, . . . , 50}.
The smoothing parameter ? of LGC is fixed at ? =
0.9.
6.3 Results
6.3.1 Comparison of ?-Edge Ratio
We first compared the ?-edge ratios of k-NN
graphs, mutual k-NN graphs, and b-matching graphs
to evaluate the quality of the graphs apart from spe-
cific classification algorithms.
For this purpose, we define the ?-edge ratio as the
yardstick to measure the quality of a graph. Here, a
?-edge of a labeled graph (G,y) is any edge (vi, vj)
for which yi 6= yj (Cesa-Bianchi et al, 2010), and
we define the ?-edge ratio of a graph as the number
of ?-edges divided by the total number of edges in
the graph. Since most graph-based semi-supervised
classification methods propagate label information
along edges, edges connecting vertices with differ-
ent labels may lead to misclassification. Hence, a
graph with a smaller ?-edge ratio is more desirable.
Figure 1 illustrates two toy graphs with extreme val-
ues of ?-edge ratio.
Figure 2 shows the plots of ?-edge ratios of the
compared graph construction methods when the val-
ues of parameters k (for k-NN and mutual k-NN
graphs) and b (for b-matching graphs) are varied. In
these plots, the y-axes denote the ?-edge ratio of the
constructed graphs. The x-axes denote the number
of edges in the constructed graphs, and not the val-
ues of parameters k or b, because setting parameters
b and k to an equal value does not achieve the same
level of sparsity (number of edges) in the resulting
graphs.
As mentioned earlier, the smaller the ?-edge ra-
tio, the more desirable. As the figure shows, mu-
tual k-NN graphs achieve smaller ?-edge ratio than
other graphs if they are compared at the same level
of graph sparsity.
The plot for b-matching graph is missing for the
20 newsgroups data, because we were unable to
complete its construction in one week2. Meanwhile,
a k-NN graph and a mutual k-NN graph for the same
dataset can be constructed in less than 15 minutes on
the same computer.
6.3.2 Classification Results
Figure 3 shows the classification accuracy of GRF
and LGC on the different types of graphs con-
structed for the interest and line datasets. As in Fig-
ure 2, the x-axes represent the sparsity of the con-
structed graphs measured by the number of edges in
the graph, which can change as the hyperparameter
(b or k) of the compared graph construction methods
2All experiments were run on a machine with 2.3 GHz AMD
Opteron 8356 processors and 256 GB RAM.
160
kNN graph b-matching graph mutual kNN graph
dataset alorithm Dense MST original +MST original +MST original +MST
Reuters GRF 43.65 72.74 81.70 80.89 84.04 84.04 85.01 84.72
Reuters LGC 43.66 71.78 82.60 82.60 84.42 84.42 84.81 84.85
20 newsgroups GRF 10.18 66.96 75.47 75.47 ?? ?? 76.31 76.46
20 newsgroups LGC 14.51 65.82 75.19 75.19 ?? ?? 75.27 75.41
Table 3: Document classification accuracies for k-NN graphs, b-matching graphs, and mutual k-NN graphs. The col-
umn for ?Dense? is the result for the graph with the original similarity matrix W ? as the adjacency matrix; i.e., without
using any graph construction (sparsification) methods. The column for ?MST? is the result the for the maximum span-
ning tree. b-matching graph construction did not complete after one week on the 20 newsgroups data, and hence no
results are shown.
vs. kNNG vs. bMG
dataset (algo) orig +MST orig +MST
Reuters (GRF)   > ?
Reuters (LGC)   ? ?
20 newsgroups (GRF)   ?? ??
20 newsgroups (LGC) ? > ?? ??
Table 4: One-sided paired t-test results of averaged ac-
curacies between using mutual k-NN graphs and other
graphs. ??, ?>?, and ??? correspond to p-value <
0.01, (0.01, 0.05], and > 0.05 respectively.
are varied.
As shown in the figure, the combination of mu-
tual k-NN graphs and the maximum spanning trees
achieves better accuracy than other graph construc-
tion methods in most cases, when they are com-
pared at the same levels of graph sparsity (number
of edges).
Table 3 summarizes the classification accuracy on
the document classification datasets. As a baseline,
the table also shows the results (?Dense?) on the
dense complete graph with the original all-pairs sim-
ilarity matrix W ? as the adjacency matrix (i.e., no
graph sparsification), as well as the results for us-
ing the maximum spanning tree alone as the graph
construction method.
In all cases, mutual k-NN graphs achieve better
classification accuracy than other graphs.
Table 4 reports the one-sided paired t-test results
of averaged accuracies with k-NN graphs and b-
matching graphs against our proposed approach, the
combination of mutual k-NN graphs and maximum
spanning trees. From Table 4, we see that mutual
k-NN graphs perform significantly better than k-
NN graphs. On the other hand, theere is no signifi-
cant difference in the accuracy of the mutual k-NN
graphs and b-matching graphs. However, mutual
k-NN graphs achieves the same level of accuracy
with b-matching graphs, at much less computation
time and are applicable to large datasets. As men-
tioned earlier, mutual k-NN graphs can be computed
with less than 15 minutes in the 20 newsgroups data,
while b-matching graphs cannot be computed in one
week.
7 Conclusion
In this paper, we have proposed to use mutual k-
NN graphs instead of the standard k-NN graphs for
graph-based semi-supervised learning. In mutual k-
NN graphs, all vertices have degree upper bounded
by k. We have demonstrated that this type of
graph construction alleviates the hub effects stated
in Radovanovic? et al (2010), which also makes the
graph more consistent with the cluster assumption.
In addition, we have shown that the weak connectiv-
ity of mutual k-NN graphs is not a serious problem
if we augment the graph with maximum spanning
trees. Experimental results on various natural lan-
guage processing datasets show that mutual k-NN
graphs lead to higher classification accuracy than the
standard k-NN graphs, when two popular label in-
ference methods are run on these graphs.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2007. Data-
driven graph construction for semi-supervised graph-
based learning in NLP. In Proc. of HLT-NAACL.
161
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine transla-
tion. In Proc. of NAACL-HLT.
Alina Beygelzimer, Sham Kakade, and John Langford.
2006. Cover trees for nearest neighbor. In Proc. of
ICML.
Rebecca Bruce and Janyce Wiebe. 1994. Word-sense
disambiguation using decomposable models. In Proc.
of ACL.
Je?ro?me Callut, Kevin Franc?oisse, Marco Saerens, and
Pierre Dupont. 2008. Semi-supervised classification
from discriminative random walks. In Proc. of ECML-
PKDD.
Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and
Giovanni Zappella. 2010. Random spanning trees and
the prediction of weighted graphs. In Proc. of ICML.
Jie Chen, Haw-ren Fang, and Yousef Saad. 2009. Fast
approximate kNN graph construction for high dimen-
sional data via recursive lanczos bisection. Journal of
Machine Learning Research, 10.
Koby Crammer, Mark Dredze, and Alex Kulesza. 2009.
Multi-class confidence weighted algorithms. In Proc.
of EMNLP.
Michael L. Fredman and Robert Endre Tarjan. 1987. Fi-
bonacci heaps and their uses in improved network op-
timization algorithms. J. ACM, 34:596?615, July.
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing
stars when there aren?t many stars: graph-based semi-
supervised learning for sentiment categorization. In
Proc. of TextGraphs Workshop on HLT-NAACL.
Bert Huang and Tony Jebara. 2007. Loopy belief prop-
agation for bipartite maximum weight b-matching. In
Proc. of AISTATS.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proc. of ICML.
Ken Lang. 1995. Newsweeder: Learning to filter net-
news. In Proc. of ICML.
Claudia Leacock, Geoffrey Towell, and Ellen Voorhees.
1993. Corpus-based statistical sense resolution. In
Proc. of ARPA Workshop on HLT.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In Proc. of
EMNLP.
David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,
G. Dietterich, and Fan Li. 2004. RCV1: A new bench-
mark collection for text categorization research. Jour-
nal of Machine Learning Research, 5.
Markus Maier, Matthias Hein, and Ulrike von Luxburg.
2009. Optimal construction of k-nearest-neighbor
graphs for identifying noisy clusters. Journal of Theo-
retical Computer Science, 410.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005.
Word sense disambiguation using label propagation
based semi-supervised learning. In Proc. of ACL.
Milos? Radovanovic?, Alexandros Nanopoulos, and Mir-
jana Ivanovic?. 2010. Hub in space: popular nearest
neighbors in high-dimensional data. Journal of Ma-
chine Learning Research, 11.
Parikshit Ram, Dongryeol Lee, William March, and
Alexander Gray. 2010. Linear-time algorithms for
pairwise statistical problems. In Proc. of NIPS.
Jason D. M. Rennie. 2001. Improving multi-class text
classification with naive bayes. Master?s thesis, Mas-
sachusetts Institute of Technology. AITR-2001-004.
Martin Szummer and Tommi Jaakkola. 2002. Partially
labeled classification with markov random walks. In
Proc. of NIPS.
Yasuo Tabei, Takeaki Uno, Masashi Sugiyama, and Koji
Tsuda. 2010. Single versus multiple sorting in all
pairs similarity search. In Proc. of ACML.
Jun Wang, Tony Jebara, and Shih-Fu. Chang. 2008.
Graph transduction via alternating minimization. In
Proc. of ICML.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scho?lkopf. 2004. Learn-
ing with local and global consistency. In Proc. of
NIPS.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In Proc. of ICML.
Xiaojin Zhu. 2005. Semi-Supervised Learning with
Graphs. Ph.D. thesis, Carnegie Mellon University.
CMU-LTI-05-192.
Xiaojin Zhu. 2008. Semi-supervised learning literature
survey. Technical Report 1530, Computer Sciences,
University of Wisconsin-Madison.
162
