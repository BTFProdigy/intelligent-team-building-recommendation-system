Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 385?390,
Sydney, July 2006. c?2006 Association for Computational Linguistics
When Conset meets Synset: A Preliminary Survey of an Ontological
Lexical Resource based on Chinese Characters
Shu-Kai Hsieh
Institute of Linguistics
Academia Sinica
Taipei, Taiwan
shukai@gate.sinica.edu.tw
Chu-Ren Huang
Institute of Linguistics
Academia Sinica
Taipei, Taiwan
churen@gate.sinica.edu.tw
Abstract
This paper describes an on-going project
concerning with an ontological lexical re-
source based on the abundant conceptual
information grounded on Chinese charac-
ters. The ultimate goal of this project is set
to construct a cognitively sound and com-
putationally effective character-grounded
machine-understandable resource.
Philosophically, Chinese ideogram has its
ontological status, but its applicability to
the NLP task has not been expressed ex-
plicitly in terms of language resource. We
thus propose the first attempt to locate Chi-
nese characters within the context of on-
tology. Having the primary success in ap-
plying it to some NLP tasks, we believe
that the construction of this knowledge re-
source will shed new light on theoretical
setting as well as the construction of Chi-
nese lexical semantic resources.
1 Introduction
In the history of western linguistics, writing has
long been viewed as a surrogate or substitute for
speech, the latter being the primary vehicle for hu-
man communication. Such ?surrogational model?
which neglects the systematicity of writing in
its own right has also occupied the predominant
views in current computational linguistic studies.
This paper is set to provide a quite different per-
spective along with the Eastern philological tra-
dition of the study of scripts, especially the ideo-
graphic one i.e., Chinese characters (Hanzi). We
believe that the conceptual knowledge information
which has been grounded on Chinese characters
can be used as a cognitively sound and compu-
tationally effective ontological lexical resource in
performing some NLP tasks, and it will have con-
tribution to the development of Semantic Web as
well.
2 Background Issues of Chinese
Ideographic Writing
2.1 Ideographic Script and Conceptual
Knowledge
From the view of writing system and cognition,
human conceptual information has been regarded
as being wired in ideographic scripts. However, in
reviewing the contemporary linguistic literatures
concerning with the discussions of the essence of
Chinese writing system, we found that the main
theoretical dispute lies in the fact that, both struc-
tural descriptions and psycholinguistic modeling
seem to presume that the notions of ideography
and phonography are mutually exclusive.
To break the theoretical impasse?, we take a
pragmatic position in claiming the tripartite prop-
erties of Chinese characters: They are logographic
(morpho-syllabic) in essence, function phonologi-
cally at the same time, and can be interpreted ideo-
graphically and implemented as concept instances
by computers.
2.2 Chinese Wordhood
Roughly put, a Chinese character is regarded as
an ideographic symbol representing syllable and
meaning of a ?morpheme? in spoken Chinese.
But unlike most affixing languages, Chinese has
a large class ofmorphemes - which Packard (2000)
calls ?bound roots? - that possess certain affixal
properties (namely, they are bound and productive
in forming words), but encode lexical rather than
385
grammatical information. These may occur as ei-
ther the left- or right-hand component of a word.
For example, the morpheme ? (/shu/; ?transport?)
can be used as either the first morpheme (e.g., ??
(/yu`n-ru`/; transport-into ?import?), or the second
morpheme (e.g., ?? /yu`n-shu/; transit-transport
?conveyance?) of a dissyllabic word, but cannot
occur in isolation.
The fuzzy boundary between free and bound
morphemes is directly related to the notori-
ous controversial notion of Chinese Wordhood.
There are multiple studies showing that to a
large extent, (trained or untrained) native speak-
ers of Chinese disagree on what a (free) mor-
pheme/word/compound is.
Such difficulty could be traced back to its histor-
ical facts. In modern Mandarin Chinese, there is a
strong tendency toward dissyllabic words, while
the predominant monosyllabic words in ancient
Chinese remain more or less a closed set. But
the conceptual knowledge encoded in monosyl-
labic morphemes still have their influence even on
contemporary texts, and thus resulting the difficul-
ties of word-marking decision.
3 Theoretical Setting
Yu et al(1999) reported that a Morpheme Knowl-
edge Base of Modern Chinese according to all Chi-
nese characters in GB2312-80 code has been con-
structed by the institute of Computational Linguis-
tics of Peking University. This Morpheme Knowl-
edge Base has been later integrated into the project
called ?Grammatical Knowledge Base of Contem-
porary Chinese?.
It is noted that the ?morphemes? adopted in this
database are monosyllabic ?bound morphemes?.
As for ?free morphemes?, that is, characters which
can be independently used as words, are not in-
cluded in the Knowledge Base. For example,
the monosyllabic character ? (/shu/,?comb?) has
(at least) two senses. For the verbal sense (?to
comb?), it can be used as a word; for the nomi-
nal sense (?a comb?), it can only be used in com-
bining with other morphemes. Therefore, only the
nominal sense of ? is included in the Knowledge
Base. However, such morpheme-based approach
can hardly escape from facing with the difficult
decision of free/bound distinction in contemporary
Chinese.
3.1 Hanzi/Word Space Model
Based on the consideration mentioned above, in
this paper, we will propose a historical, conven-
tionalized, pre-theoretical perspective in viewing
the lexical and knowledge information within Chi-
nese characters. In Figure 1, (a) illustrates a naive
Hanzi space, while (d) shows a linguistic theory-
laden result of Hanzi/Word space, where green ar-
eas denote to words, consisting of 1 to 4 char-
acters. The decision of words (green) and non-
words (white) in the space is based on certain per-
spectives (be it psycholinguistic or computational
linguistic). Instead, we take the traditional philo-
logical construct of Hanzi into consideration. By
analyzing the conceptual relations between char-
acters (b) which scatter among diverse lexical re-
sources, we construct an top-level ontology with
Hanzi as its instances (c). Rather than (a) ? (d),
which is a predominant approach in contempo-
rary linguistic theoretical construction of Chinese
Wordhood, we believe that the proposed approach
(a) ? (b) ? (c) ? (d) could not only enclose
the implicit conceptual information evolutionarily
encoded in Chinese characters, but also provide a
more clear knowledge scenario for the interaction
of characters/words in modern linguistic theoreti-
cal setting.
3.2 Conset and Character Ontology
The new model that we propose here is called
HanziNet. It relies on a novel notion called con-
set and a coarsely grained upper-level ontology
of characters.
In comparison with synset, which has become
a core notion in the construction of Wordnet-like
lexical semantic resources, we will argue that there
is a crucial difference between Word-based lexi-
cal resource and character-based lexical resource,
in that they rest with finely-differentiated informa-
tion contents represented by the nodes of network.
A synset, or synonym set in WordNet contains a
group of words,1 and each of which is synony-
mous with the other words in the same synset.
In WordNet?s design, each synset can be viewed
as a concept in a taxonomy, While in HanziNet,
we are seeking to align Hanzi which share a given
putatively primitive meaning extracted from tradi-
tional philological resources, so a new term con-
set (concept set) is proposed. A conset contains
1To put it exactly, it contains a group of lexical units,
which can be words or collocations.
386
(a) (b) (c) (d)
Figure 1: Illustrations of Hanzi/Word Spaces
a group of Chinese characters similar in concept,
and each of which shares with similar conceptual
information with the other characters in the same
conset.2
The relations between consets constitute a char-
acter ontology. Formally, it is a tree-structured
conceptual taxonomy in terms of which only two
kinds of relations are allowed: the INSTANCE-OF
(i.e., characters are instances of consets) and IS-
A relations (i.e., consets are hypernyms/hyponyms
to other consets).
Currently, frequently used monosyllabic char-
acters are assigned to at least one of 309 consets.
Following are some examples:
conset 126 (SUBJECTIVE ? EXCITABILITY ? ABILITY ? ORGANIC
FUNCTION)
?? ???????????????,
conset 130 (SUBJECTIVE? EXCITABILITY? ABILITY? SKILLS)
?????????????,
conset 133 (SUBJECTIVE? EXCITABILITY? ABILITY? INTELLECT)
?????????????,
In fact, the core assumption behind the
synset/conset distinction is non-trivial. In this
project, we assume a hypothesis of the locality
of Concept Gestalt and the context-sensibility of
Word Sense concerning with Chinese characters.
That is, characters carry two meaning dimensions:
on the one hand, they are lexicalized concepts;
2At the time of writing, about 3,600 characters have been
finished in their information construction.
on the other hands, they can be observed lin-
guistically as bound root morphemes and mono-
morphemic words according to their independent
usage in modern Chinese texts.
Figure 2 shows a schematic diagram of our pro-
posed model. In Aitchison?s (2003) terms, for the
character level, we take an ?atomic globule? net-
work viewpoint, where the characters - realized as
instances of core concept Gestalt - which share
similar conceptual information, cluster together.
The relationships between these concept Gestalt
form a rooted tree structure. Characters are thus
assigned to the leaves of the tree in terms of an
assemblage of bits. For the word level, we take
the ?cobweb? viewpoint, as words -built up from
a pool of characters- are connected to each other
through lexical semantic relations. In such case,
the network does not form a tree structure but a
more complex, long-range highly-correlated ran-
dom acyclic graphic structure.
4 Hanzi-grounded Ontological
CharacterNet
In light of the previous consideration, this sec-
tion attempts to further clarify the building blocks
of the HanziNet system, ? a Hanzi-grounded on-
tological Character Net ? with the goal to ar-
rive at a working model which will serve as a
framework for ontological knowledge processing.
Briefly, HanziNet is consisted of two main parts:
387
Figure 2: The Schematic Representation of
character-triggered tree-like conceptual hierarchy
and word-based semantic network
a character-stored machine-readable lexicon and a
top-level character ontology.
4.1 Hanzi-grounded Lexicon and Ontology
The current lexicon contains over 5000 characters,
and 30,000 derived words in total.3
The building of the lexical specification of the
entries in HanziNet includes various aspects of
Hanzi:
1. Conset(s): The conceptual code is the core
part of the MRD lexicon in HanziNet. Con-
cepts in HanziNet are indicated by means
of a label (conset name) with a code form.
In order to increase the efficiency, an ideal
strategy is to adopt the Huffmann-coding-like
method, by encoding the conceptual structure
of Hanzi as a pattern of bits set within a bit
string.4 The coding thus refers to the assign-
ment of code sequences to an character. The
sequence of edges from the root to any char-
acter yields the code for that character, and
the number of bits varies from one character
to another. Currently, for each conset (309 in
total) there are 12 characters assigned on the
average; for each character, it is assigned to
3Since this lexicon aims at establishing an knowl-
edge resource for modern Chinese NLP, characters
and words are mostly extracted from the Academia
Sinica Balanced Corpus of Modern Chinese
(http://www.sinica.edu.tw/SinicaCorpus/), those charac-
ters and words which have probably only appeared in
classical literary works, (considered ghost words in the
lexicography), will be discarded.
4This is inspired by Chu (1999)?s works.
2-3 consets on the average.5
2. Character Semantic Head (CSH) and Char-
acter Semantic Modifier (CSM) division.6
3. Shallow parts of speech (mainly Nominal(N)
and Verbal(V) tags)
4. Gloss of prototypical meaning
5. List of combined words with statistics calcu-
lated from corpus, and
6. Further aspects such as character types and
cognates: According to ancient study, char-
acters can be compartmentalized into six
groups based on the six classical principles of
character construction. Character type here
means which group the character belongs to.
And the term cognate here is defined as char-
acters that share the same CSH or CSM. Fig-
ure 3 shows a snapshot of this lexicon.
Figure 3: The character-stored lexicon: a snapshot
The second core component of the proposed re-
source is a set of hierarchically related Top Con-
cepts called Top-level Ontology (or Upper ontol-
ogy). This is similar to EuroWordnet 1.2, which is
5The disputing point here is that, if some of the mono-
syllabic morphemes are taken as words, they should be very
ambiguous in the daily linguistic context, at least more am-
biguous than the dissyllabic words. However, as we argued
previously, HanziNet takes a different perspective in locating
theoretical roles of Hanzi.
6This distinction is made based on the glyphographical
consideration, which has been a crucial topic in the studies of
traditional Chinese scriptology. Due to the limited space, this
will not be discussed here.
388
also enriched with the Top Ontology and the set of
Base Concepts (Vossen 1998).
As mentioned, a tentative set of 309 conset,
a kind of ontological categories in contrast with
synset has been proposed 7, and over 5000 charac-
ters have been used as instances in populating the
character ontology.
Methodologically, following the basic line of
OntoClear approach (Guarino and Welty (2002)),
we use simple monotonic inheritance in our ontol-
ogy design, which means that each node inherits
properties only from a single ancestor, and the in-
herited value cannot be overwritten at any point of
the ontology. The decision to keep the relations
to one single parent was made in order to guaran-
tee that the structure would be able to grow indef-
initely and still be manageable, i.e. that the tran-
sitive quality of the relations between the nodes
would not degenerate with size. Figure 4 shows a
snapshot of the character ontology.
ROOT
OBJ
SUBJ
CONCRETE
ABSTRACT
EXISTENCE
ARTIFACT
EXCITABLE
COGNITIVE
SEMIOTIC
RELATIONA
L
SENSATION
STATE
INNATE
SOCIAL
conset 1
conset 309
conset 2
conset 3
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
conset 308
conset 307
{????????????}
{????????????}
{???????????}
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
{???}
{????????????????}
{??????????????}
Figure 4: The character ontology: a snapshot
4.2 Characters in a Small World
In addition, an experiment concerning the char-
acter network that was based on the meaning as-
pects of characters, was performed from a statisti-
cal point of view. It was found that this character
network, like many other linguistic semantic net-
works (such as WordNet), exhibits a small-world
property (Watt 1998), characterized by sparse con-
nectivity, small average shortest paths between
characters, and strong local clustering. Moreover,
due to its dynamic property, it appears to exhibit
an asymptotic scale-free (Barabasi 1999) feature
7It would be interesting to compare consets with the basic
400 nodes in the upper region proposed by Hovy(2005).
Table 1: Statistical characteristics of the char-
acter network: N is the total number of
nodes(characters), k is the average number of links
per node, C is the clustering coefficient, and L is
the average shortest-path length, and Lmax is the
maximum length of the shortest path between a
pair of characters in the network.
N k C L
Actual configuration 6493 350 0.64 2.0
Random configuration 6493 350 0.06 1.5
with the connectivity of power laws distribution,
which is found in many other network systems as
well.
Our first result is that our proposed conceptual
network is highly clustered and at the same time
and has a very small length, i.e., it is a small
world model in the static aspect. Specifically,
L & Lrandom but C  Crandom. Results for the
network of characters, and a comparison with a
corresponding random network with the same pa-
rameters are shown in Table 1. N is the total num-
ber of nodes (characters), k is the average number
of links per node, C is the clustering coefficient,
and L is the average shortest path.
4.3 HanziNet in the Global Wordnet Grid
In order to promote a semantic and ontological
interoperability, we have aligned conset with the
164 Base Concepts, a shared set of concepts from
EWN in terms of Wordnet synsets and SUMO
definitions, which has been currently proposed in
the international collaborative platform of Global
Wordnet Grid.
5 Applications and Future Development
5.1 Sense Prediction and Disambiguation
Based on the initial version of the proposed re-
sources, Hsieh (2005b) has proposed a semantic
class prediction model which aims to gain the pos-
sible semantic classes of unknown two-characters
words. The results obtained shows that, with this
knowledge resource, the system can achieve fairly
high level of performance. Meaning relevant NLP
Tasks such asWord Sense Disambiguation are also
in preparation.
389
5.2 Interfacing Hantology, HanziNet and
Chinese Wordnet
Interfacing ontologies and lexical resources has
been a research topic in the coming age of se-
mantic web. In the case of Chinese, three existing
lexical resources (??Radicals::Hantology (Chou
and Huang (2005))- ? Characters::HanziNet -
? Words::Chinese Wordnet) constitutes an inte-
grated 3-level knowledge scenario which would
provide important insights into the problems of
understanding the complexities and its interaction
with Chinese natural language.
6 Conclusion
In conclusion, the goal of this research is set
to survey the unique characteristics of Chinese
Ideographs.
Though it has been well understood and agreed
upon in cognitive linguistics that concepts can be
represented in many ways, using various construc-
tions at different syntactical levels, conceptual rep-
resentation at the script level has been unfortu-
nately both undervalued and under-represented in
computational linguistics. Therefore, the Hanzi-
driven conceptual approach in this thesis might re-
quire that we consider the Chinese writing system
from a perspective that is not normally found in
canonical treatments of writing systems in con-
temporary linguistics.
Against the deep-seated tradition in contempo-
rary Chinese linguistics, which views the use of
Chinese characters in scientific theories as a mani-
festation of mathematical immaturity and interpre-
tational subjectivity, we propose the first lexical
knowledge resource based on Chinese characters
in the field of linguistic as well as in the NLP.
It is noted that HanziNet, as a general knowl-
edge resource, should not claim to be a sufficient
knowledge resource in and of itself, but instead
seek to provide a groundwork for the incremen-
tal integration of other knowledge resources for
language processing tasks. In order to augment
HanziNet, additional information will needed to
be incorporated and mapped into HanziNet. This
leads us to several avenues of future research.
Acknowledgements
The authors would like to thank the anonymous
referees for constructive comments. Thanks also
go to the institute of linguistics of Academia
Sinica for their kindly data support.
References
Aitchison, Jean. 2003. Words in the mind: an introduc-
tion to the mental lexicon. Blackwell publishing.
Barabasi, Albert-Laszlo and Reka Albert. 1999. Emer-
gence of scaling in random networks. Science,
286:509-512.
Chou, Ya-Min and Chu-Ren Huang. 2005. Hantology:
An ontology based on conventionalized conceptual-
ization. OntoLex Workshop, Korea.
Chu, Bong-Foo. 1999-. http://www.cbflabs.com
Guarino, Nicola and Chris Welty. 2002. Evaluating on-
tological decisions with OntoClean. In: Communi-
cations of the ACM. 45(2):61-65
Hovy, E.H. 2005. Methodologies for the Reliable Con-
struction of Ontological Knowledge. In : F. Dau,
M.-L. Mugnier, and G. Stumme (eds), Conceptual
Structures: Common Semantics for Sharing Knowl-
edge. Proceedings of the 13th Annual International
Conference on Conceptual Structures (ICCS 2005).
Kassel, Germany.
Hsieh, Shu-Kai. 2005(a). HanziNet: An enriched
conceptual network of Chinese characters. The 5rd
workshop on Chinese lexical semantics, China: Xi-
amen.
Hsieh, Shu-Kai. 2005(b). Word Meaning Inducing via
Character Ontology. IJINLP, SIGHAN Workshop,
Jijeu Island, South Korea.
Packard, J. L. 2000. The morphology of Chinese. Cam-
bridge, UK: Cambridge University Press.
Steyvers, M. and Tenenbaum, J.B. 2002 The Large-
Scale Structure of Semantic Networks: Statistical
Analyses and a Model of Semantic Growth. Cog-
nitive Science.
Watts, D. J. and Strogatz, S. H. 1998. Collective dy-
namics of ?small-world? networks. Nature 393:440-
42.
Yu, Shiwen, Zhu Xuefeng and Li Feng. 1999. The de-
velopment and application of modern Chinese mor-
pheme knowledge base.[in Chinese]. In: ?????
?, No.2. pp38-45.
390
Word Meaning Inducing via Character Ontology: A Survey on the
Semantic Prediction of Chinese Two-Character Words
Shu-Kai Hsieh
Seminar fu?r Sprachwissenschaft
Abt. Computerlinguistik
Universita?t Tu?bingen
72074, Germany
kai@hanzinet.org
Abstract
This paper presents a semantic class
prediction model of Chinese two-
character compound words based on
a character ontology, which is set to
be a feasible conceptual knowledge re-
source grounded in Chinese characters.
The experiment we conduct yields sat-
isfactory results which turn out to be
that the task of semantic prediction of
two-character words could be greatly
facilitated using Chinese characters as
a knowledge resource.
1 Introduction
This paper describes the theoretical considera-
tion concerning with the interaction of ontology
and morpho-semantics, and an NLP experiment
is performed to do semantic class prediction of
unknown two-character words based on the on-
tological and lexical knowledge of Chinese mor-
phemic components of words (i.e., characters).
The task that the semantic predictor (or classifier)
performs is to automatically assign the (prede-
fined) semantic thesaurus classes to the unknown
two-character words of Chinese.
Among these types of unknown words, Chen
and Chen (2000) pointed out that compound
words constitute the most productive type of un-
known words in Chinese texts. However, the
caveat at this point should be carefully formu-
lated, due to the fact that there are no unequiv-
ocal opinions concerning with some basic theo-
retical settings in Chinese morphology. The no-
tion of word, morpheme and compounding are
not exactly in accord with the definition common
within the theoretical setting of Western morphol-
ogy. To avoid unnecessary misunderstanding, the
pre-theoretical term two-character words will be
mostly used instead of compound words in this
paper.
2 Word Meaning Inducing via
Character Meaning
2.1 Morpho-Semantic Description
As known, ?bound roots? are the largest classes of
morpheme types in Chinese morphology, and they
are very productive and represent lexical rather
than grammatical information (Packard 2000).
This morphological phenomena leads many Chi-
nese linguists to view the word components (i.e.,
characters) as building blocks in the seman-
tic composition process of dis- or multisyllabic
words. In many empirical studies (Tseng and
Chen (2002); Tseng (2003); Lua (1993); Chen
(2004)), this view has been confirmed repeatedly.
In the semantic studies of Chinese word for-
mation, many descriptive and cognitive seman-
tic approaches have been proposed, such as ar-
gument structure analysis (Chang 1998) and the
frame-based semantic analysis (Chu 2004). How-
ever, among these qualitative explanation theoret-
ical models, problems often appear in the lack of
predictability on the one end of spectrum, or over-
generation on the other.1 Empirical data have
1For example, in applying Lieber?s (1992) analysis of ar-
gument structure and theta-grid in Chinese V-V compounds,
Chang (1998) found some examples which may satisfy the
semantic and syntactic constraints, but they may not be ac-
56
also shown that in many cases, ? e.g., the abun-
dance of phrasal lexical units in any natural lan-
guage, ? the principle of compositionality in a
strict sense, that is, ?the meaning of a complex
expression can be fully derivable from the mean-
ings of its component parts, and from the schemas
which sanction their combination?(Taylor 2002),
which is taken to be a fundamental proposition in
some of morpho-semantically motivated analysis,
is highly questionable.
This has given to the consideration of the em-
beddedness of linguistic meanings within broader
conceptual structures. In what follows, we will
argue that an ontology-based approach would
provide an interesting and efficient prospective
toward the character-triggered morpho-semantic
analysis of Chinese words.
2.2 Conceptual Aggregate in Compounding:
A Shift Toward Character Ontology
In prior studies, it is widely presumed that the cat-
egory (be it syntactical or semantic) of a word, is
somehow strongly associated with that of its com-
posing characters. The semantic compositionality
underlying two-character words appears in differ-
ent terms in the literature.2
Word semantic similarity calculation tech-
niques have been commonly used to retrieve the
similar compositional patterns based on semantic
taxonomic thesaurus. However, one weak point
in these studies is that they are unable to sep-
arate conceptual and semantic levels. Problem
raises when words in question are conceptually
correlated are not necessarily semantically corre-
lated, viz, they might or might not be physically
close in the CILIN thesaurus (Mei et al1998).
On closer observations, we found that most syn-
onymic words (i.e., with the same CILIN seman-
tic class) have characters which carry similar con-
ceptual information. This could be best illustrated
by examples. Table 1 shows the conceptual distri-
bution of the modifiers of an example of VV com-
pound by presuming the second character ? as a
ceptable to native speakers.
2Using statistical techniques, Lua (1993) found out that
each Chinese two-character word is a result of 16 types of
semantic transformation patterns, which are extracted from
the meanings of its constituent characters. In Chen (2004),
the combination pattern is referred to as compounding se-
mantic template.
head. The first column is the semantic class of
CILIN (middle level), the second column lists the
instances with lower level classification number,
and the third column lists their conceptual types
adopted from a character ontology we will discuss
later. As we can see, though there are 12 result-
ing semantic classes for the * ? compounds, the
modifier components of these compounds involve
only 4 concept types as follows:
11000 (SUBJECTIVE? EXCITABILITY? ABILITY? ORGANIC FUNCTION)
??,
11010 (SUBJECTIVE? EXCITABILITY? ABILITY? SKILLS) ?
YL?T??,
11011 (SUBJECTIVE? EXCITABILITY? ABILITY? INTELLECT) ?
?5??p=,
11110 (SUBJECTIVE? EXCITABILITY? SOCIAL EXPERIENCE? DEAL WITH THINGS)
Y???T??,?y???
We defined these patterns as conceptual aggre-
gate pattern in compounding. Unlike statistical
measure of the co-occurrence restrictions or asso-
ciation strength, a concept aggregate pattern pro-
vides a more knowledge-rich scenario to repre-
sent a specific manner in which concepts are ag-
gregated in the ontological background, and how
they affect the compounding words. We will pro-
pose that the semantic class prediction of Chinese
two-character words could be improved by mak-
ing use of their conceptual aggregate pattern of
head/modifier component.
3 Semantic Prediction of Unknown
Two-Character Words
The practical task intended to be experimented
here involves the automatic classification of Chi-
nese two-character words into a predetermined
number of semantic classes. Difficulties encoun-
tered in previous researches could be summarized
as follows:
First, many models (Chen and Chen
1998;2000) cannot deal with the issue of
?incompleteness? of characters in the lexicon, for
these models depend heavily on CILIN, a Chinese
Thesaurus containing only about 4,133 monosyl-
labic morphemic components (characters). As
a result, if unknown words contain characters
that are not listed in CILIN, then the prediction
task cannot be performed automatically. Second,
the ambiguity of characters is often shunned by
57
SC VV compounds Concept types of modifier component
Ee 37 ?? 11110
Fa 05 Y? 08 ?? 15 L? 11010
Fc 05 =? 11011
Gb 07 p? 11011
Ha 06 T? 11110
Hb 08 ?? 12 ?? 12 T? 12 ?? 11110
Hc 07 Y? 23 ?? 25 ?? {11110;11011}
Hi 27 ?? 27 T? {11010;11110}
Hj 25 ?? 25 ?? {11010;11110}
Hn 03 ?? 10 y? 12 Y? 11110
If 09 5? 11011
Je 12 ? 12 ?? 12 ?? 12 ?? 12 ?? 12 ?
? 12 ?? 12 ?? 12 ,? 12 i? 12 ?? 12
?? 12 T?
{11000;11110;11011;11110}
Table 1: Conceptual aggregate patterns in two-character VV (compound) words: An example of * ?
manual pre-selection of character meaning in the
training step, which causes great difficulty for an
automatic work. Third, it has long been assumed
(Lua 1997; Chen and Chen 2000) that the over-
whelming majority of Chinese compounds are
more or less endocentric, where the compounds
denote a hyponym of the head component in the
compound. E.g, ?s (?electric-mail?; e-mail)
is a kind of mail. So the process of identifying
semantic class of a compound boils down to find
and to determine the semantic class of its head
morpheme. However, there is also an amount
of exocentric and appositional compounds3
where no straightforward criteria can be made to
determine the head component. For example, in
a case of VV compound o? (?denounce-scold?,
drop-on), it is difficult (and subjective) to say
which character is the head that can assign a
semantic class to the compound.
To solve above-mentioned problems, Chen
(2004) proposed a non head-oriented character-
sense association model to retrieve the latent
senses of characters and the latent synonymous
compounds among characters by measuring sim-
ilarity of semantic template in compounding by
using a MRD. However, as the author remarked
in the final discussion of classification errors, the
performance of this model relies much on the pro-
ductivity of compounding semantic templates of
the target compounds. To correctly predict the se-
mantic category of a compound with an unpro-
ductive semantic template is no doubt very dif-
ficult due to a sparse existence of the template-
3Lua reports a result of 14.14% (Z3 type).
similar compounds. In addition, the statistical
measure of sense association does not tell us any
more about the constraints and knowledge of con-
ceptual combination.
In the following, we will propose that a knowl-
edge resource at the morpheme (character) level
could be a straightforward remedy to these prob-
lems. By treating characters as instances of con-
ceptual primitives, a character ontology thereof
might provide an interpretation of conceptual
grounding of word senses. At a coarse grain, the
character ontological model does have advantages
in efficiently defining the conceptual space within
which character-grounded concept primitives and
their relations, are implicitly located.
4 A Proposed Character
Ontology-based Approach
In carrying out the semantic prediction task,
we presume the context-freeness hypothesis, i.e.,
without resorting to any contextual information.
The consideration is taken based on the observa-
tion that native speaker seems to reconstruct their
new conceptual structure locally in the processing
of unknown compound words. On the other hand,
it has the advantage especially for those unknown
words that occur only once and hence have lim-
ited context.
In general, the approach proposed here differs
in some ways from previous research based on the
following presuppositions:
58
4.1 Character Ontology as a Knowledge
Resource
The new model that we will present below will
rely on a coarsely grained upper-level ontology
of characters.4 This character ontology is a tree-
structured conceptual taxonomy in terms of which
only two kinds of relations are allowed: the
INSTANCE-OF (i.e., certain characters are in-
stances of certain concept types) and IS-A rela-
tions (i.e., certain concept type is a kind of certain
concept type).
In the character ontology, monosyllabic char-
acters 5 are assigned to at least 6 one of 309 con-
sets (concept set), a new term which is defined as
a type of concept sharing a given putatively prim-
itive meaning. For instance, z (speak), ? (chatter),
x (say), ; (say), ? (tell), s (inform), ? (explain), ? (nar-
rate), ? (be called), H (state), these characters are as-
signed to the same conset.
Following the basic line of OntoClear method-
ology (Guarino and Welty (2002)), we use sim-
ple monotonic inheritance, which means that each
node inherits properties only from a single ances-
tor, and the inherited value cannot be overwritten
at any point of the ontology. The decision to keep
the relations to one single parent was made in or-
der to guarantee that the structure would be able
to grow indefinitely and still be manageable, i.e.
that the transitive quality of the relations between
the nodes would not degenerate with size. Fig-
ure 1 shows a snapshot of the character ontology.
4.2 Character-triggered Latent
Near-synonyms
The rationale behind this approach is that simi-
lar conceptual primitives - in terms of characters
- probably participate in similar context or have
similar meaning-inducing functions. This can
be rephrased as the following presumptions: (1).
Near-synonymic words often overlap in senses,
i.e., they have same or close semantic classes. (2).
Words with characters which share similar con-
ceptual information tend to form a latent cluster
4At the time of writing, about 5,600 characters have been
finished in their information construction. Please refer to [4]
5In fact, in addition to monosyllabic morpheme, it also
contains a few dissyllabic morphemes, and borrowed poly-
syllabic morphemes.
6This is due to the homograph.
ROOT
OBJ
SUBJ
CONCRETE
ABSTRACT
EXISTENCE
ARTIFACT
EXCITABLE
COGNITIVE
SEMIOTIC
RELATIONA
L
SENSATION
STATE
INNATE
SOCIAL
conset 1
conset 309
conset 2
conset 3
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
conset 308
conset 307
{????????????}
{????????????}
{???????????}
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
{???}
{????????????????}
{??????????????}
Figure 1: The character ontology: a snapshot
of synonyms. (2). These similar conceptual in-
formation can be formalized as conceptual aggre-
gate patterns extracted from a character ontology.
(3). Identifying such conceptual aggregate pat-
terns might thus greatly benefit the automatically
acquired near-synonyms, which give a set of good
candidates in predicting the semantic class of pre-
viously unknown ones.
The proposed semantic classification system
retrieves at first a set of near-synonym candidates
using conceptual aggregation patterns. Consid-
erations from the view of lexicography can win-
now the overgenerated candidates, that is, a final
decision of a list of near-synonym candidates is
formed on the basis of the CILIN?s verdict as to
what latent near-synonyms are. Thus the semantic
class of the target unknown two-character words
will be assigned with the semantic class of the
top-ranked near-synonym calculated by the sim-
ilarity measurement between them. This method
has advantage of avoiding the snag of apparent
multiplicity of semantic usages (ambiguity) of a
character.
Take for an example. Suppose that the seman-
tic class of a two-character word \? (protect;
Hi37) is unknown. By presuming the leftmost
character \ the head of the word, and the right-
most character ? as the modifier of the word,
59
we first identify the conset which the modifier
? belongs to. Other instances in this conset are
\, ?, {, ?, 7, G, ?, ., 1, ?, ?, etc. So we
can retrieve a set of possible near-synonym can-
didates by substitution, namely, NS1: {\\, \?,
\{, \?, \7, \G, \?, \., \1, \?, \?}; in
the same way, by presuming ? as the head, we
have a second set of possible near-synonym can-
didates, NS2: {??, ??, {?, ??, 7?, G?, ??,
.?, 1?, ??, ??}7. Aligned with CILIN, those
candidates which are also listed in the CILIN are
adopted as the final two list of the near-synonym
candidates for the unknown word \?: NS?1:
{??(Hi41), ??(Hb04;Hi37), 7?(Hi47), ?
?(Hi37),??(Hd01)}, and NS?2: {\G(Hl33),\
?(Hj33), \?(Ee39)}.
4.3 Semantic Similarity Measure of
Unknown Word and its Near-Synonyms
Given two sets of character-triggered near-
synonyms candidates, the next step is to calcu-
late the semantic similarity between the unknown
word (UW) and these near-synonyms candidates.
CILIN Thesaurus is a tree-structured taxo-
nomic semantic structure of Chinese words,
which can be seen as a special case of seman-
tic network. To calculate semantic similarity be-
tween nodes in the network can thus make use of
the structural information represented in the net-
work.
Following this information content-based
model, in measuring the semantic similarity
between unknown word and its candidate near-
synonymic words, we use a measure metric
modelled on those of Chen and Chen (2000),
which is a simplification of the Resnik algorithm
by assuming that the occurrence probability
of each leaf node is equal. Given two sets
(NS?1, NS?2) of candidate near synonyms, each
with m and n near synonyms respectively, the
similarity is calculated as in equation (1) and
(2), where scuwc1 and scuwc2 are the semantic
class(es) of the first and second morphemic com-
ponent (i.e., character) of a given unknown word,
respectively. sci and scj are the semantic classes
of the first and second morphemic components
on the list of candidate near-synonyms NS?1
7Note that in this case, \ and ? are happened to be in
the same conset.
and NS?2. f is the frequency of the semantic
classes, and the denominator is the total value of
numerator for the purpose of normalization. ?
and 1?? are the weights which will be discussed
later. The Information Load (IL) of a semantic
class sc is defined in Chen and Chen (2004):
IL(sc) = Entropy(system) ? Entropy(sc)
(3)
' (?1q
?
log2
1
q ) ? (?
1
p
?
log2
1
p)
= log2 q ? log2 p
= ? log2(
p
q ),
if there is q the number of the minimal semantic
classes in the system,8 p is the number of the se-
mantic classes subordinate sc.
4.4 Circumventing ?Head-oriented?
Presupposition
As remarked in Chen (2004), the previous re-
search concerning the automatic semantic classi-
fication of Chinese compounds (Lua 1997; Chen
and Chen 2000) presupposes the endocentric fea-
ture of compounds. That is, by supposing that
compounds are composed of a head and a modi-
fier, determining the semantic category of the tar-
get therefore boils down to determine the seman-
tic category of the head compound.
In order to circumventing the strict ?head-
determination? presumption, which might suf-
fer problems in some borderline cases of V-V
compounds, the weight value (? and 1 ? ?) is
proposed. The idea of weighting comes from
the discussion of morphological productivity in
Baayen (2001). We presume that, within a given
two-character words, the more productive, that
is, the more numbers of characters a charac-
ter can combine with, the more possible it is a
head, and the more weight should be given to it.
The weight is defined as ? = C(n,1)N , viz, the
number of candidate morphemic components di-
vided by the total number of N. For instance, in
the above-mentioned example, NS1 should gain
more weights than NS2, for ? can combine with
more characters (5 near-synonyms candidates) in
8In CILIN, q = 3915.
60
sim?(UW,NS?1) = argmaxi=1,m
IL(LCS(scuwc1, sci)) ? fi
?m
i=1 IL(LCS(scuwc1, sci)) ? fi
(?) (1)
sim?(UW,NS?2) = argmaxj=1,n
IL(LCS(scuwc2, scj)) ? fj
?n
j=1 IL(LCS(scuwc2, scj)) ? fj
(1 ? ?) (2)
NS1 than \ does in NS2 (3 near-synonyms can-
didates). In this case, ? = 58 = 0.625. It is
noted that the weight assignment should be char-
acter and position independent.
4.5 Experimental Settings
4.5.1 Resources
The following resources are used in the ex-
periments: (1)Sinica Corpus9, (2) CILIN The-
saurus (Mei et al1998) and (3) a Chinese char-
acter upper-level ontology.10 (1) is a well known
balanced Corpus for modern Chinese used in Tai-
wan. (2) CILIN Thesaurus is a Chinese The-
saurus widely accepted as a semantic categoriza-
tion standard of Chinese word in Chinese NLP.
In CILIN, a collection of about 52,206 Chinese
words are grouped in a Roget?s Thesaurus-like
structure based on categories within which there
are several 3 levels of finer clustering (12 major,
95 minor and 1428 minor semantic classes).(3) is
an on-going project of Hanzi-grounded Ontology
and Lexicon as introduced.
4.5.2 Data
We conducted an open test experiment, which
meant that the training data was different from the
testing data. 800 two-character words in CILIN
were chosen at random to serve as test data, and
all the words in the test set were assumed to be un-
known. The distribution of the grammatical cate-
gories of these data is: NN (200, 25%), VN (100,
12.5%) and VV (500, 62.5%).
4.5.3 Baseline
The baseline method assigns the semantic class
of the randomly picked head component to the se-
mantic class of the unknown word in question. It
is noted that most of the morphemic components
9http://www.sinica.edu.tw/SinicaCorpus/
10http://www.hanzinet.org/HanziOnto/
Compound types Baseline Our algorithm
V-V 12.20% 42.00%
V-N 14.00% 37.00%
N-N 11.00% 72.50%
Table 2: Accuracy in the test set (level 3)
(characters) are ambiguous, in such cases, seman-
tic class is chosen at random as well.
4.5.4 Outline of the Algorithm
Briefly, the strategy to predict the seman-
tic class of a unknown two-character word
is, to measure the semantic similarity of un-
known words and their candidate near-synonyms
which are retrieved based on the character
ontology. For any unknown word UW ,
which is the character sequence of C1C2,
the RANK(sim?(?), sim?(1 ? ?)) is com-
puted. The semantic category sc of the
candidate synonym which has the value of
MAX(sim?(?), sim?(1 ? ?)), will be the top-
ranked guess for the target unknown word.
4.6 Results and Error Analysis
The correctly predicted semantic class is the se-
matic class listed in CILIN. In the case of ambigu-
ity, when the unknown word in question belongs
to more than one semantic classes, any one of the
classes of an ambiguous word is considered cor-
rect in the evaluation.
The SC prediction algorithm was performed
on the test data for outside test in level-3 classi-
fication. The resulting accuracy is shown in Ta-
ble 2. For the purpose of comparison, Table 3
also shows the more shallow semantic classifica-
tion (the 2nd level in CILIN).
Generally, without contextual information, the
classifier is able to predict the meaning of a Chi-
nese two-character words with satisfactory accu-
61
Compound types Baseline Our algorithm
V-V 13.20% 46.20%
V-N 16.00% 42.00%
N-N 12.50% 76.50%
Table 3: Accuracy in the test set (level 2)
racy against the baseline. A further examina-
tion of the bad cases indicates that error can be
grouped into the following sources:
? Words with no semantic transparency:
Like ?proper names?, these types have no se-
mantic transparency property, i.e., the word
meanings can not be derived from their mor-
phemic components. Loan words such as ?
? (/sha?fa?/; ?sofa?) are typical examples.
? Words with weak semantic transparency:
These can be further classified into four
types:
? Appositional compounds: words whose
two characters stand in a coordinate re-
lationship, e.g. ?a (?east-west?, thing).
? Lexicalized idiomatic usage: For such
usage, each word is an indivisible con-
struct and each has its meaning which
can hardly be computed by adding up
the separate meaning of the compo-
nents of the word. The sources of these
idiomatic words might lie in the etymo-
logical past and are at best meaningless
to the modern native speaker. e.g, ??
(?salary-water?, salary).
? Metaphorical usage: the meaning of
such words are therefore different from
the literal meaning. Some testing data
is not semantically transparent due to
their metaphorical uses, For instance, ?
I (Aj) is assigned to the ?? (Bk).
? Derived words:
Such as ?? (enter). These could be filter out
using syntactical information.
? The quality and coverage of CILIN and char-
acter ontology:
Since our SC system?s test and training data
are gleaned from CILIN and the character
Compound types Our model Current best
model
V-V 42.00% 39.80% (Chen
2004)
N-N 72.50% 81.00% (Chen
and Chen 2000)
Table 4: Level-3 performance in the outside test:
a comparison
ontology, the quality and coverage play a
crucial role. For example, for the unknown
compound word ?? (/sa?o-sa?o/; ?be in tu-
mult?), there not even an example which
has ? as the first character or as the sec-
ond character. the same problem such as
falling short on coverage and data sparse-
ness goes to the character ontology, too. For
instance, there are some dissyllabic mor-
phemes which are not listed in ontology,
such as ?? (/j?`yu?/;?covet?).
4.7 Evaluation
So far as we know, no evaluation in the previous
works was done. This might be due to many rea-
sons: (1) the different scale of experiment (how
many words are in the test data?), (2) the selec-
tion of syntactic category (VV, VN or NN?) of
morphemic components, and (3) the number of
morphemic components involved (two or three-
character words?).. etc. Hence it is difficult to
compare our results to other models. Among the
current similar works, Table 4 shows that our sys-
tem outperforms Chen(2004) in VV compounds,
and approximates the Chen and Chen(2000) in
NN compounds.
5 Conclusion
In this paper, we propose a system that aims to
gain the possible semantic classes of unknown
words via similarity computation based on char-
acter ontology and CILIN thesaurus. In gen-
eral, we approach the task in a hybrid way that
combines the strengths of ontology-based and
example-based model to achieve at better result
for this task.
The scheme we use for automatic semantic
class prediction takes advantage of the presump-
tions that the conceptual information wired in
Chinese characters can help retrieve the near-
62
synonyms, and the near-synonyms constitute a
key indicator for the semantic class guess of un-
known words in question.
The results obtained show that, our SC pre-
diction algorithm can achieve fairly high level of
performance. While the work presented here in
still in progress, a first attempt to analyze a test
set of 800 examples has already shown a 43.60%
correctness for VV compounds, 41.00% for VN
compounds, and 74.50% for NN compounds at
the level-3 of CILIN. If shallow semantics is taken
into consideration, the results are even better.
Working in this framework, however, one point
as suggested by other similar approach is that,
human language processing is not limited to an
abstract ontology alone (Hong et al 2004). In
practical applications, ontologies are seldom used
as the only knowledge resources. For those un-
known words with very weak semantic trans-
parency, it would be interesting to show that an
ontology-based system can be greatly boosted
when other information sources such as metaphor
and etymological information integrated. Fu-
ture work is aimed at improving this accuracy by
adding other linguistic knowledge sources and ex-
tending the technique to WSD (Word Sense Dis-
ambiguation).
Acknowledgements
I would like to thank Erhard Hinrichs and Lothar
Lemnitzer for their useful discussions. I also
thank the anonymous referees for constructive
comments. Thanks also go to the institute of lin-
guistics of Academia Sinica for their kindly data
support.
References
Baayen, Harald. (2001). Word frequency distributions.
Kluwer Academic Publishers.
Chen, Keh-Jiann and Chao-Jan Chen. (2000). Auto-
matic semantic classification for Chinese unknown
compound nouns. COLING 2000, Saarbru?cken,
Germany.
Chen, Chao-Ren. (2004). Character-Sense association
and compounding template similarity: Automatic
semantic classification of Chinese compounds. The
3rd SIGHAN Workshop.
Chu, Yan. (2004). Semantic word formation of Chinese
compound words. Peking University Press.
HanziNet Project: http://www.hanzinet.org.
Guarino, Nicola and Chris Welty. (2002). Evaluating
ontological decisions with OntoClean. In: Commu-
nications of the ACM. 45(2):61-65.
Hong, Li and Huang (2004). Ontology-based Predic-
tion of Compound Relations: A study based on
SUMO. PACLIC 18.
Hsieh, Shu-Kai. (2005). HanziNet: An enriched con-
ceptual network of Chinese characters. The 5rd
workshop on Chinese lexical semantics, China: Xi-
amen.
Lin, Dekang. (1998). A information-theoretic defini-
tion of similarity. In:Proceeding of 15th Interna-
tional Conference of Machine Learning..
Lua, K. T. (1993). A study of Chinese word semantics
and its prediction. Computer Processing of Chinese
and Oriental Languages, Vol 7. No 2.
Lua, K.T. (1997). Prediction of meaning of bisyl-
labic Chinese words using back propagation neural
network. In:Computer Processing of Oriental Lan-
guages. 11(2).
Lua, K. T. (2002). The Semantic Transformation of
Chinese Compound Words (?x?????x<??).
The 3rd workshop on Chinese lexical semantics,
Taipei.
Packard, J. L. (2000). The morphology of Chinese.
Cambridge, UK: Cambridge University Press.
Mei et al(1998). ?2???. Dong-Hua Bookstore:
Taipei.
63
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 69?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Rethinking Chinese Word Segmentation: Tokenization, Character
Classification, or Wordbreak Identification
Chu-Ren Huang
Institute of Linguistics
Academia Sinica,Taiwan
churen@gate.sinica.edu.tw
Petr S?imon
Institute of Linguistics
Academia Sinica,Taiwan
sim@klubko.net
Shu-Kai Hsieh
DoFLAL
NIU, Taiwan
shukai@gmail.com
Laurent Pre?vot
CLLE-ERSS, CNRS
Universite? de Toulouse, France
prevot@univ-tlse2.fr
Abstract
This paper addresses two remaining chal-
lenges in Chinese word segmentation. The
challenge in HLT is to find a robust seg-
mentation method that requires no prior lex-
ical knowledge and no extensive training to
adapt to new types of data. The challenge
in modelling human cognition and acqui-
sition it to segment words efficiently with-
out using knowledge of wordhood. We pro-
pose a radical method of word segmenta-
tion to meet both challenges. The most
critical concept that we introduce is that
Chinese word segmentation is the classifi-
cation of a string of character-boundaries
(CB?s) into either word-boundaries (WB?s)
and non-word-boundaries. In Chinese, CB?s
are delimited and distributed in between two
characters. Hence we can use the distri-
butional properties of CB among the back-
ground character strings to predict which
CB?s are WB?s.
1 Introduction: modeling and theoretical
challenges
The fact that word segmentation remains a main re-
search topic in the field of Chinese language pro-
cessing indicates that there maybe unresolved theo-
retical and processing issues. In terms of processing,
the fact is that none of exiting algorithms is robust
enough to reliably segment unfamiliar types of texts
before fine-tuning with massive training data. It is
true that performance of participating teams have
steadily improved since the first SigHAN Chinese
segmentation bakeoff (Sproat and Emerson, 2004).
Bakeoff 3 in 2006 produced best f-scores at 95%
and higher. However, these can only be achieved af-
ter training with the pre-segmented training dataset.
This is still very far away from real-world applica-
tion where any varieties of Chinese texts must be
successfully segmented without prior training for
HLT applications.
In terms of modeling, all exiting algorithms suffer
from the same dilemma. Word segmentation is sup-
posed to identify word boundaries in a running text,
and words defined by these boundaries are then com-
pared with the mental/electronic lexicon for POS
tagging and meaning assignments. All existing seg-
mentation algorithms, however, presuppose and/or
utilize a large lexical databases (e.g. (Chen and Liu,
1992) and many subsequent works), or uses the po-
sition of characters in a word as the basis for seg-
mentation (Xue, 2003).
In terms of processing model, this is a contradic-
tion since segmentation should be the pre-requisite
of dictionary lookup and should not presuppose lex-
ical information. In terms of cognitive modeling,
such as for acquisition, the model must be able to ac-
count for how words can be successfully segmented
and learned by a child/speaker without formal train-
ing or a priori knowledge of that word. All current
models assume comprehensive lexical knowledge.
2 Previous work
Tokenization model. The classical model, de-
scribed in (Chen and Liu, 1992) and still adopted in
many recent works, considers text segmentation as a
69
tokenization. Segmentation is typically divided into
two stages: dictionary lookup and out of vocabulary
(OOV) word identification. This approach requires
comparing and matching tens of thousands of dic-
tionary entries in addition to guessing thousands of
OOV words. That is, this is a 104x104 scale map-
ping problem with unavoidable data sparseness.
More precisely the task consist in finding
all sequences of characters Ci, . . . , Cn such that
[Ci, . . . Cn] either matches an entry in the lexicon
or is guessed to be so by an unknown word resolu-
tion algorithm. One typical kind of the complexity
this model faces is the overlapping ambiguity where
e.g. a string [Ci ? 1, Ci, Ci + 1] contains multiple
substrings, such as [Ci ? 1, Ci, ] and [Ci,Ci + 1],
which are entries in the dictionary. The degree of
such ambiguities is estimated to fall between 5% to
20% (Chiang et al, 1996; Meng and Ip, 1999).
2.1 Character classification model
A popular recent innovation addresses the scale
and sparseness problem by modeling segmentation
as character classification (Xue, 2003; Gao et al,
2004). This approach observes that by classifying
characters as word-initial, word-final, penultimate,
etc., word segmentation can be reduced to a simple
classification problem which involves about 6,000
characters and around 10 positional classes. Hence
the complexity is reduced and the data sparseness
problem resolved. It is not surprising then that the
character classification approach consistently yields
better results than the tokenization approach. This
approach, however, still leaves two fundamental
questions unanswered. In terms of modeling, us-
ing character classification to predict segmentation
not only increases the complexity but also necessar-
ily creates a lower ceiling of performance In terms
of language use, actual distribution of characters is
affected by various factors involving linguistic vari-
ation, such as topic, genre, region, etc. Hence the
robustness of the character classification approach
is restricted.
The character classification model typically clas-
sifies all characters present in a string into at least
three classes: word Initial, Middle or Final po-
sitions, with possible additional classification for
word-middle characters. Word boundaries are in-
ferred based on the character classes of ?Initial? or
?Final?.
This method typically yields better result than the
tokenization model. For instance, Huang and Zhao
(2006) claims to have a f-score of around 97% for
various SIGHAN bakeoff tasks.
3 A radical model
We propose a radical model that returns to the
core issue of word segmentation in Chinese. Cru-
cially, we no longer pre-suppose any lexical knowl-
edge. Any unsegmented text is viewed as a string
of character-breaks (CB?s) which are evenly dis-
tributed and delimited by characters. The characters
are not considered as components of words, instead,
they are contextual background providing informa-
tion about the likelihood of whether each CB is also
a wordbreak (WB). In other words, we model Chi-
nese word segmentation as wordbreak (WB) iden-
tification which takes all CB?s as candidates and
returns a subset which also serves as wordbreaks.
More crucially, this model can be trained efficiently
with a small corpus marked with wordbreaks and
does not require any lexical database.
3.1 General idea
Any Chinese text is envisioned as se-
quence of characters and character-boundaries
CB0C1CB1C2 . . . CBi?1CiCBi . . . CBn?1CnCBn The
segmentation task is reduced to finding all CBs
which are also wordbreaks WB.
3.2 Modeling character-based information
Since CBs are all the same and do not carry any
information, we have to rely on their distribution
among different characters to obtain useful infor-
mation for modeling. In a segmented corpus, each
WB can be differentiated from a non-WB CB by the
character string before and after it. We can assume
a reduced model where either one character imme-
diately before and after a CB is considered or two
characters (bigram). These options correspond to
consider (i) only word-initial and word-final posi-
tions (hereafter the 2-CB-model or 2CBM) or (ii) to
add second and penultimate positions (hereafter the
4-CB-model or 4CBM). All these positions are well-
attested as morphologically significant.
70
3.3 The nature of segmentation
It is important to note that in this approaches,
although characters are recognized, unlike (Xue,
2003) and Huang et al (2006), charactes simply
are in the background. That is, they are the neces-
sary delimiter, which allows us to look at the string
of CB?s and obtaining distributional information of
them.
4 Implementation and experiments
In this section we slightly change our notation to
allow for more precise explanation. As noted be-
fore, Chinese text can be formalized as a sequence
of characters and intervals as illustrated in we call
this representation an interval form.
c1I1c2I2 . . . cn?1In?1cn.
In such a representation, each interval Ik is either
classified as a plain character boundary (CB) or as
a word boundary (WB).
We represent the neighborhood of the character
ci as (ci?2, Ii?2, ci?1, Ii?1, ci, Ii, ci+1, Ii+1), which
we can be simplified as (I?2, I?1, ci, I+1, I+2) by
removing all the neighboring characters and retain-
ing only the intervals.
4.1 Data collection models
This section makes use of the notation introduced
above for presenting several models accounting for
character-interval class co-occurrence.
Word based model. In this model, statistical data
about word boundary frequencies for each character
is retrieved word-wise. For example, in the case of
a monosyllabic word only two word boundaries are
considered: one before and one after the character
that constitutes the monosyllabic word in question.
The method consists in mapping all the Chinese
characters available in the training corpus to a vector
of word boundary frequencies. These frequencies
are normalized by the total frequency of the char-
acter in a corpus and thus represent probability of a
word boundary occurring at a specified position with
regard to the character.
Let us consider for example, a tri-syllabic word
W = c1c2c3, that can be rewritten as the following
interval form as W I = IB?1c1I
N
1 c2I
N
2 c3I
B
3 .
In this interval form, each interval Ik is marked
as word boundary B or N for intervals within words.
When we consider a particular character c1 in W ,
there is a word boundary at index?1 and 3. We store
this information in a mapping c1 = {?1 : 1, 3 : 1}.
For each occurrence of this character in the corpus,
we modify the character vector accordingly, each
WB corresponding to an increment of the relevant
position in the vector. Every character in every word
of the corpus in processed in a similar way.
Obviously, each character yields only information
about positions of word boundaries of a word this
particular character belongs to. This means that the
index I?1 and I3 are not necessarily incremented
everytime (e.g. for monosyllabic and bi-syllabic
words)
Sliding window model. This model does not op-
erate on words, but within a window of a give size
(span) sliding through the corpus. We have exper-
imented this method with a window of size 4. Let
us consider a string, s = ?c1c2c3c4? which is not
necessarily a word and is rewritten into an interval
form as sI = ?c1I1c2I2c3I3c4I4?. We store the
co-occurrence character/word boundaries informa-
tion in a fixed size (span) vector.
For example, we collect the information for
character c3 and thus arrive at a vector c3 =
(I1, I2, I3, I4), where 1 is incremented at the respec-
tive position ifIk = WB, zero otherwise.
This model provides slightly different informa-
tion that the previous one. For example, if
a sequence of four characters is segmented as
c1IN1 c2I
B
2 c3I
B
3 c4I
B
4 (a sequence of one bi-syllabic
and two monosyllabic words), for c3 we would also
get probability of I4, i.e. an interval with index +2
. In other words, this model enables to learn WB
probability across words.
4.2 Training corpus
In the next step, we convert our training corpus into
a corpus of interval vectors of specified dimension.
Let?s assume we are using dimension span = 4.
Each value in such a vector represents the proba-
bility of this interval to be a word boundary. This
probability is assigned by character for each position
with regard to the interval. For example, we have
segmented corpus C = c1I1c2I2 . . . cn?1In?1cn,
where each Ik is labeled as B for word boundary
or N for non-boundary.
71
In the second step, we move our 4-sized window
through the corpus and for each interval we query
a character at the corresponding position from the
interval to retrieve the word boundary occurrence
probability. This procedure provides us with a vec-
tor of 4 probability values for each interval. Since
we are creating this training corpus from an already
segmented text, a class (B or N ) is assigned to each
interval.
The testing corpus (unsegmented) is encoded in a
similar way, but does not contain the class labels B
and N .
Finally, we automatically assign probability of 0.5
for unseen events.
4.3 Predicting word boundary with a classifier
The Sinica corpus contains 6820 types of characters
(including Chinese characters, numbers, punctua-
tion, Latin alphabet, etc.). When the Sinica corpus is
converted into our interval vector corpus, it provides
14.4 million labeled interval vectors. In this first
study we have implement a baseline model, without
any pre-processing of punctuation, numbers, names.
A decision tree classifier (Ruggieri, 2004) has
been adopted to overcome the non-linearity issue.
The classifier was trained on the whole Sinica cor-
pus, i.e. on 14.4 million interval vectors. Due to
space limit, actual bakeoff experiment result will be
reported in our poster presentation.
Our best results is based on the sliding window
model, which provides better results. It has to be
emphasized that the test corpora were not processed
in any way, i.e. our method is sufficiently robust to
account for a large number of ambiguities like nu-
merals, foreign words.
5 Conclusion
In this paper, we presented a radical and robust
model of Chinese segmentation which is supported
by initial experiment results. The model does not
pre-suppose any lexical information and it treats
character strings as context which provides infor-
mation on the possible classification of character-
breaks as word-breaks. We are confident that once
a standard model of pre-segmentation, using tex-
tual encoding information to identify WB?s which
involves non-Chinese characters, will enable us to
achieve even better results. In addition, we are look-
ing at other alternative formalisms and tools to im-
plement this model to achieve the optimal results.
Other possible extensions including experiments to
simulate acquisition of wordhood knowledge to pro-
vide support of cognitive modeling, similar to the
simulation work on categorization in Chinese by
(Redington et al, 1995). Last, but not the least,
we will explore the possibility of implementing a
sharable tool for robust segmentation for all Chinese
texts without training.
References
Academia Sinica Balanced Corpus of Modern Chinese.
http://www.sinica.edu.tw/SinicaCorpus/
Chen K.J and Liu S.H. 1992. Word Identification for
Mandarin Chinese sentences. Proceedings of the 14th
conference on Computational Linguistics, p.101-107,
France.
Chiang,T.-H., J.-S. Chang, M.-Y. Lin and K.-Y. Su. 1996.
Statistical Word Segmentation. In C.-R. Huang, K.-J.
Chen and B.K. T?sou (eds.): Journal of Chinese Lin-
guistics, Monograph Series, Number 9, Readings in
Chinese Natural Language Processing, pp. 147-173.
Gao, J. and A. Wu and Mu Li and C.-N.Huang and H. Li
and X. Xia and H. Qin. 2004. Adaptive Chinese Word
Segmentation. In Proceedings of ACL-2004.
Meng, H. and C. W. Ip. 1999. An Analytical Study of
Transformational Tagging for Chinese Text. In. Pro-
ceedings of ROCLING XII. 101-122. Taipei
Ruggieri S. 2004. YaDT: Yet another Decision Tree
builder. Proceedings of the 16th International Con-
ference on Tools with Artificial Intelligence (ICTAI
2004): 260-265. IEEE Press, November 2004.
Richard Sproat and Thomas Emerson. 2003. The
First International Chinese Word Segmentation Bake-
off. Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, Sapporo, Japan, July
2003.
Xue, N. 2003. Chinese Word Segmentation as Charac-
ter Tagging. Computational Linguistics and Chinese
Language Processing. 8(1): 29-48
Redington, M. and N. Chater and C. Huang and L. Chang
and K. Chen. 1995. The Universality of Simple Dis-
tributional Methods: Identifying Syntactic Categories
in Mandarin Chinese. Presented at the Proceedings of
the International Conference on Cognitive Science and
Natural Language Processing. Dublin City University.
72
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 153?156,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Discovery of Named Entity Variants
? Grammar-driven Approaches to Non-alphabetical Transliterations
Chu-Ren Huang
Institute of Linguistics
Academia Sinica, Taiwan
churenhuang@gmail.com
Petr S?imon
Institute of Linguistics
Academia Sinica, Taiwan
sim@klubko.net
Shu-Kai Hsieh
DoFLAL
NIU, Taiwan
shukai@gmail.com
Abstract
Identification of transliterated names is a
particularly difficult task of Named Entity
Recognition (NER), especially in the Chi-
nese context. Of all possible variations of
transliterated named entities, the difference
between PRC and Taiwan is the most preva-
lent and most challenging. In this paper, we
introduce a novel approach to the automatic
extraction of diverging transliterations of
foreign named entities by bootstrapping co-
occurrence statistics from tagged and seg-
mented Chinese corpus. Preliminary experi-
ment yields promising results and shows its
potential in NLP applications.
1 Introduction
Named Entity Recognition (NER) is one of the most
difficult problems in NLP and Document Under-
standing. In the field of Chinese NER, several
approaches have been proposed to recognize per-
sonal names, date/time expressions, monetary and
percentage expressions. However, the discovery of
transliteration variations has not been well-studied
in Chinese NER. This is perhaps due to the fact
that the transliteration forms in a non-alphabetic lan-
guage such as Chinese are opaque and not easy to
compare. On the hand, there is often more than
one way to transliterate a foreign name. On the
other hand, dialectal difference as well as differ-
ent transliteration strategies often lead to the same
named entity to be transliterated differently in dif-
ferent Chinese speaking communities.
Corpus Example (Clinton) Frequency
XIN ??? 24382
CNA ??? 150
XIN ??? 0
CNA ??? 120842
Table 1: Distribution of two transliteration variants
for ?Clinton? in two sub-corpora
Of all possible variations, the cross-strait differ-
ence between PRC and Taiwan is the most prevalent
and most challenging.1The main reason may lie in
the lack of suitable corpus.
Even given some subcorpora of PRC and Taiwan
variants of Chinese, a simple contrastive approach is
still not possible. It is because: (1) some variants
might overlap and (2) there are more variants used
in each corpus due to citations or borrowing cross-
strait. Table 1 illustrates this phenomenon, where
CNA stands for Central News Agency in Taiwan,
XIN stands for Xinhua News Agency in PRC, re-
spectively.
With the availability of Chinese Gigaword Cor-
pus (CGC) and Word Sketch Engine (WSE) Tools
(Kilgarriff, 2004). We propose a novel approach
towards discovery of transliteration variants by uti-
lizing a full range of grammatical information aug-
mented with phonological analysis.
Existing literatures on processing of translitera-
tion concentrate on the identification of either the
transliterated term or the original term, given knowl-
edge of the other (e.g. (Virga and Khudanpur,
1For instance, we found at least 14 transliteration variants
for Lewinsky,such as ????????????????????????
?????????????????????????????????????
?????? and so on.
153
2003)). These studies are typically either rule-based
or statistics-based, and specific to a language pair
with a fixed direction (e.g. (Wan and Verspoor,
1998; Jiang et al, 2007)). To the best of our knowl-
edge, ours is the first attempt to discover transliter-
ated NE?s without assuming prior knowledge of the
entities. In particular, we propose that transliteration
variants can be discovered by extracting and com-
paring terms from similar linguistic context based
on CGC and WSE tools. This proposal has great po-
tential of increasing robustness of future NER work
by enabling discovery of new and unknown translit-
erated NE?s.
Our study shows that resolution of transliterated
NE variations can be fully automated. This will have
strong and positive implications for cross-lingual
and multi-lingual informational retrieval.
2 Bootstrapping transliteration pairs
The current study is based on Chinese Gigaword
Corpus (CGC) (Graff el al., 2005), a large corpus
contains with 1.1 billion Chinese characters contain-
ing data from Central News Agency of Taiwan (ca.
700 million characters), Xinhua News Agency of
PRC (ca. 400 million characters). These two sub-
corpora represent news dispatches from roughly the
same period of time, i.e. 1990-2002. Hence the two
sub-corpora can be expected to have reasonably par-
allel contents for comparative studies.2
The premises of our proposal are that transliter-
ated NE?s are likely to collocate with other translit-
erated NE?s, and that collocates of a pair of translit-
eration variants may form contrasting pairs and are
potential variants. In particular, since the transliter-
ation variations that we are interested in are those
between PRC and Taiwan Mandarin, we will start
with known contrasting pairs of these two language
variants and mine potential variant pairs from their
collocates. These potential variant pairs are then
checked for their phonological similarity to deter-
mine whether they are true variants or not. In order
to effectively select collocates from specific gram-
matical constructions, the Chinese Word Sketch3 is
adopted. In particular, we use the Word Sketch dif-
2To facilitate processing, the complete CGC was segmented
and POS tagged using the Academia Sinica segmentation and
tagging system (Ma and Huang, 2006).
3http://wordsketch.ling.sinica.edu.tw
ference (WSDiff) function to pick the grammatical
contexts as well as contrasting pairs. It is important
to bear in mind that Chinese texts are composed of
Chinese characters, hence it is impossible to com-
pare a transliterated NE with the alphabetical form
in its original language. The following characteris-
tics of a transliterated NE?s in CGC are exploited to
allow discovery of transliteration variations without
referring to original NE.
? frequent co-occurrence of named entities
within certain syntagmatic relations ? named
entities frequently co-occur in relations such as
AND or OR and this fact can be used to collect
and score mutual predictability.
? foreign named entities are typically transliter-
ated phonetically ? transliterations of the same
name entity using different characters can be
matched by using simple heuristics to map their
phonological value.
? presence and co-occurrence of named entities
in a text is dependent on a text type ? journalis-
tic style cumulates many foreign named entities
in close relations.
? many entities will occur in different domains
? famous person can be mentioned together
with someone from politician, musician, artist
or athlete. Thus allows us to make leaps from
one domain to another.
There are, however, several problems with the
phonological representation of foreign named enti-
ties in Chinese. Due to the nature of Chinese script,
NE transliterations can be realized very differently.
The following is a summary of several problems that
have to be taken into account:
? word ending: ??? vs.???? ?Arafat? or ?
?? vs.???? ?Mubarak?. The final conso-
nant is not always transliterated. XIN translit-
erations tend to try to represent all phonemes
and often add vowels to a final consonant to
form a new syllable, whereas CNA transliter-
ation tends to be shorter and may simply leave
out a final consonant.
? gender dependent choice of characters: ???
?Leslie? vs.??? ?Chris? or ???? vs. ???
154
?. Some occidental names are gender neutral.
However, the choice of characters in a personal
name in Chinese is often gender sensitive. So
these names are likely to be transliterated dif-
ferently depending on the gender of its referent.
? divergent representations caused by scope of
transliteration, e.g. both given and surname
vs. only surname: ???? / ????? ?Venus
Williams?.
? difference in phonological interpretation: ??
? vs. ??? ?Rafter? or??? vs. ??? ?Connors?.
? native vs. non-native pronunciation: ??? ?
vs. ???? ?Escudero? or ??? vs. ???
?Federer?.
2.1 Data collection
All data were collected from Chinese Gigaword Cor-
pus using Chinese Sketch Engine with WSDiff
function, which provides side-by-side syntagmatic
comparison of Word Sketches for two different
words. WSDiff query for wi and wj returns pat-
terns that are common for both words and also pat-
terns that are particular for each of them. Three data
sets are thus provided. We neglect the common pat-
terns set and concentrate only on the wordlists spe-
cific for each word.
2.2 Pairs extraction
Transliteration pairs are extracted from the two sets,
A and B, collected with WSDiff using default set
of seed pairs :
- for each seed pair in seeds retrieve WSDiff for
and/or relation, thus have pairs of word lists,
< Ai, Bi >
- for each word wii ? Ai find best matching
counterpart(s) wij ? Bi. Comparison is done
using simple phonological rules, viz. 2.3
- use newly extracted pairs as new seeds (original
seeds are stored as good pairs and not queried
any more)
- loop until there are no new pairs
Notice that even though substantial proportion of
borrowing among different communities, there is no
mixing in the local context of collocation, which
means, local collocation could be the most reliable
way to detect language variants with known variants.
2.3 Phonological comparison
All word forms are converted from Chinese script
into a phonological representation4 during the pairs
extraction phase and then these representations are
compared and similarity scores are given to all pair
candidates.
A lot of Chinese characters have multiple pro-
nunciations and thus multiple representations are de-
rived. In case of multiple pronunciations for certain
syllable, this syllable is commpared to its counter-
part from the other set. E.g. (? has three pronunci-
ations: ye`, xie?, she`. When comparing syllables such
as ?[pei,fei] and ?[fei], ? will be represented as
[fei]. In case of pairs such as ??? [ye er qin] and
??? [ye er qin], which have syllables with multi-
ple pronunciations and this multiple representations.
However, since these two potential variants share
the first two characters (out of three), they are con-
sidered as variants without superfluous phonological
checking.
Phonological representations of whole words are
then compared by Levenstein algorithm, which is
widely used to measure the similarity between two
strings. First, each syllable is split into initial and
final components: gao:g+ao. In case of syllables
without initials like er, an ? is inserted before the
syllable, thus er:?+er.
Before we ran the Levenstein measure, we also
apply phonological corrections on each pair of can-
didate representations. Rules used for these cor-
rections are derived from phonological features of
Mandarin Chinese and extended with few rules
from observation of the data: (1) For Initials, (a):
voiced/voiceless stop contrasts are considered as
similar for initials: g:k, e.g. ? [gao] (??) vs. ?
[ke] (??),d:t, b:p, (b): r:l ? [rui] (????) ? [lie]
(????) is added to distinctive feature set based on
observation. (2). For Finals, (a): pair ei:ui is eval-
uated as equivalent.5 (b): oppositions of nasalised
final is evaluated as dissimilar.
4http://unicode.org/charts/unihan.html
5Pinyin representation of phonology of Mandarin Chinese
does not follow the phonological reality exactly: [ui] = [uei]
etc.
155
2.4 Extraction algorithm
Our algorithm will potentially exhaust the whole
corpus, i.e. find most of the named entities that oc-
cur with at least few other names entities, but only
if seeds are chosen wisely and cover different do-
mains6. However, some domains might not over-
lap at all, that is, members of those domains never
appear in the corpus in relation and/or. And con-
currence of members within some domains might be
sparser than in other, e.g. politicians tend to be men-
tioned together more often than novelists. Nature of
the corpus also plays important role. It is likely to
retrieve more and/or related names from journal-
istic style. This is one of the reasons why we chose
Chinese Gigaword Corpus for this task.
3 Experiment and evaluation
We have tested our method on the Chinese Giga-
word Second Edition corpus with 11 manually se-
lected seeds Apart from the selection of the starter
seeds, the whole process is fully automatic. For this
task we have collected data from syntagmatic rela-
tion and/or, which contains words co-occurring
frequently with our seed words. When we make a
query for peoples names, it is expected that most of
the retrieved items will also be names, perhaps also
names of locations, organizations etc.
The whole experiment took 505 iterations in
which 494 pairs were extracted.
Our complete experiment with 11 pre-selected
transliteration pairs as seed took 505 iterations to
end. The iterations identified 494 effective transliter-
ation variant pairs (i.e. those which were not among
the seeds or pairs identified by earlier iteration.) All
the 494 candidate pairs were manually evaluated 445
of them are found to be actual contrast pairs, a pre-
cision of 90.01%. In addition, the number of new
transliteration pairs yielded is 4,045%, a very pro-
ductive yield for NE discovery.
Preliminary results show that this approach is
competitive against other approaches reported in
previous studies. Performances of our algorithms is
calculated in terms of precision rate with 90.01%.
6The term domain refers to politics,music,sport, film etc.
4 Conclusion and Future work
In this paper, we have shown that it is possible to
identify NE?s without having prior knowledge of
them. We also showed that, applying WSE to re-
strict grammatical context and saliency of colloca-
tion, we are able to effectively extract transliteration
variants in a language where transliteration is not
explicitly represented. We also show that a small
set of seeds is all it needs for the proposed method
to identify hundreds of transliteration variants. This
proposed method has important applications in in-
formation retrieval and data mining in Chinese data.
In the future, we will be experimenting with a dif-
ferent set of seeds in a different domain to test the
robustness of this approach, as well as to discover
transliteration variants in our fields. We will also be
focusing on more refined phonological analysis. In
addition, we would like to explore the possibility of
extending this proposal to other language pairs.
References
Jiang, L. and M.Zhou and L.f. Chien. 2007. Named En-
tity Discovery based on Transliteration and WWW [In
Chinese]. Journal of the Chinese Information Process-
ing Society. 2007 no.1. pp.23-29.
Graff, David et al 2005. Chinese Gigaword Second Edi-
tion. Linguistic Data Consortium, Philadelphia.
Ma, Wei-Yun and Huang, Chu-Ren. 2006. Uniform and
Effective Tagging of a Heterogeneous Giga-word Cor-
pus. Presented at the 5th International Conference on
Language Resources and Evaluation (LREC2006), 24-
28 May. Genoa, Italy.
Kilgarriff, Adam et al 2004. The Sketch Engine. Pro-
ceedings of EURALEX 2004. Lorient, France.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. of the ACL Workshop on Multi-
lingual Named Entity Recognition, pp.57-64.
Wan, Stephen and Cornelia Verspoor. 1998. Auto-
matic English-Chinese Name Transliteration for De-
velopment of Multiple Resources. In Proc. of COL-
ING/ACL, pp.1352-1356.
156
Constructing Taxonomy of Numerative Classifiers for Asian Languages
Kiyoaki Shirai
JAIST
kshirai@jaist.ac.jp
Takenobu Tokunaga
Tokyo Inst. of Tech.
take@cl.cs.titech.ac.jp
Chu-Ren Huang
Academia Sinica
churenhuang@gmail.com
Shu-Kai Hsieh
National Taiwan Normal Univ.
shukai@gmail.com
Tzu-Yi Kuo
Academia Sinica
ivykuo@gate.sinica.edu.tw
Virach Sornlertlamvanich
TCL, NICT
virach@tcllab.org
Thatsanee Charoenporn
TCL, NICT
thatsanee@tcllab.org
Abstract
Numerative classifiers are ubiquitous in
many Asian languages. This paper pro-
poses a method to construct a taxonomy
of numerative classifiers based on a noun-
classifier agreement database. The taxon-
omy defines superordinate-subordinate rela-
tion among numerative classifiers and rep-
resents the relations in tree structures. The
experiments to construct taxonomies were
conducted for evaluation by using data from
three different languages: Chinese, Japanese
and Thai. We found that our method was
promising for Chinese and Japanese, but in-
appropriate for Thai. It confirms that there
really is no hierarchy among Thai classifiers.
1 Introduction
Many Asian languages do not mark grammatical
numbers (singular/plural) in noun form, but use nu-
merative classifiers together with numerals instead
when describing the number of nouns. Numerative
classifiers (hereafter ?classifiers?) are used with a
limited group of nouns, in particular material nouns.
In English, for example: ?three pieces of paper?. In
Asian languages these classifiers are ubiquitous and
used with common nouns. Therefore the number of
classifiers is much larger than in Western languages.
An agreement between nouns and classifiers is also
necessary, i.e., a certain noun specifies possible clas-
sifiers. The agreement is determined based on var-
ious aspects of a noun, such as its meaning, shape,
pragmatic aspect and so on.
This paper proposes a method to automati-
cally construct a taxonomy of numerative classi-
fiers for Asian languages. The taxonomy defines
superordinate-subordinate relations between classi-
fiers. For instance, the Japanese classifier ?? (to?)?
is used for counting big animals such as elephants
and tigers, while ?? (hiki)? is used for all animals.
Since ??? can be considered more general than ?
??, ??? is the superordinate classifier of ???, rep-
resented as ???  ??? in this paper. The taxon-
omy represents such superordinate-subordinate rela-
tions between classifiers in the form of a tree struc-
ture. A taxonomy of classifiers would be fundamen-
tal knowledge for natural language processing. In
addition, it will be useful for language learners, be-
cause learning usage of classifiers is rather difficult,
especially for Western language speakers.
We evaluate the proposed method by using the
data of three Asian languages: Chinese, Japanese
and Thai.
2 Noun-classifier agreement database
First, let us introduce usages of classifiers in Asian
languages. In the following examples, ?CL? stands
for classifier.
? Chinese: yi-ju
(CL)
dian-hua
(telephone)
? ? ? a telephone
? Japanese: inu
(dog)
2 hiki
(CL)
? ? ? 2 dogs
? Thai: nakrian
(student)
3 khon
(CL)
? ? ? 3 students
397
As mentioned earlier, the agreement between nouns
and classifiers is observed. For instance, the
Japanese classifier ?hiki? in the above example
agrees with only animals. The agreement is also
found in Chinese and Thai.
The proposed method to construct a classifier tax-
onomy is based on agreement between nouns and
classifiers. First we prepare a collection of pairs
(n, c) of a noun n and a classifier c which agrees
with n for a language. The statistics of our Chinese,
Japanese, and Thai database are summarized in Ta-
ble 1.
Table 1: Noun-classifier agreement database
Chinese Japanese Thai
No. of (n,c) pairs 28,202 9,582 9,618
No. of nouns (type) 10,250 4,624 8,224
No. of CLs (type) 205 331 608
The Japanese database was built by extracting
noun-classifier pairs from a dictionary (Iida, 2004)
which enumerates nouns and their corresponding
classifiers. The Chinese database was derived from
a dictionary (Huang et al, 1997). The Thai database
consists of a mixture of two kinds of noun-classifier
pairs: 8,024 nouns and their corresponding classi-
fiers from a dictionary of a machine translation sys-
tem (CICC, 1995) and 200 from a corpus. The pairs
from the corpus were manually checked for their va-
lidity.
3 Proposed Method
3.1 Extracting superordinate-subordinate
relations of classifiers
We extracted superordinate-subordinate classifier
pairs based on inclusive relations of sets of nouns
agreeing with those classifiers. Suppose that Nk is
a set of nouns that agrees with a classifier ck. If Ni
subsumes Nj (Ni ? Nj), we can estimate that ci
subsumes cj (ci  cj). For instance, in our Japanese
database, the classifier ?? (ten)? agrees with shops
such as ?drug store?, ?kiosk? and ?restaurant?, and
these nouns also agree with ?? (ken)?, since ??? is
a classifier which agrees with any kind of building.
Thus, we can estimate the relation ???  ???.
Given a certain classifier cj , ci satisfying the fol-
lowing two conditions (1) and (2) is considered as a
N
j
N
i
Figure 1: Relation of sets of nouns agreeing with
classifiers
superordinate classifier of cj .
|Ni| > |Nj | (1)
IR(ci, cj) ? Tir
where IR(ci, cj)
def
=
|N
i
?N
j
|
|N
j
|
(2)
Condition (1) requires that a superordinate classifier
agrees with more nouns than a subordinate classifier.
IR(ci, cj) is an inclusion ratio representing to what
extent nouns in Nj are also included in Ni (the ratio
of the light gray area to the area of the small circle
in Figure 1).
Condition (2) means that if IR(ci, cj) is greater
than a certain threshold T
ir
, we estimate a
superordinate-subordinate relation between ci and
cj . The basic idea is that superordinate-subordinate
relations are extracted when Nj is a proper subset
of Ni, i.e. IR(ci, cj) = 1, but this is too strict. In
order to extract more relations, we loosen this condi-
tion such that relations are extracted when IR(ci, cj)
is large enough. If we set Tir lower, more relations
can be acquired, but they may be less reliable.
Table 2: Extraction of superordinate-subordinate re-
lations
Chinese Japanese Thai
T
ir
0.7 0.6 0.6
No. of extracted relations 251 322 239
No. of CLs not in 36 76 395
the extracted relations (18%) (23%) (61%)
Table 2 shows the results of our experiments to
extract superordinate-subordinate relations of classi-
fiers. The threshold T
ir
was determined in an ad hoc
manner for each language. The numbers of extracted
superordinate-subordinate relations are shown in the
second row in the table. Manual inspection of the
sampled relations revealed that many reasonable re-
lations were extracted. The objective evaluation of
these extracted relations will be discussed in 4.2.
398
The third row in Table 2 indicates the numbers of
classifiers which were not included in the extracted
superordinate-subordinate relations with its ratio to
the total number of classifiers in the database in
parentheses. We found that no relation is extracted
for a large number of Thai classifiers.
3.2 Constructing structure
The structure of a taxonomy is constructed based
on a set of superordinate-subordinate relations be-
tween classifiers. Currently we adopt a very naive
approach to construct structures, i.e., starting from
the most superordinate classifiers as roots, we ex-
tend trees downward to less general classifiers by
using the extracted superordinate-subordinate rela-
tions. Note that since there is more than one classi-
fier that does not have any superordinate classifiers,
we will have a set of trees rather than a single tree.
When constructing structures, redundant relations
are ignored in order to make the structures as concise
as possible. A relation is considered redundant if the
relation can be inferred by using other relations and
transitivity of the relations. The formal definition of
redundant relations is given below:
ca  cb is redundant iff ?cm : ca  cm, cm  cb
Statistics of constructed structures for each lan-
guage are shown in Table 3. More than 50 iso-
lated structures (trees) were obtained for Chinese
and Japanese, while more than 100 for Thai. We ob-
tained several large structures, the largest containing
45, 85 and 23 classifiers for Chinese, Japanese and
Thai, respectively. As indicated in the fifth row in
Table 3, however, many structures consisting of only
2 classifiers were also constructed.
Table 3: Construction of structures
Chinese Japanese Thai
No. of structures 52 54 102
No. of CLs in a structure
Average 4.9 6.3 3.3
Maximum 45 85 23
Max. depth of structures 4 3 3
No. of structures with 2 CLs 18 24 54
4 Discussion
In this section, we will discuss the results of our
experiments. First 4.1 discusses appropriateness of
our method for the three languages. Then we eval-
uate our method in more detail. The evaluation of
extracted superordinate-subordinate relations is de-
scribed in 4.2, and the evaluation of structures in 4.3.
4.1 Comparison of different languages
According to the results of our experiments, the
proposed method seems promising for Chinese and
Japanese, but not for Thai. From the Thai data,
no relation was obtained for about 60% of classi-
fiers (Table 2), and many small fragmented struc-
tures were created (Table 3).
This is because of the characteristic that nouns
and classifiers are strongly coupled in Thai, i.e.,
many classifiers agree with only one noun. In our
Thai database, 252 (41.5%) classifiers agree with
only one noun. This means that the overlap between
two noun sets Ni and Nj can be quite small, making
the inclusion ratio IR(ci, cj) very small. Out basic
idea is that we can extract superordinate-subordinate
relations between two classifiers when the overlap of
their corresponding noun sets is large. However, this
assumption does not hold in Thai classifiers. The
above facts suggest that there seems to be no hierar-
chical taxonomy of classifiers in Thai.
4.2 Evaluation of extracted relations
4.2.1 Analysis of Nouns in Nj \ Ni
As explained in 3.1, our method extracts a relation
ci  cj even when Ni does not completely subsume
Nj . We analysed nouns in the relative complement
of Ni in Nj (Nj \Ni), i.e., the dark gray area in Fig-
ure 1. The relation ci  cj implies that all nouns
which are countable with a subordinate classifier cj
are also countable with its superordinate classifier ci,
but there is no guarantee of this for nouns in Nj \Ni,
since we loosened the condition as in (2) by intro-
ducing a threshold.
To see to what extent nouns in Nj \ Ni agree
with ci as well, we manually verified the agreement
of nouns in Nj \ Ni and ci for all extracted rela-
tions ci  cj . The verification was done by native
speakers of each language. Results of the valida-
tion are summarized in Table 4. For Japanese and
Chinese, multiple judges verified the results. When
judgments conflicted, we decided the final decision
by a discussion of two judges for Japanese, and by
majority voting for Chinese. The 4th and 5th rows
399
in Table 4 show the agreement of judgments. The
?Agreement ratio? is the ratio of cases that judg-
ments agree. Since three judges verified nouns for
Chinese, we show the average of the agreement ra-
tios for two judges out of the three. The agreement
ratio and Cohen?s ? is relatively high for Japanese,
but not for Chinese. We found many uncertain cases
for Chinese nouns. For example, ?? (wei)? is a clas-
sifier used when counting people with honorific per-
spective. However, judgement if ??? can modify
nouns such as ?political prisoner? or ?local villain?
is rather uncertain.
Table 4: Analysis of nouns in Nj \ Ni
Chinese Japanese Thai
No. of nouns in N
j
\N
i
1,650 579 43
No. of nouns countable 1,195 241 24
with c
i
as well 72% 42% 56%
No. of judges 3 2 1
Agreement ratio 0.677 0.936 ?
Cohen?s ? 0.484 0.868 ?
Table 4 reveals that a considerable number of
nouns in Nj \ Ni are actually countable with ci,
meaning that our databases do not include noun-
classifier agreement exhaustively.
4.2.2 Reliability of relations ??
Based on the analysis in 4.2.1, we evaluate ex-
tracted superordinate-subordinate relations. We de-
fine the reliability R of the relation ci  cj as
R(ci  cj) =
|Ni ? Nj |+ |NCj,i|
|Nj |
, (3)
where, NCj,i is a subset of Nj \ Ni consisting of
nouns which are manually judged to agree with ci.
We can consider that the more strictly this statement
holds, the more reliable the extracted relations will
be.
Figure 2 shows the relations between the thresh-
old T
ir
and both the number of extracted relations
and their reliability. The horizontal axis indicates
the threshold T
ir
in (2). The bar charts indicate the
number of extracted relations, while the line graphs
indicate the averages of reliability of all extracted re-
lations. Of course, if we set T
ir
lower, we can extract
more relations at the cost of their reliability. How-
ever, even when T
ir
is set to the lowest value, the
averages of reliability are relatively high, i.e. 0.98
(Chinese), 0.91 (Japanese) and 0.99 (Thai). Thus
we can conclude that the extracted superordinate-
subordinate relations are reliable enough.
4.3 Evaluation of structures
As in ordinary ontologies, we will assume that prop-
erties of superordinate classifiers can be inherited to
their subordinate classifiers. In other words, a clas-
sifier taxonomy suggests transitivity of agreement
with nouns over superordinate-subordinate relations
as
c
1
 c
2
? c
2
 c
3
? c
1
 c
3
.
In order to evaluate the structures of our taxonomy,
we verify the validity of transitivity.
First, we extracted all pairs of classifiers having
an ancestor-descendant relation from our classifier
taxonomy. Hereafter we denote ancestor-descendant
pairs of classifiers as (ca, cd), where ca is an ances-
tor and cd an descendant. The path from ca to cd on
the taxonomy can be represented as
c
0
(= ca)  c1  ...  cn(= cd). (4)
We denote a superordinate-subordinate relation de-
rived by transitivity as
?
, such as c
0
?
 cn. Among
all ancestor-descendant relations, we extracted ones
with a path length of more than one, or n > 1
in (4). Then we compare R(ca
?
 cd), the re-
liability of a relation derived by transitivity, with
R(ci  ci+1) (0 ? i < n), the reliability of di-
rect relations in the path from ca to cd. If these are
comparable, we can conclude that transitivity in the
taxonomy is valid.
Table 5 shows the results of the analysis of transi-
tivity. As indicated in the column ?all? in Table 5, 78
and 86 ancestor-descendant pairs (ca, cd) were ex-
tracted from the Chinese and Japanese classifier tax-
onomy, respectively. In contrast, only 6 pairs were
extracted from the Thai taxonomy, since each struc-
ture of the Thai taxonomy is rather small as we al-
ready discussed with Table 3. Thus we have omit-
ted further analysis of Thai. The extracted ancestor-
descendant pairs of classifiers are then classified into
three cases, (A), (B) and (C). Their numbers are
shown in the last three rows in Table 5, where mini
and maxi denote the minimum and maximum of re-
liability among all direct relations R(ci  ci+1) in
the path from ca to cd.
400
Chinese Japanese Thai
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
Figure 2: Reliability of extracted superordinate-subordinate relations
Table 5: Verification of transitivity
Chinese Japanese
all direct indirect all direct indirect
No. of (c
a
, c
d
) 78 58 20 86 55 31
Average of R(c
a
?
c
d
) 0.88 0.98 0.61 0.77 0.93 0.48
(A) min
i
> R(c
a
?
c
d
) 16 (21%) 4 (7%) 12 (60%) 24 (28%) 3 (5%) 21 (68%)
(B) min
i
? R(c
a
?
c
d
) < max
i
39 (50%) 34 (59%) 5 (25%) 27 (31%) 24 (44%) 3 (9%)
(C) max
i
? R(c
a
?
c
d
) 23 (29%) 20 (34%) 3 (15%) 35 (41%) 28 (51%) 7 (23%)
In case (A), reliability of a relation derived by
transitivity, R(ca
?
 cd), is less than that of any di-
rect relations, R(ci  ci+1). In case (B), reliability
of a transitive relation is comparable with that of di-
rect relations, i.e. R(ca
?
 cd) is greater or equal to
mini and less than maxi. In case (C), the transitive
relation is more reliable than direct relations.
The average of the reliability of ca
?
 cd is rela-
tively high, 0.88 for Chinese and 0.77 for Japanese.
We also found that more than 70% of derived rela-
tions (case (B) and case (C)) are comparable to or
greater than direct relations. The above facts indi-
cate transitivity on our structural taxonomy is valid
to some degree.
From a different point of view, we divided pairs
of (ca, cd) into two other cases, ?direct? and ?indi-
rect? as shown in the columns of Table 5. The ?di-
rect? case includes the relations which are also ex-
tracted by our method. Note that such relations are
discarded as redundant ones. On the other hand, the
?indirect? case includes the relations which can not
be extracted from the database but only inferred by
using transitivity on the taxonomy. That is, they are
truly new relations. In order to calculate reliability
of ?indirect? cases, we performed additional manual
validation of nouns in Nd\Na.
However, the average of R(ca
?
 cd) in ?in-
direct? cases is not so high for both Chinese and
Japanese, as a large amount of pairs are classi-
fied into case (A). Thus it is not effective to infer
new superordinate-subordinate relations by transi-
tivity. Since we currently only adopted a very naive
method to construct a classifier taxonomy, more so-
phisticated methods should be explored in order to
prevent inferring irrelevant relations.
5 Related Work
Bond (2000) proposed a method to choose an appro-
priate classifier for a noun by referring its seman-
tic class. This method is implemented in a sentence
generation module of a machine translation system.
Similar attempts to generate both Japanese and Ko-
rean classifiers were also reported (Paik and Bond,
2001). Bender and Siegel (2004) implemented a
HPSG that handles several intricate structures in-
cluding Japanese classifiers. Matsumoto (1993)
reported his close analysis of Japanese classi-
fiers based on prototype semantics. Sornlertlam-
vanich (1994) presented an algorithm for selecting
an adequate classifier for a noun by using a cor-
pus. Their research can be regarded as a method to
construct a noun-classifier agreement database au-
401
tomatically from corpora. We used databases de-
rived from dictionaries except for a small number
of noun-classifier pairs in Thai, because we believe
dictionaries provide more reliable and stable infor-
mation than corpora, and in addition they were avail-
able and on hand. Note that we are not concerned
with frequencies of noun-classifier coocurrence in
this study. Huang (1998) proposed a method to
construct a noun taxonomy based on noun-classifier
agreement that is very similar to ours, but aims at
developing a taxonomy for nouns rather than one for
classifiers. There has not been very much work on
building resources concerning noun-classifier agree-
ment. To our knowledge, this is the first attempt to
construct a classifier taxonomy.
6 Conclusion
This paper proposed a method to construct a tax-
onomy of numerative classifiers based on a noun-
classifier agreement database. First, superordinate-
subordinate relations of two classifiers are extracted
by measuring the overlap of two sets of nouns agree-
ing with each classifier. Then these relations are
used as building blocks to build a taxonomy of
tree structures. We conducted experiments to build
classifier taxonomies for three languages: Chinese,
Japanese and Thai. The effectiveness of our method
was evaluated by measuring reliability of extracted
relations, and verifying validity of transitivity in the
taxonomy. We found that extracted relations are re-
liable, and the transitivity in the taxonomy relatively
valid. Relations inferred by transitivity, however, are
less reliable than those directly derived from noun-
classifier agreement.
Future work includes investigating a way to en-
large classifier taxonomies. Currently, not all clas-
sifiers are included in our taxonomy, and it con-
sists of a set of fragmented structures. A more so-
phisticated method to build a large taxonomy in-
cluding more classifiers should be examined. Our
method should also be refined in order to make
superordinate-subordinate relations inferred by the
transitivity more reliable. We are now investigat-
ing a stepwise method to construct taxonomies that
prefers more reliable relations, i.e. an initial tax-
onomy is built with a small number of highly reli-
able relations, and is then expanded with less reli-
able ones.
Acknowledgment
This research was carried out through financial sup-
port provided under the NEDO International Joint
Research Grant Program (NEDO Grant).
References
Emily M. Bender and Melanie Siegel. 2004. Imple-
menting the syntax of Japanese numeral classifiers. In
Proceedings of the the First International Joint Con-
ference on Natural Language Processing, pages 398?
405.
Francis Bond and Kyonghee Paik. 2000. Reusing an on-
tology to generate numeral classifiers. In Proceedings
of the COLING, pages 90?96.
CICC. 1995. CICC Thai basic dictionary. (developed by
Center of the International Cooperation for Computer-
ization).
Chu-Ren Huang, Keh-Jian Chen, and Chin-Hsiung Lai,
editors. 1997. Mandarin Daily News Dictionary of
Measure Words. Mandarin Daily News Publisher.
Chu-Ren Huang, Keh-jiann Chen, and Zhao-ming Gao.
1998. Noun class extraction from a corpus-based col-
location dictionary: An integration of computational
and qualitative approaches. In Quantitative and Com-
putational Studies of Chinese Linguistics, pages 339?
352.
Asako Iida. 2004. Kazoekata no Ziten (Dictionary for
counting things). Sho?gakukan. (in Japanese).
Yo Matsumoto. 1993. The Japanese numeral classifiers:
A study of semantic categories and lexical organiza-
tion. Linguistics, 31:667?713.
Kyonghee Paik and Francis Bond. 2001. Multilin-
gual generation of numeral classifiers using a common
ontology. In Proceedings of the 19th International
Conference on Computer Processing of Oriental Lan-
guages (ICCPOL), pages 141?147.
Virach Sornlertlamvanich, Wantanee Pantachat, and
Surapant Meknavin. 1994. Classifier assignment by
corpus-based approach. In Proceedings of the COL-
ING, pages 556?561.
402
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 19?27,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Wiktionary and NLP: Improving synonymy networks
Emmanuel Navarro
IRIT, CNRS &
Universit? de Toulouse
navarro@irit.fr
Franck Sajous
CLLE-ERSS, CNRS &
Universit? de Toulouse
sajous@univ-tlse2.fr
Bruno Gaume
CLLE-ERSS & IRIT, CNRS &
Universit? de Toulouse
gaume@univ-tlse2.fr
Laurent Pr?vot
LPL, CNRS &
Universit? de Provence
laurent.prevot@lpl-aix.fr
Hsieh ShuKai
English Department
NTNU, Taiwan
shukai@gmail.com
Kuo Tzu-Yi
Graduate Institute of Linguistics
NTU, Taiwan
tzuyikuo@ntu.edu.tw
Pierre Magistry
TIGP, CLCLP, Academia Sinica,
GIL, NTU, Taiwan
pmagistry@gmail.com
Huang Chu-Ren
Dept. of Chinese and Bilingual Studies
Hong Kong Poly U. , Hong Kong.
churenhuang@gmail.com
Abstract
Wiktionary, a satellite of the Wikipedia
initiative, can be seen as a potential re-
source for Natural Language Processing.
It requires however to be processed be-
fore being used efficiently as an NLP re-
source. After describing the relevant as-
pects of Wiktionary for our purposes, we
focus on its structural properties. Then,
we describe how we extracted synonymy
networks from this resource. We pro-
vide an in-depth study of these synonymy
networks and compare them to those ex-
tracted from traditional resources. Fi-
nally, we describe two methods for semi-
automatically improving this network by
adding missing relations: (i) using a kind
of semantic proximity measure; (ii) using
translation relations of Wiktionary itself.
Note: The experiments of this paper are based on Wik-
tionary?s dumps downloaded in year 2008. Differences may
be observed with the current versions available online.
1 Introduction
Reliable and comprehensive lexical resources con-
stitute a crucial prerequisite for various NLP tasks.
However their building cost keeps them rare. In
this context, the success of the Princeton Word-
Net (PWN) (Fellbaum, 1998) can be explained by
the quality of the resource but also by the lack of
serious competitors. Widening this observation to
more languages only makes this observation more
acute. In spite of various initiatives, costs make
resource development extremely slow or/and re-
sult in non freely accessible resources. Collabo-
rative resources might bring an attractive solution
to this difficult situation. Among them Wiktionary
seems to be the perfect resource for building com-
putational mono-lingual and multi-lingual lexica.
This paper focuses therefore on Wiktionary, how
to improve it, and on its exploitation for creating
resources.
In next section, we present some relevant infor-
mation about Wiktionary. Section 3 presents the
lexical graphs we are using and the way we build
them. Then we pay some attention to evaluation
(?4) before exploring some tracks of improvement
suggested by Wiktionary structure itself.
2 Wiktionary
As previously said, NLP suffers from a lack of
lexical resources, be it due to the low-quality or
non-existence of such resources, or to copyrights-
related problems. As an example, we consider
French language resources. Jacquin et al (2002)
highlighted the limitations and inconsistencies
from the French EuroWordnet. Later, Sagot and
Fi?er (2008) explained how they needed to re-
course to PWN, BalkaNet (Tufis, 2000) and other
resources (notably Wikipedia) to build WOLF, a
free French WordNet that is promising but still a
very preliminary resource. Some languages are
straight-off purely under-resourced.
The Web as Corpus initiative arose (Kilgarriff
and Grefenstette, 2003) as an attempt to design
tools and methodologies to use the web for over-
coming data sparseness (Keller and Lapata, 2002).
Nevertheless, this initiative raised non-trivial tech-
nical problems described in Baroni et al (2008).
Moreover, the web is not structured enough to eas-
ily and massively extract semantic relations.
In this context, Wiktionary could appear to be
a paradisiac playground for creating various lexi-
19
cal resources. We describe below the Wiktionary
resource and we explain the restrictions and prob-
lems we are facing when trying to exploit it. This
description may complete few earlier ones, for ex-
ample Zesch et al (2008a).
2.1 Collaborative editing
Wiktionary, the lexical companion to Wikipedia,
is a collaborative project to produce a free-content
multilingual dictionary.
1
As the other Wikipedia?s
satellite projects, the resource is not experts-led,
rather filled by any kind of users. The might-be
inaccuracy of the resulting resource has lengthily
been discussed and we will not debate it: see Giles
(2005) and Britannica (2006) for an illustration
of the controversy. Nevertheless, we think that
Wiktionary should be less subject (so far) than
Wikipedia to voluntary misleading content (be it
for ideological, commercial reasons, or alike).
2.2 Articles content
As one may expect, a Wiktionary article
2
may (not
systematically) give information on a word?s part
of speech, etymology, definitions, examples, pro-
nunciation, translations, synonyms/antonyms, hy-
pernyms/hyponyms, etc.
2.2.1 Multilingual aspects
Wiktionary?s multilingual organisation may be
surprising and not always meet one?s expectations
or intuitions. Wiktionaries exist in 172 languages,
but we can read on the English language main
page, ?1,248,097 entries with English definitions
from over 295 languages?. Indeed, a given wik-
tionary describes the words in its own language
but also foreign words. For example, the English
article moral includes the word in English (adjec-
tive and noun) and Spanish (adjective and noun)
but not in French. Another example, boucher,
which does not exist in English, is an article of the
English wiktionary, dedicated to the French noun
(a butcher) and French verb (to cork up).
A given wiktionary?s ?in other languages? left
menu?s links, point to articles in other wiktionar-
ies describing the word in the current language.
For example, the Fran?ais link in the dictionary
article of the English wiktionary points to an arti-
cle in the French one, describing the English word
dictionary.
1
http://en.wiktionary.org/
2
What article refers to is more fuzzy than classical entry
or acceptance means.
2.2.2 Layouts
In the following paragraph, we outline wik-
tionary?s general structure. We only consider
words in the wiktionary?s own language.
An entry consists of a graphical form and a cor-
responding article that is divided into the follow-
ing, possibly embedded, sections:
? etymology sections separate homonyms when
relevant;
? among an etymology section, different parts
of speech may occur;
? definitions and examples belong to a part of
speech section and may be subdivided into sub-
senses;
? translations, synonyms/antonyms and hy-
pernyms/hyponyms are linked to a given part of
speech, with or without subsenses distinctions.
In figure 1 is depicted an article?s layout example.
Figure 1: Layout of boot article (shortened)
About subsenses, they are identified with an in-
dex when first introduced but they may appear as
a plain text semantic feature (without index) when
used in relations (translations, synonyms, etc.). It
is therefore impossible to associate the relations
arguments to subsenses. Secondly, subsense index
appears only in the current word (the source of the
relation) and not in the target word?s article it is
linked to (see orange French N. and Adj., Jan. 10,
2008
3
).
A more serious issue appears when relations are
shared by several parts of speech sections. In Ital-
3
http://fr.wiktionary.org/w/index.php?
title=orange&oldid=2981313
20
ian, both synonyms and translations parts are com-
mon to all words categories (see for example car-
dinale N. and Adj., Apr. 26, 2009
4
).
2.3 Technical issues
As Wikipedia and the other Wikimedia Founda-
tion?s projects, the Wiktionary?s content manage-
ment system relies on the MediaWiki software
and on the wikitext. As stated in Wikipedia?s
MetaWiki article, ?no formal syntax has been de-
fined? for the MediaWiki and consequently it is
not possible to write a 100% reliable parser.
Unlike Wikipedia, no HTML dump is available
and one has to parse the Wikicode. Wikicode
is difficult to handle since wiki templates require
handwritten rules that need to be regularly up-
dated. Another difficulty is the language-specific
encoding of the information. Just to mention one,
the target language of a translation link is iden-
tified by a 2 or 3 letters ISO-639 code for most
languages. However in the Polish wiktionary the
complete name of the language name (angielski,
francuski, . . . ) is used.
2.4 Parsing and modeling
The (non-exhaustive) aforementioned list of diffi-
culties (see ?2.2.2 and ?2.3) leads to the following
consequences:
? Writing a parser for a given wiktionary is
possible only after an in-depth observation of its
source. Even an intensive work will not prevent
all errors as long as (i) no syntax-checking is made
when editing an article and (ii) flexibility with the
?tacitly agreed? layout conventions is preserved.
Better, flexibility is presented as a characteristic of
the framework:
?[. . . ] it is not a set of rigid rules. You may
experiment with deviations, but other editors
may find those deviations unacceptable, and
revert those changes. They have just as much
right to do that as you have to make them.
5
?
Moreover, a parser has to be updated every new
dump, as templates, layout conventions (and so
on) may change.
?Writing parsers for different languages is not a
simple adjustment, rather a complete overhaul.
? When extracting a network of semantic rela-
tions from a given wiktionary, some choices are
more driven by the wiktionary inner format than
scientific modelling choices. An illustration fol-
4
http://it.wiktionary.org/w/index.php?
title=cardinale&oldid=758205
5
http://en.wiktionary.org/wiki/WT:ELE
lows in ?3.2. When merging information extracted
from several languages, the homogenisation of the
data structure often leads to the choice of the poor-
est one, resulting in a loss of information.
2.5 The bigger the better?
Taking advantage of colleagues mastering various
languages, we studied the wiktionary of the fol-
lowing languages: French, English, German, Pol-
ish and Mandarin Chinese. A first remark con-
cerns the size of the resource. The official num-
ber of declared articles in a given wiktionary in-
cludes a great number of meta-articles which are
not word entries As of April 2009, the French wik-
tionary reaches the first rank
6
, before the English
one. This can be explained by the automated im-
port of public-domain dictionaries articles (Littr?
1863 and Dictionnaire de l?Acad?mie Fran?aise
1932-1935). Table 1 shows the ratio between the
total number of articles and the ?relevant? ones
(numbers based on year 2008 snapshots).
Total Meta
?
Other
??
Relevant
fr 728,266 25,244 369,948 337,074 46%
en 905,963 46,202 667,430 192,331 21%
de 88,912 7,235 49,672 32,005 36%
pl 110,369 4,975 95,241 10,153 9%
zh 131,752 8,195 112,520 1,037 0.7%
?
templates definitions, help pages, user talks, etc.
??
other languages, redirection links, etc.
Table 1: Ratio of ?relevant? articles in wiktionaries
By ?relevant?, we mean an article about a word
in the wiktionary?s own language (e.g. not an
article about a French word in the English Wik-
tionary). Among the ?relevant? articles, some
are empty and some do not contain any transla-
tion nor synonym link. Therefore, before deciding
to use Wiktionary, it is necessary to compare the
amount of extracted information contribution and
the amount of work required to obtain it .
3 Study of synonymy networks
In this section, we study synonymy networks built
from different resources. First, we introduce
some general properties of lexical networks (?3.1).
Then we explain how we build Wiktionary?s syn-
onymy network and how we analyse its proper-
ties. In ?3.3, we show how we build similar graphs
from traditional resources for evaluation purposes.
3.1 Structure of lexical networks
In the following sections, a graph G = (V,E)
is defined by a set V of n vertices and a set
E ? V
2
of m edges. In this paper, V is
6
http://meta.wikimedia.org/wiki/List_
of_Wiktionaries
21
a set of words and E is defined by a relation
E
R
7?? E : (w
1
, w
2
) ? E if and only if w
1
R
? w
2
.
Most of lexical networks, as networks extracted
from real world, are small worlds (SW) net-
works. Comparing structural characteristics of
wiktionary-based lexical networks to some stan-
dard resource should be done according to well-
known properties of SW networks (Watts and
Strogatz, 1998; Barabasi et al, 2000; Newman,
2003; Gaume et al, 2008). These properties are:
? Edge sparsity: SW are sparse in edges
m = O(n) or m = O(n log(n))
? Short paths: in SW, the average path length
(L)
7
is short. Generally there is at least one short
path between any two nodes.
? High clustering: in SW, the clustering coef-
ficient (C) that expresses the probability that two
distinct nodes adjacent to a given third one are ad-
jacent, is an order of magnitude higher than for
Erdos-Renyi (random) graphs: C
SW
 C
random
;
this indicates that the graph is locally dense, al-
though it is globally sparse.
?Heavy-tailed degree distribution: the distri-
bution of the vertices incidence degrees follows a
power law in a SW graph. The probability P (k)
that a given node has k neighbours decreases as a
power law, P (k) ? k
a
(a being a constant charac-
teristic of the graph). Random graphs conforms to
a Poisson Law.
3.2 Wiktionary?s network
Graph extraction Considering what said in
?2.2.2 and ?2.4, we made the following choices:
8
? Vertices: a vertex is built for each entry?s part
of speech.
? Parts of speech: when modeling the links
from X (X having for part of speech Pos
X
) to
one of its synonyms Y , we assume that Pos
Y
=
Pos
X
, thus building vertex Pos
Y
.Y.
? Subsenses: subsenses are flattened. First, the
subsenses are not always mentioned in the syn-
onyms section. Second, if we take into account
the subsenses, they only appear in the source of the
relation. For example, considering in figure 1 the
relation boot
syn
??? kick (both nouns), and given the
10 subsenses for boot and the 5 ones for kick, we
should build 15 vertices. And we should then add
7
Average length of the shortest path between any two
nodes.
8
These choices can clearly be discussed from a linguis-
tic point of view and judged to be biased. Nevertheless, we
adopted them as a first approximation to make the modelling
possible.
all the links between the mentioned boot?s sub-
senses and the 5 kick?s existing subsenses. This
would lead to a high number of edges, but the
graph would not be closer to the reality. The way
subsenses appear in Wiktionary are unpredictable.
"Subsenses" correspond sometimes to homonyms
or clear-cut senses of polysemous words, but can
also correspond to facets, word usage or regu-
lar polysemy. Moreover, some entries have no
subsenses distinction whereas it would be wor-
thy. More globally, the relevance of discrete word
senses has been seriously questioned, see (Victorri
and Fuchs, 1996) or (Kilgarriff, 1997) for very
convincing discussions. Two more practical rea-
sons led us to this choice. We want our method to
be reproducible for other languages and some wik-
tionaries do not include subsenses. At last, some
gold standard resources (eg. Dicosyn) have their
subsenses flattened too and we want to compare
the resources against each other.
? Edges: wiktionary?s synonymy links are ori-
ented but we made the graph symmetric. For ex-
ample, boot does not appear in kick?s synonyms.
Some words even appear as synonyms without be-
ing an entry of Wiktionary.
From the boot example (figure 1), we extract ver-
tices {N.boot, V.boot}, build {N.buskin,
N.kick, V.kick} and we add the follow-
ing (symmetrized) edges: N.boot?N.buskin,
N.boot?N.kick and V.boot?V.kick.
Graph properties By observing the table 2, we
can see that the graphs of synonyms extracted
from Wiktionary are all typical small worlds. In-
deed their l
lcc
remains short, their C
lcc
is always
greater or equal than 0.2 and their distribution
curves of the vertices incidence degree is very
close to a power law (a least-square method gives
always exponent a
lcc
? ?2.35 with a confidence
r
2
lcc
always greater than 0.89). It can also be seen
that the average incidence k
lcc
ranges from 2.32
to 3.32.
9
It means that no matter which language
9
It is noteworthy that the mean incidence of vertices is al-
most always the same (close to 2.8) no matter the graph size
is. If we assume that all wiktionary?s graphs grow in a similar
way but at different speed rates (after all it is the same frame-
work), graphs (at least their statistical properties) from differ-
ent languages can be seen as snapshots of the same graph at
different times. This would mean that the number of graphs
edges tends to grow proportionally with the number of ver-
tices. This fits with the dynamic properties of small worlds
(Steyvers and Tenenbaum, 2005). It means that for a wik-
tionary system, even with many contributions, graph density
is likely to remain constant and we will see that in compar-
ison to traditional lexical resources this density is quite low.
22
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
fr-N 18017 9650 3945 4690 2.38 10.18 0.2 -2.03 0.89
fr-A 5411 2516 1160 1499 2.58 8.86 0.23 -2.04 0.95
fr-V 3897 1792 886 1104 2.49 9.84 0.21 -1.65 0.91
en-N 22075 11545 3863 4817 2.49 9.7 0.24 -2.31 0.95
en-A 8437 4178 2486 3276 2.64 8.26 0.2 -2.35 0.95
en-V 6368 3274 2093 2665 2.55 8.33 0.2 -2.01 0.93
de-N 32824 26622 12955 18521 2.86 7.99 0.28 -2.16 0.93
de-A 5856 6591 3690 5911 3.2 6.78 0.24 -1.93 0.9
de-V 5469 7838 4574 7594 3.32 5.75 0.23 -1.92 0.9
pl-N 8941 4333 2575 3143 2.44 9.85 0.24 -2.31 0.95
pl-A 1449 731 449 523 2.33 7.79 0.21 -1.71 0.94
pl-V 1315 848 601 698 2.32 5.34 0.2 -1.61 0.92
n: number of vertices m: number of edges
k: avg. number of neighbours per vertex l: avg. path length between vertices
C: clustering rate a: power law exponent with r
2
confidence
_
lcc
: denotes on largest connected component
Table 2: Wiktionary synonymy graphs properties
or part of speech, m = O(n) as for most of SW
graphs (Newman, 2003; Gaume et al, 2008).
3.3 Building synonymy networks from
known standards
WordNet There are many possible ways for
building lexical networks from PWN. We tried
several methods but only two of them are worth
to be mentioned here. The graphs we built have
words as vertices, not synsets or senses. A first
straightforward method (method A) consists in
adding an edge between two vertices only if the
corresponding words appear as elements of the
same synset. This method produced many discon-
nected graphs of various sizes. Both the compu-
tational method we planned to use and our intu-
itions about such graphs were pointing towards a
bigger graph that would cover most of the lexical
network.
We therefore decided to exploit the hypernymy
relation. Traditional dictionaries indeed propose
hypernyms when one look for synonyms of very
specific terms, making hypernymy the closest re-
lation to synonymy at least from a lexicographic
viewpoint. However, adding all the hypernymy re-
lations resulted in a network extremely dense in
edges with some vertices having a high number of
neighbours. This was due to the tree-like organi-
sation of WordNet that gives a very special impor-
tance to higher nodes of the tree.
In the end we retained method B that consists in
adding edges in following cases:
? if two words belong to the same synset;
? if a word only appears in a synset that is a leaf
of the tree and contains only this word, then cre-
ate edges linking to words included in the hyper-
nym(s) synset.
We would like to study the evolution through time of wik-
tionaries, however this is outside the scope of this paper.
Therefore when a vertice w do not get any neigh-
bour according to method A, method B adds edges
linking w to words included in the hypernym(s)
synset of the synset {w}. We only added hyper-
nyms for the leaves of the tree in order to keep our
relations close to the synonymy idea. This idea has
already been exploited for some WordNet-based
semantic distances calculation taking into account
the depth of the relation in the tree (Leacock and
Chodorow, 1998).
Dicosyn graphs Dicosyn is a compilation of
synonym relations extracted from seven dictionar-
ies (Bailly, Benac, Du Chazaud, Guizot, Lafaye,
Larousse and Robert):
10
there is an edge r ? s if
and only if r and s have the same syntactic cate-
gory and at least one dictionary proposes s being
a synonym in the dictionary entry r. Then, each
of the three graphs (Nouns, Verbs, Adjectives) ob-
tained is made symmetric (dicosyn-fr-N, dicosyn-
fr-V and dicosyn-fr-A).
Properties of the graphs extracted Table 3
sums-up the structural properties of the synonyms
networks built from standard resources.
We can see that all the synonymy graphs ex-
tracted from PWN or Dicosyn are SW graphs.
Indeed their l
lcc
remains short, their C
lcc
is al-
ways greater or equal than 0.35 and their distri-
bution curves of the vertices incidence degree is
very close to a power law (a least-square method
gives always exponent a
lcc
near of ?2.30 with a
confidence r
2
lcc
always greater than 0.85). It can
also be observed that no matter the part of speech,
the average incidence of Dicosyn-based graphs is
always lower than WordNet ones.
10
Dicosyn has been first produced at ATILF, before being
corrected at CRISCO laboratory.
(http://elsap1.unicaen.fr/dicosyn.html)
23
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
pwn-en-N-A 117798 104929 12617 28608 4.53 9.89 0.76 -2.62 0.89
pwn-en-N-B 117798 168704 40359 95439 4.73 7.79 0.72 -2.41 0.91
pwn-en-A-A 21479 22164 4406 11276 5.12 9.08 0.75 -2.32 0.85
pwn-en-A-B 21479 46614 15945 43925 5.51 6.23 0.78 -2.09 0.9
pwn-en-V-A 11529 23019 6534 20806 6.37 5.93 0.7 -2.34 0.87
pwn-en-V-B 11529 40919 9674 39459 8.16 4.66 0.64 -2.06 0.91
dicosyn-fr-N 29372 100759 26143 98627 7.55 5.37 0.35 -2.17 0.92
dicosyn-fr-A 9452 42403 8451 41753 9.88 4.7 0.37 -1.92 0.92
dicosyn-fr-V 9147 51423 8993 51333 11.42 4.2 0.41 -1.88 0.91
Table 3: Gold standard?s synonymy graphs properties
4 Wiktionary graphs evaluation
Coverage and global SW analysis By compar-
ing tables 2 and 3, one can observe that:
? The lexical coverage of Wiktionary-based syn-
onyms graphs is always quantitatively lower than
those of standard resources although this may
change. For example, to horn (in PWN), absent
from Wiktionary in 2008, appeared in 2009. At
last, Wiktionary is more inclined to include some
class of words such as to poo (childish) or to
prefetch, to google (technical neologisms).
? The average number of synonyms for an en-
try of a Wiktionary-based resource is smaller than
those of standard resources. For example, com-
mon synonyms such as to act/to play appear in
PWN and not in Wiktionary. Nevertheless, some
other appear (rightly) in Wiktionary: to reduce/to
decrease, to cook/to microwave.
? The clustering rate of Wiktionary-based
graphs is always smaller than those of standard re-
sources. This is particularly the case for English.
However, this specificity might be due to differ-
ences between the resources themselves (Dicosyn
vs. PWN) rather than structural differences at the
linguistic level.
Evaluation of synonymy In order to evaluate
the quality of extracted synonymy graphs from
Wiktionary, we use recall and precision measure.
The objects we compare are not simple sets but
graphs (G = (V ;E)), thus we should compare
separately set of vertices (V ) and set of edges (E).
Vertices are words and edges are synonymy links.
Vertices evaluation leads to measure the resource
(a) English Wiktionary vs. Wordnet
Precision Recall
Nouns 14120/22075 = 0.64 14120/117798 = 0.12
Adj. 5874/8437 = 0.70 5874/21479 = 0.27
Verbs 5157/6368 = 0.81 5157/11529 = 0.45
(b) French Wiktionary vs. Dicosyn
Precision Recall
Nouns 10393/18017 = 0.58 10393/29372 = 0.35
Adj. 3076/5411 = 0.57 3076/9452 = 0.33
Verbs 2966/3897 = 0.76 2966/9147 = 0.32
Table 4: Wiktionary coverage
coverage whereas edges evaluation leads to mea-
sure the quality of the synonymy links in Wik-
tionary resource.
First of all, the global picture (table 4) shows
clearly that the lexical coverage is rather poor. A
lot of words included in standard resources are not
included yet in the corresponding wiktionary re-
sources. Overall the lexical coverage is always
lower than 50%. This has to be kept in mind while
looking at the evaluation of relations shown in ta-
ble 5. To compute the relations evaluation, each
resource has been first restricted to the links be-
tween words being present in each resource.
About PWN, since every link added with
method A will also be added with method B, the
precision of Wiktionary-based graphs synonyms
links will be always lower for "method A graphs"
than for "method B graphs". Precision is rather
good while recall is very low. That means that a
lot of synonymy links of the standard resources
are missing within Wiktionary. As for Dicosyn,
the picture is similar with even better precision but
very low recall.
5 Exploiting Wiktionary for improving
Wiktionary
As seen in section 4, Wiktionary-based resources
are very incomplete with regard to synonymy. We
propose two tasks for adding some of these links:
Task 1: Adding synonyms to Wiktionary by
taking into account its Small World characteristics
for proposing new synonyms.
(a) English wiktionary vs. Wordnet
Precision Recall
Nouns (A) 2503/6453 = 0.39 2503/11021 = 0.23
Nouns (B) 2763/6453 = 0.43 2763/18440 = 0.15
Adj. (A) 786/3139 = 0.25 786/5712 = 0.14
Adj. (B) 1314/3139 = 0.42 1314/12792 = 0.10
Verbs (A) 866/2667 = 0.32 866/10332 = 0.08
Verbs (B) 993/2667 = 0.37 993/18725 = 0.05
(b) French wiktionary vs. Dicosyn
Precision Recall
Nouns 3510/5075 = 0.69 3510/44501 = 0.08
Adj. 1300/1677 = 0.78 1300/17404 = 0.07
Verbs 899/1267 = 0.71 899/23968 = 0.04
Table 5: Wiktionary synonymy links precision & recall
24
Task 2: Adding synonyms to Wiktionary by
taking into account the translation relations.
We evaluate these two tasks against the bench-
marks presented in section 3.2.
5.1 Improving synonymy in Wiktionary by
exploiting its small world structure
We propose here to enrich synonymy links of Wik-
tionary by taking into account that lexical net-
works have a high clustering coefficient. Our hy-
pothesis is that missing links in Wiktionary should
be within clusters.
A high clustering coefficient means that two
words which are connected to a third one are likely
to be connected together. In other words neigh-
bours of my neighbours should also be in my
neighbourhood. We propose to reverse this prop-
erty to the following hypothesis: "neighbour of my
neighbours which are not in my neighbourhood
should be a good neighbour candidate". Thus the
first method we test consist simply in connecting
every vertex to neighbours of its neighbours. One
can repeat this operation until the expected num-
ber of edges is obtained.
11
Secondly we used the PROX approach pro-
posed by (Gaume et al, 2009). It is a stochastic
method designed for studying ?Hierarchical Small
Worlds?. Briefly put, for a given vertex u, one
computes for all other vertices v the probability
that a randomly wandering particle starting from
u stands in v after a fixed number of steps. Let
P (u, v) be this value. We propose to connect u
to the k first vertices ranked in descending order
with respect of P (u, v). We always choose k pro-
portionally to the original degree of u (number of
neighbours of u).
For a small number of steps (3 in our case) ran-
dom wanderings tend to be trapped into local clus-
ter structures. So a vertex v with a high P (u, v) is
likely to belong to the same cluster as u, which
means that a link u?v might be relevant.
Figure 2 shows precision, recall and f-score
evolution for French verbs graph when edges are
added using ?neighourhood? method (neigh), and
using ?Prox? method. Dashed line correspond to
the value theoretically obtained by choosing edges
at random. First, both methods are clearly more
efficient than a random addition, which is not sur-
prising but it seems to confirm our hypothesis that
missing edges are within clusters. Adding sharply
11
We repeat it only two times, otherwise the number of
added edges is too large.
0 2000 4000 6000 8000 10000 12000 140000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
prox3neighrandom
0 2000 4000 6000 8000 10000 12000 140000.03
0.040.05
0.060.07
0.080.09
R
0 2000 4000 6000 8000 10000 12000 140000.05
0.060.07
0.080.09
0.100.11
0.120.13
F
fr.V
Figure 2: Precision, recall and F-score of French verbs
graph enlarged using only existing synonymy links
neighbours of neighbours seems to be as good as
adding edges ranked by Prox, anyway the rank
provided by Prox permits to add a given number
of edges. This ranking can also be useful to order
potential links if one think about a user validation
system. Synonyms added by Prox and absent from
gold standards are not necessarily false.
For example Prox proposes a relevant link ab-
solve/forgive, not included in PWN. Moreover,
many false positive are still interesting to consider
for improving the resource. For example, Prox
adds relations such as hypernyms (to uncover/to
peel) or inter-domain ?synonyms? (to skin/to peel).
This is due to high clustering (see ?3.1) and to
the fact that clusters in synonymy networks corre-
lates with language concepts (Gaume et al, 2008;
Duvignau and Gaume, 2008; Gaume et al, 2009;
Fellbaum, 1999).
Finally note that results are similar for other
parts of speech and other languages.
5.2 Using Wiktionary?s translation links to
improve its synonymy network
Assuming that two words sharing many transla-
tions in different languages are likely to be syn-
onymous, we propose to use Wiktionary?s transla-
tion links to enhance the synonymy network of a
given language.
In order to rank links to be potentially added,
we use a simple Jaccard measure: let T
w
be the set
of a word w?s translations, then for every couple
of words (w,w
?
) we have:
Jaccard(w,w
?
) =
|T
w
? T
w
?
|
|T
w
? T
w
?
|
We compute this measure for every possible pair
of words and then, starting from Wiktionary?s syn-
onymy graph, we incrementally add links accord-
ing to their Jaccard rank.
25
We notice first that most of synonymy links
added by this method were not initially included
in Wiktionary?s synonymy network. For exam-
ple, regarding English verbs, 95% of 2000 best
ranked proposed links are new. Hence this method
may be efficient to improve graph density. How-
ever one can wonder about the quality of the new
added links, so we discuss precision in the next
paragraph.
In figure 3 is depicted the evolution of precision,
recall and F-score for French verbs in the enlarged
graph in regard of the total number of edges. We
use Dicosyn graph as a gold standard. The dashed
line corresponds to theoretical scores one can ex-
pect by adding randomly chosen links.
First we notice that both precision and recall
are significantly higher than we can expect from
random addition. This confirms that words shar-
ing the same translations are good synonym candi-
dates. Added links seem to be particularly relevant
at the beginning for higher Jaccard scores. From
the first dot to the second one we add about 1000
edges (whereas the original graph contains 1792
edges) and the precision only decreases from 0.71
to 0.69.
The methods we proposed in this section are
quite simple and there is room for improvement.
First, both methods can be combined in order
to improve the resource using translation links
and then using clusters structure. One can also
think to the corollary task that would consists in
adding translation links between two languages
using synonymy links of others languages.
0 2000 4000 6000 8000 10000 120000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
random
0 2000 4000 6000 8000 10000 120000.02
0.040.06
0.080.10
0.120.14
0.16
R
0 2000 4000 6000 8000 10000 120000.04
0.060.08
0.100.12
0.140.16
0.180.20
0.22
F
fr.V
Figure 3: Precision, recall and F-score of French verbs
graph enlarged using translation links
6 Conclusion and future work
This paper gave us the opportunity to share some
Wiktionary experience related lexical resources
building. We presented in addition two approaches
for improving these resources and their evaluation.
The first approach relies on the small world struc-
ture of synonymy networks. We postulated that
many missing links in Wiktionary should be added
among members of the same cluster. The second
approach assumes that two words sharing many
translations in different languages are likely to be
synonymous. The comparison with traditional re-
sources shows that our hypotheses are confirmed.
We now plan to combine both approaches.
The work presented in this paper combines a
NLP contribution involving data extraction and
rough processing of the data and a mathematical
contribution concerning graph-like resource. In
our viewpoint the second aspect of our work is
therefore complementary of other NLP contribu-
tions, like (Zesch et al, 2008b), involving more
sophisticated NLP processing of the resource.
Support for collaborative editing Our results
should be useful for setting up a more efficient
framework for Wiktionary collaborative editing.
We should be able to always propose a set of syn-
onymy relations that are likely to be. For exam-
ple, when a contributor creates or edits an arti-
cle, he may think about adding very few links but
might not bother providing an exhaustive list of
synonyms. Our tool can propose a list of potential
synonyms, ordered by relevancy. Each item of this
list would only need to be validated (or not).
Diachronic study An interesting topic for future
work is a "diachronic" study of the resource. It
is possible to access Wiktionary at several stages,
this can be used for studying how such resources
evolve. Grounded on this kind of study, one may
predict the evolution of newer wiktionaries and
foresee contributors? NLP needs. We would like
to set up a framework for everyone to test out new
methodologies for enriching and using Wiktionary
resources. Such observatory, would allow to fol-
low not only the evolution of Wiktionary but also
of Wiktionary-grounded resources, that will only
improve thanks to steady collaborative develop-
ment.
Invariants and variabality Wiktionary as a
massively mutiligual synonymy networks is an
extremely promising resource for studying the
(in)variability of semantic pairings such as
house/family, child/fruit, feel/know... (Sweetser,
1991; Gaume et al, 2009). A systematic study
within the semantic approximation framework
presented in the paper on Wiktionary data will be
carried on in the future.
26
References
A-L. Barabasi, R. Albert, H. Jeong, and G. Bianconi.
2000. Power-Law Distribution of the World Wide
Web. Science, 287. (in Technical Comments).
M. Baroni, F. Chantree, A. Kilgarriff, and S. Sharoff.
2008. Cleaneval: a Competition for Cleaning
Web Pages. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), Mar-
rakech.
Encyclopaedia Britannica. 2006. Fatally flawed: re-
futing the recent study on encyclopedic accuracy by
the journal Nature.
K. Duvignau and B. Gaume. 2008. Between words
and world: Verbal "metaphor" as semantic or prag-
matic approximation? In Proceedings of Interna-
tional Conference "Language, Communication and
Cognition", Brighton.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
C. Fellbaum. 1999. La repr?sentation des verbes
dans le r?seau s?mantique Wordnet. Langages,
33(136):27?40.
B. Gaume, K. Duvignau, L. Pr?vot, and Y. Desalle.
2008. Toward a cognitive organization for electronic
dictionaries, the case for semantic proxemy. In Col-
ing 2008: Proceedings of the Workshop on Cogni-
tive Aspects of the Lexicon (COGALEX 2008), pages
86?93, Manchester.
B. Gaume, K. Duvignau, and M. Vanhove. 2009. Se-
mantic associations and confluences in paradigmatic
networks. In M. Vanhove, editor, From Polysemy to
Semantic Change: Towards a Typology of Lexical
Semantic Associations, pages 233?264. John Ben-
jamins Publishing.
J. Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438:900?901.
C. Jacquin, E. Desmontils, and L. Monceaux. 2002.
French EuroWordNet Lexical Database Improve-
ments. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLING 2002), Mexico
City.
F. Keller and M. Lapata. 2002. Using the web to over-
come data sparseness. In Proceedings of EMNLP-
02, pages 230?237.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29:333?347.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the humanities, 31(2):91?113.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet similarity for word sense iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database, pages 265?283. MIT Press.
M. Newman. 2003. The structure and function of com-
plex networks.
B. Sagot and D. Fi?er. 2008. Building a Free French
Wordnet from Multilingual Resources. In Proceed-
ings of OntoLex 2008, Marrackech.
M. Steyvers and J. B. Tenenbaum. 2005. The large-
scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cogni-
tive Science, 29:41?78.
E. Sweetser. 1991. From etymology to pragmatics.
Cambridge University Press.
D. Tufis. 2000. Balkanet design and development of a
multilingual balkan wordnet. Romanian Journal of
Information Science and Technology, 7(1-2).
B. Victorri and C. Fuchs. 1996. La polys?mie, con-
struction dynamique du sens. Herm?s.
D.J. Watts and S.H. Strogatz. 1998. Collective dynam-
ics of small-world networks. Nature, 393:440?442.
T. Zesch, C. M?ller, and I. Gurevych. 2008a. Extract-
ing Lexical Semantic Knowledge from Wikipedia
and Wiktionary. In Proceedings of the Conference
on Language Resources and Evaluation (LREC),
Marrakech.
T. Zesch, C. Muller, and I. Gurevych. 2008b. Using
wiktionary for computing semantic relatedness. In
Proceedings of 23rd AAAI Conference on Artificial
Intelligence.
27
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 123?130,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
CWN-LMF: Chinese WordNet in the Lexical Markup Framework 
 
Lung-Hao Lee1, Shu-Kai Hsieh2, Chu-Ren Huang1,3 
 
1Institute of Linguistics, Academia Sinica 
2Department of English, National Taiwan Normal University 
3Department of Chinese & Bilingual Studies, The Hong Kong Polytechnic University 
1128 Academia Road, Section 2, Taipei 115, Taiwan 
2162 He-ping East Road, Section 1, Taipei 106, Taiwan 
3Hung Hom, Kowloon, Hong Kong 
1{lunghao,churen}@gate.sinica.edu.tw 
2shukai@ntnu.edu.tw 
3churen.huang@inet.polyu.edu.hk 
 
Abstract 
Lexical Markup Framework (LMF, ISO-
24613) is the ISO standard which provides 
a common standardized framework for the 
construction of natural language 
processing lexicons. LMF facilitates data 
exchange among computational linguistic 
resources, and also promises a convenient 
uniformity for future application. This 
study describes the design and implemen-
tation of the WordNet-LMF used to 
represent lexical semantics in Chinese 
WordNet. The compiled CWN-LMF will 
be released to the community for linguis-
tic researches.  
1 Introduction 
Princeton WordNet1 is an English lexical data-
base that groups nouns, verbs, adjectives and 
adverbs into sets of cognitive synonyms, which 
are named as synsets (Fellbaum, 1998; Miller, 
1995).  The Global WordNet Association 
(GWA)2 built on the results of Princeton Word-
Net and Euro WordNet (Vossen, 2004) is a free 
and public association that provides a platform to 
share and connect all languages in the world. For 
Mandarin Chinese in Taiwan, Huang et al (2004) 
constructed the Academia Sinica Bilingual Onto-
logical Wordnet (Sinica BOW) which integrates 
WordNet, English-Chinese Translation Equiva-
                                                 
n
1 Wordnet, available online 
at http://wordnetweb.princeton.edu/perl/webw  
2 Global WordNet Association (GWA), available on-
line at http://www.globalwordnet.org/ 
lents Database (ECTED) and SUMO for cross-
language linguistic studies. As a follow-up, Chi-
nese WordNet (CWN) has been built as a robust 
lexical knowledge system which also embodies a 
precise expression of sense relations (Huang et 
al., 2008). In recent years, WordNet-like re-
sources have become one of the most reliable 
and essential resources for linguistic studies for 
all languages (Magnini and Cavaglia, 2000; So-
ria et al 2009; Strapparava and Valitutti, 2004).  
Lexical Markup Framework (LMF, ISO-
24613) is the ISO standard which provides a 
common standardized framework for the con-
struction of natural language processing lexicons 
(Francopoulo et al, 2009).  One important pur-
pose of LMF is to define a standard for lexicons 
which covers multilingual lexical information 
(Francopoulo et al, 2006b). In this study, we 
describe the design and implementation of the 
Wordnet-LMF (Soria et al 2009) to represent 
lexical semantics in Chinese WordNet. 
The rest of this paper is organized as follows: 
Section 2 introduces Chinese WordNet and Lexi-
cal Markup Framework. Section 3 describes how 
we represent Chinese WordNet in the Lexical 
Markup Framework (CWN-LMF). Section 4 
presents an example on Chinese word sense dis-
tinction using CWN-LMF format. Quantitative 
analysis of compiled CWN-LMF is presented in 
Section 5. We also describe the application sce-
nario using CWN-LMF for information interope-
rability of lexical semantics in Section 6. Section 
7 discusses the experience and difficulties of en-
coding CWN into Wordnet-LMF.  Finally, Sec-
tion 8 concludes this study with future research.  
 
 
123
2 Related Work  
2.1 Chinese WordNet 
Creating a semantic relation-based language re-
source is a time consuming and labor intensive 
task, especially for Chinese due to the unobvious 
definition and distinction among characters, 
morphemes and words. Chinese WordNet 3  
(CWN) has been built by Academia Sinica and is 
successively extended its scope so far. Lemmas 
included in CWN mainly fall on the medium fre-
quency words. Each lexical entry is analyzed 
according to the guidelines of Chinese word 
sense distinctions (CKIP, 2003; Huang et al 
2003) which contain information including Part-
of-Speech, sense definition, example sentences, 
corresponding English synset(s) from Princeton 
WordNet, lexical semantic relations and so on. 
Unlike Princeton WordNet, CWN has not been 
constructed mainly on the synsets and semantic 
relations. Rather it focuses to provide precise 
expression for the Chinese sense division and the 
semantic relations needs to be based on the lin-
guistic theories, especially lexical semantics 
(Huang et al, 2008). Moreover, Huang et al 
(2005) designed and implemented the Sinica 
Sense Management System (SSMS) to store and 
manage word sense data generated in the analy-
sis stage. SSMS is meaning-driven. Each sense 
of a lemma is identified specifically using a 
unique identifier and given a separate entry. 
There are 8,646 lemmas / 25,961 senses until 
December 2008 have been analyzed and stored 
in SSMS. Figure 1 shows the result of sense dis-
tinction for ?? zu-ji ?footprint? as an example 
in Chinese WordNet. 
Huang et al (2004) proposed Domain Lexico-
Taxonomy (DLT) as a domain taxonomy popu-
lated with lexical entries. By using DLT with 
Chinese WordNet and Domain Taxonomy, there 
were 15,160 Chinese senses that linked and dis-
tributed in 463 domain nodes. In addition, Huang 
et al (2005) further applied DLT approach to a 
Chinese thesaurus called as CiLin and showed 
with evaluation that DLT approach is robust 
since the size and number of domain lexica in-
creased effectively.  
 
Figure1: The result of sense distinction for ?zu2 
ji1 (footprint)?. 
2.2 Lexical Markup Framework  
Lexical Markup Framework (LMF, ISO-24613) 
is the ISO standard for natural language 
processing lexicons and machine readable dic-
tionaries. The goals of LMF are to provide a 
common model for the creation and use of lexi-
cal resources, and to manage the exchange of 
data between them. Francopoulo et al (2006a; 
2009) offered a snapshot of how LMF represents 
multilingual lexicons. LMF facilitates data ex-
change among computational linguistic resources 
and also promises a convenient uniformity for 
future application. More updated information can 
be found online 
at http://www.lexicalmarkupframework.org . 
                                                
Soria et al (2009) proposed a Wordnet-LMF 
developed in the framework of the KYOTO 4  
project as a standardized interoperability format 
for the interchange of lexico-semantic informa-
tion. Wordnet-LMF is an LMF dialect tailored to 
encode lexical resources adhering to the Word-
                                                 
/
 
3 Chinese WordNet, available online 
at http://cwn.ling.sinica.edu.tw
4 KYOTO, available online at http://www.kyoto-
project.eu/  
124
Net model of lexical knowledge representation. 
Wordnet-LMF was designed by adhering to LMF 
principles yet taking into account on the one 
hand, the peculiarities of the Wordnet model, and 
on the other by trying to maximize the efficiency 
of the format.  
If we take Princeton WordNet 3.0 synset 
{footprint_1} for example, a Wordnet-LMF re-
presentation can be found in Figure 2. The de-
tails will be explained in Section 3. 
 
<Synset id=?eng-30-06645039-n? baseConcept=?1?>
<Definition gloss=?mark of a foot or shoe on a surface?>
<Statement example=?the police made casts of the 
footprints in the soft earth outside the window?/>
</Definition>
<SynsetRelations>
<SynsetRelation target=?eng-30-06798750-n?
relType=?has_hyperonym?>
</SynsetRelation>
<SynsetRelation target=?eng-30-06645266-n?
relType=?has_hyponym?>
</SynsetRelation>
</SynsetRelations>
<MonolingualExternalRefs>
<MonolingualExternalRef externalSystem=?Wordnet1.6?
externalReference=?eng-16-01234567-n?>
<MonolingualExternalRef externalSystem=?SUMO?
externalReference=?superficialPart? relType=?at?>
</MonolingualExternalRefs>
<Synset>
 
Figure 2: An example of Wordnet-LMF format. 
 
3 CWN in the Lexical Markup Frame-
xical se-
ation 
lInformation is used 
label=?Compile Chinese 
work (CWN-LMF) 
Wordnet-LMF is used to represent le
mantics in Chinese WordNet. As LexicalRe-
source is the root element in Wordnet-LMF, it 
has three children: one GlobalInformation ele-
ment, one or more Lexicon elements, zero or one 
SenseAxes element. This means the object Lexi-
calResource is the container for possibly more 
than one lexicon; inter-lingual correspondences 
are grouped in SenseAxes section. The details are 
presented as follows. 
3.1 Global Inform
The element named as Globa
to describe general information about the lexical 
resource. The attribute ?label? is a free text field. 
Example as follows: 
<GlobalInformation 
Wordnet entries using Wordnet-LMF?> 
3.2  Lexicon 
In CWN-LMF, only one element Lexicon is used 
to contain a monolingual resource as a set of 
LexicalEntry instances followed by a set of Syn-
set elements. The following attributes are speci-
fied: 
 
z languageCoding: It has ?ISO 639-3? as a 
fixed value. 
z language: The standardized 3-letter lan-
guage coding, e.g. zho, is used to spe-
cify the language represented by the 
lexical resource. It is a required 
attribute. 
z owner: It is a required attribute to speci-
fy the copyright holder 
z version: It is a required attribute to speci-
fy the resource version. 
z label: It is used to record additional in-
formation that may be needed. This 
attribute is optional. 
 
Example as follows:  
<Lexicon languageCoding=?ISO 639-3? la-
bel=?Chinese WordNet 1.6? language=?zho?, 
owner=?Academia Sinica?, version=?1.6?>. 
3.2.1 Lexical Entry 
A LexicalEntry element can contain one lemma 
and one sense and has an optional attribute ?id? 
which means a unique identifier.  
The element, Lemma, represents a word form 
chosen by convention to designate the lexical 
entry. It contains the following attributes: 
 
z partOfSpeech: It is a required attribute. 
This attribute takes as its value the 
part-of ?speech value that according 
to WordNet conventions is usually 
specified for a synset. There are four 
part-of-speech notations that are used 
in CWN-LMF. The notation ?n? is 
represented as a noun; the notation 
?v? is represented as a verb; the nota-
tion ?a? is represented as an adjective; 
the notation ?r? is represented as an 
adverb; and the other POS tags are 
represented as ?s?. 
z writtenForm: It is added in case that ?id? 
of LexicalEntry is numerical and it 
takes Unicode strings as values. This 
attribute is optional. 
 
 
125
The Sense element represents one meaning of 
a lexical entry. For WordNet representation, it 
represents the variant of a synset. Required 
attributes are:  
 
z id: It must be specified according to the 
convention used in Chinese WordNet, 
i.e. word_sense#nr.. For example, ??
?_1? means that the first sense of 
lemma ?? huan-jing ?environment?. 
z synset:  It takes as its value the ID of the 
synset to which the particular sense of 
the variant belongs. The ID of the 
synset will be described in the next 
subsection.  
 
Take the first sense of lemma ?? huan-jing 
?environment? for example, it will be represented 
as follows: 
<LexicalEntry> 
  <Lemma writtenForm="??" partOfS-
peech="n"></Lemma> 
     <Sense id="??_1" synset=" zho-16-
06640901-n"></Sense> 
</LexicalEntry> 
3.2.2 Synset 
This element encodes information about a Chi-
nese WordNet synset. Synset elements can con-
tain one Definition, optional SynsetRelations and 
MonolingualExternalRefs elements. Required 
attributes for Synset element are the following: 
 
z id: It is a unique identifier. The agreed 
syntax is ?languageCode-version-id-
POS?.  For example, ?zho-16-
06640901-n? is unique identifier of 
the first sense of lemma ?? huan-
jing ?environment?. 
z baseConcept: Values for the baseCon-
cept attribute will be numerical (1,2,3), 
which correspond to the BaseConcept 
sets. If the sense belongs to the first-
class basic words of NEDO project 
(Tokunaga et al 2006), we encode it 
as 1.  Similarly, if the sense belongs to 
second-class basic words, we encode 
it as 2. The other senses will be en-
coded as 3 if they are not basic words. 
 
The element Definition allows the representa-
tion of the gloss associated with each synset in 
attribute ?gloss?. The required attribute ?exam-
ple? of the element Statement contains the exam-
ples of use associated with the synset . 
SynsetRelations is a bracketing element for 
grouping all SynsetRelation elements. Relations 
between synsets are codified by means of Synse-
tRelation elements, one per relation. Required 
attributes are: 
 
z target: It contains the ID value of the 
synset that is the target of the relation. 
z relType: It means the particular type. 
There are nine semantic relations in 
Chinese WordNet, including 
?has_synonym?, ?has_nearsynonym?, 
?has_hypernym?, ?has_hyponym?, 
?has_holonym?, ?has_meronym?, 
?has_paranym?, ?has_antonym? and 
?has_variant?. Among them, the se-
mantic relation paranymy is used to 
refer to relation between any two lexi-
cal items belonging to the same se-
mantic classification (Huang et al 
2008). For example, the set of 
?spring/summer/fall/winter?   has pa-
ranymy relation of main concept of 
?seasons in a year?. 
 
MonolingualExternalRefs is a bracketing ele-
ment to group all MonolingualExternalRef ele-
ments. MonolingualExternalRef elements must 
be used to represent links between a Sense or 
Synset and other resources, such as an ontology, 
a database or other lexical resources. Attributes 
are: 
 
z  externalSystem: It is a required attribute 
to describe the name of the external 
resource. For instance, possible values 
are ?domain? (Magnini and Cavaglia, 
2000), ?SUMO? (Niles and Pease, 
2001), and ?Wordnet 3.0? for record-
ing SenseKey values.   
z externalReference: It means the particu-
lar identifier or node. This attribute is 
required. 
z relType: It is optional attribute. If the 
?externalSystem? is ?SUMO?. ?rel-
Type? is the type of relations with 
SUMO ontology nodes. Possible val-
ues are ?at?, ?plus?, and ?equal?. 
 
 
 
 
 
126
We use the first sense of lemma ?? huan-
jing ?environment? to illustrate as follows: 
 
<Synset id="zho-16-06640901-n" baseCon-
cept="2"> 
<Definition gloss="?????????? 
????????"> 
<Statement example="???????? 
????????????????? 
???"/> 
</Definition> 
<SynsetRelations> 
<SynsetRelation target="zho-16- 
07029502-n" relType="has_synonym"> 
</SynsetRelation> 
</SynsetRelations> 
<MonolingualExternalRefs> 
<MonolingualExternalRef externalSys 
tem="SUMO" externalRefe 
rence="GeographicArea" rel 
Type="plus"/> 
</MonolingualExternalRefs> 
</Synset> 
 
3.3 SenseAxes 
SenseAxes is a bracketing element that groups 
together SenseAxis elements used for inter-
lingual correspondences. The SenseAxis element 
is a means to group synsets belonging to differ-
ent monolingual wordnets and sharing the same 
equivalent relation to Princeton WordNet 3.0. 
Required attributes are: 
 
z id: It is a unique identifier. 
z relType: It specifies the particular type 
of correspondence among synsets be-
longing to different resources. We use 
?eq_synonym? to represent equal 
synonym relation between Chinese 
Wordnet and Princeton WordNet. 
 
For instance, Chinese synset zho-16-06640901-n 
maps onto English synset eng-30-08567235-n by 
means of an eq_synonym relation. This will be 
represented as follows: 
 
<SenseAxes> 
<SenseAxis id="sa_zho16-eng30_5709" rel 
Type="eq_synonym"> 
<Target ID="zho-16-06640901-n"/> 
<Target ID="eng-30-08567235-n"/> 
</SenseAxis> 
</SenseAxes> 
4 An Example of CWN-LMF Format 
Take ?? zi-ran ?nature? as an example shown 
in Figure 3. ?? has six senses (some of them 
are abridged in the figure). Id attribute of the first 
sense is ??_1 and its synset is called ?zho-16-
03059301-n?. This encoding of synset stands for
??_1 with the unique ID 03059301 in Chinese 
WordNet version 1.6 and its part-of-speech is 
noun. Moreover, one can also learn that??_1 
has a synonym, ???_1 (zho-16-06653601-n). 
Meanwhile, this sense is also corresponded to 
IEEE SUMO. Finally, this compiled CWN-LMF 
version is pointed to Princeton WordNet 3.0, i.e. 
Chinese synset ?zho-16-03059301-n? maps onto 
English synset ?eng-30-11408559-n? by means 
of an eq_synonym relation. 
 
<?xml version=?1.0? encoding=?UTF-8??>
<!DOCTYPE LexicalResource SYSTEM ?kyoto_wn.dtd?>
<LexicalResource>
<GlobalInformation label=?CWN-LMF? />
<Lexicon languageCoding=?ISO 693-3? label=?Chinese  
Wordnet 1.6? language=?zho? owner=?Academia Sinica?
version=?1.6? >
<LexicalEntry>
<Lemma writtenForm=???? partOfSpeech=?n?>
</Lemma>
<Sense id=???_1? synset=?zho-16-03059301-n?>
</Sense>
</LexicalEntry>
?????
<Synset id=?zho-16-03059301-n? baseConcept=?3?>
<Definition gloss=??????????????????>        
<Statement example=???????????????
?????? />
</Definition>
<SynsetRelations>
<SynsetRelation target=?zho-16-06653061-n?
relType=?has_synonym?>
</SynsetRelation>
<MonolingualExternalRefs>
<MonolingualExternalRef externalSystem=?SUMO?
externalReference=?(ComplementFn)InternationalProcess?
relType=?plus? />
</MonolingualExternalRefs>
</Synset>
?????
</Lexion>
<SenseAxes>
<SenseAxis id=?sa_zho16-eng30_17638? relType=?eq_synonym?>
<Target ID=?zho-16-03059301-n?>
<Target ID=?eng-30-11408559-n?>
</SenseAxis>
?????
</SenseAxes>
</LexicalResource>
 
Figure 3: The lemma ?? in  CWN-LMF format. 
5 Quantitative Analysis of CWN-LMF 
There are 8,646 lemmas / 25,961 senses until 
December 2008 have been analyzed in CWN 1.6. 
So far the work on Chinese word distinction is 
still ongoing. It is expected that there are more 
analyzed results in the next released version.  
127
Among analyzed 25,961 senses, there are 268 
senses and 1,217 senses that belong to the first-
class and the second ?class basic words, respec-
tively. When part-of-speech is concerned, we can 
find most of these senses belong to nouns or 
verbs. There are 12,106 nouns, 10,454 nouns, 
806 adjectives and 1,605 adverbs in CWN 1.6 
We further distinguish semantic relations of 
CWN 1.6 and found that there are 3,328 syn-
onyms, 213 near synonyms, 246 hypernyms, 38 
hyponyms, 3 holonyms, 240 paranyms, 369 an-
tonyms and 432 variants, respectively.  
The IEEE SUMO is the only external system 
for monolingual references in CWN-LMF. There 
are 21,925 senses that were pointed to SUMO so 
far. In addition, there are 17,952 senses which 
shared the same equivalent relation to Princeton 
WordNet 3.0 in CWN-LMF. 
 
6 Application Scenarios 
The EU-7 project, KYOTO (Knowledge Yield-
ing Ontologies for Transition-based Organiza-
tion), wants to make knowledge sharable be-
tween communities of people, culture, language 
and computers, by assigning meaning to text and 
giving text to meaning (Vossen et al, 2008a; 
2008b). The goal of KYOTO is a system that 
allows people in communities to define the 
meaning of their words and terms in a shared 
Wiki platform so that it becomes anchored across 
languages and cultures but also so that a comput-
er can use this knowledge to detect knowledge 
and facts in text. 
KYOTO is a generic system offering know-
ledge transition and information across different 
target groups, transgressing linguistic, cultural 
and geographic boundaries. Initially developed 
for the environmental domain, KYOTO will be 
usable in any knowledge domain for mining, or-
ganizing, and distributing information on a glob-
al scale in both European and non-European lan-
guages.   
Whereas the current Wikipedia uses free text 
to share knowledge, KYOTO will represent this 
knowledge so that a computer can understand it. 
For example, the notion of environmental foot-
print will become defined in the same way in all 
these languages but also in such a way that the 
computer knows what information is necessary 
to calculate a footprint. With these definitions it 
will be possible to find information on footprints 
in documents, websites and reports so that users 
can directly ask the computer for actual informa-
tion in their environment, for instance, what is 
the footprint of their town, their region or their 
company. 
KYOTO?s principal components are an ontol-
ogy linked to WordNets in seven different lan-
guages (Basque, Chinese, Dutch, English, Italian, 
Japanese and Spanish). Due to different natures 
of languages, the different designed architectures 
were used to develop WordNets in theses lan-
guages. A unified framework is needed for in-
formation exchange. LMF is hence adopted as 
the framework at lexical semantic level in this 
project. The WordNet in these languages are 
compiled with designed WordNet-LMF format. 
CWN-LMF will also be involved and benefit for 
cross-language interpretabilities in semantic 
search field.  
7 Discussion 
Due to characters of Chinese language, there are 
some difficulties of encoding Chinese WordNet 
into Wordnet-LMF. A brief description is pre-
sented as follows.   
Chinese WordNet was designed for Chinese 
word sense distinction and its lexical semantic 
relationships. The designed architecture belongs 
to word-driven, not synset-driven.  So in CWN-
LMF, we encoded a sense as an individual synset 
and marked up the ?has_synonym? relation when 
senses belong to the same WordNet synset.  
In addition, how to define the basic concept of 
Chinese language is difficult. So far the basic 
word lists of the NEDO project were used as pre-
liminary basis. We need a further method to dis-
tinguish baseConcept attribute of word senses. 
8 Conclusions 
This study describes the design and implementa-
tion of how the Wordnet-LMF used to represent 
lexical semantics in Chinese WordNet. CWN-
LMF is benefit for data exchange among compu-
tational linguistic resources, and also promises a 
convenient uniformity for domain-specific appli-
cations such as KYOTO in cross-language se-
mantic search field.   
Future work is investigated with several direc-
tions. We are planning to release Chinese Word-
Net 1.6 using CWN-LMF format in an xml file, 
including a XML DTD in the following days. In 
addition, the use of this lingual resource for fur-
ther linguistic research is also under investigation. 
 
 
 
128
Acknowledgements  
The authors would like to thank Prof. Claudia 
Soria for her constructive comments. This work 
was funded by National Science Council, Taiwan 
under Grants NSC 97-2923-I-001-001-MY3., 
and also cooperated with EU-FP7 KYOTO 
project. 
References  
CKIP. 2003. Sense and Sensibility Vol. I. Technical 
Report 03-01. Taipei: Academia Sinica. 
Fellbaum, C.. 1998. WordNet: an Electronic Lexical 
Database. The MIT Press. 
Francopoulo, G., Bel, N., George, M., Calzolari, N., 
Monachini, M., Pet, M. and Soria, C.. 2006a. Lexi-
cal Markup Framework (LMF) for NLP Multilin-
gual Resources. Proceedings of COLING-ACL 
Workshop on Multilingual Language Resources 
and Interoperability.  
Francopoulo, G., Bel, N., George, M., Calzolari, N., 
Monachini, M., Pet, M. and Soria, C.. 2006b. LMF 
for Multilingual, Specialized Lexicons. Proceed-
ings of the LREC Workshop on Acquiring and 
Representing Multilingual, Specialized Lexicons: 
the Case of Biomedicine.  
Francopoulo, G., Bel, N., George, M., Calzolari, N., 
Monachini, M., Pet, M. and Soria, C.. 2009. Multi-
lingual Resources for NLP in the Lexical Markup 
Framework (LMF). Language Resource and Eval-
uation. 43:57-70. 
Huang, C.-R., Chang, R.-Y. and Lee, H.-P.. 2004. 
Sinica BOW (Bilingual Ontological Wordnet): In-
tegration of Bilingual WordNet and SUMO. Pro-
ceedings of the 4th International Conference on 
Language Resources and Evaluation. 
Huang, C.-R., Chen, C.-L., Weng, C.-X., Lee, H.-P., 
Chen, Y.-X. and Chen, K.-J.. 2005. The Sinica 
Sense Management System: Design and Implemen-
tation. Computational Linguistics and Chinese 
Language Processing. 10(4): 417-430. 
Huang, C.-R., Hsieh, S.-K., Hong, J.-F., Chen, Y.-Z., 
Su, I.-L., Chen, Y.-X. and Huang, S.-W.. 2008. 
Chinese Wordnet: Design, Implementation, and 
Application of an Infrastructure for Cross-lingual 
Knowledge Processing. Proceedings of the 9th Chi-
nese Lexical Semantics Workshop. 
Huang, C.-R., Lee, H.-P. and Hong, J.-F.. 2004. Do-
main Lexico-Taxonomy: an Approach Towards 
Multi-domain Language Processing. Proceedings 
of the Asian Symposium on Natural Language 
Processing to Overcome Language Barriers. 
Huang, C.-R., Lee, H.-P. and Hong, J.-F.. 2005. The 
Robustness of Domain Lexico-Taxonomy: Ex-
panding Domain Lexicon with Cilin. Proceedings 
of the 4th ACL SIGHAN Workshop on Chinese 
Language Processing. 
Huang, C.-R., Su, I.-L., Hsiao, P.-Y. and Ke, X.-L.. 
2008. Paranymy: Enriching Ontological Know-
ledge in Wordnets. Proceedings of the 4th Global 
WordNet Conference. 
Huang, C.-R., Tsai, D. B.-S., Weng, C.-X., Chu, N.-
X., Ho, W.-R., Huang, L.-H. and Tsai, I.-N.. 2003. 
Sense and Meaning Facet: Criteria and Operational 
Guidelines for Chinese Sense Distinction. Proceed-
ings of the 4th Chinese Lexical Semantics Work-
shop. 
LMF. 2009. Lexical Markup Framework. ISO-24613. 
Geneva:ISO. 
Magnini, B. and Cavaglia, G.. 2000. Integrating Sub-
ject Field Codes into WordNet.  Proceedings of the 
2nd International Conference on Language Re-
sources and Evaluation. 
Miller, G. A.. 1995. WordNet: a Lexical Database for 
English. Communications of the ACM. 38(11): 39-
41. 
Niles, I. and Pease, A.. 2001. Toward a Standard Up-
per Ontology. Proceedings of the 2nd International 
Conference on Formal Ontology in Information 
Systems.  
Soria, C., Monachini, M. and Vossen, P.. 2009. 
Wordnet-LMF: Fleshing out a Standardized For-
mat for Wordnet Interoperability. Proceedings of 
ACM Workshop on Intercultural Collaboration. 
Soria, C., Monachini, M., Bertagna, F., Calzolari, N., 
Huang, C.-R., Hsieh, S.-K., Marchetti, A. and Tes-
coni, M.. 2009. Exploring Interoperability of Lan-
guage Resources: the Case of Cross-lingual Semi-
automatic Enrichment of Wordnets. Language Re-
source and Evaluation. 43:87-96. 
Strapparava, C. and Valitutti, A.. 2004. WordNet-
Affect: an Affective Extension of WordNet. Pro-
ceedings of the 4th International Conference on 
Language Resources and Evaluation. 
Tokuaga, T., Sornlertlamvanich, V., Charoenporn, T., 
Calzolari, N., Monachini, M., Soria, C., Huang, C.-
R., Yu, Y., Yu, H. and Prevot, L.. 2006. Infrastruc-
ture for Standardization of Asian Language Re-
sources. Proceedings of the COLING/ACL Main 
Conference Poster Sessions. 
Vossen, P.. 2004. EuroWordNet: a Multilingual Data-
base of Autonomous and Language-specific Word-
nets Connected via an Inter-Lingual-Index. Interna-
tional Journal of Linguistics. 17(2): 1-23. 
Vossen, P., Agirre, E., Calzolari, N., Fellbaum, C., 
Hsieh, S.-K., Huang, C.-R., Isahara, H., Kanzaki, 
K., Marchetti, A., Monachini, M., Neri, F., Raffael-
li, R., Rigau, G., Tescon, M. and VanGent, J.. 
2008a.   KYOTO: A System for Mining, Structur-
129
ing, and Distributing Knowledge Across Languag-
es and Cultures. Proceedings of 6th International 
Conference on Language Resource and Evaluation.  
Vossen, P., Agirre, E., Calzolari, N., Fellbaum, C., 
Hsieh, S.-K., Huang, C.-R., Isahara, H., Kanzaki, 
K., Marchetti, A., Monachini, M., Neri, F., Raffael-
li, R., Rigau, G., Tescon, M. and VanGent, J.. 
2008b.   KYOTO: A System for Mining, Structur-
ing, and Distributing Knowledge Across Languag-
es and Cultures. Proceedings of the 4th Internation-
al Global WordNet Conference.  
 
 
130
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 145?152,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Query Expansion using LMF-Compliant Lexical Resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Dain Kaplan
Tokyo Inst. of Tech.
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Xia Yingju
Fujitsu R&D Center
Chu-Ren Huang
The Hong Kong Polytec. Univ.
Shu-Kai Hsieh
National Taiwan Normal Univ.
Shirai Kiyoaki
JAIST
Abstract
This paper reports prototype multilin-
gual query expansion system relying on
LMF compliant lexical resources. The
system is one of the deliverables of a
three-year project aiming at establish-
ing an international standard for language
resources which is applicable to Asian
languages. Our important contributions
to ISO 24613, standard Lexical Markup
Framework (LMF) include its robustness
to deal with Asian languages, and its ap-
plicability to cross-lingual query tasks, as
illustrated by the prototype introduced in
this paper.
1 Introduction
During the last two decades corpus-based ap-
proaches have come to the forefront of NLP re-
search. Since without corpora there can be no
corpus-based research, the creation of such lan-
guage resources has also necessarily advanced
as well, in a mutually beneficial synergetic re-
lationship. One of the advantages of corpus-
based approaches is that the techniques used
are less language specific than classical rule-
based approaches where a human analyses the
behaviour of target languages and constructs
rules manually. This naturally led the way
for international resource standardisation, and in-
deed there is a long standing precedent in the
West for it. The Human Language Technol-
ogy (HLT) society in Europe has been particu-
larly zealous in this regard, propelling the cre-
ation of resource interoperability through a se-
ries of initiatives, namely EAGLES (Sanfilippo et
al., 1999), PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Ide et al, 2003), and LIRICS1. These
1http://lirics.loria.fr/
continuous efforts have matured into activities in
ISO-TC37/SC42, which aims at making an inter-
national standard for language resources.
However, due to the great diversity of languages
themselves and the differing degree of technolog-
ical development for each, Asian languages, have
received less attention for creating resources than
their Western counterparts. Thus, it has yet to be
determined if corpus-based techniques developed
for well-computerised languages are applicable on
a broader scale to all languages. In order to effi-
ciently develop Asian language resources, utilis-
ing an international standard in this creation has
substantial merits.
We launched a three-year project to create an
international standard for language resources that
includes Asian languages. We took the following
approach in seeking this goal.
? Based on existing description frameworks,
each research member tries to describe sev-
eral lexical entries and find problems with
them.
? Through periodical meetings, we exchange
information about problems found and gen-
eralise them to propose solutions.
? Through an implementation of an application
system, we verify the effectiveness of the pro-
posed framework.
Below we summarise our significant contribution
to an International Standard (ISO24613; Lexical
Markup Framework: LMF).
1st year After considering many characteristics
of Asian languages, we elucidated the shortcom-
ings of the LMF draft (ISO24613 Rev.9). The
draft lacks the following devices for Asian lan-
guages.
2http://www.tc37sc4.org/
145
(1) A mapping mechanism between syntactic
and semantic arguments
(2) Derivation (including reduplication)
(3) Classifiers
(4) Orthography
(5) Honorifics
Among these, we proposed solutions for (1) and
(2) to the ISO-TC37 SC4 working group.
2nd year We proposed solutions for above the
(2), (3) and (4) in the comments of the Committee
Draft (ISO24613 Rev. 13) to the ISO-TC37 SC4
working group. Our proposal was included in DIS
(Draft International Standard).
(2?) a package for derivational morphology
(3?) the syntax-semantic interface resolving the
problem of classifiers
(4?) representational issues with the richness of
writing systems in Asian languages
3rd year Since ISO 24613 was in the FDIS stage
and fairly stable, we built sample lexicons in Chi-
nese, English, Italian, Japanese, and Thai based
on ISO24613. At the same time, we implemented
a query expansion system utilising rich linguis-
tic resources including lexicons described in the
ISO 24613 framework. We confirmed that a sys-
tem was feasible which worked on the tested lan-
guages (including both Western and Asian lan-
guages) when given lexicons compliant with the
framework. ISO 24613 (LMF) was approved by
the October 2008 ballot and published as ISO-
24613:2008 on 17th November 2008.
Since we have already reported our first 2 year
activities elsewhere (Tokunaga and others, 2006;
Tokunaga and others, 2008), we focus on the
above query expansion system in this paper.
2 Query expansion using
LMF-compliant lexical resources
We evaluated the effectiveness of LMF on a mul-
tilingual information retrieval system, particularly
the effectiveness for linguistically motivated query
expansion.
The linguistically motivated query expansion
system aims to refine a user?s query by exploiting
the richer information contained within a lexicon
described using the adapted LMF framework. Our
lexicons are completely complaint with this inter-
national standard. For example, a user inputs a
keyword ?ticket? as a query. Conventional query
expansion techniques expand this keyword to a
set of related words by using thesauri or ontolo-
gies (Baeza-Yates and Ribeiro-Neto, 1999). Using
the framework proposed by this project, expand-
ing the user?s query becomes a matter of following
links within the lexicon, from the source lexical
entry or entries through predicate-argument struc-
tures to all relevant entries (Figure 1). We focus
on expanding the user inputted list of nouns to rel-
evant verbs, but the reverse would also be possible
using the same technique and the same lexicon.
This link between entries is established through
the semantic type of a given sense within a lexical
entry. These semantic types are defined by higher-
level ontologies, such as MILO or SIMPLE (Lenci
et al, 2000) and are used in semantic predicates
that take such semantic types as a restriction ar-
gument. Since senses for verbs contain a link to
a semantic predicate, using this semantic type, the
system can then find any/all entries within the lexi-
con that have this semantic type as the value of the
restriction feature of a semantic predicate for any
of their senses. As a concrete example, let us con-
tinue using the ?ticket? scenario from above. The
lexical entry for ?ticket? might contain a semantic
type definition something like in Figure 2.
<LexicalEntry ...>
<feat att="POS" val="N"/>
<Lemma>
<feat att="writtenForm"
val="ticket"/>
</Lemma>
<Sense ...>
<feat att="semanticType"
val="ARTIFACT"/>
...
</Sense>
...
</LexicalEntry>
Figure 2: Lexical entry for ?ticket?
By referring to the lexicon, we can then derive
any actions and events that take the semantic type
?ARTIFACT? as an argument.
First all semantic predicates are searched for ar-
guments that have an appropriate restriction, in
this case ?ARTIFACT? as shown in Figure 3, and
then any lexical entries that refer to these predi-
cates are returned. An equally similar definition
would exist for ?buy?, ?find? and so on. Thus,
by referring to the predicate-argument structure of
related verbs, we know that these verbs can take
146
<LexicalEntry ...>
  <feat att="POS" val="Noun"/>
  <Lemma>
    <feat att="writtenForm" val="ticket"/>
  </Lemma>
  <Sense ...>
    <feat att="semanticType" val="ARTIFACT"/>
    ...
  </Sense>
  ...
</LexicalEntry>
User Inputs
ticket
<Sense>
<SemanticFeature>
Semantic Features of type 
"restriction" that take 
Sense's semanticType
All senses for 
matched nouns
<SemanticPredicate 
  id="pred-sell-1">
  <SemanticArgument>
    <feat att="label" val="X"/>
    <feat att="semanticRole" val="Agent"/>
    <feat att="restriction" val="Human"/>
  </SemanticArgument>
  ...
  <SemanticArgument>
    <feat att="label" val="Z"/>
    <feat att="semanticRole" val="Patient"/>
    <feat att="restriction" 
          val="ARTIFACT,LOCATION"/>
  </SemanticArgument>
</SemanticPredicate>
All Semantic Predicates 
that contain matched 
Semantic Features
<Sense>
Senses that use matched 
Semantic Predicates
<LexicalEntry ...>
  <feat att="POS" val="Verb"/>
  <Lemma>
    <feat att="writtenForm" val="sell"/>
  </Lemma>
  <Sense id="sell-1" ...>
    ...
    <PredicativeRepresentation
      predicate="pred-sell-1" ...>
  </Sense>
</LexicalEntry>
<LexicalEntry>
<SemanticPredicate>
<LexicalEntry>
System outputs
"sell", ...
For each <Sense> find all 
<SemanticArgument> that 
take this semanticType as 
a feature of type 
"restriction"
Find all verbs <LexicalEntry> 
that use these 
<SemanticPredicate>
All verbs that have 
matched Senses
Figure 1: QE Process Flow
147
<LexicalEntry ...>
<feat att="POS" val="V"/>
<Lemma>
<feat att="writtenForm"
val="sell"/>
</Lemma>
<Sense id="sell-1" ...>
<feat att="semanticType"
val="Transaction"/>
<PredicativeRepresentation
predicate="pred-sell-1"
correspondences="map-sell1">
</Sense>
</LexicalEntry>
<SemanticPredicate id="pred-sell-1">
<SemanticArgument ...>
...
<feat att="restriction"
val="ARTIFACT"/>
</SemanticArgument>
</SemanticPredicate>
Figure 3: Lexical entry for ?sell? with its semantic
predicate
?ticket? in the role of object. The system then re-
turns all relevant entries, here ?buy?, ?sell? and
?find?, in response to the user?s query. Figure 1
schematically shows this flow.
3 A prototype system in detail
3.1 Overview
To test the efficacy of the LMF-compliant lexi-
cal resources, we created a system implementing
the query expansion mechanism explained above.
The system was developed in Java for its ?com-
pile once, run anywhere? portability and its high-
availability of reusable off-the-shelf components.
On top of Java 5, the system was developed us-
ing JBoss Application Server 4.2.3, the latest stan-
dard, stable version of the product at the time of
development. To provide fast access times, and
easy traversal of relational data, a RDB was used.
The most popular free open-source database was
selected, MySQL, to store all lexicons imported
into the system, and the system was accessed, as a
web-application, via any web browser.
3.2 Database
The finalised database schema is shown in Fig-
ure 4. It describes the relationships between en-
tities, and more or less mirrors the classes found
within the adapted LMF framework, with mostly
only minor exceptions where it was efficacious for
querying the data. Due to space constraints, meta-
data fields, such as creation time-stamps have been
left out of this diagram. Since the system also al-
lows for multiple lexicons to co-exist, a lexicon id
resides in every table. This foreign key has been
highlighted in a different color, but not connected
via arrows to make the diagram easier to read. In
addition, though in actuality this foreign key is not
required for all tables, it has been inserted as a con-
venience for querying data more efficiently, even
within join tables (indicated in blue). Having mul-
tiple lexical resources co-existing within the same
database allows for several advantageous features,
and will be described later. Some tables also con-
tain a text id, which stores the original id attribute
for that element found within the XML. This is
not used in the system itself, and is stored only for
reference.
3.3 System design
As mentioned above, the application is deployed
to JBoss AS as an ear-file. The system it-
self is composed of java classes encapsulating
the data contained within the database, a Pars-
ing/Importing class for handling the LMF XML
files after they have been validated, and JSPs,
which contain HTML, for displaying the inter-
face to the user. There are three main sections
to the application: Search, Browse, and Config-
ure. Explaining last to first, the Configure section,
shown in Figure 5, allows users to create a new
lexicon within the system or append to an exist-
ing lexicon by uploading a LMF XML file from
their web browser, or delete existing lexicons that
are no longer needed/used. After import, the data
may be immediately queried upon with no other
changes to system configuration, from within both
the Browse and Search sections. Regardless of
language, the rich syntactic/semantic information
contained within the lexicon is sufficient for car-
rying out query expansion on its own.
The Browse section (Figure 6) allows the user to
select any available lexicon to see the relationships
contained within it, which contains tabs for view-
ing all noun to verb connections, a list of nouns, a
list of verbs, and a list of semantic types. Each has
appropriate links allowing the user to easily jump
to a different tab of the system. Clicking on a noun
takes them to the Search section (Figure 7). In this
section, the user may select many lexicons to per-
form query extraction on, as is visible in Figure 7.
148
semantic_link 
VARCHAR (64)
sense
sense_id
PRIMARY KEY
synset_id
FOREIGN KEY
syn_sem_correspondence_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_type
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
semantic_predicate_id
PRIMARY KEY
semantic_predicate
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
semantic_argument_id
PRIMARY KEY
semantic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
semantic_feature_id
PRIMARY KEY
semantic_feature
lexicon_id
FOREIGN KEY
semantic_argument_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_predicate_to_argument
lexicon_id
FOREIGN KEY
semantic_feature_id
FOREIGN KEY
semantic_argument_id 
FOREIGN KEY
semantic_argument_to_feature
description
TEXT
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
synset_id
PRIMARY KEY
synset
written_form
VARCHAR (64) NOT NULL
part_of_speech
ENUM( 'Verb', 'Noun' , 'Unknown')
lexical_entry
text_id
VARCHAR (64)
entry_id 
PRIMARY KEY
lexicon_id 
FOREIGN KEY
semantic_feature
FOREIGN KEY
syntactic_feature
FOREIGN KEY
lexicon_id
FOREIGN KEY
argument_map_id
PRIMARY KEY
syn_sem_argument_map
lexicon_id
FOREIGN KEY
argument_map_id
FOREIGN KEY
syn_sem_correspondence_id 
FOREIGN KEY
syn_sem_correspondence_to_map
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syn_sem_correspondence_id
PRIMARY KEY
syn_sem_correspondence
lexicon_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_sense
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
frame_id
PRIMARY KEY
subcat_frame
lexicon_id
FOREIGN KEY
frame_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_subcat_frame
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syntactic_argument_id
PRIMARY KEY
syntactic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
syntactic_feature_id
PRIMARY KEY
syntactic_feature
lexicon_id
FOREIGN KEY
syntactic_argument_id
FOREIGN KEY
frame_id
FOREIGN KEY
subcat_frame_to_argument
lexicon_id
FOREIGN KEY
syntactic_feature_id
FOREIGN KEY
syntactic_argument_id 
FOREIGN KEY
syntactic_argument_to_feature
description
VARCHAR(128)
language
VARCHAR(64)
lexicon_id
PRIMARY KEY
lexicon
relation_type 
VARCHAR (64)
lexicon_id
FOREIGN KEY
related_sense_id
FOREIGN KEY
sense_id
FOREIGN KEY
sense_relation
Figure 4: Database schema
Figure 5: QE System - Configure Figure 6: QE System - Browse
149
Figure 7: QE System - Search
3.4 Semantic information
This new type of query expansion requires rich
lexical information. We augmented our data using
the SIMPLE ontology for semantic types, using
the same data for different languages. This had
the added benefit of allowing cross-language ex-
pansion as a result. In steps two and three of Fig-
ure 1 when senses are retrieved that take specific
semantic types as arguments, this process can be
done across all (or as many as are selected) lex-
icons in the database. Thus, results such as are
shown in Figure 7 are possible. In this figure the
Japanese word for ?nail? is entered, and results for
both selected languages, Japanese and Italian, are
returned. This feature requires the unification of
the semantic type ontology strata.
3.5 Possible extension
Next steps for the QE platform are to explore the
use of other information already defined within the
adapted framework, specifically sense relations.
Given to the small size of our sample lexicon, data
sparsity is naturally an issue, but hopefully by ex-
ploring and exploiting these sense relations prop-
erly, the system may be able to further expand a
user?s query to include a broader range of selec-
tions using any additional semantic types belong-
ing to these related senses. The framework also
contains information about the order in which syn-
tactic arguments should be placed. This informa-
tion should be used to format the results from the
user?s query appropriately.
4 An Additional Evaluation
We conducted some additional query expansion
experiments using a corpus that was acquired from
Chinese LDC (No. ?2004-863-009?) as a base (see
below). This corpus marked an initial achievement
in building a multi-lingual parallel corpus for sup-
porting development of cross-lingual NLP appli-
cations catering to the Beijing 2008 Olympics.
The corpus contains parallel texts in Chinese,
English and Japanese and covers 5 domains that
are closely related to the Olympics: traveling, din-
ing, sports, traffic and business. The corpus con-
sists of example sentences, typical dialogues and
articles from the Internet, as well as other language
teaching materials. To deal with the different lan-
guages in a uniform manner, we converted the cor-
pus into our proposed LMF-compliant lexical re-
sources framework, which allowed the system to
expand the query between all the languages within
the converted resources without additional modifi-
cations.
As an example of how this IR system func-
tioned, suppose that Mr. Smith will be visiting
Beijing to see the Olympic games and wants to
know how to buy a newspaper. Using this system,
he would first enter the query ?newspaper?. For
this query, with the given corpus, the system re-
turns 31 documents, fragments of the first 5 shown
below.
(1) I?ll bring an English newspaper immediately.
(2) Would you please hand me the newspaper.
(3) There?s no use to go over the newspaper ads.
(4) Let?s consult the newspaper for such a film.
(5) I have little confidence in what the newspa-
pers say.
Yet it can be seen that the displayed results are not
yet useful enough to know how to buy a newspa-
per, though useful information may in fact be in-
cluded within some of the 31 documents. Using
the lexical resources, the query expansion module
suggests ?buy?, ?send?, ?get?, ?read?, and ?sell?
as candidates to add for a revised query.
Mr. Smith wants to buy a newspaper, so he se-
lects ?buy? as the expansion term. With this query
the system returns 11 documents, fragments of the
first 5 listed below.
(6) I?d like some newspapers, please.
150
(7) Oh, we have a barber shop, a laundry, a store,
telegram services, a newspaper stand, table
tennis, video games and so on.
(8) We can put an ad in the newspaper.
(9) Have you read about the Olympic Games of
Table Tennis in today?s newspaper, Miss?
(10) newspaper says we must be cautious about
tidal waves.
This list shows improvement, as information about
newspapers and shopping is present, but still ap-
pears to lack any documents directly related to
how to buy a newspaper.
Using co-occurrence indexes, the IR system
returns document (11) below, because the noun
?newspaper? and the verb ?buy? appear in the
same sentence.
(11) You can make change at some stores, just buy
a newspaper or something.
From this example it is apparent that this sort
of query expansion is still too naive to apply to
real IR systems. It should be noted, however, that
our current aim of evaluation was in confirming
the advantage of LMF in dealing with multiple
languages, for which we conducted a similar run
with Chinese and Japanese. Results of these tests
showed that in following the LMF framework in
describing lexical resources, it was possibile to
deal with all three languages without changing the
mechanics of the system at all.
5 Discussion
LMF is, admittedly, a ?high-level? specification,
that is, an abstract model that needs to be fur-
ther developed, adapted and specified by the lex-
icon encoder. LMF does not provide any off-the-
shelf representation for a lexical resource; instead,
it gives the basic structural components of a lexi-
con, leaving full freedom for modeling the partic-
ular features of a lexical resource. One drawback
is that LMF provides only a specification manual
with a few examples. Specifications are by no
means instructions, exactly as XML specifications
are by no means instructions on how to represent
a particular type of data.
Going from LMF specifications to a true instan-
tiation of an LMF-compliant lexicon is a long way,
and comprehensive, illustrative and detailed ex-
amples for doing this are needed. Our prototype
system provides a good starting example for this
direction. LMF is often taken as a prescriptive
description, and its examples taken as pre-defined
normative examples to be used as coding guide-
lines. Controlled and careful examples of conver-
sion to LMF-compliant formats are also needed to
avoid too subjective an interpretation of the stan-
dard.
We believe that LMF will be a major base
for various SemanticWeb applications because it
provides interoperability across languages and di-
rectly contributes to the applications themselves,
such as multilingual translation, machine aided
translation and terminology access in different lan-
guages.
From the viewpoint of LMF, our prototype
demonstrates the adaptability of LMF to a rep-
resentation of real-scale lexicons, thus promoting
its adoption to a wider community. This project
is one of the first test-beds for LMF (as one of
its drawbacks being that it has not been tested on
a wide variety of lexicons), particularly relevant
since it is related to both Western and Asian lan-
guage lexicons. This project is a concrete attempt
to specify an LMF-compliant XML format, tested
for representative and parsing efficiency, and to
provide guidelines for the implementation of an
LMF-compliant format, thus contributing to the
reduction of subjectivity in interpretation of stan-
dards.
From our viewpoint, LMF has provided a for-
mat for exchange of information across differently
conceived lexicons. Thus LMF provides a stan-
dardised format for relating them to other lexical
models, in a linguistically controlled way. This
seems an important and promising achievement in
order to move the sector forward.
6 Conclusion
This paper described the results of a three-year
project for creating an international standard for
language resources in cooperation with other ini-
tiatives. In particular, we focused on query expan-
sion using the standard.
Our main contribution can be summarised as
follows.
? We have contributed to ISO TC37/SC4 ac-
tivities, by testing and ensuring the portabil-
ity and applicability of LMF to the devel-
opment of a description framework for NLP
lexicons for Asian languages. Our contribu-
tion includes (1) a package for derivational
151
morphology, (2) the syntax-semantic inter-
face with the problem of classifiers, and (3)
representational issues with the richness of
writing systems in Asian languages. As of
October 2008, LMF including our contribu-
tions has been approved as the international
standard ISO 26413.
? We discussed Data Categories necessary
for Asian languages, and exemplified sev-
eral Data Categories including reduplication,
classifier, honorifics and orthography. We
will continue to harmonise our activity with
that of ISO TC37/SC4 TDG2 with respect to
Data Categories.
? We designed and implemented an evaluation
platform of our description framework. We
focused on linguistically motivated query ex-
pansion module. The system works with lexi-
cons compliant with LMF and ontologies. Its
most significant feature is that the system can
deal with any language as far as the those lex-
icons are described according to LMF. To our
knowledge, this is the first working system
adopting LMF.
In this project, we mainly worked on three
Asian languages, Chinese, Japanese and Thai, on
top of the existing framework which was designed
mainly for European languages. We plan to dis-
tribute our results to HLT societies of other Asian
languages, requesting for their feedback through
various networks, such as the Asian language re-
source committee network under Asian Federation
of Natural Language Processing (AFNLP)3, and
the Asian Language Resource Network project4.
We believe our efforts contribute to international
activities like ISO-TC37/SC45 (Francopoulo et al,
2006).
Acknowledgments
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley.
3http://www.afnlp.org/
4http://www.language-resource.net/
5http://www.tc37sc4.org/
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006.
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
A. Sanfilippo, N. Calzolari, S. Ananiadou,
R. Gaizauskas, P. Saint-Dizier, and P. Vossen.
1999. EAGLES recommendations on semantic
encoding. EAGLES LE3-4244 Final Report.
T. Tokunaga et al 2006. Infrastructure for standard-
ization of Asian language resources. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 827?834.
T. Tokunaga et al 2008. Adapting international stan-
dard for asian language technologies. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08).
152
Coling 2010: Poster Volume, pages 937?945,
Beijing, August 2010
Word Space Modeling for Measuring Semantic Specificity in Chinese
Ching-Fen Pan
Department of English
National Taiwan Normal University
debbychingxp@hotmail.com
Shu-Kai Hsieh
Department of English
National Taiwan Normal University
shukai@gmail.com
Abstract
The aim of this study is to use the
word-space model to measure the seman-
tic loads of single verbs, profile verbal
lexicon acquisition, and explore the se-
mantic information on Chinese resulta-
tive verb compounds (RVCs). A distri-
butional model based on Academia Sinica
Balanced Corpus (ASBC) with Latent Se-
mantic Analysis (LSA) is built to investi-
gate the semantic space variation depend-
ing on the semantic loads/specificity. The
between group comparison of age-related
changes in verb style is then conducted
to suggest the influence of semantic space
on verbal acquisition. Finally, it demon-
strates how meaning exploring on RVCs
is done with semantic space.
1 Introduction
The issue of ?word space? has been gaining atten-
tion in the field of distributional semantics, cogni-
tive and computational linguistics. Various meth-
ods have been proposed to approximate words?
meanings from linguistic distance. One of the
most popular models in distributional semantics is
Latent Semantic Analysis (LSA) with dimension-
reduction technique, Singular Value Decomposi-
tion (SVD)(Landauer and Dumais, 1997; Karl-
gren and Sahlgren, 2001; Sahlgren, 2002; Wid-
dows et al, 2002). The backbone of LSA is
the co-occurrence distributional model in which
words are conceived as points scattered in a texts-
built n-dimensional space(Lenci, 2008). Rather
than trying to predict the best performing model
from a set of models, this study highlights the ex-
tent to which word space or semantic space mea-
sured from a vector-based model can access the
verbal semantics and has influence on verbal ac-
quisition.
This paper is organized as follows: Section 2 pro-
files the variation of semantic space affected by
the semantic loads of single verbs. Section 3
discusses the correlation between the developing
change in verbal lexicon and word space from the
experimental data collected by M31 project. It
will reveal how semantic space facilitates early
child verbal learning. Section 4 demonstrates how
to assess the meaning of Chinese resultative verb
compounds (RVCs) from semantic space. The re-
sults of this work are finally concluded in Section
5.
2 The Variation of Semantic Space
Between Two Verb Types (G/S) in LSA
The goal of this section is to examine the seman-
tic variation between two verb types, generic ver-
sus specific verbs. It first creates a taxonomy for
the classification of various verb groups (generic
verbs versus specific verbs) based on the seman-
tic distance with Latent Semantic Analysis (LSA)
and Cluster Analysis.
2.1 Distributional Model Based on Sinica
Corpus
The distributional model built in this survey is
based on the Chinese texts collected in Academia
1Model and Measurement of Meaning: A Cross-lingual
and Multi-disciplinary Approach of French and Mandarin
Verbs based on Distance in Paradigmatic Graphs. Project
website: http://140.112.147.149:81/m3/
937
Sinica Balanced Corpus (ASBC)2. It includes 190
files containing about 96000 word types3. The
original matrix (M ) is further decomposed into
the product of three matrices (TSDT ). These ma-
trices are then reduced into k dimensions. In the
following reconstruction process based on k di-
mensions, it multiplies out the truncated matrices
TkSkD?k and then gets a Mk matrix (the approx-
imation of X)(Landauer et al, 1998; Sahlgren,
2005; Widdows and Ferraro, 2008). The follow-
ing shows an example of finding the nearest neigh-
bors of the word da (? / to hit) via two methods
(see Table 1). For the convenience of visualization
and cluster analysis, Euclidean distance is applied
in the following study.
qu ?go? na ?take? zhao ?find?
Cosine 0.928 0.926 0.920
Distance 0.377 0.382 0.397
Table 1: Associating words of da ?hit?.
2.2 Semantic Clustering
The primary objective of cluster analysis is to ex-
amine the formation of a taxonomy: whether G
verbs and S verbs form two groups separately.
The clusters also help us grasp the semantic space
among verbs as well as the potential semantic re-
lation of them. Based on the distance matrix of
lexical items generated in the last section, this
part applied cluster analysis on the selected 150
verbs/observations4. For the convenience of com-
parison, each verb is coded with its type and a se-
rial number like zuo (?/ to do) is G1 and si (?/
2ASBC website:
http://dbo.sinica.edu.tw/ftmsbin/kiwi1/mkiwi.sh
3The hapax legomena (words occur only once in the
whole data) are not included in the matrix. The total word
types including hapax amount to 220000 or so. To avoid time
and computer consuming, we excluded those hapax from the
co-occurrence matrix.
4These 150 verbs are single verbs selected from the ex-
perimental data. In the previous study of classification,
these verbs are divided into two types (G:generic versus
S:specific). There are 78 G verbs and 45 S verbs, along with
27 U(undetermined) verbs. It is noticeable that U verbs do
not count as one type of verbs. They are floating verbs be-
tween G and S. We keep their identity as U and examine their
potential characteristics in a binary cluster analysis.
to tear) is S275.
Once the similarity measure is done, the
next procedure is to combine similar verbs into
groups. The clustering procedure starts with
each verb/observation in its own cluster, and
combines two clusters together step by step until
all the verbs are in a single cluster6 The cluster
dendrogram is plotted is Figure 1, in which
clusters are formed from the bottom to the top.
Figure 1 demonstrates that the highest split
separates these verbs into two big groups: the
left branch group and right branch group drawn
in different squares. The constituents of the two
branches are listed in Table 2. It is clear that most
of the constituent parts of the left group are G
verbs whereas S verbs count as majority in the
right group. If the left group is considered as a
group formed with G verbs and right group with
S verbs, the hit ratio7 of G verbs (74.6%) is much
higher than that of S verbs (57.1%). The cluster-
ing algorithm that we applied shows some struc-
ture, but there is no accurate separation of these
two verb types. A detailed investigation of the re-
lationship between the verb type and the distance
is discussed in the next section.
left group right group
Generic verbs 59 (64.1%) 18 (33.3%)
Specific verbs 20 (21.7%) 24 (44.5%)
Undetermined verbs 13 (14.1%) 12 (22.2%)
Hit ratio 74.6% 57.1%
Table 2: Distribution of G/S verbs in two big clus-
ters.
5In fact, only 146 of 150 verbs are being classified be-
cause four words are missed in Sinica Corpus. To avoid con-
fusion, we still call them 150 verbs in cluster analysis.
6Agglomerative method is implemented in the process
in which single points are agglomerated into larger groups.
This is termed a hierarchical cluster procedure that explores
the co-relational structure of these single verbs. In complete
linkage, all objects in a cluster are linked to each other with
the longest distance. The use of the longest distance in com-
plete linkage makes the least similar pair of objects group
together. In other words, the maximum distance of the group
results from the linkage of objects with minimum similarity.
7The hit ratio is calculated as follows:
hit ratio of G in the left group: 59/(59 + 20) = 74.6%
hit ratio of S in the right group: 24/(18 + 24) = 57.1%
It is noticeable that U verbs are temporarily ignored here.
938
2.3 Distance Variation in Small-G/S-clusters
Following the line of argumentation, this section
demonstrates how distance varies within small-G-
clusters and small-S-clusters. In order to examine
the distance difference, small-G-cluster (or small-
S-cluster) is defined as a cluster formed with the
nearest twenty words of the G verb (or S verb) tar-
get.8 In the example of one G verb yong (?/use)
coded as G5, the closest twenty words are almost
G verbs and the only one S verb is the farthest
word xie (?/write) (see Figure 2). The distance
examination of the small cluster is applied to all
of the 150 verbs studied in this survey. Table 3
has illustrated the comparison of verb types and
the distance in the small cluster. As expected, the
semantic distance is significantly affected by the
verb type of the target word in the small cluster.
The distances among words in most of the small-
G-clusters range between 0.4 and 0.8. In contrast,
over eighty percent small-S-clusters obtain a dis-
tance from 0.8 to 1.2. As for those U verbs which
can not be decided as generic or specific in the
manual tagging because of the lacking of agree-
ment, they have distance between 0.6 and 1. Their
distance shows an overlap with part of G verbs and
part of S verbs. It confirms that U verbs are in a
fuzzy zone between G verbs and S verbs.
In summary, G verbs are words with more
senses and they appear more frequently in various
context. Based on their high frequency distribu-
tion, G verbs construct a solid relation with each
other in small-G-clusters. In contrast, S verbs are
8In order to test the representative power of small-clusters
with 20 words, we have examined the clusters with 25 and
30 words as well. In all of the cases, the curves in 20-word
cluster don?t change significantly when the sample size is set
to 25 or 30. The small-G/S-clusters with the sample size
(N=20) is justified as representative.
Figure 2: The small-G-cluster of yong (?/use).
words with restricted meanings and they have rel-
atively limited distributional patterns. Due to their
low variety of patterns, S verbs are not easy to
have tight relations with other words. It shows
that words with generic meaning have high dis-
tribution variety and the distances among them
are much shorter. The lack of polysemous fea-
ture makes the specific verbs be short of vari-
ous distributional patterns and lose the opportu-
nities to form close semantic relation with oth-
ers. The semantic space among G verbs is short
enough to form a solid cluster whereas S verbs
are relatively remote from each other in seman-
tic space. The distance of each verb cluster can
help assess the verb category as generic (G) or
specific (S). Approximately 75% of generic verbs
form small clusters with distance lower than 0.8
while more than 80% of specific verbs acquire a
Figure 1: Agglomerative hierarchical cluster analysis of 150 verbs.
939
distance greater than 0.8 . As to the verbs of inde-
terminacy, they are averagely scattered in a fuzzy
zone between G and S verbs. Over 70% U verbs
are centering the distance 0.8, which suggests that
words near distance 0.8 are likely to be undeter-
mined verbs. This analysis has proved that seman-
tic space varies in accordance with verb?s meaning
specificity. The distributions in context represent
not only the linguistic behaviors but the semantic
contents of lexical items.
3 The Influence of Specificity on
Acquisition
This section assesses the influence of semantic
space on the acquisition of the verbal lexicon.
With the examination of Specific verb (S verb)
progress, this study proposes that Generic verbs
(G verbs) are acquired earlier than S verbs due
to the closer semantic space. It also testifies
whether the S verb development is a developing
trend parallel with the acquisition of conventional
verbs(Chen et al, 2008; Hsieh et al, 2009)9 from
the experimental data collected by M3 project.
Based on the developing trend of conventional
lexical items, the following parts analyze the
relation of meaning specificity and the acquisition
of lexical items.
3.1 Decreasing in Lexical Variation
The section is concerned with lexical variation
among participants within the same age group.
9They rearranged the five groups of participants into three
units and then investigated the learning trend by Replacing
Rate (Frequency of V 2freq / Frequency of V 1freq ). By
defining adults? usages as the conventional one called V1,
children?s second highest frequency verb is counted as V2.
Along with the increase of age, the number of V2 drops
slowly whereas the amount of V1 increases gradually.
It measures type-token ratios of each group
and profiles the lexical variation10 in verbal
acquisition. Data analyzed in this part include
five groups of respondents? usages of verbs to
four different films, each of which pictures one
event. Respondents are assigned into five groups
according to their age: 3-year-old, 5-year-old,
7-year-old, and 9-year-old groups have 20 respon-
dents separately while 60 respondents are in the
Adult group composed of people in their twenties.
In respondents? answers, only one single verb is
extracted from each respondent in this study. The
number of verbs in each group is equal to the
amount of participants. The first analysis begins
with the lexical variation or lexical flexibility
in these five groups. It is done with the ratio
of lexical variation: the amount of word type is
divided by the amount of word token, as shown
in Table 4. The greater number of the ratio means
the lexical variation is more abundant and the
smaller ratio means a low diversity of word types.
The ratio of lexical variation in these four films all
show a decreasing trend from 3-year-old groups
to adult groups. The quantity of different verbs
is higher in children group (3y, 5y,7y, 9y) than
that in adult group. That is, children appear more
creative in event description tasks while adults
are confined in the conventional usage. With the
decreasing trend of lexical variety, the next step
is to propose an increasing trend of specific verb
10Lexical diversity or sometimes called lexical variation
is used to mean a combination of lexical variation and lex-
ical sophistication. It is also referred to an indication of a
combination of vocabulary size and the ability to use it ef-
fectively(Malvern et al, 2004). However, lexical variation or
lexical diversity doesn?t mean lexical richness in this study.
In other kinds of experiment like writing tests, adults should
perform better than children in lexical diversity. But the ex-
perimental data applied in this study is action-naming task.
The trend of lexical variation may perform in an opposite
way.
Distance 0.4-0.6 0.6-0.8 0.8-1.0 1.0-1.2
Small-G-cluster 24 (31.2%) 32 (41.6%) 17 (22.0%) 4 (5.2%)
Total:72.8% Total:27.2%
Small-S-cluster 0 (0) 6 (13.6%) 19 (43.2%) 19 (43.2%)
Total:13.6% Total:86.4%
Small-U-cluster 1 (4%) 8 (32%) 11 (44%) 5 (20%)
Table 3: Comparison of verb types (G/S) and semantic distance within small cluster.
940
usage when the age raises. It will show that the
change is from various generic verbs to one or
two specific verbs rather than various specific
verbs.
Films carrot-
peel
paper-
crumple
plank-
saw
glass-
break
3y 0.35 0.55 0.2 0.33
5y 0.25 0.47 0.2 0.2
7y 0.3 0.2 0.25 0.1
9y 0.21 0.105 0.157 0.157
Adult 0.016 0.083 0.066 0.066
Table 4: The ratio of lexical variation (ratio =
word type/word token).
3.2 Increasing in Specific Verbs
With regard to the aim of the investigation, the
findings reported above provide evidence of the
changing trend of lexical variety in action-naming
tasks. The next step is to discover the developing
trend of verb type (G/S) usage. According to the
annotation result of verb category, each verb in the
data is now transferred into either generic (label as
G or 1) or specific (S or -1) and the proportions of
S verbs is plotted as Figure 3.
3.2.1 The Non-proportionality of S Verb
among Age Groups
A closer investigation is then implemented for
non-proportionalities by chi-squared test(Baayen,
2008). Although the proportion of S verb changes
more or less in different groups, it is still need to
confirm that whether S verbs are more frequently
used by adults than children. The hypothesis is
formulated as follows:
H0: The proportions of the two verb
types (G verb vs. S verb ) do NOT vary
in five age groups.
With Pearson?s chi-square test for four sets
of data. It is reported that the small p-values
(9.779e-07, 1.324e-09, and 1.191e-13) in the
first three sets of data (carrot-peel (f 6), paper-
crumple (f 2), and plank-saw (f 16)) suggest a
non-proportionality of S verb in different age
groups. However, the p-value (0.8467) obtained
in the last data set (glass-break (f 3)) is too
Figure 3: The proportion of S (-1) verbs to G (1)
verbs from 5 groups of respondents to four events.
large to suggest a significant variation of S verb
proportion in different age groups. It proves
that the proportions of S verb change with the
participant?s age in the three event-naming tasks
but that doesn?t happen in the glass-break (f 3)
event. Except for the data in glass-break (f 3)
event, the null hypothesis doesn?t hold in the
analysis.
3.2.2 The Relationship between S Verb and
Age
In order to test the correlation of S verb
proportion and age variation, four groups (3y, 5y,
7y, 9y) are merged into one group called Child
versus Adult group. The data are now represented
by two by two contingency tables with one
categorical dependent variable (verb types) and
one categorical independent variable (age). Here
summarizes the hypothesis:
H0: The frequency of the two verb types
(G verb vs. S verb, the dependent
variable) do NOT vary depending on
participants? age (Child vs. Adult,
the independent variable).
The result has shown that the small p-values
(2.803e-05, 0.001225, 1.754e-12) verify the
significant difference of S verb in Child group
941
and Adult group with regard to the three data
sets in carrot-peel (f 6), paper-crumple (f 2), and
plank-saw (f 16). Along with the correlation
examination, the effect size is revealed with
correlation coefficient from 0 (no correlation) to
1 (perfect correlation)(Gries, 2009). According
to the Phi value in this table, only the data in
plank-saw (f 16) has a correlation coefficient
(0.612) greater than 0.5. That is, the correlation
between S verb usage and age group is considered
as significantly correlated in the one data set
(plank-saw (f 16)). As for the other two data sets
(carrot-peel (f 6) with phi:0.379, paper-crumple
(f 2) with phi: 0.297), the correlation is not
particularly strong but it is still highly significant.
Over half of the data sets exhibit a significant
non-proportionality of S verb usage in different
age groups but the correlation of S verb and
participants? age requires.
In relation to the aim of this study, it has shown
that meaning specificity functions as a factor in
the development of verbal lexicon. The results of
the analysis also show a significant variety of S
verb between children and adults. It is plausible
to suppose that verbs with specific meaning are
acquired later than those with generic meanings.
This developing trend suggests that a closer se-
mantic space among G verbs facilitates the acqui-
sition of verb meanings whereas a distant space
among S verbs causes difficulties in meaning ac-
quiring. Once those verbs with specific meanings
are picked up, most of them will become the so-
called conventional verbs. When the conventional
use to an action is a specific verb, the progress of
S verb usage is more obvious. The usage of verbs
with specificity meaning is a developing trend of
language acquisition.
4 Meaning Exploring on Chinese
Resultative Verb Compounds (RVCs)
In the verb-event co-occurrence matrix, verbs
elicited from the same event are considered to
be verbs have the same object in a verb-object
co-occurrence matrix. With the distributional
model, it then shows how meaning specificity af-
fects the linguistic behavior and semantic content
of Chinese resultative verb compounds (RVCs).
Those RVCs with similar distributional patterns
will present a high semantic relation. This se-
mantic relation could result from the meaning of
the first verbal morpheme (Vcaus) or the second
one (Vres). It is further proposed that the verb
type (generic or specific) of Vcaus would affect the
whole meaning content of V-V compounds.
4.1 The RVC Structure in the Data
A Chinese resultative verb compound (RVC) con-
sists two main elements: the first element (Vcaus)
expresses a causing event or a state while the sec-
ond element (Vres) denotes a resulting event or the
aspectual properties of the object. According to
the Aspectual Interface Hypothesis(Tenny, 1989),
the property of an internal argument can measure
out the event. In the Chinese example, da-po bo-li
(???? / hit-break glass), the state of the object
bo-li (?? / glass) is changed into smashed and
this change points out an end point of the event.
The resultative po (? / broken) is an delimiting
expression which refers to the property of the ob-
ject. In addition to defining the second element
of an RVC as a delimiting expression, other sur-
veys label it as Vres which requires the saturation
of arguments. Four possible V-V compound argu-
ment structures are proposed in Li?s (1990) works.
In the following studies, most of RVCs require
an argument structure like (1). The first verbal
morpheme (Vcaus) has a theta-grid <1, 2> and
the second morpheme (Vres) has <1?>. Vcaus re-
quires an external argument (a person) and an in-
ternal argument (a glass). The internal argument
(a glass) is identified with the argument of Vres.
Since the internal argument of Vcaus has to be
identified with the argument of Vres, it raises the
issue that which one functions more prominent in
choosing the object of a V-V compound. From the
study of RVCs? distributional pattern, it examines
which one (Vcaus or Vres) is more salient and also
dominates the argument selection of a V-V com-
pound.
942
(1) V < 1, 2-1? > (da-po bo-li)
Vcause
da
< 1, 2 >
< person, glass >
Vres
po
< 1?>
< glass >
4.2 Semantic Assessment
The semantic links among words are built by mea-
suring the linguistic distances among them. In or-
der to examine the semantic information of RVCs,
a sub-sample with thirty-six verbs is selected to do
cluster tasks. The semantic relationships of word
in the sub-sample is visualized as a clustering tree,
as shown in Figure 4. The figure shows that an
RVC with a G verb as its Vcaus (GVcaus ? Vres)
build a close relation with other RVCs which have
the same Vres with it. Take the most extreme G
verb da (?/hit)as an example, da-lan (??/hit-
ruin) is closer to pai-lan (??/hit with palm and
ruin) than da (?/hit). On the other hand, an
RVC with an S verb as its Vcaus (SVcaus ? Vres),
are grouped with those having the same Vcaus.
The RVC, ju-kai (??/saw-open), with a S verb
ju (?/saw) as its head, forms a cluster with ju
(?/saw) and ju-duan (??/saw-crack).
Figure 4: Semantic clustering of selected verbs.
With regard to the semantic relation of RVCs
shown in the cluster plot, the next step is to jus-
tify the proportion of RVCs with the structure
GVcaus ? Vres in which Vres selects a G verb as
its Vcause. As Table 5 shows, the proportion of
GVcaus ? Vres and SVcaus ? Vres is 50% respec-
tively. That is, half of the selected seven Vres pick
up a G verbs as its head while the other half words
go with S verbs. Those Vres preferring a G head
to a S head are sui, po, lan, duan; those prefer-
ring a S verb to a G verb head are kai, diao, xia.
According to the semantic content these resulta-
tive verbs, kai, diao, xia describes the direction
of the action and the motion of objects and they
are defined as ?path? Vres in Ma and Lu?s (1997)
work. As for sui, po, lan, duan called as ?result?
Vres, they mainly express the result of the object
affected by the action. The outcome reported here
suggests that ?result? Vres is apt to have a G verb
as its head verb whereas ?path? Vres tends to pick
up a S head verb. The proposal in literatures that
Vres tends to choose a G head verb is justified as
valid when the Vres expresses the meaning of ?re-
sult? rather than ?path.?
GVcaus SVcaus
?result? Vres
sui (?/smash) da, nong,
pai, ya, qiao
si
? po (?/break) da, nong, ya,
qiao
si, ci
lan (?/ruin) da, pai si?
duan (?/crack) qie ?
Proportion 47% 15%
?path? Vres
kai (?/open) qie zhe, ju, si,
bo?
diao (?/fall) zhe, ju, si, bo
xia (?/down) bo
Proportion 3% 35%
Table 5: GVcaus ? Vres versus SVcaus ? Vres.
In summary, words with small distance re-
sulting from their similar distributional patterns
can be interpreted to be semantically similar
in a semantic cluster. The result of semantic
clustering has suggested that the meaning of
RVCs depend on either the Vcaus or the Vres. The
meaning of GVcaus ? Vres is more determined
by Vres because GVcaus is more polysemous and
the Vres becomes a prominent role to dominate
the meaning of GVcaus ? Vres. In contrast,
SVcaus?Vres focuses on the part of SVcaus since
943
SVcaus expresses its meaning specific enough.
In addition, the property of Vres also affects the
category of its head verb. When Vres like sui
belong to the ?result? Vres, it tend to choose a G
verb as its Vcaus. On the other hand, the ?path?
Vres like xia, its head verb is apt to be a S verb. It
is suggested that ?path? Vres is more likely to have
a G verb than ?path? Vres. As the empirical study
illustrates the semantic information on Chinese
RVCs are affected by the semantic space of words.
5 Conclusion
In this paper, we argue the following points:
firstly, the distributional model shows that the se-
mantic space differ clearly in accordance with the
specificity of verbs. The G verbs form tight re-
lations with each other and become a larger clus-
ter whereas the semantic space among S verbs is
too distant to become a solid group. Secondly,
semantic space has influence on the acquiring of
words? meanings. Generic verbs are earlier and
easier acquired due to the closer semantic space
among words. The developing trend of specific
verb lexicon parallel with conventional usage sug-
gests a language acquisition phenomenon. Fi-
nally, the G/S verbs play an influential role in Chi-
nese resultative compounds. The resultative verb
becomes more prominent when the first verb is
with a generic meaning. The ?result? Vres is apt
to have a G verb as its head verb whereas ?path?
Vres tends to pick up a S head verb. We believe
that results of our analysis will shed light on se-
mantic assessment and make predictions for lexi-
cal acquisition.
References
Baayen, R. H. 2008. Analyzing Linguistic Data: A
Practical Introduction to Statistics using R. Cam-
bridge University Press.
Chen, P., M.-A. Parente, K. Duvignau, L. Tonietto, and
B. Gaume. 2008. Semantic approximations in the
early verbal lexicon acquisition of chinese: Flexi-
bility against error. The 7th Workshop on Chinese
Lexical Semantics.
Gries, Stefan Thomas. 2009. Quantitative Corpus
Linguistics with R: A Practical Intriduction. Rout-
ledge.
Hsieh, Shu-Kai, Chun-Han Chang, Ivy Kuo, Hintat
Cheung, Chu-Ren Huang, and Bruno Gaume. 2009.
Bridging the gap between graph modeling and de-
velopmental psycholinguistics: An experiment on
measuring lexical proximity in chinese semantic
space. Presented at The 23rd Pacific Asia Confer-
ence on Language, Information and Computation
(PACLIC 23). Hong Kong: City University of Hong
Kong., December 3-5.
Karlgren, J. and M. Sahlgren. 2001. From words
to understanding. In Uesaka, Y., Kanerva P. and
H. Asoh, editors, Foundations of Real-World Intel-
ligence, pages 294?308.
Landauer, T. K. and S. T. Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
of the acquisition, induction, and representation of
knowledge. Psychological Review, 104:211?240.
Landauer, T. K., P. W. Foltz, and D. Laham. 1998.
Introduction to latent semantic analysis. Discourse
Processes, 25:259?284.
Lenci, A. 2008. Distributional semantics in linguistic
and cognitive research. From context to meaning:
Distributional models of the lexicon in linguistics
and cognitive science, special issue of the Italian
Journal of Linguistics, 20/1:1?31.
Li, Yafei. 1990. On v-v compounds in chinese. Natu-
ral Language and Linguistic Theory, 8:177?207.
Ma, Zhen and Jian-Ming Lu. 1997. Xingrongci zuo
jieguobuyu qingkuang kaocha yi (???????
?????(?)). Hanyuxuexi (????), 1:3?7.
Malvern, David D., Brian J. Richards, Ngono Chipere,
and Pilar Duran. 2004. Lexical diversity and lan-
guage development : quantification and assessment.
New York : Palgrave Macmillan.
Sahlgren, M. 2002. Random indexing of linguistic
units for vector-based semantic analysis. ERCIM
News, 50.
Sahlgren, Magnus. 2005. An introduction to ran-
dom indexing. In Proceedings of the Methods and
Applications of Semantic Indexing Workshop at the
7th International Conference on Terminology and
Knowledge Engineering (TKE). Copenhagen, Den-
mark.
Tenny, Carol. 1989. The aspectual interface hypothe-
sis. In Proceedings of NELS 18. University of Mas-
sachusetts at Amherst.
Widdows, Dominic and Kathleen Ferraro. 2008.
Semantic vectors: a scalable open source pack-
age and online technology management applica-
tion. In Nicoletta Calzolari (Conference Chair),
944
Khalid Choukri, Bente Maegaard Joseph Mariani
Jan Odjik Stelios Piperidis Daniel Tapias, editor,
Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco. European Language Resources Associa-
tion (ELRA).
Widdows, Dominic, Scott Cederberg, and Beate
Dorow. 2002. Visualisation techniques for
analysing meaning. In Fifth International Confer-
ence on Text, Speech and Dialogue (TSD 5), pages
107?115. Brno, Czech Republic.
945
Coling 2010: Demonstration Volume, pages 5?8,
Beijing, August 2010
PyCWN: a Python Module for Chinese Wordnet  
Yueh-Cheng Wu  
Institute of Linguistics  
Academia Sinica, Taiwan  
wyc.juju@gmail.com  
Shu-Kai Hsieh  
National Taiwan Normal University / 
Academia Sinica, Taiwan  
shukai@gmail.com  
 
Abstract  
This presentation introduces a Python module (PyCWN) for accessing and processing 
Chinese lexical resources. In particular, our focus is put on the Chinese Wordnet (CWN) that 
has been developed and released by CWN group at Academia Sinica. PyCWN provides the 
access to Chinese Wordnet (sense and relation data) under the Python environment.  The 
presenation further demonstrates how this module applies to a variety of lexical processing 
tasks as well as the potentials for multilingual lexical processing.  
1         Introduction  
In the presentation, we demonstrate a useful python module for the processing of Chinese lexical 
semantic resources, viz Chinese Wordnet (CWN). This tool is one of a series of computational 
processing modules that we have been developing, for a variety of Chinese computational lexical 
semantic tasks, such as Word sense disambiguation (WSD), Word sense induction (WSI), Automatic 
relations discovery, etc.  
Based on the OOP paradigm, this module enables a programmer to handle CWN synsets and 
lexical relations in a more efficient way. Written in the python language, it can be run on a broad 
range of platforms and with the advantages of being able to be imported into other large-scale freely 
available NLP modules (e.g. Natural Language Processing Toolkits (NLTK)) for advanced surveys.  
2         Python Modules for WordNet Processing  
Inspired by psycholinguistic theories of human lexical memory, WordNet (Miller et al 1993) has 
been considered to be an important lexical resource for both theoretical and computational linguistics. 
It is organized as a lexical network which centers on synsets (synonymous sets), and the lexical 
semantic relations (hyponymy, meronymy, etc) are intertwined with the synsets.  
The growing amount of studies and applications carried out on wordnets has led to the worldwide 
efforts in constructing wordnets of different languages, with the envisioned framework of Global 
Wordnet Grid.[1] To make good use of these wordnet data, an amount of browsers have been 
proposed. However, it is soon realized that WordNet browsers are not suitable for scaled 
computational experiments. And ad-hoc processing scripts developed separately without any 
collaboration and shared architecture did not ease the tasks in the research community.  
Later on, an open source python library called the Natural Language Toolkits (NLTK) (Bird et al 
2009) has been implemented and distributed. NLTK is designed with many rationales in mind, such 
as extensibility, modularity, etc. In NLTK, a WordNetCorpusReader, which contains classes and 
methods for retrieval of sense and relation data, and the calculation of semantic similarity, is 
designed for accessing Princeton wordnet or its variants  
Despite the fact that these tremendous works do help much in accessing wordnet data, in applying 
to Chinese Wordnet, we found that an extended re-implementation of the module is necessary due to 
the particularity of the CWN architecture, which will be elaborated on later.  
3         PyCWN: Python Modules for Chinese Lexical Ontology  
5
3.1      Chinese Wordnet  
The construction of Chinese Wordnet developed by Academia Sinica follows two lines of thought: (i) 
multilingual wordnets bootstrapping approach (cf. Sinica BOW[2]), and (ii) linguistically oriented 
analysis from scratch (cf. CWN[3]). Both of them can be merged organically. In this paper, we focus 
only on the CWN part.  
Generally speaking, NLTK WordnetCorpusReader cannot be seamlessly applied to CWN with 
the following reasons:  
   
z Distinction of Sense and Meaning Facet: CWN proposed that lexical polysemy can be 
distinguished into two levels: senses and meaning facets (Ahrens et al 1998). These two levels 
of polysemies result in a special design for synset.  
z Labeling of Paronymy: CWN defines paronymy as the relation between any two lexical items 
belonging to the same semantic classification. (Huang et al 2007), and label the relation among 
senses instead of synsets.  
z Distinction of Synonyms and Chinese Written Variants: CWN regards synonyms and variants 
differently. Variants are the corresponding words/characters that have different written forms 
but the same meaning and the identical pronunciation as the target word. In PyCWN, the 
variants are integrated into the synset of the target word. No new category is created. 
z Homographic Variants: Homographic variants are the words with same graph but unrelated 
meanings. CWN defines them as different lemmas. For instance, ?(lian2) has three lemmas. In 
PyCWN, there is no Lemma class, but the lemma information is retained in the identifier of a 
synset/sense/meaning facet. 
3.2      Architecture of PyCWN  
 
Figure 1: Main structure of PyCWN 
Classes in PyCWN follow the main structure of the Chinese Wordnet. Therefore, paronyms are 
defined between two lexical items while other semantic relations are shared within the same synset. 
Every member within a synset is a sense or a meaning facet. The Facet class has all the properties as 
Sense class, and hence is not shown above. The identifier form in CWN is word(reference_id), but 
for the incorporation to other wordnets, the identifier form in PyCWN is adjusted to be 
word.pos.reference_id. 
3.3      Demo  
For the reusability of the information extracted, all information is extracted as a string or a list. And 
because of the coding, Chinese words are not readable in lists. In order to read the result, 'print' is 
needed. The following figure is an example of the Synset and the Sense class. The Facet class has the 
same properties as Sense class. 
6
  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: The illustrations of Class methods and Sense properties. 
3.4      Cross-linguistic Lexical Studies with NLTK Wordnet Modules  
Since the synsets in CWN are already mapped to those in Princeton WordNet via lexical relations, it 
is easy to perform cross-linguistic lexical comparative studies given the fact that Princeton WordNet 
is also connected with other wordnets such as EuroWordnet. For example, the following figure shows 
that ? (da2) has a hyponym -- ? (dao4), and that the WordNet synset reach.01369399V is a 
hypernym(???) of ?(da2). Thus it is inferred that reach.01369399V should be a hypernym of ?
(dao4) as well. And the information extracted has confirmed this point of view.  
7
Figure 3: Mapping between CWN and Princeton WordNet 
3.5      Availability  
The demos will be available as both locally based and remotely accessible from 
http://lope.eng.ntnu.edu.tw/pycwn/ 
4         Conclusion  
In this presentation, we have demonstrated a python module called PyCWN for the processing of the 
data in Chinese Wordnet. Now we are also working on the incorporation of NLTK, and extension of 
the module to a larger Chinese NLP framework, which includes word segmentation and the access of 
hanzi data, the Gigaword corpus, and the bilingual ontology, etc. We believe that the whole project 
will be an important infrastructure of Chinese NLP.  
   
References  
Ahrens, K., Chang, L., Chen, K., and Huang, C., 1998, Meaning Representation and Meaning Instantiation 
for Chinese Nominals. Computational Linguistics and Chinese Language Processing, 3, 45-60.  
Bird, Steven, Ewan Klein and Edward Loper. 2009. Natural Language Processing with Python. O?Reilly.  
Huang, Chu-Ren, Shu-Kai Hsieh, Jia-Fei Hong, et al 2010.  Chinese Wordnet: Design, Implementation, and 
Application of an Infrastructure for Cross-lingual Knowledge Processing. Zhong Guo YuWen, 24(2). [in 
Chinese].  
Huang, Chu-Ren, I-Li Su, Pei-Yi Hsiao, and Xiu-Ling Ke. 2007. Paranyms, Co-Hyponyms and Antonyms: 
Representing Semantic Fields with Lexical Semantic Relations. Chinese Lexical Semantics Workshop. 2007. 
May 20-23. Hong Kong: Hong Kong Polytechnic University.  
 
[1] http://www.globalwordnet.org/gwa/gwa_grid.htm  
[2] http://bow.sinica.edu.tw/  
[3] http://cwn.ling.sinica.edu.tw/  
8
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75?80,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Shu-Kai Hsieh
Department of English
National Taiwan Normal University
Taipei, Taiwan
shukai@ntnu.edu.tw
Maurizio Tesconi
IIT
CNR
Pisa, Italy
maurizio.tesconi@iit.cnr.it
Monica Monachini
ILC
CNR
Pisa, Italy
monica.monachini@ilc.cnr.it
Piek Vossen, Roxanne Segers
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl,roxane.segers@gmail.com
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambigua-
tion systems present new challenges. The
difficulties found by supervised systems to
adapt might change the way we assess the
strengths and weaknesses of supervised
and knowledge-based WSD systems. Un-
fortunately, all existing evaluation datasets
for specific domains are lexical-sample
corpora. This task presented all-words
datasets on the environment domain for
WSD in four languages (Chinese, Dutch,
English, Italian). 11 teams participated,
with supervised and knowledge-based sys-
tems, mainly in the English dataset. The
results show that in all languages the par-
ticipants where able to beat the most fre-
quent sense heuristic as estimated from
general corpora. The most successful ap-
proaches used some sort of supervision in
the form of hand-tagged examples from
the domain.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in previous Senseval and SemEval competitions
(Kilgarriff, 2001; Mihalcea et al, 2004; Snyder
and Palmer, 2004; Pradhan et al, 2007). Spe-
cific domains pose fresh challenges to WSD sys-
tems: the context in which the senses occur might
change, different domains involve different sense
distributions and predominant senses, some words
tend to occur in fewer senses in specific domains,
the context of the senses might change, and new
senses and terms might be involved. Both super-
vised and knowledge-based systems are affected
by these issues: while the first suffer from differ-
ent context and sense priors, the later suffer from
lack of coverage of domain-related words and in-
formation.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain. All
datasets and related information are publicly avail-
able from the task websites
1
.
This task was designed in the context of Ky-
oto (Vossen et al, 2008)
2
, an Asian-European
project that develops a community platform for
modeling knowledge and finding facts across lan-
guages and cultures. The platform operates as a
Wiki system with an ontological support that so-
cial communities can use to agree on the mean-
ing of terms in specific domains of their interest.
Kyoto focuses on the environmental domain be-
cause it poses interesting challenges for informa-
tion sharing, but the techniques and platforms are
1
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
2
http://www.kyoto-project.eu/
75
independent of the application domain.
The paper is structured as follows. We first
present the preparation of the data. Section 3 re-
views participant systems and Section 4 the re-
sults. Finally, Section 5 presents the conclusions.
2 Data preparation
The data made available to the participants in-
cluded the test set proper, and background texts.
Participants had one week to work on the test set,
but the background texts where provided months
earlier.
2.1 Test datasets
The WSD-domain comprises comparable all-
words test corpora on the environment domain.
Three texts were compiled for each language by
the European Center for Nature Conservation
3
and
Worldwide Wildlife Forum
4
. They are documents
written for a general but interested public and in-
volve specific terms from the domain. The docu-
ment content is comparable across languages. Ta-
ble 1 shows the numbers for the datasets.
Although the original plan was to annotate mul-
tiword terms, and domain terminology, due to time
constraints we focused on single-word nouns and
verbs. The test set clearly marked which were
the words to be annotated. In the case of Dutch,
we also marked components of single-word com-
pounds. The format of the test set followed that of
previous all-word exercises, which we extended to
accommodate Dutch compounds. For further de-
tails check the datasets in the task website.
The sense inventory was based on publicly
available wordnets of the respective languages
(see task website for details). The annotation pro-
cedure involved double-blind annotation by ex-
perts plus adjudication, which allowed us to also
provide Inter Annotator Agreement (IAA) figures
for the dataset. The procedure was carried out us-
ing KAFnotator tool (Tesconi et al, 2010). Due
to limitations in resources and time, the English
dataset was annotated by a single expert annota-
tor. For the rest of languages, the agreement was
very good, as reported in Table 1.
Table 1 includes the results of the random base-
line, as an indication of the polysemy in each
dataset. Average polysemy is highest for English,
and lowest for Dutch.
3
http://www.ecnc.org
4
http://www.wwf.org
Total Noun Verb IAA Random
Chinese 3989 754 450 0.96 0.321
Dutch 8157 997 635 0.90 0.328
English 5342 1032 366 n/a 0.232
Italian 8560 1340 513 0.72 0.294
Table 1: Dataset numbers, including number of
tokens, nouns and verbs to be tagged, Inter-
Annotator Agreement (IAA) and precision of ran-
dom baseline.
Documents Words
Chinese 58 455359
Dutch 98 21089
English 113 2737202
Italian 27 240158
Table 2: Size of the background data.
2.2 Background data
In addition to the test datasets proper, we also pro-
vided additional documents on related subjects,
kindly provided by ECNC and WWF. Table 2
shows the number of documents and words made
available for each language. The full list with the
urls of the documents are available from the task
website, together with the background documents.
3 Participants
Eleven participants submitted more than thirty
runs (cf. Table 3). The authors classified their runs
into supervised (S in the tables, three runs), weakly
supervised (WS, four runs), unsupervised (no runs)
and knowledge-based (KB, the rest of runs)
5
. Only
one group used hand-tagged data from the domain,
which they produced on their own. We will briefly
review each of the participant groups, ordered fol-
lowing the rank obtained for English. They all par-
ticipated on the English task, with one exception
as noted below, so we report their rank in the En-
glish task. Please refer to their respective paper in
these proceedings for more details.
CFILT: They participated with a domain-
specific knowledge-based method based on Hop-
field networks (Khapra et al, 2010). They first
identify domain-dependant words using the back-
ground texts, use a graph based on hyponyms in
WordNet, and a breadth-first search to select the
most representative synsets within domain. In ad-
dition they added manually disambiguated around
one hundred examples from the domain as seeds.
5
Note that boundaries are slippery. We show the classifi-
cations as reported by the authors.
76
English
Rank Participant System ID Type P R R nouns R verbs
1 Anup Kulkarni CFILT-2 WS 0.570 0.555 ?0.024 0.594 ?0.028 0.445 ?0.047
2 Anup Kulkarni CFILT-1 WS 0.554 0.540 ?0.021 0.580 ?0.025 0.426 ?0.043
3 Siva Reddy IIITH1-d.l.ppr.05 WS 0.534 0.528 ?0.027 0.553 ?0.023 0.456 ?0.041
4 Abhilash Inumella IIITH2-d.r.l.ppr.05 WS 0.522 0.516 ?0.023 0.529 ?0.027 0.478 ?0.041
5 Ruben Izquierdo BLC20SemcorBackground S 0.513 0.513 ?0.022 0.534 ?0.026 0.454 ?0.044
- - Most Frequent Sense - 0.505 0.505 ?0.023 0.519 ?0.026 0.464 ?0.043
6 Ruben Izquierdo BLC20Semcor S 0.505 0.505 ?0.025 0.527 ?0.031 0.443 ?0.045
7 Anup Kulkarni CFILT-3 KB 0.512 0.495 ?0.023 0.516 ?0.027 0.434 ?0.048
8 Andrew Tran Treematch KB 0.506 0.493 ?0.021 0.516 ?0.028 0.426 ?0.046
9 Andrew Tran Treematch-2 KB 0.504 0.491 ?0.021 0.515 ?0.030 0.425 ?0.044
10 Aitor Soroa kyoto-2 KB 0.481 0.481 ?0.022 0.487 ?0.025 0.462 ?0.039
11 Andrew Tran Treematch-3 KB 0.492 0.479 ?0.022 0.494 ?0.028 0.434 ?0.039
12 Radu Ion RACAI-MFS KB 0.461 0.460 ?0.022 0.458 ?0.025 0.464 ?0.046
13 Hansen A. Schwartz UCF-WS KB 0.447 0.441 ?0.022 0.440 ?0.025 0.445 ?0.043
14 Yuhang Guo HIT-CIR-DMFS-1.ans KB 0.436 0.435 ?0.023 0.428 ?0.027 0.454 ?0.043
15 Hansen A. Schwartz UCF-WS-domain KB 0.440 0.434 ?0.024 0.434 ?0.029 0.434 ?0.044
16 Abhilash Inumella IIITH2-d.r.l.baseline.05 KB 0.496 0.433 ?0.024 0.452 ?0.023 0.390 ?0.044
17 Siva Reddy IIITH1-d.l.baseline.05 KB 0.498 0.432 ?0.021 0.463 ?0.026 0.344 ?0.038
18 Radu Ion RACAI-2MFS KB 0.433 0.431 ?0.022 0.434 ?0.027 0.399 ?0.049
19 Siva Reddy IIITH1-d.l.ppv.05 KB 0.426 0.425 ?0.026 0.434 ?0.028 0.399 ?0.043
20 Abhilash Inumella IIITH2-d.r.l.ppv.05 KB 0.424 0.422 ?0.023 0.456 ?0.025 0.325 ?0.044
21 Hansen A. Schwartz UCF-WS-domain.noPropers KB 0.437 0.392 ?0.025 0.377 ?0.025 0.434 ?0.043
22 Aitor Soroa kyoto-1 KB 0.384 0.384 ?0.022 0.382 ?0.024 0.391 ?0.047
23 Ruben Izquierdo BLC20Background S 0.380 0.380 ?0.022 0.385 ?0.026 0.366 ?0.037
24 Davide Buscaldi NLEL-WSD-PDB WS 0.381 0.356 ?0.022 0.357 ?0.027 0.352 ?0.049
25 Radu Ion RACAI-Lexical-Chains KB 0.351 0.350 ?0.015 0.344 ?0.017 0.368 ?0.030
26 Davide Buscaldi NLEL-WSD WS 0.370 0.345 ?0.022 0.352 ?0.027 0.328 ?0.037
27 Yoan Gutierrez Relevant Semantic Trees KB 0.328 0.322 ?0.022 0.335 ?0.026 0.284 ?0.044
28 Yoan Gutierrez Relevant Semantic Trees-2 KB 0.321 0.315 ?0.022 0.327 ?0.024 0.281 ?0.040
29 Yoan Gutierrez Relevant Cliques KB 0.312 0.303 ?0.021 0.304 ?0.024 0.301 ?0.041
- - Random baseline - 0.232 0.232 0.253 0.172
Chinese
Rank Participant System ID Type P R R nouns R verbs
- - Most Frequent Sense - 0.562 0.562 ?0.026 0.589 ?0.027 0.518 ?0.039
1 Meng-Hsien Shih HR KB 0.559 0.559 ?0.024 0.615 ?0.026 0.464 ?0.039
2 Meng-Hsien Shih GHR KB 0.517 0.517 ?0.024 0.533 ?0.035 0.491 ?0.038
- - Random baseline - 0.321 0.321 0.326 0.312
4 Aitor Soroa kyoto-3 KB 0.322 0.296 ?0.022 0.257 ?0.027 0.360 ?0.038
3 Aitor Soroa kyoto-2 KB 0.342 0.285 ?0.021 0.251 ?0.026 0.342 ?0.040
5 Aitor Soroa kyoto-1 KB 0.310 0.258 ?0.023 0.256 ?0.029 0.261 ?0.031
Dutch
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.526 0.526 ?0.022 0.575 ?0.029 0.450 ?0.034
2 Aitor Soroa kyoto-2 KB 0.519 0.519 ?0.022 0.561 ?0.027 0.454 ?0.034
- - Most Frequent Sense - 0.480 0.480 ?0.022 0.600 ?0.027 0.291 ?0.025
3 Aitor Soroa kyoto-1 KB 0.465 0.465 ?0.021 0.505 ?0.026 0.403 ?0.033
- - Random baseline - 0.328 0.328 0.350 0.293
Italian
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.529 0.529 ?0.021 0.530 ?0.024 0.528 ?0.038
2 Aitor Soroa kyoto-2 KB 0.521 0.521 ?0.018 0.522 ?0.023 0.519 ?0.035
3 Aitor Soroa kyoto-1 KB 0.496 0.496 ?0.019 0.507 ?0.020 0.468 ?0.037
- - Most Frequent Sense - 0.462 0.462 ?0.020 0.472 ?0.024 0.437 ?0.035
- - Random baseline - 0.294 0.294 0.308 0.257
Table 3: Overall results for the domain WSD datasets, ordered by recall.
This is the only group using hand-tagged data
from the target domain. Their best run ranked 1st.
IIITTH: They presented a personalized PageR-
ank algorithm over a graph constructed from
WordNet similar to (Agirre and Soroa, 2009),
with two variants. In the first (IIITH1), the vertices
of the graph are initialized following the rank-
ing scores obtained from predominant senses as in
(McCarthy et al, 2007). In the second (IIITH2),
the graph is initialized with keyness values as in
77
0.3 0.35 0.4 0.45 0.5 0.55
Rel. Cliques
Rel. Sem. Trees-2
Rel. Sem. Trees
NLEL-WSD
RACAI-Lexical-Chains
NLEL-WSD-PDB
BLC20BG
Kyoto-1
UCF-WS-domain.noPropers
IIITH2-d.r.l.ppv.05
IIITH1-d.l.ppv.05
RACAI-2MFS-BOW
IIITH1-d.l.baseline.05
IIITH2-d.r.l.baseline.05
UCF-WS-domain
HIT-CIR-DMFS
UCF-WS
RACAI-MFS
Treematch-3
Kyoto-2
Treematch-2
Treematch
CFILT-3
BLC20SC
BLC20SCBG
IIITH2-d.l.ppr.05
IIITH1-d.l.ppr.05
CFILT-1
CFILT-2
MFS
Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond
to one system (denoted in axis Y) according each recall and confidence interval (axis X ). Systems are
ordered depending on their rank.
(Rayson and Garside, 2000). Some of the runs
use sense statistics from SemCor, and have been
classified as weakly supervised. They submitted a
total of six runs, with the best run ranking 3rd.
BLC20(SC/BG/SCBG): This system is super-
vised. A Support Vector Machine was trained us-
ing the usual set of features extracted from con-
text and the most frequent class of the target word.
Semantic class-based classifiers were built from
SemCor (Izquierdo et al, 2009), where the classes
were automatically obtained exploiting the struc-
tural properties of WordNet. Their best run ranked
5th.
Treematch: This system uses a knowledge-
based disambiguation method that requires a dic-
tionary and untagged text as input. A previously
developed system (Chen et al, 2009) was adapted
to handle domain specific WSD. They built a
domain-specific corpus using words mined from
relevant web sites (e.g. WWF and ECNC) as
seeds. Once parsed the corpus, the used the de-
pendency knowledge to build a nodeset that was
used for WSD. The background documents pro-
vided by the organizers were only used to test how
exhaustive the initial seeds were. Their best run
ranked 8th.
Kyoto: This system participated in all four
languages, with a free reimplementation of
the domain-specific knowledge-based method for
WSD presented in (Agirre et al, 2009). It
uses a module to construct a distributional the-
saurus, which was run on the background text, and
a disambiguation module based on Personalized
PageRank over wordnet graphs. Different Word-
Net were used as the LKB depending on the lan-
guage. Their best run ranked 10th. Note that this
team includes some of the organizers of the task.
A strict separation was kept, in order to keep the
test dataset hidden from the actual developers of
the system.
RACAI: This participant submitted three differ-
ent knowledge-based systems. In the first, they use
the mapping to domains of WordNet (version 2.0)
in order to constraint the domains of the content
words of the test text. In the second, they choose
among senses using lexical chains (Ion and Ste-
fanescu, 2009). The third system combines the
previous two. Their best system ranked 12th.
HIT-CIR: They presented a knowledge-based
system which estimates predominant sense from
raw test. The predominant senses were calculated
with the frequency information in the provided
background text, and automatically constructed
78
thesauri from bilingual parallel corpora. The sys-
tem ranked 14.
UCFWS: This knowledge-based WSD system
was based on an algorithm originally described in
(Schwartz and Gomez, 2008), in which selectors
are acquired from the Web via searching with lo-
cal context of a given word. The sense is cho-
sen based on the similarity or relatedness between
the senses of the target word and various types
of selectors. In some runs they include predom-
inant senses(McCarthy et al, 2007). The best run
ranked 13th.
NLEL-WSD(-PDB): The system used for the
participation is based on an ensemble of different
methods using fuzzy-Borda voting. A similar sys-
tem was proposed in SemEval-2007 task-7 (Bus-
caldi and Rosso, 2007). In this case, the com-
ponent method used where the following ones:
1) Most Frequent Sense from SemCor; 2) Con-
ceptual Density ; 3) Supervised Domain Relative
Entropy classifier based on WordNet Domains;
4) Supervised Bayesian classifier based on Word-
Net Domains probabilities; and 5) Unsupervised
Knownet-20 classifiers. The best run ranked 24th.
UMCC-DLSI (Relevant): The team submitted
three different runs using a knowledge-based sys-
tem. The first two runs use domain vectors and
the third is based on cliques, which measure how
much a concept is correlated to the sentence by
obtaining Relevant Semantic Trees. Their best run
ranked 27th.
(G)HR: They presented a Knowledge-based
WSD system, which make use of two heuristic
rules (Li et al, 1995). The system enriched the
Chinese WordNet by adding semantic relations for
English domain specific words (e.g. ecology, en-
vironment). When in-domain senses are not avail-
able, the system relies on the first sense in the Chi-
nese WordNet. In addition, they also use sense
definitions. They only participated in the Chinese
task, with their best system ranking 1st.
4 Results
The evaluation has been carried out using the stan-
dard Senseval/SemEval scorer scorer2 as in-
cluded in the trial dataset, which computes preci-
sion and recall. Table 3 shows the results in each
dataset. Note that the main evaluation measure is
recall (R). In addition we also report precision (P)
and the recall for nouns and verbs. Recall mea-
sures are accompanied by a 95% confidence in-
terval calculated using bootstrap resampling pro-
cedure (Noreen, 1989). The difference between
two systems is deemed to be statistically signifi-
cant if there is no overlap between the confidence
intervals. We show graphically the results in Fig-
ure 1. For instance, the differences between the
highest scoring system and the following four sys-
tems are not statistically significant. Note that this
method of estimating statistical significance might
be more strict than other pairwise methods.
We also include the results of two baselines.
The random baseline was calculated analytically.
The first sense baseline for each language was
taken from each wordnet. The first sense baseline
in English and Chinese corresponds to the most
frequent sense, as estimated from out-of-domain
corpora. In Dutch and Italian, it followed the in-
tuitions of the lexicographer. Note that we don?t
have the most frequent sense baseline from the do-
main texts, which would surely show higher re-
sults (Koeling et al, 2005).
5 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. With
this paper we have motivated the creation of an
all-words test dataset for WSD on the environ-
ment domain in several languages, and presented
the overall design of this SemEval task.
One of the goals of the exercise was to show
that WSD systems could make use of unannotated
background corpora to adapt to the domain and
improve their results. Although it?s early to reach
hard conclusions, the results show that in each of
the datasets, knowledge-based systems are able to
improve their results using background text, and
in two datasets the adaptation of knowledge-based
systems leads to results over the MFS baseline.
The evidence of domain adaptation of supervised
systems is weaker, as only one team tried, and the
differences with respect to MFS are very small.
The best results for English are obtained by a sys-
tem that combines a knowledge-based system with
some targeted hand-tagging. Regarding the tech-
niques used, graph-based methods over WordNet
and distributional thesaurus acquisition methods
have been used by several teams.
79
All datasets and related information are publicly
available from the task websites
6
.
Acknowledgments
We thank the collaboration of Lawrence Jones-Walters, Amor
Torre-Marin (ECNC) and Karin de Boom (WWF), com-
piling the test and background documents. This work
task is partially funded by the European Commission (KY-
OTO ICT-2007-211423), the Spanish Research Department
(KNOW-2 TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings of the
12th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL09), pages 33?
41. Association for Computational Linguistics.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
Davide Buscaldi and Paolo Rosso. 2007. Upv-wsd : Com-
bining different wsd methods by means of fuzzy borda
voting. In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007), pages
434?437.
P. Chen, W. Ding, and D. Brown. 2009. A fully unsupervised
word sense disambiguation method and its evaluation on
coarse-grained all-words task. In Proceeding of the North
American Chapter of the Association for Computational
Linguistics (NAACL09).
Radu Ion and Dan Stefanescu. 2009. Unsupervised word
sense disambiguation with lexical chains and graph-based
context formalization. In Proceedings of the 4th Language
and Technology Conference: Human Language Technolo-
gies as a Challenge for Computer Science and Linguistics,
pages 190?194.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense dis-
ambiguation. In EACL ?09: Proceedings of the 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 389?397, Morristown,
NJ, USA. Association for Computational Linguistics.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense dis-
ambiguation combining corpus based and wordnet based
parameters. In Proceedings of the 5th International Con-
ference on Global Wordnet (GWC2010).
A. Kilgarriff. 2001. English Lexical Sample Task Descrip-
tion. In Proceedings of the Second International Work-
shop on evaluating Word Sense Disambiguation Systems,
Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
6
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
Natural Language Processing. HLT/EMNLP, pages 419?
426, Ann Arbor, Michigan.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995. A
wordnet-based algorithm for word sense disambiguation.
In Proceedings of The 14th International Joint Conference
on Artificial Intelligence (IJCAI95).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2007. Unsupervised acquisition of predominant
word senses. Computational Linguistics, 33(4).
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Eric W. Noreen. 1989. Computer-Intensive Methods for Test-
ing Hypotheses. John Wiley & Sons.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague, Czech Republic.
Paul Rayson and Roger Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the workshop
on Comparing corpora, pages 1?6.
Hansen A. Schwartz and Fernando Gomez. 2008. Acquir-
ing knowledge from the web to be used as selectors for
noun sense disambiguation. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CONLL08).
B. Snyder and M. Palmer. 2004. The English all-words task.
In Proceedings of the 3rd ACL workshop on the Evalua-
tion of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
M. Tesconi, F. Ronzano, S. Minutoli, C. Aliprandi, and
A. Marchetti. 2010. Kafnotator: a multilingual seman-
tic text annotation tool. In In Proceedings of the Second
International Conference on Global Interoperability for
Language Resources.
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Christiane
Fellbaum, Shu kai Hsieh, Chu-Ren Huang, Hitoshi Isa-
hara, Kyoko Kanzaki, Andrea Marchetti, Monica Mona-
chini, Federico Neri, Remo Raffaelli, German Rigau,
Maurizio Tescon, and Joop VanGent. 2008. Kyoto: a
system for mining, structuring and distributing knowl-
edge across languages and cultures. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
80
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417?420,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
Kyoto: An Integrated System for Specific Domain WSD
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle
University of the Basque Country
a.soroa@ehu.es
Monica Monachini
Istituto di Linguistica Computazionale
monica.monachini@ilc.cnr.it
Jessie Lo, Shu-Kai Hsieh
National Taiwan Normal University
shukai@ntnu.edu.tw
Wauter Bosma, Piek Vossen
Vrije Universiteit
{p.vossen,w.bosma}@let.vu.nl
Abstract
This document describes the prelimi-
nary release of the integrated Kyoto sys-
tem for specific domain WSD. The sys-
tem uses concept miners (Tybots) to ex-
tract domain-related terms and produces
a domain-related thesaurus, followed by
knowledge-based WSD based on word-
net graphs (UKB). The resulting system
can be applied to any language with a
lexical knowledge base, and is based on
publicly available software and resources.
Our participation in Semeval task #17 fo-
cused on producing running systems for
all languages in the task, and we attained
good results in all except Chinese. Due
to the pressure of the time-constraints in
the competition, the system is still under
development, and we expect results to im-
prove in the near future.
1 Introduction
In this paper we describe the participation of the
integrated Kyoto system on the ?SemEval-2010
task #17: All-words Word Sense Disambigua-
tion on a Specific Domain? task (Agirre et al,
2010). The goal of our participation was to eval-
uate the preliminary release of the integrated sys-
tem for specific domain WSD developed for the
Kyoto project
1
. Besides, we wanted to test the
performance of our domain specific WSD system
(Agirre et al, 2009) on this test set, and to inte-
grate the thesaurus construction software (Tybots)
developed for the project. The system can be run
for any language and domain if provided with a
lexical knowledge base and some background doc-
uments on the domain.
We will first present the components of our sys-
tem, followed by the experimental design and the
1
http://www.kyoto-project.eu
results. Finally, the conclusions are presented.
2 The Kyoto System for Domain Specific
WSD
We will present in turn UKB, the Tybots, and the
lexical knowledge-bases used.
2.1 UKB
UKB is a knowledge-based unsupervised WSD
system which exploits the structure of an under-
lying Language Knowledge Base (LKB) and finds
the most relevant concepts given an input con-
text (Agirre and Soroa, 2009). UKB starts by tak-
ing the LKB as a graph of concepts G = (V,E)
with a set of vertices V derived from LKB con-
cepts and a set of edges E representing relations
among them. Giving an input context, UKB ap-
plies the so called Personalized PageRank (Haveli-
wala, 2002) over it to obtain the most representa-
tive senses for the context.
PageRank (Brin and Page, 1998) is a method
for scoring the vertices V of a graph according
to each node?s structural importance. The algo-
rithm can be viewed as random walk process that
postulate the existence of a particle that randomly
traverses the graph, but at any time may jump to
a new vertex with a given damping factor (also
called teleport probability). After PageRank cal-
culation, the final weight of node i represents the
proportion of time that a random particle spends
visiting node i after a sufficiently long time. In
standard PageRank, the teleport vector is chosen
uniformly, whereas for Personalized PageRank it
is chosen from a nonuniform distribution of nodes,
specified by a teleport vector.
UKB concentrates the initial probability mass
of the teleport vector in the words occurring in
the context of the target word, causing all random
jumps on the walk to return to these words and
thus assigning a higher rank to the senses linked to
these words. Moreover, the high rank of the words
417
spreads through the links in the graph and make
all the nodes in its vicinity also receive high ranks.
Given a target word, the system checks which is
the relative ranking of its senses, and the WSD
system would output the one ranking highest.
UKB is very flexible and can be use to perform
WSD on different settings, depending on the con-
text used for disambiguating a word instance. In
this paper we use it to perform general and do-
main specific WSD, as shown in section 3. PageR-
ank is calculated by applying an iterative algo-
rithm until convergence below a given threshold
is achieved. Following usual practice, we used a
damping value of 0.85 and set the threshold value
at 0.001. We did not optimize these parameters.
2.2 Tybots
Tybots (Term Yielding Robots) are text mining
software that mine domain terms from corpus
(e.g. web pages), organizing them in a hierar-
chical structure, connecting them to wordnets and
ontologies to create a semantic model for the do-
main (Bosma and Vossen, 2010). The software is
freely available using Subversion
2
. Tybots try to
establish a view on the terminology of the domain
which is as complete as possible, discovering rela-
tions between terms and ranking terms by domain
relevance.
Preceding term extraction, we perform tok-
enization, part-of-speech tagging and lemmatiza-
tion, which is stored in Kyoto Annotation For-
mat (KAF) (Bosma et al, 2009). Tybots work
through KAF documents, acquire domain relevant
terms based on the syntactic features, gather co-
occurrence statistics to decide which terms are sig-
nificant in the domain and produce a thesaurus
with sets of related words. Section 3.3 describes
the specific settings that we used.
2.3 Lexical Knowledge bases
We used the following wordnets, as suggested by
the organizers:
WN30g: English WordNet 3.0 with gloss relations
(Fellbaum, 1998).
Dutch: The Dutch LKB is part of the Cor-
netto database version 1.3 (Vossen et al, 2008).
The Cornetto database can be obtained from
the Dutch/Flanders Taalunie
3
. Cornetto com-
prises taxonomic relations and equivalence rela-
2
http://kyoto.let.vu.nl/svn/kyoto/trunk
3
http://www.inl.nl/nl/lexica/780
#entries #synsets #rels. #WN30g
Monolingual
Chinese 8,186 14,243 20,433 20,584
Dutch 83,812 70,024 224,493 83,669
Italian 46,724 49,513 65,567 52,524
WN30g 147,306 117,522 525,351 n/a
Bilingual
Chinese-eng 8,186 141,561 566,368
Dutch-eng 83,812 188,511 833,513
Italian-eng 46,724 167,094 643,442
Table 1: Wordnets and their sizes (entries, synsets,
relations and links to WN30g).
tions from both WordNet 2.0 and 3.0. Cornetto
concepts are mapped to English WordNet 3.0.
Italian: Italwordnet (Roventini et al, 2003) was
created in the framework of the EuroWordNet,
employs the same set of semantic relations used
in EuroWordNet, and includes links to WordNet
3.0 synsets.
Chinese: The Chinese WordNet (Version 1.6) is
now partially open to the public
4
(Tsai et al,
2001). The Chinese WordNet is also mapped to
WordNet 3.0.
Table 1 shows the sizes of the graphs created
using each LKB as a source. The upper part shows
the number of lexical entries, synsets and relations
of each LKB. It also depicts the number of links to
English WordNet 3.0 synsets.
In addition, we also created bilingual graphs for
Dutch, Italian and Chinese, comprising the orig-
inal monolingual LKB, the links to WordNet 3.0
and WordNet 3.0 itself. We expected this richer
graphs to perform better performance. The sizes
of the bilingual graphs are shown in the lower side
of Table 1.
3 Experimental setting
All test documents were lemmatized and PoS-
tagged using the linguistic processors available
within the Kyoto project. In this section we de-
scribe the submitted runs.
3.1 UKB parameters
We use UKB with the default parameters. In par-
ticular, we don?t use dictionary weights, which in
the case of English come from annotated corpora.
This is done in order to make the system fully un-
supervised. It?s also worth mentioning that in the
default setting parts of speech were not used.
4
http://cwn.ling.sinica.edu.tw
418
RANK RUN P R R-NOUN R-VERB
Chinese
- 1sense 0.562 0.562 0.589 0.518
1 Best 0.559 0.559 - -
- Random 0.321 0.321 0.326 0.312
4 kyoto-3 0.322 0.296 0.257 0.360
3 kyoto-2 0.342 0.285 0.251 0.342
5 kyoto-1 0.310 0.258 0.256 0.261
Dutch
1 kyoto-3 0.526 0.526 0.575 0.450
2 kyoto-2 0.519 0.519 0.561 0.454
- 1sense 0.480 0.480 0.600 0.291
3 kyoto-1 0.465 0.465 0.505 0.403
- Random 0.328 0.328 0.350 0.293
English
1 Best 0.570 0.555 - -
- 1sense 0.505 0.505 0.519 0.454
10 kyoto-2 0.481 0.481 0.487 0.462
22 kyoto-1 0.384 0.384 0.382 0.391
- Random 0.232 0.232 0.253 0.172
Italian
1 kyoto-3 0.529 0.529 0.530 0.528
2 kyoto-2 0.521 0.521 0.522 0.519
3 kyoto-1 0.496 0.496 0.507 0.468
- 1sense 0.462 0.462 0.472 0.437
- Random 0.294 0.294 0.308 0.257
Table 2: Overall results of our runs, including pre-
cision (P) and recall (R), overall and for each PoS.
We include the First Sense (1sense) and random
baselines, as well as the best run, as provided by
the organizers.
3.2 Run1: UKB using context
The first run is an application of the UKB tool in
the standard setting, as described in (Agirre and
Soroa, 2009). Given the input text, we split it in
sentences, and we disambiguate each sentence at a
time. We extract the lemmas which have an entry
in the LKB and then apply Personalized PageR-
ank over all of them, obtaining a score for every
concept of the LKB. To disambiguate the words in
the sentence we just choose its associated concept
(sense) with maximum score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
when necessary. UKB allows two main methods
of disambiguation, namely ppr and ppr w2w. We
used the latter method, as it has been shown to per-
form best.
In this setting we used the monolingual graphs
for each language (cf. section 2.3). Note that
in this run there is no domain adaptation, it thus
serves us as a baseline for assessing the benefits of
applying domain adaptation techniques.
3.3 Run2: UKB using related words
Instead of disambiguating words using their con-
text of occurrence, we follow the method de-
scribed in (Agirre et al, 2009). The idea is to first
obtain a list of related words for each of the tar-
get words, as collected from a domain corpus. On
a second step each target word is disambiguated
using the N most related words as context (see
below). For instance, in order to disambiguate
the word environment, we would not take into
account the context of occurrence (as in Section
3.2), but we would use the list of most related
words in the thesaurus (e.g. ?biodiversity, agri-
culture, ecosystem, nature, life, climate, . . .?). Us-
ing UKB over these contexts we obtain the most
predominant sense for each target word in the do-
main(McCarthy et al, 2007), which is used to la-
bel all occurrences of the target word in the test
dataset.
In order to build the thesaurus with the lists of
related words, we used Tybots (c.f. section 2.2),
one for each corpus of the evaluation dataset, i.e.
Chinese, Dutch, English, and Italian. We used the
background documents provided by the organiz-
ers, which we processed using the linguistic pro-
cessors of the project to obtain the documents in
KAF. We used the Tybots with the following set-
tings. We discarded co-occurring words with fre-
quencies below 10
5
. Distributional similarity was
computed using (Lin, 1998). Finally, we used up
to 50 related words for each target word.
As in run1, we used the monolingual graphs for
the LKBs in each language.
3.4 Run3: UKB using related words and
bilingual graphs
The third run is exactly the same as run2, except
that we used bilingual graphs instead of monolin-
gual ones for all languages other than English (cf.
section 2.3). There is no run3 for English.
4 Results
Table 2 shows the results of our system on the
different languages. We will analyze different as-
pects of the results in turn.
Domain adaptation: Using Personalized Pager-
ank over related words (run2 and run3) con-
sistently outperforms the standard setting (run1)
in all languages. This result is consistent with
5
In the case of Dutch we did not use any threshold due to
the small size of the background corpus.
419
our previous work on English (Agirre et al,
2009), and shows that domain adaptation works
for knowledge-based systems.
Monolingual vs. Bilingual graphs: As ex-
pected, we obtained better results using the bilin-
gual graphs (run3) than with monolingual graphs
(run2), showing that the English WordNet has a
richer set of relations, and that those relations can
be successfully ported to other languages. This
confirms that aligning different wordnets at the
synset level is highly beneficial.
Overall results: the results of our runs are highly
satisfactory. In two languages (Dutch and Ital-
ian) our best runs perform better than the first
sense baseline, which is typically hard to beat for
knowledge-based systems. In English, our system
performs close but below the first sense baseline,
and in Chinese our method performed below the
random baseline.
The poor results obtained for Chinese can be
due the LKB topology; an analysis over the graph
shows that it is formed by a large number of
small components, unrelated with each other. This
?flat? structure heavily penalizes the graph based
method, which is many times unable to discrimi-
nate among the concepts of a word. We are cur-
rently inspecting the results, and we don?t discard
bugs, due to the preliminary status of our software.
In particular, we need to re-examine the output of
the Tybot for Chinese.
5 Conclusions
This paper describes the results of the prelimi-
nary release of he integrated Kyoto system for do-
main specific WSD. It comprises Tybots to con-
struct a domain-related thesaurus, and UKB for
knowledge-based WSD based on wordnet graphs.
We applied our system to all languages in the
dataset, obtaining good results. In fact, our sys-
tem can be applied to any language with a lexical
knowledge base, and is based on publicly available
software and resources. We used the wordnets and
background texts provided by the organizers of the
task.
Our results show that we were succesful in
adapting our system to the domain, as we man-
aged to beat the first sense baseline in two lan-
guages. Our results also show that adding the En-
glish WordNet to the other language wordnets via
the available links is beneficial.
Our participation focused on producing running
systems for all languages in the task, and we at-
tained good results in all except Chinese. Due to
the pressure and the time-constraints in the com-
petition, the system is still under development. We
are currently revising our system for bugs and fine-
tuning it.
Acknowledgments
This work task is partially funded by the Eu-
ropean Commission (KYOTO ICT-2007-211423),
the Spanish Research Department (KNOW-2
TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank for
word sense disambiguation. In Proceedings of EACL09,
pages 33?41. Association for Computational Linguistics.
E. Agirre, O. L?opez de Lacalle, and A. Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
E. Agirre, O. L?opez de Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense dis-
ambiguation on a specific domain. In Same volume.
W. E. Bosma and P. Vossen. 2010. Bootstrapping language
neutral term extraction. In Proceedings of LREC2010,
May.
W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,
A. Marchetti, M. Monachini, and C. Aliprandi. 2009.
KAF: a generic semantic annotation format. In Proceed-
ings of the GL2009 Workshop on Semantic Annotation.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks and
ISDN Systems, 30(1-7).
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW
?02: Proceedings of the 11th international conference on
WWW, pages 517?526, New York, NY, USA. ACM.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of ACL98, Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007.
Unsupervised acquisition of predominant word senses.
Computational Linguistics, 33(4).
A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-
cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,
and A. Zampolli. 2003. Italwordnet: building a large
semantic database for the automatic treatment of Italian.
Linguistica Computazionale, Special Issue (XVIII-XIX),
pages 745?791.
B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and
Y.S. Chuang. 2001. Definition and tests for lexical se-
mantic relations in Chinese. In Proceedings CLSW 2001.
P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van
Zutphen. 2008. The cornetto database: the architecture
and alignment issues. In Proceedings GWC 2008, pages
485?506.
420
