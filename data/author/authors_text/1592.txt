Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 534?541,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Quality Assessment of Large Scale Knowledge Resources
Montse Cuadros
IXA NLP Group
EHU/UPV
Donostia, Basque Country
mcuadros001@ikasle.ehu.es
German Rigau
IXA NLP Group
EHU/UPV
Donostia, Basque Country
german.rigau@ehu.es
Abstract
This paper presents an empirical eval-
uation of the quality of publicly avail-
able large-scale knowledge resources. The
study includes a wide range of manu-
ally and automatically derived large-scale
knowledge resources. In order to establish
a fair and neutral comparison, the qual-
ity of each knowledge resource is indi-
rectly evaluated using the same method on
a Word Sense Disambiguation task. The
evaluation framework selected has been
the Senseval-3 English Lexical Sample
Task. The study empirically demonstrates
that automatically acquired knowledge re-
sources surpass both in terms of preci-
sion and recall the knowledge resources
derived manually, and that the combina-
tion of the knowledge contained in these
resources is very close to the most frequent
sense classifier. As far as we know, this is
the first time that such a quality assessment
has been performed showing a clear pic-
ture of the current state-of-the-art of pub-
licly available wide coverage semantic re-
sources.
1 Introduction
Using large-scale semantic knowledge bases, such
as WordNet (Fellbaum, 1998), has become a
usual, often necessary, practice for most current
Natural Language Processing systems. Even now,
building large and rich enough knowledge bases
for broad?coverage semantic processing takes a
great deal of expensive manual effort involving
large research groups during long periods of de-
velopment. This fact has severely hampered the
state-of-the-art of current Natural Language Pro-
cessing (NLP) applications. For example, dozens
of person-years have been invested in the develop-
ment of wordnets for various languages (Vossen,
1998), but the data in these resources seems not to
be rich enough to support advanced concept-based
NLP applications directly. It seems that applica-
tions will not scale up to working in open domains
without more detailed and rich general-purpose
(and also domain-specific) linguistic knowledge
built by automatic means.
For instance, in more than eight years of man-
ual construction (from version 1.5 to 2.0), Word-
Net passed from 103,445 semantic relations to
204,074 semantic relations1. That is, around
twelve thousand semantic relations per year. How-
ever, during the last years the research commu-
nity has devised a large set of innovative processes
and tools for large-scale automatic acquisition of
lexical knowledge from structured or unstructured
corpora. Among others we can mention eX-
tended WordNet (Mihalcea and Moldovan, 2001),
large collections of semantic preferences acquired
from SemCor (Agirre and Martinez, 2001; Agirre
and Martinez, 2002) or acquired from British Na-
tional Corpus (BNC) (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired
from the web (Agirre and de la Calle, 2004) or
acquired from the BNC (Cuadros et al, 2005).
Obviously, all these semantic resources have
been acquired using a very different set of meth-
ods, tools and corpora, resulting on a different set
of new semantic relations between synsets. In fact,
each resource has different volume and accuracy
figures. Although isolated evaluations have been
performed by their developers in different experi-
1Symmetric relations are counted only once.
534
mental settings, to date no comparable evaluation
has been carried out in a common and controlled
framework.
This work tries to establish the relative qual-
ity of these semantic resources in a neutral envi-
ronment. The quality of each large-scale knowl-
edge resource is indirectly evaluated on a Word
Sense Disambiguation (WSD) task. In particular,
we use a well defined WSD evaluation benchmark
(Senseval-3 English Lexical Sample task) to eval-
uate the quality of each resource.
Furthermore, this work studies how these re-
sources complement each other. That is, to which
extent each knowledge base provides new knowl-
edge not provided by the others.
This paper is organized as follows: after this
introduction, section 2 describes the large-scale
knowledge resources studied in this work. Section
3 describes the evaluation framework. Section 4
presents the evaluation results of the different se-
mantic resources considered. Section 5 provides a
qualitative assessment of this empirical study and
finally, the conclusions and future work are pre-
sented in section 6.
2 Large Scale Knowledge Resources
This study covers a wide range of large-scale
knowledge resources: WordNet (WN) (Fell-
baum, 1998), eXtended WordNet (Mihalcea and
Moldovan, 2001), large collections of semantic
preferences acquired from SemCor (Agirre and
Martinez, 2001; Agirre and Martinez, 2002) or
acquired from the BNC (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired
from the web (Agirre and de la Calle, 2004) or
acquired from the BNC (Cuadros et al, 2005).
However, although these resources have been
derived using different WN versions, the research
community has the technology for the automatic
alignment of wordnets (Daude? et al, 2003). This
technology provides a mapping among synsets of
different WN versions, maintaining the compati-
bility to all the knowledge resources which use
a particular WN version as a sense repository.
Furthermore, this technology allows to port the
knowledge associated to a particular WN version
to the rest of WN versions already connected.
Using this technology, most of these resources
are integrated into a common resource called Mul-
tilingual Central Repository (MCR) (Atserias et
al., 2004). In particular, all WordNet versions, eX-
tended WordNet, and the semantic preferences ac-
quired from SemCor and BNC.
2.1 Multilingual Central Repository
The Multilingual Central Repository (MCR)2 fol-
lows the model proposed by the EuroWordNet
project. EuroWordNet (Vossen, 1998) is a multi-
lingual lexical database with wordnets for several
European languages, which are structured as the
Princeton WordNet. The Princeton WordNet con-
tains information about nouns, verbs, adjectives
and adverbs in English and is organized around the
notion of a synset. A synset is a set of words with
the same part-of-speech that can be interchanged
in a certain context. For example, <party, po-
litical party> form a synset because they can be
used to refer to the same concept. A synset is
often further described by a gloss, in this case:
?an organization to gain political power?. Finally,
synsets can be related to each other by semantic
relations, such as hyponymy (between specific and
more general concepts), meronymy (between parts
and wholes), cause, etc.
The current version of the MCR (Atserias et al,
2004) is a result of the 5th Framework MEANING
project. The MCR integrates into the same Eu-
roWordNet framework wordnets from five differ-
ent languages (together with four English Word-
Net versions). The MCR also integrates WordNet
Domains (Magnini and Cavaglia`, 2000) and new
versions of the Base Concepts and Top Concept
Ontology. The final version of the MCR contains
1,642,389 semantic relations between synsets,
most of them acquired by automatic means. This
represents almost one order of magnitude larger
than the Princeton WordNet (204,074 unique se-
mantic relations in WordNet 2.0). Table 1 summa-
rizes the main sources for semantic relations inte-
grated into the MCR.
Table 2 shows the number of semantic relations
between synsets pairs in the MCR and its overlap-
pings. Note that, most of the relations in the MCR
between synsets-pairs are unique.
Hereinafter we will refer to each semantic re-
source as follows:
? WN (Fellbaum, 1998): This knowledge re-
source uses the direct relations encoded in
WordNet 1.6 or 2.0. We also tested WN-2
(using relations at distance 1 and 2) and WN-
3 (using relations at distance 1, 2 and 3).
2http://nipadio.lsi.upc.es/?nlp/meaning
535
Source #relations
Princeton WN1.6 138,091
Selectional Preferences from SemCor 203,546
Selectional Preferences from the BNC 707,618
New relations from Princeton WN2.0 42,212
Gold relations from eXtended WN 17,185
Silver relations from eXtended WN 239,249
Normal relations from eXtended WN 294,488
Total 1,642,389
Table 1: Main sources of semantic relations
Type of Relations #relations
Total Relations 1,642,389
Different Relations 1,531,380
Unique Relations 1,390,181
Non-unique relations (>1) 70,425
Non-unique relations (>2) 341
Non-unique relations (>3) 8
Table 2: Overlapping relations in the MCR
? XWN (Mihalcea and Moldovan, 2001): This
knowledge resource uses the direct relations
encoded in eXtended WordNet.
? XWN+WN: This knowledge resource uses
the direct relations included in WN and
XWN.
? spBNC (McCarthy, 2001): This knowledge
resource contains the selectional preferences
acquired from the BNC.
? spSemCor (Agirre and Martinez, 2001;
Agirre and Martinez, 2002): This knowledge
resource contains the selectional preferences
acquired from SemCor.
? spBNC+spSemCor: This knowledge re-
source uses the selectional preferences ac-
quired from the BNC and SemCor.
? MCR (Atserias et al, 2004): This knowledge
resource uses the direct relations included in
MCR.
2.2 Automatically retrieved Topic Signatures
Topic Signatures (TS) are word vectors related to
a particular topic (Lin and Hovy, 2000). Topic
Signatures are built by retrieving context words
of a target topic from large volumes of text. In
our case, we consider word senses as topics. Ba-
sically, the acquisition of TS consists of A) ac-
quiring the best possible corpus examples for a
particular word sense (usually characterizing each
word sense as a query and performing a search on
the corpus for those examples that best match the
queries), and then, B) building the TS by deriv-
ing the context words that best represent the word
sense from the selected corpora.
For this study, we use the large-scale Topic Sig-
natures acquired from the web (Agirre and de la
Calle, 2004) and those acquired from the BNC
(Cuadros et al, 2005).
? TSWEB3: Inspired by the work of (Lea-
cock et al, 1998), these Topic Signatures
were constructed using monosemous rela-
tives from WordNet (synonyms, hypernyms,
direct and indirect hyponyms, and siblings),
querying Google and retrieving up to one
thousand snippets per query (that is, a word
sense). In particular, the method was as fol-
lows:
? Organizing the retrieved examples from
the web in collections, one collection
per word sense.
? Extracting the words and their frequen-
cies for each collection.
? Comparing these frequencies with those
pertaining to other word senses using
TFIDF (see formula 1).
? Gathering in an ordered list, the words
with distinctive frequency for one of the
collections, which constitutes the Topic
Signature for the respective word sense.
This constitutes the largest available seman-
tic resource with around 100 million relations
(between synsets and words).
? TSBNC: These Topic Signatures have been
constructed using ExRetriever4, a flexible
tool to perform sense queries on large cor-
pora.
? This tool characterizes each sense of a
word as a specific query using a declar-
ative language.
? This is automatically done by using a
particular query construction strategy,
defined a priori, and using information
from a knowledge base.
In this study, ExRetriever has been evaluated
using the BNC, WN as a knowledge base and
3http://ixa.si.ehu.es/Ixa/resources/sensecorpus
4http://www.lsi.upc.es/?nlp/meaning/downloads.html
536
TFIDF (as shown in formula 1) (Agirre and
de la Calle, 2004)5.
TFIDF (w,C) = wfwmaxwwfw ? log
N
Cfw (1)
Where w stands for word context, wf for the
word frecuency, C for Collection (all the cor-
pus gathered for a particular word sense), and
Cf stands for the Collection frecuency.
In this study we consider two different query
strategies:
? Monosemous A (queryA): (OR
monosemous-words). That is, the union
set of all synonym, hyponym and hyper-
onym words of a WordNet synset which are
monosemous nouns (these words can have
other senses as verbs, adjectives or adverbs).
? Monosemous W (queryW): (OR
monosemous-words). That is, the union
set of all words appearing as synonyms,
direct hyponyms, hypernyms indirect hy-
ponyms (distance 2 and 3) and siblings. In
this case, the nouns collected are monose-
mous having no other senses as verbs,
adjectives or adverbs.
While TSWEB use the query construction
queryW, ExRetriever use both.
3 Indirect Evaluation on Word Sense
Disambiguation
In order to measure the quality of the knowl-
edge resources described in the previous section,
we performed an indirect evaluation by using all
these resources as Topic Signatures (TS). That is,
word vectors with weights associated to a partic-
ular synset which are obtained by collecting those
word senses appearing in the synsets directly re-
lated to them 6. This simple representation tries to
be as neutral as possible with respect to the evalu-
ation framework.
All knowledge resources are indirectly evalu-
ated on a WSD task. In particular, the noun-set
5Although other measures have been tested, such as Mu-
tual Information or Association Ratio, the best results have
been obtained using TFIDF formula.
6A weight of 1 is given when the resource do not has as-
sociated weight.
of Senseval-3 English Lexical Sample task which
consists of 20 nouns. All performances are evalu-
ated on the test data using the fine-grained scoring
system provided by the organizers.
Furthermore, trying to be as neutral as possi-
ble with respect to the semantic resources studied,
we applied systematically the same disambigua-
tion method to all of them. Recall that our main
goal is to establish a fair comparison of the knowl-
edge resources rather than providing the best dis-
ambiguation technique for a particular semantic
knowledge base.
A common WSD method has been applied to
all knowledge resources. A simple word over-
lapping counting (or weighting) is performed be-
tween the Topic Signature and the test example7.
Thus, the occurrence evaluation measure counts
the amount of overlapped words and the weight
evaluation measure adds up the weights of the
overlapped words. The synset having higher over-
lapping word counts (or weights) is selected for a
particular test example. However, for TSWEB and
TSBNC the better results have been obtained us-
ing occurrences (the weights are only used to or-
der the words of the vector). Finally, we should
remark that the results are not skewed (for in-
stance, for resolving ties) by the most frequent
sense in WN or any other statistically predicted
knowledge.
Figure 3 presents an example of Topic Signa-
ture from TSWEB using queryW and the web and
from TSBNC using queryA and the BNC for the
first sense of the noun party. Although both auto-
matically acquired TS seem to be closely related to
the first sense of the noun party, they do not have
words in common.
As an example, table 4 shows a test example of
Senseval-3 corresponding to the first sense of the
noun party. In bold there are the words that ap-
pear in TSBNC-queryA. There are several impor-
tant words that appear in the text that also appear
in the TS.
4 Evaluating the quality of knowledge
resources
In order to establish a clear picture of the current
state-of-the-art of publicly available wide cover-
age knowledge resources we also consider a num-
ber of basic baselines.
7We also consider multiword terms.
537
democratic 0.0126 socialist 0.0062
tammany 0.0124 organization 0.0060
alinement 0.0122 conservative 0.0059
federalist 0.0115 populist 0.0053
missionary 0.0103 dixiecrats 0.0051
whig 0.0099 know-nothing 0.0049
greenback 0.0089 constitutional 0.0045
anti-masonic 0.0083 pecking 0.0043
nazi 0.0081 democratic-republican 0.0040
republican 0.0074 republicans 0.0039
alcoholics 0.0073 labor 0.0039
bull 0.0070 salvation 0.0038
party 4.9350 trade 1.5295
political 3.7722 parties 1.4083
government 2.4129 politics 1.2703
election 2.2265 campaign 1.2551
policy 2.0795 leadership 1.2277
support 1.8537 movement 1.2156
leader 1.8280 general 1.2034
years 1.7128 public 1.1874
people 1.7044 member 1.1855
local 1.6899 opposition 1.1751
conference 1.6702 unions 1.1563
power 1.6105 national 1.1474
Table 3: Topic Signatures for party#n#1 using TSWEB (24 out of 15881 total words) and TS-
BNC(queryA) with TFIDF (24 out of 9069 total words)
<instance id=?party.n.bnc.00008131? docsrc=?BNC?> <context> Up to the late 1960s , catholic nationalists were split between
two main political groupings . There was the Nationalist Party , a weak organization for which local priests had to provide
some kind of legitimation . As a <head>party</head> , it really only exercised a modicum of power in relation to the Stormont
administration . Then there were the republican parties who focused their attention on Westminster elections . The disorganized
nature of catholic nationalist politics was only turned round with the emergence of the civil rights movement of 1968 and the
subsequent forming of the SDLP in 1970 . </context> </instance>
Table 4: Example of test num. 00008131 for party#n which its correct sense is 1
4.1 Baselines
We have designed several baselines in order to es-
tablish a relative comparison of the performance
of each semantic resource:
? RANDOM: For each target word, this
method selects a random sense. This baseline
can be considered as a lower-bound.
? WordNet MFS (WN-MFS): This method
selects the most frequent sense (the first sense
in WordNet) of the target word.
? TRAIN-MFS: This method selects the most
frequent sense in the training corpus of the
target word.
? Train Topic Signatures (TRAIN): This
baseline uses the training corpus to directly
build a Topic Signature using TFIDF measure
for each word sense. Note that in this case,
this baseline can be considered as an upper-
bound of our evaluation framework.
Table 5 presents the F1 measure (harmonic
mean of recall and precision) of the different base-
lines. In this table, TRAIN has been calculated
with a fixed vector size of 450 words. As ex-
pected, RANDOM baseline obtains the poorest
result while the most frequent sense of Word-
Net (WN-MFS) is very close to the most frequent
sense of the training corpus (TRAIN-MFS), but
Baselines F1
TRAIN 65.1
TRAIN-MFS 54.5
WN-MFS 53.0
RANDOM 19.1
Table 5: Baselines
both are far below to the Topic Signatures acquired
using the training corpus (TRAIN).
4.2 Performance of the knowledge resources
Table 6 presents the performance of each knowl-
edge resource uploaded into the MCR and the av-
erage size of its vectors. In bold appear the best
results for precision, recall and F1 measures. The
lowest result is obtained by the knowledge directly
gathered from WN mainly because of its poor cov-
erage (Recall of 17.6 and F1 of 25.6). Its perfor-
mance is improved using words at distance 1 and
2 (F1 of 33.3), but it decreases using words at dis-
tance 1, 2 and 3 (F1 of 30.4). The best precision is
obtained by WN (46.7), but the best performance
is achieved by the combined knowledge of MCR-
spBNC8 (Recall of 42.9 and F1 of 44.1). This rep-
resents a recall 18.5 points higher than WN. That
is, the knowledge integrated into the MCR (Word-
Net, eXtended WordNet and the selectional prefer-
ences acquired from SemCor) although partly de-
rived by automatic means performs much better
8MCR without Selectional Preferences from BNC
538
KB P R F1 Av. Size
MCR-spBNC 45.4 42.9 44.1 115
MCR 41.8 40.4 41.1 235
spSemCor 43.1 38.7 40.8 56
spBNC+spSemCor 41.4 30.1 40.7 184
WN+XWN 45.5 28.1 34.7 68
WN-2 38.0 29.7 33.3 72
XWN 45.0 25.6 32.6 55
WN-3 31.6 29.3 30.4 297
spBNC 36.3 25.4 29.9 128
WN 46.7 17.6 25.6 13
Table 6: P, R and F1 fine-grained results for the
resources integrated into the MCR.
in terms of recall and F1 measures than using the
knowledge currently present in WN alone (with a
small decrease in precision). It also seems that the
knowledge from spBNC always degrades the per-
formance of their combinations9.
Regarding the baselines, all knowledge re-
sources integrated into the MCR surpass RAN-
DOM, but none achieves neither WN-MFS,
TRAIN-MFS nor TRAIN.
Figure 1 plots F1 results of the fine-grained
evaluation on the nominal part of the English lex-
ical sample of Senseval-3 of the baselines (in-
cluding upper and lower-bounds), the knowledge
bases integrated into the MCR, the best perform-
ing Topic Signatures acquired from the web and
the BNC evaluated individually and in combina-
tion with others. The figure presents F1 (Y-axis)
in terms of the size of the word vectors (X-axis)10.
In order to evaluate more deeply the quality of
each knowledge resource, we also provide some
evaluations of the combined outcomes of several
knowledge resources. The combinations are per-
formed following a very simple voting method:
first, for each knowledge resource, the scoring re-
sults obtained for each word sense are normal-
ized, and then, for each word sense, the normal-
ized scores are added up selecting the word sense
with higher score.
Regarding Topic Signatures, as expected, in
general the knowledge gathered from the web
(TSWEB) is superior to the one acquired from the
BNC either using queryA or queryW (TSBNC-
queryA and TSBNC-queryW). Interestingly, the
performance of TSBNC-queryA when using the
9All selectional preferences acquired from SemCor or the
BNC have been considered including those with very low
confidence score.
10Only varying the size of TS for TSWEB and TSBNC.
first two hundred words of the TS is slightly bet-
ter than using queryW (both using the web or the
BNC).
Although TSBNC-queryA and TSBNC-
queryW perform very similar, both knowledge
resources contain different knowledge. This is
shown when combining the outcomes of these
two different knowledge resources with TSWEB.
While no improvement is obtained when com-
bining the knowledge acquired from the web
and the BNC when using the same acquisition
method (queryW), the combination of TSWEB
and TSBNC-queryA (TSWEB+ExRetA) obtains
better F1 results than TSWEB (TSBNC-queryA
have some knowledge not included into TSWEB).
Surprisingly, the knowledge integrated into the
MCR (MCR-spBNC) surpass the knowledge from
Topic Signatures acquired from the web or the
BNC, using queryA, queryW or their combina-
tions.
Furthermore, the combination of TSWEB and
MCR-spBNC (TSWEB+MCR-spBNC) outper-
forms both resources individually indicating that
both knowledge bases contain complementary in-
formation. The maximum is achieved with TS
vectors of at most 700 words (with 49.3% preci-
sion, 49.2% recall and 49.2% F1). In fact, the
resulting combination is very close to the most
frequent sense baselines. This fact indicates that
the resulting large-scale knowledge base almost
encodes the knowledge necessary to behave as a
most frequent sense tagger.
4.3 Senseval-3 system performances
For sake of comparison, tables 7 and 8 present the
F1 measure of the fine-grained results for nouns
of the Senseval-3 lexical sample task for the best
and worst unsupervised and supervised systems,
respectively. We also include in these tables some
of the baselines and the best performing combina-
tion of knowledge resources (including TSWEB
and MCR-spBNC)11. Regarding the knowledge
resources evaluated in this study, the best com-
bination (including TSWEB and MCR-spBNC)
achieves an F1 measure much better than some su-
pervised and unsupervised systems and it is close
to the most frequent sense of WordNet (WN-MFS)
and to the most frequent sense of the training cor-
pora (TRAIN-MFS).
11Although we maintain the classification of the organiz-
ers, system s3 wsdiit used the train data.
539
Figure 1: Fine-grained evaluation results for the knowledge resources
s3 systems F1
s3 wsdiit 68.0
WN-MFS 53.0
Comb TSWEB MCR-spBNC 49.2
s3 DLSI 17.8
Table 7: Senseval-3 Unsupervised Systems
s3 systems F1
htsa3 U.Bucharest (Grozea) 74.2
TRAIN 65.1
TRAIN-MFS 54.5
DLSI-UA-LS-SU U.Alicante (Vazquez) 41.0
Table 8: Senseval-3 Supervised Systems
We must recall that the main goal of this re-
search is to establish a clear and neutral view of the
relative quality of available knowledge resources,
not to provide the best WSD algorithm using these
resources. Obviously, much more sophisticated
WSD systems using these resources could be de-
vised.
5 Quality Assessment
Summarizing, this study provides empirical evi-
dence for the relative quality of publicly avail-
able large-scale knowledge resources. The rela-
tive quality has been measured indirectly in terms
of precision and recall on a WSD task.
The study empirically demonstrates that auto-
matically acquired knowledge bases clearly sur-
pass both in terms of precision and recall the
knowledge manually encoded from WordNet (us-
ing relations expanded to one, two or three levels).
Surprisingly, the knowledge contained into the
MCR (WordNet, eXtended WordNet, Selectional
Preferences acquired automatically from SemCor)
is of a better quality than the automatically ac-
quired Topic Signatures. In fact, the knowledge
resulting from the combination of all these large-
scale resources outperforms each resource indi-
vidually indicating that these knowledge bases
contain complementary information. Finally, we
should remark that the resulting combination is
very close to the most frequent sense classifiers.
Regarding the automatic acquisition of large-
scale Topic Signatures it seems that those ac-
quired from the web are slightly better than those
acquired from smaller corpora (for instance, the
BNC). It also seems that queryW performs better
than queryA but that both methods (queryA and
540
queryW) also produce complementary knowledge.
Finally, it seems that the weights are not useful for
measuring the strength of a vote (they are only use-
ful for ordering the words in the Topic Signature).
6 Conclusions and future work
During the last years the research community has
derived a large set of semantic resources using a
very different set of methods, tools and corpus, re-
sulting on a different set of new semantic relations
between synsets. In fact, each resource has dif-
ferent volume and accuracy figures. Although iso-
lated evaluations have been performed by their de-
velopers in different experimental settings, to date
no complete evaluation has been carried out in a
common framework.
In order to establish a fair comparison, the qual-
ity of each resource has been indirectly evaluated
in the same way on a WSD task. The evaluation
framework selected has been the Senseval-3 En-
glish Lexical Sample Task. The study empirically
demonstrates that automatically acquired knowl-
edge bases surpass both in terms of precision and
recall to the knowledge bases derived manually,
and that the combination of the knowledge con-
tained in these resources is very close to the most
frequent sense classifier.
Once empirically demonstrated that the knowl-
edge resulting from MCR and Topic Signatures ac-
quired from the web is complementary and close
to the most frequent sense classifier, we plan to
integrate the Topic Signatures acquired from the
web (of about 100 million relations) into the MCR.
This process will be performed by disambiguat-
ing the Topic Signatures. That is, trying to obtain
word sense vectors instead of word vectors. This
will allow to enlarge the existing knowledge bases
in several orders of magnitude by fully automatic
methods. Other evaluation frameworks such as PP
attachment will be also considered.
7 Acknowledgements
This work is being funded by the IXA NLP
group from the Basque Country Univer-
sity, EHU/UPV-CLASS project and Basque
Government-ADIMEN project. We would like
to thank also the three anonymous reviewers for
their valuable comments.
References
E. Agirre and O. Lopez de la Calle. 2004. Publicly
available topic signatures for all wordnet nominal
senses. In Proceedings of LREC, Lisbon, Portugal.
E. Agirre and D. Martinez. 2001. Learning class-
to-class selectional preferences. In Proceedings of
CoNLL, Toulouse, France.
E. Agirre and D. Martinez. 2002. Integrating selec-
tional preferences in wordnet. In Proceedings of
GWC, Mysore, India.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and Piek Vossen. 2004. The meaning
multilingual central repository. In Proceedings of
GWC, Brno, Czech Republic.
M. Cuadros, L. Padro?, and G. Rigau. 2005. Compar-
ing methods for automatic acquisition of topic sig-
natures. In Proceedings of RANLP, Borovets, Bul-
garia.
J. Daude?, L. Padro?, and G. Rigau. 2003. Validation and
Tuning of Wordnet Mapping Techniques. In Pro-
ceedings of RANLP, Borovets, Bulgaria.
C. Fellbaum, editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24(1):147?166.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING. Strasbourg, France.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into wordnet. In Proceedings of LREC,
Athens. Greece.
D. McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Sub-
categorization Frames and Selectional Preferences.
Ph.D. thesis, University of Sussex.
R. Mihalcea and D. Moldovan. 2001. extended word-
net: Progress report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources,
Pittsburgh, PA.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers .
541
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 161?168
Manchester, August 2008
KnowNet: Building a Large Net of Knowledge from the Web
Montse Cuadros
TALP Research Center, UPC
Barcelona, Spain
cuadros@lsi.upc.edu
German Rigau
IXA NLP Group, UPV/EHU
Donostia, Spain
german.rigau@ehu.es
Abstract
This paper presents a new fully auto-
matic method for building highly dense
and accurate knowledge bases from ex-
isting semantic resources. Basically, the
method uses a wide-coverage and accu-
rate knowledge-based Word Sense Dis-
ambiguation algorithm to assign the most
appropriate senses to large sets of topi-
cally related words acquired from the web.
KnowNet, the resulting knowledge-base
which connects large sets of semantically-
related concepts is a major step towards
the autonomous acquisition of knowledge
from raw corpora. In fact, KnowNet is sev-
eral times larger than any available knowl-
edge resource encoding relations between
synsets, and the knowledge KnowNet con-
tains outperform any other resource when
is empirically evaluated in a common
framework.
1 Introduction
Using large-scale knowledge bases, such as Word-
Net (Fellbaum, 1998), has become a usual, of-
ten necessary, practice for most current Natural
Language Processing (NLP) systems. Even now,
building large and rich enough knowledge bases
for broad?coverage semantic processing takes a
great deal of expensive manual effort involving
large research groups during long periods of de-
velopment. In fact, hundreds of person-years have
been invested in the development of wordnets for
various languages (Vossen, 1998). For example, in
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
more than ten years of manual construction (from
1995 to 2006, that is from version 1.5 to 3.0),
WordNet grew from 103,445 to 235,402 semantic
relations
1
. But this data does not seem to be rich
enough to support advanced concept-based NLP
applications directly. It seems that applications
will not scale up to work in open domains without
more detailed and rich general-purpose (and also
domain-specific) semantic knowledge built by au-
tomatic means. Obviously, this fact has severely
hampered the state-of-the-art of advanced NLP ap-
plications.
However, the Princeton WordNet (WN) is by far
the most widely-used knowledge base (Fellbaum,
1998). In fact, WordNet is being used world-wide
for anchoring different types of semantic knowl-
edge including wordnets for languages other than
English (Atserias et al, 2004), domain knowledge
(Magnini and Cavagli`a, 2000) or ontologies like
SUMO (Niles and Pease, 2001) or the EuroWord-
Net Top Concept Ontology (
?
Alvez et al, 2008).
It contains manually coded information about En-
glish nouns, verbs, adjectives and adverbs and is
organized around the notion of a synset. A synset
is a set of words with the same part-of-speech that
can be interchanged in a certain context. For ex-
ample, <party, political party> form a synset be-
cause they can be used to refer to the same concept.
A synset is often further described by a gloss, in
this case: ?an organization to gain political power?
and by explicit semantic relations to other synsets.
Fortunately, during the last years the research
community has devised a large set of innovative
methods and tools for large-scale automatic acqui-
sition of lexical knowledge from structured and un-
structured corpora. Among others we can men-
1
Symmetric relations are counted only once.
161
tion eXtended WordNet (Mihalcea and Moldovan,
2001), large collections of semantic preferences
acquired from SemCor (Agirre and Martinez,
2001; Agirre and Martinez, 2002) or acquired from
British National Corpus (BNC) (McCarthy, 2001),
large-scale Topic Signatures for each synset ac-
quired from the web (Agirre and de Lacalle, 2004)
or knowledge about individuals from Wikipedia
(Suchanek et al, 2007). Obviously, all these se-
mantic resources have been acquired using a very
different methods, tools and corpora. As expected,
each semantic resource has different volume and
accuracy figures when evaluated in a common and
controlled framework (Cuadros and Rigau, 2006).
However, not all these large-scale resources en-
code semantic relations between synsets. In some
cases, only relations between synsets and words
have been acquired. This is the case of the Topic
Signatures acquired from the web (Agirre and de
Lacalle, 2004). This is one of the largest seman-
tic resources ever built with around one hundred
million relations between synsets and semantically
related words
2
.
A knowledge net or KnowNet (KN), is an exten-
sible, large and accurate knowledge base, which
has been derived by semantically disambiguating
small portions of the Topic Signatures acquired
from the web. Basically, the method uses a ro-
bust and accurate knowledge-based Word Sense
Disambiguation algorithm to assign the most ap-
propriate senses to the topic words associated to
a particular synset. The resulting knowledge-base
which connects large sets of topically-related con-
cepts is a major step towards the autonomous ac-
quisition of knowledge from raw text.
Table 1 compares the different volumes of se-
mantic relations between synset pairs of avail-
able knowledge bases and the newly created
KnowNets
3
.
Varying from five to twenty the number of pro-
cessed words from each Topic Signature, we cre-
ated automatically four different KnowNet ver-
sions with millions of new semantic relations be-
tween synsets. In fact, KnowNet is several times
larger than WordNet, and when evaluated empir-
ically in a common framework, the knowledge it
contains outperforms any other semantic resource.
After this introduction, section 2 describes the
Topic Signatures acquired from the web. Section
2
Available at http://ixa.si.ehu.es/Ixa/resources/sensecorpus
3
These KnowNet versions are available at
http://adimen.si.ehu.es
Source #relations
Princeton WN3.0 235,402
Selectional Preferences from SemCor 203,546
eXtended WN 550,922
Co-occurring relations from SemCor 932,008
New KnowNet-5 231,163
New KnowNet-10 689,610
New KnowNet-15 1,378,286
New KnowNet-20 2,358,927
Table 1: Number of synset relations
3 presents the approach we followed for building
highly dense and accurate knowledge bases from
the Topic Signatures. In section 4, we present the
evaluation framework used in this study. Section 5
describes the results when evaluating different ver-
sions of KnowNet and finally, section 6 presents
some concluding remarks and future work.
2 Topic Signatures
Topic Signatures (TS) are word vectors related to a
particular topic (Lin and Hovy, 2000). Topic Sig-
natures are built by retrieving context words of a
target topic from a large corpora. This study con-
siders word senses as topics. Basically, the acqui-
sition of TS consists of:
? acquiring the best possible corpus examples
for a particular word sense (usually character-
izing each word sense as a query and perform-
ing a search on the corpus for those examples
that best match the queries)
? building the TS by selecting the context
words that best represent the word sense from
the selected corpora.
The Topic Signatures acquired from the web
(hereinafter TSWEB) constitutes one of the largest
semantic resource available with around 100 mil-
lion relations (between synsets and words) (Agirre
and de Lacalle, 2004). Inspired by the work of
(Leacock et al, 1998), TSWEB was constructed
using monosemous relatives from WN (synonyms,
hypernyms, direct and indirect hyponyms, and sib-
lings), querying Google and retrieving up to one
thousand snippets per query (that is, a word sense),
extracting the salient words with distinctive fre-
quency using TFIDF. Thus, TSWEB consist of
large ordered lists of words with weights associ-
ated to the polysemous nouns of WN1.6. The
number of constructed topic signatures is 35,250
with an average size per signature of 6,877 words.
162
tammany#n 0.0319
federalist#n 0.0315
whig#n 0.0300
missionary#j 0.0229
Democratic#n 0.0218
nazi#j 0.0202
republican#n 0.0189
constitutional#n 0.0186
conservative#j 0.0148
socialist#n 0.0140
Table 2: TS of party#n#1 (first 10 out of 12,890
total words)
When evaluating TSWEB, we used at maximum
the first 700 words while for building KnowNet we
used at maximum the first 20 words.
For example, table 2 presents the first words
(lemmas and part-of-speech) and weights of the
Topic Signature acquired for party#n#1
4
.
3 Building highly connected and dense
knowledge bases
We acquired by fully automatic means highly
connected and dense knowledge bases by disam-
biguating small portions of the Topic Signatures
obtained from the web, increasing the total num-
ber of semantic relations from less than one mil-
lion (the current number of available relations) to
millions of new and accurate semantic relations
between synsets. We applied a knowledge?based
all?words Word Sense Disambiguation algorithm
to the Topic Signatures for deriving a sense vector
from each word vector.
3.1 SSI-Dijkstra
We have implemented a version of the Struc-
tural Semantic Interconnections algorithm (SSI), a
knowledge-based iterative approach to Word Sense
Disambiguation (Navigli and Velardi, 2005). The
SSI algorithm is very simple and consists of an ini-
tialization step and a set of iterative steps (see al-
gorithm 1).
Given W, an ordered list of words to be dis-
ambiguated, the SSI algorithm performs as fol-
lows. During the initialization step, all monose-
mous words are included into the set I of already
interpreted words, and the polysemous words are
included in P (all of them pending to be disam-
biguated). At each step, the set I is used to disam-
biguate one word of P, selecting the word sense
which is closer to the set I of already disam-
4
This format stands for word#pos#sense.
biguated words. Once a sense is selected, the word
sense is removed from P and included into I. The
algorithm finishes when no more pending words
remain in P.
Algorithm 1 SSI-Dijkstra Algorithm
SSI (T: list of terms)
for each {t ? T} do
I[t] = ?
if t is monosemous then
I[t] := the only sense of t
else
P := P ? {t}
end if
end for
repeat
P
?
:= P
for each {t ? P} do
BestSense := ?
MaxV alue := 0
for each {sense s of t} do
W [s] := 0
N [s] := 0
for each {sense s
?
? I} do
w := DijsktraShortestPath(s, s
?
)
if w > 0 then
W [s] := W [s] + (1/w)
N [s] := N [s] + 1
end if
end for
if N [s] > 0 then
NewV alue := W [s]/N [s]
if NewV alue > MaxV alue then
MaxV alue := NewV alue
BestSense := s
end if
end if
end for
if MaxV alue > 0 then
I[t] := BestSense
P := P \ {t}
end if
end for
until P 6= P
?
return (I, P);
Initially, the list I of interpreted words should in-
clude the senses of the monosemous words in W,
or a fixed set of word senses
5
. However, when dis-
5
If no monosemous words are found or if no initial senses
are provided, the algorithm could make an initial guess based
on the most probable sense of the less ambiguous word of W.
163
ambiguating a TS of a word sense s (for instance
party#n#1), the list I already includes s.
In order to measure the proximity of one synset
to the rest of synsets of I, we use part of the
knowledge already available to build a very large
connected graph with 99,635 nodes (synsets) and
636,077 edges. This graph includes the set of
direct relations between synsets gathered from
WordNet and eXtended WordNet. On that graph,
we used a very efficient graph library, Boost-
Graph
6
to compute the Dijkstra algorithm. The
Dijkstra algorithm is a greedy algorithm for com-
puting the shortest path distance between one node
an the rest of nodes of a graph. In that way, we can
compute very efficiently the shortest distance be-
tween any two given nodes of a graph. We call this
version of the SSI algorithm, SSI-Dijkstra.
SSI-Dijkstra has very interesting properties. For
instance, it always provides the minimum distance
between two synsets. That is, the algorithm always
provides an answer being the minimum distance
close or far. In contrast, the original SSI algorithm
not always provides a path distance because it de-
pends on a predefined grammar of semantic rela-
tions. In fact, the SSI-Dijkstra algorithm compares
the distances between the synsets of a word and all
the synsets already interpreted in I. At each step,
the SSI-Dijkstra algorithm selects the synset which
is closer to I (the set of already interpreted words).
Furthermore, this approach is completely lan-
guage independent. The same graph can be used
for any language having words connected to Word-
Net.
3.2 Building KnowNet
We developed KnowNet (KN), a large-scale and
extensible knowledge base, by applying SSI-
Dijkstra to each topic signature from TSWEB.
We have generated four different versions of
KnowNet applying SSI-Dijkstra to only the first
5, 10, 15 and 20 words for each TS. SSI-Dijkstra
used only the knowledge present in WordNet and
eXtended WordNet which consist of a very large
connected graph with 99,635 nodes (synsets) and
636,077 edges (semantic relations).
We generated each KnowNet by applying the
SSI-Dijkstra algorithm to the whole TSWEB (pro-
cessing the first words of each of the 35,250
topic signatures). For each TS, we obtained the
direct relations from the topic (a word sense)
6
http://www.boost.org
KB WN+XWN #relations #synsets
KN-5 3,1% 231,163 39,864
KN-10 5,0% 689,610 45,817
KN-15 6,9% 1,378,286 48,521
KN-20 8,5% 2,358,927 50,789
Table 3: Size and percentage of overlapping rela-
tions between KnowNet versions and WN+XWN
to the disambiguated word senses of the TS
(for instance, party#n#1?>federalist#n#1), but
also the indirect relations between disambiguated
words from the TS (for instance, federalist#n#1?
>republican#n#1). Finally, we removed symmet-
ric and repeated relations.
Table 3 shows the overlaping percentage be-
tween each KnowNet and the knowledge con-
tained into WordNet and eXtended WordNet, and
the total number of relations and synsets of each
resource. For instance, only 8,5% of the total di-
rect relations included into WN+XWN are also
present in KnowNet-20. This means that the rest
of relations from KnowNet-20 are new. As ex-
pected, each KnowNet is very large, ranging from
hundreds of thousands to millions of new semantic
relations between synsets among increasing sets of
synsets.
4 Evaluation framework
In order to empirically establish the relative qual-
ity of these new semantic resources, we used the
evaluation framework of task 16 of SemEval-2007:
Evaluation of wide coverage knowledge resources
(Cuadros and Rigau, 2007).
In this framework all knowledge resources are
evaluated on a common WSD task. In particu-
lar, we used the noun-sets of the English Lexi-
cal Sample task of Senseval-3 and SemEval-2007
exercises which consists of 20 and 35 nouns re-
spectively. All performances are evaluated on the
test data using the fine-grained scoring system pro-
vided by the organizers.
Furthermore, trying to be as neutral as possible
with respect to the resources studied, we applied
systematically the same disambiguation method to
all of them. Recall that our main goal is to es-
tablish a fair comparison of the knowledge re-
sources rather than providing the best disambigua-
tion technique for a particular knowledge base. All
knowledge bases are evaluated as topic signatures.
That is, word vectors with weights associated to a
particular synset which are obtained by collecting
164
those word senses appearing in the synsets directly
related to the topics. This simple representation
tries to be as neutral as possible with respect to the
resources used.
A common WSD method has been applied to all
knowledge resources. A simple word overlapping
counting is performed between the topic signature
representing a word sense and the test example
7
.
The synset having higher overlapping word counts
is selected. In fact, this is a very simple WSD
method which only considers the topical informa-
tion around the word to be disambiguated. Finally,
we should remark that the results are not skewed
(for instance, for resolving ties) by the most fre-
quent sense in WN or any other statistically pre-
dicted knowledge.
4.1 Baselines
We have designed a number of baselines in order
to establish a complete evaluation framework for
comparing the performance of each semantic re-
source on the English WSD tasks.
RANDOM: For each target word, this method
selects a random sense. This baseline can be con-
sidered as a lower-bound.
SEMCOR-MFS: This baseline selects the most
frequent sense of the target word in SemCor.
WN-MFS: This baseline is obtained by se-
lecting the most frequent sense (the first sense
in WN1.6) of the target word. WordNet word-
senses were ranked using SemCor and other sense-
annotated corpora. Thus, WN-MFS and SemCor-
MFS are similar, but not equal.
TRAIN-MFS: This baseline selects the most
frequent sense in the training corpus of the target
word.
TRAIN: This baseline uses the training corpus
to directly build a Topic Signature using TFIDF
measure for each word sense and selecting at max-
imum the first 450 words. Note that in WSD eval-
uation frameworks, this is a very basic baseline.
However, in our evaluation framework, this ?WSD
baseline? could be considered as an upper-bound.
We do not expect to obtain better topic signatures
for a particular sense than from its own annotated
corpus.
4.2 Other Large-scale Knowledge Resources
In order to measure the relative quality of the new
resources, we include in the evaluation a wide
7
We also consider those multiword terms appearing in
WN.
range of large-scale knowledge resources con-
nected to WordNet.
WN (Fellbaum, 1998): This resource uses the
different direct relations encoded in WN1.6 and
WN2.0. We also tested WN
2
using relations at dis-
tance 1 and 2, WN
3
using relations at distances 1
to 3 and WN
4
using relations at distances 1 to 4.
XWN (Mihalcea and Moldovan, 2001): This
resource uses the direct relations encoded in eX-
tended WN.
spBNC (McCarthy, 2001): This resource con-
tains 707,618 selectional preferences acquired for
subjects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This
resource contains the selectional preferences ac-
quired for subjects and objects from SemCor.
MCR (Atserias et al, 2004): This resource in-
tegrates the direct relations of WN, XWN and
spSemCor.
TSSEM (Cuadros et al, 2007): These Topic
Signatures have been constructed using Sem-
Cor.For each word-sense appearing in SemCor, we
gather all sentences for that word sense, building a
TS using TFIDF for all word-senses co-occurring
in those sentences.
4.3 Integrated Knowledge Resources
We also evaluated the performance of the integra-
tion (removing duplicated relations) of some of
these resources.
WN+XWN: This resource integrates the di-
rect relations of WN and XWN. We also tested
(WN+XWN)
2
(using either WN or XWN rela-
tions at distances 1 and 2).
MCR (Atserias et al, 2004): This resource in-
tegrates the direct relations of WN, XWN and
spSemCor.
WN+XWN+KN-20: This resource integrates
the direct relations of WN, XWN and KnowNet-
20.
5 KnowNet Evaluation
We evaluated KnowNet using the same framework
explained in section 4. That is, the noun part of the
test set from the English Senseval-3 and SemEval-
2007 English lexical sample tasks.
5.1 Senseval-3 evaluation
Table 4 presents ordered by F1 measure, the per-
formance in terms of precision (P), recall (R) and
165
KB P R F1 Av. Size
TRAIN 65.1 65.1 65.1 450
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
TSSEM 52.5 52.4 52.4 103
SEMCOR-MFS 49.0 49.1 49.0
MCR
2
45.1 45.1 45.1 26,429
WN+XWN+KN-20 44.8 44.8 44.8 671
MCR 45.3 43.7 44.5 129
KnowNet-20 44.1 44.1 44.1 610
KnowNet-15 43.9 43.9 43.9 339
spSemCor 43.1 38.7 40.8 56
KnowNet-10 40.1 40.0 40.0 154
(WN+XWN)
2
38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
KnowNet-5 35.0 35.0 35.0 44
WN
3
35.0 34.7 34.8 503
WN
4
33.2 33.1 33.2 2,346
WN
2
33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
RANDOM 19.1 19.1 19.1
Table 4: P, R and F1 fine-grained results for the
resources evaluated at Senseval-3, English Lexical
Sample Task.
F1 measure (F1, harmonic mean of recall and pre-
cision) of each knowledge resource on Senseval-3
and the average size of the TS per word-sense. The
different KnowNet versions appear marked in bold
and the baselines appear in italics.
As expected, RANDOM obtains the poorest re-
sult. The most frequent senses obtained from Sem-
Cor (SEMCOR-MFS) and WN (WN-MFS) are
both below the most frequent sense of the training
corpus (TRAIN-MFS). However, all of them are
far below to the Topic Signatures acquired using
the training corpus (TRAIN).
The best results are obtained by TSSEM (with
F1 of 52.4). The lowest result is obtained by the
knowledge directly gathered from WN mainly be-
cause of its poor coverage (R of 18.4 and F1 of
26.1). Interestingly, the knowledge integrated in
the MCR although partly derived by automatic
means performs much better in terms of precision,
recall and F1 measures than using them separately
(F1 with 18.4 points higher than WN, 9.1 than
XWN and 3.7 than spSemCor).
Despite its small size, the resources derived
from SemCor obtain better results than its coun-
terparts using much larger corpora (TSSEM vs.
TSWEB and spSemCor vs. spBNC).
Regarding the baselines, all knowledge re-
sources surpass RANDOM, but none achieves nei-
ther WN-MFS, TRAIN-MFS nor TRAIN. Only
TSSEM obtains better results than SEMCOR-MFS
and is very close to the most frequent sense of WN
(WN-MFS) and the training (TRAIN-MFS).
Regarding the expansions and combinations, the
performance of WN is improved using words at
distances up to 2, and up to 3, but it decreases using
distances up to 4. Interestingly, none of these WN
expansions achieve the results of XWN. Finally,
(WN+XWN)
2
performs better than WN+XWN
and MCR
2
slightly better than MCR
8
.
The different versions of KnowNet consistently
obtain better performances as they increase the
window size of processed words of TSWEB. As
expected, KnowNet-5 obtain the lower results.
However, it performs better than WN (and all
its extensions) and spBNC. Interestingly, from
KnowNet-10, all KnowNet versions surpass the
knowledge resources used for their construction
(WN, XWN, TSWEB and WN+XWN). In fact,
KnowNet-10 also outperforms (WN+XWN)
2
with
much more relations per sense. Also interesting
is that KnowNet-10 and KnowNet-20 obtain bet-
ter performance than spSemCor which was derived
from annotated corpora. However, KnowNet-20
only performs slightly better than KnowNet-15
while almost doubling the number of relations.
These initial results seem to be very promis-
ing. If we do not consider the resources derived
from manually sense annotated data (spSemCor,
MCR, TSSEM, etc.), KnowNet-10 performs bet-
ter that any knowledge resource derived by man-
ual or automatic means. In fact, KnowNet-15 and
KnowNet-20 outperforms spSemCor which was
derived from manually annotated corpora. This is
a very interesting result since these KnowNet ver-
sions have been derived only with the knowledge
coming from WN and the web (that is, TSWEB),
and WN and XWN as a knowledge source for SSI-
Dijkstra
9
.
Regarding the integration of resources,
WN+XWN+KN-20 performs better than MCR
and similarly to MCR
2
(having less than 50 times
its size). Also interesting is that WN+XWN+KN-
20 have better performance than their individual
resources, indicating a complementary knowledge.
In fact, WN+XWN+KN-20 performs much better
than the resources from which it derives (WN,
XWN and TSWEB).
8
No further distances have been tested
9
eXtended WordNet only has 17,185 manually labeled
senses.
166
KB P R F1 Av. Size
TRAIN 87.6 87.6 87.6 450
TRAIN-MFS 81.2 79.6 80.4
WN-MFS 66.2 59.9 62.9
WN+XWN+KN-20 53.0 53.0 53.0 627
(WN+XWN)
2
54.9 51.1 52.9 5,153
TSWEB 54.8 47.8 51.0 700
KnowNet-20 49.5 46.1 47.7 561
KnowNet-15 47.0 43.5 45.2 308
XWN 50.1 39.8 44.4 96
KnowNet-10 44.0 39.8 41.8 139
WN+XWN 45.4 36.8 40.7 101
SEMCOR-MFS 42.4 38.4 40.3
MCR 40.2 35.5 37.7 149
TSSEM 35.1 32.7 33.9 428
KnowNet-5 35.5 26.5 30.3 41
MCR
2
32.4 29.5 30.9 24,896
WN
3
29.3 26.3 27.7 584
RANDOM 27.4 27.4 27.4
WN
2
25.9 27.4 26.6 72
spSemCor 31.4 23.0 26.5 51.0
WN
4
26.1 23.9 24.9 2,710
WN 36.8 16.1 22.4 13
spBNC 24.4 18.1 20.8 290
Table 5: P, R and F1 fine-grained results for the re-
sources evaluated at SemEval-2007, English Lexi-
cal Sample Task.
5.2 SemEval-2007 evaluation
Table 5 presents ordered by F1 measure, the per-
formance in terms of precision (P), recall (R) and
F1 measure (F1) of each knowledge resource on
SemEval-2007 and its average size of the TS per
word-sense
10
. Again, the different KnowNet ver-
sions appear marked in bold and the baselines ap-
pear in italics.
As in the previous evaluation, RANDOM ob-
tains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both far below the most frequent
sense of the training corpus (TRAIN-MFS), and all
of them are below the Topic Signatures acquired
using the training corpus (TRAIN).
Interestingly, on SemEval-2007, all the knowl-
edge resources behave differently. Now, the best
individual results are obtained by TSWEB, while
in this case TSSEM obtains very modest results.
The lowest result is obtained by the knowledge en-
coded in spBNC.
Regarding the baselines, spBNC, WN (and also
WN
2
and WN
4
) and spSemCor do not surpass
RANDOM, and none achieves neither WN-MFS,
TRAIN-MFS nor TRAIN. Now, WN+XWN,
XWN, TSWEB and (WN+XWN)
2
obtain better
10
The average size is different with respect Senseval-3 be-
cause the words selected for this task are different
results than SEMCOR-MFS but far below the most
frequent sense of WN (WN-MFS) and the training
(TRAIN-MFS).
Regarding other expansions and combinations,
the performance of WN is improved using words
at distances up to 2, and up to 3, but it decreases
using distances up to 4. Again, none of these WN
expansions achieve the results of XWN. Finally,
(WN+XWN)
2
performs better than WN+XWN
and MCR
2
slightly better than MCR
11
.
On SemEval-2007, the different versions of
KnowNet consistently obtain better performances
as they incease the window size of processed
words of TSWEB. As expected, KnowNet-5 ob-
tain the lower results. However, it performs better
than spBNC, WN (and all its extensions), spSem-
Cor and MCR
2
. This time, all KnowNet ver-
sions perform worse than TSWEB. However, as in
the previous evaluation, KnowNet-10 outperforms
WN+XWN, and this time, also TSSEM and the
MCR, with much more relations per sense. Also
interesting is that from KnowNet-10, all KnowNet
versions perform better than the resources derived
from manually sense annotated corpora (spSem-
Cor, MCR, TSSEM, etc.).
Regarding the integration of resources,
WN+XWN+KN-20 performs better than any
knowledge resource derived by manual or auto-
matic means. Again, it is interesting to note that
WN+XWN+KN-20 have better performance than
their individual resources, indicating a comple-
mentary knowledge. In fact, WN+XWN+KN-20
performs much better than the resources from
which it derives (WN, XWN and TSWEB).
5.3 Discussion
When comparing the ranking of the different
knowledge resources, the different versions of
KnowNet seem to be more robust and stable
across corpora changes. For instance, in both
evaluation frameworks (Senseval-3 and SemEval-
2007), KnowNet-20 ranks 5th and 4th, respec-
tively ((WN+XWN)
2
ranks 8th and 2nd, TSSEM
ranks 1st and 10th, MCR ranks 4th and 9th,
TSWEB ranks 11th and 3rd, etc.). In fact,
WN+XWN+KN-20 ranks 3rd and 1st, respec-
tively.
11
No further distances have been tested
167
6 Conclusions and future research
It is our belief, that accurate semantic processing
(such as WSD) would rely not only on sophisti-
cated algorithms but on knowledge intensive ap-
proaches. The results presented in this paper sug-
gests that much more research on acquiring and
using large-scale semantic resources should be ad-
dressed.
The knowledge acquisition bottleneck problem
is particularly acute for open domain (and also
domain specific) semantic processing. The ini-
tial results obtained for the different versions of
KnowNet seem to be a major step towards the au-
tonomous acquisition of knowledge from raw cor-
pora, since they are several times larger than the
available knowledge resources which encode re-
lations between synsets, and the knowledge they
contain outperform any other resource when is em-
pirically evaluated in a common framework.
It remains for future research the evaluation of
these KnowNet versions in combination with other
large-scale semantic resources or in a cross-lingual
setting.
Acknowledgments
We want to thank Aitor Soroa for his technical
support and the anonymous reviewers for their
comments. This work has been supported by
KNOW (TIN2006-15049-C03-01) and KYOTO
(ICT-2007-211423).
References
Agirre, E. and O. Lopez de Lacalle. 2004. Publicly
available topic signatures for all wordnet nominal
senses. In Proceedings of LREC, Lisbon, Portugal.
Agirre, E. and D. Martinez. 2001. Learning class-
to-class selectional preferences. In Proceedings of
CoNLL, Toulouse, France.
Agirre, E. and D. Martinez. 2002. Integrating selec-
tional preferences in wordnet. In Proceedings of
GWC, Mysore, India.
?
Alvez, J., J. Atserias, J. Carrera, S. Climent, A. Oliver,
and G. Rigau. 2008. Consistent annotation of eu-
rowordnet with the top concept ontology. In Pro-
ceedings of Fourth International WordNet Confer-
ence (GWC?08).
Atserias, J., L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and Piek Vossen. 2004. The mean-
ing multilingual central repository. In Proceedings
of GWC, Brno, Czech Republic.
Cuadros, M. and G. Rigau. 2006. Quality assessment
of large scale knowledge resources. In Proceedings
of the EMNLP.
Cuadros, M. and G. Rigau. 2007. Semeval-2007
task 16: Evaluation of wide coverage knowledge re-
sources. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007).
Cuadros, M., G. Rigau, and M. Castillo. 2007. Eval-
uating large-scale knowledge resources across lan-
guages. In Proceedings of RANLP.
Fellbaum, C., editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
Leacock, C., M. Chodorow, and G. Miller. 1998.
Using Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24(1):147?166.
Lin, C. and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. In
Proceedings of COLING. Strasbourg, France.
Magnini, B. and G. Cavagli`a. 2000. Integrating subject
field codes into wordnet. In Proceedings of LREC,
Athens. Greece.
McCarthy, D. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Sub-
categorization Frames and Selectional Preferences.
Ph.D. thesis, University of Sussex.
Mihalcea, R. and D. Moldovan. 2001. extended word-
net: Progress report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources,
Pittsburgh, PA.
Navigli, R. and P. Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach to
word sense disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI),
27(7):1063?1074.
Niles, I. and A. Pease. 2001. Towards a standard up-
per ontology. In Proceedings of the 2nd Interna-
tional Conference on Formal Ontology in Informa-
tion Systems (FOIS-2001), pages 17?19. Chris Welty
and Barry Smith, eds.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In 16th international World Wide Web con-
ference (WWW 2007), New York, NY, USA. ACM
Press.
Vossen, P., editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers .
168
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 81?86,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 16: Evaluation of Wide Coverage Knowledge Resources
Montse Cuadros
TALP Research Center
Universitat Polite?cnica de Catalunya
Barcelona, Spain
cuadros@lsi.upc.edu
German Rigau
IXA NLP Group
Euskal Herriko Unibersitatea
Donostia, Spain
german.rigau@ehu.es
Abstract
This task tries to establish the relative qual-
ity of available semantic resources (derived
by manual or automatic means). The qual-
ity of each large-scale knowledge resource
is indirectly evaluated on a Word Sense Dis-
ambiguation task. In particular, we use
Senseval-3 and SemEval-2007 English Lex-
ical Sample tasks as evaluation bechmarks
to evaluate the relative quality of each re-
source. Furthermore, trying to be as neu-
tral as possible with respect the knowledge
bases studied, we apply systematically the
same disambiguation method to all the re-
sources. A completely different behaviour is
observed on both lexical data sets (Senseval-
3 and SemEval-2007).
1 Introduction
Using large-scale knowledge bases, such as Word-
Net (Fellbaum, 1998), has become a usual, often
necessary, practice for most current Natural Lan-
guage Processing (NLP) systems. Even now, build-
ing large and rich enough knowledge bases for
broad?coverage semantic processing takes a great
deal of expensive manual effort involving large re-
search groups during long periods of development.
In fact, dozens of person-years have been invested in
the development of wordnets for various languages
(Vossen, 1998). For example, in more than ten years
of manual construction (from version 1.5 to 2.1),
WordNet passed from 103,445 semantic relations to
245,509 semantic relations1. That is, around one
thousand new relations per month. But this data
does not seems to be rich enough to support ad-
vanced concept-based NLP applications directly. It
seems that applications will not scale up to work-
ing in open domains without more detailed and rich
general-purpose (and also domain-specific) seman-
tic knowledge built by automatic means.
Fortunately, during the last years, the research
community has devised a large set of innovative
methods and tools for large-scale automatic acqui-
sition of lexical knowledge from structured and un-
structured corpora. Among others we can men-
tion eXtended WordNet (Mihalcea and Moldovan,
2001), large collections of semantic preferences ac-
quired from SemCor (Agirre and Martinez, 2001;
Agirre and Martinez, 2002) or acquired from British
National Corpus (BNC) (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired from
the web (Agirre and de la Calle, 2004) or acquired
from the BNC (Cuadros et al, 2005). Obviously,
these semantic resources have been acquired using a
very different set of methods, tools and corpora, re-
sulting on a different set of new semantic relations
between synsets (or between synsets and words).
Many international research groups are working
on knowledge-based WSD using a wide range of ap-
proaches (Mihalcea, 2006). However, less attention
has been devoted on analysing the quality of each
semantic resource. In fact, each resource presents
different volume and accuracy figures (Cuadros et
al., 2006).
In this paper, we evaluate those resources on the
1Symmetric relations are counted only once.
81
SemEval-2007 English Lexical Sample task. For
comparison purposes, we also include the results of
the same resources on the Senseval-3 English Lex-
ical sample task. In both cases, we used only the
nominal part of both data sets and we also included
some basic baselines.
2 Evaluation Framework
In order to compare the knowledge resources, all the
resources are evaluated as Topic Signatures (TS).
That is, word vectors with weights associated to a
particular synset. Normally, these word vectors are
obtained by collecting from the resource under study
the word senses appearing as direct relatives. This
simple representation tries to be as neutral as possi-
ble with respect to the resources studied.
A common WSD method has been applied to
all knowledge resources on the test examples of
Senseval-3 and SemEval-2007 English lexical sam-
ple tasks. A simple word overlapping counting is
performed between the Topic Signature and the test
example. The synset having higher overlapping
word counts is selected. In fact, this is a very sim-
ple WSD method which only considers the topical
information around the word to be disambiguated.
Finally, we should remark that the results are not
skewed (for instance, for resolving ties) by the most
frequent sense in WN or any other statistically pre-
dicted knowledge.
As an example, table 1 shows a test example of
SemEval-2007 corresponding to the first sense of the
noun capital. In bold there are the words that appear
in its corresponding Topic Signature acquired from
the web.
Note that although there are several important
related words, the WSD process implements ex-
act word form matching (no preprocessing is per-
formed).
2.1 Basic Baselines
We have designed a number of basic baselines in
order to establish a complete evaluation framework
for comparing the performance of each semantic re-
source on the English WSD tasks.
RANDOM: For each target word, this method se-
lects a random sense. This baseline can be consid-
ered as a lower-bound.
Baselines P R F1
TRAIN 65.1 65.1 65.1
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
SEMCOR-MFS 49.0 49.1 49.0
RANDOM 19.1 19.1 19.1
Table 2: P, R and F1 results for English Lexical Sam-
ple Baselines of Senseval-3
SemCor MFS (SEMCOR-MFS): This method
selects the most frequent sense of the target word
in SemCor.
WordNet MFS (WN-MFS): This method selects
the first sense in WN1.6 of the target word.
TRAIN-MFS: This method selects the most fre-
quent sense in the training corpus of the target word.
Train Topic Signatures (TRAIN): This baseline
uses the training corpus to directly build a Topic Sig-
nature using TFIDF measure for each word sense.
Note that this baseline can be considered as an
upper-bound of our evaluation.
Table 2 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of Senseval-3. In this table, TRAIN
has been calculated with a vector size of at maxi-
mum 450 words. As expected, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both below the most frequent sense
of the training corpus (TRAIN-MFS). However, all
of them are far below the Topic Signatures acquired
using the training corpus (TRAIN).
Table 3 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of SemEval-2007. Again, TRAIN
has been calculated with a vector size of at max-
imum 450 words. As before, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both far below the most frequent
sense of the training corpus (TRAIN-MFS), and all
of them are below the Topic Signatures acquired us-
ing the training corpus (TRAIN).
Comparing both lexical sample sets, SemEval-
2007 data appears to be more skewed and simple for
WSD systems than the data set from Senseval-3: less
82
<instance id=?19:0@11@wsj/01/wsj 0128@wsj@en@on? docsrc=?wsj?> <context>
? A sweeping restructuring of the industry is possible . ? Standard & Poor ?s Corp. says First Boston , Shearson
and Drexel Burnham Lambert Inc. , in particular , are likely to have difficulty shoring up their credit standing in
months ahead . What worries credit-rating concerns the most is that Wall Street firms are taking long-term risks
with their own <head> capital </head> via leveraged buy-out and junk bond financings . That ?s a departure from
their traditional practice of transferring almost all financing risks to investors . Whereas conventional securities
financings are structured to be sold quickly , Wall Street ?s new penchant for leveraged buy-outs and junk bonds is
resulting in long-term lending commitments that stretch out for months or years .
</context> </instance>
Table 1: Example of test id for capital#n which its correct sense is 1
Baselines P R F1
TRAIN 87.6 87.6 87.6
TRAIN-MFS 81.2 79.6 80.4
WN-MFS 66.2 59.9 62.9
SEMCOR-MFS 42.4 38.4 40.3
RANDOM 27.4 27.4 27.4
Table 3: P, R and F1 results for English Lexical Sam-
ple Baselines of SemEval-2007
polysemous (as shown by the RANDOM baseline),
less similar than SemCor word sense frequency dis-
tributions (as shown by SemCor-MFS), more simi-
lar to the first sense of WN (as shown by WN-MFS),
much more skewed to the first sense of the training
corpus (as shown by TRAIN-MFS), and much more
easy to be learned (as shown by TRAIN).
3 Large scale knowledge Resources
The evaluation presented here covers a wide range
of large-scale semantic resources: WordNet (WN)
(Fellbaum, 1998), eXtended WordNet (Mihalcea
and Moldovan, 2001), large collections of seman-
tic preferences acquired from SemCor (Agirre and
Martinez, 2001; Agirre and Martinez, 2002) or ac-
quired from the BNC (McCarthy, 2001), large-scale
Topic Signatures for each synset acquired from the
web (Agirre and de la Calle, 2004) or SemCor (Lan-
des et al, 2006).
Although these resources have been derived us-
ing different WN versions, using the technology for
the automatic alignment of wordnets (Daude? et al,
2003), most of these resources have been integrated
into a common resource called Multilingual Cen-
tral Repository (MCR) (Atserias et al, 2004) main-
taining the compatibility among all the knowledge
resources which use a particular WN version as a
sense repository. Furthermore, these mappings al-
low to port the knowledge associated to a particular
WN version to the rest of WN versions.
The current version of the MCR contains 934,771
semantic relations between synsets, most of them
acquired by automatic means. This represents al-
most four times larger than the Princeton WordNet
(245,509 unique semantic relations in WordNet 2.1).
Hereinafter we will refer to each semantic re-
source as follows:
WN (Fellbaum, 1998): This resource uses the
direct relations encoded in WN1.6 or WN2.0 (for
instance, tree#n#1?hyponym?>teak#n#2). We also
tested WN2 (using relations at distances 1 and 2),
WN3 (using relations at distances 1 to 3) and WN4
(using relations at distances 1 to 4).
XWN (Mihalcea and Moldovan, 2001): This re-
source uses the direct relations encoded in eXtended
WN (for instance, teak#n#2?gloss?>wood#n#1).
WN+XWN: This resource uses the direct rela-
tions included in WN and XWN. We also tested
(WN+XWN)2 (using either WN or XWN relations
at distances 1 and 2, for instance, tree#n#1?related?
>wood#n#1).
spBNC (McCarthy, 2001): This resource contains
707,618 selectional preferences acquired for sub-
jects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This re-
source contains the selectional preferences acquired
for subjects and objects from SemCor (for instance,
read#v#1?tobj?>book#n#1).
MCR (Atserias et al, 2004): This resource
uses the direct relations included in MCR but ex-
cluding spBNC because of its poor performance.
Thus, MCR contains the direct relations from
WN (as tree#n#1?hyponym?>teak#n#2), XWN
(as teak#n#2?gloss?>wood#n#1), and spSemCor
(as read#v#1?tobj?>book#n#1) but not the indi-
83
Source #relations
Princeton WN1.6 138,091
Selectional Preferences from SemCor 203,546
New relations from Princeton WN2.0 42,212
Gold relations from eXtended WN 17,185
Silver relations from eXtended WN 239,249
Normal relations from eXtended WN 294,488
Total 934,771
Table 4: Semantic relations uploaded in the MCR
rect relations of (WN+XWN)2 (tree#n#1?related?
>wood#n#1). We also tested MCR2 (using rela-
tions at distances 1 and 2), which also integrates
(WN+XWN)2 relations.
Table 4 shows the number of semantic relations
between synset pairs in the MCR.
3.1 Topic Signatures
Topic Signatures (TS) are word vectors related to a
particular topic (Lin and Hovy, 2000). Topic Signa-
tures are built by retrieving context words of a target
topic from large corpora. In our case, we consider
word senses as topics.
For this study, we use two different large-scale
Topic Signatures. The first constitutes one of the
largest available semantic resource with around 100
million relations (between synsets and words) ac-
quired from the web (Agirre and de la Calle, 2004).
The second has been derived directly from SemCor.
TSWEB2: Inspired by the work of (Leacock et
al., 1998), these Topic Signatures were constructed
using monosemous relatives from WordNet (syn-
onyms, hypernyms, direct and indirect hyponyms,
and siblings), querying Google and retrieving up to
one thousand snippets per query (that is, a word
sense), extracting the words with distinctive fre-
quency using TFIDF. For these experiments, we
used at maximum the first 700 words of each TS.
TSSEM: These Topic Signatures have been con-
structed using the part of SemCor having all words
tagged by PoS, lemmatized and sense tagged ac-
cording to WN1.6 totalizing 192,639 words. For
each word-sense appearing in SemCor, we gather
all sentences for that word sense, building a TS us-
ing TFIDF for all word-senses co-occurring in those
sentences.
2http://ixa.si.ehu.es/Ixa/resources/
sensecorpus
political party#n#1 2.3219
party#n#1 2.3219
election#n#1 1.0926
nominee#n#1 0.4780
candidate#n#1 0.4780
campaigner#n#1 0.4780
regime#n#1 0.3414
identification#n#1 0.3414
government#n#1 0.3414
designation#n#3 0.3414
authorities#n#1 0.3414
Table 5: Topic Signatures for party#n#1 obtained
from Semcor (11 out of 719 total word senses)
.
In table 5, there is an example of the first word-
senses we calculate from party#n#1.
The total number of relations between WN
synsets acquired from SemCor is 932,008.
4 Evaluating each resource
Table 6 presents ordered by F1 measure, the perfor-
mance of each knowledge resource on Senseval-3
and the average size of the TS per word-sense. The
average size of the TS per word-sense is the number
of words associated to a synset on average. Obvi-
ously, the best resources would be those obtaining
better performances with a smaller number of asso-
ciated words per synset. The best results for preci-
sion, recall and F1 measures are shown in bold. We
also mark in italics those resources using non-direct
relations.
Surprisingly, the best results are obtained by
TSSEM (with F1 of 52.4). The lowest result is ob-
tained by the knowledge directly gathered from WN
mainly because of its poor coverage (R of 18.4 and
F1 of 26.1). Also interesting, is that the knowledge
integrated in the MCR although partly derived by
automatic means performs much better in terms of
precision, recall and F1 measures than using them
separately (F1 with 18.4 points higher than WN, 9.1
than XWN and 3.7 than spSemCor).
Despite its small size, the resources derived from
SemCor obtain better results than its counterparts
using much larger corpora (TSSEM vs. TSWEB and
spSemCor vs. spBNC).
Regarding the basic baselines, all knowledge re-
sources surpass RANDOM, but none achieves nei-
ther WN-MFS, TRAIN-MFS nor TRAIN. Only
84
KB P R F1 Av. Size
TSSEM 52.5 52.4 52.4 103
MCR2 45.1 45.1 45.1 26,429
MCR 45.3 43.7 44.5 129
spSemCor 43.1 38.7 40.8 56
(WN+XWN)2 38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
WN3 35.0 34.7 34.8 503
WN4 33.2 33.1 33.2 2,346
WN2 33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
Table 6: P, R and F1 fine-grained results for the
resources evaluated individually at Senseval-03 En-
glish Lexical Sample Task.
TSSEM obtains better results than SEMCOR-MFS
and is very close to the most frequent sense of WN
(WN-MFS) and the training (TRAIN-MFS).
Table 7 presents ordered by F1 measure, the per-
formance of each knowledge resource on SemEval-
2007 and its average size of the TS per word-sense3.
The best results for precision, recall and F1 mea-
sures are shown in bold. We also mark in italics
those resources using non-direct relations.
Interestingly, on SemEval-2007, all the knowl-
edge resources behave differently. Now, the best
results are obtained by (WN+XWN)2 (with F1 of
52.9), followed by TSWEB (with F1 of 51.0). The
lowest result is obtained by the knowledge encoded
in spBNC mainly because of its poor precision (P of
24.4 and F1 of 20.8).
Regarding the basic baselines, spBNC, WN (and
also WN2 and WN4) and spSemCor do not sur-
pass RANDOM, and none achieves neither WN-
MFS, TRAIN-MFS nor TRAIN. Now, WN+XWN,
XWN, TSWEB and (WN+XWN)2 obtain better re-
sults than SEMCOR-MFS but far below the most
frequent sense of WN (WN-MFS) and the training
(TRAIN-MFS).
5 Combination of Knowledge Resources
In order to evaluate deeply the contribution of each
knowledge resource, we also provide some results
of the combined outcomes of several resources. The
3The average size is different with respect Senseval-3 be-
cause the words selected for this task are different
KB P R F1 Av. Size
(WN+XWN)2 54.9 51.1 52.9 5,153
TSWEB 54.8 47.8 51.0 700
XWN 50.1 39.8 44.4 96
WN+XWN 45.4 36.8 40.7 101
MCR 40.2 35.5 37.7 149
TSSEM 35.1 32.7 33.9 428
MCR2 32.4 29.5 30.9 24,896
WN3 29.3 26.3 27.7 584
WN2 25.9 27.4 26.6 72
spSemCor 31.4 23.0 26.5 51.0
WN4 26.1 23.9 24.9 2,710
WN 36.8 16.1 22.4 13
spBNC 24.4 18.1 20.8 290
Table 7: P, R and F1 fine-grained results for the
resources evaluated individually at SemEval-2007,
English Lexical Sample Task .
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 55.5
Table 8: F1 fine-grained results for the 4 system-
combinations on Senseval-3
combinations are performed following a very basic
strategy (Brody et al, 2006).
Rank-Based Combination (Rank): Each se-
mantic resource provides a ranking of senses of the
word to be disambiguated. For each sense, its place-
ments according to each of the methods are summed
and the sense with the lowest total placement (clos-
est to first place) is selected.
Table 8 presents the F1 measure result with re-
spect this method when combining four different se-
mantic resources on the Senseval-3 test set.
Regarding the basic baselines, this combination
outperforms the most frequent sense of SemCor
(SEMCOR-MFS with F1 of 49.1), WN (WN-MFS
with F1 of 53.0) and, the training data (TRAIN-MFS
with F1 of 54.5).
Table 9 presents the F1 measure result with re-
spect the rank mthod when combining the same four
different semantic resources on the SemEval-2007
test set.
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 38.9
Table 9: F1 fine-grained results for the 4 system-
combinations on SemEval-2007
85
In this case, the combination of the four resources
obtains much lower result. Regarding the baselines,
this combination performs lower than the most fre-
quent senses from SEMCOR, WN or the training
data. This could be due to the poor individual per-
formance of the knowledge derived from SemCor
(spSemCor, TSSEM and MCR, which integrates
spSemCor). Possibly, in this case, the knowledge
comming from SemCor is counterproductive. Inter-
estingly, the knowledge derived from other sources
(XWN from WN glosses and TSWEB from the
web) seems to be more robust with respect corpus
changes.
6 Conclusions
Although this task had no participants, we provide
the performances of a large set of knowledge re-
sources on two different test sets: Senseval-3 and
SemEval-2007 English Lexical Sample task. We
also provide the results of a system combination of
four large-scale semantic resources. When evalu-
ated on Senseval-3, the combination of knowledge
sources surpass the most-frequent classifiers. How-
ever, a completely different behaviour is observed
on SemEval-2007 data test. In fact, both corpora
present very different characteristics. The results
show that some resources seems to be less depen-
dant than others to corpus changes.
Obviously, these results suggest that much more
research on acquiring, evaluating and using large-
scale semantic resources should be addressed.
7 Acknowledgements
We want to thank the valuable comments of the
anonymous reviewers. This work has been partially
supported by the projects KNOW (TIN2006-15049-
C03-01) and ADIMEN (EHU06/113).
References
E. Agirre and O. Lopez de la Calle. 2004. Publicly avail-
able topic signatures for all wordnet nominal senses.
In Proceedings of LREC, Lisbon, Portugal.
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL,
Toulouse, France.
E. Agirre and D. Martinez. 2002. Integrating selectional
preferences in wordnet. In Proceedings of GWC,
Mysore, India.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and Piek Vossen. 2004. The mean-
ing multilingual central repository. In Proceedings of
GWC, Brno, Czech Republic.
S. Brody, R. Navigli, and M. Lapata. 2006. Ensem-
ble methods for unsupervised wsd. In Proceedings of
COLING-ACL, pages 97?104.
M. Cuadros, L. Padro?, and G. Rigau. 2005. Comparing
methods for automatic acquisition of topic signatures.
In Proceedings of RANLP, Borovets, Bulgaria.
M. Cuadros, L. Padro?, and G. Rigau. 2006. An empirical
study for automatic acquisition of topic signatures. In
Proceedings of GWC, pages 51?59.
J. Daude?, L. Padro?, and G. Rigau. 2003. Validation and
Tuning of Wordnet Mapping Techniques. In Proceed-
ings of RANLP, Borovets, Bulgaria.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
S. Landes, C. Leacock, and R. Tengi. 2006. Build-
ing a semantic concordance of english. In WordNet:
An electronic lexical database and some applications.
MIT Press, Cambridge,MA., 1998, pages 97?104.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
166.
C. Lin and E. Hovy. 2000. The automated acquisition of
topic signatures for text summarization. In Proceed-
ings of COLING. Strasbourg, France.
D. McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Subcate-
gorization Frames and Selectional Preferences. Ph.D.
thesis, University of Sussex.
R. Mihalcea and D. Moldovan. 2001. extended wordnet:
Progress report. In Proceedings of NAACL Workshop
on WordNet and Other Lexical Resources, Pittsburgh,
PA.
R. Mihalcea. 2006. Knowledge based methods for word
sense disambiguation. In E. Agirre and P. Edmonds
(Eds.) Word Sense Disambiguation: Algorithms and
applications., volume 33 of Text, Speech and Lan-
guage Technology. Springer.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers .
86
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 833?837,
Dublin, Ireland, August 23-24, 2014.
V3: Unsupervised Generation of Domain Aspect Terms for
Aspect Based Sentiment Analysis
Aitor Garc??a-Pablos,
Montse Cuadros, Se
?
an Gaines
Vicomtech-IK4 research centre
Mikeletegi 57, San Sebastian, Spain
{agarciap,mcuadros}@vicomtech.org
German Rigau
IXA Group
Euskal Herriko Unibertsitatea,
San Sebastian, Spain
german.rigau@ehu.es
Abstract
This paper presents V3, an unsupervised
system for aspect-based Sentiment Analy-
sis when evaluated on the SemEval 2014
Task 4. V3 focuses on generating a list
of aspect terms for a new domain using a
collection of raw texts from the domain.
We also implement a very basic approach
to classify the aspect terms into categories
and assign polarities to them.
1 Introduction
The automatic analysis of opinions, within the
framework of opinion mining or sentiment anal-
ysis, has gained a huge importance during the last
decade due to the amount of review web sites,
blogs and social networks producing everyday a
massive amount of new content (Pang and Lee,
2008; Liu, 2012; Zhang and Liu, 2014). This con-
tent usually contains opinions about different enti-
ties, products or services. Trying to cope with this
large amounts of textual data is unfeasible with-
out the help of automatic Opinion Mining tools
which try to detect, identify, classify, aggregate
and summarize the opinions expressed about dif-
ferent topics (Hu and Liu, 2004) (Popescu and Et-
zioni, 2005) (Wu et al., 2009) (Zhang et al., 2010).
In this framework, aspect based opinion mining
systems aim to detect the sentiment at ?aspect?
level (i.e. the precise feature being opinionated in
a clause or sentence).
In this paper we describe our system presented
in the SemEval 2014 task 4
1
Aspect Based Senti-
ment Analysis (Pontiki et al., 2014), which focuses
on detecting opinionated aspect terms (e.g. wine
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
1
http://alt.qcri.org/semeval2014/
task4/
list and menu in restaurant domain, and hard disk
and battery life in laptop domain), their categories
and polarities in customer review sentences.
The task provides two training datasets, one of
restaurant reviews and other of laptop reviews.
The restaurant review dataset consists of over
3,000 English sentences from restaurant reviews
borrowed from (Ganu et al., 2009). The laptop
review dataset consists of over 3,000 English sen-
tences extracted from customer reviews. The task
is divided in four different subtasks: subtask 1 as-
pect term extraction, subtask 2 aspect term polar-
ity detection, subtask 3 aspect category detection,
subtask 4 aspect category polarity detection. Our
system mainly focused on subtask 1, but we have
also participated in the other subtasks.
The paper is organized as follows: section 2
presents our approach, section 3 details the im-
provement methods used for the aspects term se-
lection and section 4 focus on category and polar-
ity tagging. Finally section 5 presents the results
obtained and section 6 draws the conclusions and
future work.
2 Our approach
We have adapted the double-propagation tech-
nique described in (Qiu et al., 2009; Qiu et al.,
2011). This method consists of using a minimal
seed list of aspect terms and opinion words and
propagate them through an unlabelled domain-
related corpus using a set of propagation rules.
The goal is to obtain an extended aspect term and
opinion word lists. (Qiu et al., 2009) define opin-
ion words as words that convey some positive or
negative sentiment polarities. They only extract
nouns as aspect terms and adjectives as opinion
words, and we assume the same restriction.
The propagation rules have the form of depen-
dency relations and some part-of-speech restric-
tions. Some rules extract new aspect terms, and
others extract new opinion words. Table 1 shows
833
the rules used in our approach, similar to those de-
tailed in (Qiu et al., 2011) with some modifica-
tions. In this table, T stands for aspect term (i.e.
a word already in the aspect terms set) and O for
opinion word (i.e. a word already in the opinion
words set). W means any word. The dependency
types used are amod, dobj, subj and conj, which
stand for adjectival modifier, direct object, subject
and conjunction respectively. Additional restric-
tions on the Part-Of-Speech (POS) of the words
present in the rule are shown in the third column
of the table. The last column indicates to which
set (aspect terms or opinion words) the new word
is added.
To obtain the dependency trees and word lem-
mas and POS tags, we use the Stanford NLP tools
2
(De Marneffe et al., 2006). Our initial seed words
are just the adjectives good and bad, which are
added to the initial opinion words set. The ini-
tial aspect terms set starts empty. Each sentence
in the dataset is analysed to obtain its dependency
tree and the rules are checked sequentially. If rule
is triggered, then the word indicated by that rule
is added to the corresponding set (aspect terms
or opinion words, depending on the rule). These
new words can be then used to trigger the propa-
gation rules later. After the last sentence the pro-
cess starts from the beginning to check the rules
with the newly added words. The process stops
when no more words have been added during a
full dataset iteration.
3 Selecting aspect term candidates
The double-propagation process populates both
sets of domain aspect terms and domain opinion
words, but we focus our attention in the aspect
terms set. Due to the nature of the process it tends
to generate hundreds of different potential aspect
terms, many of them being incorrect. We apply
some additional processes to improve the list.
3.1 Ranking the aspect terms
One way to reduce the undesired terms is to rank
them, pushing the incorrect aspect terms to the
bottom of the list and using only a certain subset
of top ranked terms. In order to rank this list we
have modelled the double-propagation process as
a undirected graph population process. Each new
aspect term or opinion word discovered by apply-
2
http://nlp.stanford.edu/software/
lex-parser.shtml
ing a propagation rule is added as a vertex to the
graph. The rule used to extract the new word is
added as an edge to the graph, connecting the orig-
inal word and the newly discovered word.
We have applied the well-known PageRank al-
gorithm (Brin and Page, 1998) to score the vertices
of the graph. To calculate the PageRank scores
we have used the JUNG framework
3
(OMadad-
hain et al., 2005), a set of Java libraries to work
with graphs. The value of the alpha parameter that
represents the probability of a random jump to any
node of the graph has been left at 0.15 (in the lit-
erature it is recommended an alpha value between
0.1 and 0.2). The aspect terms are then ordered us-
ing their associated score, being the most relevant
aspect term, the one with the highest score. Then
the list can be trimmed to a certain amount of top
ranked terms, trying to balance the precision and
recall of the resulting list.
3.2 Filtering undesired words
The double-propagation method always intro-
duces many undesired words. Some of these un-
desired words appear very frequently and are com-
bined with a large number of words. So, they tend
to also appear in high positions in the ranking.
Many of these words are easy to identify, and they
are not likely to be useful aspect terms in any do-
main. Examples of these words are: nothing, ev-
erything, thing, anyone, someone, somebody, etc.
In this work we use a domain agnostic stop word
list to deal with this kind of words. The authors
of the original double-propagation approach use
some clause and frequency based heuristics that
we do not employ here.
3.3 Detecting multiword terms
Many aspect terms are not just single words, but
compounds and multiword terms (e.g. wine list,
hard disk drive, battery life, etc.). In the origi-
nal double-propagation paper, the authors consider
adjacent nouns to a given aspect term as multiword
terms and perform an a posteriori pruning based
on the frequency of the combination. We have
tried to add multiword terms without increasing
the amount of noise in the resulting list. One of the
approaches included in the system exploits Word-
Net
4
(Fellbaum, 1999), and the following simple
rules:
3
http://jung.sourceforge.net
4
http://wordnet.princeton.edu/
834
Rule Observations Constraints Action
R11 O? amod?W W is a noun W?T
R12 O?dobj?W1?subj?W2 W2 is a noun W2?T
R21 T? amod?W W is an adjective W?O
R22 T? subj?W1? dobj?W2 W2 is an adjective W2? O
R31 T? conj?W W is a noun W? T
R32 T? subj?W1? dobj?W2 W2 is a noun W? T
R41 O? conj?W W is an adjective W? O
R42 O? Dep1?W1? Dep2?W2 Dep1==Dep2, W2 is an adjective W2? O
Table 1: Propagation rules.
? If word N and word N+1 are nouns, and the
combination is an entry in WordNet (or in
Wikipedia, see below). E.g.: battery life
? If word N is an adjective and word N+1 is
a noun, and the combination is an entry in
WordNet. E.g.: hot dog, happy hour
? If word N is an adjective, word N+1 is a noun,
and word n is a relational adjective in Word-
Net (lexical file 01). E.g.: Thai food
In order to improve the coverage of the Word-
Net approach, we also check if a combination of
two consecutive nouns appears as a Wikipedia ar-
ticle title. Wikipedia articles refer to real word
concepts and entities, so if a combination of words
is a title of a Wikipedia article it is very likely
that this word combination is also meaningful (e.g.
DVD player, USB port, goat cheese, pepperoni
pizza). We limit the lookup in Wikipedia titles just
to combination of nouns to avoid the inclusion of
incorrect aspect terms.
4 Assigning categories and polarities
Despite we have focused our attention on acquir-
ing aspect terms from a domain, we have also par-
ticipated in the rest of subtasks: grouping aspect
terms into a fixed set of categories, and assigning
polarities to both aspect terms and categories.
To group the aspect terms into categories, we
have employed WordNet similarities. The idea
is to compare the detected aspect terms against a
term or group of terms representative of the tar-
get categories. In this case the categories (only
for restaurants) were food, service, price, ambi-
ence and anecdotes/miscellaneous.
Initially, the representative word for each cate-
gory (except for the anecdotes/miscellaneous) was
the name of the category itself. We use the similar-
ity measure described by (Wu and Palmer, 1994).
Detected aspect terms are compared to the set of
representative words on each category, and they
are assigned to the category with a higher similar-
ity result. For example using this approach, the
similarity between food and cheese is 0.8, while
similarity between service and cheese is 0.25, and
between price and cheese is 0.266. Thus, in this
case cheese is assigned to the category food.
If the similarity does not surpass a given min-
imum threshold (manually set to 0.7), the current
aspect term is not assigned to the category to avoid
assigning a wrong category just because the other
were even less similar. After classifying the as-
pect terms of a given sentence into categories, we
assign those categories to the sentence. If no cat-
egory has been assigned, then we use the anec-
dotes/miscellaneous category as the default one.
This approach is quite naive and it has many
limitations. It works quite well for the category
food, classifying ingredients and meals, but it fails
when the category or the aspect terms are more
vague or abstract. In addition, we do not perform
any kind of word sense disambiguation or sense
pruning, which probably would discard unrelated
senses.
For detecting the polarity we have used the
SentiWords (Guerini et al., 2013; Warriner et al.,
2013) as a polarity lexicon. Using direct depen-
dency relations between aspect terms and polarity
bearing words we assign the polarity value from
the lexicon to the aspect term. We make a simple
count of the polarities of the aspect terms classi-
fied under a certain category to assign the polarity
of that category in a particular sentence.
5 Evaluation
The run submitted to the SemEval task 4 compe-
tition was based on 25k unlabelled sentences ex-
tracted from domain related reviews (for restau-
rants and laptops) obtained by scraping different
websites. We used these unlabelled sentences to
execute our unsupervised system to generate and
835
Restaur. aspect terms Precision Recall F-score
SemEval Baseline 0.525 0.427 0.471
V3 (S) 0.656 0.562 0.605
V3 (W) 0.571 0.641 0.604
V3 (W+S) 0.575 0.645 0.608
Table 2: Results on the restaurant review test set.
Laptops aspect terms Precision Recall F-score
SemEval Baseline 0.443 0.298 0.356
V3 (S) 0.265 0.276 0.271
V3 (W) 0.321 0.425 0.366
V3 (W+S) 0.279 0.444 0.343
Table 3: Results on the laptop review test set.
rank the aspect term lists. Then we used those
aspect term lists to annotate the sentences using
a simple lemma matching approach between the
words. The generated aspect term lists were lim-
ited to the first ranked 550 items after some initial
experiments with the SemEval training sets.
The SemEval test datasets (restaurants and lap-
tops) contain about 800 sentences each. The
restaurant dataset contains 1,134 labelled gold as-
pect term spans, and the laptop dataset contains
634 labelled gold aspect term spans. We compare
the results against the SemEval baseline which is
calculated using the scripts provided by the Se-
mEval organizers. This baseline splits the dataset
into train and test subsets, and uses all the labelled
aspect terms in the train subset to build a dictio-
nary of aspect terms. Then it simply uses that dic-
tionary to label the test subset for evaluation.
Tables 2 and 3 show the performance of our sys-
tem with respect to the baselines in both datasets.
?V3 (S)? stands for our system only using the Se-
mEval test data (as our approach is unsupervised
it learns from the available texts for the task). (W)
refers to the results using our own dataset scraped
from the Web. Finally (W+S) refers to the results
using both SemEval and our Web dataset mixed
together. The best results are highlighted in bold.
For subtask 1, although our system outperforms
the baseline in terms of F-score in both datasets, in
the competition our system obtained quite modest
results ranking 24th and 26th out of 29 participants
Restaur. categories Precision Recall F-score
SemEval Baseline 0.671 0.602 0.638
V3 0.638 0.569 0.602
Table 4: Results on restaurant category detection
using the test set.
Polarity detection accuracy Baseline V3
Restaur. aspect terms 0.642 0.597
Restaur. categories 0.656 0.472
Laptop aspect terms 0.510 0.538
Table 5: Results for the polarity classification sub-
tasks (subtasks 2 and 4).
for restaurants and laptops respectively.
One of the most important source of errors are
the multiword aspect term detection. In the Se-
mEval datasets, about the 25% of the gold aspect
terms are multiword terms. In both datasets we
find a large number of names of recipes and meals,
composed by two, three or even more words,
which cannot appear in our aspect term lists be-
cause we limit the multiword length up to two
words.
As mentioned in the introduction our approach
focuses mainly in the aspects so the approach for
detecting categories and polarities needs more at-
tention. Table 4 presents our results on category
detection and table 5 our results on polarities. The
results are quite poor so we do not comment on
them here. We will address these subtasks in fu-
ture work.
6 Conclusions and future work
In this paper we propose a simple and unsuper-
vised system able to bootstrap and rank a list
of domain aspect terms from a set of unlabelled
domain texts. We use a double-propagation ap-
proach, and we model the obtained terms and their
relations as a graph. Then, we apply the PageRank
algorithm to score the obtained terms. Despite the
modest results, our unsupervised system for de-
tecting aspect terms performs better than the su-
pervised baseline. In our future work we will try
to improve the way we deal with multiword terms
to reduce the amount of incorrect aspect terms and
generate a better ranking. We also plan to try
different methods for the category grouping, and
explore knowledge-based word sense disambigua-
tion methods for improving the current system.
Acknowledgements
This work has been partially funded by SKaTer
(TIN2012-38584-C06-02) and OpeNER (FP7-
ICT-2011-SME- DCL-296451).
836
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1):107?
117.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Gayatree Ganu, N Elhadad, and A Marian. 2009. Be-
yond the Stars: Improving Rating Predictions using
Review Text Content. WebDB, (WebDB):1?6.
Marco Guerini, Lorenzo Gatti, and Marco Turchi.
2013. Sentiment analysis: How to derive prior
polarities from sentiwordnet. arXiv preprint
arXiv:1309.5843.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. AAAI.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Joshua OMadadhain, Danyel Fisher, Padhraic Smyth,
Scott White, and Yan-Biao Boey. 2005. Analysis
and visualization of network data using jung. Jour-
nal of Statistical Software, 10(2):1?35.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
AM Popescu and Oren Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. Natural lan-
guage processing and text mining, (October):339?
346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding Domain Sentiment Lexicon
through Double Propagation. IJCAI.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics, (July 2010).
Amy Beth Warriner, Victor Kuperman, and Marc Brys-
baert. 2013. Norms of valence, arousal, and dom-
inance for 13,915 english lemmas. Behavior re-
search methods, 45(4):1191?1207.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138. Association for Com-
putational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3-Volume 3, pages 1533?1541. Association
for Computational Linguistics.
Lei Zhang and Bing Liu. 2014. Aspect and Entity
Extraction for Opinion Mining. Data Mining and
Knowledge Discovery for Big Data.
L Zhang, Bing Liu, SH Lim, and E O?Brien-Strain.
2010. Extracting and ranking product features in
opinion documents. Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
(August):1462?1470.
837
KnowNet:
A Proposal for Building Highly
Connected and Dense
Knowledge Bases from the Web
Montse Cuadros
TALP Research Center, UPC, Barcelona (Spain)
email: cuadros@lsi.upc.edu
German Rigau
IXA NLP Group, UPV/EHU, Donostia (Spain)
email: german.rigau@ehu.es
Abstract
This paper presents a new fully automatic method for building highly
dense and accurate knowledge bases from existing semantic resources.
Basically, the method uses a wide-coverage and accurate knowledge-
based Word Sense Disambiguation algorithm to assign the most appro-
priate senses to large sets of topically related words acquired from the
web. KnowNet, the resulting knowledge-base which connects large sets
of semantically-related concepts is a major step towards the autonomous
acquisition of knowledge from raw corpora. In fact, KnowNet is several
times larger than any available knowledge resource encoding relations
between synsets, and the knowledge that KnowNet contains outperform
any other resource when empirically evaluated in a common multilingual
framework.
71
72 Cuadros and Rigau
1 Introduction
Using large-scale knowledge bases, such as WordNet (Fellbaum, 1998), has become a
usual, often necessary, practice for most current Natural Language Processing (NLP)
systems. Even now, building large and rich enough knowledge bases for broad?
coverage semantic processing takes a great deal of expensive manual effort involv-
ing large research groups during long periods of development. In fact, hundreds
of person-years have been invested in the development of wordnets for various lan-
guages (Vossen, 1998). For example, in more than ten years of manual construction
(from 1995 to 2006, that is from version 1.5 to 3.0), WordNet passed from 103,445 to
235,402 semantic relations1. But this data does not seems to be rich enough to support
advanced concept-based NLP applications directly. It seems that applications will not
scale up to working in open domains without more detailed and rich general-purpose
(and also domain-specific) semantic knowledge built by automatic means. Obviously,
this fact has severely hampered the state-of-the-art of advanced NLP applications.
However, the Princeton WordNet is by far the most widely-used knowledge base
(Fellbaum, 1998). In fact, WordNet is being used world-wide for anchoring differ-
ent types of semantic knowledge including wordnets for languages other than English
(Atserias et al, 2004), domain knowledge (Magnini and Cavagli?, 2000) or ontolo-
gies like SUMO (Niles and Pease, 2001) or the EuroWordNet Top Concept Ontology
(?lvez et al, 2008). It contains manually coded information about nouns, verbs, ad-
jectives and adverbs in English and is organised around the notion of a synset. A
synset is a set of words with the same part-of-speech that can be interchanged in a cer-
tain context. For example, <party, political_party> form a synset because they can
be used to refer to the same concept. A synset is often further described by a gloss, in
this case: "an organisation to gain political power" and by explicit semantic relations
to other synsets.
Fortunately, during the last years the research community has devised a large set of
innovative methods and tools for large-scale automatic acquisition of lexical knowl-
edge from structured and unstructured corpora. Among others we can mention eX-
tended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic pref-
erences acquired from SemCor (Agirre and Martinez, 2001, 2002) or acquired from
British National Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for
each synset acquired from the web (Agirre and de la Calle, 2004) or knowledge about
individuals from Wikipedia (Suchanek et al, 2007). Obviously, all these semantic re-
sources have been acquired using a very different set of processes (Snow et al, 2006),
tools and corpora. In fact, each semantic resource has different volume and accuracy
figures when evaluated in a common and controlled framework (Cuadros and Rigau,
2006).
However, not all available large-scale resources encode semantic relations between
synsets. In some cases, only relations between synsets and words have been acquired.
This is the case of the Topic Signatures (Agirre et al, 2000) acquired from the web
(Agirre and de la Calle, 2004). This is one of the largest semantic resources ever built
with around one hundred million relations between synsets and semantically related
1Symmetric relations are counted only once.
KnowNet: A Proposal for Building Knowledge Bases from the Web 73
words.2
A knowledge net or KnowNet, is an extensible, large and accurate knowledge
base, which has been derived by semantically disambiguating the Topic Signatures
acquired from the web. Basically, the method uses a robust and accurate knowledge-
based Word Sense Disambiguation algorithm to assign the most appropriate senses
to the topic words associated to a particular synset. The resulting knowledge-base
which connects large sets of topically-related concepts is a major step towards the au-
tonomous acquisition of knowledge from raw text. In fact, KnowNet is several times
larger than WordNet and the knowledge contained in KnowNet outperformsWordNet
when empirically evaluated in a common framework.
Table 1 compares the different volumes of semantic relations between synset pairs
of available knowledge bases and the newly created KnowNets3.
Table 1: Number of synset relations
Source #relations
Princeton WN3.0 235,402
Selectional Preferences from SemCor 203,546
eXtended WN 550,922
Co-occurring relations from SemCor 932,008
New KnowNet-5 231,163
New KnowNet-10 689,610
New KnowNet-15 1,378,286
New KnowNet-20 2,358,927
Varying from five to twenty the number of processed words from each Topic Signa-
ture, we created automatically four different KnowNets with millions of new semantic
relations between synsets.
After this introduction, Section 2 describes the Topic Signatures acquired from the
web. Section 3 presents the approach we plan to follow for building highly dense and
accurate knowledge bases. Section 4 describes the methods we followed for building
KnowNet. In Section 5, we present the evaluation framework used in this study. Sec-
tion 6 describes the results when evaluating different versions of KnowNet and finally,
Section 7 presents some concluding remarks and future work.
2 Topic Signatures
Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy,
2000). Topic Signatures are built by retrieving context words of a target topic from
large corpora. In our case, we consider word senses as topics. Basically, the acquisi-
tion of TS consists of:
? acquiring the best possible corpus examples for a particular word sense (usually
characterising each word sense as a query and performing a search on the corpus
2Available at http://ixa.si.ehu.es/Ixa/resources/sensecorpus
3These KnowNet versions can be downloaded from http://adimen.si.ehu.es
74 Cuadros and Rigau
Table 2: TS of party#n#1 (first 10 out of 12,890 total words)
tammany#n 0.0319
alinement#n 0.0316
federalist#n 0.0315
whig#n 0.0300
missionary#j 0.0229
Democratic#n 0.0218
nazi#j 0.0202
republican#n 0.0189
constitutional#n 0.0186
organization#n 0.0163
for those examples that best match the queries)
? building the TS by deriving the context words that best represent the word sense
from the selected corpora.
The Topic Signatures acquired from the web (hereinafter TSWEB) constitutes one
of the largest available semantic resources with around 100 million relations (between
synsets and words) (Agirre and de la Calle, 2004). Inspired by the work of Leacock
et al (1998), TSWEB was constructed using monosemous relatives from WN (syn-
onyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google and
retrieving up to one thousand snippets per query (that is, a word sense), extracting
the salient words with distinctive frequency using TFIDF. Thus, TSWEB consist of a
large ordered list of words with weights associated to each of the senses of the poly-
semous nouns of WordNet 1.6. The number of constructed topic signatures is 35,250
with an average size per signature of 6,877 words. When evaluating TSWEB, we used
at maximum the first 700 words while for building KnowNet we used at maximum the
first 20 words.
For example, Table 2 present the first words (lemmas and part-of-speech) and
weights of the Topic Signature acquired for party#n#1.
3 Building highly connected and dense knowledge bases
It is our belief, that accurate semantic processing (such as WSD) would rely not
only on sophisticated algorithms but on knowledge intensive approaches. In fact,
the cycling arquitecture of the MEANING4 project demonstrated that acquiring bet-
ter knowledge allow to perform better Word Sense Disambiguation (WSD) and that
having improvedWSD systems we are able to acquire better knowledge (Rigau et al,
2002).
Thus, we plan to acquire by fully automatic means highly connected and dense
knowledge bases from large corpora or the web by using the knowledge already avail-
able, increasing the total number of relations from less than one million (the current
number of available relations) to millions.
4http://www.lsi.upc.edu/~nlp/meaning
KnowNet: A Proposal for Building Knowledge Bases from the Web 75
The current proposal consist of:
? to follow Cuadros et al (2005) and Cuadros and Rigau (2006) for acquiring
highly accurate Topic Signatures for all monosemous words in WordNet (for
instance, using InfoMap (Dorow and Widdows, 2003)). That is, to acquire
word vectors closely related to a particular monosemousword (for instance, air-
port#n#1) from BNC or other large text collections like GigaWord, Wikipedia
or the web.
? to apply a very accurate knowledge?based all?words disambiguation algorithm
to the Topic Signatures in order to obtain sense vectors instead of word vectors
(for instance, using a version of Structural Semantic Interconnections algorithm
(SSI) (Navigli and Velardi, 2005)).
For instance, consider the first ten weighted words (with Part-of-Speech) appear-
ing in the Topic Signature (TS) of the word sense airport#n#1 corresponding to the
monosemous word airport, as shown in Table 3. This TS has been obtained from
BNC using InfoMap. From the ten words appearing in the TS, two of them do not
appear in WN (corresponding to the proper names heathrow#n and gatwick#n), four
words are monosemous (airport#n, airfield#n, travelling#n and passenger#n) and four
other are polysemous (flight#n, train#n, station#n and ferry#n).
Table 3: First ten words with weigths and number of senses in WN of the Topic
Signature for airport#n#1 obtained from BNC using InfoMap
word+pos weight #senses
airport#n 1.000000 1
heathrow#n 0.843162 0
gatwick#n 0.768215 0
flight#n 0.765804 9
airfield#n 0.740861 1
train#n 0.739805 6
travelling#n 0.732794 1
passenger#n 0.722912 1
station#n 0.722364 4
ferry#n 0.717653 2
SSI-Dijkstra
We have implemented a version of the Structural Semantic Interconnections algorithm
(SSI), a knowledge-based iterative approach to Word Sense Disambiguation (Navigli
and Velardi, 2005). The SSI algorithm is very simple and consists of an initialisation
step and a set of iterative steps. GivenW, an ordered list of words to be disambiguated,
the SSI algorithm performs as follows. During the initialisation step, all monosemous
words are included into the set I of already interpreted words, and the polysemous
words are included in P (all of them pending to be disambiguated). At each step, the
76 Cuadros and Rigau
Table 4: Minimum distances from airport#n#1
Synsets Distance
4 6
4530 5
64713 4
29767 3
597 2
20 1
1 0
set I is used to disambiguate one word of P, selecting the word sense which is closer
to the set I of already disambiguated words. Once a sense is disambiguated, the word
sense is removed from P and included into I. The algorithm finishes when no more
pending words remain in P.
Initially, the list I of interpretedwords should include the senses of the monosemous
words inW, or a fixed set of word senses5. However, in this case, when disambiguating
a TS derived from a monosemous word m, the list I includes since the beginning at
least the sense of the monosemous word m (in our example, airport#n#1).
In order to measure the proximity of one synset (of the word to be disambiguated at
each step) to a set of synsets (those word senses already interpreted in I), the original
SSI uses an in-house knowledge base derived semi-automatically which integrates a
variety of online resources (Navigli, 2005). This very rich knowledge-base is used to
calculate graph distances between synsets. In order to avoid the exponential explosion
of possibilities, not all paths are considered. They used a context-free grammar of
relations trained on SemCor to filter-out inappropriate paths and to provide weights to
the appropriate paths.
Instead, we use part of the knowledge already available to build a very large con-
nected graph with 99,635 nodes (synsets) and 636,077 edges (the set of direct relations
between synsets gathered from WordNet and eXtended WordNet). On that graph, we
used a very efficient graph library to compute the Dijkstra algorithm.6 The Dijkstra
algorithm is a greedy algorithm that computes the shortest path distance between one
node an the rest of nodes of a graph. In that way, we can compute very efficiently
the shortest distance between any two given nodes of a graph. This version of the SSI
algorithm is called SSI-Dijkstra.
For instance, Table 4 shows the volumes of the minimumdistances from airport#n#1
to the rest of the synsets of the graph. Interestingly, from airport#n#1 all synsets of
the graph are accessible following paths of at maximum six edges. While there is
only one synset at distance zero (airport#n#1) and twenty synsets directly connected
to airport#n#1, 95% of the total graph is accessible at distance four or less.
SSI-Dijkstra has very interesting properties. For instance, SSI-Dijkstra always pro-
5If no monosemous words are found or if no initial senses are provided, the algorithm could make an
initial guess based on the most probable sense of the less ambiguous word of W.
6See http://www.boost.org
KnowNet: A Proposal for Building Knowledge Bases from the Web 77
vides an answer when comparing the distances between the synsets of a word and all
the synsets already interpreted in I. That is, the Dijkstra algorithm always provides
an answer being the minimum distance close or far7. At each step, the SSI-Dijkstra
algorithm selects the synset which is closer to I (the set of already interpreted words).
Table 5 presents the result of the word?sense disambiguation process with the SSI-
Dijkstra algorithm on the TS presented in Table 38. Now, part of the TS obtained
from BNC using InfoMap have been disambiguated at a synset level resulting on a
word?sense disambiguated TS. Those words not present in WN1.6 have been ignored
(heathrow and gatwick). Some others, being monosemous in WordNet were consid-
ered already disambiguated (travelling, passenger, airport and airfield). But the rest,
have been correctly disambiguated (flight with nine senses, train with six senses, sta-
tion with four and ferry with two).
Table 5: Sense disambiguated TS for airport#n#1 obtained from BNC using InfoMap
and SSI-Dijkstra
.
Word Offset-WN Weight Gloss
flight#n 00195002n 0.017 a scheduled trip by plane between designated
airports
travelling#n 00191846n 0 the act of going from one place to another
train#n 03528724n 0.012 a line of railway cars coupled together and drawn
by a locomotive
passenger#n 07460409n 0 a person travelling in a vehicle (a boat or bus or
car or plane or train etc) who is not operating it
station#n 03404271n 0.019 a building equipped with special equipment and
personnel for a particular purpose
airport#n 02175180n 0 an airfield equipped with control tower and hangers
as well as accommodations for passengers and cargo
ferry#n 02671945n 0.010 a boat that transports people or vehicles across a
body of water and operates on a regular schedule
airfield#n 02171984n 0 a place where planes take off and land
This sense disambiguated TS represents seven direct new semantic relations be-
tween airport#n#1 and the first words of the TS. It could be directly integrated into a
new knowledge base (for instance, airport#n#1 ?related?> flight#n#9), but also all the
indirect relations of the disambiguated TS (for instance, flight#n#9 ?related?> trav-
elling#n#1). In that way, having n disambiguated word senses, a total of (n2 ? n)/2
relations could be created. That is, for the ten initial words of the TS of airport#n#1,
twenty-eight new direct relations between synsets could be created.
This process could be repeated for all monosemous words of WordNet appearing
in the selected corpus. The total number of monosemous words in WN1.6 is 98,953.
Obviously, not all these monosemous words are expected to appear in the corpus.
However, we expect to obtain in that way several millions of new semantic relations
between synsets. This method will allow to derive by fully automatic means a huge
knowledge base with millions of new semantic relations.
7In contrast, the original SSI algorithm not always provides a path distance because it depends on the
grammar.
8It took 4.6 seconds to disambiguate the TS on a modern personal computer.
78 Cuadros and Rigau
Furthermore, this approach is completely language independent. It could be re-
peated for any language having words connected to WordNet.
It remains for further study and research, how to convert the relations created in
that way to more specific and informed relations.
4 Building KnowNet
As a proof of concept, we developed KnowNet (KN), a large-scale and extensible
knowledge base obtained by applying the SSI-Dijkstra algorithm to each topic signa-
ture from TSWEB. That is, instead of using InfoMap and a large corpora for acquiring
new Topic Signatures for all the monosemous terms in WN, we used the already avail-
able TSWEB. We have generated four different versions of KonwNet applying SSI-
Dijkstra to the first 5, 10, 15 and 20 words for each TS. SSI-Dijkstra used only the
knowledge present in WordNet and eXtended WordNet which consist of a very large
connected graph with 99,635 nodes (synsets) and 636,077 edges (semantic relations).
We generated each KnowNet by applying the SSI-Dijkstra algorithm to the whole
TSWEB (processing the first words of each of the 35,250 topic signatures). For each
TS, we obtained the direct and indirect relations from the topic (a word sense) to the
disambiguated word senses of the TS. Then, as explained in Section 3, we also gen-
erated the indirect relations for each TS. Finally, we removed symmetric and repeated
relations.
Table 6 shows the percentage of the overlapping between each KnowNet with re-
spect the knowledge contained intoWordNet and eXtendedWordNet, the total number
of relations and synsets of each resource. For instance, only an 8,6% of the total rela-
tions included into WN+XWN are also present in KN-20. This means that the rest of
relations from KN-20 are new. This table also shows the different KnowNet volumes.
As expected, each KnowNet is very large, ranging from hundreds of thousands to
millions of new semantic relations between synsets among increasing sets of synsets.
Surprisingly, the overlapping between the semantic relations of KnowNet and the
knowledge bases used for building the SSI-Dijkstra graph (WordNet and eXtended
WordNet) is very small, possibly indicating disjunct types of knowledge.
Table 6: Size and percentage of overlapping relations between KnowNet versions and
WN+XWN
KB WN+XWN #relations #synsets
KN-5 3.2% 231,164 39,837
KN-10 5.4% 689,610 45,770
KN-15 7.0% 1,378,286 48,461
KN-20 8.6% 2,358,927 50,705
Table 7 presents the percentage of overlapping relations between KnowNet ver-
sions. The upper triangular part of the matrix presents the overlapping percentage
covered by larger KnowNet versions.That is, most of the knowledge from KN-5 is
also contained in larger versions of KnowNet. Interestingly, the knowledge contained
into KN-10 is only partially covered by KN-15 and KN-20. The lower triangular
KnowNet: A Proposal for Building Knowledge Bases from the Web 79
part of the matrix presents the overlapping percentage covered by smaller KnowNet
versions.
Table 7: Percentage of overlapping relations between KnowNet versions
overlapping KN-5 KN-10 KN-15 KN-20
KN-5 100 93,3 97,7 97,2
KN-10 31,2 100 88,5 88,9
KN-15 16,4 44,4 100 97.14
KN-20 9,5 26,0 56,7 100
5 Evaluation framework
In order to empirically establish the relative quality of these KnowNet versions with
respect already available semantic resources, we used the noun-set of Senseval-3 En-
glish Lexical Sample task which consists of 20 nouns.
Trying to be as neutral as possible with respect to the resources studied, we applied
systematically the same disambiguation method to all of them. Recall that our main
goal is to establish a fair comparison of the knowledge resources rather than providing
the best disambiguation technique for a particular resource. Thus, all the semantic re-
sources studied are evaluated as Topic Signatures. That is, word vectors with weights
associated to a particular synset (topic) which are obtained by collecting those word
senses appearing in the synsets directly related to the topics.
A common WSD method has been applied to all knowledge resources. A simple
word overlapping counting is performed between the Topic Signature and the test
example9. The synset having higher overlapping word counts is selected. In fact, this
is a very simple WSD method which only considers the topical information around
the word to be disambiguated. All performances are evaluated on the test data using
the fine-grained scoring system provided by the organisers. Finally, we should remark
that the results are not skewed (for instance, for resolving ties) by the most frequent
sense in WN or any other statistically predicted knowledge.
5.1 Baselines
We have designed a number of baselines in order to establish a complete evaluation
framework for comparing the performance of each semantic resource on the English
WSD task.
RANDOM: For each target word, this method selects a random sense. This base-
line can be considered as a lower-bound.
SEMCOR-MFS: This baseline selects the most frequent sense of the target word
in SemCor.
WN-MFS: This baseline is obtained by selecting the most frequent sense (the first
sense in WN1.6) of the target word. WordNet word-senses were ranked using SemCor
and other sense-annotated corpora. Thus, WN-MFS and SemCor-MFS are similar, but
not equal.
9We also consider the multiword terms.
80 Cuadros and Rigau
TRAIN-MFS: This baseline selects the most frequent sense in the training corpus
of the target word.
TRAIN: This baseline uses the training corpus to directly build a Topic Signature
using TFIDF measure for each word sense. Note that in WSD evaluation frameworks,
this is a very basic baseline. However, in our evaluation framework, this "WSD base-
line" could be considered as an upper-bound. We do not expect to obtain better topic
signatures for a particular sense than from its own annotated corpus.
5.2 Large-scale Knowledge Resources
In order to measure the relative quality of the new resources, we include in the evalu-
ation a wide range of large-scale knowledge resources connected to WordNet.
WN (Fellbaum, 1998): This resource uses the different direct relations encoded in
WN1.6 and WN2.0. We also tested WN2 using relations at distance 1 and 2, WN3
using relations at distances 1 to 3 and WN4 using relations at distances 1 to 4.
XWN (Mihalcea and Moldovan, 2001): This resource uses the direct relations en-
coded in eXtended WN.
WN+XWN: This resource uses the direct relations included in WN and XWN. We
also tested (WN+XWN)2 (using either WN or XWN relations at distances 1 and 2).
spBNC (McCarthy, 2001): This resource contains 707,618 selectional preferences
acquired for subjects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This resource contains the selectional
preferences acquired for subjects and objects from SemCor.
MCR (Atserias et al, 2004): This resource uses the direct relations of WN, XWN
and spSemCor (we excluded spBNC because of its poor performance).
TSSEM (Cuadros et al, 2007): These Topic Signatures have been constructed
using the part of SemCor having all words tagged by PoS, lemmatized and sense
tagged according to WN1.6 totalizing 192,639 words. For each word-sense appearing
in SemCor, we gather all sentences for that word sense, building a TS using TFIDF
for all word-senses co-occurring in those sentences.
6 KnowNet Evaluation
We evaluated KnowNet using the framework of Section 5, that is, the noun part of the
test set from the Senseval-3 English lexical sample task.
Table 8 presents ordered by F1 measure, the performance in terms of precision
(P), recall (R) and F1 measure (F1, harmonic mean of recall and precision) of each
knowledge resource on Senseval-3 and its average size of the TS per word-sense. The
different KnowNet versions appear marked in bold and the baselines appear in italics.
In this table, TRAIN has been calculated with a vector size of at maximum 450 words.
As expected, RANDOM baseline obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN (WN-MFS) are both below the
most frequent sense of the training corpus (TRAIN-MFS). However, all of them are
far below to the Topic Signatures acquired using the training corpus (TRAIN).
The best resources would be those obtaining better performances with a smaller
number of related words per synset. The best results are obtained by TSSEM (with
F1 of 52.4). The lowest result is obtained by the knowledge directly gathered from
WN mainly because of its poor coverage (R of 18.4 and F1 of 26.1). Interestingly,
KnowNet: A Proposal for Building Knowledge Bases from the Web 81
the knowledge integrated in the MCR although partly derived by automatic means
performs much better in terms of precision, recall and F1 measures than using them
separately (F1 with 18.4 points higher than WN, 9.1 than XWN and 3.7 than spSem-
Cor).
Despite its small size, the resources derived from SemCor obtain better results than
its counterparts using much larger corpora (TSSEM vs. TSWEB and spSemCor vs.
spBNC).
Regarding the baselines, all knowledge resources surpass RANDOM, but none
achieves neither WN-MFS, TRAIN-MFS nor TRAIN. Only TSSEM obtains better
results than SEMCOR-MFS and is very close to the most frequent sense of WN (WN-
MFS) and the training (TRAIN-MFS).
The different versions of KnowNet consistently obtain better performances as they
increase the window size of processed words of TSWEB. As expected, KnowNet-
5 obtain the lower results. However, it performs better than WN (and all its ex-
tensions) and spBNC. Interestingly, from KnowNet-10, all KnowNet versions sur-
pass the knowledge resources used for their construction (WN, XWN, TSWEB and
WN+XWN).
Furthermore, the integration of WN+XWN+KN?20 performs better than MCR
and similarly to MCR2 (having less than 50 times its size). It is also interesting to note
that WN+XWN+KN?20 has a better performance than their individual resources,
indicating a complementary knowledge. In fact, WN+XWN+KN?20 performs much
better than the resources from which it derives (WN, XWN and TSWEB).
These initial results seem to be very promising. If we do not consider the re-
sources derived frommanually sense annotated data (spSemCor, MCR, TSSEM, etc.),
KnowNet-10 performs better that any knowledge resource derived by manual or au-
tomatic means. In fact, KnowNet-15 and KnowNet-20 outperforms spSemCor which
was derived from manually annotated corpora. This is a very interesting result since
these KnowNet versions have been derived only with the knowledge coming from
WN and the web (that is, TSWEB), and WN and XWN as a knowledge source for
SSI-Dijkstra (eXtended WordNet only has 17,185 manually labelled senses).
7 Conclusions and future research
The initial results obtained for the different versions of KnowNet seem to be very
promising, since they seem to be of a better quality than other available knowledge
resources encoding relations between synsets derived from non-annotated sense cor-
pora.
We tested all these resources and the different versions of KnowNet on SemEval-
2007 English Lexical Sample Task (Cuadros and Rigau, 2008a). When comparing
the ranking of the different knowledge resources, the different versions of KnowNet
seem to be more robust and stable across corpora changes than the rest of resources.
Furthermore, we also tested the performance of KnowNet when ported to Spanish (as
the Spanish WordNet is also integrated into the MCR). Starting from KnowNet-10,
all KnowNet versions perform better than any other knowledge resource on Spanish
derived by manual or automatic means (including the MCR) (Cuadros and Rigau,
2008b).
82 Cuadros and Rigau
Table 8: P, R and F1 fine-grained results for the resources evaluated at Senseval-3,
English Lexical Sample Task
KB P R F1 Av. Size
TRAIN 65.1 65.1 65.1 450
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
TSSEM 52.5 52.4 52.4 103
SEMCOR-MFS 49.0 49.1 49.0
MCR2 45.1 45.1 45.1 26,429
WN+XWN+KN-20 44.8 44.8 44.8 671
MCR 45.3 43.7 44.5 129
KnowNet-20 44.1 44.1 44.1 610
KnowNet-15 43.9 43.9 43.9 339
spSemCor 43.1 38.7 40.8 56
KnowNet-10 40.1 40.0 40.0 154
(WN+XWN)2 38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
KnowNet-5 35.0 35.0 35.0 44
WN3 35.0 34.7 34.8 503
WN4 33.2 33.1 33.2 2,346
WN2 33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
RANDOM 19.1 19.1 19.1
In sum, this is a preliminary step towards improved KnowNets we plan to obtain
exploiting the Topic Signatures derived from monosemous words as explained in Sec-
tion 3.
Acknowledgments
We want to thank Aitor Soroa for his technical support and the anonymous reviewers
for their comments. This work has been supported by KNOW (TIN2006-15049-C03-
01) and KYOTO (ICT-2007-211423).
References
Agirre, E., O. Ansa, D. Martinez, and E. Hovy (2000). Enriching very large ontologies
with topic signatures. In Proceedings of ECAI?00 workshop on Ontology Learning,
Berlin, Germany.
Agirre, E. and O. L. de la Calle (2004). Publicly available topic signatures for all
wordnet nominal senses. In Proceedings of LREC, Lisbon, Portugal.
KnowNet: A Proposal for Building Knowledge Bases from the Web 83
Agirre, E. and D. Martinez (2001). Learning class-to-class selectional preferences. In
Proceedings of CoNLL, Toulouse, France.
Agirre, E. and D. Martinez (2002). Integrating selectional preferences in wordnet. In
Proceedings of GWC, Mysore, India.
?lvez, J., J. Atserias, J. Carrera, S. Climent, A. Oliver, and G. Rigau (2008). Consis-
tent annotation of eurowordnet with the top concept ontology. In Proceedings of
Fourth International WordNet Conference (GWC?08).
Atserias, J., L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and P. Vossen
(2004). The meaning multilingual central repository. In Proceedings of GWC,
Brno, Czech Republic.
Cuadros, M., L. Padr?, and G. Rigau (2005). Comparing methods for automatic ac-
quisition of topic signatures. In Proceedings of RANLP, Borovets, Bulgaria.
Cuadros, M. and G. Rigau (2006). Quality assessment of large scale knowledge re-
sources. In Proceedings of the EMNLP.
Cuadros, M. and G. Rigau (2008a). KnowNet: Building a Ln?arge Net of Knowledge
from the Web. In Proceedings of COLING.
Cuadros, M. and G. Rigau (2008b). Multilingual Evaluation of KnowNet. In Pro-
ceedings of SEPLN.
Cuadros, M., G. Rigau, and M. Castillo (2007). Evaluating large-scale knowledge
resources across languages. In Proceedings of RANLP.
Dorow, B. and D. Widdows (2003). Discovering corpus-specific word senses. In
EACL, Budapest.
Fellbaum, C. (1998). WordNet. An Electronic Lexical Database. The MIT Press.
Leacock, C., M. Chodorow, and G. Miller (1998). Using Corpus Statistics and Word-
Net Relations for Sense Identification. Computational Linguistics 24(1), 147?166.
Lin, C. and E. Hovy (2000). The automated acquisition of topic signatures for text
summarization. In Proceedings of COLING. Strasbourg, France.
Magnini, B. and G. Cavagli? (2000). Integrating subject field codes into wordnet. In
Proceedings of LREC, Athens. Greece.
McCarthy, D. (2001). Lexical Acquisition at the Syntax-Semantics Interface: Diathe-
sis Aternations, Subcategorization Frames and Selectional Preferences. Ph. D.
thesis, University of Sussex.
Mihalcea, R. and D. Moldovan (2001). extended wordnet: Progress report. In Pro-
ceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pitts-
burgh, PA.
84 Cuadros and Rigau
Navigli, R. (2005). Semi-automatic extension of large-scale linguistic knowledge
bases. In Proc. of 18th FLAIRS International Conference (FLAIRS), Clearwater
Beach, Florida.
Navigli, R. and P. Velardi (2005). Structural semantic interconnections: a knowledge-
based approach to word sense disambiguation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence (PAMI) 27(7), 1063?1074.
Niles, I. and A. Pease (2001). Towards a standard upper ontology. In C. Welty and
B. Smith (Eds.), Proc. of the 2nd International Conference on Formal Ontology in
Information Systems (FOIS-2001), pp. 17?19.
Rigau, G., B. Magnini, E. Agirre, P. Vossen, and J. Carroll (2002). Meaning: A
roadmap to knowledge technologies. In Proceedings of COLING?2002 Workshop
on A Roadmap for Computational Linguistics, Taipei, Taiwan.
Snow, R., D. Jurafsky, and A. Y. Ng (2006). Semantic taxonomy induction from
heterogenous evidence. In Proceedings of COLING-ACL.
Suchanek, F. M., G. Kasneci, and G. Weikum (2007). Yago: A Core of Semantic
Knowledge. In 16th international World Wide Web conference (WWW 2007), New
York, NY, USA. ACM Press.
Vossen, P. (1998). EuroWordNet: A Multilingual Database with Lexical Semantic
Networks. Kluwer Academic Publishers.
