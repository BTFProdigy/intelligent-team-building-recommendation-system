Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 65?72
Manchester, August 2008
Good Neighbors Make Good Senses:
Exploiting Distributional Similarity for Unsupervised WSD
Samuel Brody
School of Informatics
University of Edinburgh
s.brody@sms.ed.ac.uk
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
We present an automatic method for sense-
labeling of text in an unsupervised manner.
The method makes use of distributionally
similar words to derive an automatically
labeled training set, which is then used to
train a standard supervised classifier for
distinguishing word senses. Experimental
results on the Senseval-2 and Senseval-3
datasets show that our approach yields sig-
nificant improvements over state-of-the-art
unsupervised methods, and is competitive
with supervised ones, while eliminating
the annotation cost.
1 Introduction
Word sense disambiguation (WSD), the task of
identifying the intended meaning (sense) of words
in context, is a long-standing problem in Natural
Language Processing. Sense disambiguation is of-
ten characterized as an intermediate task, which is
not an end in itself, but has the potential to improve
many applications. Examples include summariza-
tion (Barzilay and Elhadad, 1997), question an-
swering (Ramakrishnan et al, 2003) and machine
translation (Chan and Ng, 2007).
WSD is commonly treated as a supervised clas-
sification task. Assuming we have access to data
that has been hand-labeled with correct word
senses, we can train a classifier to assign senses
to unseen words in context. While this approach
often achieves high accuracy, adequately large
sense labeled data sets are unfortunately difficult
to obtain. For many words, domains, languages,
and sense inventories they are unavailable, and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
in most cases it is unreasonable to expect to ac-
quire them. Ng (1997) estimates that a high accu-
racy domain-independent system for WSD would
probably need a corpus of about 3.2 million sense
tagged words. At a throughput of one word per
minute (Edmonds, 2000), this would require about
27 person-years of human annotation effort.
SemCor (Fellbaum, 1998) is one of the few cor-
pora that have been manually annotated for all
words ? it contains sense labels for 23,346 lem-
mas. In spite of being widely used, SemCor con-
tains too few tagged instances for the majority of
polysemous words (typically fewer than 10 each).
Supervised methods require much larger data sets
than this to perform adequately.
The problem of obtaining sufficient labeled
data, often referred to as the data acquisition bot-
tleneck, creates a significant barrier to the use of
supervised WSD methods in real world applica-
tions. In this work we wish to take advantage of the
high accuracy and strong capabilities of supervised
methods, while eliminating the need for human an-
notation of training data. Our approach exploits a
sense inventory such asWordNet (Fellbaum, 1998)
and corpus data to automatically create a collec-
tion of sense labeled instances which can subse-
quently serve to train any supervised classifier. The
key premise of our work is that a word?s senses
can be broadly described by semantically related
words. So, rather than laboriously annotating all
instances of a polysemous word with its senses, we
collect instances of its related words and treat them
as sense labels for the target word. The method
is inexpensive, language-independent, and can be
used to create large sense-labeled data without
human intervention. Our results demonstrate sig-
nificant improvements over state-of-the-art unsu-
pervised methods that do not make use of hand-
labeled annotations.
In the following section we provide an overview
65
of existing work on unsupervised WSD. Section 3
introduces our method for automatically creat-
ing sense annotations. We present our evaluation
framework in Section 4 and results in Section 5.
2 Related Work
The data requirements for supervisedWSD and the
current paucity of suitably annotated corpora for
many languages and text genres, has sparked con-
siderable interest in unsupervised methods. These
typically come in two flavors: (1) developing al-
gorithms that assign word senses without relying
on a sense-labeled corpus (Lesk, 1986; Galley and
McKeown, 2003) and (2) making use of pseudo-
labels, i.e., labelled data that has not been specifi-
cally annotated for sense disambiguation purposes
but contains some form of sense distinctions (Gale
et al, 1992; Leacock et al, 1998). We briefly dis-
cuss representative examples of both approaches,
with a bias to those closely related to our own
work.
Unsupervised Algorithms One of the first ap-
proaches to unsupervised WSD, and the founda-
tion of many algorithms to come, was originally
introduced by Lesk (1986). The method assigns a
sense to a target ambiguous word by comparing the
dictionary definitions of each of its senses with the
words in the surrounding context. The sense whose
definition has the highest overlap (i.e., words in
common) with the context is assumed to be the
correct one. Despite its simplicity, the algorithm
provides a good baseline for comparison. Cover-
age can be increased by augmenting the dictionary
definition (gloss) of each sense with the glosses of
related words and senses (Banerjee and Pedersen,
2003).
Although most algorithms disambiguate word
senses in context, McCarthy et al (2004) propose
a method that does not rely on contextual cues.
Their algorithm capitalizes on the fact that the dis-
tribution of word senses is highly skewed. A large
number of frequent words is often associated with
one dominant sense. Indeed, current supervised
methods rarely outperform the simple heuristic of
choosing the most common sense in the training
data (henceforth ?the first sense heuristic?), despite
taking local context into account. Rather than ob-
taining the first sense via annotating word senses
manually, McCarthy et al propose to acquire first
senses automatically and use them for disambigua-
tion. Thus, by design, their algorithm assigns the
same sense to all instances of a polysemous word.
Their approach is based on the observation that
distributionally similar neighbors often provide
cues about a word?s senses. Assuming that a set
of neighbors is available, the algorithm quantifies
the degree of similarity between the neighbors and
the sense descriptions of the polysemous word.
The sense with the highest overall similarity is the
first sense. Specifically, the approach makes use of
two similarity measures which complement each
other and provide a large amount of data regarding
the word senses. Distributional similarity indicates
the similarity between words in the distributional
feature space, whereas WordNet similarity in the
?semantic? space, is used to discover which sense
of the ambiguous word is used in the corpus, and
causing the distributional similarity.
Pseudo-labels as Training Instances Gale et al
(1992) pioneered the use of parallel corpora as a
source of sense-tagged data. Their key insight is
that different translations of an ambiguous word
can serve to distinguish its senses. Ng et al (2003)
extend this approach further and demonstrate that
it is feasible for large scale WSD. They gather
examples from English-Chinese parallel corpora
and use automatic word alignment as a means
of obtaining a translation dictionary. Translations
are next assigned to senses of English ambiguous
words. English instances corresponding to these
translations serve as training data.
It has become common to use related words
from a dictionary to learn contextual cues for WSD
(Mihalcea, 2002). Perhaps the first incarnation of
this idea is found in Leacock et al (1998), who
describe a system for acquiring topical contexts
that can be used to distinguish between senses.
For each sense, related monosemous words are ex-
tracted from WordNet using the various relation-
ship connections between sense entries (e.g., hy-
ponymy, hypernymy). Their system then queries
the Web with these related words. The contexts
surrounding the relatives of a specific sense are
presumed to be indicators of that sense, and used
for disambiguation. A similar idea, proposed by
Yarowsky (1992), is to use a thesaurus and acquire
informative contexts from words in the same cate-
gory as the target.
Our own work uses insights gained from unsu-
pervised methods with the aim of creating large
datasets of sense-labeled instances without explicit
manual coding. Unlike Ng et al (2003) our algo-
rithm works on monolingual corpora, which are
66
much more abundant than parallel ones, and is
fully automatic. In their approach translations and
their English senses must be associated manually.
Similarly to McCarthy et al (2004), we assume
that words related to the target word are useful in-
dicators of its senses. Importantly, our method dis-
ambiguates words in context and is able to assign
additional senses, besides the first one.
3 Method
As discussed earlier, our aim is to alleviate the
need for manual annotation by creating a large
dataset labeled with word senses without human
intervention. This dataset can be subsequently
used by any supervised machine learning algo-
rithm. We assume here that we have access to a
corpus and a sense inventory. We first obtain a list
of words that are semantically related to our tar-
get word. In the remainder of this paper we use the
term ?neighbors? to refer to these words. Next, we
separate the neighbors into sense-specific groups.
Finally, we replace the occurrences of each neigh-
bor in our corpus with an instance of the target
word, labeled with the matching sense for that
neighbor. The procedure has two important steps:
(1) acquiring neighbors and (2) associating them
with appropriate senses. We describe our imple-
mentation of each stage in more detail below.
Neighbor Acquisition Considerable latitude is
allowed in specifying appropriate neighbors for the
target word. Broadly speaking, the neighbors can
be extracted from a corpus or from a semantic re-
source, for example the dictionary providing the
sense inventory. A wealth of algorithms have been
proposed in the literature for acquiring distribu-
tional neighbors from a corpus (see Weeds (2003)
for an overview). They differ as to which features
they consider and how they use the distributional
statistics to calculate similarity.
Lin?s (1998) information-theoretic similarity
measure is commonly used in lexicon acquisition
tasks and has demonstrated good performance in
unsupervised WSD (McCarthy et al, 2004). It op-
erates over dependency relations. A word w is de-
scribed by a set T (w) of co-occurrence triplets
< w,r,w
?
>, which can be viewed as a sparsely
represented feature vector, where r represents
the type of relation (e.g., object-of , subject-of ,
modified-by) between w and its dependent w
?
. The
similarity between w
1
and w
2
is then defined as:
?
(r,w)?T (w
1
)?T (w
2
)
I(w
1
,r,w)+ I(w
2
,r,w)
?
(r,w)?T (w
1
)
I(w
1
,r,w)+
?
(r,w)?T (w
2
)
I(w
2
,r,w)
where I(w,r,w
?
) is the information value of w with
regard to (r,w
?
), defined as:
I(w,r,w
?
) = log
count(w,r,w
?
) ? count(r)
count(?,r,w
?
) ? count(w,r,?)
The measure is used to estimate the pairwise simi-
larity between the target word and all other words
in the corpus (with the same part of speech); the
k words most similar to the target are selected as
its neighbors.
A potential caveat with Lin?s (1998) distribu-
tional similarity measure is its reliance on syn-
tactic information for obtaining dependency rela-
tions. Parsing resources may not be available for
all languages or domains. An alternative is to use a
measure of distributional similarity which consid-
ers word collocation statistics and therefore does
not require a syntactic parser (see Weeds (2003)).
As mentioned earlier, it is also possible to ob-
tain neighbors simply by consulting a semantic
dictionary. In WordNet, for example, we can as-
sume that WordNet relations, (e.g., hypernymy,
hyponymy, synonymy) indicate words which are
semantic neighbors. An advantage of using dis-
tributional neighbors is that they reflect the char-
acteristics of the corpus we wish to disambiguate
and are potentially better suited for capturing sense
differences across genres and domains, whereas
dictionary-based neighbors are corpus-invariant.
Associating Neighbors with Senses If the
neighbors are extracted from WordNet, it is not
necessary to associate them with their senses as
they are already assigned a specific sense. Distri-
butional similarity methods, however, do not pro-
vide a way to distinguish which neighbors per-
tain to each sense of the target. For that purpose,
we adapt a method proposed by McCarthy et al
(2004). Specifically, for each acquired neighbor,
we choose the sense of the target which gives
the highest semantic similarity score to any sense
of the neighbor. There are a large number of se-
mantic similarity measures to choose from (see
Budanitsky and Hirst (2001) for an overview).
We use Lesk?s measure as modified by Banerjee
and Pedersen (2003) for two reasons. First, it has
67
been shown to perform well in the related task
of predominant sense detection (McCarthy et al,
2004). Second, it has the advantage of relying only
upon the sense definitions, rather than the complex
graph structure which is unique to WordNet. This
makes the method more suitable for use with other
sense inventories.
Note that unlike McCarthy et al (2004), we
are associating neighbors with senses, rather than
merely trying to detect the predominant sense, and
therefore we require more precision in our selec-
tion. When it is unclear which sense of the target
word is most similar to a given neighbor (when the
scores of two or more senses are close together),
that neighbor is discarded.
As an example, consider the word sense, which
has four meanings
1
in WordNet: (1) a general con-
scious awareness (e.g., a sense of security), (2) the
meaning of a word (e.g., the dictionary gave sev-
eral senses for the word), (3) sound practical judg-
ment (e.g., I can?t see the sense in doing it now),
and (4) a natural appreciation or ability (e.g., keen
musical sense). On the British National Corpus
(BNC), using Lin?s (1998) similarity method, we
retrieve the following neighbors for the first and
second sense, respectively:
1. awareness, feeling, instinct, enthusiasm, sen-
sation, vision, tradition, consciousness, anger,
panic, loyalty
2. emotion, belief, meaning, manner, necessity,
tension, motivation
No neighbors are associated with the last two
senses, indicating that they are not prevalent
enough in the BNC to be detected by this method.
Once sense-specific neighbors are acquired, the
next stage is to replace all instances of the neigh-
bors in the corpus with the target ambiguous word
labeled with the appropriate sense. For example,
when encountering the sentence ?... attempt to
state the meaning of a word?, our method would
automatically transform this to ?... attempt to state
the sense (s#2) of a word.? These pseudo-labeled
instances comprise the training instances we pro-
vide to our machine learning algorithms.
4 Experimental Setup
We evaluated the performance of our approach on
benchmark datasets. In this section we give details
1
We are using the coarse-grained representation according
to Senseval 2 annotators. The sense definitions are simplified
for the sake of brevity.
regarding our training and test data, and describe
the features and machine learners we employed.
Finally, we discuss the methods to which we com-
pare our approach.
4.1 Data
Our experiments use a subset of the data provided
for the English lexical sample task in the Sen-
seval 2 (Preiss and Yarowsky, 2001) and Sense-
val 3 (Mihalcea and Edmonds, 2004) evaluation
exercises. Since our method does not require hand
tagged training data, we merged the provided train-
ing and test data into a single test set.
As a proof of concept we focus on the disam-
biguation of nouns, since they constitute the largest
portion of content words (50% in the BNC). In ad-
dition, WordNet, which is our semantic resource
and point of comparison, has a wide coverage
of nouns. Also, for many tasks and applications
(e.g., web queries) nouns are the most frequently
encountered part-of-speech (Jansen et al, 2000).
We made use of the coarse-grained sense group-
ings provided for both Senseval datasets. For many
applications (e.g., information retrieval) coarsely
defined senses are more useful (see Snow et al
(2007) for discussion).
Our training data was created from the BNC us-
ing different ways of obtaining the neighbors of the
target word. As described in Section 3 we retrieved
neighbors using Lin?s (1998) similarity measure
on a RASP parsed (Briscoe and Carroll, 2002) ver-
sion of the BNC. We used subject and object de-
pendencies, as well as adjective and noun modifier
dependencies. We also created training data sets
using collocational neighbors. Specifically, using
the InfoMap toolkit
2
, we constructed vector-based
representations for individual words from the BNC
using a term-document matrix and the cosine sim-
ilarity measure. Vectors were initially constructed
with 1,000 dimensions, the most frequent con-
tent words. The space was reduced to 100 dimen-
sions with singular value decomposition. Finally,
we also extracted neighbors from WordNet using
first-order and sibling relations (i.e., hyponyms of
the same hypernym). A problem often encountered
when using dictionary-based neighbors is that they
are themselves polysemous, and the related sense
is often not the most prominent one in the corpus,
which leads to noisy data. We therefore experi-
mented with using all neighbors for a given word
2
http://infomap.stanford.edu/
68
?The philosophical explanation of authority is not an
attempt to state the sense of a word.?
Contextual features
?10 words explanation, of, authority, be, ...
?5 words an, attempt, to, state, of, a, ...
Collocational features
-2/+0 n-gram state the X
-1/+1 n-gram the X of
-0/+2 n-gram X of a
-2/+0 POS n-gram Verb Det X
-1/+1 POS n-gram Det X Prep
-0/+2 POS n-gram X Prep Det
Syntactic features
Object of Verb obj of state
Table 1: Example sentence and extracted features
for the word sense; X denotes the target word.
or only those which are monosemous and hope-
fully less noisy. In all cases we used 50 neighbors,
the most similar nouns to the target.
4.2 Features
We used a rich feature space based on lemmas,
part-of-speech (POS) tags and a variety of posi-
tional and syntactic relationships of the target word
capturing both immediate local context and wider
context. These feature types have been widely used
in WSD algorithms (see Lee and Ng (2002) for an
evaluation of their effectiveness). Their use is illus-
trated on a sample English sentence for the target
word sense in Table 1.
4.3 Supervised Classifiers
One of our evaluation goals was to examine the
effect of our training-data creation procedure on
different types of classifiers and determine which
ones are most suited for use with our method. We
therefore chose three supervised classifiers (sup-
port vector machines, maximum entropy, and label
propagation) which are based on different learn-
ing paradigms and have shown competitive per-
formance in WSD (Niu et al, 2005; Preiss and
Yarowsky, 2001; Mihalcea and Edmonds, 2004).
We summarize below their main characteristics
and differences.
Support Vector Machines SVMs model clas-
sification as the problem of finding a separating
hyperplane in a high dimensional vector space.
They focus on differentiating between the most
problematic cases ? instances which are close to
each other in the high dimensional space, but have
different labels. They are discriminative, rather
than generative, and do not explicitly model the
classes. SVMs have been applied successfully in
many NLP tasks. We used the multi-class bound-
constrained support vector classification (SVC)
version of SVM described in Hsu and Lin (2001)
and implemented in the BSVM package
3
. All pa-
rameters were set to their default values with the
exception of the misclassification penalty, which
was set to a high value (1,000) to penalize labeling
all instances with the most frequent sense.
Maximum Entropy Model Maximum entropy-
based classifiers are a common alternative to other
probabilistic classifiers, such as Naive Bayes, and
have received much interest in various NLP tasks
ranging from part-of-speech tagging to parsing
and text classification. They represent a probabilis-
tic, global constrained approach. They assume a
uniform, zero-knowledge model, under the con-
straints of the training dataset. The classifier finds
the (unique) maximal entropy model which con-
forms to the expected feature distribution of the
training data. In our experiments, we usedMegam
4
a publicly available maximum entropy classifier
(Daum?e III, 2004) with the default parameters.
Label Propagation The basic Label Propaga-
tion algorithm (Zhu and Ghahramani, 2002) repre-
sents labeled and unlabeled instances as nodes in
an undirected graph with weighted edges. Initially
only the known data nodes are labeled. The goal
is to propagate labels from labeled to unlabeled
points along the weighted edges. The weights are
based on distance in a high-dimensional space. At
each iteration, only the original labels are fixed,
whereas the propagated labels are ?soft?, and may
change in subsequent iterations. This property al-
lows the final labeling to be affected by more dis-
tant labels, that have propagated further, and gives
the algorithm a global aspect. We used SemiL
5
, a
publicly available implementation of label propa-
gation (all parameters were set to default values).
4.4 Comparison with State-of-the-art
As an upper bound, we considered the accuracy
of our classifiers when trained on the manually-
labeled Senseval data (using the same experimen-
tal settings and 5-fold crossvalidation). This can be
used to estimate the expected decrease in accuracy
caused solely by the use of our automatic sense la-
beling method. We also compared our approach to
other unsupervised ones. These include McCarthy
3
http://www.csie.ntu.edu.tw/?cjlin/bsvm/
4
http://www.isi.edu/?hdaume/megam/index.html
5
http://www.engineers.auckland.ac.nz/?vkec001
69
et al?s (2004) method for inferring the predomi-
nant sense and Lesk?s (1986) algorithm. We modi-
fied the latter slightly so as to increase its coverage
and used McCarthy et al?s first sense heuristic to
disambiguate unknown instances where no overlap
was found. For McCarthy et al we used parame-
ters they report as optimal.
5 Results
The evaluation of our method was motivated by
three questions: (1) How do different choices in
constructing the pseudo-labeled training data af-
fect WSD performance? Here, we would like to
assess whether the origin of the target word neigh-
bors (e.g., from a corpus or dictionary) matters.
(2) What is the degree of noise and subsequent
loss in accuracy incurred by our method? (3) How
does the proposed approach compare against other
unsupervised methods? In particular, we are in-
terested to find out whether we outperform Mc-
Carthy et al?s (2004) related method for predomi-
nant sense detection.
5.1 The Choice of Neighbors
Our results are summarized in Table 2. We re-
port accuracy (rather than F-score) since all al-
gorithms labeled all instances. The three center
columns present our results with the automatically
constructed training sets.
The best accuracies are observed when the la-
bels are created from distributionally similar words
using Lin?s (1998) dependency-based similarity
measure (Depend). We observe a small decrease in
performance (within the range of 2%?4%) when
using collocational neighbors without any syntac-
tic information.
6
Using the neighbors provided by
WordNet leads to worse results than using dis-
tributional neighbors. The differences in perfor-
mance are significant
7
(p < 0.01) on both Sense-
val datasets for all classifiers and for bothWordNet
configurations, i.e., using all neighbors (AllWN)
vs. monosemous ones (MonoWN).
This result may seem counterintuitive since
neighbors provided by a semantic resource are
based on expert knowledge and are often more ac-
curate than those obtained automatically. However,
semantic resources like WordNet are designed to
be as general as possible without a specific cor-
pus or domain in mind. They will therefore pro-
vide related words for all senses, even rare ones,
6
We omit these results from the table for brevity.
7
Throughout, we report significance using a ?
2
test.
which may not appear in our chosen corpus. Distri-
butional methods, on the other hand, are anchored
in the corpus. The extracted neighbors are usu-
ally relevant and representative of the corpus. An-
other drawback of resource-based neighbors is that
they often do not share local behavior, i.e., they
do not appear in the same immediate local con-
text and do not share the same syntax. For this rea-
son, the useful information that can be extracted
from their contexts tends to be topical (e.g., words
that are indicative of the domain), rather than lo-
cal (e.g., grammatical dependencies). Topical in-
formation is mostly useful when the difference be-
tween senses can be attributed to a specific domain.
However, for many words and senses, this is not
the case (Leacock et al, 1998).
5.2 Comparison against Manual Labels
The rightmost column of Table 2 shows the accu-
racy of our classifiers when these are trained on
the manually annotated Senseval datasets. In gen-
eral, all algorithms exhibit a similar level of per-
formance when trained on hand-coded data, with
slightly lower scores for Senseval 3. On Sense-
val 2, the SVM is significantly better than the other
two classifiers (p < 0.01). On Senseval 3, label
propagation is significantly worse than the others
(p < 0.01). The results shown here do not repre-
sent the highest achievable performance in a su-
pervised setting, but rather those obtained with-
out extensive parameter tuning. The best perform-
ing systems on coarse-grained nouns in Sense-
val 2 and 3 (Preiss and Yarowsky, 2001; Mihalcea
and Edmonds, 2004) achieved approximately 76%
and 80%, respectively. Besides being more finely
tuned, these systems employed more sophisticated
learning paradigms (e.g., ensemble learning).
When we compare the results from the manu-
ally labeled data to those achieved with the dis-
tributional neighbors, we can see that use of our
pseudo-labels results in accuracies that are ap-
proximately 8?10% lower. Since the results were
achieved using the same feature set and classi-
fier settings, the comparison provides an estimate
of the expected decrease in accuracy due only to
our unsupervised tagging method. With more de-
tailed feature engineering and more sophisticated
machine learning methods, we could probably im-
prove our classifiers? performance on the automat-
ically labeled dataset. Also note that improvements
in supervised methods can be expected to automat-
ically translate to improvements in unsupervised
70
Senseval 2 AllWN MonoWN Depend Manual
SVM 48.12 53.29 64.38 72.52
MaxEnt 40.93 52.11 62.32 71.91
LP 42.67 49.54 63.32 69.28
McCarthy 59.98
Lesk 48.12
Senseval 3 AllWN MonoWN Depend Manual
SVM 53.16 46.32 57.47 71.22
MaxEnt 49.67 44.85 57.35 71.75
LP 47.41 43.60 60.60 67.57
McCarthy 57.14
Lesk 48.66
Table 2: Accuracy (%) on Senseval 2 and 3 lexical
samples. Support vector machines (SVM), maxi-
mum entropy (MaxEnt) and label propagation (LP)
are trained on automatically and manually labeled
data sets
WSD using our method.
Interestingly, label propagation performed rela-
tively poorly on the manually labeled data. How-
ever, it ranks highly when using the automatic la-
bels. This may be due to the fact that LP is the
only algorithm that does not separate the train-
ing and test set (it is principally a semi-supervised
method), allowing the properties of both to influ-
ence the structure of the resulting graph. Since the
instances in the training data are not actual occur-
rences of the target word, it is important to learn
which instances in the training set are closest to a
given instance in the test set. The two other algo-
rithms only attempt to distinguish between classes
in the training set.
5.3 Other Unsupervised Methods
As shown in Table 2 our classifiers are signifi-
cantly better than Lesk on both Senseval datasets
(p < 0.01). They also significantly outperform the
automatically acquired predominant sense (Mc-
Carthy) on Senseval 2 (for the Maximum Entropy
classifier, the difference is significant at p < 0.05).
On Senseval 3, all classifiers quantitatively outper-
form the first sense heuristic, but the difference is
statistically significant only for label propagation
(p < 0.01). The differences in performance on the
two datasets can be explained by analyzing their
sense distributions. Senseval 3 has a higher level
of ambiguity (4.35 senses per word, on average,
compared to 3.28 for Senseval 2), and is there-
fore a more difficult dataset. Although Senseval 3
has a slightly lower percentage of first sense in-
stances, the higher ambiguity means that the skew
is, in fact, much higher than in Senseval 2. A high
Senseval 2 Depend Manual
SVM 14.3 (60.1) 16.9 (60.4)
MaxEnt 6.3 (66.9) 17.1 (56.7)
LP 8.9 (63.3) 14.8 (49.4)
Senseval 3 Depend Manual
SVM 17.6 (45.0) 23.3 (60.0)
MaxEnt 8.5 (55.0) 23.7 (60.9)
LP 5.6 (60.9) 17.8 (53.5)
Table 3: Percentage of non-first instances in auto-
matically and manually labeled training data; num-
bers in parentheses show the classifiers? accuracy
on these instances.
skew towards the predominant sense means there
are less instances from which we can learn about
the rarer senses, and that we run a higher risk when
labeling an instance as one of the rarer senses (in-
stead of defaulting to the predominant one).
Our method shares some of the principles of
McCarthy et al?s (2004) unsupervised algorithm.
However, instead of focusing on detecting a sin-
gle predominant sense throughout the corpus, we
build a dataset that will allow us to learn about and
identify all existing (prevalent) senses. Despite the
fact that the first-sense heuristic is a strong base-
line, and fall-back option in case of limited local
information, it is not a true context-specific WSD
algorithm. Any approach that ignores local con-
text, and labels all instances with a single sense
has limited effectiveness when WSD is needed
in an application. Context-indifferent methods run
the risk of completely mistaking the predominant
sense, thereby mis-labeling most of the instances,
whereas approaches that consider local context are
less prone to such large-scope errors.
We further analyzed the performance of our
method by examining instances labeled with
senses other than the most frequent one. Table 3
shows the percentage of such instances depend-
ing on the machine learner and type of training
data (automatic versus manual) being employed.
It also presents the classifiers? accuracy (figures
in parentheses) with regard to only the non-first
senses. When trained on the automatically labeled
data, our classifiers tend to be more conservative
in assigning non-first senses. Interestingly, we ob-
tain similar accuracies with the classifiers trained
on the manually labeled data, even though the lat-
ter assign more non-first senses. It is worth noting
that the SVM labels two to three times as many
instances with non-first-sense labels, yet achieves
similar levels of overall accuracy to the other clas-
71
sifiers (compare Tables 2 and 3) and only slightly
lower accuracy on the non-first senses. This would
make it a better choice when it is important to have
more data on rarer senses.
6 Conclusions and Future Work
We have presented an unsupervised approach to
WSD which retains many of the advantages of su-
pervised methods, while being free of the costly
requirement for human annotation. We demon-
strate that classifiers trained using our method can
out-perform state-of-the-art unsupervised meth-
ods, and approach the accuracy of fully-supervised
methods trained on manually-labeled data.
In the future we plan to scale our system to
the all-words task. There is nothing inherent in
our method that restricts us to the lexical sample,
which we chose primarily to assess the feasibil-
ity of our ideas. Another interesting direction con-
cerns the use of our method in a semi-supervised
setting. For example, we could automatically ac-
quire labeled instances for words whose senses are
rare in a manually tagged dataset. Finally, we could
potentially improve accuracy, at the expense of
coverage, by estimating confidence scores on the
classifiers? predictions, and assigning labels only
to instances with high confidence.
Acknowledgments
The authors acknowledge the support of EPSRC
(grant EP/C538447/1) and would like to thank
David Talbot for his insightful suggestions.
References
S. Banerjee, T. Pedersen. 2003. Extended gloss overlaps as
a measure of semantic relatedness. In Proc. of the 18th
IJCAI, 805?810, Acapulco.
R. Barzilay, M. Elhadad. 1997. Using lexical chains for text
summarization. In Proc. of the Intelligent Scalable Text
Summarization Workshop, Madrid, Spain.
T. Briscoe, J. Carroll. 2002. Robust accurate statistical an-
notation of general text. In Proc. of the 3rd LREC, 1499?
1504, Las Palmas, Gran Canaria.
A. Budanitsky, G. Hirst. 2001. Semantic distance in Word-
Net: An experimental, application-oriented evaluation of
five measures. In Proc. of the ACL Worskhop on WordNet
and Other Lexical Resources, Pittsburgh, PA.
Y. S. Chan, H. T. Ng. 2007. Word sense disambiguation
improves statistical machine translation. In Proc. of the
45th ACL, 33?40, Prague, Czech Republic.
H. Daum?e III. 2004. Notes on CG and LM-BFGS optimiza-
tion of logistic regression.
P. Edmonds. 2000. Designing a task for SENSEVAL-2, 2000.
Technical note.
C. Fellbaum, ed. 1998. WordNet: An Electronic Database.
MIT Press, Cambridge, MA.
W. Gale, K. Church, D. Yarowsky. 1992. A method for dis-
ambiguating word senses in a large corpus. Computers
and the Humanities, 26(2):415?439.
M. Galley, K. McKeown. 2003. Improving word sense dis-
ambiguation in lexical chaining. In Proc. of the 18th IJ-
CAI, 1486?1488, Acapulco.
C. Hsu, C. Lin. 2001. A comparison of methods for multi-
class support vector machines, 2001. Technical report, De-
partment of Computer Science and Information Engineer-
ing, National Taiwan University, Taipei, Taiwan.
B. J. Jansen, A. Spink, A. Pfaff. 2000. Linguistic aspects
of web queries, 2000. American Society of Information
Science, Chicago.
C. Leacock, M. Chodorow, G. A. Miller. 1998. Using cor-
pus statistics and wordnet relations for sense identification.
Computational Linguistics, 24(1):147?165.
Y. K. Lee, H. T. Ng. 2002. An empirical evaluation of knowl-
edge sources and learning algorithms for word sense dis-
ambiguation. In Proc. of the EMNLP, 41?48, NJ.
M. Lesk. 1986. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from
an ice cream cone. In Proc. of the 5th SIGDOC, 24?26,
New York, NY.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proc. of the ACL/COLING, 768?774, Montreal.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004. Finding
predominant senses in untagged text. In Proc. of the 42th
ACL, 280?287, Barcelona, Spain.
R. F. Mihalcea, P. Edmonds, eds. 2004. Proc. of the
SENSEVAL-3, Barcelona, 2004.
R. F. Mihalcea. 2002. Word sense disambiguation with
pattern learning and automatic feature selection. Natural
Language Engineering, 8(4):343?358.
H. T. Ng, B. Wang, Y. S. Chan. 2003. Exploiting parallel
texts for word sense disambiguation: an empirical study.
In Proc. of the 41st ACL, 455?462, Sapporo, Japan.
H. T. Ng. 1997. Getting serious about word sense disam-
biguation. In Proc. of the ACL SIGLEX Workshop on Tag-
ging Text with Lexical Semantics: Why, What, and How?,
1?7, Washington, DC.
Z. Y. Niu, D. H. Ji, C. L. Tan. 2005. Word sense disam-
biguation using label propagation based semi-supervised
learning. In Proc. of the 43rd ACL, 395?402, Ann Arbor.
J. Preiss, D. Yarowsky, eds. 2001. Proc. of the 2nd Interna-
tional Workshop on Evaluating Word Sense Disambigua-
tion Systems, Toulouse, France, 2001.
G. Ramakrishnan, A. Jadhav, A. Joshi, S. Chakrabarti,
P. Bhattacharyya. 2003. Question answering via Bayesian
inference on lexical relations. In Proc. of the ACL 2003
workshop on Multilingual summarization and QA, 1?10.
R. Snow, S. Prakash, D. Jurafsky, A. Y. Ng. 2007. Learning
to merge word senses. In Proc. of the EMNLP/CoNLL,
1005?1014, Prague, Czech Republic.
J. Weeds. 2003. Measures and Applications of Lexical Dis-
tributional Similarity. Ph.D. thesis, University of Sussex.
D. Yarowsky. 1992. Word-sense disambiguation using statis-
tical models of Roget?s categories trained on large corpora.
In Proc. of the 14th COLING, 454?460, Nantes, France.
X. Zhu, Z. Ghahramani. 2002. Learning from labeled and
unlabeled data with label propagation. Technical report,
CMU-CALD-02, 2002.
72
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 97?104
Manchester, August 2008
ParaMetric: An Automatic Evaluation Metric for Paraphrasing
Chris Callison-Burch
Center for Speech and Language Processing
Johns Hopkins University
3400 N. Charles St.
Baltimore, MD 21218
Trevor Cohn Mirella Lapata
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Abstract
We present ParaMetric, an automatic eval-
uation metric for data-driven approaches to
paraphrasing. ParaMetric provides an ob-
jective measure of quality using a collec-
tion of multiple translations whose para-
phrases have been manually annotated.
ParaMetric calculates precision and recall
scores by comparing the paraphrases dis-
covered by automatic paraphrasing tech-
niques against gold standard alignments of
words and phrases within equivalent sen-
tences. We report scores for several estab-
lished paraphrasing techniques.
1 Introduction
Paraphrasing is useful in a variety of natural lan-
guage processing applications including natural
language generation, question answering, multi-
document summarization and machine translation
evaluation. These applications require paraphrases
for a wide variety of domains and language us-
age. Therefore building hand-crafted lexical re-
sources such as WordNet (Miller, 1990) would be
far too laborious. As such, a number of data-driven
approaches to paraphrasing have been developed
(Lin and Pantel, 2001; Barzilay and McKeown,
2001; Barzilay and Lee, 2003; Pang et al, 2003;
Quirk et al, 2004; Bannard and Callison-Burch,
2005). Despite this spate of research, no objective
evaluation metric has been proposed.
In absence of a repeatable automatic evaluation,
the quality of these paraphrasing techniques was
gauged using subjective manual evaluations. Sec-
tion 2 gives a survey of the various evaluation
methodologies used in previous research. It has
not been possible to directly compare paraphrasing
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
techniques, because each one was evaluated using
its own idiosyncratic experimental design. More-
over, because these evaluations were performed
manually, they are difficult to replicate.
We introduce an automatic evaluation metric,
called ParaMetric, which uses paraphrasing tech-
niques to be compared and enables an evaluation
to be easily repeated in subsequent research. Para-
Metric utilizes data sets which have been annotated
with paraphrases. ParaMetric compares automatic
paraphrases against reference paraphrases.
In this paper we:
? Present a novel automatic evaluation metric
for data-driven paraphrasing methods;
? Describe how manual alignments are cre-
ated by annotating correspondences between
words in multiple translations;
? Show how phrase extraction heuristics from
statistical machine translation can be used to
enumerate paraphrases from the alignments;
? Report ParaMetric scores for a number of ex-
isting paraphrasing methods.
2 Related Work
No consensus has been reached with respect to the
proper methodology to use when evaluating para-
phrase quality. This section reviews past methods
for paraphrase evaluation.
Researchers usually present the quality of their
automatic paraphrasing technique in terms of a
subjective manual evaluation. These have used
a variety of criteria. For example, Barzilay
and McKeown (2001) evaluated their paraphrases
by asking judges whether paraphrases were ?ap-
proximately conceptually equivalent.? Ibrahim
et al (2003) asked judges whether their para-
phrases were ?roughly interchangeable given the
genre.? Bannard and Callison-Burch (2005) re-
placed phrases with paraphrases in a number of
97
sentences and asked judges whether the substi-
tutions ?preserved meaning and remained gram-
matical.? These subjective evaluations are rather
vaguely defined and not easy to reproduce.
Others evaluate paraphrases in terms of whether
they improve performance on particular tasks.
Callison-Burch et al (2006b) measure improve-
ments in translation quality in terms of Bleu score
(Papineni et al, 2002) and in terms of subjective
human evaluation when paraphrases are integrated
into a statistical machine translation system. Lin
and Pantel (2001) manually judge whether a para-
phrase might be used to answer questions from the
TREC question-answering track. To date, no one
has used task-based evaluation to compare differ-
ent paraphrasing methods. Even if such an eval-
uation were performed, it is unclear whether the
results would hold for a different task. Because of
this, we strive for a general evaluation rather than
a task-specific one.
Dolan et al (2004) create a set of manual word
alignments between pairs of English sentences.
We create a similar type of data, as described in
Section 4. Dolan et al use heuristics to draw pairs
of English sentences from a comparable corpus
of newswire articles, and treat these as potential
paraphrases. In some cases these sentence pairs
are good examples of paraphrases, and in some
cases they are not. Our data differs because it
is drawn from multiple translations of the same
foreign sentences. Barzilay (2003) suggested that
multiple translations of the same foreign source
text were a perfect source for ?naturally occur-
ring paraphrases? because they are samples of text
which convey the same meaning but are produced
by different writers. That being said, it may be
possible to use Dolan et als data toward a similar
end. Cohn et al (to appear) compares the use of
the multiple translation corpus with the MSR cor-
pus for this task.
The work described here is similar to work in
summarization evaluation. For example, in the
Pyramid Method (Nenkova et al, 2007) content
units that are similar across human-generated sum-
maries are hand-aligned. These can have alter-
native wordings, and are manually grouped. The
idea of capturing these and building a resource for
evaluating summaries is in the same spirit as our
methodology.
3 Challenges for Evaluating Paraphrases
Automatically
There are several problems inherent to automati-
cally evaluating paraphrases. First and foremost,
developing an exhaustive list of paraphrases for
any given phrase is difficult. Lin and Pantel (2001)
illustrate the difficulties that people have generat-
ing a complete list of paraphrases, reporting that
they missed many examples generated by a sys-
tem that were subsequently judged to be correct. If
a list of reference paraphrases is incomplete, then
using it to calculate precision will give inaccurate
numbers. Precision will be falsely low if the sys-
tem produces correct paraphrases which are not in
the reference list. Additionally, recall is indeter-
minable because there is no way of knowing how
many correct paraphrases exist.
There are further impediments to automatically
evaluating paraphrases. Even if we were able to
come up with a reasonably exhaustive list of para-
phrases for a phrase, the acceptability of each para-
phrase would vary depending on the context of
the original phrase (Szpektor et al, 2007). While
lexical and phrasal paraphrases can be evaluated
by comparing them against a list of known para-
phrases (perhaps customized for particular con-
texts), this cannot be naturally done for struc-
tural paraphrases which may transform whole sen-
tences.
We attempt to resolve these problems by hav-
ing annotators indicate correspondences in pairs
of equivalent sentences. Rather than having peo-
ple enumerate paraphrases, we asked that they per-
form the simper task of aligning paraphrases. Af-
ter developing these manual ?gold standard align-
ments? we can gauge how well different automatic
paraphrases are at aligning paraphrases within
equivalent sentences. By evaluating the perfor-
mance of paraphrasing techniques at alignment,
rather than at matching a list of reference para-
phrases, we obviate the need to have a complete
set of paraphrases.
We describe how sets of reference paraphrases
can be extracted from the gold standard align-
ments. While these sets will obviously be frag-
mentary, we attempt to make them more complete
by aligning groups of equivalent sentences rather
than only pairs. The paraphrase sets that we extract
are appropriate for the particular contexts. More-
over they may potentially be used to study struc-
tural paraphrases, although we do not examine that
98
an
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resign
to
him
want
others
while
,
him
impeach
to
propose
people
some
a
n
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resignation
his
tender
to
him
want
who
those
and
him
impeaching
propose
who
those
are
there
.
voluntarily
office
leave
to
him
want
some
and
him
against
indictment
an
proposing
are
some
a
n
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
Figure 1: Pairs of English sentences were aligned
by hand. Black squares indicate paraphrase corre-
spondences.
in this paper.
4 Manually Aligning Paraphrases
We asked monolingual English speakers to align
corresponding words and phrases across pairs of
equivalent English sentences. The English sen-
tences were equivalent because they were transla-
tions of the same foreign language text created by
different human translators. Our annotators were
instructed to align parts of the sentences which
had the same meaning. Annotators were asked
to prefer smaller one-to-one word alignments, but
were allowed to create one-to-many and many-to-
many alignments where appropriate. They were
given a set of annotation guidelines covering spe-
cial cases such as repetition, pronouns, genitives,
phrasal verbs and omissions (Callison-Burch et al,
2006a). The manual correspondences are treated
as gold standard alignments.
We use a corpus that contains eleven En-
glish translations of Chinese newswire documents,
which were commissioned from different transla-
tion agencies by the Linguistics Data Consortium
1
.
The data was created for the Bleu machine trans-
lation evaluation metric (Papineni et al, 2002),
which uses multiple translations as a way of cap-
turing allowable variation in translation. Whereas
the Bleu metric requires no further information,
our method requires a one-off annotation to explic-
itly show which parts of the multiple translations
constitute paraphrases.
The rationale behind using a corpus with eleven
translations was that a greater number of transla-
tions would likely result in a greater number of
paraphrases for each phrase. Figure 1 shows the
alignments that were created between one sen-
tence and three of its ten corresponding transla-
tions. Table 1 gives a list of non-identical words
and phrases that can be paired by way of the word
alignments. These are the basic paraphrases con-
tained within the three sentence pairs. Each phrase
has up to three paraphrases. The maximum num-
ber of paraphrases for a given span in each sen-
tence is bounded by the number of equivalent sen-
tences that it is paired with.
In addition to these basic paraphrases, longer
paraphrases can also be obtained using the heuris-
tic presented in Och and Ney (2004) for extract-
ing phrase pairs (PP) from word alignments A, be-
tween a foreign sentence f
J
1
and an English sen-
1
See LDC catalog number 2002T01.
99
some some people, there are those who
want propose, are proposing
to impeach an indictment against, impeach-
ing
and while
others some, those who
expect want
step down resign, leave office voluntarily,
tender his resignation
Table 1: Non-identical words and phrases which
are identified as being in correspondence by the
alignments in Figure 1.
tence e
I
1
:
PP (f
J
1
, e
I
1
, A) = {(f
j+m
j
, e
i+n
i
) :
?(i
?
, j
?
) ? A : j ? j
?
? j + m ? i ? i
?
? i + n
??(i
?
, j
?
) ? A : j ? j
?
? j + m? ? i ? i
?
? i + n}
When we apply the phrase extraction heuris-
tic to aligned English sentences, we add the con-
straint f
j+m
j
6= e
i+n
i
to exclude phrases that are
identical. This heuristic would allow ?some peo-
ple propose to impeach him,? ?some are proposing
an indictment against him,? and ?there are those
who propose impeaching him? to be extracted
as paraphrases of ?some want to impeach him.?
The heuristic extracts a total of 142 non-identical
phrase pairs from the three sentences given in Fig-
ure 1.
For the results reported in this paper, annotators
aligned 50 groups of 10 pairs of equivalent sen-
tences, for a total of 500 sentence pairs. These
were assembled by pairing the first of the LDC
translations with the other ten (i.e. 1-2, 1-3, 1-4,
..., 1-11). The choice of pairing one sentence with
the others instead of doing all pairwise combina-
tions was made simply because the latter would
not seem to add much information. However, the
choice of using the first translator as the key was
arbitrary.
Annotators corrected a set of automatic word
alignments that were created using Giza++ (Och
and Ney, 2003), which was trained on a total of
109,230 sentence pairs created from all pairwise
combinations of the eleven translations of 993 Chi-
nese sentences.
The average amount of time spent on each of
the sentence pairs was 77 seconds, with just over
eleven hours spent to annotate all 500 sentence
pairs. Although each sentence pair in our data
set was annotated by a single annotator, Cohn et
al. (to appear) analyzed the inter-annotator agree-
ment for randomly selected phrase pairs from the
same corpus, and found inter-annotator agreement
of
?
C = 0.85 over the aligned words and
?
C = 0.63
over the alignments between basic phrase pairs,
where
?
C is measure of inter-rater agreement in the
style of Kupper and Hafner (1989).
5 ParaMetric Scores
We can exploit the manually aligned data to com-
pute scores in two different fashions. First, we
can calculate how well an automatic paraphrasing
technique is able to align the paraphrases in a sen-
tence pair. Second, we can calculate the lower-
bound on precision for a paraphrasing technique
and its relative recall by enumerating the para-
phrases from each of the sentence groups. The first
of these score types does not require groups of sen-
tences, only pairs.
We calculate alignment accuracy by comparing
the manual alignments for the sentence pairs in the
test corpus with the alignments that the automatic
paraphrasing techniques produce for the same
sentence pairs. We enumerate all non-identical
phrase pairs within the manually word-aligned
sentence pairs and within the automatically word
aligned sentence pairs using PP . We calculate the
precision and recall of the alignments by taking
the intersection of the paraphrases extracted from
the manual alignments M , and the paraphrases
extracted from a system?s alignments S:
Align
Prec
=
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S) ? PP (e
1
, e
2
,M)|
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S)|
Align
Recall
=
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S) ? PP (e
1
, e
2
,M)|
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
,M)|
Where e
1
, e
2
are pairs of English sentence from
the test corpus.
Measuring a paraphrasing method?s perfor-
mance on the task of aligning the paraphrases is
somewhat different than what most paraphrasing
methods do. Most methods produce a list of para-
phrases for a given input phrase, drawing from
a large set of rules or a corpus larger than our
small test set. We therefore also attempt to mea-
sure precision and recall by comparing the set of
100
paraphrases that method M produces for phrase p
that occurs in sentence s. We denote this set as
para
M
(p, s), where s is an optional argument for
methods that constrain their paraphrases based on
context.
Our reference sets of paraphrases are generated
in a per group fashion. We enumerate the reference
paraphrases for phrase p in sentence s in group G
as
para
REF
(p
1
, s
1
, G) =
{p
2
: ?(p
1
, p
2
) ?
?
<s
1
,s
2
,A>?G
PP (s
1
, s
2
, A)}
The maximum size of this set is the number of
sentence pairs in G. Because this set of reference
paraphrases is incomplete, we can only calculate
a lower bound on the precision of a paraphrasing
method and its recall relative to the reference
paraphrases. We call these LB-Precision and
Rel-Recall and calculate them as follows:
LB-Precision =
?
<s,G>?C
?
p?s
|para
M
(p, s) ? para
REF
(p
1
, s,G)|
|para
M
(p, s)|
Rel-Recall =
?
<s,G>?C
?
p?s
|para
M
(p, s) ? para
REF
(p
1
, s,G)|
|para
REF
(p
1
, s,G)|
For these metrics we require the test corpus
C to be a held-out set and restrict the automatic
paraphrasing techniques from drawing paraphrases
from it. The idea is instead to see how well these
techniques are able to draw paraphrases from the
other sources of data which they would normally
use.
6 Paraphrasing Techniques
There are a number of established methods for
extracting paraphrases from data. We describe
the following methods in this section and evaluate
them in the next:
? Pang et al (2003) used syntactic alignment to
merge parse trees of multiple translations,
? Quirk et al (2004) treated paraphrasing as
monolingual statistical machine translation,
? Bannard and Callison-Burch (2005) used
bilingual parallel corpora to extract para-
phrases.
S
NP VP
NN
persons
AUX
were
CD
12
VP
VB
killed
S
NP VP
NN
people
VB
died
CD
twelve
VB
NP VP
CD NN
12
twelve
people
persons
...
were
...
died
...
killed
AUX VP
BEG END
12
twelve
people
persons
died
were
killed
Tree 1 Tree 2
+
Parse Forest
Word Lattice
Merge
Linearize
Figure 2: Pang et al (2003) created word graphs
by merging parse trees. Paths with the same start
and end nodes are treated as paraphrases.
Pang et al (2003) use multiple translations to
learn paraphrases using a syntax-based alignment
algorithm, illustrated in Figure 2. Parse trees were
merged into forests by grouping constituents of the
same type (for example, the two NPs and two VPs
are grouped). Parse forests were mapped onto fi-
nite state word graphs by creating alternative paths
for every group of merged nodes. Different paths
within the resulting word lattice are treated as para-
phrases of each other. For example, in the word lat-
tice in Figure 2, people were killed, persons died,
persons were killed, and people died are all possi-
ble paraphrases of each other.
Quirk et al (2004) treated paraphrasing as
?monolingual statistical machine translation.?
They created a ?parallel corpus? containing pairs
of English sentences by drawing sentences with a
low edit distance from news articles that were writ-
ten about the same topic on the same date, but pub-
lished by different newspapers. They formulated
the problem of paraphrasing in probabilistic terms
in the same way it had been defined in the statisti-
cal machine translation literature:
e?
2
= argmax
e
2
p(e
2
|e
1
)
= argmax
e
2
p(e
1
|e
2
)p(e
2
)
101
I do not believe in mutilating dead bodies
 
cad?veresno soy partidaria mutilarde
cad?veres de inmigrantes ilegales ahogados a la playatantosarrojaEl mar ...
corpsesSo many of drowned illegals get washed up on beaches ...
Figure 3: Bannard and Callison-Burch (2005) ex-
tracted paraphrases by equating English phrases
that share a common translation.
Where p(e
1
|e
2
) is estimated by training word
alignment models over the ?parallel corpus? as in
the IBM Models (Brown et al, 1993), and phrase
translations are extracted from word alignments as
in the Alignment Template Model (Och, 2002).
Bannard and Callison-Burch (2005) also used
techniques from statistical machine translation to
identify paraphrases. Rather than drawing pairs
of English sentences from a comparable corpus,
Bannard and Callison-Burch (2005) used bilingual
parallel corpora. They identified English para-
phrases by pivoting through phrases in another lan-
guage. They located foreign language translations
of an English phrase, and treated the other En-
glish translations of those foreign phrases as poten-
tial paraphrases. Figure 3 illustrates how a Span-
ish phrase can be used as a point of identifica-
tion for English paraphrases in this way. Bannard
and Callison-Burch (2005) defined a paraphrase
probability p(e
2
|e
1
) in terms of the translation
model probabilities p(f |e
1
) and p(e
2
|f). Since e
1
can translate as multiple foreign language phrases,
they sum over f , and since multiple parallel cor-
pora can be used they summed over each parallel
corpus C:
e?
2
= arg max
e
2
6=e
1
p(e
2
|e
1
)
? arg max
e
2
6=e
1
?
C
?
f in C
p(f |e
1
)p(e
2
|f)
7 Comparing Paraphrasing Techniques
with ParaMetric
7.1 Training data for word alignments
In order to calculate Align
Prec
and Align
Recall
for the different paraphrasing techniques, we had
them automatically align the 500 manually aligned
sentence pairs in our test sets.
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
Align
Prec
.62 .65 .73
Align
Recall
.11 .10 .46
LB-Precision .14 .33 .68
Rel-Recall .07 .03 .01
Table 2: Summary results for scoring the different
paraphrasing techniques using our proposed auto-
matic evaluations.
Bo Pang provided syntactic alignments for the
500 sentence pairs. The word lattices combine the
groups of sentences. When measuring alignment
quality, we took pains to try to limit the extracted
phrase pairs to those which occurred in each sen-
tence pair, but we acknowledge that our methodol-
ogy may be flawed.
We created training data for the monolingual
statistical machine translation method using all
pairwise combination of eleven English transla-
tions in LDC2002T01. All combinations of the
eleven translations of the 993 sentences in that
corpus resulted in 109,230 sentence pairs with
3,266,769 words on each side. We used this data
to train an alignment model, and applied it to the
500 sentence pairs in our test set.
We used the parallel corpus method to align
each pair of English sentences by creating interme-
diate alignments through their Chinese source sen-
tences. The bilingual word alignment model was
trained on a Chinese-English parallel corpus from
the NIST MT Evaluation consisting of 40 million
words. This was used to align the 550 Chinese-
English sentence pairs constructed from the test
set.
7.2 Training data for precision and recall
Each of the paraphrasing methods generated para-
phrases for LB-Precision and Rel-Recall us-
ing larger training sets of data than for the align-
ments. For the syntax-based alignment method,
we excluded the 50 word lattices corresponding
to the test set. We used the remaining 849 lat-
tices for the LDC multiple translation corpus.
For the monolingual statistical machine transla-
tion method, we downloaded the Microsoft Re-
search Paraphrase Phrase Table, which contained
paraphrases for nearly 9 million phrases, gener-
102
ated from the method described in Quirk et al
(2004). For the parallel corpus method, we de-
rived paraphrases from the entire Europarl corpus,
which contains parallel corpora between English
and 10 other languages, with approximately 30
million words per language. We limited both the
Quirk et al (2004) and the Bannard and Callison-
Burch (2005) paraphrases to those with a probabil-
ity greater than or equal to 1%.
7.3 Results
Table 2 gives a summary of how each of the para-
phrasing techniques scored using the four different
automatic metrics. The precision of their align-
ments was in the same ballpark, with each para-
phrasing method reaching above 60%. The mono-
lingual SMT method vastly outstripped the others
in terms of recall and therefore seems to be the
best on the simplified task of aligning paraphrases
within pairs of equivalent sentences.
For the task of generating paraphrases from un-
restricted resources, the monolingual SMT method
again had the highest precision, although time
time its recall was quite low. The 500 manually
aligned sentence pairs contained 14,078 unique
paraphrases for phrases of 5 words or less. The
monolingual SMT method only posited 230 para-
phrases with 156 of them being correct. By con-
trast, the syntactic alignment method posited 1,213
with 399 correct, and the parallel corpus method
posited 6,914 with 998 correct. Since the refer-
ence lists are incomplete by their very nature, the
LB-Precision score gives a lower-bound on the
precision, and the Rel-Recall gives recall only
with respect to the partial list of paraphrases.
Table 3 gives the performance of the differ-
ent paraphrasing techniques for different phrase
lengths.
8 Conclusions
In this paper we defined a number of automatic
scores for data-driven approaches to paraphrasing,
which we collectively dub ?ParaMetric?. We dis-
cussed the inherent difficulties in automatically as-
sessing paraphrase quality. These are due primar-
ily to the fact that it is exceedingly difficult to
create an exhaustive list of paraphrases. To ad-
dress this problem, we introduce an artificial task
of aligning paraphrases within pairs of equivalent
English sentences, which guarantees accurate pre-
cision and recall numbers. In order to measure
alignment quality, we create a set of gold standard
alignments. While the creation of this data does
require some effort, it seems to be a manageable
amount, and the inter-annotator agreement seems
reasonable.
Since alignment is not perfectly matched with
what we would like automatic paraphrasing tech-
niques to do, we also use the gold standard align-
ment data to measure a lower bound on the preci-
sion of a method?s paraphrases, as well as its recall
relative to the limited set of paraphrases. Future
studies should examine how well these scores rank
different paraphrasing methods when compared to
human judgments. Follow up work should inves-
tigate the number of equivalent English sentences
that are required for reasonably complete lists of
paraphrases. In this work we aligned sets of eleven
different English sentences, but we acknowledge
that such a data set is rare and might make it dif-
ficult to port this method to other domains or lan-
guages.
The goal of this work is to develop a set of
scores that both allows different paraphrasing tech-
niques to be compared objectively and provides an
easily repeatable method for automatically evalu-
ating paraphrases. This has hitherto not been pos-
sible. The availability of an objective, automatic
evaluation metric for paraphrasing has the poten-
tial to impact research in the area in a number of
ways. It not only allows for the comparison of dif-
ferent approaches to paraphrasing, as shown in this
paper, but also provides a way to tune the parame-
ters of a single system in order to optimize its qual-
ity.
Acknowledgments
The authors are grateful to Bo Pang for providing
the word lattices from her method, to Stefan Rie-
zler for his comments on an early draft of this pa-
per, and to Michelle Bland for proofreading. This
work was supported by the National Science Foun-
dation under Grant No. 0713448. The views and
findings are the authors? alone.
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-2005), Ann Ar-
bor, Michigan.
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
103
Align
Prec
Align
Recall
LB-Precision Rel-Recall
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
Length = 1 .54 .48 .64 .24 .18 .56 .15 .25 .59 .20 .16 .02
Length ? 2 .56 .56 .69 .19 .13 .52 .15 .31 .66 .18 .10 .03
Length ? 3 .59 .60 .71 .14 .12 .49 .15 .32 .66 .13 .06 .02
Length ? 4 .60 .63 .72 .12 .11 .48 .14 .33 .68 .09 .04 .01
Length ? 5 .62 .65 .73 .11 .10 .46 .14 .33 .68 .07 .03 .01
Table 3: Results for paraphrases of continuous subphrases of various lengths.
ing multiple-sequence alignment. In Proceedings of
HLT/NAACL-2003, Edmonton, Alberta.
Barzilay, Regina and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2001).
Barzilay, Regina. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Gener-
ation. Ph.D. thesis, Columbia University, New York.
Brown, Peter, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?311, June.
Callison-Burch, Chris, Trevor Cohn, and Mirella Lap-
ata. 2006a. Annotation guidelines for paraphrase
alignment. Tech report, University of Edinburgh.
Callison-Burch, Chris, Philipp Koehn, and Miles Os-
borne. 2006b. Improved statistical machine
translation using paraphrases. In Proceedings of
HLT/NAACL-2006, New York, New York.
Cohn, Trevor, Chris Callison-Burch, and Mirella Lap-
ata. to appear. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Compu-
tational Linguistics.
Dolan, Bill, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Ibrahim, Ali, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the Second Inter-
national Workshop on Paraphrasing (ACL 2003).
Kupper, Lawrence L. and Kerry B. Hafner. 1989. On
assessing interrater agreement for multiple attribute
responses. Biometrics, 45(3):957?967.
Lin, Dekang and Patrick Pantel. 2001. Discovery of
inference rules from text. Natural Language Engi-
neering, 7(3):343?360.
Miller, George A. 1990. Wordnet: An on-line lexical
database. Special Issue of the International Journal
of Lexicography, 3(4).
Nenkova, Ani, Rebecca Passonneau, and Kathleen
McKeown. 2007. The pyramid method: incorporat-
ing human content selection variation in summariza-
tion evaluation. ACM Transactions on Speech and
Language Processing, 4(2).
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Och, Franz Josef. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen Department of
Computer Science, Aachen, Germany.
Pang, Bo, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences.
In Proceedings of HLT/NAACL-2003, Edmonton,
Alberta.
Papineni, Kishore, Salim Roukos, ToddWard, andWei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2002), Philadelphia, Penn-
sylvania.
Quirk, Chris, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2004), Barcelona, Spain.
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007), Prague, Czech Republic.
104
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137?144
Manchester, August 2008
Sentence Compression Beyond Word Deletion
Trevor Cohn and Mirella Lapata
School of Informatics
University of Edinburgh
{tcohn,mlap}@inf.ed.ac.uk
Abstract
In this paper we generalise the sen-
tence compression task. Rather than sim-
ply shorten a sentence by deleting words
or constituents, as in previous work, we
rewrite it using additional operations such
as substitution, reordering, and insertion.
We present a new corpus that is suited
to our task and a discriminative tree-to-
tree transduction model that can naturally
account for structural and lexical mis-
matches. The model incorporates a novel
grammar extraction method, uses a lan-
guage model for coherent output, and can
be easily tuned to a wide range of compres-
sion specific loss functions.
1 Introduction
Automatic sentence compression can be broadly
described as the task of creating a grammatical
summary of a single sentence with minimal in-
formation loss. It has recently attracted much at-
tention, in part because of its relevance to appli-
cations. Examples include the generation of sub-
titles from spoken transcripts (Vandeghinste and
Pan, 2004), the display of text on small screens
such as mobile phones or PDAs (Corston-Oliver,
2001), and, notably, summarisation (Jing, 2000;
Lin, 2003).
Most prior work has focused on a specific
instantiation of sentence compression, namely
word deletion. Given an input sentence of
words, w
1
, w
2
. . . w
n
, a compression is formed
by dropping any subset of these words (Knight
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
and Marcu, 2002). The simplification renders the
task computationally feasible, allowing efficient
decoding using a dynamic program (Knight and
Marcu, 2002; Turner and Charniak, 2005; McDon-
ald, 2006). Furthermore, constraining the problem
to word deletion affords substantial modeling flex-
ibility. Indeed, a variety of models have been suc-
cessfully developed for this task ranging from in-
stantiations of the noisy-channel model (Knight
and Marcu, 2002; Galley and McKeown, 2007;
Turner and Charniak, 2005), to large-margin learn-
ing (McDonald, 2006; Cohn and Lapata, 2007),
and Integer Linear Programming (Clarke, 2008).
However, the simplification also renders the task
somewhat artificial. There are many rewrite opera-
tions that could compress a sentence, besides dele-
tion, including reordering, substitution, and inser-
tion. In fact, professional abstractors tend to use
these operations to transform selected sentences
from an article into the corresponding summary
sentences (Jing, 2000).
Therefore, in this paper we consider sentence
compression from a more general perspective and
generate abstracts rather than extracts. In this
framework, the goal is to find a summary of the
original sentence which is grammatical and con-
veys the most important information without nec-
essarily using the same words in the same or-
der. Our task is related to, but different from,
paraphrase extraction (Barzilay, 2003). We must
not only have access to paraphrases (i.e., rewrite
rules), but also be able to combine them in order to
generate new text, while attempting to produce a
shorter resulting string. Quirk et al (2004) present
an end-to-end paraphrasing system inspired by
phrase-based machine translation that can both ac-
quire paraphrases and use them to generate new
strings. However, their model is limited to lexical
substitution ? no reordering takes place ? and is
137
lacking the compression objective.
Once we move away from extractive compres-
sion we are faced with two problems. First, we
must find an appropriate training set for our ab-
stractive task. Compression corpora are not natu-
rally available and existing paraphrase corpora do
not normally contain compressions. Our second
problem concerns the modeling task itself. Ideally,
our learning framework should handle structural
mismatches and complex rewriting operations.
In what follows, we first present a new cor-
pus for abstractive compression which we created
by having annotators compress sentences while
rewriting them. Besides obtaining useful data for
modeling purposes, we also demonstrate that ab-
stractive compression is a meaningful task. We
then present a tree-to-tree transducer capable of
transforming an input parse tree into a compressed
parse tree. Our approach is based on synchronous
tree substitution grammar (STSG, Eisner (2003)),
a formalism that can account for structural mis-
matches, and is trained discriminatively. Specifi-
cally, we generalise the model of Cohn and Lapata
(2007) to our abstractive task. We present a novel
tree-to-tree grammar extraction method which ac-
quires paraphrases from bilingual corpora and en-
sure coherent output by including a ngram lan-
guage model as a feature. We also develop a num-
ber of loss functions suited to the abstractive com-
pression task. We hope that some of the work de-
scribed here might be of relevance to other gen-
eration tasks such as machine translation (Eisner,
2003), multi-document summarisation (Barzilay,
2003), and text simplification (Carroll et al, 1999).
2 Abstractive Compression Corpus
A stumbling block to studying abstractive sentence
compression is the lack of widely available corpora
for training and testing. Previous work has been
conducted almost exclusively on Ziff-Davis, a cor-
pus derived automatically from document abstract
pairs (Knight and Marcu, 2002), or on human-
authored corpora (Clarke, 2008). Unfortunately,
none of these data sources are suited to our prob-
lem since they have been produced with a sin-
gle rewriting operation, namely word deletion. Al-
though there is a greater supply of paraphrasing
corpora, such as the Multiple-Translation Chinese
(MTC) corpus
1
and theMicrosoft Research (MSR)
Paraphrase Corpus (Quirk et al, 2004), they are
also not ideal, since they have not been created
1
Available by the LDC, Catalog Number LDC2002T01,
ISBN 1-58563-217-1.
with compression in mind. They contain ample
rewriting operations, however they do not explic-
itly target information loss.
For the reasons just described, we created our
own corpus. We collected 30 newspaper articles
(575 sentences) from the British National Corpus
(BNC) and the American News Text corpus, for
which we obtained manual compressions. In or-
der to confirm that the task was feasible, five of
these documents were initially compressed by two
annotators (not the authors). The annotators were
given instructions that explained the task and de-
fined sentence compression with the aid of exam-
ples. They were asked to paraphrase while preserv-
ing the most important information and ensuring
the compressed sentences remained grammatical.
They were encouraged to use any rewriting opera-
tions that seemed appropriate, e.g., to delete words,
add new words, substitute them or reorder them.
Assessing inter-annotator agreement is notori-
ously difficult for paraphrasing tasks (Barzilay,
2003) since there can be many valid outputs for
a given input. Also our task is doubly subjective
in deciding which information to remove from the
sentence and how to rewrite it. In default of an
agreement measure that is well suited to the task
and takes both decisions into account, we assessed
them separately. We first examined whether the an-
notators compressed at a similar level. The com-
pression rate was 56% for one annotator and 54%
for the other.
2
We also assessed whether they
agreed in their rewrites by measuring BLEU (Pap-
ineni et al, 2002). The inter-annotator BLEU score
was 23.79%, compared with the source agreement
BLEU of only 13.22%. Both the compression rate
and BLEU score indicate that the task is well-
defined and the compressions valid. The remain-
ing 25 documents were compressed by a single an-
notator to ensure consistency. All our experiments
used the data from this annotator.
3
Table 1 illustrates some examples from our cor-
pus. As can be seen, some sentences contain a sin-
gle rewrite operation. For instance, a PP is para-
phrased with a genitive (see (1)), a subordinate
clause with a present participle (see (2)), a passive
sentence with an active one (see (3)). However, in
most cases many rewrite decisions take place all
at once. Consider sentence (4). Here, the conjunc-
tion high winds and snowfalls is abbreviated to
2
The term ?compression rate? refers to the percentage of
words retained in the compression.
3
Available from http://homepages.inf.ed.ac.uk/
tcohn/paraphrase.
138
1a. The future of the nation is in your hands.
1b. The nation?s future is in your hands.
2a. As he entered a polling booth in Katutura, he said.
2b. Entering a polling booth in Katutura, he said.
3a. Mr Usta was examined by Dr Raymond Crockett, a
Harley Street physician specialising in kidney disease.
3b. Dr Raymond Crockett, a Harley Street physician, ex-
amined Mr Usta.
4a. High winds and snowfalls have, however, grounded
at a lower level the powerful US Navy Sea Stallion
helicopters used to transport the slabs.
4b. Bad weather, however, has grounded the helicopters
transporting the slabs.
5a. To experts in international law and relations, the US
action demonstrates a breach by a major power of in-
ternational conventions.
5b. Experts say the US are in breach of international con-
ventions.
Table 1: Compression examples from our corpus; (a) sen-
tences are the source, (b) sentences the target.
bad weather and the infinitive clause to transport
to the present participle transporting. Note that the
prenominal modifiers US Navy Sea Stallion and
the verb used have been removed. In sentence (5),
the verb say is added and the NP a breach by a
major power of international conventions is para-
phrased by the sentence the US are in breach of
international conventions.
3 Basic Model
Our work builds on the model developed by Cohn
and Lapata (2007). They formulate sentence com-
pression as a tree-to-tree rewriting task. A syn-
chronous tree substitution grammar (STSG, Eisner
(2003)) licenses the space of all possible rewrites.
Each grammar rule is assigned a weight, and
these weights are learnt in discriminative training.
For prediction, a specialised generation algorithm
finds the best scoring compression using the gram-
mar rules. Cohn and Lapata apply this model to ex-
tractive compression with state-of-the-art results.
This model is appealing for our task for several
reasons. Firstly, the synchronous grammar pro-
vides expressive power to model consistent syn-
tactic effects such as reordering, changes in non-
terminal categories and lexical substitution. Sec-
ondly, it is discriminatively trained, which allows
for the incorporation of all manner of powerful fea-
tures. Thirdly, the learning framework can be tai-
lored to the task by choosing an appropriate loss
function. In the following we describe their model
in more detail with emphasis on the synchronous
grammar, the model structure, and the prediction
and training algorithms. Section 4 presents our ex-
tensions and modifications.
Grammar The grammar defines a space of
tree pairs over uncompressed and compressed sen-
Grammar rules:
?S, S? ? ?NP
1
VBD
2
NP
3
, NP
1
VBD
2
NP
3
?
?S, S? ? ?NP
1
VBD
2
NP
3
, NP
3
was VBN
2
by NP
1
?
?NP, NP? ? ?he, him?
?NP, NP? ? ?he, he?
?NP, NP? ? ?he, Peter?
?VBD, VBN? ? ?sang, sung?
?NP, NP? ? ?a song, a song?
Input tree:
[S [NP He
NP
[VP sang
VBD
[NP a
DT
song
NN
]]]
Output trees:
[S [NP He] [VP sang [NP a song]]]
[S [NP Him] [VP sang [NP a song]]]
[S [NP Peter] [VP sang [NP a song]]]
[S [NP A song] [VP was [VP sung [PP by he]]]]
[S [NP A song] [VP was [VP sung [PP by him]]]]
[S [NP A song] [VP was [VP sung [PP by Peter]]]]
Figure 1: Example grammar and the output trees it licences
for an input tree. The numbered boxes in the rules denote
linked variables. Pre-terminal categories are not shown for the
output trees for the sake of brevity.
tences, which we refer to henceforth as the source
and target. We use the grammar to find the set of
sister target sentences for a given source sentence.
Figure 1 shows a toy grammar and the set of possi-
ble target (output) trees for the given source (input)
tree. Each output tree is created by applying a se-
ries of grammar rules, where each rule matches a
fragment of the source and creates a fragment of
the target tree. A rule in the grammar consists of
a pair of elementary trees and a mapping between
the variables (frontier non-terminals) in both trees.
A derivation is a sequence of rules yielding a target
tree with no remaining variables.
Cohn and Lapata (2007) extract a STSG from
a parsed, word-aligned corpus of source and tar-
get sentences. Specifically, they extract the mini-
mal set of synchronous rules which can describe
each tree pair. These rules are minimal in the sense
that they cannot be made smaller (e.g., by replac-
ing a subtree with a variable) while still honouring
the word-alignment.
Decoding The grammar allows us to search
for all sister trees for a given tree. The decoder
maximises over this space:
y
?
=argmax
y:S(y)=x
?(y) (1)
where ?(y) =
?
r?y
??(r, S(y)), ?? (2)
Here x is the source (uncompressed) tree, y
is a derivation which produces the source tree,
S(y) = x, and a target tree, T (y),
4
and r is a gram-
mar rule. The ? function scores the derivation and
4
Equation 1 optimises over derivations rather than target
trees to allow tractable inference.
139
is defined in (2) as a linear function over the rules
used. Each rule?s score is an inner product between
its feature vector, ?(r,y
S
), and the model parame-
ters, ?. The feature functions are set by hand, while
the model parameters are learned in training.
The maximisation problem in (1) can be solved
efficiently using a dynamic program. Derivations
will have common sub-structures whenever they
transduce the same source sub-tree into a target
sub-tree. This is captured in a chart, leading to
an efficient bottom-up algorithm. The asymptotic
time complexity of this search is O(SR) where S
is the number of source nodes andR is the number
of rules matching a given node.
Training The model is trained using
SVM
struct
, a large margin method for structured
output problems (Joachims, 2005; Tsochantaridis
et al, 2005). This training method allows the use
of a configurable loss function, ?(y
?
,y), which
measures the extent to which the model?s predic-
tion, y, differs from the reference, y
?
. Central
to training is the search for a derivation which
is both high scoring and has high loss compared
to the gold standard.
5
This requires finding the
maximiser of H(y) in one of:
H
s
= (1? ??(y
?
)??(y), ??)?(y
?
,y)
H
m
= ?(y
?
,y)? ??(y
?
)??(y), ??
(3)
where the subscripts s and m denote slack and
margin rescaling, which are different formulations
of the training problem (see Tsochantaridis et al
(2005) and Taskar et al (2003) for details).
The search for the maximiser of H(y) in (3)
requires the tracking of the loss value. This can
be achieved by extending the decoding algorithm
such that the chart cells also store the loss param-
eters (e.g., for precision, the number of true and
false positives (Joachims, 2005)). Consequently,
this extension leads to a considerably higher time
and space complexity compared to decoding. For
example, with precision loss the time complexity
is O(S
3
R) as each step must consider O(S
2
) pos-
sible loss parameter values.
4 Extensions
In this section we present our extensions of Cohn
and Lapata?s (2007) model. The latter was de-
signed with the simpler extractive compression in
mind and cannot be readily applied to our task.
5
Spurious ambiguity in the grammar means that there are
often many derivations linking the source and target. We fol-
low Cohn and Lapata (2007) by choosing the derivation with
the most rules, which should provide good generalisation.
Grammar It is relatively straightforward to
extract a grammar from our corpus. This grammar
will contain many rules encoding deletions and
structural transformations but there will be many
unobserved paraphrases, no matter how good the
extraction method (recall that our corpus consists
solely of 565 sentences). For this reason, we ex-
tract a grammar from our abstractive corpus in the
manner of Cohn and Lapata (2007) (see Section 5
for details) and augment it with a larger gram-
mar obtained from a parallel bilingual corpus. Cru-
cially, our second grammar will not contain com-
pression rules, just paraphrasing ones. We leave it
to the model to learn which rules serve the com-
pression objective.
Our paraphrase grammar extraction method
uses bilingual pivoting to learn paraphrases over
syntax tree fragments, i.e., STSG rules. Pivoting
treats the paraphrasing problem as a two-stage
translation process. Some English text is translated
to a foreign language, and then translated back into
English (Bannard and Callison-Burch, 2005):
p(e
?
|e) =
?
f
p(e
?
|f)p(f |e) (4)
where p(f |e) is the probability of translating
an English string e into a foreign string f and
p(e
?
|f) the probability of translating the same for-
eign string into some other English string e
?
. We
thus obtain English-English translation probabili-
ties p(e
?
|e) by marginalizing out the foreign text.
Instead of using strings (Bannard and Callison-
Burch, 2005), we use elementary trees on the En-
glish side, resulting in a monolingual STSG. We
obtain the elementary trees and foreign strings us-
ing the GKHM algorithm (Galley et al, 2004).
This takes as input a bilingual word-aligned corpus
with trees on one side, and finds the minimal set
of tree fragments and their corresponding strings
which is consistent with the word alignment. This
process is illustrated in Figure 2 where the aligned
pair on the left gives rise to the rules shown on
the right. Note that the English rules and for-
eign strings shown include variable indices where
they have been generalised. We estimate p(f |e)
and p(e
?
|f) from the set of tree-to-string rules
and then then pivot each tree fragment to produce
STSG rules. Figure 3 illustrates the process for the
[VP does not VP] fragment.
Modeling and Decoding Our grammar is
much larger and noisier than a grammar extracted
solely for deletion-based compression. So, in or-
der to encourage coherence and inform lexical se-
140
SNP VP
VBZ
does
RB
goHe not
ne pasIl va
PRP VP
NP
He
Il
PRP
go
vaVP
VP
VBZ
does
RB
not
ne    pas
VP
S
NP VP
       
1
2
1
1
1 2
Figure 2: Tree-to-string grammar extraction using the GHKM
algorithm, showing the aligned sentence pair and the resulting
rules as tree fragments and their matching strings. The boxed
numbers denote variables.
VP
VBZ
does
RB
not
ne     pas
VP
n ' 
ne
ne peut
...
VP
MD
will
RB
not
VB
VP
VBP
do
RB
not
VB
1
1
1
1
1
1
1
Figure 3: Pivoting the [VP does not VP] fragment.
lection we incorporate a ngram language model
(LM) as a feature. This requires adapting the scor-
ing function, ?, in (2) to allow features over target
ngrams:
?(y) =
?
r?y
??(r, S(y)), ??+
?
m?T (y)
??(m,S(y)), ??
(5)
where m are the ngrams and ? is a new fea-
ture function over these ngrams (we use only one
ngram feature: the trigram log-probability). Sadly,
the scoring function in (5) renders the chart-based
search used for training and decoding intractable.
In order to provide sufficient context to the chart-
based algorithm, we must also store in each chart
cell the n ? 1 target tokens at the left and right
edges of its yield. This is equivalent to using as
our grammar the intersection between the original
grammar and the ngram LM (Chiang, 2007), and
increases the decoding complexity to an infeasible
O(SRL
2(n?1)V
)whereL is the size of the lexicon.
We adopt a popular approach in syntax-inspired
machine translation to address this problem (Chi-
ang, 2007). The idea is to use a beam-search over
the intersection grammar coupled with the cube-
pruning heuristic. The beam limits the number of
items in a given chart cell to a fixed constant, re-
gardless of the number of possible LM contexts
and non-terminal categories. Cube-pruning further
limits the number of items considered for inclu-
sion in the beam, reducing the time complexity
to a more manageable O(SRBV ) where B is the
beam size. We refer the interested reader to Chiang
(2007) for details.
Training The extensions to the model in (5)
also necessitate changes in the training proce-
dure. Recall that training the basic model of Cohn
and Lapata (2007) requires finding the maximiser
of H(y) in (3). Their model uses a chart-based al-
gorithm for this purpose. As in decoding we also
use a beam search for training, thereby avoiding
the exponential time complexity of exact search.
The beam search requires an estimate of the qual-
ity for incomplete derivations. We use the margin
rescaling objective, H
m
in (3), and approximate
the loss using the current (incomplete) loss param-
eter values in each chart cell. We use a wide beam
of 200 unique items or 500 items in total to reduce
the impact of the approximation.
Our loss functions are tailored to the task and
draw inspiration from metrics developed for ex-
tractive compression but also for summarisation
and machine translation. They are based on the
Hamming distance over unordered bags of items.
This measures the number of predicted items that
did not appear in the reference, along with a
penalty for short output:
?
hamming
(y
?
,y) = f+max (l ? (t+ f), 0) (6)
where t and f are the number of true and false
positives, respectively, when comparing the pre-
dicted target, y, with the reference, y
?
, and l is
the length of the reference. The second term pe-
nalises short output, as predicting very little or
nothing would otherwise be unpenalised. We have
three Hamming loss functions over: 1) tokens,
2) ngrams (n ? 3), or 3) CFG productions. These
losses all operate on unordered bags and there-
fore might reward erroneous predictions. For ex-
ample, a permutation of the reference tokens has
zero token-loss. The CFG and ngram losses have
overlapping items which encode a partial order,
and therefore are less affected.
In addition, we developed a fourth loss func-
tion to measure the edit distance between the
model?s prediction and the reference, both as bags-
of-tokens. This measures the number of insertions
and deletions. In contrast to the previous loss func-
tions, this requires the true positive counts to be
clipped to the number of occurrences of each type
in the reference. The edit distance is given by:
?
edit
(y
?
,y) = p+ q ? 2
?
i
min(p
i
, q
i
) (7)
where p and q denote the number of target tokens
in the predicted and reference derivation, respec-
tively, and p
i
and q
i
are the counts for type i.
141
?ADJP,NP? ? ?subject [PP to NP
1
], part [PP of NP
1
]? (T)
?ADVP,RB? ? ?as well, also? (T)
?ADJP,JJ? ? ?too little, insufficient? (P)
?S,S? ? ?S
1
and S
2
, S
2
and S
1
? (P)
?NP,NP? ? ?DT
1
NN
2
, DT
1
NN
2
? (S)
?NP,NP? ? ?DT
1
NN
2
, NN
2
? (S)
Table 2: Sample grammar rules extracted from the training
set (T), pivoted set (P) or generated from the source (S).
5 Experimental Design
In this section we present our experimental set-
up for assessing the performance of our model.
We give details on the corpora and grammars we
used, model parameters and features,
6
the baseline
used for comparison with our approach, and ex-
plain how our system output was evaluated.
Grammar Extraction Our grammar used
rules extracted directly from our compression cor-
pus (the training partition, 480 sentences) and a
bilingual corpus (see Table 2 for examples). The
former corpus was word-aligned using the Berke-
ley aligner (Liang et al, 2006) initialised with
a lexicon of word identity mappings, and parsed
with Bikel?s (2002) parser. From this we extracted
grammar rules following the technique described
in Cohn and Lapata (2007). For the pivot grammar
we use the French-English Europarl v2 which con-
tains approximately 688K sentences. Again, the
corpus was aligned using the Berkeley aligner and
the English side was parsed with Bikel?s parser. We
extracted tree-to-string rules using our implemen-
tation of the GHKM method. To ameliorate the ef-
fects of poor alignments on the grammar, we re-
moved singleton rules before pivoting.
In addition to the two grammars described, we
scanned the source trees in the compression cor-
pus and included STSG rules to copy each CFG
production or delete up to two of its children. This
is illustrated in Table 2 where the last two rules are
derived from the CFG production NP?DT NN in
the source tree. All trees are rooted with a distin-
guished TOP non-terminal which allows the ex-
plicit modelling of sentence spanning sub-trees.
These grammars each had 44,199 (pivot), 7,813
(train) and 22,555 (copy) rules. We took their
union, resulting in 58,281 unique rules and 13,619
unique source elementary trees.
Model Parameters Our model was trained
on 480 sentences, 36 sentences were used for de-
velopment and 59 for testing. We used a variety
of syntax-based, lexical and compression-specific
6
The software and corpus can be downloaded from
http://homepages.inf.ed.ac.uk/tcohn/paraphrase.
For every rule:
origin of rule
for each origin, o: log p
o
(s, t), log p
o
(s|t), log p
o
(t|s)
s
R
, t
R
, s
R
? t
R
s, t, s ? t, s = t
both s and t are pre-terminals and s = t or s 6= t
number of terminals/variables/dropped variables
ordering of variables as numbers/non-terminals
non-terminal sequence of vars identical after reordering
pre-terminal or terminal sequences are identical
number/identity of common/inserted/dropped terminals
source is shorter/longer than target
target is a compression of the source using deletes
For every ngram :
log p(w
i
|w
i?1
i?(n?1)
)
Table 3: The feature set. Rules were drawn from the training
set, bilingual pivoting and directly from the source trees. s and
t are the source and target elementary trees in a rule, the sub-
script
R
references the root non-terminal, w are the terminals
in the target tree.
features (196,419 in total). These are summarised
in Table 3. We also use a trigram language model
trained on the BNC (100 million words) using the
SRI Language Modeling toolkit (Stolcke, 2002),
with modified Kneser-Ney smoothing.
An important parameter in our modeling frame-
work is the choice of loss function. We evaluated
the loss functions presented in Section 4 on the de-
velopment set. We ran our system for each of the
four loss functions and asked two human judges
to rate the output on a scale of 1 to 5. The Ham-
ming loss over tokens performed best with a mean
rating of 3.18, closely followed by the edit dis-
tance (3.17). We chose the former over the latter
as it is less coarsely approximated during search.
Baseline There are no existing models that
can be readily trained on our abstractive com-
pression data. Instead, we use Cohn and Lapata?s
(2007) extractive model as a baseline. The latter
was trained on an extractive compression corpus
drawn from the BNC (Clarke, 2008) and tuned
to provide a similar compression rate to our sys-
tem. Note that their model is a strong baseline:
it performed significantly better than competitive
approaches (McDonald, 2006) across a variety of
compression corpora.
Evaluation Methodology Sentence compres-
sion output is commonly evaluated by eliciting
human judgments. Following Knight and Marcu
(2002), we asked participants to rate the grammati-
cality of the target compressions and howwell they
preserved the most important information from
the source. In both cases they used a five point
rating scale where a high number indicates bet-
ter performance. We randomly selected 30 sen-
tences from the test portion of our corpus. These
142
Models Grammaticality Importance CompR
Extract 3.10
?
2.43
?
82.5
Abstract 3.38
?
2.85
?
? 79.2
Gold 4.51 4.02 58.4
Table 4: Mean ratings on compression output elicited by hu-
mans;
?
: significantly different from the gold standard;
?
: sig-
nificantly different from the baseline.
sentences were compressed automatically by our
model and the baseline. We also included gold
standard compressions. Our materials thus con-
sisted of 90 (30 ? 3) source-target sentences. We
collected ratings from 22 unpaid volunteers, all
self reported native English speakers. Both studies
were conducted over the Internet using a custom
built web interface.
6 Results
Our results are summarised in Table 4, where we
show the mean ratings for our system (Abstract),
the baseline (Extract), and the gold standard. We
first performed an Analysis of Variance (ANOVA)
to examine the effect of different system compres-
sions. The ANOVA revealed a reliable effect on
both grammaticality and importance (significant
over both subjects and items (p < 0.01)).
We next examined in more detail between-
system differences. Post-hoc Tukey tests revealed
that our abstractive model received significantly
higher ratings than the baseline in terms of impor-
tance (? < 0.01). We conjecture that this is due
to the synchronous grammar we employ which
is larger and more expressive than the baseline.
In the extractive case, a word sequence is either
deleted or retained. We may, however, want to re-
tain the meaning of the sequence while rendering
the sentence shorter, and this is precisely what our
model can achieve, e.g., by allowing substitutions.
As far as grammaticality is concerned, our abstrac-
tive model is numerically better than the extrac-
tive baseline but the difference is not statistically
significant. Note that our model has to work a lot
harder than the baseline to preserve grammatical-
ity since we allow arbitrary rewrites which may
lead to agreement or tense mismatches, and selec-
tional preference violations. The scope for errors is
greatly reduced when performing solely deletions.
Finally, both the abstractive and extractive out-
puts are perceived as significantly worse than the
gold standard both in terms of grammaticality
and importance (? < 0.01). This is not surpris-
ing: human-authored compressions are more fluent
and tend to omit genuinely superfluous informa-
tion. This is also mirrored in the compression rates
shown in Table 4. When compressing, humans em-
O: Kurtz came from Missouri, and at the age of 14, hitch-
hiked to Los Angeles seeking top diving coaches.
E: Kurtz came from Missouri, and at 14, hitch-hiked to Los
Angeles seeking top diving coaches.
A: Kurtz hitch-hiked to Los Angeles seeking top diving
coaches.
G: Kurtz came from Missouri, and at 14, hitch-hiked to Los
Angeles seeking diving coaches.
O: The scheme was intended for people of poor or moderate
means.
E: The scheme was intended for people of poor means.
A: The scheme was planned for poor people.
G: The scheme was intended for the poor.
O: He died last Thursday at his home from complications
following a fall, said his wife author Margo Kurtz.
E: He died last at his home from complications following a
fall, said wife, author Margo Kurtz.
A: His wife author Margo Kurtz died from complications
after a decline.
G: He died from complications following a fall.
O: But a month ago, she returned to Britain, taking the chil-
dren with her.
E: She returned to Britain, taking the children.
A: But she took the children with him.
G: But she returned to Britain with the children.
Table 5: Compression examples including human and system
output (O: original sentence, E: Extractive model, A: Abstrac-
tive model, G: gold standard)
ploy not only linguistic but also world knowledge
which is not accessible to our model. Although the
system can be forced to match the human compres-
sion rate, the grammaticality and information con-
tent both suffer. More sophisticated features could
allow the system to narrow this gap.
We next examined the output of our system in
more detail by recording the number of substitu-
tions, deletions and insertions it performed on the
test data. Deletions accounted for 67% of rewrite
operations, substitutions for 27%, and insertions
for 6%. Interestingly, we observe a similar ratio
in the human compressions. Here, deletions are
also the most common rewrite operation (69%) fol-
lowed by substitutions (24%), and insertions (7%).
The ability to perform substitutions and insertions
increases the compression potential of our system,
but can also result in drastic meaning changes. In
most cases (63%) the compressions produced by
our system did not distort the meaning of the orig-
inal. Humans are clearly better at this, 96.5% of
their compressions were meaning preserving.
We illustrate example output of our system in
Table 5. For comparison we also present the gold
standard compressions and baseline output. In the
first sentence the system rendered Kurtz the sub-
ject of hitch-hiked. At the same time it deleted the
verb and its adjunct from the first conjunct (came
from Missouri ) as well as the temporal modi-
fier at the age of 14 from the second conjunct.
The second sentence shows some paraphrasing:
the verb intended is substituted with planned and
143
poor is now modifying people rather than means.
In the third example, our system applies multi-
ple rewrites. It deletes last Thursday at his home,
moves wife author Margo Kurtz to the subject po-
sition, and substitutes fall with decline. Unfortu-
nately, the compressed sentence expresses a rather
different meaning from the original. It is not Margo
Kurtz who died but her husband. Finally, our last
sentence illustrates a counter-intuitive substitution,
the pronoun her is rewritten as him. This is because
they share the French translation lui and thus piv-
oting learns to replace the less common word (in
legal corpora) her with him. This problem could
be addressed by pivoting over multiple bitexts with
different foreign languages.
Possible extensions and improvements to the
current model are many and varied. Firstly, as
hinted at above, the model would benefit from ex-
tensive feature engineering, including source con-
ditioned features and ngram features besides the
LM. A richer grammar would also boost perfor-
mance. This could be found by pivoting over more
bitexts in many foreign languages or making use
of existing or paraphrase corpora. Finally, we plan
to apply the model to other paraphrasing tasks in-
cluding fully abstractive document summarisation
(Daum?e III and Marcu, 2002).
Acknowledgements
The authors acknowledge the support of EPSRC
(grants GR/T04540/01 and GR/T04557/01).
Special thanks to Phil Blunsom, James Clarke and
Miles Osborne for their insightful suggestions.
References
C. Bannard, C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proceedings of
the 43rd ACL, 255?262, Ann Arbor, MI.
R. Barzilay. 2003. Information Fusion for Multi-
Document Summarization: Praphrasing and Gener-
ation. Ph.D. thesis, Columbia University.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of the HLT, 24?27, San Diego, CA.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,
J. Tait. 1999. Simplifying text for language impaired
readers. In Proceedings of the 9th EACL, 269?270,
Bergen, Norway.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
J. Clarke. 2008. Global Inference for Sentence Com-
pression: An Integer Linear Programming Approach.
Ph.D. thesis, University of Edinburgh.
T. Cohn, M. Lapata. 2007. Large margin synchronous
generation and its application to sentence compres-
sion. In Proceedings of the EMNLP/CoNLL, 73?82,
Prague, Czech Republic.
S. Corston-Oliver. 2001. Text Compaction for Dis-
play on Very Small Screens. In Proceedings of the
NAACL Workshop on Automatic Summarization, 89?
98, Pittsburgh, PA.
H. Daum?e III, D. Marcu. 2002. A noisy-channel model
for document compression. In Proceedings of the
40th ACL, 449?456, Philadelphia, PA.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of
the ACL Interactive Poster/Demonstration Sessions,
205?208, Sapporo, Japan.
M. Galley, K. McKeown. 2007. Lexicalized Markov
grammars for sentence compression. In Proceedings
of the NAACL/HLT, 180?187, Rochester, NY.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004.
What?s in a translation rule? In Proceedings of the
HLT/NAACL, 273?280, Boston, MA.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of the ANLP, 310?
315, Seattle, WA.
T. Joachims. 2005. A support vector method for multi-
variate performance measures. In Proceedings of the
22nd ICML, 377?384, Bonn, Germany.
K. Knight, D. Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
P. Liang, B. Taskar, D. Klein. 2006. Alignment by
agreement. In Proceedings of the HLT/NAACL, 104?
111, New York, NY.
C.-Y. Lin. 2003. Improving summarization perfor-
mance by sentence compression ? a pilot study. In
Proceedings of the 6th International Workshop on
Information Retrieval with Asian Languages, 1?8,
Sapporo, Japan.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proceed-
ings of the 11th EACL, 297?304, Trento, Italy.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th ACL,
311?318, Philadelphia, PA.
C. Quirk, C. Brockett, W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation.
In Proceedings of the EMNLP, 142?149, Barcelona,
Spain.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the ICSLP, Den-
ver, CO.
B. Taskar, C. Guestrin, D. Koller. 2003. Max margin
Markov networks. In Proceedings of NIPS 16.
I. Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun.
2005. Large margin methods for structured and in-
terdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
J. Turner, E. Charniak. 2005. Supervised and unsu-
pervised learning for sentence compression. In Pro-
ceedings of 43rd ACL, 290?297, Ann Arbor, MI.
V. Vandeghinste, Y. Pan. 2004. Sentence compression
for automated subtitling: A hybrid approach. In Pro-
ceedings of the ACL Workshop on Text Summariza-
tion, 89?95, Barcelona, Spain.
144
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1?11, Prague, June 2007. c?2007 Association for Computational Linguistics
Modelling Compression with Discourse Constraints
James Clarke and Mirella Lapata
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
jclarke@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Sentence compression holds promise for
many applications ranging from summarisa-
tion to subtitle generation and subtitle gen-
eration. The task is typically performed on
isolated sentences without taking the sur-
rounding context into account, even though
most applications would operate over entire
documents. In this paper we present a dis-
course informed model which is capable of
producing document compressions that are
coherent and informative. Our model is in-
spired by theories of local coherence and
formulated within the framework of Integer
Linear Programming. Experimental results
show significant improvements over a state-
of-the-art discourse agnostic approach.
1 Introduction
The computational treatment of sentence compres-
sion has recently attracted much attention in the
literature. The task can be viewed as producing a
summary of a single sentence that retains the most
important information and remains grammatically
correct (Jing 2000). Sentence compression is com-
monly expressed as a word deletion problem: given
an input sentence of words W = w1,w2, . . . ,wn, the
aim is to produce a compression by removing any
subset of these words (Knight and Marcu 2002).
Sentence compression can potentially benefit
many applications. For example, in summarisation,
a compression mechanism could improve the con-
ciseness of the generated summaries (Jing 2000;
Lin 2003). Sentence compression could be also
used to automatically generate subtitles for tele-
vision programs; the transcripts cannot usually be
used verbatim due to the rate of speech being too
high (Vandeghinste and Pan 2004). Other applica-
tions include compressing text to be displayed on
small screens (Corston-Oliver 2001) such as mobile
phones or PDAs, and producing audio scanning de-
vices for the blind (Grefenstette 1998).
Most work to date has focused on a rather sim-
ple formulation of sentence compression that does
not allow any rewriting operations, besides word re-
moval. Moreover, compression is performed on iso-
lated sentences without taking into account their sur-
rounding context. An advantage of this simple view
is that it renders sentence compression amenable to
a variety of learning paradigms ranging from in-
stantiations of the noisy-channel model (Galley and
McKeown 2007; Knight and Marcu 2002; Turner
and Charniak 2005) to Integer Linear Programming
(Clarke and Lapata 2006a) and large-margin online
learning (McDonald 2006).
In this paper we take a closer look at one of
the simplifications associated with the compression
task, namely that sentence reduction can be realised
in isolation without making use of discourse-level
information. This is clearly not true ? professional
abstracters often rely on contextual cues while creat-
ing summaries (Endres-Niggemeyer 1998). Further-
more, determining what information is important in
a sentence is influenced by a variety of contextual
factors such as the discourse topic, whether the sen-
tence introduces new entities or events that have not
been mentioned before, and the reader?s background
knowledge.
The simplification is also at odds with most appli-
cations of sentence compression which aim to cre-
ate a shorter document rather than a single sentence.
The resulting document must not only be grammat-
1
ical but also coherent if it is to function as a re-
placement for the original. However, this cannot be
guaranteed without knowing how the discourse pro-
gresses from sentence to sentence. To give a simple
example, a contextually aware compression system
could drop a word or phrase from the current sen-
tence, simply because it is not mentioned anywhere
else in the document and is therefore deemed unim-
portant. Or it could decide to retain it for the sake of
topic continuity.
We are interested in creating a compression model
that is appropriate for documents and sentences. To
this end, we assess whether discourse-level informa-
tion is helpful. Our analysis is informed by two pop-
ular models of discourse, Centering Theory (Grosz
et al 1995) and lexical chains (Morris and Hirst
1991). Both approaches model local coherence ?
the way adjacent sentences bind together to form a
larger discourse. Our compression model is an ex-
tension of the integer programming formulation pro-
posed by Clarke and Lapata (2006a). Their approach
is conceptually simple: it consists of a scoring func-
tion coupled with a small number of syntactic and
semantic constraints. Discourse-related information
can be easily incorporated in the form of additional
constraints. We employ our model to perform sen-
tence compression throughout a whole document
(by compressing sentences sequentially) and evalu-
ate whether the resulting text is understandable and
informative using a question-answering task. Our
method yields significant improvements over a dis-
course agnostic state-of-the-art compression model
(McDonald 2006).
2 Related Work
Sentence compression has been extensively stud-
ied across different modelling paradigms and has
received both generative and discriminative formu-
lations. Most generative approaches (Galley and
McKeown 2007; Knight and Marcu 2002; Turner
and Charniak 2005) are instantiations of the noisy-
channel model, whereas discriminative formulations
include decision-tree learning (Knight and Marcu
2002), maximum entropy (Riezler et al 2003),
support vector machines (Nguyen et al 2004),
and large-margin learning (McDonald 2006). These
models are trained on a parallel corpus of long
source sentences and their target compressions. Us-
ing a rich feature set derived from parse trees, the
models learn either which constituents to delete or
which words to place adjacently in the compression
output. Relatively few approaches dispense with the
parallel corpus and generate compressions in an un-
supervised manner using either a scoring function
(Clarke and Lapata 2006a; Hori and Furui 2004) or
compression rules that are approximated from a non-
parallel corpus such as the Penn Treebank (Turner
and Charniak 2005).
Our work differs from previous approaches in two
key respects. First, we present a compression model
that is contextually aware; decisions on whether to
remove or retain a word (or phrase) are informed by
its discourse properties (e.g., whether it introduces a
new topic, whether it is semantically related to the
previous sentence). Second, we apply our compres-
sion model to entire documents rather than isolated
sentences. This is more in the spirit of real-world ap-
plications where the goal is to generate a condensed
and coherent text. Previous work on summarisation
has also utilised discourse information (e.g., Barzi-
lay and Elhadad 1997; Daume? III and Marcu 2002;
Marcu 2000; Teufel and Moens 2002). However, its
application to document compression is novel to our
knowledge.
3 Discourse Representation
Obtaining an appropriate representation of discourse
is the first step towards creating a compression
model that exploits contextual information. In this
work we focus on the role of local coherence as
this is prerequisite for maintaining global coherence.
Ideally, we would like our compressed document to
maintain the discourse flow of the original. For this
reason, we automatically annotate the source docu-
ment with discourse-level information which is sub-
sequently used to inform our compression proce-
dure. We first describe our algorithms for obtaining
discourse annotations and then present our compres-
sion model.
3.1 Centering Theory
Centering Theory (Grosz et al 1995) is an entity-
orientated theory of local coherence and salience.
Although an utterance in discourse may contain sev-
eral entities, it is assumed that a single entity is
salient or ?centered?, thereby representing the cur-
rent focus. One of the main claims underlying cen-
tering is that discourse segments in which succes-
2
sive utterances contain common centers are more
coherent than segments where the center repeatedly
changes.
Each utterance Ui in a discourse segment has a
list of forward-looking centers, C f (Ui) and a unique
backward-looking center, Cb(Ui). C f (Ui) represents
a ranking of the entities invoked by Ui according
to their salience. The Cb of the current utterance
Ui, is the highest-ranked element in C f (Ui?1) that is
also in Ui. The Cb thus links Ui to the previous dis-
course, but it does so locally since Cb(Ui) is chosen
from Ui?1.
Centering Algorithm So far we have presented
centering without explicitly stating how the con-
cepts ?utterance?, ?entities? and ?ranking? are in-
stantiated. A great deal of research has been devoted
into fleshing these out and many different instantia-
tions have been developed in the literature (see Poe-
sio et al 2004 for details). Since our aim is to iden-
tify centers in discourse automatically, our param-
eter choice is driven by two considerations, robust-
ness and ease of computation.
We therefore follow previous work (e.g., Milt-
sakaki and Kukich 2000) in assuming that the unit of
an utterance is the sentence (i.e., a main clause with
accompanying subordinate and adjunct clauses).
This is in line with our compression task which also
operates over sentences. We determine which en-
tities are invoked by a sentence using two meth-
ods. First, we perform named entity identification
and coreference resolution on each document using
LingPipe1, a publicly available system. Named en-
tities and all remaining nouns are added to the C f
list. Entity matching between sentences is required
to determine the Cb of a sentence. This is done using
the named entity?s unique identifier (as provided by
LingPipe) or by the entity?s surface form in the case
of nouns not classified as named entities.
Entities are ranked according to their grammatical
roles; subjects are ranked more highly than objects,
which are in turn ranked higher than other grammat-
ical roles (Grosz et al 1995); ties are broken using
left-to-right ordering of the grammatical roles in the
sentence (Tetreault 2001). We identify grammatical
roles with RASP (Briscoe and Carroll 2002). For-
mally, our centering algorithm is as follows (where
Ui corresponds to sentence i):
1LingPipe can be downloaded from http://www.
alias-i.com/lingpipe/.
1. Extract entities from Ui.
2. Create C f (Ui) by ranking the entities in
Ui according to their grammatical role
(subjects > objects > others).
3. Find the highest ranked entity in C f (Ui?1)
which occurs in C f (Ui), set the entity to
be Cb(Ui).
The above procedure involves several automatic
steps (named entity recognition, coreference reso-
lution, identification of grammatical roles) and will
unavoidably produce some noisy annotations. So,
there is no guarantee that the right Cb will be iden-
tified or that all sentences will be marked with a Cb.
The latter situation also occurs in passages that con-
tain abrupt changes in topic. In such cases, none of
the entities realised in Ui will occur in C f (Ui?1).
Rather than accept that discourse information may
be absent in a sentence, we turn to lexical chains
as an alternative means of capturing topical content
within a document.
3.2 Lexical Chains
Lexical cohesion refers to the degree of semantic re-
latedness observed among lexical items in a docu-
ment. The term was coined by Halliday and Hasan
(1976) who observed that coherent documents tend
to have more related terms or phrases than inco-
herent ones. A number of linguistic devices can be
used to signal cohesion; these range from repeti-
tion, to synonymy, hyponymy and meronymy. Lexi-
cal chains are a representation of lexical cohesion as
sequences of semantically related words (Morris and
Hirst 1991) and provide a useful means for describ-
ing the topic flow in discourse. For instance, a docu-
ment with many different lexical chains will prob-
ably contain several topics. And main topics will
tend to be represented by dense and long chains.
Words participating in such chains are important for
our compression task ? they reveal what the docu-
ment is about ? and in all likelihood should not be
deleted.
Lexical Chains Algorithm Barzilay and Elhadad
(1997) describe a technique for text summarisation
based on lexical chains. Their algorithm uses Word-
Net to build chains of nouns (and noun compounds).
These are ranked heuristically by a score based on
their length and homogeneity. A summary is then
produced by extracting sentences corresponding to
3
strong chains, i.e., chains whose score is two stan-
dard deviations above the average score.
Like Barzilay and Elhadad (1997), we wish to
determine which lexical chains indicate the most
prevalent discourse topics. Our assumption is that
terms belonging to these chains are indicative of the
document?s main focus and should therefore be re-
tained in the compressed output. Barzilay and El-
hadad?s scoring function aims to identify sentences
(for inclusion in a summary) that have a high con-
centration of chain members. In contrast, we are in-
terested in chains that span several sentences. We
thus score chains according to the number of sen-
tences their terms occur in. For example, the chain
{house3, home3, loft3, house5} (where wordi de-
notes word occurring in sentence i) would be given
a score of two as the terms only occur in two sen-
tences. We assume that a chain signals a prevalent
discourse topic if it occurs throughout more sen-
tences than the average chain. The scoring algorithm
is outlined more formally below:
1. Compute the lexical chains for the document.
2. Score(Chain) = Sentences(Chain).
3. Discard chains if Score(Chain) < Avg(Score).
4. Mark terms from the remaining chains as being
the focus of the document.
We use the method of Galley and McKeown (2003)
to compute lexical chains for each document.2 This
is an improved version of Barzilay and Elhadad?s
(1997) original algorithm.
Before compression takes place, all documents
are pre-processed using the centering and lexical
chain algorithms described above. In each sentence
we mark the center Cb(Ui) if one exists. Words (or
phrases) that are present in the current sentence and
function as the center in the next sentence Cb(Ui+1)
are also flagged. Finally, words are marked if they
are part of a prevalent chain. An example of our dis-
course annotation is given in Figure 1.
4 The Compression Model
Our model is an extension of the approach put for-
ward in Clarke and Lapata (2006a). Their work tack-
les sentence compression as an optimisation prob-
lem. Given a long sentence, a compression is formed
by retaining the words that maximise a scoring func-
2The software is available from http://www1.cs.
columbia.edu/?galley/.
Bad



weather dashed hopes of attempts to halt
the




flow1 during what was seen as a lull in
the lava?s momentum. Experts say that even
if the eruption stopped




today2 , the pressure of
lava piled up behind for six




miles3 would
bring debris cascading down on to the


 
town
anyway. Some estimate the volcano is pouring out
one million tons of debris a




day2 , at a




rate1
of 15




ft3 per




second2 , from a fissure that opened
in mid-December.
The Italian Army




yesterday2 detonated 400lb of
dynamite 3,500 feet up Mount Etna?s slopes.
Figure 1: Excerpt of document from our test set with
discourse annotations. Centers are in double boxes;
terms occurring in lexical chains are in oval boxes.
Words with the same subscript are members of the
same chain (e.g., today, day, second, yesterday)
tion. The latter is essentially a language model cou-
pled with a few constraints ensuring that the re-
sulting output is grammatical. The language model
and the constraints are encoded as linear inequal-
ities whose solution is found using Integer Linear
Programming (ILP, Vanderbei 2001; Winston and
Venkataramanan 2003).
We selected this model for several reasons. First
it does not require a parallel corpus and thus can be
ported across domains and text genres, whilst de-
livering state-of-the-art results (see Clarke and La-
pata 2006a for details). Second, discourse-level in-
formation can be easily incorporated by augment-
ing the constraint set. This is not the case for other
approaches (e.g., those based on the noisy channel
model) where compression is modelled by gram-
mar rules indicating which constituents to delete in a
syntactic context. Third, the ILP framework delivers
a globally optimal solution by searching over the en-
tire compression space3 without employing heuris-
tics or approximations during decoding.
We begin by recapping the formulation of Clarke
and Lapata (2006a). Let W = w1,w2, . . . ,wn denote
a sentence for which we wish to generate a com-
pression. A set of binary decision variables repre-
sent whether each word wi should be included in the
3For a sentence of length n, there are 2n compressions.
4
compression or not. Let:
yi =
{
1 if wi is in the compression
0 otherwise ?i ? [1 . . .n]
A trigram language model forms the backbone of
the compression model. The language model is for-
mulated as an integer program with the introduction
of extra decision variables indicating which word
sequences should be retained or dropped from the
compression. Let:
pi =
{
1 if wi starts the compression
0 otherwise ?i ? [1 . . .n]
qi j =
?
?
?
1 if sequence wi,w j ends
the compression ?i ? [1 . . .n?1]
0 otherwise ? j ? [i+ 1 . . .n]
xi jk =
?
?
?
1 if sequence wi,w j,wk ?i ? [1 . . .n?2]
is in the compression ? j ? [i+ 1 . . .n?1]
0 otherwise ?k ? [ j + 1 . . .n]
The objective function is expressed in Equa-
tion (1). It is the sum of all possible trigrams mul-
tiplied by the appropriate decision variable. The ob-
jective function also includes a significance score for
each word multiplied by the decision variable for
that word (see the last summation term in (1)). This
score highlights important content words in a sen-
tence and is defined in Section 4.1.
maxz =
n
?
i=1
pi ?P(wi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k= j+1
xi jk ?P(wk|wi,w j)
+
n?1
?
i=0
n
?
j=i+1
qi j ?P(end|wi,w j)
+
n
?
i=1
yi ? I(wi) (1)
subject to:
yi, pi,qi j,xi jk = 0 or 1 (2)
A set of sequential constraints4 are added to the
problem to only allow results which combine valid
trigrams.
4We have omitted sequential constraints due to space limi-
tations. The full details are given in Clarke and Lapata (2006a).
4.1 Significance Score
The significance score is an attempt at capturing the
gist of a sentence. It gives more weight to content
words that appear in the deepest level of embed-
ding in the syntactic tree representing the source
sentence:
I(wi) =
l
N
? fi log FaFi (3)
The score is computed over a large corpus where wi
is a content word (i.e., a noun or verb), fi and Fi are
the frequencies of wi in the document and corpus
respectively, and Fa is the sum of all content words
in the corpus. l is the number of clause constituents
above wi, and N is the deepest level of embedding.
4.2 Sentential Constraints
The model also contains a small number of
sentence-level constraints. Their aim is to preserve
the meaning and structure of the original sentence
as much as possible. The majority of constraints
revolve around modification and argument struc-
ture and are defined over parse trees or gram-
matical relations. For example, the following con-
straint template disallows the inclusion of modifiers
(e.g., nouns, adjectives) without their head words:
yi ? y j ? 0 (4)
?i, j : w j modifies wi
Other constraints force the presence of modifiers
when the head is retained in the compression. This
way, it is ensured that negation will be preserved in
the compressed output:
yi ? y j = 0 (5)
?i, j : w j modifies wi ? w j = not
Argument structure constraints make sure that
the resulting compression has a canonical argument
structure. For instance a constraint ensures that if a
verb is present in the compression then so are its ar-
guments:
yi ? y j = 0 (6)
?i, j : w j ? subject/object of verb wi
Finally, Clarke and Lapata (2006a) propose one
discourse constraint which forces the system to pre-
serve personal pronouns in the compressed output:
yi = 1 (7)
?i : wi ? personal pronouns
5
4.3 Discourse Constraints
In addition to the constraints described above, our
model includes constraints relating to the centering
and lexical chains representations discussed in Sec-
tion 3. Recall that after some pre-processing, each
sentence is marked with: its own center Cb(Ui), the
center Cb(Ui+1) of the sentence following it and
words that are members of high scoring chains cor-
responding to the document?s focus. We introduce
two new types of constraints based on these addi-
tional knowledge sources.
The first constraint is the centering constraint
which operates over adjacent sentences. It ensures
that the Cb identified in the source sentence is re-
tained in the target compression. If present, the en-
tity realised as the Cb in the following sentence is
also retained:
yi = 1 (8)
?i : wi ? {Cb(Ui),Cb(Ui+1)}
Consider for example the discourse in Figure 1. The
constraints generated from Equation (8) will require
the compression to retain lava in the first two sen-
tences and debris in sentences two and three.
We also add a lexical chain constraint. This ap-
plies only to nouns which are members of prevalent
chains:
yi = 1 (9)
?i : wi ? document focus lexical chain
This constraint is complementary to the centering
constraint; the sentences it applies to do not have to
be adjacent and the entities under consideration are
not restricted to a specific syntactic role (e.g., sub-
ject or object). See for instance the words flow and
rate in Figure 1 which are members of the same
chain (marked with subscript one). According to
constraint (9) both words must be included in the
compressed document.
The constraints just described ensure that the
compressed document will retain the discourse flow
of the original and will preserve terms indicative
of important topics. We argue that these constraints
will additionally benefit sentence-level compres-
sion, as words which are not signalled as discourse
relevant can be dropped.
4.4 Applying the Constraints
Our compression system is given a (sentence sepa-
rated) document as input. The ILP model just pre-
sented is then applied sequentially to all sentences
to generate a compressed version of the original. We
thus create and solve an ILP for every sentence.5 In
the formulation of Clarke and Lapata (2006a) a sig-
nificance score (see Section 4.1) highlights which
nouns and verbs to include in the compression. As
far as nouns are concerned, our discourse constraints
perform a similar task. Thus, when a sentence con-
tains discourse annotations, we are inclined to trust
them more and only calculate the significance score
for verbs.
During development it was observed that apply-
ing all discourse constraints simultaneously (see
Equations (7)?(9)) results in relatively long com-
pressions. To counter this, we employ these con-
straints using a back-off strategy that relies on pro-
gressively less reliable information. Our back-off
model works as follows: if centering information is
present, we apply the appropriate constraints (Equa-
tion (8)). If no centers are present, we back-off to the
lexical chain information using Equation (9), and in
the absence of the latter we back-off to the pronoun
constraint (Equation (7)). Finally, if discourse infor-
mation is entirely absent from the sentence, we de-
fault to the significance score. Sentential constraints
(see Section 4.2) are applied throughout irrespec-
tively of discourse constraints. In our test data (see
Section 5 for details), the centering constraint was
used in 68.6% of the sentences. The model backed
off to lexical chains for 13.7% of the test sentences,
whereas the pronoun constraint was applied in 8.5%.
Finally, the noun and verb significance score was
used on the remaining 9.2%. An example of our sys-
tem?s output for the text in Figure 1 is given in Fig-
ure 2.
5 Experimental Set-up
In this section we present our experimental set-up.
We briefly introduce the model used for compar-
ison with our approach and give details regarding
our compression corpus and parameter estimation.
Finally, we describe our evaluation methodology.
5We use the publicly available lp solve solver (http://
www.geocities.com/lpsolve/).
6
Bad weather dashed hopes to halt the flow during
what was seen as lull in lava?s momentum. Ex-
perts say that even if eruption stopped, the pres-
sure of lava piled would bring debris cascading.
Some estimate volcano is pouring million tons of
debris from fissure opened in mid-December. The
Army yesterday detonated 400lb of dynamite.
Figure 2: System output on excerpt from Figure 1.
Comparison with state-of-the-art An obvious
evaluation experiment would involve comparing
the ILP model without any discourse constraints
against the discourse informed model presented in
this work. Unfortunately, the two models obtain
markedly different compression rates6 which ren-
ders the comparison of their outputs problematic. To
put the comparison on an equal footing, we evalu-
ated our approach against a state-of-the-art model
that achieves a compression rate similar to ours
without taking discourse-level information into ac-
count. McDonald (2006) formalises sentence com-
pression in a discriminative large-margin learning
framework as a classification task: pairs of words
from the source sentence are classified as being ad-
jacent or not in the target compression. A large
number of features are defined over words, parts
of speech, phrase structure trees and dependen-
cies. These are gathered over adjacent words in the
compression and the words in-between which were
dropped.
It is important to note that McDonald (2006) is not
a straw-man system. It achieves highly competitive
performance compared with Knight and Marcu?s
(2002) noisy channel and decision tree models. Due
to its discriminative nature, the model is able to use
a large feature set and to optimise compression ac-
curacy directly. In other words, McDonald?s model
has a head start against our own model which does
not utilise a parallel corpus and has only a few con-
straints. The comparison of the two systems allows
us to investigate whether discourse information is re-
dundant when using a powerful sentence compres-
sion model.
Corpus Previous work on sentence compres-
sion has used almost exclusively the Ziff-Davis,
6The discourse agnostic ILP model has a compression rate
of 81.2%; when discourse constraints are include the rate drops
to 65.4%.
a compression corpus derived automatically from
document-abstract pairs (Knight and Marcu 2002).
Unfortunately, this corpus is not suitable for our
purposes since it consists of isolated sentences. We
thus created a document-based compression corpus
manually. Following Clarke and Lapata (2006a), we
asked annotators to produce compressions for 82
stories (1,629 sentences) from the BNC and the LA
Times Washington Post.7 48 documents (962 sen-
tences) were used for training, 3 for development (63
sentences), and 31 for testing (604 sentences).
Parameter Estimation Our parameters for the
ILP model followed closely Clarke and Lapata
(2006a). We used a language model trained on
25 million tokens from the North American News
corpus. The significance score was based on 25
million tokens from the same corpus. Our re-
implementation of McDonald (2006) used an identi-
cal feature set, and a slightly modified loss function
to encourage compression on our data set.8
Evaluation Previous studies evaluate how well-
formed the automatically derived compressions are
out of context. The target sentences are typi-
cally rated by naive subjects on two dimensions,
grammaticality and importance (Knight and Marcu
2002). Automatic evaluation measures have also
been proposed. Riezler et al (2003) compare the
grammatical relations found in the system output
against those found in a gold standard using F-score
which Clarke and Lapata (2006b) show correlates
reliably with human judgements.
Following previous work, sentence-based com-
pressions were evaluated automatically using F-
score computed over grammatical relations which
we obtained by RASP (Briscoe and Carroll 2002).
Besides individual sentences, our goal was to evalu-
ate the compressed document as whole. Our evalu-
ation methodology was motivated by two questions:
(1) are the documents readable? and (2) how much
key information is preserved between the source
document and its target compression? We assume
here that the compressed document is to function as
a replacement for the original. We can thus measure
the extent to which the compressed version can be
7The corpus is available from http://homepages.inf.
ed.ac.uk/s0460084/data/.
8McDonald?s (2006) results are reported on the Ziff-Davis
corpus.
7
What is posing a threat to the town? (lava)
What hindered attempts to stop the lava flow?
(bad weather)
What did the Army do first to stop the lava flow?
(detonate explosives)
Figure 3: Example questions with answer key.
used to find answers for questions which are derived
from the original and represent its core content.
We therefore employed a question-answering
evaluation paradigm which has been previously used
for summarisation evaluation and text comprehen-
sion (Mani et al 2002; Morris et al 1992). The
overall objective of our Q&A task is to determine
how accurate each document (generated by differ-
ent compression systems) is at answering questions.
For this we require a methodology for constructing
Q&A pairs and for scoring each document.
Two annotators were independently instructed
to create Q&A pairs for the original documents
in the test set. Each annotator read the document
and then drafted no more than ten questions and
answers related to its content. Annotators were
asked to create factual-based questions which re-
quired an unambiguous answer; these were typically
who/what/where/when/how style questions. Anno-
tators then compared and revised their question-
answer pairs to create a common agreed upon set.
Revisions typically involved merging questions, re-
wording and simplifying questions, and in some
cases splitting a question into multiple questions.
Documents for which too few questions were cre-
ated or for which questions or answers were too am-
biguous were removed. This left an evaluation set
of six documents with between five to eight con-
cise questions per document. Some example ques-
tions corresponding to the document from Figure 1
are given in Figure 3; correct answers are shown in
parentheses.
Compressed documents and their accompanying
questions were presented to human subjects who
were asked to provide answers as best they could.
We elicited answers for six documents in three com-
pression conditions: gold standard, using the ILP
discourse model, and McDonald?s (2006) model.
Each participant was also asked to rate the readabil-
ity of the compressed document on a seven point
scale. A Latin Square design prevented participants
Model CompR F-Score
McDonald 60.1% 36.0%?
Discourse ILP 65.4% 39.6%
Gold Standard 70.3% ??
Table 1: Compression results: compression rate and
relation-based F-score; ? sig. diff. from Discourse
ILP (p < 0.05 using the Student t test).
Model Readability Q&A
McDonald 2.6? 53.7%??
Discourse ILP 3.0? 68.3%
Gold Standard 5.5? 80.7%
Table 2: Human Evaluation Results: average read-
ability ratings and average percentage of questions
answered correctly. ?: sig. diff. from Gold Standard;
?: sig. diff. from Discourse ILP.
from seeing two different compressions of the same
document.
The study was conducted remotely over the In-
ternet. Participants were presented with a set of in-
structions that explained the Q&A task and provided
examples. Subjects were first asked to read the com-
pressed document and rate its readability. Questions
were then presented one at a time and participants
were allowed to consult the document for the an-
swer. Once a participant had provided an answer
they were not allowed to modify it. Thirty unpaid
volunteers took part in our Q&A study. All were self
reported native English speakers.
The answers provided by the participants were
scored against the answer key. Answers were con-
sidered correct if they were identical to the answer
key or subsumed by it. For instance, Mount Etna
was considered a right answer to the first question
from Figure 3. A compressed document receives a
full score if subjects have answered all questions re-
lating to it correctly.
6 Results
As a sanity check, we first assessed the compres-
sions produced by our model and McDonald (2006)
on a sentence-by-sentence basis without taking the
documents into account. There is no hope for gener-
ating shorter documents if the compressed sentences
are either too wordy or too ungrammatical. Table 1
shows the compression rates (CompR) for the two
8
systems and evaluates the quality of their output us-
ing F-score based on grammatical relations. As can
be seen, the Discourse ILP compressions are slightly
longer than McDonald (65.4% vs. 60.1%) but closer
to the human gold standard (70.3%). This is not sur-
prising, the Discourse ILP model takes the entire
document into account, and compression decisions
will be slightly more conservative. The Discourse
ILP?s output is significantly better than McDonald in
terms of F-score, indicating that discourse-level in-
formation is generally helpful. Both systems could
use further improvement as inter-annotator agree-
ment on this data yields an F-score of 65.8%.
Let us now consider the results of our document-
based evaluation. Table 2 shows the mean readabil-
ity ratings obtained for each system and the per-
centage of questions answered correctly. We used
an Analysis of Variance (ANOVA) to examine the ef-
fect of compression type (McDonald, Discourse ILP,
Gold Standard). The ANOVA revealed a reliable ef-
fect on both readability and Q&A. Post-hoc Tukey
tests showed that McDonald and the Discourse ILP
model do not differ significantly in terms of read-
ability. However, they are significantly less read-
able than the gold standard (? < 0.05). For the Q&A
task we observe that our system is significantly bet-
ter than McDonald (? < 0.05) and not significantly
worse than the gold standard.
These results indicate that the automatic systems
lag behind the human gold standard in terms of
readability. When reading entire documents, sub-
jects are less tolerant of ungrammatical construc-
tions. We also find out that despite relatively low
readability, the documents are overall understand-
able. The discourse informed model generates more
informative documents ? the number of questions
answered correctly increases by 15% in comparison
to McDonald. This is an encouraging result suggest-
ing that there may be advantages in developing com-
pression models that exploit contextual information.
7 Conclusions and Future Work
In this paper we proposed a novel method for au-
tomatic sentence compression. Central in our ap-
proach is the use of discourse-level information
which we argue is an important prerequisite for doc-
ument (as opposed to sentence) compression. Our
model uses integer programming for inferring glob-
ally optimal compressions in the presence of lin-
guistically motivated constraints. Our discourse con-
straints aim to capture local coherence and are in-
spired by centering theory and lexical chains. We
showed that our model can be successfully em-
ployed to produce compressed documents that pre-
serve most of the original?s core content.
Our approach to document compression differs
from most summarisation work in that our sum-
maries are fairly long. However, we believe this is
the first step into understanding how compression
can help summarisation. In the future, we will in-
terface our compression model with sentence ex-
traction. The discourse annotations can help guide
the extraction method into selecting topically re-
lated sentences which can consequently be com-
pressed together. The compression rate can be tai-
lored through additional constraints which act on
the output length to ensure precise word limits are
obeyed.
We also plan to study the effect of global dis-
course structure (Daume? III and Marcu 2002) on the
compression task. In general, we will assess the im-
pact of discourse information more systematically
by incorporating it into generative and discrimina-
tive modelling paradigms.
Acknowledgements We are grateful to Ryan Mc-
Donald for his help with the re-implementation of
his system and our annotators Vasilis Karaiskos
and Sarah Luger. Thanks to Simone Teufel, Alex
Lascarides, Sebastian Riedel, and Bonnie Web-
ber for insightful comments and suggestions. La-
pata acknowledges the support of EPSRC (grant
GR/T04540/01).
References
Barzilay, R. and M. Elhadad. 1997. Using lexical
chains for text summarization. In Proceedings of
the Intelligent Scalable Text Summarization Work-
shop (ISTS), ACL-97.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceed-
ings of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC?2002).
Las Palmas, Gran Canaria, pages 1499?1504.
Clarke, James and Mirella Lapata. 2006a.
Constraint-based sentence compression: An
integer programming approach. In Proceedings
of the COLING/ACL 2006 Main Conference
9
Poster Sessions. Sydney, Australia, pages
144?151.
Clarke, James and Mirella Lapata. 2006b. Models
for sentence compression: A comparison across
domains, training requirements and evaluation
measures. In Proceedings of the 21st Inter-
national Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association
for Computational Linguistics. Sydney, Australia,
pages 377?384.
Corston-Oliver, Simon. 2001. Text Compaction for
Display on Very Small Screens. In Proceedings of
the NAACL Workshop on Automatic Summariza-
tion. Pittsburgh, PA, pages 89?98.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-
channel model for document compression. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL
2002). Philadelphia, PA, pages 449?456.
Endres-Niggemeyer, Brigitte. 1998. Summarising
Information. Springer, Berlin.
Galley, Michel and Kathleen McKeown. 2003.
Improving word sense disambiguation in lexi-
cal chaining. In Proceedings of 18th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI?03). pages 1486?1488.
Galley, Michel and Kathleen McKeown. 2007. Lex-
icalized markov grammars for sentence compres-
sion. In In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL-HLT?2007). Rochester, NY.
Grefenstette, Gregory. 1998. Producing Intelligent
Telegraphic Text Reduction to Provide an Audio
Scanning Service for the Blind. In Proceedings of
the AAAI Symposium on Intelligent Text Summa-
rization. Stanford, CA, pages 111?117.
Grosz, Barbara J., Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for modeling
the local coherence of discourse. Computational
Linguistics 21(2):203?225.
Halliday, M. A. K. and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman, London.
Hori, Chiori and Sadaoki Furui. 2004. Speech sum-
marization: an approach through word extraction
and a method for evaluation. IEICE Transactions
on Information and Systems E87-D(1):15?25.
Jing, Hongyan. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
6th conference on Applied Natural Language Pro-
cessing (ANLP?2000). Seattle, WA, pages 310?
315.
Knight, Kevin and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artificial
Intelligence 139(1):91?107.
Lin, Chin-Yew. 2003. Improving summarization
performance by sentence compression ? a pilot
study. In Proceedings of the 6th International
Workshop on Information Retrieval with Asian
Languages. Sapporo, Japan, pages 1?8.
Mani, Inderjeet, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
2002. SUMMAC: A text summarization evalua-
tion. Natural Language Engineering 8(1):43?68.
Marcu, Daniel. 2000. The Theory and Practice of
Discourse Parsing and Summarization. The MIT
Press, Cambridge, MA.
McDonald, Ryan. 2006. Discriminative sentence
compression with soft syntactic constraints. In
Proceedings of the 11th EACL. Trento, Italy.
Miltsakaki, Eleni and Karen Kukich. 2000. The
role of centering theory?s rough-shift in the teach-
ing and evaluation of writing skills. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?2000).
pages 408?415.
Morris, A., G. Kasper, and D. Adams. 1992. The
effects and limitations of automated text condens-
ing on reading comprehension performance. In-
formation Systems Research 3(1):17?35.
Morris, Jane and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indi-
cator of the structure of text. Computational Lin-
guistics 17(1):21?48.
Nguyen, Minh Le, Akira Shimazu, Susumu
Horiguchi, Tu Bao Ho, and Masaru Fukushi.
2004. Probabilistic sentence reduction using
support vector machines. In Proceedings of
the 20th COLING. Geneva, Switzerland, pages
743?749.
Poesio, Massimo, Rosemary Stevenson, Barbara Di
Eugenio, and Janet Hitzeman. 2004. Centering: a
10
parametric theory and its instantiations. Compu-
tational Linguistics 30(3):309?363.
Riezler, Stefan, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence con-
densation using ambiguity packing and stochas-
tic disambiguation methods for lexical-functional
grammar. In Proceedings of the HLT/NAACL. Ed-
monton, Canada, pages 118?125.
Tetreault, Joel R. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics 27(4):507?520.
Teufel, Simone and Marc Moens. 2002. Summa-
rizing scientific articles ? experiments with rele-
vance and rhetorical status. Computational Lin-
guistics 28(4):409?446.
Turner, Jenine and Eugene Charniak. 2005. Su-
pervised and unsupervised learning for sentence
compression. In Proceedings of the 43rd ACL.
Ann Arbor, MI, pages 290?297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence
compression for automated subtitling: A hybrid
approach. In Proceedings of the ACL Workshop
on Text Summarization. Barcelona, Spain, pages
89?95.
Vanderbei, Robert J. 2001. Linear Programming:
Foundations and Extensions. Kluwer Academic
Publishers, Boston, 2nd edition.
Winston, Wayne L. and Munirpallam Venkatara-
manan. 2003. Introduction to Mathematical Pro-
gramming. Brooks/Cole.
11
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 12?21, Prague, June 2007. c?2007 Association for Computational Linguistics
Using Semantic Roles to Improve Question Answering
Dan Shen
Spoken Language Systems
Saarland University
Saarbruecken, Germany
dan@lsv.uni-saarland.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Shallow semantic parsing, the automatic
identification and labeling of sentential con-
stituents, has recently received much atten-
tion. Our work examines whether seman-
tic role information is beneficial to question
answering. We introduce a general frame-
work for answer extraction which exploits
semantic role annotations in the FrameNet
paradigm. We view semantic role assign-
ment as an optimization problem in a bipar-
tite graph and answer extraction as an in-
stance of graph matching. Experimental re-
sults on the TREC datasets demonstrate im-
provements over state-of-the-art models.
1 Introduction
Recent years have witnessed significant progress in
developing methods for the automatic identification
and labeling of semantic roles conveyed by senten-
tial constituents.1 The success of these methods, of-
ten referred to collectively as shallow semantic pars-
ing (Gildea and Jurafsky, 2002), is largely due to the
availability of resources like FrameNet (Fillmore et
al., 2003) and PropBank (Palmer et al, 2005), which
document the surface realization of semantic roles in
real world corpora.
More concretely, in the FrameNet paradigm, the
meaning of predicates (usually verbs, nouns, or ad-
jectives) is conveyed by frames, schematic repre-
sentations of situations. Semantic roles (or frame
1The approaches are too numerous to list; we refer the inter-
ested reader to Carreras and Ma`rquez (2005) for an overview.
elements) are defined for each frame and corre-
spond to salient entities present in the evoked situ-
ation. Predicates with similar semantics instantiate
the same frame and are attested with the same roles.
The FrameNet database lists the surface syntactic
realizations of semantic roles, and provides anno-
tated example sentences from the British National
Corpus. For example, the frame Commerce Sell has
three core semantic roles, namely Buyer, Goods, and
Seller ? each expressed by an indirect object, a di-
rect object, and a subject (see sentences (1a)?(1c)).
It can also be attested with non-core (peripheral)
roles (e.g., Means, Manner, see (1d) and (1e)) that
are more generic and can be instantiated in sev-
eral frames, besides Commerce Sell. The verbs sell,
vend, and retail can evoke this frame, but also the
nouns sale and vendor.
(1) a. [Lee]Seller sold a textbook [to
Abby]Buyer.
b. [Kim]Seller sold [the sweater]Goods.
c. [My company]Seller has sold [more
than three million copies]Goods.
d. [Abby]Seller sold [the car]Goods [for
cash]Means.
e. [He]Seller [reluctanctly]Manner sold
[his rock]Goods.
By abstracting over surface syntactic configura-
tions, semantic roles offer an important first step to-
wards deeper text understanding and hold promise
for a range of applications requiring broad cover-
age semantic processing. Question answering (QA)
is often cited as an obvious beneficiary of semantic
12
role labeling (Gildea and Jurafsky, 2002; Palmer et
al., 2005; Narayanan and Harabagiu, 2004). Faced
with the question Q: What year did the U.S. buy
Alaska? and the retrieved sentence S: . . .before Rus-
sia sold Alaska to the United States in 1867, a hypo-
thetical QA system must identify that United States
is the Buyer despite the fact that it is attested in one
instance as a subject and in another as an object.
Once this information is known, isolating the correct
answer (i.e., 1867 ) can be relatively straightforward.
Although conventional wisdom has it that seman-
tic role labeling ought to improve answer extraction,
surprising little work has been done to this effect
(see Section 2 for details) and initial results have
been mostly inconclusive or negative (Sun et al,
2005; Kaisser, 2006). There are at least two good
reasons for these findings. First, shallow semantic
parsers trained on declarative sentences will typi-
cally have poor performance on questions and gen-
erally on out-of-domain data. Second, existing re-
sources do not have exhaustive coverage and recall
will be compromised, especially if the question an-
swering system is expected to retrieve answers from
unrestricted text. Since FrameNet is still under de-
velopment, its coverage tends to be more of a prob-
lem in comparison to other semantic role resources
such as PropBank.
In this paper we propose an answer extraction
model which effectively incorporates FrameNet-
style semantic role information. We present an auto-
matic method for semantic role assignment which is
conceptually simple and does not require extensive
feature engineering. A key feature of our approach
is the comparison of dependency relation paths at-
tested in the FrameNet annotations and raw text. We
formalize the search for an optimal role assignment
as an optimization problem in a bipartite graph. This
formalization allows us to find an exact, globally op-
timal solution. The graph-theoretic framework goes
some way towards addressing coverage problems re-
lated with FrameNet and allows us to formulate an-
swer extraction as a graph matching problem. As a
byproduct of our main investigation we also exam-
ine the issue of FrameNet coverage and show how
much it impacts performance in a TREC-style ques-
tion answering setting.
In the following section we provide an overview
of existing work on question answering systems that
exploit semantic role-based lexical resources. Then
we define our learning task and introduce our ap-
proach to semantic role assignment and answer ex-
traction in the context of QA. Next, we present our
experimental framework and data. We conclude the
paper by presenting and discussing our results.
2 Related Work
Question answering systems have traditionally de-
pended on a variety of lexical resources to bridge
surface differences between questions and potential
answers. WordNet (Fellbaum, 1998) is perhaps the
most popular resource and has been employed in
a variety of QA-related tasks ranging from query
expansion, to axiom-based reasoning (Moldovan et
al., 2003), passage scoring (Paranjpe et al, 2003),
and answer filtering (Leidner et al, 2004). Besides
WordNet, recent QA systems increasingly rely on
syntactic information as a means of abstracting over
word order differences and structural alternations
(e.g., passive vs. active voice). Most syntax-based
QA systems (Wu et al, 2005) incorporate some
means of comparison between the tree representing
the question with the subtree surrounding the answer
candidate. The assumption here is that appropriate
answers are more likely to have syntactic relations
in common with their corresponding question. Syn-
tactic structure matching has been applied to pas-
sage retrieval (Cui et al, 2005) and answer extrac-
tion (Shen and Klakow, 2006).
Narayanan and Harabagiu (2004) were the first
to stress the importance of semantic roles in an-
swering complex questions. Their system identifies
predicate argument structures by merging semantic
role information from PropBank and FrameNet. Ex-
pected answers are extracted by performing proba-
bilistic inference over the predicate argument struc-
tures in conjunction with a domain specific topic
model. Sun et al (2005) incorporate semantic analy-
sis in their TREC05 QA system. They use ASSERT
(Pradhan et al, 2004), a publicly available shallow
semantic parser trained on PropBank, to generate
predicate-argument structures which subsequently
form the basis of comparison between question and
answer sentences. They find that semantic analysis
does not boost performance due to the low recall
of the semantic parser. Kaisser (2006) proposes a
13
SemStruc ac1SemStruc ac2
SemStruc aci
SemStruc q
Sent. Model I
Q Model I
Model II Answer
Figure 1: Architecture of answer extraction
question paraphrasing method based on FrameNet.
Questions are assigned semantic roles by matching
their dependency relations with those attested in the
FrameNet annotations. The assignments are used to
create question reformulations which are submitted
to Google for answer extraction. The semantic role
assignment module is not probabilistic, it relies on
strict matching, and runs into severe coverage prob-
lems.
In line with previous work, our method exploits
syntactic information in the form of dependency re-
lation paths together with FrameNet-like semantic
roles to smooth lexical and syntactic divergences be-
tween question and answer sentences. Our approach
is less domain dependent and resource intensive than
Narayanan and Harabagiu (2004), it solely employs
a dependency parser and the FrameNet database. In
contrast to Kaisser (2006), we model the semantic
role assignment and answer extraction tasks numer-
ically, thereby alleviating the coverage problems en-
countered previously.
3 Problem Formulation
We briefly summarize the architecture of the QA
system we are working with before formalizing the
mechanics of our FrameNet-based answer extraction
module. In common with previous work, our over-
all approach consists of three stages: (a) determining
the expected answer type of the question, (b) retriev-
ing passages likely to contain answers to the ques-
tion, and (c) performing a match between the ques-
tion words and retrieved passages in order to extract
the answer. In this paper we focus on the last stage:
question and answer sentences are normalized to a
FrameNet-style representation and answers are re-
trieved by selecting the candidate whose semantic
structure is most similar to the question.
The architecture of our answer extraction mod-
ule is shown in Figure 1. Semantic structures for
questions and sentences are automatically derived
using the model described in Section 4 (Model I). A
semantic structure SemStruc = ?p,Set(SRA)? con-
sists of a predicate p and a set of semantic role as-
signments Set(SRA). p is a word or phrase evok-
ing a frame F of FrameNet. A semantic role assign-
ment SRA is a ternary structure ?w,SR,s?, consist-
ing of frame element w, its semantic role SR, and
score s indicating to what degree SR qualifies as a
label for w.
For a question q, we generate a semantic struc-
ture SemStrucq. Question words, such as what, who,
when, etc., are considered expected answer phrases
(EAPs). We require that EAPs are frame elements
of SemStrucq. Likely answer candidates are ex-
tracted from answer sentences following some pre-
processing steps detailed in Section 6. For each
candidate ac, we derive its semantic structure
SemStrucac and assume that ac is a frame ele-
ment of SemStrucac. Question and answer seman-
tic structures are compared using a model based on
graph matching detailed in Section 5 (Model II).
We calculate the similarity of all derived pairs
?SemStrucq,SemStrucac? and select the candidate
with the highest value as an answer for the question.
4 Semantic Structure Generation
Our method crucially exploits the annotated sen-
tences in the FrameNet database together with the
output of a dependency parser. Our guiding assump-
tion is that sentences that share dependency rela-
tions will also share semantic roles as long as they
evoke the same or related frames. This is motivated
by much research in lexical semantics (e.g., Levin
(1993)) hypothesizing that the behavior of words,
particularly with respect to the expression and in-
terpretation of their arguments, is to a large ex-
tent determined by their meaning. We first describe
how predicates are identified and then introduce our
model for semantic role labeling.
Predicate Identification Predicate candidates are
identified using a simple look-up procedure which
compares POS-tagged tokens against FrameNet en-
tries. For efficiency reasons, we make the simplify-
ing assumption that questions have only one predi-
cate which we select heuristically: (1) verbs are pre-
14
ferred to other parts of speech, (2) if there is more
than one verb in the question, preference is given to
the verb with the highest level of embedding in the
dependency tree, (3) if no verbs are present, a noun
is chosen. For example, in Q: Who beat Floyd Pat-
terson to take the title away?, beat, take away, and
title are identified as predicate candidates and beat
is selected the main predicate of the question. For
answer sentences, we require that the predicate is ei-
ther identical or semantically related to the question
predicate (see Section 5).
In the example given above, the predicate beat
evoques a single frame (i.e., Cause harm). However,
predicates often have multiple meanings thus evo-
quing more than one frame. Knowing which is the
appropriate frame for a given predicate impacts the
semantic role assignment task; selecting the wrong
frame will unavoidably result in erroneous semantic
roles. Rather than disambiguiting polysemous pred-
icates prior to semantic role assignment, we perform
the assignment for each frame evoqued by the pred-
icate.
Semantic Role Assignment Before describing
our approach to semantic role labeling we define
dependency relation paths. A relation path R is a
relation sequence ?r1,r2, ...,rL?, in which rl (l =
1,2, ...,L) is one of predefined dependency relations
with suffix of traverse direction. An example of a
relation path is R = ?sub jU ,ob jD?, where the sub-
scripts U and D indicate upward and downward
movement in trees, respectively. Given an unanno-
tated sentence whose roles we wish to label, we as-
sume that words or phrases w with a dependency
path connecting them to p are frame elements. Each
frame element is represented by an unlabeled depen-
dency path Rw which we extract by traversing the
dependency tree from w to p. Analogously, we ex-
tract from the FrameNet annotations all dependency
paths RSR that are labeled with semantic role infor-
mation and correspond to p. We next measure the
compatibility of labeled and unlabeled paths as fol-
lows:
s(w,SR) =
maxRSR?M [sim(Rw,RSR) ?P(RSR)]
(2)
where M is the set of dependency relation paths
for SR in FrameNet, sim(Rw,RSR) the similarity be-
tween paths Rw and RSR weighted by the relative
w SRw SR
(a) (b)
Figure 2: Sample original bipartite graph (a) and its
subgraph with edge covers (b). In each graph, the
left partition represents frame elements and the right
partition semantic roles.
frequency of RSR in FrameNet (P(RSR)). We con-
sider both core and non-core semantic roles instan-
tiated by frames with at least one annotation in
FrameNet. Core roles tend to have more annotations
in Framenet and consequently are considered more
probable.
We measure sim(Rw,RSR), by adapting a string
kernel to our task. Our hypothesis is that the more
common substrings two dependency paths have,
the more similar they are. The string kernel we
used is similar to Leslie (2002) and defined as
the sum of weighted common dependency rela-
tion subsequences between Rw and RSR. For effi-
ciency, we consider only unigram and bigram sub-
sequences. Subsequences are weighted by a metric
akin to t f ? id f which measures the degree of asso-
ciation between a candidate SR and the dependency
relation r present in the subsequence:
weightSR(r) = fr ? log
(
1+ Nnr
)
(3)
where fr is the frequency of r occurring in SR; N is
the total number of SRs evoked by a given frame;
and nr is the number of SRs containing r.
For each frame element we thus generate a set
of semantic role assignments Set(SRA). This initial
assignment can be usefully represented as a com-
plete bipartite graph in which each frame element
(word or phrase) is connected to the semantic roles
licensed by the predicate and vice versa. (see Fig-
ure 2a). Edges are weighted and represent how com-
patible the frame elements and semantic roles are
(see equation (2)). Now, for each frame element w
15
Q: Who discovered prions?S: 1997: Stanley B. Prusiner, United States, discovery of prions, ...
SemStruc q
p: discover
Original SR assignments: 
Optimized SR assignments: 
0.06 Cognizer
Phenomenon
Ground
State
Evidence
EAP
prions 
0000
0.01
0.1
0.05
0.05
0.02
0.06 Cognizer
Phenomenon
Ground
State
Evidence
EAP
prions 
0.1
0.05
0.05
0.02
SemStruc ac (ac: Stanley B. Prusiner)
p: discovery
Original SR assignments: 
Optimized SR assignments: 
0.25 Cognizer
Phenomenon
Topic
Evidence
ac
prions 
0.15
0.2
0.16
0.25 Cognizer
Phenomenon
Topic
Evidence
ac
prions 
0.15
0.2
0.16
0.120.07
0 0
Figure 3: Semantic structures induced by our model
for a question and answer sentence
we could simply select the semantic role with the
highest score. However, this decision procedure is
local, i.e., it yields a semantic role assignment for
each frame element independently of all other ele-
ments. We therefore may end up with the same role
being assigned to two frame elements or with frame
elements having no role at all. We remedy this short-
coming by treating the semantic role assignment as
a global optimization problem.
Specifically, we model the interaction between all
pairwise labeling decisions as a minimum weight
bipartite edge cover problem (Eiter and Mannila,
1997; Cormen et al, 1990). An edge cover is a sub-
graph of a bipartite graph so that each node is linked
to at least one node of the other partition. This yields
a semantic role assignment for all frame elements
(see Figure 2b where frame elements and roles are
adjacent to an edge). Edge covers have been success-
fully applied in several natural language processing
tasks, including machine translation (Taskar et al,
2005) and annotation projection (Pado? and Lapata,
2006).
Formally, optimal edge cover assignments are so-
lutions of following optimization problem:
max
E is edge cover ?(ndw,ndSR)?E s(nd
w,ndSR)(4)
where, s(ndw,ndSR) is the compatibility score be-
tween the frame element node ndw and semantic role
node ndSR. Edge covers can be computed efficiently
in cubic time using algorithms for the equivalent
linear assignment problem. Our experiments used
Jonker and Volgenant?s (1987) solver.2
Figure 3 shows the semantic role assignments
generated by our model for the question Q: Who
discovered prions? and the candidate answer sen-
tence S: 1997: Stanley B. Prusiner, United States,
discovery of prions. . . Here we identify two predi-
cates, namely discover and discovery. The expected
answer phrase (EAP) who and the answer candi-
date Stanley B. Prusiner are assigned the COGNIZER
role. Note that frame elements can bear multiple se-
mantic roles. By inducing a soft labeling we hope to
render the matching of questions and answers more
robust, thereby addressing to some extent the cover-
age problems associated with FrameNet.
5 Semantic Structure Matching
We measure the similarity between a question and
its candidate answer by matching their predicates
and semantic role assignments. Since SRs are frame-
specific, we prioritize frame matching to SR match-
ing. Two predicates match if they evoke the same
frame or one of its hypernyms (or hyponyms). The
latter are expressed by the Inherits From and Is In-
herited By relations in the frame definitions. If the
predicates match, we examine whether the assigned
semantic roles match. Since we represent SR assign-
ments as graphs with edge covers, we can also for-
malize SR matching as a graph matching problem.
The similarity between two graphs is measured
as the sum of similarities between their subgraphs.
We first decompose a graph into subgraphs consist-
ing of one frame element node w and a set of SR
nodes connected to it. The similarity between two
subgraphs SubG1, and SubG2 is then formalized as:
(5) Sim(SubG1,SubG2) =
?
ndSR1 ? SubG1
ndSR2 ? SubG2
ndSR1 = ndSR2
1
|s(ndw,ndSR1 )? s(ndw,ndSR2 )|+1
where, ndSR1 and ndSR2 are semantic role nodes con-
nected to a frame element node ndw in SubG1 and
2The software is available from http://www.magiclogic.
com/assignment.html .
16
1757[11, 20]
2117[21, 50]
439[51, 100] 40[101, INF)
33800
1175[1, 5]1287[6, 10]
Figure 4: Distribution of Numbers of Predicates and
annotated sentences; each sub-pie, lists the number
of predicates (above) with their corresponding range
of annotated sentences (below)
SubG2, respectively. s(ndw,ndsr1 ) and s(ndw,ndSR2 )
are edge weights between two nodes in correspond-
ing subgraphs (see (2)). Our intuition here is that
the more semantic roles two subgraphs share for a
given frame element, the more similar they are and
the closer their corresponding edge weights should
be. Edge weights are normalized by dividing by the
sum of all edges in a subgraph.
6 Experimental Setup
Data All our experiments were performed on the
TREC02?05 factoid questions. We excluded NIL
questions since TREC doesn?t supply an answer for
them. We used the FrameNet V1.3 lexical database.
It contains 10,195 predicates grouped into 795 se-
mantic frames and 141,238 annotated sentences.
Figure 4 shows the number of annotated sentences
available for different predicates. As can be seen,
there are 3,380 predicates with no annotated sen-
tences and 1,175 predicates with less than 5 anno-
tated sentences. All FrameNet sentences, questions,
and answer sentences were parsed using MiniPar
(Lin, 1994), a robust dependency parser.
As mentioned in Section 4 we extract depen-
dency relation paths by traversing the dependency
tree from the frame element node to the predicate
node. We used all dependency relations provided
by MiniPar (42 in total). In order to increase cov-
erage, we combine all relation paths for predicates
that evoke the same frame and are labeled with the
same POS tag. For example, found and establish
are both instances of the frame Intentionally create
but the database does not have any annotated sen-
tences for found.v. In default of not assigning any
role labels for found.v, our model employs the rela-
tion paths for the semantically related establish.v.
Preprocessing Here we summarize the steps of
our QA system preceding the assignment of seman-
tic structure and answer extraction. For each ques-
tion, we recognize its expected answer type (e.g., in
Q: Which record company is Fred Durst with? we
would expect the answer to be an ORGANIZA-
TION ). Answer types are determined using classi-
fication rules similar to Li and Roth (2002). We also
reformulate questions into declarative sentences fol-
lowing the strategy proposed in Brill et al (2002).
The reformulated sentences are submitted as
queries to an IR engine for retrieving sentences with
relevant answers. Specifically, we use the Lemur
Toolkit3, a state-of-the-art language model-driven
search engine. We work only with the 50 top-ranked
sentences as this setting performed best in previ-
ous experiments of our QA system. We also add to
Lemur?s output gold standard sentences, which con-
tain and support an answer for each question. Specif-
ically, documents relevant for each question are re-
trieved from the AQUAINT Corpus4 according to
TREC supplied judgments. Next, sentences which
match both the TREC provided answer pattern and
at least one question key word are extracted and their
suitability is manually judged by humans. The set of
relevant sentences thus includes at least one sentence
with an appropriate answer as well as sentences that
do not contain any answer specific information. This
setup is somewhat idealized, however it allows us to
evaluate in more detail our answer extraction mod-
ule (since when an answer is not found, we know it
is the fault of our system).
Relevant sentences are annotated with their
named entities using Lingpipe5, a MUC-based
named entity recognizer. When we successfully
classify a question with an expected answer type
3See http://www.lemurproject.org/ for details.
4This corpus consists of English newswire texts and is used
as the main document collection in official TREC evaluations.
5The software is available from www.alias-i.com/
lingpipe/
17
(e.g., ORGANIZATION in the example above), we
assume that all NPs attested in the set of relevant
sentences with the same answer type are candidate
answers; in cases where no answer type is found
(e.g., as in Q: What are prions made of? ), all NPs
in the relevant answers set are considered candidate
answers.
Baseline We compared our answer extraction
method to a QA system that exploits solely syntac-
tic information without making use of FrameNet or
any other type of role semantic annotations. For each
question, the baseline identifies key phrases deemed
important for answer identification. These are verbs,
noun phrases, and expected answer phrases (EAPs,
see Section 3). All dependency relation paths con-
necting a key phrase and an EAP are compared to
those connecting the same key phrases and an an-
swer candidate. The similarity of question and an-
swer paths is computed using a simplified version
of the similarity measure6 proposed in Shen and
Klakow (2006).
Our second baseline employs Shalmaneser (Erk
and Pado?, 2006), a publicly available shallow se-
mantic parser7, for the role labeling task instead of
the graph-based model presented in Section 4. The
software is trained on the FrameNet annotated sen-
tences using a standard feature set (see Carreras and
Ma`rquez (2005) for details). We use Shalmaneser
to parse questions and answer sentences. The parser
makes hard decisions about the presence or absence
of a semantic role. Unfortunately, this prevents us
from using our method for semantic structure match-
ing (see Section 5) which assumes a soft labeling.
We therefore came up with a simple matching strat-
egy suitable for the parser?s output. For question
and answer sentences matching in their frame as-
signment, phrases bearing the same semantic role as
the EAP are considered answer candidates. The lat-
ter are ranked according to word overlap (i.e., iden-
tical phrases are ranked higher than phrases with no
6Shen and Klakow (2006) use a dynamic time warping al-
gorithm to calculate the degree to which dependency relation
paths are correlated. Correlations for individual relations are es-
timated from training data whereas we assume a binary value (1
for identical relations and 0 otherwise). The modification was
necessary to render the baseline system comparable to our an-
swer extraction model which is unsupervised.
7The software is available from http://www.coli.
uni-saarland.de/projects/salsa/shal/ .
overlap at all).
7 Results
Our evaluation was motivated by the following ques-
tions: (1) How does the incompleteness of FrameNet
impact QA performance on the TREC data sets? In
particular, we wanted to examine whether there are
questions for which in principle no answer can be
found due to missing frame entries or missing an-
notated sentences. (2) Are all questions and their
corresponding answers amenable to a FrameNet-
style analysis? In other words, we wanted to assess
whether questions and answers often evoke the same
or related frames (with similar roles). This is a pre-
requisite for semantic structure matching and ulti-
mately answer extraction. (3) Do the graph-based
models introduced in this paper bring any perfor-
mance gains over state-of-the-art shallow semantic
parsers or more conventional syntax-based QA sys-
tems? Recall that our graph-based models were de-
signed especially for the QA answer extraction task.
Our results are summarized in Tables 1?3. Table 1
records the number of questions to be answered for
the TREC02?05 datasets (Total). We also give infor-
mation regarding the number of questions which are
in principle unanswerable with a FrameNet-style se-
mantic role analysis.
Column NoFrame shows the number of questions
which don?t have an appropriate frame or predicate
in the database. For example, there is currently no
predicate entry for sponsor or sink (e.g., Q: Who
is the sponsor of the International Criminal Court?
and Q: What date did the Lusitania sink? ). Column
NoAnnot refers to questions for which no semantic
role labeling is possible because annotated sentences
for the relevant predicates are missing. For instance,
there are no annotations for win (e.g., Q: What divi-
sion did Floyd Patterson win? ) or for hit (e.g., Q:
What was the Beatles? first number one hit? ). This
problem is not specific to our method which admit-
tedly relies on FrameNet annotations for performing
the semantic role assignment (see Section 4). Shal-
low semantic parsers trained on FrameNet would
also have trouble assigning roles to predicates for
which no data is available.
Finally, column NoMatch reports the number of
questions which cannot be answered due to frame
18
Data Total NoFrame NoAnnot NoMatch Rest
TREC02 444 87 (19.6) 29 (6.5) 176 (39.6) 152 (34.2)
TREC03 380 55 (14.5) 30 (7.9) 183 (48.2) 112 (29.5)
TREC04 203 47 (23.1) 14 (6.9) 67 (33.0) 75 (36.9)
TREC05 352 70 (19.9) 23 (6.5) 145 (41.2) 114 (32.4)
Table 1: Number of questions which cannot be answered using a FrameNet style semantic analysis; numbers
in parentheses are percentages of Total (NoFrame: frames or predicates are missing; NoAnnot: annotated
sentences are missing, NoMatch: questions and candidate answers evoke different frames.
mismatches. Consider Q: What does AARP stand
for? whose answer is found in S: The American
Association of Retired Persons (AARP) qualify for
discounts. . .. The answer and the question evoke dif-
ferent frames; in fact here a semantic role analysis is
not relevant for locating the right answer. As can be
seen NoMatch cases are by far the most frequent.
The number of questions remaining after excluding
NoFrame, NoAnnot, and NoMatch are shown under
the Rest heading in Table 1.
These results indicate that FrameNet-based se-
mantic role analysis applies to approximately 35%
of the TREC data. This means that an extraction
module relying solely on FrameNet will have poor
performance, since it will be unable to find answers
for more than half of the questions beeing asked. We
nevertheless examine whether our model brings any
performance improvements on this limited dataset
which is admittedly favorable towards a FrameNet
style analysis. Table 2 shows the results of our an-
swer extraction module (SemMatch) together with
two baseline systems. The first baseline uses only
dependency relation path information (SynMatch),
whereas the second baseline (SemParse) uses Shal-
maneser, a state-of-the-art shallow semantic parser
for the role labeling task. We consider an answer
correct if it is returned with rank 1. As can be seen,
SemMatch is significantly better than both Syn-
Match and SemParse, whereas the latter is signifi-
cantly worse than SynMatch.
Although promising, the results in Table 2 are not
very informative, since they show performance gains
on partial data. Instead of using our answer extrac-
tion model on its own, we next combined it with the
syntax-based system mentioned above (SynMatch,
see also Section 6 for details). If FrameNet is indeed
helpful for QA, we would expect an ensemble sys-
Model TREC02 TREC03 TREC04 TREC05
SemParse 13.16 8.92 17.33 13.16
SynMatch 35.53? 33.04? 40.00? 36.84?
SemMatch 53.29?? 49.11?? 54.67?? 59.65??
Table 2: System Performance on subset of TREC
datasets (see Rest column in Table 1); ?: signifi-
cantly better than SemParse; ?: significantly better
than SynMatch (p < 0.01, using a ?2 test).
Model TREC02 TREC03 TREC04 TREC05
SynMatch 32.88? 30.70? 35.95? 34.38?
+SemParse 25.23 23.68 28.57 26.70
+SemMatch 38.96?? 35.53?? 42.36?? 41.76??
Table 3: System Performance on TREC datasets (see
Total column in Table 1); ?: significantly better than
+SemParse; ?: significantly better than SynMatch
(p < 0.01, using a ?2 test).
tem to yield better performance over a purely syn-
tactic answer extraction module. The two systems
were combined as follows. Given a question, we first
pass it to our FrameNet model; if an answer is found,
our job is done; if no answer is returned, the ques-
tion is passed on to SynMatch. Our results are given
in Table 3. +SemMatch and +SemParse are ensem-
ble systems using SynMatch together with the QA
specific role labeling method proposed in this pa-
per and Shalmaneser, respectively. We also compare
these systems against SynMatch on its own.
We can now attempt to answer our third ques-
tion concerning our model?s performance on the
TREC data. Our experiments show that a FrameNet-
enhanced answer extraction module significantly
outperforms a similar module that uses only syn-
tactic information (compare SynMatch and +Sem-
Match in Table 3). Another interesting finding is that
19
the shallow semantic parser performs considerably
worse in comparison to our graph-based models and
the syntax-based system. Inspection of the parser?s
output highlights two explanations for this. First, the
shallow semantic parser has difficulty assigning ac-
curate semantic roles to questions (even when they
are reformulated as declarative sentences). And sec-
ondly, it tends to favor precision over recall, thus re-
ducing the number of questions for which answers
can be found. A similar finding is reported in Sun et
al. (2005) for a PropBank trained parser.
8 Conclusion
In this paper we assess the contribution of semantic
role labeling to open-domain factoid question an-
swering. We present a graph-based answer extrac-
tion model which effectively incorporates FrameNet
style role semantic information and show that it
achieves promising results. Our experiments show
that the proposed model can be effectively combined
with a syntax-based system to obtain performance
superior to the latter when used on its own. Fur-
thermore, we demonstrate performance gains over a
shallow semantic parser trained on the FrameNet an-
notated corpus. We argue that performance gains are
due to the adopted graph-theoretic framework which
is robust to coverage and recall problems.
We also provide a detailed analysis of the appro-
priateness of FrameNet for QA. We show that per-
formance can be compromised due to incomplete
coverage (i.e., missing frame or predicate entries
as well as annotated sentences) but also because of
mismatching question-answer representations. The
question and the answer may evoke different frames
or the answer simply falls outside the scope of a
given frame (i.e., in a non predicate-argument struc-
ture). Our study shows that mismatches are rela-
tively frequent and motivates the use of semantically
informed methods in conjunction with syntax-based
methods.
Important future directions lie in evaluating the
contribution of alternative semantic role frameworks
(e.g., PropBank) to the answer extraction task and
developing models that learn semantic roles di-
rectly from unannotated text without the support
of FrameNet annotations (Grenager and Manning,
2006). Beyond question answering, we also plan to
investigate the potential of our model for shallow
semantic parsing since our experience so far has
shown that it achieves good recall.
Acknowledgements We are grateful to Sebastian Pado?
for running Shalmaneser on our data. Thanks to Frank Keller
and Amit Dubey for insightful comments and suggestions. The
authors acknowledge the support of DFG (Shen; PhD stu-
dentship within the International Postgraduate College ?Lan-
guage Technology and Cognitive Systems?) and EPSRC (Lap-
ata; grant EP/C538447/1).
References
E. Brill, S. Dumais, M. Banko. 2002. An analysis of the
askMSR question-answering system. In Proceedings
of the EMNLP, 257?264, Philadelphia, PA.
X. Carreras, L. Ma`rquez, eds. 2005. Proceedings of the
CoNLL shared task: Semantic role labelling, 2005.
T. Cormen, C. Leiserson, R. Rivest. 1990. Introduction
to Algorithms. MIT Press.
H. Cui, R. X. Sun, K. Y. Li, M. Y. Kan, T. S. Chua.
2005. Question answering passage retrieval using de-
pendency relations. In Proceedings of the ACM SIGIR,
400?407. ACM Press.
T. Eiter, H. Mannila. 1997. Distance measures for
point sets and their computation. Acta Informatica,
34(2):109?133.
K. Erk, S. Pado?. 2006. Shalmaneser - a flexible toolbox
for semantic role assignment. In Proceedings of the
LREC, 527?532, Genoa, Italy.
C. Fellbaum, ed. 1998. WordNet. An Electronic Lexical
Database. MIT Press, Cambridge/Mass.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?
288.
T. Grenager, C. D. Manning. 2006. Unsupervised dis-
covery of a statistical verb lexicon. In Proceedings of
the EMNLP, 1?8, Sydney, Australia.
R. Jonker, A. Volgenant. 1987. A shortest augmenting
path algorithm for dense and sparse linear assignment
problems. Computing, 38:325?340.
M. Kaisser. 2006. Web question answering by exploiting
wide-coverage lexical resources. In Proceedings of the
11th ESSLLI Student Session, 203?213.
J. Leidner, J. Bos, T. Dalmas, J. Curran, S. Clark, C. Ban-
nard, B. Webber, M. Steedman. 2004. The qed open-
domain answer retrieval system for TREC 2003. In
Proceedings of the TREC, 595?599.
C. Leslie, E. Eskin, W. S. Noble. 2002. The spectrum
kernel: a string kernel for SVM protein classification.
In Proceedings of the Pacific Biocomputing Sympo-
sium, 564?575.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
20
X. Li, D. Roth. 2002. Learning question classifiers. In
Proceedings of the 19th COLING, 556?562, Taipei,
Taiwan.
D. K. Lin. 1994. PRINCIPAR?an efficient, broad-
coverage, principle-based parser. In Proceedings of
the 15th COLING, 482?488.
D. Moldovan, C. Clark, S. Harabagiu, S. Maiorano.
2003. COGEX: A logic prover for question answer-
ing. In Proceedings of the HLT/NAACL, 87?93, Ed-
monton, Canada.
S. Narayanan, S. Harabagiu. 2004. Question answering
based on semantic structures. In Proceedings of the
19th COLING, 184?191.
S. Pado?, M. Lapata. 2006. Optimal constituent alignment
with edge covers for semantic projection. In Proceed-
ings of the COLING/ACL, 1161?1168.
M. Palmer, D. Gildea, P. Kingsbury. 2005. The Propo-
sition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
D. Paranjpe, G. Ramakrishnan, S. Srinivasa. 2003. Pas-
sage scoring for question answering via bayesian infer-
ence on lexical relations. In Proceedings of the TREC,
305?210.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, D. Jurafsky.
2004. Shallow semantic parsing using support vector
machines. In Proceedings of the HLT/NAACL, 141?
144, Boston, MA.
D. Shen, D. Klakow. 2006. Exploring correlation of de-
pendency relation paths for answer extraction. In Pro-
ceedings of the COLING/ACL, 889?896.
R. X. Sun, J. J. Jiang, Y. F. Tan, H. Cui, T. S. Chua,
M. Y. Kan. 2005. Using syntactic and semantic re-
lation analysis in question answering. In Proceedings
of the TREC.
B. Taskar, S. Lacoste-Julien, D. Klein. 2005. A discrim-
inative matching approach to word alignment. In Pro-
ceedings of the HLT/EMNLP, 73?80, Vancouver, BC.
M. Wu, M. Y. Duan, S. Shaikh, S. Small, T. Strzalkowski.
2005. University at albany?s ilqua in trec 2005. In
Proceedings of the TREC, 77?83.
21
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 73?82, Prague, June 2007. c?2007 Association for Computational Linguistics
Large Margin Synchronous Generation
and its Application to Sentence Compression
Trevor Cohn and Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, United Kingdom
{tcohn,mlap}@inf.ed.ac.uk
Abstract
This paper presents a tree-to-tree transduc-
tion method for text rewriting. Our model
is based on synchronous tree substitution
grammar, a formalism that allows local dis-
tortion of the tree topology and can thus
naturally capture structural mismatches. We
describe an algorithm for decoding in this
framework and show how the model can
be trained discriminatively within a large
margin framework. Experimental results on
sentence compression bring significant im-
provements over a state-of-the-art model.
1 Introduction
Recent years have witnessed increasing interest in
text-to-text generation methods for many natural
language processing applications ranging from text
summarisation to question answering and machine
translation. At the heart of these methods lies the
ability to perform rewriting operations according to
a set of prespecified constraints. For example, text
simplification identifies which phrases or sentences
in a document will pose reading difficulty for a given
user and substitutes them with simpler alternatives
(Carroll et al, 1999). Sentence compression pro-
duces a summary of a single sentence that retains the
most important information while remaining gram-
matical (Jing, 2000).
Ideally, we would like a text-to-text rewriting sys-
tem that is not application specific. Given a parallel
corpus of training examples, we should be able to
learn rewrite rules and how to combine them in order
to generate new text. A great deal of previous work
has focused on the rule induction problem (Barzilay
andMcKeown, 2001; Pang et al, 2003; Lin and Pan-
tel, 2001; Shinyama et al, 2002), whereas relatively
little emphasis has been placed on the actual gen-
eration task (Quirk et al, 2004). A notable excep-
tion is sentence compression for which end-to-end
rewriting systems are commonly developed (Knight
and Marcu, 2002; Turner and Charniak, 2005; Gal-
ley and McKeown, 2007; Riezler et al, 2003; Mc-
Donald, 2006). The appeal of this task lies in its
simplified formulation as a single rewrite operation,
namely word deletion (Knight and Marcu, 2002).
Solutions to the compression task have been cast
mostly in a supervised learning setting (but see
Clarke and Lapata (2006a), Hori and Furui (2004),
and Turner and Charniak (2005) for unsupervised
methods). Rewrite rules are learnt from a parsed
parallel corpus and subsequently used to find the
best compression from the set of all possible com-
pressions for a given sentence. A common assump-
tion is that the tree structures representing long sen-
tences and their compressions are isomorphic. Con-
sequently, the models are not generally applicable
to other text rewriting problems since they cannot
readily handle structural mismatches and more com-
plex rewriting operations such as substitutions or
insertions. A related issue is that the tree structure
of the compressed sentences is often poor; most al-
gorithms delete words or constituents without pay-
ing too much attention to the structure of the com-
pressed sentence. However, without an explicit gen-
eration mechanism that allows tree transformations,
there is no guarantee that the compressions will have
well-formed syntactic structures. And it will not be
easy to process them for subsequent generation or
analysis tasks.
In this paper we present a text-to-text rewriting
73
model that scales to non-isomorphic cases and can
thus naturally account for structural and lexical di-
vergences. Our approach is inspired by synchronous
tree substitution grammar (STSG, Eisner (2003))
a formalism that allows local distortion of the tree
topology. We show how such a grammar can be in-
duced from a parallel corpus and propose a large
margin model for the rewriting task which can be
viewed as a weighted tree-to-tree transducer. Our
learning framework makes use of the algorithm put
forward by Tsochantaridis et al (2005) which ef-
ficiently learns a prediction function to minimise a
given loss function. Experiments on sentence com-
pression show significant improvements over the
state-of-the-art. Beyond sentence compression and
related text-to-text generation problems (e.g., para-
phrasing), our model is generally applicable to tasks
involving structural mapping. Examples include ma-
chine translation (Eisner, 2003) or semantic parsing
(Zettlemoyer and Collins, 2005).
2 Related Work
Knight and Marcu (2002) proposed a noisy-channel
formulation of sentence compression based on syn-
chronous context-free grammar (SCFG). The lat-
ter is a generalisation of the context-free grammar
(CFG) formalism to simultaneously produce strings
in two languages. In the case of sentence compres-
sion, the grammar rules have two right hand sides,
one corresponding to the source (long) sentence and
the other to its target compression. The synchronous
derivations are learnt from a parallel corpus and their
probabilities are estimated generatively.
Given a long sentence, l, the aim is to find the
corresponding compressed sentence, s, which max-
imises P(s)P(l|s) (here P(s) is the source model
and P(l|s) the channel model.) Modifications of this
model are reported in Turner and Charniak (2005)
and Galley and McKeown (2007) with improved re-
sults. The channel model is limited to tree deletion
and does not allow any type of tree re-organisation.
Non-isomorphic tree structures are common when
translating between languages. It is therefore not
surprising that most previous work on tree rewrit-
ing falls within the realm of machine translation.
Proposals include Eisner?s (2003) synchronous tree
substitution grammar (STSG), Melamed?s (2004)
multitext grammar, and Graehl and Knight?s (2004)
tree-to-tree transducers. Despite differences in for-
malism, all these approaches model the translation
process using tree-based probabilistic transduction
rules. The grammar induction process requires EM
training which can be computationally expensive es-
pecially if all synchronous rules are considered.
Our work formulates sentence compression in the
framework of STSG (Eisner, 2003). We propose a
novel grammar induction algorithm that does not
require EM training and is coupled with a sepa-
rate large margin training process (Tsochantaridis
et al, 2005) for weighting each rule. McDonald
(2006) also presents a sentence compression model
that uses a discriminative large margin algorithm.
However, we differ in two important respects. First,
our generation algorithm is more powerful, perform-
ing complex tree transformations, whereas McDon-
ald only considers simple word deletion. Being tree-
based, the generation algorithm is better able to pre-
serve the grammaticality of the compressed output.
Second, our model can be tuned to a wider range of
loss functions (e.g.,tree-based measures).
3 Problem Formulation
We formulate sentence compression as an instance
of the general problem of learning a mapping from
input patterns x ? X to discrete structured objects
y ? Y . Our training sample consists of a parallel
corpus of input (uncompressed) and output (com-
pressed) pairs (x1,y1) . . .(xn,yn) ? X ? Y and our
task is to predict a target labelled tree y from a
source labelled tree x. As we describe below, y is
not precisely a target tree, but instead derivations
which generate both the source and the target tree.
We model the dependency between x and y as a
weighted STSG. Grammar rules are of the form
?X ,Y ? ? ??,?,?? where ? and ? are elementary
trees composed of a mixture of terminal and non-
terminals rooted with non-terminals X andY respec-
tively, and ? is a set of variable correspondences
between pairs of frontier non-terminals in ? and ?.
A grammar rule specifies that we can substitute the
trees ? and ? for corresponding X and Y nodes in the
source and target trees respectively. For example, the
rule:
?NP, NP? ? ?[DT 1 ADJP NN 2 ]NP, [DT 1 NN 2 ]NP?
74
allows adjective phrases to be dropped from the
source tree within an NP. The indices x are used to
specify the variable correspondences, ?.
Each grammar rule has a score from which the
overall score of a compression y for sentence x
can be derived. These scores are learnt discrimina-
tively using the large margin technique proposed by
Tsochantaridis et al (2005). The synchronous rules
are combined using a chart-based parsing algorithm
(Eisner, 2003) to generate the derivation (i.e., com-
pressed tree) with the highest score.
We begin by describing our STSG generation al-
gorithm in Section 3.1. We next explain how a syn-
chronous grammar is induced from a parallel corpus
of original sentences and their compressions (Sec-
tion 3.2) and give the details of our learning frame-
work (Section 3.3).
3.1 Generation
Generation aims to find the best target tree for a
given source tree using the transformations specified
by the synchronous grammar. (We discuss how we
obtain this grammar in the following section.)
y? =max
y?Y
score(x,y;w) (1)
where y ranges over all target derivations (and there-
fore trees), w is a parameter vector and score(?) is
an objective function measuring the quality of the
derivation. In common with many parsing methods,
we encounter a problem with spurious ambiguity:
i.e., there may be many derivations (sequences of
rule applications) which produce the same target
tree. Ideally we would sum up the scores over all
these derivations, however for the sake of tractability
we instead take the maximum score. This allows us
to pose the maximisation problem over derivations
rather than target trees.
The generation algorithm uses a dynamic pro-
gram defined over the constituents in the source
tree as shown in Figure 1 (see also Eisner (2003)).
The algorithm makes the assumption that the scor-
ing function decomposes with the derivation, such
that a partial score can be evaluated at each step,
i.e., score(x,y;w) = ?r?y score(r;w) where r are
the rules used in the derivation. This method builds
a chart of the best scoring partial derivation for
each source subtree headed by a given target non-
terminal. The inductive step is applied recursively
1: for all nodes, n, in source tree (bottom-up) do
2: for all rules, r with left side matching node, nr = n do
3: s = score(r)
4: for all variables v in r do
5: score = score+ chart[nv,cv]
6: end for
7: update chart[n,cr] with score, s, if better than current
8: end for
9: end for
10: cbest = argmaxc chart[root,c]
11: find best derivation using back-pointers from (root,cbest)
Figure 1: Generation algorithm to find the best
derivation. nr and nv are the source nodes indexed
by the rule?s source side (root and variable), while
cr and cv are the non-terminal categories of the rule?s
target side (root and variable).
is very good and includes ...
AUX RB JJ CC
VPVP
ADJP
VP
Figure 2: Example of a rule application during gen-
eration. The dashed area shows a matching rule for
the VP node.
bottom-up, and involves applying a grammar rule
to a node in the source tree. Rules with substitution
variables in their frontier are scored with reference
to the chart for the matching nodes and target non-
terminal categories. Once the process is complete,
we can read the best score from the chart cell for the
root node, and the best derivation can be constructed
by traversing back-pointers also stored in the chart.
This is illustrated in Figure 2 where the rule
?VP,VP?? ?[[isAUX ADJP 1 ]VP CCVP]VP, [isAUX NP 1 ]VP? is
applied to the top VP node. The score of the result-
ing tree would reference the chart to calculate the
score for the best target tree at the ADJP node with
syntactic category NP.
3.2 Grammar Induction
Our induction algorithm automatically finds gram-
mar rules from a word-aligned parsed parallel cor-
pus. The rules are pairs of elementary trees (i.e., tree
fragments) whose leaf nodes are linked by the word
alignments. These leaves can be either terminal or
non-terminal symbols. Initially, the algorithm ex-
75
tracts tree pairs from word aligned text by choos-
ing aligned constituents in the source and the tar-
get. These pairs are then generalised using subtrees
which are also extracted, resulting in synchronous
rules with variable nodes. The set of aligned tree
pairs are extracted using the alignment template
method (Och and Ney, 2004), constrained to syntac-
tic constituent pairs:
C = {(nS,nT ), (?(s, t) ? A ? s ? Y (nS)? t ? Y (nT ))?
(@(s, t) ? A ? (s ? Y (nS)Y t ? Y (nT )))}
where nS and nT are source and target tree nodes
(subtrees),A = {(s, t)} is the set of word alignments
(pairs of word-indices), Y (?) returns the yield span
for a subtree and Y is the exclusive-or operator.
The next step is to generalise the candidate pairs
by replacing subtrees with variable nodes. We could
fully trust the word alignments and adopt a strat-
egy in which the rules are generalised as much as
possible and thus include little lexicalisation. Fig-
ure 3 shows a simple sentence pair and the result-
ing synchronous rules according to this generalisa-
tion strategy. Alternatively, we could extract every
possible rule by including unlexicalised rules, lexi-
calised rules and their combination. The downside
here is that the total number of possible rules is fac-
torial in the size of the candidate set. We address this
problem by limiting the number of variables and the
recursion depth, and by filtering out singleton rules.
There is no guarantee that the induced rules will
generalise well to a testing set. For example, the test-
ing data may have a rule which was not seen in the
training set (e.g., a new terminal or non terminal).
In this case no rule can be applied and subsequently
generation fails. For this reason we allow the model
to duplicate any CFG production from the source
tree, and uses a feature to flag that this rule was un-
seen in training. These SCFG rules are then merged
with the induced rules and fed into the feature detec-
tion module (see Section 3.3 for details).
3.3 The Large Margin Model
We now describe how the parameters of our STSG
generation system are fit to a supervised training set.
For a given source tree, the space of sister target
trees implied by the synchronous grammar is often
very large, and the majority of these trees are un-
Th
e
do
cu
m
en
ta
tio
n
is ve
ry
go
od
an
d
in
cl
ud
es
a tu
to
ria
l
to ge
t
yo
u
st
ar
te
d
.
Documentation
is
very
good
.
S
.
V
P
V
P
N
P
S
V
P
V
P
S
V
P
V
B
N
N
P
P
R
P
V
B
TON
N
D
T
V
B
Z
C
C
V
P
A
D
JP
JJR
B
A
U
X
N
P
N
N
D
T
S
.
VP
ADJP
JJ
RB
AUX
NP NN
?S,S? ? ?[NP 1 VP 2 . 3 ]S, [NP 1 VP 2 . 3 ]S?
?NP,NP? ? ?[DT NN 1 ]NP, [NN 1 ]NP?
?NN,NN? ? ?documentationNN ,DocumentationNN?
?VP,VP? ? ?VP 1 CC VP,VP 1 ?
?VP,VP? ? ?AUX 1 ADJP 2 ,AUX 1 ADJP 2 ?
?AUX ,AUX? ? ?isAUX , isAUX ?
?ADJP,ADJP? ? ?[RB 1 JJ 2 ]ADJP, [RB 1 JJ 2 ]ADJP?
?RB,RB? ? ?veryRB,veryRB?
?JJ,JJ? ? ?goodADJ ,goodADJ?
?., .? ? ?.., ..?
Figure 3: Induced synchronous grammar from a sen-
tence pair using a strategy that extracts general rules.
grammatical or are poor compressions. The train-
ing procedure learns weights such that the model
can discriminate between these trees and predict a
good target tree. For this we develop a discriminative
training process which learns a weighted tree-to-tree
transducer. Our model is based on Tsochantaridis et
al.?s (2005) framework for learning Support Vector
Machines (SVMs) with structured output spaces, us-
ing the SVMstruct implementation.1 We briefly sum-
marise the approach below; for a more detailed de-
scription we refer the interested reader to Tsochan-
taridis et al (2005).
Traditionally SVMs learn a linear classifier that
separates two or more classes with the largest pos-
sible margin. Analogously, structured SVMs at-
tempt to separate the correct structure from all other
1http://svmlight.joachims.org/svm struct.html
76
structures with a large margin. Given an input in-
stance x, we search for the optimum output y under
the assumption that x and y can be adequately de-
scribed using a combined feature vector representa-
tion ?(x,y). Recall that x are the source trees and y
are synchronous derivations which generate both x
and a target tree.
f (x;w) = argmax
y?Y
?w,?(x,y)? (2)
The goal of the training procedure is to find a param-
eter vector w such that it satisfies the condition:
?i,?y ? Y \yi : ?w,?(xi,yi)??(xi,y)? ? 0 (3)
where xi,yi are the ith training source tree and tar-
get derivation. To obtain a unique solution ? there
will be several parameter vectors w satisfying (3)
if the training instances are linearly separable ?
Tsochantaridis et al (2005) select the w that max-
imises the minimum distance between yi and the
closest runner-up structure.
The framework also incorporates a loss function.
This property is particularly appealing in the context
of sentence compression and generally text-to-text
generation. For example, a compression that differs
from the gold standard with respect to one or two
words should be treated differently from a compres-
sion that bears no resemblance to it. Another impor-
tant factor is the length of the compression. Com-
pressions whose length is similar to the gold stan-
dard should be be preferable to longer or shorter
output. A loss function ?(yi,y) quantifies the accu-
racy of prediction y with respect to the true output
value yi. We give details of the loss functions we
employed for the compression task below.
We are now ready to state the learning objective
for the structured SVM. We use the soft-margin for-
mulation which allows errors in the training set, via
the slack variables ?i:
min
w,?
1
2
||w||2 +
C
n
n
?
i=1
?i, ?i ? 0 (4)
?i,?y ? Y \yi : ?w,??(y)? ? 1?
?i
?(yi,y)
Slack variables ?i are introduced here for each train-
ing example xi, C is a constant that controls the
trade-off between training error minimisation and
margin maximisation, and ??(y) is a shorthand for
?(xi,yi)??(xi,y) (see (3)). Note that slack vari-
ables are rescaled with the inverse loss incurred in
each of the linear constraints.2
The optimisation problem in (4) is approximated
using a polynomial time cutting plane algorithm
(Tsochantaridis et al, 2005). This optimisation cru-
cially relies on finding the constraint incurring the
maximum cost. The cost function for slack rescaling
can be formulated as:
H(y) = (1????i(y),w?)?(yi,y) (5)
In order to adapt this framework to our genera-
tion problem, we must provide the feature map-
ping ?(x,y), a loss function ?(yi,y), and a max-
imiser y? = argmaxy?Y H(y) (see (5)). The following
sections describe how these are instantiated in the
sentence compression task.
Feature Mapping We devised a general feature
set suitable for compression and paraphrasing. Our
feature space is defined over source trees (x) and
target derivations (y). All features apply to a single
grammar rule; a feature vector for a derivation is ex-
pressed as the sum of the feature vectors for each
rule in this derivation.
We make use of syntactic, lexical, and com-
pression specific features. Our simplest syntac-
tic feature is the identity of a synchronous rule.
Specifically, we record its source tree, its target
tree and their combination. We also include rule
frequencies ?(target|source), ?(source|target) and
?(source, target). Another feature records the fre-
quencies of the CFG productions used in the tar-
get side of a rule. This allows the model to learn
the weights of a CFG generation grammar, as a
proxy for a language model. Using scores from a
pre-trained CFG grammar or an n-gram language
model might be preferable when the training sample
is small, however we leave this as future work. Our
last syntactic feature keeps track of the source root
and the target root non-terminals. Our lexical fea-
tures contain the list of tokens in the source yield,
target yield, and both. We also use words as features.
2Alternatively, the loss function can be used to rescale the
margin. This approach is less desirable as it is not scale invari-
ant (Tsochantaridis et al, 2005). We also found empirically that
slack-rescaling slightly outperforms margin rescaling on our
compression task.
77
Finally, we have implemented a set of
compression-specific features. These include a
feature that detects if the yield of the target side
of a synchronous rule is a subset of the yield of
its source. We also take note of the edit operations
(i.e., removal, insertion) required to transform the
source side into the target. Edit operations are
recorded separately for trees and their yields. In
order to encourage compression, we also count the
number of words on the target, the number of rules
used in the derivation and the number of dropped
variables.
Loss Functions The large margin configuration
sketched above is quite modular and in theory a wide
range of loss functions could be specified. Examples
include edit-distance, precision, F-score, BLEU and
tree-based measures. In practice, the loss function
should be compatible with our maximisation algo-
rithm which requires the objective function to de-
compose along the same lines as the tree derivation.3
Given this restriction, we define a loss based
on position-independent unigram precision (Prec)
which penalises errors in the yield independently
for each word. Although fairly intuitive, this loss
is far from ideal. First, it maximally rewards re-
peatedly predicting the same word if the latter is
in the reference target tree. Secondly, it may bias
towards overly short output which drops core in-
formation ? one-word compressions will tend to
have higher precision than longer output. To coun-
teract this, we introduce two brevity penalty mea-
sures (BP) inspired by BLEU (Papineni et al, 2002)
which we incorporate into the loss function, using a
product, loss = 1?Prec ?BP:
BP1 = exp(1?max(1,
r
c
)) (6)
BP2 = exp(1?max(
c
r
,
r
c
))
where r is the reference length and c is the candidate
length.
BP1 is asymmetric, it has value one when c ? r
and decays to zero when c < r. Note that precision
should decay when c > r as extra output will often
not match the reference. BP2 is two-sided: it has
3Optimising non-decompositional loss functions compli-
cates the objective function, which then cannot be solved ef-
ficiently using a dynamic program.
value one when c = r and decays towards zero for
c < r and c > r. In both cases, brevity is assessed
against the gold standard target (not the source) to
allow the system to learn the correct degree of com-
pression from the training data.
Maximisation Algorithm Our algorithm finds the
maximising derivation for H(y) in (5). This deriva-
tion will have a high loss and a high score under the
model, and therefore represents the most-violated
constraint which is then added to the SVM?s work-
ing set of constraints (see (4)).
The standard generation method from Section 3.1
cannot be used without modification to find the best
scoring derivation since it does not account for the
loss function or the gold standard derivation. In-
stead, we stratify the generation chart with the num-
ber of true and false positive tokens predicted, as de-
scribed in Joachims (2005). These contingency val-
ues allow us to compute the precision and brevity
penalty (see (6)) for each complete derivation. This
is then combined with the derivation score and the
gold standard derivation score to give H(y).
The gold standard derivation features, ?(xi,yi),
must be calculated from a derivation linking the
source tree to the gold target tree. As there may
be many such derivations, we find a unique deriva-
tion using the smallest rules possible (for maximum
generality). This is done using a dynamic program,
similar to the inside-outside algorithm used in pars-
ing. Other strategies are also possible, however we
leave this to future work. Finally, we can find the
global maximum H(y) by maximising over all the
root chart entries.
4 Evaluation Set-up
In this section we present our experimental set-up
for assessing the performance of the max margin
model described above. We give details of the cor-
pora used, briefly introduce McDonald?s (2006) sen-
tence compression model used for comparison with
our approach, and explain how system output was
evaluated.
Corpora We evaluated our system on two dif-
ferent corpora. The first is the compression cor-
pus of Knight and Marcu (2002) derived automati-
cally from the document-abstract pairs of the Ziff-
78
Davis corpus. Previous compression work has al-
most exclusively used this corpus. Our experiments
follow Knight and Marcu?s partition of training, test,
and development sets (1,002/36/12 instances). We
also present results on Clarke and Lapata?s (2006a)
Broadcast News corpus.4 This corpus was created
manually (annotators were asked to produce com-
pressions for 50 Broadcast news stories) and poses
more of a challenge than Ziff-Davis. Being a speech
corpus, it often contains incomplete and ungram-
matical utterances and speech artefacts such as dis-
fluencies, false starts and hesitations. Furthermore,
spoken utterances have varying lengths, some are
very wordy whereas others cannot be reduced any
further. Thus a hypothetical compression system
trained on this domain should be able to leave some
sentences uncompressed. Again we used Clarke and
Lapata?s training, test, and development set split
(882/410/78 instances).
Comparison with State-of-the-art We evaluated
our approach against McDonald?s (2006) discrimi-
native model. This model is a good basis for compar-
ison for several reasons. First, it achieves compet-
itive performance with Knight and Marcu?s (2002)
decision tree and noisy channel models. Second, it
also uses large margin learning. Sentence compres-
sion is formulated as a string-to-substring mapping
problem with a deletion-based Hamming loss. Re-
call that our formulation involves a tree-to-tree map-
ping. Third, it uses a feature space complementary to
ours. For example features are defined between ad-
jacent words, and syntactic evidence is incorporated
indirectly into the model. In contrast our model re-
lies on synchronous rules to generate valid compres-
sions and does not explicitly incorporate adjacency
features. We used an implementation of McDonald
(2006) for comparison of results (Clarke and Lapata,
2007).
Evaluation Measures In line with previous work
we assessed our model?s output by eliciting hu-
man judgements. Participants were presented with
an original sentence and its compression and asked
to rate the latter on a five point scale based on the in-
formation retained and its grammaticality. We con-
ducted two separate elicitation studies, one for the
4The corpus can be downloaded from http://homepages.
inf.ed.ac.uk/s0460084/data/.
O: I just wish my parents and my other teachers could
be like this teacher, so we could communicate.
M: I wish my teachers could be like this teacher.
S: I wish my teachers could be like this, so we could
communicate.
G: I wish my parents and other teachers could be like
this, so we could communicate.
O: Earlier this week, in a conference call with analysts,
the bank said it boosted credit card reserves by $350
million.
M: Earlier said credit card reserves by $350 million.
S: In a conference call with analysts, the bank boosted
card reserves by $350 million.
G: In a conference call with analysts the bank said it
boosted credit card reserves by $350 million.
Table 1: Compression examples from the Broadcast
news corpus (O: original sentence, M: McDonald
(2006), S: STSG, G: gold standard)
Ziff-Davis and one for the Broadcast news dataset.
In both cases our materials consisted of 96 source-
target sentences. These included gold standard com-
pressions and the output of our system and Mc-
Donald?s (2006). We were able to obtain ratings on
the entire Ziff-Davis test set as it has only 32 in-
stances; this was not possible for Broadcast news
as the test section consists of 410 instances. Conse-
quently, we randomly selected 32 source-target sen-
tences to match the size of the Ziff-Davis test set.5
We collected ratings from 60 unpaid volunteers, all
self reported native English speakers. Both studies
were conducted over the Internet. Examples of our
experimental items are given in Table 1.
We also report results using F1 computed over
grammatical relations (Riezler et al, 2003). We
chose F1 (as opposed to accuracy or edit distance-
based measures) as Clarke and Lapata (2006b) show
that it correlates reliably with human judgements.
5 Experiments
The framework presented in Section 3 is quite flex-
ible. Depending on the grammar induction strategy,
choice of features, loss function and maximisation
algorithm, different classes of models can be de-
rived. Before presenting our results in detail we dis-
cuss the specific model employed in our experiments
and explain how its parameters were instantiated.
In order to build a compression model we need
5A Latin square design ensured that subjects did not see two
different compressions of the same sentence.
79
60 65 70 75 80 85
45
50
55
60
compression rate
F1
ll
l
l
PrecPrec.BP1Prec.BP2
Figure 4: Compression rate vs. grammatical rela-
tions F1 using unigram precision alone and in com-
bination with two brevity penalties.
a parallel corpus of syntax trees. We obtained syn-
tactic analyses for source and target sentences with
Bikel?s (2002) parser. Our corpora were automat-
ically aligned with Giza++ (Och et al, 1999) in
both directions between source and target and sym-
metrised using the intersection heuristic (Koehn et
al., 2003). Each word in the lexicon was also aligned
with itself. This was necessary in order to inform
Giza++ about word identity. Unparseable sentences
and those longer than 50 tokens were removed from
the data set.
We induced a synchronous tree substitution gram-
mar from the Ziff-Davis and Broadcast news cor-
pora using the method described in Section 3.2. We
extracted all maximally general synchronous rules.
These were complemented with more specific rules
from conjoining pairs of general rules. The specific
rules were pruned to remove singletons and those
rules with more than 3 variables. Grammar rules
were represented by the features described in Sec-
tion 3.3.
An important parameter for our compression task
is the appropriate choice of loss function. Ideally, we
would like a loss function that encourages compres-
sion without overly aggressive information loss. Fig-
ure 4 plots compression rate against grammatical re-
lations F1 using each of the loss functions presented
in Section 3.3 on the Ziff-Davis development set.6
As can be seen with unigram precision alone (Prec)
6We obtained a similar plot for the Broadcast News corpus
but omit it due to lack of space.
Ziff-Davis CompR RelF1
McDonald06 66.2 45.8
STSG 56.8 54.3
Gold standard 57.2 ?
Broadcast News CompR RelF1
McDonald06 68.6 47.6
STSG 73.7 53.4?
Gold standard 76.1 ?
Table 2: Results using grammatical relations F1
(?: sig. diff. from McDonald06; p < 0.01 using the
Student t test)
the system produces overly short output, whereas
the one-sided brevity penalty (BP1) achieves the op-
posite effect. The two-sided brevity penalty (BP2)
seems to strike the right balance: it encourages com-
pression while achieving good F-scores. This sug-
gests that important information is retained in spite
of significant compression. We also varied the regu-
larisation parameter C (see (4)) over a range of val-
ues on the development set and found that setting it
to 0.01 yields overall good performance across cor-
pora and loss functions.
We now present our results on the test set. These
were obtained with a model that uses slack rescal-
ing and a precision-based loss function with a two-
sided brevity penalty (C = 0.01). Table 2 shows the
average compression rates (CompR) for McDonald
(2006) and our model (STSG) as well as their perfor-
mance according to grammatical relations F1. The
row ?Gold standard? displays human-produced com-
pression rates. Notice that our model obtains com-
pression rates similar to the gold standard, whereas
McDonald tends to compress less on Ziff-Davis and
more on Broadcast news. As far as F1 is concerned,
we see that STSG outperforms McDonald on both
corpora. The difference in F1 is statistically signifi-
cant on Broadcast news but not on Ziff-Davis (which
consists solely of 32 sentences).
Table 3 presents the results of our elicitation
study. We carried out an Analysis of Variance
(ANOVA) to examine the effect of system type (Mc-
Donald06, STSG, Gold standard) on the compres-
sion ratings. The ANOVA revealed a reliable effect
on both corpora. We used post-hoc Tukey tests to
80
Model Ziff-Davis Broadcast news
McDonald06 2.82? 2.16?
STSG 3.20?? 2.63?
Gold standard 3.72 3.05
Table 3: Mean ratings on compression output
elicited by humans (?: sig. diff. from McDon-
ald06 (? < 0.05); ? sig. diff. from Gold standard
(? < 0.01); using post-hoc Tukey tests)
examine whether the mean ratings for each sys-
tem differed significantly. The Tukey tests showed
that STSG is perceived as significantly better than
McDonald06. There is no significant difference be-
tween STSG and the gold standard compressions on
the Broadcast news; both systems are significantly
worse than the gold standard on Ziff-Davis.
These results are encouraging, indicating that our
highly expressive framework is a good model for
sentence compression. Under several experimental
conditions we obtain better performance than previ-
ous work. Importantly, the model described here is
not compression-specific, it could be easily adapted
to other tasks, corpora or languages (for which
syntactic analysis tools are available). Being su-
pervised, our model learns to fit the compression
rate of the training data. In this sense, it is some-
what inflexible as it cannot easily adapt to a spe-
cific rate given by a user or imposed by an appli-
cation (e.g., when displaying text on small screens).
Compression rate can be indirectly manipulated by
adopting loss functions that encourage or discourage
compression (see Figure 4), but admittedly in other
frameworks (e.g., Clarke and Lapata (2006a)) the
length of the compression can be influenced more
naturally.
In our formulation of the compression problem,
a derivation is characterised by a single inventory
of features. This entails that the feature space can-
not in principle distinguish between derivations that
use the same rules, applied in a different order. Al-
though, this situation does not arise often in our
dataset, we believe that it can be ameliorated by in-
tersecting a language model with our generation al-
gorithm (Chiang, 2005).
6 Conclusions and Future Work
In this paper we have presented a novel method
for sentence compression cast in the framework of
structured learning. We develop a system that gener-
ates compressions using a synchronous tree substi-
tution grammar whose weights are discriminatively
trained within a large margin model. We also de-
scribe an appropriate algorithm than can be used in
both training (i.e., learning the model weights) and
decoding (i.e., finding the most plausible compres-
sion under the model). The proposed formulation al-
lows us to capture rewriting operations that go be-
yond word deletion and can be easily tuned to spe-
cific loss functions directly related to the problem at
hand. We empirically evaluate our approach against
a state-of-the art model (McDonald, 2006) and show
performance gains on two compression corpora.
Future research will follow three directions. First,
we will extend the framework to incorporate po-
sition dependent loss functions. Examples include
the Hamming distance or more sophisticated func-
tions that take the tree structure of the source and
target sentences into account. Such functions can
be supported by augmenting our generation algo-
rithm with a beam search. Secondly, the present pa-
per used a relatively simple feature set. Our inten-
tion was to examine our model?s performance with-
out extensive feature engineering. Nevertheless, im-
provements should be possible by incorporating fea-
tures defined over n-grams and dependencies (Mc-
Donald, 2006). Finally, the experiments presented
in this work use a grammar acquired from the train-
ing corpus. However, there is nothing inherent in our
formalisation that restricts us to this particular gram-
mar. We therefore plan to investigate the potential
of our method with unsupervised or semi-supervised
grammar induction techniques for additional rewrit-
ing tasks including paraphrase generation and ma-
chine translation.
Acknowledgements The authors acknowledge the sup-
port of EPSRC (grants GR/T04540/01 and GR/T04557/01).
We are grateful to James Clarke for sharing his implementation
of McDonald (2006) with us. Special thanks to Philip Blunsom
for insightful comments and suggestions.
81
References
R. Barzilay, K. McKeown. 2001. Extracting paraphrases
from a parallel corpus. In Proceedings of ACL/EACL,
50?57, Toulouse, France.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of HLT, 24?27, San Diego, CA.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,
J. Tait. 1999. Simplifying text for language impaired
readers. In Proceedings of EACL, 269?270, Bergen,
Norway.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the
43rd ACL, 263?270, Ann Arbor, MI.
J. Clarke, M. Lapata. 2006a. Constraint-based sentence
compression: An integer programming approach. In
Proceedings of COLING/ACLMain Conference Poster
Sessions, 144?151, Sydney, Australia.
J. Clarke, M. Lapata. 2006b. Models for sentence com-
pression: A comparison across domains, training re-
quirements and evaluation measures. In Proceedings
of COLING/ACL, 377?384, Sydney, Australia.
J. Clarke, M. Lapata. 2007. Modelling compression
with discourse constraints. In Proceedings of EMNLP-
CoNLL, Prague, Czech Republic.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of the ACL
Interactive Poster/Demonstration Sessions, 205?208,
Sapporo, Japan.
M. Galley, K. McKeown. 2007. Lexicalized Markov
grammars for sentence compression. In Proceedings
of NAACL/HLT, 180?187, Rochester, NY.
J. Grael, K. Knight. 2004. Training tree transducers. In
Proceedings of NAACL/HLT, 105?112, Boston, MA.
C. Hori, S. Furui. 2004. Speech summarization: an ap-
proach through word extraction and a method for eval-
uation. IEICE Transactions on Information and Sys-
tems, E87-D(1):15?25.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP, 310?315,
Seattle, WA.
T. Joachims. 2005. A support vector method for mul-
tivariate performance measures. In Proceedings of
ICML, 377?384, Bonn, Germany.
K. Knight, D. Marcu. 2002. Summarization beyond sen-
tence extraction: a probabilistic approach to sentence
compression. Artificial Intelligence, 139(1):91?107.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT/NAACL, 48?
54, Edmonton, Canada.
D. Lin, P. Pantel. 2001. Discovery of inference rules for
question answering. Natural Language Engineering,
7(4):342?360.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL, 297?304, Trento, Italy.
I. D. Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL, 653?660, Barcelona,
Spain.
F. J. Och, H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
F. J. Och, C. Tillmann, H. Ney. 1999. Improved align-
ment models for statistical machine translation. In
Proceedings of EMNLP/VLC, 20?28, College Park,
MD.
B. Pang, K. Knight, D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proceedings
of NAACL, 181?188, Edmonton, Canada.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, 311?318,
Philadelphia, PA.
C. Quirk, C. Brockett, W. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In Pro-
ceedings of EMNLP, 142?149, Barcelona, Spain.
S. Riezler, T. H. King, R. Crouch, A. Zaenen. 2003. Sta-
tistical sentence condensation using ambiguity pack-
ing and stochastic disambiguation methods for lexical-
functional grammar. In Proceedings of HLT/NAACL,
118?125, Edmonton, Canada.
Y. Shinyama, S. Sekine, K. Sudo, R. Grishman. 2002.
Automatic paraphrase acquisition from news articles.
In Proceedings of HLT, 40?46, San Diego, CA.
I. Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun.
2005. Large margin methods for structured and in-
terdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
J. Turner, E. Charniak. 2005. Supervised and unsuper-
vised learning for sentence compression. In Proceed-
ings of ACL, 290?297, Ann Arbor, MI.
L. S. Zettlemoyer, M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of UAI, 825?830, Edinburgh, UK.
82
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11?20,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Graph Alignment for Semi-Supervised Semantic Role Labeling
Hagen F?urstenau
Dept. of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
hagenf@coli.uni-saarland.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Unknown lexical items present a major
obstacle to the development of broad-
coverage semantic role labeling systems.
We address this problem with a semi-
supervised learning approach which ac-
quires training instances for unseen verbs
from an unlabeled corpus. Our method re-
lies on the hypothesis that unknown lexical
items will be structurally and semantically
similar to known items for which annota-
tions are available. Accordingly, we rep-
resent known and unknown sentences as
graphs, formalize the search for the most
similar verb as a graph alignment prob-
lem and solve the optimization using inte-
ger linear programming. Experimental re-
sults show that role labeling performance
for unknown lexical items improves with
training data produced automatically by
our method.
1 Introduction
Semantic role labeling, the task of automatically
identifying the semantic roles conveyed by sen-
tential constituents, has recently attracted much at-
tention in the literature. The ability to express the
relations between predicates and their arguments
while abstracting over surface syntactic configu-
rations holds promise for many applications that
require broad coverage semantic processing. Ex-
amples include information extraction (Surdeanu
et al, 2003), question answering (Narayanan
and Harabagiu, 2004), machine translation (Boas,
2005), and summarization (Melli et al, 2005).
Much progress in the area of semantic role la-
beling is due to the creation of resources like
FrameNet (Fillmore et al, 2003), which document
the surface realization of semantic roles in real
world corpora. Such data is paramount for de-
veloping semantic role labelers which are usually
based on supervised learning techniques and thus
require training on role-annotated data. Examples
of the training instances provided in FrameNet are
given below:
(1) a. If [you]
Agent
[carelessly]
Manner
chance going back there, you
deserve what you get.
b. Only [one winner]
Buyer
purchased
[the paintings]
Goods
c. [Rachel]
Agent
injured [her
friend]
Victim
[by closing the car
door on his left hand]
Means
.
Each verb in the example sentences evokes a frame
which is situation-specific. For instance, chance
evokes the Daring frame, purchased the Com-
merce buy frame, and injured the Cause harm
frame. In addition, frames are associated with
semantic roles corresponding to salient entities
present in the situation evoked by the predicate.
The semantic roles for the frame Daring are Agent
and Manner, whereas for Commerce buy these are
Buyer and Goods. A system trained on large
amounts of such hand-annotated sentences typi-
cally learns to identify the boundaries of the argu-
ments of the verb predicate (argument identifica-
tion) and label themwith semantic roles (argument
classification).
A variety of methods have been developed for
semantic role labeling with reasonably good per-
formance (F
1
measures in the low 80s on standard
test collections for English; we refer the interested
reader to the proceedings of the SemEval-2007
shared task (Baker et al, 2007) for an overview
of the state-of-the-art). Unfortunately, the reliance
on training data, which is both difficult and highly
expensive to produce, presents a major obstacle
to the widespread application of semantic role la-
beling across different languages and text gen-
res. The English FrameNet (version 1.3) is not
11
a small resource ? it contains 502 frames cov-
ering 5,866 lexical entries and 135,000 annotated
sentences. Nevertheless, by virtue of being un-
der development it is incomplete. Lexical items
(i.e., predicates evoking existing frames) are miss-
ing as well as frames and annotated sentences
(their number varies greatly across lexical items).
Considering how the performance of supervised
systems degrades on out-of-domain data (Baker
et al, 2007), not to mention unseen events, semi-
supervised or unsupervised methods seem to offer
the primary near-term hope for broad coverage se-
mantic role labeling.
In this work, we develop a semi-supervised
method for enhancing FrameNet with additional
annotations which could then be used for clas-
sifier training. We assume that an initial set of
labeled examples is available. Then, faced with
an unknown predicate, i.e., a predicate that does
not evoke any frame according to the FrameNet
database, we must decide (a) which frames it be-
longs to and (b) how to automatically annotate
example sentences containing the predicate. We
solve both problems jointly, using a graph align-
ment algorithm. Specifically, we view the task
of inferring annotations for new verbs as an in-
stance of a structural matching problem and fol-
low a graph-based formulation for pairwise global
network alignment (Klau, 2009). Labeled and un-
labeled sentences are represented as dependency-
graphs; we formulate the search for an optimal
alignment as an integer linear program where dif-
ferent graph alignments are scored using a func-
tion based on semantic and structural similarity.
We evaluate our algorithm in two ways. We assess
how accurate it is in predicting the frame for an
unknown verb and also evaluate whether the an-
notations we produce are useful for semantic role
labeling.
In the following section we provide an overview
of related work. Next, we describe our graph-
alignment model in more detail (Section 3) and
present the resources and evaluation methodology
used in our experiments (Section 4). We conclude
the paper by presenting and discussing our results.
2 Related Work
Much previous work has focused on creating
FrameNet-style annotations for languages other
than English. A common strategy is to exploit
parallel corpora and transfer annotations from
English sentences onto their translations (Pad?o
and Lapata, 2006; Johansson and Nugues, 2006).
Other work attempts to automatically augment the
English FrameNet in a monolingual setting either
by extending its coverage or by creating additional
training data.
There has been growing interest recently in
determining the frame membership for unknown
predicates. This is a challenging task, FrameNet
currently lists 502 frames with example sentences
which are simply too many (potentially related)
classes to consider for a hypothetical system.
Moreover, predicates may have to be assigned to
multiple frames, on account of lexical ambiguity.
Previous work has mainly used WordNet (Fell-
baum, 1998) to extend FrameNet. For example,
Burchardt et al (2005) apply a word sense dis-
ambiguation system to annotate predicates with
a WordNet sense and hyponyms of these predi-
cates are then assumed to evoke the same frame.
Johansson and Nugues (2007) treat this problem
as an instance of supervised classification. Using
a feature representation based also on WordNet,
they learn a classifier for each frame which decides
whether an unseen word belongs to the frame or
not. Pennacchiotti et al (2008) create ?distribu-
tional profiles? for frames. Each frame is repre-
sented as a vector, the (weighted) centroid of the
vectors representing the meaning of the predicates
it evokes. Unknown predicates are then assigned
to the most similar frame. They also propose a
WordNet-based model that computes the similar-
ity between the synsets representing an unknown
predicate and those activated by the predicates of
a frame.
All the approaches described above are type-
based. They place more emphasis on extending
the lexicon rather than the annotations that come
with it. In our earlier work (F?urstenau and Lapata,
2009) we acquire new training instances, by pro-
jecting annotations from existing FrameNet sen-
tences to new unseen ones. The proposed method
is token-based, however, it only produces annota-
tions for known verbs, i.e., verbs that FrameNet
lists as evoking a given frame.
In this paper we generalize the proposals of
Pennacchiotti et al (2008) and F?urstenau and Lap-
ata (2009) in a unified framework. We create train-
ing data for semantic role labeling of unknown
predicates by projection of annotations from la-
beled onto unlabeled data. This projection is con-
12
ceptualized as a graph alignment problem where
we seek to find a globally optimal alignment sub-
ject to semantic and structural constraints. Instead
of predicting the same frame for each occurence of
an unknown predicate, we consider a set of candi-
date frames and allow projection from any labeled
predicate that can evoke one of these frames. This
allows us to make instance-based decisions and
thus account for predicate ambiguity.
3 Graph Alignment Method
Our approach acquires annotations for an un-
known frame evoking verb by selecting sen-
tences featuring this verb from a large unlabeled
corpus (the expansion corpus). The choice is
based upon a measure of similarity between the
predicate-argument structure of the unknown verb
and those of similar verbs in a manually labeled
corpus (the seed corpus). We formulate the prob-
lem of finding the most similar verbs as the search
for an optimal graph alignment (we represent
labeled and unlabeled sentences as dependency
graphs). Conveniently, this allows us to create la-
beled training instances for the unknown verb by
projecting role labels from the most similar seed
instance. The annotations can be subsequently
used for training a semantic role labeler.
Given an unknown verb, the first step is to nar-
row down the number of frames it could poten-
tially evoke. FrameNet provides definitions for
more than 500 frames, of which we entertain only
a small number. This is done using a method sim-
ilar to Pennacchiotti et al (2008). Each frame
is represented in a semantic space as the cen-
troid of the vectors of all its known frame evoking
verbs. For an unknown verb we then consider as
frame candidates the k closest frames according to
a measure of distributional similarity (which we
compute between the unknown verb?s vector and
the frame centroid vector). We provide details of
the semantic space we used in our experiments in
Section 4.
Next, we compare each sentence featuring the
unknown verb in question to labeled sentences fea-
turing known verbs which according to FrameNet
evoke any of the k candidate frames. If sufficiently
similar seeds exist, the unlabeled sentence is anno-
tated by projecting role labels from the most sim-
ilar one. The similarity score of this best match is
recorded as a measure of the quality (or reliability)
of the new instance. After carrying out this pro-
Body movement
FEE
??
~
~
~
~
~
~
~
~
~
Agent

_
e
i
k
m
p
r
u
y



Body part






~
|
}

and
SUBJ
xxq
q
q
q
q
q
q
q
q
q
CONJ

CONJ
''
O
O
O
O
O
O
O
O
O
O
O
O
O
Herkimer
MOD

blink
DOBJ

nod
MOD

Old
eye
DET

wisely
his
Figure 1: Annotated dependency graph for the
sentenceOld Herkimer blinked his eye and nodded
wisely. The alignment domain is indicated in bold
face. Labels in italics denote frame roles, whereas
grammatical roles are rendered in small capitals.
The verb blink evokes the frame Body Movement.
cedure for all sentences in the expansion corpus
featuring an unknown verb, we collect the highest
scoring new instances and add them back to our
seed corpus as new training items. In the follow-
ing we discuss in more detail how the similarity of
predicate-argument structures is assessed.
3.1 Alignment Scoring
Let s be a semantically labeled dependency graph
in which node n
FEE
represents the frame evoking
verb. Here, we use the term ?labeled? to indi-
cate that the graph contains semantic role labels
in addition to grammatical role labels (e.g., sub-
ject or object). Let g be an unlabeled graph
and n
target
a verbal node in it. The ?unlabeled?
graph contains grammatical roles but no semantic
roles. We wish to find an alignment between the
predicate-argument structures of n
FEE
and n
target
,
respectively. Such an alignment takes the form of
a function ? from a set M of nodes of s (the align-
ment domain) to a set N of nodes of g (the align-
ment range). These two sets represent the rele-
vant predicate-argument structures within the two
graphs; nodes that are not members of these sets
are excluded from any further computations.
If there were no mismatches between (frame)
semantic arguments and syntactic arguments, we
would expect all roles in s to be instantiated by
syntactic dependents in n
FEE
. This is usually the
case but not always. We cannot therefore sim-
13
ply define M as the set of direct dependents of
the predicate, but also have to consider complex
paths between n
FEE
and role bearing nodes. An
example is given in Figure 1, where the role Agent
is filled by a node which is not dominated by the
frame evoking verb blink ; instead, it is connected
to blink by the complex path (CONJ
?1
, SUBJ). For
a given seed s we build a list of all such complex
paths and also include all nodes of s connected
to n
FEE
by one of these paths. We thus define the
alignment domain M as:
1. the predicate node n
FEE
2. all direct dependents of n
FEE
, except auxil-
iaries
3. all nodes on complex paths originating
in n
FEE
4. single direct dependents of any preposition or
conjunction node which is in (2) or end-point
of a complex path covered in (3)
The last rule ensures that the semantic heads
of prepositional phrases and conjunctions are in-
cluded in the alignment domain.
The alignment range N is defined in a similar
way. However, we cannot extract complex paths
from the unlabeled graph g, as it does not con-
tain semantic role information. Therefore, we use
the same list of complex paths extracted from s.
Note that this introduces an unavoidable asymme-
try into our similarity computation.
An alignment is a function ? : M ? N?{?}
which is injective for all values except ?,
i.e., ?(n
1
) = ?(n
2
) 6= ? ? n
1
= n
2
. We score the
similarity of two subgraphs expressed by an align-
ment function ? by the following term:
?
n?M
?(n)6=?
sem(n,?(n))+? ?
?
(n
1
,n
2
)?E(M)
(?(n
1
),?(n
2
))?E(N)
syn
(
r
n
1
n
2
,r
?(n
1
)
?(n
2
)
)
(2)
Here, sem represents a semantic similarity mea-
sure between graph nodes and syn a syntactic sim-
ilarity measure between the grammatical role la-
bels of graph edges. E(M) and E(N) are the sets
of all graph edges between nodes of M and nodes
of N, respectively, and r
n
1
n
2
denotes the grammati-
cal relation between nodes n
1
and n
2
.
Equation (2) expresses the similarity between
two predicate-argument structures in terms of the
sum of semantic similarity scores of aligned graph
nodes and the sum of syntactic similarity scores of
aligned graph edges. The relative weight of these
two sums is determined by the parameter ?. Fig-
ure 2 shows an example of an alignment between
two dependency graphs. Here, the aligned node
pairs thud and thump, back and rest, against and
against, as well as wall and front contribute se-
mantic similarity scores, while the three edge pairs
SUBJ and SUBJ, IOBJ and IOBJ, as well as DOBJ
and DOBJ contribute syntactic similarity scores.
We normalize the resulting score so that it al-
ways falls within the interval [0,1]. To take into
account unaligned nodes in both the alignment do-
main and the alignment range, we divide Equa-
tion (2) by:
?
|M| ? |N|+?
?
|E(M)| ? |E(N)| (3)
A trivial alignment of a seed with itself where all
semantic and syntactic scores are 1 will thus re-
ceive a score of:
|M| ?1+? ? |E(M)| ?1
?
|M|
2
+?
?
E(M)
2
= 1 (4)
which is the largest possible similarity score. The
lowest possible score is obviously 0, assuming that
the semantic and syntactic scores cannot be nega-
tive.
Considerable latitude is available in selecting
the semantic and syntactic similarity measures.
With regard to semantic similarity, WordNet is a
prime contender and indeed has been previously
used to acquire new predicates in FrameNet (Pen-
nacchiotti et al, 2008; Burchardt et al, 2005; Jo-
hansson and Nugues, 2007). Syntactic similarity
may be operationalized in many ways, for exam-
ple by taking account a hierarchy of grammatical
relations (Keenan and Comrie, 1977). Our experi-
ments employed relatively simple instantiations of
these measures. We did not make use of Word-
Net, as we were interested in exploring the set-
ting where WordNet is not available or has limited
coverage. Therefore, we approximate the seman-
tic similarity between two nodes via distributional
similarity. We present the details of the semantic
space model we used in Section 4.
If n and n
?
are both nouns, verbs or adjectives,
we set:
sem(n,n
?
) := cos(~v
n
,~v
n
?
) (5)
where ~v
n
and ~v
n
?
are the vectors representing the
lemmas of n and n
?
respectively. If n and n
?
14
Impact
FEE
OO


Impactor

_
i
w
	





Impactee

_
V
J
9
.
(
$
!

thud
((
SUBJ
zzv
v
v
v
v
v
v
v
v
v
IOBJ
%%
K
K
K
K
K
K
K
K
K
K
thump
SUBJ
{{v
v
v
v
v
v
v
v
v
IOBJ
%%
K
K
K
K
K
K
K
K
K
back
DET

''
against
DOBJ

66
rest
DET

IOBJ
$$
H
H
H
H
H
H
H
H
H
H
against
DOBJ

his wall
DET

77
the of
DOBJ

front
DET

IOBJ
$$
I
I
I
I
I
I
I
I
I
I
the
body
DET

the of
DOBJ

his
cage
DET

the
Figure 2: The dotted arrows show aligned nodes in the graphs for the two sentences His back thudded
against the wall. and The rest of his body thumped against the front of the cage. (Graph edges are also
aligned to each other.) The alignment domain and alignment range are indicated in bold face. The verb
thud evokes the frame Impact.
are identical prepositions or conjunctions we set
sem(n,n
?
) := 1. In all other cases sem(n,n
?
) := 0.
As far as syntactic similarity is concerned, we
chose the simplest metric possible and set:
syn
(
r,r
?
)
:=
{
1 if r = r
?
0 otherwise
(6)
3.2 Alignment Search
The problem of finding the best alignment ac-
cording to the scoring function presented in Equa-
tion (2) can be formulated as an integer linear pro-
gram. Let the binary variables x
ik
indicate whether
node n
i
of graph s is aligned to node n
k
of graph g.
Since it is not only nodes but also graph edges
that must be aligned we further introduce binary
variables y
i jkl
, where y
i jkl
= 1 indicates that the
edge between nodes n
i
and n
j
of graph s is aligned
to the edge between nodes n
k
and n
l
of graph g.
This follows a general formulation of the graph
alignment problem based on maximum structural
matching (Klau, 2009). In order for the x
ik
and
y
i jkl
variables to represent a valid alignment, the
following constraints must hold:
1. Each node of s is aligned to at most one node
of g:
?
k
x
ik
? 1
2. Each node of g is aligned to at most one node
of s:
?
i
x
ik
? 1
3. Two edges may only be aligned if their
adjacent nodes are aligned: y
i jkl
? x
ik
and
y
i jkl
? x
jl
The scoring function then becomes:
?
i,k
sem(n
i
,n
k
)x
ik
+? ?
?
i, j,k,l
syn
(
r
n
i
n
j
,r
n
k
n
l
)
y
i jkl
(7)
We solve this optimization problem with a ver-
sion of the branch-and-bound algorithm (Land
and Doig, 1960). In general, this graph align-
ment problem is NP-hard (Klau, 2009) and usually
solved approximately following a procedure simi-
lar to beam search. However, the special structure
of constraints 1 to 3, originating from the required
injectivity of the alignment function, allows us to
solve the optimization exactly. Our implementa-
tion of the branch-and-bound algorithm does not
generally run in polynomial time, however, we
found that in practice we could efficiently com-
pute optimal alignments in almost all cases (less
than 0.1% of alignment pairs in our data could not
be solved in reasonable time). This relatively be-
nign behavior depends crucially on the fact that
we do not have to consider alignments between
15
full graphs, and the number of nodes in the aligned
subgraphs is limited.
4 Experimental Design
In this section we present our experimental set-up
for assessing the performance of our method. We
give details on the data sets we used, describe the
baselines we adopted for comparison with our ap-
proach, and explain how our system output was
evaluated.
Data Our experiments used annotated sentences
from FrameNet as a seed corpus. These were
augmented with automatically labeled sentences
from the BNC which we used as our expan-
sion corpus. FrameNet sentences were parsed
with RASP (Briscoe et al, 2006). In addi-
tion to phrase structure trees, RASP delivers a
dependency-based representation of the sentence
which we used in our experiments. FrameNet role
annotations were mapped onto those dependency
graph nodes that corresponded most closely to the
annotated substring (see F?urstenau (2008) for a de-
tailed description of the mapping algorithm). BNC
sentences were also parsed with RASP (Andersen
et al, 2008).
We randomly split the FrameNet corpus
1
into 80% training set, 10% test set, and 10% de-
velopment set. Next, all frame evoking verbs in
the training set were ordered by their number of
occurrence and split into two groups, seen and un-
seen. Every other verb from the ordered list was
considered unseen. This quasi-random split covers
a broad range of predicates with a varying number
of annotations. Accordingly, the FrameNet sen-
tences in the training and test sets were divided
into the sets train seen, train unseen, test seen,
and test unseen. As we explain below, this was
necessary for evaluation purposes.
The train seen dataset consisted of 24,220 sen-
tences, with 1,238 distinct frame evoking verbs,
whereas train unseen contained 24,315 sentences
with the same number of frame evoking verbs.
Analogously, test seen had 2,990 sentences and
817 unique frame evoking verbs; the number
of sentences in test unseen was 3,064 (with
847 unique frame evoking verbs).
Model Parameters The alignment model pre-
sented in Section 3 crucially relies on the similar-
1
Here, we consider only FrameNet example sentences
featuring verbal predicates.
ity function that scores potential alignments (see
Equation (2)). This function has a free parameter,
the weight ? for determining the relative contri-
bution of semantic and syntactic similarity. We
tuned ? using leave-one-out cross-validation on
the development set. For each annotated sentence
in this set we found its most similar other sentence
and determined the best alignment between the
two dependency graphs representing them. Since
the true annotations for each sentence were avail-
able, it was possible to evaluate the accuracy of our
method for any ? value. We did this by compar-
ing the true annotation of a sentence to the anno-
tation its nearest neighbor would have induced by
projection. Following this procedure, we obtained
best results with ? = 0.2.
The semantic similarity measure relies on a se-
mantic space model which we built on a lemma-
tized version of the BNC. Our implementation fol-
lowed closely the model presented in F?urstenau
and Lapata (2009) as it was used in a similar
task and obtained good results. Specifically, we
used a context window of five words on either
side of the target word, and 2,000 vector dimen-
sions. These were the common context words in
the BNC. Their values were set to the ratio of the
probability of the context word given the target
word to the probability of the context word over-
all. Semantic similarity was measured using the
cosine of the angle between the vectors represent-
ing any two words. The same semantic space was
used to create the distributional profile of a frame
(which is the centroid of the vectors of its verbs).
For each unknown verb, we consider the k most
similar frame candidates (again similarity is mea-
sured via cosine). Our experiments explored dif-
ferent values of k ranging from 1 to 10.
Evaluation Our evaluation assessed the perfor-
mance of a semantic frame and role labeler with
and without the annotations produced by our
method. The labeler followed closely the im-
plementation described in Johansson and Nugues
(2008). We extracted features from dependency
parses corresponding to those routinely used in
the semantic role labeling literature (see Baker
et al (2007) for an overview). SVM classifiers
were trained
2
with the LIBLINEAR library (Fan
et al, 2008) and learned to predict the frame
name, role spans, and role labels. We followed
2
The regularization parameterC was set to 0.1.
16
Figure 3: Frame labeling accuracy on high,
medium and low frequency verbs, before and af-
ter applying our expansion method; the labeler de-
cides among k = 1, . . . ,10 candidate frames.
the one-versus-one strategy for multi-class classi-
fication (Friedman, 1996).
Specifically, the labeler was trained on the
train seen data set without any access to training
instances representative of the ?unknown? verbs in
test unseen. We then trained the labeler on a larger
set containing train seen and new training exam-
ples obtained with our method. To do this, we used
train seen as the seed corpus and the BNC as the
expansion corpus. For each ?unknown? verb in
train unseen we obtained BNC sentences with an-
notations projected from their most similar seeds.
The quality of these sentences as training instances
varies depending on their similarity to the seed.
In our experiments we added to the training set
the 20 highest scoring BNC sentences per verb
(adding less or more instances led to worse per-
formance).
The average number of frames which can be
evoked by a verb token in the set test unseen
was 1.96. About half of them (1,522 instances)
can evoke only one frame, 22% can evoke two
frames, and 14 instances can evoke up to 11 differ-
ent frames. Finally, there are 120 instances (4%)
in test unseen for which the correct frame is not
annotated on any sentence in train seen.
Figure 4: Role labeling F
1
for high, medium, and
low frequency verbs (roles of mislabeled frames
are counted as wrong); the labeler decides among
k = 1, . . . ,10 candidate frames.
5 Results
We first examine how well our method performs
at frame labeling. We partitioned the frame evok-
ing verbs in our data set into three bands (High,
Medium, and Low) based on an equal division
of the range of their occurrence frequency in the
BNC. As frequency is strongly correlated with
polysemy, the division allows us to assess how
well our method is performing at different degrees
of ambiguity. Figure 3 summarizes our results for
High, Medium, and Low frequency verbs. The
number of verbs in each band are 282, 282, and
283, respectively. We compare the frame accuracy
of a labeler trained solely on the annotations avail-
able in FrameNet (Without expansion) against a
labeler that also uses annotations created with our
method (After expansion). Both classifiers were
employed in a setting where they had to decide
among k candidate frames. These were the k most
similar frames to the unknown verb in question.
We also show the accuracy of a simple baseline
labeler, which randomly chooses one of the k can-
didate frames.
The graphs in Figure 3 show that for verbs in the
Medium and Low frequency bands, both classi-
fiers (with and without expansion) outperform the
baseline of randomly choosing among k candidate
frames. Interestingly, rather than defaulting to the
most similar frame (k = 1), we observe that ac-
17
Figure 5: Hybrid frame labeling accuracy (k = 1
for High frequency verbs).
curacy improves when frame selection is viewed
as a classification task. The classifier trained on
the expanded training set consistently outperforms
the one trained on the original training set. While
this is also true for the verbs in the High frequency
band, labeling accuracy peaks at k = 1 and does
not improve when more candidate frames are con-
sidered. This is presumably due to the skewed
sense distributions of high frequency verbs, and
defaulting to the most likely sense achieves rela-
tively good performance.
Next, we evaluated our method on role label-
ing, again by comparing the performance of our
role labeler on the expanded and original train-
ing set. Since role and frame labeling are inter-
dependent, we count all predicted roles of an in-
correctly predicted frame as wrong. This unavoid-
ably results in low role labeling scores, but allows
us to directly compare performance across differ-
ent settings (e.g., different number of candidate
frames, with or without expansion). Figure 4 re-
ports labeled F
1
for verbs in the High, Medium
and Low frequency bands. The results are simi-
lar to those obtained for frame labeling; the role
labeler trained on the the expanded training set
consistently outperforms the labeler trained on the
unexpanded one. (There is no obvious baseline
for role labeling, which is a complex task involv-
ing the prediction of frame labels, identification of
the role bearing elements, and assignment of role
labels.) Again, for High frequency verbs simply
defaulting to k = 1 performs best.
Taken together, our results on frame and role
labeling indicate that our method is not very effec-
tive for High frequency verbs (which in practice
should be still annotated manually). We there-
Figure 6: Hybrid role labeling F
1
(k = 1 for High
frequency verbs).
fore also experimented with a hybrid approach
that lets the classifier choose among k candi-
dates for Medium and Low frequency verbs and
defaults to the most similar candidate for High
frequency verbs. Results for this approach are
shown in Figures 5 and 6. All differences be-
tween the expanded and the unexpanded classi-
fier when choosing between the same k > 1 can-
didates are significant according to McNemar?s
test (p < .05). The best frame labeling accu-
racy (26.3%) is achieved by the expanded classi-
fier when deciding among k = 6 candidate frames.
This is significantly better (p < .01) than the best
performance of the unexpanded classifier (25.0%),
which is achieved at k = 2. Role labeling results
follow a similar pattern. The best expanded classi-
fier (F
1
=14.9% at k = 6) outperforms the best un-
expanded one (F
1
=14.1% at k = 2). The difference
in performance as significant at p < 0.05, using
stratified shuffling (Noreen, 1989).
6 Conclusions
This paper presents a novel semi-supervised ap-
proach for reducing the annotation effort involved
in creating resources for semantic role labeling.
Our method acquires training instances for un-
known verbs (i.e., verbs that are not evoked by
existing FrameNet frames) from an unlabeled cor-
pus. A key assumption underlying our work is
that verbs with similar meanings will have sim-
ilar argument structures. Our task then amounts
to finding the seen instances that resemble the un-
seen instances most, and projecting their annota-
tions. We represent this task as a graph alignment
problem, and formalize the search for an optimal
alignment as an integer linear program under an
18
objective function that takes semantic and struc-
tural similarity into account.
Experimental results show that our method im-
proves frame and role labeling accuracy, espe-
cially for Medium and Low frequency verbs. The
overall frame labeling accuracy may seem low.
There are at least two reasons for this. Firstly, the
unknown verb might have a frame for which no
manual annotation exists. And secondly, many er-
rors are due to near-misses, i.e., we assign the un-
known verb a wrong frame which is nevertheless
very similar to the right one. In this case, accuracy
will not give us any credit.
An obvious direction for future work concerns
improving our scoring function. Pennacchiotti
et al (2008) show that WordNet-based similarity
measures outperform their simpler distributional
alternatives. An interesting question is whether the
incorporation of WordNet-based similarity would
lead to similar improvements in our case. Also
note that currently our method assigns unknown
lexical items to existing frames. A better alterna-
tive would be to decide first whether the unknown
item can be classified at all (because it evokes a
known frame) or whether it represents a genuinely
novel frame for which manual annotation must be
provided.
Acknowledgments The authors acknowledge
the support of DFG (IRTG 715) and EPSRC (grant
GR/T04540/01). We are grateful to Richard Jo-
hansson for his help with the re-implementation of
his semantic role labeler. Special thanks to Man-
fred Pinkal for valuable feedback on this work.
References
?istein E. Andersen, Julien Nioche, Ted Briscoe,
and John Carroll. 2008. The BNC Parsed with
RASP4UIMA. In Proceedings of the 6th Interna-
tional Language Resources and Evaluation Confer-
ence, pages 865?869, Marrakech, Morocco.
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 99?104, Prague, Czech Republic.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases.
International Journal of Lexicography, 18(4):445?
478.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, pages 77?80, Sydney, Australia.
Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet Detour to FrameNet. In Proceed-
ings of the GLDV 200Workshop GermaNet II, Bonn,
Germany.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. Journal of
Machine Learning Research, 9:1871?1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Jerome H. Friedman. 1996. Another approach to poly-
chotomous classification. Technical report, Depart-
ment of Statistics, Stanford University.
Hagen F?urstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 220?228, Athens, Greece.
Hagen F?urstenau. 2008. Enriching frame semantic re-
sources with dependency graphs. In Proceedings of
the 6th Language Resources and Evaluation Confer-
ence, pages 1478?1484, Marrakech, Morocco.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 436?443, Syd-
ney, Australia.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Richard
Johansson and Pierre Nugues, editors, FRAME
2007: Building Frame Semantics Resources for
Scandinavian and Baltic Languages, pages 27?30,
Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role la-
beling. In Proceedings of the 22nd International
Conference on Computational Linguistics, pages
393?400, Manchester, UK.
E. Keenan and B. Comrie. 1977. Noun phrase acces-
sibility and universal grammar. Linguistic Inquiry,
8:62?100.
Gunnar W. Klau. 2009. A new graph-based method
for pairwise global network alignment. BMC Bioin-
formatics, 10 (Suppl 1).
A.H. Land and A.G. Doig. 1960. An automatic
method for solving discrete programming problems.
Econometrica, 28:497?520.
19
Gabor Melli, Yang Wang, Yurdong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of
SQUASH, the SFU question answering summary
handler for the duc-2005 summarization task. In
Proceedings of the HLT/EMNLP Document Under-
standing Workshop, Vancouver, Canada.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693?701, Geneva,
Switzerland.
E. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Sebastian Pad?o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161?1168, Sydney,
Australia.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of FrameNet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 457?465, Honolulu,
Hawaii.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15, Sap-
poro, Japan.
20
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430?439,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Language Models Based on Semantic Composition
Jeff Mitchell and Mirella Lapata
School of Informatics, University of Edinburgh
Edinburgh EH8 9LW, UK
jeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we propose a novel statistical
language model to capture long-range se-
mantic dependencies. Specifically, we ap-
ply the concept of semantic composition to
the problem of constructing predictive his-
tory representations for upcoming words.
We also examine the influence of the un-
derlying semantic space on the composi-
tion task by comparing spatial semantic
representations against topic-based ones.
The composition models yield reductions
in perplexity when combined with a stan-
dard n-gram language model over the
n-gram model alone. We also obtain per-
plexity reductions when integrating our
models with a structured language model.
1 Introduction
Statistical language modeling plays an important
role in many areas of natural language process-
ing including speech recognition, machine trans-
lation, and information retrieval. The prototypi-
cal use of language models is to assign proba-
bilities to sequences of words. By invoking the
chain rule, these probabilities are generally es-
timated as the product of conditional probabili-
ties P(w
i
|h
i
) of a word w
i
given the history of
preceding words h
i
? w
i?1
1
. In theory, the history
could span any number of words up to w
i
such as
sentences or even a paragraphs. In practice, how-
ever, it has proven challenging to deal with the
combinatorial growth in the number of possible
histories which in turn impacts reliable parame-
ter estimation. A simple and effective strategy is
to truncate the chain rule to include only the n-1
preceding words (n is often set within the range
of 3?5). The simplification reduces the number of
free parameters. However, low values of n impose
an artificially local horizon to the language model,
and compromise its ability to capture long-range
dependencies, such as syntactic relationships, se-
mantic or thematic constraints.
The literature offers many examples of how to
overcome this limitation, essentially by allowing
the modulation of probabilities by dependencies
which extend to words beyond the n-gram horizon.
Cache language models (Kuhn and de Mori, 1992)
increase the probability of words observed in the
history, e.g., by some factor which decays expo-
nentially with distance. Trigger models (Rosen-
feld, 1996) go a step further by allowing arbi-
trary word pairs to be incorporated into the cache.
Structured language models (e.g., Roark (2001))
go beyond the representation of history as a lin-
ear sequence of words to capture the syntactic con-
structions in which these words are embedded.
It is also possible to build representations of
history which are semantic rather than syntactic
(Bellegarda (2000; Coccaro and Jurafsky (1998;
Gildea and Hofmann (1999)). In this approach, es-
timates for the probabilities of upcoming words
are derived from a comparison of their semantic
content with the content of the history so far. The
semantic representations, in this case, are vectors
derived from the distributional properties of words
in a corpus, based on the insight that words which
are semantically similar will be found in similar
contexts (Harris, 1968; Firth, 1957). Although the
the construction of a semantic representation for
the history is crucial to this approach, the under-
lying vector-based models are primarily designed
to represent isolated words rather than word se-
quences. Ideally, we would like to compose the
meaning of the history out of its constituent parts.
This is by no means a new idea. Much work in lin-
guistic theory (Partee, 1995; Montague, 1974) has
been devoted to compositionality, the process of
determining the meaning of complex expressions
from simpler ones. Previous work either ignores
this issue (e.g., Bellegarda (2000)) or simply com-
430
putes the centroid of the vectors representing the
history (e.g., Coccaro and Jurafsky (1998)). This is
motivated primarily by mathematical convenience
rather than by empirical evidence.
In our earlier work (Mitchell and Lapata, 2008)
we formulated composition as a function of two
vectors and introduced a variety of models based
on addition and multiplication. In this paper we
apply vector composition to the problem of con-
structing predictive history representations for lan-
guage modeling. Besides integrating composition
with language modeling, a task which is novel to
our knowledge, our approach also serves as a valu-
able testbed of our earlier framework which we
originally evaluated on a small scale verb-subject
similarity task. We also investigate how the choice
of the underlying semantic representation inter-
acts with the choice of composition function by
comparing a spatial model that represents words
as vectors in a high-dimensional space against a
probabilistic model that represents words as topic
distributions.
Our results show that the proposed composi-
tion models yield reductions in perplexity when
combined with a standard n-gram model over
the n-gram model alone. We also show that with
an appropriate composition function spatial mod-
els outperform the more sophisticated topic mod-
els. Finally, we obtain further perplexity reduc-
tions when our models are integrated with a struc-
tured language model, indicating that the two ap-
proaches to language modeling are complemen-
tary.
2 Background
2.1 Distributional Models of Semantics
The insight that words with similar meanings will
tend to be distributed in similar contexts has given
rise to a number of approaches that construct
semantic representations from corpora. Broadly
speaking, these models come in two flavors. Se-
mantic space models represent the meaning of
words in terms of vectors, with the vector compo-
nents being derived from the distributional statis-
tics of those words. Essentially, these models pro-
vide a simple procedure for constructing spatial
representations of word meaning. Topic models, in
contrast, impose a probabilistic model onto those
distributional statistics, under the assumption that
hidden topic variables drive the process that gener-
ates words. Both approaches represent the mean-
ings of words in terms of an n-dimensional series
of values, but whereas the semantic space model
treats those values as defining a vector with spatial
properties, the topic model treats them as a proba-
bility distribution.
A simple and popular (McDonald, 2000; Bul-
linaria and Levy, 2007; Lowe, 2000) way to con-
struct a semantic space model is to associate each
vector component with a particular context word,
and assign it a value based on the strength of
its co-occurrence with the target (i.e., the word
for which a semantic representation is being con-
structed). For example, in Mitchell and Lapata
(2008) we used the 2,000 most frequent content
words in a corpus as their contexts, and defined
co-occurrence in terms of the context word be-
ing present in a five word window on either side
of the target word. We calculated the ratio of the
probability of the context word given the target
word to the overall probability of the context word
and use these values as their vector components.
This procedure has the benefits of simplicity and
also of being largely free of any additional the-
oretical assumptions over and above the distribu-
tional approach to semantics. This is not to say that
more sophisticated approaches have not been de-
veloped or that they are not useful. Much work has
been devoted to enriching semantic space mod-
els with syntactic information (e.g., Grefenstette
(1994; Pad?o and Lapata (2007)), selectional pref-
erences (Erk and Pad?o, 2008) or with identifying
optimal ways of defining the vector components
(e.g., Bullinaria and Levy (2007)).
The semantic space discussed thus far is based
on word co-occurrence statistics. However, the
statistics of how words are distributed across the
documents also carry useful semantic informa-
tion. Latent Semantic Analysis (LSA, Landauer
and Dumais (1997) utilizes precisely this distribu-
tional information to uncover hidden semantic fac-
tors by means of dimensionality reduction. Singu-
lar value decomposition (SVD, Berry et al (1994))
is applied to a word-document co-occurrence ma-
trix which is factored into a product of a number
of other matrices; one of them represents words in
terms of the semantic factors and another repre-
sents documents in terms of the same factors. The
algebraic relation between these matrices can be
used to show that any document vector is a linear
combination of the vectors representing the words
it contains. Thus, within this paradigm it is nat-
431
ural to treat multi-word structures as a ?pseudo-
document? and represent them via linear combi-
nations of word vectors.
Due to its generality, LSA has proven a valuable
analysis tool with a wide range of applications.
However, the SVD procedure is somewhat ad-hoc
lacking a sound statistical foundation. Probabilis-
tic Latent Semantic Analysis (pLSA, Hofmann
(2001)) casts the relationship between documents
and words in terms of a generative model based on
a set of hidden topics. Documents are represented
by distributions over topics and topics are distri-
butions over words. Thus the mixture of topics
in any document determines its vocabulary. Maxi-
mum likelihood estimation of these distributions
over a word-document matrix has a comparable
effect to SVD in LSA: a set of hidden semantic
factors, in this case topics, are extracted and docu-
ments and words are represented by these topics.
Latent Dirichlet Allocation (Griffiths et al,
2007; Blei et al, 2003) enhances further the math-
ematical foundation of this approach. Whereas
pLSA treats each document as a separate, inde-
pendent mixture of topics, LDA assumes that the
topic distributions of documents are generated by
a Dirichlet distribution. Thus, LDA is a probabilis-
tic model of the whole document collection. In this
model the process of generating a document can
be described as follows:
1. draw a multinomial distribution ? from a
Dirichlet distribution parametrized by ?
2. for each word in a document:
(a) draw a topic z
k
from the multinomial
distribution characterized by ?
(b) draw a word from a multinomial distri-
bution conditioned on the topic z
k
and
word probabilities ?
Under this model, constructing a representation
for a multi-word sequence amounts to estimating
the topic proportions for that sequence.
1
Struc-
ture here arises from the mathematical form of the
model, as opposed to any linguistic assumptions.
Without anticipating our results too much, we
should point out that several features of the LDA
model are likely to affect the representation of
1
Estimating the posterior distribution P(?,z|w,?,?) of
the hidden variables given an observed collection of docu-
ments w is intractable in general; however, a variety of ap-
proximate inference algorithms have been proposed in the
literature (e.g., Blei et al (2003; Griffiths et al (2007)).
multi-word sequences. Firstly, it is a top-down
generative model (the topic proportions for a doc-
ument are first selected and then this drives the
generation of words) as opposed to a bottom-up
constructive process (words modulate each other
to produce a complex representation of their com-
bination). Secondly, the top level Dirichlet distri-
bution is likely to lead to documents being dom-
inated by a small number of topics, producing
sparse vectors. And lastly, the assumption that
words are generated independently means the in-
teraction between them is not modeled.
2.2 Language Modeling using Semantic
Representations
A common approach to embedding semantic rep-
resentations within language modeling is to mea-
sure the semantic similarity between an upcoming
word and its history and use it to modify the prob-
abilities from an n-gram model. In this way, the
n-gram?s sensitivity to short-range dependencies
is enriched with information about longer-range
semantic coherence. Much of previous work has
taken this approach (Bellegarda, 2000; Coccaro
and Jurafsky, 1998; Wandmacher and Antoine,
2007), whilst relying on LSA to provide seman-
tic representations for individual words. Some au-
thors (Coccaro and Jurafsky, 1998; Wandmacher
and Antoine, 2007) use the geometric notion of
a vector centroid to construct representations of
history, whereas others (Bellegarda, 2000; Deng
and Khundanpur, 2003) use the idea of a ?pseudo-
document?, which is derived from the algebraic
relation between documents and words assumed
within LSA. They all derive P(w
i
|h
i
), the probabil-
ity of an upcoming word given its history, from the
cosine similarity measure which must be somehow
normalized in order to yield well-formed probabil-
ity estimates.
The approach of Gildea and Hofmann (1999)
overcomes this difficulty by using representations
constructed with pLSA, which have a direct prob-
abilistic interpretation. As a result, the probabil-
ity of an upcoming word given the history can be
derived naturally and directly, avoiding the need
for ad-hoc transformations. In constructing their
representation of history, Gildea and Hofmann
(1999) use an online Expectation Maximization
process, which derives from the probabilistic basis
of pLSA, to update the history with new words.
Extensions on the basic semantic language
432
models sketched above involve representing the
history by multiple LSA models of varying granu-
larity in an attempt to capture topic, subtopic, and
local information (Zhang and Rudnicky, 2002); in-
corporating syntactic information by building the
semantic space over words and their syntactic an-
notations (Kanejiya et al, 2004); and treating the
LSA similarity as a feature in a maximum entropy
language model (Deng and Khundanpur, 2003).
3 Composition Models
The problem of vector composition has re-
ceived relatively little attention within natural lan-
guage processing. Attempts to use tensor products
(Smolensky, 1990; Clark et al, 2008; Widdows,
2008) as a means of binding one vector to another
face major computational difficulties as their di-
mensionality grows exponentially with the num-
ber of constituents being composed. To overcome
this problem, other techniques (Plate, 1995) have
been proposed in which the binding of two vectors
results in a vector which has the same dimension-
ality as its components. Crucially, the success of
these methods depends on the assumption that the
vector components are randomly distributed. This
is problematic for modeling language which has
regular structure.
Given the above considerations, in Mitchell and
Lapata (2008) we introduce a general framework
for studying vector composition, which we formu-
late as a function f of two vectors u and v:
h= f (u,v) (1)
where h denotes the composition of u and v. Dif-
ferent composition models arise, depending on
how f is chosen. Our earlier work (Mitchell and
Lapata, 2008) explored two broad classes of mod-
els based on additive and multiplicative functions.
Additive models are the most common method
of vector combination in the literature. They have
been applied to a wide variety of tasks includ-
ing document coherence (Foltz et al, 1998), es-
say grading (Landauer and Dumais, 1997), mod-
eling selectional restrictions (Kintsch, 2001), and
notably language modeling (Coccaro and Jurafsky,
1998; Wandmacher and Antoine, 2007):
h
i
= u
i
+ v
i
(2)
Vector addition (or averaging, which is equivalent
under the cosine similarity measure) is a computa-
tionally efficient composition model as it does not
increase the dimensionality of the resulting vector.
However, the idea of averaging is somewhat coun-
terintuitive from a linguistic perspective. Compo-
sition of simple elements onto more complex ones
must allow the construction of novel meanings
which go beyond those of the individual elements
(Pinker, 1994).
In Mitchell and Lapata (2008) we argue that
composition models based on multiplication ad-
dress this problem:
h
i
= u
i
? v
i
(3)
Whereas the addition of vectors ?lumps their con-
tent together?, multiplication picks out the content
relevant to their combination by scaling each com-
ponent of one with the strength of the correspond-
ing component of the other. This argument is ap-
pealing, especially if one is interested in explain-
ing how the meaning of a verb is modulated by
its subject. Here, we also develop a complemen-
tary, probabilistic argument for the validity of this
model.
Let us assume that semantic vectors are based
on components defined as the ratio of the condi-
tional probability of a context word given the tar-
get word to the overall probability of the context
word.
v
i
=
p(context
i
|target)
p(context
i
)
(4)
These vectors represent the distributional proper-
ties of a given target word in terms of the strength
of its co-occurrence with a set of context words.
Dividing through by the overall probability of each
context word prevents the vectors being dominated
by the most frequent context words, which will of-
ten also have the highest conditional probabilities.
Let us assume vectors u and v represent tar-
get words w
1
and w
2
. Now, when we compose
these vectors using the multiplicative model and
the components definition in (4), we obtain:
h
i
= v
i
?u
i
=
p(c
i
|w
1
)
p(c
i
)
p(c
i
|w
2
)
p(c
i
)
(5)
And by Bayes? theorem:
h
i
=
p(w
1
|c
i
)p(w
2
|c
i
)
p(w
1
)p(w
2
)
(6)
Assuming w
1
and w
2
are independent and apply-
ing Bayes? theorem again, h
i
becomes:
h
i
?
p(w
1
w
2
|c
i
)
p(w
1
w
2
)
=
p(c
i
|w
1
w
2
)
p(c
i
)
(7)
433
By comparing to (4), we can see that the expres-
sion on the right hand side gives us something akin
to the vector components we would expect when
our target is the co-occurrence of w
1
and w
2
. Thus,
for the multiplicative model, the combined vec-
tor h
i
can be thought of as an approximation to
a vector representing the distributional properties
of the phrase w
1
w
2
.
If multiplication results in a vector which is
something like the representation of w
1
and w
2
,
then addition produces a vector which is more like
the representation of w
1
or w
2
. Suppose we were
unsure whether a word token x was an instance
of w
1
or of w
2
. It would be reasonable to express
the probabilities of context words around this to-
ken in terms of the probabilities for w
1
and w
2
,
assuming complete uncertainty between them:
p(c
i
|x) =
1
2
p(c
i
|w
1
)+
1
2
p(c
i
|w
2
) (8)
Therefore, we could represent x with a vector,
based on these probabilities, having the compo-
nents:
x
i
=
1
2
p(c
i
|w
1
)
p(c
i
)
+
1
2
p(c
i
|w
2
)
p(c
i
)
(9)
Which is exactly the vector averaging approach to
semantic composition. As more vectors are com-
bined, vector addition will lead to greater general-
ity rather than greater specificity. The multiplica-
tive approach, on the other hand, picks out the
components of the constituents that are relevant
to the combination, and represents more faithfully
the properties of their conjunction.
As an aside, we should point out that our earlier
work (Mitchell and Lapata, 2008) introduced sev-
eral other models, additive and multiplicative, be-
sides the ones discussed here. We selected the ad-
ditive model as a baseline and also due to its over-
whelming popularity in the language modeling lit-
erature. The multiplicative model presented above
performed best in our evaluation study (i.e., pre-
dicting verb-subject similarity).
4 Language Modeling
Estimating Probabilities In language modeling
our aim is to derive probabilities, p(w|h), given
the semantic representations of word, w, and its
history, h, based on the assumption that probable
words should be semantically coherent with the
history. Semantic coherence is commonly mea-
sured via the cosine of the angle between two vec-
tors:
sim(w,h) =
w ?h
|w||h|
(10)
w ?h=
?
i
w
i
h
i
(11)
where w ? h is the dot product of w and h. Coc-
caro and Jurafsky (1998) utilize this measure in
their approach to language modeling. Unfortu-
nately, they find it necessary to resort to a number
of ad-hoc mechanisms to turn the cosine similari-
ties into useful probabilities. The primary problem
with the cosine measure is that, although its values
lie between 0 and 1, they do not sum to 1, as prob-
abilities must. Thus, some form of normalization
is required. A further problem concerns the fact
that such a measure takes no account of the under-
lying frequency of w, which is crucial for a proba-
bilistic model. For example, encephalon and brain
are roughly synonymous, and may be equally sim-
ilar to some context, but brain may nonetheless be
much more likely, as it is generally more common.
An ideal measure would take account of the un-
derlying probabilities of the elements involved and
produce values that sum to 1. Our approach is to
modify the dot product (equation (11)) on which
the cosine measure is based. Assuming that our
vector components are given by equation (4), the
dot product becomes:
w ?h=
?
i
p(c
i
|w)
p(c
i
)
p(c
i
|h)
p(c
i
)
(12)
which we modify to derive probabilities as fol-
lows:
p(w|h) = p(w)
?
i
p(c
i
|w)
p(c
i
)
p(c
i
|h)
p(c
i
)
p(c
i
) (13)
This expression now weights the sum with the in-
dependent probabilities of the context words and
the word to be predicted. That this is indeed a valid
probability can be seen by the fact it is equiva-
lent to
?
i
p(w|c
i
)p(c
i
|h). However, in constructing
a representation of the history h, it is more conve-
nient to work with equation (13) as it is based on
vector components and can be readily used with
the composition models presented in Mitchell and
Lapata (2008).
Equation (13) allows us to derive probabilities
from vectors representing a word and its prior his-
tory. We must also construct a representation of
434
the history up to the nth word of a sentence. To do
this, we combine, via some (additive or multiplica-
tive) function f , the vector representing that word
with the vector representing the history up to n?1
words:
h
n
= f (w
n
,h
n?1
) (14)
h
1
= w
1
(15)
One issue that must be resolved in implement-
ing equation (14) is that the history vector should
remain correctly normalized. In other words, the
products h
i
? p(c
i
) must themselves be a valid dis-
tribution over context words. So, after each vec-
tor composition the history vector is normalized
as follows:
h
i
=
?
h
i
?
j
?
h
j
? p(c
i
)
(16)
Equations (13)?(16) define a language model
that incorporates vector composition. To generate
probability estimates, it requires a set of word vec-
tors whose components are based on the ratio of
probabilities described by equation (4).
Our discussion thus far has assumed a spatial
semantic space model similar to that employed in
Mitchell and Lapata (2008). However, there is no
reason why the vectors should not be constructed
by some other means. As mentioned earlier, in the
LDA topic model, words are represented as dis-
tributions over topics. These distributions are es-
sentially components of a vector v corresponding
to the target word for which we wish to construct
a semantic representation. Analogously to equa-
tion (4), we convert these probabilities to ratios of
probabilities:
v
i
=
p(topic
i
|target)
p(topic
i
)
(17)
Integrating with Other LanguageModels The
models defined above are based on little more than
semantic coherence. As such they will be only
weakly predictive, since they largely ignore word
order, which n-grammodels primarily exploit. The
simplest means to integrate semantic information
with a standard language model involves combin-
ing two probability estimates as a weighted sum:
p(w|h) = ?
1
p
1
(w|h)+(1??)p
2
(w|h) (18)
Linear interpolation is guaranteed to produce
valid probabilities, and has been used, for exam-
ple, to integrate structured language models with
n-gram models (Roark, 2001). However, it will
work best when the models being combined are
roughly equally predictive and have complemen-
tary strengths and weaknesses. If one model is
much weaker than the other, linear interpolation
will typically produce a model of intermediate
strength (i.e., worse than the better model), with
the weaker model contributing a form of smooth-
ing at best.
Therefore, based on equation (13), we express
our semantic probabilities as the product of the
unigram probability, p(w), and a semantic com-
ponent, ?, which determines the factor by which
this probability should be scaled up or down given
the context in which it occurs.
p(w|h) = p(w) ??(w,h) (19)
?(w,h) =
?
i
p(c
i
|w)
p(c
i
)
p(c
i
|h)
p(c
i
)
p(c
i
) (20)
Thus, it seems reasonable to integrate the n-gram
model by replacing the unigram probabilities with
the n-gram versions.
2
p?(w
n
) = p(w
n
|w
n?1
n?2
) ??(w
n
,h) (21)
To obtain a true probability estimate we normalize
p?(w
n
) by dividing through the sum of all word
probabilities:
p(w
n
|w
n?1
n?2
,h) =
p?(w
n
)
?
w
p?(w)
(22)
In integrating our semantic model with an n-gram
model, we allow the latter to handle short range
dependencies and have the former handle the
longer dependencies outside the n-gram window.
For this reason, the history h used by the semantic
model in the prediction of w
n
only includes words
up to w
n?3
(i.e., only words outside the n-gram).
We also integrate our models with a structured
language model (Roark, 2001). However, in this
case we use linear interpolation (equation (18))
because the models are roughly equally predic-
tive and also because linear interpolation is widely
used when structured language models are com-
bined with n-grams and other information sources.
This approach also has the benefit of allowing the
2
Equation (21) can also be expressed as p(w
n
|w
n?1
n?2
,h) ?
p(w
n
|w
n?1
n?2
)p(w
n
|h)
p(w
n
)
, Which is equivalent to assuming that h is
conditionally independent of w
n?1
n?2
(Gildea and Hofmann,
1999).
435
models to be combined without out the need to
renormalize the probabilities. In the case of the
structured language model, normalizing across the
whole vocabulary would be prohibitive.
5 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the models pre-
sented above. We give details on our training pro-
cedure and parameter estimation, and present the
methods used for comparison with our approach.
Method Following previous work (e.g., Belle-
garda (2000)) we integrated our compositional
language models with a standard n-gram model
(see equation (21)). We experimented with addi-
tive and multiplicative composition functions, and
two semantic representations (LDA and the sim-
pler semantic space model), resulting in four com-
positional models. In addition, we compared our
models against a state of the art structured lan-
guage model in order to assess the extent to which
the information provided by the semantic repre-
sentation is complementary to syntactic structure.
Our experiments used Roark?s (2001) grammar-
based language model. Similarly to standard lan-
guage models, it computes the probability of the
next word based upon the previous words of the
sentence. This is done by computing a subset of all
possible grammatical relations for the prior words
and then estimating the probability of the next
grammatical structure and the probability of see-
ing the next word given each of the prior gram-
matical relations. When estimating the probability
of the next word, the model conditions on the two
prior heads of constituents, thereby using informa-
tion about word triples (like a trigram model).
All our models were evaluated by computing
perplexity on the test set. Roughly, this quanti-
fies the degree of unpredictability in a probabil-
ity distribution, such that a fair k-sided dice would
have a perplexity of k. More precisely, perplexity
is the reciprocal of the geometric average of the
word probabilities and a lower score indicates bet-
ter predictions.
Parameter Estimation The compositional lan-
guage models were trained on the BLLIP corpus,
a collection of texts from the Wall Street Journal
(years 1987?89). The training corpus consisted of
38,521,346 words. We used a development corpus
of 50,006 words and a test corpus of similar size.
All words were converted to lowercase and num-
bers were replaced with the symbol ?num?. A vo-
cabulary of 20,000 words was chosen and the re-
maining tokens were replaced with ?unk?.
Following Mitchell and Lapata (2008), we con-
structed a simple semantic space based on co-
occurrence statistics from the BLLIP training set.
We used the 2,000 most frequent word types as
contexts and a symmetric five word window. Vec-
tor components were defined as in equation (4).
Contrary to our earlier work, we did not lemma-
tize the corpus before constructing the vectors as
in the context of language modeling this was not
appropriate. We also trained the LDA model on
BLLIP, using Blei et al?s (2003) implementation.
3
We experimented with different numbers of topics
on the development set (from 10 to 200) and re-
port results on the test set with 100 topics. In our
experiments, the hyperparameter ? was initialized
to 0.5, and the ? word probabilities were initial-
ized randomly.
We integrated our compositional models with a
trigram model which we also trained on BLLIP.
The model was built using the SRILM toolkit
(Stolcke, 2002) with backoff and Good-Turing
smoothing. Ideally, we would have liked to train
Roark?s (2001) parser on the same data as that
used for the semantic models. However, this would
require a gold standard treebank several times
larger than those currently available. Following
previous work on structured language modeling
(Roark, 2001; Charniak, 2001; Chelba and Jelinek,
1998), we therefore trained the parser on sections
2?21 of the Penn Treebank containing 936,017
words. Note that Roark?s (2001) parser produces
prefix probabilities for each word of a sentence
which we converted to conditional probabilities by
dividing each current probability by the previous
one.
6 Results
Table 1 shows perplexity results when the com-
positional models are combined with an n-gram
model. With regard to the simple semantic space
model (SSM) we observe that both additive and
multiplicative approaches to constructing history
are successful in reducing perplexity over the
n-gram baseline, with the multiplicative model
outperforming the additive one. This confirms the
3
Available from http://www.cs.princeton.edu/
?blei/lda-c/index.html.
436
Model Perplexity
n-gram 78.72
n-gram+Add
SSM
76.65
n-gram + Multiply
SSM
75.01
n-gram+Add
LDA
76.60
n-gram+Multiply
LDA
123.93
parser 173.35
n-gram + parser 75.22
n-gram + parser + Add
SSM
73.45
n-gram + parser + Multiply
SSM
71.32
n-gram + parser + Add
LDA
71.58
n-gram + parser + Multiply
LDA
87.93
Table 1: Perplexities for n-gram, composition and
structured language models, and their combina-
tions; subscripts
SSM
and
LSA
refer to the semantic
space and LDA models, respectively.
hypothesis that for this type of semantic space the
multiplicative vector combination function pro-
duces representations which have a sounder prob-
abilistic basis.
The results for the LDA model are also reported
in the table. This model reduces perplexity with an
additive composition function, but performs worse
than the n-gram with a multiplicative function. For
comparison, Figure 1 plots the perplexity of the
combined LDA and n-gram models against the
number of topics. Increasing the number of top-
ics produces higher dimensional representations
which ought to be richer, more detailed and there-
fore more predictive. While this is true for the
additive model, a greater number of topics actu-
ally increases the perplexity of the multiplicative
model, indicating it has become less predictive.
We compared these perplexity reductions
against those obtained with a structured lan-
guage model. Following Roark (2001), we com-
bined the structured language model with a
trigram model using linear interpolation (the
weights were optimized on the development
set). This model (n-gram + parser) performs
comparably to our best compositional model
(n-gram + Multiply
SSM
). While both models in-
corporate long range dependencies, the parser is
trained on a hand annotated treebank, whereas the
compositional model uses raw text, albeit from
a larger corpus. Interestingly, when interpolating
the trigram with the parser and the compositional
models, we obtain additional perplexity reduc-
tions. This suggests that the semantic models are
Figure 1: Perplexity versus Number of Topics for
the LDA models using additive and multiplicative
composition functions.
encoding useful predictive information about long
range dependencies, which is distinct from and po-
tentially complementary to the parser?s syntactic
information about such dependencies. Note that
the semantic space multiplicative model yields the
highest perplexity reduction in this suite of exper-
iments followed by the LDA additive model.
7 Conclusions
In this paper we advocated the use of vector
composition models for language modeling. Us-
ing semantic representations of words outside the
n-gram window, we enhanced a trigram model
with longer range dependencies. We compared
composition models based on addition and multi-
plication and examined the influence of the under-
lying semantic space on the composition task. Our
results indicate that the multiplicative composition
function produced the most predictive representa-
tions with a simple semantic space. Interestingly,
its effect in the LDA setting was detrimental. In-
creasing the representational power of the LDA
model, by using a greater number of topics, ren-
dered the multiplicative model less predictive.
These results, together with the basic mathe-
matical structure of the LDA model, suggest that
it may not be well suited to forming represen-
tations for word sequences. In particular, the as-
sumption that words are generated independently
within documents prevents the interactions be-
tween words being modeled. This assumption,
along with the Dirichlet prior on document distri-
butions tends to lead to highly sparse word vec-
437
tors, with a typical word being strongly associated
with only one or two topics. Multiplication of a
number of these vectors generally produces a vec-
tor in which most of these associations have been
obliterated by the sparse components, resulting in
a representation with little predictive power.
These shortcomings arise from the mathemati-
cal formulation of LDA, which is not directed at
modeling the semantic interaction between words.
An interesting future direction would be to opti-
mize the vector components of the probabilistic
model over a suitable training corpus, in order to
derive a vector model of semantics adapted specif-
ically to the task of composition. We also plan to
investigate more sophisticated composition mod-
els that take syntactic structure into account. Our
results on interpolating the compositional mod-
els with a parser indicate that there is substantial
mileage to be gained by combining syntactic and
semantic dependencies.
Acknowledgements We are grateful to Brian
Roark for making his parser available to us.
Thanks to Frank Keller and Victor Lavrenko
for insightful comments and suggestions. This
work was supported by the Economic and So-
cial Research Council [grant number PTA-030-
2006-00341] and the Engineering and Physi-
cal Sciences Research Council [grant number
GR/T04540/01].
References
Jerome R. Bellegarda. 2000. Exploiting latent se-
mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279?1296.
Michael W. Berry, Susan T. Dumais, and Gavin W.
O?Brien. 1994. Using linear algebra for intelligent
information retrieval. SIAM Review, 37(4):573?595.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
J.A. Bullinaria and J.P. Levy. 2007. Extracting seman-
tic representations from word co-occurrence statis-
tics: A computational study. Behavior Research
Methods, 39:510?526.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of 35th Annual
Meeting of the Association for Computational Lin-
guistics and 8th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 116?123, Toulouse, France.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In
Proceedings of the 17th International Conference on
Computational Linguistics and 36th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 225?231, Montr?eal, Canada.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
2nd Symposium on Quantum Interaction, pages
133?140, Oxford, UK. College Publications.
Noah Coccaro and Daniel Jurafsky. 1998. Towards
better integration of semantic predictors in satistical
language modeling. In Proceedings of the 5th Inter-
national Conference on Spoken Language Process-
ing, pages 2403?2406, Sydney, Australia.
Yonggang Deng and Sanjeev Khundanpur. 2003. La-
tent semantic information in maximum entropy lan-
guage models for conversational speech recognition.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56?63, Edmonton, AL.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906, Honolulu, Hawaii.
J. R. Firth. 1957. A synopsis of linguistic theory 1930?
1955. In Studies in Linguistic Analysis, pages 1?32.
Philological Society, Oxford.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence
with latent semantic analysis. Discourse Process,
15:285?307.
Daniel Gildea and Thomas Hofmann. 1999. Topic-
based language models using EM. In Proceedings of
the 6th European Conference on Speech Communi-
ation and Technology, pages 2167?2170, Budapest,
Hungary.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211?244.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 41(2):177?196.
Dharmendra Kanejiya, Arun Kumar, and Surendra
Prasad. 2004. Statistical language modeling with
performance benchmarks using various levels of
438
syntactic-semantic information. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 1161?1167, Geneva, Switzerland.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Roland Kuhn and Renato de Mori. 1992. A cache
based natural language model for speech recogni-
tion. IEEE Transactions on Pattern Analysis and
Machine Intelligence, (14):570?583.
T. K. Landauer and S. T. Dumais. 1997. A solution
to Plato?s problem: the latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211?240.
Will Lowe. 2000. Topographic Maps of Semantic
Space. Ph.D. thesis, University of Edinburgh.
Scott McDonald. 2000. Environmental Determinants
of Lexical Processing Effort. Ph.D. thesis, Univer-
sity of Edinburgh.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, OH.
R. Montague. 1974. English as a formal language. In
R. Montague, editor, Formal Philosophy. Yale Uni-
versity Press, New Haven, CT.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
B. Partee. 1995. Lexical semantics and compositional-
ity. In Lila Gleitman and Mark Liberman, editors,
Invitation to Cognitive Science Part I: Language,
pages 311?360. MIT Press, Cambridge, MA.
S. Pinker. 1994. The Language Instinct: How the Mind
Creates Language. HarperCollins, New York.
Tony A. Plate. 1995. Holographic reduced represen-
tations. IEEE Transactions on Neural Networks,
6(3):623?641.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Roni Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10:187?228.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures
in connectionist systems. Artificial Intelligence,
46:159?216.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904, Denver, CO.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with seman-
tic information for a word prediction component.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 506?513, Prague, Czech
Republic.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
2nd Symposium on Quantum Interaction, Oxford,
UK. College Publications.
Rong Zhang and Alexander I. Rudnicky. 2002. Im-
prove latent semantic analysis based language model
by integrating multiple level knowldege. In Pro-
ceedings of the 7th International Conference on Spo-
ken Language Processing, pages 893?897, Denver,
CO.
439
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 103?111,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Bayesian Word Sense Induction
Samuel Brody
Dept. of Biomedical Informatics
Columbia University
samuel.brody@dbmi.columbia.edu
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
Sense induction seeks to automatically
identify word senses directly from a cor-
pus. A key assumption underlying pre-
vious work is that the context surround-
ing an ambiguous word is indicative of
its meaning. Sense induction is thus typ-
ically viewed as an unsupervised cluster-
ing problem where the aim is to partition
a word?s contexts into different classes,
each representing a word sense. Our work
places sense induction in a Bayesian con-
text by modeling the contexts of the am-
biguous word as samples from a multi-
nomial distribution over senses which
are in turn characterized as distributions
over words. The Bayesian framework pro-
vides a principled way to incorporate a
wide range of features beyond lexical co-
occurrences and to systematically assess
their utility on the sense induction task.
The proposed approach yields improve-
ments over state-of-the-art systems on a
benchmark dataset.
1 Introduction
Sense induction is the task of discovering automat-
ically all possible senses of an ambiguous word. It
is related to, but distinct from, word sense disam-
biguation (WSD) where the senses are assumed to
be known and the aim is to identify the intended
meaning of the ambiguous word in context.
Although the bulk of previous work has been
devoted to the disambiguation problem1, there are
good reasons to believe that sense induction may
be able to overcome some of the issues associ-
ated with WSD. Since most disambiguation meth-
ods assign senses according to, and with the aid
1Approaches to WSD are too numerous to list; We refer
the interested reader to Agirre et al (2007) for an overview
of the state of the art.
of, dictionaries or other lexical resources, it is dif-
ficult to adapt them to new domains or to lan-
guages where such resources are scarce. A re-
lated problem concerns the granularity of the sense
distinctions which is fixed, and may not be en-
tirely suitable for different applications. In con-
trast, when sense distinctions are inferred directly
from the data, they are more likely to represent
the task and domain at hand. There is little risk
that an important sense will be left out, or that ir-
relevant senses will influence the results. Further-
more, recent work in machine translation (Vickrey
et al, 2005) and information retrieval (Ve?ronis,
2004) indicates that induced senses can lead to im-
proved performance in areas where methods based
on a fixed sense inventory have previously failed
(Carpuat and Wu, 2005; Voorhees, 1993).
Sense induction is typically treated as an un-
supervised clustering problem. The input to the
clustering algorithm are instances of the ambigu-
ous word with their accompanying contexts (rep-
resented by co-occurrence vectors) and the output
is a grouping of these instances into classes cor-
responding to the induced senses. In other words,
contexts that are grouped together in the same
class represent a specific word sense. In this paper
we adopt a novel Bayesian approach and formalize
the induction problem in a generative model. For
each ambiguous word we first draw a distribution
over senses, and then generate context words ac-
cording to this distribution. It is thus assumed that
different senses will correspond to distinct lexical
distributions. In this framework, sense distinctions
arise naturally through the generative process: our
model postulates that the observed data (word con-
texts) are explicitly intended to communicate a la-
tent structure (their meaning).
Our work is related to Latent Dirichlet Allo-
cation (LDA, Blei et al 2003), a probabilistic
model of text generation. LDA models each doc-
ument using a mixture over K topics, which are
in turn characterized as distributions over words.
103
The words in the document are generated by re-
peatedly sampling a topic according to the topic
distribution, and selecting a word given the chosen
topic. Whereas LDA generates words from global
topics corresponding to the whole document, our
model generates words from local topics chosen
based on a context window around the ambiguous
word. Document-level topics resemble general do-
main labels (e.g., finance, education) and cannot
faithfully model more fine-grained meaning dis-
tinctions. In our work, therefore, we create an in-
dividual model for every (ambiguous) word rather
than a global model for an entire document col-
lection. We also show how multiple information
sources can be straightforwardly integrated with-
out changing the underlying probabilistic model.
For instance, besides lexical information we may
want to consider parts of speech or dependen-
cies in our sense induction problem. This is in
marked contrast with previous LDA-based mod-
els which mostly take only word-based informa-
tion into account. We evaluate our model on a
recently released benchmark dataset (Agirre and
Soroa, 2007) and demonstrate improvements over
the state-of-the-art.
The remainder of this paper is structured as fol-
lows. We first present an overview of related work
(Section 2) and then describe our Bayesian model
in more detail (Sections 3 and 4). Section 5 de-
scribes the resources and evaluation methodology
used in our experiments. We discuss our results in
Section 6, and conclude in Section 7.
2 Related Work
Sense induction is typically treated as a cluster-
ing problem, where instances of a target word
are partitioned into classes by considering their
co-occurring contexts. Considerable latitude is
allowed in selecting and representing the co-
occurring contexts. Previous methods have used
first or second order co-occurrences (Purandare
and Pedersen, 2004; Schu?tze, 1998), parts of
speech (Purandare and Pedersen, 2004), and gram-
matical relations (Pantel and Lin, 2002; Dorow
and Widdows, 2003). The size of the context win-
dow also varies, it can be a relatively small, such as
two words before and after the target word (Gauch
and Futrelle, 1993), the sentence within which the
target is found (Bordag, 2006), or even larger, such
as the 20 surrounding words on either side of the
target (Purandare and Pedersen, 2004).
In essence, each instance of a target word
is represented as a feature vector which subse-
quently serves as input to the chosen clustering
method. A variety of clustering algorithms have
been employed ranging from k-means (Purandare
and Pedersen, 2004), to agglomerative clustering
(Schu?tze, 1998), and the Information Bottleneck
(Niu et al, 2007). Graph-based methods have also
been applied to the sense induction task. In this
framework words are represented as nodes in the
graph and vertices are drawn between the tar-
get and its co-occurrences. Senses are induced by
identifying highly dense subgraphs (hubs) in the
co-occurrence graph (Ve?ronis, 2004; Dorow and
Widdows, 2003).
Although LDA was originally developed as a
generative topic model, it has recently gained
popularity in the WSD literature. The inferred
document-level topics can help determine coarse-
grained sense distinctions. Cai et al (2007) pro-
pose to use LDA?s word-topic distributions as fea-
tures for training a supervised WSD system. In a
similar vein, Boyd-Graber and Blei (2007) infer
LDA topics from a large corpus, however for un-
supervised WSD. Here, LDA topics are integrated
with McCarthy et al?s (2004) algorithm. For each
target word, a topic is sampled from the docu-
ment?s topic distribution, and a word is generated
from that topic. Also, a distributional neighbor is
selected based on the topic and distributional sim-
ilarity to the generated word. Then, the word sense
is selected based on the word, neighbor, and topic.
Boyd-Graber et al (2007) extend the topic mod-
eling framework to include WordNet senses as a
latent variable in the word generation process. In
this case the model discovers both the topics of
the corpus and the senses assigned to each of its
words.
Our own model is also inspired by LDA but cru-
cially performs word sense induction, not disam-
biguation. Unlike the work mentioned above, we
do not rely on a pre-existing list of senses, and do
not assume a correspondence between our auto-
matically derived sense-clusters and those of any
given inventory.2 A key element in these previous
attempts at adapting LDA forWSD is the tendency
to remain at a high level, document-like, setting.
In contrast, we make use of much smaller units
of text (a few sentences, rather than a full doc-
ument), and create an individual model for each
(ambiguous) word type. Our induced senses are
few in number (typically less than ten). This is in
marked contrast to tens, and sometimes hundreds,
2Such a mapping is only performed to enable evaluation
and comparison with other approaches (see Section 5).
104
of topics commonly used in document-modeling
tasks.
Unlike many conventional clustering meth-
ods (e.g., Purandare and Pedersen 2004; Schu?tze
1998), our model is probabilistic; it specifies
a probability distribution over possible values,
which makes it easy to integrate and combine with
other systems via mixture or product models. Fur-
thermore, the Bayesian framework allows the in-
corporation of several information sources in a
principled manner. Our model can easily handle an
arbitrary number of feature classes (e.g., parts of
speech, dependencies). This functionality in turn
enables us to evaluate which linguistic informa-
tion matters for the sense induction task. Previous
attempts to handle multiple information sources
in the LDA framework (e.g., Griffiths et al 2005;
Barnard et al 2003) have been task-specific and
limited to only two layers of information. Our
model provides this utility in a general framework,
and could be applied to other tasks, besides sense
induction.
3 The Sense Induction Model
The core idea behind sense induction is that con-
textual information provides important cues re-
garding a word?s meaning. The idea dates back to
(at least) Firth (1957) (?You shall know a word by
the company it keeps?), and underlies most WSD
and lexicon acquisition work to date. Under this
premise, we should expect different senses to be
signaled by different lexical distributions.
We can place sense induction in a probabilis-
tic setting by modeling the context words around
the ambiguous target as samples from a multino-
mial sense distribution. More formally, we will
write P(s) for the distribution over senses s of
an ambiguous target in a specific context win-
dow and P(w|s) for the probability distribution
over context words w given sense s. Each word wi
in the context window is generated by first sam-
pling a sense from the sense distribution, then
choosing a word from the sense-context distribu-
tion. P(si = j) denotes the probability that the jth
sense was sampled for the ith word token and
P(wi|si = j) the probability of context word wi un-
der sense j. The model thus specifies a distribution
over words within a context window:
P(wi) =
S
?
j=1
P(wi|si = j)P(si = j) (1)
where S is the number of senses. We assume that
each target word hasC contexts and each context c
? ? s w Nc
C
?(?)
Figure 1: Bayesian sense induction model; shaded
nodes represent observed variables, unshaded
nodes indicate latent variables. Arrows indi-
cate conditional dependencies between variables,
whereas plates (the rectangles in the figure) refer
to repetitions of sampling steps. The variables in
the lower right corner refer to the number of sam-
ples.
consists of Nc word tokens. We shall write ?( j) as a
shorthand for P(wi|si = j), the multinomial distri-
bution over words for sense j, and ?(c) as a short-
hand for the distribution of senses in context c.
Following Blei et al (2003) we will assume that
the mixing proportion over senses ? is drawn from
a Dirichlet prior with parameters ?. The role of
the hyperparameter ? is to create a smoothed sense
distribution. We also place a symmetric Dirichlet ?
on ? (Griffiths and Steyvers, 2002). The hyper-
parmeter ? can be interpreted as the prior observa-
tion count on the number of times context words
are sampled from a sense before any word from
the corpus is observed. Our model is represented
in graphical notation in Figure 1.
The model sketched above only takes word in-
formation into account. Methods developed for su-
pervised WSD often use a variety of information
sources based not only on words but also on lem-
mas, parts of speech, collocations and syntactic re-
lationships (Lee and Ng, 2002). The first idea that
comes to mind, is to use the same model while
treating various features as word-like elements. In
other words, we could simply assume that the con-
texts we wish to model are the union of all our
features. Although straightforward, this solution
is undesirable. It merges the distributions of dis-
tinct feature categories into a single one, and is
therefore conceptually incorrect, and can affect the
performance of the model. For instance, parts-of-
speech (which have few values, and therefore high
probability), would share a distribution with words
(which are much sparser). Layers containing more
elements (e.g. 10 word window) would overwhelm
105
? ?
s f Nc1
C
s f Nc2...
s f Ncn
?1(?1)
?2(?2)
?n(?n)
Figure 2: Extended sense induction model; inner
rectangles represent different sources (layers) of
information. All layers share the same, instance-
specific, sense distribution (?), but each have their
own (multinomial) sense-feature distribution (?).
Shaded nodes represent observed features f ; these
can be words, parts of speech, collocations or de-
pendencies.
smaller ones (e.g. 1 word window).
Our solution is to treat each information source
(or feature type) individually and then combine
all of them together in a unified model. Our un-
derlying assumption is that the context window
around the target word can have multiple represen-
tations, all of which share the same sense distribu-
tion.We illustrate this in Figure 2 where each inner
rectangle (layer) corresponds to a distinct feature
type. We will naively assume independence be-
tween multiple layers, even though this is clearly
not the case in our task. The idea here is to model
each layer as faithfully as possible to the empirical
data while at the same time combining information
from all layers in estimating the sense distribution
of each target instance.
4 Inference
Our inference procedure is based on Gibbs sam-
pling (Geman and Geman, 1984). The procedure
begins by randomly initializing all unobserved
random variables. At each iteration, each random
variable si is sampled from the conditional distri-
bution P(si|s?i) where s?i refers to all variables
other than si. Eventually, the distribution over sam-
ples drawn from this process will converge to the
unconditional joint distribution P(s) of the unob-
served variables (provided certain criteria are ful-
filled).
In our model, each element in each layer is a
variable, and is assigned a sense label (see Fig-
ure 2, where distinct layers correspond to differ-
ent representations of the context around the tar-
get word). From these assignments, we must de-
termine the sense distribution of the instance as a
whole. This is the purpose of the Gibbs sampling
procedure. Specifically, in order to derive the up-
date function used in the Gibbs sampler, we must
provide the conditional probability of the i-th vari-
able being assigned sense si in layer l, given the
feature value fi of the context variable and the cur-
rent sense assignments of all the other variables in
the data (s?i):
p(si|s?i, f ) ? p( fi|s, f?i,?) ? p(si|s?i,?) (2)
The probability of a single sense assignment, si,
is proportional to the product of the likelihood (of
feature fi, given the rest of the data) and the prior
probability of the assignment.
(3)
p( fi|s, f?i,?) =
Z
p( fi|l,s,?) ? p(?| f?i,?l)d?=
#( fi,si)+?l
#(si)+Vl ??l
For the likelihood term p( fi|s, f?i,?), integrating
over all possible values of the multinomial feature-
sense distribution ? gives us the rightmost term in
Equation 3, which has an intuitive interpretation.
The term #( fi,si) indicates the number of times
the feature-value fi was assigned sense si in the
rest of the data. Similarly, #(si) indicates the num-
ber of times the sense assignment si was observed
in the data. ?l is the Dirichlet prior for the feature-
sense distribution ? in the current layer l, and Vl
is the size of the vocabulary of that layer, i.e., the
number of possible feature values in the layer. In-
tuitively, the probability of a feature-value given
a sense is directly proportional to the number of
times we have seen that value and that sense-
assignment together in the data, taking into ac-
count a pseudo-count prior, expressed through ?.
This can also be viewed as a form of smoothing.
A similar approach is taken with regards to the
prior probability p(si|s?i,?). In this case, how-
ever, all layers must be considered:
p(si|s?i,?) =?
l
?l ? p(si|l,s?i,?l) (4)
106
Here ?l is the weight for the contribution of layer l,
and ?l is the portion of the Dirichlet prior for the
sense distribution ? in the current layer. Treating
each layer individually, we integrate over the pos-
sible values of ?, obtaining a similar count-based
term:
(5)
p(si|l,s?i,?l) =
Z
p(si|l,s?i,?) ? p(?| f?i,?l)d?=
#l(si)+?l
#l+S ??l
where #l(si) indicates the number of elements in
layer l assigned the sense si, #l indicates the num-
ber of elements in layer l, i.e., the size of the layer
and S the number of senses.
To distribute the pseudo counts represented by
? in a reasonable fashion among the layers, we
define ?l = #l#m ?? where #m = ?l #l, i.e., the total
size of the instance. This distributes ? according
to the relative size of each layer in the instance.
p(si|l,s?i,?l)=
#l(si)+ #l#m ??
#l+S ? #l#m ??
=
#m ? #l(si)#l +?
#m+S ??
(6)
Placing these values in Equation 4 we obtain the
following:
p(si|s?i,?) =
#m ??l ?l ?
#l(si)
#l +?
#m+S ??
(7)
Putting it all together, we arrive at the final update
equation for the Gibbs sampling:
p(si|s?i, f )?
#( fi,si)+?l
#(si)+Vl ??l
?
#m ??l ?l ?
#l(si)
#l +?
#m+S ??
(8)
Note that when dealing with a single layer, Equa-
tion 8 collapses to:
p(si|s?i, f ) ?
#( fi,si)+?
#(si)+V ??
?
#m(si)+?
#m+S ??
(9)
where #m(si) indicates the number of elements
(e.g., words) in the context window assigned to
sense si. This is identical to the update equation
in the original, word-based LDA model.
The sampling algorithm gives direct estimates
of s for every context element. However, in view
of our task, we are more interested in estimating ?,
the sense-context distribution which can be ob-
tained as in Equation 7, but taking into account
all sense assignments, without removing assign-
ment i. Our system labels each instance with the
single, most probable sense.
5 Evaluation Setup
In this section we discuss our experimental set-up
for assessing the performance of the model pre-
sented above. We give details on our training pro-
cedure, describe our features, and explain how our
system output was evaluated.
Data In this work, we focus solely on inducing
senses for nouns, since they constitute the largest
portion of content words. For example, nouns rep-
resent 45% of the content words in the British Na-
tional Corpus. Moreover, for many tasks and ap-
plications (e.g., web queries, Jansen et al 2000)
nouns are the most frequent and most important
part-of-speech.
For evaluation, we used the Semeval-2007
benchmark dataset released as part of the sense
induction and discrimination task (Agirre and
Soroa, 2007). The dataset contains texts from the
Penn Treebank II corpus, a collection of articles
from the first half of the 1989 Wall Street Jour-
nal (WSJ). It is hand-annotated with OntoNotes
senses (Hovy et al, 2006) and has 35 nouns. The
average noun ambiguity is 3.9, with a high (almost
80%) skew towards the predominant sense. This is
not entirely surprising since OntoNotes senses are
less fine-grained than WordNet senses.
We used two corpora for training as we wanted
to evaluate our model?s performance across differ-
ent domains. The British National Corpus (BNC)
is a 100 million word collection of samples of
written and spoken language from a wide range of
sources including newspapers, magazines, books
(both academic and fiction), letters, and school es-
says as well as spontaneous conversations. This
served as our out-of-domain corpus, and con-
tained approximately 730 thousand instances of
the 35 target nouns in the Semeval lexical sample.
The second, in-domain, corpus was built from se-
lected portions of the Wall Street Journal. We used
all articles (excluding the Penn Treebank II por-
tion used in the Semeval dataset) from the years
1987-89 and 1994 to create a corpus of similar size
to the BNC, containing approximately 740 thou-
sand instances of the target words.
Additionally, we used the Senseval 2 and 3 lex-
ical sample data (Preiss and Yarowsky, 2001; Mi-
halcea and Edmonds, 2004) as development sets,
for experimenting with the hyper-parameters of
our model (see Section 6).
Evaluation Methodology Agirre and Soroa
(2007) present two evaluation schemes for as-
sessing sense induction methods. Under the first
107
scheme, the system output is compared to the
gold standard using standard clustering evalua-
tion metrics (e.g., purity, entropy). Here, no at-
tempt is made to match the induced senses against
the labels of the gold standard. Under the second
scheme, the gold standard is partitioned into a test
and training corpus. The latter is used to derive a
mapping of the induced senses to the gold stan-
dard labels. The mapping is then used to calculate
the system?s F-Score on the test corpus.
Unfortunately, the first scheme failed to dis-
criminate among participating systems. The one-
cluster-per-word baseline outperformed all sys-
tems, except one, which was only marginally bet-
ter. The scheme ignores the actual labeling and
due to the dominance of the first sense in the data,
encourages a single-sense approach which is fur-
ther amplified by the use of a coarse-grained sense
inventory. For the purposes of this work, there-
fore, we focused on the second evaluation scheme.
Here, most of the participating systems outper-
formed the most-frequent-sense baseline, and the
rest obtained only slightly lower scores.
Feature Space Our experiments used a feature
set designed to capture both immediate local con-
text, wider context and syntactic context. Specifi-
cally, we experimented with six feature categories:
?10-word window (10w),?5-word window (5w),
collocations (1w), word n-grams (ng), part-of-
speech n-grams (pg) and dependency relations
(dp). These features have been widely adopted in
various WSD algorithms (see Lee and Ng 2002 for
a detailed evaluation). In all cases, we use the lem-
matized version of the word(s).
The Semeval workshop organizers provided a
small amount of context for each instance (usu-
ally a sentence or two surrounding the sentence
containing the target word). This context, as well
as the text in the training corpora, was parsed us-
ing RASP (Briscoe and Carroll, 2002), to extract
part-of-speech tags, lemmas, and dependency in-
formation. For instances containing more than one
occurrence of the target word, we disambiguate
the first occurrence. Instances which were not cor-
rectly recognized by the parser (e.g., a target word
labeled with the wrong lemma or part-of-speech),
were automatically assigned to the largest sense-
cluster.3
3This was the case for less than 1% of the instances.
3 4 5 6 7 8 9Number of Senses83
84
85
86
87
88
F
-
S
c
o
r
e
 
(
%
)
In-Domain (WSJ)Out-of-Domain (BNC)
Figure 3: Model performance with varying num-
ber of senses on the WSJ and BNC corpora.
6 Experiments
Model Selection The framework presented in
Section 3 affords great flexibility in modeling the
empirical data. This however entails that several
parameters must be instantiated. More precisely,
our model is conditioned on the Dirichlet hyper-
parameters ? and ? and the number of senses S.
Additional parameters include the number of iter-
ations for the Gibbs sampler and whether or not
the layers are assigned different weights.
Our strategy in this paper is to fix ? and ?
and explore the consequences of varying S. The
value for the ? hyperparameter was set to 0.02.
This was optimized in an independent tuning ex-
periment which used the Senseval 2 (Preiss and
Yarowsky, 2001) and Senseval 3 (Mihalcea and
Edmonds, 2004) datasets. We experimented with
? values ranging from 0.005 to 1. The ? parame-
ter was set to 0.1 (in all layers). This value is often
considered optimal in LDA-related models (Grif-
fiths and Steyvers, 2002). For simplicity, we used
uniform weights for the layers. The Gibbs sampler
was run for 2,000 iterations. Due to the random-
ized nature of the inference procedure, all reported
results are average scores over ten runs.
Our experiments used the same number of
senses for all the words, since tuning this number
individually for each word would be prohibitive.
We experimented with values ranging from three
to nine senses. Figure 3 shows the results obtained
for different numbers of senses when the model is
trained on the WSJ (in-domain) and BNC (out-of-
domain) corpora, respectively. Here, we are using
the optimal combination of layers for each system
(which we discuss in the following section in de-
108
Senses of drug (WSJ)
1. U.S., administration, federal, against, war, dealer
2. patient, people, problem, doctor, company, abuse
3. company, million, sale, maker, stock, inc.
4. administration, food, company, approval, FDA
Senses of drug (BNC)
1. patient, treatment, effect, anti-inflammatory
2. alcohol, treatment, patient, therapy, addiction
3. patient, new, find, effect, choice, study
4. test, alcohol, patient, abuse, people, crime
5. trafficking, trafficker, charge, use, problem
6. abuse, against, problem, treatment, alcohol
7. people, wonder, find, prescription, drink, addict
8. company, dealer, police, enforcement, patient
Table 1: Senses inferred for the word drug from
the WSJ and BNC corpora.
tail). For the model trained on WSJ, performance
peaks at four senses, which is similar to the av-
erage ambiguity in the test data. For the model
trained on the BNC, however, the best results are
obtained using twice as many senses. Using fewer
senses with the BNC-trained system can result in
a drop in accuracy of almost 2%. This is due to
the shift in domain. As the sense-divisions of the
learning domain do not match those of the target
domain, finer granularity is required in order to en-
compass all the relevant distinctions.
Table 1 illustrates the senses inferred for the
word drug when using the in-domain and out-of-
domain corpora, respectively. The most probable
words for each sense are also shown. Firstly, note
that the model infers some plausible senses for
drug on the WSJ corpus (top half of Table 1).
Sense 1 corresponds to the ?enforcement? sense
of drug, Sense 2 refers to ?medication?, Sense 3
to the ?drug industry? and Sense 4 to ?drugs re-
search?. The inferred senses for drug on the BNC
(bottom half of Table 1) are more fine grained. For
example, the model finds distinct senses for ?med-
ication? (Sense 1 and 7) and ?illegal substance?
(Senses 2, 4, 6, 7). It also finds a separate sense
for ?drug dealing? (Sense 5) and ?enforcement?
(Sense 8). Because the BNC has a broader fo-
cus, finer distinctions are needed to cover as many
senses as possible that are relevant to the target do-
main (WSJ).
Layer Analysis We next examine which indi-
vidual feature categories are most informative
in our sense induction task. We also investigate
whether their combination, through our layered
1-Layer
10w 86.9
5w 86.8
1w 84.6
ng 83.6
pg 82.5
dp 82.2
MFS 80.9
5-Layers
-10w 83.1
-5w 83.0
-1w 83.0
-ng 83.0
-pg 82.7
-dp 84.7
all 83.3
Combination
10w+5w 87.3%
5w+pg 83.9%
1w+ng 83.2%
10w+pg 83.3%
1w+pg 84.5%
10w+pg+dep 82.2%
MFS 80.9%
Table 2: Model performance (F-score) on the WSJ
with one layer (left), five layers (middle), and se-
lected combinations of layers (right).
model (see Figure 2), yields performance im-
provements. We used 4 senses for the system
trained on WSJ and 8 for the system trained on
the BNC (? was set to 0.02 and ? to 0.1)
Table 2 (left side) shows the performance of our
model when using only one layer. The layer com-
posed of words co-occurring within a ?10-word
window (10w), and representing wider, topical, in-
formation gives the highest scores on its own. It
is followed by the ?5 (5w) and ?1 (1w) word
windows, which represent more immediate, local
context. Part-of-speech n-grams (pg) and word n-
grams (ng), on their own, achieve lower scores,
largely due to over-generalization and data sparse-
ness, respectively. The lowest-scoring single layer
is the dependency layer (dp), with performance
only slightly above the most-frequent-sense base-
line (MFS). Dependency information is very infor-
mative when present, but extremely sparse.
Table 2 (middle) also shows the results obtained
when running the layered model with all but one
of the layers as input. We can use this informa-
tion to determine the contribution of each layer by
comparing to the combined model with all layers
(all). Because we are dealing with multiple lay-
ers, there is an element of overlap involved. There-
fore, each of the word-window layers, despite rel-
atively high informativeness on its own, does not
cause as much damage when it is absent, since
the other layers compensate for the topical and lo-
cal information. The absence of the word n-gram
layer, which provides specific local information,
does not make a great impact when the 1w and pg
layers are present. Finally, we can see that the ex-
tremely sparse dependency layer is detrimental to
the multi-layer model as a whole, and its removal
increases performance. The sparsity of the data in
this layer means that there is often little informa-
tion on which to base a decision. In these cases,
the layer contributes a close-to-uniform estimation
109
1-Layer
10w 84.6
5w 84.6
1w 83.6
pg 83.1
ng 82.8
dp 81.1
MFS 80.9
5-Layers
-10w 83.3
-5w 82.8
-1w 83.5
-pg 83.2
-ng 82.9
-dp 84.7
all 84.1
Combination
10w+5w 85.5%
5w+pg 83.5%
1w+ng 83.5%
10w+pg 83.4%
1w+pg 84.1%
10w+pg+dep 81.7%
MFS 80.9%
Table 3: Model performance (F-score) on the BNC
with one layer (left), five layers (middle), and se-
lected combinations of layers (right).
of the sense distribution, which confuses the com-
bined model.
Other layer combinations obtained similar re-
sults. Table 2 (right side) shows the most informa-
tive two and three layer combinations. Again, de-
pendencies tend to decrease performance. On the
other hand, combining features that have similar
performance on their own is beneficial. We obtain
the best performance overall with a two layered
model combining topical (+10w) and local (+5w)
contexts.
Table 3 replicates the same suite of experiments
on the BNC corpus. The general trends are similar.
Some interesting differences are apparent, how-
ever. The sparser layers, notably word n-grams
and dependencies, fare comparatively worse. This
is expected, since the more precise, local, infor-
mation is likely to vary strongly across domains.
Even when both domains refer to the same sense
of a word, it is likely to be used in a different
immediate context, and local contextual informa-
tion learned in one domain will be less effective
in the other. Another observable difference is that
the combined model without the dependency layer
does slightly better than each of the single layers.
The 1w+pg combination improves over its compo-
nents, which have similar individual performance.
Finally, the best performing model on the BNC
also combines two layers capturing wider (10w)
and more local (5w) contextual information (see
Table 3, right side).
Comparison to State-of-the-Art Table 4 com-
pares our model against the two best performing
sense induction systems that participated in the
Semeval-2007 competition. IR2 (Niu et al, 2007)
performed sense induction using the Information
Bottleneck algorithm, whereas UMND2 (Peder-
sen, 2007) used k-means to cluster second order
co-occurrence vectors associated with the target
System F-Score
10w, 5w (WSJ) 87.3
I2R 86.8
UMND2 84.5
MFS 80.9
Table 4: Comparison of the best-performing
Semeval-07 systems against our model.
word. These models and our own model signif-
icantly outperform the most-frequent-sense base-
line (p < 0.01 using a ?2 test). Our best sys-
tem (10w+5w on WSJ) is significantly better than
UMND2 (p < 0.01) and quantitatively better than
IR2, although the difference is not statistically sig-
nificant.
7 Discussion
This paper presents a novel Bayesian approach to
sense induction. We formulated sense induction
in a generative framework that describes how the
contexts surrounding an ambiguous word might
be generated on the basis of latent variables. Our
model incorporates features based on lexical in-
formation, parts of speech, and dependencies in a
principled manner, and outperforms state-of-the-
art systems. Crucially, the approach is not specific
to the sense induction task and can be adapted for
other applications where it is desirable to take mul-
tiple levels of information into account. For exam-
ple, in document classification, one could consider
an accompanying image and its caption as possi-
ble additional layers to the main text.
In the future, we hope to explore more rigor-
ous parameter estimation techniques. Goldwater
and Griffiths (2007) describe a method for inte-
grating hyperparameter estimation into the Gibbs
sampling procedure using a prior over possible
values. Such an approach could be adopted in our
framework, as well, and extended to include the
layer weighting parameters, which have strong po-
tential for improving the model?s performance. In
addition, we could allow an infinite number of
senses and use an infinite Dirichlet model (Teh
et al, 2006) to automatically determine how many
senses are optimal. This provides an elegant so-
lution to the model-order problem, and eliminates
the need for external cluster-validation methods.
Acknowledgments The authors acknowledge
the support of EPSRC (grant EP/C538447/1).
We are grateful to Sharon Goldwater for her feed-
back on earlier versions of this work.
110
References
Agirre, Eneko, Llu??s Ma`rquez, and Richard Wicentowski, ed-
itors. 2007. Proceedings of the SemEval-2007. Prague,
Czech Republic.
Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007. Prague, Czech
Republic, pages 7?12.
Barnard, K., P. Duygulu, D. Forsyth, N. De Freitas, D. M.
Blei, andM. I. Jordan. 2003. Matching words and pictures.
J. of Machine Learning Research 3(6):1107?1135.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learning
Research 3:993?1022.
Bordag, Stefan. 2006. Word sense induction: Triplet-based
clustering and automatic evaluation. In Proceedings of the
11th EACL. Trento, Italy, pages 137?144.
Boyd-Graber, Jordan and David Blei. 2007. Putop: Turning
predominant senses into a topic model for word sense dis-
ambiguation. In Proceedings of SemEval-2007. Prague,
Czech Republic, pages 277?281.
Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the EMNLP-CoNLL. Prague, Czech Republic,
pages 1024?1033.
Briscoe, Ted and John Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Cai, J. F., W. S. Lee, and Y. W. Teh. 2007. Improving word
sense disambiguation using topic features. In Proceedings
of the EMNLP-CoNLL. Prague, Czech Republic, pages
1015?1023.
Carpuat, Marine and Dekai Wu. 2005. Word sense disam-
biguation vs. statistical machine translation. In Proceed-
ings of the 43rd ACL. Ann Arbor, MI, pages 387?394.
Dorow, Beate and Dominic Widdows. 2003. Discovering
corpus-specific word senses. In Proceedings of the 10th
EACL. Budapest, Hungary, pages 79?82.
Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955.
Oxford: Philological Society.
Gauch, Susan and Robert P. Futrelle. 1993. Experiments in
automatic word class and word sense identification for in-
formation retrieval. In Proceedings of the 3rd Annual Sym-
posium on Document Analysis and Information Retrieval.
Las Vegas, NV, pages 425?434.
Geman, S. and D. Geman. 1984. Stochastic relaxation, Gibbs
distribution, and Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence 6(6):721?741.
Goldwater, Sharon and Tom Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In Pro-
ceedings of the 45th ACL. Prague, Czech Republic, pages
744?751.
Griffiths, Thomas L., Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and syn-
tax. In Lawrence K. Saul, Yair Weiss, and Le?on Bottou,
editors, Advances in Neural Information Processing Sys-
tems 17, MIT Press, Cambridge, MA, pages 537?544.
Griffiths, Tom L. and Mark Steyvers. 2002. A probabilistic
approach to semantic representation. In Proeedings of the
24th Annual Conference of the Cognitive Science Society.
Fairfax, VA, pages 381?386.
Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the HLT, Companion Vol-
ume: Short Papers. Association for Computational Lin-
guistics, New York City, USA, pages 57?60.
Jansen, B. J., A. Spink, and A. Pfaff. 2000. Linguistic aspects
of web queries.
Lee, Yoong Keok and Hwee Tou Ng. 2002. An empirical
evaluation of knowledge sources and learning algorithms
for word sense disambiguation. In Proceedings of the
EMNLP. Morristown, NJ, USA, pages 41?48.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant senses in untagged text.
In Proceedings of the 42nd ACL. Barcelona, Spain, pages
280?287.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of the SENSEVAL-3. Barcelona.
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2r: Three systems for word sense discrimination, chinese
word sense disambiguation, and english word sense dis-
ambiguation. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007). As-
sociation for Computational Linguistics, Prague, Czech
Republic, pages 177?182.
Pantel, Patrick and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th KDD. New
York, NY, pages 613?619.
Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the
sense induction task of senseval-4. In Proceedings of
SemEval-2007. Prague, Czech Republic, pages 394?397.
Preiss, Judita and David Yarowsky, editors. 2001. Proceed-
ings of the 2nd International Workshop on Evaluating
Word Sense Disambiguation Systems. Toulouse, France.
Purandare, Amruta and Ted Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and similarity
spaces. In Proceedings of the CoNLL. Boston, MA, pages
41?48.
Schu?tze, Hinrich. 1998. Automatic word sense discrimina-
tion. Computational Linguistics 24(1):97?123.
Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association 101(476):1566?1581.
Ve?ronis, Jean. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language
18(3):223?252.
Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne
Koller. 2005. Word-sense disambiguation for machine
translation. In Proceedings of the HLT/EMNLP. Vancou-
ver, pages 771?778.
Voorhees, Ellen M. 1993. Using wordnet to disambiguate
word senses for text retrieval. In Proceedings of the 16th
SIGIR. New York, NY, pages 171?180.
111
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 220?228,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Semi-Supervised Semantic Role Labeling
Hagen Fu?rstenau
Dept. of Computational Linguistics
Saarland University
Saarbru?cken, Germany
hagenf@coli.uni-saarland.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Large scale annotated corpora are pre-
requisite to developing high-performance
semantic role labeling systems. Unfor-
tunately, such corpora are expensive to
produce, limited in size, and may not be
representative. Our work aims to reduce
the annotation effort involved in creat-
ing resources for semantic role labeling
via semi-supervised learning. Our algo-
rithm augments a small number of man-
ually labeled instances with unlabeled ex-
amples whose roles are inferred automat-
ically via annotation projection. We for-
mulate the projection task as a generaliza-
tion of the linear assignment problem. We
seek to find a role assignment in the un-
labeled data such that the argument sim-
ilarity between the labeled and unlabeled
instances is maximized. Experimental re-
sults on semantic role labeling show that
the automatic annotations produced by our
method improve performance over using
hand-labeled instances alone.
1 Introduction
Recent years have seen a growing interest in the
task of automatically identifying and labeling the
semantic roles conveyed by sentential constituents
(Gildea and Jurafsky, 2002). This is partly due to
its relevance for applications ranging from infor-
mation extraction (Surdeanu et al, 2003; Mos-
chitti et al, 2003) to question answering (Shen and
Lapata, 2007), paraphrase identification (Pado? and
Erk, 2005), and the modeling of textual entailment
relations (Tatu and Moldovan, 2005). Resources
like FrameNet (Fillmore et al, 2003) and Prop-
Bank (Palmer et al, 2005) have also facilitated the
development of semantic role labeling methods by
providing high-quality annotations for use in train-
ing. Semantic role labelers are commonly devel-
oped using a supervised learning paradigm1 where
a classifier learns to predict role labels based on
features extracted from annotated training data.
Examples of the annotations provided in
FrameNet are given in (1). Here, the meaning of
predicates (usually verbs, nouns, or adjectives) is
conveyed by frames, schematic representations of
situations. Semantic roles (or frame elements) are
defined for each frame and correspond to salient
entities present in the situation evoked by the pred-
icate (or frame evoking element). Predicates with
similar semantics instantiate the same frame and
are attested with the same roles. In our exam-
ple, the frameCause harm has three core semantic
roles, Agent, Victim, and Body part and can be in-
stantiated with verbs such as punch, crush, slap,
and injure. The frame may also be attested with
non-core (peripheral) roles that are more generic
and often shared across frames (see the roles De-
gree, Reason, and Means, in (1c) and (1d)).
(1) a. [Lee]Agent punched [John]Victim
[in the eye]Body part.
b. [A falling rock]Cause crushed [my
ankle]Body part.
c. [She]Agent slapped [him]Victim
[hard]Degree [for his change of
mood]Reason.
d. [Rachel]Agent injured [her
friend]Victim [by closing the car
door on his left hand]Means.
The English FrameNet (version 1.3) contains
502 frames covering 5,866 lexical entries. It also
comes with a set of manually annotated exam-
ple sentences, taken mostly from the British Na-
tional Corpus. These annotations are often used
1The approaches are too numerous to list; we refer the
interested reader to the proceedings of the SemEval-2007
shared task (Baker et al, 2007) for an overview of the state-
of-the-art.
220
as training data for semantic role labeling sys-
tems. However, the applicability of these sys-
tems is limited to those words for which labeled
data exists, and their accuracy is strongly corre-
lated with the amount of labeled data available.
Despite the substantial annotation effort involved
in the creation of FrameNet (spanning approxi-
mately twelve years), the number of annotated in-
stances varies greatly across lexical items. For in-
stance, FrameNet contains annotations for 2,113
verbs; of these 12.3% have five or less annotated
examples. The average number of annotations per
verb is 29.2. Labeled data is thus scarce for indi-
vidual predicates within FrameNet?s target domain
and would presumably be even scarcer across do-
mains. The problem is more severe for languages
other than English, where training data on the
scale of FrameNet is virtually non-existent. Al-
though FrameNets are being constructed for Ger-
man, Spanish, and Japanese, these resources are
substantially smaller than their English counter-
part and of limited value for modeling purposes.
One simple solution, albeit expensive and time-
consuming, is to manually create more annota-
tions. A better alternative may be to begin with
an initial small set of labeled examples and aug-
ment it with unlabeled data sufficiently similar to
the original labeled set. Suppose we have man-
ual annotations for sentence (1a). We shall try and
find in an unlabeled corpus other sentences that
are both structurally and semantically similar. For
instance, we may think that Bill will punch me in
the face and I punched her hard in the head re-
semble our initial sentence and are thus good ex-
amples to add to our database. Now, in order to
use these new sentences as training data we must
somehow infer their semantic roles. We can prob-
ably guess that constituents in the same syntactic
position must have the same semantic role, espe-
cially if they refer to the same concept (e.g., ?body
parts?) and thus label in the face and in the head
with the role Body part. Analogously, Bill and
I would be labeled as Agent and me and her as
Victim.
In this paper we formalize the method sketched
above in order to expand a small number of
FrameNet-style semantic role annotations with
large amounts of unlabeled data. We adopt a learn-
ing strategy where annotations are projected from
labeled onto unlabeled instances via maximizing
a similarity function measuring syntactic and se-
mantic compatibility. We formalize the annotation
projection problem as a generalization of the linear
assignment problem and solve it efficiently using
the simplex algorithm. We evaluate our algorithm
by comparing the performance of a semantic role
labeler trained on the annotations produced by our
method and on a smaller dataset consisting solely
of hand-labeled instances. Results in several ex-
perimental settings show that the automatic anno-
tations, despite being noisy, bring significant per-
formance improvements.
2 Related Work
The lack of annotated data presents an obstacle
to developing many natural language applications,
especially when these are not in English. It is
therefore not surprising that previous efforts to re-
duce the need for semantic role annotation have
focused primarily on non-English languages.
Annotation projection is a popular framework
for transferring frame semantic annotations from
one language to another by exploiting the transla-
tional and structural equivalences present in par-
allel corpora. The idea here is to leverage the ex-
isting English FrameNet and rely on word or con-
stituent alignments to automatically create an an-
notated corpus in a new language. Pado? and Lap-
ata (2006) transfer semantic role annotations from
English onto German and Johansson and Nugues
(2006) from English onto Swedish. A different
strategy is presented in Fung and Chen (2004),
where English FrameNet entries are mapped to
concepts listed in HowNet, an on-line ontology
for Chinese, without consulting a parallel corpus.
Then, Chinese sentences with predicates instan-
tiating these concepts are found in a monolin-
gual corpus and their arguments are labeled with
FrameNet roles.
Other work attempts to alleviate the data re-
quirements for semantic role labeling either by re-
lying on unsupervised learning or by extending ex-
isting resources through the use of unlabeled data.
Swier and Stevenson (2004) present an unsuper-
vised method for labeling the arguments of verbs
with their semantic roles. Given a verb instance,
their method first selects a frame from VerbNet, a
semantic role resource akin to FrameNet and Prop-
Bank, and labels each argument slot with sets of
possible roles. The algorithm proceeds iteratively
by first making initial unambiguous role assign-
ments, and then successively updating a probabil-
221
ity model on which future assignments are based.
Being unsupervised, their approach requires no
manual effort other than creating the frame dic-
tionary. Unfortunately, existing resources do not
have exhaustive coverage and a large number of
verbs may be assigned no semantic role informa-
tion since they are not in the dictionary in the
first place. Pennacchiotti et al (2008) address
precisely this problem by augmenting FrameNet
with new lexical units if they are similar to an ex-
isting frame (their notion of similarity combines
distributional and WordNet-based measures). In
a similar vein, Gordon and Swanson (2007) at-
tempt to increase the coverage of PropBank. Their
approach leverages existing annotations to handle
novel verbs. Rather than annotating new sentences
that contain novel verbs, they find syntactically
similar verbs and use their annotations as surro-
gate training data.
Our own work aims to reduce but not entirely
eliminate the annotation effort involved in creating
training data for semantic role labeling. We thus
assume that a small number of manual annotations
is initially available. Our algorithm augments
these with unlabeled examples whose roles are in-
ferred automatically. We apply our method in a
monolingual setting, and thus do not project an-
notations between languages but within the same
language. In contrast to Pennacchiotti et al (2008)
and Gordon and Swanson (2007), we do not aim
to handle novel verbs, although this would be a
natural extension of our method. Given a verb
and a few labeled instances exemplifying its roles,
we wish to find more instances of the same verb
in an unlabeled corpus so as to improve the per-
formance of a hypothetical semantic role labeler
without having to annotate more data manually.
Although the use of semi-supervised learning is
widespread in many natural language tasks, rang-
ing from parsing to word sense disambiguation, its
application to FrameNet-style semantic role label-
ing is, to our knowledge, novel.
3 Semi-Supervised Learning Method
Our method assumes that we have access to a
small seed corpus that has been manually anno-
tated. This represents a relatively typical situation
where some annotation has taken place but not on
a scale that is sufficient for high-performance su-
pervised learning. For each sentence in the seed
corpus we select a number of similar sentences
Fluidic motion
FEE
~~}
}
}
}
}
}
}
}
}
}
Fluid
		
h
k
n
q
t
y
}







Path
||





~
z
feel
SUBJ
uukkk
kk
kk
kk
kk
kk
kk
k
AUX
{{vv
v
v
v
v
v
v
v
DOBJ

XCOMP
%%K
KK
KK
KK
KK
K
we can course
SUBJ
yyss
ss
ss
ss
ss
IOBJ

MOD
((P
PP
PP
PP
PP
PP
P
blood
DET

through
DOBJ

again
the vein
DET

our
Figure 1: Labeled dependency graph with seman-
tic role annotations for the frame evoking ele-
ment (FEE) course in the sentence We can feel the
blood coursing through our veins again. The frame
is Fluidic motion, and its roles are Fluid and Path.
Directed edges (without dashes) represent depen-
dency relations between words, edge labels denote
types of grammatical relations (e.g., SUBJ, AUX).
from an unlabeled expansion corpus. These are
automatically annotated by projecting relevant se-
mantic role information from the labeled sentence.
The similarity between two sentences is opera-
tionalized by measuring whether their arguments
have a similar structure and whether they express
related meanings. The seed corpus is then en-
larged with the k most similar unlabeled sentences
to form the expanded corpus. In what follows we
describe in more detail how we measure similarity
and project annotations.
3.1 Extracting Predicate-Argument
Structures
Our method operates over labeled dependency
graphs. We show an example in Figure 1 for
the sentence We can feel the blood coursing
through our veins again. We represent verbs
(i.e., frame evoking elements) in the seed and
unlabeled corpora by their predicate-argument
structure. Specifically, we record the direct de-
pendents of the predicate course (e.g., blood
or again in Figure 1) and their grammatical
roles (e.g., SUBJ, MOD). Prepositional nodes
are collapsed, i.e., we record the preposition?s
object and a composite grammatical role (like
IOBJ THROUGH, where IOBJ stands for ?preposi-
tional object? and THROUGH for the preposition
itself). In addition to direct dependents, we also
222
Lemma GramRole SemRole
blood SUBJ Fluid
vein IOBJ THROUGH Path
again MOD ?
Table 1: Predicate-argument structure for the verb
course in Figure 1.
consider nodes coordinated with the predicate as
arguments. Finally, for each argument node we
record the semantic roles it carries, if any. All sur-
face word forms are lemmatized. An example of
the argument structure information we obtain for
the predicate course (see Figure 1) is shown in Ta-
ble 1.
We obtain information about grammatical roles
from the output of RASP (Briscoe et al, 2006),
a broad-coverage dependency parser. However,
there is nothing inherent in our method that re-
stricts us to this particular parser. Any other
parser with broadly similar dependency output
could serve our purposes.
3.2 Measuring Similarity
For each frame evoking verb in the seed corpus our
method creates a labeled predicate-argument re-
presentation. It also extracts all sentences from the
unlabeled corpus containing the same verb. Not
all of these sentences will be suitable instances
for adding to our training data. For example, the
same verb may evoke a different frame with dif-
ferent roles and argument structure. We therefore
must select sentences which resemble the seed an-
notations. Our hypothesis is that verbs appearing
in similar syntactic and semantic contexts will be-
have similarly in the way they relate to their argu-
ments.
Estimating the similarity between two predi-
cate argument structures amounts to finding the
highest-scoring alignment between them. More
formally, given a labeled predicate-argument
structure pl with m arguments and an unla-
beled predicate-argument structure pu with n ar-
guments, we consider (and score) all possible
alignments between these arguments. A (partial)
alignment can be viewed as an injective function
? : M? ? {1, . . . , n} where M? ? {1, . . . ,m}.
In other words, an argument i of pl is aligned to
argument ?(i) of pu if i ? M?. Note that this al-
lows for unaligned arguments on both sides.
We score each alignment ? using a similarity
function sim(?) defined as:
?
i?M?
(
A ? syn(gli, g
u
?(i)) + sem(w
l
i, w
u
?(i))?B
)
where syn(gli, g
u
?(i)) denotes the syntactic similar-
ity between grammatical roles gli and g
u
?(i) and
sem(wli, w
u
?(i)) the semantic similarity between
head words wli and w
u
?(i).
Our goal is to find an alignment such
that the similarity function is maximized:
?? := argmax
?
sim(?). This optimization
problem is a generalized version of the linear
assignment problem (Dantzig, 1963). It can be
straightforwardly expressed as a linear program-
ming problem by associating each alignment ?
with a set of binary indicator variables xij :
xij :=
{
1 if i ? M? ? ?(i) = j
0 otherwise
The similarity objective function then becomes:
m?
i=1
n?
j=1
(
A ? syn(gli, g
u
j ) + sem(w
l
i, w
u
j )?B
)
xij
subject to the following constraints ensuring that ?
is an injective function on some M?:
n?
j=1
xij ? 1 for all i = 1, . . . ,m
m?
i=1
xij ? 1 for all j = 1, . . . , n
Figure 2 graphically illustrates the alignment
projection problem. Here, we wish to project
semantic role information from the seed blood
coursing through our veins again onto the un-
labeled sentence Adrenalin was still coursing
through her veins. The predicate course has three
arguments in the labeled sentence and four in the
unlabeled sentence (represented as rectangles in
the figure). There are 73 possible alignments in
this example. In general, for any m and n argu-
ments, where m ? n, the number of alignments
is
?m
k=0
m!n!
(m?k)!(n?k)!k! . Each alignment is scored
by taking the sum of the similarity scores of the in-
dividual alignment pairs (e.g., between blood and
be, vein and still ). In this example, the highest
scoring alignment is between blood and adrenalin,
vein and vein, and again and still, whereas be is
223
left unaligned (see the non-dotted edges in Fig-
ure 2). Note that only vein and blood carry seman-
tic roles (i.e., Fluid and Path) which are projected
onto adrenalin and vein, respectively.
Finding the best alignment crucially depends
on estimating the syntactic and semantic similar-
ity between arguments. We define the syntactic
measure on the grammatical relations produced
by RASP. Specifically, we set syn(gli, g
u
?(i)) to 1
if the relations are identical, to a ? 1 if the rela-
tions are of the same type but different subtype2
and to 0 otherwise. To avoid systematic errors,
syntactic similarity is also set to 0 if the predicates
differ in voice. We measure the semantic similar-
ity sem(wli, w
u
?(i)) with a semantic space model.
The meaning of each word is represented by a vec-
tor of its co-occurrences with neighboring words.
The cosine of the angle of the vectors represent-
ingwl andwu quantifies their similarity (Section 4
describes the specific model we used in our exper-
iments in more detail).
The parameter A counterbalances the impor-
tance of syntactic and semantic information, while
the parameter B can be interpreted as the lowest
similarity value for which an alignment between
two arguments is possible. An optimal align-
ment ?? cannot link arguments i0 of pl and j0
of pu, if A ? syn(gli0 , g
u
j0) + sem(w
l
i0 , w
u
j0) < B
(i.e., either i0 /? M?? or ??(i0) 6= j0). This
is because for an alignment ? with ?(i0) = j0
we can construct a better alignment ?0, which is
identical to ? on all i 6= i0, but leaves i0 un-
aligned (i.e., i0 /? M?0). By eliminating a neg-
ative term from the scoring function, it follows
that sim(?0) > sim(?). Therefore, an alignment ?
satisfying ?(i0) = j0 cannot be optimal and con-
versely the optimal alignment ?? can never link
two arguments with each other if the sum of their
weighted syntactic and semantic similarity scores
is below B.
3.3 Projecting Annotations
Once we obtain the best alignment ?? between pl
and pu, we can simply transfer the role of each
role-bearing argument i of pl to the aligned argu-
ment ??(i) of pu, resulting in a labeling of pu.
To increase the accuracy of our method we dis-
card projections if they fail to transfer all roles
of the labeled to the unlabeled dependency graph.
2This concerns fine-grained distinctions made by the
parser, e.g., the underlying grammatical roles in passive con-
structions.
Fluid //___ bloodSUBJ
//
!!


adrenalin
SUBJ
Path //___ veinIOBJ THROUGH
==
//
!!

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 be
AUX
again
MOD
FF
==
//
!!
still
MOD
vein
IOBJ THROUGH
Figure 2: Alignments between the argument
structures representing the clauses blood coursing
through our veins again and Adrenalin was still
coursing through her veins; non-dotted lines illus-
trate the highest scoring alignment.
This can either be the case if pl does not cover all
roles annotated on the graph (i.e., there are role-
bearing nodes which we do not recognize as argu-
ments of the frame evoking verb) or if there are
unaligned role-bearing arguments (i.e., i /? M??
for a role-bearing argument i of pl).
The remaining projections form our expan-
sion corpus. For each seed instance we select
the k most similar neighbors to add to our training
data. The parameter k controls the trade-off be-
tween annotation confidence and expansion size.
4 Experimental Setup
In this section we discuss our experimental setup
for assessing the usefulness of the method pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe the se-
mantic labeler we used in our experiments and ex-
plain how its output was evaluated.
Corpora Our seed corpus was taken from
FrameNet. The latter contains approximately
2,000 verb entries out of which we randomly se-
lected a sample of 100. We next extracted all an-
notated sentences for each of these verbs. These
sentences formed our gold standard corpus, 20%
of which was reserved as test data. We used
the remaining 80% as seeds for training purposes.
We generated seed corpora of various sizes by
randomly reducing the number of annotation in-
stances per verb to a maximum of n. An addi-
tional (non-overlapping) random sample of 100
verbs was used as development set for tuning the
parameters for our method. We gathered unla-
beled sentences from the BNC.
224
The seed and unlabeled corpora were parsed
with RASP (Briscoe et al, 2006). The FrameNet
annotations in the seed corpus were converted
into dependency graphs (see Figure 1) using the
method described in Fu?rstenau (2008). Briefly,
the method works by matching nodes in the de-
pendency graph with role bearing substrings in
FrameNet. It first finds the node in the graph
which most closely matches the frame evoking
element in FrameNet. Next, individual graph
nodes are compared against labeled substrings in
FrameNet to transfer all roles onto their closest
matching graph nodes.
Parameter Estimation The similarity function
described in Section 3.2 has three free parameters.
These are the weight A which determines the rel-
ative importance of syntactic and semantic infor-
mation, the parameter B which determines when
two arguments cannot be aligned and the syntactic
score a for almost identical grammatical roles. We
optimized these parameters on the development
set using Powell?s direction set method (Brent,
1973) with F1 as our loss function. The optimal
values for A, B and a were 1.76, 0.41 and 0.67,
respectively.
Our similarity function is further parametrized
in using a semantic space model to compute the
similarity between two words. Considerable lat-
itude is allowed in specifying the parameters of
vector-based models. These involve the defi-
nition of the linguistic context over which co-
occurrences are collected, the number of com-
ponents used (e.g., the k most frequent words
in a corpus), and their values (e.g., as raw co-
occurrence frequencies or ratios of probabilities).
We created a vector-based model from a lem-
matized version of the BNC. Following previ-
ous work (Bullinaria and Levy, 2007), we opti-
mized the parameters of our model on a word-
based semantic similarity task. The task involves
examining the degree of linear relationship be-
tween the human judgments for two individual
words and vector-based similarity values. We ex-
perimented with a variety of dimensions (ranging
from 50 to 500,000), vector component definitions
(e.g., pointwise mutual information or log likeli-
hood ratio) and similarity measures (e.g., cosine or
confusion probability). We used WordSim353, a
benchmark dataset (Finkelstein et al, 2002), con-
sisting of relatedness judgments (on a scale of 0
to 10) for 353 word pairs.
We obtained best results with a model using a
context window of five words on either side of the
target word, the cosine measure, and 2,000 vec-
tor dimensions. The latter were the most com-
mon context words (excluding a stop list of func-
tion words). Their values were set to the ratio of
the probability of the context word given the tar-
get word to the probability of the context word
overall. This configuration gave high correlations
with the WordSim353 similarity judgments using
the cosine measure.
Solving the Linear Program A variety of algo-
rithms have been developed for solving the linear
assignment problem efficiently. In our study, we
used the simplex algorithm (Dantzig, 1963). We
generate and solve an LP of every unlabeled sen-
tence we wish to annotate.
Semantic role labeler We evaluated our method
on a semantic role labeling task. Specifically, we
compared the performance of a generic seman-
tic role labeler trained on the seed corpus and
a larger corpus expanded with annotations pro-
duced by our method. Our semantic role labeler
followed closely the implementation of Johans-
son and Nugues (2008). We extracted features
from dependency parses corresponding to those
routinely used in the semantic role labeling liter-
ature (see Baker et al (2007) for an overview).
SVM classifiers were trained to identify the argu-
ments and label them with appropriate roles. For
the latter we performed multi-class classification
following the one-versus-one method3 (Friedman,
1996). For the experiments reported in this paper
we used the LIBLINEAR library (Fan et al, 2008).
The misclassification penalty C was set to 0.1.
To evaluate against the test set, we linearized
the resulting dependency graphs in order to obtain
labeled role bracketings like those in example (1)
and measured labeled precision, labeled recall and
labeled F1. (Since our focus is on role labeling and
not frame prediction, we let our role labeler make
use of gold standard frame annotations, i.e., label-
ing of frame evoking elements with frame names.)
5 Results
The evaluation of our method was motivated by
three questions: (1) How do different training set
sizes affect semantic role labeling performance?
3Given n classes the one-versus-one method builds
n(n? 1)/2 classifiers.
225
TrainSet Size Prec (%) Rec (%) F1 (%)
0-NN 849 35.5 42.0 38.5
1-NN 1205 36.4 43.3 39.5
2-NN 1549 38.1 44.1 40.9?
3-NN 1883 37.9 43.7 40.6?
4-NN 2204 38.0 43.9 40.7?
5-NN 2514 37.4 43.9 40.4?
self train 1609 34.0 41.0 37.1
Table 2: Semantic role labeling performance using
different amounts of training data; the seeds are
expanded with their k nearest neighbors; ?: F1 is
significantly different from 0-NN (p < 0.05).
Training size varies depending on the number of
unlabeled sentences added to the seed corpus. The
quality of these sentences also varies depending
on their similarity to the seed sentences. So,
we would like to assess whether there is a trade-
off between annotation quality and training size.
(2) How does the size of the seed corpus influence
role labeling performance? Here, we are interested
to find out what is the least amount of manual
annotation possible for our method to have some
positive impact. (3) And finally, what are the an-
notation savings our method brings?
Table 2 shows the performance of our semantic
role labeler when trained on corpora of different
sizes. The seed corpus was reduced to at most 10
instances per verb. Each row in the table corre-
sponds to adding the k nearest neighbors of these
instances to the training data. When trained solely
on the seed corpus the semantic role labeler yields
a (labeled) F1 of 38.5%, (labeled) recall is 42.0%
and (labeled) precision is 35.5% (see row 0-NN
in the table). All subsequent expansions yield
improved precision and recall. In all cases ex-
cept k = 1 the improvement is statistically signif-
icant (p < 0.05). We performed significance test-
ing onF1 using stratified shuffling (Noreen, 1989),
an instance of assumption-free approximative ran-
domization testing. As can be seen, the optimal
trade-off between the size of the training corpus
and annotation quality is reached with two nearest
neighbors. This corresponds roughly to doubling
the number of training instances. (Due to the re-
strictions mentioned in Section 3.3 a 2-NN expan-
sion does not triple the number of instances.)
We also compared our results against a self-
training procedure (see last row in Table 2). Here,
we randomly selected unlabeled sentences corre-
sponding in number to a 2-NN expansion, labeled
them with our role labeler, added them to the train-
ing set, and retrained. Self-training resulted in per-
formance inferior to the baseline of adding no un-
labeled data at all (see the first row in Table 2).
Performance decreased even more with the addi-
tion of more self-labeled instances. These results
indicate that the similarity function is crucial to the
success of our method.
An example of the annotations our method pro-
duces is given below. Sentence (2a) is the seed.
Sentences (2b)?(2e) are its most similar neighbors.
The sentences are presented in decreasing order of
similarity.
(2) a. [He]Theme stared and came
[slowly]Manner [towards me]Goal.
b. [He]Theme had heard the shooting
and come [rapidly]Manner [back to-
wards the house]Goal.
c. Without answering, [she]Theme left
the room and came [slowly]Manner
[down the stairs]Goal.
d. [Then]Manner [he]Theme won?t come
[to Salisbury]Goal.
e. Does [he]Theme always come round
[in the morning]Goal [then]Manner?
As we can see, sentences (2b) and (2c) accu-
rately identify the semantic roles of the verb come
evoking the frame Arriving. In (2b) He is la-
beled as Theme, rapidly as Manner, and towards
the house as Goal. Analogously, in (2c) she is
the Theme, slowly is Manner and down the stairs
is Goal. The quality of the annotations decreases
with less similar instances. In (2d) then is marked
erroneously as Manner, whereas in (2e) only the
Theme role is identified correctly.
To answer our second question, we varied the
size of the training corpus by varying the num-
ber of seeds per verb. For these experiments we
fixed k = 2. Table 3 shows the performance of the
semantic role labeler when the seed corpus has one
annotation per verb, five annotations per verb, and
so on. (The results for 10 annotations are repeated
from Table 2). With 1, 5 or 10 instances per verb
our method significantly improves labeling perfor-
mance. We observe improvements in F1 of 1.5%,
2.1%, and 2.4% respectively when adding the 2
most similar neighbors to these training corpora.
Our method also improves F1 when a 20 seeds
226
TrainSet Size Prec (%) Rec (%) F1 (%)
? 1 seed 95 24.9 31.3 27.7
+ 2-NN 170 26.4 32.6 29.2?
? 5 seeds 450 29.7 38.4 33.5
+ 2-NN 844 31.8 40.4 35.6?
? 10 seeds 849 35.5 42.0 38.5
+ 2-NN 1549 38.1 44.1 40.9?
? 20 seeds 1414 38.7 46.1 42.1
+ 2-NN 2600 40.5 46.7 43.4
all seeds 2323 38.3 47.0 42.2
+ 2-NN 4387 39.5 46.7 42.8
Table 3: Semantic role labeling performance us-
ing different numbers of seed instances per verb in
the training corpus; the seeds are expanded with
their k = 2 nearest neighbors; ?: F1 is signifi-
cantly different from seed corpus (p < 0.05).
corpus or all available seeds are used, however the
difference is not statistically significant.
The results in Table 3 also allow us to draw
some conclusions regarding the relative quality
of manual and automatic annotation. Expand-
ing a seed corpus with 10 instances per verb im-
proves F1 from 38.5% to 40.9%. We can com-
pare this to the labeler?s performance when trained
solely on the 20 seeds corpus (without any ex-
pansion). The latter has approximately the same
size as the expanded 10 seeds corpus. Interest-
ingly, F1 on this exclusively hand-annotated cor-
pus is only 1.2% better than on the expanded cor-
pus. So, using our expansion method on a 10 seeds
corpus performs almost as well as using twice as
many manual annotations. Even in the case of the
5 seeds corpus, where there is limited informa-
tion for our method to expand from, we achieve
an improvement from 33.5% to 35.6%, compared
to 38.5% for manual annotation of about the same
number of instances. In sum, while additional
manual annotation is naturally more effective for
improving the quality of the training data, we can
achieve substantial proportions of these improve-
ments by automatic expansion alone. This is a
promising result suggesting that it is possible to
reduce annotation costs without drastically sacri-
ficing quality.
6 Conclusions
This paper presents a novel method for reducing
the annotation effort involved in creating resources
for semantic role labeling. Our strategy is to ex-
pand a manually annotated corpus by projecting
semantic role information from labeled onto un-
labeled instances. We formulate the projection
problem as an instance of the linear assignment
problem. We seek to find role assignments that
maximize the similarity between labeled and un-
labeled instances. Similarity is measured in terms
of structural and semantic compatibility between
argument structures.
Our method improves semantic role labeling
performance in several experimental conditions. It
is especially effective when a small number of an-
notations is available for each verb. This is typi-
cally the case when creating frame semantic cor-
pora for new languages or new domains. Our ex-
periments show that expanding such corpora with
our method can yield almost the same relative im-
provement as using exclusively manual annota-
tion.
In the future we plan to extend our method
in order to handle novel verbs that are not at-
tested in the seed corpus. Another direction con-
cerns the systematic modeling of diathesis alter-
nations (Levin, 1993). These are currently only
captured implicitly by our method (when the se-
mantic similarity overrides syntactic dissimilar-
ity). Ideally, we would like to be able to system-
atically identify changes in the realization of the
argument structure of a given predicate. Although
our study focused solely on FrameNet annotations,
we believe it can be adapted to related annotation
schemes, such as PropBank. An interesting ques-
tion is whether the improvements obtained by our
method carry over to other role labeling frame-
works.
Acknowledgments The authors acknowledge
the support of DFG (IRTG 715) and EPSRC
(grant GR/T04540/01). We are grateful to
Richard Johansson for his help with the re-
implementation of his semantic role labeler.
References
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 99?104, Prague, Czech Republic.
R. P. Brent. 1973. Algorithms for Minimization with-
out Derivatives. Prentice-Hall, Englewood Cliffs,
NJ.
227
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, pages 77?80, Sydney, Australia.
J. A. Bullinaria and J. P. Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
George B. Dantzig. 1963. Linear Programming and
Extensions. Princeton University Press, Princeton,
NJ, USA.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Jerome H. Friedman. 1996. Another approach to poly-
chotomous classification. Technical report, Depart-
ment of Statistics, Stanford University.
Pascale Fung and Benfeng Chen. 2004. BiFrameNet:
Bilingual frame semantics resources construction
by cross-lingual induction. In Proceedings of the
20th International Conference on Computational
Linguistics, pages 931?935, Geneva, Switzerland.
Hagen Fu?rstenau. 2008. Enriching frame semantic re-
sources with dependency graphs. In Proceedings of
the 6th Language Resources and Evaluation Confer-
ence, Marrakech, Morocco.
Daniel Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguis-
tics, 28:3:245?288.
Andrew Gordon and Reid Swanson. 2007. General-
izing semantic role annotations across syntactically
similar verbs. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 192?199, Prague, Czech Republic.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 436?443, Syd-
ney, Australia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role la-
beling. In Proceedings of the 22nd International
Conference on Computational Linguistics, pages
393?400, Manchester, UK.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Alessandro Moschitti, Paul Morarescu, and Sanda
Harabagiu. 2003. Open-domain information extrac-
tion via automatic semantic labeling. In Proceed-
ings of FLAIRS 2003, pages 397?401, St. Augustine,
FL.
E. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Sebastian Pado? and Katrin Erk. 2005. To cause
or not to cause: Cross-lingual semantic matching
for paraphrase modelling. In Proceedings of the
EUROLAN Workshop on Cross-Linguistic Knowl-
edge Induction, pages 23?30, Cluj-Napoca, Roma-
nia.
Sebastian Pado? and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161?1168, Sydney,
Australia.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):71?
106.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of FrameNet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 457?465, Honolulu,
Hawaii.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of the joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Computational Natural Language Learning, pages
12?21, Prague, Czech Republic.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15, Sap-
poro, Japan.
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labelling. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 95?102. Bacelona,
Spain.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of the joint Human Language Technology
Conference and Conference on Empirical Methods
in Natural Language Processing, pages 371?378,
Vancouver, BC.
228
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 257?264, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Discourse Chunking and its Application to Sentence Compression
Caroline Sporleder and Mirella Lapata
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
{csporled,mlap}@inf.ed.ac.uk
Abstract
In this paper we consider the problem of
analysing sentence-level discourse struc-
ture. We introduce discourse chunking
(i.e., the identification of intra-sentential
nucleus and satellite spans) as an al-
ternative to full-scale discourse parsing.
Our experiments show that the proposed
modelling approach yields results com-
parable to state-of-the-art while exploit-
ing knowledge-lean features and small
amounts of discourse annotations. We also
demonstrate how discourse chunking can
be successfully applied to a sentence com-
pression task.
1 Introduction
The computational treatment of discourse phenom-
ena has recently attracted much attention, partly due
to their increasing importance for potential appli-
cations. In summarisation, for example, the extrac-
tion of sentences to include in a summary crucially
depends on their rhetorical status (Marcu, 2000;
Teufel and Moens, 2002); one might want to extract
contrastive or explanatory statements while omit-
ting sentences that contain background information.
In information extraction, discourse-level knowl-
edge can be used to identify co-referring events
(Humphreys et al, 1997) and to determine their tem-
poral order. Discourse processing could further en-
hance question answering systems by interpreting
the user?s question either in isolation or in the con-
text of preceding questions (Chai and Jing, 2004).
Discourse analysis is often viewed as a parsing
task. Rhetorical Structure Theory (RST, Mann and
Thomson, 1988), one of the most influential frame-
works in discourse processing, represents texts by
trees whose leaves correspond to elementary dis-
course units (edus) and whose nodes specify how
these and larger units (e.g., multi-sentence seg-
ments) are linked to each other by rhetorical rela-
tions (e.g., Contrast, Elaboration). Discourse units
are further characterised in terms of their text im-
portance: nuclei denote central segments, whereas
satellites denote peripheral ones.
Recent advances in discourse modelling have
greatly benefited from the availability of resources
annotated with discourse-level information such as
the RST Discourse Treebank (RST-DT, Carlson et
al., 2002). Even though discourse parsing at the
document-level still poses a significant challenge to
data-driven methods, sentence-level discourse mod-
els (e.g., Soricut and Marcu, 2003) trained on the
RST-DT have attained accuracies comparable to hu-
man performance. The availability of discourse an-
notations is partly responsible for the success of
these models. Another important reason is the devel-
opment of robust syntactic parsers (e.g., Charniak,
2000) that can be used to provide critical structural
and lexical information to the discourse parser. Un-
fortunately, discourse annotated corpora are largely
absent for languages other than English. Further-
more, reliance on syntactic parsing renders dis-
course parsing practically impossible for languages
for which state-of-the-art parsers are unavailable.
In this paper we propose discourse chunking as an
alternative to discourse parsing. Analogous to sen-
tence chunking, discourse chunking is an interme-
diate step towards full parsing. Following an RST-
style analysis, we focus solely on two subtasks:
(a) discourse segmentation, i.e., determining which
word sequences form edus and (b) inferring whether
these edus function as nuclei or satellites. The moti-
vation for tackling these subtasks is two-fold. First,
they are of crucial importance for full-scale dis-
course parsing. For example, Soricut and Marcu
(2003) show that perfect discourse segmentation de-
livers an error reduction of 29% in the performance
of their discourse parser. Second, some applications
may not require full-scale discourse parsing. For ex-
ample, it has been shown that nuclearity is important
257
for summarisation, i.e., nuclei are more likely to be
retained when summarising than satellites (Marcu,
2000). While nuclearity alone may not be sufficient
for document summarisation (Marcu, 1998), such
knowledge could prove useful at the sentence level,
for example for producing sentence compressions.
The algorithms introduced in this paper are pur-
posely knowledge-lean. We abstain from using syn-
tactic parsers or semantic databases such as Word-
Net (Fellbaum, 1998), thus exploring the portabil-
ity of our methods to languages for which such
resources are not available. We employ lexical
and low-level syntactic information (e.g., parts of
speech, syntactic chunks) and show that the perfor-
mance of our discourse chunker on the two subtasks
(mentioned above) is comparable to that of a state-
of-the-art sentence-level discourse parser (Soricut
and Marcu, 2003). We also assess its application po-
tential on a sentence compression task (Knight and
Marcu, 2003).
2 Related Work
Initial work towards the development of discourse
parsers has primarily relied on hand-crafted rules for
specifying world knowledge or constraints on tree
structures (e.g., Hobbs 1993). Recent work has seen
the emergence of treebanks annotated with discourse
structure, thus enabling the development of more
robust, data-driven models. Marcu (2000) presents
a shift-reduce parsing model that segments texts
into edus and determines how they should be as-
sembled into rhetorical structure trees. Soricut and
Marcu (2003) introduce a syntax-based sentence-
level discourse parser, which consists of two compo-
nents: a statistical segmentation model and a parser
working on the output of the segmenter. Both com-
ponents are trained on the RST-DT and exploit lexi-
cal features as well as syntactic dominance features
(which are taken from syntactic parse trees).
Given that discourse-level information plays an
important role in human summarisation (Endres-
Niggemeyer, 1998), it is not surprising that mod-
els of discourse structure have found use in auto-
matic summarisation. For instance, Marcu (2000)
proposes a summarisation algorithm that builds an
RST tree for the entire text, and identifies its most
important parts according to discourse salience.
Our work differs from previous approaches in
two key respects. First, we do not attempt to pro-
duce a hierarchical discourse structure. We intro-
duce discourse chunking, a less resource demanding
task than full discourse parsing. We show that good
said Mr. Smith as the market plunged.
Nucleus Satellite Satellite
Attribution
Nucleus Circumstance
"I am optimistic"
Figure 1: Discourse Tree in RST-DT
chunking performance can be achieved with low-
level information. Second, we apply our discourse
chunker to sentence compression. Although previ-
ous approaches have utilised discourse information
for document summarisation, its application to sen-
tence condensation is novel to our knowledge.
3 Discourse Chunking
3.1 Data and Representation
We propose a supervised machine learning approach
to discourse chunking. Our data were obtained from
the RST-DT (Carlson et al, 2002), which consists of
385 Wall Street Journal articles manually annotated
with discourse structures in the framework of Mann
and Thompson (1987). An example of an RST-based
tree representation is shown in Figure 1; rectangu-
lar boxes denote edus and arcs indicate which re-
lations (e.g., Circumstance or Attribution) hold be-
tween them. Relations are typically binary with one
unit being the nucleus (indicated by arrows in Fig-
ure 1) and the other the satellite, but multi-nuclear
and non-binary relations are also possible.
We are only interested in the lowest level of the
tree, i.e., we aim to identify the edus and determine
whether they are nuclei or satellites. For example,
in the sentence in Figure 1 we want to identify the
three edus ?I am optimistic?, said Mr. Smith, and as
the market plunged. and determine that the first of
these functions as a nucleus at the lowest level of
the tree whereas the latter two function as satellites.
We do not try to determine that the first two edus
are merged at a higher level and then function as the
overall nucleus of the sentence.
The discourse chunking task assumes a non-
hierarchical representation. We converted each
sentence-level discourse tree into a flat chunk rep-
resentation by assigning each token (i.e., word or
punctuation mark) a tag encoding its nuclearity sta-
tus at the edu level. We adopted the chunk repre-
sentation proposed by Ramshaw and Marcus (1995)
and used four different tags: B-NUC and B-SAT for
nucleus and satellite-initial tokens, and I-NUC and
I-SAT for non-initial tokens, i.e., tokens inside a nu-
cleus and satellite span. As all tokens belong either
258
to a nucleus or a satellite span, we do not need a spe-
cial tag (typically denoted by O in syntactic chunk-
ing) to indicate elements outside a chunk. The chunk
representation for the sentence in Figure 1 is thus:
?/B-NUC I/I-NUC am/I-NUC optimistic/I-NUC
?/I-NUC said/B-SAT Mr./I-SAT Smith/I-SAT
as/B-SAT the/I-SAT market/I-SAT plunged/I-
SAT ./I-SAT
Discourse and sentence structure do not always
correspond, and for 5% of sentences in the RST-DT
no discourse tree exists. We excluded these from our
data. We also disregarded sentences without internal
structure, i.e., those which consist of only one edu.
The RST-DT is partitioned into a training (342 arti-
cles) and test set (43 articles). We preserved this split
in all our experiments. 52 articles in the RST-DT are
doubly annotated. We used these to compute human
agreement on the discourse chunking task (see Sec-
tion 4.1).
3.2 Modelling
Using a chunk-based representation effectively ren-
ders discourse processing a sequence labelling task.
Two modelling approaches are possible. The sim-
plest model performs segmentation and labelling si-
multaneously. In our case this involves training a
classifier that labels each token with one of our four
tags (i.e., B-NUC, I-NUC, B-SAT, I-SAT). Alterna-
tively, we could treat discourse chunking as two dis-
tinct subtasks involving two binary classifiers: a seg-
menter, which determines the chunk boundaries and
assigns each token a chunk-initial (B) or non-chunk-
initial tag (I), and a labeller, which classifies each
chunk identified by the segmenter as either nucleus
(NUC) or satellite (SAT).1
The second approach has a number of advantages.
First, abstracting away from a token-based represen-
tation in the second step makes it easier to model
sentence-level distributional properties of nuclei and
satellites, e.g., the fact that every sentence has at
least one nucleus. This can be achieved by incor-
porating additional features into the labeller, such
as the number of chunks in the sentence or the
length of the current chunk. A two-step approach
also avoids the creation of illegal chunk sequences,
such as ?B-SAT I-NUC?. However, a potential draw-
back is that the number of training examples for the
labeller is reduced as the instances to be classified
are chunks rather than tokens. We explore the per-
formance of the one-step and the two-step methods
in Sections 4.2 and 4.3, respectively.
1A similar approach has been proposed for syntactic chunk-
ing, e.g., Tjong Kim Sang (2000).
A variety of learning schemes can be employed
for the discourse chunking task. We have experi-
mented with Boosting (Schapire and Singer, 2000),
Conditional Random Fields (Lafferty et al, 2001),
and Support Vector Machines (Vapnik, 1998). Dis-
cussion of our results focuses exclusively on boost-
ing, since it had a slight advantage over the other
methods. Boosting combines many simple, mod-
erately accurate categorisation rules into a sin-
gle, highly accurate rule. We used BoosTexter?s
(Schapire and Singer, 2000) implementation, which
combines boosting with simple decision rules. The
system permits three different types of features:
numeric, nominal and ?text?. Text-valued features
can, for example, encode sequences of words or
parts of speech. BoosTexter applies n-gram mod-
els when forming classification hypotheses for text-
valued features.
3.3 Features for the Token-Based Models
While we use similar features for all our classifiers,
their concrete implementation depends on whether
the classifier is token-based (i.e., the one-step model
and the segmenter in the two-step method) or span-
based (i.e., the labeller in the two-step method). We
first describe the features for the former.
Each token is represented as a feature vector en-
coding information about the token itself and its con-
text. We intentionally limited our features to a basic
set representing grammatical, syntactic, and lexical
information.
Tokens This feature simply encodes the identity
of the current token; we used raw tokens, without
lemmatisation or stemming.
Part-of-Speech Tags Tokens were also anno-
tated with parts of speech using a publicly available
state-of-the-art tagger (Mikheev, 1997).
Syntactic Chunks Chunk information is a valu-
able cue for determining discourse segments; it is
unlikely that a segment boundary occurs within a
syntactic chunk. We applied a chunker (Mikheev,
1997) to our data to discover noun and verb phrase
chunks. The chunker assigned one of five labels to
each token, encoding the first element of a noun or
verb chunk (B-NP and B-VP, respectively), a non-
initial element in a chunk (I-NP and I-VP), and an
element outside a chunk (O). We used these chunk
labels directly as features and also encoded gener-
alisations over chunk and boundary types (i.e., VP
vs. NP and B vs. I, respectively).
Clause Information Knowing where clause
boundaries lie is important for segmentation, since
259
discourse segments often correspond to clauses. We
used a rule-based algorithm (Leffa, 1998) to iden-
tify clauses from the syntactic chunker?s output and
recorded for every token whether it is clause-initial
(S) or not (X).
Discourse Connectives Discourse connectives
such as but often indicate which rhetorical relation
holds between two spans. While we do not aim to in-
fer the relation proper, knowing the type of relation
holding between spans often helps in determining
whether they should be labelled as nucleus or satel-
lite. For example, Contrast relations (e.g., signalled
by but) hold between two nuclei whereas Cause re-
lations (e.g., signalled by because) hold between a
nucleus and a satellite. Hence, we recorded the pres-
ence of discourse connectives in a sentence to cap-
ture, albeit in a shallow manner, the interdependency
between rhetorical relations and nuclearity.
We used Knott?s (1996) inventory of discourse
connectives and encoded two types of information
for each token: (a) whether the token is a connective
(C) or not (X) and (b) the identity of the connective
if the token is a connective (zero otherwise).2
Token Position For each token we calculated its
relative position in the sentence (defined as the to-
ken position divided by the number of tokens). This
information is useful to capture potential positional
differences between nuclei and satellites, i.e., it may
be that nuclei are more likely at the beginning of a
sentence than at the end.
Context In addition to the nine features above,
which encode information about the token itself, we
also implemented 16 contextual features to encode
information about its neighbouring tokens. Syntac-
tic chunking approaches typically capture contextual
information by defining a small window of a few to-
kens to the left and right of the current token (see
Veenstra, 1998). However, we used the whole sen-
tence as context, since BoosTexter is fairly good at
determining automatically relevant n-grams within a
longer string of tokens. We included this contextual
information for all nominal features; that is, we en-
coded not only the string of preceding and following
tokens but also the string of preceding and following
part-of-speech tags, syntactic chunk labels, clause
labels, and connectives. For example, we had three
token features, one encoding the current token itself,
and two contextual features (one encoding the string
2Some words can have syntactic as well as discourse mark-
ing functions (e.g., but sometimes functions as a synonym for
except rather than as a Contrast marker). We do not disam-
biguate between these two usages.
of preceding tokens, and one encoding the string of
following tokens); similarly we had three part-of-
speech features, nine syntactic chunk features three
using the complete chunk tags, three using only the
chunk type, and three using the boundary type), and
so on.
3.4 Features for the Span-Based Model
For the labeller we encoded information about
spans rather than tokens. This gave rise to six non-
contextual, text-valued features: the string of tokens
in the current span, their parts of speech, syntactic
chunk tags, clause tags, and the presence and iden-
tity of connectives. The positional feature was re-
defined in terms of relative span position, i.e., the
position of the current span divided by the number
of spans in the sentence. We restricted contextual
features to information about immediately preced-
ing and following spans (within a sentence). We did
not include information about non-adjacent spans
because only a minority of sentences in our data con-
tained more than three spans. Again, we included
contextual information for all nominal features. Fi-
nally, to capture intra-sentential span-structure, we
added the following features:
Span Length Span length was measured in
terms of the number of tokens in it and was repre-
sented by three features: the length of the current
span, and the lengths of its adjacent spans. Span
length information captures differences in the aver-
age length of nuclei and satellite spans.
Number of Spans We encoded the number of
spans in the sentence overall and the number of
spans preceding and following the current span.
4 Experiments
In this section we describe the experiments that as-
sess the merits of the discourse chunking framework
introduced above. We also give details regarding pa-
rameter estimation and training for our models and
introduce the baseline and state-of-the-art methods
used for comparison with our approach.
4.1 Upper Bound
Before presenting the results of our modelling ex-
periments, it is worth considering how well humans
agree on discourse chunk segmentation and labelling
in order to establish an upper bound for the task. We
measured both unlabelled and labelled agreement on
the 52 doubly annotated RST-DT texts. The former
measures whether humans agree in placing chunk
boundaries, whereas the latter additionally measures
260
whether humans agree in assigning chunk labels.
To facilitate comparison with our models we report
inter-annotator agreement in terms of accuracy and
F-score.3 For the unlabelled case we also report Win-
dow Difference (WDiff), a commonly used evalua-
tion measure for segmentation tasks (Pevzner and
Hearst, 2002). It returns values between 0 (identical
segmentations) and 1 (maximally different segmen-
tations) and differs from accuracy in that predicted
boundaries which are only slightly off are penalised
less than those which are completely wrong.
Human agreement is relatively high4 on both seg-
mentation and span labelling (see Table 1), which
can be explained by the fact that (i) the RST-DT
annotators were given very detailed and precise in-
structions and (ii) assigning boundaries and labels
is an easier task than creating full-scale discourse
trees.
4.2 One-Step Chunking
For the one-step chunking method, our training set
consists of approximately 130,000 instances (i.e., to-
kens). We set aside 10% as a development set for
optimising BoosTexter?s parameters (i.e., the num-
ber of training iterations and the maximal length of
the n-grams considered for text-valued features). We
then re-trained BoosTexter with the optimal setting
(700 iterations, n = 2) and applied it to the test set,
which contained around 15,500 instances.
By default, the one-step method treats every token
in isolation, i.e., it assigns each token a tag without
taking its neighbouring tags into account. This is not
an entirely adequate model, since the likelihood of a
tag is influenced by its surrounding tags. For exam-
ple, the probability of a token being tagged as I-NUC
should increase if the preceding token was tagged
as B-NUC. One way to take information about sur-
rounding tags into account is by stacking classifiers,
i.e., adding the output of one classifier to the input
of another. Stacking is frequently used in chunking
tasks (e.g., Veenstra, 1998). We stack two BoosTex-
ter classifiers, by adding the string of all preceding
and following tags (within a given sentence) to each
token?s feature vector for the second classifier.
It would be possible to generate training mate-
rial for the second classifier directly from the orig-
inal training set by using the gold standard output
tags in the augmented feature vector. However, we
3For the unlabelled case, we report the F-score on bound-
aries; for the labelled case, we report the average F-score over
all class labels weighted by class frequency in the training set.
4Using the Kappa statistic agreement on segmentation
is K = .97 and on span labelling K = .81.
found that this leads BoosTexter to rely too much
on these tags, largely ignoring other features. This
causes problems when the model is applied to the
test set where the class tags are predicted and may
contain errors. Hence, we applied the original model
(BT-1-Step) to obtain predicted output tags for the
training data and then used these, rather than the
gold standard tags, to train the second classifier.
Similarly, during testing, we first applied BT-1-Step
and used its output tags to augmented the feature
vectors of the second classifier.
For comparison, we also applied two baseline
models to our data. The first (BaseMaj) is obtained
by always assigning the tag that is most common
in the training data (I-NUC). This strategy makes
no attempt at guessing span boundaries. The second
(BaseClMaj) indirectly assesses the importance of
clause boundary detection. It implements a strategy
which assumes that span boundaries always coin-
cide with clause boundaries. To obtain clause bound-
aries, we used the gold standard annotation of our
data in the Penn Treebank. We then labelled all
clause-initial tokens as B-NUC and all other tokens
as I-NUC. Note, that the use of gold standard clause
boundaries makes this a relatively high baseline. We
also applied Spade5, Soricut and Marcu?s (2003)
sentence-level discourse parser (see Section 2) to
our test set. For evaluation purposes, Spade?s out-
put was converted to our chunk representation. It is
important to note that Spade is a much more sophis-
ticated model than the ones presented in this paper.
We therefore do not expect to be able to obtain a bet-
ter performance. It is nevertheless interesting to see
how far one can go with a modest feature space and
considerably less structural information.
Table 1 shows the results. A set of diacritics is
used to indicate significance (on accuracy) through-
out this paper, see Table 2. On the segmentation task
(unlabelled) BT-1-Step and its stacked variant sig-
nificantly outperform the majority baseline (Base-
Maj) but are significantly less accurate than Base-
ClMaj, which uses gold standard clause boundaries.
The two BoosTexter models also perform signifi-
cantly worse than Spade on segmentation. However,
the higher WDiff for Spade on the segmentation task
suggests that the boundaries predicted by our mod-
els contain more ?near misses? than those predicted
by Spade. When segmentation and span labelling are
taken into account (labelled), our one-step models
significantly outperform both baselines but are sig-
nificantly less accurate than Spade. Classifier stack-
5The software is publicly available from http://www.isi.
edu/licensed-sw/spade/.
261
unlabelled labelled
Models Acc % F-score WDiff Acc % F-score
BaseMaj 88.50 ? .4021 53.87 38.77
BaseClMaj 93.51 70.06 .2008 56.64 43.62
BT-1-Step 90.07???$ 64.64 .2148 74.40???$ 74.13
BT-1-Step, stacked 91.86?6 ??$ 68.95 .1795 75.55???$ 75.37
BT-2-Step 97.37???$ 88.28 .0733 78.27?? 6 ?$ 78.38
BT-2-Step, stacked 97.41???$ 88.40 .0727 76.31???$ 76.34
Spade 93.49?6 ?$ 87.06 .5071 79.21??$ 80.91
Humans 99.05 97.96 .0012 89.10 89.03
Table 1: Results on discourse segmentation and span labelling
Symbols Meaning
? 6 ? (not) sig different from BaseMaj
? 6 ? (not) sig different from BaseClMaj
? 6 ? (not) sig different from Spade
$ 6 $ (not) sig different from Humans
Table 2: Meaning of diacritics indicating statistical
significance (?2 tests, p < 0.05)
ing leads to slight improvements over the simple
BoosTexter model, but the difference is not statis-
tically significant.
4.3 Two-Step Chunking
In the two-step model, chunking consists of two
separate subtasks: segmentation and labelling. To
generate training material for the segmenter, we re-
placed the four chunk labels in the original data set
by their corresponding boundary labels (B, I). For
the labeller, training instances are spans rather than
tokens. We used the gold standard span boundaries
to convert the original training set to a span-based
representation. This new training set contained
around 15,000 instances (compared to 130,000 in-
stances in the token-based set). For both the seg-
menter and labeller, we set aside 10% of the ma-
terial as development data to optimise BoosTexter?s
parameters (900 iterations, n = 3 for segmentation,
and 600 iterations, n = 2 for labelling).
For testing, we first applied the segmenter to ob-
tain discourse chunk boundaries. We then used the
predicted boundaries to convert the test data into a
span-based representation, which we then used as
input for the labeller. For evaluation, the output of
the labeller was converted back to a token-based rep-
resentation. As with one-step chunking, we also im-
plemented a stacked variant, stacking both the seg-
mentation and the labelling models.
It can be seen in Table 1 that the two-step mod-
els outperform the one-step models. This difference
is significant except for the stacked model on the la-
belling task (labelled). Both two-step models signif-
icantly outperform both baselines on segmentation
(unlabelled) and labelling (labelled). They also sig-
nificantly outperform Spade on the boundary pre-
diction task, which is in itself an important sub-
task for discourse parsing. The unstacked two-step
BoosTexter model performs comparably to Spade
with respect to labelled accuracy; the difference be-
tween the two models is not statistically signifi-
cant. Hence, we achieve results similar to Spade but
with much simpler and knowledge-leaner features.
As with the one-step method, the stacked model
performs (insignificantly) better than its unstacked
counterpart on the segmentation task. However, on
the labelling task, the stacked variant performs sig-
nificantly worse. We conjecture that the reduced
training set size for the labeller causes the stacked
model (which is effectively trained twice) to overfit.
Expectedly, all models perform significantly worse
than humans on both tasks.
To assess whether our discourse chunker could
be ported to languages for which discourse tree-
banks are not yet available, we investigated how
much annotated data is required to achieve satis-
factory results. Assuming that annotators proceed
sentence-by-sentence, we varied the amount of sen-
tences in our training data and determined its ef-
fect on the learner?s (BT-2-Step) performance. Fig-
ure 2 shows that satisfactory labelled and unlabelled
performance (86.52% and 74.64% F-score, respec-
tively) can be achieved with approximately half the
training data (i.e., around 2,000 sentences). In fact,
using the entire data set yields a moderate increase
of 1.78% for the unlabelled task and 3.68% for the
labelled task. Hence, it seems that our knowledge-
lean method is suitable even for relatively small
training sets. We next examine whether the two-step
chunking model can be usefully employed in a prac-
tical application such as sentence compression.
262
0 472 949 1,428 1,8872,3502,8233,2903,8524,2584,734Number of sentences in training data
6065
7075
8085
9095
100
F-sco
re
UnlabelledLabelled
Figure 2: Learning curve for discourse segmentation
(unlabelled) and span labelling (labelled)
4.4 Sentence Compression
Sentence compression can be likened to summari-
sation at the sentence level. The task has an imme-
diate impact on several applications ranging from
summarisation to audio scanning devices for the
blind and caption generation (see Knight and Marcu,
2002 and the references therein). Previous data-
driven approaches (Knight and Marcu, 2003; Riezler
et al, 2003) relied on parallel corpora to determine
what is important in a sentence. The models learned
correspondences between long sentences and their
shorter counterparts, typically employing a rich fea-
ture space induced from parse trees. The task is chal-
lenging since the compressed sentences should re-
tain essential information and convey it grammati-
cally.
Here, we propose a complementary approach
which utilises discourse chunking. A compressed
sentence can be obtained from the output of the
chunker simply by removing satellites. We thus cap-
italise on RST?s (Mann and Thompson, 1987) no-
tion of nuclearity and the widely held assumption
that spans functioning as satellites can often be
deleted without disrupting coherence. To evaluate
the compressions produced by our chunking model,
we elicited judgements from human subjects. We de-
scribe our elicitation study and results as follows.
Data We randomly selected 40 sentences from
the test portion of the RST-DT. Average sentence
length was 38.75. The sentences were compressed
by chunking them with our (unstacked) two-step
model (BT-2-Step) and then dropping satellites. We
applied the same strategy to derive compressed sen-
tences from the output of Spade (Soricut and Marcu,
2003), and also produced human compressions. Fi-
Original
Administration officials traveling with President Bush in
Costa Rica interpreted Mr. Ortega?s wavering as a sign that
he isn?t responding to the military attacks so much as he is
searching for ways to strengthen his hand prior to the elec-
tions.
Baseline
Administration officials interpreted Mr. Ortega?s wavering.
BT-2-Step
Administration officials interpreted Mr. Ortega?s wavering as
a sign that he isn?t responding to the military attacks so much
as he is searching for ways.
Spade
Administration officials traveling with President Bush in
Costa Rica interpreted Mr. Ortega?s wavering as a sign.
Human
Administration officials interpreted Mr. Ortega?s wavering as
a sign that he is searching for ways to strengthen his hand prior
to the elections.
Table 3: Example compressions
Compression AvgLen Rating
Baseline 9.70 1.93
BT-2-Step 22.06 3.21
Spade 19.09 3.10
Humans 20.07 3.83
Table 4: Mean ratings for automatic compressions
nally, we added a simple baseline compression al-
gorithm proposed by Jing and McKeown (2000)
which removed all prepositional phrases, clauses, to-
infinitives, and gerunds. Both the baseline and Spade
operate on parse trees which were obtained from
Charniak?s (2000) parser. Our set of experimental
materials contained 4?40 = 160 compressions.
Procedure and Subjects We obtained com-
pression ratings during an elicitation study com-
pleted by 45 unpaid volunteers, all native speaker
of English. The study was conducted remotely over
the Internet. Participants first saw a set of instruc-
tions that explained the task, and defined sentence
compression using multiple examples. The materi-
als consisted of the original sentences together with
their compressed versions. They were randomised in
lists following a Latin square design ensuring that
no two compressions in a list were generated from
the same sentence. As in Knight and Marcu?s (2003)
study, participants were asked to use a five point
scale to rate the systems? compressions (taking into
account the felicity of the compression as well as its
grammaticality); they were told that all outputs were
generated automatically. Examples of the compres-
sions our participants saw are given in Table 3.
Results We carried out an Analysis of Variance
(ANOVA) to examine the effect of different types
of compressions (Baseline, BT-2-Step, Spade, and
Human). Statistical tests were done using the mean
263
of the ratings shown in Table 4. The ANOVA re-
vealed a reliable effect of compression type by sub-
jects (F1(3,90) = 149.50, p < 0.001) and by items
(F2(3;117) = 40.23, p < 0.001). Post-hoc Tukey
tests indicated that human compressions are per-
ceived as significantly better than the compressions
produced by the baseline, BT-2-Step, and Spade
(? = 0.01). The discourse chunker and Spade are
significantly better than the baseline (? = 0.01). The
Tukey test revealed no statistically significant dif-
ference between these two algorithms (? = 0.01).
To summarise, both BoosTexter and Spade perform
closer to human performance than the baseline; yet,
humans perform significantly better than our com-
pression algorithms.
5 Conclusions
In this paper we proposed discourse chunking as an
alternative to full-scale parsing. Central in our ap-
proach is the use of low-level syntactic and gram-
matical information which we argue holds promise
for the development of discourse processing mod-
els across languages and domains. We showed that
a knowledge-lean feature space achieves good per-
formance both on segmentation and span labelling.
Furthermore, we assessed the application potential
of our chunker and showed that it can be success-
fully employed to generate sentence compressions,
thus confirming one of RST?s main claims regard-
ing the nuclearity of discourse spans (at least on the
sentence-level).
An important future direction lies in extending
our model to the document-level and the assign-
ment of rhetorical relations, thus going beyond the
basic nucleus-satellite distinction. Our results indi-
cate that a modular approach to discourse process-
ing (i.e., treating segmentation as separate from la-
belling) could increase performance. In the future,
we plan to investigate how to combine our chunker
with models like Spade for improved prediction on
both local and global levels.
Acknowledgments
The authors acknowledge the support of EPSRC (Sporleder,
grant GR/R40036/01; Lapata, grant GR/T04540/01). Thanks
to Amit Dubey, Ben Hutchinson, Alex Lascarides, Simone
Teufel, and three anonymous reviewers for helpful comments
and suggestions.
References
L. Carlson, D. Marcu, M. E. Okurowski. 2002. RST Discourse
Treebank. Linguistic Data Consortium, 2002.
J. Chai, R. Jing. 2004. Discourse structure for context question
answering. In Proceedings of the Workshop on Pragmatics
of Question Answering at HLT-NAACL 2004, 23?30.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st NAACL, 132?139.
B. Endres-Niggemeyer. 1998. Summarising Information.
Springer, Berlin.
C. Fellbaum, ed. 1998. WordNet: An Electronic Database.
MIT Press, Cambridge, MA.
J. R. Hobbs, M. Stickel, D. Appelt, P. Martin. 1993. Interpre-
tation as abduction. Journal of Artificial Intelligence, 63(1?
2):69?142.
K. Humphreys, R. Gaizauskas, S. Azzam. 1997. Event coref-
erence for information extraction. In Proceedings of the
ACL Workshop on Operational Factors in Practical Robust
Anaphora Resolution for Unrestricted Texts, 75?81.
H. Jing, K. McKeown. 2000. Cut and paste summarization. In
Proceedings of the 1st NAACL, 178?185.
K. Knight, D. Marcu. 2003. Summarization beyond sentence
extraction: A probabilistic approach to sentence compres-
sion. Artificial Intelligence, 139(1):91?107.
A. Knott. 1996. A Data-Driven Methodology for Motivating
a Set of Coherence Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of Edinburgh.
J. Lafferty, A. McCallum, F. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th ICML, 282?289.
V. J. Leffa. 1998. Clause processing in complex sentences. In
Proceedings of the 1st LREC, 937?943.
W. C. Mann, S. A. Thompson. 1987. Rhetorical structure the-
ory: A theory of text organization. Technical Report ISI/RS-
87-190, ISI, Los Angeles, CA, 1987.
D. Marcu. 1998. To build text summaries of high quality, nu-
clearity is not sufficient. In Working Notes of the AAAI-98
Spring Symposium on Intelligent Text Summarization, 1?8.
D. Marcu. 2000. The Theory and Practice of Discourse Parsing
and Summarization. The MIT Press, Cambridge, MA.
A. Mikheev. 1997. The LTG part of speech tagger. Technical
report, University of Edinburgh, 1997.
L. Pevzner, M. Hearst. 2002. A critique and improvement of
an evaluation metric for text segmentation. Computational
Linguistics, 28(1):19?36.
L. A. Ramshaw, M. P. Marcus. 1995. Text chunking using
transformation-based learning. In Proceedings of the 3rd
ACL Workshop on Very Large Corpora, 82?94.
S. Riezler, T. H. King, R. Crouch, A. Zaenen. 2003. Statistical
sentence condensation using ambiguity packing and stochas-
tic disambiguation methods for lexical-functional grammar.
In Proceedings of HLT/NAACL 2003, 118?125.
R. E. Schapire, Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
R. Soricut, D. Marcu. 2003. Sentence level discourse parsing
using syntactic and lexical information. In Proceedings of
HLT/NAACL 2003.
S. Teufel, M. Moens. 2002. Summarizing scientific articles ?
experiments with relevance and rhetorical status. Computa-
tional Linguistics, 28(4):409?446.
E. F. Tjong Kim Sang. 2000. Text chunking by system combi-
nation. In Proceedings of CoNLL-00, 151?153.
V. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience, New York.
J. Veenstra. 1998. Fast NP chunking using memory-based
learning techniques. In Proceedings of BENELEARN, 71?
79.
264
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 331?338, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Collective Content Selection for Concept-To-Text Generation
Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
regina@csail.mit.edu
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
A content selection component deter-
mines which information should be con-
veyed in the output of a natural language
generation system. We present an effi-
cient method for automatically learning
content selection rules from a corpus and
its related database. Our modeling frame-
work treats content selection as a col-
lective classification problem, thus allow-
ing us to capture contextual dependen-
cies between input items. Experiments
in a sports domain demonstrate that this
approach achieves a substantial improve-
ment over context-agnostic methods.
1 Introduction
Content selection is a fundamental task in concept-
to-text generation (Reiter and Dale, 2000). A practi-
cal generation system typically operates over a large
database with multiple entries that could potentially
be included in a text. A content selection compo-
nent determines what subset of this information to
include in the generated document.
For example, consider the task of automatically
generating game summaries, given a database con-
taining statistics on Americal football. Table 1
shows an excerpt from such a database, and its cor-
responding game summary written by a journalist.
A single football game is typically documented in
hundreds of database entries ? all actions, player
positions, and scores are recorded, along with a wide
range of comparative and aggregate statistics. Only
a small fraction of this information is featured in a
game summary. The content selection component
aims to identify this subset.1
In existing generation systems the content se-
lection component is manually crafted. Specify-
ing content selection rules is, however, notoriously
difficult, prohibitively so in large domains. It in-
volves the analysis of a large number of texts from a
domain-relevant corpus, familiarity with the associ-
ated database, and consultation with domain experts.
Moreover, the task must be repeated for each domain
anew.
This paper proposes a data-driven method for
learning the content-selection component for a
concept-to-text generation system. We assume that
the learning algorithm is provided with a parallel
corpus of documents and a corresponding database,
in which database entries that should appear in doc-
uments are marked.
One possible approach is to formulate content se-
lection as a standard binary classification task: pre-
dict whether an item is to be included on the basis
of its attributes alone. In fact, this method is com-
monly used for content selection in text summariza-
tion (e.g., Kupiec et al, 1995). However, by treating
each instance in isolation, we cannot guarantee that
the selected database entries are related in a mean-
ingful way, which is essential for the generation of a
coherent text.
Rather than selecting each item separately, we
propose a method for collective content selection,
where all candidates are considered simultaneously
for selection. Collective selection thereby allows
us to explicitly optimize coherence in the generated
1The organization of the selected information and its sur-
face realization is typically handled by other components of the
generation system, which are outside the scope of this paper.
331
Passing
PLAYER CP/AT YDS AVG TD INT
Brunell 17/38 192 6.0 0 0
Garcia 14/21 195 9.3 1 0
. . . . . . . . . . . . . . . . . .
Rushing
PLAYER REC YDS AVG LG TD
Suggs 22 82 3.7 25 1
. . . . . . . . . . . . . . . . . .
Fumbles
PLAYER FUM LOST REC YDS
Coles 1 1 0 0
Portis 1 1 0 0
Davis 0 0 1 0
Little 0 0 1 0
. . . . . . . . . . . . . . .
Suggs rushed for 82 yards and scored a
touchdown in the fourth quarter, leading
the Browns to a 17-13 win over the Wash-
ington Redskins on Sunday. Jeff Garcia
went 14-of-21 for 195 yards and a TD for
the Browns, who didn?t secure the win until
Coles fumbled with 2:08 left. The Redskins
(1-3) can pin their third straight loss on go-
ing just 1-for-11 on third downs, mental mis-
takes and a costly fumble by Clinton Por-
tis. Brunell finished 17-of-38 for 192 yards,
but was unable to get into any rhythm because
Cleveland?s defense shut down Portis. The
Browns faked a field goal, but holder Der-
rick Frost was stopped short of a first down.
Brunell then completed a 13-yard pass to
Coles, who fumbled as he was being taken
down and Browns safety Earl Little recov-
ered.
Table 1: Sample target game description and example of database entries; boldface indicates correspon-
dences between the text and the database (CP/AT: completed out of attempted, YDS: yards, AVG: average,
TD: touchdown, INT: interception, REC: received, LG: longest gain, FUM: fumble).
text: semantically related entries are often selected
together. In essence, the algorithm seeks a subset
of candidates that is consistent with the individual
preferences of each candidate, and at the same time
maximally satisfies contextual constraints. A graph-
based formulation of this optimization problem al-
lows us to find an exact, globally optimal solution,
using a min-cut algorithm.
Collective content selection is particularly ben-
eficial to generation systems that operate over re-
lational databases. Rich structural information
available in a database can be readily utilized to
determine semantic relatedness between different
database entries. For instance, we can easily find
all actions (e.g., touchdowns and fumbles) associ-
ated with a specific player in a game, which could be
relevant for generating a summary centered around
an individual. We show how to utilize database re-
lations for discovering meaningful contextual links
between database entries.
We evaluate our collective content selection
model in a sports domain. The proposed content
selection component operates over a large database
containing descriptive statistics about American
football games. Our model yields a 10% increase in
F-score, when compared to a standard classification
approach, thus demonstrating the benefits of collec-
tive content selection on this complex domain. Fur-
thermore, our results empirically confirm the contri-
bution of discourse constraints for content selection.
In the following section, we provide an overview
of existing work on content selection. Then, we de-
fine the learning task and introduce our approach for
collective content selection. Next, we present our
experimental framework and data. We conclude the
paper by presenting and discussing our results.
2 Related Work
The generation literature provides multiple exam-
ples of content selection components developed for
various domains (Kukich, 1983; McKeown, 1985;
Sripada et al, 2001; Reiter and Dale, 2000). A com-
mon theme across different approaches is the em-
phasis on coherence: related information is selected
?to produce a text that hangs together? (McKeown,
1985). Similarly, our method is also guided by co-
herence constraints. In our case these constraints are
derived automatically, while in symbolic generation
systems coherence is enforced by analyzing a large
number of texts from a domain-relevant corpus and
332
careful hand-crafting of content selection rules.
Duboue and McKeown (2003) were the first to
propose a method for learning content selection
rules automatically, thus going beyond mere corpus
analysis. They treat content selection as a classifi-
cation task. Given a collection of texts associated
with a domain-specific database, their model learns
whether a database entry should be selected for pre-
sentation or not. Their modeling approach uses an
expressive feature space while considering database
entries in isolation.
Similarly to Duboue and McKeown (2003), we
view content selection as a classification task and
learn selection rules from a database and its corre-
sponding corpus. In contrast to them, we consider
all database entries simultaneously, seeking a glob-
ally optimal selection. Thus, we avoid the need for
extensive feature engineering by incorporating dis-
course constraints into the learning framework. In
addition, we assess whether data-driven methods for
content selection scale up to large databases with
thousands of interrelated entries, by evaluating our
model in a sports domain. Previous work (Duboue
and McKeown, 2003) has tackled the content selec-
tion problem for biographical summaries, a simpler
domain with fewer entities and interactions among
them.
3 The Task
We assume that the content selection component
takes as input a set of database entries.2 Each en-
try has a type and a set of attributes associated with
its type. For instance, the database shown in Table 1
contains entries of three types ? Passing, Rushing
and Fumbles. Two entries are of type Passing, and
each of them has six attributes ? PLAYER, CP/AT,
YDS, AVG, TD, INT. In addition, each entry has a la-
bel that specifies whether it should be included in a
generated text or not.
During the training process, the learning algo-
rithm is provided with n sets of database entries,
each associated with a label whose value is known.
In practice, we only require a parallel corpus of
game summaries and database entries ? label val-
ues are derived automatically via alignment (see
Section 4 for more details).
2A terminological note: a database entry is analogous to a
row in a relational table; throughout this paper we use the terms
entity and database entry interchangeably.
The goal of the content selection component is
to select entries from a database, i.e., to determine
whether their label values are 0 or 1. Under this for-
mulation, content selection is restricted to informa-
tion available in the database; there is no attempt to
induce new facts through inference.
In the next section, we describe our learning
framework, and explain how it is applied to the con-
tent selection task.
3.1 The Collective Classification Approach
Generation of a coherent text crucially depends on
our ability to select entities that are related in a
meaningful way (McKeown, 1985). A content se-
lection component that considers every entity in iso-
lation does not have any means to enforce this im-
portant discourse constraint. We therefore formulate
content selection as a collective classification task,
where all entities that belong to the same database
(i.e., the same football game) are considered simul-
taneously. This framework thus enables us to en-
force contextual constraints by selecting related en-
tities.
When considered in isolation, some database en-
tries are more likely to be selected than others. In
the American football domain, for example, entries
of type Rushing are often extracted if they yield a
touchdown.3 Other Rushing entries (e.g., which do
not deliver scoring points) are typically omitted. In
general, the attributes of an entry can provide use-
ful cues for predicting whether it should be selected.
Therefore, we can perform content selection by ap-
plying a standard classifier on each entry. In Sec-
tion 3.2, we explain in more detail how such a clas-
sifier can be trained.
We can also decide about entity selection by an-
alyzing how entities relate to each other in the
database. For instance, in a game where both quar-
terbacks4 score, it is fairly unorthodox to mention
the passing statistics for only one of them. Label as-
signments in which either both quarterbacks are se-
lected, or both of them are omitted should be there-
3A touchdown is the primary method of scoring in American
football; a touchdown is worth six points and is accomplished
by gaining legal possession of the ball in the opponent?s end
zone.
4A quarterback in American football is the leader of a team?s
offense. In most offenses his primary duty is passing the ball.
Quarterbacks are typically evaluated on their passing statistics,
including total yardage, completion ratio, touchdowns, and the
ability to avoid interceptions.
333
fore preferred. This relation between quarterback
passing statistics exemplifies one type of link that
can hold between entities. Other link types may
encode contextual constraints, for instance captur-
ing temporal and locational information. (In Sec-
tion 3.3, we describe a method for discovering link
types which encapsulate meaningful contextual de-
pendencies.) By taking into account links between
related entities, a content selection component can
enforce dependencies in the labeling of related enti-
ties.
Our goal is to select a subset of database entities
that maximally satisfies linking constraints and is
as consistent as possible with the individual prefer-
ences of each entity. Thus, content selection can be
naturally stated as an optimization problem ? we
wish to find a label assignment that minimizes the
cost of violating the above constraints.
Let C+ and C? be a set of selected and omitted en-
tities, respectively; ind+(x) and ind?(x) are scores
that capture the individual preference of x to be ei-
ther selected or omitted, and linkL(x,y) reflects the
degree of dependence between the labels of x and y
based on a link of type L. Thus, the optimal label
assignment for database entries x1, . . . ,xn will mini-
mize:
?
x?C+
ind?(x)+ ?
x?C?
ind+(x)+?
L
?
xi?C+
x j?C?
linkL(xi,x j)
The first two elements in this expression cap-
ture the penalty for assigning entities to classes
against their individual preferences. For instance,
the penalty for selecting an entry x ? C+ will equal
ind?(x), i.e., x?s individual preference of being om-
mitted. The third term captures a linking penalty for
all pairs of entities (xi,x j) that are connected by a
link of type L, and are assigned to different classes.
This formulation is similar to the energy mini-
mization framework, which is commonly used in
image analysis (Besag, 1986; Boykov et al, 1999)
and has been recently applied in natural language
processing (Pang and Lee, 2004). The principal ad-
vantages of this formulation lie in its computational
properties. Despite seeming intractable ? the num-
ber of possible subsets to consider for selection is
exponential in the number of database entities ? the
inference problem has an exact solution. Provided
that the scores ind+(x), ind?(x), and linkL(x,y) are
positive, we can find a globally optimal label as-
signment in polynomial time by computing a min-
imal cut partition in an appropriately constructed
graph (Greig et al, 1989).
In the following we first discuss how individual
preference scores are estimated. Next, we describe
how to induce links and estimate their scores.
3.2 Computing Individual Preference Scores
The individual preference scores are estimated by
considering the values of entity attributes, recorded
in the database. The type and number of the at-
tributes are determined by the entity type. There-
fore, we separately estimate individual preference
scores for each entity type. For example, individ-
ual scores for entities of type Passing are computed
based on six attributes : PLAYER, CP/AT, YDS, AVG,
TD, INT (see Table 1).
Considerable latitude is available when selecting
a classifier for delivering the individual preference
scores. In our experiments we used the publicly
available BoosTexter system (Schapire and Singer,
2000). BoosTexter implements a boosting algo-
rithm that combines many simple, moderately accu-
rate categorization rules into a single, highly accu-
rate rule. For each example, it outputs a prediction
along with a weight whose magnitude indicates the
classifier?s confidence in the prediction. We thus set
the individual preference scores to the weights ob-
tained from BoosTexter. The weights range from ?1
to 1; we obtained non-negative numbers, simply by
adding 1.
It is important to note that BoosTexter is a fairly
effective classifier. When applied to text categoriza-
tion (Schapire and Singer, 2000), it outperformed a
number of alternative classification methods, includ-
ing Naive Bayes, decision trees, and k-nearest neigh-
bor.
3.3 Link Selection and Scoring
The success of collective classification depends on
finding links between entities with similar label pref-
erences. In our application ? concept-to-text gen-
eration, it is natural to define entity links in terms
of their database relatedness. Since the underlying
database contains rich structural information, we can
explore a wide range of relations between database
entities.
The problem here is finding a set of links that
334
capture important contextual dependencies among
many possible combinations. Instead of manu-
ally specifying this set, we propose a corpus-driven
method for discovering links automatically. Auto-
matic link induction can greatly reduce human ef-
fort. Another advantage of the method is that it can
potentially identify relations that might escape a hu-
man expert and yet, when explicitly modeled, aid in
content selection.
We induce important links by adopting a
generate-and-prune approach. We first automati-
cally create a large pool of candidate links. Next, we
select only links with aconsistent label distributions.
Construction of Candidate Links An important
design decision is the type of links that we allow
our algorithm to consider. Since our ultimate goal is
the generation of a coherent text, we wish to focus
on links that capture semantic connectivity between
database entities. An obvious manifestation of se-
mantic relatedness is attribute sharing. Therefore,
we consider links across entities with one or more
shared attributes. An additional constraint is implied
by computational considerations: our optimization
framework, based on minimal cuts in graphs, sup-
ports only pairwise links, so we restrict our attention
to binary relations.
We generate a range of candidate link types us-
ing the following template: For every pair of entity
types Ei and E j, and for every attribute k that is asso-
ciated with both of them, create a link of type Li, j,k.
A pair of entities ?a,b? is linked by Li, j,k , if a is of
type Ei, b is of type E j and they have the same value
for the attribute k. For example, a link that asso-
ciates statistics on Passing and Rushing performed
by the same player is an instantiation of the above
with Ei = Rushing, E j = Passing, and k = Player.
In a similar fashion, we construct link types that
connect together entities with two or three attributes
in common. Multiple pairs of entries can be con-
nected by the same link type.
If the database consists of n entity types, and the
number of attribute types is bounded by m, then
the number of link types constructed by this process
does not exceed O(n2(m +
(
m
2
)
+
(
m
3
)
)) ? O(n2m3).
In practice, this bound is much lower, since only a
few attributes are shared among entity types. Links
can be efficiently computed using SQL?s SELECT op-
erator.
Link Filtering Only a small fraction of the auto-
matically generated link types will capture meaning-
ful contextual dependencies. To filter out spurious
links, we turn to the labels of the entities partici-
pating in each link. Only link types in which en-
tities have a similar distribution of label values are
selected from the pool of candidates.
We measure similarity in label distribution using
the ?2 test. This test has been successfully applied to
similar tasks, such as feature selection in text clas-
sification (Rogati and Yang, 2002), and can be eas-
ily extended to our application. Given a binary link,
our null hypothesis H0 is that the labels of entities
related by L are independent. For each link, we
compute the ?2 score over a 2-by-2 table that stores
joint label values of entity pairs, computed across all
database entries present in the training set. For links
with ?2 > ?, the null hypothesis is rejected, and the
link is considered a valid discourse constraint. The
value of ? is set to 3.84, which corresponds to a 5%
level of statistical significance.
Link Weights The score of a link type L is defined
as follows:
linkL(x,y) =
{
?L i f (x,y) are linked by L
0 otherwise
We estimate link weights ?L using simulated anneal-
ing. The goal is to find weight values that minimize
an objective function, defined as the error rate on
the development set5 (see Section 4 for details). The
individual scores and the link structure of the enti-
ties in the development set are predicted automat-
ically using the models trained on the training set.
Starting from a random assignment of weight val-
ues, we compute the objective function and generate
new weight values using Parks? (1990) method. The
procedure stops when no sufficient progress is ob-
served in subsequent iterations.
4 Evaluation Framework
We apply the collective classification method just
presented to the task of automatically learning con-
tent selection rules from a database containing
football-related information. In this section, we first
present the sport domain we are working with, and
5Our objective function cannot be optimized analytically.
We therefore resort to heuristic search methods such as simu-
lated annealing.
335
Entity Type Attr Inst %Aligned Entity Type Attr Inst %Aligned
Defense 8 14,077 0.00 Passing 5 1,185 59.90
Drive 10 11,111 0.00 Team comparison 4 14,539 0.00
Play-by-Play 8 83,704 3.03 Punt-returns 8 940 5.74
Fumbles 8 2,937 17.78 Punting 9 950 0.87
Game 6 469 0.00 Receiving 8 6,337 11.19
Interceptions 6 894 45.05 Rushing 8 3,631 9.17
Kicking 8 943 26.93 Scoring-sum 9 3,639 53.34
Kickoff-returns 8 1,560 5.24 Team 3 4 0.00
Officials 8 464 0.00
Table 2: Entity types and their attributes in the NFL database; percentage of database entries that are aligned
to summary sentences.
describe how we collected a corpus for evaluating
collective content selection. Next, we explain how
we automatically obtained annotated data for train-
ing and testing our model.
Data As mentioned previously our goal is to
generate descriptions of football games. The
sports domain has enjoyed popularity among natu-
ral language generation practitioners (Robin, 1994;
Tanaka-Ishii et al, 1998). The appeal is partly due
to the nature of the domain ? it exhibits several
fixed patterns in content organization and is there-
fore amenable to current generation approaches. At
the same time, it is complex enough to present chal-
lenges at almost all stages of the generation process.
We compiled a corpus of descriptions of football
games from the web. More specifically, we obtained
game summaries from the official site of the Ameri-
can National Football League6 (NFL). We collected
summaries for the 2003 and 2004 seasons. These
are typically written by Associated Press journalists.
The corpus consists of 468 texts in total (436,580
words). The average summary length is 46.8 sen-
tences.
The site not only contains a summary for each
game, but also a wealth of statistics describing the
performance of individual players and their teams.
It includes a scoring summary and a play-by-play
summary giving details of the most important events
in the game together with temporal (i.e., time re-
maining) and positional (i.e., location in the field)
information. In sum, for each game the site offers
a rich repository of tabulated information which we
translated into a relational database. An excerpt of
6See http://www.nfl.com/scores.
the database is shown in Table 1. Table 2 displays
the entity types contained in our NFL database and
lists the number of attributes (Attr) and instantia-
tions (Inst) per type. The database contains 73,400
entries in total.
Alignment Recall that our collective classification
method is supervised. The training instances are
database entries and the class labels indicate whether
an instance should be selected for presentation or
not. We could obtain this information via manual an-
notation performed by domain experts. Instead, we
opted for a less costly, automatic solution that yields
large quantities of training and testing data. To in-
fer which database entries correspond to sentences
in the verbalized game summaries, we used a sim-
ple anchor-based alignment technique. In our do-
main, numbers and proper names appear with high
frequency, and they constitute reliable anchors for
alignment. Similar to previous work (Duboue and
McKeown, 2003; Sripada et al, 2001), we employ
a simple matching procedure that considers anchor
overlap between entity attributes and sentence to-
kens.
Overall, the alignment procedure produced 7,513
pairs. 7.1% of the database entries were verbalized
in our corpus and 31.7% of the corpus sentences had
a database entry. Table 2 presents the proportion of
database entries which are verbalized in our corpus,
broken down by entity type (see %Aligned).
To evaluate the accuracy of this procedure, we
compared our output with a gold-standard align-
ment produced by a domain expert. After analyz-
ing the data from five games, the expert produced
52 alignment pairs; 47 of these pairs were identified
336
Majority Baseline Standard Classifier Collective Classifier
Prec Rec F-score Prec Rec F-score Prec Rec F-score
Mean 29.40 68.19 40.09 44.88 62.23 49.75 52.71 76.50 60.15
Min 3.57 28.57 6.45 12.50 8.33 13.33 12.50 27.27 19.05
Max 57.14 100.00 65.12 76.92 100.00 75.00 100.00 100.00 100.00
Std Dev 10.93 15.75 12.25 15.36 18.33 13.98 21.29 18.93 19.66
Table 3: Results on content selection (precision, recall and F-score are averages over individual game sum-
maries); comparison between the majority baseline, standard and collective classification.
by the automatic alignment. In addition, three pairs
produced by the program did not match the gold-
standard alignment. Thus, the automatic method
achieved 94.0% precision and 90.4% recall.
Data Annotation For training and testing pur-
poses, we only considered entity types for
which alignments were observed in our corpus
(e.g., Fumbles, Interceptions; see Table 2).
Types without alignments can be trivially regarded
as inappropriate for selection in the generated text.
We considered database entries for which we found
verbalizations in the corpus as positive instances
(i.e., they should be selected); accordingly, non-
verbalized entries were considered negative in-
stances (i.e., they should not be selected). The
overall dataset contained 105,792 instances (corre-
sponding to 468 game summaries). Of these, 15%
(68 summaries) were reserved for testing. We held
out 1,930 instances (10 summaries) from the train-
ing data for development purposes.
5 Results
Our results are summarized in Table 3. We compare
the performance of the collective classifier against a
standard classifier. This can be done in our frame-
work, simply by setting the link scores to zero. We
also report the performance of a majority baseline.
The latter was obtained by defaulting to the major-
ity class for each entity type in the training data. As
can be seen from Table 2, only for two relations ?
Passing and Scoring-sum ? the majority class
predicts that the corresponding database instances
should be selected for presentation.
Our results confirm that a content selection com-
ponent can be automatically engineered for the foot-
ball domain. The collective classifier achieves an
F-score of 60.15%. This result compares favor-
ably with Duboue and McKeown (2003) whose best
model has an F-score of 51.00% on a simpler do-
main. Our method has high recall (we want to
avoid missing out information that should be pre-
sented in the output) but tends to overgenerate as
demonstrated by the relatively moderate precision
in Table 3. Erroneous content selection decisions
could be remedied by other components later in the
generation process. Alternatively, the obtained con-
tent selection rules could be further refined or post-
processed by a domain expert. Finally, better clas-
sification performance should be possible with more
expressive feature sets. As we can see from the weak
performance of the standard classifier, attribute val-
ues of database entries may not be sufficiently strong
predictors. Considering additional features tailored
to the NFL domain could further enhance perfor-
mance. However, feature selection is not one of the
main objectives of this work.
Our results empirically validate the importance of
discourse constraints for content selection (Table 4
illustrates examples of constraints that the model
discovered). We observe that adding contextual in-
formation leads to a 10.4% F-score increase over the
standard classifier. We used a paired t test to exam-
ine whether the differences are statistically signifi-
cant. The collective model significantly outperforms
the standard model on both precision (t = 4.824,
p < 0.01) and recall (t = 8.445, p < 0.01). It is also
significantly better than the majority baseline, both
in terms of recall (t = 3.181, p < 0.01) and preci-
sion (t = 8.604, p < 0.01). The standard classifier
performs significantly better than the majority base-
line on precision (t = 7.043, p < 0.01) but worse on
recall (t =-2.274, p < 0.05).
6 Conclusions and Future Work
In this paper we have presented a novel, data-driven
method for automating content selection. Central
337
{?a,b? | a ? Sum?b ? Sum?a.Quarter = b.Quarter}
{?a,b? | a ? Sum?b ? Play?Sum.Player1 = Play.Player1 ?Sum.Action = Play.Action}
{?a,b? | a ? Fumbles?b ? Interceptions ?Fumbles.Player = Interceptions.Player}
Table 4: Examples of automatically derived links.
to our approach is the use of a collective classifi-
cation model that captures contextual dependencies
between input items. We show that incorporation
of discourse constraints yields substantial improve-
ment over context-agnostic methods. Our approach
is linguistically grounded, computationally efficient,
and viable in practical applications.
In the future, we plan to explore how to integrate
more refined discourse models in the content selec-
tion process. Currently, we consider a limited set of
contextual dependencies based on attribute similar-
ity. Ideally, we would like to express more complex
relations between items. For instance, we may want
to represent disjunctive constraints, such as ?at least
one of the defense players should be mentioned in
the summary.? Such dependencies can be efficiently
handled in a collective classification framework by
using approximate probabilistic inference (Taskar et
al., 2002). Another promising approach is the com-
bination of our automatically acquired cross-entity
links with domain knowledge.
Needless to say, content selection is one of sev-
eral components within a working generation sys-
tem. An interesting question is how to integrate our
component into a generation pipeline, using feed-
back from other components to guide collective con-
tent selection.
Acknowledgments
The authors acknowledge the support of the National Science
Foundation (Barzilay; CAREER grant IIS-0448168 and grant
IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).
We are grateful to Eli Barzilay for his help with data collec-
tion, and Luke Zettelmoyer who explained the many rules of
American football to us. Thanks to Michael Collins, Amit
Dubey, Noemie Elhadad, Dina Katabi, Frank Keller, Igor
Malioutov, Smaranda Muresan, Martin Rinard, Kevin Simler
and the anonymous reviewers for helpful comments and sug-
gestions. Any opinions, findings, and conclusions or recom-
mendations expressed above are those of the authors and do not
necessarily reflect the views of the National Science Foundation
or EPSRC.
References
J. Besag. 1986. On the statistical analysis of dirty pic-
tures. Journal of the Royal Statistical Society, 48:259?
302.
Y. Boykov, O. Veksler, R. Zabih. 1999. Fast approximate
energy minimization via graph cuts. In ICCV, 377?
384.
P. A. Duboue, K. R. McKeown. 2003. Statistical acqui-
sition of content selection rules for natural language
generation. In Proceedings of the EMNLP, 121?128.
D. Greig, B. Porteous, A. Seheult. 1989. Exact maxi-
mum a posteriori estimation for binary images. Jour-
nal of the Royal Statistical Society, 51(2):271?279.
K. Kukich. 1983. Design of a knowledge-based report
generator. In Proceedings of the ACL, 145?150.
J. Kupiec, J. O. Pedersen, F. Chen. 1995. A trainable
document summarizer. In Proceedings of the SIGIR,
68?73.
K. R. McKeown. 1985. Text Generation: Using Dis-
course Strategies and Focus Constraints to Generate
Natural Language Text. Cambridge University Press.
B. Pang, L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based
on minimum cuts. In Proceedings of the ACL, 271?
278, Barcelona, Spain.
G. Parks. 1990. An intelligent stochastic optimization
routine for nuclear fuel cycle design. Nuclear Tech-
nology, 89:233?246.
E. Reiter, R. Dale. 2000. Building Natural Language
Generation Systems. Cambridge University Press,
Cambridge.
J. Robin. 1994. Revision-Based Generation of Natu-
ral Language Summaries Providing Historical Back-
ground. Ph.D. thesis, Columbia University.
M. Rogati, Y. Yang. 2002. High-performing feature se-
lection for text classification. In Proceedings of the
CIKM, 659?661.
R. E. Schapire, Y. Singer. 2000. Boostexter: A boosting-
based system for text categorization. Machine Learn-
ing, 39(2/3):135?168.
S. G. Sripada, E. Reiter, J. Hunter, J. Yu. 2001. A two-
stage model for content determination. In Proceedings
of the ACL-ENLG, 3?10.
K. Tanaka-Ishii, K. Hasida, I. Noda. 1998. Reactive
content selection in the generation of real-time soc-
cer commentary. In Proceedings of the ACL/COLING,
1282?1288.
B. Taskar, P. Abbeel, D. Koller. 2002. Discriminative
probabilistic models for relational data. In Proceed-
ings of the UAI, 485?495.
338
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 859?866, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Cross-linguistic Projection of Role-Semantic Information
Sebastian Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
pado@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
This paper considers the problem of auto-
matically inducing role-semantic annota-
tions in the FrameNet paradigm for new
languages. We introduce a general frame-
work for semantic projection which ex-
ploits parallel texts, is relatively inexpen-
sive and can potentially reduce the amount
of effort involved in creating semantic re-
sources. We propose projection models
that exploit lexical and syntactic informa-
tion. Experimental results on an English-
German parallel corpus demonstrate the
advantages of this approach.
1 Introduction
Shallow semantic parsing, the task of automatically
identifying the semantic roles conveyed by senten-
tial constituents, has recently attracted much atten-
tion, partly because of its increasing importance for
potential applications. For instance, information ex-
traction (Surdeanu et al, 2003), question answer-
ing (Narayanan and Harabagiu, 2004) and machine
translation (Boas, 2002) could stand to benefit from
broad coverage semantic processing.
The FrameNet project (Fillmore et al, 2003)
has played a central role in this endeavour by
providing a large lexical resource based on se-
mantic roles. In FrameNet, meaning is represented
by frames, schematic representations of situations.
Semantic roles are frame-specific, and are called
frame elements. The database associates frames with
lemmas (verbs, nouns, adjectives) that can evoke
them (called frame-evoking elements or FEEs), lists
the possible syntactic realisations of their seman-
tic roles, and provides annotated examples from the
British National Corpus (Burnard, 1995). The avail-
ability of rich annotations for the surface realisation
of semantic roles has triggered interest in semantic
parsing and enabled the development of data-driven
models (e.g., Gildea and Jurafsky, 2002).
Frame: DEPARTING
THEME The officer left the house.
The plane leaves at seven.
His departure was delayed.
SOURCE We departed from New York.
He retreated from his opponent.
The woman left the house.Fr
am
e
E
le
m
en
ts
F
E
E
s abandon.v, desert.v, depart.v, departure.n,
emerge.v, emigrate.v, emigration.n, escape.v,
escape.n, leave.v, quit.v, retreat.v, retreat.n,
split.v, withdraw.v, withdrawal.n
Table 1: Example of FrameNet frame
Table 1 illustrates an example from the FrameNet
database, the DEPARTING frame. It has two roles, a
THEME which is the moving object and a SOURCE
expressing the initial position of the THEME. The
frame elements are realised by different syntactic ex-
pressions. For instance, the THEME is typically an
NP, whereas the SOURCE is often expressed by a
prepositional phrase (see the expressions in boldface
in Table 1). The DEPARTING frame can be evoked
by abandon, desert, depart, and several other verbs
as well as nouns (see the list of FEEs in Table 1).
Although recent advances in semantic parsing1
have greatly benefited from the availability of the
English FrameNet, unfortunately such resources are
largely absent for other languages. The English
FrameNet (Version 1.1) contains 513 frames cov-
ering 7,125 lexical items and has been under de-
velopment for approximately six years. Although
FrameNets are currently under construction for Ger-
man, Spanish, and Japanese, these resources are still
in their infancy and of limited value for modelling
purposes. Methods for acquiring FrameNets from
corpora automatically would greatly reduce the hu-
man effort involved and facilitate their development
for new languages.
In this paper, we propose a method which em-
ploys parallel corpora for acquiring frame elements
1Approaches to modelling semantic parsing are too numer-
ous to list; see Carreras and M?rquez (2005) for an overview.
859
and their syntactic realisations (see the upper half of
Table 1) for new languages. Our method leverages
the existing English FrameNet to overcome the re-
source shortage in other languages by exploiting the
translational and structural equivalences present in
aligned data. The idea underlying our approach can
be summarised as follows: (1) given a pair of sen-
tences E (English) and L (new language) that are
translations of each other, annotate E with seman-
tic roles; and then (2) project these roles onto L. In
this manner, we induce semantic structure on the L
side of the parallel text, which can then serve as data
for training a statistical semantic parser for L that is
independent of the parallel corpus.
We first assess if the main assumption of semantic
projection is warranted (Section 3), namely whether
frames and semantic roles exhibit a high degree of
parallelism across languages. Then we propose two
broad classes of projection models that utilise lexi-
cal and syntactic information (Section 4), and show
experimentally that roles can be projected from En-
glish onto German with high accuracy (Section 5).
We conclude the paper by discussing the implica-
tions of our results and future work (Section 6).
2 Related work
A number of recent studies exploit parallel cor-
pora for cross-linguistic knowledge induction. In
this paradigm, annotations for resource-rich lan-
guages like English are projected onto another lan-
guage through aligned parallel texts. Yarowsky et
al. (2001) propose several projection algorithms for
deriving monolingual tools (ranging from part-of-
speech taggers, to chunkers and morphological anal-
ysers) without additional annotation cost. Hwa et
al. (2002) assess the degree of syntactic parallelism
in dependency relations between English and Chi-
nese. Their results show that, although assuming di-
rect correspondence is often too restrictive, syntactic
projection yields good enough annotations to train
a dependency parser. Smith and Smith (2004) ex-
plore syntactic projection further by proposing an
English-Korean bilingual parser integrated with a
word translation model.
Previous work has primarily focused on the pro-
jection of morphological and grammatico-syntactic
information. Inducing semantic resources from low
density languages still poses a significant challenge
to data-driven methods. The challenge is recognised
by Fung and Chen (2004) who construct a Chinese
FrameNet by mapping English FrameNet entries to
concepts listed in HowNet2, an on-line ontology for
Chinese, however without exploiting parallel texts.
The present work extends previous approaches on
annotation projection by inducing FrameNet seman-
tic roles from parallel corpora. Analogously to Hwa
et al (2002), we investigate whether there are indeed
semantic correspondences between two languages,
since there is little hope for projecting meaningful
annotations in nonparallel semantic structures. Sim-
ilarly to Fung and Chen (2004) we automatically in-
duce semantic role annotations for a target language.
In contrast to them, we resort to parallel corpora as a
source of semantic equivalence. Thus, we avoid the
need for a target concept dictionary in addition to the
English FrameNet. We propose a general framework
for semantic projection that can incorporate different
knowledge sources. To our knowledge, the frame-
work and its application to semantic role projection
are novel.
3 Creation of a Gold Standard Corpus
Sample Selection. To evaluate the output of our
projection algorithms, we created a gold standard
corpus of English-German sentence pairs with man-
ual FrameNet frame and role annotations. The sen-
tences were sampled from Europarl (Koehn, 2002),
a corpus of professionally translated proceedings of
the European Parliament. Europarl is available in
11 languages with up to 20 million words per lan-
guage aligned at the document and sentence level.
Recall that frame projection is only meaningful if
the same frame is appropriate for both sentences in
a projection pair. This constrains sample selection
for two reasons: first, FrameNet is as yet incom-
plete with respect to its coverage. So, a randomly
selected sentence pair may evoke novel frames or
novel senses of already existing frames (e.g., the
?greeting? sense of hail which is currently not listed
in FrameNet). Second, due to translational variance,
there is no a priori guarantee that words which are
mutual translations evoke the same frame. For ex-
ample, the English verb finish is often translated
in German by the adverb abschlie?end, which ar-
guably cannot have a role set identical to finish. Re-
lying solely on the English FrameNet database for
sampling would yield many sentence pairs which
are either inappropriate for the present study (be-
cause they do not evoke the same frames) or simply
problematic for annotation since they are outside the
2See http://www.keenage.com/zhiwang/e_zhiwang.
html.
860
present coverage of the database.
For the above reasons, our sample selection pro-
cedure was informed by two existing resources,
the English FrameNet and SALSA, a FrameNet-
compatible database for German currently under de-
velopment (Erk et al, 2003). We first used the pub-
licly available GIZA++ (Och and Ney, 2003) soft-
ware to induce English-German word alignments.
Next, we gathered all German-English sentences
in the corpus that had at least one pair of aligned
words (we,wg), which were listed in FrameNet and
SALSA, respectively, and had at least one frame
in common. These sentences exemplify 83 frame
types, 696 lemma pairs, and 265 unique English and
178 unique German lemmas. Sentence pairs were
grouped into three bands according to their frame
frequency (High, Medium, Low). We randomly se-
lected 380 pairs from each band. The total sample
consisted of ,140 sentence pairs.
This procedure produces a realistic corpus sample
for the role projection task; similar samples can be
drawn for new language pairs using either existing
bilingual dictionaries (Fung and Chen, 2004) or au-
tomatically constructed semantic lexicons (Pad? and
Lapata, 2005).
Annotation. Two annotators, with native-level
proficiency in German and English, manually la-
belled the parallel corpus with semantic information.
Their task was to identify the frame for a given pred-
icate in a sentence, and assign the corresponding
roles. They were provided with detailed guidelines
that explained the task using multiple examples.
During annotation, they had access to parsed ver-
sions of the sentences in question (see Section 5 for
details), and to the English FrameNet and SALSA.
The annotation proceeded in three phases: a train-
ing phase (40 sentences), a calibration phase (100
sentences), and a production mode phase (1000 sen-
tences). In the calibration phase, sentences were
doubly annotated to assess inter-annotator agree-
ment. In production mode, sentences were split into
two distinct sets, each of which was annotated by a
single coder. We ensured that no annotator saw both
parts of any sentence pair to guarantee independent
annotation of the bilingual data. Each coder anno-
tated approximately the same amount of data in En-
glish and German.
Table 2 shows the results of our inter-annotator
agreement study. In addition to the widely used
Kappa statistic, we computed a number of different
agreement measures: the ratio of frames common
Measure English German All
Frame Match 0.90 0.87 0.88
Role Match 0.95 0.95 0.95
Span Match 0.85 0.83 0.84
Kappa 0.86 0.90 0.87
Table 2: Monolingual inter-annotation agreement on
the calibration set
Measure Precision Recall F-score
Frame Match 0.72 0.72 0.72
Role Match 0.91 0.92 0.91
Table 3: Cross-lingual semantic parallelism between
English and German
between two sentences (Frame Match), the ratio of
common roles (Role Match), and the ratio of roles
with identical spans (Span Match). As can be seen,
annotators tend to agree in frame assignment; dis-
agreements are mainly due to fuzzy distinctions be-
tween frames (e.g., between AWARENESS and CER-
TAINTY). As can be seen from Table 2, annotators
agree in what roles to assign (Role Match is 0.95 for
both English and German); agreeing on their exact
spans is a harder problem.
Semantic Parallelism. Since we obtained par-
allel FrameNet annotations for English and German,
we were able to investigate the degree of semantic
parallelism between the two languages. More specif-
ically, we treated the German annotation as gold
standard against which we compared the English an-
notations. To facilitate comparisons with the output
of our automatic projection methods (see Section 4),
we measured parallelism using precision and recall.
Frames and frame roles were counted as matching if
they were annotated in a sentence, regardless of their
spans. The results are shown in Table 3.
The cross-lingual data exhibit more than twice the
amount of frame differences than monolingual data
(compare Tables 2 and 3). This indicates that frame
disambiguation methods must be employed in auto-
matic role projection to ensure that two aligned to-
kens evoke the same frame. However, frame disam-
biguation is outside the scope of the present paper.
On the positive side, role agreement is rela-
tively high (0.91 F-score). This indicates that in
cases where frames match across languages, seman-
tic roles could be accurately transferred (provided
that these languages diverge little in their argument
structure). This observation offers support for the
861
projection approach put forward in this paper. Note,
however, that a practical projection system could at-
tain this level of performance only if it could employ
an oracle to recover annotators? decisions about the
span of roles. We can obtain a more realistic upper
bound for an automatic system from the monolin-
gual Role Span agreement figure (F-score 0.84). The
latter represents a ceiling for the agreement we can
expect from sentences annotated by different anno-
tators.
4 Projection of Semantic Information
In this section, we formalise the semantic projection
task and give the details of our modelling approach.
All models discussed here project semantic annota-
tions from a source language to a target language.
As explained earlier, our present study is only con-
cerned with the projection of roles between match-
ing frames.
4.1 Problem Formulation
We assume that we are provided with source and tar-
get sentences represented as sets of entities es ? Es
and et ? Et . These entities can be words, con-
stituents, phrases, or other groupings. In addition,
we are given the semantic annotation of the source
sentences from which we can directly read off the
source semantic role assignment as : R? 2Es , where
R is the set of semantic roles. The goal of the pro-
jection is to specify the target semantic role assign-
ments at : R? 2Et , which are unknown.3
Clearly, effecting the projection requires estab-
lishing some form of match between the source and
target entities. We therefore formalise projection as
a function which maps the source role assignment
and a set of matches M ? Es?Et onto a new target
role assignment:
pro j : (As?M)? (R? 2
Et ) (1)
By way of currying, we can state the new target role
assignment as a function which directly computes a
set of target entities, given the source role assign-
ment, a set of entity matches, and a role:
at : (As?M?R)? 2
Et (2)
According to this formalisation, the crucial part of
semantic projection is to identify a correct and ex-
haustive set of entity matches. Obviously, this raises
3Without loss of generality, we limit ourselves to one frame
per sentence, as does FrameNet.
r ? R Semantic role
ts ? Ts, tt ? Tt Source, target tokens
al ? Al : Ts ? 2Tt Word alignment
as ? As : R? 2Ts Source role assignment
at : (As?Al?R)? 2Tt Projected target role as-
signment
Table 4: Notation and signature summary for word-
based projection
the question of what linguistic information is appro-
priate for establishingM. Unfortunately, any attempt
to compute a match based on categorical data de-
rived from linguistic analyses (e.g., parts of speech,
phrase types or grammatical relations), needs to em-
pirically derive cross-linguistic similarities between
categories, a task which must be repeated for every
new language pair, and requires additional data.
Rather than postulating an ad hoc similarity func-
tion, we use word alignments to derive informa-
tion about semantic roles in the target language. Our
first model family (Section 4.2) relies exclusively
on this knowledge source. Although potentially use-
ful as a proxy for semantic equivalence, automati-
cally induced alignments are often noisy, thus lead-
ing to errors in annotation projection (Yarowsky et
al., 2001). For example, function words commonly
diverge across languages and are systematically mis-
aligned; furthermore, alignments are restricted to
single words rather than word combinations. This
observation motivates a second model family with a
bias towards linguistically meaningful entities (Sec-
tion 4.3). Such entities can be constituents derived
from the output of a parser or non-recursive syntac-
tic structures (i.e., chunks).
In this paper we compare simple word align-
ment models against more resource intensive models
that utilise constituent-based information and exam-
ine whether syntactic knowledge significantly con-
tributes to semantic projection.
4.2 Word-based Projection Model
The first model family uses source and target word
tokens as entities for projection. In this framework,
projection models can be defined by deriving the set
of matches M directly from word alignments. The
resulting signatures are shown in Table 4.
Our first projection model assigns to each role
r with source span s(r) the set of all target tokens
which are aligned to a token in the source span:
aw(as,al,r) =
[
ts?as(r)
al(ts) (3)
862
John and Mary left
Johann und Maria gingen
Departing
Departing
Figure 1:Word alignment-based semantic projection
of Role THEME (shadowed), Frame DEPARTING
The main shortcoming of this model is that it cannot
capture an important linguistic property of semantic
roles, namely that they almost always cover contigu-
ous stretches of text. We can repair non-contiguous
projections by applying a ?convex complementing?
heuristic to the output of (3), which fills all holes
in a sequence of tokens, without explicit recourse to
syntactic information. We define the convex comple-
menting heuristic as:
acw(as,al,r) = {tt | min(i(at1))? i(tt)
?max(i(at1))}
(4)
where i returns the index of a token t.
The two models just described are illustrated in
Figure 1. The frame DEPARTING is introduced by
left and gingen in English and German, respectively.
For simplicity, we only show the edges correspond-
ing to the THEME role. In English, the THEME is re-
alised by the words John and Mary. The dotted lines
show the available word alignments. The projection
of the THEME role according to (3) consists only
of the tokens {Johann, Maria} (shown by the plain
black lines); the convex complementing heuristic in
model (4) adds the token und, resulting in the (cor-
rect) convex set {Johann, und, Maria}.
4.3 Constituent-based Projection Model
Our second model family attempts to make up for
errors in the word alignment by projecting from and
to constituents. In this study, our constituents are ob-
tained from full parse trees (see Section 5 for de-
tails). Models which use non-recursive structures are
also possible; however, we leave this to future work.
The main difference from word-based projection
models is the introduction of constituent information
as an intermediate level; we thus construct a con-
stituent alignment for which only a subset of word
alignments has to be accurate. The appropriate sig-
natures and notation for constituent-based projection
are summarised in Table 5.
In order to keep the model as flexible as pos-
sible, and to explore the influence of different de-
sign decisions, we model constituent-based projec-
tion as two independently parameterisable subtasks:
first we compute a real-valued similarity function
between source and target constituents; then, we em-
ploy the similarity function to align relevant con-
stituents and project the role information.
Similarity functions. In principle, any function
which matches the signature in Table 5 could be
used. In practice, the use of linguistic knowledge
runs into the problem of defining similarity between
category-based representations discussed above. For
this reason, we limit ourselves to two simple similar-
ity functions based on word overlap: Given source
and target constituents cs and ct , we define the word
overlap ow of cs with ct as the proportion of tokens
within ct aligned to tokens within cs. Let yield(c)
denote the set of tokens in the yield of a constituent
c, then:
ow(cs,ct) =
|(
S
ts?yield(cs) al(ts))? yield(ct)|
|yield(ct)|
(5)
Since the asymmetry of this overlap measure leads
to high overlap scores for small target constituents,
we define word overlap similarity, as the product of
two constituents? mutual overlap:
sim(cs,ct) = o(cs,ct) ?o(ct ,cs) (6)
Simple word-based overlap has one undesired char-
acteristic: larger constituents tend to be less similar
because of missing alignments (e.g., between func-
tion words). Since content words are arguably more
important for the role projection task, we define a
second overlap measure, content word overlap owc,
which takes only nouns, verbs and adjectives into
account. Let yieldc(c) denote the set of tokens in the
yield of c that are content words, then:
owc(cs,ct) =
|(
S
ts?yieldc(cs) al(ts))? yieldc(ct)|
|yieldc(ct)|
(7)
Constituent alignment. Considerable latitude
is available in interpreting a similarity function to
derive a constituent alignment. Due to space limita-
tions, we demonstrate two basic models.
Our first forward constituent alignment model
(a f c), aligns source constituents that form the span
863
r ? R Semantic role
cs ?Cs,ct ?Ct Source and target con-
stituents
yield :C ? T Yield of a constituent
yieldc :C ? T Content word yield of a
constituent
al ? Al : Ts ? 2Tt Word alignment
as ? As : R? 2Cs Source role assignment
sim :Cs?Ct ? R+ Constituent similarity
at : As?Sim?R? 2Ct Projected target role as-
signment
Table 5: Notation and signature summary for
constituent-based projection
of a role to a single target constituent. We compute
the similarity of a target constituent ct to a set of
source constituents cs ? as(r) by taking the product
similarity for each source and target constituent pair:
a f c(as,sim,r) = argmax
ct?Ct
?
cs?as(r)
sim(cs,ct) (8)
This projection model forces the target role assign-
ment to be a function, i.e., it makes the somewhat
simplifying assumption that each role corresponds
to a single target constituent.
Our second backward constituent alignment
model (abc) proceeds in the opposite direction: it it-
erates over target constituents and attempts to de-
termine their most similar source constituent for
each ct . If the aligned source constituent is labelled
with a role, it is projected onto ct :
abc(as,sim,r) = {ct |(argmax
cs?Cs
sim(cs,ct)) ? as(r)}
(9)
In general, abc allows for more flexible role pro-
jection: it will sometimes decide not to project a
role at all (if the source constituents are dissimilar
to any target constituents), or it can assign a role
to more than one target constituent; however, this
means that there is less control over what is pro-
jected, and wrong alignments can lead to wrong re-
sults more easily.
Finally, if no word alignments are found for
complete source or target constituents, the maxi-
mal similarity rating in abc or ab f will be zero.
This is often the case for semantically weak single-
word constituents such as demonstrative pronouns
(e.g., [That] is right./ [Das] ist richtig.). When we
observe this phenomenon, we heuristically skip un-
aligned constituents (zero skipping).
Figure 2 contrasts the two constituent-based pro-
jection models using the frame QUESTIONING as
He asked all of them
Er fragte alle von ihnen
NP
3
PP
2
NP
1
NP
4
PP
5
NP
6
Questioning
Questioning
NP1 PP2 NP3
NP4 0.33 0.5 1
PP5 0.67 1 0.5
NP6 0.33 0 0
Figure 2: Constituent-based semantic projection of
role ADDRESSEE (shadowed), frame QUESTION-
ING. Below: Constituent similarity matrix.
an example. Again, we only show one role, AD-
DRESSEE, indicated by the shadowed box in Fig-
ure 2. Note that the object NP in German was mis-
parsed as an NP and a PP, a relatively frequent er-
ror. The difference between the two decision proce-
dures can be explained straightforwardly by look-
ing at the table below the graph, which shows the
similarity matrix for the constituents according to
equation (6). In this table, the source constituents
(indices 1?3) correspond to columns, and the tar-
get constituents (indices 4?6) to rows. The align-
ment model in (8) iterates over labelled source con-
stituents (here only NP1) and chooses the row with
the highest value as the target constituent for a can-
didate role. In our case, this is the PP5 (cell in bold-
face). In contrast, model (9) iterates over all target
constituents (i.e., rows) and checks if the most sim-
ilar source constituent bears a role label. Since NP1
is the most similar constituent for NP6 (underlined
cell), (9) assigns the QUESTIONING role to NP6.
5 Experiments
Evaluation Framework. We implemented the
models described in the previous section and used
them to project semantic information from En-
glish onto German. For the constituent-based mod-
els, constituent information was obtained from the
output of Collins? parser (1997) for English and
Dubey?s parser (2004) for German. Words were
864
Model Precision Recall F-score
w 0.41 0.40 0.41
cw 0.46 0.45 0.46
Upper bound 0.85 0.84 0.84
Table 6: Results for word-based projection models
aligned using the default setting4 of GIZA++ (Och
and Ney, 2003), a publicly available implementa-
tion of the IBM models and HMM word alignment
models. We evaluated the projected roles against the
?gold standard? roles obtained from the manual an-
notation (see Section 3). We also compared our re-
sults to the upper bound given by the inter-annotator
agreement on the calibration data set.
Results. Table 6 shows our results for the word-
based projection models. The simplest word-based
model (aw), obtains an F-score of 0.41. This is a
good result considering that the model does not ex-
ploit any linguistic information (e.g., parts of speech
or syntactic structure). It also supports our hypothe-
sis that word alignments are useful for the role pro-
jection task. The convex complementing heuristic
(acw) delivers an F-score increase of five points over
the ?words only? model, simply by making up for
holes in the word alignment.
We evaluated eight instantiations of the
constituent-based projection models; the results are
shown in Table 7. The best model (in boldface) uses
forward constituent alignment, content word-based
overlap similarity, and zero skipping. We observe
that backward constituent alignment-based models
(1?4) perform similarly to word-based projection
models (the F-score ranges between 0.40 and 0.45).
However, they obtain considerably higher precision
(albeit lower recall) than the word-based models.
This may be an advantage if the projected data
is destined for training target-language semantic
parsers. This precision/recall pattern appears to be
a direct result of abc, which only projects a role
from cs to ct if cs ?wins? against all other source
constituents, thus resulting in reliable, but overly
cautious projections, which cannot not be further
improved by zero skipping.
The forward constituent alignment models (5?8)
show consistently higher performance than word-
based models and models 1?4, indicating that the
stronger assumptions made by forward alignment
4The training scheme involved five iterations of Model 1,
five iterations of the HMM model, five iterations of Model 3,
and five iterations of Model 4.
Model al o 0-skip Precision Recall F-score
1 bc w no 0.70 0.33 0.45
2 bc w yes 0.70 0.33 0.45
3 bc wc no 0.65 0.32 0.42
4 bc wc yes 0.65 0.32 0.42
5 f c w no 0.61 0.60 0.60
6 f c w yes 0.66 0.60 0.63
7 f c wc no 0.62 0.60 0.61
8 fc wc yes 0.70 0.60 0.65
Upper bound 0.85 0.84 0.84
Table 7: Results for constituent-based projection
models (al: constituent alignment model; o: overlap
measure; 0-skip: zero skipping)
are justified in the data. In addition, we also find
that we can increase precision by concentrating on
reliable alignments. This is achieved by using the
zero skipping heuristic (compare the odd vs. even-
numbered models in Table 7) and by computing
overlap on content words (compare Models 6 vs. 8,
and 5 vs. 7).
We used the ?2 test to examine whether the dif-
ferences observed between the two classes of mod-
els are statistically significant. The best constituent-
based model significantly outperforms the best
word-based model both in terms of precision
(?2 = 114.47, p < 0.001) and recall (?2 = 400.40,
p < 0.001). Both projection models perform signifi-
cantly worse than humans (p < 0.001).
Discussion. Our results confirm that constituent
information is important for the semantic projection
task. Our best model adopts a conservative strat-
egy which enforces a one-to-one correspondence be-
tween roles and target constituents. This strategy
leads to high precision, however recall lags behind
(see Model 8 in Table 7). Manual inspection of the
projection output revealed that an important source
of missing roles are word alignments gaps. Such
gaps are not only due to noisy alignments, but also
reflect genuine structural differences between trans-
lated sentences. Consider the following (simplified)
example for the STATEMENT frame (introduced by
say) and its semantic role STATEMENT (introduced
by we):
(10) We
Wir
claim
behaupten
X
X
and
und
we
?
say
sagen
Y
Y
The word alignment correctly aligns the German
pronoun wir with the first English we and leaves
865
the second occurrence unaligned. Since there is no
corresponding German word for the second we, pro-
jection of the SPEAKER role fails. In future work,
this problem could be handled with explicit identi-
fication of empty categories (see Dienes and Dubey,
2003).
6 Conclusions
In this paper, we argue that parallel corpora show
promise in relieving the lexical acquisition bottle-
neck for low density languages. We proposed se-
mantic projection as a means of obtaining FrameNet
annotations automatically without additional human
effort. We examined semantic parallelism, a prereq-
uisite for accurate projection, and showed that se-
mantic roles can be successfully projected for pred-
icate pairs with matching frame assignments. Sim-
ilarly to previous work (Hwa et al, 2002), we find
that some mileage can be gained by assuming di-
rect correspondence between two languages. How-
ever, linguistic knowledge is key in obtaining mean-
ingful projections. Our experiments show that the
use of constituent information yields substantial im-
provements over relying on word alignment alone.
Nevertheless, the word-based models offer a good
starting point for low-density languages for which
parsers are not available. Their output could be fur-
ther post-processed manually or automatically using
bootstrapping techniques (Riloff and Jones, 1999).
We have presented a general, flexible framework
for semantic projection which can be easily applied
to other languages. An important direction for fu-
ture work lies in the assessment of more shallow
syntactic information (i.e., chunks) which can be ob-
tained more easily for new languages, and generally
in the integration of more linguistic knowledge to
guide projection. Finally, we will incorporate into
our projection approach automatic semantic role an-
notations for the source language and investigate the
potential of the projected annotations for training se-
mantic parsers for the target language.
Acknowledgements. The authors acknowledge
the support of DFG (Pad?; grant PI-154/9-2) and
EPSRC (Lapata; grant GR/T04540/01). Thanks to
B. Kouchnir and P. Kreischer for their annotation.
References
H. C. Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings of LREC 2002,
1364?1371, Las Palmas, Canary Islands.
L. Burnard, 1995. The Users Reference Guide for the
British National Corpus. British National Corpus
Consortium, Oxford University Computing Service,
1995.
X. Carreras, L. M?rquez, eds. 2005. Proceedings of the
CoNLL shared task: Semantic role labelling, 2005.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL/EACL
1997, 16?23, Madrid, Spain.
P. Dienes, A. Dubey. 2003. Antecedent recovery: Exper-
iments with a trace tagger. In Proceedings of EMNLP
2003, 33?40, Sapporo, Japan.
A. Dubey. 2004. Statistical parsing for German: Mod-
elling syntactic properties and annotation differences.
Ph.D. thesis, Saarland University.
K. Erk, A. Kowalski, S. Pad?, M. Pinkal. 2003. Towards
a resource for lexical semantics: A large German cor-
pus with extensive semantic annotation. In Proceed-
ings of ACL 2003, 537?544, Sapporo, Japan.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250.
P. Fung, B. Chen. 2004. BiFrameNet: Bilingual frame
semantics resources construction by cross-lingual in-
duction. In Proceedings of COLING 2004, 931?935,
Geneva, Switzerland.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?
288.
R. Hwa, P. Resnik, A. Weinberg, O. Kolak. 2002. Eval-
uation translational correspondance using annotation
projection. In Proceedings of ACL 2002, 392?399,
Philadelphia, PA.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation. Draft.
S. Narayanan, S. Harabagiu. 2004. Question answering
based on semantic structures. In Proceedings of COL-
ING 2004, 693?701, Geneva, Switzerland.
F. J. Och, H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
S. Pad?, M. Lapata. 2005. Cross-lingual bootstrapping
for semantic lexicons. In Proceedings of AAAI 2005,
Pittsburgh, PA.
E. Riloff, R. Jones. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In
Proceedings of AAAI 1999, Orlando, FL.
D. A. Smith, N. A. Smith. 2004. Bilingual parsing with
factored estimation: Using English to parse Korean.
In Proceedings of EMNLP 2004, 49?56, Barcelona,
Spain.
M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proceedings of ACL 2003, 8?15,
Sapporo, Japan.
D. Yarowsky, G. Ngai, R. Wicentowski. 2001. Inducing
multilingual text analysis tools via robust projection
across aligned corpora. In Proceedings of HLT 2001,
161?168.
866

c? 2002 Association for Computational Linguistics
The Disambiguation of Nominalizations
Maria Lapata?
University of Edinburgh
This article addresses the interpretation of nominalizations, a particular class of compound nouns
whose head noun is derived from a verb and whose modifier is interpreted as an argument of this
verb. Any attempt to automatically interpret nominalizations needs to take into account: (a) the
selectional constraints imposed by the nominalized compound head, (b) the fact that the relation
of the modifier and the head noun can be ambiguous, and (c) the fact that these constraints can
be easily overridden by contextual or pragmatic factors. The interpretation of nominalizations
poses a further challenge for probabilistic approaches since the argument relations between a head
and its modifier are not readily available in the corpus. Even an approximation that maps the
compound head to its underlying verb provides insufficient evidence. We present an approach
that treats the interpretation task as a disambiguation problem and show how we can ?re-create?
the missing distributional evidence by exploiting partial parsing, smoothing techniques, and
contextual information. We combine these distinct information sources using Ripper, a system
that learns sets of rules from data, and achieve an accuracy of 86.1% (over a baseline of 61.5%)
on the British National Corpus.
1. Introduction
The automatic interpretation of compound nouns has been a long-standing problem
for natural language processing (NLP). Compound nouns in English have three basic
properties that present difficulties for their interpretation: (a) the compounding process
is extremely productive (this means that a hypothetical system would have to interpret
previously unseen instances), (b) the semantic relationship between the compound
head and its modifier is implicit (this means that it cannot be easily recovered from
syntactic or morphological analysis), and (c) the interpretation can be influenced by a
variety of contextual and pragmatic factors.
A considerable amount of effort has gone into specifying the set of semantic rela-
tions that hold between a compound head and its modifier (Levi 1978; Warren 1978;
Finin 1980; Isabelle 1984). Levi (1978), for example, distinguishes two types of com-
pound nouns: (a) compounds consisting of two nouns that are related by one of nine
recoverably deletable predicates (e.g., cause relates onion tears, for relates pet spray;
see the examples in (1)) and (b) nominalizations, that is, compounds whose heads are
nouns derived from a verb and whose modifiers are interpreted as arguments of the
related verb (e.g., a car lover loves cars; see the examples in (2)?(4)). The prenominal
modifier can be either a noun or an adjective (see the examples in (2)). The nominal-
ized verb can take a subject (see (3a)), a direct object (see (3b)) or a prepositional object
(see (3c)).
(1) a. onion tears cause
b. vegetable soup have
? Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail:
mlap@cogsci.ed.ac.uk
358
Computational Linguistics Volume 28, Number 3
c. music box make
d. steam iron use
e. pine tree be
f. night flight in
g. pet spray for
h. peanut butter from
i. abortion problem about
(2) a. parental refusal subj
b. cardiac massage obj
c. heart massage obj
d. sound synthesizer obj
(3) a. child behavior subj
b. car lover obj
c. soccer competition at|in
(4) a. government promotion subj|obj
b. satellite observation subj|obj
Besides Levi (1978), a fair number of researchers (Warren 1978; Finin 1980; Isabelle
1984; Leonard 1984) agree that there is a limited number of regularly recurring relations
between a compound head and its modifier. There is far less agreement when it comes
to the type and number of these relations. The relations vary from Levi?s (1978) recov-
erably deletable predicates to Warren?s (1978) paraphrases and Finin?s (1980) role nom-
inals. Leonard (1984) proposes eight relations, and Warren (1978) proposes six basic re-
lations, whereas the number of relations proposed by Finin (1980) is potentially infinite.
The attempt to restrict the semantic relations between the compound head and its
modifier to a prespecified number and type has been criticized by Downing (1977),
who has shown (through a series of psycholinguistic experiments) that the underlying
relations can be influenced by a variety of pragmatic factors and cannot therefore be
presumed to be easily enumerable. Sparck Jones (1983, page 4) further notes ?that
observations about the semantic relation holding between the compound head and its
modifier can only be remarks about tendencies and not about absolutes.? Consider,
for instance, the compound onion tears (see (1a)). The relationship cause is one of
the possible interpretations the compound may receive. One could easily imagine a
context in which the tears are for or about the onion. Consider example1 (5a), taken
from Downing (1977, page 818). Here apple-juice seat refers to the situation in which
someone is instructed to sit in a seat in front of which a glass of apple juice has been
placed. Given this particular state of affairs, none of the relations in (1) can be used
to successfully interpret apple-juice seat. Such considerations have led Selkirk (1982) to
1 Unless stated otherwise the example sentences were taken from the British National Corpus and in
some cases simplified for purposes of clarity.
359
Lapata The Disambiguation of Nominalizations
claim that only nominalizations are amenable to linguistic characterization, leaving all
other compounds to be explained by pragmatics or discourse. A similar approach is put
forward by Hobbs et al (1993) for all types of compounds, including nominalizations:
any two nouns can be combined, and the relation between these nouns is entirely
underspecified, to be resolved pragmatically.
(5) a. A friend of mine was once instructed to sit in the apple-juice seat.
b. By the end of the 1920s, government promotion of agricultural
development in Niger was limited, consisting mainly of crop trials and
model sheep and ostrich farms.
Less controversy arises with regard to nominalizations, perhaps because of the
small number of allowable relations. Most approaches follow Levi (1978) in distin-
guishing nominalizations as a separate class of compounds, the exception being Finin
(1980), who claims that most compounds are nominalizations, even in cases in which
the head noun is not morphologically derived from a verb (see the examples in (1)).
Under Finin?s analysis the head book in the compound recipe book is a role nominal, that
is, a noun that refers to a particular thematic role of another concept. This means that
book refers to the object role of write, which is filled by recipe. It is not clear, however,
how the implicit verb is to be recovered or why write is more appropriate than read in
this example.
Despite the small number of relations between the nominalized head and its mod-
ifier, the interpretation of nominalizations can readily change in different contexts. In
some cases, the relation of the modifier and the nominalized verb (e.g., subject or
object) can be predicted either from the subcategorization properties of the verb or
from the semantics of the nominalization suffix of the head noun. Consider (3a), for
example. Here child can be only the subject of behavior, since the verb behave is intran-
sitive. In (3b) the agentive suffix -er of the head noun lover indicates that the modifier
car is the object of the verb love. In other cases, the relation of the modifier and the
head noun is genuinely ambiguous. Out of context the compounds government promo-
tion and satellite observation (see example (4)) can receive either a subject or an object
interpretation. One might argue that the preferred analysis for government promotion is
?government that is promoted by someone.? This interpretation can be easily overrid-
den in context, however, as shown in Example (5b): here it is the government that is
doing the promotion.
The automatic interpretation of compound nouns poses a challenge for empirical
approaches, since the relations between a head and its modifier are not readily avail-
able in a corpus, and therefore they have to be somehow retrieved and approximated.
Given the data sparseness and the parameter estimation difficulties, it is not surprising
that a far greater number of symbolic than probabilistic solutions have been proposed
for the automatic interpretation of compound nouns. With the exception of Wu (1993)
and Lauer (1995), who use probabilistic models for compound noun interpretation (see
Section 7 for details), most algorithms rely on hand-crafted knowledge bases or dic-
tionaries that contain detailed semantic information for each noun; a sequence of rules
exploit a knowledge base to choose the correct interpretation for a given compound
(Finin 1980; McDonald 1982; Leonard 1984; Vanderwende 1994).
In what follows we develop a probabilistic model for the interpretation of nominal-
izations. We focus on nominalizations whose prenominal modifier is either the under-
lying subject or direct object of the verb corresponding to the nominalized compound
head. In other words, we focus on examples like (3a, 3b) and ignore for the moment
360
Computational Linguistics Volume 28, Number 3
nominalizations whose heads correspond to verbs taking prepositional complements
(see example (3c)). Nominalizations are attractive from an empirical perspective: the
amount of relations is small (i.e., subject or object, at least if one focuses on direct
objects only) and fairly uncontroversial (see the discussion above). Although the rela-
tions are not attested in the corpus, they can be retrieved and approximated through
parsing. The probabilistic interpretation of nominalizations can provide a lower bound
for the difficulty of the compound interpretation task: if we cannot interpret nominal-
izations successfully, there is little hope for modeling more complex semantic relations
stochastically (see the examples in (1)).
We present a probabilistic algorithm that treats the interpretation task as a dis-
ambiguation problem. Our approach relies on the simplifying assumption that the
relation of the nominalized head and its modifier noun can be approximated by the
relation of the latter and the verb from which the head is derived. This approach works
insofar as the verb-argument relations from which the nominalizations are derived are
attested in the corpus. We show that a large number of verb-argument configurations
do not occur in the corpus, something that is perhaps not surprising considering the
ease with which novel compounds are created (Levi 1978). We estimate the frequen-
cies of unseen verb-argument pairs by experimenting with three types of smoothing
techniques proposed in the literature (back-off smoothing, class-based smoothing, and
distance-weighted averaging) and show that their combination achieves good perfor-
mance. Furthermore, we explore the contribution of context to the disambiguation task
and show that performance is increased by taking contextual features into account.
Our best results are achieved by combining the predictions of our probabilistic model
with contextual information.
The remainder of this article is organized as follows: in Section 2 we present
a simple statistical model for the interpretation of nominalizations and describe the
procedure used to collect the data for our experiments. Section 3 presents details on
how the parameters of the model were estimated and gives a brief overview on the
smoothing methods with which we experimented. Section 4 describes the algorithm
used for the interpretation of nominalizations, and Section 5 reports the results of
several experiments that achieve a combined accuracy of 86.1% on the British National
Corpus (BNC). Section 6 discusses the findings. In Section 7 we review related work,
and we conclude in Section 8.
2. The Model
2.1 Guessing Argument Relations
As explained in Section 1, nominalizations are compounds whose head noun is a nom-
inalized verb and whose prenominal modifier is derived from either the underlying
subject or the underlying object of that verb (Levi 1978). Our goal, given a nominaliza-
tion, is to develop a procedure for inferring whether the modifier stands in a subject
or object relation with respect to the head noun. In other words, we need to assign
probabilities to the two different relations (subj, obj). For each relation rel we calculate
the simple expression P(rel | n1, n2) given in (6).
P(rel | n1, n2) =
f (n1, rel , n2)
f (n1, n2)
(6)
Since we have a choice between two outcomes we will use a likelihood ratio to
compare the two relation probabilities (Mosteller and Wallace 1964; Hindle and Rooth
1993). In particular we will compute the log of the ratio of the probability P(obj | n1, n2)
361
Lapata The Disambiguation of Nominalizations
to the probability P(subj | n1, n2). We will call this log-likelihood ratio the argument
relation (RA) score.
RA(rel , n1, n2) = log2
P(obj | n1, n2)
P(subj | n1, n2)
(7)
Notice, however, that we cannot read off f (n1, rel , n2) directly from the corpus.
What we can obtain from a corpus (through parsing) is the number of times a noun is
the object or the subject of a given verb. By making the simplifying assumption that
the relation of the nominalized head and its modifier noun is the same as the relation
between the latter and the verb from which the head is derived, we can rewrite (6) as
follows:
P(rel | n1, n2) ?
f (vn2 , rel , n1)
?
i
f (vn2 , rel i, n1)
(8)
where f (vn2 , rel , n1) is the frequency with which the modifier noun n1 is found in the
corpus as the subject or object of vn2 , the verb from which the head noun is derived.
The sum
?
i f (vn2 , rel i, n1) is a normalization factor.
2.2 Parameter Estimation
2.2.1 Verb-Argument Tuples. We estimated the parameters of the model outlined
in the previous section from a part-of-speech-tagged and lemmatized version of the
BNC, a 100-million-word collection of samples of written and spoken language from a
wide range of sources designed to represent current British English (Burnard 1995). To
estimate the term f (vn2 , rel , n1), the corpus was automatically parsed by Cass (Abney
1996), a robust chunk parser designed for the shallow analysis of noisy text. The main
feature of Cass is its finite-state cascade technique. A finite-state cascade is a sequence
of nonrecursive levels: phrases at one level are built on phrases at the previous level
without containing same-level or higher-level phrases. We used the parser?s built-in
function to extract tuples of verb subjects and verb objects (see (9)).
(9) a. change situation subj
b. come off heroin obj
c. deal with situation obj
(10) a. isolated people subj
b. smile good subj
The tuples obtained from the parser?s output are an imperfect source of infor-
mation about argument relations. Bracketing errors, as well as errors in identifying
chunk categories accurately, result in tuples whose lexical items do not stand in a
verb-argument relationship. For example, inspection of the original BNC sentences
from which (10a) and (10b) were derived revealed that the verb is missing from the
former and the noun is missing from the latter (see the sentences in (11)).
(11) a. Wenger found that more than half the childless old people in her
study of rural Wales saw a relative, a sibling, niece, nephew or cousin
at least once a week, though in inner city London there were more
isolated old people.
b. I smiled my best smile down the line.
362
Computational Linguistics Volume 28, Number 3
Table 1
Tuples extracted from the BNC.
Tokens Types
Relation Parser Filtering Tuples Verbs Nouns
subj 4,491,386 4,095,578 588,333 10,852 41,336
obj 2,631,752 2,598,069 615,328 9,490 35,846
Table 2
Deverbal suffixes.
Suffix Nominalization
-er drink ? drinker
-or direct ? director
-ant disinfect ? disinfectant
-ee employ ? employee
-ation educate ? education
-ment arrange ? arrangement
-al refuse ? refusal
-ing hire ? hiring
Table 3
Conversion.
Verb ? Noun
release ? release
arrest ? arrest
compromise ? compromise
attempt ? attempt
To compile a comprehensive count of verb-argument relations, we tried to elimi-
nate from the parser?s output tuples containing erroneous verbs and nouns like those
in (10). We did this by matching the verbs contained in the tuples against a list of all
words tagged as verbs and nouns in the BNC. Tuples containing words not included in
the list were discarded. Furthermore, we discarded tuples containing verbs or nouns
attested in a verb-argument relationship only once. This resulted in 588,333 distinct
verb-subject pairs and 615,328 distinct verb-object pairs (see Table 1, which contains
information about the tuples extracted from the corpus before and after the filtering
described earlier in the paragraph).
2.2.2 The Data. So far we have been using the term nominalization to refer to two-word
compounds whose head is derived from a verb. Morphologically speaking, nominal-
ization is a word formation process by which a noun is derived from a verb, usually
by means of suffixation (Quirk et al 1985). A list of deverbal suffixes (i.e., suffixes that
form nouns when attached to verb bases) is given in Table 2. Nominalizations can also
be created by conversion, the word formation process whereby ?an item is adapted
or converted to a new word-class without the addition of an affix? (Quirk et al 1985,
page 1009). Examples of conversion are shown in Table 3.
It is beyond the scope of the present study to develop an algorithm that auto-
matically detects nominalizations in a corpus. In the experiments described in the
subsequent sections compounds with deverbal heads were obtained as follows:
1. Two-word compound nouns were extracted from the BNC using a
heuristic that looks for consecutive pairs of nouns that are neither
preceded nor succeeded by a noun (Lauer 1995).
2. A dictionary of deverbal nouns was created using two sources:
(a) nomlex (Macleod et al 1998), a dictionary of nominalizations
363
Lapata The Disambiguation of Nominalizations
containing 827 lexical entries, and (b) celex (Burnage 1990), a general
morphological dictionary that contains 5,111 nominalizations; both
dictionaries list the verbs from which the nouns are derived. Sample
dictionary entries are given in Tables 2 and 3.
3. Candidate nominalizations were obtained from the compounds acquired
from the BNC by selecting noun-noun sequences whose head (i.e.,
rightmost noun) was one of the deverbal nouns contained in either
celex or nomlex. The procedure resulted in 172,797 potential types of
nominalizations.
From these candidate nominalizations a random sample of 1,277 tokens was selected.
The sample was manually inspected, and compounds with modifiers whose relation
to the head noun was other than subject or object were discarded. In particular nom-
inalizations were discarded if: (a) the relation between the head and the modifier
was any of the semantic relations listed in (1) (e.g., cause, have, make); these com-
pounds represented 28.0% of the sample; (b) the head was derived from verbs taking
prepositional objects (see example (3c)); these nominalizations represented 9.2% of the
sample. After manual inspection the sample contained 796 nominalizations (62.8%
of the initial sample). These tokens were used for the experiments described in Sec-
tion 5.
2.2.3 Mapping. To estimate the frequency, f (vn2 , rel , n1), the nominalized heads were
mapped to their corresponding verbs. Inspection of the frequencies of the verb-argu-
ment tuples contained in our data (796 tokens) revealed that 480 verb-noun pairs
(60.3%) had a verb-object frequency of zero in the corpus. Similarly, 503 verb-noun
pairs (63.2%) had a verb-subject frequency of zero. Furthermore, a total of 373 tuples
(46.9%) were not attested at all in the BNC either in a verb-object or verb-subject
relation. This finding is not entirely unexpected, considering that compounds are typ-
ically used as a text compression device (Marsh 1984), that is, to pack meaning into
a minimal amount of linguistic structure. If a nominalization is chosen over a more
elaborate structure (i.e., a sentence), then it is not surprising that some verb-argument
configurations will not occur in the corpus. Furthermore, some nominalizations are
conventionalized (e.g., business administration, health organization) and are therefore at-
tested more frequently than their verb-subject or verb-object counterparts.
We re-created the frequencies of unseen verb-argument pairs by experimenting
with three types of smoothing techniques proposed in the literature: back-off smooth-
ing (Katz 1987), class-based smoothing (Resnik 1993; Lauer 1995), and distance-
weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999). We
present these three smoothing variants and their underlying assumptions in the fol-
lowing section.
3. Smoothing
Smoothing techniques have been used in a variety of statistical NLP applications as a
means of addressing data sparseness, an inherent problem for statistical methods that
rely on the relative frequencies of word combinations. The problem arises when the
probability of word combinations that do not occur in the training data needs to be
estimated. The smoothing methods proposed in the literature (overviews are provided
by Dagan, Lee, and Pereira (1999) and Lee (1999)) can be generally divided into three
types: discounting (Katz 1987), class-based smoothing (Resnik 1993; Brown et al 1992;
364
Computational Linguistics Volume 28, Number 3
Pereira, Tishby, and Lee 1993), and distance-weighted averaging (Grishman and Sterling
1994; Dagan, Lee, and Pereira 1999).
Discounting methods decrease the probability of previously seen events so that the
total probability of observed word co-occurrences is less than one, leaving some prob-
ability mass to be redistributed among unseen co-occurrences. Class-based smoothing
and distance-weighted averaging both rely on an intuitively simple idea: interword
dependencies are modeled by relying on the corpus evidence available for words that
are similar to the words of interest. The two approaches differ in the way they measure
word similarity. Distance-weighted averaging estimates word similarity from lexical
co-occurrence information; namely, it finds similar words by taking into account the
linguistic contexts in which they occur: two words are similar if they occur in similar
contexts. In class-based smoothing, classes are used as the basis according to which the
co-occurrence probability of unseen word combinations is estimated. Classes can be
induced directly from the corpus using distributional clustering (Pereira, Tishby, and
Lee 1993; Brown et al 1992; Lee and Pereira 1999) or taken from a manually crafted
taxonomy (Resnik 1993). In the latter case the taxonomy is used to provide a mapping
from words to conceptual classes.
Distance-weighted averaging differs from distributional clustering in that it does
not explicitly cluster words. Although both methods make use of the evidence of
words similar to the words of interest, distributional clustering assigns to each word
a probability distribution over clusters to which it may belong; co-occurrence proba-
bilities can then be estimated on the basis of the average of the clusters to which the
words in the co-occurrence belong. This means that word co-occurrences are modeled
by taking general word clusters into account and that the same set of clusters is used
for different co-occurrences. Distance-weighted averaging does not explicitly create
general word clusters. Instead, unseen co-occurrences are estimated by averaging the
set of co-occurrences most similar to the target unseen co-occurrence, and a differ-
ent set of similar neighbors (i.e., distributionally similar words) is used for different
co-occurrences.
In language modeling, smoothing techniques are typically evaluated by showing
that a language model that uses smoothed estimates incurs a reduction in perplexity
on test data over a model that does not employ smoothed estimates (Katz 1987).
Dagan, Lee, and Pereira (1999) use perplexity to compare back-off smoothing against
distance-weighted averaging methods within the context of language modeling for
speech recognition and show that the latter outperform the former. They also compare
different distance-weighted averaging methods on a pseudoword disambiguation task
in which the language model decides which of two verbs v1 and v2 is more likely to
take a noun n as its object. The method being tested must reconstruct which of the
unseen (v1, n) and (v2, n) is a valid verb-object combination. The same task is used by
Lee and Pereira (1999) in a detailed comparison between distributional clustering and
distance-weighted averaging that demonstrates that the two methods yield comparable
results.
In our experiments we re-created co-occurrence frequencies for unseen verb-sub-
ject and verb-object pairs using three maximally different approaches: back-off smooth-
ing, class-based smoothing using a predefined taxonomy, and distance-weighted av-
eraging. We preferred taxonomic class-based methods over distributional clustering
mainly because we wanted to compare directly methods that use distributional infor-
mation inherent in the corpus without making external assumptions with regard to
how concepts and their similarity are represented with methods that quantify sim-
ilarity relationships based on information present in a hand-crafted taxonomy. Fur-
thermore, as Lee and Pereira?s (1999) results indicate that distributional clustering
365
Lapata The Disambiguation of Nominalizations
and distance-weighted averaging obtain similar levels of performance, we restricted
ourselves to the latter.
We evaluated the contribution of the different smoothing methods on the nomi-
nalization task by exploring how each method and their combination influences dis-
ambiguation performance. Sections 3.1?3.3 review discounting, class-based smooth-
ing, and distance-weighted averaging. Section 4 introduces an algorithm that uses
smoothed verb-argument tuples to arrive at the interpretation of nominalizations.
3.1 Back-Off Smoothing
Back-off n-gram models were initially proposed by Katz (1987) for speech recognition
but have also been successfully used to disambiguate the attachment site of struc-
turally ambiguous prepositional phrases (Collins and Brooks 1995). The main idea
behind back-off smoothing is to adjust maximum likelihood estimates like (8) so that
the total probability of observed word co-occurrences is less than one, leaving some
probability mass to be redistributed among unseen co-occurrences. In general the fre-
quency of observed word sequences is discounted using the Good-Turing estimate (see
Katz (1987) and Church and Gale (1991) for details on Good-Turing estimation), and
the probability of unseen sequences is estimated by using lower-level conditional dis-
tributions. Assuming that the numerator f (vn2 , rel , n1) in (8) is zero we can approximate
P(rel | n1, n2) by backing off to P(rel | n1):
P(rel | n1, n2) = ?
f (rel , n1)
f (n1)
(12)
where ? is a normalization constant that ensures that the probabilities sum to one. If
the frequency f (rel , n1) is also zero, backing off continues by making use of P(rel).
3.2 Class-Based Smoothing
Generally speaking, taxonomic class-based smoothing re-creates co-occurrence fre-
quencies based on information provided by lexical resources such as WordNet (Miller
et al 1990) or Roget?s publicly available thesaurus. In the case of verb-argument tuples,
we use taxonomic information to estimate the frequencies f (vn2 , rel , n1) by substituting
for the word n1 occurring in an argument position the concept with which it is repre-
sented in the taxonomy (Resnik 1993). So f (vn2 , rel , n1) can be estimated by counting
the number of times the concept corresponding to n1 was observed as the argument
of the verb vn2 in the corpus.
This would be a straightforward task if each word was always represented in
the taxonomy by a single concept or if we had a corpus of verb-argument tuples
labeled explicitly with taxonomic information. Lacking such a corpus we need to take
into consideration the fact that words in a taxonomy may belong to more than one
conceptual class: counts of verb-argument configurations are reconstructed for each
conceptual class by dividing the contribution from the argument by the number of
classes to which it belongs (Resnik 1993; Lauer 1995):
f (vn2 , rel , c) ?
?
n?1?c
f (vn2 , rel , n
?
1)
|classes(n?1)|
(13)
where f (vn2 , rel , n
?
1) is the number of times the verb vn2 was observed with concept
c ? classes(n?1) bearing the argument relation rel (i.e., subject or object) and |classes(n?1)|
is the number of conceptual classes to which n?1 belongs.
366
Computational Linguistics Volume 28, Number 3
Table 4
Frequency estimation for group registration using WordNet.
Verb Class f (vn2 ,obj, n1) f (vn2 ,subj, n1)
register ?abstraction? 16.26 7.28
register ?entity? 14.10 4.50
register ?object? 8.02 1.56
register ?set? .65 .07
register ?substance? .70 .08
Consider, for example, the tuple register group (derived from the compound group
registration), which is not attested in the BNC. The word group has two senses in
WordNet and belongs to five conceptual classes (?abstraction?, ?entity?, ?object?,
?set?, and ?substance?). This means that the frequency f (vn2 , rel , c) will be constructed
for each of the five classes, as shown in Table 4. Suppose now that we see the tuple
register patient in the corpus. The word patient has two senses in WordNet and belongs
to seven conceptual classes (?case?, ?person?, ?life form?, ?entity?, ?causal agent?,
?sick person?, ?unfortunate?), one of which is ?entity?. This means that we will
increment the observed co-occurrence count of register and ?entity? by 17 . Since we
do not know which is the actual class of the noun group in the corpus, we weight the
contribution of each class by taking the average of the constructed frequencies for all
five classes:
f (vn2 , rel , n1) =
?
c?classes(n1)
?
n?1?c
f (vn2 ,rel ,n
?
1)
|classes(n?1)|
|classes(n1)|
(14)
Following (14) the frequencies f (register , obj, group) and f (register , subj, group) are
39.73
5 and
13.49
5 , respectively. Note that the estimation of the frequency f (vn2 , rel , n1) (see
equations (13) and (14)) crucially relies on the simplifying assumption that the argu-
ment of a verb is distributed evenly across its conceptual classes. This simplification
is necessary unless we have a corpus of verb-argument pairs labeled explicitly with
taxonomic information. The task of finding the right class for representing the argu-
ment of a given predicate is a research issue on its own (Clark and Weir 2001; Li and
Abe 1998; Carroll and McCarthy 2000), and a detailed comparison between different
methods for accomplishing this task is beyond the scope of the present study.
3.3 Distance-Weighted Averaging
Distance-weighted averaging induces classes of similar words from word co-occur-
rences without making reference to a taxonomy. Instead, it is based on the assumption
that if a word w?1 is similar to word w1, then w
?
1 can provide information about the
frequency of unseen word pairs involving w1 (Dagan, Lee, and Pereira 1999). A key
feature of this type of smoothing is the function that measures distributional similarity
from co-occurrence frequencies.
Several measures of distributional similarity have been proposed in the literature
(Dagan, Lee, and Pereira 1999; Lee 1999). We used two measures, the Jensen-Shannon
divergence and the confusion probability. The choice of these two measures was mo-
tivated by work described in Dagan, Lee, and Pereira (1999), in which the Jensen-
Shannon divergence outperforms related similarity measures (such as the confusion
probability or the L1 norm) on a pseudodisambiguation task that uses verb-object
pairs. The confusion probability has been used by several authors to smooth word co-
367
Lapata The Disambiguation of Nominalizations
occurrence probabilities (Essen and Steinbiss 1992; Grishman and Sterling 1994) and
shown to give promising performance. Grishman and Sterling (1994) in particular em-
ploy the confusion probability to re-create the frequencies of verb-noun co-occurrences
in which the noun is the object or the subject of the verb in question. In the following
we describe these two similarity measures and show how they can be used to re-create
the frequencies for unseen verb-argument tuples (for a more detailed description see
Dagan, Lee, and Pereira (1999)).
3.3.1 Confusion Probability. The confusion probability PC is an estimate of the prob-
ability that a word w1 can be substituted for a word w?1, in the sense of being found
in the same contexts. In other words, the metric expresses how probable it is for word
w?1 to occur in contexts in which word w1 occurs. A large confusion probability value
indicates that the two words w?1 and w1 appear in similar contexts. PC is estimated as
follows:
PC(w1 | w?1) =
?
s
P(w1 | s)P(s | w?1) (15)
where PC(w1 | w?1) is the probability that word w?1 occurs in the same contexts s as word
w1, averaged over these contexts. Given a tuple of the form w1, rel , w2, we can either
treat w1, rel as context and smooth over the noun w2 or rel , w2 as context and smooth
over the verb w1. We opted for the latter for two reasons. Theoretically speaking, it is
the verb that imposes the semantic restrictions on its arguments and not vice versa. The
idea that semantically similar verbs have similar subcategorizational and selectional
patterns is by no means new and has been extensively argued for by Levin (1993).
Computational efficiency considerations also favor an approach that treats rel , w2 as
context: the nouns w2 outnumber the verbs w1 by a factor of four (see Table 1). When
verb-argument tuples are taken into consideration, (8) can be rewritten as follows:
PC(w1 | w?1) =
?
rel ,w2
P(w1 | rel , w2)P(rel , w2 | w?1)
=
?
rel ,w2
f (w1,rel ,w2)
f (rel ,w2)
f (w?1,rel ,w2)
f (w?1)
(16)
The confusion probability can be computed efficiently, since it involves summation
only over the common contexts rel , w2.
3.3.2 Jensen-Shannon Divergence. The Jensen-Shannon divergence J is an informa-
tion-theoretic measure. It recasts the concept of distributional similarity into a mea-
sure of the ?distance? between two probability distributions. The value of the Jensen-
Shannon divergence ranges from zero for identical distributions to log 2 for maximally
different distributions. J is defined as:
J(w1, w?1) =
1
2
[
D
(
w1
?
?
?
?
w1 + w?1
2
)
+ D
(
w?1
?
?
?
?
w1 + w?1
2
)]
(17)
D(w1?w?1) =
?
rel ,w2
P(rel , w2 | w1) log
P(rel , w2 | w1)
P(rel , w2 | w?1)
(18)
where w1 is a shorthand for P(rel , w2 | w1) and w?1 for P(rel , w2 | w?1); D in (17) is
the Kullback-Leibler divergence, a measure of the dissimilarity between two proba-
bility distributions (see equation (18)) and (w1 + w?1)/2 is a shorthand for the average
distribution:
1
2
(P(rel , w2 | w1) + P(rel , w2 | w?1)) (19)
368
Computational Linguistics Volume 28, Number 3
Given a set of nominalizations n1 n2:
1. map the head noun n2 to the verb vn2 from which it is derived;
2. retrieve frequencies f (vn2 ,obj, n1) and f (vn2 , subj, n1) from the BNC;
3. if f (vn2 ,obj, n1) < k then re-create fs(vn2 ,obj, n1);
4. if f (vn2 , subj, n1) < k then re-create fs(vn2 , subj, n1);
5. calculate probabilities P(obj | n1, n2) and P(subj | n1, n2);
6. compute RA(rel, n1, n2);
7. if RA ? j then n1 is the object of n2;
8. else n1 is the subject of n2.
Figure 1
Disambiguation algorithm for nominalizations.
Similarly to the confusion probability, the computation of J depends only on the
common contexts rel , w2. Recall that the Jensen-Shannon divergence is a dissimilarity
measure. The dissimilarity measure is transformed into a similarity measure using a
weight function WJ(w, w?1):
WJ(w1, w?1) = 10
??J(w1,w?1) (20)
The parameter ? controls the relative influence of the neighbors (i.e., distributionally
similar words) closest to w1: if ? is high, only neighbors extremely close to w1 con-
tribute to the estimate, whereas if ? is low, distant neighbors also contribute to the
estimate.
We estimate the frequency of an unseen verb-argument tuple by taking into ac-
count the similar w1s and the contexts in which they occur (Grishman and Sterling
1994):
fs(w1, rel , w2) =
?
w?1
sim(w1, w?1)f (w
?
1, rel , w2) (21)
where sim(w1, w?1) is a function of the similarity between w1 and w
?
1. In our experiments
the confusion probability PC(w1 | w?1) and the Jensen-Shannon divergence WJ(w1, w1?)
were substituted for sim(w1, w?1).
4. The Disambiguation Algorithm
The disambiguation algorithm for nominalizations is summarized in Figure 1. The al-
gorithm uses verb-argument tuples to infer the relation holding between the modifier
and its nominalized head. When the co-occurrence frequency of the verb-argument re-
lations is zero, verb-argument tuples are smoothed using one of the methods described
in Section 3.
Once frequencies (either actual or reconstructed through smoothing) for verb-argu-
ment relations have been obtained, the RA score determines the relation between the
head n1 and its modifier n2 (see Section 2). The sign of the RA score indicates which
relation, subject or object, is more likely: a positive RA score indicates an object relation,
whereas a negative score indicates a subject relation. Depending on the task and the
data at hand, we can require that an object or subject analysis be preferred only if RA
exceeds a certain threshold j (see steps 7 and 8 in Figure 1). We can also impose a
threshold k on the type of verb-argument tuples we smooth. If, for instance, we know
369
Lapata The Disambiguation of Nominalizations
Table 5
RA score for verb-argument tuples extracted from the BNC.
Verb-noun f (vn2,obj, n1) f (vn2,subj, n1) RA
administer student 0 0 .96
establish unit 22 1 .55
promote government 3 10 ?1.73
that the parser?s output is noisy, then we might choose to smooth not only unseen verb-
argument pairs but also pairs with nonzero corpus frequencies (e.g., f (verbn2 , rel , n1)
? 1; see steps 3 and 4 in Figure 1).
Consider, for example, the compound student administration: its corresponding
verb-noun configuration (e.g., administer student) is not attested in the BNC. This is
a case in which we need smoothed estimates for both f (vn2, obj, n1) and f (vn2, subj,
n1). The re-created frequencies using the class-based smoothing method described in
Section 3.2 are 5.06 and 2.59, respectively, yielding an RA score of .96 (see Table 5),
which means that it is more likely that student is the object of administration. Consider
now the compound unit establishment: here, we have very little evidence in the corpus
with respect to the verb-subject relation (see Table 5, where f (establish , subj, unit) = 1).
Assuming we have set the threshold k to 2 (see steps 4 and 5 in Figure 1) we need only
re-create the frequency for the subject relation (e.g., 14.99 using class-based smooth-
ing). The resulting RA score is again positive (see Table 5), which indicates that there
is a greater probability for unit to be the object of establishment than for it to be the
subject. Finally, consider the compound government promotion: counts for both subject
and object relations are found in the BNC (see Table 5), in which case no smoothing
is involved; we need only calculate the RA score (see step 6 in Figure 1), which is
negative, indicating that government is more likely to be the subject of promotion than
its object.
5. Experiments
5.1 Methodology
The algorithm described in the previous section and the smoothing variants were
evaluated on the task of disambiguating nominalizations. As detailed above, the
Jensen-Shannon divergence and confusion probability measures are parameterized.
This means that we need to establish empirically the best parameter values for the
size of the vocabulary (i.e., number of verbs used to find the nearest neighbors)
and, for the Jensen-Shannon divergence, the effect of the ? parameter. Recall from
Section 2.2.2 that we obtained 796 nominalizations from the BNC. From these, 596
were used as training data for finding the optimal parameters for the two variants of
distance-weighted averaging. The 596 nominalizations were also used to find the op-
timal thresholds for the interpretation algorithm. The remaining 200 nominalizations
were retained as test data and also to evaluate whether human judges can reliably
disambiguate the argument relation between the nominalized head and its modifier
(see Experiment 1).
In Experiment 2 we investigate how the different smoothing techniques detailed
in Section 3 influence the disambiguation task. As far as class-based smoothing is con-
cerned, we experiment with two concept hierarchies, Roget?s thesaurus and WordNet.
Although no parameter tuning is necessary for class-based and back-off smoothing, we
370
Computational Linguistics Volume 28, Number 3
maintain the train/test data distinction also for these methods to facilitate comparisons
with distance-weighted averaging.
We also examine whether knowledge of the semantics of the suffix of the nominal-
ized head can improve performance. We run two versions of the algorithm presented
in Section 4: in one version the algorithm assumes no prior knowledge about the se-
mantics of the nominalization suffix (see Figure 1); in the other version the algorithm
estimates the probabilities P(obj | n1, n2) and P(subj | n1, n2) only for compounds with
nominalization suffixes other than -er, -or, -ant, or -ee. For compounds with suffixes -er,
-or and -ant (e.g., datum holder, car collector, water disinfectant), the algorithm defaults
to an object interpretation, and it defaults to a subject analysis for compounds with
the suffix -ee (e.g., university employee). Compounds with heads ending in these four
suffixes represented 13.6% of the compounds contained in the train set and 10.8% of
the compounds in the test set.
In Experiment 3 we explore how the combination of the different smoothing meth-
ods influences disambiguation performance; we also consider context as an additional
predictor of the argument relation of a deverbal head and its modifier and combine
these distinct information sources using Ripper (Cohen 1996), a machine learning sys-
tem that induces sets of rules from preclassified examples.
In what follows we briefly describe our study on assessing how well humans
agree on disambiguating nominalizations. This study establishes an upper bound for
the task against which our automatic methods will be compared. Sections 5.3 and 5.4
present our results on the disambiguation task.
5.2 Experiment 1: Agreement
Two graduate students in linguistics decided whether modifiers were the subject or
object of a given nominalized head. The judges were given a page of guidelines but no
prior training. The nominalizations were disambiguated in context: the judges were
given the corpus sentence in which the nominalization occurred together with the
previous and following sentence. We measured the judges? agreement using the kappa
coefficient (Siegel and Castellan 1988), which is the ratio of the proportion of times P(A)
that k raters agree (corrected by chance agreement P(E)) to the maximum proportion
of times the raters would agree (corrected for chance agreement):
K =
P(A)? P(E)
1 ? P(E) (22)
If there is a complete agreement among the raters, then K = 1, whereas if there is
no agreement among the raters (other than the agreement that would be expected to
occur by chance), then K = 0.
The judges? agreement on the disambiguation task was K = .78 (N = 200, k = 2).
This translates into a percentage agreement of 89.7%. Although the Kappa coefficient
has a number of advantages over percentage agreement (e.g., it takes into account the
expected chance interrater agreement; see Carletta (1996) for details), we also report
percentage agreement as it allows us to compare straightforwardly the human perfor-
mance and the automatic methods described below, whose performance will also be
reported in terms of percentage agreement. Furthermore, percentage agreement estab-
lishes an intuitive upper bound for the task (i.e., 89.7%), allowing us to interpret how
well our empirical models are doing in relation to humans.
Finally, note that the level of agreement was good, given that the judges were
provided with minimal instructions and no prior training. Even though context was
provided to aid the disambiguation task, however, the judges were not in complete
371
Lapata The Disambiguation of Nominalizations
Figure 2
Disambiguation accuracy as the number of similar neighbors (i.e., number of verbs over which
the similarity function is calculated) is varied for PC and J.
agreement. This points to the intrinsic difficulty of the task at hand. Argument re-
lations and consequently selectional restrictions are influenced by several pragmatic
factors that may not be readily inferred from the immediate context (see Section 6 for
discussion).
5.3 Experiment 2: Comparison of Smoothing Variants
Before reporting the results of the disambiguation task, we describe our initial ex-
periments on finding the optimal parameter settings for the two distance-weighted
averaging smoothing methods.
Figure 2 shows how performance on the disambiguation task varies with respect
to the number and frequency of verbs over which the similarity function is calculated.
The y-axis in Figure 2 shows how performance on the training set varies (for both PC
and J divergence) when verb-argument pairs are selected for the 1,000 most frequent
verbs in the corpus, the 2,000 most frequent verbs in the corpus, etc. (x-axis). The best
performance for both similarity functions is achieved with the 2,000 most frequent
verbs. Furthermore, J and PC yield comparable performances (68.0% and 68.3%, re-
spectively under that condition). Another important observation is that performance
deteriorates less severely for PC than for J as the number of verbs increases: when
all verbs for which verb-argument tuples are extracted from the BNC are used, the
accuracy for PC is 66.9%, whereas the accuracy for J is 62.8%. These results are perhaps
unsurprising: verb-argument pairs with low-frequency verbs introduce noise due to
the errors inherent in the partial parser. Table 6 shows the 10 closest words to the
verb accept according to PC as the number of verbs is varied: the quality of the closest
neighbors deteriorates with the inclusion of less frequent verbs.
Finally, we analyzed the role of the parameter ?. Recall that ? appears in the weight
function for the Jensen-Shannon divergence and controls the influence of the most
similar words: the contribution of the closest neighbors increases with a high value
for ?. Figure 3 shows how the value of ? affects performance on the disambiguation
task when the similarity function is computed for the 1,000 and 2,000 most frequent
verbs in the corpus. It is clear that performance is low with high or very low ? values
372
Computational Linguistics Volume 28, Number 3
Table 6
Ten closest words to verb accept for PC.
Number of Most Frequent Verbs
1,000 2,000 3,000 4,000 5,000 >5,000
accept decline decline decline decline incl
refuse accept tender tender re-issued decline
reject refuse accept abdicate co-manage re-issued
submit delegate table accept tender co-manage
endorse reject disclaim table oversubscribe tender
approve repudiate plate wangle backdate goodwill
issue hitch shirk disclaim abdicate oversubscribe
implement shoulder refuse plate accept pre-arrange
acknowledge delegate proffer shirk table backdate
incur ratify apportion disdain wangle abdicate
Figure 3
Disambiguation accuracy for J as ? is varied for the 1,000 and 2,000 most frequent verbs in the
BNC.
(e.g., ? ? {2, 9}). We chose to set the parameter ? to five, and the results shown in
Figure 2 have been produced for this value for all verb frequency classes.
Table 7 shows how the three types of smoothing, back-off (B), class-based (using
WordNet (Wn) and Roget (Ro)), and distance-weighted averaging (using confusion
probability (PC) and the Jensen-Shannon divergence (J)), influence performance in
predicting the relation between a modifier and its nominalized head. For the distance-
weighted averaging methods we report the results obtained with the optimal param-
eter settings (? = 5; 2,000 most frequent verbs). The results in Table 7 were obtained
without taking the semantics of the nominalization suffix (-er, -or, -ant, -ee) into account
(see Section 5.1).
Let us concentrate on the training set first. The back-off method is outperformed
by all other methods, although its performance is comparable to that of class-based
smoothing using Roget?s thesaurus (63.1% and 65.1%, respectively). Distance-weighted
averaging methods outperform concept-based methods, although not considerably
(accuracy on the training set was 68.3% for PC and 68.0% for class-based smoothing
373
Lapata The Disambiguation of Nominalizations
Table 7
Disambiguation performance without
nominalization suffixes.
Methods Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
B 63.1 ? 1.98 69.6 ? 3.31
PC 68.3 ? 1.90 75.8 ? 3.08
J 68.0 ? 1.91 69.1 ? 3.33
Wn 68.0 ? 1.91 72.7 ? 3.20
Ro 65.1 ? 1.95 68.6 ? 3.34
Table 8
Disambiguation performance with
nominalization suffixes.
Methods Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
B 67.5 ? 1.92 69.6 ? 3.31
PC 70.6 ? 1.87 76.3 ? 3.06
J 69.0 ? 1.89 69.6 ? 3.31
Wn 70.5 ? 1.87 74.2 ? 3.15
Ro 67.5 ? 1.92 69.6 ? 3.31
using WordNet). Furthermore, the particular concept hierarchy used for class-based
smoothing seems to have an effect on disambiguation performance: an increase of
approximately 3.0% is obtained by using WordNet instead of Roget?s thesaurus. One
explanation might be that Roget?s thesaurus is too coarse-grained a taxonomy for
the task at hand. We used the chi-square statistic to examine whether the observed
performance is better than the simple default strategy of always choosing an object
relation, which yields an accuracy of 59.0% in the training data (see D in Table 7). The
proportion of nominalizations classified correctly was significantly greater than 59.0%
(p < .01) for all methods but back-off (B) and Roget (Ro).
Similar results are observed on the test set. Again PC outperforms all other meth-
ods, achieving an accuracy of 75.8% (see Table 7). The portion of nominalizations
classified correctly by PC is significantly greater than 61.5% (?2 = 9.37, p < .01), which
is the percentage of object relations in the test set. The second-best method is class-
based smoothing using WordNet (see Table 7). WordNet?s performance is also signif-
icantly better (?2 = 5.64, p < .05) than the baseline. The back-off method, class-based
smoothing using Roget?s thesaurus, and J yield comparable results (see Table 7).
Table 8 shows how each method performs when knowledge about the semantics of
the nominalization suffix is taken into account. Recall that compounds with agentive
and passive suffixes (i.e., -er, -or, -ant, and -ee) represent 13.6% of the training data and
10.8% of the test data. A general observation is that knowledge of the semantics of the
nominalization suffix does not dramatically influence accuracy. Performance on the test
data increases 1.5% for Wn , 1.0% for Ro and 0.5% for distance-weighted averaging (see
J and PC in Table 8). We observe no increase in performance for back-off smoothing
(see Tables 7 and 8). These results suggest that the nominalization suffixes do not
contribute much additional information to the interpretation task, as their meaning
can be successfully retrieved from the corpus.
An interesting question is the extent to which any of the different methods agree
in their assignments of subject and object relations. We investigated this by calculating
the methods? agreement on the training set using the Kappa coefficient. We calculated
the Kappa coefficient for all pairwise combinations of the five smoothing variants.
The results are reported in Table 9. The highest agreement is observed for PC and the
class-based smoothing using the WordNet taxonomy (K = .75). Agreement between J
and PC as well as agreement between Wn and Ro is rather low (K = .53 and K = .46,
respectively). Note that generally low agreement is observed when B is paired with
J, PC, Wn, or Ro. This is not entirely unexpected, given the assumptions underlying
the different smoothing techniques. Both class-based and distance-weighted averaging
methods recreate the frequency of unseen word combinations by relying on corpus
374
Computational Linguistics Volume 28, Number 3
Table 9
Agreement between smoothing methods.
B J PC Wn
J .31
PC .26 .53
Wn .01 .37 .75
Ro .25 .26 .49 .46
Table 10
Performance at predicting argument
relations.
Train (%) Test (%)
Methods subj obj subj obj
B 41.6 78.1 38.0 87.8
PC 47.4 82.9 54.9 87.8
J 34.7 91.2 35.2 88.6
Wn 47.8 82.1 49.3 86.2
Ro 50.6 74.4 46.5 81.3
evidence for words that are distributionally similar to the words of interest. In distance-
weighted averaging smoothing, word similarity is estimated from lexical co-occurrence
information, whereas in taxonomic class-based smoothing, similarity emerges from
the hierarchical organization of conceptual information. Back-off smoothing, however,
incorporates no notion of similarity: unseen sequences are estimated using not similar
conditional distributions, but lower-level ones. This also relates to the fact that B?s
performance is lower than Wn and PC (see Table 7), which suggests that smoothing
methods that incorporate linguistic hypotheses (i.e., the notion of similarity) perform
better than methods relying simply on co-occurrence distributions. To summarize, the
agreement values in Table 9 suggest that methods inducing similarity relationships
from corpus co-occurrence statistics are not necessarily incompatible with methods that
quantify similarity using manually crafted taxonomies and that different smoothing
techniques may be appropriate for different tasks.
Table 10 shows how the different methods compare for the task of predicting
the individual argument relations for the training and test sets. A general observa-
tion is that all methods are fairly good at predicting object relations. Predicting sub-
ject relations is considerably harder: no method exceeds an accuracy of 54.9% (see
Table 10). One explanation for this is that selectional constraints imposed on sub-
jects can be more easily overridden by pragmatic and contextual factors than those
imposed on objects. Furthermore, selectional constraints on subjects are normally
weaker than on objects. J is particularly good at predicting object relations, whereas
PC yields the best performance when it comes to predicting subject relations (see
Table 10).
5.4 Experiment 3: Using Ripper to Disambiguate Nominalizations
An obvious question is whether a better performance can be achieved by combining
the five smoothing variants, given that they seem to provide complementary infor-
mation for predicting argument relations. For example, Wn , Ro, and PC are relatively
good for the prediction of subject relations , whereas J is best for the prediction of ob-
ject relations (see Table 10). Furthermore, note that the probabilistic model introduced
in Section 2 and the algorithm based on it (see Section 4) ignore contextual informa-
tion that can provide important cues for disambiguating nominalizations. Consider the
nominalization government promotion in (23a), which was assigned an object (instead
of a subject) interpretation by all smoothing variants except Wn. Contextual informa-
tion could help assign the correct interpretation in cases in which the head of the
compound is followed by prepositions such as of (see (23a)) or into (see (23b)).
375
Lapata The Disambiguation of Nominalizations
(23) a. It was not felt necessary to take account of government promotion of
unionism.
b. But politicians are calling for the Republic?s Government to start a
Court inquiry into Ross? alleged links with firms in Eire.
In the following we first examine whether combination of the five smoothing vari-
ants improves performance at predicting the argument relations for nominalizations
(see Section 5.4.1). We then proceed to study the influence of context on the inter-
pretation task; we explore the contribution of context alone (see Section 5.4.2) and in
combination with the different smoothing variants (see Section 5.4.3). The different
information sources are combined using Ripper (Cohen 1996), a system that induces
classification rules from a set of preclassified examples. Ripper takes as input the
classes to be learned (in our case the classification is binary, i.e., subject or object), the
names and possible values of a set of features, and training data specifying the class
and feature values for each training example. In our experiments the features are the
smoothing variants and the tokens surrounding the nominalizations in question. The
feature vector in (24a) represents the individual predictions of B, Wn , Ro, J, and PC
for the interpretation of government promotion (see (23a)). We encode the context sur-
rounding nominalizations using two distinct representations: (a) parts of speech and
(b) lemmas. In both cases we encode the position of the tokens with respect to the
nominalization in question. The feature vector in (24b) consists of the nominalization
court inquiry (see (23b)), represented by its parts of speech (nn1 and nn1, respectively)
and a context of five words to its right and five words to its left, also reduced to their
parts of speech. In (24c) the same tokens are represented by their lemmas.
(24) a. [obj, subj, obj, obj, obj]
b. [pos, nn0, to0, vvi, aj0, nn1, nn1, prp, pos, aj0, nn2, prp]
c. [?s government to start a court inquiry into Ross ?s alleged link]
Ripper is trained on vectors of values like the ones presented in (24) and out-
puts a classification model for classifying future examples. The model is learned using
greedy search guided by an information gain metric and is expressed as an ordered
set of if-then rules. For our experiments Ripper was trained on the 596 nominaliza-
tions on which the smoothing methods were compared and tested on the 200 unseen
nominalizations for which the interjudge agreement was previously calculated (see
Section 5.2).
5.4.1 Combination of Smoothing Variants. Table 11 shows Ripper?s performance
when different combinations of smoothing variants (i.e., features) are used without
taking context into account. All results in Table 11 were obtained using the version of
the interpretation algorithm that takes suffix semantics into account (see Section 5.3).
As shown in Table 11, the combination of all five smoothing variants achieves a per-
formance of 80.4%.2 Table 11 further reports the accuracy achieved when removing
2 An anonymous reviewer pointed out that suffix information could be alternatively exploited by
including the ending suffix of the nominalization head as an additional feature for the classification
task. The latter approach yields comparable performance to our original idea of defaulting to the
argument structure denoted by the nominalization suffix. When B, J, PC, Ro , and Wn are used as
features together with nominalization suffixes (-age, -ion, -ment, etc.), Ripper?s performance is 79.9%
? 1.65 on the training data and 80.3% ? 2.95% on the test data.
376
Computational Linguistics Volume 28, Number 3
Table 11
Disambiguation performance using the
smoothing variants as features.
Features Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
B, J, PC, Ro, Wn 80.2 ? 1.63 80.4 ? 2.86
B, J, PC, Wn 80.2 ? 1.68 80.4 ? 2.88
B, J, PC, Ro 78.5 ? 1.68 79.9 ? 2.88
B, J, Wn, Ro 80.7 ? 1.62 79.9 ? 2.88
J, PC, Ro, Wn 80.7 ? 1.62 78.4 ? 2.96
B, PC, Wn, Ro 79.8 ? 1.64 74.7 ? 3.13
Table 12
Ripper?s performance at predicting
argument relations.
Train (%) Test (%)
Features subj obj subj obj
B, J, PC, Ro, Wn 66.5 89.7 73.2 84.6
B, J, PC, Wn 66.5 89.7 73.2 84.6
B, J, Wn, Ro 71.4 87.2 78.9 80.5
B, J, PC, Ro 71.4 87.2 78.9 80.5
J, PC, Ro, Wn 69.4 88.6 71.8 82.1
B, PC, Wn, Ro 63.3 91.5 50.7 88.6
a single feature. Evaluation on subsets of features allows us to explore the contribu-
tion of individual features to the classification task by comparing the subsets to the
full feature set. We see that removal of Ro has no effect on the results, whereas re-
moval of J produces a 5.7% performance decrease. Removing Wn or PC yields the
same decrease in performance (i.e., 0.5%). This is not surprising, since PC and Wn
tend to agree in their assignments of subject and object relations (see the methods?
agreement in Table 9), and therefore their combination is not expected to be very in-
formative. Absence of J from the feature set yields the most dramatic performance
decrease. This is not unexpected, given that J is the best predictor for object relations
and that PC and WordNet behave similarly with respect to their interpretation deci-
sions. In general we observe that the combination of smoothing variants outperforms
their individual performances (compare Tables 11 and 8). Comparison of Ripper?s
best performance (80.4%) against the individual smoothing methods reveals a 10.8%
accuracy increase over B, J, and Ro, a 4.1% increase over PC, and a 6.2% increase
over Wn .
We further analyzed Ripper?s performance at predicting object and subject rela-
tions. This information is displayed in Table 12, in which we show how performance
varies on the full set of n size features (i.e., five) and each of its n?1 size subsets. As can
be seen in Table 12, accuracy at predicting subject relations increases when smoothing
variants are combined (compare Tables 12 and 10). In fact, combination of B, J, Wn ,
and Ro (or B, J, PC, and Ro) performs best at predicting subject relations, achieving
an increase of 24% over PC, the best individual predictor for subject relations (see Ta-
ble 10). In sum, our results show that combination of the different smoothing variants
(using Ripper) achieves better results than each individual method. Our overall perfor-
mance (i.e., 80.4%) outperforms the default baseline significantly, by 18.9% (?2 = 17.33,
p < .05) and is 9.3% lower than the upper bound established in our agreement study
(see Section 5.2). In what follows we first examine the independent contribution of
context to the disambiguation performance and then turn to its combination with our
five smoothing variants.
5.4.2 The Contribution of Context. We evaluated the influence of context by varying
both the position and the size of the window of tokens (i.e., lemmas or parts of speech)
surrounding the nominalization. We varied the window size parameter between one
and five words before and after the nominalization target. We use the symbols l and
r for left and right context, respectively, subscripts to denote the context encoding
(i.e., lemmas or parts of speech), and numbers to express the size of the window
377
Lapata The Disambiguation of Nominalizations
Table 13
Disambiguation performance using right
context encoded as lemmas.
Features Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
rl = 1 70.8 ? 1.86 68.0 ? 3.36
rl = 2 70.1 ? 1.88 68.6 ? 3.34
rl = 3 68.8 ? 1.90 67.5 ? 3.37
rl = 4 68.8 ? 1.90 67.5 ? 3.37
rl = 5 68.8 ? 1.90 67.5 ? 3.37
Table 14
Disambiguation performance using left
content encoded as lemmas.
Features Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
ll = 1 66.9 ? 1.93 64.9 ? 3.43
ll = 2 70.5 ? 1.87 67.5 ? 3.37
ll = 3 70.6 ? 1.87 67.0 ? 3.83
ll = 4 67.8 ? 1.92 65.5 ? 3.42
ll = 5 65.3 ? 1.95 63.9 ? 3.46
Table 15
Disambiguation performance using right
context encoded as POS tags.
Features Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
rp = 1 64.9 ? 1.96 65.5 ? 3.42
rp = 2 65.8 ? 1.95 62.4 ? 3.49
rp = 3 64.4 ? 1.96 63.4 ? 3.47
rp = 4 65.3 ? 1.95 63.4 ? 3.47
rp = 5 65.9 ? 1.94 62.9 ? 3.48
Table 16
Disambiguation performance using left
content encoded as POS tags.
Features Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
lp = 1 63.9 ? 1.97 66.0 ? 3.41
lp = 2 68.1 ? 1.91 64.4 ? 3.45
lp = 3 67.1 ? 1.93 66.5 ? 3.40
lp = 4 65.6 ? 1.95 65.0 ? 3.43
lp = 5 66.6 ? 1.93 61.9 ? 3.50
surrounding the candidate compound. For example, ll = 5 represents a window of
five tokens, encoded as lemmas, to the left of the candidate compound.
Tables 13 and 14 show the influence of right and left context, respectively, repre-
sented as lemmas. The best peformances are achieved with a window of two words
to the right or left of the candidate nominalization (see the features rl = 2 and ll = 2 in
Tables 13 and 14, respectively). Combination of the best left and right features (rl = 2,
ll = 2) does not increase the disambiguation performance (70.4% ? 1.86% on the train-
ing and 66.5% ? 3.41% on the test data). Note that the disambiguation performance
simply using contextual features is not considerably worse than the performance of
some smoothing variants (see Table 7). Contextual features encoded as lemmas out-
perform part-of-speech (POS) tags, for which the best performance is achieved with a
window of one token to the right or a window of three tokens to the left of the can-
didate nominalization (see Tables 15 and 16). As in the case of lemmas, combination
of the best left and right features (rp = 1, lp = 3) does not yield better results (66.3% ?
1.94% on the training data and 66.5% ? 3.40% on the test data). The lower performance
of POS tags is not entirely unexpected: lemmas capture lexical dependencies that are
somewhat lost when a more general level of representation is introduced. For example,
Ripper assigns a subject interpretation when for immediately follows a nominalization
head (e.g., staff requirement for reconnaissance). This rule cannot be induced when for is
represented by its part of speech (e.g., PRP), as there are a number of prepositions
that can follow the nominalization head, but only a few of them provide cues for its
argument structure.
Table 17 shows the performance of the best contextual features for the task of
predicting the individual argument relations. The contextual features are consistently
better at predicting object than subject relations. This is not surprising, given that ob-
378
Computational Linguistics Volume 28, Number 3
Table 17
Performance at predicting argument relations using context.
Train (%) Test (%)
Methods subj obj subj obj
rl = 2 28.0 99.2 20.8 96.7
ll = 2 36.2 94.1 13.8 97.5
lp = 3 33.7 90.1 29.1 88.5
rp = 1 22.6 94.1 20.8 91.8
ject relations represent the majority in both the training and test data; furthermore,
identifying superficial features that are good predictors for subject relations is a rel-
atively hard task. For example, even though Ripper identifies prepositions (e.g., of,
to) following the nominalization head and certain frequent nominalization heads (e.g.,
behavior) as indicators of subject relations, it has no means of guessing the transitivity
of deverbal heads in the absense of syntactic cues. Consider example (25a), in which
neither left nor right context is informative with regard to the fact that intervene is
intransitive.
Finally, there are some cases in which the syntactic cues can be misleading, as
adjacency to the nominalization target does not necessarily indicate argument struc-
ture. This is shown in (25b), in which youth is classified as the subject of manager.
Although on the surface youth manager at is analogous to nominalizations followed
by of (e.g., government promotion of), the prepositional phrase at Wimbledon in (25b) is
simply locative and not the argument of manager.
(25) a. If the second reminder produces no result or the reply to either
reminder seems to indicate the need for court intervention the matter
will be referred to a master or district judge.
b. He was youth manager at Wimbledon when I held a similar position
at Palace.
5.4.3 Combination of Context with Smoothing Variants. In this section we investi-
gate whether the combination of surface contextual features with the predictions of
the different smoothing methods has an effect on the disambiguation performance.
Although context is good at predicting object relations, it performs poorly at guessing
subject relations (see Table 17). We expect the combination of context with smoothing
variants (some of which, e.g., Wn , Ro, and PC, perform relatively well at the predicting
subject relations) to improve performance. Recall that the probabilistic model intro-
duced in Section 2.1 and the interpretation algorithm that makes use of it attempt the
interpretation of nominalizations without taking contextual cues into account. Here,
we examine how well the different smoothing variants perform in the presence of
contextual information. Table 18 shows Ripper?s performance when the best context
(i.e., rl = 2) is combined with a single smoothing method and with all five variants.
For the smoothing variants, we used the version of the interpretation algorithm that
takes suffix semantics into account (see Table 8).
Comparison between Tables 8 and 18 reveals that the inclusion of context generally
increases performance. Combination of B with the best context yields a 6.7% increase
over B; an increase of 8.8% (over J) and 7.7% (over Ro) is observed when J and Ro
are combined with context, respectively. No increase in performance is observed when
379
Lapata The Disambiguation of Nominalizations
Table 18
Disambiguation performance using context and smoothing variants.
Methods Train (%) Test (%)
D 59.0 ? 2.01 61.5 ? 3.50
rl = 2, B 78.2 ? 1.69 76.3 ? 3.06
rl = 2, PC 75.0 ? 1.78 76.3 ? 3.06
rl = 2, J 81.5 ? 1.59 78.4 ? 2.96
rl = 2, Wn 88.9 ? 1.29 86.1 ? 2.49
rl = 2, Ro 78.5 ? 1.68 77.3 ? 3.00
B, J, PC, Ro, Wn, rl = 2 84.4 ? 1.49 85.1 ? 2.57
Table 19
Argument relations using context and smoothing variants.
Train (%) Test (%)
Methods subj obj subj obj
rl = 2, B 69.9 83.6 61.3 85.7
rl = 2, PC 63.9 82.2 54.9 88.6
rl = 2, J 72.9 87.2 66.7 85.7
rl = 2, Wn 87.3 90.0 74.7 93.3
rl = 2, Ro 69.1 84.7 64.0 85.7
B, J, PC, Ro, Wn, rl = 2 75.0 90.6 72.0 93.3
context is combined with PC (see Table 18), whereas combination of Wn with context
yields a 11.9% increase over Wn alone. Combining all five smoothing variants with
context yields an increase of 4.7% over just the combination of B, J, PC, Ro, and Wn
(see Table 12). Our best performance (i.e., 86.1%) is achieved when Wn is combined
with right context (rl = 2); this performance is significantly better than the simple
strategy of always defaulting to a subject classification, which yields an accuracy of
61.5% (?2 = 30.64, p < .05), and only 3.6% lower than the upper bound of 89.7%.
As shown in Table 19, the inclusion of context increases accuracy when it comes to
the prediction of subject relations (with the exception of PC, which is relatively good
at predicting subject relations, and therefore in that case the inclusion of context does
not add much useful information). The combination of Wn with rl = 2 achieves the
highest accuracy (87.3%) at predicting subject relations.
6. Discussion
We have described an empirical approach for the automatic interpretation of nominal-
izations. We cast the interpretation task as a disambiguation problem and proposed
a statistical model for inferring the argument relations holding between a deverbal
head and its modifier. Our experiments revealed that the interpretation task suffers
from data sparseness: even an approximation that maps the nominalized head to its
underlying verb does not provide sufficient evidence for quantifying the argument
relation of a modifier noun and its nominalized head.
We showed how the argument relations (which are not readily available in the cor-
pus) can be retrieved by using partial parsing and smoothing techniques that exploit
380
Computational Linguistics Volume 28, Number 3
distributional and taxonomic information. We compensated for the lack of sufficient
distributional information using either methods that directly recreate the frequencies
of word combinations or contextual features whose distribution in the corpus indi-
rectly provides information about nominalizations. We compared and contrasted a
variety of smoothing approaches proposed in the literature and demonstrated that
their combination yields satisfactory results for the demanding task of semantic dis-
ambiguation. We also explored the contribution of context and showed that it is use-
ful for the disambiguation task. Our approach is applicable to domain-independent
unrestricted text and does not require the hand coding of semantic information. In
the following sections we discuss our results and their potential usefulness for NLP
applications. We also address the limitations of our approach and sketch potential
extensions.
6.1 The Interpretation of Nominalizations
Our results indicate that a simple probabilistic model that uses smoothed counts (see
the interpretation algorithm in Section 4) yields a significant increase over the base-
line without taking context into account. Distance-weighted smoothing using PC and
class-based smoothing using WordNet achieve the best results (76.3% and 74.2%, re-
spectively). Combination of different smoothing methods (using Ripper) yields an
overall performance of 80.4%, again without taking context into consideration. Con-
text alone achieves a disambiguation performance of 68.6%, approximating the per-
formance of some of the smoothing variants (see Tables 9 and 13). This result suggests
that simple features that can be easily retrieved and estimated from the corpus contain
enough information to capture generalizations about the behavior of nominalizations.
As expected, the combination of smoothed probabilities with context outperforms the
accuracy of individual smoothing variants. The combination of WordNet with a right
context of size two achieves an accuracy of 86.1%, compared to an upper bound for
the task (i.e., intersubject agreement) of 89.7%. This is an important result considering
the simplifications in the system and the sparse data problems encountered in the
estimation of the model probabilities. The second-best performance is achieved when
J is combined with context (78.4%; see Table 18). This result shows that information
inherent in the corpus can make up for the lack of distributional evidence and further-
more that it is possible to extract semantic information from corpora (even if they are
not semantically annotated in any way) without recourse to pre-existing taxonomies
such as WordNet.
6.2 Limitations and Extensions
To a certain extent the difficulty of interpreting nominalizations is due to their context
dependence. Although the approach presented in the previous sections takes immedi-
ate context into account, it does so in a shallow manner, without having access to the
meaning of the words surrounding the nominalization target, their syntactic depen-
dencies, or the general discourse context within which the compound is embedded.
Consider example (26a), in which the compound computer guidance receives a subject
interpretation (e.g., the computer guides the chef). Our approach cannot detect that the
computer here is ascribed animate qualities and opts for the most likely interpretation
(i.e., an object analysis). In some cases the modifier stands in a metonymic relation to
its head. Consider the examples in sentences (26b, 26c), in which the nominalizations
industry reception and market acceptance can be thought of as instances of the metonymic
schema ?whole for part? (Lakoff and Johnson 1980). In example (26b) it is the indus-
try as a whole that receives the guests rather than lasmo, which is one of its parts,
381
Lapata The Disambiguation of Nominalizations
whereas in (26c) the modifier market in market acceptance refers to the opinion leaders,
who are part of the market.
(26) a. Of course, none of this means that the equipment is taking anything
away from the chef?s own individual skills which are irreplaceable.
What it does ensure is that the chef has complete control over some of
the most vital tools of his trade, with computer guidance as an
important aid.
b. The final evening saw more than 300 guests attend an industry
reception, hosted by lasmo.
c. Marketers interested in the development and introduction of new
products will be particularly interested in the attitude of opinion
leaders to these products, for their general market acceptance can
be slowed down or speeded up by the views of such people.
Consider now sentence (27a). The nominalization student briefing is ambiguous,
even though it is presented within its immediate context. Taking more context into ac-
count (see (27a)) does not provide enough disambiguation information either, although
perhaps it introduces a slight bias in favor of an object interpretation (i.e., someone is
briefing the students). For this particular example, we would have to know what the
document within which student briefing occurs is about (i.e., a list of teaching guide-
lines for university lecturers). The sentences in (27) are taken from a document section
entitled ?Work Experience? that emphasizes the importance of work experience for
students. Given all this background information, it becomes apparent that it is not the
students who are doing the briefing in (27b).
(27) a. Explain to both students and organisations the role of work experience
in personal development and its part in the planned programme.
b. Provide comprehensive guidelines on the work experience which
includes a student briefing, an employer briefing and a student work
checklist.
The observation that discourse or pragmatic context may influence interpretations
is by no means new or particular to nominalisations. Sparck Jones (1983) observes that
a variety of factors can potentially influence the interpretation of compound nouns in
general. These factors range from syntactic analysis (e.g., to arrive at an interpretation
of the compound onion tears, it is necessary to identify that tears is a noun and not the
third-person singular of the verb tear) to semantic information (e.g., for interpreting
onion tears, it is important to know that onions cannot be tears or that tears are not
made of onions) and pragmatic information. Pragmatic inference may be called for in
cases in which syntactic or semantic information is straightforwardly supplied, even
where the local text context provides rich information bearing on the interpretation of
the compound. Copestake and Lascarides (1997) and Lascarides and Copestake (1998)
make the same observation for a variety of constructions such as compound nouns,
adjective-noun combinations and verb-argument relations. Consider the sentences in
(28)?(30). The discourse in (28) favors the interpretation ?bag for cotton clothes? for
cotton bag over the more likely interpretation ?bag made of cotton.? Although fast
programmer is typically a programmer who programs fast, when the adjective-noun
combination is embedded in a context like (29a, 29b), the less likely meaning ?a
382
Computational Linguistics Volume 28, Number 3
programmer who runs fast? is triggered. Finally, although it is more likely to enjoy
reading a book rather than eating it, the context in (30) triggers the latter interpreta-
tion.
(28) a. Mary sorted her clothes into various bags made from plastic.
b. She put her skirt into the cotton bag.
(29) a. All the office personnel took part in the company sports day last week.
b. One of the programmers was a good athlete, but the other was
struggling to finish the courses.
c. The fast programmer came first in the 100m.
(30) a. My goat eats anything.
b. He really enjoyed your book.
Pragmatic context may be particularly important for the interpretation of com-
pound nouns. Because compounds can be used as a text compression device (Marsh
1984), it is plausible that pragmatic inference is required to supply the compound?s in-
terpretation. This observation is somewhat supported by our interannotator agreement
experiment (see Section 5.2). Even though our participants were provided with some
context, the agreement among them was not complete (they reached a K of .78, when
absolute agreement is 1). Although our approach takes explicit contextual information
into account, it is agnostic to discourse or pragmatic information. Encoding pragmatic
information would involve considerable manual effort. Furthermore, a hypothetical
statistical learner that takes pragmatic information into account would have not only
to deal with data sparseness but furthermore to detect cases in which conflicts arise
between discourse information and the likelihood of a given interpretation.
Our experiments focused on nominalizations derived from verbs specifically sub-
categorizing for direct objects. Although nominalizations whose verbs take prepo-
sitional frames (e.g., oil painting, soccer competition) represent a small fraction of the
nominalizations found in the corpus (9.2%), a more general approach would have
to take those verbs into account. This task is harder than interpreting direct objects,
since to estimate the frequency f (vn2 , rel , n1), one needs first to determine with some
degree of accuracy the attachment site of the prepositional phrase. Taking into account
prepositional phrases and their attachment sites can also be useful for the interpreta-
tion of compounds other than nominalizations. Consider the compound noun pet spray
from (1). Assuming that pet spray can be ?spray for pets,? ?spray in pets,? ?spray about
pets,? or ?spray from pets,? we can derive the most likely interpretation by looking at
which types of prepositional phrases (e.g., for pets, about pets) are most likely to attach
to spray. Note that in cases in which the expressions spray for pets and spray in pets are
not attested in the corpus, their respective co-occurrence frequencies can be re-created
using the techniques presented in Section 3.
Finally, the approach advocated here can be straightforwardly extended to nomi-
nalizations with adjectival modifiers (e.g., parental refusal; see the examples in (2)). In
most cases the adjective in question is derived from a noun, and any inference process
on the argument relations between the head noun and the adjectival modifier could
take advantage of this information.
383
Lapata The Disambiguation of Nominalizations
6.3 Relevance for NLP Applications
Robust semantic ambiguity resolution is challenging for current NLP systems. Al-
though general-purpose taxonomies like WordNet or Roget?s thesaurus are useful for
certain interpretation tasks, such resources are not exhaustive or generally available for
languages other than English. Furthermore, the compound noun interpretation task
involves acquiring semantic information that is linguistically implicit and therefore
not directly available in corpora or taxonomic resources. Indeed, interpreting com-
pound nouns is often analyzed in the linguistics literature in terms of (impractical)
general-purpose reasoning with pragmatic information such as real-world knowledge
(e.g., Hobbs et al 1993; see Section 7 for details). We show that it is feasible to learn
implicit semantic information automatically from the corpus by utilizing linguistically
principled approximations, surface syntactic cues, and (when available) taxonomic
information.
The interpretation of compound nouns is important for several NLP tasks, notably
machine translation. Consider the nominalization satellite observation (taken from (4a)),
which may mean ?observation by satellite? or ?observation of satellites.? To translate
satellite observation into Spanish, we have to work out whether satellite is the subject or
object of the verb observe. In the first case satellite observation translates as observacio?n
por satelite (observation by satellite), whereas in the latter it translates as observacio?n de
satelites (observation of satellites). In this case the implicit linguistic information has
to be retrieved and disambiguated to obtain a meaningful translation. Information
retrieval is another relevant application in which again the underlying meaning must
be rendered explicit. Consider a search engine faced with the query cancer treatment.
Presumably one would not like to obtain information about cancer or treatment in
general, but about methods or medicines that help treat cancer. So knowledge about
the fact that cancer is the object of treatment could help rank relevant documents (i.e.,
documents in which cancer is the object of the verb treat) before nonrelevant ones or
restrict the number of retrieved documents.
7. Related Work
In this section we review previous work on the interpretation of compound nouns and
compare it to our own work. Despite the differences among them, most approaches
require large amounts of hand-crafted knowledge, place emphasis on the recovery of
relations other than nominalizations (see the examples in (1)), contain no quantitative
evaluation (the exceptions are Leonard (1984), Vanderwende (1994), and Lauer (1995)),
and generally assume that context dependence is either negligible or of little impact.
Most symbolic approaches are limited to a specific domain because of the large effort
involved in hand-coding semantic information and are distinguished in two main
types: concept-based and rule-based.
Under the concept-based approach, each noun in the compound is associated with
a concept and various slots. Compound interpretation reduces to slot filling, that is,
evaluating how appropriate concepts are as fillers of particular slots. A scoring sys-
tem evaluates each possible interpretation and selects the highest-scoring analysis.
Examples of the approach are Finin (1980) and McDonald (1982). As no qualitative
evaluation is reported in these studies, it is difficult to assess how their methods per-
form, although it is clear that considerable effort needs to be invested in the encoding
of the appropriate semantic knowledge.
Under the rule-based approach, interpretation is performed by sequential rule
application. A fixed set of rules is applied in a fixed order, and the first rule that is
semantically compatible with the nouns forming the compound results in the most
384
Computational Linguistics Volume 28, Number 3
plausible interpretation. The approach was introduced by Leonard (1984), was based
on a hand-crafted lexicon, and achieved an accuracy of 76.0% (on the training set).
Vanderwende (1994) further developed a rule-based algorithm that does not rely on
a hand-crafted lexicon but extracts the required semantic information from an on-line
dictionary instead. The system achieved an accuracy of 52.0%.
A variant of the concept-based approach uses unification to constrain the seman-
tic relations between nouns represented as feature structures. Jones (1995) used a
typed graph?based unification formalism and default inheritance to specify features
for nouns whose combination results in different interpretations. Again no evaluation
is reported, although Jones points out that ambiguity can be a problem, as all possible
interpretations are produced for a given compound. Wu (1993) provides a statistical
framework for the unification-based approach and develops an algorithm for approx-
imating the probabilities of different possible interpretations using the maximum-
entropy principle. No evaluation of the algorithm?s performance is given. The ap-
proach remains knowledge intensive, however, as it requires manual construction of
the feature structures.
Lauer (1995) provides a probabilistic model of compound noun paraphrasing
(e.g., state laws are ?the laws of the state,? war story is ?a story about war?) that assigns
probabilities to different paraphrases using a corpus in conjunction with Roget?s the-
saurus. Lauer does not address the interpretation of nominalizations or compounds
with hyponymic relations (see example (1e)) and takes into account only prepositional
paraphrases of compounds (e.g., of, for, in, at, etc.). Lauer?s model makes predictions
about the meaning of compound nouns on the basis of observations about preposi-
tional phrases. The model combines the probability of the modifier given a certain
preposition with the probability of the head given the same preposition and assumes
that these two probabilities are independent.
Consider, for instance, the compound war story. To derive the intended interpre-
tation (i.e., ?story about war?), the model takes into account the frequency of story
about and about war. For the modifier and head noun are substituted the concepts with
which they are represented in Roget?s thesaurus, and the frequency of a concept and a
preposition is calculated accordingly (see Section 3.2). Lauer?s (1995) model achieves
an accuracy of 47.0%. The result is difficult to interpret, given that no experiments
with humans are performed and therefore the optimal performance on the task is un-
known. Lauer acknowledges that data sparseness can be a problem for the estimation
of the model parameters and also that the assumption of independence between the
head and its modifier is unrealistic and leads to errors in some cases.
Although it is generally acknowledged that context, both intra- and intersentential,
may influence the interpretation task, contextual factors are typically ignored, with the
exception of Hobbs et al (1993), who propose that the interpretation of a compound
can be achieved via abductive inference. To interpret a compound one must prove
the logical form of its constituent parts from what is mutually known. The amount
of world knowledge required to work out what is mutually known, however, renders
such an approach infeasible in practice. Furthermore, Hobbs et al?s approach does
not capture linguistic constraints on compound noun formation and as a result cannot
predict that a noun-noun sequence like cancer lung (under the interpretation ?cancer
in the lung?) is odd.
Unlike previous work, we did not attempt to recover the semantic relations holding
between a head and its modifier (see (1)). Instead, we focused on the less ambitious
task of interpreting nominalizations, that is, compounds whose heads are derived
from a verb and whose modifiers are interpreted as its arguments. Similarly to Lauer
(1995), we have proposed a simple probabilistic model that uses information about
385
Lapata The Disambiguation of Nominalizations
the distributional properties of words and domain-independent symbolic knowledge
(i.e., WordNet, Roget?s thesaurus). Unlike Lauer, we have addressed the sparse-data
problem by directly comparing and contrasting a variety of smoothing approaches pro-
posed in the literature and have shown that these methods yield satisfactory results
for the demanding task of semantic disambiguation. Furthermore, we have shown
that the combination of different sources of taxonomic and nontaxonomic information
(using Ripper) is effective for tasks facing data sparseness. In contrast to previous
approaches, we explored the effect of context on the interpretation task and showed
that its inclusion generally improves disambiguation performance. We combined dif-
ferent information sources (e.g., contextual features and smoothing variants) using
Ripper. Although the use of classifiers has been widespread in studies concerning dis-
course segmentation (Passonneau and Litman 1997), the disambiguation of discourse
cues (Siegel and McKeown 1994), the acquisition of lexical semantic classes (Merlo
and Stevenson 1999; Siegel 1999), the automatic identification of user corrections in
spoken dialogue systems (Hirschberg, Litman, and Swerts 2001), and word sense dis-
ambiguation (Pedersen 2001), the treatment of the interpretation of compound nouns
as a classification task is, to our knowledge, novel.
Our approach can be easily adapted to account for Lauer?s (1995) paraphras-
ing task. Instead of assuming that the probability of the compound modifier given
a preposition is independent from the probability of the compound head given the
same preposition, a more straightforward model would take into account the joint
probability of the head, the preposition, and the modifier. In cases in which a certain
head, preposition, and modifier combination is not attested in the corpus (e.g., story
about war), the methodology put forward in Experiments 2 and 3 could be used to
re-create its frequency (see also the discussion in Section 6).
Unlike previous approaches, we provide an upper bound for the task. Recall from
Section 5.2 that an experiment with humans was performed to evaluate whether the
task can be performed reliably. In doing so we took context into account, and as
a result we established a higher upper bound for the task than would have been
the case if context was not taken into account. Furthermore, it is not clear whether
subjects could arrive at consistent interpretations for nominalizations out of context.
Downing?s (1977) experiments show that, when asked to interpret compounds out of
context, participants tend to come up with a variety of interpretations that are not
always compatible. For example, for the compound bullet hole, the interpretations ?a
hole made by a bullet,? ?a hole shaped like a bullet,? ?a fast-moving hole,? ?a hole
in which to hide bullets,? and ?a hole into which to throw (bullet) casings? were
provided.
8. Conclusions
In this article we presented work on the automatic interpretation of nominalizations
(i.e., compounds whose heads are derived from a verb and whose modifiers are inter-
preted as its arguments). Nominalizations pose a challenge for empirical approaches,
as the argument relations between a head and its modifier are not readily available in a
corpus, and therefore they have to be somehow retrieved and approximated. Approx-
imating the nominalized head to its corresponding verb and estimating the frequency
of verb-noun relations instead of noun-noun relations accounts for only half of the
nominalizations attested in the corpus.
Our experiments revealed that data sparseness can be overcome by taking ad-
vantage of smoothing methods and surface contextual information. We have directly
compared and contrasted a variety of smoothing approaches proposed in the literature
386
Computational Linguistics Volume 28, Number 3
and have shown that these methods yield satisfactory results for the demanding task of
semantic disambiguation, especially when coupled with contextual information. Our
experiments have shown that contextual information that is easily obtainable from
a corpus and computationally cheap is good at predicting object relations, whereas
the computationally more expensive smoothing variants are better at guessing subject
relations. Combination of context with smoothing variants yields better performance
over either context or smoothing alone.
We combined different information sources (i.e., contextual features and smoothing
variants) using Ripper. Although a considerable body of previous research has treated
several linguistic phenomena as classification tasks, the interpretation of compound
nouns has so far been based on the availability of symbolic knowledge. We show that
the application of probabilistic learning to the interpretation of compound nouns is
novel and promising. Finally, our experiments revealed that information inherent in
the corpus can make up for the lack of distributional evidence by taking advantage of
smoothing methods that rely simply on verb-argument tuples extracted from a large
corpus and surface contextual information without strictly presupposing the existence
of annotated data or taxonomic information.
Acknowledgments
This work was supported by the Alexander
S. Onassis Foundation and ESRC grant
number R000237772 (Data Intensive
Semantics and Pragmatics). Thanks to Frank
Keller, Alex Lascarides, Scott McDonald,
and three anonymous reviewers for
valuable comments.
References
Abney, Steve. 1996. ?Partial parsing via
finite-state cascades.? In John Carroll,
editor, Workshop on Robust Parsing,
pages 8?15, Prague. European Summer
School in Logic, Language and
Information, Prague.
Brown, Peter F., Vincent J. Della Pietra,
Peter V. de Souza, and Robert L. Mercer.
1992. Class-based n-gram models of
natural language. Computational Linguistics
18(4):467?479.
Burnage, Gavin. 1990. ?Celex?A guide for
users.? Technical Report, Centre for
Lexical Information, University of
Nijmegan, Nijmegan, Netherlands.
Burnard, Lou. 1995. Users Guide for the British
National Corpus. British National Corpus
Consortium, Oxford University
Computing Service, Oxford.
Carletta, Jean. 1996. Agreement on
classification tasks: The Kappa statistic.
Computational Linguistics 22(2):249?254.
Carroll, John and Diana McCarthy. 2000.
Word sense disambiguation using
automatically acquired verbal preferences.
Computers and the Humanities
34(1/2):109?114.
Church, Kenneth W. and William A. Gale.
1991. A comparison of the enhanced
Good-Turing and deleted estimation
methods for estimating probabilities of
English bigrams. Computer Speech and
Language 5:19?54.
Clark, Stephen and David Weir. 2001.
?Class-based probability estimation using
a semantic hierarchy.? In Proceedings of the
Second North American Annual Meeting of
the Association for Computational Linguistics,
pages 95?102, Pittsburgh, Pennsylvania.
Cohen, William W. 1996. ?Learning trees
and rules with set-valued features.? In
Proceedings of the 13th National Conference on
Artificial Intelligence, pages 709?716,
Portland, Oregon. AAAI Press, Menlo
Park, California.
Collins, Michael and James Brooks. 1995.
?Prepositional phrase attachment through
a backed-off model.? In David Yarowsky
and Kenneth W. Church, editors,
Proceedings of the Third Workshop on Very
Large Corpora, pages 27?38, Cambridge,
Massachusetts.
Copestake, Ann and Alex Lascarides. 1997.
?Integrating symbolic and statistical
representations: The lexicon pragmatics
interface.? In Proceedings of the 35th Annual
Meeting of the Association for Computational
Linguistics, pages 136?143, Madrid, Spain.
Dagan, Ido, Lilian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models of
word cooccurrence probabilities. Machine
Learning 34(1?3):43?69.
Downing, Pamela. 1977. On the creation
and use of English compound nouns.
Language 53(4):810?842.
Essen, Ute and Volker Steinbiss. 1992.
?Co-occurrence smoothing for stochastic
387
Lapata The Disambiguation of Nominalizations
language modeling.? In Proceedings of
International Conference on Acoustics Speech
and Signal Processing, volume 1,
pages 161?164, San Francisco, California.
Finin, Tim 1980. ?The semantic
interpretation of nominal compounds.? In
Proceedings of First National Conference on
Artificial Intelligence, pages 310?315,
Stanford, California. AAAI Press, Menlo
Park, California.
Grishman, Ralph and John Sterling. 1994.
?Generalizing automatically generated
selectional patterns.? In Proceedings of the
15th International Conference on
Computational Linguistics, pages 742?747,
Kyoto, Japan.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics 19(1):103?120.
Hirschberg, Julia, Diane Litman, and Marc
Swerts. 2001. ?Identifying user corrections
automatically in spoken dialogue
systems.? In Proceedings of the Second North
American Annual Meeting of the Association
for Computational Linguistics,
pages 208?215, Pittsburgh, Pennsylvania.
Hobbs, Jerry R., Mark Stickel, Douglas
Appelt, and Paul Martin. 1993.
Interpretation as abduction. Journal of
Artificial Intelligence 63(1?2):69?142.
Isabelle, Pierre. 1984. ?Another look at
nominal compounds.? In Proceedings of the
10th International Conference on
Computational Linguistics and 22nd Annual
Meeting of the Association for Computational
Linguistics, pages 509?516, Stanford,
California.
Jones, Bernard. 1995. ?Predicating nominal
compounds.? In Proceedings of the 17th
Annual Conference of the Cognitive Science
Society, pages 130?135, Pittsburgh,
Pennsylvania.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing 33(3):400?401.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago.
Lascarides, Alex and Alex Copestake. 1998.
Pragmatics and word meaning. Journal of
Linguistics 34(2):387?414.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on
Compound Nouns. Ph.D. dissertation,
Macquarie University, Sydney, Australia.
Lee, Lilian. 1999. ?Measures of
distributional similarity.? In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 25?32,
College Park, Maryland.
Lee, Lilian and Fernando Pereira. 1999.
?Distributional similarity models:
Clustering vs. nearest neighbors.? In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics,
pages 33?40, College Park, Maryland.
Leonard, Rosemary. 1984. The Interpretation
of English Noun Sequences on the Computer.
North-Holland, Amsterdam.
Levi, Judith N. 1978. The Syntax and
Semantics of Complex Nominals. Academic
Press, New York.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics
24(2):217?244.
Macleod, Catherine, Ralph Grishman,
Adam Meyers, Leslie Barrett, and Ruth
Reeves. 1998. ?Nomlex: A lexicon of
nominalizations.? In Proceedings of the
Eighth International Congress of the European
Association for Lexicography, pages 187?193,
Lie`ge, Belgium.
Marsh, Elaine. 1984. ?A computational
analysis of complex noun phrases in
Navy messages.? In Proceedings of the 10th
International Conference on Computational
Linguistics, pages 505?508, Stanford,
California.
McDonald, David. 1982. Understanding
Noun Compounds. Ph.D. dissertation,
Carnegie Mellon University, Pittsburgh,
Pennsylvania.
Merlo, Paola and Suzanne Stevenson. 1999.
?Automatic verb classification using
distributions of grammatical features.? In
Proceedings of the Ninth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 45?51,
Bergen, Norway.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine J. Miller. 1990. Introduction to
WordNet: An on-line lexical database.
International Journal of Lexicography
3(4):235?244.
Mosteller, Frederick and David L. Wallace.
1964. Inference and Disputed Authorship: The
Federalist. Addison-Wesley, London.
Passonneau, Rebecca J. and Diane J. Litman.
1997. Discourse segmentation by human
and automated means. Computational
Linguistics 23(1):103?140.
Pedersen, Ted. 2001. ?A decision tree of
bigrams is an accurate predictor of word
sense.? In Proceedings of the Second North
American Annual Meeting of the Association
388
Computational Linguistics Volume 28, Number 3
for Computational Linguistics, pages 79?86,
Pittsburgh, Pennsylvania.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. ?Distributional
clustering of English words.? In
Proceedings of the 31st Annual Meeting of the
Association for Computational Linguistics,
pages 183?190, Columbus, Ohio.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, London.
Resnik, Philip Stuart. 1993. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. dissertation,
University of Pennsylvania, Philadelphia.
Selkirk, Elizabeth. 1982. The Syntax of Words.
MIT Press, Cambridge, Massachusetts.
Siegel, Eric V. 1999. ?Corpus-based linguistic
indicators for aspectual classification.? In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics,
pages 112?119, College Park, Maryland.
Siegel, Eric V. and Kathleen R. McKeown.
1994. ?Emergent linguistic rules from
inducing decision trees: Disambiguating
discourse clue words.? In Proceedings of the
12th National Conference on Artificial
Intelligence, pages 820?826, Seattle,
Washington. AAAI Press, Menlo Park,
California.
Siegel, Sidney and N. John Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill, New York.
Sparck Jones, Karen. 1983. ?Compound
noun interpretation problems.? Technical
Report 45, Computer Laboratory,
Cambridge University, Cambridge,
England.
Vanderwende, Lucy. 1994. ?Algorithm for
automatic interpretation of noun
sequences.? In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 782?788, Kyoto, Japan.
Warren, Beatrice. 1978. Semantic Patterns of
Noun-Noun Compounds. Acta Universitatis
Gothoburgensis, Go?teborg, Sweden.
Wu, Dekai. 1993. ?Approximating
maximum-entropy ratings for evidential
parsing and semantic interpretation.? In
Proceedings of 13th International Joint
Conference on Artificial Intelligence,
pages 1290?1296, Chamberry, France.
Morgan Kaufmann.
Automatic Evaluation of Information
Ordering: Kendall?s Tau
Mirella Lapata?
University of Edinburgh
This article considers the automatic evaluation of information ordering, a task underlying many
text-based applications such as concept-to-text generation and multidocument summarization.
We propose an evaluation method based on Kendall?s ?, a metric of rank correlation. The method
is inexpensive, robust, and representation independent. We show that Kendall?s ? correlates
reliably with human ratings and reading times.
1. Introduction
The systematic evaluation of natural language processing (NLP) systems is an impor-
tant prerequisite for assessing their quality and improving their performance. Tradition-
ally, human involvement is called for in evaluating systems that generate textual output.
Examples include text generation, summarization, and, notably, machine translation.
Human evaluations consider many aspects of automatically generated texts ranging
from grammaticality to content selection, fluency, and readability (Teufel and van
Halteren 2004; Nenkova 2005; Mani 2001; White and O?Connell 1994).
The relatively high cost of producing human judgments, especially when evalua-
tions must be performed quickly and frequently, has encouraged many researchers to
seek ways of evaluating system output automatically. Papineni et al (2002) proposed
BLEU, a method for evaluating candidate translations by comparing them against ref-
erence translations (using n-gram co-occurrence overlap). Along the same lines, the
content of a system summary can be assessed by measuring its similarity to one or
more manual summaries (Hovy and Lin 2003). Bangalore, Rambow, and Whittaker
(2000) introduce a variety of quantitative measures for evaluating the accuracy of an
automatically generated sentence against a reference corpus string.
Despite differences in application and form, automatic evaluation methods usually
involve the following desiderata. First, they measure numeric similarity or closeness
of system output to one or several gold standards. Second, they are inexpensive,
robust, and ideally language independent. Third, correlation with human judgments
is an important part of creating and testing an automated metric. For instance,
several studies have shown that BLEU correlates with human ratings on machine
translation quality (Papineni et al 2002; Doddington 2002; Coughlin 2003). Bangalore,
Rambow, and Whittaker (2000) demonstrate that tree-based evaluation metrics for
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
E-mail: mlap@inf.ed.ac.uk
Submission received: 28 December 2005; accepted for publication: 6 May 2006.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
surface generation correlate significantly with human judgments on sentence quality
and understandability. Given their simplicity, automatic evaluation methods cannot be
considered as a direct replacement for human evaluations (see Callison-Burch, Osborne,
and Koehn [2006] for discussion on some problematic aspects of BLEU). However, they
can be usefully employed during system development, for example, for quickly
assessing modeling ideas or for comparing across different system configurations
(Papineni et al 2002; Bangalore, Rambow, and Whittaker 2000).
Automatic methods have concentrated on evaluation aspects concerning lexical
choice (e.g., words or phrases shared between reference and system translations), con-
tent selection (e.g., document units shared between reference and system summaries),
and grammaticality (e.g., how many insertions, substitutions, or deletions are required
to transform a generated sentence to a reference string). Another promising, but,
less studied, avenue for automatic evaluation is information ordering. The task con-
cerns finding an acceptable ordering for a set of preselected information-bearing items
(Lapata 2003; Barzilay and Lee 2004). It is an essential step in concept-to-text genera-
tion, multidocument summarization, and other text synthesis problems. Depending on
the application and domain at hand, the items to be ordered may vary greatly from
propositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees
(Mellish et al 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). It is therefore
not surprising that evaluation methods have concentrated primarily on the generated
orders, thus abstracting away from the items themselves.
More concretely, Lapata (2003) proposed the use of Kendall?s ?, a measure of rank
correlation, as a means of estimating the distance between a system-generated and a
human-generated gold-standard order. Rank correlation is an appealing way of eval-
uating information ordering: It is a well-understood and widely used measure of the
strength of association between two variables; it is computed straightforwardly and
can operate over distinct linguistic units (e.g., sentences, trees, or propositions). Indeed,
several studies have adopted Kendall?s ? as a performance measure for evaluating
the output of information-ordering components both in the context of concept-to-text
generation (Karamanis and Mellish 2005; Karamanis 2003) and summarization (Lapata
2003; Barzilay and Lee 2004; Okazaki, Matsuo, and Ishizuka 2004).
Despite its growing popularity, no study to date has investigated whether
Kendall?s ? correlates with human judgments on the information-ordering task. This
is in marked contrast with other automatic evaluation methods that have been shown
to correlate with human assessments. In this article, we aim to rectify this and un-
dertake two studies that examine whether there is indeed a relationship between ?
and behavioral data. We first briefly introduce Kendall?s ? and explain how it can be
employed for evaluating information ordering (Section 2). Next, we present a controlled
experimental study that examines whether Kendall?s ? is correlated with human ratings
(Section 3).
A commonly raised criticism of the judgment elicitation methodology is that it is
not fine-grained enough to rule out possible confounds. In the information-ordering
task, for example, we cannot be certain that subjects rate a document low because it is
genuinely badly organized and, therefore, difficult to comprehend or because they are
unfamiliar with its content or simply disinterested or distracted. Similar confounds also
arise in the evaluation of the output of MT systems, where it may be difficult to tease
apart whether subjects? ratings reflect their assessment of the quality of the translated
text or its subject matter and structure. To eliminate such confounds, we follow our
judgment elicitation study with an on-line reading experiment and demonstrate that ?
is also correlated with processing time (Section 4). Our second experiment provides
472
Lapata Automatic Evaluation of Information Ordering
additional evidence for the validity of ? as a measure of text well-formedness. Discus-
sion of our results concludes the article.
2. Kendall?s Measure
In common with other automatic evaluation methods, we assume that we have access
to a reference output that in most cases will be created by one or several humans. Our
task is to compare a system-produced ordering of items against a reference order. For
ease of exposition, let us assume that our information-ordering component is part of a
generation application whose ultimate goal is to generate coherent and understandable
text. It is not crucially important how the items to be ordered are represented. They can
be facts in a database (Duboue and McKeown 2001), propositions (Karamanis 2003),
discourse trees (Mellish et al 1998), or sentences (Lapata 2003; Barzilay and Lee 2004).
Now, we can think of the items as objects for which a ranking must be produced.
Table 1 gives an example of a reference text containing 10 items (A?J) and the orders
(i.e., rankings) produced by two hypothetical systems. We can then calculate how much
the system orders differ from the reference order, the underlying assumption being that
acceptable orders should be fairly similar to the reference. A number of metrics can be
employed for this purpose, such as Spearman?s correlation coefficient (rs) for ranked
data, Cayley distance, or Kendall?s ? (see Lebanon and Lafferty [2002] for an overview).
Here we describe Kendall?s ? (Kendall 1938) and explain why it is an appropriate choice
for information-ordering tasks.
Let Y = y1 . . . yn be a set of items to be ranked. Let ? and ? denote two distinct
orderings of Y, and S(?,?) the minimum number of adjacent transpositions needed to
bring ? to ?. Kendall?s ? is defined as:
? = 1 ? 2S(?,?)
N(N ? 1)/2 (1)
where N is the number of objects (i.e., items) being ranked. As can be seen, Kendall?s
? is based on the number transpositions, that is, interchanges of consecutive elements,
necessary to rearrange ? into ?. In Table 1 the number of transpositions can be calculated
by counting the number of intersections of the lines. The ? between the Reference and
System 1 is 0.82, between the Reference and System 2 is 0.24, and between the two
systems is 0.15. The metric ranges from ?1 (inverse ranks) to 1 (identical ranks). The
calculation of ? must be appropriately modified when there are tied rankings (Hays
1994; Siegel and Castellan 1988).
Kendall?s ? seems particularly appropriate for the information-ordering tasks con-
sidered in this article. The metric is sensitive to the fact that some items may be always
Table 1
Example of reference order and system orders for a text consisting of 10 items.
A B C D E F G H I J
Reference 1 2 3 4 5 6 7 8 9 10
System 1 2 1 5 3 4 6 7 9 8 10
System 2 10 2 3 4 5 6 7 8 9 1
473
Computational Linguistics Volume 32, Number 4
ordered next to each other even though their absolute orders might differ. It also penal-
izes inverse rankings. Comparison between the Reference and System 2 gives a ? of 0.24
even though the orders between the two models are identical modulo the beginning
and the end. This seems appropriate given that flipping the introduction in a document
with the conclusions seriously disrupts coherence.
Kendall?s ? is less widely used than Spearman?s rank correlation coefficient (rs).
Both coefficients use the same amount of information in the data, and thus both have
the same sensitivity to detect the existence of association. This means that for a given
data set, both measures will lead to rejection of the null hypothesis at the same level of
significance. However, the two measures have different underlying scales, and, numeri-
cally, they are not directly comparable to each other. Siegel and Castellan (1988) express
the relationship of the two measures in terms of the inequality:
?1 ? 3?? 2rs ? 1 (2)
More importantly, Kendall?s ? and rs have different interpretations. Kendall?s ?
can be interpreted as a simple function of the probability of observing concordant
and discordant pairs (Kerridge 1975). In other words, it is the difference between the
probability that in the observed data two variables are in the same order versus the
probability that they are in different orders (the probability is rescaled to range from ?1
to 1 as is customary for correlation; see equation (1)). Unfortunately, no simple meaning
can be attributed to rs. The latter is the same as a Pearson product?moment correlation
coefficient (rp) computed for values consisting of ranks. Although r2 represents the
percent of variance shared by two variables in the case of rp, its interpretation is less
straightforward for rs, where it refers to the percent of variance of two ranks. It is
difficult to draw any meaningful conclusions with regard to information ordering based
on the variance of ranks.
In practice, while both correlations frequently provide similar answers, there are
situations where they diverge. For example, the statistical distribution of ? approaches
the normal distribution faster than rs (Kendall and Gibbons 1990), thus offering an
advantage for small to moderate sample studies with 30 or fewer data points. This is
crucial when experiments are conducted with a small number of subjects (a situation
common in NLP) or test items. Another related issue concerns sample size. Spearman?s
rank correlation coefficient is a biased statistic (Kendall and Gibbons 1990). The smaller
the sample the more rs diverges from the true population value, usually underestimat-
ing it. In contrast, Kendall?s ? does not provide a biased estimate of the true correlation.
Furthermore, ? maintains good control of type I error rates (i.e., rejecting the null
hypothesis when it is actually true). Arndt, Turvey, and Andreasen (1999) undertake
an extensive empirical study and show that the number of times ? incorrectly signals
a significant correlation when there is none is close to the nominal 5% using a p < 0.05
significance criterion. For a more detailed discussion of the advantages of ? over rs,
we refer the interested reader to Kendall and Gibbons (1990) and Arndt, Turvey, and
Andreasen (1999).
3. Experiment 1: Judgment Elicitation
To assess whether Kendall?s ? reliably correlates with human ratings, it is necessary
to have access to several different orderings of the same input. In what follows we
474
Lapata Automatic Evaluation of Information Ordering
describe our method for assembling a set of experimental materials and collecting
human judgments.
3.1 Method
3.1.1 Design and Materials. Our goal here is to establish whether ? correlates with
human judgments on overall text understandability and coherence. A system that
randomly pastes together sentences or facts from a database will ultimately produce
a badly organized document lacking coherence. A good automatic evaluation method
should assign low values to such documents and higher values to documents that are
easy for humans to read and understand.
We could elicit judgments by asking humans to rate the output of an information-
ordering component. The ratings could be then correlated with ? values representing
the difference between system and reference orders. Such a comparison is, however,
undesirable for a number of reasons. First, the system may be biased toward very
good or very bad orders. This means that our hypothetical study would only exam-
ine a restricted and potentially skewed range of ? values. Furthermore, in concept-
to-text generation applications, information ordering typically operates over symbolic
representations that will be unfamiliar to naive informants and could potentially distort
their judgments. A related issue arises in text-to-text generation applications where the
produced documents are not necessarily grammatical, for example, when a summary is
the output of an information fusion component (Barzilay 2003; Radev and McKeown
1998). Again, it is difficult to control whether informants judge the ordering or the
grammaticality of the texts.
To make the judgment task easier, we concentrated on a document representa-
tion familiar to our participants, namely, sentences. We simulated the output of an
information-ordering component by randomly generating different sentence orders
for a reference text. We elicited judgments for eight texts of the same length (eight
sentences). The texts were randomly sampled from a corpus collected by Barzilay
and Lee (2004) (sampling took place over eight-sentence-length documents only). The
corpus consists of Associated Press articles on the topic of natural disasters, drug-related
criminal offenses, clashes between armies and rebel groups, and narratives from the U.S.
National Transportation Safety Board database.1
A document consisting of eight sentences can be sequenced in 8! ways. We exhaus-
tively enumerated all possible orderings and calculated their ? value against the refer-
ence order found in the corpus.2 Figure 1 shows how many different orders correspond
to a given ? value. For example, there is only one order with a ? of 1 or ?1, whereas
there are 3,736 orders with ? 0.07 or ?0.07.
Ideally, we should elicit judgments on orders corresponding to all 29 values from
Figure 1. Unfortunately, this would render our experimental design unwieldy. As-
suming we randomly select one order for each value, our participants would have to
judge 29 ? 8 = 232 texts. In order to strike a balance between a manageable design
and a wide range of ? values, we split the ? range into eight bins (see Figure 2). For
each text, an order was randomly sampled from each bin. Thus, our set of materials
consisted of 8 ? 8 = 64 texts. Pronouns that could not be resolved intra-sententially
were substituted by their referents to avoid creating coherence violation artifacts. For
1 The corpus is available from http://people.csail.mit.edu/regina/struct/.
2 Notice that the number of permutations and range of ? values is the same for all our texts, since they all
have the same length.
475
Computational Linguistics Volume 32, Number 4
Figure 1
Range of ? values for a document consisting of eight sentences.
the same reason, we excluded from our materials texts containing discourse connectives
(e.g., but, therefore).
3.1.2 Procedure. During the elicitation study, participants were presented with texts
and asked to judge how comprehensible they were on a seven-point scale. They were
told that some texts would be perfectly understandable, whereas others would be fairly
incoherent and the order of the sentences might seem scrambled.
Figure 2
Range of ? values when collapsed across eight bins.
476
Lapata Automatic Evaluation of Information Ordering
The study was conducted remotely over the Internet. Participants first saw a set
of instructions that explained the task and provided several examples of well- and
badly organized texts, together with examples of numerical estimates. From our set
of materials we generated 8 lists (each consisting of 8 texts) following a Latin square
design. Each subject was randomly assigned to one list. The procedure ensured that no
two texts in a given list corresponded to the same reference text. It was emphasized that
there were no correct answers and that subjects should base their judgments on first
impressions, not spending too much time on any one text. Example stimuli are shown
in Table 2.
The subjects accessed the experiment using their Web browser. Experimental in-
structions and materials were administered via CGI scripts. A number of safeguards
were put in place to ensure the authenticity of the subjects taking part. Participants
had to provide their e-mail address and were asked to fill in a short questionnaire
including basic demographic information (name, age, sex, handedness, and language
background). Subjects? e-mail addresses were automatically checked for plausibility
and subjects with fake addresses were removed. The elicited responses were also
screened to identify (and eliminate) subjects taking part in the experiment more
than once.
3.1.3 Subjects. The experiment was completed by 189 unpaid volunteers, all self-
reported native speakers of English. Subjects were recruited by postings to local e-mail
lists; they had to be linguistically naive, neither linguists nor students of linguistics
were allowed to participate. Four subjects were eliminated because they were non-
native English speakers. The data of six subjects were excluded after inspection of their
responses revealed anomalies in their ratings. For example, they either provided ratings
outside the prespecified scale (1?7) or rated all documents uniformly. This left 179
subjects for analysis (approximately 22 per text). Forty-nine of our participants were
Table 2
Example stimuli representing a well- (top) and badly- (bottom) organized document.
Police arrested 18 people Saturday in an alleged international ring that smuggled hashish in
from Morocco for distribution in Europe. The group allegedly smuggled the hashish to Cadiz,
on Spain?s southern coast, and then used trains to transport it to Barcelona and Italy. The group,
based in Seville with ties in Las Palmas, Barcelona, Morocco and Italy, was headed by the Rufos
family, police said. Police seized 100 kilograms (220 pounds) of hashish, 10 million pesetas (dlrs
80,000), nine vehicles, rifles, computers, mobile phones, video cameras and false identification
papers. Arrests were made in Seville, Las Palmas and Barcelona. Police did not provide names of
suspects, or nationalities of those arrested. Southern Spain is a main gateway for hashish being
smuggled into Europe from northern Africa. Hundreds of kilograms (pounds) are seized each
week.
The group allegedly smuggled the hashish to Cadiz, on Spain?s southern coast, and then used
trains to transport it to Barcelona and Italy. Hundreds of kilograms (pounds) are seized each week.
Southern Spain is a main gateway for hashish being smuggled into Europe from northern Africa.
Arrests were made in Seville, Las Palmas and Barcelona. Police did not provide names of suspects,
or nationalities of those arrested. Police seized 100 kilograms (220 pounds) of hashish, 10 million
pesetas (dlrs 80,000), nine vehicles, rifles, computers, mobile phones, video cameras and false
identification papers. The group, based in Seville with ties in Las Palmas, Barcelona, Morocco
and Italy, was headed by the Rufos family, police said. Police arrested 18 people Saturday in an
alleged international ring that smuggled hashish in from Morocco for distribution in Europe.
477
Computational Linguistics Volume 32, Number 4
female and 42 male. The age of the subjects ranged from 18 to 60 years. The mean was
28.5 years.
3.2 Results
The judgments were averaged to provide a single rating per text. We first analyzed the
correspondence of human ratings and ? values by performing an analysis of variance
(ANOVA). Recall that ? represents the degree of similarity between a synthetically gen-
erated text and a reference text. In our case, the reference texts are the original human-
authored documents from our corpus. Our participants judge how well a document is
organized without having access to the original reference.
Our ANOVA analysis had one factor (i.e., ? value) with eight levels corresponding
to the eight bins discussed in Section 3.1 (see Figure 2). The ANOVA showed that this
factor was significant in both by-subjects and by-items analyses: F1(7, 1239) = 42.60, p <
0.01; F2(7, 56) = 2.77, p < 0.01. Table 3 shows the average subject ratings and descriptive
statistics for each of the eight bins. Post hoc Tukey tests indicated that the ratings for
texts with ? values from Bin 1 were significantly different from the ratings assigned to
all other bins (? = 0.01). Although ratings for Bins 2, 3, 4, and 5 did not significantly
differ from each other, they all differed from Bins 6, 7, and 8. The results of the ANOVA
show that our participants tended to give high scores to texts with high ? values and
low scores to texts with low ? values.
We next used correlation analysis to explore the linear relationship between sub-
jects? ratings and Kendall?s ?. The comparison yielded a Pearson correlation coefficient
of r = 0.45 (p < 0.01, N = 64). Figure 3 plots the relationship between judgments and
? values. To get a better understanding of how this automatic evaluation method
compares with human judgments, we examined how well our raters agreed in their
assessment. To calculate intersubject agreement we used leave-one-out resampling. The
technique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and
has been previously used for measuring how well humans agree on judging seman-
tic similarity (Resnik and Diab 2000; Resnik 1999), adjective plausibility (Lapata and
Lascarides 2003), and text coherence (Barzilay and Lapata 2005).
The set of m subjects? responses was divided into two sets: a set of size m ? 1
(i.e., the response data of all but one subject) and a set of size one (i.e., the response
data of a single subject). We then correlated the mean ratings of the former set with the
ratings of the latter. This was repeated m times. Since we had 179 subjects, we performed
Table 3
Average subject ratings for binned ? values and descriptive statistics.
Bins Mean Min Max SD
1 5.348 3.000 7.000 1.236
2 4.916 1.000 7.000 1.612
3 4.927 2.000 7.000 1.580
4 4.470 1.000 7.000 1.489
5 4.382 1.000 7.000 1.559
6 4.208 1.000 7.000 1.600
7 4.028 1.000 7.000 1.702
8 3.966 1.000 7.000 1.558
478
Lapata Automatic Evaluation of Information Ordering
Figure 3
Correlation of elicited judgments and ? values.
178 correlation analyses and report their mean.3 The average intersubject agreement
was r = 0.56 (min = 0.001, max = 0.94, SD = 0.25), thus indicating that ??s agreement
with the human data is not far from the average human agreement.
4. Experiment 2: Kendall?s Tau and Processing Effort
A potential criticism of our previous study is that it is based solely on ratings. The
problem with this off-line measure is that it indicates whether participants find a text
easy or difficult to comprehend, without, however, isolating the causes for this difficulty.
For example, the ratings may reflect not only what subjects think about how a text is
organized but also their (un)familiarity with its genre or style, their lack of attention, or
disinterest in the subject matter. To ascertain that this is not the case, we conducted a
follow-up experiment whose aim was to explore the relationship between Kendall?s ?
and processing effort. Much work in psychology (McKoon and Ratcliff 1992; Britton
1991) indicates that low-coherence texts require more inferences and therefore take
longer to read. If Kendall?s ? does indeed capture aspects of overall document organi-
zation and coherence, then documents assigned a high ? value should take less time to
read than documents with low ? values. Unlike ratings, reading times are an immediate
measure of processing effort that participants cannot consciously control or modulate.
4.1 Method
4.1.1 Design and Materials. The experiment was designed to assess the relation of
Kendall?s ? with processing effort. Our selection of materials was informed by the
ANOVA results presented in Experiment 1. We used the same eight reference texts
from the previous experiment. For each text we randomly selected three synthetically
generated orders, each from Bin 1 (high ? value), Bins 2?4 (medium ? value), and Bins 5?
8 (low ? value). In other words, we collapsed Bins 2?4 and Bins 5?8, since the ANOVA
3 We cannot apply the commonly used kappa statistic for measuring intersubject agreement since it is
appropriate for nominal scales, whereas our texts are rated on an ordinal scale.
479
Computational Linguistics Volume 32, Number 4
Table 4
Mean reading times (in milliseconds) for three experimental conditions.
Mean Min Max SD
High 5762.6 1963.3 9111.1 1429.0
Medium 6499.0 1574.0 10344.5 2017.5
Low 7250.4 1428.0 15000.0 3121.3
revealed that ratings for these bins were not significantly different. Our set of materials
consisted of 8 ? 3 = 24 texts.
4.1.2 Procedure. The presentation of stimuli and collection of responses was controlled
by E-Prime software4 (version 1.1) running on a Dell Optiplex GX270 with an Intel
Pentium 4 processor and 512 MB memory. The experiment started with a practice ses-
sion comprising two texts, each eight sentences long. Then eight texts were presented;
the presentation followed a Latin square design, thus ensuring that no subject saw the
same text twice.
The texts were presented one sentence at a time. The participant pressed the space
bar to proceed from one sentence to the next. Participants were instructed to read
the texts at their own pace and to press the space bar after each sentence once they
were certain that they understood it. Participants? reading time was recorded for each
sentence. After the final sentence was displayed, subjects were asked a comprehension
yes/no question to make sure that they were actually reading the texts rather than
pressing the space bar randomly.
4.1.3 Subjects. The experiment was completed by 32 volunteers, all self-reported native
speakers of English. The experiment was administered in the laboratory and subjects
were paid ?5 for their participation. None of the subjects had previously participated in
Experiment 1.
4.2 Results
Sentence reading times were averaged to provide reading times for each text. As a first
step, the reading time data were screened to remove errors and outliers. Errors consisted
of items where the subjects had incorrectly answered the comprehension question. This
affected 12.3% of the data. Reading times beyond 2.5 standard deviations above or
below the mean for a particular participant were replaced with the mean plus this cut-
off value. This adjustment of outliers affected 9.7% of the data. Mean reading times for
each experimental condition (high, medium, low) are shown in Table 4.
ANOVA showed significant differences in reading times [F1(2, 62) = 6.39, p < 0.01;
F2(2, 14) = 4.23, p < 0.05]. Post hoc Tukey tests revealed that high-? texts were read sig-
nificantly faster than medium- and low-? texts (? = 0.01). Reading times for medium-?
texts were not significantly different from low-? texts.
4 E-prime is a suite of tools for creating and running experiments while allowing for millisecond precision
data collection. For more information see http://www.pstnet.com/products/e-prime/.
480
Lapata Automatic Evaluation of Information Ordering
We next examine through correlation analysis whether there is a linear relationship
between reading times and ? values. We regressed ? values and reading times following
the procedure5 recommended in Lorch and Myers (1990). The regression yielded a
Pearson correlation coefficient of r = ?0.48 (p < 0.01). Expectedly, reading times are
also significantly correlated with human ratings: Pearson?s r = ?0.47 (p < 0.01).6
To summarize, the results of our second experiment provide additional evidence
for the use of Kendall?s ? as a measure of text well-formedness. It correlates not only
with human ratings but also with reading times. The latter constitute much more
fine-grained behavioral data, directly associated with processing effort: Less well-
structured documents tend to have low ? values and cause longer reading times,
whereas documents with high ? values tend to be better organized and cause shorter
reading times.
5. Discussion
In this article, we argue that Kendall?s ? can be used as an automatic evaluation
method for information-ordering tasks. We have undertaken a judgment elicitation
study demonstrating that ? correlates reliably with human judgments. We have also
shown that ? correlates with processing effort?texts with high ? values take less time
to read than texts with low ? values. We have presented behavioral evidence collected
via two distinct experimental paradigms suggesting that Kendall?s ? is an ecologically
valid measure of document well-formedness and structure.
An attractive feature of the ? evaluation method is that it is representation inde-
pendent. It can therefore be used to evaluate both symbolic and statistical generation
systems. We do not view ? as an alternative to human evaluations; rather we consider its
role complementary. It can be used during system development for tracking incremental
progress or as an easy way of assessing whether an idea is promising. It can also be
used to compare systems that employ comparable information-ordering strategies and
operate over the same input. Furthermore, statistical generation systems (Lapata 2003;
Barzilay and Lee 2004; Karamanis and Manurung 2002; Mellish et al 1998) could use ?
as a means of directly optimizing information ordering, much in the same way MT
systems optimize model parameters using BLEU as a measure of translation quality
(Och 2003).
The ? evaluations presented in this article used a single reference text. Previous
work (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Karamanis and Mellish
2005) has shown that there may be many acceptable orders for a set of information-
bearing items, although topically related sentences seem to appear together (Barzilay,
Elhadad, and McKeown 2002). A straightforward way to incorporate multiple refer-
ences in the evaluation paradigm discussed here is to compute the ? statistic N times
for every reference?system output pair and report the mean. A more interesting future
direction is to weight transpositions (see Section 2) according to agreements or disagree-
ments in the set of multiple references. A possible implementation of this idea would
5 Lorch and Myers (1990) argue that it is not appropriate to average over subjects when dealing with
repeated measures designs. Instead they propose three methods that effect regression analysis on
reading times collected from individual subjects. We refer the interested reader to Lorch and Myers
(1990) and Baayen (2004) for further discussion.
6 The correlation coefficients are negative since longer reading times correspond to lower ratings and
? values.
481
Computational Linguistics Volume 32, Number 4
be to compute ? against one (randomly selected) reference, but change the metric so
as to give fractional counts (i.e., less than one) to transpositions that are not uniformly
attested in the reference set.
Naturally, Kendall?s ? is not the only automatic evaluation method that can be
employed to assess information ordering. Barzilay and Lee (2004) and Barzilay and
Lapata (2005) measure accuracy as the percentage of test items for which the system
gives preference to the gold-standard reference order. This measure allows us to com-
pare the output of different systems; however, it only rewards orders identical to the
gold standard, and considers all other orders deviating from it deficient. Barzilay and
Lee (2004) propose an additional evaluation measure based on ranks. Assuming that a
system can exhaustively generate all possible orders for a set of items (with a certain
probability), they report the rank given to the reference order when all possible orders
are sorted by their probability. The best possible rank is 0 and the worst rank is N! ? 1.
A system that gives a high rank to the reference order is considered worse than a
system that gives it a low rank. However, not all systems are designed to exhaustively
enumerate all possible permutations for a given document or have indeed a scoring
mechanism that can rank alternative document renderings. Duboue and McKeown
(2002) employ an alignment algorithm that allows them to compare the output of their
algorithm with a gold-standard order. The alignment algorithm works by considering
the similarity between system-generated and gold-standard facts. The similarity func-
tion is domain dependent (Duboue and McKeown [2002] generate postcardiac surgery
medical briefings) and would presumably have to be redefined for a different set of facts
in another domain.
Kendall?s ? can be easily used to evaluate the output of automatic systems, irre-
spectively of the domain or application at hand. It requires no additional tuning and
correlates reliably with behavioral data. Since it is a similarity measure, it can be used to
evaluate system output that is not necessarily identical to the gold standard. Also note
that ? could be used to compare across systems operating over similar input/output
even if reference texts are not available. For example, ? could identify outlier systems
with output radically different from the mean.
Acknowledgments
The author acknowledges the support of
EPSRC (grant GR/T04540/01). Thanks to
Frank Keller, Nikiforos Karamanis, Scott
McDonald, and two anonymous reviewers
for helpful comments and suggestions.
References
Arndt, Stephan, Carolyn Turvey, and
Nancy C. Andreasen. 1999. Correlating
and predicting psychiatric symptom
ratings: Spearman?s r versus Kendall?s tau
correlation. Journal of Psychiatric Research,
33:97?104.
Baayen, Harald R. 2004. Statistics in
psycholinguistics: A critique of some
current gold standards. In Mental Lexicon
Working Papers 1. University of Alberta,
Edmonton, pages 1?45.
Bangalore, Srinivas, Owen Rambow, and
Steven Whittaker. 2000. Evaluation metrics
for generation. In Proceedings of the INLG,
pages 1?8, Mitzpe Ramon, Israel.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization:
Paraphrasing and Generation. Ph.D. thesis,
Columbia University.
Barzilay, Regina, Noemie Elhadad, and
Kathleen R. McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata. 2005.
Modeling local coherence: An entity-based
approach. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 141?148, Ann Arbor.
Barzilay, Regina and Lillian Lee. 2004.
Catching the drift: Probabilistic content
models, with applications to generation
and summarization. In Proceedings of the
2nd Human Language Technology Conference
482
Lapata Automatic Evaluation of Information Ordering
and Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 113?120, Boston, MA.
Britton, Bruce K. 1991. Using Kintsch?s
computational model to improve
instructional text: Effects of repairing
inference calls on recall and cognitive
structures. Journal of Educational Psychology,
83(3):329?345.
Callison-Burch, Chris, Miles Osborne, and
Philipp Koehn. 2006. Re-evaluating the
role of BLEU in machine translation
research. In Proceedings of the 11th
Conference of the European Chapter of the
Association of Computational Linguistics,
pages 249?256, Trento, Italy.
Coughlin, Deborah. 2003. Correlating
automated and human assessments of
machine translation quality. In Proceedings
of MT Summit IX, pages 63?70,
New Orleans.
Dimitromanolaki, Aggeliki and Ion
Androutsopoulos. 2003. Learning to order
facts for discourse planning in natural
language generation. In Proceedings of the
9th European Workshop on Natural Language
Generation, pages 113?120, Budapest,
Hungary.
Doddington, George. 2002. Automatic
evaluation of machine translation quality
using n-gram cooccurrence statistics. In
Human Language Technology: Notebook
Proceedings, pages 128?132, San Diego.
Duboue, Pablo and Kathleen R. McKeown.
2002. Content planner construction via
evolutionary algorithms and a corpus-
based fitness function. In Proceedings of
INLG 2002, pages 89?96, New York.
Duboue, Pablo A. and Kathleen R.
McKeown. 2001. Empirically estimating
order constraints for content planning in
generation. In Proceedings of the 39th
Annual Meeting of the Association for
Computational Linguistics, pages 172?179,
Toulouse, France.
Hays, William L. 1994. Statistics. Harcourt
Brace College Publishers, New York,
3rd edition.
Hovy, Eduard and Chin-Yew Lin. 2003.
Automatic evaluation of summaries using
N-gram co-occurrence statistics. In
Proceedings of the 1st Human Language
Technology Conference and Annual Meeting of
the North American Chapter of the Association
for Computational Linguistics, pages 71?78,
Edmonton, Canada.
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, University of Edinburgh.
Karamanis, Nikiforos and Hisar Maruli
Manurung. 2002. Stochastic text
structuring using the principle of
continuity. In Proceedings of the 2nd
International Conference on Natural Language
Generation, pages 81?88, New York.
Karamanis, Nikiforos and Chris Mellish.
2005. Using a corpus of sentence orderings
defined by many experts to evaluate
metrics of coherence for text structuring.
In Proceedings of the 10th European
Workshop on Natural Language Generation,
pages 174?179, Aberdeen, Scotland.
Kendall, Maurice G. 1938. A new measure of
rank correlation. Biometrika, 30:81?93.
Kendall, Maurice G. and Jean Dickinson
Gibbons. 1990. Rank Correlation Methods.
Oxford University Press, New York.
Kerridge, D. 1975. The interpretation of
rank correlations. Applied Statistics,
24(2):257?258.
Lapata, Maria and Alex Lascarides. 2003. A
probabilistic account of logical metonymy.
Computational Linguistics, 29(2):263?317.
Lapata, Mirella. 2003. Probabilistic text
structuring: Experiments with sentence
ordering. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics, pages 545?552, Sapporo, Japan.
Lebanon, Guy and John Lafferty. 2002.
Combining rankings using conditional
probability models on permutations. In
Proceedings of the 19th International
Conference on Machine Learning. San
Francisco, CA: Morgan Kaufmann
Publishers, pages 363?370.
Lorch, Robert F. and Jerome L. Myers. 1990.
Regression analyses of repeated measures
data in cognitive research. Journal of
Experimental Psychology: Learning, Memory,
and Cognition, 16(1):149?157.
Mani, Inderjeet. 2001. Automatic
Summarization. John Benjamins Pub Co.,
Amsterdam; Philadelphia.
McKoon, Gail and Roger Ratcliff. 1992.
Inference during reading. Psychological
Review, 99(3):440?446.
Mellish, Chris, Alistair Knott, Jon
Oberlander, and Mick O? Donnell. 1998.
Experiments using stochastic search for
text planning. In Proceedings of the 9th
International Workshop on Natural Language
Generation, pages 98?107, Ontario, Canada.
Nenkova, Ani. 2005. Automatic text
summarization of newswire: Lessons
learned from the document understanding
conference. In Proceedings of the 20th
National Conference on Artificial Intelligence,
pages 1436?1441, Pittsburgh, PA.
483
Computational Linguistics Volume 32, Number 4
Och, Franz Joseph. 2003. Minimum error
rate training in statistical machine
translation. In Proceedings of the 41st
Annual Meeting of the Association for
Computational Linguistics, pages 160?167,
Sapporo, Japan.
Okazaki, Naoaki, Yutaka Matsuo, and
Mitsuru Ishizuka. 2004. Improving
chronological sentence ordering by
precedence relation. In Proceedings
of the 20th International Conference on
Computational Linguistics, Geneva,
Switzerland, pages 750?756.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Radev, Dragomir and Kathleen R. McKeown.
1998. Generating natural language
summaries from multiple on-line sources.
Computational Linguistics, 24(3):469?500.
Resnik, Philip. 1999. Semantic similarity
in a taxonomy: An information-based
measure and its application to problems
of ambiguity in natural language.
Journal of Artificial Intelligence Research,
pages 95?130.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In Proceedings
of the 22nd Annual Conference of the
Cognitive Science Society. Lawrence
Erlbaum Associates, Mahwah, NJ,
pages 399?404.
Siegel, Sidney and N. John Castellan. 1988.
Non Parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New York.
Teufel, Simone and Hans van Halteren.
2004. Evaluating information content
by factoid analysis: Human annotation
and stability. In Dekang Lin and
Dekai Wu, editors, Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 419?426,
Barcelona.
Weiss, Sholom M. and Casimir A.
Kulikowski. 1991. Computer Systems that
Learn: Classification and Prediction Methods
from Statistics, Neural Nets, Machine
Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
White, John S. and T. O?Connell. 1994.
The ARPA MT evaluation methodologies:
Evolution, lessons, and future approaches.
In Proceedings of the First Conference
of the Association for Machine Translation
in the Americas, pages 193?205,
Columbia, MD.
484
Modeling Local Coherence:
An Entity-Based Approach
Regina Barzilay?
Massachusetts Institute of Technology
Mirella Lapata??
University of Edinburgh
This article proposes a novel framework for representing and measuring local coherence. Central
to this approach is the entity-grid representation of discourse, which captures patterns of entity
distribution in a text. The algorithm introduced in the article automatically abstracts a text
into a set of entity transition sequences and records distributional, syntactic, and referential
information about discourse entities. We re-conceptualize coherence assessment as a learning
task and show that our entity-based representation is well-suited for ranking-based generation
and text classification tasks. Using the proposed representation, we achieve good performance on
text ordering, summary coherence evaluation, and readability assessment.
1. Introduction
A key requirement for any system that produces text is the coherence of its output.
Not surprisingly, a variety of coherence theories have been developed over the years
(e.g., Mann and Thomson 1988; Grosz et al 1995) and their principles have found
application in many symbolic text generation systems (e.g., Scott and de Souza 1990;
Kibble and Power 2004). The ability of these systems to generate high quality text,
almost indistinguishable from human writing, makes the incorporation of coherence
theories in robust large-scale systems particularly appealing. The task is, however,
challenging considering that most previous efforts have relied on handcrafted rules,
valid only for limited domains, with no guarantee of scalability or portability (Reiter
and Dale 2000). Furthermore, coherence constraints are often embedded in complex
representations (e.g., Asher and Lascarides 2003) which are hard to implement in a
robust application.
This article focuses on local coherence, which captures text relatedness at the level
of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global
coherence and has received considerable attention in computational linguistics (Foltz,
Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller
? Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar
Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu.
?? School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for
publication: 5 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
2004; Karamanis et al 2004). It is also supported bymuch psycholinguistic evidence. For
instance, McKoon and Ratcliff (1992) argue that local coherence is the primary source of
inference-making during reading.
The key premise of our work is that the distribution of entities in locally coher-
ent texts exhibits certain regularities. This assumption is not arbitrary?some of these
regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein
1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The
algorithm introduced in the article automatically abstracts a text into a set of entity tran-
sition sequences, a representation that reflects distributional, syntactic, and referential
information about discourse entities.
We argue that the proposed entity-based representation of discourse allows us
to learn the properties of coherent texts from a corpus, without recourse to manual
annotation or a predefined knowledge base. We demonstrate the usefulness of this rep-
resentation by testing its predictive power in three applications: text ordering, automatic
evaluation of summary coherence, and readability assessment.
We formulate the first two problems?text ordering and summary evaluation?as
ranking problems, and present an efficiently learnable model that ranks alternative ren-
derings of the same information based on their degree of local coherence. Such a mecha-
nism is particularly appropriate for generation and summarization systems as they can
produce multiple text realizations of the same underlying content, either by varying pa-
rameter values, or by relaxing constraints that control the generation process. A system
equipped with a ranking mechanism could compare the quality of the candidate
outputs, in much the same way speech recognizers employ language models at the
sentence level.
In the text-ordering task our algorithm has to select a maximally coherent sen-
tence order from a set of candidate permutations. In the summary evaluation task,
we compare the rankings produced by the model against human coherence judgments
elicited for automatically generated summaries. In both experiments, our method yields
improvements over state-of-the-art models. We also show the benefits of the entity-
based representation in a readability assessment task, where the goal is to predict the
comprehension difficulty of a given text. In contrast to existing systems which focus on
intra-sentential features, we explore the contribution of discourse-level features to this
task. By incorporating coherence features stemming from the proposed entity-based
representation, we improve the performance of a state-of-the-art readability assessment
system (Schwarm and Ostendorf 2005).
In the following section, we provide an overview of entity-based theories of lo-
cal coherence and outline previous work on its computational treatment. Then, we
introduce our entity-based representation, and define its linguistic properties. In the
subsequent sections, we present our three evaluation tasks, and report the results of our
experiments. Discussion of the results concludes the article.
2. Related Work
Our approach is inspired by entity-based theories of local coherence, and is well-suited
for developing a coherence metric in the context of a ranking-based text generation
system. We first summarize entity-based theories of discourse, and overview previous
attempts for translating their underlying principles into computational coherence mod-
els. Next, we describe ranking approaches to natural language generation and focus on
coherence metrics used in current text planners.
2
Barzilay and Lapata Modeling Local Coherence
2.1 Entity-Based Approaches to Local Coherence
Linguistic Modeling. Entity-based accounts of local coherence have a long tradition
within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday
and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi,
and Weinstein 1995). A unifying assumption underlying different approaches is that
discourse coherence is achieved in view of the way discourse entities are introduced
and discussed. This observation is commonly formalized by devising constraints on the
linguistic realization and distribution of discourse entities in coherent texts.
At any point in the discourse, some entities are considered more salient than
others, and consequently are expected to exhibit different properties. In Centering
Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and
Hahn 1999; Poesio et al 2004), salience concerns how entities are realized in an utterance
(e.g., whether they are they pronominalized or not). In other theories, salience is defined
in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and
Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More
refined accounts expand the notion of salience from a binary distinction to a scalar one;
examples include Prince?s (1981) familiarity scale, and Givon?s (1987) and Ariel?s (1988)
givenness-continuum.
The salience status of an entity is often reflected in its grammatical function and
the linguistic form of its subsequent mentions. Salient entities are more likely to ap-
pear in prominent syntactic positions (such as subject or object), and to be introduced
in a main clause. The linguistic realization of subsequent mentions?in particular,
pronominalization?is so tightly linked to salience that in some theories (e.g., Givon
1987) it provides the sole basis for defining a salience hierarchy. The hypothesis is that
the degree of underspecification in a referring expression indicates the topical status of
its antecedent (e.g., pronouns refer to very salient entities, whereas full NPs refer to less
salient ones). In Centering Theory, this phenomenon is captured in the Pronoun Rule,
and Givon?s Scale of Topicality and Ariel?s Accessibility Marking Scale propose a graded
hierarchy of underspecification that ranges from zero anaphora to full noun phrases,
and includes stressed and unstressed pronouns, demonstratives with modifiers, and
definite descriptions.
Entity-based theories capture coherence by characterizing the distribution of en-
tities across discourse utterances, distinguishing between salient entities and the rest.
The intuition here is that texts about the same discourse entity are perceived to be
more coherent than texts fraught with abrupt switches from one topic to the next. The
patterned distribution of discourse entities is a natural consequence of topic continuity
observed in a coherent text. Centering Theory formalizes fluctuations in topic continuity
in terms of transitions between adjacent utterances. The transitions are ranked, that
is, texts demonstrating certain types of transitions are deemed more coherent than texts
where such transitions are absent or infrequent. For example, CONTINUE transitions
require that two utterances have at least one entity in common and are preferred
over transitions that repeatedly SHIFT from one entity to the other. Givon?s (1987) and
Hoey?s (1991) accounts of discourse continuity complement local measurements by
considering global characteristics of entity distribution, such as the lifetime of an entity
in discourse and the referential distance between subsequent mentions.
Computational Modeling. An important practical question is how to translate principles
of these linguistic theories into a robust coherence metric. A great deal of research
has been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich
3
Computational Linguistics Volume 34, Number 1
2000; Hasler 2004; Karamanis et al 2004). Such translation is challenging in several
respects: one has to determine ways of combining the effects of various constraints and
to instantiate parameters of the theory that are often left underspecified. Poesio et al
(2004) note that even for fundamental concepts of Centering Theory such as ?utterance,?
?realization,? and ?ranking,? multiple?and often contradictory?interpretations have
been developed over the years, because in the original theory these concepts are not
explicitly fleshed out. For instance, in some Centering papers, entities are ranked with
respect to their grammatical function (Brennan, Friedman, and Pollard 1987; Walker,
Iida, and Cote 1994; Grosz, Joshi, andWeinstein 1995), and in others with respect to their
position in Prince?s (1981) givenness hierarchy (Strube and Hahn 1999) or their thematic
role (Sidner 1979). As a result, two ?instantiations? of the same theory make different
predictions for the same input. Poesio et al (2004) explore alternative specifications
proposed in the literature, and demonstrate that the predictive power of the theory is
highly sensitive to its parameter definitions.
A common methodology for translating entity-based theories into computational
models is to evaluate alternative specifications on manually annotated corpora. Some
studies aim to find an instantiation of parameters that is most consistent with observable
data (Strube and Hahn 1999; Karamanis et al 2004; Poesio et al 2004). Other studies
adopt a specific instantiation with the goal of improving the performance of a metric on
a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays
with entity transition information, and show that the distribution of transitions corre-
lates with human grades. Analogously, Hasler (2004) investigates whether Centering
Theory can be used in evaluating the readability of automatic summaries by annotating
human and machine generated extracts with entity transition information.
The present work differs from these approaches in goal andmethodology. Although
our work builds upon existing linguistic theories, we do not aim to directly implement
or refine any of them in particular. We provide our model with sources of knowledge
identified as essential by these theories, and leave it to the inference procedure to
determine the parameter values and an optimal way to combine them. From a design
viewpoint, we emphasize automatic computation for both the underlying discourse
representation and the inference procedure. Thus, our work is complementary to com-
putational models developed onmanually annotated data (Miltsakaki and Kukich 2000;
Hasler 2004; Poesio et al 2004). Automatic, albeit noisy, feature extraction allows us
to perform a large scale evaluation of differently instantiated coherence models across
genres and applications.
2.2 Ranking Approaches in Natural Language Generation
Ranking approaches have enjoyed an increasing popularity at all stages in the
generation pipeline, ranging from text planning to surface realization (Knight and
Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al 1998; Walker, Rambow,
and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an
underlying system produces a potentially large set of candidate outputs, with respect
to various text generation rules encoded as hard constraints. Not all of the resulting
alternatives will correspond to well-formed texts, and of those which may be judged ac-
ceptable, some will be preferable to others. The candidate generation phase is followed
by an assessment phase in which the candidates are ranked based on a set of desirable
properties encoded in a ranking function. The top-ranked candidate is selected for
presentation. A two-stage generate-and-rank architecture circumvents the complexity
4
Barzilay and Lapata Modeling Local Coherence
of traditional generation systems, where numerous, often conflicting constraints, have
to be encoded during development in order to produce a single high-quality output.
Because the focus of our work is on text coherence, we discuss here rank-
ing approaches applied to text planning (see Walker et al [2001] and Knight and
Hatzivassiloglou [1995] for ranking approaches to sentence planning and surface re-
alization, respectively). The goal of text planning is to determine the content of a text
by selecting a set of information-bearing units and arranging them into a structure that
yields well-formed output. Depending on the system, text plans are represented as dis-
course trees (Mellish et al 1998) or linear sequences of propositions (Karamanis 2003).
Candidate text structures may differ in terms of the selected propositions, the sequence
in which facts are presented, the topology of the tree, or the order in which entities are
introduced. A set of plausible candidates can be created via stochastic search (Mellish
et al 1998) or by a symbolic text planner following different text-formation rules (Kibble
and Power 2004). The best candidate is chosen using an evaluation or ranking function
often encoding coherence constraints. Although the type and complexity of constraints
vary greatly across systems, they are commonly inspired by Rhetorical Structure Theory
or entity-based constraints similar to the ones captured by our method. For instance,
the ranking function used by Mellish et al gives preference to plans where consecutive
facts mention the same entities and is sensitive to the syntactic environment in which
the entity is first introduced (e.g., in a subject or object position). Karamanis finds
that a ranking function based solely on the principle of continuity achieves competi-
tive performance against more sophisticated alternatives when applied to ordering
short descriptions of museum artifacts.1 In other applications, the ranking function is
more complex, integrating rules from Centering Theory along with stylistic constraints
(Kibble and Power 2004).
A common feature of current implementations is that the specification of the rank-
ing function?feature selection and weighting?is performed manually based on the
intuition of the system developer. However, even in a limited domain this task has
proven difficult. Mellish et al (1998; page 100) note: ?The problem is far too complex
and our knowledge of the issues involved so meager that only a token gesture can be
made at this point.? Moreover, these ranking functions operate over semantically rich
input representations that cannot be created automatically without extensive knowl-
edge engineering. The need for manual coding impairs the portability of existing meth-
ods for coherence ranking to new applications, most notably to text-to-text generation
applications, such as summarization.
In the next section, we present a method for coherence assessment that overcomes
these limitations: We introduce an entity-based representation of discourse that is auto-
matically computed from raw text; we argue that the proposed representation reveals
entity transition patterns characteristic of coherent texts. The latter can be easily trans-
lated into a large feature space which lends itself naturally to the effective learning of a
ranking function, without explicit manual involvement.
3. The Coherence Model
In this section we describe our entity-based representation of discourse. We explain how
it is computed and how entity transition patterns are extracted. We also discuss how
1 Each utterance in the discourse refers to at least one entity in the utterance that precedes it.
5
Computational Linguistics Volume 34, Number 1
these patterns can be encoded as feature vectors appropriate for performing coherence-
related ranking and classification tasks.
3.1 The Entity-Grid Discourse Representation
Each text is represented by an entity grid, a two-dimensional array that captures
the distribution of discourse entities across text sentences. We follow Miltsakaki and
Kukich (2000) in assuming that our unit of analysis is the traditional sentence (i.e., a
main clause with accompanying subordinate and adjunct clauses). The rows of the
grid correspond to sentences, and the columns correspond to discourse entities. By
discourse entity we mean a class of coreferent noun phrases (we explain in Section 3.3
how coreferent entities are identified). For each occurrence of a discourse entity in the
text, the corresponding grid cell contains information about its presence or absence
in a sequence of sentences. In addition, for entities present in a given sentence, grid
cells contain information about their syntactic role. Such information can be expressed
in many ways (e.g., using constituent labels or thematic role information). Because
grammatical relations figure prominently in entity-based theories of local coherence (see
Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to
a string from a set of categories reflecting whether the entity in question is a subject (S),
object (O), or neither (X). Entities absent from a sentence are signaled by gaps (?).
Grammatical role information can be extracted from the output of a broad-coverage
dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical
parser (Collins 1997; Charniak 2000). We discuss how this information was computed
for our experiments in Section 3.3.
Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2.
Because the text contains six sentences, the grid columns are of length six. Consider
for instance the grid column for the entity trial, [O ? ? ? ? X]. It records that trial is
present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the
sentences. Also note that the grid in Table 1 takes coreference resolution into account.
Even though the same entity appears in different linguistic forms, for example,Microsoft
Corp., Microsoft, and the company, it is mapped to a single entry in the grid (see the
column introduced byMicrosoft in Table 1).
Table 1
A fragment of the entity grid. Noun phrases are represented by their head nouns. Grid cells
correspond to grammatical roles: subjects (S), objects (O), or neither (X).
D
e
p
a
rt
m
e
n
t
T
ri
a
l
M
ic
ro
so
ft
E
v
id
e
n
ce
C
o
m
p
e
ti
to
rs
M
a
rk
e
ts
P
ro
d
u
ct
s
B
ra
n
d
s
C
a
se
N
e
ts
ca
p
e
S
o
ft
w
a
re
T
a
ct
ic
s
G
o
v
e
rn
m
e
n
t
S
u
it
E
a
rn
in
g
s
1 S O S X O ? ? ? ? ? ? ? ? ? ? 1
2 ? ? O ? ? X S O ? ? ? ? ? ? ? 2
3 ? ? S O ? ? ? ? S O O ? ? ? ? 3
4 ? ? S ? ? ? ? ? ? ? ? S ? ? ? 4
5 ? ? ? ? ? ? ? ? ? ? ? ? S O ? 5
6 ? X S ? ? ? ? ? ? ? ? ? ? ? O 6
6
Barzilay and Lapata Modeling Local Coherence
Table 2
Summary augmented with syntactic annotations for grid computation.
1 [The Justice Department]S is conducting an [anti-trust trial]O against [Microsoft Corp.]X
with [evidence]X that [the company]S is increasingly attempting to crush [competitors]O.
2 [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its own
products]S are not competitive enough to unseat [established brands]O.
3 [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring
[Netscape]O into merging [browser software]O.
4 [Microsoft]S claims [its tactics]S are commonplace and good economically.
5 [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb [competition]O
through [collusion]X is [a violation of the Sherman Act]O.
6 [Microsoft]S continues to show [increased earnings]O despite [the trial]X.
When a noun is attested more than once with a different grammatical role in the
same sentence, we default to the role with the highest grammatical ranking: subjects are
ranked higher than objects, which in turn are ranked higher than the rest. For example,
the entity Microsoft is mentioned twice in Sentence 1 with the grammatical roles x (for
Microsoft Corp.) and s (for the company), but is represented only by s in the grid (see
Tables 1 and 2).
3.2 Entity Grids as Feature Vectors
A fundamental assumption underlying our approach is that the distribution of entities
in coherent texts exhibits certain regularities reflected in grid topology. Some of these
regularities are formalized in Centering Theory as constraints on transitions of the
local focus in adjacent sentences. Grids of coherent texts are likely to have some dense
columns (i.e., columns with just a few gaps, such as Microsoft in Table 1) and many
sparse columns which will consist mostly of gaps (see markets and earnings in Table 1).
One would further expect that entities corresponding to dense columns are more often
subjects or objects. These characteristics will be less pronounced in low-coherence texts.
Inspired by Centering Theory, our analysis revolves around patterns of local entity
transitions. A local entity transition is a sequence {S,O, X, ?}n that represents entity
occurrences and their syntactic roles in n adjacent sentences. Local transitions can be
easily obtained from a grid as continuous subsequences of each column. Each transition
will have a certain probability in a given grid. For instance, the probability of the
transition [S ?] in the grid from Table 1 is 0.08 (computed as a ratio of its frequency
[i.e., six] divided by the total number of transitions of length two [i.e., 75]). Each text
can thus be viewed as a distribution defined over transition types.
We can now go one step further and represent each text by a fixed set of transition
sequences using a standard feature vector notation. Each grid rendering j of a document
di corresponds to a feature vector ?(xij) = (p1(xij), p2(xij), . . . , pm(xij)), where m is the
number of all predefined entity transitions, and pt(xij) the probability of transition t
in grid xij. This feature vector representation is usefully amenable to machine learning
algorithms (see our experiments in Sections 4?6). Furthermore, it allows the consid-
eration of large numbers of transitions which could potentially uncover novel entity
distribution patterns relevant for coherence assessment or other coherence-related tasks.
Note that considerable latitude is available when specifying the transition types to
be included in a feature vector. These can be all transitions of a given length (e.g., two
or three) or the most frequent transitions within a document collection. An example of
7
Computational Linguistics Volume 34, Number 1
a feature space with transitions of length two is illustrated in Table 3. The second row
(introduced by d1) is the feature vector representation of the grid in Table 1.
3.3 Grid Construction: Linguistic Dimensions
One of the central research issues in developing entity-based models of coherence is
determining what sources of linguistic knowledge are essential for accurate prediction,
and how to encode them succinctly in a discourse representation. Previous approaches
tend to agree on the features of entity distribution related to local coherence?the
disagreement lies in the way these features are modeled.
Our study of alternative encodings is not a mere duplication of previous ef-
forts (Poesio et al 2004) that focus on linguistic aspects of parameterization. Because we
are interested in an automatically constructed model, we have to take into account com-
putational and learning issues when considering alternative representations. Therefore,
our exploration of the parameter space is guided by three considerations: the linguistic
importance of a parameter, the accuracy of its automatic computation, and the size of the
resulting feature space. From the linguistic side, we focus on properties of entity distri-
bution that are tightly linked to local coherence, and at the same time allow for multiple
interpretations during the encoding process. Computational considerations prevent us
from considering discourse representations that cannot be computed reliably by exist-
ing tools. For instance, we could not experiment with the granularity of an utterance?
sentence versus clause?because available clause separators introduce substantial noise
into a grid construction. Finally, we exclude representations that will explode the size of
the feature space, thereby increasing the amount of data required for training themodel.
Entity Extraction. The accurate computation of entity classes is key to computing mean-
ingful entity grids. In previous implementations of entity-basedmodels, classes of coref-
erent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis
et al 2004; Poesio et al 2004), but this is not an option for our model. An obvious
solution for identifying entity classes is to employ an automatic coreference resolution
tool that determines which noun phrases refer to the same entity in a document.
Current approaches recast coreference resolution as a classification task. A pair
of NPs is classified as coreferring or not based on constraints that are learned from
an annotated corpus. A separate clustering mechanism then coordinates the possibly
contradictory pairwise classifications and constructs a partition on the set of NPs. In
our experiments, we employ Ng and Cardie?s (2002) coreference resolution system.
The system decides whether two NPs are coreferent by exploiting a wealth of lexical,
grammatical, semantic, and positional features. It is trained on the MUC (6?7) data sets
and yields state-of-the-art performance (70.4 F-measure onMUC-6 and 63.4 onMUC-7).
Table 3
Example of a feature-vector document representation using all transitions of length two given
syntactic categories S, O, X, and ?.
S S S O S X S ? O S O O O X O ? X S X O X X X ? ? S ? O ? X ? ?
d1 .01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59
d2 .02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36
d3 .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39
8
Barzilay and Lapata Modeling Local Coherence
Although machine learning approaches to coreference resolution have been rea-
sonably successful?state-of-the-art coreference tools today reach an F-measure2 of
70% when trained on newspaper texts?it is unrealistic to assume that such tools will
be readily available for different domains and languages. We therefore consider an
additional approach to entity extraction where entity classes are constructed simply by
clustering nouns on the basis of their identity. In other words, each noun in a text cor-
responds to a different entity in a grid, and two nouns are considered coreferent only if
they are identical. Under this viewMicrosoft Corp. from Table 2 (Sentence 1) corresponds
to two entities, Microsoft and Corp., which are in turn distinct from the company. This
approach is only a rough approximation to fully fledged coreference resolution, but it
is simple from an implementational perspective and produces consistent results across
domains and languages.
Grammatical Function. Several entity-based approaches assert that grammatical function
is indicative of an entity?s prominence in discourse (Hudson, Tanenhaus, and Dell 1986;
Kameyama 1986; Brennan, Friedman, and Pollard 1987; Grosz, Joshi, and Weinstein
1995). Most theories discriminate between subject, object, and the remaining grammati-
cal roles: subjects are ranked higher than objects, and these are ranked higher than other
grammatical functions.
In our framework, we can easily assess the impact of syntactic knowledge by
modifying how transitions are represented in the entity grid. In syntactically aware
grids, transitions are expressed by four categories: s, o, x and ?, whereas in simplified
grids, we only record whether an entity is present (x) or absent (?) in a sentence.
We employ a robust statistical parser (Collins 1997) to determine the constituent
structure for each sentence, fromwhich subjects (s), objects (o), and relations other than
subject or object (x) are identified. The phrase-structure output of Collins?s parser is
transformed into a dependency tree from which grammatical relations are extracted.
Passive verbs are recognized using a small set of patterns, and the underlying deep
grammatical role for arguments involved in the passive construction is entered in the
grid (see the grid cell o forMicrosoft, Sentence 2, Table 2). For more details on the gram-
matical relations extraction component we refer the interested reader to Barzilay (2003).
Salience. Centering and other discourse theories conjecture that the way an entity is
introduced and mentioned depends on its global role in a given discourse. We evaluate
the impact of salience information by considering two types of models: The first model
treats all entities uniformly, whereas the second one discriminates between transitions
of salient entities and the rest. We identify salient entities based on their frequency,3 fol-
lowing the widely accepted view that frequency of occurrence correlates with discourse
prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991).
To implement a salience-based model, we modify our feature generation proce-
dure by computing transition probabilities for each salience group separately, and then
2 When evaluating the output of coreference algorithms, performance is typically measured using a
model-theoretic scoring scheme proposed in Vilain et al (1995). The scoring algorithm computes the
recall error by taking each equivalence class S in the gold standard and determining the number of
coreference links m that would have to be added to the system?s output to place all entities in S into
the same equivalence class produced by the system. Recall error then is the sum of ms divided by the
number of links in the gold standard. Precision error is computed by reversing the roles of the gold
standard and system output.
3 The frequency threshold is empirically determined on the development set. See Section 4.2 for further
discussion.
9
Computational Linguistics Volume 34, Number 1
combining them into a single feature vector. For n transitions with k salience classes,
the feature space will be of size n? k. While we can easily build a model with multiple
salience classes, we opt for a binary distinction (i.e., k = 2). This is more in line with
theoretical accounts of salience (Chafe 1976; Grosz, Joshi, and Weinstein 1995) and
results in a moderate feature space for which reliable parameter estimation is possible.
Considering a large number of salience classes would unavoidably increase the number
of features. Parameter estimation in such a space requires a large sample of training
examples that is unavailable for most domains and applications.
Different classes of models can be defined along the linguistic dimensions just dis-
cussed. Our experiments will consider several models with varying degrees of linguistic
complexity, while attempting to strike a balance between expressivity of representation
and ease of computation. In the following sections we evaluate their performance on
three tasks: sentence ordering, summary coherence rating, and readability assessment.
3.4 Learning
Equipped with the feature vector representation introduced herein, we can view co-
herence assessment as a machine learning problem. When considering text generation
applications, it is desirable to rank rather than classify instances: There is often no single
coherent rendering of a given text but many different possibilities that can be partially
ordered. It is therefore not surprising that systems often employ scoring functions to
select the most coherent output among alternative renderings (see the discussion in
Section 2.2). In this article we argue that encoding texts as entity transition sequences
constitutes an appropriate feature set for learning (rather than manually specifying)
such a ranking function (see Section 4 for details). We present two task-based exper-
iments that put this hypothesis to the test: information ordering (Experiment 1) and
summary coherence rating (Experiment 2). Both tasks can be naturally formulated as
ranking problems; the learner takes as input a set of alternative renderings of the
same document and ranks them based on their degree of local coherence. Examples
of such renderings are a set of different sentence orderings of the same text and a set
of summaries produced by different systems for the same document. Note that in both
ranking experiments we assume that the algorithm is provided with a limited number
of alternatives. In practice, the space of candidates can be vast, and finding the optimal
candidate may require pairing our ranking algorithmwith a decoder similar to the ones
used in machine translation (Germann et al 2004).
Although the majority of our experiments fall within the generate-and-rank frame-
work previously sketched, nothing prevents the use of our feature vector representation
for conventional classification tasks. We offer an illustration in Experiment 3, where
features extracted from entity grids are used to enhance the performance of a readability
assessment system. Here, the learner takes as input a set of documents labeled with
discrete classes (e.g., denoting whether a text is difficult or easy to read) and learns to
make predictions for unseen instances (see Section 6 for details on the machine learning
paradigm we employ).
4. Experiment 1: Sentence Ordering
Text structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al 2004)
are commonly evaluated by their performance at information-ordering. The task con-
cerns determining a sequence in which to present a pre-selected set of information-
10
Barzilay and Lapata Modeling Local Coherence
bearing items; this is an essential step in concept-to-text generation, multi-document
summarization, and other text-synthesis problems. The information bearing items can
be database entries (Karamanis et al 2004), propositions (Mellish et al 1998) or sen-
tences (Lapata 2003; Barzilay and Lee 2004). In sentence ordering, a document is viewed
as a bag of sentences and the algorithm?s task is to try to find the ordering which
maximizes coherence according to some criterion (e.g., the probability of an order).
As explained previously, we use our coherence model to rank alternative sentence
orderings instead of trying to find an optimal ordering. We do not assume that local
coherence is sufficient to uniquely determine a maximally coherent ordering?other
constraints clearly play a role here. It is nevertheless a key property of well-formed
text (documents lacking local coherence are naturally globally incoherent), and a model
which takes it into account should be able to discriminate coherent from incoher-
ent texts. In our sentence-ordering task we generate random permutations of a test
document and measure how often a permutation is ranked higher than the original
document. A non-deficient model should prefer the original text more frequently than
its permutations (see Section 4.2 for details).
We begin by explaining how a ranking function can be learned for the sentence
ordering task. Next, we give details regarding the corpus used for our experiments,
describe the methods used for comparison with our approach, and note the evaluation
metric employed for assessing model performance. Our results are presented in Sec-
tion 4.3.
4.1 Modeling
Our training set consists of ordered pairs of alternative renderings (xij, xik) of the same
document di, where xij exhibits a higher degree of coherence than xik (we describe in
Section 4.2 how such training instances are obtained). Without loss of generality, we
assume j > k. The goal of the training procedure is to find a parameter vector w that
yields a ?ranking score? function which minimizes the number of violations of pairwise
rankings provided in the training set
?(xij, xik) ? r
? : w ? ?(xij) > w ? ?(xik)
where (xij, xik) ? r? if xij is ranked higher than xik for the optimal ranking r? (in the
training data), and ?(xij) and ?(xik) are a mapping onto features representing the
coherence properties of renderings xij and xik. In our case the features correspond to
the entity transition probabilities introduced in Section 3.2. Thus, the ideal ranking
function, represented by the weight vector wwould satisfy the condition
w ? (?(xij)? ?(xik)) > 0 ?j, i, k such that j > k
The problem is typically treated as a Support Vector Machine constraint optimization
problem, and can be solved using the search technique described in Joachims (2002).
This approach has been shown to be highly effective in various tasks ranging from
collaborative filtering (Joachims 2002) to parsing (Toutanova, Markova, and Manning
2004). Other discriminative formulations of the ranking problem are possible (Collins
2002; Freund et al 2003); however, we leave this to future work.
11
Computational Linguistics Volume 34, Number 1
Table 4
The size of the training and test instances for the Earthquakes and Accidents corpora (measured
by the number of pairs that contain the original order and a random permutation of this order).
Training Testing
Earthquakes 1,896 2,056
Accidents 2,095 2,087
Once the ranking function is learned, unseen renderings (xij, xik) of document di
can be ranked simply by computing the valuesw??(xij) andw
??(xik) and sorting them
accordingly. Here,w? is the optimized parameter vector resulting from training.
4.2 Method
Data. To acquire a large collection for training and testing, we create synthetic data,
wherein the candidate set consists of a source document and permutations of its sen-
tences. This framework for data acquisition enables large-scale automatic evaluation
and is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003;
Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004). The underlying assump-
tion is that the original sentence order in the source document must be coherent, and
so we should prefer models that rank it higher than other permutations. Because we
do not know the relative quality of different permutations, our corpus includes only
pairwise rankings that comprise the original document and one of its permutations.
Given k original documents, each with n randomly generated permutations, we obtain
k ? n (trivially) annotated pairwise rankings for training and testing.
Using the technique described herein, we collected data4 in two different genres:
newspaper articles and accident reports written by government officials. The first col-
lection consists of Associated Press articles from the North American News Corpus
on the topic of earthquakes (Earthquakes). The second includes narratives from the
National Transportation Safety Board?s aviation accident database (Accidents). Both
corpora have documents of comparable length?the average number of sentences is 10.4
and 11.5, respectively. For each set, we used 100 source articles with up to 20 randomly
generated permutations for training.5 A similar methodwas used to obtain the test data.
Table 4 shows the size of the training and test corpora used in our experiments. We held
out 10 documents (i.e., 200 pairwise rankings) from the training data for development
purposes.
Features and Parameter Settings. In order to investigate the contribution of linguistic
knowledge on model performance we experimented with a variety of grid representa-
tions resulting in different parameterizations of the feature space fromwhich our model
is learned. We focused on three sources of linguistic knowledge?syntax, coreference
resolution, and salience?which play a prominent role in entity-based analyses of dis-
4 The collections are available from http://people.csail.mit.edu/regina/coherence/.
5 Short texts may have less than 20 permutations. The corpus described in the original ACL publication
(Barzilay and Lapata 2005) contained a number of duplicate permutations. These were removed from
the current version of the corpus.
12
Barzilay and Lapata Modeling Local Coherence
course coherence (see Section 3.3 for details). An additional motivation for our study
was to explore the trade-off between robustness and richness of linguistic annotations.
NLP tools are typically trained on human-authored texts, and may deteriorate in per-
formance when applied to automatically generated texts with coherence violations.
We thus compared a linguistically rich model against models that use more im-
poverished representations. More concretely, our full model (Coreference+Syntax+
Salience+) uses coreference resolution, denotes entity transition sequences via gram-
matical roles, and differentiates between salient and non-salient entities. Our less-
expressive models (seven in total) use only a subset of these linguistic features
during the grid construction process. We evaluated the effect of syntactic knowl-
edge by eliminating the identification of grammatical relations and recording solely
whether an entity is present or absent in a sentence. This process created a class
of four models of the form Coreference[+/?]Syntax?Salience[+/?]. The effect of
fully fledged coreference resolution was assessed by creating models where entity
classes were constructed simply by clustering nouns on the basis of their identity
(Coreference?Syntax[+/?]Salience[+/?]). Finally, the contribution of salience was
measured by comparing the full model which accounts separately for patterns of salient
and non-salient entities against models that do not attempt to discriminate between
them (Coreference[+/?]Syntax[+/?]Salience?).
We would like to note that in this experiment we apply a coreference resolution tool
to the original text and then generate permutations for the pairwise ranking task. An
alternative design is to apply coreference resolution to permuted texts. Because existing
methods for coreference resolution take into consideration the order of noun phrases in
a text, the accuracy of these tools on permuted sentence sequences is close to random.
Therefore, we opt to resolve coreference within the original text. Although this design
has an oracle feel to it, it is not uncommon in practical applications. For instance, in text
generation systems, content planners often operate over fully specified semantic rep-
resentations, and can thus take advantage of coreference information during sentence
ordering.
Besides variations in the underlying linguistic representation, our model is also
specified by two free parameters: the frequency threshold used to identify salient en-
tities and the length of the transition sequence. These parameters were tuned separately
for each data set on the corresponding held-out development set. Optimal salience-
based models were obtained for entities with frequency ?2. The optimal transition
length was ?3.6
In our ordering experiments, we used Joachims?s (2002) SVMlight package for train-
ing and testing with all parameters set to their default values.
Comparison with State-of-the-Art Methods. We compared the performance of our algo-
rithm against two state-of-the-art models proposed by Foltz, Kintsch, and Landauer
(1998) and Barzilay and Lee (2004). These models rely largely on lexical information
for assessing document coherence, contrary to our models which are in essence un-
lexicalized. Recall from Section 3 that our approach captures local coherence by mod-
eling patterns of entity distribution in discourse, without taking note of their lexical
instantiations. In the following we briefly describe the lexicalized models we employed
in our comparative study and motivate their selection.
6 The models we used in our experiments are available from http://people.csail.mit.edu/
regina/coherence/ and http://homepages.inf.ed.ac.uk/mlap/coherence/.
13
Computational Linguistics Volume 34, Number 1
Foltz, Kintsch, and Landauer (1998) model measures coherence as a function of
semantic relatedness between adjacent sentences. The underlying intuition here is that
coherent texts will contain a high number of semantically related words. Semantic
relatedness is computed automatically using Latent Semantic Analysis (LSA; Landauer
and Dumais 1997) from raw text without employing syntactic or other annotations. In
this framework, a word?s meaning is captured in a multi-dimensional space by a vector
representing its co-occurrence with neighboring words. Co-occurrence information is
collected in a frequencymatrix, where each row corresponds to a uniqueword, and each
column represents a given linguistic context (e.g., sentence, document, or paragraph).
Foltz, Kintsch, and Landauer?s model use singular value decomposition (SVD; Berry,
Dumais, and O?Brien 1994) to reduce the dimensionality of the space. The transforma-
tion renders sparse matrices more informative and can be thought of as a means of
uncovering latent structure in distributional data. The meaning of a sentence is next
represented as a vector by taking the mean of the vectors of its words. The similarity
between two sentences is determined by measuring the cosine of their means:
sim(S1,S2) = cos(?( S1),?( S2))
=
n
?
j=1
?j( S1)?j( S2)
?
n
?
j=1
(?j( S1))
2
?
n
?
j=1
(?j( S2))
2
(1)
where ?(Si) =
1
|Si|
?
u?Si u, and u is the vector for word u. An overall text coherence
measure can be easily obtained by averaging the cosines for all pairs of adjacent sen-
tences Si and Si+1:
coherence(T) =
n?1
?
i=1
cos(Si,Si+1)
n? 1 (2)
This model is a good point of comparison for several reasons: (a) it is fully automatic
and has relatively few parameters (i.e., the dimensionality of the space and the choice of
similarity function), (b) it correlates reliably with human judgments and has been used
to analyze discourse structure, and (c) it models an aspect of local coherence which is
orthogonal to ours. The LSAmodel is lexicalized: coherence amounts to quantifying the
degree of semantic similarity between sentences. In contrast, our model does not incor-
porate any notion of similarity: coherence is encoded in terms of transition sequences
that are document-specific rather than sentence-specific.
Our implementation of the LSA model followed closely Foltz, Kintsch, and
Landauer (1998). We constructed vector-based representations for individual words
from a lemmatized version of the North American News Corpus7 (350 million words)
using a term?document matrix. We used SVD to reduce the semantic space to 100
dimensions obtaining thus a space similar to LSA. We estimated the coherence of a doc-
ument using Equations (1) and (2). A ranking can be trivially inferred by comparing the
7 Our selection of this corpus was motivated by two factors: (a) the corpus is large enough to yield a
reliable semantic space, and (b) it consists of news stories and is therefore similar in style, vocabulary,
and content to most of the corpora employed in our coherence experiments.
14
Barzilay and Lapata Modeling Local Coherence
coherence score assigned to the original document against each of its permutations. Ties
are resolved randomly.
Both LSA and our entity-grid model are local?they model sentence-to-sentence
transitions without being aware of global document structure. In contrast, the content
models developed by Barzilay and Lee (2004) learn to represent more global text prop-
erties by capturing topics and the order in which these topics appear in texts from the
same domain. For instance, a typical earthquake newspaper report contains information
about the quake?s epicenter, how much it measured, the time it was felt, and whether
there were any victims or damage. By encoding constraints on the ordering of these
topics, content models have a pronounced advantage in modeling document structure
because they can learn to represent how documents begin and end, but also how the
discourse shifts from one topic to the next. Like LSA, the content models are lexicalized;
however, unlike LSA, they are domain-specific, and would expectedly yield inferior
performance on out-of-domain texts.
Barzilay and Lee (2004) implemented content models using anHMMwherein states
correspond to distinct topics (for instance, the epicenter of an earthquake or the number
of victims), and state transitions represent the probability of changing from one topic
to another, thereby capturing possible topic-presentation orderings within a domain.
Topics refer to text spans of varying granularity and length. Barzilay and Lee used
sentences in their experiments, but clauses or paragraphs would also be possible.
Barzilay and Lee (2004) employed their content models to find a high-probability
ordering for a document whose sentences had been randomly shuffled. Here, we use
content models for the simpler coherence ranking task. Given two text permutations,
we estimate their likelihood according to their HMMmodel and select the text with the
highest probability. Because the two candidates contain the same set of sentences, the
assumption is that a more probable text corresponds to an ordering that is more typical
for the domain of interest.
In our experiments, we built two content models, one for the Accidents corpus and
one for the Earthquake corpus. Although these models are trained in an unsupervised
fashion, a number of parameters related to the model topology (i.e., number of states
and smoothing parameters) affect their performance. These parameters were tuned on
the development set and chosen so as to optimize the models? performance on the
pairwise ranking task.
Evaluation Metric. Given a set of pairwise rankings (an original document and one of
its permutations), we measure accuracy as the ratio of correct predictions made by the
model over the size of the test set. In this setup, random prediction results in an accuracy
of 50%.
4.3 Results
Impact of Linguistic Representation. We first investigate how different types of linguistic
knowledge influence our model?s performance. Table 5 shows the accuracy on the or-
dering task when the model is trained on different grid representations. As can be seen,
in both domains, the full model Coreference+Syntax+Salience+ significantly outper-
forms a linguistically naive model which simply records the presence (and absence)
of entities in discourse (Coreference?Syntax?Salience?). Moreover, we observe that
linguistically impoverished models consistently perform worse than their linguisti-
cally elaborate counterparts. We assess whether differences in accuracy are statistically
15
Computational Linguistics Volume 34, Number 1
Table 5
Accuracy measured as a fraction of correct pairwise rankings in the test set. Coreference[+/?]
indicates whether coreference information has been used in the construction of the entity grid.
Similarly, Syntax[+/?] and Salience[+/?] reflect the use of syntactic and salience information.
Diacritics ** (p < .01) and * (p < .05) indicate whether differences in accuracy between the full
model (Coreference+Syntax+Salience+) and all other models are significant (using a Fisher
Sign test).
Model Earthquakes Accidents
Coreference+Syntax+Salience+ 87.2 90.4
Coreference+Syntax+Salience? 88.3 90.1
Coreference+Syntax?Salience+ 86.6 88.4??
Coreference?Syntax+Salience+ 83.0?? 89.9
Coreference+Syntax?Salience? 86.1 89.2
Coreference?Syntax+Salience? 82.3?? 88.6?
Coreference?Syntax?Salience+ 83.0?? 86.5??
Coreference?Syntax?Salience? 81.4?? 86.0??
HMM-based Content Models 88.0 75.8??
Latent Semantic Analysis 81.0?? 87.3??
significant using a Fisher Sign Test. Specifically, we compare the full model against each
of the less expressive models (see Table 5).
Let us first discuss in more detail how the contribution of different knowl-
edge sources varies across domains. On the Earthquakes corpus every model that
does not use coreference information (Coreference?Syntax[+/?]Salience[+/?]) per-
forms significantly worse than models augmented with coreference (Coreference+
Syntax[+/?]Salience[+/?]). This effect is less pronounced on the Accidents corpus,
especially for model Coreference?Syntax+Salience+ whose accuracy drops only
by 0.5% (the difference between Coreference?Syntax+Salience+ and Coreference+
Syntax+Salience+ is not statistically significant). The same model?s performance de-
creases by 4.2% on the Earthquakes corpus. This variation can be explained by differ-
ences in entity realization between the two domains. In particular, the two corpora vary
in the amount of coreference they employ; texts from the Earthquakes corpus contain
many examples of referring expressions that our simple identity-based approach cannot
possibly resolve. Consider for instance the text in Table 6. Here, the expressions the
same area, the remote region, and site all refer to Menglian county. In comparison, the text
from the Accidents corpus contains fewer referring expressions, in fact entities are often
repeated verbatim across several sentences, and therefore could be straightforwardly
resolved with a shallow approach (see the pilot, the pilot, the pilot in Table 6).
The omission of syntactic information causes a drop in accuracy for models applied
to the Accidents corpus. This effect is less noticeable on the Earthquakes corpus (com-
pare the performance of model Coreference+Syntax?Salience+ on the two corpora).
We explain this variation by the substantial difference in the type/token ratio between
the two domains?12.1 for Earthquakes versus 5.0 for Accidents. The low type/token
ratio for Accidents means that most sentences in a text have some words in common.
For example, the entities pilot, airplane, and airport appear in multiple sentences in the
text from Table 6. Because there is so much repetition in this domain, the syntax-free
grids will be relatively similar for both coherent (original) and incoherent texts (permu-
tations). In fact, inspection of the grids from the Accidents corpus reveals that they have
many sequences of the form [X X X], [X ? ? X], [X X ? ?], and [? ? X X] in common,
16
Barzilay and Lapata Modeling Local Coherence
Table 6
Two texts from the Earthquakes and Accidents corpus. One entity class for each document is
shown to demonstrate the difference in referring expressions used in the two corpora.
Example Text from Earthquakes
A strong earthquake hit the China-Burma border early Wednesday morning, but there
were no reports of deaths, according to China?s Central Seismology Bureau. The 7.3 quake
hit
?
?

?
Menglian county at 5:46 am.
?
?

?
The same area was struck by a 6.2 temblor early Monday
morning, the bureau said. The county is on the China-Burma border, and is a sparsely populated,
mountainous region. The bureau?s XuWei said some buildings sustained damage and there were
some injuries, but he had no further details. Communication with
?
?

?
the remote region is difficult,
and satellite phones sent from the neighboring province of Sichuan have not yet reached
?
?

?
the site.
However, he said the likelihood of deaths was low because residents should have been evacuated
from
?
?

?
the area following Monday?s quake.
Example Text from Accidents
When
?
?

?
the pilot failed to arrive for his brother?s college graduation, concerned family members
reported that he and his airplane were missing. A search was initiated, and the Civil Air Patrol
located the airplane on top of Pine Mountain. According to
?
?

?
the pilot ?s flight log, the intended
destination was Pensacola, FL, with intermediate stops for fuel at Thomson, GA, and Greenville,
AL. Airport personal at Thomson confirmed that the airplane landed about 1630 on 11/6/97.
They reported that
?
?

?
the pilot purchased 26.5 gallons of 100LL fuel and departed about 1700.
Witnesses at the Thomson Airport stated that when he took off, the weather was marginal VFR
and deteriorating rapidly. Witnesses near Pine Mountain stated that the visibility at the time of
the accident was about 1/4 mile in haze/fog.
whereas such sequences are more common in coherent Earthquakes documents and
more sparse in their permutations. This indicates that syntax-free analysis can suffi-
ciently discriminate coherent from incoherent texts in the Earthquakes domain, while
a more refined representation of entity transition types is required for the Accidents
domain.
The contribution of salience is less pronounced in both domains?the differ-
ence in performance between the full model (Coreference+Syntax+Salience+) and
its salience-agnostic counterpart (Coreference+Syntax+Salience+) is not statisti-
cally significant. Salience-based models do deliver some benefits for linguistically
impoverished models?for instance, Coreference?Syntax?Salience+ improves over
Coreference?Syntax?Salience? (p< 0.06) on the Earthquakes corpus.We hypothesize
that the small contribution of salience is related to the way it is currently represented.
Addition of this knowledge source to our grid representation, doubles the number
of features that serve as input to the learning algorithm. In other words, salience-
aware models need to learn twice as many parameters as salience-free models, while
having access to the same amount of training data. Achieving any improvement in these
conditions is challenging.
Comparison with State-of-the-Art Methods.We next discuss the performance of the HMM-
based content models (Barzilay and Lee 2004) and LSA (Foltz, Kintsch, and Landauer
1998) in comparison to our model (Coreference+Syntax+Salience+).
17
Computational Linguistics Volume 34, Number 1
First, note that the entity-grid model significantly outperforms LSA on both do-
mains (p < .01 using a Sign test, see Table 5). In contrast to our model, LSA is nei-
ther entity-based nor unlexicalized: It measures the degree of semantic overlap across
successive sentences, without handling discourse entities in a special way (all content
words in a sentence contribute towards its meaning). We attribute our model?s superior
performance, despite the lack of lexicalization, to three factors: (a) the use of more
elaborate linguistic knowledge (coreference and grammatical role information); (b) a
more holistic representation of coherence (recall that our entity grids operate over texts
rather than individual sentences; furthermore, entity transitions can span more than
two consecutive sentences, something which is not possible with the LSA model); and
(c) exposure to domain relevant texts (the LSA model used in our experiments was not
particularly tuned to the Earthquakes or Accidents corpus). Our semantic space was
created from a large news corpus (see Section 4.2) covering a wide variety of topics
and writing styles. This is necessary for constructing robust vector representations that
are not extremely sparse. We thus expect the grid models to be more sensitive to the
discourse conventions of the training/test data.
The accuracy of the HMM-based content modes is comparable to the grid model on
the Earthquakes corpus (the difference is not statistically significant) but is significantly
lower on the Accidents texts (see Table 5). Although the grid model yields similar
performance on the two domains, content models exhibit high variability. These results
are not surprising. The analysis presented in Barzilay and Lee (2004) shows that the
Earthquakes texts are quite formulaic in their structure, following the editorial style of
the Associated Press. In contrast, the Accidents texts are more challenging for content
models?reports in this set do not undergo centralized editing and therefore exhibit
more variability in lexical choice and style. The LSA model also significantly outper-
forms the content model on the Earthquakes domain (p < .01 using a Sign test). Being a
local model, LSA is less sensitive to the way documents are structured and is therefore
more likely to deliver consistent performance across domains.
The comparison in Table 5 covers a broad spectrum of coherence models. At one
end of the spectrum is LSA, a lexicalized model of local discourse coherence which is
fairly robust and domain independent. In the middle of the spectrum lies our entity-
grid model, which is unlexicalized but linguistically informed and goes beyond sim-
ple sentence-to-sentence transitions without, however, fully modeling global discourse
structure. At the other end of the spectrum are the HMM-based content models, which
are both global and lexicalized. Our results indicate that these models are complemen-
tary and that their combination could yield improved results. For example, we could
lexicalize our entity grids or supply the content models with local information either in
the style of LSA or as entity transitions. However, we leave this to future work.
Training Requirements.We now examine in more detail the training requirements for the
entity-grid models. Although for our ordering experiments we obtained training data
cheaply, this will not generally be the case and some effort will have to be invested
in collecting appropriate data with coherence ratings. We thus address two questions:
(1) Howmuch training data is required for achieving satisfactory performance? (2) How
domain sensitive are the entity-grid models? In other words, does their performance
degrade gracefully when applied to out-of-domain texts?
Figure 1 shows learning curves for the best performing model (Coreference+
Syntax+Salience+) on the Earthquakes and Accidents corpora. We observe that the
amount of data required depends on the domain at hand. The Accidents texts are more
repetitive and therefore less training data is required to achieve good performance. The
18
Barzilay and Lapata Modeling Local Coherence
Figure 1
Learning curves for the entity-based model Coreference+Syntax+Salience+ on the
Earthquakes and Accidents corpora.
learning curve is steeper for the Earthquakes documents. Irrespective of the domain
differences, the model reaches good accuracies when half of the data set is used (1,000
pairwise rankings). This is encouraging, because for some applications (e.g., summa-
rization) large amounts of training data may be not readily available.
Table 7 illustrates the accuracy of the best performing model Coreference+
Syntax+Salience+ when trained on the Earthquakes corpus and tested on Accidents
texts and reversely when trained on the Accident corpus and tested on Earthquakes
documents. We also illustrate how this model performs when trained and tested on
a data set that contains texts from both domains. For the latter experiment the train-
ing data set was created by randomly sampling 50 Earthquakes and 50 Accidents
documents.
Table 7
Accuracy of entity-based model (Coreference+Syntax+Salience+) and HHM-based content
model on out-of-domain texts. Diacritics ** (p < .01) and * (p < .05) indicate whether
performances on in-domain and out-of-domain data are significantly different using a Fisher
Sign Test.
Coreference+Syntax+Salience






Train
Test Earthquakes Accidents
Earthquakes 87.3 67.0??
Accidents 69.7?? 90.4
EarthAccid 86.7 88.5?
HMM-Based Content Models






Train
Test Earthquakes Accidents
Earthquakes 88.0 31.7??
Accidents 60.3?? 75.8
19
Computational Linguistics Volume 34, Number 1
As can be seen from Table 7, the model?s performance degrades considerably
(approximately by 20%) when tested on out-of-domain texts. On the positive side,
the model?s out-of-domain performance is better than chance (i.e., 50%). Furthermore,
once the model is trained on data representative of both domains, it performs almost
as well as a model which has been trained exclusively on in-domain texts (see the
row EarthAccid in Table 7). To put these results into context, we also considered the
cross-domain performance of the content models. As Table 7 shows, the decrease in
performance is more dramatic for the content models. In fact, the model trained on
the Earthquakes domain plummets below the random baseline when applied to the
Accidents domain. These results are expected for content models?the two domains
have little overlap in topics and do not share structural constraints. Note that the LSA
model is not sensitive to cross-domain issues. The semantic space is constructed over
many different domains without taking into account style or writing conventions.
The cross-training performance of the entity-based models is somewhat puzzling:
these models are not lexicalized, and one would expect that valid entity transitions
are preserved across domains. Although transition types are not domain-specific, their
distribution could vary from one domain to another. To give a simple example, some
domains will have more entities than others (e.g., descriptive texts). In other words,
entity transitions capture not only text coherence properties, but also reflect stylistic
and genre-specific discourse properties. This hypothesis is indirectly confirmed by the
observed differences in the contribution of various linguistic features across the two
domains discussed above. Cross-domain differences in the distribution and occurrence
of entities have been also observed in other empirical studies of local coherence. For
instance, Poesio et al (2004) show differences in transition types between instructional
texts and descriptions of museum texts. In Section 6, we show that features derived
from the entity grid help determine the readability level for a given text, thereby
verifying more directly the hypothesis that the grid representation captures stylistic
discourse factors.
The results presented so far suggest that adapting the proposed model to a new
domain would involve some effort in collecting representative texts with associated
coherence ratings. Thankfully, the entity grids are constructed in a fully automatic
fashion, without requiring manual annotation. This contrasts with traditional imple-
mentations of Centering Theory that operate over linguistically richer representations
that are typically hand-coded.
5. Experiment 2: Summary Coherence Rating
We further test the ability of our method to assess coherence by comparing model
induced rankings against rankings elicited by human judges. Admittedly, the synthetic
data used in the ordering task only partially approximates coherence violations that
human readers encounter in machine generated texts. A representative example of
such texts are automatically generated summaries which often contain sentences taken
out of context and thus display problems with respect to local coherence (e.g., dan-
gling anaphors, thematically unrelated sentences). A model that exhibits high agree-
ment with human judges not only accurately captures the coherence properties of
the summaries in question, but ultimately holds promise for the automatic evaluation
of machine-generated texts. Existing automatic evaluation measures such as BLEU
(Papineni et al 2002) and ROUGE (Lin and Hovy 2003) are not designed for the
coherence assessment task, because they focus on content similarity between system
output and reference texts.
20
Barzilay and Lapata Modeling Local Coherence
5.1 Modeling
Summary coherence rating can be also formulated as a ranking learning task. We are
assuming that the learner has access to several summaries corresponding to the same
document or document cluster. Such summaries can be produced by several systems
that operate over identical inputs or by a single system (e.g., by varying the compression
length or by switching on or off individual system modules, for example a sentence
compression or anaphora resolution module). Similarly to the sentence ordering task,
our training data includes pairs of summaries (xij, xik) of the same document(s) di,
where xij is more coherent than xik. An optimal learner should return a ranking r
? that
orders the summaries according to their coherence. As in Experiment 1 we adopt an
optimization approach and follow the training regime put forward by Joachims (2002).
5.2 Method
Data. Our evaluation was based on materials from the Document Understanding Con-
ference (DUC 2003), which include multi-document summaries produced by human
writers and by automatic summarization systems. In order to learn a ranking, we
require a set of summaries, each of which has been rated in terms of coherence. One
stumbling block to performing this kind of evaluation is the coherence ratings them-
selves, which are not routinely provided byDUC summary evaluators. In DUC 2003, the
quality of automatically generated summaries was assessed along several dimensions
ranging from grammatically, to content selection, fluency, and readability. Coherence
was indirectly evaluated by noting the number of sentences indicating an awkward
time sequence, suggesting a wrong cause?effect relationship, or being semantically
incongruent with their neighboring sentences.8 Unfortunately, the observed coherence
violations were not fine-grained enough to be of use in our rating experiments. In
the majority of cases DUC evaluators noted either 0 or 1 violations; however, without
judging the coherence of the summary as a whole, we cannot know whether a single
violation disrupts coherence severely or not.
We therefore obtained judgments for automatically generated summaries from hu-
man subjects.9 We randomly selected 16 input document clusters and five systems that
had produced summaries for these sets, along with reference summaries composed by
humans. Coherence ratings were collected during an elicitation study by 177 unpaid
volunteers, all native speakers of English. The study was conducted remotely over the
Internet. Participants first saw a set of instructions that explained the task, and defined
the notion of coherence using multiple examples. The summaries were randomized in
lists following a Latin square design ensuring that no two summaries in a given list
were generated from the same document cluster. Participants were asked to use a seven-
point-scale to rate how coherent the summaries were without having seen the source
texts. The ratings (approximately 23 per summary) given by our subjects were averaged
to provide a rating between 1 and 7 for each summary.
The reliability of the collected judgments is crucial for our analysis; we therefore
performed several tests to validate the quality of the annotations. First, we measured
how well humans agree in their coherence assessment. We employed leave-one-out
8 See question 12 in http://duc.nist.gov/duc2003/quality.html.
9 The ratings are available from http://homepages.inf.ed.ac.uk/mlap/coherence/.
21
Computational Linguistics Volume 34, Number 1
resampling10 (Weiss and Kulikowski 1991), by correlating the data obtained from each
participant with the mean coherence ratings obtained from all other participants. The
inter-subject agreement was r = .768 (p < .01.) Second, we examined the effect of differ-
ent types of summaries (human- vs. machine-generated.) An ANOVA revealed a reliable
effect of summary type: F(1; 15) = 20.38, p < .01 indicating that human summaries are
perceived as significantly more coherent than system-generated ones. Finally, we also
compared the elicited ratings against the DUC evaluations using correlation analysis.
The human judgments were discretized to two classes (i.e., 0 or 1) using entropy-based
discretization (Witten and Frank 2000). We found a moderate correlation between the
human ratings and DUC coherence violations (r = .41, p < .01). This is expected given
that DUC evaluators were using a different scale and and were not explicitly assessing
summary coherence.
The summaries used in our rating elicitation study form the basis of a corpus
used for the development of our entity-based coherence models. To increase the size
of our training and test sets, we augmented the materials used in the elicitation study
with additional DUC summaries generated by humans for the same input sets. We
assumed that these summaries were maximally coherent. As mentioned previously, our
participants tend to rate human-authored summaries higher than machine-generated
ones. To ensure that we do not tune a model to a particular system, we used the output
summaries of distinct systems for training and testing. Our set of training materials
contained 6? 16 summaries (average length 4.8), yielding
(
6
2
)
? 16 = 240 pairwise rank-
ings. Because human summaries often have identical (high) scores, we eliminated pairs
of such summaries from the training set. Consequently, the resulting training corpus
consisted of 144 summaries. In a similar fashion, we obtained 80 pairwise rankings for
the test set. Six documents from the training data were used as a development set.
Features, Parameter Settings, and Training Requirements.We examine the influence of lin-
guistic knowledge on model performance by comparing models with varying degrees
of linguistic complexity. To be able to assess the performance of our models across tasks
(e.g., sentence ordering vs. summarization), we experimented with the same model
types introduced in the previous experiment (see Section 4.3). We also investigate the
training requirements for these models on the summary coherence task.
Experiment 1 differs from the present study in the way coreference information
was obtained. In Experiment 1, a coreference resolution tool was applied to human-
written texts, which are grammatical and coherent. Here, we apply a coreference tool
to automatically generated summaries. Because many summaries in our corpus are
fraught with coherence violations, the performance of a coreference resolution tool
is likely to drop. Unfortunately, resolving coreference in the input documents would
require a multi-document coreference tool, which is currently unavailable to us.
As in Experiment 1, the frequency threshold and the length of the transition se-
quence were optimized on the development set. Optimal salience-based models were
obtained for entities with frequency ?2. The optimal transition length was ?2. All
models were trained and tested using SVMlight (Joachims 2002).
Comparison with State-of-the-Art Methods. Our results were compared to the LSA model
introduced in Experiment 1 (see Section 4.2 for details). Unfortunately, we could not
10 We cannot apply the commonly used Kappa statistic for measuring agreement because it is appropriate
for nominal scales, whereas our summaries are rated on an ordinal scale.
22
Barzilay and Lapata Modeling Local Coherence
employ Barzilay and Lee?s (2004) content models for the summary ranking task. Being
domain-dependent, thesemodels require access to domain representative texts for train-
ing. Our summary corpus, however, contains texts frommultiple domains and does not
provide an appropriate sample for reliably training content models.
5.3 Results
Impact of Linguistic Representation. Our results are summarized in Table 8. Similarly
to the sentence ordering task, we observe that the linguistically impoverished model
Coreference?Syntax?Salience? exhibits decreased accuracy when compared against
models that operate overmore sophisticated representations. However, the contribution
of individual knowledge sources differs in this task. For instance, coreference resolu-
tion improved model performance in ordering, but it causes a decrease in accuracy
in summary evaluation (compare the models Coreference+Syntax+Salience+ and
Coreference?Syntax+Salience+ in Tables 5 and 8). This drop in performance can be
attributed to two factors both related to the fact that our summary corpus contains
many machine-generated texts. First, an automatic coreference resolution tool will be
expected to be less accurate on our corpus, because it was trained on well-formed
human-authored texts. Second, automatic summarization systems do not use anaphoric
expressions as often as humans do. Therefore, a simple entity clustering method is more
suitable for automatic summaries.
Both salience and syntactic information contribute to the accuracy of the ranking
model. The impact of each of these knowledge sources in isolation is not dramatic?
dropping either of them yields some decrease in accuracy, but the difference is not sta-
tistically significant. However, eliminating both salience and syntactic information sig-
nificantly decreases performance (compare Coreference?Syntax+Salience+ against
Coreference+Syntax?Salience? and Coreference?Syntax?Salience? in Table 8).
Figure 2 shows the learning curve for our best model Coreference?Syntax+
Salience+. Although the model performs poorly when trained on a small fraction of the
data, it stabilizes relatively fast (with 80 pairwise rankings), and does not improve after
Table 8
Summary ranking accuracy measured as fraction of correct pairwise rankings in the test set.
Coreference[+/?] indicates whether anaphoric information has been used when constructing
the entity grid. Similarly, Syntax[+/?] and Salience[+/?] reflect the use of syntactic and
salience information. Diacritics ** (p < .01) and * (p < .05) indicate whether Coreference?
Syntax+Salience+ is significantly different from all other models (using a Fisher Sign Test).
Model Accuracy
Coreference+Syntax+Salience+ 80.0
Coreference+Syntax+Salience? 75.0
Coreference+Syntax?Salience+ 78.8
Coreference?Syntax+Salience+ 83.8
Coreference+Syntax?Salience? 71.3?
Coreference?Syntax+Salience? 78.8
Coreference?Syntax?Salience+ 77.5
Coreference?Syntax?Salience? 73.8?
Latent Semantic Analysis 52.5??
23
Computational Linguistics Volume 34, Number 1
Figure 2
Learning curve for the entity-based model Coreference?Syntax+Salience+ applied to the
summary ranking task.
a certain point. These results suggest that further improvements to summary ranking
are unlikely to come from adding more annotated data.
Comparison with the State-of-the-Art. As in Experiment 1, we compared the best per-
forming grid model (Coreference?Syntax+Salience+) against LSA (see Table 8). The
formermodel significantly outperforms the latter (p < .01) by awidemargin. LSA is per-
haps at a disadvantage here because it has been exposed only to human-authored texts.
Machine-generated summaries are markedly distinct from human texts even when
these are incoherent (as in the case of our ordering experiment). For example, manual
inspection of our summary corpus revealed that low-quality summaries often contain
repetitive information. In such cases, simply knowing about high cross-sentential over-
lap is not sufficient to distinguish a repetitive summary from a well-formed one.
Furthermore, note that in contrast to the documents in Experiment 1, the summaries
being ranked here differ in lexical choice. Some are written by humans (and are thus
abstracts), whereas others have been produced by systems following different summa-
rization paradigms (some systems perform rewriting whereas others extract sentences
verbatim from the source documents). This means that LSA may consider a summary
coherent simply because its vocabulary is familiar (i.e., it contains words for which
reliable vectors have been obtained). Analogously, a summary with a large number
of out-of-vocabulary lexical items will be given low similarity scores, irrespective of
whether it is coherent or not. This is not uncommon in summaries with many proper
names. These often do not overlap with the proper names found in the North American
News Corpus used for training the LSA model. Lexical differences exert much less
influence on the entity-grid model which abstracts away from alternative verbalizations
of the same content and captures coherence solely on the basis of grid topology.
6. Experiment 3: Readability Assessment
So far, our experiments have explored the potential of the proposed discourse repre-
sentation for coherence modeling. We have presented several classes of grid models
24
Barzilay and Lapata Modeling Local Coherence
achieving good performance in discerning coherent from incoherent texts. Our experi-
ments also reveal a surprising property of grid models: Even though these models are
not lexicalized, they are domain- and style-dependent. In this section, we investigate in
detail this feature of grid models. Here, we move away from the coherence rating task
and put the entity-grid representation further to the test by examining whether it can
be usefully employed in style classification. Specifically, we embed our entity grids into
a system that assesses document readability. The term describes the ease with which
a document can be read and understood. The quantitative measurement of readability
has attracted considerable interest and debate over the last 70 years (see Mitchell [1985]
and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP
technology (Schwarm and Ostendorf 2005).
A number of readability formulas have been developed with the primary aim of
assessing whether texts or books are suitable for students at particular grade levels
or ages. Many readability methods focus on simple approximations of semantic factors
concerning the words used and syntactic factors concerning the length or structure of
sentences (Gunning 1952; Kincaid et al 1975; Chall and Dale 1995; Stenner 1996; Katz
and Bauer 2001). Despite their widespread applicability in education and technical
writing (Kincaid et al 1981), readability formulas are often criticized for being too
simplistic; they systematically ignoremany important factors that affect readability such
as discourse coherence and cohesion, layout and formatting, use of illustrations, the
nature of the topic, the characteristics of the readers, and so forth.
Schwarm and Ostendorf (2005) developed amethod for assessing readability which
addresses some of the shortcomings of previous approaches. By recasting readability
assessment as a classification task, they are able to combine several knowledge sources
ranging from traditional reading level measures, to statistical language models, and
syntactic analysis. Evaluation results show that their system outperforms two com-
monly used reading level measures (the Flesch-Kincaid Grade Level index and Lexile).
In the following we build on their approach and examine whether the entity-grid rep-
resentation introduced in this article contributes to the readability assessment task. The
incorporation of coherence-based information in the measurement of text readability is,
to our knowledge, novel.
6.1 Modeling
We follow Schwarm andOstendorf (2005) in treating readability assessment as a classifi-
cation task. The unit of classification is a single article and the learner?s task is to predict
whether it is easy or difficult to read. A variety of machine learning techniques are
amenable to this problem. Because our goal was to replicate Schwarm and Ostendorf?s
system as closely as possible, we followed their choice of support vector machines
(SVMs) (Joachims 1998b) for our classification experiments. Our training sample there-
fore consisted of n documents such that
(x1, y1), . . . , ( xn, yn) xi ? N, yi ? {?1,+1}
where xi is a feature vector for the ith document in the training sample and yi its
(positive or negative) class label. In the basic SVM framework, we try to separate the
positive and negative instances by a hyperplane. This means that there is a weight
25
Computational Linguistics Volume 34, Number 1
Table 9
Excerpts from the Britannica readability corpus
The Lemma Valletta in Britannica
Also spelled Valletta, seaport and capital of Malta, on the northeast coast of the island. The
nucleus of the city is built on the promontory of Mount Sceberras that runs like a tongue into
the middle of a bay, which it thus divides into two harbours, Grand Harbour to the east and
Marsamxett (Marsamuscetto) Harbour to the west. Built after the Great Siege of Malta in 1565,
which checked the advance of Ottoman power in southern Europe, it was named after Jean Parisot
de la Valette, grand master of the order of Hospitallers (Knights of St. John of Jerusalem), and
became the Maltese capital in 1570. The Hospitallers were driven out by the French in 1798, and
a Maltese revolt against the French garrison led to Valletta?s seizure by the British in 1800.
The Lemma Valletta in Britannica Elementary
A port city, Valletta is the capital of the island country of Malta in the Mediterranean Sea. Valletta
is located on the eastern coast of the largest island, which is also named Malta. Valletta lies on a
peninsula?a land mass surrounded by water on three sides. It borders Marsamxett Harbor to the
north and Grand Harbor to the south. The eastern end of the city juts out into the Mediterranean.
Valletta was planned in the 16th century by the Italian architect Francesco Laparelli. To make
traveling through Valletta easier, Laparelli designed the city in a grid pattern with straight streets
that crossed each other and ran the entire width and length of the town. Valletta was one of the
first towns to be laid out in this way.
vector w and a threshold b, so that all positive training examples are on one side of the
hyperplane, while all negative ones lie on the other side. This is equivalent to requiring
yi[(w ? xi)+ b] > 0
Finding the optimal hyperplane is an optimization problem which can be solved
efficiently using the procedure described in Vapnik (1998). SVMs have been widely
used for many NLP tasks ranging from text classification (Joachims 1998b), to syntactic
chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al
2005).
6.2 Method
Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003)
from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version
targeted at children. The corpus contains 107 articles from the full version of the encyclo-
pedia and their corresponding simplified articles from Britannica Elementary (214 articles
in total). Although these texts are not explicitly annotated with grade levels, they still
represent two broad readability categories, namely, easy and difficult.11 Examples of
these two categories are given in Table 9.
11 The Britannica corpus was also used by Schwarm and Ostendorf (2005); in addition they make use of a
corpus compiled from theWeekly Reader, an educational newspaper with documents targeted at grade
levels 2?5. Unfortunately, this corpus is not publicly available.
26
Barzilay and Lapata Modeling Local Coherence
Features and Parameter Settings.We created two system versions: the first one used solely
Schwarm and Ostendorf (2005) features;12 the second one employed a richer feature
space?we added the entity-based representation proposed here to their original feature
set. We will briefly describe the readability-related features used in our systems and
direct the interested reader to Schwarm and Ostendorf for a more detailed discussion.
Schwarm and Ostendorf (2005) use three broad classes of features: syntactic, se-
mantic, and their combination. Their syntactic features are average sentence length and
features extracted from parse trees computed using Charniak?s (2000) parser. The latter
include average parse tree height, average number of NPs, average number of VPs, and
average number of subordinate clauses (SBARs). We computed average sentence length
by measuring the number of tokens per sentence.
Their semantic features include the average number of syllables per word, and
language model perplexity scores. A unigram, bigram, and trigram model was esti-
mated for each class, and perplexity scores were used to assess their performance on
test data. Following Schwarm and Ostendorf (2005) we used information gain to select
words that were good class discriminants. All remaining words were replaced by their
parts of speech. The vocabulary thus consisted of 300 words with high information
gain and 36 Penn Treebank part-of-speech tags. The language models were estimated
using maximum likelihood estimation and smoothed with Witten-Bell discounting.
The language models described in this article were all built using the CMU statistical
language modeling toolkit (Clarkson and Rosenfeld 1997). Our perplexity scores were
six in total (2 classes ? 3 language models).
Finally, the Flesch-Kincaid Grade Level score was included as a feature that cap-
tures both syntactic and semantic text properties. The Flesch-Kincaid formula estimates
readability as a combination of the the average number of syllables per word and the
average number of words per sentence:
0.39
(
total words
total sentences
)
+ 11.8
(
total syllables
total words
)
? 15.59 (3)
We also enriched Schwarm and Ostendorf?s (2005) feature space with coherence-
based features. Each document was represented as a feature vector using the entity tran-
sition notation introduced in Section 3. We experimented with two models that yielded
good performances in our previous experiments: Coreference+Syntax+Salience+ (see
Experiment 1) and Coreference?Syntax+Salience+ (see Experiment 2). The transition
length was ?2 and entities were considered salient if they occurred ?2 times. As in our
previous experiments, we compared the entity-based representation against LSA. The
latter is a measure of the semantic relatedness across pairs of sentences. We could not
apply the HMM-based content models (Barzilay and Lee 2004) to the readability data
set. The encyclopedia lemmas are written by different authors and consequently vary
considerably in structure and vocabulary choice. Recall that these models are suitable
for more restricted domains and texts that are more formulaic in nature.
12 Schwarm and Ostendorf (2005) define out-of-vocabulary (OOV) scores relative to the most common
words in grade 2, the lowest grade level in their corpus; it was not possible to estimate OOV scores,
because we did not have access to grade 2 texts.
27
Computational Linguistics Volume 34, Number 1
Table 10
The contribution of coherence-based features to the automatic readability assessment task.
Diacritics ** (p < .01) and * (p < .05) indicate whether differences in accuracy between
Schwarm and Ostendorf and all other models are significant (using a Fisher Sign test).
Model Accuracy
Schwarm & Ostendorf 78.56
Schwarm & Ostendorf, Coreference+Syntax+Salience+ 88.79?
Schwarm & Ostendorf, Coreference?Syntax+Salience+ 79.49
Schwarm & Ostendorf, Latent Semantic Analysis 78.56
Coreference+Syntax+Salience+ 50.90??
Coreference?Syntax+Salience+ 49.55??
Latent Semantic Analysis 48.58??
The different systems were trained and tested on the Britannica corpus using five-
fold cross-validation.13 The languagemodels were created anew for every fold using the
documents in the training data. We use Joachims? (1998a) SVMlight package for training
and testing with all parameters set to their default values.
Evaluation Metric. We measure classification accuracy (i.e., the number of classes as-
signed correctly by the SVM over the size of the test set). We report accuracy averaged
over folds. A chance baseline (selecting one class at random) yields an accuracy of 50%.
Our training and test sets have the same number of documents for the two readability
categories.
6.3 Results
Table 10 summarizes our results on the readability assessment task. We first com-
pared Schwarm and Ostendorf?s (2005) system against a system that incorporates
entity-based coherence features (see rows 3?4 in Table 10). As can be seen, the sys-
tem?s accuracy significantly increases by 10% when the full feature set is included
(Coreference+Syntax+Salience+). Entity-grid features that do not incorporate corefer-
ence information (Coreference?Syntax+Salience+) perform numerically better (com-
pare row 1 and 3 in Table 10); however, the difference is not statistically significant.
The superior performance of the Coreference+Syntax+Salience+ feature set is not
entirely unexpected. Inspection of our corpus revealed that easy and difficult texts differ
in their distribution of pronouns and coreference chains in general. Easy texts tend to
employ less coreference and the use of personal pronouns is relatively sparse. To give
a concrete example, the pronoun they is attested 173 times in the difficult corpus and
only 73 in the easy corpus. This observation suggests that coreference information is a
good indicator of the level of reading difficulty and explains why its omission from the
entity-based feature space yields inferior performance.
13 The data for the experiments reported here can be found at http://homepages.inf.ed.ac.uk/
mlap/coherence/.
28
Barzilay and Lapata Modeling Local Coherence
Furthermore, note that discourse-level information is absent from Schwarm and
Ostendorf?s (2005) original model. The latter employs a large number of lexical and
syntactic features which capture sentential differences among documents. Our entity-
based representation supplements their feature space with information spanning two or
more successive sentences. We thus are able to model stylistic differences in readability
that go beyond syntax and lexical choice. Besides coreference, our feature representa-
tion captures important information about the presence and distribution of entities in
discourse. For example, difficult texts tend to have twice as many entities as easy ones.
Consequently, easy and difficult texts are represented by entity transition sequences
with different probabilities (e.g., the sequences [S S] and [S O] are more probable in
difficult texts). Interestingly, when coherence is quantified using LSA, we observe no
improvement to the classification task. The LSA scores capture lexical or semantic text
properties similar to those expressed by the Flesch Kincaid index and the perplexity
scores (e.g., word repetition). It is therefore not surprising that their inclusion in the
feature set does not increase performance.
We also evaluated the training requirements for the readability system described
herein. Figure 3 shows the learning curve for Schwarm and Ostendorf?s (2005) model
enhanced with the Coreference+Syntax+Salience+ feature space and on its own. As
can be seen, both models perform relatively well when trained on small data sets
(e.g., 20?40 documents) and reach peak accuracy with half of the training data. The
inclusion of discourse-based features consistently increases accuracy irrespective of the
amount of training data available. Figure 3 thus suggests that better feature engineering
is likely to bring further performance improvements on the readability task.
Our results indicate that the entity-based text representation introduced here cap-
tures aspects of text readability and can be successfully incorporated into a practical
system. Coherence is by no means the sole predictor of readability. In fact, on its own,
it performs poorly on this task as demonstrated when using either LSA or the entity-
based feature space without Schwarm and Ostendorf?s (2005) features (see rows 5?7 in
Table 10). Rather, we claim that coherence is one among many factors contributing to
text readability and that our entity-grid representation is well-suited for text classifica-
tion tasks such as reading level assessment.
Figure 3
Learning curve for Schwarm and Ostendorf?s (2005) model on its own and enhanced with the
Coreference+Syntax+Salience+ feature space.
29
Computational Linguistics Volume 34, Number 1
7. Discussion and Conclusions
In this article we proposed a novel framework for representing and measuring text co-
herence. Central to this framework is the entity-grid representation of discourse, which
we argue captures important patterns of sentence transitions. We re-conceptualize co-
herence assessment as a learning task and show that our entity-based representation
is well-suited for ranking-based generation and text classification tasks. Using the
proposed representation, we achieve good performance on text ordering, summary
coherence evaluation, and readability assessment.
The entity grid is a flexible, yet computationally tractable, representation. We
investigated three important parameters for grid construction: the computation of
coreferring entity classes, the inclusion of syntactic knowledge, and the influence of
salience. All these knowledge sources figure prominently in theories of discourse
(see Section 2) and are considered important in determining coherence. Our results
empirically validate the importance of salience and syntactic information (expressed
by S, O, X, and ?) for coherence-based models. The combination of both knowledge
sources (Syntax+Salience) yields models with consistently good performance for all
our tasks.
The benefits of full coreference resolution are less uniform. This is partly due to
mismatches between training and testing conditions. The system we employ (Ng and
Cardie 2002) was trained on human-authored newspaper texts. The corpora we used
in our sentence ordering and readability assessment experiments are somewhat similar
(i.e., human-authored narratives), whereas our summary coherence rating experiment
employed machine generated texts. It is therefore not surprising that coreference reso-
lution delivers performance gains on the first two tasks but not on the latter (see Table 5
in Section 4 and Table 10 in Section 6.3). Our results further show that in lieu of an
automatic coreference resolution system, entity classes can be approximated simply
by string matching. The latter is a good indicator of nominal coreference; it is often
included as a feature in machine learning approaches to coreference resolution (Soon,
Ng, and Lim 2001; Ng and Cardie 2002) and is relatively robust (i.e., likely to deliver
consistent results in the face of different domains and genres).
It is important to note that, although inspired by entity-based theories of discourse
coherence, our approach is not a direct implementation of any theory in particular.
Rather, we sacrifice linguistic faithfulness for automatic computation and breadth of
coverage. Despite approximations and unavoidable errors (e.g., in the parser?s output),
our results indicate that entity grids are a useful representational framework across
tasks and text genres. In agreement with Poesio et al (2004) we find that pronomi-
nalization is a good indicator of document coherence. We also find that coherent texts
are characterized by transitions with particular properties which do not hold for all
discourses. Contrary to Centering Theory, we remain agnostic to the type of transi-
tions that our models capture (e.g., CONTINUE, SHIFT). We simply record whether an
entity is mentioned in the discourse and in what grammatical role. Our experiments
quantitatively measured the predictive power of various linguistic features for several
coherence-related tasks. Crucially, we find that ourmodels are sensitive to the domain at
hand and the type of texts under consideration (human-authored vs. machine generated
texts). This is an unavoidable consequence of the grid representation, which is entity-
specific. Differences in entity distribution indicate not only differences in coherence, but
also in writing conventions and style. Similar observations have been made in other
work which is closer in spirit to Centering?s claims (Hasler 2004; Karamanis et al 2004;
Poesio et al 2004).
30
Barzilay and Lapata Modeling Local Coherence
An important future direction lies in augmenting our entity-based representation
with more fine-grained lexico-semantic knowledge. One way to achieve this goal is to
cluster entities based on their semantic relatedness, thereby creating a grid represen-
tation over lexical chains (Morris and Hirst 1991). An entirely different approach is to
develop fully lexicalized models, akin to traditional language models. Cache language
models (Kuhn and De Mori 1990) seem particularly promising in this context. The
granularity of syntactic information is another topic that warrants further investigation.
So far we have only considered the contribution of ?core? grammatical relations to
the grid construction. Expanding our grammatical categories to modifiers and adjuncts
may provide additional information, in particular when consideringmachine generated
texts. We also plan to investigate whether the proposed discourse representation and
modeling approaches generalize across different languages. For instance the identifi-
cation and extraction of entities poses additional challenges in grid construction for
Chinese where word boundaries are not denoted orthographically (by space). Similar
challenges arise in German, a language with a large number of inflected forms and
productive derivational processes (e.g., compounding) not indicated by orthography.
In the discourse literature, entity-based theories are primarily applied at the level
of local coherence, while relational models, such as Rhetorical Structure Theory (Mann
and Thomson 1988; Marcu 2000), are used to model the global structure of discourse.
We plan to investigate how to combine the two for improved prediction on both local
and global levels, with the ultimate goal of handling longer texts.
Acknowledgments
The authors acknowledge the support of
the National Science Foundation (Barzilay;
CAREER grant IIS-0448168 and grant
IIS-0415865) and EPSRC (Lapata; grant
GR/T04540/01). We are grateful to Claire
Cardie and Vincent Ng for providing us
the results of their coreference system
on our data. Thanks to Eli Barzilay, Eugene
Charniak, Michael Elhadad, Noemie
Elhadad, Nikiforos Karamanis, Frank Keller,
Alex Lascarides, Igor Malioutov, Smaranda
Muresan, Martin Rinard, Kevin Simler,
Caroline Sporleder, Chao Wang, Bonnie
Webber, and three anonymous reviewers
for helpful comments and suggestions.
Any opinions, findings, and conclusions or
recommendations expressed herein are those
of the authors and do not necessarily reflect
the views of the National Science Foundation
or EPSRC.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of the
42nd Annual Meeting of the Association for
Computational Linguistics, pages 399?406,
Barcelona, Spain.
Ariel, Mira. 1988. Referring and accessibility.
Journal of Linguistics, 24:65?87.
Asher, Nicholas and Alex Lascarides. 2003.
Logics of Conversation. Cambridge
University Press, Cambridge, England.
Barzilay, Regina. 2003. Information Fusion
for Multi-Document Summarization:
Praphrasing and Generation. Ph.D. thesis,
Columbia University, New York.
Barzilay, Regina and Noemie Elhadad. 2003.
Sentence alignment for monolingual
comparable corpora. In Proceedings of the
8th Conference on Empirical Methods in
Natural Language Processing, pages 25?32,
Sapporo, Japan.
Barzilay, Regina and Mirella Lapata. 2005.
Modeling local coherence: An entity-based
approach. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 141?148, Ann Arbor, MI.
Barzilay, Regina and Lillian Lee. 2004.
Catching the drift: Probabilistic content
models, with applications to generation
and summarization. In Proceedings of the
2nd Human Language Technology Conference
and Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 113?120, Boston, MA.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O?Brien. 1994. Using linear
algebra for intelligent information
retrieval. SIAM Review, 37(4):573?595.
Brennan, Susan E., Marilyn W. Friedman,
and Charles J. Pollard. 1987. A centering
approach to pronouns. In Proceedings of the
31
Computational Linguistics Volume 34, Number 1
25th Annual Meeting of the Association for
Computational Linguistics, pages 155?162,
Palo Alto, CA.
Briscoe, Ted and John Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 1499?1504, Las Palmas,
Canary Islands.
Chafe, Wallace L. 1976. Givenness,
contrastiveness, definiteness, subjects,
topics, and point of view. In Charles N. Li,
editor, Subject and Topic. Academic Press,
New York, pages 25?55.
Chall, Jeanne S. 1958. Readability: An
Appraisal of Research and Application.
Number 34 in Bureau of Educational
Research Monographs. Ohio State
University Press, Columbus.
Chall, Jeanne S. and Edgar Dale. 1995.
Readability Revisited: The New Dale-Chall
Readability Formula. Brookline Books,
Cambridge, MA.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 132?139,
Seattle, WA.
Clark, Herbert H. and Susan E. Haviland.
1977. Comprehension and the given-new
contract. In Roy O. Freedle, editor,
Discourse Production and Comprehension.
Ablex, Norwood, NJ, pages 1?39.
Clarkson, Philip and Ronald Rosenfeld.
1997. Statistical language modeling.
In Proceedings of ESCA EuroSpeech?97,
pages 2707?2710, Rhodes, Greece.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the Association for Computational
Linguistics and 8th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 16?23,
Madrid, Spain.
Collins, Michael. 2002. Discriminative
reranking for natural language parsing.
In Proceedings of the 17th International
Conference on Machine Learning,
pages 175?182, Palo Alto, CA.
Foltz, Peter W., Walter Kintsch, and
Thomas K. Landauer. 1998. Textual
coherence using latent semantic analysis.
Discourse Processes, 25(2&3):285?307.
Freund, Yovav, Raj Iyer, Robert E. Schapire,
and Yoram Singer. 2003. An efficient
boosting algorithm for combining
preferencs.Machine Learning, 4:933?969.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2004. Fast and optimal decoding for
machine translation. Artificial Intelligence,
154(1?2):127?143.
Givon, Talmy. 1987. Beyond foreground and
background. In Russell S. Tomlin, editor,
Coherence and Grounding in Discourse.
Benjamins, Amsterdam/Philadelphia,
pages 175?188.
Grosz, Barbara, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Gundel, Jaenette K., Nancy Hedberg, and
Ron Zacharski. 1993. Cognitive status
and the form of referring expressions
in discourse. Language, 69(2):274?307.
Gunning, Robert. 1952. The Technique of Clear
Writing. McGraw Hill, New York.
Halliday, M. A. K. and Ruqaiya Hasan. 1976.
Cohesion in English. Longman, London.
Hasler, Laura. 2004. An investigation into
the use of centering transitions for
summarisation. In Proceedings of the
7th Annual CLUK Research Colloquium,
pages 100?107, Birmingham, UK.
Hoey, Michael. 1991. Patterns of Lexis in Text.
Oxford University Press, Oxford, England.
Hudson, S. B., M. K. Tanenhaus, and G. S.
Dell. 1986. The effect of the discourse
center on the local coherence of a
discourse. In Proceedings of the 8th Annual
Meeting of the Cognitive Science Society,
pages 96?101, Amherst, MA.
Joachims, Thorsten. 1998a. Making
large-scale support vector machine
learning practical. In Bernard Scho?lkopf,
Christopher Burges, and Alexander Smola,
editors, Advances in Kernel Methods:
Support Vector Machines. MIT Press,
Cambridge, MA.
Joachims, Thorsten. 1998b. Text
categorization with support vector
machines: Learning with many relevant
features. In Proceedings of the European
Conference on Machine Learning,
pages 137?142, Berlin, Springer.
Joachims, Thorsten. 2002. Optimizing search
engines using clickthrough data. In
Proceedings of ACM Conference on Knowledge
Discovery and Data Mining, pages 133?142,
Chicago, IL.
Kameyama, Megumi. 1986. A
property-sharing constraint in centering.
In Proceedings of the 24th Annual Meeting of
the Association for Computational Linguistics,
pages 200?206, New York.
32
Barzilay and Lapata Modeling Local Coherence
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, University of Edinburgh,
Edinburgh, Scotland.
Karamanis, Nikiforos, Massimo Poesio,
Chris Mellish, and Jon Oberlander.
2004. Evaluating centering-based
metrics of coherence for text structuring
using a reliably annotated corpus. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 391?398, Barcelona, Spain.
Karttunen, Lauri. 1976. Discourse referents.
In James D. McCawley, editor, Syntax
and Semantics: Notes from the Linguistic
Underground, volume 7. Academic Press,
New York, pages 363?386.
Katz, Irvin R. and Malcolm I. Bauer. 2001.
Sourcefinder: Course preparation via
linguistically targeted web search. Journal
of Educational Technology and Society,
4(3):45?49.
Kibble, Rodger and Richard Power. 2004.
Optimising referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Kincaid, J. Peter, James Aagard, John O?Hara,
and Larry Cottrell. 1981. Computer
readability editing system. IEEE
Transactions on Professional Communication,
1(24):34?81.
Kincaid, Peter J., Robert P. Fishburne,
Richard L. Rodgers, and Brad S. Chissom.
1975. Derivation of new readability
formulas for Navy enlisted personnel.
Research Branch Report 8-75, U.S.
Naval Air Station, Memphis, TN.
Knight, Kevin and Vasileios Hatzivassiloglou.
1995. Two-level, many-path generation. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
pages 252?260, Cambridge, MA.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Thorsten Joachims, editor, Proceedings
of the 2nd Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 192?199,
Pittsburgh, PA.
Kuhn, R. and R. De Mori. 1990. A
cache-based natural language model for
speech recognition. IEEE Transactions on
PAMI, 12(6):570?583.
Kuno, Susumu. 1972. Functional sentence
perspective. Linguistic Inquiry, 3:269?320.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato?s problem:
The latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211?240.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings
of the 17th International Conference on
Computational Linguistics and 36th
Annual Meeting of the Association for
Computational Linguistics, pages 704?710,
Montre?al, Canada.
Lapata, Mirella. 2003. Probabilistic text
structuring: Experiments with sentence
ordering. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics, pages 545?552, Sapporo, Japan.
Lin, Chin-Yew and Eduard H. Hovy. 2003.
Automatic evaluation of summaries using
n-gram co-occurrence statistics. In
Proceedings of the 2nd Human Language
Technology Conference and Annual Meeting of
the North American Chapter of the Association
for Computational Linguistics, pages 71?78,
Boston, MA.
Lin, Dekang. 2001. LaTaT: Language and
text analysis tools. In Proceedings of the 1st
International Conference on Human Language
Technology Research, pages 222?227,
San Francisco, CA.
Mann, William C. and Sandra A. Thomson.
1988. Rhetorical structure theory. Text,
8(3):243?281.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
MIT Press, Cambridge, MA.
McKoon, Gail and Roger Ratcliff. 1992.
Inference during reading. Psychological
Review, 99(3):440?446.
Mellish, Chris, Mick O?Donnell, Jon
Oberlander, and Alistair Knott. 1998.
Experiments using stochastic search
for text planning. In Proceedings of the
9th International Workshop on Natural
Language Generation, pages 98?107,
New Brunswick, NJ.
Miltsakaki, Eleni and Karen Kukich. 2000.
The role of centering theory?s rough-shift
in the teaching and evaluation of writing
skills. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 408?415, Hong Kong.
Mitchell, James V. 1985. The Ninth Mental
Measurements Yearbook. University of
Nebraska Press, Lincoln.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 1(17):21?43.
Ng, Vincent and Claire Cardie. 2002.
Improving machine learning approaches
33
Computational Linguistics Volume 34, Number 1
to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
Philadelphia, PA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: A parametric theory and
its instantiations. Computational Linguistics,
30(3):309?363.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Dan Jurafsky. 2005. Support vector
learning for semantic argument
classification.Machine Learning,
60(1):11?39.
Prince, Ellen. 1978. A comparison of wh-clefts
and it-clefts in discourse. Language,
54:883?906.
Prince, Ellen. 1981. Toward a taxonomy of
given-new information. In Peter Cole,
editor, Radical Pragmatics. Academic Press,
New York, pages 223?255.
Reiter, Ehud and Robert Dale. 2000.
Building Natural-Language Generation
Systems. Cambridge University Press,
Cambridge, England.
Schwarm, Sarah E. and Mari Ostendorf. 2005.
Reading level assessment using support
vector machines and statistical language
models. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 523?530, Ann Arbor, MI.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press,
New York, pages 47?73.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English Discourse. Ph.D.
thesis, MIT.
Soon, W. M., Hwee Tou Ng, and D. C. Y. Lim.
2001. A machine learning approach to
coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Stenner, A. Jackson. 1996. Measuring
reading comprehension with the
lexile framework. Presented at the
California Comparability Symposium,
Burlingame, CA.
Strube, Michael and Udo Hahn. 1999.
Functional centering?Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Toutanova, Kristina, Penka Markova, and
Christopher D. Manning. 2004. The leaf
projection path view of parse trees:
Exploring string kernels for HPSG
parse selection. In Proceedings of the
Conference on Empirical Methods
in Natural Language Processing,
pages 166?173, Barcelona, Spain.
Vapnik, Vladimir. 1998. Statistical Learning
Theory. Wiley, Chichester, UK.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of the 6th
Message Understanding Conference (MUC-6),
pages 45?52, San Francisco, CA.
Walker, Marilyn, Masayo Iida, and Sharon
Cote. 1994. Japanese discourse and the
process of centering. Computational
Linguistics, 20(2):193?232.
Walker, Marilyn, Aravind Joshi, and
Ellen Prince, editors. 1998. Centering
Theory in Discourse. Clarendon Press,
Oxford, UK.
Walker, Marilyn A., Owen Rambow, and
Monica Rogati. 2001. Spot: A trainable
sentence planner. In Proceedings of the
2nd Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 17?24, Pittsburgh, PA.
Weiss, Sholom M. and Casimir A.
Kulikowski. 1991. Computer Systems that
Learn: Classification and Prediction Methods
from, Statistics, Neural Nets, Machine
Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
Witten, Ian H. and Eibe Frank. 2000. Data
Mining: Practical Machine Learning Tools
and Techniques with Java Implementations.
Morgan Kaufman, San Mateo, CA.
34
Constructing Corpora for the Development
and Evaluation of Paraphrase Systems
Trevor Cohn?
University of Edinburgh
Chris Callison-Burch??
Johns Hopkins University
Mirella Lapata?
University of Edinburgh
Automatic paraphrasing is an important component in many natural language processing tasks.
In this article we present a new parallel corpus with paraphrase annotations. We adopt a defini-
tion of paraphrase based on word alignments and show that it yields high inter-annotator agree-
ment. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is
appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed
in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1)
and also in developing linguistically rich paraphrase models based on syntactic structure.
1. Introduction
The ability to paraphrase text automatically carries much practical import for many
NLP applications ranging from summarization (Barzilay 2003; Zhou et al 2006) to
question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine
translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising
that recent years have witnessed increasing interest in the acquisition of paraphrases
from real world corpora. These are most often monolingual corpora containing parallel
translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and
Marcu 2003). Truly bilingual corpora consisting of documents and their translations have
also been used to acquire paraphrases (Bannard and Callison-Burch 2005; Callison-
Burch 2007) as well as comparable corpora such as collections of articles produced
by two different newswire agencies about the same events (Barzilay and Elhadad
2003).
Although paraphrase induction algorithms differ in many respects?for example,
the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle)
or structural (last week?s fighting, the battle last week), and are represented as words or
? School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk.
?? Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218.
E-mail: ccb@cs.jhu.edu.
? School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for
publication: 26 March 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
syntax trees?they all rely on some form of alignment for extracting paraphrase pairs.
In its simplest form, the alignment can range over individual words, as is often done
in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments
range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay
and Lee 2003).
The obtained paraphrases are typically evaluated via human judgments. Para-
phrase pairs are presented to judges who are asked to decide whether they are seman-
tically equivalent, that is, whether they can be generally substituted for one another in
the same context without great information loss (Barzilay and Lee 2003; Barzilay and
McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In
some cases the automatically acquired paraphrases are compared against manually gen-
erated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance
increase for a specific application, such as machine translation (Callison-Burch, Koehn,
and Osborne 2006).
Unfortunately, manually evaluating paraphrases in this way has at least three draw-
backs. First, it is infeasible to perform frequent evaluations when assessing incremental
system changes or tuning system parameters. Second, it is difficult to replicate results
presented in previous work because there is no standard corpus, and no standard evalu-
ation methodology. Consequently comparisons across systems are few and far between.
The third drawback concerns the evaluation studies themselves, which primarily focus
on precision. Recall is almost never evaluated directly in the literature. And this is
for a good reason: There is no guarantee that participants will identify the same set
of paraphrases as each other or with a computational model. The problem relates to
the nature of the paraphrasing task, which has so far eluded formal definition (see
the discussion in Barzilay [2003]). Such a definition is not so crucial when assessing
precision, because subjects are asked to rate the paraphrases without actually having to
identify them. However, recall might be measured with respect to some set of ?gold-
standard? paraphrases which will have to be collected according to some concrete
definition.
In this article we present a resource that could potentially be used to address
these problems. Specifically, we create a monolingual parallel corpus with human
paraphrase annotations. Our working definition of paraphrase is based on word and
phrase1 alignments between semantically equivalent sentences. Other definitions are
possible, for instance we could have asked our annotators to identify all constituents
that are more or less meaning preserving in our parallel corpus. We chose to work
with alignments for two reasons. First, the notion of alignment appears to be central
in paraphrasing?most existing paraphrase induction algorithms rely on alignments
either implicitly or explicitly for identifying paraphrase units. Secondly, research in
machine translation, where several gold-standard alignment corpora have been created,
shows that word alignments can be identified reliably by annotators (Melamed 1998;
Och andNey 2000b;Mihalcea and Pedersen 2003;Martin,Mihalcea, and Pedersen 2005).
We therefore create word alignments similar to those observed in machine transla-
tion, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links
between words. Alignment blocks larger than one-to-one are used to specify phrase
correspondences.
1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of
words, whether it is a syntactic constituent or not. See Section 2 for details.
598
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
In the following section we explain how our corpus was created and summarize our
annotation guidelines. Section 3 gives the details of an agreement study, demonstrating
that our annotators can identify and align paraphrases reliably. We measure agreement
using alignment overlap measures from the SMT literature, and also introduce a novel
agreement statistic for non-enumerable labeling spaces. Section 4 illustrates how the
corpus can be used in paraphrase research, for example, as a test set for evaluating
the output of automatic systems or as a training set for the development of paraphrase
systems. Discussion of our results concludes the article.
2. Corpus Creation and Annotation
Our corpus was compiled from three data sources that have been previously used for
paraphrase induction (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003;
Dolan, Quirk, and Brockett 2004): the Multiple-Translation Chinese (MTC) corpus,
Jules Verne?s Twenty Thousand Leagues Under the Sea novel (Leagues), and the Microsoft
Research (MSR) paraphrase corpus. These are monolingual parallel corpora, aligned at
the sentence level. Both source and target sentences are in English, and express the same
content using different surface forms.
The MTC corpus contains news stories from three sources of journalistic Mandarin
Chinese text.2 These stories were translated into English by 11 translation agencies.
Because the majority of the translators were non-native English speakers, occasionally
translations contain syntactic or grammatical errors and are not entirely fluent. After
inspection, we identified four translators with consistently fluent English, and used
their sentences for our corpus. The Leagues corpus contains two English translations
of the French novel Twenty Thousand Leagues Under the Sea. The corpus was created
by Tagyoung Chung and manually aligned at the paragraph level.3 In order to obtain
sentence level paraphrase pairs, we sampled from the subset of one-to-one sentence
alignments. The MSR corpus was harvested automatically from online news sources.4
The obtained sentence pairs were further submitted to judges who rated them as being
semantically equivalent or not (Dolan, Quirk, and Brockett 2004). We only used seman-
tically equivalent pairs. The sentence pairs were filtered for length (? 50) and length
ratio (? 1 : 9 between the shorter and longer sentence). This was necessary to prune out
incorrectly aligned sentences.
We randomly sampled 300 sentence pairs from each corpus (900 in total). Of these,
300 pairs (100 per corpus) were first annotated by two coders to assess inter-annotator
agreement. The remaining 600 sentence pairs were split into two distinct sets, each
consisting of 300 sentences (100 per corpus), and were annotated by a single coder.
Each coder annotated the same amount of data. In addition, we obtained a trial set
of 50 sentences from the MTC corpus which was used for familiarizing our annotators
with the paraphrase alignment task (this set does not form part of the corpus). In sum,
we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly
annotated.
To speed up the annotation process, the data sources were first aligned automati-
cally and then hand-corrected.We usedGiza++ (Och andNey 2003), a publicly available
2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1.
3 The corpus can be downloaded from http://www.isi.edu/?knight/.
4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D9-
20CD-47E3-85BC-A2F65CD28042/Details.aspx.
599
Computational Linguistics Volume 34, Number 4
implementation of the IBM word alignment models (Brown et al 1993). Giza++ was
trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair-
ings of English translations as training instances. This resulted in 55 =
11?(11?1)
2 training
pairs per sentence and a total of 54, 615 training pairs. In addition, we augmented the
training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan
(2004). This follows standard practice in SMT where entries from a bilingual dictionary
are added to the training set (Och and Ney 2000a), except in our case the ?dictionary?
is monolingual and specifies that each word type can be paraphrased as itself. This is
necessary in order to inform Giza++ about word identity.
A common problem with automatic word alignments is that they are asymmetric:
one source word can only be aligned to one target word, whereas one target word can
be aligned to multiple source words. In SMT, word alignments are typically predicted
in both directions: source-to-target and target-to-source. These two alignments are then
merged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).
Symmetrization improves the alignment quality compared to that of a single directional
model, while also allowing a greater range of alignment types (i.e., some many-to-
one, one-to-many, and many-to-many alignments can be produced). Analogously, we
obtained word alignments in both directions6 which we subsequently merged by taking
their intersection. This resulted in a high precision and low recall alignment.
Our annotators (two linguistics graduates) were given pairs of sentences and asked
to show which parts of these were in correspondence by aligning them on a word-by-
word basis.7 Our definition of alignment was fairly general (Och andNey 2003): Given a
source string X = x1, . . . , xN and a target string Y = y1, . . . , yM, an alignmentA between
two word strings is the subset of the Cartesian product of the word positions:
A ? {(n,m) : n = 1, . . . ,N;m = 1, . . . ,M} (1)
We did not provide a formal definition of what constitutes a correspondence. As a
rule of thumb, annotators were told to align words or phrases x ? y in two sentences
(X,Y) whenever the words x could be substituted for y in Y, or vice versa. This relation-
ship should hold within the context of the sentence pair in question: the relation x ? y
need not hold in general contexts. Trivially this definition allowed for identical word
pairs.
Following common practice (Och, Tillmann, and Ney 1999; Och and Ney 2003;
Daume? III and Marcu 2004), we distinguished between sure (S) and possible (P) align-
ments, where S ? P. The intuition here is that sure alignments are clear-cut decisions
and typical of genuinely substitutable words or phrases, whereas possible alignments
flag a correspondence that has slightly divergent syntax or semantics. Annotators were
encouraged to produce sure alignments. They were also instructed to prefer smaller
alignments whenever possible, but were allowed to create larger block alignments.
Smaller alignments were generally used to indicate lexical correspondences, whereas
block alignments were reserved for non-compositional phrase pairs (e.g., idiomatic
expressions) or simply expressions with radically different syntax or vocabulary. In
5 The IBM alignment models require a large amount of parallel data to yield reliable alignments. We
therefore selected the MTC for training purposes as it was the largest of our parallel corpora.
6 We used five iterations for each of Model 1, Model 2, and the HMMmodel.
7 The annotation was conducted using a Web-based alignment tool available at
http://demo.linearb.co.uk/paraphrases/.
600
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Figure 1
Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid. Black
squares represent sure alignment, gray squares represent possible alignment.
cases where information in one sentence was not present in the other, the annotators
were asked to leave this information unaligned.
Finally, annotators were given a list of heuristics to help them decide how to
make alignments in cases of ambiguity. These heuristics handled the alignment of
named entities (e.g., George Bush) and definite descriptions (e.g., the president), tenses
(e.g., had been and shall be), noun phrases with mismatching determiners (e.g., a man
and the man), verb complexes (e.g., was developed and had been developed), phrasal
verbs (e.g., take up and accept), genitives (e.g., Bush?s infrequent speeches and the infre-
quent speeches by Bush), pronouns, repetitions, typographic errors, and approximate
correspondences. For more details, we refer the interested reader to our annotation
guidelines.8
Figure 1 shows the alignment for two sentence pairs from the MTC corpus. The
first pair (Australia is concerned with the issue of carbon dioxide emissions. ? The problem
of greenhouse gases has attracted Australia?s attention.) contains examples of word-to-
word (the ? The; issue ? problem; of ? of ; Australia ? Australia) and many-to-many
alignments (carbon dioxide emissions ? greenhouse gases). Importantly, we do not use
a large many-to-many block for Australia is concerned with and has attracted Australia?s
attention because it is possible to decompose the two phrases into smaller alignments.
The second sentence pair illustrates a possible alignment (could have very long term
effects? was of profound significance) indicated by the gray squares. Possible alignments
are used here because the two phrases only loosely correspond to each other. Possible
alignments are also used to mark significant changes in syntax where the words denote
a similar concept: for example, in cases where two words have the same stem but are
8 Both the corpus and the annotation guidelines can be found at: http://homepages.inf.ed.ac.uk/
tcohn/paraphrase corpus.html.
601
Computational Linguistics Volume 34, Number 4
expressed with different parts of speech, (e.g., co-operative ? cooperation) or when two
verbs are used that are not synonyms (e.g., this is also? this also marks).
3. Human Agreement
As mentioned in the previous section, 300 sentence pairs (100 pairs from each sub-
corpus) were doubly annotated, in order to measure inter-annotator agreement. Here,
we treat one annotator as gold-standard (reference) andmeasure the extent to which the
other annotator deviates from this reference.
Word-Based Measures. The standard technique for evaluating word alignments is to
represent them as a set of links (i.e., pairs of words) and compare them against gold-
standard alignments. The quality of an alignmentA (defined in Equation (1)) compared
to reference alignment B can be then computed using standard recall, precision, and
F1 measures (Och and Ney 2003):
Precision =
|AS ? BP|
|AS|
Recall =
|AP ? BS|
|BS|
F1 = 2 ? Precision ? Recall
Precision+ Recall
(2)
where the subscripts S and P denote sure and possible word alignments, respectively.
Note that both precision and recall are asymmetric in that they compare sets of possible
and sure alignments. This is designed to be maximally generous: sure predictions
which are present in the reference as possibles are not penalized in precision, and the
converse applies for recall. We adopt Fraser and Marcu (2007)?s definition of F1, an
F-measure between precision and recall over the sure and possibles. They argue that
it is a better alternative to the commonly used Alignment Error Rate (AER), which
does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-
lingual, in order to avoid artificial score inflation, we limit the precision and recall
calculations to consider only pairs of non-identical words (and phrases, as discussed
subsequently).
To give an example, consider the sentence pairs in Figure 2, whose alignments have
been produced by the two annotators A (left) and B (right). Table 1 shows the individual
word alignments for each annotator and their type (sure or possible). In order to mea-
sure F1, we must first estimate Precision and Recall (see Equation (2)). Treating annota-
tor B as the gold standard, |AS| = 4, |BS| = 5, |AS ? BP| = 4, and |AP ? BS| = 4. This
results in a precision of 44 = 1, a recall of
4
5 , and F1 of
2?1?0.8
1+0.8 = 0.89. Note that we ignore
alignments over identical words (i.e., discussed ? discussed, the ? the, and ? and,
. ? .).
Phrase-Based Measures. The given definitions are all word-based; however, our annota-
tors, and several paraphrasing models, create correspondences not only between words
but also between phrases. To take this into account, we also evaluate these measures
over larger blocks (similar to Ayan and Dorr [2006]). Specifically, we extract phrase
pairs from the alignments produced by our annotators using a modified version of the
standard SMT phrase extraction heuristic (Och, Tillmann, and Ney 1999). The heuristic
9 Fraser and Marcu (2007) also argue for an unbalanced F-measure to bias towards recall. This is shown to
correlate better with translation quality. For paraphrasing it is not clear if such a bias would be beneficial.
602
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Figure 2
Sample sentence pair showing the word alignments from two annotators.
extracts all phrase pairs consistent with the word alignment. These include phrase pairs
whose words are aligned to each other or nothing, but not to words outside the phrase
boundaries.10 The phrase extraction heuristic creates masses of phrase pairs, many of
which are of dubious quality. This is often due to the inclusion of unaligned words or
simply to the extraction of overly-large phrase pairs which might be better decomposed
into smaller units. For our purposes we wish to be maximally conservative in how we
process the data, and therefore we do not extract phrase pairs with unaligned words on
their boundaries.
Figure 3 illustrates the types of phrase pairs our extraction heuristic permits. Here,
the pair and reached ? and arrived at is consistent with the word alignment. In contrast,
the pair and reached ? and arrived isn?t; there is an alignment outside the hypothetical
phrase boundary which is not accounted for (reached is also aligned to at). The phrase
pair and reached an ? and arrived at is consistent with the word alignment; however it
has an unaligned word (i.e., an) on the phrase boundary, which we disallow.
Our phrase extraction procedure distinguishes between two types of phrase pairs:
atomic, that is, the smallest possible phrase pairs, and composite, which can be created
by combining smaller phrase pairs. For example, the phrase pair and reached ? and
arrived at in Figure 3 is composite, as it can be decomposed into and ? and and
reached ? arrived at. Table 2 shows the atomic and composite phrase pairs extracted
from the possible alignments produced by annotators A and B for the sentence pair in
Figure 2.
We compute recall, precision, and F1 over the phrase pairs extracted from the word
alignments as follows:
Precision =
|Apatom ? B
p|
|Apatom|
Recall =
|Ap ? Bpatom|
|Bpatom|
F1 = 2 ? Precision ? Recall
Precision+ Recall
(3)
10 The term phrase is not used here in the linguistic sense; many extracted phrases will not be constituents.
603
Computational Linguistics Volume 34, Number 4
Table 1
Single word pairs specified by the word alignments from Figure 2, for two annotators, A and B.
The column entries specify the alignment type for each annotator, either sure (S) or possible (P).
Dashes indicate that the word pair was not predicted by the annotator. Italics denote lexically
identical word pairs.
Word alignments A B
they ? both ? P
they ? parties P P
discussed ? discussed S S
the ? the S S
aspects ? specific P ?
in ? specific P P
detail ? specific P P
aspects ? issues P S
in ? issues P ?
detail ? issues P ?
and ? and S S
reached ? arrived S S
reached ? at ? S
an ? a S S
extensive ? general S P
agreement ? consensus S S
. ? . S S
Figure 3
Validity of phrase pairs according to the phrase extraction heuristic. Only the leftmost phrase
pair is valid. The others are inconsistent with the alignment or have an unaligned word on a
boundary, respectively, indicated by a cross.
where Ap and Bp are the predicted and reference phrase pairs, respectively, and
the atom subscript denotes the subset of atomic phrase pairs, Apatom ? A
p. As shown
in Equation (3) we measure precision and recall between atomic phrase pairs and
the full space of atomic and composite phrase pairs. This ensures that we do not
multiply reward composite phrase pair combinations,11 while also not unduly pe-
nalizing non-matching phrase pairs which are composed of atomic phrase pairs in
11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore
might multiply count phrase pairs.
604
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Table 2
Phrase pairs are specified by the word alignments from Figure 2, using the possible alignments.
The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the
remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs.
?This phrase pair is atomic in A but composite in B.
Atomic phrase pairs A B
they ? parties P ?
they ? both parties ? P
discussed ? discussed S S
the ? the S S
aspects in detail ? specific issues P ?
in detail ? specific ? P
aspects ? issues ? S
and ? and S S
reached ? arrived S S
reached ? arrived at ? S
reached an ? arrived at a S P?
an ? a S S
extensive ? general S P
agreement ? consensus S S
. ? . S S
Composite phrase pairs A B
they discussed ? both parties discussed ? P
they discussed ? parties discussed P ?
they discussed the ? both parties discussed the ? P
they discussed the ? parties discussed the P ?
they ... reached an ? both parties ... arrived at a P ?
the aspects in detail ? the specific issues P P
reached an extensive ? arrived at a general S S
extensive agreement . ? general consensus . S S
...
the reference. Returning to the example in Table 2, with annotator B as the gold
standard, |Apatom| = 7, |B
p
atom| = 8, |A
p
atom ? B
p| = 5, and |Ap ? Bpatom| = 4. Consequently,
precision= 57 = 0.71, recall=
4
8 = 0.50, and F1=
2?0.71?0.50
0.71+0.50 = 0.59. Again we ignore
identical phrase pairs.
A potential caveat here concerns the quality of the atomic phrase pairs, which are
automatically induced and may not correspond to linguistic intuition. To evaluate this,
we had two annotators review a random sample of 166 atomic phrase pairs drawn from
the MTC corpus (sure), classifying each phrase pair as correct, incorrect, or uncertain
given the sentence pair as context. From this set, 73% were deemed correct, 22% un-
certain, and 5% incorrect.12 Annotators agreed in their decisions 75% of the time (using
the Kappa13 statistic, their agreement is 0.61). This confirms that the phrase-extraction
process produces reliable phrase pairs from our word-aligned data (although we cannot
claim that it is exhaustive).
12 Taking a more conservative position by limiting the proportion of unaligned words within the phrase
pair improves these figures monotonically to 90% correct and 0% incorrect (fully aligned phrase pairs).
13 This Kappa is computed over three nominal categories (correct, incorrect, and uncertain) and should not
be confused with the agreement measure we develop in the following section for phrase pairs.
605
Computational Linguistics Volume 34, Number 4
Chance-Corrected Agreement. Besides precision and recall, inter-annotator agreement is
commonly measured using the Kappa statistic (Cohen 1960). Thus is a desirable mea-
sure because it is adjusted for agreement due purely to chance:
? =
Pr(A)? Pr(E)
1? Pr(E)
(4)
where Pr(A) is the proportion of times two coders14 agree, corrected by Pr(E), the
proportion of times we would expect them to agree by chance.
Kappa is a suitable agreement measure for nominal data. An example would be a
classification task, where two coders must assign n linguistic instances (e.g., sentences
or words) into one of m categories. Given this situation, it would be possible for each
coder to assign each instance to the same category. Kappa allows us to quantify whether
the coders agree with each other about the category membership of each instance. It is
relatively straightforward to estimate Pr(A)?it is the proportion of instances on which
the two coders agree. Pr(E) requires a model of what would happen if the coders were
to assign categories randomly. Under the assumption that coders r1 and r2 are indepen-
dent, the chance of them agreeing on the jth category is the product of each of them
assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the
sum of this product across all categories: Pr(E) =
m
?
j=1
Pr(Cj|r1) Pr(Cj|r2). The literature
describes two different methods for estimating Pr(Cj|ri). Either a separate distribution
is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott
1955; Fleiss 1971; Siegel and Castellan 1988).We refer the interested reader to Di Eugenio
and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion.
Unfortunately, Kappa is not universally suited to every categorization task. A prime
example is structured labeling problems that allow a wide variety of output categories.
Importantly, the number and type of categories is not fixed in advance and can vary
from instance to instance. In parsing, annotators are given a sentence for which they
must specify a tree, of which there is an exponential number in the sentence length. Sim-
ilarly, in our case the space of possible alignments for a sentence pair is also exponential
in the input sentence lengths. Considering these annotations as nominal variables is
inappropriate.
Besides, alignments are only an intermediate representation that we have used to
facilitate the annotation of paraphrases. Ideally, we would like to measure agreement
over the set of phrase pairs which are specified by our annotators (via the word align-
ments), not the alignment matrices themselves.
Kupper and Hafner (1989) present an alternative measure similar to Kappa that is
especially designed for sets of variables:
C? =
??? ?0
1? ?0
, (5)
where ?? =
I
?
i=1
|Ai ? Bi|
min(|Ai|, |Bi|)
, and ?0 =
1
Ik
?
i
min(|Ai|, |Bi|)
14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976). For
simplicity?s sake our discussion and subsequent examples involve two coders. Also note that we use the
term coder instead of the more common rater. This is because in our task the annotators must identify
(a.k.a. code) the paraphrases rather than rate them.
606
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Here, Ai and Bi are the coders? predictions on sentence pair i from our corpus of I
sentence pairs. Each prediction is a subset of the full space of k items. Expression (5)
measures the agreement (or concordance) between coders A and B and follows the
general form of Kappa from Equation (4), which is defined analogously with Pr(A) and
Pr(E) taking the roles of ?? and ?0, but with different definitions.
Kupper and Hafner (1989) developed their agreement measure with medical diag-
nostic tasks in mind. For example, two physicians classify subjects into k = 3 diagnostic
categories and wish to find out whether they agree in their diagnoses. Here, each coder
must decidewhich (possibly empty) subset from k categories best describes each subject.
The size of k is thus invariant with the instance under consideration. This is not true
in our case, where k will vary across sentence pairs as sentences of different lengths
license different numbers of phrase pairs. More critically, the formulation in Equa-
tion (5) assumes that items in the set are independent: All subsets of the same car-
dinality as k are equally likely, and no combination is impossible. This independence
assumption is inappropriate for the paraphrase annotation task. The phrase extraction
heuristic allows each contiguous span in a sentence to be aligned to either zero or one
span in the other sentence; that is, nominating a phrase pair precludes the choice of
many other possible phrase pairs. Consequently relatively few of the subsets of the
full set of possible phrase pairs are valid. Formally, an alignment can specify only
O(N2) phrase pairs from a total set of k = O(N4) possible phrase pairs. This disparity in
magnitudes leads to increasingly underestimated ?? for larger N, namely, limN?? ?0 =
limN??O(N
2)/O(N4) = 0. The end result is an overestimate of C? on longer sentences.
For these reasons, we adapt the method of Kupper and Hafner (1989) to account for
our highly interdependent item sets. We use C? from Equation (5) as our agreement sta-
tistic defined over sets of atomic phrase pairs, that is, A = Apatom,B = B
p
atom. We redefine
?0 as follows:
?0 =
1
I
I
?
i=1
?
Apatom
?
Bpatom
Pr(Apatom) Pr(B
p
atom)
|Apatom ? B
p
atom|
min(|Apatom|, |B
p
atom|)
(6)
where Apatom and B
p
atom range over the sets of atomic phrase pairs licensed by sentence
pair i, and Pr(Apatom) and Pr(B
p
atom) are priors over these sets for each annotator. A conse-
quence of dropping the independence assumptions is that calculating ?0 is considerably
more difficult.
While it may be possible to calculate ?0 analytically, this gets increasingly compli-
cated for larger phrase pairs or with an expressive prior. For the sake of flexibility we
estimate ?0 using Monte Carlo sampling. Specifically, we approximate the full sum by
drawing samples from a prior distribution over sets of phrase pairs for each of our
annotators (Pr(Apatom) and Pr(B
p
atom) in Equation (6)). These samples are then compared
using the intersection metric. This is repeated many times and the results are then
averaged. More formally:
??0 =
1
I
I
?
i=1
1
J
J
?
j=1
|Apatom
( j)
? Bpatom
( j)
|
min(|Apatom
( j)
|, |Bpatom
( j)
|)
(7)
where for each sentence pair, i, we draw J samples of pairs of sets of phrase pairs,
(Apatom,B
p
atom). We use J = 1, 000, which is ample to give reliable estimates. So far we have
607
Computational Linguistics Volume 34, Number 4
not defined how we sample valid sets of phrase pairs. This is done via the word align-
ments. Recall that the annotators start out with alignments from an automatic word-
aligner. Firstly, we develop a distribution to predict how often an annotator changes a
cell from the initial alignment matrix. We model the number of changes made with a
binomial distribution, that is, each local change is assumed independent and has a fixed
probability, Pr(edit|r,Ni,Mi) where r is the coder andNi andMi are the sentence lengths.
This distribution is fit to each annotator?s predictions using a linear function over the
combined length of two sentences. Next we sample word alignments. Each sample
starts with the automatic alignment, and each cell is changed with probability Pr(edit).
These changes are binary, swapping alignments for non-alignments and vice versa.
Finally, the phrase-extraction heuristic is run over the alignment matrix to produce a
set of phrase pairs. This is done for each annotator, A and B, after which we have a
sample, (Apatom,B
p
atom). Each sample is then fed into Equation (7). Admittedly, this is not
themost accurate prior, as annotators are not just randomly changing the alignment, but
instead are influenced by the content expressed by the sentence pair and other factors
such as syntactic complexity. However, this prior produces estimates for ??0 which are
several orders of magnitude larger than those using Kupper and Hafner?s model of ?0
in Equation (5).
We now illustrate the process of measuring chance-corrected agreement, C?, with
respect to the example in Figure 2. Here, |Apatom| = 7, |B
p
atom| = 8, |A
p
atom ? B
p
atom| = 4, and
therefore ?? = 47 = 0.571. For this sentence our annotators edited eight and nine align-
ment cells, respectively, of the initial alignment matrix. This translates into Pr(edit|r =
A) = 812?13 = 5.13% and Pr(edit|r = B) = 5.77%. Given these priors, we run the Monte
Carlo sampling process from Equation (7), which results in ??0 = 0.147. Combining the
agreement estimate, ??, and chance correction estimate, ??0, using Equation (6) results in
C? = 0.571?0.1471?0.147 = 0.497.
Now, imagine a hypothetical case where ?? = 47 = 0.571 (i.e., the agreement is the
same as before), annotator B edits nine alignment cells, but annotator A chooses not
to make any edits. This leads to an increased estimate of ??0 = 0.259 and a decreased
C? = 0.442. If both annotators were not to make any edits, ??0 = 1 and C? = ??. Interest-
ingly, at the other extreme when Pr(edit|r = A) = Pr(edit|r = B) = 1, agreement is also
perfect, ??0 = 1 and C? = ??. This is because only one phrase pair can be extracted
which consists of the two full sentences.
Results. Tables 3 and 4 display agreement statistics on our three corpora using precision,
recall, F1, and C?. Specifically, we estimate C? by aggregating ?? and ??0 into corpus-
level estimates. Table 3 shows agreement scores for individual words, whereas Table 4
shows agreement for phrase pairs. In both cases the agreement is computed over non-
identical word and phrase pairs which are more likely to correspond to paraphrases.
The agreement figures are broken down into possible (Poss) and sure alignments (Sure)
for precision and recall.
When agreement is measured over words, our annotators obtain high F1 on all
three corpora (MTC, Leagues, and News). Recall on Possibles seems worse on the
News corpus when compared to MTC or Leagues. This is to be expected because this
corpus was automatically harvested from the Web, and some of its instances may not
be representative examples of paraphrases. For example, it is common for one sentence
to provide considerably more details than the other, despite the fact that both describe
the same event. The annotators in turn have difficulty deciding whether such instances
are valid paraphrases. The C? scores for the three corpora are in the same ballpark.
608
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Table 3
Inter-annotator agreement using precision, recall, F1, and C?; the agreement is measured over
words.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.79 0.59 Prec 0.85 0.73 Prec 0.78 0.55
Rec 0.77 0.73 Rec 0.74 0.75 Rec 0.57 0.70
F1 0.76 F1 0.79 F1 0.74
C? 0.85 C? 0.87 C? 0.89
Table 4
Inter-annotator agreement using precision, recall, F1, and C?; the agreement is measured over
atomic phrase pairs.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.77 0.67 Prec 0.74 0.72 Prec 0.72 0.68
Rec 0.77 0.66 Rec 0.77 0.73 Rec 0.69 0.81
F1 0.71 F1 0.74 F1 0.76
C? 0.63 C? 0.62 C? 0.53
Interestingly, C? is highest on the News corpus, whereas F1 is lowest. Whereas precision
and recall are normalized by the number of predictions from annotators A and B,
respectively, C? is normalized by the minimum number of predictions between the two.
Therefore, when the predictions are highly divergent, C? will paint a rosier picture than
F1 (which is the combination of precision and recall). This indeed seems to be the case
for the News corpus, where precision and recall have a higher spread in comparison to
the other two corpora (see the Poss column in Table 3).
Agreement scores tend to be lower when taking phrases into account (see Table 4).
This is expected because annotators are faced with a more complex task; they must
generally make more decisions: for example, determining the phrase boundaries and
how to align their constituent words. An exception to this trend is the News corpus
where the F1 is higher for phrase pairs than for individual word pairs. This is due to the
fact that there are many similar sentence pairs in this data. These have many identical
words and a few different words. The differences are often in a clump (e.g., person
names, verb phrases), rather than distributed throughout the sentence. The annotators
tend to block align these and there is a large scope for disagreement.Whereas estimating
agreement over words heavily penalizes block differences, when phrases are taken
into account in the F1 measure, these are treated more leniently. Note that C? is not
so lenient, as it measures agreement over the sets of atomic phrase pairs rather than
between atomic and composite phrase pairs in the F1 measure. This means that under
C?, choosing different granularities of phrases will be penalized, but would not have
been under the F1 measure.
In Figure 4we show how C? varies with sentence length for our three corpora. Specif-
ically, we plot observed agreement ??, chance agreement ?0, and C? against sentence pairs
609
Computational Linguistics Volume 34, Number 4
Figure 4
Agreement statistics plotted against sentence length for the three sub-corpora. Each group of
three columns correspond to ??, ??0, and C?, respectively. The statistics were measured over
non-identical phrase pairs using all phrase pairs, atomic and composite.
Table 5
Agreement between automatic Giza++ predicted word alignments and our manually corrected
alignments, measured over atomic phrase pairs.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.58 0.55 Prec 0.63 0.60 Prec 0.63 0.65
Rec 0.42 0.49 Rec 0.39 0.47 Rec 0.50 0.64
F1 0.53 F1 0.54 F1 0.63
binned by (the shorter) sentence length. In all cases we observe that chance agreement
is substantially lower than observed agreement for all sentence lengths. We also see that
C? tends to be higher for shorter sentences. Differences in C? across sentence lengths are
mostly of small magnitude across all three corpora. This indicates that disagreements
may be due to other factors, besides sentence length.
Unfortunately, there are no comparable annotation studies that would allow us
to gauge the quality of the obtained agreements. The use of precision, recall, and F1
is widespread in SMT, but these measures evaluate automatic alignments against a
gold standard, rather than the agreement between two or more annotators (but see
Melamed [1998] for an exception). Nevertheless, we would expect the humans to agree
more with each other than with Giza++, given that the latter produces many erroneous
word alignments and is not specifically tuned to the paraphrasing task. Table 5 shows
agreement between one annotator and Giza++ for atomic phrase pairs.15 We obtained
similar results for the other annotator and with the word-based measures. As can be
seen, human?Giza++ agreement is much lower than human?human agreement on all
three corpora (compare Tables 5 and 4). Taken together the results in Tables 3?5 show
a substantial level of agreement, thus indicating that our definition of paraphrases via
word alignments can yield reliable annotations. In the following section we discuss how
our corpus can be usefully employed in the study of paraphrasing.
15 Note that we cannot meaningfully measure C? for this data because the Giza++ predictions are already
being used to estimate ?0 in our formulation. Consequently, P(A) = P(B) and C? is zero.
610
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
4. Experiments
Our annotated corpus can be used in a number of ways to help paraphrase research:
for example, to inform the linguistic analysis of paraphrases, as a training set for the
development of discriminative paraphrase systems, and as a test set for the automatic
evaluation of computational models. Here, we briefly demonstrate some of these uses.
Paraphrase Modeling.Much previous research has focused on lexical paraphrases (but see
Lin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions). We argue
that our corpus should support a richer range of structural (syntactic) paraphrases.
To demonstrate this we have extracted paraphrase rules from our annotations using
the grammar induction algorithm from Cohn and Lapata (2007). Briefly, the algorithm
extracts tree pairs from word-aligned text by choosing aligned constituents in a pair of
equivalent sentences. These pairs are then generalized by factoring out aligned subtrees,
thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable
nodes.
We parsed the MTC corpus with Bikel?s (2002) parser and extracted synchronous
rules from the gold-standard alignments. A sample of these rules are shown in Figure 5.
Here we see three lexical paraphrases, followed by five structural paraphrases. In
example 4, also is replaced with moreover and is moved to the start of the sentence from
the pre-verbal position. Examples 5?8 show various reordering operations, where the
boxed numbers indicate correspondences between non-terminals in the two sides of the
rules.
The synchronous rules in Figure 5 provide insight into the process of paraphrasing
at the syntactic level, and also a practical means for developing algorithms for para-
phrase generation?a task which has received little attention to date. For instance, we
could envisage a paraphrase model that transforms parse trees of an input sentence
into parse trees that represent a sentential paraphrase of that sentence. Our corpus can
be used to learn this mapping using discriminative methods (Cowan, Kuc?erova?, and
Collins 2006; Cohn and Lapata 2007).
Evaluation Set. As mentioned in Section 1, it is currently difficult to compare competing
approaches due to the effort involved in eliciting manual judgments of paraphrase
output. Our corpus could fill the role of a gold-standard test set, allowing for automatic
evaluation techniques.
Developing measures for automatic paraphrase evaluation is outside the scope of
this article. Nevertheless, we illustrate how the corpus can be used for this purpose.
For example we could easily measure the precision and recall of an automatic system
Figure 5
Synchronous grammar rules extracted from the MTC corpus.
611
Computational Linguistics Volume 34, Number 4
against our annotations. Computing precision and recall for an individual system is not
perhaps the most meaningful test, considering the large potential for paraphrasing in
a given sentence pair. A better evaluation strategy would include a comparison across
many systems on the same corpus. We could then rank these systems without, however,
paying so much attention to the absolute precision and recall values. We expect these
comparisons to yield relatively low numbers for many reasons. First and foremost the
task is hard, as shown by our inter-annotator agreement figures in Tables 3 and 4.
Secondly, there may be valid paraphrases that the systems identify but are not listed
in our gold standard. Thirdly, systems may have different biases, for example, towards
producing more lexical or syntactic paraphrases, but our comparison would not take
this into account. Despite all these considerations, we believe that comparison against
our corpus would treat these systems on an equal footing against the same materials
while factoring out nonessential degrees of freedom inherent in human elicitation stud-
ies (e.g., attention span, task familiarity, background).
We evaluated the performance of two systems against our corpus. Our first system
is simply Giza++ trained on the 55, 615 sentence pairs described in Section 4. The
second system uses a co-training-based paraphrase extraction algorithm (Barzilay and
McKeown 2001). It was also trained on the MTC part 1 corpus, on the same data set
used for Giza++, with its default parameters. For each system, we filtered the predicted
paraphrases to just those which match part of a sentence pair in the test set. These
paraphrases were then compared to the sure phrase pairs extracted from our manually
aligned corpus. Giza++?s precision is 55% and recall 49% (see Table 5). The co-training
system obtained a precision of 30% and recall of 16%. To confirm the accuracy of
the precision estimate, we performed a human evaluation on a sample of 48 of the
predicted paraphrases which were treated as errors. Of these, 63% were confirmed as
being incorrect and only 20%were acceptable (the remaining were uncertain). The inter-
annotator agreement in Table 4 can be used as an upper bound for precision and recall
(precision for Sure phrase pairs is 67% and recall 66%). These results seem to suggest
that a hypothetical paraphrase extractor based on automatic word alignments would
obtain performance superior to the co-training approach. However, we must bear in
mind that the co-training system is highly parametrized and was not specifically tuned
to our data set.
5. Conclusions
In this article we have presented a human-annotated paraphrase corpus and argued
that it can be usefully employed for the evaluation and modeling of paraphrases. We
have defined paraphrases as word alignments in a corpus containing pairs of equivalent
sentences and shown that these can be reliably identified by annotators. In measur-
ing agreement, we used the standard measures of precision, recall, and F1, but also
proposed a novel formulation of chance-corrected agreement for word (and phrase)
alignments. Beyond alignment, our formulation could be applied to other structured
tasks including parsing and sequence labeling.
The uses of the corpus are many and varied. It can serve as a test set for eval-
uating the precision and recall of paraphrase induction systems trained on parallel
monolingual corpora. The corpus could be further used to develop new evaluation
metrics for paraphrase acquisition or novel paraphrasing models. An exciting avenue
for future research concerns paraphrase prediction, that is, determiningwhen and how to
paraphrase single sentence input. Because our corpus contains paraphrase annotations
at the sentence level, it could provide a natural test-bed for prediction algorithms.
612
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Acknowledgments
The authors acknowledge the support of the
EPSRC (Cohn, grant GR/T04557/01;
Lapata, grant GR/T04540/01), the National
Science Foundation (Callison-Burch, grant
IIS-071344), and the EuroMatrix project
(Callison-Burch) funded by the European
Commission (6th Framework Programme).
We are grateful to our annotators Vasilis
Karaiskos and Tom Segler. Thanks to Regina
Barzilay for providing us the output of her
system on our data and to the anonymous
referees whose feedback helped to
substantially improve the present article.
References
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Compute System
Sciences, 3(1):37?56.
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for Computational
Linguistics. Computational Linguistics.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 9?16,
Sydney.
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 597?604, Ann Arbor, MI.
Bartko, John J. and William T. Carpenter.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307?317.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization:
Paraphrasing and Generation. Ph.D. thesis,
Columbia University, New York, NY.
Barzilay, Regina and Noemie Elhadad.
2003. Sentence alignment for monolingual
comparable corpora. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 25?32,
Sapporo.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: An unsupervised
approach using multiple-sequence
alignment. In Proceedings of the Human
Language Technology Conference and the
Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 16?23, Edmonton.
Barzilay, Regina and Kathy McKeown. 2001.
Extracting paraphrases from a parallel
corpus. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 50?57, Toulouse.
Bikel, Daniel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of the Human
Language Technology Conference,
pages 24?27, San Diego, CA.
Brown, Peter F., Stephen A. Della-Pietra,
Vincent J. Della-Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Callison-Burch, Chris. 2007. Paraphrasing and
Translation. Ph.D. thesis, University of
Edinburgh, Edinburgh, Scotland.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of the Human Language
Technology Conference and Annual Meeting of
the North American Chapter of the Association
for Computational Linguistics, pages 17?24,
New York, NY.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:37?46.
Cohn, Trevor and Mirella Lapata. 2007. Large
margin synchronous generation and its
application to sentence compression. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing and
on Computational Natural Language
Learning, pages 73?82, Prague.
Cowan, Brooke, Ivona Kuc?erova?, and
Michael Collins. 2006. A discriminative
model for tree-to-tree translation. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 232?241, Sydney.
Daume? III, Hal and Daniel Marcu.
2004. A phrase-based HMM approach
to document/abstract alignment.
In Proceedings of the 2004 Conference
on Empirical Methods in Natural
Language Processing, pages 119?126,
Barcelona.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Dolan, William, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction
of large paraphrase corpora: Exploiting
massively parallel news sources. In
Proceedings of the 20th International
Conference on Computational Linguistics,
pages 350?356, Geneva.
613
Computational Linguistics Volume 34, Number 4
Duboue, Pablo and Jennifer Chu-Carroll.
2006. Answering the question you wish
they had asked: The impact of
paraphrasing for question answering.
In Proceedings of the Human Language
Technology Conference of the NAACL,
Companion Volume: Short Papers,
pages 33?36, New York, NY.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Fraser, Alexander and Daniel Marcu. 2007.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293?303.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Human Language Technology
Conference and Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 48?54,
Edmonton.
Kupper, Lawrence L. and Kerry B. Hafner.
1989. On assessing interrater agreement
for multiple attribute responses. Biometrics,
45(3):957?967.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):342?360.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 67?74,
Ann Arbor, MI.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The Blinker
project. IRCS Technical Report #98-07,
University of Pennsylvania,
Philadelphia, PA.
Mihalcea, Rada and Ted Pedersen. 2003. An
evaluation exercise for word alignment. In
Proceedings of the HLT-NAACL Workshop on
Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,
pages 1?6, Edmonton.
Och, Franz Josef and Hermann Ney. 2000a. A
comparison of alignment models for
statistical machine translation. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 1086?1090, Saarbru?cken.
Och, Franz Josef and Hermann Ney.
2000b. Improved statistical alignment
models. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 440?447,
Hong Kong.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?52.
Och, Franz Josef, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20?28, College Park, MD.
Pang, Bo, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple
translations: Extracting paraphrases and
generating new sentences. In Proceedings of
the Human Language Technology Conference
and the Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 181?188,
Edmonton.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 142?149, Barcelona.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale. Public
Opinion Quarterly, 19:127?141.
Siegel, Sidney and N. John Castellan. 1988.
Non Parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New York.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Munteanu, and Eduard Hovy. 2006.
Paraeval: Using paraphrases to
evaluate summaries automatically. In
Proceedings of the Human Language
Technology Conference, pages 447?454,
New York, NY.
614
Proceedings of NAACL HLT 2007, pages 348?355,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Information Retrieval Approach to Sense Ranking
Mirella Lapata and Frank Keller
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
{mlap,keller}@inf.ed.ac.uk
Abstract
In word sense disambiguation, choosing
the most frequent sense for an ambigu-
ous word is a powerful heuristic. However,
its usefulness is restricted by the availabil-
ity of sense-annotated data. In this paper,
we propose an information retrieval-based
method for sense ranking that does not re-
quire annotated data. The method queries
an information retrieval engine to estimate
the degree of association between a word
and its sense descriptions. Experiments on
the Senseval test materials yield state-of-
the-art performance.We also show that the
estimated sense frequencies correlate reli-
ably with native speakers? intuitions.
1 Introduction
Word sense disambiguation (WSD), the ability to
identify the intended meanings (senses) of words
in context, is crucial for accomplishing many NLP
tasks that require semantic processing. Examples in-
clude paraphrase acquisition, discourse parsing, or
metonymy resolution. Applications such as machine
translation (Vickrey et al, 2005) and information re-
trieval (Stokoe, 2005) have also been shown to ben-
efit from WSD.
Given the importance of WSD for basic NLP
tasks and multilingual applications, much work has
focused on the computational treatment of sense
ambiguity, primarily using data-driven methods.
Most accurate WSD systems to date are super-
vised and rely on the availability of training data
(see Yarowsky and Florian 2002; Mihalcea and Ed-
monds 2004 and the references therein). Although
supervised methods typically achieve better perfor-
mance than unsupervised alternatives, their appli-
cability is limited to those words for which sense
labeled data exists, and their accuracy is strongly
correlated with the amount of labeled data avail-
able. Furthermore, current supervised approaches
rarely outperform the simple heuristic of choosing
the most common or dominant sense in the train-
ing data (henceforth ?the first sense heuristic?), de-
spite taking local context into account. One reason
for this is the highly skewed distribution of word
senses (McCarthy et al, 2004a). A large number of
frequent content words is often associated with only
one dominant sense.
Obtaining the first sense via annotation is ob-
viously costly and time consuming. Sense anno-
tated corpora are not readily available for different
languages or indeed sense inventories. Moreover,
a word?s dominant sense will vary across domains
and text genres (the word court in legal documents
will most likely mean tribunal rather than yard).
It is therefore not surprising that recent work (Mc-
Carthy et al, 2004a; Mohammad and Hirst, 2006;
Brody et al, 2006) attempts to alleviate the anno-
tation bottleneck by inferring the first sense auto-
matically from raw text. Automatically acquired first
senses will undoubtedly be noisy when compared to
human annotations. Nevertheless, they can be use-
fully employed in two important tasks: (a) to create
preliminary annotations, thus supporting the ?anno-
tate automatically, correct manually? methodology
used to provide high volume annotation in the Penn
Treebank project; and (b) in combination with super-
vised WSD methods that take context into account;
for instance, such methods could default to the dom-
inant sense for unseen words or words with uninfor-
mative contexts.
This paper focuses on a knowledge-lean sense
ranking method that exploits a sense inventory like
WordNet and corpus data to automatically induce
dominant senses. The proposed method infers the
associations between words and sense descriptions
automatically by querying an IR engine whose in-
dex terms have been compiled from the corpus
of interest. The approach is inexpensive, language-
independent, requires minimal supervision, and uses
no additional knowledge other than the word senses
proper and morphological query expansions. We
348
evaluate our method on two tasks. First, we use
the acquired dominant senses to disambiguate the
meanings of words in the Senseval-2 (Palmer et al,
2001) and Senseval-3 (Snyder and Palmer, 2004)
data sets. Second, we simulate native speakers? intu-
itions about the salience of word meanings and ex-
amine whether the estimated sense frequencies cor-
relate with sense production data. In all cases our ap-
proach outperforms a naive baseline and yields per-
formances comparable to state of the art.
In the following section, we provide an overview
of existing work on sense ranking. In Section 3, we
introduce our IR-based method, and describe several
sense ranking models. In Section 4, we present our
results. Discussion of our results and future work
conclude the paper (Section 5).
2 Related Work
McCarthy et al (2004a) were the first to pro-
pose a computational model for acquiring dominant
senses from text corpora. Key in their approach is
the observation that distributionally similar neigh-
bors often provide cues about a word?s senses. The
model quantifies the degree of similarity between
a word?s sense descriptions and its closest neigh-
bors, thus delivering a ranking over senses where the
most similar sense is intuitively the dominant sense.
Their method exploits two notions of similarity,
distributional and semantic. Distributionally similar
words are acquired from the British National Cor-
pus using an information-theoretic similarity mea-
sure (Lin, 1998) operating over dependency re-
lations (e.g., verb-subject, verb-object). The latter
are obtained from the output of Briscoe and Car-
roll?s (2002) parser. The semantic similarity between
neighbors and senses is measured using a manually
crafted taxonomy such as WordNet (see Budanitsky
and Hirst 2001 for an overview of WordNet-based
similarity measures).
Mohammad and Hirst (2006) propose an algo-
rithm for inferring dominant senses without rely-
ing on distributionally similar neighbors. Their ap-
proach capitalizes on the collocational nature of
semantically related words. Assuming a coarse-
grained sense inventory (e.g., the Macquarie The-
saurus), it first creates a matrix whose columns rep-
resent all categories (senses) c1 . . .cn in the inven-
tory and rows the ambiguous target words w1 . . .wm;
the matrix cells record the number of times a tar-
get word ti co-occurs with category c j within a win-
dow of size s. Using an appropriate statistical test,
they estimate the relative strength of association be-
tween an ambiguous word and each of its senses.
The sense with the highest association is the pre-
dominant sense.
Our work shares with McCarthy et al (2004a) and
Mohammad and Hirst (2006) the objective of infer-
ring dominant senses automatically. We propose a
knowledge-lean method that relies on word associa-
tion and requires no syntactic annotation. The latter
may be unavailable when working with languages
other than English for which state-of-the-art parsers
or taggers have not been developed. Mohammad and
Hirst (2006) estimate the co-occurrence frequency
of a word and its sense descriptors by considering
small window sizes of up to five words. These esti-
mates will be less reliable for moderately frequent
words or for sense inventories with many senses.
Our approach is more robust to sparse data ? we
work with document-based frequencies ? and thus
suitable for both coarse and fine grained sense in-
ventories. Furthermore, it is computationally inex-
pensive; in contrast to McCarthy et al (2004a) we
do not rely on the structure of the sense inventory
for measuring the similarity between synonyms and
their senses. Moreover, unlike Mohammad and Hirst
(2006), our algorithm only requires co-occurrence
frequencies for the target word and its senses, with-
out considering all senses in the inventory and all
words in the corpus simultaneously.
3 Method
3.1 Motivation
Central in our approach is the assumption that con-
text provides important cues regarding a word?s
meaning. The idea dates back at least to Firth (1957)
(?You shall know a word by the company it keeps?)
and underlies most WSD work to date. Another ob-
servation that has found wide application in WSD is
that words tend to exhibit only one sense in a given
discourse or document (Gale et al, 1992). Further-
more, documents are typically written with certain
topics in mind which are often indicated by word
distributional patterns (Harris, 1982).
For example, documents talking about congres-
sional tenure are likely to contain words such as term
of office or incumbency, whereas documents talking
about legal tenure (i.e., the right to hold property)
349
are likely to include the words right or land. Now,
we could estimate which sense of tenure is most
prevalent simply by comparing whether tenure co-
occurs more often with term of office than with land
provided we knew that both of these terms are se-
mantically related to tenure. Fortunately, senses in
WordNet (and related taxonomies) are represented
by synonym terms. So, all we need to do for esti-
mating a word?s sense frequencies is to count how
often it co-occurs with its synonyms. We adopt here
a fairly broad definition of co-occurrence, two words
co-occur if they are attested in the same document.
We could obtain such counts from any document
collection; however, to facilitate comparisons with
prior work (e.g., McCarthy et al 2004a), all our ex-
periments use the British National Corpus (BNC). In
what follows we describe in detail how we retrieve
co-occurrence counts from the BNC and how we ac-
quire dominant senses.
3.2 Dominant Sense Acquisition
Throughout the paper we use the term frequency as a
shorthand for document frequency, i.e., the number
of documents that contain a word or a set of words
which may or may not be adjacent. The method
we propose here exploits document frequencies of
words and their sense definitions. We base our dis-
cussion below on the WordNet sense inventory and
its representation of senses in terms of synonym
sets (synsets). However, our approach is not lim-
ited to this particular lexicon; any dictionary with
synonym-based sense definitions could serve our
purposes.
As an example consider the noun tenure, which
has the following senses in WordNet:
(1) Sense 1
tenure, term of office, incumbency
=> term
Sense 2
tenure, land tenure
=> legal right
The senses are represented by the two synsets
{tenure, term of office, incumbency} and
{tenure, land tenure}. (The hypernyms for each
sense are also listed; indicated by the arrows.) We
can now approximate the frequency with which a
word w1 occurs with the sense s by computing its
synonym frequencies: for each word w2 ? syns(s),
the set of synonyms of s, we field a query of the form
w1 AND w2. These synonym frequencies can then be
used to determine the most frequent sense of w1 in a
variety of ways (to be detailed below).
The synsets for the two senses in (1) give rise to
the queries in (2) and (3). Note that two queries are
generated for the first synset, as it contains two syn-
onyms of the target word tenure.
(2) a. "tenure" AND "term of office"
b. "tenure" AND "incumbency"
(3) "tenure" AND "land tenure"
For example, query (2-a) will return the number of
documents in which tenure and term of office co-
occur. Presumably, tenure is mainly used in its dom-
inant sense in these documents. In the same way,
query (3) will return documents in which tenure is
used in the sense of land tenure. Note that this way
of approximating synonym frequencies as document
frequencies crucially relies on the ?one sense per
discourse? hypothesis (Gale et al, 1992), under the
assumption that a document counts as a discourse
for word sense disambiguation purposes.
Apart from synonym frequencies, we also gener-
ate hypernym frequencies by submitting queries of
the form w1 AND w2, for each w2 ? hype(s), the set of
immediate hypernyms of the sense s. The hypernym
queries for the two senses of tenure are:
(4) "tenure" AND "term"
(5) "tenure" AND "legal right"
Hypernym queries are particularly useful for synsets
of size one, i.e., where a word in a given sense has
no synonyms, and is only differentiated from other
senses by its hypernyms.
Before submitting queries such as the ones in
(2) and (3) to an IR engine, we perform query
expansion to make sure that all relevant in-
flected forms are included. For example the query
term "tenure" is expanded to ("tenure" OR
"tenures"), i.e., both singular and plural noun
forms are generated. Similarly, all inflected verb
forms are generated, e.g., "keep up" gives rise to
the query term ("keep up" OR "keeps up" OR
"keeping up" OR "kept up"). John Carroll?s
suite of morphological tools (morpha and morphg)
is used to generate inflected forms for verbs and
350
nouns.1
The queries generated this way are then submitted
to an IR engine to obtain document counts. Specifi-
cally, we indexed the BNC using GLIMPSE (Global
Implicit Search) a fast and flexible indexing and
query system2 (Manber and Wu, 1994). GLIMPSE
supports approximate and exact matching, Boolean
queries, wild cards, regular expressions, and many
other options. The text is divided into equal size
blocks and an inverted index is created containing
the words and the block numbers in which they oc-
cur. Given a query, GLIMPSE will retrieve the rele-
vant documents using a two-level search method. It
will first locate the query in the inverted index and
then use sequential search to find an exact answer.
Once synonym frequencies and hypernym fre-
quencies are in place, we can compute a word?s pre-
dominant sense in a number of ways. First, we can
vary the way the frequency of a given sense is esti-
mated based on synonym frequencies:
? Sum: The frequency of a given synset is com-
puted as the sum of the synonym frequen-
cies. For example, the frequency of the dom-
inant sense of tenure would be computed by
adding up the document frequencies returned
by queries (2-a) and (2-b).
? Average (Avg): The frequency of a synset is
computed by taking the average of synonym
frequencies.
? Highest (High): The frequency of a synset is
determined by the synonym with the highest
frequency.
Secondly, we can vary whether or not hypernyms are
taken into account:
? No hypernyms (?Hyp): Only the synonym
frequencies are included when computing the
frequency of a synset. For example, only the
queries of (2-a) and (2-b) are relevant for esti-
mating the dominant sense of tenure.
? Hypernyms (+Hyp): Both synonym and hy-
pernym frequencies are taken into account
1The tools can be downloaded from http://www.
informatics.susx.ac.uk/research/nlp/carroll/
morph.html.
2The software can be downloaded from http:
//webglimpse.net/download.php
when computing sense frequency. For example,
the frequency for the senses of tenure would
be computed based on the document frequen-
cies returned by queries (2-a), (2-b), and (4)
(by summing, averaging, or taking the highest
value, as before).
The third option relates to whether the sense fre-
quencies are used in raw or in normalized form:
? Non-normalized (?Norm): The raw synonym
frequencies are used as estimates of sense fre-
quencies.
? Normalized (+Norm): Sense frequencies are
computed by dividing the word-synonym fre-
quency by the frequency of the synonym in
isolation. For example, the normalized fre-
quency for (2-a) is computed by dividing
the document frequency for "tenure" AND
"term of office" by the document fre-
quency for "term of office". Normalizing
takes into account the fact that the members of
the synset of a sense may differ in frequency.
The combination of the above parameters yields 12
sense ranking models. We explore the parameter
space exhaustively on the Senseval-2 benchmark
data set. The best performing model on this data set
is then used in all our subsequent experiments. We
use Senseval-2 as a development set, but we also
demonstrate that a far smaller manually annotated
sample is sufficient for selecting the best model.
4 Experiments
Our experiments were driven by three questions:
(1) Is WSD feasible at all with a model that does
not employ any syntactic or semantic knowledge?
Recall that McCarthy et al (2004a) propose a model
that crucially relies on a robust parser for estimat-
ing dominant senses. (2) What is the best parameter
setting for our model? (3) Do the acquired dominant
senses correlate with human judgments? If our sense
frequencies exhibit no such correlation, it is unlikely
that they will be useful in practical applications.
To address the first two questions we use the in-
duced first senses to perform WSD on the Senseval-
2 and Senseval-3 data sets. For our third question we
compare native speakers? semantic intuitions against
the BNC sense frequencies.
351
?Norm +Norm
+Hyp ?Hyp +Hyp ?Hyp
P R P R P R P R
Sum 42.3 40.8 46.3 44.6 45.9 44.3 48.6 46.8
High 51.6 49.8 51.1 49.3 57.2 55.1 59.7 57.6
Avg 44.1 42.6 48.5 46.8 49.6 47.8 51.5 49.6
Table 1: Results for Senseval-2 data by model in-
stantiation
4.1 Model Selection
The goal of our first experiment is to establish which
model configuration (see Section 3.2) is best suited
for the WSD task. We thus varied how the overall
frequency is computed (Sum, High, Avg), whether
hyponyms are included (?Hyp), and whether the
frequencies are normalized (?Norm). To explore the
parameter space, we used the Senseval-2 all-words
test data as our development set. This data set con-
sists of three documents from the Wall Street Jour-
nal containing approximately 2,400 content words.
Following McCarthy et al (2004a), we first use our
method to find the dominant sense for all word types
in the corpus and then use that sense to disambiguate
tokens without taking contextual information into
account. We used WordNet 1.7.1 (Fellbaum, 1998)
senses.3
We compared our results to a baseline that se-
lects for each word type a random sense, assumes
it is the dominant one, and uses it to disambiguate
all instances of the target word (McCarthy et al,
2004a). We also report the WSD performance of a
more competitive baseline that always chooses the
sense with the largest synset as the dominant sense.
Consider again the word tenure from Section 3.2.
According to this baseline, the dominant sense for
tenure is the first one since it is represented by the
largest synset (three members).
Our results on Senseval-2 are summarized in Ta-
ble 1. We observe that models that do not include
hypernyms yield consistently better precision and
recall than models that include them. On the one
hand, hypernyms render the estimated sense distri-
butions less sparse. On the other hand, they intro-
duce considerable noise; the resulting sense frequen-
cies are often similar ? the same hypernyms can be
3Senseval-2 is annotated with WordNet 1.7 senses which
we converted to 1.7.1 using a publicly available mapping (see
http://www.cs.unt.edu/?rada/downloads.html).
BaseR BaseS Model
P R P R P R N
Noun 26.8 25.4 45.8 43.4 53.1?# 50.2?# 1,063
Verb 11.2 11.1 19.9 19.5 48.2?# 47.3?# 569
Adj 22.1 21.4 56.5 56.0 56.7? 56.2? 451
Adv 48.0 45.9 66.4 62.9 86.4?# 81.8?# 301
All 26.3 25.4 42.2 40.7 59.7?# 57.6?# 2,384
Table 2: Results of best model (High, +Norm,
?Hyp) for Senseval-2 data by part of speech
(?: sig. diff. from BaseR, #: sig. diff. from BaseS;
p < 0.01 using ?2 test)
shared among several senses ? and selecting one pre-
dominant sense over the other can be due to very
small frequency differences. We also find that mod-
els with normalized document counts outperform
models without normalization. This is not surpris-
ing, there is ample evidence in the literature (Mo-
hammad and Hirst, 2006; Turney, 2001) that associ-
ation measures (e.g., conditional probability, mutual
information) are better indicators of lexical similar-
ity than raw frequency. Finally, selecting the syn-
onym with the highest frequency (and defaulting to
its sense) achieves better results in comparison to av-
eraging or summing over all synsets.
In sum, the best performing model is High,
+Norm, ?Hyp, achieving a precision of 59.7% and
a recall of 57.9%. The results for this model are bro-
ken down by part of speech in Table 2. Here, we
also include a comparison with the random base-
line (BaseR) and a baseline that selects the dominant
sense by synset size (BaseS). We observe that the
optimal model significantly outperforms both base-
lines on the complete data set (see row All in Ta-
ble 2) and on most individual parts of speech (perfor-
mances are comparable for our model and BaseS on
adjectives). BaseS is far better than BaseR and gen-
erally harder to beat. Defaulting to synset size in the
absence of any other information is a good heuristic;
large synsets often describe frequent senses. Vari-
ants of our model that select a dominant sense by
summing over synset members are closest to this
baseline. Note that our best performing model does
not rely on synset size; it simply selects the synonym
with the highest frequency, despite the fact that it
might belong to a large or small synset. We con-
jecture that its superior performance is due to the
collocational nature of semantic similarity (Turney,
352
?Norm +Norm
+Hyp ?Hyp +Hyp ?Hyp
P R P R P R P R
Sum 42.3 40.8 46.3 44.6 45.2 44.7 44.6 44.0
High 51.6 49.8 51.1 49.3 55.0 54.3 61.3 60.5
Avg 44.1 42.6 48.5 46.8 51.5 50.8 50.4 49.8
Table 3: Results for 10% of Senseval-2 data by
model instantiation
2001).
In order to establish that High, +Norm, ?Hyp is
the optimal model, we utilized the whole Senseval-
2 data set. Using such a large dataset is more likely
to yield a stable parameter setting, but it also raises
the question whether parameter optimization could
take place on a smaller dataset which is less costly
to produce. Table 3 explores the parameter space on
a sample randomly drawn from Senseval-2 that con-
tains only 240 tokens (i.e., one tenth of the original
data set). The behavior of our models on this smaller
sample is comparable to that on the entire Senseval-
2 data. Importantly, both sets yield the same best
model, i.e., High, +Norm, ?Hyp. In the remainder
of this paper we will use this model for further ex-
periments without additional parameter tuning.
4.2 Application to Senseval-3 Data
We next evaluate our best model the on the
Senseval-3 English all-words data set. Senseval-3
consists of two Wall Street Journal articles and
one excerpt from the Brown corpus (approximately
5,000 content words in total). Similarly to the ex-
periments reported in the previous section, we used
WordNet 1.7.1. We calculate recall and precision
with the Senseval-3 scorer.
Our results are given in Table 4. Besides the
two baselines (BaseR and BaseS), we also com-
pare our model to McCarthy et al (2004b)4 and
the best unsupervised (IRST-DDD) and supervised
(GAMBLE) systems that participated in Senseval-3.
IRST-DDD was developed by Strapparava et al
(2004) and performs domain driven disambiguation.
Specifically, the approach compares the domain of
the context surrounding the target word with the do-
mains of its senses and uses a version of WordNet
4Comparison against Mohammad and Hirst (2006) was not
possible since they use a sense inventory other than WordNet
(i.e., Roget?s thesaurus) and evaluate their model on artificially
generated sense-tagged data.
P R
BaseR 23.1#?$? 22.7#?$?
BaseS 36.6??$? 35.9??$?
McCarthy 49.0?#$? 43.0?#$?
IR-Model 58.0?#?? 57.0?#??
IRST-DDD 58.3?#?? 58.2?#??
Semcor 62.4?#?$ 62.4?#?$
GAMBLE 65.1?#?$? 65.2?#?$?
Table 4: Comparison of results on Senseval-3 data
(?: sig. diff. from BaseR, #: sig. diff. from BaseS,
?: sig. diff. from McCarthy, $: sig. diff. from IR-
Model, ?: sig. diff. from SemCor; p < 0.01 using
?2 test)
BaseR BaseS Model
P R P R P R N
Noun 27.8 12.2 41.1 41.0 58.1?# 58.0?# 900
Verb 12.8 4.6 20.0 19.9 61.0?# 60.8?# 732
Adj 29.2 5.2 56.5 56.5 50.3? 50.3? 363
Adv 100.0 0.6 100.0 81.2 100.0 81.2 16
All 23.1 22.7 36.6 35.9 58.0?# 57.0?# 2,011
Table 5: Results of best model (High, +Norm,
?Hyp) for Senseval-3 data by part of speech
(?: sig. diff. from BaseR, #: sig. diff. from BaseS;
p < 0.01 using ?2 test)
augmented with domain labels (e.g., economy, ge-
ography). GAMBL (Decadt et al, 2004) is a super-
vised system: a classifier is trained for each ambigu-
ous word using memory-based learning. We also re-
port the performance achieved by defaulting to the
first WordNet entry for a given word and part of
speech. Entries in WordNet are ranked according
to the sense frequency estimates obtained from the
manually annotated SemCor corpus. First senses ob-
tained from SemCor will be naturally less noisy than
those computed by our method which does not make
use of manual annotation in any way. We therefore
consider the WSD performance achieved with Sem-
Cor first senses as an upper bound for automatically
acquired first senses.
Our model significantly outperforms the two
baselines and McCarthy et al (2004b). Its precision
and recall according to individual parts of speech is
shown in Table 5. The model performs comparably
to IRST-DDD and significantly worse than GAM-
BLE. This is not entirely surprising given that GAM-
353
BLE is a supervised system trained on a variety
of manually annotated resources including SemCor,
data from previous Senseval workshops and the ex-
ample sentences in WordNet 1.7.1. GAMBLE is the
only system that significantly outperforms the Sem-
Cor upper bound. Finally, note that our model is
conceptually simpler than McCarthy et al (2004b)
and IRST-DDD. It neither requires a parser (for ob-
taining distributionally similar neighbors) nor any
knowledge other than WordNet (e.g., domain la-
bels). This makes our method portable to languages
for which syntactic analysis tools and elaborate se-
mantic resources are not available.
4.3 Modeling Human Data
Research in psycholinguistics has shown that the
meanings of ambiguous words are not perceived as
equally salient in the absence of a biasing context
(Durkin and Manning, 1989; Twilley et al, 1994).
Rather, language users often ascribe dominant and
subordinate meanings to polysemous words. Previ-
ous studies have elicited intuitions with regard to
word senses using a free association task. For ex-
ample, Durkin and Manning (1989) collected asso-
ciation norms from native speakers for 175 ambigu-
ous words. They asked subjects to read each word
and write down the first meaning that came to mind.
The words were presented out of context. From the
subjects? responses, they computed sense frequen-
cies, which revealed that most words were attributed
a particular meaning with a markedly higher fre-
quency than other meanings.
In this experiment, we examine whether our
model agrees with human intuitions regarding the
prevalence of word senses. We inferred the dominant
meanings for the polysemous words used in Durkin
and Manning (1989). These exhibit a relatively high
degree of ambiguity (the average number of senses
per word is three) and cover a wide variety of parts
of speech (for the full set of words and elicited
sense frequencies see their Appendix A, pp. 501?
609). One stumbling block to using this data are
the meanings associated with the ambiguous words.
These were provided by native English speakers and
may not necessarily correspond to senses described
by trained lexicographers. Fortunately, we were able
to map most of them (except for six which we dis-
carded) on WordNet synsets (version 1.6); two an-
notators performed the mapping by comparing the
sense descriptions provided by Durkin and Manning
act Freq answer Freq
pretense/performance 37 response 81
to perform 30 solution 18
to take action 16
division 12
a deed 3
Table 6: Meaning frequencies for act and answer;
normative data from Durkin and Manning (1989)
to WordNet synsets. The annotators agreed in their
assignments 81% of the time. Disagreements were
resolved through mediation.
Examples of Durkin and Manning?s (1989)
normative data are given in Table 6. The sense
response for answer was mapped to the WordNet
synset {answer, reply, response} (Sense 1),
the sense solution was mapped to the synset
{solution, answer, result, resolution,
solvent} (Sense 2), etc. Durkin and Manning did
not take part of speech ambiguity into account, as
Table 6 shows, subjects came up with meanings
relating to the verb and noun part of speech of act.
We explored the relationship between the sense
frequencies provided by human subjects and those
estimated by our model by computing the Spearman
rank correlation coefficient ?. We obtained sense
frequencies from the BNC using the best model
from Section 4.1 (High, +Norm, ?Hyp). We found
that the resulting sense frequencies were signifi-
cantly correlated with the human sense frequencies
(? = 0.384, p < 0.01). We performed the same ex-
periment using McCarthy et al?s (2004a) model,
which also achieved a significant correlation (? =
0.316, p < 0.01). This result provides an additional
validation of our model as it demonstrates that the
sense frequencies it generates can capture the sense
preferences of naive human subjects (rather than
trained lexicographers).
5 Discussion
In this paper we proposed an IR-based approach
for inducing dominant senses automatically. Our
method estimates the degree of association between
words and their sense descriptions (represented by
synsets in WordNet) simply by querying an IR en-
gine. Evaluation on the Senseval data sets showed
that our model significantly outperformed a naive
random sense baseline and a more competitive one
354
based on synset size. Our method was significantly
better than McCarthy et al (2004b) on Senseval-2
and Senseval-3. On the latter data set, its perfor-
mance was comparable to that of the best unsuper-
vised system (Strapparava et al, 2004).
An important future direction lies in evaluating
the disambiguation potential of our models across
domains and languages. Furthermore, our experi-
ments have relied on WordNet for providing the
appropriate sense descriptions. Future work must
assess whether the models presented in this pa-
per can be extended to alternative sense invento-
ries (e.g., dictionary definitions) that may differ in
granularity and structure. We will also experiment
with a wider range of lexical association measures
for quantifying the similarity of a word and its
synonyms. Examples include odds ratio (Moham-
mad and Hirst, 2006) and Turney?s (2001) IR-based
pointwise mutual information (PMI-IR).
Our experiments revealed that the IR-based model
is particularly good at disambiguating certain parts
of speech (e.g., verbs, see Tables 2 and 5). A promis-
ing direction is the combination of different ranking
models (Brody et al, 2006) and the integration of
dominant sense models with supervised WSD.
Acknowledgments We are grateful to Diana Mc-
Carthy for her help with this work. The au-
thors acknowledge the support of EPSRC (grant
EP/C538447/1).
References
Briscoe, Ted and John Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evaluation.
Las Palmas, Gran Canaria, pages 1499?1504.
Brody, Samuel, Roberto Navigli, and Mirella Lapata. 2006. En-
semble methods for unsupervised WSD. In Proceedings of
the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics. Sydney, Australia, pages 97?
104.
Budanitsky, Alexander and Graeme Hirst. 2001. Semantic
distance in WordNet: An experimental, application-oriented
evaluation of five measures. In Proceedings of the NAACL
Workshop on WordNet and Other Lexical Resources. Pitts-
burgh, PA.
Decadt, Bart, Ve?ronique Hoste, Walter Daelemans, and Antal
van den Bosch. 2004. GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. In Mihalcea and Edmonds
(2004), pages 108?112.
Durkin, Kevin and Jocelyn Manning. 1989. Polysemy and
the subjective lexicon: Semantic relatedness and the salience
of intraword senses. Journal of Psycholinguistic Research
18(6):577?612.
Fellbaum, Christiane, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955.
Oxford: Philological Society.
Gale, William A., Kenneth W. Church, and David Yarowsky.
1992. A method for disambiguating word senses in a large
corpus. Computers and the Humanities 26(5?6):415?439.
Harris, Zellig. 1982. Discourse and sublanguage. In R. Kit-
tredge and J. Lehrberger, editors, Language in Restricted
Semantic Domains, Walter de Gruyter, Berlin; New York,
pages 231?236.
Lin, Dekang. 1998. An information-theoretic definition of sim-
ilarity. In Proceedings of the 15th International Conference
on Machine Learning. Madison, WI, pages 296?304.
Manber, Udi and Sun Wu. 1994. GLIMPSE: a tool to search
through entire file systems. In Proceedings of USENIX Win-
ter 1994 Technical Conference. San Francisco, CA, pages
23?32.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.
2004a. Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics. Barcelona, pages 279?286.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.
2004b. Using automatically acquired predominant senses
for word sense disambiguation. In Mihalcea and Edmonds
(2004), pages 151?154.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of Senseval-3: The 3rd International Workshop on the
Evaluation of Systems for the Semantic Analysis of Text.
Barcelona.
Mohammad, Saif and Graeme Hirst. 2006. Determining word
sense dominance using a thesaurus. In Proceedings of the
11th Conference of the European Chapter of the Association
for Computational Linguistics. Trento, Italy, pages 121?128.
Palmer, Martha, Christiane Fellbaum, Scott Cotton, Lauren
Delfs, and Hoa Trang Dang. 2001. English tasks: All words
and verb lexical sample. In Proceedings of Senseval-2: The
3rd International Workshop on the Evaluation of Systems for
the Semantic Analysis of Text. Toulouse.
Snyder, Benjamin and Martha Palmer. 2004. The English all-
words task. In Mihalcea and Edmonds (2004).
Stokoe, Christopher. 2005. Differentiating homonymy and pol-
ysemy in information retrieval. In Proceedings of the Human
Language Technology Conference and the Conference on
Empirical Methods in Natural Language Processing. Van-
couver, pages 403?410.
Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano. 2004.
Word-sense disambiguation for machine translation. In Mi-
halcea and Edmonds (2004), pages 229?234.
Turney, Peter D. 2001. Mining the web for synonyms: PMI-IR
versus LSA on TOEFL. In Proceedings of the 12th European
Conference on Machine Learning. Freiburg, Germany, pages
491?502.
Twilley, L. C., P. Dixon, D. Taylor, and K. Clark. 1994. Univer-
sity of Alberta norms of relative meaning frequency for 566
homographs. Memory and Cognition 22(1):111?126.
Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne
Koller. 2005. Word-sense disambiguation for machine trans-
lation. In Proceedings of the Human Language Technology
Conference and the Conference on Empirical Methods in
Natural Language Processing. Vancouver, pages 771?778.
Yarowsky, David and Radu Florian. 2002. Evaluating sense dis-
ambiguation across diverse parameter spaces. Natural Lan-
guage Engineering 9(4):293?310.
355
Proceedings of the 43rd Annual Meeting of the ACL, pages 141?148,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Modeling Local Coherence: An Entity-based Approach
Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
regina@csail.mit.edu
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
This paper considers the problem of auto-
matic assessment of local coherence. We
present a novel entity-based representa-
tion of discourse which is inspired by Cen-
tering Theory and can be computed au-
tomatically from raw text. We view co-
herence assessment as a ranking learning
problem and show that the proposed dis-
course representation supports the effec-
tive learning of a ranking function. Our
experiments demonstrate that the induced
model achieves significantly higher ac-
curacy than a state-of-the-art coherence
model.
1 Introduction
A key requirement for any system that produces
text is the coherence of its output. Not surprisingly,
a variety of coherence theories have been devel-
oped over the years (e.g., Mann and Thomson, 1988;
Grosz et al 1995) and their principles have found
application in many symbolic text generation sys-
tems (e.g., Scott and de Souza, 1990; Kibble and
Power, 2004). The ability of these systems to gener-
ate high quality text, almost indistinguishable from
human writing, makes the incorporation of coher-
ence theories in robust large-scale systems partic-
ularly appealing. The task is, however, challenging
considering that most previous efforts have relied on
handcrafted rules, valid only for limited domains,
with no guarantee of scalability or portability (Re-
iter and Dale, 2000). Furthermore, coherence con-
straints are often embedded in complex representa-
tions (e.g., Asher and Lascarides, 2003) which are
hard to implement in a robust application.
This paper focuses on local coherence, which
captures text relatedness at the level of sentence-to-
sentence transitions, and is essential for generating
globally coherent text. The key premise of our work
is that the distribution of entities in locally coherent
texts exhibits certain regularities. This assumption is
not arbitrary ? some of these regularities have been
recognized in Centering Theory (Grosz et al, 1995)
and other entity-based theories of discourse.
The algorithm introduced in the paper automat-
ically abstracts a text into a set of entity transi-
tion sequences, a representation that reflects distri-
butional, syntactic, and referential information about
discourse entities. We argue that this representation
of discourse allows the system to learn the proper-
ties of locally coherent texts opportunistically from
a given corpus, without recourse to manual annota-
tion or a predefined knowledge base.
We view coherence assessment as a ranking prob-
lem and present an efficiently learnable model that
orders alternative renderings of the same informa-
tion based on their degree of local coherence. Such
a mechanism is particularly appropriate for gener-
ation and summarization systems as they can pro-
duce multiple text realizations of the same underly-
ing content, either by varying parameter values, or
by relaxing constraints that control the generation
process. A system equipped with a ranking mech-
anism, could compare the quality of the candidate
outputs, much in the same way speech recognizers
employ language models at the sentence level.
Our evaluation results demonstrate the effective-
ness of our entity-based ranking model within the
general framework of coherence assessment. First,
we evaluate the utility of the model in a text order-
ing task where our algorithm has to select a max-
imally coherent sentence order from a set of can-
didate permutations. Second, we compare the rank-
ings produced by the model against human coher-
ence judgments elicited for automatically generated
summaries. In both experiments, our method yields
141
a significant improvement over a state-of-the-art co-
herence model based on Latent Semantic Analysis
(Foltz et al, 1998).
In the following section, we provide an overview
of existing work on the automatic assessment of lo-
cal coherence. Then, we introduce our entity-based
representation, and describe our ranking model.
Next, we present the experimental framework and
data. Evaluation results conclude the paper.
2 Related Work
Local coherence has been extensively studied within
the modeling framework put forward by Centering
Theory (Grosz et al, 1995; Walker et al, 1998;
Strube and Hahn, 1999; Poesio et al, 2004; Kibble
and Power, 2004). One of the main assumptions un-
derlying Centering is that a text segment which fore-
grounds a single entity is perceived to be more co-
herent than a segment in which multiple entities are
discussed. The theory formalizes this intuition by in-
troducing constraints on the distribution of discourse
entities in coherent text. These constraints are for-
mulated in terms of focus, the most salient entity in
a discourse segment, and transition of focus between
adjacent sentences. The theory also establishes con-
straints on the linguistic realization of focus, sug-
gesting that it is more likely to appear in prominent
syntactic positions (such as subject or object), and to
be referred to with anaphoric expressions.
A great deal of research has attempted to translate
principles of Centering Theory into a robust coher-
ence metric (Miltsakaki and Kukich, 2000; Hasler,
2004; Karamanis et al, 2004). Such a translation is
challenging in several respects: one has to specify
the ?free parameters? of the system (Poesio et al,
2004) and to determine ways of combining the ef-
fects of various constraints. A common methodol-
ogy that has emerged in this research is to develop
and evaluate coherence metrics on manually anno-
tated corpora. For instance, Miltsakaki and Kukich
(2000) annotate a corpus of student essays with tran-
sition information, and show that the distribution of
transitions correlates with human grades. Karamanis
et al (2004) use a similar methodology to compare
coherence metrics with respect to their usefulness
for text planning in generation.
The present work differs from these approaches
in two key respects. First, our method does not re-
quire manual annotation of input texts. We do not
aim to produce complete centering annotations; in-
stead, our inference procedure is based on a dis-
course representation that preserves essential entity
transition information, and can be computed auto-
matically from raw text. Second, we learn patterns
of entity distribution from a corpus, without attempt-
ing to directly implement or refine Centering con-
straints.
3 The Coherence Model
In this section we introduce our entity-based repre-
sentation of discourse. We describe how it can be
computed and how entity transition patterns can be
extracted. The latter constitute a rich feature space
on which probabilistic inference is performed.
Text Representation Each text is represented
by an entity grid, a two-dimensional array that cap-
tures the distribution of discourse entities across text
sentences. We follow Miltsakaki and Kukich (2000)
in assuming that our unit of analysis is the tradi-
tional sentence (i.e., a main clause with accompa-
nying subordinate and adjunct clauses). The rows of
the grid correspond to sentences, while the columns
correspond to discourse entities. By discourse en-
tity we mean a class of coreferent noun phrases. For
each occurrence of a discourse entity in the text, the
corresponding grid cell contains information about
its grammatical role in the given sentence. Each grid
column thus corresponds to a string from a set of
categories reflecting the entity?s presence or absence
in a sequence of sentences. Our set consists of four
symbols: S (subject), O (object), X (neither subject
nor object) and ? (gap which signals the entity?s ab-
sence from a given sentence).
Table 1 illustrates a fragment of an entity grid
constructed for the text in Table 2. Since the text
contains six sentences, the grid columns are of
length six. Consider for instance the grid column for
the entity trial, [O ? ? ? ? X]. It records that trial
is present in sentences 1 and 6 (as O and X respec-
tively) but is absent from the rest of the sentences.
Grid Computation The ability to identify and
cluster coreferent discourse entities is an impor-
tant prerequisite for computing entity grids. The
same entity may appear in different linguistic forms,
e.g., Microsoft Corp., Microsoft, and the company,
but should still be mapped to a single entry in the
grid. Table 1 exemplifies the entity grid for the text
in Table 2 when coreference resolution is taken into
account. To automatically compute entity classes,
142
D
ep
ar
tm
en
t
Tr
ia
l
M
ic
ro
so
ft
Ev
id
en
ce
Co
m
pe
tit
or
s
M
ar
ke
ts
Pr
od
uc
ts
B
ra
nd
s
Ca
se
N
et
sc
ap
e
So
ftw
ar
e
Ta
ct
ic
s
G
ov
er
n
m
en
t
Su
it
Ea
rn
in
gs
1 S O S X O ? ? ? ? ? ? ? ? ? ? 1
2 ? ? O ? ? X S O ? ? ? ? ? ? ? 2
3 ? ? S O ? ? ? ? S O O ? ? ? ? 3
4 ? ? S ? ? ? ? ? ? ? ? S ? ? ? 4
5 ? ? ? ? ? ? ? ? ? ? ? ? S O ? 5
6 ? X S ? ? ? ? ? ? ? ? ? ? ? O 6
Table 1: A fragment of the entity grid. Noun phrases
are represented by their head nouns.
1 [The Justice Department]S is conducting an [anti-trust
trial]O against [Microsoft Corp.]X with [evidence]X that
[the company]S is increasingly attempting to crush
[competitors]O.
2 [Microsoft]O is accused of trying to forcefully buy into
[markets]X where [its own products]S are not competitive
enough to unseat [established brands]O .
3 [The case]S revolves around [evidence]O of [Microsoft]S
aggressively pressuring [Netscape]O into merging
[browser software]O .
4 [Microsoft]S claims [its tactics]S are commonplace and
good economically.
5 [The government]S may file [a civil suit]O ruling
that [conspiracy]S to curb [competition]O through
[collusion]X is [a violation of the Sherman Act]O.
6 [Microsoft]S continues to show [increased earnings]O de-
spite [the trial]X.
Table 2: Summary augmented with syntactic anno-
tations for grid computation.
we employ a state-of-the-art noun phrase coref-
erence resolution system (Ng and Cardie, 2002)
trained on the MUC (6?7) data sets. The system de-
cides whether two NPs are coreferent by exploit-
ing a wealth of features that fall broadly into four
categories: lexical, grammatical, semantic and posi-
tional.
Once we have identified entity classes, the next
step is to fill out grid entries with relevant syn-
tactic information. We employ a robust statistical
parser (Collins, 1997) to determine the constituent
structure for each sentence, from which subjects (s),
objects (o), and relations other than subject or ob-
ject (x) are identified. Passive verbs are recognized
using a small set of patterns, and the underlying deep
grammatical role for arguments involved in the pas-
sive construction is entered in the grid (see the grid
cell o for Microsoft, Sentence 2, Table 2).
When a noun is attested more than once with a dif-
ferent grammatical role in the same sentence, we de-
fault to the role with the highest grammatical rank-
ing: subjects are ranked higher than objects, which
in turn are ranked higher than the rest. For exam-
ple, the entity Microsoft is mentioned twice in Sen-
tence 1 with the grammatical roles x (for Microsoft
Corp.) and s (for the company), but is represented
only by s in the grid (see Tables 1 and 2).
Coherence Assessment We introduce a method
for coherence assessment that is based on grid rep-
resentation. A fundamental assumption underlying
our approach is that the distribution of entities in
coherent texts exhibits certain regularities reflected
in grid topology. Some of these regularities are for-
malized in Centering Theory as constraints on tran-
sitions of local focus in adjacent sentences. Grids of
coherent texts are likely to have some dense columns
(i.e., columns with just a few gaps such as Microsoft
in Table 1) and many sparse columns which will
consist mostly of gaps (see markets, earnings in Ta-
ble 1). One would further expect that entities cor-
responding to dense columns are more often sub-
jects or objects. These characteristics will be less
pronounced in low-coherence texts.
Inspired by Centering Theory, our analysis re-
volves around patterns of local entity transitions.
A local entity transition is a sequence {S,O,X,?}n
that represents entity occurrences and their syntactic
roles in n adjacent sentences. Local transitions can
be easily obtained from a grid as continuous subse-
quences of each column. Each transition will have a
certain probability in a given grid. For instance, the
probability of the transition [S ?] in the grid from
Table 1 is 0.08 (computed as a ratio of its frequency
(i.e., six) divided by the total number of transitions
of length two (i.e., 75)). Each text can thus be viewed
as a distribution defined over transition types. We
believe that considering all entity transitions may
uncover new patterns relevant for coherence assess-
ment.
We further refine our analysis by taking into ac-
count the salience of discourse entities. Centering
and other discourse theories conjecture that the way
an entity is introduced and mentioned depends on
its global role in a given discourse. Therefore, we
discriminate between transitions of salient entities
and the rest, collecting statistics for each group sep-
arately. We identify salient entities based on their
143
S
S
S
O
S
X
S
?
O
S
O
O
O
X
O
?
X
S
X
O
X
X
X
?
?
S
?
O
?
X
?
?
d1 0 0 0 .03 0 0 0 .02 .07 0 0 .12 .02 .02 .05 .25
d2 0 0 0 .02 0 .07 0 .02 0 0 .06 .04 0 0 0 .36
d3 .02 0 0 .03 0 0 0 .06 0 0 0 .05 .03 .07 .07 .29
Table 3: Example of a feature-vector document rep-
resentation using all transitions of length two given
syntactic categories: S, O, X, and ?.
frequency,1 following the widely accepted view that
the occurrence frequency of an entity correlates with
its discourse prominence (Morris and Hirst, 1991;
Grosz et al, 1995).
Ranking We view coherence assessment as a
ranking learning problem. The ranker takes as input
a set of alternative renderings of the same document
and ranks them based on their degree of local coher-
ence. Examples of such renderings include a set of
different sentence orderings of the same text and a
set of summaries produced by different systems for
the same document. Ranking is more suitable than
classification for our purposes since in text gener-
ation, a system needs a scoring function to com-
pare among alternative renderings. Furthermore, it
is clear that coherence assessment is not a categori-
cal decision but a graded one: there is often no single
coherent rendering of a given text but many different
possibilities that can be partially ordered.
As explained previously, coherence constraints
are modeled in the grid representation implicitly by
entity transition sequences. To employ a machine
learning algorithm to our problem, we encode tran-
sition sequences explicitly using a standard feature
vector notation. Each grid rendering j of a docu-
ment di is represented by a feature vector ?(xi j) =
(p1(xi j), p2(xi j), . . . , pm(xi j)), where m is the num-
ber of all predefined entity transitions, and pt(xi j)
the probability of transition t in grid xi j . Note that
considerable latitude is available when specifying
the transition types to be included in a feature vec-
tor. These can be all transitions of a given length
(e.g., two or three) or the most frequent transitions
within a document collection. An example of a fea-
ture space with transitions of length two is illustrated
in Table 3.
The training set consists of ordered pairs of ren-
derings (xi j,xik), where xi j and xik are renderings
1The frequency threshold is empirically determined on the
development set. See Section 5 for further discussion.
of the same document di, and xi j exhibits a higher
degree of coherence than xik . Without loss of gen-
erality, we assume j > k. The goal of the training
procedure is to find a parameter vector ~w that yields
a ?ranking score? function ~w ? ?(xi j), which mini-
mizes the number of violations of pairwise rankings
provided in the training set. Thus, the ideal ~w would
satisfy the condition ~w ?(?(xi j)??(xik)) > 0 ? j, i,k
such that j > k. The problem is typically treated as
a Support Vector Machine constraint optimization
problem, and can be solved using the search tech-
nique described in Joachims (2002a). This approach
has been shown to be highly effective in various
tasks ranging from collaborative filtering (Joachims,
2002a) to parsing (Toutanova et al, 2004).
In our ranking experiments, we use Joachims?
(2002a) SVMlight package for training and testing
with all parameters set to their default values.
4 Evaluation Set-Up
In this section we describe two evaluation tasks that
assess the merits of the coherence modeling frame-
work introduced above. We also give details regard-
ing our data collection, and parameter estimation.
Finally, we introduce the baseline method used for
comparison with our approach.
4.1 Text Ordering
Text structuring algorithms (Lapata, 2003; Barzi-
lay and Lee, 2004; Karamanis et al, 2004)
are commonly evaluated by their performance at
information-ordering. The task concerns determin-
ing a sequence in which to present a pre-selected set
of information-bearing items; this is an essential step
in concept-to-text generation, multi-document sum-
marization, and other text-synthesis problems. Since
local coherence is a key property of any well-formed
text, our model can be used to rank alternative sen-
tence orderings. We do not assume that local coher-
ence is sufficient to uniquely determine the best or-
dering ? other constraints clearly play a role here.
However, we expect that the accuracy of a coherence
model is reflected in its performance in the ordering
task.
Data To acquire a large collection for training
and testing, we create synthetic data, wherein the
candidate set consists of a source document and per-
mutations of its sentences. This framework for data
acquisition is widely used in evaluation of ordering
algorithms as it enables large scale automatic evalu-
144
ation. The underlying assumption is that the orig-
inal sentence order in the source document must
be coherent, and so we should prefer models that
rank it higher than other permutations. Since we do
not know the relative quality of different permuta-
tions, our corpus includes only pairwise rankings
that comprise the original document and one of its
permutations. Given k original documents, each with
n randomly generated permutations, we obtain k ? n
(trivially) annotated pairwise rankings for training
and testing.
Using the technique described above, we col-
lected data in two different genres: newspaper ar-
ticles and accident reports written by government
officials. The first collection consists of Associated
Press articles from the North American News Cor-
pus on the topic of natural disasters. The second in-
cludes narratives from the National Transportation
Safety Board?s database2 . Both sets have documents
of comparable length ? the average number of sen-
tences is 10.4 and 11.5, respectively. For each set, we
used 100 source articles with 20 randomly generated
permutations for training. The same number of pair-
wise rankings (i.e., 2000) was used for testing. We
held out 10 documents (i.e., 200 pairwise rankings)
from the training data for development purposes.
4.2 Summary Evaluation
We further test the ability of our method to assess
coherence by comparing model induced rankings
against rankings elicited by human judges. Admit-
tedly, the information ordering task only partially
approximates degrees of coherence violation using
different sentence permutations of a source docu-
ment. A stricter evaluation exercise concerns the as-
sessment of texts with naturally occurring coherence
violations as perceived by human readers. A rep-
resentative example of such texts are automatically
generated summaries which often contain sentences
taken out of context and thus display problems with
respect to local coherence (e.g., dangling anaphors,
thematically unrelated sentences). A model that ex-
hibits high agreement with human judges not only
accurately captures the coherence properties of the
summaries in question, but ultimately holds promise
for the automatic evaluation of machine-generated
texts. Existing automatic evaluation measures such
as BLEU (Papineni et al, 2002) and ROUGE (Lin
2The collections are available from http://www.csail.
mit.edu/regina/coherence/.
and Hovy, 2003), are not designed for the coherence
assessment task, since they focus on content similar-
ity between system output and reference texts.
Data Our evaluation was based on materi-
als from the Document Understanding Conference
(DUC, 2003), which include multi-document sum-
maries produced by human writers and by automatic
summarization systems. In order to learn a rank-
ing, we require a set of summaries, each of which
have been rated in terms of coherence. We therefore
elicited judgments from human subjects.3 We ran-
domly selected 16 input document clusters and five
systems that had produced summaries for these sets,
along with summaries composed by several humans.
To ensure that we do not tune a model to a particu-
lar system, we used the output summaries of distinct
systems for training and testing. Our set of train-
ing materials contained 4 ? 16 summaries (average
length 4.8), yielding (42
)
?16 = 96 pairwise rankings.
In a similar fashion, we obtained 32 pairwise rank-
ings for the test set. Six documents from the training
data were used as a development set.
Coherence ratings were obtained during an elic-
itation study by 177 unpaid volunteers, all native
speakers of English. The study was conducted re-
motely over the Internet. Participants first saw a set
of instructions that explained the task, and defined
the notion of coherence using multiple examples.
The summaries were randomized in lists following a
Latin square design ensuring that no two summaries
in a given list were generated from the same docu-
ment cluster. Participants were asked to use a seven
point scale to rate how coherent the summaries were
without having seen the source texts. The ratings
(approximately 23 per summary) given by our sub-
jects were averaged to provide a rating between 1
and 7 for each summary.
The reliability of the collected judgments is cru-
cial for our analysis; we therefore performed sev-
eral tests to validate the quality of the annota-
tions. First, we measured how well humans agree
in their coherence assessment. We employed leave-
one-out resampling4 (Weiss and Kulikowski, 1991),
by correlating the data obtained from each par-
ticipant with the mean coherence ratings obtained
from all other participants. The inter-subject agree-
3The ratings are available from http://homepages.inf.
ed.ac.uk/mlap/coherence/.
4We cannot apply the commonly used Kappa statistic for
measuring agreement since it is appropriate for nominal scales,
whereas our summaries are rated on an ordinal scale.
145
ment was r = .768. Second, we examined the ef-
fect of different types of summaries (human- vs.
machine-generated.) An ANOVA revealed a reliable
effect of summary type: F(1;15) = 20.38, p < 0.01
indicating that human summaries are perceived as
significantly more coherent than system-generated
ones. Finally, the judgments of our participants ex-
hibit a significant correlation with DUC evaluations
(r = .41, p < 0.01).
4.3 Parameter Estimation
Our model has two free parameters: the frequency
threshold used to identify salient entities and the
length of the transition sequence. These parameters
were tuned separately for each data set on the corre-
sponding held-out development set. For our ordering
and summarization experiments, optimal salience-
based models were obtained for entities with fre-
quency ? 2. The optimal transition length was ? 3
for ordering and ? 2 for summarization.
4.4 Baseline
We compare our algorithm against the coherence
model proposed by Foltz et al (1998) which mea-
sures coherence as a function of semantic related-
ness between adjacent sentences. Semantic related-
ness is computed automatically using Latent Se-
mantic Analysis (LSA, Landauer and Dumais 1997)
from raw text without employing syntactic or other
annotations. This model is a good point of compari-
son for several reasons: (a) it is fully automatic, (b) it
is a not a straw-man baseline; it correlates reliably
with human judgments and has been used to analyze
discourse structure, and (c) it models an aspect of
coherence which is orthogonal to ours (their model
is lexicalized).
Following Foltz et al (1998) we constructed
vector-based representations for individual words
from a lemmatized version of the North American
News Text Corpus5 (350 million words) using a
term-document matrix. We used singular value de-
composition to reduce the semantic space to 100 di-
mensions obtaining thus a space similar to LSA. We
represented the meaning of a sentence as a vector
by taking the mean of the vectors of its words. The
similarity between two sentences was determined by
measuring the cosine of their means. An overall text
coherence measure was obtained by averaging the
cosines for all pairs of adjacent sentences.
5Our selection of this corpus was motivated by its similarity
to the DUC corpus which primarily consists of news stories.
In sum, each text was represented by a single
feature, its sentence-to-sentence semantic similar-
ity. During training, the ranker learns an appropriate
threshold value for this feature.
4.5 Evaluation Metric
Model performance was assessed in the same way
for information ordering and summary evaluation.
Given a set of pairwise rankings, we measure accu-
racy as the ratio of correct predictions made by the
model over the size of the test set. In this setup, ran-
dom prediction results in an accuracy of 50%.
5 Results
The evaluation of our coherence model was driven
by two questions: (1) How does the proposed model
compare to existing methods for coherence assess-
ment that make use of distinct representations?
(2) What is the contribution of linguistic knowledge
to the model?s performance? Table 4 summarizes the
accuracy of various configurations of our model for
the ordering and coherence assessment tasks.
We first compared a linguistically rich grid model
that incorporates coreference resolution, expressive
syntactic information, and a salience-based feature
space (Coreference+Syntax+Salience) against the
LSA baseline (LSA). As can be seen in Table 4, the
grid model outperforms the baseline in both ordering
and summary evaluation tasks, by a wide margin.
We conjecture that this difference in performance
stems from the ability of our model to discriminate
between various patterns of local sentence transi-
tions. In contrast, the baseline model only measures
the degree of overlap across successive sentences,
without taking into account the properties of the en-
tities that contribute to the overlap. Not surprisingly,
the difference between the two methods is more pro-
nounced for the second task ? summary evaluation.
Manual inspection of our summary corpus revealed
that low-quality summaries often contain repetitive
information. In such cases, simply knowing about
high cross-sentential overlap is not sufficient to dis-
tinguish a repetitive summary from a well-formed
one.
In order to investigate the contribution of linguis-
tic knowledge on model performance we compared
the full model introduced above against models us-
ing more impoverished representations. We focused
on three sources of linguistic knowledge ? syntax,
coreference resolution, and salience ? which play
146
Model Ordering (Set1) Ordering (Set2) Summarization
Coreference+Syntax+Salience 87.3 90.4 68.8
Coreference+Salience 86.9 88.3 62.5
Syntax+Salience 83.4 89.7 81.3
Coreference+Syntax 76.5 88.8 75.0
LSA 72.1 72.1 25.0
Table 4: Ranking accuracy measured as the fraction of correct pairwise rankings in the test set.
a prominent role in Centering analyses of discourse
coherence. An additional motivation for our study is
exploration of the trade-off between robustness and
richness of linguistic annotations. NLP tools are typ-
ically trained on human-authored texts, and may de-
teriorate in performance when applied to automati-
cally generated texts with coherence violations.
Syntax To evaluate the effect of syntactic
knowledge, we eliminated the identification of
grammatical relations from our grid computation
and recorded solely whether an entity is present or
absent in a sentence. This leaves only the coref-
erence and salience information in the model, and
the results are shown in Table 4 under (Corefer-
ence+Salience). The omission of syntactic informa-
tion causes a uniform drop in performance on both
tasks, which confirms its importance for coherence
analysis.
Coreference To measure the effect of fully-
fledged coreference resolution, we constructed en-
tity classes simply by clustering nouns on the ba-
sis of their identity. In other words, each noun in a
text corresponds to a different entity in a grid, and
two nouns are considered coreferent only if they
are identical. The performance of the model (Syn-
tax+Salience) is shown in the third row of Table 4.
While coreference resolution improved model
performance in ordering, it caused a decrease in ac-
curacy in summary evaluation. This drop in per-
formance can be attributed to two factors related
to the nature of our corpus ? machine-generated
texts. First, an automatic coreference resolution tool
expectedly decreases in accuracy because it was
trained on well-formed human-authored texts. Sec-
ond, automatic summarization systems do not use
anaphoric expressions as often as humans do. There-
fore, a simple entity clustering method is more suit-
able for automatic summaries.
Salience Finally, we evaluate the contribution
of salience information by comparing our orig-
inal model (Coreference+Syntax+Salience) which
accounts separately for patterns of salient and
non-salient entities against a model that does not
attempt to discriminate between them (Corefer-
ence+Syntax). Our results on the ordering task indi-
cate that models that take salience information into
account consistently outperform models that do not.
The effect of salience is less pronounced for the
summarization task when it is combined with coref-
erence information (Coreference + Salience). This is
expected, since accurate identification of coreferring
entities is prerequisite to deriving accurate salience
models. However, as explained above, our automatic
coreference tool introduces substantial noise in our
representation. Once this noise is removed (see Syn-
tax+Salience), the salience model has a clear advan-
tage over the other models.
6 Discussion and Conclusions
In this paper we proposed a novel framework for
representing and measuring text coherence. Central
to this framework is the entity grid representation
of discourse which we argue captures important pat-
terns of sentence transitions. We re-conceptualize
coherence assessment as a ranking task and show
that our entity-based representation is well suited for
learning an appropriate ranking function; we achieve
good performance on text ordering and summary co-
herence evaluation.
On the linguistic side, our results yield empirical
support to some of Centering Theory?s main claims.
We show that coherent texts are characterized by
transitions with particular properties which do not
hold for all discourses. Our work, however, not only
validates these findings, but also quantitatively mea-
sures the predictive power of various linguistic fea-
tures for the task of coherence assessment.
An important future direction lies in augmenting
our entity-based model with lexico-semantic knowl-
edge. One way to achieve this goal is to cluster enti-
ties based on their semantic relatedness, thereby cre-
147
ating a grid representation over lexical chains (Mor-
ris and Hirst, 1991). An entirely different approach
is to develop fully lexicalized models, akin to tra-
ditional language models. Cache language mod-
els (Kuhn and Mori, 1990) seem particularly promis-
ing in this context.
In the discourse literature, entity-based theories
are primarily applied at the level of local coherence,
while relational models, such as Rhetorical Structure
Theory (Mann and Thomson, 1988; Marcu, 2000),
are used to model the global structure of discourse.
We plan to investigate how to combine the two for
improved prediction on both local and global levels,
with the ultimate goal of handling longer texts.
Acknowledgments
The authors acknowledge the support of the National Science
Foundation (Barzilay; CAREER grant IIS-0448168 and grant
IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).
We are grateful to Claire Cardie and Vincent Ng for providing
us the results of their system on our data. Thanks to Eli Barzilay,
Eugene Charniak, Michael Elhadad, Noemie Elhadad, Frank
Keller, Alex Lascarides, Igor Malioutov, Smaranda Muresan,
Martin Rinard, Kevin Simler, Caroline Sporleder, Chao Wang,
Bonnie Webber and three anonymous reviewers for helpful
comments and suggestions. Any opinions, findings, and con-
clusions or recommendations expressed above are those of the
authors and do not necessarily reflect the views of the National
Science Foundation or EPSRC.
References
N. Asher, A. Lascarides. 2003. Logics of Conversation.
Cambridge University Press.
R. Barzilay, L. Lee. 2004. Catching the drift: Probabilis-
tic content models, with applications to generation and
summarization. In Proceedings of HLT-NAACL, 113?
120.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the
ACL/EACL, 16?23.
P. W. Foltz, W. Kintsch, T. K. Landauer. 1998. Textual
coherence using latent semantic analysis. Discourse
Processes, 25(2&3):285?307.
B. Grosz, A. K. Joshi, S. Weinstein. 1995. Centering:
A framework for modeling the local coherence of dis-
course. Computational Linguistics, 21(2):203?225.
L. Hasler. 2004. An investigation into the use of cen-
tering transitions for summarisation. In Proceedings
of the 7th Annual CLUK Research Colloquium, 100?
107, University of Birmingham.
T. Joachims. 2002a. Optimizing search engines using
clickthrough data. In Proceesings of KDD, 133?142.
N. Karamanis, M. Poesio, C. Mellish, J. Oberlander.
2004. Evaluating centering-based metrics of coher-
ence for text structuring using a reliably annotated cor-
pus. In Proceedings of the ACL, 391?398.
R. Kibble, R. Power. 2004. Optimising referential co-
herence in text generation. Computational Linguistics,
30(4):401?416.
R. Kuhn, R. D. Mori. 1990. A cache-based natural lan-
guage model for speech recognition. IEEE Transac-
tions on PAMI, 12(6):570?583.
T. K. Landauer, S. T. Dumais. 1997. A solution to Plato?s
problem: The latent semantic analysis theory of ac-
quisition, induction and representation of knowledge.
Psychological Review, 104(2):211?240.
M. Lapata. 2003. Probabilistic text structuring: Exper-
iments with sentence ordering. In Proceedings of the
ACL, 545?552.
C.-Y. Lin, E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, 71?78.
W. C. Mann, S. A. Thomson. 1988. Rhetorical structure
theory. Text, 8(3):243?281.
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
E. Miltsakaki, K. Kukich. 2000. The role of centering
theory?s rough-shift in the teaching and evaluation of
writing skills. In Proceedings of the ACL, 408?415.
J. Morris, G. Hirst. 1991. Lexical cohesion computed by
thesaural relations as an indicator of the structure of
text. Computational Linguistics, 1(17):21?43.
V. Ng, C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of the ACL, 104?111.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu:
a method for automatic evaluation of machine transla-
tion. In Proceedings of the ACL, 311?318.
M. Poesio, R. Stevenson, B. D. Eugenio, J. Hitzeman.
2004. Centering: a parametric theory and its instan-
tiations. Computational Linguistics, 30(3):309?363.
E. Reiter, R. Dale. 2000. Building Natural-Language
Generation Systems. Cambridge University Press.
D. Scott, C. S. de Souza. 1990. Getting the message
across in RST-based text generation. In R. Dale,
C. Mellish, M. Zock, eds., Current Research in Nat-
ural Language Generation, 47?73. Academic Press.
M. Strube, U. Hahn. 1999. Functional centering ?
grounding referential coherence in information struc-
ture. Computational Linguistics, 25(3):309?344.
K. Toutanova, P. Markova, C. D. Manning. 2004. The
leaf projection path view of parse trees: Exploring
string kernels for HPSG parse selection. In Proceed-
ings of the EMNLP, 166?173.
M. Walker, A. Joshi, E. Prince, eds. 1998. Centering
Theory in Discourse. Clarendon Press.
S. M. Weiss, C. A. Kulikowski. 1991. Computer Sys-
tems that Learn: Classification and Prediction Meth-
ods from, Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann.
148
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 97?104,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ensemble Methods for Unsupervised WSD
Samuel Brody
School of Informatics
University of Edinburgh
s.brody@sms.ed.ac.uk
Roberto Navigli
Dipartimento di Informatica
Universita di Roma ?La Sapienza?
navigli@di.uniroma1.it
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
Combination methods are an effective way
of improving system performance. This
paper examines the benefits of system
combination for unsupervised WSD. We
investigate several voting- and arbiter-
based combination strategies over a di-
verse pool of unsupervised WSD systems.
Our combination methods rely on predom-
inant senses which are derived automati-
cally from raw text. Experiments using the
SemCor and Senseval-3 data sets demon-
strate that our ensembles yield signifi-
cantly better results when compared with
state-of-the-art.
1 Introduction
Word sense disambiguation (WSD), the task of
identifying the intended meanings (senses) of
words in context, holds promise for many NLP
applications requiring broad-coverage language
understanding. Examples include summarization,
question answering, and text simplification. Re-
cent studies have also shown that WSD can ben-
efit machine translation (Vickrey et al, 2005) and
information retrieval (Stokoe, 2005).
Given the potential of WSD for many NLP
tasks, much work has focused on the computa-
tional treatment of sense ambiguity, primarily us-
ing data-driven methods. Most accurate WSD sys-
tems to date are supervised and rely on the avail-
ability of training data, i.e., corpus occurrences of
ambiguous words marked up with labels indicat-
ing the appropriate sense given the context (see
Mihalcea and Edmonds 2004 and the references
therein). A classifier automatically learns disam-
biguation cues from these hand-labeled examples.
Although supervised methods typically achieve
better performance than unsupervised alternatives,
their applicability is limited to those words for
which sense labeled data exists, and their accu-
racy is strongly correlated with the amount of la-
beled data available (Yarowsky and Florian, 2002).
Furthermore, obtaining manually labeled corpora
with word senses is costly and the task must be
repeated for new domains, languages, or sense in-
ventories. Ng (1997) estimates that a high accu-
racy domain independent system for WSD would
probably need a corpus of about 3.2 million sense
tagged words. At a throughput of one word per
minute (Edmonds, 2000), this would require about
27 person-years of human annotation effort.
This paper focuses on unsupervised methods
which we argue are useful for broad coverage
sense disambiguation. Unsupervised WSD algo-
rithms fall into two general classes: those that per-
form token-based WSD by exploiting the simi-
larity or relatedness between an ambiguous word
and its context (e.g., Lesk 1986); and those that
perform type-based WSD, simply by assigning
all instances of an ambiguous word its most fre-
quent (i.e., predominant) sense (e.g., McCarthy
et al 2004; Galley and McKeown 2003). The pre-
dominant senses are automatically acquired from
raw text without recourse to manually annotated
data. The motivation for assigning all instances
of a word to its most prevalent sense stems from
the observation that current supervised approaches
rarely outperform the simple heuristic of choos-
ing the most common sense in the training data,
despite taking local context into account (Hoste
et al, 2002). Furthermore, the approach allows
sense inventories to be tailored to specific do-
mains.
The work presented here evaluates and com-
pares the performance of well-established unsu-
pervised WSD algorithms. We show that these
algorithms yield sufficiently diverse outputs, thus
motivating the use of combination methods for im-
proving WSD performance. While combination
approaches have been studied previously for su-
pervised WSD (Florian et al, 2002), their use
in an unsupervised setting is, to our knowledge,
novel. We examine several existing and novel
combination methods and demonstrate that our
combined systems consistently outperform the
97
state-of-the-art (e.g., McCarthy et al 2004). Im-
portantly, our WSD algorithms and combination
methods do not make use of training material in
any way, nor do they use the first sense informa-
tion available in WordNet.
In the following section, we briefly describe the
unsupervised WSD algorithms considered in this
paper. Then, we present a detailed comparison of
their performance on SemCor (Miller et al, 1993).
Next, we introduce our system combination meth-
ods and report on our evaluation experiments. We
conclude the paper by discussing our results.
2 The Disambiguation Algorithms
In this section we briefly describe the unsuper-
vised WSD algorithms used in our experiments.
We selected methods that vary along the follow-
ing dimensions: (a) the type of WSD performed
(i.e., token-based vs. type-based), (b) the represen-
tation and size of the context surrounding an am-
biguous word (i.e., graph-based vs. word-based,
document vs. sentence), and (c) the number and
type of semantic relations considered for disam-
biguation. We base most of our discussion below
on the WordNet sense inventory; however, the ap-
proaches are not limited to this particular lexicon
but could be adapted for other resources with tra-
ditional dictionary-like sense definitions and alter-
native structure.
Extended Gloss Overlap Gloss Overlap was
originally introduced by Lesk (1986) for perform-
ing token-based WSD. The method assigns a sense
to a target word by comparing the dictionary defi-
nitions of each of its senses with those of the words
in the surrounding context. The sense whose defi-
nition has the highest overlap (i.e., words in com-
mon) with the context words is assumed to be the
correct one. Banerjee and Pedersen (2003) aug-
ment the dictionary definition (gloss) of each sense
with the glosses of related words and senses. The
extended glosses increase the information avail-
able in estimating the overlap between ambiguous
words and their surrounding context.
The range of relationships used to extend the
glosses is a parameter, and can be chosen from
any combination of WordNet relations. For every
sense sk of the target word we estimate:
SenseScore(sk) = ?
Rel?Relations
Overlap(context,Rel(sk))
where context is a simple (space separated) con-
catenation of all words wi for ?n ? i ? n, i 6= 0 ina context window of length ?n around the target
word w0. The overlap scoring mechanism is also
parametrized and can be adjusted to take the into
account gloss length or to ignore function words.
Distributional and WordNet Similarity
McCarthy et al (2004) propose a method for
automatically ranking the senses of ambiguous
words from raw text. Key in their approach is the
observation that distributionally similar neighbors
often provide cues about a word?s senses. As-
suming that a set of neighbors is available, sense
ranking is equivalent to quantifying the degree
of similarity among the neighbors and the sense
descriptions of the polysemous word.
Let N(w) = {n1,n2, . . . ,nk} be the k most (dis-tributionally) similar words to an ambiguous tar-
get word w and senses(w) = {s1,s2, . . .sn} the setof senses for w. For each sense si and for eachneighbor n j, the algorithm selects the neighbor?ssense which has the highest WordNet similarity
score (wnss) with regard to si. The ranking scoreof sense si is then increased as a function of theWordNet similarity score and the distributional
similarity score (dss) between the target word and
the neighbor:
RankScore(si) = ?
n j?Nw
dss(w,n j)
wnss(si,n j)
?
s?i?senses(w)
wnss(s?i,n j)
where wnss(si,n j) = max
nsx?senses(n j)
wnss(si,nsx).
The predominant sense is simply the sense with
the highest ranking score (RankScore) and can be
consequently used to perform type-based disam-
biguation. The method presented above has four
parameters: (a) the semantic space model repre-
senting the distributional properties of the target
words (it is acquired from a large corpus repre-
sentative of the domain at hand and can be aug-
mented with syntactic relations such as subject or
object), (b) the measure of distributional similarity
for discovering neighbors (c) the number of neigh-
bors that the ranking score takes into account, and
(d) the measure of sense similarity.
Lexical Chains Lexical cohesion is often rep-
resented via lexical chains, i.e., sequences of re-
lated words spanning a topical text unit (Mor-
ris and Hirst, 1991). Algorithms for computing
lexical chains often perform WSD before infer-
ring which words are semantically related. Here
we describe one such disambiguation algorithm,
proposed by Galley and McKeown (2003), while
omitting the details of creating the lexical chains
themselves.
Galley and McKeown?s (2003) method consists
of two stages. First, a graph is built represent-
ing all possible interpretations of the target words
98
in question. The text is processed sequentially,
comparing each word against all words previously
read. If a relation exists between the senses of the
current word and any possible sense of a previous
word, a connection is formed between the appro-
priate words and senses. The strength of the con-
nection is a function of the type of relationship and
of the distance between the words in the text (in
terms of words, sentences and paragraphs). Words
are represented as nodes in the graph and seman-
tic relations as weighted edges. Again, the set of
relations being considered is a parameter that can
be tuned experimentally.
In the disambiguation stage, all occurrences of a
given word are collected together. For each sense
of a target word, the strength of all connections
involving that sense are summed, giving that sense
a unified score. The sense with the highest unified
score is chosen as the correct sense for the target
word. In subsequent stages the actual connections
comprising the winning unified score are used as a
basis for computing the lexical chains.
The algorithm is based on the ?one sense per
discourse? hypothesis and uses information from
every occurrence of the ambiguous target word in
order to decide its appropriate sense. It is there-
fore a type-based algorithm, since it tries to de-
termine the sense of the word in the entire doc-
ument/discourse at once, and not separately for
each instance.
Structural Semantic Interconnections In-
spired by lexical chains, Navigli and Velardi
(2005) developed Structural Semantic Intercon-
nections (SSI), a WSD algorithm which makes use
of an extensive lexical knowledge base. The latter
is primarily based on WordNet and its standard re-
lation set (i.e., hypernymy, meronymy, antonymy,
similarity, nominalization, pertainymy) but is also
enriched with collocation information represent-
ing semantic relatedness between sense pairs. Col-
locations are gathered from existing resources
(such as the Oxford Collocations, the Longman
Language Activator, and collocation web sites).
Each collocation is mapped to the WordNet sense
inventory in a semi-automatic manner (Navigli,
2005) and transformed into a relatedness edge.
Given a local word context C = {w1, ...,wn},SSI builds a graph G = (V,E) such that V =
n
S
i=1
senses(wi) and (s,s?) ? E if there is at least
one interconnection j between s (a sense of the
word) and s? (a sense of its context) in the lexical
knowledge base. The set of valid interconnections
is determined by a manually-created context-free
Method WSD Context Relations
LexChains types document first-order
Overlap tokens sentence first-order
Similarity types corpus higher-order
SSI tokens sentence higher-order
Table 1: Properties of the WSD algorithms
grammar consisting of a small number of rules.
Valid interconnections are computed in advance
on the lexical database, not at runtime.
Disambiguation is performed in an iterative
fashion. At each step, for each sense s of a word
in C (the set of senses of words yet to be disam-
biguated), SSI determines the degree of connectiv-
ity between s and the other senses in C :
SSIScore(s) =
?
s??C\{s} ?j?Interconn(s,s?)
1
length( j)
?
s??C\{s}
|Interconn(s,s?)|
where Interconn(s,s?) is the set of interconnec-
tions between senses s and s?. The contribution of a
single interconnection is given by the reciprocal of
its length, calculated as the number of edges con-
necting its ends. The overall degree of connectiv-
ity is then normalized by the number of contribut-
ing interconnections. The highest ranking sense s
of word wi is chosen and the senses of wi are re-moved from the context C . The procedure termi-
nates when either C is the empty set or there is no
sense such that its SSIScore exceeds a fixed thresh-
old.
Summary The properties of the different
WSD algorithms just described are summarized
in Table 1. The methods vary in the amount of
data they employ for disambiguation. SSI and Ex-
tended Gloss Overlap (Overlap) rely on sentence-
level information for disambiguation whereas Mc-
Carthy et al (2004) (Similarity) and Galley and
McKeown (2003) (LexChains) utilize the entire
document or corpus. This enables the accumula-
tion of large amounts of data regarding the am-
biguous word, but does not allow separate consid-
eration of each individual occurrence of that word.
LexChains and Overlap take into account a re-
stricted set of semantic relations (paths of length
one) between any two words in the whole docu-
ment, whereas SSI and Similarity use a wider set
of relations.
99
3 Experiment 1: Comparison of
Unsupervised Algorithms for WSD
3.1 Method
We evaluated the disambiguation algorithms out-
lined above on two tasks: predominant sense ac-
quisition and token-based WSD. As previously
explained, Overlap and SSI were not designed for
acquiring predominant senses (see Table 1), but
a token-based WSD algorithm can be trivially
modified to acquire predominant senses by dis-
ambiguating every occurrence of the target word
in context and selecting the sense which was cho-
sen most frequently. Type-based WSD algorithms
simply tag all occurrences of a target word with its
predominant sense, disregarding the surrounding
context.
Our first set of experiments was conducted on
the SemCor corpus, on the same 2,595 polyse-
mous nouns (53,674 tokens) used as a test set by
McCarthy et al (2004). These nouns were attested
in SemCor with a frequency > 2 and occurred in
the British National Corpus (BNC) more than 10
times. We used the WordNet 1.7.1 sense inventory.
The following notation describes our evaluation
measures: W is the set of all noun types in the
SemCor corpus (|W | = 2,595), and W f is the setof noun types with a dominant sense. senses(w)
is the set of senses for noun type w, while fs(w)and fm(w) refer to w?s first sense according to theSemCor gold standard and our algorithms, respec-
tively. Finally, T (w) is the set of tokens of w and
senses(t) denotes the sense assigned to token t ac-cording to SemCor.
We first measure how well our algorithms can
identify the predominant sense, if one exists:
Accps =
|{w ?W f | fs(w) = fm(w)}|
|Wf |
A baseline for this task can be easily defined for
each word type by selecting a sense at random
from its sense inventory and assuming that this is
the predominant sense:
Baselinesr =
1
|Wf | ?w ?W f
1
|senses(w)|
We evaluate the algorithms? disambiguation per-
formance by measuring the ratio of tokens for
which our models choose the right sense:
Accwsd =
?
w?W
|{t ? T (w)| fm(w) = senses(t)}|
?
w?W
|T (w)|
In the predominant sense detection task, in case of
ties in SemCor, any one of the predominant senses
was considered correct. Also, all algorithms were
designed to randomly choose from among the top
scoring options in case of a tie in the calculated
scores. This introduces a small amount of ran-
domness (less than 0.5%) in the accuracy calcu-
lation, and was done to avoid the pitfall of default-
ing to the first sense listed in WordNet, which is
usually the actual predominant sense (the order of
senses in WordNet is based primarily on the Sem-
Cor sense distribution).
3.2 Parameter Settings
We did not specifically tune the parameters of our
WSD algorithms on the SemCor corpus, as our
goal was to use hand labeled data solely for testing
purposes. We selected parameters that have been
considered ?optimal? in the literature, although
admittedly some performance gains could be ex-
pected had parameter optimization taken place.
For Overlap, we used the semantic relations
proposed by Banerjee and Pedersen (2003),
namely hypernyms, hyponyms, meronyms,
holonyms, and troponym synsets. We also
adopted their overlap scoring mechanism which
treats each gloss as a bag of words and assigns an
n word overlap the score of n2. Function words
were not considered in the overlap computation.
For LexChains, we used the relations reported
in Galley and McKeown (2003). These are all
first-order WordNet relations, with the addition of
the siblings ? two words are considered siblings
if they are both hyponyms of the same hypernym.
The relations have different weights, depending
on their type and the distance between the words
in the text. These weights were imported from
Galley and McKeown into our implementation
without modification.
Because the SemCor corpus is relatively small
(less than 700,00 words), it is not ideal for con-
structing a neighbor thesaurus appropriate for Mc-
Carthy et al?s (2004) method. The latter requires
each word to participate in a large number of co-
occurring contexts in order to obtain reliable dis-
tributional information. To overcome this prob-
lem, we followed McCarthy et al and extracted
the neighbor thesaurus from the entire BNC. We
also recreated their semantic space, using a RASP-
parsed (Briscoe and Carroll, 2002) version of the
BNC and their set of dependencies (i.e., Verb-
Object, Verb-Subject, Noun-Noun and Adjective-
Noun relations). Similarly to McCarthy et al, we
used Lin?s (1998) measure of distributional simi-
larity, and considered only the 50 highest ranked
100
Method Accps Accwsd/dir Accwsd/ps
Baseline 34.5 ? 23.0
LexChains 48.3??$ ? 40.7?#?$
Overlap 49.4??$ 36.5$ 42.5??$
Similarity 54.9? ? 46.5?$
SSI 53.7? 42.7 47.9?
UpperBnd 100 ? 68.4
Table 2: Results of individual disambiguation al-
gorithms on SemCor nouns2 (?: sig. diff. from
Baseline, ?: sig. diff. from Similarity, $: sig diff.
from SSI, #: sig. diff. from Overlap, p < 0.01)
neighbors for a given target word. Sense similar-
ity was computed using the Lesk?s (Banerjee and
Pedersen, 2003) similarity measure1.
3.3 Results
The performance of the individual algorithms is
shown in Table 2. We also include the baseline
discussed in Section 3 and the upper bound of
defaulting to the first (i.e., most frequent) sense
provided by the manually annotated SemCor. We
report predominant sense accuracy (Accps), andWSD accuracy when using the automatically ac-
quired predominant sense (Accwsd/ps). For token-based algorithms, we also report their WSD per-
formance in context, i.e., without use of the pre-
dominant sense (Accwsd/dir).As expected, the accuracy scores in the WSD
task are lower than the respective scores in the
predominant sense task, since detecting the pre-
dominant sense correctly only insures the correct
tagging of the instances of the word with that
first sense. All methods perform significantly bet-
ter than the baseline in the predominant sense de-
tection task (using a ?2-test, as indicated in Ta-
ble 2). LexChains and Overlap perform signif-
icantly worse than Similarity and SSI, whereas
LexChains is not significantly different from Over-
lap. Likewise, the difference in performance be-
tween SSI and Similarity is not significant. With
respect to WSD, all the differences in performance
are statistically significant.
1This measure is identical to the Extended gloss Overlapfrom Section 2, but instead of searching for overlap betweenan extended gloss and a word?s context, the comparison isdone between two extended glosses of two synsets.2The LexChains results presented here are not directlycomparable to those reported by Galley and McKeown(2003), since they tested on a subset of SemCor, and includedmonosemous nouns. They also used the first sense in Sem-Cor in case of ties. The results for the Similarity method areslightly better than those reported by McCarthy et al (2004)due to minor improvements in implementation.
Overlap LexChains Similarity
LexChains 28.05
Similarity 35.87 33.10
SSI 30.48 31.67 37.14
Table 3: Algorithms? pairwise agreement in de-
tecting the predominant sense (as % of all words)
Interestingly, using the predominant sense de-
tected by the Gloss Overlap and the SSI algo-
rithm to tag all instances is preferable to tagging
each instance individually (compare Accwsd/dirand Accwsd/ps for Overlap and SSI in Table 2).This means that a large part of the instances which
were not tagged individually with the predominant
sense were actually that sense.
A close examination of the performance of the
individual methods in the predominant-sense de-
tection task shows that while the accuracy of all
the methods is within a range of 7%, the actual
words for which each algorithm gives the cor-
rect predominant sense are very different. Table 3
shows the degree of overlap in assigning the ap-
propriate predominant sense among the four meth-
ods. As can be seen, the largest amount of over-
lap is between Similarity and SSI, and this cor-
responds approximately to 23 of the words theycorrectly label. This means that each of these two
methods gets more than 350 words right which the
other labels incorrectly.
If we had an ?oracle? which would tell us
which method to choose for each word, we would
achieve approximately 82.4% in the predominant
sense task, giving us 58% in the WSD task. We
see that there is a large amount of complementa-
tion between the algorithms, where the successes
of one make up for the failures of the others. This
suggests that the errors of the individual methods
are sufficiently uncorrelated, and that some advan-
tage can be gained by combining their predictions.
4 Combination Methods
An important finding in machine learning is that
a set of classifiers whose individual decisions are
combined in some way (an ensemble) can be more
accurate than any of its component classifiers, pro-
vided that the individual components are relatively
accurate and diverse (Dietterich, 1997). This sim-
ple idea has been applied to a variety of classi-
fication problems ranging from optical character
recognition to medical diagnosis, part-of-speech
tagging (see Dietterich 1997 and van Halteren
et al 2001 for overviews), and notably supervised
101
WSD (Florian et al, 2002).
Since our effort is focused exclusively on un-
supervised methods, we cannot use most ma-
chine learning approaches for creating an en-
semble (e.g., stacking, confidence-based combina-
tion), as they require a labeled training set. We
therefore examined several basic ensemble com-
bination approaches that do not require parameter
estimation from training data.
We define Score(Mi,s j) as the (normalized)score which a method Mi gives to word sense s j.The predominant sense calculated by method Mifor word w is then determined by:
PS(Mi,w) = argmax
s j?senses(w)
Score(Mi,s j)
All ensemble methods receive a set {Mi}ki=1 of in-dividual methods to combine, so we denote each
ensemble method by MethodName({Mi}ki=1).
Direct Voting Each ensemble component has
one vote for the predominant sense, and the sense
with the most votes is chosen. The scoring func-
tion for the voting ensemble is defined as:
Score(Voting({Mi}ki=1),s)) =
k
?
i=1
eq[s,PS(Mi,w)]
where eq[s,PS(Mi,w)] =
{ 1 if s = PS(Mi,w)0 otherwise
Probability Mixture Each method provides
a probability distribution over the senses. These
probabilities (normalized scores) are summed, and
the sense with the highest score is chosen:
Score(ProbMix({Mi}ki=1),s)) =
k
?
i=1
Score(Mi,s)
Rank-Based Combination Each method
provides a ranking of the senses for a given target
word. For each sense, its placements according to
each of the methods are summed and the sense
with the lowest total placement (closest to first
place) wins.
Score(Ranking({Mi}ki=1),s)) =
k
?
i=1
(?1)?Placei(s)
where Placei(s) is the number of distinct scoresthat are larger or equal to Score(Mi,s).
Arbiter-based Combination One WSD
method can act as an arbiter for adjudicating dis-
agreements among component systems. It makes
sense for the adjudicator to have reasonable
performance on its own. We therefore selected
Method Accps Accwsd/ps
Similarity 54.9 46.5
SSI 53.5 47.9
Voting 57.3?$ 49.8?$
PrMixture 57.2?$ 50.4?$?
Rank-based 58.1?$ 50.3?$?
Arbiter-based 56.3?$ 48.7?$?
UpperBnd 100 68.4
Table 4: Ensemble Combination Results (?: sig.
diff. from Similarity, $: sig. diff. from SSI, ?: sig.
diff. from Voting, p < 0.01)
SSI as the arbiter since it had the best accuracy on
the WSD task (see Table 2). For each disagreed
word w, and for each sense s of w assigned by
any of the systems in the ensemble {Mi}ki=1, wecalculate the following score:
Score(Arbiter({Mi}ki=1),s) = SSIScore?(s)
where SSIScore?(s) is a modified version of the
score introduced in Section 2 which exploits as a
context for s the set of agreed senses and the re-
maining words of each sentence. We exclude from
the context used by SSI the senses of w which were
not chosen by any of the systems in the ensem-
ble . This effectively reduces the number of senses
considered by the arbiter and can positively influ-
ence the algorithm?s performance, since it elimi-
nates noise coming from senses which are likely
to be wrong.
5 Experiment 2: Ensembles for
Unsupervised WSD
5.1 Method and Parameter Settings
We assess the performance of the different en-
semble systems on the same set of SemCor nouns
on which the individual methods were tested. For
the best ensemble, we also report results on dis-
ambiguating all nouns in the Senseval-3 data set.
We focus exclusively on nouns to allow com-
parisons with the results obtained from SemCor.
We used the same parameters as in Experiment 1
for constructing the ensembles. As discussed ear-
lier, token-based methods can disambiguate target
words either in context or using the predominant
sense. SSI was employed in the predominant sense
setting in our arbiter experiment.
5.2 Results
Our results are summarized in Table 4. As can be
seen, all ensemble methods perform significantly
102
Ensemble Accps Accwsd/ps
Rank-based 58.1 50.3
Overlap 57.6 (?0.5) 49.7 (?0.6)
LexChains 57.2 (?0.7) 50.2 (?0.1)
Similarity 56.3 (?1.8) 49.4 (?0.9)
SSI 56.3 (?1.8) 48.2 (?2.1)
Table 5: Decrease in accuracy as a result of re-
moval of each method from the rank-based ensem-
ble.
better than the best individual methods, i.e., Simi-
larity and SSI. On the WSD task, the voting, prob-
ability mixture, and rank-based ensembles signif-
icantly outperform the arbiter-based one. The per-
formances of the probability mixture, and rank-
based combinations do not differ significantly but
both ensembles are significantly better than vot-
ing. One of the factors contributing to the arbiter?s
worse performance (compared to the other ensem-
bles) is the fact that in many cases (almost 30%),
none of the senses suggested by the disagreeing
methods is correct. In these cases, there is no way
for the arbiter to select the correct sense. We also
examined the relative contribution of each compo-
nent to overall performance. Table 5 displays the
drop in performance by eliminating any particular
component from the rank-based ensemble (indi-
cated by ?). The system that contributes the most
to the ensemble is SSI. Interestingly, Overlap and
Similarity yield similar improvements in WSD ac-
curacy (0.6 and 0.9, respectively) when added to
the ensemble.
Figure 1 shows the WSD accuracy of the best
single methods and the ensembles as a function of
the noun frequency in SemCor. We can see that
there is at least one ensemble outperforming any
single method in every frequency band and that
the rank-based ensemble consistently outperforms
Similarity and SSI in all bands. Although Similar-
ity has an advantage over SSI for low and medium
frequency words, it delivers worse performance
for high frequency words. This is possibly due to
the quality of neighbors obtained for very frequent
words, which are not semantically distinct enough
to reliably discriminate between different senses.
Table 6 lists the performance of the rank-based
ensemble on the Senseval-3 (noun) corpus. We
also report results for the best individual method,
namely SSI, and compare our results with the best
unsupervised system that participated in Senseval-
3. The latter was developed by Strapparava et al
(2004) and performs domain driven disambigua-
tion (IRST-DDD). Specifically, the approach com-
1-4 5-9 10-19 20-99 100+Noun frequency bands
40
42
44
46
48
50
52
54
WS
D A
ccu
rac
y (%
)
SimilaritySSIArbiter
VotingProbMixRanking
Figure 1: WSD accuracy as a function of noun fre-
quency in SemCor
Method Precision Recall Fscore
Baseline 36.8 36.8 36.8
SSI 62.5 62.5 62.5
IRST-DDD 63.3 62.2 61.2
Rank-based 63.9 63.9 63.9
UpperBnd 68.7 68.7 68.7
Table 6: Results of individual disambiguation al-
gorithms and rank-based ensemble on Senseval-3
nouns
pares the domain of the context surrounding the
target word with the domains of its senses and uses
a version of WordNet augmented with domain la-
bels (e.g., economy, geography). Our baseline se-
lects the first sense randomly and uses it to disam-
biguate all instances of a target word. Our upper
bound defaults to the first sense from SemCor. We
report precision, recall and Fscore. In cases where
precision and recall figures coincide, the algorithm
has 100% coverage.
As can be seen the rank-based, ensemble out-
performs both SSI and the IRST-DDD system.
This is an encouraging result, suggesting that there
may be advantages in developing diverse classes
of unsupervised WSD algorithms for system com-
bination. The results in Table 6 are higher than
those reported for SemCor (see Table 4). This is
expected since the Senseval-3 data set contains
monosemous nouns as well. Taking solely polyse-
mous nouns into account, SSI?s Fscore is 53.39%
and the ranked-based ensemble?s 55.0%. We fur-
ther note that not all of the components in our en-
semble are optimal. Predominant senses for Lesk
and LexChains were estimated from the Senseval-
3 data, however a larger corpus would probably
yield more reliable estimates.
103
6 Conclusions and Discussion
In this paper we have presented an evaluation
study of four well-known approaches to unsuper-
vised WSD. Our comparison involved type- and
token-based disambiguation algorithms relying on
different kinds of WordNet relations and different
amounts of corpus data. Our experiments revealed
two important findings. First, type-based disam-
biguation yields results superior to a token-based
approach. Using predominant senses is preferable
to disambiguating instances individually, even for
token-based algorithms. Second, the outputs of
the different approaches examined here are suffi-
ciently diverse to motivate combination methods
for unsupervised WSD. We defined several ensem-
bles on the predominant sense outputs of individ-
ual methods and showed that combination systems
outperformed their best components both on the
SemCor and Senseval-3 data sets.
The work described here could be usefully em-
ployed in two tasks: (a) to create preliminary an-
notations, thus supporting the ?annotate automati-
cally, correct manually? methodology used to pro-
vide high volume annotation in the Penn Treebank
project; and (b) in combination with supervised
WSD methods that take context into account; for
instance, such methods could default to an unsu-
pervised system for unseen words or words with
uninformative contexts.
In the future we plan to integrate more com-
ponents into our ensembles. These include not
only domain driven disambiguation algorithms
(Strapparava et al, 2004) but also graph theoretic
ones (Mihalcea, 2005) as well as algorithms that
quantify the degree of association between senses
and their co-occurring contexts (Mohammad and
Hirst, 2006). Increasing the number of compo-
nents would allow us to employ more sophisti-
cated combination methods such as unsupervised
rank aggregation algorithms (Tan and Jin, 2004).
Acknowledgements
We are grateful to Diana McCarthy for her help with thiswork and to Michel Galley for making his code availableto us. Thanks to John Carroll and Rob Koeling for in-sightful comments and suggestions. The authors acknowl-edge the support of EPSRC (Brody and Lapata; grantEP/C538447/1) and the European Union (Navigli; InteropNoE (508011)).
References
Banerjee, Satanjeev and Ted Pedersen. 2003. Extended glossoverlaps as a measure of semantic relatedness. In Proceed-
ings of the 18th IJCAI. Acapulco, pages 805?810.
Briscoe, Ted and John Carroll. 2002. Robust accurate statis-tical annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Dietterich, T. G. 1997. Machine learning research: Four cur-rent directions. AI Magazine 18(4):97?136.
Edmonds, Philip. 2000. Designing a task for SENSEVAL-2.Technical note.
Florian, Radu, Silviu Cucerzan, Charles Schafer, and DavidYarowsky. 2002. Combining classifiers for word sense dis-ambiguation. Natural Language Engineering 1(1):1?14.
Galley, Michel and Kathleen McKeown. 2003. Improvingword sense disambiguation in lexical chaining. In Pro-
ceedings of the 18th IJCAI. Acapulco, pages 1486?1488.Hoste, Ve?ronique, Iris Hendrickx, Walter Daelemans, andAntal van den Bosch. 2002. Parameter optimization formachine-learning of word sense disambiguation. Lan-
guage Engineering 8(4):311?325.
Lesk, Michael. 1986. Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pine conefrom an ice cream cone. In Proceedings of the 5th SIG-
DOC. New York, NY, pages 24?26.
Lin, Dekang. 1998. An information-theoretic definition ofsimilarity. In Proceedings of the 15th ICML. Madison,WI, pages 296?304.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-roll. 2004. Finding predominant senses in untagged text.In Proceedings of the 42th ACL. Barcelona, Spain, pages280?287.
Mihalcea, Rada. 2005. Unsupervised large-vocabulary wordsense disambiguation with graph-based algorithms for se-quence data labeling. In Proceedings of the HLT/EMNLP.Vancouver, BC, pages 411?418.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of the SENSEVAL-3. Barcelona, Spain.
Miller, George A., Claudia Leacock, Randee Tengi, andRoss T. Bunker. 1993. A semantic concordance. In Pro-
ceedings of the ARPA HLT Workshop. Morgan Kaufman,pages 303?308.
Mohammad, Saif and Graeme Hirst. 2006. Determining wordsense dominance using a thesaurus. In Proceedings of the
EACL. Trento, Italy, pages 121?128.
Morris, Jane and Graeme Hirst. 1991. Lexical cohesion com-puted by thesaural relations as an indicator of the structureof text. Computational Linguistics 1(17):21?43.
Navigli, Roberto. 2005. Semi-automatic extension of large-scale linguistic knowledge bases. In Proceedings of the
18th FLAIRS. Florida.
Navigli, Roberto and Paola Velardi. 2005. Structural seman-tic interconnections: a knowledge-based approach to wordsense disambiguation. PAMI 27(7):1075?1088.
Ng, Tou Hwee. 1997. Getting serious about word sense dis-ambiguation. In Proceedings of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why, What,
and How?. Washington, DC, pages 1?7.
Stokoe, Christopher. 2005. Differentiating homonymy andpolysemy in information retrieval. In Proceedings of the
HLT/EMNLP. Vancouver, BC, pages 403?410.
Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano.2004. Word-sense disambiguation for machine transla-tion. In Proceedings of the SENSEVAL-3. Barcelona,Spain, pages 229?234.
Tan, Pang-Ning and Rong Jin. 2004. Ordering patterns bycombining opinions from multiple sources. In Proceed-
ings of the 10th KDD. Seattle, WA, pages 22?25.
van Halteren, Hans, Jakub Zavrel, and Walter Daelemans.2001. Improving accuracy in word class tagging throughcombination of machine learning systems. Computational
Linguistics 27(2):199?230.
Vickrey, David, Luke Biewald, Marc Teyssier, and DaphneKoller. 2005. Word-sense disambiguation for machinetranslation. In Proceedings of the HLT/EMNLP. Vancou-ver, BC, pages 771?778.
Yarowsky, David and Radu Florian. 2002. Evaluating sensedisambiguation across diverse parameter spaces. Natural
Language Engineering 9(4):293?310.
104
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 377?384,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Models for Sentence Compression: A Comparison across Domains,
Training Requirements and Evaluation Measures
James Clarke and Mirella Lapata
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
jclarke@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Sentence compression is the task of pro-
ducing a summary at the sentence level.
This paper focuses on three aspects of
this task which have not received de-
tailed treatment in the literature: train-
ing requirements, scalability, and auto-
matic evaluation. We provide a novel com-
parison between a supervised constituent-
based and an weakly supervised word-
based compression algorithm and exam-
ine how these models port to different do-
mains (written vs. spoken text). To achieve
this, a human-authored compression cor-
pus has been created and our study high-
lights potential problems with the auto-
matically gathered compression corpora
currently used. Finally, we assess whether
automatic evaluation measures can be
used to determine compression quality.
1 Introduction
Automatic sentence compression has recently at-
tracted much attention, in part because of its affin-
ity with summarisation. The task can be viewed
as producing a summary of a single sentence that
retains the most important information while re-
maining grammatically correct. An ideal compres-
sion algorithm will involve complex text rewriting
operations such as word reordering, paraphrasing,
substitution, deletion, and insertion. In default of
a more sophisticated compression algorithm, cur-
rent approaches have simplified the problem to a
single rewriting operation, namely word deletion.
More formally, given an input sentence of words
W = w1,w2, . . . ,wn, a compression is formed bydropping any subset of these words. Viewing the
task as word removal reduces the number of pos-
sible compressions to 2n; naturally, many of these
compressions will not be reasonable or grammati-
cal (Knight and Marcu 2002).
Sentence compression could be usefully em-
ployed in wide range of applications. For exam-
ple, to automatically generate subtitles for televi-
sion programs; the transcripts cannot usually be
used verbatim due to the rate of speech being too
high (Vandeghinste and Pan 2004). Other applica-
tions include compressing text to be displayed on
small screens (Corston-Oliver 2001) such as mo-
bile phones or PDAs, and producing audio scan-
ning devices for the blind (Grefenstette 1998).
Algorithms for sentence compression fall into
two broad classes depending on their training re-
quirements. Many algorithms exploit parallel cor-
pora (Jing 2000; Knight and Marcu 2002; Riezler
et al 2003; Nguyen et al 2004a; Turner and Char-
niak 2005; McDonald 2006) to learn the corre-
spondences between long and short sentences in
a supervised manner, typically using a rich feature
space induced from parse trees. The learnt rules
effectively describe which constituents should be
deleted in a given context. Approaches that do
not employ parallel corpora require minimal or
no supervision. They operationalise compression
in terms of word deletion without learning spe-
cific rules and can therefore rely on little linguistic
knowledge such as part-of-speech tags or merely
the lexical items alone (Hori and Furui 2004). Al-
ternatively, the rules of compression are approxi-
mated from a non-parallel corpus (e.g., the Penn
Treebank) by considering context-free grammar
derivations with matching expansions (Turner and
Charniak 2005).
Previous approaches have been developed and
tested almost exclusively on written text, a no-
table exception being Hori and Furui (2004) who
focus on spoken language. While parallel cor-
pora of original-compressed sentences are not nat-
urally available in the way multilingual corpora
are, researchers have obtained such corpora auto-
matically by exploiting documents accompanied
by abstracts. Automatic corpus creation affords
the opportunity to study compression mechanisms
377
cheaply, yet these mechanisms may not be repre-
sentative of human performance. It is unlikely that
authors routinely carry out sentence compression
while creating abstracts for their articles. Collect-
ing human judgements is the method of choice for
evaluating sentence compression models. How-
ever, human evaluations tend to be expensive and
cannot be repeated frequently; furthermore, com-
parisons across different studies can be difficult,
particularly if subjects employ different scales, or
are given different instructions.
In this paper we examine some aspects of the
sentence compression task that have received lit-
tle attention in the literature. First, we provide a
novel comparison of supervised and weakly su-
pervised approaches. Specifically, we study how
constituent-based and word-based methods port to
different domains and show that the latter tend to
be more robust. Second, we create a corpus of
human-authored compressions, and discuss some
potential problems with currently used compres-
sion corpora. Finally, we present automatic evalu-
ation measures for sentence compression and ex-
amine whether they correlate reliably with be-
havioural data.
2 Algorithms for Sentence Compression
In this section we give a brief overview of the algo-
rithms we employed in our comparative study. We
focus on two representative methods, Knight and
Marcu?s (2002) decision-based model and Hori
and Furui?s (2004) word-based model.
The decision-tree model operates over parallel
corpora and offers an intuitive formulation of sen-
tence compression in terms of tree rewriting. It
has inspired many discriminative approaches to
the compression task (Riezler et al 2003; Nguyen
et al 2004b; McDonald 2006) and has been
extended to languages other than English (see
Nguyen et al 2004a). We opted for the decision-
tree model instead of the also well-known noisy-
channel model (Knight and Marcu 2002; Turner
and Charniak 2005). Although both models yield
comparable performance, Turner and Charniak
(2005) show that the latter is not an appropriate
compression model since it favours uncompressed
sentences over compressed ones.1
Hori and Furui?s (2004) model was originally
developed for Japanese with spoken text in mind,
1The noisy-channel model uses a source model trainedon uncompressed sentences. This means that the most likelycompressed sentence will be identical to the original sen-tence as the likelihood of a constituent deletion is typicallyfar lower than that of leaving it in.
SHIFT transfers the first word from the input list ontothe stack.
REDUCE pops the syntactic trees located at the topof the stack, combines them into a new tree and thenpushes the new tree onto the top of the stack.
DROP deletes from the input list subsequences of wordsthat correspond to a syntactic constituent.
ASSIGNTYPE changes the label of the trees at the topof the stack (i.e., the POS tag of words).
Table 1: Stack rewriting operations
it requires minimal supervision, and little linguis-
tic knowledge. It therefor holds promise for lan-
guages and domains for which text processing
tools (e.g., taggers, parsers) are not readily avail-
able. Furthermore, to our knowledge, its perfor-
mance on written text has not been assessed.
2.1 Decision-based Sentence Compression
In the decision-based model, sentence compres-
sion is treated as a deterministic rewriting process
of converting a long parse tree, l, into a shorter
parse tree s. The rewriting process is decomposed
into a sequence of shift-reduce-drop actions that
follow an extended shift-reduce parsing paradigm.
The compression process starts with an empty
stack and an input list that is built from the orig-
inal sentence?s parse tree. Words in the input list
are labelled with the name of all the syntactic con-
stituents in the original sentence that start with it.
Each stage of the rewriting process is an operation
that aims to reconstruct the compressed tree. There
are four types of operations that can be performed
on the stack, they are illustrated in Table 1.
Learning cases are automatically generated
from a parallel corpus. Each learning case is ex-
pressed by a set of features and represents one of
the four possible operations for a given stack and
input list. Using the C4.5 program (Quinlan 1993)
a decision-tree model is automatically learnt. The
model is applied to a parsed original sentence in
a deterministic fashion. Features for the current
state of the input list and stack are extracted and
the classifier is queried for the next operation to
perform. This is repeated until the input list is
empty and the stack contains only one item (this
corresponds to the parse for the compressed tree).
The compressed sentence is recovered by travers-
ing the leaves of the tree in order.
2.2 Word-based Sentence Compression
The decision-based method relies exclusively on
parallel corpora; the caveat here is that appropri-
ate training data may be scarce when porting this
model to different text domains (where abstracts
378
are not available for automatic corpus creation) or
languages. To alleviate the problems inherent with
using a parallel corpus, we have modified a weakly
supervised algorithm originally proposed by Hori
and Furui (2004). Their method is based on word
deletion; given a prespecified compression length,
a compression is formed by preserving the words
which maximise a scoring function.
To make Hori and Furui?s (2004) algorithm
more comparable to the decision-based model, we
have eliminated the compression length parameter.
Instead, we search over all lengths to find the com-
pression that gives the maximum score. This pro-
cess yields more natural compressions with vary-
ing lengths. The original score measures the sig-
nificance of each word (I) in the compression and
the linguistic likelihood (L) of the resulting word
combinations.2 We add some linguistic knowledge
to this formulation through a function (SOV ) that
captures information about subjects, objects and
verbs. The compression score is given in Equa-
tion (1). The lambdas (?I , ?SOV , ?L) weight thecontribution of the individual scores:
S(V ) =
M
?
i=1
?II(vi)+?sovSOV (vi)
+?LL(vi|vi?1,vi?2) (1)
The sentence V = v1,v2, . . . ,vm (of M words)that maximises the score S(V ) is the best com-
pression for an original sentence consisting of N
words (M < N). The best compression can be
found using dynamic programming. The ??s in
Equation (1) can be either optimised using a small
amount of training data or set manually (e.g., if
short compressions are preferred to longer ones,
then the language model should be given a higher
weight). Alternatively, weighting could be dis-
pensed with by including a normalising factor in
the language model. Here, we follow Hori and Fu-
rui?s (2004) original formulation and leave the nor-
malisation to future work. We next introduce each
measure individually.
Word significance score The word signifi-
cance score I measures the relative importance of
a word in a document. It is similar to tf-idf, a term
weighting score commonly used in information re-
trieval:
I(wi) = fi log FAFi (2)
2Hori and Furui (2004) also have a confidence score basedupon how reliable the output of an automatic speech recog-nition system is. However, we need not consider this scorewhen working with written text and manual transcripts.
Where wi is the topic word of interest (topic wordsare either nouns or verbs), fi is the frequency of wiin the document, Fi is the corpus frequency of wiand FA is the sum of all topic word occurrences inthe corpus (?i Fi).
Linguistic score The linguistic score?s
L(vi|vi?1,vi?2) responsibility is to select somefunction words, thus ensuring that compressions
remain grammatical. It also controls which topic
words can be placed together. The score mea-
sures the n-gram probability of the compressed
sentence.
SOV Score The SOV score is based on the in-
tuition that subjects, objects and verbs should not
be dropped while words in other syntactic roles
can be considered for removal. This score is based
solely on the contents of the sentence considered
for compression without taking into account the
distribution of subjects, objects or verbs, across
documents. It is defined in (3) where fi is the doc-ument frequency of a verb, or word bearing the
subject/object role and ?default is a constant weightassigned to all other words.
SOV (wi) =
?
?
?
fi if wi in subject, objector verb role
?default otherwise (3)
The SOV score is only applied to the head word of
subjects and objects.
3 Corpora
Our intent was to assess the performance of the
two models just described on written and spo-
ken text. The appeal of written text is understand-
able since most summarisation work today fo-
cuses on this domain. Speech data not only pro-
vides a natural test-bed for compression applica-
tions (e.g., subtitle generation) but also poses ad-
ditional challenges. Spoken utterances can be un-
grammatical, incomplete, and often contain arte-
facts such as false starts, interjections, hesitations,
and disfluencies. Rather than focusing on sponta-
neous speech which is abundant in these artefacts,
we conduct our study on the less ambitious do-
main of broadcast news transcripts. This lies in-
between the extremes of written text and sponta-
neous speech as it has been scripted beforehand
and is usually read off an autocue.
One stumbling block to performing a compara-
tive study between written data and speech data
is that there are no naturally occurring parallel
379
speech corpora for studying compression. Auto-
matic corpus creation is not a viable option ei-
ther, speakers do not normally create summaries
of their own utterances. We thus gathered our own
corpus by asking humans to generate compres-
sions for speech transcripts.
In what follows we describe how the manual
compressions were performed. We also briefly
present the written corpus we used for our exper-
iments. The latter was automatically constructed
and offers an interesting point of comparison with
our manually created corpus.
Broadcast News Corpus Three annotators
were asked to compress 50 broadcast news sto-
ries (1,370 sentences) taken from the HUB-4
1996 English Broadcast News corpus provided by
the LDC. The HUB-4 corpus contains broadcast
news from a variety of networks (CNN, ABC,
CSPAN and NPR) which have been manually tran-
scribed and split at the story and sentence level.
Each document contains 27 sentences on average
and the whole corpus consists of 26,151 tokens.3
The Robust Accurate Statistical Parsing (RASP)
toolkit (Briscoe and Carroll 2002) was used to au-
tomatically tokenise the corpus.
Each annotator was asked to perform sentence
compression by removing tokens from the original
transcript. Annotators were asked to remove words
while: (a) preserving the most important infor-
mation in the original sentence, and (b) ensuring
the compressed sentence remained grammatical. If
they wished they could leave a sentence uncom-
pressed by marking it as inappropriate for com-
pression. They were not allowed to delete whole
sentences even if they believed they contained no
information content with respect to the story as
this would blur the task with abstracting.
Ziff-Davis Corpus Most previous work (Jing
2000; Knight and Marcu 2002; Riezler et al 2003;
Nguyen et al 2004a; Turner and Charniak 2005;
McDonald 2006) has relied on automatically con-
structed parallel corpora for training and evalua-
tion purposes. The most popular compression cor-
pus originates from the Ziff-Davis corpus ? a col-
lection of news articles on computer products. The
corpus was created by matching sentences that oc-
cur in an article with sentences that occur in an
abstract (Knight and Marcu 2002). The abstract
sentences had to contain a subset of the original
sentence?s words and the word order had to remain
the same.
3The compression corpus is available at http://
homepages.inf.ed.ac.uk/s0460084/data/.
A1 A2 A3 Av. Ziff-Davis
Comp% 88.0 79.0 87.0 84.4 97.0
CompR 73.1 79.0 70.0 73.0 47.0
Table 2: Compression Rates (Comp% measures
the percentage of sentences compressed; CompR
is the mean compression rate of all sentences)
1 2 3 4 5 6 7 8 9 10Length of word span dropped
0
0.1
0.2
0.3
0.4
0.5
Re
lati
ve 
num
ber
 of 
dro
ps
Annotator 1Annotator 2Annotator 3Ziff-Davis
+
Figure 1: Distribution of span of words dropped
Comparisons Following the classification
scheme adopted in the British National Corpus
(Burnard 2000), we assume throughout this paper
that Broadcast News and Ziff-Davis belong to dif-
ferent domains (spoken vs. written text) whereas
they represent the same genre (i.e., news). Table 2
shows the percentage of sentences which were
compressed (Comp%) and the mean compression
rate (CompR) for the two corpora. The annota-
tors compress the Broadcast News corpus to a
similar degree. In contrast, the Ziff-Davis corpus
is compressed much more aggressively with a
compression rate of 47%, compared to 73% for
Broadcast News. This suggests that the Ziff-Davis
corpus may not be a true reflection of human
compression performance and that humans tend
to compress sentences more conservatively than
the compressions found in abstracts.
We also examined whether the two corpora dif-
fer with regard to the length of word spans be-
ing removed. Figure 1 shows how frequently word
spans of varying lengths are being dropped. As can
be seen, a higher percentage of long spans (five
or more words) are dropped in the Ziff-Davis cor-
pus. This suggests that the annotators are remov-
ing words rather than syntactic constituents, which
provides support for a model that can act on the
word level. There is no statistically significant dif-
ference between the length of spans dropped be-
tween the annotators, whereas there is a signif-
icant difference (p < 0.01) between the annota-
tors? spans and the Ziff-Davis? spans (using the
380
Wilcoxon Test).
The compressions produced for the Broadcast
News corpus may differ slightly to the Ziff-Davis
corpus. Our annotators were asked to perform
sentence compression explicitly as an isolated
task rather than indirectly (and possibly subcon-
sciously) as part of the broader task of abstracting,
which we can assume is the case with the Ziff-
Davis corpus.
4 Automatic Evaluation Measures
Previous studies relied almost exclusively on
human judgements for assessing the well-
formedness of automatically derived com-
pressions. Although human evaluations of
compression systems are not as large-scale as in
other fields (e.g., machine translation), they are
typically performed once, at the end of the de-
velopment cycle. Automatic evaluation measures
would allow more extensive parameter tuning
and crucially experimentation with larger data
sets. Most human studies to date are conducted
on a small compression sample, the test portion
of the Ziff-Davis corpus (32 sentences). Larger
sample sizes would expectedly render human
evaluations time consuming and generally more
difficult to conduct frequently. Here, we review
two automatic evaluation measures that hold
promise for the compression task.
Simple String Accuracy (SSA, Bangalore et al
2000) has been proposed as a baseline evaluation
metric for natural language generation. It is based
on the string edit distance between the generated
output and a gold standard. It is a measure of the
number of insertion (I), deletion (D) and substi-
tution (S) errors between two strings. It is defined
in (4) where R is the length of the gold standard
string.
Simple String Accuracy = (1? I +D+S
R
) (4)
The SSA score will assess whether appropriate
words have been included in the compression.
Another stricter automatic evaluation method
is to compare the grammatical relations found in
the system compressions against those found in a
gold standard. This allows us ?to measure the se-
mantic aspects of summarisation quality in terms
of grammatical-functional information? (Riezler
et al 2003). The standard metrics of precision,
recall and F-score can then be used to measure
the quality of a system against a gold standard.
Our implementation of the F-score measure used
the grammatical relations annotations provided by
RASP (Briscoe and Carroll 2002). This parser is
particularly appropriate for the compression task
since it provides parses for both full sentences
and sentence fragments and is generally robust
enough to analyse semi-grammatical compres-
sions. We calculated F-score over all the relations
provided by RASP (e.g., subject, direct/indirect
object, modifier; 15 in total).
Correlation with human judgements is an im-
portant prerequisite for the wider use of automatic
evaluation measures. In the following section we
describe an evaluation study examining whether
the measures just presented indeed correlate with
human ratings of compression quality.
5 Experimental Set-up
In this section we present our experimental set-
up for assessing the performance of the two al-
gorithms discussed above. We explain how differ-
ent model parameters were estimated. We also de-
scribe a judgement elicitation study on automatic
and human-authored compressions.
Parameter Estimation We created two vari-
ants of the decision-tree model, one trained on
the Ziff-Davis corpus and one on the Broadcast
News corpus. We used 1,035 sentences from the
Ziff-Davis corpus for training; the same sentences
were previously used in related work (Knight and
Marcu 2002). The second variant was trained on
1,237 sentences from the Broadcast News corpus.
The training data for both models was parsed us-
ing Charniak?s (2000) parser. Learning cases were
automatically generated using a set of 90 features
similar to Knight and Marcu (2002).
For the word-based method, we randomly
selected 50 sentences from each training set
to optimise the lambda weighting parame-
ters4. Optimisation was performed using Pow-
ell?s method (Press et al 1992). Recall from Sec-
tion 2.2 that the compression score has three
main parameters: the significance, linguistic, and
SOV scores. The significance score was calcu-
lated using 25 million tokens from the Broadcast
News corpus (spoken variant) and 25 million to-
kens from the North American News Text Cor-
pus (written variant). The linguistic score was es-
timated using a trigram language model. The lan-
guage model was trained on the North Ameri-
4To treat both models on an equal footing, we attemptedto train the decision-tree model solely on 50 sentences. How-ever, it was unable to produce any reasonable compressions,presumably due to insufficient learning instances.
381
can corpus (25 million tokens) using the CMU-
Cambridge Language Modeling Toolkit (Clarkson
and Rosenfeld 1997) with a vocabulary size of
50,000 tokens and Good-Turing discounting. Sub-
jects, objects, and verbs for the SOV score were
obtained from RASP (Briscoe and Carroll 2002).
All our experiments were conducted on sen-
tences for which we obtained syntactic analyses.
RASP failed on 17 sentences from the Broadcast
news corpus and 33 from the Ziff-Davis corpus;
Charniak?s (2000) parser successfully parsed the
Broadcast News corpus but failed on three sen-
tences from the Ziff-Davis corpus.
Evaluation Data We randomly selected
40 sentences for evaluation purposes, 20 from
the testing portion of the Ziff-Davis corpus (32
sentences) and 20 sentences from the Broadcast
News corpus (133 sentences were set aside for
testing). This is comparable to previous studies
which have used the 32 test sentences from the
Ziff-Davis corpus. None of the 20 Broadcast
News sentences were used for optimisation. We
ran the decision-tree system and the word-based
system on these 40 sentences. One annotator was
randomly selected to act as the gold standard for
the Broadcast News corpus; the gold standard
for the Ziff-Davis corpus was the sentence that
occurred in the abstract. For each original sen-
tence we had three compressions; two generated
automatically by our systems and a human au-
thored gold standard. Thus, the total number of
compressions was 120 (3x40).
Human Evaluation The 120 compressions
were rated by human subjects. Their judgements
were also used to examine whether the automatic
evaluation measures discussed in Section 4 corre-
late reliably with behavioural data. Sixty unpaid
volunteers participated in our elicitation study, all
were self reported native English speakers. The
study was conducted remotely over the Internet.
Participants were presented with a set of instruc-
tions that explained the task and defined sentence
compression with the aid of examples. They first
read the original sentence with the compression
hidden. Then the compression was revealed by
pressing a button. Each participant saw 40 com-
pressions. A Latin square design prevented sub-
jects from seeing two different compressions of
the same sentence. The order of the sentences was
randomised. Participants were asked to rate each
compression they saw on a five point scale taking
into account the information retained by the com-
pression and its grammaticality. They were told all
o: Apparently Fergie very much wants to have a career intelevision.d: A career in television.w: Fergie wants to have a career in television.g: Fergie wants a career in television.
o: Many debugging features, including user-defined breakpoints and variable-watching and message-watchingwindows, have been added.d: Many debugging features.w: Debugging features, and windows, have been added.g: Many debugging features have been added.
o: As you said, the president has just left for a busy threedays of speeches and fundraising in Nevada, Californiaand New Mexico.d: As you said, the president has just left for a busy threedays.w: You said, the president has left for three days ofspeeches and fundraising in Nevada, California andNew Mexico.g: The president left for three days of speeches andfundraising in Nevada, California and New Mexico.
Table 3: Compression examples (o: original sen-
tence, d: decision-tree compression, w: word-
based compression, g: gold standard)
compressions were automatically generated. Ex-
amples of the compressions our participants saw
are given in Table 3.
6 Results
Our experiments were designed to answer three
questions: (1) Is there a significant difference
between the compressions produced by super-
vised (constituent-based) and weakly unsuper-
vised (word-based) approaches? (2) How well
do the two models port across domains (written
vs. spoken text) and corpora types (human vs. au-
tomatically created)? (3) Do automatic evaluation
measures correlate with human judgements?
One of our first findings is that the the decision-
tree model is rather sensitive to the style of training
data. The model cannot capture and generalise sin-
gle word drops as effectively as constituent drops.
When the decision-tree is trained on the Broadcast
News corpus, it is unable to create suitable com-
pressions. On the evaluation data set, 75% of the
compressions produced are the original sentence
or the original sentence with one word removed.
It is possible that the Broadcast News compres-
sion corpus contains more varied compressions
than those of the Ziff-Davis and therefore a larger
amount of training data would be required to learn
a reliable decision-tree model. We thus used the
Ziff-Davis trained decision-tree model to obtain
compressions for both corpora.
Our results are summarised in Tables 4 and 5.
Table 4 lists the average compression rates for
382
Broadcast News CompR SSA F-score
Decision-tree 0.55 0.34 0.40
Word-based 0.72 0.51 0.54
gold standard 0.71 ? ?
Ziff-Davis CompR SSA F-score
Decision-tree 0.58 0.20 0.34
Word-based 0.60 0.19 0.39
gold standard 0.54 ? ?
Table 4: Results using automatic evaluation mea-
sures
Compression Broadcast News Ziff-Davis
Decision-tree 2.04 2.34
Word-based 2.78 2.43
gold standard 3.87 3.53
Table 5: Mean ratings from human evaluation
each model as well as the models? performance ac-
cording to the two automatic evaluation measures
discussed in Section 4. The row ?gold standard?
displays human-produced compression rates. Ta-
ble 5 shows the results of our judgement elicitation
study.
The compression rates (CompR, Table 4) indi-
cate that the decision-tree model compresses more
aggressively than the word-based model. This is
due to the fact that it mostly removes entire con-
stituents rather than individual words. The word-
based model is closer to the human compres-
sion rate. According to our automatic evaluation
measures, the decision-tree model is significantly
worse than the word-based model (using the Stu-
dent t test, SSA p < 0.05, F-score p < 0.05) on
the Broadcast News corpus. Both models are sig-
nificantly worse than humans (SSA p < 0.05, F-
score p < 0.01). There is no significant difference
between the two systems using the Ziff-Davis cor-
pus on both simple string accuracy and relation
F-score, whereas humans significantly outperform
the two systems.
We have performed an Analysis of Variance
(ANOVA) to examine whether similar results are
obtained when using human judgements. Statisti-
cal tests were done using the mean of the ratings
(see Table 5). The ANOVA revealed a reliable ef-
fect of compression type by subjects and by items
(p < 0.01). Post-hoc Tukey tests confirmed that
the word-based model outperforms the decision-
tree model (? < 0.05) on the Broadcast news cor-
pus; however, the two models are not significantly
Measure Ziff-Davis Broadcast News
SSA 0.171 0.348*
F-score 0.575** 0.532**
*p < 0.05 **p < 0.01
Table 6: Correlation (Pearson?s r) between evalu-
ation measures and human ratings. Stars indicate
level of statistical significance.
different when using the Ziff-Davis corpus. Both
systems perform significantly worse than the gold
standard (? < 0.05).
We next examine the degree to which the auto-
matic evaluation measures correlate with human
ratings. Table 6 shows the results of correlating
the simple string accuracy (SSA) and relation F-
score against compression judgements. The SSA
does not correlate on both corpora with human
judgements; it thus seems to be an unreliable mea-
sure of compression performance. However, the F-
score correlates significantly with human ratings,
yielding a correlation coefficient of r = 0.575 on
the Ziff-Davis corpus and r = 0.532 on the Broad-
cast news. To get a feeling for the difficulty of
the task, we assessed how well our participants
agreed in their ratings using leave-one-out resam-
pling (Weiss and Kulikowski 1991). The technique
correlates the ratings of each participant with the
mean ratings of all the other participants. The aver-
age agreement is r = 0.679 on the Ziff-Davis cor-
pus and r = 0.746 on the Broadcast News corpus.
This result indicates that F-score?s agreement with
the human data is not far from the human upper
bound.
7 Conclusions and Future Work
In this paper we have provided a comparison be-
tween a supervised (constituent-based) and a min-
imally supervised (word-based) approach to sen-
tence compression. Our results demonstrate that
the word-based model performs equally well on
spoken and written text. Since it does not rely
heavily on training data, it can be easily extended
to languages or domains for which parallel com-
pression corpora are scarce. When no parallel cor-
pora are available the parameters can be manu-
ally tuned to produce compressions. In contrast,
the supervised decision-tree model is not partic-
ularly robust on spoken text, it is sensitive to the
nature of the training data, and did not produce ad-
equate compressions when trained on the human-
authored Broadcast News corpus. A comparison
of the automatically gathered Ziff-Davis corpus
383
with the Broadcast News corpus revealed impor-
tant differences between the two corpora and thus
suggests that automatically created corpora may
not reflect human compression performance.
We have also assessed whether automatic eval-
uation measures can be used for the compression
task. Our results show that grammatical relations-
based F-score (Riezler et al 2003) correlates re-
liably with human judgements and could thus be
used to measure compression performance auto-
matically. For example, it could be used to assess
progress during system development or for com-
paring across different systems and system config-
urations with much larger test sets than currently
employed.
In its current formulation, the only function
driving compression in the word-based model
is the language model. The word significance
and SOV scores are designed to single out im-
portant words that the model should not drop. We
have not yet considered any functions that encour-
age compression. Ideally these functions should be
inspired from the underlying compression process.
Finding such a mechanism is an avenue of future
work. We would also like to enhance the word-
based model with more linguistic knowledge; we
plan to experiment with syntax-based language
models and more richly annotated corpora.
Another important future direction lies in apply-
ing the unsupervised model presented here to lan-
guages with more flexible word order and richer
morphology than English (e.g., German, Czech).
We suspect that these languages will prove chal-
lenging for creating grammatically acceptable
compressions. Finally, our automatic evaluation
experiments motivate the use of relations-based F-
score as a means of directly optimising compres-
sion quality, much in the same way MT systems
optimise model parameters using BLEU as a mea-
sure of translation quality.
Acknowledgements
We are grateful to our annotators Vasilis Karaiskos, Beata
Kouchnir, and Sarah Luger. Thanks to Jean Carletta, Frank
Keller, Steve Renals, and Sebastian Riedel for helpful com-
ments and suggestions. Lapata acknowledges the support of
EPSRC (grant GR/T04540/01).
References
Bangalore, Srinivas, Owen Rambow, and Steve Whittaker.2000. Evaluation metrics for generation. In Proceedings
of the 1st INLG. Mitzpe Ramon, Israel, pages 1?8.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Spain, pages 1499?1504.
Burnard, Lou. 2000. The Users Reference Guide for the
British National Corpus (World Edition). British NationalCorpus Consortium, Oxford University Computing Ser-vice.
Charniak, Eugene. 2000. A maximum-entropy-inspiredparser. In Proceedings of the 1st NAACL. San Francisco,CA, pages 132?139.
Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical lan-guage modeling using the CMU?cambridge toolkit. In
Proceedings of Eurospeech. Rhodes, Greece, pages 2707?2710.
Corston-Oliver, Simon. 2001. Text Compaction for Displayon Very Small Screens. In Proceedings of the NAACL
Workshop on Automatic Summarization. Pittsburgh, PA,pages 89?98.
Grefenstette, Gregory. 1998. Producing Intelligent Tele-graphic Text Reduction to Provide an Audio Scanning Ser-vice for the Blind. In Proceedings of the AAAI Symposium
on Intelligent Text Summarization. Stanford, CA, pages111?117.
Hori, Chiori and Sadaoki Furui. 2004. Speech summariza-tion: an approach through word extraction and a methodfor evaluation. IEICE Transactions on Information and
Systems E87-D(1):15?25.
Jing, Hongyan. 2000. Sentence Reduction for Automatic TextSummarization. In Proceedings of the 6th ANLP. Seat-tle,WA, pages 310?315.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression. Artificial Intelligence 139(1):91?107.
McDonald, Ryan. 2006. Discriminative sentence compres-sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy, pages 297?304.
Nguyen, Minh Le, Susumu Horiguchi, Akira Shimazu, andBao Tu Ho. 2004a. Example-based sentence reduction us-ing the hidden Markov model. ACM TALIP 3(2):146?158.
Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi,Tu Bao Ho, and Masaru Fukushi. 2004b. Probabilisticsentence reduction using support vector machines. In Pro-
ceedings of the 20th COLING. Geneva, Switzerland, pages743?749.
Press, William H., Saul A. Teukolsky, William T. Vetterling,and Brian P. Flannery. 1992. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge University Press,New York, NY, USA.
Quinlan, J. R. 1993. C4.5 ? Programs for Machine Learn-
ing. The Morgan Kaufmann series in machine learning.Morgan Kaufman Publishers.
Riezler, Stefan, Tracy H. King, Richard Crouch, and AnnieZaenen. 2003. Statistical sentence condensation usingambiguity packing and stochastic disambiguation meth-ods for lexical-functional grammar. In Proceedings of the
HLT/NAACL. Edmonton, Canada, pages 118?125.
Turner, Jenine and Eugene Charniak. 2005. Supervised andunsupervised learning for sentence compression. In Pro-
ceedings of the 43rd ACL. Ann Arbor, MI, pages 290?297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence compres-sion for automated subtitling: A hybrid approach. In Pro-
ceedings of the ACL Workshop on Text Summarization.Barcelona, Spain, pages 89?95.
Weiss, Sholom M. and Casimir A. Kulikowski. 1991. Com-
puter systems that learn: classification and prediction
methods from statistics, neural nets, machine learning,
and expert systems. Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.
384
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1161?1168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Optimal Constituent Alignment with Edge Covers for Semantic Projection
Sebastian Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
pado@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Given a parallel corpus, semantic projec-
tion attempts to transfer semantic role an-
notations from one language to another,
typically by exploiting word alignments.
In this paper, we present an improved
method for obtaining constituent align-
ments between parallel sentences to guide
the role projection task. Our extensions
are twofold: (a) we model constituent
alignment as minimum weight edge cov-
ers in a bipartite graph, which allows us to
find a globally optimal solution efficiently;
(b) we propose tree pruning as a promising
strategy for reducing alignment noise. Ex-
perimental results on an English-German
parallel corpus demonstrate improvements
over state-of-the-art models.
1 Introduction
Recent years have witnessed increased interest in
data-driven methods for many natural language
processing (NLP) tasks, ranging from part-of-
speech tagging, to parsing, and semantic role la-
belling. The success of these methods is due partly
to the availability of large amounts of training data
annotated with rich linguistic information. Unfor-
tunately, such resources are largely absent for al-
most all languages except English. Given the data
requirements for supervised learning, and the cur-
rent paucity of suitable data for many languages,
methods for generating annotations (semi-)auto-
matically are becoming increasingly popular.
Annotation projection tackles this problem by
leveraging parallel corpora and the high-accuracy
tools (e.g., parsers, taggers) available for a
few languages. Specifically, through the use of
word alignments, annotations are transfered from
resource-rich languages onto low density ones.
The projection process can be decomposed into
three steps: (a) determining the units of projection;
these are typically words but can also be chunks
or syntactic constituents; (b) inducing alignments
between the projection units and projecting anno-
tations along these alignments; (c) reducing the
amount of noise in the projected annotations, often
due to errors and omissions in the word alignment.
The degree to which analyses are parallel across
languages is crucial for the success of projection
approaches. A number of recent studies rely on
this notion of parallelism and demonstrate that an-
notations can be adequately projected for parts of
speech (Yarowsky and Ngai, 2001; Hi and Hwa,
2005), chunks (Yarowsky and Ngai, 2001), and de-
pendencies (Hwa et al, 2002).
In previous work (Pad? and Lapata, 2005) we
considered the annotation projection of seman-
tic roles conveyed by sentential constituents such
as AGENT, PATIENT, or INSTRUMENT. Semantic
roles exhibit a high degree of parallelism across
languages (Boas, 2005) and thus appear amenable
to projection. Furthermore, corpora labelled with
semantic role information can be used to train
shallow semantic parsers (Gildea and Jurafsky,
2002), which could in turn benefit applications in
need of broad-coverage semantic analysis. Exam-
ples include question answering, information ex-
traction, and notably machine translation.
Our experiments concentrated primarily on the
first projection step, i.e., establishing the right
level of linguistic analysis for effecting projec-
tion. We showed that projection schemes based
on constituent alignments significantly outperform
schemes that rely exclusively on word alignments.
A local optimisation strategy was used to find con-
stituent alignments, while relying on a simple fil-
tering technique to handle noise.
The study described here generalises our earlier
semantic role projection framework in two impor-
tant ways. First, we formalise constituent projec-
tion as the search for aminimum weight edge cover
in a weighted bipartite graph. This formalisation
1161
efficiently yields constituent alignments that are
globally optimal. Second, we propose tree prun-
ing as a general noise reduction strategy, which ex-
ploits both structural and linguistic information to
enable projection. Furthermore, we quantitatively
assess the impact of noise on the task by evaluating
both on automatic and manual word alignments.
In Section 2, we describe the task of role-
semantic projection and the syntax-based frame-
work introduced in Pad? and Lapata (2005). Sec-
tion 3 explains how semantic role projection can
be modelled with minimum weight edge covers in
bipartite graphs. Section 4 presents our tree prun-
ing strategy. We present our evaluation framework
and results in Section 5. A discussion of related
and future work concludes the paper.
2 Cross-lingual Semantic Role projection
Semantic role projection is illustrated in Figure 1
using English and German as the source-target
language pair. We assume a FrameNet-style se-
mantic analysis (Fillmore et al, 2003). In this
paradigm, the semantics of predicates and their
arguments are described in terms of frames, con-
ceptual structures which model prototypical situ-
ations. The English sentence Kim promised to be
on time in Figure 1 is an instance of the COM-
MITMENT frame. In this particular example, the
frame introduces two roles, i.e., SPEAKER (Kim)
and MESSAGE (to be on time). Other possible,
though unrealised, roles are ADDRESSEE, MES-
SAGE, and TOPIC. The COMMITMENT frame can
be introduced by promise and several other verbs
and nouns such as consent or threat.
We also assume that frame-semantic annota-
tions can be obtained reliably through shallow
semantic parsing.1 Following the assignment of
semantic roles on the English side, (imperfect)
word alignments are used to infer semantic align-
ments between constituents (e.g., to be on time
is aligned with p?nktlich zu kommen), and the
role labels are transferred from one language to
the other. Note that role projection can only take
place if the source predicate (here promised ) is
word-aligned to a target predicate (here versprach )
evoking the same frame; if this is not the case
(e.g., in metaphors), projected roles will not be
generally appropriate.
We represent the source and target sentences
as sets of linguistic units, Us and Ut , respectively.
1See Carreras and M?rquez (2005) for an overview of re-
cent approaches to semantic parsing.
Kim versprach, p?nktlich zu kommen
Kim promised to be on time
S
S
NP
NP
Commitment
M
e
s
s
a
g
e
S
p
e
a
k
e
r
Commitment
S
p
e
a
k
e
r
M
e
s
s
a
g
e
Figure 1: Projection of semantic roles from En-
glish to German (word alignments as dotted lines)
The assignment of semantic roles on the source
side is a function roles : R ? 2Us from roles to
sets of source units. Constituent alignments are
obtained in two steps. First, a real-valued func-
tion sim : Us ?Ut ? R estimates pairwise simi-
larities between source and target units. To make
our model robust to alignment noise, we use only
content words to compute the similarity func-
tion. Next, a decision procedure uses the similar-
ity function to determine the set of semantically
equivalent, i.e., aligned units A?Us?Ut . Once A
is known, semantic projection reduces to transfer-
ring the semantic roles from the source units onto
their aligned target counterparts:
rolet(r) = {ut |?us ? roles(r) : (us,ut) ? A}
In Pad? and Lapata (2005), we evaluated two
main parameters within this framework: (a) the
choice of linguistic units and (b) methods for com-
puting semantic alignments. Our results revealed
that constituent-based models outperformed word-
based ones by a wide margin (0.65 Fscore
vs. 0.46), thus demonstrating the importance of
bracketing in amending errors and omissions in
the automatic word alignment. We also com-
pared two simplistic alignment schemes, back-
ward alignment and forward alignment. The
first scheme aligns each target constituent to its
most similar source constituent, whereas the sec-
ond (A f ) aligns each source constituent to its most
similar target constituent:
A f = {(us,ut) |ut = argmax
u?t?Ut
sim(us,u
?
t)}
1162
An example constituent alignment obtained from
the forward scheme is shown in Figure 2 (left
side). The nodes represent constituents in the
source and target language and the edges indicate
the resulting alignment. Forward alignment gener-
ally outperformed backward alignment (0.65 Fs-
core vs. 0.45). Both procedures have a time com-
plexity quadratic in the maximal number of sen-
tence nodes: O(|Us||Ut |) = O(max(|Us|, |Ut |)2).
A shortcoming common to both decision proce-
dures is that they are local, i.e., they optimise the
alignment for each node independently of all other
nodes. Consider again Figure 2. Here, the for-
ward procedure creates alignments for all source
nodes, but leaves constituents from the target set
unaligned (see target node (1)). Moreover, local
alignment methods constitute a rather weak model
of semantic equivalence since they allow one tar-
get node to correspond to any number of source
nodes (see target node (3) in Figure 2, which is
aligned to three source nodes). In fact, by allow-
ing any alignment between constituents, the lo-
cal models can disregard important linguistic in-
formation, thus potentially leading to suboptimal
results. We investigate this possibility by propos-
ing well-understood global optimisation models
which suitably constrain the resulting alignments.
Besides matching constituents reliably, poor
word alignments are a major stumbling block
for achieving accurate projections. Previous re-
search addresses this problem in a post-processing
step, by reestimating parameter values (Yarowsky
and Ngai, 2001), by applying transformation
rules (Hwa et al, 2002), by using manually la-
belled data (Hi and Hwa, 2005), or by relying on
linguistic criteria (Pad? and Lapata, 2005). In this
paper, we present a novel filtering technique based
on tree pruning which removes extraneous con-
stituents in a preprocessing stage, thereby disasso-
ciating filtering from the alignment computation.
In the remainder of this paper, we present the
details of our global optimisation and filtering
techniques. We only consider constituent-based
models, since these obtained the best performance
in our previous study (Pad? and Lapata, 2005).
3 Globally optimal constituent alignment
We model constituent alignment as a minimum
weight bipartite edge cover problem. A bipartite
graph is a graph G = (V,E) whose node set V is
partitioned into two nonempty sets V1 and V2 in
such a way that every edge E joins a node in V1
to a node in V2. In a weighted bipartite graph a
weight is assigned to each edge. An edge cover is
a subgraph of a bipartite graph so that each node is
linked to at least one node of the other partition. A
minimum weight edge cover is an edge cover with
the least possible sum of edge weights.
In our projection application, the two parti-
tions are the sets of source and target sentence
constituents, Us and Ut , respectively. Each source
node is connected to all target nodes and each tar-
get node to all source nodes; these edges can be
thought of as potential constituent alignments. The
edge weights, which represent the (dis)similarity
between nodes us and ut are set to 1? sim(us,ut).2
The minimum weight edge cover then represents
the alignment with the maximal similarity be-
tween source and target constituents. Below, we
present details on graph edge covers and a more
restricted kind, minimum weight perfect bipartite
matchings. We also discuss their computation.
Edge covers Given a bipartite graph G, a min-
imum weight edge cover Ae can be defined as:
Ae = argmin
Edge cover E
?
(us,ut)?E
1? sim(us,ut)
An example edge cover is illustrated in Figure 2
(middle). Edge covers are somewhat more con-
strained compared to the local model described
above: all source and target nodes have to take part
in some alignment. We argue that this is desirable
in modelling constituent alignment, since impor-
tant linguistic units will not be ignored. As can be
seen, edge covers allow one-to-many alignments
which are common when translating from one lan-
guage to another. For example, an English con-
stituent might be split into several German con-
stituents or alternatively two English constituents
might be merged into a single German constituent.
In Figure 2, the source nodes (3) and (4) corre-
spond to target node (4). Since each node of either
side has to participate in at least one alignment,
edge covers cannot account for insertions arising
when constituents in the source language have no
counterpart in their target language, or vice versa,
as is the case for deletions.
Weighted perfect bipartite matchings Per-
fect bipartite matchings are a more constrained
version of edge covers, in which each node has ex-
actly one adjacent edge. This restricts constituent
2The choice of similarity function is discussed in Sec-
tion 5.
1163
23
4
5
6
1
2
3
4
1
U
s
U
t
r
1
r
2
r
2
r
1
r
2
2
3
4
5
6
1
2
3
4
1
U
s
U
t
r
1
r
2
r
2
r
1
r
2
2
3
4
5
6
1
2
3
4
1
U
s
U
t
r
1
r
2
r
2
r
1
r
2
d
d
Figure 2: Constituent alignments and role projections resulting from different decision procedures
(Us,Ut : sets of source and target constituents; r1,r2: two semantic roles). Left: local forward alignment;
middle: edge cover; right: perfect matching with dummy nodes
alignment to a bijective function: each source
constituent is linked to exactly one target con-
stituent, and vice versa. Analogously, a minimum
weight perfect bipartite matching Am is a mini-
mum weight edge cover obeying the one-to-one
constraint:
Am = argmin
Matching M
?
(us,ut)?M
1? sim(us,ut)
An example of a perfect bipartite matching is
given in Figure 2 (right), where each node has ex-
actly one adjacent edge. Note that the target side
contains two nodes labelled (d), a shorthand for
?dummy? node. Since sentence pairs will often
differ in length, the resulting graph partitions will
have different sizes as well. In such cases, dummy
nodes are introduced in the smaller partition to
enable perfect matching. Dummy nodes are as-
signed a similarity of zero with all other nodes.
Alignments to dummy nodes (such as for source
nodes (3) and (6)) are ignored during projection.
Perfect matchings are more restrictive models
of constituent alignment than edge covers. Being
bijective, the resulting alignments cannot model
splitting or merging operations at all. Insertions
and deletions can be modelled only indirectly by
aligning nodes in the larger partition to dummy
nodes on the other side (see the source side in Fig-
ure 2 where nodes (3) and (6) are aligned to (d)).
Section 5 assesses if these modelling limitations
impact the quality of the resulting alignments.
Algorithms Minimum weight perfect match-
ings in bipartite graphs can be computed effi-
ciently in cubic time using algorithms for net-
work optimisation (Fredman and Tarjan, 1987;
timeO(|Us|2 log |Us|+ |Us|2|Ut |)) or algorithms for
the equivalent linear assignment problem (Jonker
and Volgenant, 1987; time O(max(|Us|, |Ut |)3)).
Their complexity is a linear factor slower than the
quadratic runtime of the local optimisation meth-
ods presented in Section 2.
The computation of (general) edge covers has
been investigated by Eiter and Mannila (1997) in
the context of distance metrics for point sets. They
show that edge covers can be reduced to minimum
weight perfect matchings of an auxiliary bipar-
tite graph with two partitions of size |Us|+ |Ut |.
This allows the computation of general minimum
weight edge covers in time O((|Us|+ |Ut |)3).
4 Filtering via Tree Pruning
We introduce two filtering techniques which effec-
tively remove constituents from source and target
trees before alignment takes place. Tree pruning as
a preprocessing step is more general and more effi-
cient than our original post-processing filter (Pad?
and Lapata, 2005) which was embedded into the
similarity function. Not only does tree pruning not
interfere with the similarity function but also re-
duces the size of the graph, thus speeding up the
algorithms discussed in the previous section.
We present two instantiations of tree pruning:
word-based filtering, which subsumes our earlier
method, and argument-based filtering, which elim-
inates unlikely argument candidates.
Word-based filtering This technique re-
moves terminal nodes from parse trees accord-
ing to certain linguistic or alignment-based crite-
ria. We apply two word-based filters in our ex-
periments. The first removes non-content words,
i.e., all words which are not adjectives, adverbs,
verbs, or nouns, from the source and target sen-
1164
Kim versprach, p?nktlich zu kommen.
VP
S
VP
S
Figure 3: Filtering of unlikely arguments (predi-
cate in boldface, potential arguments in boxes).
tences (Pad? and Lapata, 2005). We also use a
novel filter which removes all words which remain
unaligned in the automatic word alignment. Non-
terminal nodes whose terminals are removed by
these filters, are also pruned.
Argument filtering Previous work in shal-
low semantic parsing has demonstrated that not
all nodes in a tree are equally probable as seman-
tic roles for a given predicate (Xue and Palmer,
2004). In fact, assuming a perfect parse, there is
a ?set of likely arguments?, to which almost all
semantic roles roles should be assigned to. This
set of likely arguments consists of all constituents
which are a child of some ancestor of the pred-
icate, provided that (a) they do not dominate the
predicate themselves and (b) there is no sentence
boundary between a constituent and its predicate.
This definition covers long-distance dependencies
such as control constructions for verbs, or support
constructions for nouns and adjectives, and can be
extended slightly to accommodate coordination.
This argument-based filter reduces target trees
to a set of likely arguments. In the example in Fig-
ure 3, all tree nodes are removed except Kim and
p?nktlich zu kommen.
5 Evaluation Set-up
Data For evaluation, we used the parallel cor-
pus3 from our earlier work (Pad? and Lapata,
2005). It consists of 1,000 English-German sen-
tence pairs from the Europarl corpus (Koehn,
2005). The sentences were automatically parsed
(using Collin?s 1997 parser for English and
Dubey?s 2005 parser for German), and manually
annotated with FrameNet-like semantic roles (see
Pad? and Lapata 2005 for details.)
Word alignments were computed with the
GIZA++ toolkit (Och and Ney, 2003), using the
3The corpus can be downloaded from http://www.
coli.uni-saarland.de/~pado/projection/.
entire English-German Europarl bitext as training
data (20M words). We used the GIZA++ default
settings to induce alignments for both directions
(source-target, target-source). Following common
practise in MT (Koehn et al, 2003), we considered
only their intersection (bidirectional alignments
are known to exhibit high precision). We also pro-
duced manual word alignments for all sentences
in our corpus, using the GIZA++ alignments as a
starting point and following the Blinker annotation
guidelines (Melamed, 1998).
Method and parameter choice The con-
stituent alignment models we present are unsu-
pervised in that they do not require labelled data
for inferring correct alignments. Nevertheless, our
models have three parameters: (a) the similarity
measure for identifying semantically equivalent
constituents; (b) the filtering procedure for remov-
ing noise in the data (e.g., wrong alignments); and
(c) the decision procedure for projection.
We retained the similarity measure introduced
in Pad? and Lapata (2005) which computes the
overlap between a source constituent and its can-
didate projection, in both directions. Let y(cs) and
y(ct) denote the yield of a source and target con-
stituent, respectively, and al(T ) the union of all
word alignments for a token set T :
sim(cs,ct) =
|y(ct)?al(y(cs))|
|y(cs)|
|y(cs)?al(y(ct))|
|y(ct)|
We examined three filtering procedures (see Sec-
tion 4): removing non-aligned words (NA), re-
moving non-content words (NC), and removing
unlikely arguments (Arg). These were combined
with three decision procedures: local forward
alignment (Forward), perfect matching (Perf-
Match), and edge cover matching (EdgeCover)
(see Section 3). We used Jonker and Vol-
genant?s (1987) solver4 to compute weighted per-
fect matchings.
In order to find optimal parameter settings for
our models, we split our corpus randomly into a
development and test set (both 50% of the data)
and examined the parameter space exhaustively
on the development set. The performance of the
best models was then assessed on the test data.
The models had to predict semantic roles for Ger-
man, using English gold standard roles as input,
and were evaluated against German gold standard
4The software is available from http://www.
magiclogic.com/assignment.html.
1165
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 66.0 56.5 60.9
PerfMatch 71.7 54.7 62.1
N
o
F
il
te
r
EdgeCover 65.6 57.3 61.2
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 74.1 56.1 63.9
PerfMatch 73.3 62.1 67.2
N
A
F
il
te
r
EdgeCover 70.5 62.9 66.5
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 64.3 47.8 54.8
PerfMatch 73.1 56.9 64.0
N
C
F
il
te
r
EdgeCover 67.5 57.0 61.8
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 69.9 60.7 65.0
PerfMatch 80.4 48.1 60.2
A
rg
F
il
te
r
EdgeCover 69.6 60.6 64.8
UpperBnd 85.0 84.0 84.0
Table 1: Model comparison using intersective alignments (development set)
roles. To gauge the extent to which alignment er-
rors are harmful, we present results both on inter-
sective and manual alignments.
Upper bound and baseline In Pad? and La-
pata (2005), we assessed the feasibility of seman-
tic role projection by measuring how well anno-
tators agreed on identifying roles and their spans.
We obtained an inter-annotator agreement of 0.84
(F-score), which can serve as an upper bound for
the projection task. As a baseline, we use a sim-
ple word-based model (WordBL) from the same
study. The units of this model are words, and the
span of a projected role is the union of all target
terminals aligned to a terminal of the source role.
6 Results
Development set Our results on the develop-
ment set are summarised in Table 1. We show how
performance varies for each model according to
different filtering procedures when automatically
produced word alignments are used. No filtering
is applied to the baseline model (WordBL).
Without filtering, local and global models yield
comparable performance. Models based on perfect
bipartite matchings (PerfMatch) and edge covers
(EdgeCover) obtain slight F-score improvements
over the forward alignment model (Forward). It
is worth noticing that PerfMatch yields a signifi-
cantly higher precision (using a ?2 test, p < 0.01)
than Forward and EdgeCover. This indicates that,
even without filtering, PerfMatch delivers rather
accurate projections, however with low recall.
Model performance seems to increase with tree
pruning. When non-aligned words are removed
(Table 1, NA Filter), PerfMatch and EdgeCover
reach an F-score of 67.2 and 66.5, respectively.
This is an increase of approximately 3% over the
local Forward model. Although the latter model
yields high precision (74.1%), its recall is sig-
nificantly lower than PerfMatch and EdgeCover
(p < 0.01). This demonstrates the usefulness of
filtering for the more constrained global models
which as discussed in Section 3 can only represent
a limited set of alignment possibilities.
The non-content words filter (NC filter) yields
smaller improvements. In fact, for the Forward
model, results are worse than applying no filter-
ing at all. We conjecture that NC is an overly
aggressive filter which removes projection-critical
words. This is supported by the relatively low re-
call values. In comparison to NA, recall drops
by 8.3% for Forward and by almost 6% for Perf-
Match and EdgeCover. Nevertheless, both Perf-
Match and EdgeCover outperform the local For-
ward model. PerfMatch is the best performing
model reaching an F-score of 64.0%.
We now consider how the models behave when
the argument-based filter is applied (Arg, Table 1,
bottom). As can be seen, the local model benefits
most from this filter, whereas PerfMatch is worst
affected; it obtains its highest precision (80.4%) as
well as its lowest recall (48.1%). This is somewhat
expected since the filter removes the majority of
nodes in the target partition causing a proliferation
of dummy nodes. The resulting edge covers are
relatively ?unnatural?, thus counterbalancing the
advantages of global optimisation.
To summarise, we find on the development set
that PerfMatch in the NA Filter condition obtains
the best performance (F-score 67.2%), followed
closely by EdgeCover (F-score 66.5%) in the same
1166
Model Prec Rec F-score
WordBL 45.7 45.0 43.3
Forward (Arg) 72.4 63.2 67.5
PerfMatch (NA) 75.7 63.7 69.2
EdgeCover (NA) 73.0 64.9 68.7
In
te
rs
ec
tiv
e
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 62.1 60.7 61.4
Forward (Arg) 72.2 68.6 70.4
PerfMatch (NA) 75.7 67.5 71.4
EdgeCover (NA) 71.9 69.3 70.6M
an
ua
l
UpperBnd 85.0 84.0 84.0
Table 2: Model comparison using intersective and
manual alignments (test set)
condition. In general, PerfMatch seems less sensi-
tive to the type of filtering used; it yields best re-
sults in three out of four filtering conditions (see
boldface figures in Table 1). Our results further in-
dicate that Arg boosts the performance of the local
model by guiding it towards linguistically appro-
priate alignments.5
A comparative analysis of the output of Perf-
Match and EdgeCover revealed that the two mod-
els make similar errors (85% overlap). Disagree-
ments, however, arise with regard to misparses.
Consider as an example the sentence pair:
The Charter is [NP an opportunity to
bring the EU closer to the people.]
Die Charta ist [NP eine Chance], [S die
EU den B?rgern n?herzubringen.]
An ideal algorithm would align the English NP
to both the German NP and S. EdgeCover, which
can model one-to-many-relationships, acts ?con-
fidently? and aligns the NP to the German S to
maximise the overlap similarity, incurring both a
precision and a recall error. PerfMatch, on the
other hand, cannot handle one-to-many relation-
ships, acts ?cautiously? and aligns the English NP
to a dummy node, leading to a recall error. Thus,
even though EdgeCover?s analysis is partly right,
it will come out worse than PerfMatch, given the
current dataset and evaluation method.
Test set We now examine whether our results
carry over to the test data. Table 2 shows the
5Experiments using different filter combinations did not
lead to performance gains over individual filters and are not
reported here due to lack of space.
performance of the best models (Forward (Arg),
PerfMatch (NA), and EdgeCover (NA)) on auto-
matic (Intersective) and manual (Manual) align-
ments.6 All models perform significantly better
than the baseline but significantly worse than the
upper bound (both in terms of precision and recall,
p < 0.01). PerfMatch and EdgeCover yield better
F-scores than the Forward model. In fact, Perf-
Match yields a significantly better precision than
Forward (p < 0.01).
Relatively small performance gains are ob-
served when manual alignments are used. The F-
score increases by 2.9% for Forward, 2.2% for
PerfMatch, and 1.9% for EdgeCover. Also note
that this better performance is primarily due to a
significant increase in recall (p < 0.01), but not
precision. This is an encouraging result indicating
that our filters and graph-based algorithms elim-
inate alignment noise to a large extent. Analysis
of the models? output revealed that the remain-
ing errors are mostly due to incorrect parses (none
of the parsers employed in this work were trained
on the Europarl corpus) but also to modelling de-
ficiencies. Recall from Section 3 that our global
models cannot currently capture one-to-zero cor-
respondences, i.e., deletions and insertions.
7 Related work
Previous work has primarily focused on the pro-
jection of grammatical (Yarowsky and Ngai, 2001)
and syntactic information (Hwa et al, 2002). An
exception is Fung and Chen (2004), who also
attempt to induce FrameNet-style annotations in
Chinese. Their method maps English FrameNet
entries to concepts listed in HowNet7, an on-line
ontology for Chinese, without using parallel texts.
The present work extends our earlier projection
framework (Pad? and Lapata, 2005) by proposing
global methods for automatic constituent align-
ment. Although our models are evaluated on the
semantic role projection task, we believe they also
show promise in the context of statistical ma-
chine translation. Especially for systems that use
syntactic information to enhance translation qual-
ity. For example, Xia and McCord (2004) exploit
constituent alignment for rearranging sentences in
the source language so as to make their word or-
6Our results on the test set are slightly higher in compar-
ison to the development set. The fluctuation reflects natural
randomness in the partitioning of our corpus.
7See http://www.keenage.com/zhiwang/e_
zhiwang.html.
1167
der similar to that of the target language. They
learn tree reordering rules by aligning constituents
heuristically using a naive local optimisation pro-
cedure analogous to forward alignment. A simi-
lar approach is described in Collins et al (2005);
however, the rules are manually specified and the
constituent alignment step reduces to inspection of
the source-target sentence pairs. The global opti-
misation models presented in this paper could be
easily employed for the reordering task common
to both approaches.
Other approaches treat rewrite rules not as a
preprocessing step (e.g., for reordering source
strings), but as a part of the translation model
itself (Gildea, 2003; Gildea, 2004). Constituent
alignments are learnt by estimating the probabil-
ity of tree transformations, such as node deletions,
insertions, and reorderings. These models have a
greater expressive power than our edge cover mod-
els; however, this implies that approximations are
often used to make computation feasible.
8 Conclusions
In this paper, we have proposed a novel method
for obtaining constituent alignments between par-
allel sentences and have shown that it is use-
ful for semantic role projection. A key aspect of
our approach is the formalisation of constituent
alignment as the search for a minimum weight
edge cover in a bipartite graph. This formalisation
provides efficient mechanisms for aligning con-
stituents and yields results superior to heuristic ap-
proaches. Furthermore, we have shown that tree-
based noise filtering techniques are essential for
good performance.
Our approach rests on the assumption that con-
stituent alignment can be determined solely from
the lexical similarity between constituents. Al-
though this allows us to model constituent align-
ments efficiently as edge covers, it falls short of
modelling translational divergences such as substi-
tutions or insertions/deletions. In future work, we
will investigate minimal tree edit distance (Bille,
2005) and related formalisms which are defined
on tree structures and can therefore model diver-
gences explicitly. However, it is an open ques-
tion whether cross-linguistic syntactic analyses are
similar enough to allow for structure-driven com-
putation of alignments.
Acknowledgments The authors acknowledge
the support of DFG (Pad?; grant Pi-154/9-2) and
EPSRC (Lapata; grant GR/T04540/01).
References
P. Bille. 2005. A survey on tree edit distance and related
problems. Theoretical Computer Science, 337(1-3):217?
239.
H. C. Boas. 2005. Semantic frames as interlingual represen-
tations for multilingual lexical databases. International
Journal of Lexicography, 18(4):445?478.
X. Carreras, L. M?rquez, eds. 2005. Proceedings of the
CoNLL shared task: Semantic role labelling, Boston, MA,
2005.
M. Collins, P. Koehn, I. Kuc?erov?. 2005. Clause restructur-
ing for statistical machine translation. In Proceedings of
the 43rd ACL, 531?540, Ann Arbor, MI.
M. Collins. 1997. Three generative, lexicalised models for
statistical parsing. In Proceedings of the ACL/EACL, 16?
23, Madrid, Spain.
A. Dubey. 2005. What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. In Pro-
ceedings of the 43rd ACL, 314?321, Ann Arbor, MI.
T. Eiter, H. Mannila. 1997. Distance measures for point sets
and their computation. Acta Informatica, 34(2):109?133.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003. Back-
ground to FrameNet. International Journal of Lexicogra-
phy, 16:235?250.
M. L. Fredman, R. E. Tarjan. 1987. Fibonacci heaps and
their uses in improved network optimization algorithms.
Journal of the ACM, 34(3):596?615.
P. Fung, B. Chen. 2004. BiFrameNet: Bilingual frame se-
mantics resources construction by cross-lingual induction.
In Proceedings of the 20th COLING, 931?935, Geneva,
Switzerland.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Proceedings of the 41st ACL, 80?87, Sap-
poro, Japan.
D. Gildea. 2004. Dependencies vs. constituents for tree-
based alignment. In Proceedings of the EMNLP, 214?221,
Barcelona, Spain.
C. Hi, R. Hwa. 2005. A backoff model for bootstrapping
resources for non-english languages. In Proceedings of
the HLT/EMNLP, 851?858, Vancouver, BC.
R. Hwa, P. Resnik, A. Weinberg, O. Kolak. 2002. Evaluation
of translational correspondence using annotation projec-
tion. In Proceedings of the 40th ACL, 392?399, Philadel-
phia, PA.
R. Jonker, T. Volgenant. 1987. A shortest augmenting path
algorithm for dense and sparse linear assignment prob-
lems. Computing, 38:325?340.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the HLT/NAACL, 127?133,
Edmonton, AL.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In Proceedings of the MT Summit X,
Phuket, Thailand.
I. D. Melamed. 1998. Manual annotation of translational
equivalence: The Blinker project. Technical Report IRCS
TR #98-07, IRCS, University of Pennsylvania, 1998.
F. J. Och, H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?52.
S. Pad?, M. Lapata. 2005. Cross-lingual projection
of role-semantic information. In Proceedings of the
HLT/EMNLP, 859?866, Vancouver, BC.
F. Xia, M. McCord. 2004. Improving a statistical MT system
with automatically learned rewrite patterns. In Proceed-
ings of the 20th COLING, 508?514, Geneva, Switzerland.
N. Xue, M. Palmer. 2004. Calibrating features for seman-
tic role labeling. In Proceedings of the EMNLP, 88?94,
Barcelona, Spain.
D. Yarowsky, G. Ngai. 2001. Inducing multilingual text
analysis tools via robust projection across aligned corpora.
In Proceedings of the HLT, 161?168, San Diego, CA.
1168
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 144?151,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Constraint-based Sentence Compression
An Integer Programming Approach
James Clarke and Mirella Lapata
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
jclarke@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The ability to compress sentences while
preserving their grammaticality and most
of their meaning has recently received
much attention. Our work views sentence
compression as an optimisation problem.
We develop an integer programming for-
mulation and infer globally optimal com-
pressions in the face of linguistically moti-
vated constraints. We show that such a for-
mulation allows for relatively simple and
knowledge-lean compression models that
do not require parallel corpora or large-
scale resources. The proposed approach
yields results comparable and in some
cases superior to state-of-the-art.
1 Introduction
A mechanism for automatically compressing sen-
tences while preserving their grammaticality and
most important information would greatly bene-
fit a wide range of applications. Examples include
text summarisation (Jing 2000), subtitle genera-
tion from spoken transcripts (Vandeghinste and
Pan 2004) and information retrieval (Olivers and
Dolan 1999). Sentence compression is a complex
paraphrasing task with information loss involv-
ing substitution, deletion, insertion, and reordering
operations. Recent years have witnessed increased
interest on a simpler instantiation of the compres-
sion problem, namely word deletion (Knight and
Marcu 2002; Riezler et al 2003; Turner and Char-
niak 2005). More formally, given an input sen-
tence of words W = w1,w2, . . . ,wn, a compressionis formed by removing any subset of these words.
Sentence compression has received both gener-
ative and discriminative formulations in the liter-
ature. Generative approaches (Knight and Marcu
2002; Turner and Charniak 2005) are instantia-
tions of the noisy-channel model: given a long sen-
tence l, the aim is to find the corresponding short
sentence s which maximises the conditional prob-
ability P(s|l). In a discriminative setting (Knight
and Marcu 2002; Riezler et al 2003; McDonald
2006), sentences are represented by a rich fea-
ture space (typically induced from parse trees) and
the goal is to learn rewrite rules indicating which
words should be deleted in a given context. Both
modelling paradigms assume access to a training
corpus consisting of original sentences and their
compressions.
Unsupervised approaches to the compression
problem are few and far between (see Hori and Fu-
rui 2004 and Turner and Charniak 2005 for excep-
tions). This is surprising considering that parallel
corpora of original-compressed sentences are not
naturally available in the way multilingual corpora
are. The scarcity of such data is demonstrated by
the fact that most work to date has focused on a
single parallel corpus, namely the Ziff-Davis cor-
pus (Knight and Marcu 2002). And some effort
into developing appropriate training data would be
necessary when porting existing algorithms to new
languages or domains.
In this paper we present an unsupervised model
of sentence compression that does not rely on a
parallel corpus ? all that is required is a corpus
of uncompressed sentences and a parser. Given a
long sentence, our task is to form a compression
by preserving the words that maximise a scoring
function. In our case, the scoring function is an
n-gram language model, ?with a few strings at-
tached?. While straightforward to estimate, a lan-
guage model is a fairly primitive scoring function:
it has no notion of the overall sentence structure,
grammaticality or underlying meaning. We thus
couple our language model with a small number
of structural and semantic constraints capturing
global properties of the compression process.
We encode the language model and linguistic
constraints as linear inequalities and use Integer
Programming (IP) to infer compressions that are
consistent with both. The IP formulation allows us
to capture global sentence properties and can be
easily manipulated to provide compressions tai-
lored for specific applications. For example, we
144
could prevent overly long or overly short compres-
sions or generally avoid compressions that lack
a main verb or consist of repetitions of the same
word.
In the following section we provide an overview
of previous approaches to sentence compression.
In Section 3 we motivate the treatment of sentence
compression as an optimisation problem and for-
mulate our language model and constraints in the
IP framework. Section 4 discusses our experimen-
tal set-up and Section 5 presents our results. Dis-
cussion of future work concludes the paper.
2 Previous Work
Jing (2000) was perhaps the first to tackle the sen-
tence compression problem. Her approach uses
multiple knowledge sources to determine which
phrases in a sentence to remove. Central to her
system is a grammar checking module that spec-
ifies which sentential constituents are grammati-
cally obligatory and should therefore be present
in the compression. This is achieved using sim-
ple rules and a large-scale lexicon. Other knowl-
edge sources include WordNet and corpus evi-
dence gathered from a parallel corpus of original-
compressed sentence pairs. A phrase is removed
only if it is not grammatically obligatory, not the
focus of the local context and has a reasonable
deletion probability (estimated from the parallel
corpus).
In contrast to Jing (2000), the bulk of the re-
search on sentence compression relies exclusively
on corpus data for modelling the compression
process without recourse to extensive knowledge
sources (e.g., WordNet). Approaches based on the
noisy-channel model (Knight and Marcu 2002;
Turner and Charniak 2005) consist of a source
model P(s) (whose role is to guarantee that the
generated compression is grammatical), a chan-
nel model P(l|s) (capturing the probability that
the long sentence l is an expansion of the com-
pressed sentence s), and a decoder (which searches
for the compression s that maximises P(s)P(l|s)).
The channel model is typically estimated using
a parallel corpus, although Turner and Charniak
(2005) also present semi-supervised and unsu-
pervised variants of the channel model that esti-
mate P(l|s) without parallel data.
Discriminative formulations of the compres-
sion task include decision-tree learning (Knight
and Marcu 2002), maximum entropy (Riezler
et al 2003), support vector machines (Nguyen
et al 2004), and large-margin learning (McDonald
2006). We describe here the decision-tree model
in more detail since we will use it as a basis for
comparison when evaluating our own models (see
Section 4). According to this model, compression
is performed through a tree rewriting process in-
spired by the shift-reduce parsing paradigm. A se-
quence of shift-reduce-drop actions are performed
on a long parse tree, l, to create a smaller tree, s.
The compression process begins with an input
list generated from the leaves of the original sen-
tence?s parse tree and an empty stack. ?Shift? oper-
ations move leaves from the input list to the stack
while ?drop? operations delete from the input list.
Reduce operations are used to build trees from the
leaves on the stack. A decision-tree is trained on a
set of automatically generated learning cases from
a parallel corpus. Each learning case has a target
action associated with it and is decomposed into a
set of indicative features. The decision-tree learns
which action to perform given this set of features.
The final model is applied in a deterministic fash-
ion in which the features for the current state are
extracted and the decision-tree is queried. This is
repeated until the input list is empty and the final
compression is recovered by traversing the leaves
of resulting tree on the stack.
While most compression models operate over
constituents, Hori and Furui (2004) propose a
model which generates compressions through
word deletion. The model does not utilise parallel
data or syntactic information in any form. Given a
prespecified compression rate, it searches for the
compression with the highest score according to a
function measuring the importance of each word
and the linguistic likelihood of the resulting com-
pressions (language model probability). The score
is maximised through a dynamic programming al-
gorithm.
Although sentence compression has not been
explicitly formulated as an optimisation problem,
previous approaches have treated it in these terms.
The decoding process in the noisy-channel model
searches for the best compression given the source
and channel models. However, the compression
found is usually sub-optimal as heuristics are used
to reduce the search space or is only locally op-
timal due to the search method employed. The
decoding process used in Turner and Charniak?s
(2005) model first searches for the best combina-
tion of rules to apply. As they traverse their list
of compression rules they remove sentences out-
side the 100 best compressions (according to their
channel model). This list is eventually truncated
to 25 compressions.
In other models (Hori and Furui 2004; McDon-
ald 2006) the compression score is maximised
145
using dynamic programming. The latter guaran-
tees we will find the global optimum provided the
principle of optimality holds. This principle states
that given the current state, the optimal decision
for each of the remaining stages does not depend
on previously reached stages or previously made
decisions (Winston and Venkataramanan 2003).
However, we know this to be false in the case of
sentence compression. For example, if we have
included modifiers to the left of a head noun in
the compression then it makes sense that we must
include the head also. With a dynamic program-
ming approach we cannot easily guarantee such
constraints hold.
3 Problem Formulation
Our work models sentence compression explicitly
as an optimisation problem. There are 2n possible
compressions for each sentence and while many
of these will be unreasonable (Knight and Marcu
2002), it is unlikely that only one compression
will be satisfactory. Ideally, we require a func-
tion that captures the operations (or rules) that can
be performed on a sentence to create a compres-
sion while at the same time factoring how desir-
able each operation makes the resulting compres-
sion. We can then perform a search over all possi-
ble compressions and select the best one, as deter-
mined by how desirable it is.
Our formulation consists of two basic compo-
nents: a language model (scoring function) and a
small number of constraints ensuring that the re-
sulting compressions are structurally and semanti-
cally valid. Our task is to find a globally optimal
compression in the presence of these constraints.
We solve this inference problem using Integer Pro-
gramming without resorting to heuristics or ap-
proximations during the decoding process. Integer
programming has been recently applied to several
classification tasks, including relation extraction
(Roth and Yih 2004), semantic role labelling (Pun-
yakanok et al 2004), and the generation of route
directions (Marciniak and Strube 2005).
Before describing our model in detail, we in-
troduce some of the concepts and terms used in
Linear Programming and Integer Programming
(see Winston and Venkataramanan 2003 for an in-
troduction). Linear Programming (LP) is a tool
for solving optimisation problems in which the
aim is to maximise (or minimise) a given function
with respect to a set of constraints. The function
to be maximised (or minimised) is referred to as
the objective function. Both the objective function
and constraints must be linear. A number of deci-
sion variables are under our control which exert
influence on the objective function. Specifically,
they have to be optimised in order to maximise
(or minimise) the objective function. Finally, a set
of constraints restrict the values that the decision
variables can take. Integer Programming is an ex-
tension of linear programming where all decision
variables must take integer values.
3.1 Language Model
Assume we have a sentence W = w1,w2, . . . ,wnfor which we wish to generate a compression.
We introduce a decision variable for each word
in the original sentence and constrain it to be bi-
nary; a value of 0 represents a word being dropped,
whereas a value of 1 includes the word in the com-
pression. Let:
yi =
{ 1 if wi is in the compression0 otherwise ?i? [1 . . .n]
If we were using a unigram language model,
our objective function would maximise the overall
sum of the decision variables (i.e., words) multi-
plied by their unigram probabilities (all probabili-
ties throughout this paper are log-transformed):
maxz = n?
i=1
yi ?P(wi)
Thus if a word is selected, its corresponding yi isgiven a value of 1, and its probability P(wi) ac-cording to the language model will be counted in
our total score, z.
A unigram language model will probably gener-
ate many ungrammatical compressions. We there-
fore use a more context-aware model in our objec-
tive function, namely a trigram model. Formulat-
ing a trigram model in terms of an integer program
becomes a more involved task since we now must
make decisions based on word sequences rather
than isolated words. We first create some extra de-
cision variables:
pi =
{1 if wi starts the compression0 otherwise ?i ? [1 . . .n]
qi j =
?
?
?
1 if sequence wi,w j endsthe compression ?i ? [1 . . .n?1]
0 otherwise ? j ? [i+1 . . .n]
xi jk =
?
?
?
1 if sequence wi,w j,wk ?i ? [1 . . .n?2]is in the compression ? j ? [i+1 . . .n?1]
0 otherwise ?k ? [ j +1 . . .n]
Our objective function is given in Equation (1).
This is the sum of all possible trigrams that can
occur in all compressions of the original sentence
where w0 represents the ?start? token and wi is the
ith word in sentence W . Equation (2) constrains
146
the decision variables to be binary.
maxz = n?
i=1
pi ?P(wi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k= j+1
xi jk ?P(wk|wi,w j)
+
n?1
?
i=0
n
?
j=i+1
qi j ?P(end|wi,w j) (1)
subject to:
yi, pi,qi j,xi jk = 0 or 1 (2)
The objective function in (1) allows any combi-
nation of trigrams to be selected. This means that
invalid trigram sequences (e.g., two or more tri-
grams containing the symbol ?end?) could appear
in the output compression. We avoid this situation
by introducing sequential constraints (on the de-
cision variables yi,xi jk, pi, and qi j) that restrict theset of allowable trigram combinations.
Constraint 1 Exactly one word can begin a
sentence.
n
?
i=1
pi = 1 (3)
Constraint 2 If a word is included in the sen-
tence it must either start the sentence or be pre-
ceded by two other words or one other word and
the ?start? token w0.
yk ? pk ?
k?2
?
i=0
k?1
?
j=1
xi jk = 0 (4)
?k : k ? [1 . . .n]
Constraint 3 If a word is included in the sen-
tence it must either be preceded by one word and
followed by another or it must be preceded by one
word and end the sentence.
y j ?
j?1
?
i=0
n
?
k= j+1
xi jk ?
j?1
?
i=0
qi j = 0 (5)
? j : j ? [1 . . .n]
Constraint 4 If a word is in the sentence it
must be followed by two words or followed by one
word and then the end of the sentence or it must be
preceded by one word and end the sentence.
yi ?
n?1
?
j=i+1
n
?
k= j+1
xi jk ?
n
?
j=i+1
qi j ?
i?1
?
h=0
qhi = 0 (6)
?i : i ? [1 . . .n]
Constraint 5 Exactly one word pair can end
the sentence.
n?1
?
i=0
n
?
j=i+1
qi j = 1 (7)
Example compressions using the trigram model
just described are given in Table 1. The model in
O: He became a power player in Greek Politics in1974, when he founded the socialist Pasok Party.LM: He became a player in the Pasok.Mod: He became a player in the Pasok Party.Sen: He became a player in politics.Sig: He became a player in politics when he foundedthe Pasok Party.O: Finally, AppleShare Printer Server, formerly aseparate package, is now bundled with Apple-Share File Server.LM: Finally, AppleShare, a separate, AppleShare.Mod: Finally, AppleShare Server, is bundled.Sen: Finally, AppleShare Server, is bundled withServer.Sig: AppleShare Printer Server package is now bun-dled with AppleShare File Server.
Table 1: Compression examples (O: original sen-
tence, LM: compression with the trigram model,
Mod: compression with LM and modifier con-
straints, Sen: compression with LM, Mod and
sentential constraints, Sig: compression with LM,
Mod, Sen, and significance score)
its current state does a reasonable job of modelling
local word dependencies, but is unable to capture
syntactic dependencies that could potentially al-
low more meaningful compressions. For example,
it does not know that Pasok Party is the object
of founded or that Appleshare modifies Printer
Server.
3.2 Linguistic Constraints
In this section we propose a set of global con-
straints that extend the basic language model pre-
sented in Equations (1)?(7). Our aim is to bring
some syntactic knowledge into the compression
model and to preserve the meaning of the original
sentence as much as possible. Our constraints are
linguistically and semantically motivated in a sim-
ilar fashion to the grammar checking component
of Jing (2000). Importantly, we do not require any
additional knowledge sources (such as a lexicon)
beyond the parse and grammatical relations of the
original sentence. This is provided in our experi-
ments by the Robust Accurate Statistical Parsing
(RASP) toolkit (Briscoe and Carroll 2002). How-
ever, there is nothing inherent in our formulation
that restricts us to RASP; any other parser with
similar output could serve our purposes.
Modifier Constraints Modifier constraints
ensure that relationships between head words and
their modifiers remain grammatical in the com-
pression:
yi ? y j ? 0 (8)
?i, j : w j ? wi?s ncmods
yi ? y j ? 0 (9)
?i, j : w j ? wi?s detmods
147
Equation (8) guarantees that if we include a non-
clausal modifier (ncmod) in the compression then
the head of the modifier must also be included; this
is repeated for determiners (detmod) in (9).
We also want to ensure that the meaning of the
original sentence is preserved in the compression,
particularly in the face of negation. Equation (10)
implements this by forcing not in the compression
when the head is included. A similar constraint
is added for possessive modifiers (e.g., his, our),
as shown in Equation (11). Genitives (e.g., John?s
gift) are treated separately, mainly because they
are encoded as different relations in the parser (see
Equation (12)).
yi ? y j = 0 (10)
?i, j : w j ? wi?s ncmods?w j = not
yi ? y j = 0 (11)
?i, j : w j ? wi?s possessive detmods
yi ? y j = 0 (12)
?i, j : wi ? possessive ncmods
?w j = possessive
Compression examples with the addition of the
modifier constraints are shown in Table 1. Al-
though the compressions are grammatical (see the
inclusion of Party due to the modifier Pasok and
Server due to AppleShare), they are not entirely
meaning preserving.
Sentential Constraints We also define a few
intuitive constraints that take the overall sentence
structure into account. The first constraint (Equa-
tion (13)) ensures that if a verb is present in the
compression then so are its arguments, and if any
of the arguments are included in the compression
then the verb must also be included. We thus force
the program to make the same decision on the
verb, its subject, and object.
yi ? y j = 0 (13)
?i, j : w j ? subject/object of verb wi
Our second constraint forces the compression to
contain at least one verb provided the original sen-
tence contains one as well:
?
i?verbs
yi ? 1 (14)
Other sentential constraints include Equa-
tions (15) and (16) which apply to prepositional
phrases, wh-phrases and complements. These con-
straints force the introducing term (i.e., the prepo-
sition, complement or wh-word) to be included in
the compression if any word from within the syn-
tactic constituent is also included. The reverse is
also true, i.e., if the introducing term is included at
least one other word from the syntactic constituent
should also be included.
yi ? y j ? 0 (15)
?i, j : w j ? PP/COMP/WH-P
?wi starts PP/COMP/WH-P
?
i?PP/COMP/WH-P
yi ? y j ? 0 (16)
? j : w j starts PP/COMP/WH-P
We also wish to handle coordination. If two head
words are conjoined in the original sentence, then
if they are included in the compression the coordi-
nating conjunction must also be included:
(1? yi)+ y j ? 1 (17)
(1? yi)+ yk ? 1 (18)
yi +(1? y j)+(1? yk) ? 1 (19)
?i, j,k : w j ?wk conjoined by wi
Table 1 illustrates the compression output when
sentential constraints are added to the model. We
see that politics is forced into the compression due
to the presence of in; furthermore, since bundled
is in the compression, its object with Server is in-
cluded too.
Compression-related Constraints Finally,
we impose some hard constraints on the com-
pression output. First, Equation (20) disallows
anything within brackets in the original sentence
from being included in the compression. This
is a somewhat superficial attempt at excluding
parenthetical and potentially unimportant material
from the compression. Second, Equation (21)
forces personal pronouns to be included in the
compression. The constraint is important for
generating coherent document as opposed to
sentence compressions.
yi = 0 (20)
?i : wi ? brackets
yi = 1 (21)
?i : wi ? personal pronouns
It is also possible to influence the length of the
compressed sentence. For example, Equation (22)
forces the compression to contain at least b tokens.
Alternatively, we could force the compression to
be exactly b tokens (by substituting ? with =
in (22)) or to be less than b tokens (by replacing ?
with ?).1
n
?
i=1
yi ? b (22)
3.3 Significance Score
While the constraint-based language model pro-
duces more grammatical output than a regular lan-
1Compression rate can be also limited to a range by in-cluding two inequality constraints.
148
guage model, the sentences are typically not great
compressions. The language model has no notion
of which content words to include in the compres-
sion and thus prefers words it has seen before. But
words or constituents will be of different relative
importance in different documents or even sen-
tences.
Inspired by Hori and Furui (2004), we add to
our objective function (see Equation (1)) a signif-
icance score designed to highlight important con-
tent words. Specifically, we modify Hori and Fu-
rui?s significance score to give more weight to con-
tent words that appear in the deepest level of em-
bedding in the syntactic tree. The latter usually
contains the gist of the original sentence:
I(wi) =
l
N
? fi log FaFi (23)The significance score above is computed using a
large corpus where wi is a topic word (i.e., a nounor verb), fi and Fi are the frequency of wi in thedocument and corpus respectively, and Fa is thesum of all topic words in the corpus. l is the num-
ber of clause constituents above wi, and N is thedeepest level of embedding. The modified objec-
tive function is given below:
maxz = n?
i=1
yi ? I(wi)+
n
?
i=1
pi ?P(wi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k= j+1
xi jk ?P(wk|wi,w j)
+
n?1
?
i=0
n
?
j=i+1
qi j ?P(end|wi,w j) (24)
A weighting factor could be also added to the ob-
jective function, to counterbalance the importance
of the language model and the significance score.
4 Evaluation Set-up
We evaluated the approach presented in the pre-
vious sections against Knight and Marcu?s (2002)
decision-tree model. This model is a good basis for
comparison as it operates on parse trees and there-
fore is aware of syntactic structure (as our models
are) but requires a large parallel corpus for training
whereas our models do not; and it yields compara-
ble performance to the noisy-channel model.2 The
decision-tree model was compared against two
variants of our IP model. Both variants employed
the constraints described in Section 3.2 but dif-
fered in that one variant included the significance
2Turner and Charniak (2005) argue that the noisy-channelmodel is not an appropriate compression model since it usesa source model trained on uncompressed sentences and as aresult tends to consider compressed sentences less likely thanuncompressed ones.
score in its objective function (see (24)), whereas
the other one did not (see (1)). In both cases the
sequential constraints from Section 3.1 were ap-
plied to ensure that the language model was well-
formed. We give details below on the corpora we
used and explain how the different model parame-
ters were estimated. We also discuss how evalua-
tion was carried out using human judgements.
Corpora We evaluate our systems on two dif-
ferent corpora. The first is the compression corpus
of Knight and Marcu (2002) derived automatically
from document-abstract pairs of the Ziff-Davis
corpus. This corpus has been used in most pre-
vious compression work. We also created a com-
pression corpus from the HUB-4 1996 English
Broadcast News corpus (provided by the LDC).
We asked annotators to produce compressions for
50 broadcast news stories (1,370 sentences).3
The Ziff-Davis corpus is partitioned into train-
ing (1,035 sentences) and test set (32 sentences).
We held out 50 sentences from the training for de-
velopment purposes. We also split the Broadcast
News corpus into a training and test set (1,237/133
sentences). Forty sentences were randomly se-
lected for evaluation purposes, 20 from the test
portion of the Ziff-Davis corpus and 20 from the
Broadcast News corpus test set.
Parameter Estimation The decision-tree
model was trained, using the same feature set
as Knight and Marcu (2002) on the Ziff-Davis
corpus and used to obtain compressions for both
test corpora.4 For our IP models, we used a
language model trained on 25 million tokens from
the North American News corpus using the CMU-
Cambridge Language Modeling Toolkit (Clarkson
and Rosenfeld 1997) with a vocabulary size of
50,000 tokens and Good-Turing discounting.
The significance score used in our second model
was calculated using 25 million tokens from the
Broadcast News Corpus (for the spoken data) and
25 million tokens from the American News Text
Corpus (for the written data). Finally, the model
that includes the significance score was optimised
against a loss function similar to McDonald
(2006) to bring the language model and the score
into harmony. We used Powell?s method (Press
et al 1992) and 50 sentences (randomly selected
from the training set).
3The corpus is available from http://homepages.inf.
ed.ac.uk/s0460084/data/.4We found that the decision-tree was unable to producemeaningful compressions when trained on the BroadcastNews corpus (in most cases it recreated the original sen-tence). Thus we used the decision model trained on Ziff-Davis to generate Broadcast News compressions.
149
We also set a minimum compression length (us-
ing the constraint in Equation (22)) in both our
models to avoid overly short compressions. The
length was set at 40% of the original sentence
length or five tokens, whichever was larger. Sen-
tences under five tokens were not compressed.
In our modeling framework, we generate and
solve an IP for every sentence we wish to com-
press. We employed lp solve for this purpose, an
efficient Mixed Integer Programming solver.5 Sen-
tences typically take less than a few seconds to
compress on a 2 GHz Pentium IV machine.
Human Evaluation As mentioned earlier, the
output of our models is evaluated on 40 exam-
ples. Although the size of our test set is compa-
rable to previous studies (which are typically as-
sessed on 32 sentences from the Ziff-Davis cor-
pus), the sample is too small to conduct signif-
icance testing. To counteract this, human judge-
ments are often collected on compression out-
put; however the evaluations are limited to small
subject pools (often four judges; Knight and
Marcu 2002; Turner and Charniak 2005; McDon-
ald 2006) which makes difficult to apply inferen-
tial statistics on the data. We overcome this prob-
lem by conducting our evaluation using a larger
sample of subjects.
Specifically, we elicited human judgements
from 56 unpaid volunteers, all self reported na-
tive English speakers. The elicitation study was
conducted over the Internet. Participants were pre-
sented with a set of instructions that explained the
sentence compression task with examples. They
were asked to judge 160 compressions in to-
tal. These included the output of the three au-
tomatic systems on the 40 test sentences paired
with their gold standard compressions. Partici-
pants were asked to read the original sentence and
then reveal its compression by pressing a button.
They were told that all compressions were gen-
erated automatically. A Latin square design en-
sured that subjects did not see two different com-
pressions of the same sentence. The order of the
sentences was randomised. Participants rated each
compression on a five point scale based on the in-
formation retained and its grammaticality. Exam-
ples of our experimental items are given in Table 2.
5 Results
Our results are summarised in Table 3 which de-
tails the compression rates6 and average human
5The software is available from http://www.
geocities.com/lpsolve/.6We follow previous work (see references) in using theterm ?compression rate? to refer to the percentage of words
O: Apparently Fergie very much wants to have a ca-reer in television.G: Fergie wants a career in television.D: A career in television.LM: Fergie wants to have a career.Sig: Fergie wants to have a career in television.O: The SCAMP module, designed and built byUnisys and based on an Intel process, contains theentire 48-bit A-series processor.G: The SCAMP module contains the entire 48-bit A-series processor.D: The SCAMP module designed Unisys and basedon an Intel process.LM: The SCAMP module, contains the 48-bit A-seriesprocessor.Sig: The SCAMP module, designed and built byUnisys and based on process, contains the A-series processor.
Table 2: Compression examples (O: original sen-
tence, G: Gold standard, D: Decision-tree, LM: IP
language model, Sig: IP language model with sig-
nificance score)
Model CompR Rating
Decision-tree 56.1% 2.22??
LangModel 49.0% 2.23??
LangModel+Significance 73.6% 2.83?
Gold Standard 62.3% 3.68?
Table 3: Compression results; compression rate
(CompR) and average human judgements (Rat-
ing); ?: sig. diff. from gold standard; ?: sig. diff.
from LangModel+Significance
ratings (Rating) for the three systems and the gold
standard. As can be seen, the IP language model
(LangModel) is most aggressive in terms of com-
pression rate as it reduces the original sentences
on average by half (49%). Recall that we enforce a
minimum compression rate of 40% (see (22)). The
fact that the resulting compressions are longer, in-
dicates that our constraints instill some linguistic
knowledge into the language model, thus enabling
it to prefer longer sentences over extremely short
ones. The decision-tree model compresses slightly
less than our IP language model at 56.1% but still
below the gold standard rate. We see a large com-
pression rate increase from 49% to 73.6% when
we introduce the significance score into the objec-
tive function. This is around 10% higher than the
gold standard compression rate.
We now turn to the results of our elicitation
study. We performed an Analysis of Variance
(ANOVA) to examine the effect of different system
compressions. Statistical tests were carried out on
the mean of the ratings shown in Table 3. We ob-
serve a reliable effect of compression type by sub-
retained in the compression.
150
jects (F1(3,165) = 132.74, p < 0.01) and items(F2(3,117) = 18.94, p < 0.01). Post-hoc Tukeytests revealed that gold standard compressions are
perceived as significantly better than those gener-
ated by all automatic systems (? < 0.05). There is
no significant difference between the IP language
model and decision-tree systems. However, the IP
model with the significance score delivers a sig-
nificant increase in performance over the language
model and the decision tree (? < 0.05).
These results indicate that reasonable compres-
sions can be obtained with very little supervision.
Our constraint-based language model does not
make use of a parallel corpus, whereas our second
variant uses only 50 parallel sentences for tuning
the weights of the objective function. The models
described in this paper could be easily adapted to
other domains or languages provided that syntac-
tic analysis tools are to some extent available.
6 Conclusions and Future Work
In this paper we have presented a novel method
for automatic sentence compression. A key aspect
of our approach is the use of integer program-
ming for inferring globally optimal compressions
in the presence of linguistically motivated con-
straints. We have shown that such a formulation
allows for a relatively simple and knowledge-lean
compression model that does not require parallel
corpora or access to large-scale knowledge bases.
Our results demonstrate that the IP model yields
performance comparable to state-of-the-art with-
out any supervision. We also observe significant
performance gains when a small amount of train-
ing data is employed (50 parallel sentences). Be-
yond the systems discussed in this paper, the ap-
proach holds promise for other models using de-
coding algorithms for searching the space of pos-
sible compressions. The search process could be
framed as an integer program in a similar fashion
to our work here.
We obtain our best results using a model whose
objective function includes a significance score.
The significance score relies mainly on syntactic
and lexical information for determining whether
a word is important or not. An appealing future
direction is the incorporation of discourse-based
constraints into our models. The latter would high-
light topical words at the document-level instead
of considering each sentence in isolation. An-
other important issue concerns the portability of
the models presented here to other languages and
domains. We plan to apply our method to lan-
guages with more flexible word order than English
(e.g., German) and more challenging spoken do-
mains (e.g., meeting data) where parsing technol-
ogy may be less reliable.
Acknowledgements
Thanks to Jean Carletta, Amit Dubey, Frank Keller, Steve
Renals, and Sebastian Riedel for helpful comments and sug-
gestions. Lapata acknowledges the support of EPSRC (grant
GR/T04540/01).
References
Briscoe, E. J. and J. Carroll. 2002. Robust accurate statisti-cal annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical lan-guage modeling using the CMU?cambridge toolkit. In
Proceedings of Eurospeech. Rhodes, Greece, pages 2707?2710.
Hori, Chiori and Sadaoki Furui. 2004. Speech summariza-tion: an approach through word extraction and a methodfor evaluation. IEICE Transactions on Information and
Systems E87-D(1):15?25.
Jing, Hongyan. 2000. Sentence reduction for automatic textsummarization. In Proceedings of the 6th ANLP. Seattle,WA, pages 310?315.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression. Artificial Intelligence 139(1):91?107.
Marciniak, Tomasz and Michael Strube. 2005. Beyond thepipeline: Discrete optimization in NLP. In Proceedings of
the 9th CoNLL. Ann Arbor, MI, pages 136?143.
McDonald, Ryan. 2006. Discriminative sentence compres-sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy, pages 297?304.
Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi,Tu Bao Ho, and Masaru Fukushi. 2004. Probabilistic sen-tence reduction using support vector machines. In Pro-
ceedings of the 20th COLING. Geneva, Switzerland, pages743?749.
Olivers, S. H. and W. B. Dolan. 1999. Less is more; eliminat-ing index terms from subordinate clauses. In Proceedings
of the 37th ACL. College Park, MD, pages 349?356.
Press, William H., Saul A. Teukolsky, William T. Vetterling,and Brian P. Flannery. 1992. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge University Press.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav Zimak.2004. Semantic role labeling via integer linear program-ming inference. In Proceedings of the 20th COLING.Geneva, Switzerland, pages 1346?1352.
Riezler, Stefan, Tracy H. King, Richard Crouch, and AnnieZaenen. 2003. Statistical sentence condensation usingambiguity packing and stochastic disambiguation meth-ods for lexical-functional grammar. In Proceedings of
the HLT/NAACL. Edmonton, Canada, pages 118?125.
Roth, Dan and Wen-tau Yih. 2004. A linear programmingformulation for global inference in natural language tasks.In Proceedings of the 8th CoNLL. Boston, MA, pages 1?8.
Turner, Jenine and Eugene Charniak. 2005. Supervised andunsupervised learning for sentence compression. In Pro-
ceedings of the 43rd ACL. Ann Arbor, MI, pages 290?297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence compres-sion for automated subtitling: A hybrid approach. In Pro-
ceedings of the ACL Workshop on Text Summarization.Barcelona, Spain, pages 89?95.
Winston, Wayne L. and Munirpallam Venkataramanan.2003. Introduction to Mathematical Programming.Brooks/Cole.
151
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 728?735,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Machine Translation by Triangulation:
Making Effective Use of Multi-Parallel Corpora
Trevor Cohn and Mirella Lapata
Human Computer Research Centre, School of Informatics
University of Edinburgh
{tcohn,mlap}@inf.ed.ac.uk
Abstract
Current phrase-based SMT systems perform
poorly when using small training sets. This
is a consequence of unreliable translation es-
timates and low coverage over source and
target phrases. This paper presents a method
which alleviates this problem by exploit-
ing multiple translations of the same source
phrase. Central to our approach is triangula-
tion, the process of translating from a source
to a target language via an intermediate third
language. This allows the use of a much
wider range of parallel corpora for train-
ing, and can be combined with a standard
phrase-table using conventional smoothing
methods. Experimental results demonstrate
BLEU improvements for triangulated mod-
els over a standard phrase-based system.
1 Introduction
Statistical machine translation (Brown et al, 1993)
has seen many improvements in recent years, most
notably the transition from word- to phrase-based
models (Koehn et al, 2003). Modern SMT sys-
tems are capable of producing high quality transla-
tions when provided with large quantities of training
data. With only a small training sample, the trans-
lation output is often inferior to the output from us-
ing larger corpora because the translation algorithm
must rely on more sparse estimates of phrase fre-
quencies and must also ?back-off? to smaller sized
phrases. This often leads to poor choices of target
phrases and reduces the coherence of the output. Un-
fortunately, parallel corpora are not readily available
in large quantities, except for a small subset of the
world?s languages (see Resnik and Smith (2003) for
discussion), therefore limiting the potential use of
current SMT systems.
In this paper we provide a means for obtaining
more reliable translation frequency estimates from
small datasets. We make use of multi-parallel cor-
pora (sentence aligned parallel texts over three or
more languages). Such corpora are often created
by international organisations, the United Nations
(UN) being a prime example. They present a chal-
lenge for current SMT systems due to their rela-
tively moderate size and domain variability (exam-
ples of UN texts include policy documents, proceed-
ings of meetings, letters, etc.). Our method translates
each target phrase, t, first to an intermediate lan-
guage, i, and then into the source language, s. We
call this two-stage translation process triangulation
(Kay, 1997). We present a probabilistic formulation
through which we can estimate the desired phrase
translation distribution (phrase-table) by marginali-
sation, p(s|t) =
?
i p(s, i|t).
As with conventional smoothing methods (Koehn
et al, 2003; Foster et al, 2006), triangulation in-
creases the robustness of phrase translation esti-
mates. In contrast to smoothing, our method allevi-
ates data sparseness by exploring additional multi-
parallel data rather than adjusting the probabilities of
existing data. Importantly, triangulation provides us
with separately estimated phrase-tables which could
be further smoothed to provide more reliable dis-
tributions. Moreover, the triangulated phrase-tables
can be easily combined with the standard source-
target phrase-table, thereby improving the coverage
over unseen source phrases.
As an example, consider Figure 1 which shows
the coverage of unigrams and larger n-gram phrases
when using a standard source target phrase-table, a
triangulated phrase-table with one (it) and nine lan-
guages (all), and a combination of standard and tri-
angulated phrase-tables (all+standard). The phrases
were harvested from a small French-English bitext
728
and evaluated against a test set. Although very few
small phrases are unknown, the majority of larger
phrases are unseen. The Italian and all results show
that triangulation alone can provide similar or im-
proved coverage compared to the standard source-
target model; further improvement is achieved by
combining the triangulated and standard models
(all+standard). These models and datasets will be
described in detail in Section 3.
We also demonstrate that triangulation can be
used on its own, that is without a source-target dis-
tribution, and still yield acceptable translation out-
put. This is particularly heartening, as it provides a
means of translating between the many ?low den-
sity? language pairs for which we don?t yet have a
source-target bitext. This allows SMT to be applied
to a much larger set of language pairs than was pre-
viously possible.
In the following section we provide an overview
of related work. Section 3 introduces a generative
formulation of triangulation. We present our evalua-
tion framework in Section 4 and results in Section 5.
2 Related Work
The idea of using multiple source languages for
improving the translation quality of the target lan-
guage dates back at least to Kay (1997), who ob-
served that ambiguities in translating from one lan-
guage onto another may be resolved if a transla-
tion into some third language is available. Systems
which have used this notion of triangulation typi-
cally create several candidate sentential target trans-
lations for source sentences via different languages.
A single translation is then selected by finding the
candidate that yields the best overall score (Och and
Ney, 2001; Utiyama and Isahara, 2007) or by co-
training (Callison-Burch and Osborne, 2003). This
ties in with recent work on ensemble combinations
of SMT systems, which have used alignment tech-
niques (Matusov et al, 2006) or simple heuristics
(Eisele, 2005) to guide target sentence selection and
generation. Beyond SMT, the use of an intermediate
language as a translation aid has also found appli-
cation in cross-lingual information retrieval (Gollins
and Sanderson, 2001).
Callison-Burch et al (2006) propose the use of
paraphrases as a means of dealing with unseen
source phrases. Their method acquires paraphrases
by identifying candidate phrases in the source lan-
1 2 3 4 5 6
phrase length
propo
rtion 
of tes
t even
ts in p
hrase
 table
0.005
0.01
0.02
0.05
0.1
0.2
0.5
1 standardItalianallall + standard
Figure 1: Coverage of fr ? en test phrases using a 10,000 sen-
tence bitext. The standard model is shown alongside triangu-
lated models using one (Italian) or nine other languages (all).
guage, translating them into multiple target lan-
guages, and then back to the source. Unknown
source phrases are substituted by the back-translated
paraphrases and translation proceeds on the para-
phrases.
In line with previous work, we exploit multi-
ple source corpora to alleviate data sparseness and
increase translation coverage. However, we differ
in several important respects. Our method oper-
ates over phrases rather than sentences. We propose
a generative formulation which treats triangulation
not as a post-processing step but as part of the trans-
lation model itself. The induced phrase-table entries
are fed directly into the decoder, thus avoiding the
additional inefficiencies of merging the output of
several translation systems.
Although related to Callison-Burch et al (2006)
our method is conceptually simpler and more gen-
eral. Phrase-table entries are created via multiple
source languages without the intermediate step of
paraphrase extraction, thereby reducing the expo-
sure to compounding errors. Our phrase-tables may
well contain paraphrases but these are naturally in-
duced as part of our model, without extra processing
effort. Furthermore, we improve the translation esti-
mates for both seen and unseen phrase-table entries,
whereas Callison-Burch et al concentrate solely on
unknown phrases. In contrast to Utiyama and Isa-
hara (2007), we employ a large number of inter-
mediate languages and demonstrate how triangu-
lated phrase-tables can be combined with standard
phrase-tables to improve translation output.
729
en varm kartoffeleen hete aardappel uma batata quente
une patate une patate chaudd?licate une question d?licate
a hot potato
source
intermediate
target
Figure 2: Triangulation between English (source) and French (target), showing three phrases in Dutch, Danish and Portuguese,
respectively. Arrows denote phrases aligned in a language pair and also the generative translation process.
3 Triangulation
We start with a motivating example before formalis-
ing the mechanics of triangulation. Consider trans-
lating the English phrase a hot potato1 into French,
as shown in Figure 2. In our corpus this English
phrase occurs only three times. Due to errors in
the word alignment the phrase was not included in
the English-French phrase-table. Triangulation first
translates hot potato into a set of intermediate lan-
guages (Dutch, Danish and Portuguese are shown in
the figure), and then these phrases are further trans-
lated into the target language (French). In the ex-
ample, four different target phrases are obtained, all
of which are useful phrase-table entries. We argue
that the redundancy introduced by a large suite of
other languages can correct for errors in the word
alignments and also provide greater generalisation,
since the translation distribution is estimated from a
richer set of data-points. For example, instances of
the Danish en varm kartoffel may be used to trans-
late several English phrases, not only a hot potato.
In general we expect that a wider range of pos-
sible translations are found for any source phrase,
simply due to the extra layer of indirection. So, if a
source phrase tends to align with two different tar-
get phrases, then we would also expect it to align
with two phrases in the ?intermediate? language.
These intermediate phrases should then each align
with two target phrases, yielding up to four target
phrases. Consequently, triangulation will often pro-
duce more varied translation distributions than the
standard source-target approach.
3.1 Formalisation
We now formalise triangulation as a generative
probabilistic process operating independently on
phrase pairs. We start with the conditional distri-
bution over three languages, p(s, i|t), where the ar-
guments denote phrases in the source, intermediate
1An idiom meaning a situation for which no one wants to
claim responsibility.
and target language, respectively. From this distri-
bution, we can find the desired conditional over the
source-target pair by marginalising out the interme-
diate phrases:2
p(s|t) =
?
i
p(s|i, t)p(i|t)
?
?
i
p(s|i)p(i|t) (1)
where (1) imposes a simplifying conditional inde-
pendence assumption: the intermediate phrase fully
represents the information (semantics, syntax, etc.)
in the source phrase, rendering the target phrase re-
dundant in p(s|i, t).
Equation (1) requires that all phrases in the
intermediate-target bitext must also be found in the
source-intermediate bitext, such that p(s|i) is de-
fined. Clearly this will often not be the case. In these
situations we could back-off to another distribution
(by discarding part, or all, of the conditioning con-
text), however we take a more pragmatic approach
and ignore the missing phrases. This problem of
missing contexts is uncommon in multi-parallel cor-
pora, but is more common when the two bitexts are
drawn from different sources.
While triangulation is intuitively appealing, it
may suffer from a few problems. Firstly, as with any
SMT approach, the translation estimates are based
on noisy automatic word alignments. This leads to
many errors and omissions in the phrase-table. With
a standard source-target phrase-table these errors are
only encountered once, however with triangulation
they are encountered twice, and therefore the errors
will compound. This leads to more noisy estimates
than in the source-target phrase-table.
Secondly, the increased exposure to noise means
that triangulation will omit a greater proportion of
large or rare phrases than the standard method. An
2Equation (1) is used with the source and target arguments
reversed to give p(t|s).
730
alignment error in either of the source-intermediate
or intermediate-target bitexts can prevent the extrac-
tion of a source-target phrase pair. This effect can be
seen in Figure 1, where the coverage of the Italian
triangulated phrase-table is worse than the standard
source-target model, despite the two models using
the same sized bitexts. As we explain in the next
section, these problems can be ameliorated by us-
ing the triangulated phrase-table in conjunction with
a standard phrase-table.
Finally, another potential problem stems from the
independence assumption in (1), which may be an
oversimplification and lead to a loss of information.
The experiments in Section 5 show that this effect is
only mild.
3.2 Merging the phrase-tables
Once induced, the triangulated phrase-table can be
usefully combined with the standard source-target
phrase-table. The simplest approach is to use linear
interpolation to combine the two (or more) distribu-
tions, as follows:
p(s, t) =
?
j
?jpj(s, t) (2)
where each joint distribution, pj , has a non-negative
weight, ?j , and the weights sum to one. The joint
distribution for triangulated phrase-tables is defined
in an analogous way to Equation (1). We expect
that the standard phrase-table should be allocated
a higher weight than triangulated phrase-tables, as
it will be less noisy. The joint distribution is now
conditionalised to yield p(s|t) and p(t|s), which are
both used as features in the decoder. Note that the re-
sulting conditional distribution will be drawn solely
from one input distribution when the conditioning
context is unseen in the remaining distributions. This
may lead to an over-reliance on unreliable distribu-
tions, which can be ameliorated by smoothing (e.g.,
Foster et al (2006)).
As an alternative to linear interpolation, we also
employ a weighted product for phrase-table combi-
nation:
p(s|t) ?
?
j
pj(s|t)?j (3)
This has the same form used for log-linear training
of SMT decoders (Och, 2003), which allows us to
treat each distribution as a feature, and learn the mix-
ing weights automatically. Note that we must indi-
vidually smooth the component distributions in (3)
to stop zeros from propagating. For this we use
Simple Good-Turing smoothing (Gale and Samp-
son, 1995) for each distribution, which provides es-
timates for zero count events.
4 Experimental Design
Corpora We used the Europarl corpus (Koehn,
2005) for experimentation. This corpus consists of
about 700,000 sentences of parliamentary proceed-
ings from the European Union in eleven European
languages. We present results on the full corpus for a
range of language pairs. In addition, we have created
smaller parallel corpora by sub-sampling 10,000
sentence bitexts for each language pair. These cor-
pora are likely to have minimal overlap ? about
1.5% of the sentences will be shared between each
pair. However, the phrasal overlap is much greater
(10 to 20%), which allows for triangulation using
these common phrases. This training setting was
chosen to simulate translating to or from a ?low
density? language, where only a few small indepen-
dently sourced parallel corpora are available. These
bitexts were used for direct translation and triangula-
tion. All experimental results were evaluated on the
ACL/WMT 20053 set of 2,000 sentences, and are
reported in BLEU percentage-points.
Decoding Pharaoh (Koehn, 2003), a beam-
search decoder, was used to maximise:
T? = argmax
T
?
j
fj(T,S)?j (4)
where T and S denote a target and source sentence
respectively. The parameters, ?j , were trained using
minimum error rate training (Och, 2003) to max-
imise the BLEU score (Papineni et al, 2002) on
a 150 sentence development set. We used a stan-
dard set of features, comprising a 4-gram language
model, distance based distortion model, forward
and backward translation probabilities, forward and
backward lexical translation scores and the phrase-
and word-counts. The translation models and lex-
ical scores were estimated on the training corpus
which was automatically aligned using Giza++ (Och
et al, 1999) in both directions between source and
target and symmetrised using the growing heuristic
(Koehn et al, 2003).
3For details see http://www.statmt.org/wpt05/
mt-shared-task.
731
Lexical weights The lexical translation score is
used for smoothing the phrase-table translation esti-
mate. This represents the translation probability of a
phrase when it is decomposed into a series of inde-
pendent word-for-word translation steps (Koehn et
al., 2003), and has proven a very effective feature
(Zens and Ney, 2004; Foster et al, 2006). Pharaoh?s
lexical weights require access to word-alignments;
calculating these alignments between the source and
target words in a phrase would prove difficult for
a triangulated model. Therefore we use a modified
lexical score, corresponding to the maximum IBM
model 1 score for the phrase pair:
lex(t|s) =
1
Z
max
a
?
k
p(tk|sak) (5)
where the maximisation4 ranges over all one-to-
many alignments and Z normalises the score by the
number of possible alignments.
The lexical probability is obtained by interpo-
lating a relative frequency estimate on the source-
target bitext with estimates from triangulation, in
the same manner used for phrase translations in (1)
and (2). The addition of the lexical probability fea-
ture yielded a substantial gain of up to two BLEU
points over a basic feature set.
5 Experimental Results
The evaluation of our method was motivated by
three questions: (1) How do different training re-
quirements affect the performance of the triangu-
lated models presented in this paper? We expect
performance gains with triangulation on small and
moderate datasets. (2) Is machine translation out-
put influenced by the choice of the intermediate lan-
guage/s? Here, we would like to evaluate whether
the number and choice of intermediate languages
matters. (3) What is the quality of the triangulated
phrase-table? In particular, we are interested in the
resulting distribution and whether it is sufficiently
distinct from the standard phrase-table.
5.1 Training requirements
Before reporting our results, we briefly discuss the
specific choice of model for our experiments. As
mentioned in Section 3, our method combines the
4The maximisation in (5) can be replaced with a sum with
similar experimental results.
standard interp +indic separate
en ? de 12.03 12.66 12.95 12.25
fr ? en 23.02 24.63 23.86 23.43
Table 1: Different feature sets used with the 10K training
corpora, using a single language (es) for triangulation. The
columns refer to standard, uniform interpolation, interpolation
with 0-1 indicator features, and separate phrase-tables, respec-
tively.
triangulated phrase-table with the standard source-
target one. This is desired in order to compensate for
the noise incurred by the triangulation process. We
used two combination methods, namely linear inter-
polation (see (2)) and a weighted geometric mean
(see (3)).
Table 1 reports the results for two translation tasks
when triangulating with a single language (es) us-
ing three different feature sets, each with different
translation features. The interpolation model uses
uniform linear interpolation to merge the standard
and triangulated phrase-tables. Non-uniform mix-
tures did not provide consistent gains, although,
as expected, biasing towards the standard phrase-
table was more effective than against. The indicator
model uses the same interpolated distribution along
with a series of 0-1 indicator features to identify the
source of each event, i.e., if each (s, t) pair is present
in phrase-table j. We also tried per-context features
with similar results. The separate model has a sepa-
rate feature for each phrase-table.
All three feature sets improve over the standard
source-target system, while the interpolated features
provided the best overall performance. The rela-
tively poorer performance of the separate model
is perhaps surprising, as it is able to differentially
weight the component distributions; this is probably
due to MERT not properly handling the larger fea-
ture sets. In all subsequent experiments we report
results using linear interpolation.
As a proof of concept, we first assessed the ef-
fect of triangulation on corpora consisting of 10,000
sentence bitexts. We expect triangulation to de-
liver performance gains on small corpora, since a
large number of phrase-table entries will be un-
seen. In Table 2 each entry shows the BLEU score
when using the standard phrase-table and the ab-
solute improvement when using triangulation. Here
we have used three languages for triangulation
(it ? {de, en, es, fr}\{s, t}). The source-target lan-
guages were chosen so as to mirror the evaluation
setup of NAACL/WMT. The translation tasks range
732
s ? t ? de en es fr
de - 17.58 16.84 18.06
- +1.20 +1.99 +1.94
en 12.45 - 23.83 24.05
+1.22 - +1.04 +1.48
es 12.31 23.83 - 32.69
+2.24 +1.35 - +0.85
fr 11.76 23.02 31.22 -
+2.41 +2.24 +1.30 -
Table 2: BLEU improvements over the standard phrase-table
(top) when interpolating with three triangulated phrase-tables
(bottom) on the small training sample.
from easy (es ? fr) to very hard (de ? en). In all
cases triangulation resulted in an improvement in
translation quality, with the highest gains observed
for the most difficult tasks (to and from German).
For these tasks the standard systems have poor cov-
erage (due in part to the sizeable vocabulary of Ger-
man phrases) and therefore the gain can be largely
explained by the additional coverage afforded by the
triangulated phrase-tables.
To test whether triangulation can also improve
performance of larger corpora we ran six separate
translation tasks on the full Europarl corpus. The
results are presented in Table 3, for a single trian-
gulation language used alone (triang) or uniformly
interpolated with the standard phrase-table (interp).
These results show that triangulation can produce
high quality translations on its own, which is note-
worthy, as it allows for SMT between a much larger
set of language pairs. Using triangulation in con-
junction with the standard phrase-table improved
over the standard system in most instances, and
only degraded performance once. The improvement
is largest for the German tasks which can be ex-
plained by triangulation providing better robustness
to noisy alignments (which are often quite poor for
German) and better estimates of low-count events.
The difficulty of aligning German with the other lan-
guages is apparent from the Giza++ perplexity: the
final Model 4 perplexities for German are quite high,
as much as double the perplexity for more easily
aligned language pairs (e.g., Spanish-French).
Figure 3 shows the effect of triangulation on dif-
ferent sized corpora for the language pair fr ? en.
It presents learning curves for the standard system
and a triangulated system using one language (es).
As can be seen, gains from triangulation only di-
minish slightly for larger training corpora, and that
task standard interm triang interp
de ? en 23.85 es 23.48 24.36
en ? de 17.24 es 16.28 17.42
es ? en 30.48 fr 29.06 30.52
en ? es 29.09 fr 28.19 29.09
fr ? en 29.66 es 29.59 30.36
en ? fr 30.07 es 28.94 29.62
Table 3: Results on the full training set showing triangulation
with a single language, both alone (triang) and alongside a stan-
dard model (interp).
l
l
l
l
size of training bitext(s)
BLE
U sc
ore
10K 40K 160K 700K
22
24
26
28
30
l standardtrianginterp
Figure 3: Learning curve for fr ? en translation for the standard
source-target model and a triangulated model using Spanish as
an intermediate language.
the purely triangulated models have very competi-
tive performance. The gain from interpolation with
a triangulated model is roughly equivalent to having
twice as much training data.
Finally, notice that triangulation may benefit
when the sentences in each bitext are drawn from the
same source, in that there are no unseen ?intermedi-
ate? phrases, and therefore (1) can be easily evalu-
ated. We investigate this by examining the robust-
ness of our method in the face of disjoint bitexts.
The concepts contained in each bitext will be more
varied, potentially leading to better coverage of the
target language. In lieu of a study on different do-
main bitexts which we plan for the future, we bi-
sected the Europarl corpus for fr ? en, triangulat-
ing with Spanish. The triangulated models were pre-
sented with fr-es and es-en bitexts drawn from either
the same half of the corpus or from different halves,
resulting in scores of 28.37 and 28.13, respectively.5
These results indicate that triangulation is effective
5The baseline source-target system on one half has a score
of 28.85.
733
triang interp
BLE
U sc
ore
19
20
21
22
23
24
25
fi (?14.26)
da
da
de
de
el
el
es
es
fi
it
it
nl
nl
pt
pt
sv
sv
Figure 4: Comparison of different triangulation languages for
fr ? en translation, relative to the standard model (10K training
sample). The bar for fi has been truncated to fit on the graph.
for disjoint bitexts, although ideally we would test
this with independently sourced parallel texts.
5.2 The choice of intermediate languages
The previous experiments used an ad-hoc choice
of ?intermediate? language/s for triangulation, and
we now examine which languages are most effec-
tive. Figure 4 shows the efficacy of the remaining
nine languages when translating fr ? en. Minimum
error-rate training was not used for this experiment,
or the next shown in Figure 5, in order to highlight
the effect of the changing translation estimates. Ro-
mance languages (es, it, pt) give the best results,
both on their own and when used together with the
standard phrase-table (using uniform interpolation);
Germanic languages (de, nl, da, sv) are a distant sec-
ond, with the less related Greek and Finnish the least
useful. Interpolation yields an improvement for all
?intermediate? languages, even Finnish, which has a
very low score when used alone.
The same experiment was repeated for en ? de
translation with similar trends, except that the
Germanic languages out-scored the Romance lan-
guages. These findings suggest that ?intermediate?
languages which exhibit a high degree of similarity
with the source or target language are desirable. We
conjecture that this is a consequence of better auto-
matic word alignments and a generally easier trans-
lation task, as well as a better preservation of infor-
mation between aligned phrases.
Using a single language for triangulation clearly
improves performance, but can we realise further
improvements by using additional languages? Fig-
1 2 3 4 5 6 7 8 9
# intermediate languages
BLEU
 scor
e
22
23
24
25
26 trianginterp
Figure 5: Increasing the number of intermediate languages used
for triangulation increases performance for fr ? en (10K train-
ing sample). The dashed line shows the BLEU score for the
standard phrase-table.
ure 5 shows the performance profile for fr ? en
when adding languages in a fixed order. The lan-
guages were ordered by family, with Romance be-
fore Germanic before Greek and Finnish. Each ad-
dition results in an increase in performance, even for
the final languages, from which we expect little in-
formation. The purely triangulated (triang) and in-
terpolated scores (interp) are converging, suggesting
that the source-target bitext is redundant given suf-
ficient triangulated data. We obtained similar results
for en ? de.
5.3 Evaluating the quality of the phrase-table
Our experimental results so far have shown that
triangulation is not a mere approximation of the
source-target phrase-table, but that it extracts addi-
tional useful translation information. We now as-
sess the phrase-table quality more directly. Com-
parative statistics of a standard and a triangulated
phrase-table are given in Table 4. The coverage over
source and target phrases is much higher in the stan-
dard table than the triangulated tables, which reflects
the reduced ability of triangulation to extract large
phrases ? despite the large increase in the num-
ber of events. The table also shows the overlapping
probability mass which measures the sum of prob-
ability in one table for which the events are present
in the other. This shows that the majority of mass
is shared by both tables (as joint distributions), al-
though there are significant differences. The Jensen-
Shannon divergence is perhaps more appropriate for
the comparison, giving a relatively high divergence
734
standard triang
source phrases (M) 8 2.5
target phrases (M) 7 2.5
events (M) 12 70
overlapping mass 0.646 0.750
Table 4: Comparative statistics of the standard triangulated table
on fr ? en using the full training set and Spanish as an inter-
mediate language.
of 0.3937. This augurs well for the combination of
standard and triangulated phrase-tables, where di-
versity is valued. The decoding results (shown in
Table 3 for fr ? en) indicate that the two meth-
ods have similar efficacy, and that their interpolated
combination provides the best overall performance.
6 Conclusion
In this paper we have presented a novel method for
obtaining more reliable translation estimates from
small datasets. The key premise of our work is that
multi-parallel data can be usefully exploited for im-
proving the coverage and quality of phrase-based
SMT. Our triangulation method translates from a
source to a target via one or many intermediate lan-
guages. We present a generative formulation of this
process and show how it can be used together with
the entries of a standard source-target phrase-table.
We observe large performance gains when trans-
lating with triangulated models trained on small
datasets. Furthermore, when combined with a stan-
dard phrase-table, our models also yield perfor-
mance improvements on larger datasets. Our exper-
iments revealed that triangulation benefits from a
large set of intermediate languages and that perfor-
mance is increased when languages of the same fam-
ily to the source or target are used as intermediates.
We have just scratched the surface of the possi-
bilities for the framework discussed here. Important
future directions lie in combining triangulation with
richer means of conventional smoothing and using
triangulation to translate between low-density lan-
guage pairs.
Acknowledgements The authors acknowledge
the support of EPSRC (grants GR/T04540/01 and
GR/T04557/01). Special thanks to Markus Becker, Chris
Callison-Burch, David Talbot and Miles Osborne for their
helpful comments.
References
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, R. L. Mercer. 1993.
The mathematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics, 19(2):263?311.
C. Callison-Burch, M. Osborne. 2003. Bootstrapping parallel
corpora. In Proceedings of the NAACL Workshop on Build-
ing and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, Edmonton, Canada.
C. Callison-Burch, P. Koehn, M. Osborne. 2006. Improved sta-
tistical machine translation using paraphrases. In Proceed-
ings of the HLT/NAACL, 17?24, New York, NY.
A. Eisele. 2005. First steps towards multi-engine machine
translation. In Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts, 155?158, Ann Arbor, MI.
G. Foster, R. Kuhn, H. Johnson. 2006. Phrase-table smooth-
ing for statistical machine translation. In Proceedings of the
EMNLP, 53?61, Sydney, Australia.
W. A. Gale, G. Sampson. 1995. Good-turing frequency esti-
mation without tears. Journal of Quantitative Linguistics,
2(3):217?237.
T. Gollins, M. Sanderson. 2001. Improving cross language
retrieval with triangulated translation. In Proceedings of the
SIGIR, 90?95, New Orleans, LA.
M. Kay. 1997. The proper place of men and machines in lan-
guage translation. Machine Translation, 12(1?2):3?23.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of the HLT/NAACL, 48?
54, Edomonton, Canada.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, Uni-
versity of Southern California, Los Angeles, California.
P. Koehn. 2005. Europarl: A parallel corpus for evaluation of
machine translation. In Proceedings of MT Summit, Phuket,
Thailand.
E. Matusov, N. Ueffing, H. Ney. 2006. Computing consesus
translation from multiple machine translation systems us-
ing enhanced hypotheses alignment. In Proceedings of the
EACL, 33?40, Trento, Italy.
F. J. Och, H. Ney. 2001. Statistical multi-source translation. In
Proceedings of the MT Summit, 253?258, Santiago de Com-
postela, Spain.
F. J. Och, C. Tillmann, H. Ney. 1999. Improved alignment
models for statistical machine translation. In Proceedings of
the EMNLP and VLC, 20?28, University of Maryland, Col-
lege Park, MD.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of the ACL, 160?167, Sap-
poro, Japan.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. BLEU: A
method for automatic evaluation of machine translation. In
Proceedings of the ACL, 311?318, Philadelphia, PA.
P. Resnik, N. A. Smith. 2003. The Web as a parallel corpus.
Computational Linguistics, 29(3):349?380.
M. Utiyama, H. Isahara. 2007. A comparison of pivot methods
for phrase-based statistical machine translation. In Proceed-
ings of the HLT/NAACL, 484?491, Rochester, NY.
R. Zens, H. Ney. 2004. Improvements in phrase-based statisti-
cal machine translation. In D. M. Susan Dumais, S. Roukos,
eds., Proceedings of the HLT/NAACL, 257?264, Boston,
MA.
735
Proceedings of ACL-08: HLT, pages 236?244,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Vector-based Models of Semantic Composition
Jeff Mitchell and Mirella Lapata
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
jeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
This paper proposes a framework for repre-
senting the meaning of phrases and sentences
in vector space. Central to our approach is
vector composition which we operationalize
in terms of additive and multiplicative func-
tions. Under this framework, we introduce a
wide range of composition models which we
evaluate empirically on a sentence similarity
task. Experimental results demonstrate that
the multiplicative models are superior to the
additive alternatives when compared against
human judgments.
1 Introduction
Vector-based models of word meaning (Lund and
Burgess, 1996; Landauer and Dumais, 1997) have
become increasingly popular in natural language
processing (NLP) and cognitive science. The ap-
peal of these models lies in their ability to rep-
resent meaning simply by using distributional in-
formation under the assumption that words occur-
ring within similar contexts are semantically similar
(Harris, 1968).
A variety of NLP tasks have made good use
of vector-based models. Examples include au-
tomatic thesaurus extraction (Grefenstette, 1994),
word sense discrimination (Schu?tze, 1998) and dis-
ambiguation (McCarthy et al, 2004), collocation ex-
traction (Schone and Jurafsky, 2001), text segmen-
tation (Choi et al, 2001) , and notably information
retrieval (Salton et al, 1975). In cognitive science
vector-based models have been successful in simu-
lating semantic priming (Lund and Burgess, 1996;
Landauer and Dumais, 1997) and text comprehen-
sion (Landauer and Dumais, 1997; Foltz et al,
1998). Moreover, the vector similarities within such
semantic spaces have been shown to substantially
correlate with human similarity judgments (McDon-
ald, 2000) and word association norms (Denhire and
Lemaire, 2004).
Despite their widespread use, vector-based mod-
els are typically directed at representing words in
isolation and methods for constructing representa-
tions for phrases or sentences have received little
attention in the literature. In fact, the common-
est method for combining the vectors is to average
them. Vector averaging is unfortunately insensitive
to word order, and more generally syntactic struc-
ture, giving the same representation to any construc-
tions that happen to share the same vocabulary. This
is illustrated in the example below taken from Lan-
dauer et al (1997). Sentences (1-a) and (1-b) con-
tain exactly the same set of words but their meaning
is entirely different.
(1) a. It was not the sales manager who hit the
bottle that day, but the office worker with
the serious drinking problem.
b. That day the office manager, who was
drinking, hit the problem sales worker with
a bottle, but it was not serious.
While vector addition has been effective in some
applications such as essay grading (Landauer and
Dumais, 1997) and coherence assessment (Foltz
et al, 1998), there is ample empirical evidence
that syntactic relations across and within sentences
are crucial for sentence and discourse processing
(Neville et al, 1991; West and Stanovich, 1986)
and modulate cognitive behavior in sentence prim-
ing (Till et al, 1988) and inference tasks (Heit and
236
Rubinstein, 1994).
Computational models of semantics which use
symbolic logic representations (Montague, 1974)
can account naturally for the meaning of phrases or
sentences. Central in these models is the notion of
compositionality ? the meaning of complex expres-
sions is determined by the meanings of their con-
stituent expressions and the rules used to combine
them. Here, semantic analysis is guided by syntactic
structure, and therefore sentences (1-a) and (1-b) re-
ceive distinct representations. The downside of this
approach is that differences in meaning are qualita-
tive rather than quantitative, and degrees of similar-
ity cannot be expressed easily.
In this paper we examine models of semantic
composition that are empirically grounded and can
represent similarity relations. We present a gen-
eral framework for vector-based composition which
allows us to consider different classes of models.
Specifically, we present both additive and multi-
plicative models of vector combination and assess
their performance on a sentence similarity rating ex-
periment. Our results show that the multiplicative
models are superior and correlate significantly with
behavioral data.
2 Related Work
The problem of vector composition has received
some attention in the connectionist literature, partic-
ularly in response to criticisms of the ability of con-
nectionist representations to handle complex struc-
tures (Fodor and Pylyshyn, 1988). While neural net-
works can readily represent single distinct objects,
in the case of multiple objects there are fundamen-
tal difficulties in keeping track of which features are
bound to which objects. For the hierarchical struc-
ture of natural language this binding problem be-
comes particularly acute. For example, simplistic
approaches to handling sentences such as John loves
Mary and Mary loves John typically fail to make
valid representations in one of two ways. Either
there is a failure to distinguish between these two
structures, because the network fails to keep track
of the fact that John is subject in one and object
in the other, or there is a failure to recognize that
both structures involve the same participants, be-
cause John as a subject has a distinct representation
from John as an object. In contrast, symbolic repre-
sentations can naturally handle the binding of con-
stituents to their roles, in a systematic manner that
avoids both these problems.
Smolensky (1990) proposed the use of tensor
products as a means of binding one vector to an-
other. The tensor product u? v is a matrix whose
components are all the possible products uiv j of the
components of vectors u and v. A major difficulty
with tensor products is their dimensionality which is
higher than the dimensionality of the original vec-
tors (precisely, the tensor product has dimensional-
ity m? n). To overcome this problem, other tech-
niques have been proposed in which the binding of
two vectors results in a vector which has the same
dimensionality as its components. Holographic re-
duced representations (Plate, 1991) are one imple-
mentation of this idea where the tensor product is
projected back onto the space of its components.
The projection is defined in terms of circular con-
volution a mathematical function that compresses
the tensor product of two vectors. The compression
is achieved by summing along the transdiagonal el-
ements of the tensor product. Noisy versions of the
original vectors can be recovered by means of cir-
cular correlation which is the approximate inverse
of circular convolution. The success of circular cor-
relation crucially depends on the components of the
n-dimensional vectors u and v being randomly dis-
tributed with mean 0 and variance 1
n
. This poses
problems for modeling linguistic data which is typi-
cally represented by vectors with non-random struc-
ture.
Vector addition is by far the most common
method for representing the meaning of linguistic
sequences. For example, assuming that individual
words are represented by vectors, we can compute
the meaning of a sentence by taking their mean
(Foltz et al, 1998; Landauer and Dumais, 1997).
Vector addition does not increase the dimensional-
ity of the resulting vector. However, since it is order
independent, it cannot capture meaning differences
that are modulated by differences in syntactic struc-
ture. Kintsch (2001) proposes a variation on the vec-
tor addition theme in an attempt to model how the
meaning of a predicate (e.g., run) varies depending
on the arguments it operates upon (e.g, the horse ran
vs. the color ran). The idea is to add not only the
vectors representing the predicate and its argument
but also the neighbors associated with both of them.
The neighbors, Kintsch argues, can ?strengthen fea-
tures of the predicate that are appropriate for the ar-
gument of the predication?.
237
animal stable village gallop jokey
horse 0 6 2 10 4
run 1 8 4 4 0
Figure 1: A hypothetical semantic space for horse and
run
Unfortunately, comparisons across vector compo-
sition models have been few and far between in the
literature. The merits of different approaches are il-
lustrated with a few hand picked examples and pa-
rameter values and large scale evaluations are uni-
formly absent (see Frank et al (2007) for a criticism
of Kintsch?s (2001) evaluation standards). Our work
proposes a framework for vector composition which
allows the derivation of different types of models
and licenses two fundamental composition opera-
tions, multiplication and addition (and their combi-
nation). Under this framework, we introduce novel
composition models which we compare empirically
against previous work using a rigorous evaluation
methodology.
3 Composition Models
We formulate semantic composition as a function
of two vectors, u and v. We assume that indi-
vidual words are represented by vectors acquired
from a corpus following any of the parametrisa-
tions that have been suggested in the literature.1 We
briefly note here that a word?s vector typically rep-
resents its co-occurrence with neighboring words.
The construction of the semantic space depends on
the definition of linguistic context (e.g., neighbour-
ing words can be documents or collocations), the
number of components used (e.g., the k most fre-
quent words in a corpus), and their values (e.g., as
raw co-occurrence frequencies or ratios of probabil-
ities). A hypothetical semantic space is illustrated in
Figure 1. Here, the space has only five dimensions,
and the matrix cells denote the co-occurrence of the
target words (horse and run) with the context words
animal, stable, and so on.
Let p denote the composition of two vectors u
and v, representing a pair of constituents which
stand in some syntactic relation R. Let K stand for
any additional knowledge or information which is
needed to construct the semantics of their composi-
1A detailed treatment of existing semantic space models is
outside the scope of the present paper. We refer the interested
reader to Pado? and Lapata (2007) for a comprehensive overview.
tion. We define a general class of models for this
process of composition as:
p = f (u,v,R,K) (1)
The expression above allows us to derive models for
which p is constructed in a distinct space from u
and v, as is the case for tensor products. It also
allows us to derive models in which composition
makes use of background knowledge K and mod-
els in which composition has a dependence, via the
argument R, on syntax.
To derive specific models from this general frame-
work requires the identification of appropriate con-
straints to narrow the space of functions being con-
sidered. One particularly useful constraint is to
hold R fixed by focusing on a single well defined
linguistic structure, for example the verb-subject re-
lation. Another simplification concerns K which can
be ignored so as to explore what can be achieved in
the absence of additional knowledge. This reduces
the class of models to:
p = f (u,v) (2)
However, this still leaves the particular form of the
function f unspecified. Now, if we assume that p
lies in the same space as u and v, avoiding the issues
of dimensionality associated with tensor products,
and that f is a linear function, for simplicity, of the
cartesian product of u and v, then we generate a class
of additive models:
p = Au+Bv (3)
where A and B are matrices which determine the
contributions made by u and v to the product p. In
contrast, if we assume that f is a linear function of
the tensor product of u and v, then we obtain multi-
plicative models:
p = Cuv (4)
where C is a tensor of rank 3, which projects the
tensor product of u and v onto the space of p.
Further constraints can be introduced to reduce
the free parameters in these models. So, if we as-
sume that only the ith components of u and v con-
tribute to the ith component of p, that these com-
ponents are not dependent on i, and that the func-
tion is symmetric with regard to the interchange of u
238
and v, we obtain a simpler instantiation of an addi-
tive model:
pi = ui + vi (5)
Analogously, under the same assumptions, we ob-
tain the following simpler multiplicative model:
pi = ui ? vi (6)
For example, according to (5), the addition of the
two vectors representing horse and run in Fig-
ure 1 would yield horse+ run = [1 14 6 14 4].
Whereas their product, as given by (6), is
horse ? run = [0 48 8 40 0].
Although the composition model in (5) is com-
monly used in the literature, from a linguistic per-
spective, the model in (6) is more appealing. Sim-
ply adding the vectors u and v lumps their contents
together rather than allowing the content of one vec-
tor to pick out the relevant content of the other. In-
stead, it could be argued that the contribution of the
ith component of u should be scaled according to its
relevance to v, and vice versa. In effect, this is what
model (6) achieves.
As a result of the assumption of symmetry, both
these models are ?bag of words? models and word
order insensitive. Relaxing the assumption of sym-
metry in the case of the simple additive model pro-
duces a model which weighs the contribution of the
two components differently:
pi = ?ui +?vi (7)
This allows additive models to become more
syntax aware, since semantically important con-
stituents can participate more actively in the com-
position. As an example if we set ? to 0.4
and ? to 0.6, then horse = [0 2.4 0.8 4 1.6]
and run = [0.6 4.8 2.4 2.4 0], and their sum
horse+ run = [0.6 5.6 3.2 6.4 1.6].
An extreme form of this differential in the contri-
bution of constituents is where one of the vectors,
say u, contributes nothing at all to the combination:
pi = v j (8)
Admittedly the model in (8) is impoverished and
rather simplistic, however it can serve as a simple
baseline against which to compare more sophisti-
cated models.
The models considered so far assume that com-
ponents do not ?interfere? with each other, i.e., that
only the ith components of u and v contribute to the
ith component of p. Another class of models can be
derived by relaxing this constraint. To give a con-
crete example, circular convolution is an instance of
the general multiplicative model which breaks this
constraint by allowing u j to contribute to pi:
pi = ?
j
u j ? vi? j (9)
It is also possible to re-introduce the dependence
on K into the model of vector composition. For ad-
ditive models, a natural way to achieve this is to in-
clude further vectors into the summation. These vec-
tors are not arbitrary and ideally they must exhibit
some relation to the words of the construction under
consideration. When modeling predicate-argument
structures, Kintsch (2001) proposes including one or
more distributional neighbors, n, of the predicate:
p = u+v+?n (10)
Note that considerable latitude is allowed in select-
ing the appropriate neighbors. Kintsch (2001) con-
siders only the m most similar neighbors to the pred-
icate, from which he subsequently selects k, those
most similar to its argument. So, if in the composi-
tion of horse with run, the chosen neighbor is ride,
ride = [2 15 7 9 1], then this produces the repre-
sentation horse+ run+ ride = [3 29 13 23 5]. In
contrast to the simple additive model, this extended
model is sensitive to syntactic structure, since n is
chosen from among the neighbors of the predicate,
distinguishing it from the argument.
Although we have presented multiplicative and
additive models separately, there is nothing inherent
in our formulation that disallows their combination.
The proposal is not merely notational. One poten-
tial drawback of multiplicative models is the effect
of components with value zero. Since the product
of zero with any number is itself zero, the presence
of zeroes in either of the vectors leads to informa-
tion being essentially thrown away. Combining the
multiplicative model with an additive model, which
does not suffer from this problem, could mitigate
this problem:
pi = ?ui +?vi + ?uivi (11)
where ?, ?, and ? are weighting constants.
239
4 Evaluation Set-up
We evaluated the models presented in Section 3
on a sentence similarity task initially proposed by
Kintsch (2001). In his study, Kintsch builds a model
of how a verb?s meaning is modified in the context of
its subject. He argues that the subjects of ran in The
color ran and The horse ran select different senses
of ran. This change in the verb?s sense is equated to
a shift in its position in semantic space. To quantify
this shift, Kintsch proposes measuring similarity rel-
ative to other verbs acting as landmarks, for example
gallop and dissolve. The idea here is that an appro-
priate composition model when applied to horse and
ran will yield a vector closer to the landmark gallop
than dissolve. Conversely, when color is combined
with ran, the resulting vector will be closer to dis-
solve than gallop.
Focusing on a single compositional structure,
namely intransitive verbs and their subjects, is a
good point of departure for studying vector combi-
nation. Any adequate model of composition must be
able to represent argument-verb meaning. Moreover
by using a minimal structure we factor out inessen-
tial degrees of freedom and are able to assess the
merits of different models on an equal footing. Un-
fortunately, Kintsch (2001) demonstrates how his
own composition algorithm works intuitively on a
few hand selected examples but does not provide a
comprehensive test set. In order to establish an inde-
pendent measure of sentence similarity, we assem-
bled a set of experimental materials and elicited sim-
ilarity ratings from human subjects. In the following
we describe our data collection procedure and give
details on how our composition models were con-
structed and evaluated.
Materials and Design Our materials consisted
of sentences with an an intransitive verb and its sub-
ject. We first compiled a list of intransitive verbs
from CELEX2. All occurrences of these verbs with
a subject noun were next extracted from a RASP
parsed (Briscoe and Carroll, 2002) version of the
British National Corpus (BNC). Verbs and nouns
that were attested less than fifty times in the BNC
were removed as they would result in unreliable vec-
tors. Each reference subject-verb tuple (e.g., horse
ran) was paired with two landmarks, each a syn-
onym of the verb. The landmarks were chosen so
as to represent distinct verb senses, one compatible
2http://www.ru.nl/celex/
with the reference (e.g., horse galloped ) and one in-
compatible (e.g., horse dissolved ). Landmarks were
taken from WordNet (Fellbaum, 1998). Specifically,
they belonged to different synsets and were maxi-
mally dissimilar as measured by the Jiang and Con-
rath (1997) measure.3
Our initial set of candidate materials consisted
of 20 verbs, each paired with 10 nouns, and 2 land-
marks (400 pairs of sentences in total). These were
further pretested to allow the selection of a subset
of items showing clear variations in sense as we
wanted to have a balanced set of similar and dis-
similar sentences. In the pretest, subjects saw a
reference sentence containing a subject-verb tuple
and its landmarks and were asked to choose which
landmark was most similar to the reference or nei-
ther. Our items were converted into simple sentences
(all in past tense) by adding articles where appropri-
ate. The stimuli were administered to four separate
groups; each group saw one set of 100 sentences.
The pretest was completed by 53 participants.
For each reference verb, the subjects? responses
were entered into a contingency table, whose rows
corresponded to nouns and columns to each possi-
ble answer (i.e., one of the two landmarks). Each
cell recorded the number of times our subjects se-
lected the landmark as compatible with the noun or
not. We used Fisher?s exact test to determine which
verbs and nouns showed the greatest variation in
landmark preference and items with p-values greater
than 0.001 were discarded. This yielded a reduced
set of experimental items (120 in total) consisting of
15 reference verbs, each with 4 nouns, and 2 land-
marks.
Procedure and Subjects Participants first saw
a set of instructions that explained the sentence sim-
ilarity task and provided several examples. Then
the experimental items were presented; each con-
tained two sentences, one with the reference verb
and one with its landmark. Examples of our items
are given in Table 1. Here, burn is a high similarity
landmark (High) for the reference The fire glowed,
whereas beam is a low similarity landmark (Low).
The opposite is the case for the reference The face
3We assessed a wide range of semantic similarity measures
using the WordNet similarity package (Pedersen et al, 2004).
Most of them yielded similar results. We selected Jiang and
Conrath?s measure since it has been shown to perform consis-
tently well across several cognitive and NLP tasks (Budanitsky
and Hirst, 2001).
240
Noun Reference High Low
The fire glowed burned beamed
The face glowed beamed burned
The child strayed roamed digressed
The discussion strayed digressed roamed
The sales slumped declined slouched
The shoulders slumped slouched declined
Table 1: Example Stimuli with High and Low similarity
landmarks
glowed. Sentence pairs were presented serially in
random order. Participants were asked to rate how
similar the two sentences were on a scale of one
to seven. The study was conducted remotely over
the Internet using Webexp4, a software package de-
signed for conducting psycholinguistic studies over
the web. 49 unpaid volunteers completed the exper-
iment, all native speakers of English.
Analysis of Similarity Ratings The reliability
of the collected judgments is important for our eval-
uation experiments; we therefore performed several
tests to validate the quality of the ratings. First, we
examined whether participants gave high ratings to
high similarity sentence pairs and low ratings to low
similarity ones. Figure 2 presents a box-and-whisker
plot of the distribution of the ratings. As we can see
sentences with high similarity landmarks are per-
ceived as more similar to the reference sentence. A
Wilcoxon rank sum test confirmed that the differ-
ence is statistically significant (p < 0.01). We also
measured how well humans agree in their ratings.
We employed leave-one-out resampling (Weiss and
Kulikowski, 1991), by correlating the data obtained
from each participant with the ratings obtained from
all other participants. We used Spearman?s ?, a non
parametric correlation coefficient, to avoid making
any assumptions about the distribution of the simi-
larity ratings. The average inter-subject agreement5
was ? = 0.40. We believe that this level of agree-
ment is satisfactory given that naive subjects are
asked to provide judgments on fine-grained seman-
tic distinctions (see Table 1). More evidence that
this is not an easy task comes from Figure 2 where
we observe some overlap in the ratings for High and
Low similarity items.
4http://www.webexp.info/
5Note that Spearman?s rho tends to yield lower coefficients
compared to parametric alternatives such as Pearson?s r.
High Low0
1
2
3
4
5
6
7
Figure 2: Distribution of elicited ratings for High and
Low similarity items
Model Parameters Irrespectively of their form,
all composition models discussed here are based on
a semantic space for representing the meanings of
individual words. The semantic space we used in
our experiments was built on a lemmatised version
of the BNC. Following previous work (Bullinaria
and Levy, 2007), we optimized its parameters on a
word-based semantic similarity task. The task in-
volves examining the degree of linear relationship
between the human judgments for two individual
words and vector-based similarity values. We ex-
perimented with a variety of dimensions (ranging
from 50 to 500,000), vector component definitions
(e.g., pointwise mutual information or log likelihood
ratio) and similarity measures (e.g., cosine or confu-
sion probability). We used WordSim353, a bench-
mark dataset (Finkelstein et al, 2002), consisting of
relatedness judgments (on a scale of 0 to 10) for 353
word pairs.
We obtained best results with a model using a
context window of five words on either side of the
target word, the cosine measure, and 2,000 vector
components. The latter were the most common con-
text words (excluding a stop list of function words).
These components were set to the ratio of the proba-
bility of the context word given the target word to
the probability of the context word overall. This
configuration gave high correlations with the Word-
Sim353 similarity judgments using the cosine mea-
sure. In addition, Bullinaria and Levy (2007) found
that these parameters perform well on a number of
other tasks such as the synonymy task from the Test
of English as a Foreign Language (TOEFL).
Our composition models have no additional pa-
241
rameters beyond the semantic space just described,
with three exceptions. First, the additive model
in (7) weighs differentially the contribution of the
two constituents. In our case, these are the sub-
ject noun and the intransitive verb. To this end,
we optimized the weights on a small held-out set.
Specifically, we considered eleven models, varying
in their weightings, in steps of 10%, from 100%
noun through 50% of both verb and noun to 100%
verb. For the best performing model the weight
for the verb was 80% and for the noun 20%. Sec-
ondly, we optimized the weightings in the combined
model (11) with a similar grid search over its three
parameters. This yielded a weighted sum consisting
of 95% verb, 0% noun and 5% of their multiplica-
tive combination. Finally, Kintsch?s (2001) additive
model has two extra parameters. The m neighbors
most similar to the predicate, and the k of m neigh-
bors closest to its argument. In our experiments we
selected parameters that Kintsch reports as optimal.
Specifically, m was set to 20 and m to 1.
Evaluation Methodology We evaluated the
proposed composition models in two ways. First,
we used the models to estimate the cosine simi-
larity between the reference sentence and its land-
marks. We expect better models to yield a pattern of
similarity scores like those observed in the human
ratings (see Figure 2). A more scrupulous evalua-
tion requires directly correlating all the individual
participants? similarity judgments with those of the
models.6 We used Spearman?s ? for our correlation
analyses. Again, better models should correlate bet-
ter with the experimental data. We assume that the
inter-subject agreement can serve as an upper bound
for comparing the fit of our models against the hu-
man judgments.
5 Results
Our experiments assessed the performance of seven
composition models. These included three additive
models, i.e., simple addition (equation (5), Add),
weighted addition (equation (7), WeightAdd), and
Kintsch?s (2001) model (equation (10), Kintsch), a
multiplicative model (equation (6), Multiply), and
also a model which combines multiplication with
6We avoided correlating the model predictions with aver-
aged participant judgments as this is inappropriate given the or-
dinal nature of the scale of these judgments and also leads to a
dependence between the number of participants and the magni-
tude of the correlation coefficient.
Model High Low ?
NonComp 0.27 0.26 0.08**
Add 0.59 0.59 0.04*
WeightAdd 0.35 0.34 0.09**
Kintsch 0.47 0.45 0.09**
Multiply 0.42 0.28 0.17**
Combined 0.38 0.28 0.19**
UpperBound 4.94 3.25 0.40**
Table 2: Model means for High and Low similarity
items and correlation coefficients with human judgments
(*: p < 0.05, **: p < 0.01)
addition (equation (11), Combined). As a baseline
we simply estimated the similarity between the ref-
erence verb and its landmarks without taking the
subject noun into account (equation (8), NonComp).
Table 2 shows the average model ratings for High
and Low similarity items. For comparison, we also
show the human ratings for these items (Upper-
Bound). Here, we are interested in relative dif-
ferences, since the two types of ratings correspond
to different scales. Model similarities have been
estimated using cosine which ranges from 0 to 1,
whereas our subjects rated the sentences on a scale
from 1 to 7.
The simple additive model fails to distinguish be-
tween High and Low Similarity items. We observe
a similar pattern for the non compositional base-
line model, the weighted additive model and Kintsch
(2001). The multiplicative and combined models
yield means closer to the human ratings. The dif-
ference between High and Low similarity values es-
timated by these models are statistically significant
(p < 0.01 using the Wilcoxon rank sum test). Fig-
ure 3 shows the distribution of estimated similarities
under the multiplicative model.
The results of our correlation analysis are also
given in Table 2. As can be seen, all models are sig-
nificantly correlated with the human ratings. In or-
der to establish which ones fit our data better, we ex-
amined whether the correlation coefficients achieved
differ significantly using a t-test (Cohen and Cohen,
1983). The lowest correlation (? = 0.04) is observed
for the simple additive model which is not signif-
icantly different from the non-compositional base-
line model. The weighted additive model (? = 0.09)
is not significantly different from the baseline either
or Kintsch (2001) (? = 0.09). Given that the basis
242
High Low
0
0.2
0.4
0.6
0.8
1
Figure 3: Distribution of predicted similarities for the
vector multiplication model on High and Low similarity
items
of Kintsch?s model is the summation of the verb, a
neighbor close to the verb and the noun, it is not
surprising that it produces results similar to a sum-
mation which weights the verb more heavily than
the noun. The multiplicative model yields a better
fit with the experimental data, ? = 0.17. The com-
bined model is best overall with ? = 0.19. However,
the difference between the two models is not statis-
tically significant. Also note that in contrast to the
combined model, the multiplicative model does not
have any free parameters and hence does not require
optimization for this particular task.
6 Discussion
In this paper we presented a general framework for
vector-based semantic composition. We formulated
composition as a function of two vectors and intro-
duced several models based on addition and multi-
plication. Despite the popularity of additive mod-
els, our experimental results showed the superior-
ity of models utilizing multiplicative combinations,
at least for the sentence similarity task attempted
here. We conjecture that the additive models are
not sensitive to the fine-grained meaning distinc-
tions involved in our materials. Previous applica-
tions of vector addition to document indexing (Deer-
wester et al, 1990) or essay grading (Landauer et al,
1997) were more concerned with modeling the gist
of a document rather than the meaning of its sen-
tences. Importantly, additive models capture com-
position by considering all vector components rep-
resenting the meaning of the verb and its subject,
whereas multiplicative models consider a subset,
namely non-zero components. The resulting vector
is sparser but expresses more succinctly the meaning
of the predicate-argument structure, and thus allows
semantic similarity to be modelled more accurately.
Further research is needed to gain a deeper un-
derstanding of vector composition, both in terms of
modeling a wider range of structures (e.g., adjective-
noun, noun-noun) and also in terms of exploring the
space of models more fully. We anticipate that more
substantial correlations can be achieved by imple-
menting more sophisticated models from within the
framework outlined here. In particular, the general
class of multiplicative models (see equation (4)) ap-
pears to be a fruitful area to explore. Future direc-
tions include constraining the number of free param-
eters in linguistically plausible ways and scaling to
larger datasets.
The applications of the framework discussed here
are many and varied both for cognitive science and
NLP. We intend to assess the potential of our com-
position models on context sensitive semantic prim-
ing (Till et al, 1988) and inductive inference (Heit
and Rubinstein, 1994). NLP tasks that could benefit
from composition models include paraphrase iden-
tification and context-dependent language modeling
(Coccaro and Jurafsky, 1998).
References
E. Briscoe, J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd
International Conference on Language Resources and
Evaluation, 1499?1504, Las Palmas, Canary Islands.
A. Budanitsky, G. Hirst. 2001. Semantic distance in
WordNet: An experimental, application-oriented eval-
uation of five measures. In Proceedings of ACL Work-
shop on WordNet and Other Lexical Resources, Pitts-
burgh, PA.
J. Bullinaria, J. Levy. 2007. Extracting semantic rep-
resentations from word co-occurrence statistics: A
computational study. Behavior Research Methods,
39:510?526.
F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent se-
mantic analysis for text segmentation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, 109?117, Pittsburgh, PA.
N. Coccaro, D. Jurafsky. 1998. Towards better integra-
tion of semantic predictors in statistical language mod-
eling. In Proceedings of the 5th International Confer-
ence on Spoken Language Processsing, Sydney, Aus-
tralia.
243
J. Cohen, P. Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
G. Denhire, B. Lemaire. 2004. A computational model
of children?s semantic memory. In Proceedings of the
26th Annual Meeting of the Cognitive Science Society,
297?302, Chicago, IL.
C. Fellbaum, ed. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, E. Ruppin. 2002. Placing
search in context: The concept revisited. ACM Trans-
actions on Information Systems, 20(1):116?131.
J. Fodor, Z. Pylyshyn. 1988. Connectionism and cogni-
tive architecture: A critical analysis. Cognition, 28:3?
71.
P. W. Foltz, W. Kintsch, T. K. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse Process, 15:285?307.
S. Frank, M. Koppen, L. Noordman, W. Vonk. 2007.
World knowledge in computational models of dis-
course comprehension. Discourse Processes. In press.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley, New York.
E. Heit, J. Rubinstein. 1994. Similarity and property ef-
fects in inductive reasoning. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
20:411?422.
J. J. Jiang, D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, Taiwan.
W. Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
T. K. Landauer, S. T. Dumais. 1997. A solution to Plato?s
problem: the latent semantic analysis theory of ac-
quisition, induction and representation of knowledge.
Psychological Review, 104(2):211?240.
T. K. Landauer, D. Laham, B. Rehder, M. E. Schreiner.
1997. How well can passage meaning be derived with-
out using word order: A comparison of latent semantic
analysis and humans. In Proceedings of 19th Annual
Conference of the Cognitive Science Society, 412?417,
Stanford, CA.
K. Lund, C. Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Be-
havior Research Methods, Instruments & Computers,
28:203?208.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004.
Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, 280?287,
Barcelona, Spain.
S. McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
R. Montague. 1974. English as a formal language. In
R. Montague, ed., Formal Philosophy. Yale University
Press, New Haven, CT.
H. Neville, J. L. Nichol, A. Barss, K. I. Forster, M. F. Gar-
rett. 1991. Syntactically based sentence prosessing
classes: evidence form event-related brain potentials.
Journal of Congitive Neuroscience, 3:151?165.
S. Pado?, M. Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Lin-
guistics, 33(2):161?199.
T. Pedersen, S. Patwardhan, J. Michelizzi. 2004. Word-
Net::similarity - measuring the relatedness of con-
cepts. In Proceedings of the 5th Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, 38?41, Boston, MA.
T. A. Plate. 1991. Holographic reduced representations:
Convolution algebra for compositional distributed rep-
resentations. In Proceedings of the 12th Interna-
tional Joint Conference on Artificial Intelligence, 30?
35, Sydney, Australia.
G. Salton, A. Wong, C. S. Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613?620.
P. Schone, D. Jurafsky. 2001. Is knowledge-free induc-
tion of multiword unit dictionary headwords a solved
problem? In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, 100?
108, Pittsburgh, PA.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connec-
tionist systems. Artificial Intelligence, 46:159?216.
R. E. Till, E. F. Mross, W. Kintsch. 1988. Time course of
priming for associate and inference words in discourse
context. Memory and Cognition, 16:283?299.
S. M. Weiss, C. A. Kulikowski. 1991. Computer Sys-
tems that Learn: Classification and Prediction Meth-
ods from Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann, San Mateo,
CA.
R. F. West, K. E. Stanovich. 1986. Robust effects of
syntactic structure on visual word processing. Journal
of Memory and Cognition, 14:104?112.
244
Proceedings of ACL-08: HLT, pages 272?280,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Automatic Image Annotation Using Auxiliary Text Information
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The availability of databases of images labeled
with keywords is necessary for developing and
evaluating image annotation models. Dataset
collection is however a costly and time con-
suming task. In this paper we exploit the vast
resource of images available on the web. We
create a database of pictures that are natu-
rally embedded into news articles and propose
to use their captions as a proxy for annota-
tion keywords. Experimental results show that
an image annotation model can be developed
on this dataset alne without the overhead of
manual annotation. We also demonstrate that
the news article associated with the picture
can be used to boost image annotation perfor-
mance.
1 Introduction
As the number of image collections is rapidly grow-
ing, so does the need to browse and search them.
Recent years have witnessed significant progress in
developing methods for image retrieval1, many of
which are query-based. Given a database of images,
each annotated with keywords, the query is used to
retrieve relevant pictures under the assumption that
the annotations can essentially capture their seman-
tics.
One stumbling block to the widespread use of
query-based image retrieval systems is obtaining the
keywords for the images. Since manual annotation
is expensive, time-consuming and practically infea-
sible for large databases, there has been great in-
1The approaches are too numerous to list; we refer the inter-
ested reader to Datta et al (2005) for an overview.
terest in automating the image annotation process
(see references). More formally, given an image I
with visual features Vi = {v1,v2, . . . ,vN} and a set
of keywords W = {w1,w2, . . . ,wM}, the task con-
sists in finding automatically the keyword subset
WI ? W , which can appropriately describe the im-
age I. Indeed, several approaches have been pro-
posed to solve this problem under a variety of learn-
ing paradigms. These range from supervised clas-
sification (Vailaya et al, 2001; Smeulders et al,
2000) to instantiations of the noisy-channel model
(Duygulu et al, 2002), to clustering (Barnard et al,
2002), and methods inspired by information retrieval
(Lavrenko et al, 2003; Feng et al, 2004).
Obviously in order to develop accurate image an-
notation models, some manually labeled data is re-
quired. Previous approaches have been developed
and tested almost exclusively on the Corel database.
The latter contains 600 CD-ROMs, each contain-
ing about 100 images representing the same topic
or concept, e.g., people, landscape, male. Each topic
is associated with keywords and these are assumed
to also describe the images under this topic. As an
example consider the pictures in Figure 1 which are
classified under the topic male and have the descrip-
tion keywords man, male, people, cloth, and face.
Current image annotation methods work well
when large amounts of labeled images are available
but can run into severe difficulties when the number
of images and keywords for a given topic is rela-
tively small. Unfortunately, databases like Corel are
few and far between and somewhat idealized. Corel
contains clusters of many closely related images
which in turn share keyword descriptions, thus al-
lowing models to learn image-keyword associations
272
Figure 1: Images from the Corel database, exemplifying
the concept male with keyword descriptions man, male,
people, cloth, and face.
reliably (Tang and Lewis, 2007). It is unlikely that
models trained on this database will perform well
out-of-domain on other image collections which are
more noisy and do not share these characteristics.
Furthermore, in order to develop robust image anno-
tation models, it is crucial to have large and diverse
datasets both for training and evaluation.
In this work, we aim to relieve the data acquisition
bottleneck associated with automatic image annota-
tion by taking advantage of resources where images
and their annotations co-occur naturally. News arti-
cles associated with images and their captions spring
readily to mind (e.g., BBC News, Yahoo News). So,
rather than laboriously annotating images with their
keywords, we simply treat captions as labels. These
annotations are admittedly noisy and far from ideal.
Captions can be denotative (describing the objects
the image depicts) but also connotative (describ-
ing sociological, political, or economic attitudes re-
flected in the image). Importantly, our images are not
standalone, they come with news articles whose con-
tent is shared with the image. So, by processing the
accompanying document, we can effectively learn
about the image and reduce the effect of noise due
to the approximate nature of the caption labels. To
give a simple example, if two words appear both in
the caption and the document, it is more likely that
the annotation is genuine.
In what follows, we present a new database con-
sisting of articles, images, and their captions which
we collected from an on-line news source. We
then propose an image annotation model which can
learn from our noisy annotations and the auxil-
iary documents. Specifically, we extend and mod-
ify Lavrenko?s (2003) continuous relevance model
to suit our task. Our experimental results show that
this model can successfully scale to our database,
without making use of explicit human annotations
in any way. We also show that the auxiliary docu-
ment contains important information for generating
more accurate image descriptions.
2 Related Work
Automatic image annotation is a popular task in
computer vision. The earliest approaches are closely
related to image classification (Vailaya et al, 2001;
Smeulders et al, 2000), where pictures are assigned
a set of simple descriptions such as indoor, out-
door, landscape, people, animal. A binary classifier
is trained for each concept, sometimes in a ?one vs
all? setting. The focus here is mostly on image pro-
cessing and good feature selection (e.g., colour, tex-
ture, contours) rather than the annotation task itself.
Recently, much progress has been made on the
image annotation task thanks to three factors. The
availability of the Corel database, the use of unsu-
pervised methods and new insights from the related
fields of natural language processing and informa-
tion retrieval. The co-occurrence model (Mori et al,
1999) collects co-occurrence counts between words
and image features and uses them to predict anno-
tations for new images. Duygulu et al (2002) im-
prove on this model by treating image regions and
keywords as a bi-text and using the EM algorithm to
construct an image region-word dictionary.
Another way of capturing co-occurrence informa-
tion is to introduce latent variables linking image
features with words. Standard latent semantic anal-
ysis (LSA) and its probabilistic variant (PLSA) have
been applied to this task (Hofmann, 1998). Barnard
et al (2002) propose a hierarchical latent model
in order to account for the fact that some words
are more general than others. More sophisticated
graphical models (Blei and Jordan, 2003) have also
been employed including Gaussian Mixture Models
(GMM) and Latent Dirichlet Allocation (LDA).
Finally, relevance models originally developed for
information retrieval, have been successfully applied
to image annotation (Lavrenko et al, 2003; Feng et
al., 2004). A key idea behind these models is to find
the images most similar to the test image and then
use their shared keywords for annotation.
Our approach differs from previous work in two
273
important respects. Firstly, our ultimate goal is to de-
velop an image annotation model that can cope with
real-world images and noisy data sets. To this end
we are faced with the challenge of building an ap-
propriate database for testing and training purposes.
Our solution is to leverage the vast resource of im-
ages available on the web but also the fact that many
of these images are implicitly annotated. For exam-
ple, news articles often contain images whose cap-
tions can be thought of as annotations. Secondly, we
allow our image annotation model access to knowl-
edge sources other than the image and its keywords.
This is relatively straightforward in our case; an im-
age and its accompanying document have shared
content, and we can use the latter to glean informa-
tion about the former. But we hope to illustrate the
more general point that auxiliary linguistic informa-
tion can indeed bring performance improvements on
the image annotation task.
3 BBC News Database
Our database consists of news images which are
abundant. Many on-line news providers supply pic-
tures with news articles, some even classify news
into broad topic categories (e.g., business, world,
sports, entertainment). Importantly, news images of-
ten display several objects and complex scenes and
are usually associated with captions describing their
contents. The captions are image specific and use a
rich vocabulary. This is in marked contrast to the
Corel database whose images contain one or two
salient objects and a limited vocabulary (typically
around 300 words).
We downloaded 3,361 news articles from the
BBC News website.2 Each article was accompa-
nied with an image and its caption. We thus created
a database of image-caption-document tuples. The
documents cover a wide range of topics including
national and international politics, advanced tech-
nology, sports, education, etc. An example of an en-
try in our database is illustrated in Figure 2. Here,
the image caption is Marcin and Florent face intense
competition from outside Europe and the accompa-
nying article discusses EU subsidies to farmers. The
images are usually 203 pixels wide and 152 pix-
els high. The average caption length is 5.35 tokens,
and the average document length 133.85 tokens. Our
2http://news.bbc.co.uk/
Figure 2: A sample from our BBC News database. Each
entry contains an image, a caption for the image, and the
accompanying document with its title.
captions have a vocabulary of 2,167 words and our
documents 6,253. The vocabulary shared between
captions and documents is 2,056 words.
4 Extending the Continuous Relevance
Annotation Model
Our work is an extension of the continuous rele-
vance annotation model put forward in Lavrenko
et al (2003). Unlike other unsupervised approaches
where a set of latent variables is introduced, each
defining a joint distribution on the space of key-
words and image features, the relevance model cap-
tures the joint probability of images and annotated
words directly, without requiring an intermediate
clustering stage. This model is a good point of de-
parture for our task for several reasons, both theo-
retical and empirical. Firstly, expectations are com-
puted over every single point in the training set and
274
therefore parameters can be estimated without EM.
Indeed, Lavrenko et al achieve competitive perfor-
mance with latent variable models. Secondly, the
generation of feature vectors is modeled directly,
so there is no need for quantization. Thirdly, as we
show below the model can be easily extended to in-
corporate information outside the image and its key-
words.
In the following we first lay out the assumptions
underlying our model. We next describe the contin-
uous relevance model in more detail and present our
extensions and modifications.
Assumptions Since we are using a non-
standard database, namely images embedded in doc-
uments, it is important to clarify what we mean by
image annotation, and how the precise nature of our
data impacts the task. We thus make the following
assumptions:
1. The caption describes the content of the image
directly or indirectly. Unlike traditional image
annotation where keywords describe salient ob-
jects, captions supply more detailed informa-
tion, not only about objects, and their attributes,
but also events. In Figure 2 the caption men-
tions Marcin and Florent the two individuals
shown in the picture but also the fact that they
face competition from outside Europe.
2. Since our images are implicitly rather than ex-
plicitly labeled, we do not assume that we can
annotate all objects present in the image. In-
stead, we hope to be able to model event-related
information such as ?what happened?, ?who
did it?, ?when? and ?where?. Our annotation
task is therefore more semantic in nature than
traditionally assumed.
3. The accompanying document describes the
content of the image. This is trivially true for
news documents where the images convention-
ally depict events, objects or people mentioned
in the article.
To validate these assumptions, we performed the
following experiment on our BBC News dataset.
We randomly selected 240 image-caption pairs
and manually assessed whether the caption content
words (i.e., nouns, verbs, and adjectives) could de-
scribe the image. We found out that the captions
express the picture?s content 90% of the time. Fur-
thermore, approximately 88% of the nouns in sub-
ject or object position directly denote salient picture
objects. We thus conclude that the captions contain
useful information about the picture and can be used
for annotation purposes.
Model Description The continuous relevance
image annotation model (Lavrenko et al, 2003)
generatively learns the joint probability distribu-
tion P(V,W ) of words W and image regions V . The
key assumption here is that the process of generating
images is conditionally independent from the pro-
cess of generating words. Each annotated image in
the training set is treated as a latent variable. Then
for an unknown image I, we estimate:
P(VI,WI) = ?
s?D
P(VI|s)P(WI|s)P(s), (1)
where D is the number of images in the training
database, VI are visual features of the image regions
representing I, WI are the keywords of I, s is a la-
tent variable (i.e., an image-annotation pair), and
P(s) the prior probability of s. The latter is drawn
from a uniform distribution:
P(s) =
1
ND
(2)
where ND is number of the latent variables in the
training database D.
When estimating P(VI|s), the probability of im-
age regions and words, Lavrenko et al (2003) rea-
sonably assume a generative Gaussian kernel distri-
bution for the image regions:
(3)P(VI|s) =
NVI
?
r=1
Pg(vr|s)
=
NVI
?
r=1
1
nsv
nsv
?
i=1
exp{(vr ? vi)T??1(vr ? vi)}
?
2kpik |?|
where NVI is the number of regions in image I, vr the
feature vector for region r in image I, nsv the number
of regions in the image of latent variable s, vi the fea-
ture vector for region i in s?s image, k the dimension
of the image feature vectors and ? the feature covari-
ance matrix. According to equation (3), a Gaussian
kernel is fit to every feature vector vi corresponding
to region i in the image of the latent variable s. Each
kernel here is determined by the feature covariance
matrix ?, and for simplicity, ? is assumed to be a
diagonal matrix: ?= ?I, where I is the identity ma-
trix; and ? is a scalar modulating the bandwidth of
275
the kernel whose value is optimized on the develop-
ment set.
Lavrenko et al (2003) estimate the word prob-
abilities P(WI|s) using a multinomial distribution.
This is a reasonable assumption in the Corel dataset,
where the annotations have similar lengths and the
words reflect the salience of objects in the image (the
multinomial model tends to favor words that appear
multiple times in the annotation). However, in our
dataset the annotations have varying lengths, and do
not necessarily reflect object salience. We are more
interested in modeling the presence or absence of
words in the annotation and thus use the multiple-
Bernoulli distribution to generate words (Feng et al,
2004). And rather than relying solely on annotations
in the training database, we can also take the accom-
panying document into account using a weighted
combination.
The probability of sampling a set of words W
given a latent variable s from the underlying multiple
Bernoulli distribution that has generated the training
set D is:
P(W |s) = ?
w?W
P(w|s) ?
w/?W
(1?P(w|s)) (4)
where P(w|s) denotes the probability of the w?th
component of the multiple Bernoulli distribution.
Now, in estimating P(w|s) we can include the docu-
ment as:
Pest(w|s) = ?Pest(w|sa)+(1??)Pest(w|sd) (5)
where ? is a smoothing parameter tuned on the de-
velopment set, sa is the annotation for the latent vari-
able s and sd its corresponding document.
Equation (5) smooths the influence of the annota-
tion words and allows to offset the negative effect of
the noise inherent in our dataset. Since our images
are implicitly annotated, there is no guarantee that
the annotations are all appropriate. By taking into
account Pest(w|sd), it is possible to annotate an im-
age with a word that appears in the document but is
not included in the caption.
We use a Bayesian framework for estimat-
ing Pest(w|sa). Specifically, we assume a beta prior
(conjugate to the Bernoulli distribution) for each
word:
Pest(w|sa) =
? bw,sa +Nw
?+D
(6)
where ? is a smoothing parameter estimated on the
development set, bw,sa is a Boolean variable denoting
whether w appears in the annotation sa, and Nw is
the number of latent variables that contain w in their
annotations.
We estimate Pest(w|sd) using maximum likeli-
hood estimation (Ponte and Croft, 1998):
Pest(w|sd) =
numw,sd
numsd
(7)
where numw,sd denotes the frequency of w in the ac-
companying document of latent variable s and numsd
the number of all tokens in the document. Note that
we purposely leave Pest unsmoothed, since it is used
as a means of balancing the weight of word frequen-
cies in annotations. So, if a word does not appear in
the document, the possibility of selecting it will not
be greater than ? (see Equation (5)).
Unfortunately, including the document in the es-
timation of Pest(w|s) increases the vocabulary which
in turn increases computation time. Given a test
image-document pair, we must evaluate P(w|VI) for
every w in our vocabulary which is the union of
the caption and document words. We reduce the
search space, by scoring each document word with
its tf ? idf weight (Salton and McGill, 1983) and
adding the n-best candidates to our caption vocabu-
lary. This way the vocabulary is not fixed in advance
for all images but changes dynamically depending
on the document at hand.
Re-ranking the Annotation Hypotheses It is
easy to see that the output of our model is a ranked
word list. Typically, the k-best words are taken to
be the automatic annotations for a test image I
(Duygulu et al, 2002; Lavrenko et al, 2003; Jeon
andManmatha, 2004) where k is a small number and
the same for all images.
So far we have taken account of the auxiliary doc-
ument rather naively, by considering its vocabulary
in the estimation of P(W |s). Crucially, documents
are written with one or more topics in mind. The im-
age (and its annotations) are likely to represent these
topics, so ideally our model should prefer words that
are strong topic indicators. A simple way to imple-
ment this idea is by re-ranking our k-best list accord-
ing to a topic model estimated from the entire docu-
ment collection.
Specifically, we use Latent Dirichlet Allocation
(LDA) as our topic model (Blei et al, 2003). LDA
276
represents documents as a mixture of topics and has
been previously used to perform document classi-
fication (Blei et al, 2003) and ad-hoc information
retrieval (Wei and Croft, 2006) with good results.
Given a collection of documents and a set of latent
variables (i.e., the number of topics), the LDAmodel
estimates the probability of topics per document and
the probability of words per topic. The topic mix-
ture is drawn from a conjugate Dirichlet prior that
remains the same for all documents.
For our re-ranking task, we use the LDA model
to infer the m-best topics in the accompanying doc-
ument. We then select from the output of our model
those words that are most likely according to these
topics. To give a concrete example, let us assume
that for a given image our model has produced
five annotations, w1, w2, w3, w4, and w5. However,
according to the LDA model neither w2 nor w5
are likely topic indicators. We therefore remove w2
and w5 and substitute them with words further down
the ranked list that are topical (e.g., w6 and w7).
An advantage of using LDA is that at test time we
can perform inference without retraining the topic
model.
5 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the model pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe our fea-
tures, and present the baseline methods used for
comparison with our approach.
Data Our model was trained and tested on the
database introduced in Section 3. We used 2,881
image-caption-document tuples for training, 240 tu-
ples for development and 240 for testing. The docu-
ments and captions were part-of-speech tagged and
lemmatized with Tree Tagger (Schmid, 1994).Words
other than nouns, verbs, and adjectives were dis-
carded. Words that were attested less than five times
in the training set were also removed to avoid unre-
liable estimation. In total, our vocabulary consisted
of 8,309 words.
Model Parameters Images are typically seg-
mented into regions prior to training. We impose a
fixed-size rectangular grid on each image rather than
attempting segmentation using a general purpose al-
gorithm such as normalized cuts (Shi and Malik,
Color
average of RGB components, standard deviation
average of LUV components, standard deviation
average of LAB components, standard deviation
Texture
output of DCT transformation
output of Gabor filtering (4 directions, 3 scales)
Shape
oriented edge (4 directions)
ratio of edge to non-edge
Table 2: Set of image features used in our experiments.
2000). Using a grid avoids unnecessary errors from
image segmentation algorithms, reduces computa-
tion time, and simplifies parameter estimation (Feng
et al, 2004). Taking the small size and low resolu-
tion of the BBC News images into account, we av-
eragely divide each image into 6?5 rectangles and
extract features for each region. We use 46 features
based on color, texture, and shape. They are summa-
rized in Table 2.
The model presented in Section 4 has a few pa-
rameters that must be selected empirically on the
development set. These include the vocabulary size,
which is dependent on the n words with the high-
est tf ? idf scores in each document, and the num-
ber of topics for the LDA-based re-ranker. We ob-
tained best performance with n set to 100 (no cutoff
was applied in cases where the vocabulary was less
than 100). We trained an LDA model with 20 top-
ics on our document collection using David Blei?s
implementation.3 We used this model to re-rank the
output of our annotation model according to the
three most likely topics in each document.
Baselines We compared our model against
three baselines. The first baseline is based on tf ? idf
(Salton and McGill, 1983). We rank the document?s
content words (i.e., nouns, verbs, and adjectives) ac-
cording to their tf ? idf weight and select the top k
to be the final annotations. Our second baseline sim-
ply annotates the image with the document?s title.
Again we only use content words (the average title
length in the training set was 4.0 words). Our third
baseline is Lavrenko et al?s (2003) continuous rel-
evance model. It is trained solely on image-caption
3Available from http://www.cs.princeton.edu/?blei/
lda-c/index.html.
277
Model Top 10 Top 15 Top 20
Precision Recall F1 Precision Recall F1 Precision Recall F1
tf ? idf 4.37 7.09 5.41 3.57 8.12 4.86 2.65 8.89 4.00
DocTitle 9.22 7.03 7.20 9.22 7.03 7.20 9.22 7.03 7.20
Lavrenko03 9.05 16.01 11.81 7.73 17.87 10.71 6.55 19.38 9.79
ExtModel 14.72 27.95 19.82 11.62 32.99 17.18 9.72 36.77 15.39
Table 1: Automatic image annotation results on the BBC News database.
pairs, uses a vocabulary of 2,167 words and the same
features as our extended model.
Evaluation Our evaluation follows the exper-
imental methodology proposed in Duygulu et al
(2002). We are given an un-annotated image I and
are asked to automatically produce suitable anno-
tations for I. Given a set of image regions VI , we
use equation (1) to derive the conditional distribu-
tion P(w|VI). We consider the k-best words as the an-
notations for I. We present results using the top 10,
15, and 20 annotation words. We assess our model?s
performance using precision/recall and F1. In our
task, precision is the percentage of correctly anno-
tated words over all annotations that the system sug-
gested. Recall, is the percentage of correctly anno-
tated words over the number of genuine annotations
in the test data. F1 is the harmonic mean of precision
and recall. These measures are averaged over the set
of test words.
6 Results
Our experiments were driven by three questions:
(1) Is it possible to create an annotation model from
noisy data that has not been explicitly hand labeled
for this task? (2) What is the contribution of the
auxiliary document? As mentioned earlier, consid-
ering the document increases the model?s compu-
tational complexity, which can be justified as long
as we demonstrate a substantial increase in perfor-
mance. (3) What is the contribution of the image?
Here, we are trying to assess if the image features
matter. For instance, we could simply generate an-
notation words by processing the document alone.
Our results are summarized in Table 1. We com-
pare the annotation performance of the model pro-
posed in this paper (ExtModel) with Lavrenko et
al.?s (2003) original continuous relevance model
(Lavrenko03) and two other simpler models which
do not take the image into account (tf ? idf and Doc-
Title). First, note that the original relevance model
performs best when the annotation output is re-
stricted to 10 words with an F1 of 11.81% (recall
is 9.05 and precision 16.01). F1 is marginally worse
with 15 output words and decreases by 2% with 20.
This model does not take any document-based in-
formation into account, it is trained solely on image-
caption pairs. On the Corel test set the same model
obtains a precision of 19.0% and a recall of 16.0%
with a vocabulary of 260 words. Although these re-
sults are not strictly comparable with ours due to the
different nature of the training data (in addition, we
output 10 annotation words, whereas Lavrenko et al
(2003) output 5), they give some indication of the
decrease in performance incurred when using a more
challenging dataset. Unlike Corel, our images have
greater variety, non-overlapping content and employ
a larger vocabulary (2,167 vs. 260 words).
When the document is taken into account (see
ExtModel in Table 1), F1 improves by 8.01% (re-
call is 14.72% and precision 27.95%). Increasing
the size of the output annotations to 15 or 20 yields
better recall, at the expense of precision. Eliminat-
ing the LDA reranker from the extended model de-
creases F1 by 0.62%. Incidentally, LDA can be also
used to rerank the output of Lavrenko et al?s (2003)
model. LDA also increases the performance of this
model by 0.41%.
Finally, considering the document alone, without
the image yields inferior performance. This is true
for the tf ? idf model and the model based on the
document titles.4 Interestingly, the latter yields pre-
cision similar to Lavrenko et al (2003). This is prob-
ably due to the fact that the document?s title is in a
sense similar to a caption. It often contains words
that describe the document?s gist and expectedly
4Reranking the output of these models with LDA slightly
decreases performance (approximately by 0.2%).
278
tf ? idf breastfeed, medical,
intelligent, health, child
culturalism, faith, Muslim, sepa-
rateness, ethnic
ceasefire, Lebanese, disarm, cab-
inet, Haaretz
DocTitle Breast milk does not boost IQ UK must tackle ethnic tensions Mid-East hope as ceasefire begins
Lavrenko03 woman, baby, hospital, new,
day, lead, good, England,
look, family
bomb, city, want, day, fight,
child, attack, face, help, govern-
ment
war, carry, city, security, Israeli,
attack, minister, force, govern-
ment, leader
ExtModel breastfeed, intelligent, baby,
mother, tend, child, study,
woman, sibling, advantage
aim, Kelly, faith, culturalism,
community, Ms, tension, com-
mission, multi, tackle, school
Lebanon, Israeli, Lebanese,
aeroplane, troop, Hezbollah,
Israel, force, ceasefire, grey
Caption Breastfed babies tend to be
brighter
Segregation problems were
blamed for 2001?s Bradford riots
Thousands of Israeli troops are in
Lebanon as the ceasefire begins
Figure 3: Examples of annotations generated by our model (ExtModel), the continuous relevance model (Lavrenko03),
and the two baselines based on tf ? idf and the document title (DocTitle). Words in bold face indicate exact matches,
underlined words are semantically compatible. The original captions are in the last row.
some of these words will be also appropriate for the
image. In fact, in our dataset, the title words are a
subset of those found in the captions.
Examples of the annotations generated by our
model are shown in Figure 3. We also include the
annotations produced by Lavrenko et. al?s (2003)
model and the two baselines. As we can see our
model annotates the image with words that are not
always included in the caption. Some of these are
synonyms of the caption words (e.g., child and intel-
ligent in left image of Figure 3), whereas others ex-
press additional information (e.g., mother, woman).
Also note that complex scene images remain chal-
lenging (see the center image in Figure 3). Such im-
ages are better analyzed at a higher resolution and
probably require more training examples.
7 Conclusions and Future Work
In this paper, we describe a new approach for the
collection of image annotation datasets. Specifically,
we leverage the vast resource of images available
on the Internet while exploiting the fact that many
of them are labeled with captions. Our experiments
show that it is possible to learn an image annotation
model from caption-picture pairs even if these are
not explicitly annotated in any way. We also show
that the annotation model benefits substantially from
additional information, beyond the caption or image.
In our case this information is provided by the news
documents associated with the pictures. But more
generally our results indicate that further linguistic
knowledge is needed to improve performance on the
image annotation task. For instance, resources like
WordNet (Fellbaum, 1998) can be used to expand
the annotations by exploiting information about is-a
relationships.
The uses of the database discussed in this article
are many and varied. An interesting future direction
concerns the application of the proposed model in a
semi-supervised setting where the annotation output
is iteratively refined with some manual intervention.
Another possibility would be to use the document
to increase the annotation keywords by identifying
synonyms or even sentences that are similar to the
image caption. Also note that our analysis of the ac-
companying document was rather shallow, limited
to part of speech tagging. It is reasonable to assume
that results would improve with more sophisticated
preprocessing (i.e., named entity recognition, pars-
ing, word sense disambiguation). Finally, we also
believe that the model proposed here can be usefully
employed in an information retrieval setting, where
the goal is to find the image most relevant for a given
query or document.
279
References
K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas,
D. Blei, and M. Jordan. 2002. Matching words
and pictures. Journal of Machine Learning Research,
3:1107?1135.
D. Blei and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference, pages 127?134, Toronto, ON.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
R. Datta, J. Li, and J. Z. Wang. 2005. Content-based im-
age retrieval ? approaches and trends of the new age.
In Proceedings of the International Workshop on Mul-
timedia Information Retrieval, pages 253?262, Singa-
pore.
P. Duygulu, K. Barnard, J. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary. In
Proceedings of the 7th European Conference on Com-
puter Vision, pages 97?112, Copenhagen, Danemark.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
S. Feng, V. Lavrenko, and R. Manmatha. 2004. Mul-
tiple Bernoulli relevance models for image and video
annotation. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recognition,
pages 1002?1009, Washington, DC.
T. Hofmann. 1998. Learning and representing topic.
A hierarchical mixture model for word occurrences
in document databases. In Proceedings of the Con-
ference for Automated Learning and Discovery, pages
408?415, Pittsburgh, PA.
J. Jeon and R. Manmatha. 2004. Using maximum en-
tropy for automatic image annotation. In Proceed-
ings of the 3rd International Conference on Image and
Video Retrieval, pages 24?32, Dublin City, Ireland.
V. Lavrenko, R. Manmatha, and J. Jeon. 2003. A model
for learning the semantics of pictures. In Proceedings
of the 16th Conference on Advances in Neural Infor-
mation Processing Systems, Vancouver, BC.
Y.Mori, H. Takahashi, and R. Oka. 1999. Image-to-word
transformation based on dividing and vector quantiz-
ing images with words. In Proceedings of the 1st In-
ternational Workshop on Multimedia Intelligent Stor-
age and Retrieval Management, Orlando, FL.
J. M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference, pages 275?281, New York, NY.
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill, New York.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
J. Shi and J. Malik. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 22(8):888?905.
A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and
R. Jain. 2000. Content-based image retrieval at the
end of the early years. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(12):1349?
1380.
J. Tang and P. H. Lewis. 2007. A study of quality is-
sues for image auto-annotation with the Corel data-set.
IEEE Transactions on Circuits and Systems for Video
Technology, 17(3):384?389.
A. Vailaya, M. Figueiredo, A. Jain, and H. Zhang. 2001.
Image classification for content-based indexing. IEEE
Transactions on Image Processing, 10:117?130.
X. Wei and B. W. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proeedings of the 29th
Annual International ACM SIGIR Conference, pages
178?185, Seattle, WA.
280
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 217?225,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning to Tell Tales: A Data-driven Approach to Story Generation
Neil McIntyre and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
n.d.mcintyre@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Computational story telling has sparked
great interest in artificial intelligence,
partly because of its relevance to educa-
tional and gaming applications. Tradition-
ally, story generators rely on a large repos-
itory of background knowledge contain-
ing information about the story plot and
its characters. This information is detailed
and usually hand crafted. In this paper we
propose a data-driven approach for gen-
erating short children?s stories that does
not require extensive manual involvement.
We create an end-to-end system that real-
izes the various components of the gen-
eration pipeline stochastically. Our system
follows a generate-and-and-rank approach
where the space of multiple candidate sto-
ries is pruned by considering whether they
are plausible, interesting, and coherent.
1 Introduction
Recent years have witnessed increased interest in
the use of interactive language technology in ed-
ucational and entertainment applications. Compu-
tational story telling could play a key role in these
applications by effectively engaging learners and
assisting them in creating a story. It could also al-
low teachers to generate stories on demand that
suit their classes? needs. And enhance the enter-
tainment value of role-playing games1. The major-
ity of these games come with a set of pre-specified
plots that the players must act out. Ideally, the plot
should adapt dynamically in response to the play-
ers? actions.
Computational story telling has a longstanding
tradition in the field of artificial intelligence. Early
work has been largely inspired by Propp?s (1968)
1A role-playing game (RPG) is a game in which the par-
ticipants assume the roles of fictional characters and act out
an adventure.
typology of narrative structure. Propp identified in
Russian fairy tales a small number of recurring
units (e.g., the hero is defeated, the villain causes
harm) and rules that could be used to describe their
relation (e.g., the hero is pursued and the rescued).
Story grammars (Thorndyke, 1977) were initially
used to capture Propp?s high-level plot elements
and character interactions. A large body of more
recent work views story generation as a form of
agent-based planning (Theune et al, 2003; Fass,
2002; Oinonen et al, 2006). The agents act as
characters with a list of goals. They form plans
of action and try to fulfill them. Interesting stories
emerge as agents? plans interact and cause failures
and possible replanning.
Perhaps the biggest challenge faced by compu-
tational story generators is the amount of world
knowledge required to create compelling stories.
A hypothetical system must have information
about the characters involved, how they inter-
act, what their goals are, and how they influence
their environment. Furthermore, all this informa-
tion must be complete and error-free if it is to be
used as input to a planning algorithm. Tradition-
ally, this knowledge is created by hand, and must
be recreated for different domains. Even the sim-
ple task of adding a new character requires a whole
new set of action descriptions and goals.
A second challenge concerns the generation
task itself and the creation of stories character-
ized by high-quality prose. Most story genera-
tion systems focus on generating plot outlines,
without considering the actual linguistic structures
found in the stories they are trying to mimic (but
see Callaway and Lester 2002 for a notable ex-
ception). In fact, there seems to be little com-
mon ground between story generation and natural
language generation (NLG), despite extensive re-
search in both fields. The NLG process (Reiter and
Dale, 2000) is often viewed as a pipeline consist-
ing of content planning (selecting and structuring
the story?s content), microplanning (sentence ag-
217
gregation, generation of referring expressions, lex-
ical choice), and surface realization (agreement,
verb-subject ordering). However, story generation
systems typically operate in two phases: (a) creat-
ing a plot for the story and (b) transforming it into
text (often by means of template-based NLG).
In this paper we address both challenges fac-
ing computational story telling. We propose a
data-driven approach to story generation that does
not require extensive manual involvement. Our
goal is to create stories automatically by leverag-
ing knowledge inherent in corpora. Stories within
the same genre (e.g., fairy tales, parables) typically
have similar structure, characters, events, and vo-
cabularies. It is precisely this type of information
we wish to extract and quantify. Of course, build-
ing a database of characters and their actions is
merely the first step towards creating an automatic
story generator. The latter must be able to select
which information to include in the story, in what
order to present it, how to convert it into English.
Recent work in natural language generation has
seen the development of learning methods for re-
alizing each of these tasks automatically with-
out much hand coding. For example, Duboue and
McKeown (2002) and Barzilay and Lapata (2005)
propose to learn a content planner from a paral-
lel corpus. Mellish et al (1998) advocate stochas-
tic search methods for document structuring. Stent
et al (2004) learn how to combine the syntactic
structure of elementary speech acts into one or
more sentences from a corpus of good and bad ex-
amples. And Knight and Hatzivassiloglou (1995)
use a language model for selecting a fluent sen-
tence among the vast number of surface realiza-
tions corresponding to a single semantic represen-
tation. Although successful on their own, these
methods have not been yet integrated together into
an end-to-end probabilistic system. Our work at-
tempts to do this for the story generation task,
while bridging the gap between story generators
and NLG systems.
Our generator operates over predicate-argument
and predicate-predicate co-occurrence statistics
gathered from corpora. These are used to pro-
duce a large set of candidate stories which are
subsequently ranked based on their interesting-
ness and coherence. The top-ranked candidate
is selected for presentation and verbalized us-
ing a language model interfaced with RealPro
(Lavoie and Rambow, 1997), a text generation
engine. This generate-and-rank architecture cir-
cumvents the complexity of traditional generation
This is a fat hen.
The hen has a nest in the box.
She has eggs in the nest.
A cat sees the nest, and can get the eggs.
The sun will soon set.
The cows are on their way to the barn.
One old cow has a bell on her neck.
She sees the dog, but she will not run.
The dog is kind to the cows.
Figure 1: Children?s stories from McGuffey?s
Eclectic Primer Reader; it contains primary read-
ing matter to be used in the first year of school
work.
systems, where numerous, often conflicting con-
straints, have to be encoded during development
in order to produce a single high-quality output.
As a proof of concept we initially focus on
children?s stories (see Figure 1 for an example).
These stories exhibit several recurrent patterns and
are thus amenable to a data-driven approach. Al-
though they have limited vocabulary and non-
elaborate syntax, they nevertheless present chal-
lenges at almost all stages of the generation pro-
cess. Also from a practical point of view, chil-
dren?s stories have great potential for educational
applications (Robertson and Good, 2003). For in-
stance, the system we describe could serve as an
assistant to a person who wants suggestions as to
what could happen next in a story. In the remain-
der of this paper, we first describe the components
of our story generator (Section 2) and explain how
these are interfaced with our story ranker (Sec-
tion 3). Next, we present the resources and evalu-
ation methodology used in our experiments (Sec-
tion 4) and discuss our results (Section 5).
2 The Story Generator
As common in previous work (e.g., Shim and Kim
2002), we assume that our generator operates in an
interactive context. Specifically, the user supplies
the topic of the story and its desired length. By
topic we mean the entities (or characters) around
which the story will revolve. These can be a list
of nouns such as dog and duck or a sentence, such
as the dog chases the duck. The generator next
constructs several possible stories involving these
entities by consulting a knowledge base containing
information about dogs and ducks (e.g., dogs bark,
ducks swim) and their interactions (e.g., dogs
chase ducks, ducks love dogs). We conceptualize
218
the dog chases the duck
the dog barks the duck runs away
the dog catches the duck the duck escapes
Figure 2: Example of a simplified story tree.
the story generation process as a tree (see Figure 2)
whose levels represent different story lengths. For
example, a tree of depth 3 will only generate sto-
ries with three sentences. The tree encodes many
stories efficiently, the nodes correspond to differ-
ent sentences and there is no sibling order (the
tree in Figure 2 can generate three stories). Each
sentence in the tree has a score. Story generation
amounts to traversing the tree and selecting the
nodes with the highest score
Specifically, our story generator applies two
distinct search procedures. Although we are ul-
timately searching for the best overall story at
the document level, we must also find the most
suitable sentences that can be generated from the
knowledge base (see Figure 4). The space of pos-
sible stories can increase dramatically depending
on the size of the knowledge base so that an ex-
haustive tree search becomes computationally pro-
hibitive. Fortunately, we can use beam search to
prune low-scoring sentences and the stories they
generate. For example, we may prefer sentences
describing actions that are common for their char-
acters. We also apply two additional criteria in se-
lecting good stories, namely whether they are co-
herent and interesting. At each depth in the tree
we maintain the N-best stories. Once we reach the
required length, the highest scoring story is pre-
sented to the user. In the following we describe
the components of our system in more detail.
2.1 Content Planning
As mentioned earlier our generator has access to
a knowledge base recording entities and their in-
teractions. These are essentially predicate argu-
ment structures extracted from a corpus. In our ex-
periments this knowledge base was created using
the RASP relational parser (Briscoe and Carroll,
2002). We collected all verb-subject, verb-object,
verb-adverb, and noun-adjective relations from the
parser?s output and scored them with the mutual
dog:SUBJ:bark whistle:OBJ:dog
dog:SUBJ:bite treat:OBJ:dog
dog:SUBJ:see give:OBJ:dog
dog:SUBJ:like have: OBJ:dog
hungry:ADJ:dog lovely:ADJ:dog
Table 1: Relations for the noun dog with high
MI scores (SUBJ is a shorthand for subject-of,
OBJ for object-of and ADJ for adjective-of).
information-based metric proposed in Lin (1998):
MI = ln
(
? w,r,w? ? ? ? ?,r,? ?
? w,r,? ? ? ? ?,r,w? ?
)
(1)
where w and w? are two words with relation type r.
? denotes all words in that particular relation and
? w,r,w? ? represents the number of times w,r,w?
occurred in the corpus. These MI scores are used
to inform the generation system about likely entity
relationships at the sentence level. Table 1 shows
high scoring relations for the noun dog extracted
from the corpus used in our experiments (see Sec-
tion 4 for details).
Note that MI weighs binary relations which in
some cases may be likely on their own without
making sense in a ternary relation. For instance, al-
though both dog:SUBJ:run and president:OBJ:run
are probable we may not want to create the sen-
tence ?The dog runs for president?. Ditransitive
verbs pose a similar problem, where two incongru-
ent objects may appear together (the sentence John
gives an apple to the highway is semantically odd,
whereas John gives an apple to the teacher would
be fine). To help reduce these problems, we need
to estimate the likelihood of ternary relations. We
therefore calculate the conditional probability:
p(a1,a2 | s,v) =
? s,v,a1,a2 ?
? s,v,?,? ?
(2)
where s is the subject of verb v, a1 is the first argu-
ment of v and a2 is the second argument of v and
v,s,a1 6= ?. When a verb takes two arguments, we
first consult (2), to see if the combination is likely
before backing off to (1).
The knowledge base described above can only
inform the generation system about relationships
on the sentence level. However, a story created
simply by concatenating sentences in isolation
will often be incoherent. Investigations into the
interpretation of narrative discourse (Asher and
Lascarides, 2003) have shown that lexical infor-
mation plays an important role in determining
219
SUBJ:chase
OBJ:chase
SUBJ:run
SUBJ:escape
SUBJ:fall
OBJ:catch SUBJ:frighten
SUBJ:jump
1 2
2
6
5
8
1
5
Figure 3: Graph encoding (partially ordered)
chains of events
the discourse relations between propositions. Al-
though we don?t have an explicit model of rhetor-
ical relations and their effects on sentence order-
ing, we capture the lexical inter-dependencies be-
tween sentences by focusing on events (verbs)
and their precedence relationships in the corpus.
For every entity in our training corpus we extract
event chains similar to those proposed by Cham-
bers and Jurafsky (2008). Specifically, we identify
the events every entity relates to and record their
(partial) order. We assume that verbs sharing the
same arguments are more likely to be semantically
related than verbs with no arguments in common.
For example, if we know that someone steals and
then runs, we may expect the next action to be that
they hide or that they are caught.
In order to track entities and their associated
events throughout a text, we first resolve entity
mentions using OpenNLP2. The list of events per-
formed by co-referring entities and their gram-
matical relation (i.e., subject or object) are sub-
sequently stored in a graph. The edges between
event nodes are scored using the MI equation
given in (1). A fragment of the action graph
is shown in Figure 3 (for simplicity, the edges
in the example are weighted with co-occurrence
frequencies). Contrary to Chambers and Juraf-
sky (2008) we do not learn global narrative
chains over an entire corpus. Currently, we con-
sider local chains of length two and three (i.e.,
chains of two or three events sharing gram-
matical arguments). The generator consults the
graph when selecting a verb for an entity. It
will favor verbs that are part of an event chain
(e.g., SUBJ:chase ? SUBJ:run ? SUBJ:fall in
Figure 3). This way, the search space is effectively
pruned as finding a suitable verb in the current sen-
tence is influenced by the choice of verb in the next
sentence.
2See http://opennlp.sourceforge.net/.
2.2 Sentence Planning
So far we have described how we gather knowl-
edge about entities and their interactions, which
must be subsequently combined into a sentence.
The backbone of our sentence planner is a gram-
mar with subcategorization information which we
collected from the lexicon created by Korhonen
and Briscoe (2006) and the COMLEX dictionary
(Grishman et al, 1994). The grammar rules act
as templates. They each take a verb as their head
and propose ways of filling its argument slots. This
means that when generating a story, the choice of
verb will affect the structure of the sentence. The
subcategorization templates are weighted by their
probability of occurrence in the reference dictio-
naries. This allows the system to prefer less elab-
orate grammatical structures. The grammar rules
were converted to a format compatible with our
surface realizer (see Section 2.3) and include in-
formation pertaining to mood, agreement, argu-
ment role, etc.
Our sentence planner aggregates together infor-
mation from the knowledge base, without how-
ever generating referring expressions. Although
this would be a natural extension, we initially
wanted to assess whether the stochastic approach
advocated here is feasible at all, before venturing
towards more ambitious components.
2.3 Surface Realization
The surface realization process is performed by
RealPro (Lavoie and Rambow (1997)). The sys-
tem takes an abstract sentence representation and
transforms it into English. There are several gram-
matical issues that will affect the final realization
of the sentence. For nouns we must decide whether
they are singular or plural, whether they are pre-
ceded by a definite or indefinite article or with no
article at all. Adverbs can either be pre-verbal or
post-verbal. There is also the issue of selecting
an appropriate tense for our generated sentences,
however, we simply assume all sentences are in
the present tense. Since we do not know a priori
which of these parameters will result in a gram-
matical sentence, we generate all possible combi-
nations and select the most likely one according to
a language model. We used the SRI toolkit to train
a trigram language model on the British National
Corpus, with interpolated Kneser-Ney smoothing
and perplexity as the scoring metric for the gener-
ated sentences.
220
root
dog
. . . bark
bark(dog) bark at(dog,OBJ)
bark at(dog,duck) bark at(dog,cat)
bark(dog,ADV)
bark(dog,loudly)
hide run
duck
quack
. . .
run
. . .
fly
. . .
Figure 4: Simplified generation example for the in-
put sentence the dog chases the duck.
2.4 Sentence Generation Example
It is best to illustrate the generation procedure with
a simple example (see Figure 4). Given the sen-
tence the dog chases the duck as input, our gen-
erator assumes that either dog or duck will be the
subject of the following sentence. This is a some-
what simplistic attempt at generating coherent sto-
ries. Centering (Grosz et al, 1995) and other dis-
course theories argue that topical entities are likely
to appear in prominent syntactic positions such as
subject or object. Next, we select verbs from the
knowledge base that take the words duck and dog
as their subject (e.g., bark, run, fly). Our beam
search procedure will reduce the list of verbs to
a small subset by giving preference to those that
are likely to follow chase and have duck and dog
as their subjects or objects.
The sentence planner gives a set of possible
frames for these verbs which may introduce ad-
ditional entities (see Figure 4). For example, bark
can be intransitive or take an object or adver-
bial complement. We select an object for bark,
by retrieving from the knowledge base the set
of objects it co-occurs with. Our surface real-
izer will take structures like ?bark(dog,loudly)?,
?bark at(dog,cat)?, ?bark at(dog,duck)? and gen-
erate the sentences the dog barks loudly, the dog
barks at the cat and the dog barks at the duck. This
procedure is repeated to create a list of possible
candidates for the third sentence, and so on.
As Figure 4 illustrates, there are many candidate
sentences for each entity. In default of generating
all of these exhaustively, our system utilizes the
MI scores from the knowledge base to guide the
search. So, at each choice point in the generation
process, e.g., when selecting a verb for an entity or
a frame for a verb, we consider the N best alterna-
tives assuming that these are most likely to appear
in a good story.
3 Story Ranking
We have so far described most modules of our
story generator, save one important component,
namely the story ranker. As explained earlier, our
generator produces stories stochastically, by rely-
ing on co-occurrence frequencies collected from
the training corpus. However, there is no guaran-
tee that these stories will be interesting or coher-
ent. Engaging stories have some element of sur-
prise and originality in them (Turner, 1994). Our
stories may simply contain a list of actions typi-
cally performed by the story characters. Or in the
worst case, actions that make no sense when col-
lated together.
Ideally, we would like to be able to discern in-
teresting stories from tedious ones. Another im-
portant consideration is their coherence. We have
to ensure that the discourse smoothly transitions
from one topic to the next. To remedy this, we
developed two ranking functions that assess the
candidate stories based on their interest and coher-
ence. Following previous work (Stent et al, 2004;
Barzilay and Lapata, 2007) we learn these ranking
functions from training data (i.e., stories labeled
with numeric values for interestingness and coher-
ence).
Interest Model A stumbling block to assessing
how interesting a story may be, is that the very no-
tion of interestingness is subjective and not very
well understood. Although people can judge fairly
reliably whether they like or dislike a story, they
have more difficulty isolating what exactly makes
it interesting. Furthermore, there are virtually no
empirical studies investigating the linguistic (sur-
face level) correlates of interestingness. We there-
fore conducted an experiment where we asked par-
ticipants to rate a set of human authored stories in
terms of interest. Our stories were Aesop?s fables
since they resemble the stories we wish to gener-
ate. They are fairly short (average length was 3.7
sentences) and with a few characters. We asked
participants to judge 40 fables on a set of crite-
ria: plot, events, characters, coherence and interest
(using a 5-point rating scale). The fables were split
into 5 sets of 8; each participant was randomly as-
signed one of the 5 sets to judge. We obtained rat-
221
ings (440 in total) from 55 participants, using the
WebExp3 experimental software.
We next investigated if easily observable syn-
tactic and lexical features were correlated with in-
terest. Participants gave the fables an average in-
terest rating of 3.05. For each story we extracted
the number of tokens and types for nouns, verbs,
adverbs and adjectives as well as the number
of verb-subject and verb-object relations. Using
the MRC Psycholinguistic database4 tokens were
also annotated along the following dimensions:
number of letters (NLET), number of phonemes
(NPHON), number of syllables (NSYL), written
frequency in the Brown corpus (Kucera and Fran-
cis 1967; K-F-FREQ), number of categories in the
Brown corpus (K-F-NCATS), number of samples
in the Brown corpus (K-F-NSAMP), familiarity
(FAM), concreteness (CONC), imagery (IMAG),
age of acquisition (AOA), and meaningfulness
(MEANC and MEANP).
Correlation analysis was used to assess the de-
gree of linear relationship between interest ratings
and the above features. The results are shown in
Table 2. As can be seen the highest predictor is the
number of objects in a story, followed by the num-
ber of noun tokens and types. Imagery, concrete-
ness and familiarity all seem to be significantly
correlated with interest. Story length was not a
significant predictor. Regressing the best predic-
tors from Table 2 against the interest ratings yields
a correlation coefficient of 0.608 (p < 0.05). The
predictors account uniquely for 37.2% of the vari-
ance in interest ratings. Overall, these results indi-
cate that a model of story interest can be trained
using shallow syntactic and lexical features. We
used the Aesop?s fables with the human ratings as
training data fromwhich we extracted features that
shown to be significant predictors in our correla-
tion analysis. Word-based features were summed
in order to obtain a representation for the en-
tire story. We used Joachims?s (2002) SVMlight
package for training with cross-validation (all pa-
rameters set to their default values). The model
achieved a correlation of 0.948 (Kendall?s tau)
with the human ratings on the test set.
Coherence Model As well as being interesting
we have to ensure that our stories make sense
to the reader. Here, we focus on local coher-
ence, which captures text organization at the level
3See http://www.webexp.info/.
4http://www.psy.uwa.edu.au/mrcdatabase/uwa_
mrc.htm
Interest Interest
NTokens 0.188?? NLET 0.120?
NTypes 0.173?? NPHON 0.140??
VTokens 0.123? NSYL 0.125??
VTypes 0.154?? K-F-FREQ 0.054
AdvTokens 0.056 K-F-NCATS 0.137??
AdvTypes 0.051 K-F-NSAMP 0.103?
AdjTokens 0.035 FAM 0.162??
AdjTypes 0.029 CONC 0.166??
NumSubj 0.150?? IMAG 0.173??
NumObj 0.240?? AOA 0.111?
MEANC 0.169?? MEANP 0.156??
Table 2: Correlation values for the human ratings
of interest against syntactic and lexical features;
? : p < 0.05, ?? : p < 0.01.
of sentence to sentence transitions. We created a
model of local coherence using using the Entity
Grid approach described in Barzilay and Lapata
(2007). This approach represents each document
as a two-dimensional array in which the columns
correspond to entities and the rows to sentences.
Each cell indicates whether an entity appears in a
given sentence or not and whether it is a subject,
object or neither. This entity grid is then converted
into a vector of entity transition sequences. Train-
ing the model required examples of both coher-
ent and incoherent stories. An artificial training set
was created by permuting the sentences of coher-
ent stories, under the assumption that the original
story is more coherent than its permutations. The
model was trained and tested on the Andrew Lang
fairy tales collection5 on a random split of the data.
It ranked the original stories higher than their cor-
responding permutations 67.40% of the time.
4 Experimental Setup
In this section we present our experimental set-up
for assessing the performance of our story genera-
tor. We give details on our training corpus, system,
parameters (such as the width of the beam), the
baselines used for comparison, and explain how
our system output was evaluated.
Corpus The generator was trained on 437 sto-
ries from the Andrew Lang fairy tale corpus.6 The
stories had an average length of 125.18 sentences.
The corpus contained 15,789 word tokens. We
5Aesop?s fables were too short to learn a coherence
model.
6See http://www.mythfolklore.net/andrewlang/.
222
discarded word tokens that did not appear in the
Children?s Printed Word Database7, a database of
printed word frequencies as read by children aged
between five and nine.
Story search When searching the story space,
we set the beam width to 500. This means that
we allow only 500 sentences to be considered at
a particular depth before generating the next set of
sentences in the story. For each entity we select the
five most likely events and event sequences. Anal-
ogously, we consider the five most likely subcate-
gorization templates for each verb. Considerable
latitude is available when applying the ranking
functions. We may use only one of them, or one
after the other, or both of them. To evaluate which
system configuration was best, we asked two hu-
man evaluators to rate (on a 1?5 scale) stories pro-
duced in the following conditions: (a) score the
candidate stories using the interest function first
and then coherence (and vice versa), (b) score the
stories simultaneously using both rankers and se-
lect the story with the highest score. We also ex-
amined how best to prune the search space, i.e., by
selecting the highest scoring stories, the lowest
scoring one, or simply at random. We created ten
stories of length five using the fairy tale corpus for
each permutation of the parameters. The results
showed that the evaluators preferred the version
of the system that applied both rankers simultane-
ously and maintained the highest scoring stories in
the beam.
Baselines We compared our system against two
simpler alternatives. The first one does not use
a beam. Instead, it decides deterministically how
to generate a story on the basis of the most
likely predicate-argument and predicate-predicate
counts in the knowledge base. The second one
creates a story randomly without taking any co-
occurrence frequency into account. Neither of
these systems therefore creates more than one
story hypothesis whilst generating.
Evaluation The system generated stories for
10 input sentences. These were created using com-
monly occurring sentences in the fairy tales corpus
(e.g., The family has the baby, The monkey climbs
the tree, The giant guards the child). Each sys-
tem generated one story for each sentence result-
ing in 30 (3?10) stories for evaluation. All sto-
ries had the same length, namely five sentences.
Human judges (21 in total) were asked to rate the
7http://www.essex.ac.uk/psychology/cpwd/
System Fluency Coherence Interest
Random 1.95? 2.40? 2.09?
Deterministic 2.06? 2.53? 2.09?
Rank-based 2.20 2.65 2.20
Table 3: Human evaluation results: mean story rat-
ings for three versions of our system; ?: signifi-
cantly different from Rank-based.
stories on a scale of 1 to 5 for fluency (was the
sentence grammatical?), coherence (does the story
make sense overall?) and interest (how interesting
is the story?). The stories were presented in ran-
dom order. Participants were told that all stories
were generated by a computer program. They were
instructed to rate more favorably interesting sto-
ries, stories that were comprehensible and overall
grammatical.
5 Results
Our results are summarized in Table 3 which lists
the average human ratings for the three systems.
We performed an Analysis of Variance (ANOVA)
to examine the effect of system type on the story
generation task. Statistical tests were carried out
on the mean of the ratings shown in Table 3 for
fluency, coherence, and interest. We observed a re-
liable effect of system type by subjects and items
on all three dimensions. Post-hoc Tukey tests re-
vealed that the stories created with our rank-
based system are perceived as significantly better
in terms of fluency, interest, and coherence than
those generated by both the deterministic and ran-
dom systems (? < 0.05). The deterministic system
is not significantly better than the random one ex-
cept in terms of coherence.
These results are not entirely surprising. The
deterministic system maintains a local restricted
view of what constitutes a good story. It creates
a story by selecting isolated entity-event relation-
ships with high MI scores. As a result, the stories
are unlikely to have a good plot. Moreover, it tends
to primarily favor verb-object or verb-subject re-
lations, since these are most frequent in the cor-
pus. The stories thus have little structural varia-
tion and feel repetitive. The random system uses
even less information in generating a story (entity-
action relationships are chosen at random without
taking note of the MI scores). In contrast to these
baselines, the rank-based system assesses candi-
date stories more globally. It thus favors coher-
ent stories, with varied word choice and structure.
223
The family has the baby The giant guards the child
R
an
do
m The family has the baby. The family is how to emptyup to a fault. The baby vanishes into the cave. The fam-
ily meets with a stranger. The baby says for the boy to
fancy the creature.
The giant guards the child. The child calls for the
window to order the giant. The child suffers from a
pleasure. The child longer hides the forest. The child
reaches presently.
D
et
er
m The family has the baby. The family rounds up the
waist. The family comes in. The family wonders. The
family meets with the terrace.
The giant guards the child. The child rescues the clutch.
The child beats down on a drum. The child feels out of
a shock. The child hears from the giant.
R
an
k-
ba
se
d The family has the baby. The baby is to seat the lady at
the back. The baby sees the lady in the family. The fam-
ily marries a lady for the triumph. The family quickly
wishes the lady vanishes.
The giant guards the child. The child rescues the son
from the power. The child begs the son for a pardon.
The giant cries that the son laughs the happiness out of
death. The child hears if the happiness tells a story.
Table 4: Stories generated by the random, deterministic, and rank-based systems.
A note of caution here concerns referring expres-
sions which our systems cannot at the moment
generate. This may have disadvantaged the stories
overall, rendering them stylistically awkward.
The stories generated by both the determinis-
tic and random systems are perceived as less in-
teresting in comparison to the rank-based system.
This indicates that taking interest into account is a
promising direction even though the overall inter-
estingness of the stories we generate is somewhat
low (see third column in Table 3). Our interest
ranking function was trained on well-formed hu-
man authored stories. It is therefore possible that
the ranker was not as effective as it could be sim-
ply because it was applied to out-of-domain data.
An interesting extension which we plan for the
future is to evaluate the performance of a ranker
trained on machine generated stories.
Table 4 illustrates the stories generated by each
system for two input sentences. The rank-based
stories read better overall and are more coherent.
Our subjects also gave them high interest scores.
The deterministic system tends to select simplis-
tic sentences which although read well by them-
selves do not lead to an overall narrative. Interest-
ingly, the story generated by the random system
for the input The family has the baby, scored high
on interest too. The story indeed contains interest-
ing imagery (e.g. The baby vanishes into the cave)
although some of the sentences are syntactically
odd (e.g. The family is how to empty up to a fault).
6 Conclusions and Future Work
In this paper we proposed a novel method to
computational story telling. Our approach has
three key features. Firstly, story plot is created
dynamically by consulting an automatically cre-
ated knowledge base. Secondly, our generator re-
alizes the various components of the generation
pipeline stochastically, without extensive manual
coding. Thirdly, we generate and store multiple
stories efficiently in a tree data structure. Story
creation amounts to traversing the tree and select-
ing the nodes with the highest score. We develop
two scoring functions that rate stories in terms
of how coherent and interesting they are. Experi-
mental results show that these bring improvements
over versions of the system that rely solely on
the knowledge base. Overall, our results indicate
that the overgeneration-and-ranking approach ad-
vocated here is viable in producing short stories
that exhibit narrative structure. As our system can
be easily rertrained on different corpora, it can po-
tentially generate stories that vary in vocabulary,
style, genre, and domain.
An important future direction concerns a more
detailed assessment of our search procedure. Cur-
rently we don?t have a good estimate of the type of
stories being overlooked due to the restrictions we
impose on the search space. An appealing alterna-
tive is the use of Genetic Algorithms (Goldberg,
1989). The operations of mutation and crossover
have the potential of creating more varied and
original stories. Our generator would also bene-
fit from an explicit model of causality which is
currently approximated by the entity chains. Such
a model could be created from existing resources
such as ConceptNet (Liu and Davenport, 2004),
a freely available commonsense knowledge base.
Finally, improvements such as the generation of
referring expressions and the modeling of selec-
tional restrictions would create more fluent stories.
Acknowledgements The authors acknowledge
the support of EPSRC (grant GR/T04540/01).
We are grateful to Richard Kittredge for his help
with RealPro. Special thanks to Johanna Moore
for insightful comments and suggestions.
224
References
Asher, Nicholas and Alex Lascarides. 2003. Logics of Con-
versation. Cambridge University Press.
Barzilay, Regina and Mirella Lapata. 2005. Collective con-
tent selection for concept-to-text generation. In Proceed-
ings of the HLT/EMNLP. Vancouver, pages 331?338.
Barzilay, Regina and Mirella Lapata. 2007. Modeling local
coherence: An entity-based approach. Computational Lin-
guistics 34(1):1?34.
Briscoe, E. and J. Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Callaway, Charles B. and James C. Lester. 2002. Narrative
prose generation. Artificial Intelligence 2(139):213?252.
Chambers, Nathanael and Dan Jurafsky. 2008. Unsupervised
learning of narrative event chains. In Proceedings of ACL-
08: HLT . Columbus, OH, pages 789?797.
Duboue, Pablo A. and Kathleen R. McKeown. 2002. Con-
tent planner construction via evolutionary algorithms and
a corpus-based fitness function. In Proceedings of the 2nd
INLG. Ramapo Mountains, NY.
Fass, S. 2002. Virtual Storyteller: An Approach to Compu-
tational Storytelling. Master?s thesis, Dept. of Computer
Science, University of Twente.
Goldberg, David E. 1989. Genetic Algorithms in Search, Op-
timization and Machine Learning. Addison-Wesley Long-
man Publishing Co., Inc., Boston, MA.
Grishman, Ralph, Catherine Macleod, and Adam Meyers.
1994. COMLEX syntax: Building a computational lexi-
con. In Proceedings of the 15th COLING. Kyoto, Japan,
pages 268?272.
Grosz, Barbara J., Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguistics
21(2):203?225.
Joachims, Thorsten. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the 8th ACM
SIGKDD. Edmonton, AL, pages 133?142.
Knight, Kevin and Vasileios Hatzivassiloglou. 1995. Two-
level, many-paths generation. In Proceedings of the 33rd
ACL. Cambridge, MA, pages 252?260.
Korhonen, Y. Krymolowski, A. and E.J. Briscoe. 2006. A
large subcategorization lexicon for natural language pro-
cessing applications. In Proceedings of the 5th LREC.
Genova, Italy.
Kucera, Henry and Nelson Francis. 1967. Computational
Analysis of Present-day American English. Brown Uni-
versity Press, Providence, RI.
Lavoie, Benoit and Owen Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings of the
5th ANCL. Washington, D.C., pages 265?268.
Lin, Dekang. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th COLING. Montre?al,
QC, pages 768?774.
Liu, Hugo and Glorianna Davenport. 2004. ConceptNet: a
practical commonsense reasoning toolkit. BT Technology
Journal 22(4):211?226.
Mellish, Chris, Alisdair Knott, Jon Oberlander, and Mick
O?Donnell. 1998. Experiments using stochastic search for
text planning. In Eduard Hovy, editor, Proceedings of the
9th INLG. New Brunswick, NJ, pages 98?107.
Oinonen, K.M., M. Theune, A. Nijholt, and J.R.R. Uijlings.
2006. Designing a story database for use in automatic
story generation. In R. Harper, M. Rauterberg, and
M. Combetto, editors, Entertainment Computing ? ICEC
2006. Springer Verlag, Berlin, volume 4161 of Lecture
Notes in Computer Science, pages 298?301.
Propp, Vladimir. 1968. The Morphology of Folk Tale. Uni-
versity of Texas Press, Austin, TX.
Reiter, E and R Dale. 2000. Building Natural-Language Gen-
eration Systems. Cambridge University Press.
Robertson, Judy and Judith Good. 2003. Ghostwriter: A nar-
rative virtual environment for children. In Proceedings of
IDC2003. Preston, England, pages 85?91.
Shim, Yunju and Minkoo Kim. 2002. Automatic short story
generator based on autonomous agents. In Proceedings of
PRIMA. London, UK, pages 151?162.
Stent, Amanda, Rashmi Prasad, and Marilyn Walker. 2004.
Trainable sentence planning for complex information pre-
sentation in spoken dialog systems. In Proceedings of the
42nd ACL. Barcelona, Spain, pages 79?86.
Theune, M., S. Faas, D.K.J. Heylen, and A. Nijholt. 2003.
The virtual storyteller: Story creation by intelligent agents.
In S. Gbel, N. Braun, U. Spierling, J. Dechau, and H. Di-
ener, editors, TIDSE-2003. Fraunhofer IRB Verlag, Darm-
stadt, pages 204?215.
Thorndyke, Perry W. 1977. Cognitive structures in compre-
hension and memory of narrative discourse. Cognitive
Psychology 9(1):77?110.
Turner, Scott T. 1994. The creative process: A computer
model of storytelling and creativity. Erlbaum, Hillsdale,
NJ.
225
Using the Web to Overcome Data Sparseness
Frank Keller and Maria Lapata
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
fkeller, mlapg@cogsci.ed.ac.uk
Olga Ourioupina
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
ourioupi@coli.uni-sb.de
Abstract
This paper shows that the web can be em-
ployed to obtain frequencies for bigrams
that are unseen in a given corpus. We
describe a method for retrieving counts
for adjective-noun, noun-noun, and verb-
object bigrams from the web by querying
a search engine. We evaluate this method
by demonstrating that web frequencies
and correlate with frequencies obtained
from a carefully edited, balanced corpus.
We also perform a task-based evaluation,
showing that web frequencies can reliably
predict human plausibility judgments.
1 Introduction
In two recent papers, Banko and Brill (2001a;
2001b) criticize the fact that current NLP algo-
rithms are typically optimized, tested, and compared
on fairly small data sets (corpora with millions of
words), even though data sets several orders of mag-
nitude larger are available, at least for some tasks.
Banko and Brill go on to demonstrate that learning
algorithms typically used for NLP tasks benefit sig-
nificantly from larger training sets, and their perfor-
mance shows no sign of reaching an asymptote as
the size of the training set increases.
Arguably, the largest data set that is available
for NLP is the web, which currently consists of
at least 968 million pages.1 Data retrieved from
the web therefore provides enormous potential
1This is the number of pages indexed by Google in
March 2002, as estimated by Search Engine Showdown (see
http://www.searchengineshowdown.com/).
for training NLP algorithms, if Banko and Brill?s
findings generalize. There is a small body of
existing research that tries to harness the potential
of the web for NLP. Grefenstette and Nioche (2000)
and Jones and Ghani (2000) use the web to
generate corpora for languages where elec-
tronic resources are scarce, while Resnik (1999)
describes a method for mining the web for bilin-
gual texts. Mihalcea and Moldovan (1999) and
Agirre and Martinez (2000) use the web for word
sense disambiguation, and Volk (2001) proposes a
method for resolving PP attachment ambiguities
based on web data.
A particularly interesting application is pro-
posed by Grefenstette (1998), who uses the web
for example-based machine translation. His task is
to translate compounds from French into English,
with corpus evidence serving as a filter for candi-
date translations. As an example consider the French
compound groupe de travail. There are five transla-
tion of groupe and three translations for travail (in
the dictionary that Grefenstette (1998) is using), re-
sulting in 15 possible candidate translations. Only
one of them, viz., work group has a high corpus
frequency, which makes it likely that this is the
correct translation into English. Grefenstette (1998)
observes that this approach suffers from an acute
data sparseness problem if the corpus counts are
obtained from a conventional corpus such as the
British National Corpus (BNC) (Burnard, 1995).
However, as Grefenstette (1998) demonstrates, this
problem can be overcome by obtaining counts
through web searches, instead of relying on the
BNC. Grefenstette (1998) therefore effectively uses
the web as a way of obtaining counts for compounds
that are sparse in the BNC.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 230-237.
                         Proceedings of the Conference on Empirical Methods in Natural
While this is an important initial result, it raises
the question of the generality of the proposed ap-
proach to overcoming data sparseness. It remains
to be shown that web counts are generally useful
for approximating data that is sparse or unseen in
a given corpus. It seems possible, for instance, that
Grefenstette?s (1998) results are limited to his par-
ticular task (filtering potential translations) or to his
particular linguistic phenomenon (noun-noun com-
pounds). Another potential problem is the fact that
web counts are far more noisy than counts obtained
from a well-edited, carefully balanced corpus such
as the BNC. The effect of this noise on the useful-
ness of the web counts is largely unexplored.
The aim of the present paper is to generalize
Grefenstette?s (1998) findings by testing the hypoth-
esis that the web can be employed to obtain frequen-
cies for bigrams that are unseen in a given corpus.
Instead of having a particular task in mind (which
would introduce a sampling bias), we rely on sets of
bigrams that are randomly selected from the corpus.
We use a web-based approach not only for noun-
noun bigrams, but also for adjective-noun and verb-
object bigrams, so as to explore whether this ap-
proach generalizes to different predicate-argument
combinations. We evaluate our web counts in two
different ways: (a) comparison with actual corpus
frequencies, and (b) task-based evaluation (predict-
ing human plausibility judgments).
2 Obtaining Frequencies from the Web
2.1 Sampling Bigrams
Two types of adjective-noun bigrams were used in
the present study: seen bigrams, i.e., bigrams that
occur in a given corpus, and unseen bigrams, i.e.,
bigrams that fail to occur in the corpus. For the
seen adjective-noun bigrams, we used the data of
Lapata et al (1999), who compiled a set of 90 bi-
grams as follows. First, 30 adjectives were randomly
chosen from a lemmatized version of the BNC so
that each adjective had exactly two senses accord-
ing to WordNet (Miller et al, 1990) and was unam-
biguously tagged as ?adjective? 98.6% of the time.
The 30 adjectives ranged in BNC frequency from 1.9
to 49.1 per million. Gsearch (Corley et al, 2001),
a chart parser which detects syntactic patterns in a
tagged corpus by exploiting a user-specified con-
text free grammar and a syntactic query, was used
to extract all nouns occurring in a head-modifier re-
lationship with one of the 30 adjectives. Bigrams in-
volving proper nouns or low-frequency nouns (less
than 10 per million) were discarded. For each ad-
jective, the set of bigrams was divided into three fre-
quency bands based on an equal division of the range
of log-transformed co-occurrence frequencies. Then
one bigram was chosen at random from each band.
Lapata et al (2001) compiled a set of 90 unseen
adjective-noun bigrams using the same 30 adjec-
tives. For each adjective, the Gsearch chunker was
used to compile a list of all nouns that failed to co-
occur in a head-modifier relationship with the adjec-
tive. Proper nouns and low-frequency nouns were
discarded from this list. Then each adjective was
paired with three randomly chosen nouns from its
list of non-co-occurring nouns.
For the present study, we applied the procedure
used by Lapata et al (1999) and Lapata et al (2001)
to noun-noun bigrams and to verb-object bigrams,
creating a set of 90 seen and 90 unseen bigrams for
each type of predicate-argument relationship. More
specifically, 30 nouns and 30 verbs were chosen ac-
cording to the same criteria proposed for the adjec-
tive study (i.e., minimal sense ambiguity and unam-
biguous part of speech). All nouns modifying one of
the 30 nouns were extracted from the BNC using a
heuristic which looks for consecutive pairs of nouns
that are neither preceded nor succeeded by another
noun (Lauer, 1995). Verb-object bigrams for the
30 preselected verbs were obtained from the BNC
using Cass (Abney, 1996), a robust chunk parser de-
signed for the shallow analysis of noisy text. The
parser?s output was post-processed to remove brack-
eting errors and errors in identifying chunk cate-
gories that could potentially result in bigrams whose
members do not stand in a verb-argument relation-
ship (see Lapata (2001) for details on the filtering
process). Only nominal heads were retained from
the objects returned by the parser. As in the adjec-
tive study, noun-noun bigrams and verb-object bi-
grams with proper nouns or low-frequency nouns
(less than 10 per million) were discarded. The sets
of noun-noun and verb-object bigrams were divided
into three frequency bands and one bigram was cho-
sen at random from each band.
The procedure described by Lapata et al (2001)
was followed for creating sets of unseen noun-noun
and verb-object bigrams: for each of noun or verb,
we compiled a list of all nouns with which it failed
to co-occur with in a noun-noun or verb-object bi-
gram in the BNC. Again, Lauer?s (1995) heuristic
and Abney?s (1996) partial parser were used to iden-
tify bigrams, and proper nouns and low-frequency
nouns were excluded. For each noun and verb, three
bigrams were randomly selected from the set of their
non-co-occurring nouns.
Table 1 lists examples for the seen and unseen
noun-noun and verb-object bigrams generated by
this procedure.
2.2 Obtaining Web Counts
Web counts for bigrams were obtained using a sim-
ple heuristic based on queries to the search engines
Altavista and Google. All search terms took into
account the inflectional morphology of nouns and
verbs.
The search terms for verb-object bigrams matched
not only cases in which the object was directly ad-
jacent to the verb (e.g., fulfill obligation), but also
cases where there was an intervening determiner
(e.g., fulfill the/an obligation). The following search
terms were used for adjective-noun, noun-noun, and
verb-object bigrams, respectively:
(1) "A N", where A is the adjective and N is the sin-
gular or plural form of the noun.
(2) "N1 N2" where N1 is the singular form of the
first noun and N2 is the singular or plural form
of the second noun.
(3) "V Det N" where V is the infinitive, singular
present, plural present, past, perfect, or gerund
for of the verb, Det is the determiner the, a or
the empty string, and N is the singular or plural
form of the noun.
Note that all searches were for exact matches, which
means that the search terms were required to be di-
rectly adjacent on the matching page. This is en-
coded using quotation marks to enclose the search
term. All our search terms were in lower case.
For Google, the resulting bigram frequencies
were obtained by adding up the number of pages
that matched the expanded forms of the search terms
in (1), (2), and (3). Altavista returns not only the
number of matches, but also the number of words
adj-noun noun-noun verb-object
Altavista 14 10 16
Google 5 3 5
Table 2: Number of zero counts returned by the
queries to search engines (unseen bigrams)
that match the search term. We used this count, as it
takes multiple matches per page into account, and is
thus likely to produce more accurate frequencies.
The process of obtaining bigram frequencies from
the web can be automated straightforwardly using a
script that generates all the search terms for a given
bigram (from (1)?(3)), issues an Altavista or Google
query for each of the search terms, and then adds
up the resulting number of matches for each bigram.
We applied this process to all the bigrams in our data
set, covering seen and unseen adjective-noun, noun-
noun, and verb-object bigrams, i.e., 540 bigrams in
total.
A small number of bigrams resulted in zero
counts, i.e., they failed to yield any matches in the
web search. Table 2 lists the number of zero bigrams
for both search engines. Note that Google returned
fewer zeros than Altavista, which presumably indi-
cates that it indexes a larger proportion of the web.
We adjusted the zero counts by setting them to one.
This was necessary as all further analyses were car-
ried out on log-transformed frequencies.
Table 3 lists the descriptive statistics for the
bigram counts we obtained using Altavista and
Google.
From these data, we computed the average fac-
tor by which the web counts are larger than the
BNC counts. The results are given in Table 4 and
indicate that the Altavista counts are between 331
and 467 times larger than the BNC counts, while
the Google counts are between 759 and 977 times
larger than the BNC counts. As we know the size
of the BNC (100 million words), we can use these
figures to estimate the number of words on the web:
between 33.1 and 46.7 billion words for Altavista,
and between 75.9 and 97.7 billion words for Google.
These estimates are in the same order of magnitude
as Grefenstette and Nioche?s (2000) estimate that
48.1 billion words of English are available on the
web (based on Altavista counts in February 2000).
noun-noun bigrams
high medium low unseen predicate
process 1.14 user .95 gala 0 collection, clause, coat directory
television 1.53 satellite .95 edition 0 chain, care, vote broadcast
plasma 1.78 nylon 1.20 unit .60 fund, theology, minute membrane
verb-object bigrams
predicate high medium low unseen
fulfill obligation 3.87 goal 2.20 scripture .69 participant, muscle, grade
intensify problem 1.79 effect 1.10 alarm 0 score, quota, chest
choose name 3.74 law 1.61 series 1.10 lift, bride, listener
Table 1: Example stimuli for seen and unseen noun-noun and verb-object bigrams (with log-transformed
BNC counts)
seen bigrams
adj-noun noun-noun verb-object
Min Max Mean SD Min Max Mean SD Min Max Mean SD
Altavista 0 5.67 3.55 1.06 .67 6.28 3.41 1.21 0 5.46 3.20 1.14
Google 1.26 5.98 3.89 1.00 .90 6.11 3.66 1.20 0 5.85 3.56 1.16
BNC 0 2.19 .90 .69 0 2.14 .74 .64 0 2.55 .68 .58
unseen bigrams
adj-noun noun-noun verb-object
Min Max Mean SD Min Max Mean SD Min Max Mean SD
Altavista 0 4.04 1.29 .94 0 3.80 1.08 1.12 0 3.72 1.38 1.06
Google 0 3.99 1.68 .96 0 4.00 1.42 1.09 0 4.07 1.76 1.04
Table 3: Descriptive statistics for web counts and BNC counts (log-transformed)
adj-noun noun-noun verb-object
Altavista 447 467 331
Google 977 831 759
Table 4: Average factor by which the web counts are
larger than the BNC counts (seen bigrams)
3 Evaluation
3.1 Evaluation Against Corpus Frequencies
While the procedure for obtaining web counts de-
scribed in Section 2.2 is very straightforward, it also
has obvious limitations. Most importantly, it is based
on bigrams formed by adjacent words, and fails to
take syntactic variants into account (other than in-
tervening determiners for verb-object bigrams). In
the case of Google, there is also the problem that the
counts are based on the number of matching pages,
not the number of matching words. Finally, there is
the problem that web data is very noisy and unbal-
anced compared to a carefully edited corpus like the
BNC.
Given these limitations, it is necessary to explore
if there is a reliable relationship between web counts
and BNC counts. Once this is assured, we can ex-
plore the usefulness of web counts for overcoming
data sparseness. We carried out a correlation analy-
sis to determine if there is a linear relationship be-
tween the BNC counts and Altavista and Google
counts. The results of this analysis are listed in Ta-
ble 5. All correlation coefficients reported in this pa-
per refer to Pearson?s r and were computed on log-
transformed counts.
A high correlation coefficient was obtained across
the board, ranging from .675 to .822 for Altavista
counts and from .737 to .849 for Google counts.
This indicates that web counts approximate BNC
counts for the three types of bigrams under inves-
tigation, with Google counts slightly outperform-
ing Altavista counts. We conclude that our simple
adj-noun noun-noun verb-object
Altavista .821** .744** .675**
Google .849** .737** .751**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 5: Correlation of BNC counts with web counts
(seen bigrams)
heuristics (see (1)?(3)) are sufficient to obtain use-
ful frequencies from the web. It seems that the large
amount of data available for web counts outweighs
the associated problems (noisy, unbalanced, etc.).
Note that the highest coefficients were obtained
for adjective-noun bigrams, which probably indi-
cates that this type of predicate-argument relation-
ship is least subject to syntactic variation and thus
least affected by the simplifications of our search
heuristics.
3.2 Task-based Evaluation
Previous work has demonstrated that corpus counts
correlate with human plausibility judgments for
adjective-noun bigrams. This results holds for both
seen bigrams (Lapata et al, 1999) and for unseen
bigrams whose counts were recreated using smooth-
ing techniques (Lapata et al, 2001). Based on these
findings, we decided to evaluate our web counts on
the task of predicting plausibility ratings. If the web
counts for bigrams correlate with plausibility judg-
ments, then this indicates that the counts are valid,
in the sense of being useful for predicting intuitive
plausibility.
Lapata et al (1999) and Lapata et al (2001) col-
lected plausibility ratings for 90 seen and 90 unseen
adjective-noun bigrams (see Section 2.1) using mag-
nitude estimation. Magnitude estimation is an exper-
imental technique standardly used in psychophysics
to measure judgments of sensory stimuli (Stevens,
1975), which Bard et al (1996) and Cowart (1997)
have applied to the elicitation of linguistic judg-
ments. Magnitude estimation requires subjects to
assign numbers to a series of linguistic stimuli in
a proportional fashion. Subjects are first exposed
to a modulus item, which they assign an arbitrary
number. All other stimuli are rated proportional
to the modulus. In the experiments conducted by
Lapata et al (1999) and Lapata et al (2001), native
speakers of English were presented with adjective-
noun bigrams and were asked to rate the degree
of adjective-noun fit proportional to the modulus
item. The resulting judgments were normalized by
dividing them by the modulus value and by log-
transforming them. Lapata et al (1999) report a cor-
relation of .570 between mean plausibility judg-
ments and BNC counts for the seen adjective-
noun bigrams. For unseen adjective-noun bigrams,
Lapata et al (2001) found a correlation of .356 be-
tween mean judgments and frequencies recreated
using class-based smoothing (Resnik, 1993).
In the present study, we used the plausibil-
ity judgments collected by Lapata et al (1999) and
Lapata et al (2001) for adjective-noun bigrams and
conducted additional experiments to obtain noun-
noun and verb-object judgments for the materi-
als described in Section 2.1. We used the same
experimental procedure as the original study (see
Lapata et al (1999) and Lapata et al (2001) for de-
tails). Four experiments were carried out, one each
for seen and unseen noun-noun bigrams, and for
seen and unseen verb-object bigrams. Unlike the
adjective-noun and the noun-noun bigrams, the
verb-object bigrams were not presented to subjects
in isolation, but embedded in a minimal sentence
context involving a proper name as the subject
(e.g., Paul fulfilled the obligation).
The experiments were conducted over the web
using the WebExp software package (Keller et al,
1998). A series of previous studies has shown that
data obtained using WebExp closely replicates re-
sults obtained in a controlled laboratory setting;
this was demonstrated for acceptability judgments
(Keller and Alexopoulou, 2001), co-reference judg-
ments (Keller and Asudeh, 2001), and sentence
completions (Corley and Scheepers, 2002). These
references also provide a detailed discussion of the
WebExp experimental setup.
Table 6 lists the descriptive statistics for all
six judgment experiments: the original experiments
by Lapata et al (1999) and Lapata et al (2001) for
adjective-noun bigrams, and our new ones for noun-
noun and verb-object bigrams.
We used correlation analysis to compare web
counts with plausibility judgments for seen
adjective-noun, noun-noun, and verb-object bi-
grams. Table 7 (top half) lists the correlation
coefficients that were obtained when correlat-
adj-noun bigrams noun-noun bigrams verb-object bigrams
N Min Max Mean SD N Min Max Mean SD N Min Max Mean SD
Seen 30 ?.85 .11 ?.13 .22 25 ?.15 .69 .40 .21 27 ?.52 .45 .12 .24
Unseen 41 ?.56 .37 ?.07 .20 25 ?.49 .52 ?.01 .23 21 ?.51 .28 ?.16 .22
Table 6: Descriptive statistics for plausibility judgments (log-transformed); N is the number of subjects used
in each experiment
ing log-transformed web and BNC counts with
log-transformed plausibility judgments.
The results show that both Altavista and Google
counts correlate with plausibility judgments for seen
bigrams. Google slightly outperforms Altavista: the
correlation coefficient for Google ranges from .624
to .693, while for Altavista, it ranges from .638 to
.685. A surprising result is that the web counts con-
sistently achieve a higher correlation with the judg-
ments than the BNC counts, which range from .488
to .569. We carried out a series of one-tailed t-tests
to determine if the differences between the correla-
tion coefficients for the web counts and the corre-
lation coefficients for the BNC counts were signifi-
cant. For the adjective-noun bigrams, the difference
between the BNC coefficient and the Altavista coef-
ficient failed to reach significance (t(87) = 1.46, p >
.05), while the Google coefficient was significantly
higher than the BNC coefficient (t(87) = 1.78, p <
.05). For the noun-noun bigrams, both the Altavista
and the Google coefficients were significantly higher
than the BNC coefficient (t(87) = 2.94, p < .01 and
t(87) = 3.06, p < .01). Also for the verb-object bi-
grams, both the Altavista coefficient and the Google
coefficient were significantly higher than the BNC
coefficient (t(87) = 2.21, p < .05 and t(87) = 2.25,
p < .05). In sum, for all three types of bigrams, the
correlation coefficients achieved with Google were
significantly higher than the ones achieved with the
BNC. For Altavista, the noun-noun and the verb-
object coefficients were higher than the coefficients
obtained from the BNC.
Table 7 (bottom half) lists the correlations co-
efficients obtained by comparing log-transformed
judgments with log-transformed web counts for un-
seen adjective-noun, noun-noun, and verb-object bi-
grams. We observe that the web counts consistently
show a significant correlation with the judgments,
the coefficient ranging from .466 to .588 for Al-
seen bigrams
adj-noun noun-noun verb-object
Altavista .642** .685** .638**
Google .650** .693** .624**
BNC .569** .517** .488**
unseen bigrams
Altavista .466** .588** .568**
Google .446** .611** .542**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 7: Correlation of plausibility judgments with
web counts and BNC counts
tavista counts, and from .446 to .611 for the Google
counts. Note that a small number of bigrams pro-
duced zero counts even in our web queries; these fre-
quencies were set to one for the correlation analysis
(see Section 2.2).
To conclude, this evaluation demonstrated that
web counts reliably predict human plausibility judg-
ments, both for seen and for unseen predicate-
argument bigrams. In the case of Google counts
for seen bigrams, we were also able to show that
web counts are a better predictor of human judg-
ments than BNC counts. These results show that our
heuristic method yields useful frequencies; the sim-
plifications we made in obtaining the counts, as well
as the fact that web data are noisy, seem to be out-
weighed by the fact that the web is up to three orders
of magnitude larger than the BNC (see our estimate
in Section 2.2).
4 Conclusions
This paper explored a novel approach to overcoming
data sparseness. If a bigram is unseen in a given cor-
pus, conventional approaches recreate its frequency
using techniques such as back-off, linear interpo-
lation, class-based smoothing or distance-weighted
averaging (see Dagan et al (1999) and Lee (1999)
for overviews). The approach proposed here does
not recreate the missing counts, but instead re-
trieves them from a corpus that is much larger (but
also much more noisy) than any existing corpus: it
launches queries to a search engine in order to deter-
mine how often a bigram occurs on the web.
We systematically investigated the validity of
this approach by using it to obtain frequencies for
predicate-argument bigrams (adjective-noun, noun-
noun, and verb-object bigrams). We first applied
the approach to seen bigrams randomly sampled
from the BNC. We found that the counts obtained
from the web are highly correlated with the counts
obtained from the BNC, which indicates that web
queries can generate frequencies that are compara-
ble to the ones obtained from a balanced, carefully
edited corpus such as the BNC.
Secondly, we performed a tasked-based evalua-
tion that used the web frequencies to predict hu-
man plausibility judgments for predicate-argument
bigrams. The results show that web counts corre-
late reliably with judgments, for all three types of
predicate-argument bigrams tested, both seen and
unseen. For the seen bigrams, we showed that the
web frequencies correlate better with judged plausi-
bility than the BNC frequencies.
To summarize, we have proposed a simple heuris-
tic for obtaining bigram counts from the web. Using
two different types of evaluation, we demonstrated
that this simple heuristic is sufficient to obtain useful
frequency estimates. It seems that the large amount
of data available outweighs the problems associated
with using the web as a corpus (such as the fact that
it is noisy and unbalanced).
In future work, we plan to compare web counts
for unseen bigrams with counts recreated using
standard smoothing algorithms, such as similarity-
based smoothing (Dagan et al, 1999) or class-based
smoothing (Resnik, 1993). If web counts correlate
reliable with smoothed counts, then this provides
further evidence for our claim that the web can be
used to overcome data sparseness.
References
Steve Abney. 1996. Partial parsing via finite-state cas-
cades. In John Carroll, editor, Workshop on Robust
Parsing, pages 8?15, 8th European Summer School in
Logic, Language and Information, Prague.
Eneko Agirre and David Martinez. 2000. Exploring
automatic word sense disambiguation with decision
lists and the web. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics,
Saarbru?cken/Luxembourg/Nancy.
Michele Banko and Eric Brill. 2001a. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In James Allan, editor, Proceed-
ings of the 1st International Conference on Human
Language Technology Research, San Francisco. Mor-
gan Kaufmann.
Michele Banko and Eric Brill. 2001b. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics and the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, Toulouse.
Ellen Gurman Bard, Dan Robertson, and Antonella So-
race. 1996. Magnitude estimation of linguistic ac-
ceptability. Language, 72(1):32?68.
Lou Burnard, 1995. Users Guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Service.
Martin Corley and Christoph Scheepers. 2002. Syntac-
tic priming in English sentence production: Categori-
cal and latency evidence from an internet-based study.
Psychonomic Bulletin and Review, 9(1).
Steffan Corley, Martin Corley, Frank Keller, Matthew W.
Crocker, and Shari Trewin. 2001. Finding syntac-
tic structure in unparsed corpora: The Gsearch cor-
pus query system. Computers and the Humanities,
35(2):81?94.
Wayne Cowart. 1997. Experimental Syntax: Applying
Objective Methods to Sentence Judgments. Sage Pub-
lications, Thousand Oaks, CA.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence prob-
abilities. Machine Learning, 34(1):43?69.
Gregory Grefenstette and Jean Nioche. 2000. Estima-
tion of English and non-English language use on the
WWW. In Proceedings of the RIAO Conference on
Content-Based Multimedia Information Access, pages
237?246, Paris.
Gregory Grefenstette. 1998. The World Wide Web as a
resource for example-based machine translation tasks.
In Proceedings of the ASLIB Conference on Translat-
ing and the Computer, London.
Rosie Jones and Rayid Ghani. 2000. Automatically
building a corpus for a minority language from the
web. In Proceedings of the Student Research Work-
shop at the 38th Annual Meeting of the Association for
Computational Linguistics, pages 29?36, Hong Kong.
Frank Keller and Theodora Alexopoulou. 2001. Phonol-
ogy competes with syntax: Experimental evidence for
the interaction of word order and accent placement in
the realization of information structure. Cognition,
79(3):301?372.
Frank Keller and Ash Asudeh. 2001. Constraints on lin-
guistic coreference: Structural vs. pragmatic factors.
In Johanna D. Moore and Keith Stenning, editors, Pro-
ceedings of the 23rd Annual Conference of the Cog-
nitive Science Society, pages 483?488, Mahwah, NJ.
Lawrence Erlbaum Associates.
Frank Keller, Martin Corley, Steffan Corley, Lars
Konieczny, and Amalia Todirascu. 1998. WebExp:
A Java toolbox for web-based psychological experi-
ments. Technical Report HCRC/TR-99, Human Com-
munication Research Centre, University of Edinburgh.
Maria Lapata, Scott McDonald, and Frank Keller. 1999.
Determinants of adjective-noun plausibility. In Pro-
ceedings of the 9th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 30?36, Bergen.
Maria Lapata, Frank Keller, and Scott McDonald. 2001.
Evaluating smoothing algorithms against plausibility
judgments. In Proceedings of the 39th Annual Meet-
ing of the Association for Computational Linguistics
and the 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
346?353, Toulouse.
Maria Lapata. 2001. A corpus-based account of regular
polysemy: The case of context-sensitive adjectives. In
Proceedings of the 2nd Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Pittsburgh, PA.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University, Sydney.
Lilian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 25?32,
University of Maryland, College Park.
Rada Mihalcea and Dan Moldovan. 1999. A method
for word sense disambiguation of unrestricted text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 152?158,
University of Maryland, College Park.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?244.
Philip Stuart Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania, Philadelphia,
PA.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, University of
Maryland, College Park.
S. S. Stevens. 1975. Psychophysics: Introduction to its
Perceptual, Neural, and Social Prospects. John Wiley,
New York.
Martin Volk. 2001. Exploiting the WWW as a corpus
to resolve PP attachment ambiguities. In Paul Rayson,
Andrew Wilson, Tony McEnery, Andrew Hardie, and
Shereen Khoja, editors, Proceedings of the Corpus
Linguistics Conference, pages 601?606, Lancaster.
XML-Based NLP Tools for Analysing and Annotating Medical Language
Claire Grover, Ewan Klein, Mirella Lapata and Alex Lascarides
Division of Informatics
The University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
{C.Grover, E.Klein, M.Lapata, A.Lascarides}@ed.ac.uk
Abstract
We describe the use of a suite of highly flexible
XML-based NLP tools in a project for processing and
interpreting text in the medical domain. The main
aim of the paper is to demonstrate the central role
that XML mark-up and XML NLP tools have played
in the analysis process and to describe the resultant
annotated corpus of MEDLINE abstracts. In addition
to the XML tools, we have succeeded in integrating
a variety of non-XML ?off the shelf? NLP tools into
our pipelines, so that their output is added into the
mark-up. We demonstrate the utility of the anno-
tations that result in two ways. First, we investigate
how they can be used to improve parse coverage of a
hand-crafted grammar that generates logical forms.
And second, we investigate how they contribute to
automatic lexical semantic acquisition processes.
1 Introduction
In this paper we describe our use of XML for an anal-
ysis of medical language which involves a number
of complex linguistic processing stages. The ulti-
mate aim of the project is to to acquire lexical se-
mantic information from MEDLINE through parsing,
however, a fundamental tenet of our approach is that
higher-level NLP activities benefit hugely from be-
ing based on a reliable and well-considerered initial
stage of tokenisation. This is particularly true for
language tasks in the biomedical and other technical
domains since general purpose NLP technology may
stumble at the first hurdle when confronted with
character strings that represent specialised techni-
cal vocabulary. Once firm foundations are laid then
one can achieve better performance from e.g. chun-
kers and parsers than might otherwise be the case.
We show how well-founded tools, especially XML-
based ones, can enable a variety of NLP components
to be bundled together in different ways to achieve
different types of analysis. Note that in fields such
as information extraction (IE) it is common to use
statistical text classification methods for data anal-
ysis. Our more linguistic approach may be of as-
sistence in IE: see Craven and Kumlien (1999) for
discussion of methods for IE from MEDLINE.
Our processing paradigm is XML-based. As a
mark-up language for NLP tasks, XML is expres-
sive and flexible yet constrainable. Furthermore,
there exist a wide range of XML-based tools for NLP
applications which lend themselves to a modular,
pipelined approach to processing whereby linguis-
tic knowledge is computed and added as XML an-
notations in an incremental fashion. In processing
MEDLINE abstracts we have built a number of such
pipelines using as key components the programs
distributed with the LT TTT and LT XML toolsets
(Grover et al, 2000; Thompson et al, 1997). We
have also successfully integrated non-XML public-
domain tools into our pipelines and incorporated
their output into the XML mark-up using the LT XML
program xmlperl (McKelvie, 2000).
In Section 2 we describe our use of XML-based
tokenisation tools and techniques and in Sections 3
and 4 we describe two different approaches to
analysing MEDLINE data which are built on top of
the tokenisation. The first approach uses a hand-
coded grammar to give complete syntactic and se-
mantic analyses of sentences. The second approach
performs a shallower statistically-based analysis
which yields ?grammatical relations? rather than
full logical forms. This information about gram-
matical relations is used in a statistically-trained
model which disambiguates the semantic relations
in noun compounds headed by deverbal nominali-
sations. For this second approach we compare two
separate methods of shallow analysis which require
the use of two different part-of-speech taggers.
2 Pre-parsing of Medline Abstracts
For the work reported here, we have used the
OHSUMED corpus of MEDLINE abstracts (Hersh et
<RECORD>
<ID>395</ID>
<MEDLINE-ID>87052477</MEDLINE-ID>
<SOURCE>Clin Pediatr (Phila) 8703; 25(12):617-9 </SOURCE>
<MESH>
Adolescence; Alcoholic Intoxication/BL/*EP; Blood Glucose/AN; Canada; Child; Child, Preschool;
Electrolytes/BL; Female; Human; Hypoglycemia/ET; Infant; Male; Retrospective Studies.
</MESH>
<TITLE>Ethyl alcohol ingestion in children. A 15-year review.</TITLE>
<PTYPE>JOURNAL ARTICLE.</PTYPE>
<ABSTRACT>
<SENT><W P=?DT?>A</W> <W P=?JJ?>retrospective</W>
<W P=?NN? LM=?study?>study</W><W P=?VBD? LM=?be?>was</W>
<W P=?VBN? LM=?conduct?>conducted</W><W P=?IN?>by</W> <W P=?NN? LM=?chart?>chart</W>
<W P=?NNS? LM=?review?>reviews</W><W P=?IN? >of</W> <W P=?CD?>27</W>
<W P=?NNS? LM=?patient?>patients</W> <W P=?IN?>with</W> <W P=?JJ?>documented</W>
<W P=?NN? LM=?ethanol?>ethanol</W><W P=?NN? LM=?ingestion?>ingestion</W><W P=?.?>.</W>
</SENT> <SENT> .. . </SENT> <SENT> .. . </SENT>
</ABSTRACT>
<AUTHOR>Leung AK.</AUTHOR>
</RECORD>
Figure 1: A sample from the XML-marked-up OHSUMED Corpus
al., 1994) which contains 348,566 references taken
from the years 1987?1991. Not every reference
contains an abstract, thus the total number of ab-
stracts in the corpus is 233,443. The total number of
words in those abstracts is 38,708,745 and the ab-
stracts contain approximately 1,691,383 sentences
with an average length of 22.89 words.
By pre-parsing we mean identification of word
tokens and sentence boundaries and other lower-
level processing tasks such as part-of-speech (POS)
tagging and lemmatisation. These initial stages of
processing form the foundation of our NLP work
with MEDLINE abstracts and our methods are flex-
ible enough that the representation of pre-parsing
can be easily tailored to suit the input needs of sub-
sequent higher-level processors. We start by con-
verting the OHSUMED corpus from its original for-
mat to an XML format (see Figure 1). From this
point on we pass the data through pipelines which
are composed of calls to a variety of XML-based
tools from the LT TTT and LT XML toolsets. The
core program in our pipelines is the LT TTT program
fsgmatch, a general purpose transducer which pro-
cesses an input stream and rewrites it using rules
provided in a hand-written grammar file, where the
rewrite usually takes the form of the addition of
XML mark-up. Typically, fsgmatch rules specify
patterns over sequences of XML elements and use a
regular expression language to identify patterns in-
side the character strings (PCDATA) which are the
content of elements. For example, the following
rule for decimals such as ?.25? is searching for a
sequence of two S elements where the first contains
the string ?.? as its PCDATA content and the second
has been identified as a cardinal number (C=?CD?,
e.g. any sequence of digits). When these two S el-
ements are found, they are wrapped in a W element
with the attribute C=?CD? (targ sg). (Here S ele-
ments encode character sequences, see below, and
W elements encode words.)
<RULE name="decimal" targ_sg="W[C=?CD?]">
<REL match="S/#??[\.]$"></REL>
<REL match="S[C=?CD?]"></REL>
</RULE>
Subparts of a pipeline can be thought of as dis-
tinct modules so that pipelines can be configured to
different tasks. A typical pipeline starts with a two-
<S C=?UCA?>A</S><S C=?LCA?>rterial</S>
<S C=?WS?> </S><S C=?UCA?>P</S>
<S C=?LCA?>a</S><S C=?UCA?>O</S>
<S C=?CD?>2</S><S C=?WS?> </S>
<S C=?LCA?>as</S><S C=?WS?> </S>
<S C=?LCA?>measured</S>
Figure 2: Character Sequence (S) Mark-up
stage process to identify word tokens within ab-
stracts. First, sequences of characters are bundled
into S (sequence) elements using fsgmatch. For each
class of character a sequence of one or more in-
stances is identified and the type is recorded as the
value of the attribute C (UCA=upper case alphabetic,
LCA=lower case alphabetic, WS=white space etc.).
Figure 2 shows the string Arterial PaO2 as mea-
sured marked up for S elements (line breaks added
for formatting purposes). Every single character in-
cluding white space and newline is contained in S
elements which become building blocks for the next
call to fsgmatch where words are identified. An al-
ternative approach would find words in a single step
but our two-step method provides a cleaner set of
word-level rules which are more easily modified and
tailored to different purposes: modifiability is criti-
cal since the definition of what is a word can differ
from one subsequent processing step to another.
A pipeline which first identifies words and then
performs sentence boundary identification and POS
tagging followed by lemmatisation is shown in Fig-
ure 3 (somewhat simplified and numbering added
for ease of exposition). The Perl program in step 1
wraps the input inside an XML header and footer
as a first step towards conversion to XML. Step 2
calls fsgmatch with the grammar file ohsumed.gr to
identify the fields of an OHSUMED entry and convert
them into XML mark-up: each abstract is put inside
a RECORD element which contains sub-structure re-
flecting e.g. author, title, MESH code and the ab-
stract itself. From this point on, all processing is di-
rected at the ABSTRACT elements through the query
?.*/ABSTRACT?1. Steps 3 and 4 make calls to fsg-
match to identify S and W (word) elements as de-
scribed above and after this point, in step 5, the S
mark-up is discarded (using the LT TTT program
sgdelmarkup) since it has now served its purpose.
Step 6 contains a call to the other main LT TTT
program, ltpos (Mikheev, 1997), which performs
both sentence identification and POS tagging. The
subquery (-qs) option picks out ABSTRACTs as the
elements within RECORDs (-q option) that are to
be processed; the -qw option indicates that the in-
put has already been segmented into words marked
1The query language that the LT TTT and LT XML tools use
is a specialised XML query language which pinpoints the part
of the XML tree-structure that is to be processed at that point.
This query language pre-dates XPath and in expressiveness it
constitutes a subset of XPath except that it also allows regular
expressions over text content. Future plans include modifying
out tools to allow for the use of XPath as a query language.
up as W elements; the -sent option indicates that
sentences should be wrapped as SENT elements; the
-tag option is an instruction to output POS tags and
the -pos attr option indicates that POS tags should
be encoded as the value of the attribute P on W ele-
ments. The final resource.xml names the resource
file that ltpos is to use. Note that the tagset used
by ltpos is the Penn Treebank tagset (Marcus et al,
1994).
1. ohs2xml.perl \
2. | fsgmatch -q ".*/TEXT" ohsumed.gr \
3. | fsgmatch -q ".*/ABSTRACT" pretok.gr \
4. | fsgmatch ".*/ABSTRACT" tok.gr \
5. | sgdelmarkup -q ".*/S" \
6. | ltpos -q ".*/RECORD" -qs ".*/ABSTRACT" \
-qw ".*/W" -sent SENT \
-tag -pos_attr P resource.xml \
7. | xmlperl lemma.rule
Figure 3: Basic Tokenisation Pipeline
Up to this point, each module in the pipeline has
used one of the LT TTT or LT XML programs which
are sensitive to XML structure. There are, however,
a large number of tools available from the NLP com-
munity which could profitably be used but which are
not XML-aware. We have integrated some of these
tools into our pipelines using the LT XML program
xmlperl. This is a program which makes underly-
ing use of an XML parser so that rules defined in
a rule file can be directed at particular parts of the
XML tree-structure. The actions in the rules are de-
fined using the full capabilities of Perl. This gives
the potential for a much wider range of transforma-
tions of the input than fsgmatch allows and, in par-
ticular, we use Perl?s stream-handling capabilities
to pass the content of XML elements out to a non-
XML program, receive the result back and encode it
back in the XML mark-up. Step 7 of the pipeline in
Figure 3 shows a call to xmlperl with the rule file
lemma.rule. This rule file invokes Minnen et al?s
(2000) morpha lemmatiser: the PCDATA content of
each verbal or nominal W element is passed to the
lemmatiser and the lemma that is returned is en-
coded as the value of the attribute LM. A sample
of the output from the pipeline is shown in Figure 1.
3 Deep Grammatical Analysis
As part of our work with OHSUMED, we have
been attempting to improve the coverage of a hand-
crafted, linguistically motivated grammar which
provides full-syntactic analysis paired with logical
forms. The grammar and parsing system we use
is the wide-coverage grammar, morphological anal-
yser and lexicon provided by the Alvey Natural Lan-
guage Tools (ANLT) system (Carroll et al 1991,
Grover et al 1993). Our first aim was to increase
coverage up to a reasonable level so that parse rank-
ing techniques could then be applied.
The ANLT grammar is a feature-based unification
grammar based on the GPSG formalism (Gazdar et
al., 1985). In this framework, lexical entries carry
a significant amount of information including sub-
categorisation information. Thus the practical parse
success of the grammar is significantly dependent
on the quality of the lexicon. The ANLT grammar
is distributed with a large lexicon and, while this
provides a core of commonly-occurring lexical en-
tries, there remains a significant problem of inade-
quate lexical coverage. If we try to parse OHSUMED
sentences using the ANLT lexicon and no other re-
sources, we achieve very poor results (2% coverage)
because most of the medical domain words are sim-
ply not in the lexicon and there is no ?robustness?
strategy built into ANLT. Rather than pursue the
labour-intensive course of augmenting the lexicon
with domain-specific lexical resources, we have de-
veloped a solution which does not require that new
lexicons be derived for each new domain type and
which has robustness built into the strategy. Fur-
thermore, this solution does not preclude the use of
specialist lexical resources if these can be used to
achieve further improvements in performance.
Our approach relies on the sophisticated XML-
based tokenisation and POS tagging described in the
previous section and it builds on this by combin-
ing POS tag information with the existing ANLT lex-
ical resources. We preserve POS tag information for
content words (nouns, verbs, adjectives, adverbs)
since this is usually reliable and informative and
we dispose of POS tags for function words (com-
plementizers, determiners, particles, conjunctions,
auxiliaries, pronouns, etc.) since the ANLT hand-
written entries for these are more reliable and are
tuned to the needs of the grammar. Furthermore,
unknown words are far more likely to be content
words, so knowledge of the POS tag will most often
be needed for content words.
Having retained content word tags, we use them
during lexical look-up in one of two ways. If the
word exists in the lexicon with the same basic cat-
egory as the POS tag then the POS tag plays a ?dis-
ambiguating? role, filtering out entries for the word
with different categories. If, on the other hand, the
word is not in the lexicon or it is not in the lexicon
with the relevant category, then a basic underspeci-
fied entry for the POS tag is used as the lexical entry
for the word, thereby allowing the parse to proceed.
For example, if the following partially tagged sen-
tence is input to the parser, it is successfully parsed.
We studied VBD the value NN of
transcutaneous JJ carbon NN dioxide NN
monitoring NN during transport NN
Without the tags the parse would fail since the word
transcutaneous is not in the ANLT lexicon. Further-
more, monitoring is present in the lexicon but as a
verb and not as a noun. For both these words, or-
dinary lexical look-up fails and the entries for the
tags have to be used instead. Note that the case
of monitoring would be problematic for a strategy
where tagging is used only in case lexical look-up
fails, since here it is incomplete rather than failed.
The implementation of our word tag pair look-up
method is specific to the ANLT system and uses its
morphological analysis component to treat tags as a
novel kind of affix. Space considerations preclude
discussion of this topic here but see Grover and Las-
carides (2001) for further details.
Another impediment to parse coverage is the
prevalence of technical expressions and formulae in
biomedical and other technical language. For ex-
ample, the following sentence has a straightforward
overall syntactic structure but the ANLT grammar
does not contain specialist rules for handling ex-
pressions such as 5.0+/-0.4 grams tension and thus
the parse would fail.
Control tissues displayed a reproducible response to
bethanechol stimulation at different calcium
concentrations with an ED50 of 0.4 mM calcium
and a peak response of 5.0+/-0.4 grams tension.
Our response to issues like these is to place a fur-
ther layer of processing in between the output of
the initial tokenisation pipeline in Figure 3 and the
input to the parser. Since the ANLT system is not
XML-based, we already use xmlperl to convert sen-
tences to the ANLT input format of one sentence per
line with tags appended to words using an under-
score. We can add a number of other processes at
this point to implement a strategy of using fsgmatch
grammars to package up technical expressions so as
to render them innocuous to the parser. Thus all
of the following ?words? have been identified using
fsgmatch rules and can be passed to the parser as
unanalysable units. The classification of these ex-
amples as nouns reflects a hypothesis that they can
slot into the correct parse as noun phrases but there
is room for experimentation since the conversion to
parser input format can rewrite the tag in any way.
<W P=?NN?>P less than 0.001</W>
<W P=?NN?>166 +/- 77 mg/dl</W>
<W P=?NN?>2 to 5 cc/day</W>
<W P=?NN?>2.5 mg i.v.</W>
In addition to these kinds of examples, we also
package up other less technical expressions such as
common multi-word words and spelled out num-
bers:
<W P=?CD?>thirty-five</W> thirty-five CD
<W P=?CD?>Twenty one</W> Twenty?one CD
<W P=?IN?>In order to</W> In?order?to IN
<W P=?JJ?>in vitro</W> in?vitro JJ
In order to measure the effectiveness of our at-
tempts to improve coverage, we conducted an ex-
periment where we parsed 200 sentences taken at
random from OHSUMED. We processed the sen-
tences in three different ways and gathered parse
success rates for each of the three methods. Ver-
sion 1 established a ?no-intervention? baseline by
using the initial pipeline in Figure 3 to identify
words and sentences but otherwise discarding all
other mark-up. Version 2 addressed the lexical ro-
bustness issue by retaining POS tags to be used by
the grammar in the way outlined above. Version 3
applied the full set of preprocessing techniques in-
cluding the packaging-up of formulaic and other
technical expressions. The parse results for these
runs are as follows:
Version 1 Version 2 Version 3
Parses 4 (2%) 32 (16%) 79 (39.5%)
Even in Version 3, coverage is still not very high but
the difference between the three versions demon-
strates that our approach has made significant in-
roads into the problem. Moreover, the increase in
coverage was achieved without any significant al-
terations to the general-purpose grammar and the
tokenisation of formulaic expressions was by no
means comprehensive.
4 Shallow Analysis
In contrast to the full syntactic analysis experi-
ments described in the previous section, here we
describe two distinct methods of shallow analy-
sis from which we acquire frequency information
which is used to predict lexical semantic relations
in a particular kind of noun compound.
4.1 The Task
The aim of the processing in this task is to pre-
dict the relationship between a deverbal nominalisa-
tion head and its modifier in noun-noun compounds
such as tube placement, antibody response, pain re-
sponse, helicopter transport. In these examples, the
meaning of the head noun is closely related to the
meaning of the verb from which it derives and the
relationship between this noun and its modifier can
typically be matched onto a relationship between
the verb and one of its arguments. For example,
there is a correspondence between the compound
tube placement and the verb plus direct object string
place the tube. When we interpret the compound
we describe the role that the modifier plays in terms
of the argument position it would fill in the corre-
sponding verbal construction:
tube placement object
antibody response subject
pain response to-object
helicopter transport by-object
We can infer that tube in tube placement fills the
object role in the place relation by gathering in-
stances from the corpus of the verb place and dis-
covering that tube occurs more frequently in object
position than in other positions and that the object
interpretation is therefore more probable.
To interpret such compounds in this way, we need
access to information about the verbs from which
the head nouns are derived. Specifically, for each
verb, we need counts of the frequency with which
it occurs with each noun in each of its argument
slots. Ultimately, in fact, in view of the sparse data
problem, we need to back off from specific noun in-
stances to noun classes (see Section 4.4). The cur-
rent state-of-the-art in NLP provides a number of
routes to acquiring grammatical relations informa-
tion about verbs, and for our experiment we chose
two methods in order to be able to compare the tech-
niques and assess their utility.
4.2 Chunking with Cass
Our first method of acquiring verb grammatical re-
lations is that used by Lapata (2000) for a similar
task on more general linguistic data. This method
uses Abney?s (1996) Cass chunker which uses the
finite-state cascade technique. A finite-state cas-
cade is a sequence of non-recursive levels: phrases
at one level are built on phrases at the previous
level without containing same level or higher-level
phrases. Two levels of particular importance are
chunks and simplex clauses. A chunk is the non-
recursive core of intra-clausal constituents extend-
ing from the beginning of the constituent to its head,
excluding post-head dependents (i.e., NP, VP, PP),
whereas a simplex clause is a sequence of non-
recursive clauses (Abney, 1996). Cass recognizes
chunks and simplex clauses using a regular expres-
sion grammar without attempting to resolve attach-
ment ambiguities. The parser comes with a large-
scale grammar for English and a built-in tool that
extracts predicate-argument tuples out of the parse
trees that Cass produces. Thus the tool identifies
subjects and objects as well as PPs without how-
ever distinguishing arguments from adjuncts. We
consider verbs followed by the preposition by and
a head noun as instances of verb-subject relations.
Our verb-object tuples also include prepositional
objects even though these are not explicitly iden-
tified by Cass. We assume that PPs adjacent to the
verb and headed by either of the prepositions in, to,
for, with, on, at, from, of, into, through, upon are
prepositional objects.
The input to the process is the entire OHSUMED
corpus after it has been converted to XML, to-
kenised, split into sentences and POS tagged us-
ing ltpos as described in Section 2. The output of
this tokenisation is converted to Cass?s input format
which is a non-XML file with one word per line and
tags separated by tab. We achieve this conversion
using xmlperl with a simple rule file. The output
of Cass and the grammatical relations processor is a
list of each verb-argument pair in the corpus:
manage :obj refibrillation
respond :subj psoriasis
access :to system
4.3 Shallow Parsing with the Tag Sequence
Grammar
Our second method of acquiring verb grammati-
cal relations uses the statistical parser developed by
Briscoe and Carroll (1993, 1997) which is an ex-
tension of the ANLT grammar development system
which we used for our deep grammatical analysis as
reported in Section 3 above. The statistical parser,
known as the Tag Sequence Grammar (TSG), uses a
hand-crafted grammar where the lexical entries are
for POS tags rather than words themselves. Thus it
is strings of tags that are parsed rather than strings
of words. The statistical part of the system is the
parse ranking component where probabilities are as-
sociated with transitions in an LR parse table. The
grammar does not achieve full-coverage but on the
OHSUMED corpus we were able to obtain parses for
99.05% of sentences. The number of parses found
per sentence ranges from zero into the thousands
but the system returns the highest ranked parse ac-
cording to the statistical ranking method. We do
not have an accurate measure of how many of the
highest ranked parses are actually correct but even a
partially incorrect parse may still yield useful gram-
matical relations data.
In recent developments (Carroll and Briscoe,
2001), the TSG authors have developed an algorithm
for mapping TSG parse trees to representations of
grammatical relations within the sentence in the fol-
lowing format:
These centres are efficiently trapped in proteins at low
temperatures
(|ncsubj| |trap| |centre| |obj|)
(|iobj| |in| |trap| |protein|)
(|detmod| |centre| |These|)
(|mod| |trap| |efficiently|)
(|aux| |trap| |be|)
(|ncmod| |temperature| |low|)
(|ncmod| |at| |trap| |temperature|)
This format can easily be mapped to the same for-
mat as described in Section 4.2 to give counts of the
number of times a particular verb occurs with a par-
ticular noun as its subject, object or prepositional
object.
As explained above, the TSG parses sequences
of tags, however it requires a different tagset from
that produced by ltpos, namely the CLAWS2 tagset
(Garside, 1987). To prepare the corpus for parsing
with the TSG we therefore tagged it with Elworthy?s
(1994) tagger and since this is a non-XML tool we
used xmlperl to invoke it and to incorporate its re-
sults back into the XML mark-up. Sentences were
then prepared as input to the TSG?this involved us-
ing xmlperl to replace words by their lemmas and to
convert to ANLT input format:
These DD2 centre NN2 be VBR efficiently RR
trap VVN in II protein NN2 at II low JJ
temperature NN2
The lemmas are needed in order that the TSG out-
puts them rather than inflected words in the gram-
matical relations output shown above.
4.4 Compound Interpretation
Having collected two different sets of frequency
counts from the entire OHSUMED corpus for verbs
and their arguments, we performed an experiment to
discover (a) whether it is possible to reliably predict
semantic relations in nominalisation-headed com-
pounds and (b) whether the two methods of col-
lecting frequency counts make any significant dif-
ference to the process.
To collect data for the experiment we needed to
add to the mark-up already created by the basic
pipeline in Figure 3, (a) to mark up deverbal nomi-
nalisations with information about their verbal stem
to give nominalisation-verb equivalences and (b) to
mark up compounds in order to collect samples of
two-word compounds headed by deverbal nominal-
isations. For the first task we combined further use
of the lemmatiser with the use of lexical resources.
In a first pass we used the morpha lemmatiser to
find the verbal stem for -ing nominalisations such
as screening and then we looked up the remaining
nouns in a nominalisation lexicon which we created
by combining the nominalisation list which is pro-
vided by UMLS (2000) with the NOMLEX nominali-
sation lexicon (MacLeod et al, 1998) As a result of
these stages, most of the deverbal nominalisations
can be marked up with a VSTEM attribute whose
value is the verbal stem:
<W P=?NN? LM=?reaction? VSTEM=?react?>reaction</W>
<W P=?NN? LM=?growth? VSTEM=?grow?>growth</W>
<W P=?NN? LM=?control? VSTEM=?control?>control</W>
<W P=?NN? LM=?coding? VSTEM=?code?>coding</W>
To mark up compounds we developed an fsgmatch
grammar for compounds of all lengths and kinds
and we used this to process a subset of the first two
years of the corpus.
We interpret nominalisations in the biomedical
domain using a machine learning approach which
combines syntactic, semantic, and contextual fea-
tures. Using the LT XML program sggrep we
extracted all sentences containing two-word com-
pounds headed by deverbal nominalisations and
from this we took a random sample of 1,000 nom-
inalisations. These were manually disambiguated
using the following categories which denote the
argument relation between the deverbal head and
its modifier: SUBJ (age distribution), OBJ (weight
loss), WITH (graft replacement), FROM (blood elim-
ination), AGAINST (seizure protection), FOR (non-
stress test ), IN (vessel obstruction), BY (aerosol ad-
ministration), OF (water deprivation), ON (knee op-
eration), and TO (treatment response). We also in-
cluded the categories NA (non applicable) for nom-
inalisations with relations other than the ones pre-
dicted by the underlying verb?s subcategorisation
frame (e.g., death stroke) and NV (non deverbal) for
compounds that were wrongly identified as nomi-
nalisations.
We treated the interpretation of nominalisations
as a classification task and experimented with dif-
ferent features using the C4.5 decision tree learner
(Quinlan, 1993). Some of the features we took into
account were the context surrounding the candidate
nominalisations (encoded as words or POS-tags), the
number of times a modifier was attested as an argu-
ment of the verb corresponding to the nominalised
head, and the nominalisation affix of the deverbal
head (e.g., -ation, -ment). In the face of sparse
data, linguistic resources such as WordNet (Miller
and Charles, 1991) and UMLS were used to recre-
ate distributional evidence absent from our corpus.
We obtained several different classification models
as a result of using different marked-up versions of
the corpus, different parsers, and different linguistic
resources. Full details of the results are described
in Grover et al (2002); we only have space for a
brief summary here. Our best results achieved an
accuracy of 73.6% (over a baseline of 58.5%) when
using the type of affixation of the deverbal head, the
TSG, and WordNet for recreating missing frequen-
cies.
5 Conclusions
We have performed a number of different NLP tasks
on the OHSUMED corpus of MEDLINE abstracts
ranging from low-level tokenisation through shal-
low parsing to deep syntactic and semantic analy-
sis. We have used XML as our processing paradigm
and we believe that without the core XML tools the
task would have become extremely hard. Further-
more, we have built fully-automatic pipelines and
have not resorted to hand-coding at any point so that
our output annotations are completely reproducable
and our resources are reusable on new data. Our
approach of building a firm foundation of low-level
tokenisation has proved invaluable for a variety of
higher-level tasks.
The XML-annotated OHSUMED corpus which has
resulted from our project will be useful for a num-
ber of different tasks in the biomedical domain. For
this reason we are developing a web-site from which
many of our resources (including the pipelines
described in this paper) are available: http://
www.ltg.ed.ac.uk/disp/. In addition, we pro-
vide various marked-up and tokenised versions of
OHSUMED, including the output of the parsers de-
scribed here.
References
Steven Abney. 1996. Partial parsing via finite-state
cascades. In John Carroll, editor, Proceedings
of Workshop on Robust Parsing at Eighth Sum-
mer School in Logic, Language and Information,
pages 8?15. University of Sussex.
Ted Briscoe and John Carroll. 1993. Generalised
probabilistic LR parsing of natural language (cor-
pora) with unification grammars. Computational
Linguistics, 19(1):25?60.
Ted Briscoe and John Carroll. 1997. Automatic
extraction of subcategorization from corpora. In
Proceedings of the Fifth ACL Conference on Ap-
plied Natural Language Processing, 356?363.
John Carroll and Ted Briscoe. 2001. High preci-
sion extraction of grammatical relations. In Pro-
ceedings of the 7th ACL/SIGPARSE International
Workshop on Parsing Technologies, pages 78?89,
Beijing, China.
Mark Craven and Johan Kumlien. 1999. Construct-
ing biological knowledge bases by extracting in-
formation from text sources. In Proceedings of
the 7th Interntaional Conference on Intelligent
Systems for Molecular Biology (ISMB-99).
David Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ACL conference on Applied Natural Lan-
guage Processing, pages 53?58, Stuttgart, Ger-
many.
Roger Garside. 1987. The CLAWS word-tagging
system. In Roger Garside, Geoffrey Leech, and
Geoffrey Sampson, editors, The Computational
Analysis of English. Longman, London.
Gerald Gazdar, Ewan Klein, Geoff Pullum, and Ivan
Sag. 1985. Generalized Phrase Structure Gram-
mar. Basil Blackwell, London.
Claire Grover, Colin Matheson, Andrei Mikheev,
and Marc Moens. 2000. LT TTT?a flexible
tokenisation tool. In LREC 2000?Proceedings
of the Second International Conference on Lan-
guage Resources and Evaluation, pages 1147?
1154.
Claire Grover and Alex Lascarides. 2001. XML-
based data preparation for robust deep parsing.
In Proceedings of the Joint EACL-ACL Meeting
(ACL-EACL 2001).
Claire Grover, Mirella Lapata and Alex Lascarides.
2002. A Comparison of Parsing Technologies for
the Biomedical Domain. Submitted to Journal of
Natural Language Engineering.
William Hersh, Chris Buckley, TJ Leone, and David
Hickam. 1994. OHSUMED: an interactive re-
trieval evaluation and new large test collection for
research. In W. Bruce Croft and C. J. van Rijsber-
gen, editors, Proceedings of the 17th Annual In-
ternational Conference on Research and Devel-
opment in Information Retrieval, pages 192?201.
Maria Lapata. 2000. The automatic interpretation
of nominalizations. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 716?721, Austin, TX.
Catherine MacLeod, Ralph Grishman, Adam Mey-
ers, Leslie Barrett, and Ruth Reeves. 1998.
NOMLEX: a lexicon of nominalisations. In EU-
RALEX?98, pages 187?194.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn treebank: annotating
predicate argument structure. In ARPA Human
Language Technologies Workshop.
David McKelvie. 2000. XMLPERL 1.7.2. A Rule
Based XML Transformation Language http://
www.cogsci.ed.ac.uk/?dmck/xmlperl.
Andrei Mikheev. 1997. Automatic rule induction
for unknown word guessing. Computational Lin-
guistics, 23(3):405?423.
George A. Miller and William G. Charles. 1991.
Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1?28.
Guido Minnen, John Carroll, and Darren Pearce.
2000. Robust, applied morphological generation.
In Proceedings of 1st International Natural Lan-
guage Conference (INLG ?2000).
Ross J. Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufman, San Mateo,
CA.
Henry S. Thompson, Richard Tobin, David McK-
elvie, and Chris Brew. 1997. LT XML. Soft-
ware API and toolkit for XML processing. http:
//www.ltg.ed.ac.uk/software/.
UMLS. 2000. Unified Medical Language System
(UMLS) Knowledge Sources. National Library of
Medicine, Bethesda (MD), 11th edition edition.
A Corpus-based Account of Regular Polysemy: The Case of
Context-sensitive Adjectives
Maria Lapata
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
mlap@coli.uni-sb.de
Abstract
In this paper we investigate polysemous adjectives whose
meaning varies depending on the nouns they modify
(e.g., fast). We acquire the meanings of these adjectives
from a large corpus and propose a probabilistic model
which provides a ranking on the set of possible interpre-
tations. We identify lexical semantic information auto-
matically by exploiting the consistent correspondences
between surface syntactic cues and lexical meaning.
We evaluate our results against paraphrase judgments
elicited experimentally from humans and show that the
model?s ranking of meanings correlates reliably with hu-
man intuitions: meanings that are found highly probable
by the model are also rated as plausible by the subjects.
1 Introduction
Much recent work in lexical semantics has been con-
cerned with accounting for regular polysemy, i.e., the
regular and predictable sense alternations certain classes
of words are subject to. Adjectives, more than other cat-
egories, are a striking example of regular polysemy since
they are able to take on different meanings depending on
their context, viz., the noun or noun class they modify
(see Pustejovsky (1995) and the references therein).
The adjective fast in (1) receives different interpre-
tations when modifying the nouns programmer, plane
and scientist. A fast programmer is typically a program-
mer who programs quickly, a fast plane is typically a
plane that flies quickly, a fast scientist can be a scien-
tist who publishes papers quickly, who performs exper-
iments quickly, who observes something quickly, who
reasons, thinks, or runs quickly. Interestingly, adjectives
like fast are ambiguous across and within the nouns they
modify. A fast plane is not only a plane that flies quickly,
but also a plane that lands, takes off, turns, or travels
quickly. Even the more restrictive fast programmer al-
lows more than one interpretation. One can easily think
of a context where a fast programmer thinks, runs or
talks quickly.
(1) a. fast programmer
b. fast plane
c. fast scientist
The work reported in this paper was carried out while the author
was at the Division of Informatics, University of Edinburgh.
(2) a. easy problem
b. difficult language
c. good cook
d. good soup
Adjectives like fast have been extensively studied in the
lexical semantics literature and their properties have been
known at least since Vendler (1968). The meaning of
adjective-noun combinations like those in (1) and (2) are
usually paraphrased with a verb modified by the adjective
in question or its corresponding adverb. For example, an
easy problem is ?a problem that is easy to solve? or ?a
problem that one can solve easily?. In order to account
for the meaning of these combinations Vendler (1968,
92) points out that ?in most cases not one verb, but a fam-
ily of verbs is needed?. Vendler further observes that the
noun figuring in an adjective-noun combination is usu-
ally the subject or object of the paraphrasing verb. Al-
though fast usually triggers a verb-subject interpretation
(see (1)), easy and difficult trigger verb-object interpre-
tations (see (2a,b)). An easy problem is usually a prob-
lem that is easy to solve, whereas a difficult language is a
language that is difficult to learn, speak, or write. Adjec-
tives like good allow either verb-subject or verb-object
interpretations: a good cook is a cook who cooks well
whereas good soup is soup that tastes good or soup that
is good to eat.
Pustejovsky (1995) avoids enumerating the various
senses for adjectives like fast by exploiting the seman-
tics of the nouns they modify. Pustejovsky treats nouns
as having a qualia structure as part of their lexical en-
tries, which among other things, specifies possible events
associated with the entity. For example, the telic (pur-
pose) role of the qualia structure for problem has a value
equivalent to solve. When the adjective easy is combined
with problem, it predicates over the telic role of prob-
lem and consequently the adjective-noun combination
receives the interpretation a problem that is easy to solve.
Pustejovsky (1995) does not give an exhaustive list
of the telic roles a given noun may have. Furthermore,
in cases where more than one interpretations are pro-
vided (see Vendler (1968)), no information is given with
respect to the likelihood of these interpretations. Out-
of context, the number of interpretations for fast scien-
tist is virtually unlimited, yet some interpretations are
more likely than others: fast scientist is more likely to
be a scientist who performs experiments quickly or who
publishes quickly than a scientist who draws or drinks
quickly.
In this paper we focus on polysemous adjective-noun
combinations (see (1) and (2)) and attempt to address
the following questions: (a) Can the meanings of these
adjective-noun combinations be acquired automatically
from corpora? (b) Can we constrain the number of inter-
pretations by providing a ranking on the set of possible
meanings? (c) Can we determine if an adjective has a
preference for a verb-subject or verb-object interpreta-
tion? We provide a probabilistic model which combines
distributional information about how likely it is for any
verb to be modified by the adjective in the adjective-
noun combination or its corresponding adverb with in-
formation about how likely it is for any verb to take
the modified noun as its object or subject. We obtain
quantitative information about verb-adjective modifica-
tion and verb-argument relations from the British Na-
tional Corpus (BNC), a 100 million word collection of
samples of written and spoken language from a wide
range of sources designed to represent current British En-
glish (Burnard, 1995). We evaluate our results by com-
paring the model?s predictions against human judgments
and show that the model?s ranking of meanings correlates
reliably with human intuitions.
2 The Model
2.1 Formalization of Adjective-Noun Polysemy
In order to come up with the meaning of ?plane that flies
quickly? for fast plane we would like to find in the cor-
pus a sentence whose subject is the noun plane or planes
and whose main verb is fly, which in turn is modified by
the adverbs fast or quickly. In the general case we want
to paraphrase the meaning of an adjective-noun combi-
nation by finding the verbs that take the head noun as
their subject or object and are modified by an adverb
corresponding to the modifying adjective. This can be
expressed as the joint probability P(a,n,v,rel) where v
is the verbal predicate modified by the adverb a (derived
from the adjective present in the adjective-noun combi-
nation) bearing the argument relation rel (i.e., subject or
object) to the head noun n. We rewrite P(a,n,v,rel) using
the chain rule in (3).
P(a,n,v,rel) =(3)
P(v) P(njv) P(ajv,n) P(reljv,n,a)
Although the parameters P(v) and P(njv) can be straight-
forwardly estimated from the BNC, the estimation of
P(reljv,n,a) and P(ajv,n) is somewhat problematic. In
order to obtain P(reljv,n,a) we must estimate the fre-
quency f (v,n,a,rel) (see (4)).
P(reljv,n,a) = f (v,n,a,rel)f (v,n,a)(4)
One way to acquire f (v,n,a,rel) would be to fully parse
the corpus so as to identify the verbs which take the
head noun n as their subject or object and are modified
by the adverb a. Even if we could accurately parse the
corpus, it is questionable whether we can find enough
data for the estimation of f (v,n,a,rel). There are only
six sentences in the entire BNC that can be used to es-
timate f (v,n,a,rel) for the adjective-noun combination
fast plane (see (5a)?(5f)). The interpretations ?plane that
swoops in fast?, ?plane that drops down fast? and ?plane
that flies fast? are all equally likely, since they are at-
tested in the corpus only once. This is rather counter-
intuitive since fast planes are more likely to fly than
swoop in fast. For the adjective-noun combination fast
programmer there is only one sentence relevant for the
estimation of f (v,n,a,rel) in which the modifying ad-
verbial is not fast but the semantically related quickly
(see (6)). The sparse data problem carries over to the es-
timation of the frequency f (v,n,a).
(5) a. Three planes swooped in, fast and low.
b. The plane was dropping down fast towards
Bangkok.
c. The unarmed plane flew very fast and very
high.
d. The plane went so fast it left its sound behind.
e. And the plane?s going slightly faster than the
Hercules or Andover.
f. He is driven by his ambition to build a plane
that goes faster than the speed of sound.
(6) It means that programmers will be able to develop
new applications more quickly.
We avoid these estimation problems by reducing the pa-
rameter space. In particular, we make the following in-
dependence assumptions:
P(ajv,n)  P(ajv)(7)
P(reljv,n,a)  P(reljv,n)(8)
We assume that the likelihood of an adverb modifying
a verb is independent of the verb?s arguments (see (7)).
Accordingly, we assume that knowing that the adverb a
modifying the verb v will contribute little information to
the likelihood of the relation rel which depends more on
the verb and its argument n (see (8)). By substituting (7)
and (8) into (3), P(a,n,v,rel) can be written as:
P(a,n,v,rel)  P(v) P(njv) P(ajv) P(reljv,n)(9)
We estimate the probabilities P(v), P(njv), P(ajv), and
P(reljv,n) as follows:
P(v) =
f (v)
?
i
f (vi)(10)
P(njv) =
f (n,v)
f (v)(11)
P(ajv) =
f (a,v)
f (v)(12)
P(reljv,n) = f (rel,v,n)f (v,n)(13)
By substituting equations (10)?(13) into (9) and simpli-
fying the relevant terms, (9) is rewritten as follows:
P(a,n,v,rel)  f (rel,v,n)  f (a,v)f (v) ?
i
f (vi)(14)
Depending on the data (noisy or not) and the task at hand
we may choose to estimate the probability P(v,n,a,rel)
from reliable corpus frequencies only (e.g., f (a,v) > 1
and f (rel,v,n) > 1). If we know the interpretation pref-
erence of a given adjective (i.e., subject or object), we
may vary only the term v, keeping the terms n, a and rel
constant. Alternatively, as we show in Experiment 1 (see
Section 3), we may acquire the interpretation preferences
automatically by varying both the terms rel and v.
2.2 Parameter Estimation
We estimated the parameters described in the previous
section from a part-of-speech tagged and lemmatized
version of the BNC (100 million words). The estimation
of the terms f (v) and ?i f (vi) (see (14)) reduces to the
number of times a given verb is attested in the corpus.
In order to estimate the terms f (rel,v,n) and f (a,v) the
corpus was automatically parsed by Cass (Abney, 1996),
a robust chunk parser designed for the shallow analysis
of noisy text. We used the parser?s built-in function to ex-
tract tuples of verb-subjects and verb-objects (see (15)).
The tuples obtained from the parser?s output are an im-
perfect source of information about argument relations.
Bracketing errors as well as errors in identifying chunk
categories accurately result in tuples whose lexical items
do not stand in a verb-argument relationship. For exam-
ple, the verb is missing from (16a) and the noun is miss-
ing from (16b).
(15) a. change situation SUBJ
b. come off heroin OBJ
c. deal with situation OBJ
(16) a. isolated people SUBJ
b. smile good SUBJ
In order to compile a comprehensive count of verb-
argument relations we discarded tuples containing verbs
or nouns attested in a verb-argument relationship only
once. Non-auxiliary instances of the verb be (e.g., OBJ be
embassy) were also eliminated since they contribute no
semantic information with respect to the events or states
that are possibly associated with the noun with which the
adjective is combined. Particle verbs (see (15b)) were
retained only if the particle was adjacent to the verb.
Verbs followed by the preposition by and a head noun
were considered instances of verb-subject relations. The
verb-object tuples also included prepositional objects
(see (15c)). It was assumed that PPs adjacent to the verb
headed by either of the prepositions in, to, for, with,
on, at, from, of, into, through, upon were prepositional
objects. This resulted in 737,390 distinct types of verb-
subject pairs and 1,077,103 distinct types of verb-object
pairs.
Generally speaking, the frequency f (a,v) represents
not only a verb modified by an adverb derived from the
adjective in question (see (17a)) but also constructions
like the ones in (17b,c) where the adjective takes an in-
finitival VP complement whose logical subject can be re-
alized as a for-PP (see (17c)). It is relatively straight-
forward to develop an automatic process which maps
an adjective to its corresponding adverb, modulo excep-
tions and idiosyncrasies. However in the experiments de-
scribed in the following sections this mapping was man-
ually specified.
(17) a. comfortable chair ! a chair on which one sits
comfortably
b. comfortable chair ! a chair that is comfort-
able to sit on
c. comfortable chair ! a chair that is comfort-
able for me to sit on
We estimated the frequency f (a,v) by collapsing the
counts from cases where the adjective was followed by
an infinitival complement (see (17b,c)) and cases where
the verb was modified by the adverb corresponding to
the related adjective (see (17a)). We focused only on in-
stances where the verb and the adverbial phrase modify-
ing it (AdvP) were adjacent and extracted the verb and
the head of the AdvP immediately following or preced-
ing it. From constructions with adjectives immediately
followed by infinitival complements with an optionally
intervening for-PP (see (17c)) we extracted the adjective
and the main verb of the infinitival complement.
2.3 Comparison against the Literature
In what follows we explain the properties of the model by
applying it to a small number of adjective-noun combina-
tions taken from the lexical semantics literature. Table 1
gives the interpretations of eight adjective-noun com-
binations discussed in Pustejovsky (1995) and Vendler
(1968). Table 2 shows the five most likely interpretations
for these combinations as derived by the model discussed
in the previous sections (v1 is the most likely interpreta-
tion, v2 is the second most likely interpretation, etc.).
First notice that our model predicts variation in mean-
ing when the same adjective modifies different nouns by
providing different interpretations for easy problem and
easy planet (see Table 2). Our model agrees with Vendler
(1968) in the interpretation of easy problem (see Tables 1
and 2). Furthermore, it provides the additional meanings
?a problem that is easy to deal with, identify, tackle, and
handle?. Although the model does not derive Vendler?s
interpretation of easy planet, it produces complementary
meanings such as ?a planet that is easy to predict, iden-
tify, plunder, work with?. Similarly, although the model
does not discover the suggested interpretation for good
umbrella it comes up with the plausible meaning ?an um-
brella that covers well?. In fact the latter can be consid-
ered as a subtype of the meaning suggested by Puste-
jovsky (1995): an umbrella functions well if it opens
well, closes well, covers well, etc. Although Pustejovsky
suggests only a subject-related interpretation for good
umbrella, the model also derives plausible object-related
interpretations: ?an umbrella that is good to keep, good
for waving, good to hold, good to run for, good to leave?.
Adjective Interpretation
easy problem a problem that is easy to solve (Vendler, 1968, 97)
easy planet a planet that is easy to observe (Vendler, 1968, 99)
good umbrella an umbrella that functions well (Pustejovsky, 1995, 43)
good shoe a shoe that is good for wearing, for walking (Vendler, 1968, 99)
fast horse a horse that runs fast (Vendler, 1968, 92)
difficult language a language that is difficult to speak, learn, write, understand (Vendler, 1968, 99)
careful scientist a scientist who observes, performs, runs experiments carefully (Vendler, 1968, 92)
comfortable chair a chair on which one sits comfortably (Vendler, 1968, 98)
Table 1: Paraphrases for adjective-noun combinations taken from the literature
P(v,n,a,rel) v1 v2 v3 v4 v5
P(v,problem,easy,OBJ) solve deal with identify tackle handle
P(v,planet,easy,OBJ) predict identify plunder see on work with
P(v,umbrella,good,SUBJ) cover
P(v,umbrella,good,OBJ) keep wave hold run for leave
P(v,shoe,good,OBJ) wear keep buy get stick
P(v,horse, fast,OBJ) run learn go come rise
P(v, language,difficult,OBJ) understand interpret learn use speak
P(v,careful,scientist,SUBJ) calculate proceed investigate study analyse
P(v,comfortable,chair,OBJ) sink into sit on lounge in relax in nestle in
Table 2: Model-derived paraphrases for adjective-noun combinations, ranked in order of likelihood
The model and Vendler (1968) agree in their inter-
pretation of the pairs good shoe and fast horse. The
model additionally acquires the fairly plausible mean-
ings ?a shoe that is good to keep, to buy, and get? for
good shoe and ?a horse that learns, goes, comes and rises
fast? for fast horse. The model?s interpretations for dif-
ficult language are a superset of the meanings suggested
by Vendler (see Table 1). The model?s interpretations for
careful scientist seem intuitively plausible (even though
they don?t overlap with those suggested by Vendler). Fi-
nally, note that the meanings derived for comfortable
chair are also plausible (the second most likely meaning
is the one suggested by Vendler, see Table 1).
The examples in Table 1 may not be entirely represen-
tative of the types of polysemous adjective-noun combi-
nations occurring in unrestricted text since they are taken
from linguistic texts where emphasis is given on explain-
ing polysemy with examples that straightforwardly illus-
trate it. In other words, the adjective-noun combinations
in Table 1 may be too easy for the model to handle. In
Experiment 1 (see Section 3) we test our model on poly-
semous adjective-noun combinations randomly sampled
from the BNC, and formally evaluate our results against
human judgments.
3 Experiment 1: Comparison against
Human Judgments
3.1 Method
The ideal test of the proposed model of adjective-
noun polysemy will be with randomly chosen materi-
als. We evaluate the acquired meanings by comparing
the model?s rankings against judgments of meaning para-
phrases elicited experimentally from human subjects. By
comparing the model-derived meanings against human
intuitions we are able to explore: (a) whether plausi-
ble meanings are ranked higher than implausible ones;
(b) whether the model can be used to derive the argu-
ment preferences for a given adjective, i.e., whether the
adjective is biased towards a subject or object interpre-
tation or whether it is equi-biased; (c) whether there is a
linear relationship between the model-derived likelihood
of a given meaning and its perceived plausibility, using
correlation analysis.
3.1.1 Materials and Design
We chose nine adjectives according to a set of minimal
criteria and paired each adjective with 10 nouns ran-
domly selected from the BNC. We chose the adjectives
as follows: we first compiled a list of all the polysemous
adjectives mentioned in the lexical semantics literature
(Vendler, 1968; Pustejovsky, 1995). From these we ran-
domly sampled nine adjectives (difficult, easy, fast, good,
hard, right, safe, slow, wrong). These adjectives had to
be unambiguous with respect to their part-of-speech:
each adjective was unambiguously tagged as ?adjective?
98.6% of the time, measured as the number of different
part-of-speech tags assigned to the word in the BNC.
We identified adjective-noun pairs using Gsearch
(Corley et al, 2000), a chart parser which detects syn-
tactic patterns in a tagged corpus by exploiting a user-
specified context free grammar and a syntactic query.
Gsearch was run on a lemmatized version of the BNC so
as to compile a comprehensive corpus count of all nouns
occurring in a modifier-head relationship with each of the
nine adjectives. From the syntactic analysis provided by
Probability BandAdjective-noun High Medium Low
difficult customer satisfy ?20.27 help ?22.20 drive ?22.64
easy food cook ?18.94 introduce ?21.95 finish ?23.15
fast pig catch ?23.98 stop ?24.30 use ?25.66
good postcard send ?20.17 draw ?22.71 look at ?23.34
hard number remember ?20.30 use ?21.15 create ?22.69
right school apply to ?19.92 complain to ?21.48 reach ?22.90
safe drug release ?22.24 try ?23.38 start ?25.56
slow child adopt ?19.90 find ?22.50 forget ?22.79
wrong colour use ?21.78 look for ?22.78 look at ?24.89
Table 3: Randomly selected example stimuli with log-transformed probabilities derived by the model
the parser we extracted a table containing the adjective
and the head of the noun phrase following it. In the case
of compound nouns, we only included sequences of two
nouns, and considered the rightmost occurring noun as
the head.
We used the model outlined in Section 2 to derive
meanings for the 90 adjective-noun combinations. We
employed no threshold on the frequencies f (a,v) and
f (rel,v,n). In order to obtain the frequency f (a,v) the
adjective was mapped to its corresponding adverb. In par-
ticular, good was mapped to good and well, fast to fast,
easy to easily, hard to hard, right to rightly and right,
safe to safely and safe, slow to slowly and slow and
wrong to wrongly and wrong. The adverbial function of
the adjective difficult is expressed only periphrastically
(i.e., in a difficult manner, with difficulty). As a result,
the frequency f (difficult,v) was estimated only on the
basis of infinitival constructions (see (17)). We estimated
the probability P(a,n,v,rel) for each adjective-noun pair
by varying both the terms v and rel.
In order to generate stimuli covering a wide range
of model-derived paraphrases corresponding to differ-
ent degrees of likelihood, for each adjective-noun com-
bination we divided the set of the derived meanings into
three probability ?bands? (High, Medium, and Low) of
equal size and randomly chose one interpretation from
each band. The division ensured that the experimen-
tal stimuli represented the model?s behavior for likely
and unlikely paraphrases and enabled us to test the hy-
pothesis that likely paraphrases correspond to high rat-
ings and unlikely paraphrases correspond to low rat-
ings. We performed separate divisions for object-related
and subject-related paraphrases resulting in a total of six
interpretations for each adjective-noun combination, as
we wanted to determine whether there are differences
in the model?s predictions with respect to the argument
function (i.e., object or subject) and also because we
wanted to compare experimentally-derived adjective bi-
ases against model-derived biases. Example stimuli (with
object-related interpretations only) are shown in Table 3
for each of the nine adjectives.
Our experimental design consisted of the factors
adjective-noun pair (Pair), grammatical function (Func)
and probability band (Band). The factor Pair included 90
adjective-noun combinations. The factor Func had two
levels, subject and object, whereas the factor Band had
three levels, High, Medium and Low. This yielded a to-
tal of Pair  Func  Band = 90 2 3 = 540 stim-
uli. The number of the stimuli was too large for sub-
jects to judge in one experimental session. We limited
the size of the design by selecting a total of 270 stimuli
as follows: our initial design created two sets of stimuli,
270 subject-related stimuli and 270 object-related stim-
uli. For each stimuli set we randomly selected five nouns
for each of the nine adjectives together with their cor-
responding interpretations in the three probability bands
(High, Medium, Low). This yielded a total of Pair 
Func  Band = 4523 = 270 stimuli. This way, stim-
uli were created for each adjective in both subject-related
and object-related interpretations.
We administered the 270 stimuli to two separate sub-
ject groups. Each group saw 135 stimuli consisting
of interpretations for all adjectives with both subject-
related and object-related interpretations. Each exper-
imental item consisted of an adjective-noun pair and
a sentence paraphrasing its meaning. Paraphrases were
created by the experimenter by converting the model?s
output to a simple phrase, usually a noun modified by a
relative clause. A native speaker of English was asked
to confirm that the paraphrases were syntactically well-
formed.
3.1.2 Procedure
The experimental paradigm was Magnitude Estima-
tion (ME), a technique standardly used in psychophysics
to measure judgments of sensory stimuli Stevens (1975),
which Bard et al (1996) and Cowart (1997) have applied
to the elicitation of linguistic judgments. ME has been
shown to provide fine-grained measurements of linguis-
tic acceptability which are robust enough to yield statis-
tically significant results, while being highly replicable
both within and across speakers.
ME requires subjects to assign numbers to a series of
linguistic stimuli in a proportional fashion. Subjects are
first exposed to a modulus item, to which they assign
an arbitrary number. All other stimuli are rated propor-
tional to the modulus. In this way, each subject can es-
tablish their own rating scale, thus yielding maximally
fine-grained data and avoiding the known problems with
the conventional ordinal scales for linguistic data (Bard
et al, 1996; Schu?tze, 1996).
In the present experiment, the subjects were instructed
to judge how well a sentence paraphrases an adjective-
noun combination proportional to a modulus item. The
experiment was conducted remotely over the Inter-
net. Subjects accessed the experiment using their web
browser, which established an Internet connection to the
experimental server running WebExp 2.1 (Keller et al,
1998), an interactive software package for administer-
ing web-based psychological experiments. Subjects first
saw a set of instructions that explained the ME tech-
nique and included some examples, and had to fill in a
short questionnaire including basic demographic infor-
mation. Each subject group saw 135 experimental stim-
uli (i.e., adjective-noun pairs and their paraphrases). Sub-
jects were assigned to subject groups at random, and a
random stimulus order was generated for each subject.
3.1.3 Subjects
The experiment was completed by 60 unpaid volunteers,
all native speakers of English. Subjects were recruited
via postings to local Usenet newsgroups.
3.2 Results
As is standard in magnitude estimation studies (Bard et
al., 1996), statistical tests were done using geometric
means to normalize the data (the geometric mean is the
mean of the logarithms of the ratings).
We first performed an analysis of variance (ANOVA) to
determine whether there is a relation between the para-
phrases derived by the model and their perceived likeli-
hood. In particular, we tested the hypothesis that mean-
ings assigned high probabilities by the model are per-
ceived as better paraphrases by the subjects and cor-
respondingly that meanings with low probabilities are
perceived as worse paraphrases. The descriptive statis-
tics for log-transformed model-derived probabilities are
shown in Table 4. The ANOVA revealed that the Prob-
ability Band effect was significant, in both by-subjects
and by-items analyses: F1(2,118) = 101.46, p < .01;
F2(2,88) = 29.07, p < .01. The geometric mean of
the ratings in the High band was ?.0005, compared to
Medium items at ?.1754 and Low items at ?.2298 (see
Table 5). Post-hoc Tukey tests indicated that the differ-
ences between all pairs of conditions were significant at
? = .01 in the by-subjects analysis. The difference be-
tween High and Medium items as well as High and Low
items was significant at ? = .01 in the by-items analysis,
whereas the difference between Medium and Low items
did not reach significance. These results show that mean-
ing paraphrases derived by the model correspond to hu-
man intuitions: paraphrases assigned high probabilities
by the model are perceived as better than paraphrases that
are assigned low probabilities.
We further explored the linear relationship between
the subjects? rankings and the corpus-based model, using
correlation analysis. The elicited judgments were com-
Rank ? SD SE Min Max
High ?20.5 1.71 .18 ?24.0 ?15.9
Medium ?22.6 .99 .10 ?25.2 ?20.2
Low ?23.9 .86 .18 ?25.9 ?22.5
Table 4: Descriptive statistics for model-derived proba-
bilities
Rank ? SD SE Min Max
High ?.0005 .2974 .0384 ?.68 .49
Medium ?.1754 .3284 .0424 ?.70 .31
Low ?.2298 .3279 .0423 ?.68 .37
Table 5: Descriptive statistics for Experiment 1, by sub-
jects
pared with the interpretation probabilities which were
obtained from the model described in Section 2 to exam-
ine the extent to which the proposed interpretations cor-
relate with human intuitions. A comparison between our
model and the human judgments yielded a Pearson corre-
lation coefficient of .40 (p < .01, N = 270). This verifies
the Probability Band effect discovered by the ANOVA, in
an analysis which compares the individual interpretation
likelihood for each item with elicited interpretation pref-
erences, instead of collapsing all the items in three equiv-
alence classes (i.e., High, Medium, Low). In order to
evaluate whether the grammatical function has any effect
on the relationship between the model-derived meanings
and the human judgments, we split the items into those
that received a subject interpretation versus those that re-
ceived an object interpretation. A comparison between
our model and the human judgments yielded a corre-
lation of r = .53 (p < .01, N = 135) for object-related
items and a correlation of r = .21 (p < .05, N = 135)
for subject-related items. Note that a weaker correlation
is obtained for subject-related interpretations. One expla-
nation for that could be the parser?s performance, i.e., the
parser is better at extracting verb-object tuples than verb-
subject tuples. Another hypothesis (which we test be-
low) is that most adjectives included in the experimental
stimuli have an object-bias, and therefore subject-related
interpretations are generally less preferred than object-
related ones.
An important question is how well humans agree in
their paraphrase judgments for adjective-noun combina-
tions. Inter-subject agreement gives an upper bound for
the task and allows us to interpret how well the model is
doing in relation to humans. For each subject group we
performed correlations on the elicited judgments using
leave-one-out resampling (Weiss and Kulikowski, 1991).
For the first group, the average inter-subject agreement
was .67 (Min = .03, Max = .82, SD = .14), and for the
second group .65 (Min = .05, Max = .82, SD = .14).
This means that our model performs satisfactorily given
that humans do not perfectly agree in their judgments.
The elicited judgments can be further used to derive
Adj Model ? SD SE Subjects ? SD SE
diffi-
p
OBJ ?21.6 1.36 .04
p
OBJ .07 .36 .07
cult SUBJ ?21.8 1.34 .05 SUBJ ?.29 .28 .05
easy
p
OBJ ?21.6 1.51 .05
p
OBJ .10 .34 .06
SUBJ ?22.1 1.36 .06 SUBJ ?.14 .23 .04
fast OBJ ?24.2 1.27 .13 OBJ ?.35 .29 .05
p
SUBJ ?23.8 1.40 .14
p
SUBJ ?.15 .45 .08
good OBJ ?22.1 1.28 .06 OBJ ?.01 .39 .07
SUBJ ?22.3 1.10 .07 SUBJ ?.16 .30 .05
hard
p
OBJ ?21.7 1.53 .06
p
OBJ .01 .34 .06
SUBJ ?22.1 1.35 .06 SUBJ ?.25 .24 .04
right
p
OBJ ?21.7 1.36 .04
p
OBJ ?.01 .25 .05
SUBJ ?21.8 1.24 .04 SUBJ ?.24 .44 .08
safe OBJ ?22.7 1.48 .10
p
OBJ .01 .25 .05
p
SUBJ ?22.4 1.59 .12 SUBJ ?.34 .43 .08
slow OBJ ?22.5 1.53 .08 OBJ ?.30 .48 .08
SUBJ ?22.3 1.50 .07
p
SUBJ ?.09 .24 .04
wrong OBJ ?23.2 1.33 .08
p
OBJ ?.04 .25 .05
SUBJ ?23.3 1.30 .08 SUBJ ?.24 .37 .08
Table 6: Log-transformed model-derived and subject-
based argument preferences for polysemous adjectives
the grammatical function preferences (i.e., subject or ob-
ject) for a given adjective. In particular, we can determine
which is the preferred interpretation for individual adjec-
tives and compare these preferences against the ones pro-
duced by our model. Argument preferences can be easily
derived from the model?s output by comparing subject-
related and object-related paraphrases. For each adjective
we gathered the subject and object-related interpretations
derived by the model and performed an ANOVA in order
to determine the significance of the Grammatical Func-
tion effect.
We interpret a significant effect as bias towards a par-
ticular grammatical function. We classify an adjective as
object-biased if the mean of the judgments for the object
interpretation of this particular adjective is larger than the
mean for the subject interpretation; subject-biased adjec-
tives are classified accordingly, whereas adjectives for
which no effect of Grammatical Function is found are
classified as equi-biased. Table 6 shows the biases for the
nine adjectives as derived by our model. The presence
of the symbol
p
indicates significance of the Grammat-
ical Function effect as well as the direction of the bias.
Argument preferences were elicited from human sub-
jects in a similar fashion. For each adjective we gathered
the elicited responses pertaining to subject- and object-
related interpretations and performed an ANOVA. The bi-
ases and the significance of the Grammatical Function
effect (p) are shown in Table 6.
Comparison of the biases derived from the model with
ones derived from the elicited judgments shows that the
model and the humans are in agreement for all adjec-
tives but slow, wrong and safe. On the basis of human
judgments slow has a subject bias, whereas wrong has
an object bias. Although the model could not reproduce
this result there is a tendency in the right direction (see
Table 6).
Note that in our correlation analysis reported above the
elicited judgments were compared against model-derived
paraphrases without taking argument preferences into ac-
count. We would expect a correct model to produce intu-
itive meanings at least for the interpretation a given ad-
jective favors. We further examined the model?s behav-
ior by performing separate correlation analyses for pre-
ferred and dispreferred biases as determined previously
by the ANOVAs conducted for each adjective. Since the
adjective good was equi-biased we included both biases
(i.e., object-related and subject-related) in both correla-
tion analyses. The comparison between our model and
the human judgments yielded a Pearson correlation co-
efficient of .52 (p < .01, N = 150) for the preferred in-
terpretations and a correlation of .23 (p < .01, N = 150)
for the dispreferred interpretations. The result indicates
that our model is particularly good at deriving meanings
corresponding to the argument-bias for a given adjective.
However, the dispreferred interpretations also correlate
significantly with human judgments, which suggests that
the model derives plausible interpretations even in cases
where the default argument bias is overridden.
4 Experiment 2: Comparison against
Naive Baseline
The probabilistic model described in Section 2 explic-
itly takes adjective/adverb and verb co-occurrences into
account. However, one could derive meanings for poly-
semous adjective-noun combinations by solely concen-
trating on verb-noun relations, ignoring thus the adjec-
tive/adverb and verb dependencies. For example, in or-
der to interpret the combination easy problem we could
simply take into account the types of activities which
are related with problems (i.e., solving them, setting
them, etc.). This simplification is consistent with Puste-
jovsky?s (1995) claim that polysemous adjectives like
easy are predicates, modifying the events associated with
the noun. A ?naive? or ?baseline? model would be one
which simply takes into account the number of times the
noun in the adjective-noun pair acts as the subject or ob-
ject of a given verb, ignoring the adjective completely.
4.1 Naive Model
Given an adjective-noun combination we are interested
in finding the verbs whose object or subject is the noun
appearing in the adjective-noun combination. This can be
simply expressed as P(vjrel,n), the conditional probabil-
ity of a verb v given an argument-noun relation rel,n:
P(vjrel,n) = f (v,rel,n)f (rel,n)(18)
The model in (18) assumes that the meaning of an
adjective-noun combination is independent of the ad-
jective in question. The model in (18) would come up
with the same probabilities for fast plane and wrong
plane since it does not take the identity of the modi-
fying adjective into account. We estimated the frequen-
cies f (v,rel,n) and f (rel,n) from verb-object and verb-
subject tuples extracted from the BNC using Cass (Ab-
ney, 1996).
4.2 Method
Using the naive model we calculated the meaning prob-
ability for each of the 270 stimuli included in Experi-
ment 1. Through correlation analysis we explored the
linear relationship between the elicited judgments and
the naive baseline model. We further directly compared
the two models, our initial, linguistically more informed
model, and the naive baseline.
4.3 Results
Using correlation analysis we explored which model
performs better at deriving meanings for adjective-noun
combinations. A comparison between the naive model?s
probabilities and the human judgments yielded a Pearson
correlation coefficient of .25 (p < .01, N = 270). Recall
that we obtained a correlation of .40 (p < .01, N = 270)
when comparing our original model to the human judg-
ments. Not surprisingly the two models are intercorre-
lated (r = .38, p < .01, N = 270). An important question
is whether the difference between the two correlation co-
efficients (r = .40 and r = .25) is due to chance. Compar-
ison of the two correlation coefficients revealed that their
difference was significant (t(267) = 2.42, p < .01). This
means that our original model performs reliably better
than a naive baseline at deriving interpretations for poly-
semous adjective-noun combinations.
We further compared the naive baseline model and
the human judgments separately for subject-related and
object-related items. The comparison yielded a correla-
tion of r = .29 (p < .01, N = 135) for object interpreta-
tions. Recall that our original model yielded a correlation
coefficient of .53. The two correlation coefficients were
significantly different (t(132) = 3.03, p < .01). No cor-
relation was found for the naive model when compared
against elicited subject interpretations (r = .09, p = .28,
N = 135).
5 Conclusions
In this paper we showed how adjectival meanings can be
acquired from a large corpus and provided a probabilis-
tic model which derives a preference ordering on the set
of possible interpretations. Our model does not only ac-
quire clusters of meanings (following Vendler?s (1968)
insight) but furthermore can be used to obtain argument
preferences for a given adjective.
We rigorously evaluated the results of our model by
eliciting paraphrase judgments from subjects naive to lin-
guistic theory. Comparison between our model and hu-
man judgments yielded a reliable correlation of .40 when
the upper bound for the task (i.e., inter-subject agree-
ment) is approximately .65. Furthermore, our model per-
formed reliably better than a naive baseline model, which
only achieved a correlation of .25. Although adjective-
noun polysemy is a well researched phenomenon in
the theoretical linguistics literature, the experimental ap-
proach advocated here is new to our knowledge.
Furthermore, the proposed model can be viewed as
complementary to linguistic theory: it automatically de-
rives a ranking of meanings, thus distinguishing likely
from unlikely interpretations. Even if linguistic theory
was able to enumerate all possible interpretations for a
given adjective (note that in the case of polysemous ad-
jectives we would have to take into account all nouns
or noun classes the adjective could possibly modify)
it has no means to indicate which ones are likely and
which ones are not. Our model fares well on both tasks.
It recasts the problem of adjective-noun polysemy in a
probabilistic framework deriving a large number of in-
terpretations not readily available from linguistic intro-
spection. The information acquired from the corpus can
be also used to quantify the argument preferences of a
given adjective. These are only implicit in the lexical
semantics literature where certain adjectives are exclu-
sively given a verb-subject or verb-object interpretation.
We have demonstrated that we can empirically derive ar-
gument biases for a given adjective that correspond to
human intuitions.
References
Steve Abney. 1996. Partial parsing via finite-state cascades. In
John Carroll, editor, Workshop on Robust Parsing, pages 8?
15, Prague. European Summer School in Logic, Language
and Information.
Ellen Gurman Bard, Dan Robertson, and Antonella Sorace.
1996. Magnitude estimation of linguistic acceptability.
Language, 72(1):32?68.
Lou Burnard, 1995. Users Guide for the British National Cor-
pus. British National Corpus Consortium, Oxford Univer-
sity Computing Service.
Steffan Corley, Martin Corley, Frank Keller, Matthew W.
Crocker, and Shari Trewin. 2000. Finding syntactic struc-
ture in unparsed corpora: The Gsearch corpus query system.
Computers and the Humanities. To appear.
Wayne Cowart. 1997. Experimental Syntax: Applying Ob-
jective Methods to Sentence Judgments. Sage Publications,
Thousand Oaks, CA.
Frank Keller, Martin Corley, Steffan Corley, Lars Konieczny,
and Amalia Todirascu. 1998. WebExp: A Java toolbox
for web-based psychological experiments. Technical Re-
port HCRC/TR-99, Human Communication Research Cen-
tre, University of Edinburgh.
James Pustejovsky. 1995. The Generative Lexicon. The MIT
Press, Cambridge, MA.
Carson T. Schu?tze. 1996. The Empirical Base of Linguis-
tics: Grammaticality Judgments and Linguistic Methodol-
ogy. University of Chicago Press, Chicago.
S. Smith Stevens. 1975. Psychophysics: Introduction to its
Perceptual, Neural, and Social Prospects. John Wiley, New
York.
Zeno Vendler. 1968. Adjectives and Nominalizations. Mou-
ton, The Hague.
Sholom M. Weiss and Casimir A. Kulikowski. 1991. Com-
puter Systems that Learn: Classification and Prediction
Methods from Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann, San Mateo, CA.
Evaluating Smoothing Algorithms against Plausibility Judgements
Maria Lapata and Frank Keller
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
fmlap, kellerg@coli.uni-sb.de
Scott McDonald
Language Technology Group
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
scottm@cogsci.ed.ac.uk
Abstract
Previous research has shown that the
plausibility of an adjective-noun com-
bination is correlated with its corpus
co-occurrence frequency. In this paper,
we estimate the co-occurrence frequen-
cies of adjective-noun pairs that fail to
occur in a 100 million word corpus
using smoothing techniques and com-
pare them to human plausibility rat-
ings. Both class-based smoothing and
distance-weighted averaging yield fre-
quency estimates that are significant
predictors of rated plausibility, which
provides independent evidence for the
validity of these smoothing techniques.
1 Introduction
Certain combinations of adjectives and nouns are
perceived as more plausible than others. A classi-
cal example is strong tea, which is highly plausi-
ble, as opposed to powerful tea, which is not. On
the other hand, powerful car is highly plausible,
whereas strong car is less plausible. It has been
argued in the theoretical literature that the plausi-
bility of an adjective-noun pair is largely a collo-
cational (i.e., idiosyncratic) property, in contrast
to verb-object or noun-noun plausibility, which is
more predictable (Cruse, 1986; Smadja, 1991).
The collocational hypothesis has recently
been investigated in a corpus study by
Lapata et al (1999). This study investigated
potential statistical predictors of adjective-noun
plausibility by using correlation analysis to com-
pare judgements elicited from human subjects
with five corpus-derived measures: co-occurrence
frequency of the adjective-noun pair, noun
frequency, conditional probability of the noun
given the adjective, the log-likelihood ratio, and
Resnik?s (1993) selectional association measure.
All predictors but one were positively correlated
with plausibility; the highest correlation was
obtained with co-occurrence frequency. Resnik?s
selectional association measure surprisingly
yielded a significant negative correlation with
judged plausibility. These results suggest that
the best predictor of whether an adjective-noun
combination is plausible or not is simply how
often the adjective and the noun collocate in a
record of language experience.
As a predictor of plausibility, co-occurrence
frequency has the obvious limitation that it can-
not be applied to adjective-noun pairs that never
occur in the corpus. A zero co-occurrence count
might be due to insufficient evidence or might
reflect the fact that the adjective-noun pair is in-
herently implausible. In the present paper, we ad-
dress this problem by using smoothing techniques
(distance-weighted averaging and class-based
smoothing) to recreate missing co-occurrence
counts, which we then compare to plausibility
judgements elicited from human subjects. By
demonstrating a correlation between recreated
frequencies and plausibility judgements, we show
that these smoothing methods produce realistic
frequency estimates for missing co-occurrence
data. This approach allows us to establish the va-
lidity of smoothing methods independent from a
specific natural language processing task.
2 Smoothing Methods
Smoothing techniques have been used in a variety
of statistical natural language processing applica-
tions as a means to address data sparseness, an in-
herent problem for statistical methods which rely
on the relative frequencies of word combinations.
The problem arises when the probability of word
combinations that do not occur in the training
data needs to be estimated. The smoothing meth-
ods proposed in the literature (overviews are pro-
vided by Dagan et al (1999) and Lee (1999)) can
be generally divided into three types: discount-
ing (Katz, 1987), class-based smoothing (Resnik,
1993; Brown et al, 1992; Pereira et al, 1993),
and distance-weighted averaging (Grishman and
Sterling, 1994; Dagan et al, 1999).
Discounting methods decrease the probability
of previously seen events so that the total prob-
ability of observed word co-occurrences is less
than one, leaving some probability mass to be re-
distributed among unseen co-occurrences.
Class-based smoothing and distance-weighted
averaging both rely on an intuitively simple idea:
inter-word dependencies are modelled by relying
on the corpus evidence available for words that
are similar to the words of interest. The two ap-
proaches differ in the way they measure word
similarity. Distance-weighted averaging estimates
word similarity from lexical co-occurrence infor-
mation, viz., it finds similar words by taking into
account the linguistic contexts in which they oc-
cur: two words are similar if they occur in sim-
ilar contexts. In class-based smoothing, classes
are used as the basis according to which the co-
occurrence probability of unseen word combina-
tions is estimated. Classes can be induced directly
from the corpus (Pereira et al, 1993; Brown et al,
1992) or taken from a manually crafted taxonomy
(Resnik, 1993). In the latter case the taxonomy is
used to provide a mapping from words to concep-
tual classes.
In language modelling, smoothing techniques
are typically evaluated by showing that a lan-
guage model which uses smoothed estimates in-
curs a reduction in perplexity on test data over a
model that does not employ smoothed estimates
(Katz, 1987). Dagan et al (1999) use perplexity
to compare back-off smoothing against distance-
weighted averaging methods and show that the
latter outperform the former. They also com-
pare different distance-weighted averaging meth-
ods on a pseudo-word disambiguation task where
the language model decides which of two verbs
v1 and v2 is more likely to take a noun n as its
object. The method being tested must reconstruct
which of the unseen (v1,n) and (v2,n) is a valid
verb-object combination.
In our experiments we recreated co-occurrence
frequencies for unseen adjective-noun pairs using
two different approaches: taxonomic class-based
smoothing and distance-weighted averaging.1 We
evaluated the recreated frequencies by comparing
them with plausibility judgements elicited from
human subjects. In contrast to previous work, this
type of evaluation does not presuppose that the
recreated frequencies are needed for a specific
natural language processing task. Rather, our aim
is to establish an independent criterion for the
validity of smoothing techniques by comparing
them to plausibility judgements, which are known
to correlate with co-occurrence frequency (Lapata
et al, 1999).
In the remainder of this paper we present class-
1Discounting methods were not included as
Dagan et al (1999) demonstrated that distance-weighted
averaging achieves better language modelling performance
than back-off.
based smoothing and distance-weighted averag-
ing as applied to unseen adjective-noun combina-
tions (see Sections 2.1 and 2.2). Section 3 details
our judgement elicitation experiment and reports
our results.
2.1 Class-based Smoothing
We recreated co-occurrence frequencies for un-
seen adjective-noun pairs using a simplified ver-
sion of Resnik?s (1993) selectional association
measure. Selectional association is defined as the
amount of information a given predicate carries
about its argument, where the argument is rep-
resented by its corresponding classes in a taxon-
omy such as WordNet (Miller et al, 1990). This
means that predicates which impose few restric-
tions on their arguments have low selectional as-
sociation values, whereas predicates selecting for
a restricted number of arguments have high se-
lectional association values. Consider the verbs
see and polymerise: intuitively there is a great
variety of things which can be seen, whereas
there is a very specific set of things which can
be polymerised (e.g., ethylene). Resnik demon-
strated that his measure of selectional associa-
tion successfully captures this intuition: selec-
tional association values are correlated with verb-
argument plausibility as judged by native speak-
ers.
However, Lapata et al (1999) found that the
success of selectional association as a predictor
of plausibility does not seem to carry over to
adjective-noun plausibility. There are two poten-
tial reasons for this: (1) the semantic restrictions
that adjectives impose on the nouns with which
they combine appear to be less strict than the
ones imposed by verbs (consider the adjective su-
perb which can combine with nearly any noun);
and (2) given their lexicalist nature, adjective-
noun combinations may defy selectional restric-
tions yet be intuitively plausible (consider the pair
sad day, where sadness is not an attribute of day).
To address these problems, we replaced
Resnik?s information-theoretic measure with a
simpler measure which makes no assumptions
with respect to the contribution of a semantic
class to the total quantity of information provided
by the predicate about the semantic classes of
its argument. We simply substitute the noun oc-
curring in the adjective-noun combination with
the concept by which it is represented in the
taxonomy and estimate the adjective-noun co-
occurrence frequency by counting the number of
times the concept corresponding to the noun is ob-
served to co-occur with the adjective in the cor-
pus. Because a given word is not always repre-
sented by a single class in the taxonomy (i.e., the
Adjective Class f (a,n)
proud hentityi 13.70
proud hlife fromi 9.80
proud hcausal agenti 9.50
proud hpersoni 9.00
proud hleaderi .75
proud hsuperiori .08
proud hsupervisori .00
Table 1: Frequency estimation for proud chief us-
ing WordNet
noun co-occurring with an adjective can gener-
ally be the realisation of one of several conceptual
classes), we constructed the frequency counts for
an adjective-noun pair for each conceptual class
by dividing the contribution from the adjective by
the number of classes to which it belongs (Lauer,
1995; Resnik, 1993):
f (a,c)  ?
n02c
f (a,n0)
jclasses(n0)j(1)
where f (a,n0) is the number of times the ad-
jective a was observed in the corpus with con-
cept c 2 classes(n0) and jclasses(n0)j is the num-
ber of conceptual classes noun n0 belongs to. Note
that the estimation of the frequency f (a,c) relies
on the simplifying assumption that the noun co-
occurring with the adjective is distributed evenly
across its conceptual classes. This simplification
is necessary unless we have a corpus of adjective-
noun pairs labelled explicitly with taxonomic in-
formation.2
Consider the pair proud chief which is
not attested in the British National Corpus
(BNC) (Burnard, 1995). The word chief has
two senses in WordNet and belongs to seven
conceptual classes (hcausal agenti, hentityi,
hleaderi, hlife formi, hpersoni, hsuperiori,
and hsupervisori) This means that the co-
occurrence frequency of the adjective-noun pair
will be constructed for each of the seven classes,
as shown in Table 1. Suppose for example that
we see the pair proud leader in the corpus. The
word leader has two senses in WordNet and
belongs to eight conceptual classes (hpersoni,
hlife fromi, hentityi, hcausal agenti,
hfeaturei, hmerchandisei, hcommodityi, and
hobjecti). The words chief and leader have four
conceptual classes in common, i.e., hpersoni and
hlife formi, hentityi, and hcausal agenti.
This means that we will increment the observed
co-occurrence count of proud and hpersoni,
proud and hlife formi, proud and hentityi,
and proud and hcausal agenti by 18 . Since we
2There are several ways of addressing this problem, e.g.,
by discounting the contribution of very general classes by
finding a suitable class to represent a given concept (Clark
and Weir, 2001).
do not know the actual class of the noun chief in
the corpus, we weight the contribution of each
class by taking the average of the constructed
frequencies for all seven classes:
f (a,n) =
?
c2classes(n)
?
n02c
f (a,n0)
jclasses(n0)j
jclasses(n)j(2)
Based on (2) the recreated frequency for the pair
proud chief in the BNC is 6.12 (see Table 1).
2.2 Distance-Weighted Averaging
Distance-weighted averaging induces classes of
similar words from word co-occurrences with-
out making reference to a taxonomy. A key fea-
ture of this type of smoothing is the function
which measures distributional similarity from co-
occurrence frequencies. Several measures of dis-
tributional similarity have been proposed in the
literature (Dagan et al, 1999; Lee, 1999). We
used two measures, the Jensen-Shannon diver-
gence and the confusion probability. Those two
measures have been previously shown to give
promising performance for the task of estimat-
ing the frequencies of unseen verb-argument pairs
(Dagan et al, 1999; Grishman and Sterling, 1994;
Lapata, 2000; Lee, 1999). In the following we
describe these two similarity measures and show
how they can be used to recreate the frequencies
for unseen adjective-noun pairs.
Jensen-Shannon Divergence. The Jensen-
Shannon divergence is an information-theoretic
measure that recasts the concept of distributional
similarity into a measure of the ?distance?
(i.e., dissimilarity) between two probability
distributions.
Let w1 and w01 be an unseen sequence of
two words whose distributional similarity is to
be determined. Let P(w2jw1) denote the condi-
tional probability of word w2 given word w1 and
P(w2jw01) denote the conditional probability of
w2 given w01. For notational simplicity we write
p(w2) for P(w2jw1) and q(w2) for P(w2jw01). The
Jensen-Shannon divergence is defined as the av-
erage Kullback-Leibler divergence of each of two
distributions to their average distribution:
J(p,q) =
1
2

D

p
?
?
?
?
p+q
2

+D

q
?
?
?
?
p+q
2

(3)
where (p+q)/2 denotes the average distribution:
1
2
(
P(w2jw1)+P(w2jw01)
(4)
The Kullback-Leibler divergence is an
information-theoretic measure of the dissim-
ilarity of two probability distributions p and q,
defined as follows:
D(pjjq) = ?
i
pi log
pi
qi
(5)
In our case the distributions p and q are the
conditional probability distributions P(w2jw1)
and P(w2jw01), respectively. Computation of the
Jensen-Shannon divergence depends only on the
linguistic contexts w2 which the two words w1
and w01 have in common. The Jensen-Shannon di-
vergence, a dissimilarity measure, is transformed
to a similarity measure as follows:
WJ(p,q) = 10??J(p,q)(6)
The parameter ? controls the relative influence of
the words most similar to w1: if ? is high, only
words extremely similar to w1 contribute to the
estimate, whereas if ? is low, less similar words
also contribute to the estimate.
Confusion Probability. The confusion proba-
bility is an estimate of the probability that word
w01 can be substituted by word w1, in the sense of
being found in the same linguistic contexts.
Pc(w1jw01) = ?
w2
P(w1jw2)P(w2jw01)(7)
where Pc(w01jw1) is the probability that word w01
occurs in the same contexts w2 as word w1, aver-
aged over these contexts.
Let w2w1 be two unseen co-occurring words.
We can estimate the conditional probability
P(w2jw1) of the unseen word pair w2w1 by com-
bining estimates for co-occurrences involving
similar words:
PSIM(w2jw1) = ?
w012S(w1)
W (w1,w01)
N(w1)
P(w2jw01)(8)
where S(w1) is the set of words most similar to
w1, W (w1,w01) is the similarity function between
w1 and w01, and N(w1) is a normalising factor
N(w1) = ?w01 W (w1,w01). The conditional proba-
bility PSIM(w2jw1) can be trivially converted to
co-occurrence frequency as follows:
f (w1,w2) = PSIM(w2jw1) f (w1)(9)
Parameter Settings. We experimented with
two approaches to computing P(w2jw01): (1) us-
ing the probability distribution P(nja), which dis-
covers similar adjectives and treats the noun as
the context; and (2) using P(ajn), which discovers
similar nouns and treats the adjective as the con-
text. These conditional probabilities can be easily
estimated from their relative frequency in the cor-
pus as follows:
P(nja) =
f (a,n)
f (a) P(ajn) =
f (a,n)
f (n)(10)
The performance of distance-weighted averaging
depends on two parameters: (1) the number of
items over which the similarity function is com-
puted (i.e., the size of the set S(w1) denoting the
set of words most similar to w1), and (2) the
Jensen-Shannon Confusion Probability
proud chief proud chief
young chairman lone venture
old venture adverse chairman
dying government grateful importance
wealthy leader sole force
lone official wealthy representative
dead scientist elderly president
rich manager registered official
poor initiative dear manager
elderly president deliberate director
Table 2: The ten most similar adjectives to proud
and the ten most similar nouns to chief
value of the parameter ? (which is only relevant
for the Jensen-Shannon divergence). In this study
we recreated adjective-noun frequencies using
the 1,000 and 2,000 most frequent items (nouns
and adjectives), for both the confusion probabil-
ity and the Jensen-Shannon divergence.3 Further-
more, we set ? to .5, which experiments showed
to be the best value for this parameter.
Once we know which words are most simi-
lar to the either the adjective or the noun (irre-
spective of the function used to measure similar-
ity) we can exploit this information in order to
recreate the co-occurrence frequency for unseen
adjective-noun pairs. We use the weighted aver-
age of the evidence provided by the similar words,
where the weight given to a word w01 depends
on its similarity to w1 (see (8) and (9)). Table 2
shows the ten most similar adjectives to the word
proud and then the ten most similar nouns to the
word chief using the Jensen-Shannon divergence
and the confusion probability. Here the similarity
function was calculated over the 1,000 most fre-
quent adjectives in the BNC.
3 Collecting Plausibility Ratings
In order to evaluate the smoothing methods intro-
duced above, we first needed to establish an inde-
pendent measure of plausibility. The standard ap-
proach used in experimental psycholinguistics is
to elicit judgements from human subjects; in this
section we describe our method for assembling
the set of experimental materials and collecting
plausibility ratings for these stimuli.
3.1 Method
Materials. We used a part-of-speech annotated,
lemmatised version of the BNC. The BNC is a
large, balanced corpus of British English, consist-
ing of 90 million words of text and 10 million
words of speech. Frequency information obtained
3These were shown to be the best parameter settings by
Lapata (2000). Note that considerable latitude is available
when setting these parameters; there are 151,478 distinct ad-
jective types and 367,891 noun types in the BNC.
Adjective Nouns
hungry tradition innovation prey
guilty system wisdom wartime
temporary conception surgery statue
naughty regime rival protocol
Table 3: Example stimuli for the plausibility
judgement experiment
from the BNC can be expected to be a reason-
able approximation of the language experience of
a British English speaker.
The experiment used the same set of 30 adjec-
tives discussed in Lapata et al (1999). These ad-
jectives were chosen to be minimally ambiguous:
each adjective had exactly two senses according
to WordNet and was unambiguously tagged as
?adjective? 98.6% of the time, measured as the
number of different part-of-speech tags assigned
to the word in the BNC. For each adjective we
obtained all the nouns (excluding proper nouns)
with which it failed to co-occur in the BNC.
We identified adjective-noun pairs by using
Gsearch (Corley et al, 2001), a chart parser which
detects syntactic patterns in a tagged corpus by
exploiting a user-specified context free grammar
and a syntactic query. From the syntactic anal-
ysis provided by the parser we extracted a ta-
ble containing the adjective and the head of the
noun phrase following it. In the case of compound
nouns, we only included sequences of two nouns,
and considered the rightmost occurring noun as
the head. From the adjective-noun pairs obtained
this way, we removed all pairs where the noun
had a BNC frequency of less than 10 per million,
in order to reduce the risk of plausibility ratings
being influenced by the presence of a noun un-
familiar to the subjects. Each adjective was then
paired with three randomly-chosen nouns from its
list of non-co-occurring nouns. Example stimuli
are shown in Table 3.
Procedure. The experimental paradigm was
magnitude estimation (ME), a technique stan-
dardly used in psychophysics to measure judge-
ments of sensory stimuli (Stevens, 1975), which
Bard et al (1996) and Cowart (1997) have ap-
plied to the elicitation of linguistic judgements.
The ME procedure requires subjects to estimate
the magnitude of physical stimuli by assigning
numerical values proportional to the stimulus
magnitude they perceive. In contrast to the 5- or
7-point scale conventionally used to measure hu-
man intuitions, ME employs an interval scale, and
therefore produces data for which parametric in-
ferential statistics are valid.
ME requires subjects to assign numbers to
a series of linguistic stimuli in a proportional
Plaus Jena Confa Jenn Confn
Jena .058
Confa .214* .941**
Jenn .124 .781** .808**
Confn .232* .782** .864** .956**
WN .356** .222* .348** .451** .444**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 4: Correlation matrix for plausibility and
the five smoothed frequency estimates
fashion. Subjects are first exposed to a modulus
item, which they assign an arbitrary number. All
other stimuli are rated proportional to the modu-
lus. In this way, each subject can establish their
own rating scale, thus yielding maximally fine-
graded data and avoiding the known problems
with the conventional ordinal scales for linguis-
tic data (Bard et al, 1996; Cowart, 1997; Schu?tze,
1996).
In the present experiment, subjects were pre-
sented with adjective-noun pairs and were asked
to rate the degree of adjective-noun fit propor-
tional to a modulus item. The experiment was car-
ried out using WebExp, a set of Java-Classes for
administering psycholinguistic studies over the
World-Wide Web (Keller et al, 1998). Subjects
first saw a set of instructions that explained the
ME technique and included some examples, and
had to fill in a short questionnaire including basic
demographic information. Each subject saw the
entire set of 90 experimental items.
Subjects. Forty-one native speakers of English
volunteered to participate. Subjects were re-
cruited over the Internet by postings to relevant
newsgroups and mailing lists.
3.2 Results
Correlation analysis was used to assess the degree
of linear relationship between plausibility ratings
(Plaus) and the three smoothed co-occurrence
frequency estimates: distance-weighted averaging
using Jensen-Shannon divergence (Jen), distance-
weighted averaging using confusion probability
(Conf), and class-based smoothing using Word-
Net (WN). For the two similarity-based measures,
we smoothed either over the similarity of the ad-
jective (subscript a) or over the similarity of the
noun (subscript n). All frequency estimates were
natural log-transformed.
Table 4 displays the results of the corre-
lation analysis. Mean plausibility ratings were
significantly correlated with co-occurrence fre-
quency recreated using our class-based smooth-
ing method based on WordNet (r = .356, p <
.01).
As detailed in Section 2.2, the Jensen-Shannon
divergence and the confusion probability are pa-
rameterised measures. There are two ways to
smooth the frequency of an adjective-noun com-
bination: over the distribution of adjectives or
over the distribution of nouns. We tried both ap-
proaches and found a moderate correlation be-
tween plausibility and both the frequency recre-
ated using distance-weighted averaging and con-
fusion probability. The correlation was significant
both for frequencies recreated by smoothing over
adjectives (r = .214, p < .05) and over nouns
(r = .232, p < .05). However, co-occurrence fre-
quency recreated using the Jensen-Shannon di-
vergence was not reliably correlated with plausi-
bility. Furthermore, there was a reliable correla-
tion between the two Jensen-Shannon measures
Jena and Jenn (r = .781, p < .01), and similarly
between the two confusion measures Confa and
Confn (r = .864, p < .01). We also found a high
correlation between Jena and Confa (r = .941,
p < .01) and Jenn and Confn (r = .956, p < .01).
This indicates that the two similarity measures
yield comparable results for the given task.
We also examined the effect of varying one
further parameter (see Section 2.2). The recre-
ated frequencies were initially estimated using
the n = 1,000 most similar items. We examined
the effects of applying the two smoothing meth-
ods using a set of similar items of twice the size
(n = 2,000). No improvement in terms of the cor-
relations with rated plausibility was found when
using this larger set, whether smoothing over the
adjective or the noun: a moderate correlation with
plausibility was found for Confa (r = .239, p <
.05) and Confn (r = .239, p < .05), while the cor-
relation with Jena and Jenn was not significant.
An important question is how well people agree
in their plausibility judgements. Inter-subject
agreement gives an upper bound for the task and
allows us to interpret how well the smoothing
techniques are doing in relation to the human
judges. We computed the inter-subject correlation
on the elicited judgements using leave-one-out re-
sampling (Weiss and Kulikowski, 1991). Aver-
age inter-subject agreement was .55 (Min = .01,
Max = .76, SD = .16). This means that our ap-
proach performs satisfactorily given that there is
a fair amount of variability in human judgements
of adjective-noun plausibility.
One remaining issue concerns the validity
of our smoothing procedures. We have shown
that co-occurrence frequencies recreated using
smoothing techniques are significantly correlated
with rated plausibility. But this finding consti-
tutes only indirect evidence for the ability of this
method to recreate corpus evidence; it depends on
the assumption that plausibility and frequency are
adequate indicators of each other?s values. Does
WN Jena Confa Jenn Confn
Actual freq. .218* .324** .646** .308** .728**
Plausibility .349** .268* .395** .247* .416**
*p < .05 (2-tailed) **p < .01 (2-tailed)
Table 5: Correlation of recreated frequencies with
actual frequencies and plausibility (using Lapata
et al?s (1999) stimuli)
smoothing accurately recreate the co-occurrence
frequency of combinations that actually do occur
in the corpus? To address this question, we ap-
plied the class-based smoothing procedure to a
set of adjective-noun pairs that occur in the cor-
pus with varying frequencies, using the materials
from Lapata et al (1999).
First, we removed all relevant adjective-noun
combinations from the corpus. Effectively we
assumed a linguistic environment with no evi-
dence for the occurrence of the pair, and thus
no evidence for any linguistic relationship be-
tween the adjective and the noun. Then we recre-
ated the co-occurrence frequencies using class-
based smoothing and distance-weighted averag-
ing, and log-transformed the resulting frequen-
cies. Both methods yielded reliable correlation
between recreated frequency and actual BNC fre-
quency (see Table 5 for details). This result pro-
vides additional evidence for the claim that these
smoothing techniques produce reliable frequency
estimates for unseen adjective-noun pairs. Note
that the best correlations were achieved for Confa
and Confn (r = .646, p < .01 and r = .728, p <
.01, respectively).
Finally, we carried out a further test of the
quality of the recreated frequencies by correlat-
ing them with the plausibility judgements re-
ported by Lapata et al (1999). Again, a signifi-
cant correlation was found for all methods (see
Table 5). However, all correlations were lower
than the correlation of the actual frequencies
with plausibility (r = .570, p < .01) reported
by Lapata et al (1999). Note also that the con-
fusion probability outperformed Jensen-Shannon
divergence, in line with our results on unfamiliar
adjective-noun pairs.
3.3 Discussion
Lapata et al (1999) demonstrated that the co-
occurrence frequency of an adjective-noun com-
bination is the best predictor of its rated plausibil-
ity. The present experiment extended this result to
adjective-noun pairs that do not co-occur in the
corpus.
We applied two smoothing techniques in order
to recreate co-occurrence frequency and found
that the class-based smoothing method was the
best predictor of plausibility. This result is inter-
guilty dangerous stop giant
guilty dangerous stop giant
interested certain moon company
innocent different employment manufacturer
injured particular length artist
labour difficult detail industry
socialist other page firm
strange strange time star
democratic similar potential master
ruling various list army
honest bad turn rival
Table 6: The ten most similar words to the adjec-
tives guilty and dangerous and the nouns stop and
giant discovered by the Jensen-Shannon measure
esting because the class-based method does not
use detailed knowledge about word-to-word rela-
tionships in real language; instead, it relies on the
notion of equivalence classes derived from Word-
Net, a semantic taxonomy. It appears that making
predictions about plausibility is most effectively
done by collapsing together the speaker?s experi-
ence with other words in the semantic class occu-
pied by the target word.
The distance-weighted averaging smoothing
methods yielded a lower correlation with plausi-
bility (in the case of the confusion probability),
or no correlation at all (in the case of the Jensen-
Shannon divergence). The worse performance of
distance-weighted averaging is probably due to
the fact that this method conflates two kinds of
distributional similarity: on the one hand, it gen-
erates words that are semantically similar to the
target word. On the other hand, it also generates
words whose syntactic behaviour is similar to that
of the target word. Rated plausibility, however,
seems to be more sensitive to semantic than to
syntactic similarity.
As an example refer to Table 6, which displays
the ten most distributionally similar words to the
adjectives guilty and dangerous and to the nouns
stop and giant discovered by the Jensen-Shannon
measure. The set of similar words is far from se-
mantically coherent. As far as the adjective guilty
is concerned the measure discovered antonyms
such as innocent and honest. Semantically unre-
lated adjectives such as injured, democratic, or in-
terested are included; it seems that their syntactic
behaviour is similar to that of guilty, e.g., they all
co-occur with party. The same pattern can be ob-
served for the adjective dangerous, to which none
of the discovered adjectives are intuitively seman-
tically related, perhaps with the exception of bad.
The set of words most similar to the noun stop
also does not appear to be semantically coherent.
This problem with distance-weighted averag-
ing is aggravated by the fact that the adjective
or noun that we smooth over can be polysemous.
Take the set of similar words for giant, for in-
stance. The words company, manufacturer, indus-
try and firm are similar to the ?enterprise? sense
of giant, whereas artist, star, master are similar
to the ?important/influential person? sense of gi-
ant. However, no similar word was found for ei-
ther the ?beast? or ?heavyweight person? sense of
giant. This illustrates that the distance-weighted
averaging approach fails to take proper account
of the polysemy of a word. The class-based ap-
proach, on the other hand, relies on WordNet, a
lexical taxonomy that can be expected to cover
most senses of a given lexical item.
Recall that distance-weighted averaging dis-
covers distributionally similar words by look-
ing at simple lexical co-occurrence information.
In the case of adjective-noun pairs we concen-
trated on combinations found in the corpus in
a head-modifier relationship. This limited form
of surface-syntactic information does not seem
to be sufficient to reproduce the detailed knowl-
edge that people have about the semantic relation-
ships between words. Our class-based smoothing
method, on the other hand, relies on the semantic
taxonomy of WordNet, where fine-grained con-
ceptual knowledge about words and their rela-
tions is encoded. This knowledge can be used to
create semantically coherent equivalence classes.
Such classes will not contain antonyms or items
whose behaviour is syntactically related, but not
semantically similar, to the words of interest.
To summarise, it appears that distance-
weighted averaging smoothing is only partially
successful in reproducing the linguistic depen-
dencies that characterise and constrain the forma-
tion of adjective-noun combinations. The class-
based smoothing method, however, relies on a
pre-defined taxonomy that allows these depen-
dencies to be inferred, and thus reliably estimates
the plausibility of adjective-noun combinations
that fail to co-occur in the corpus.
4 Conclusions
This paper investigated the validity of smoothing
techniques by using them to recreate the frequen-
cies of adjective-noun pairs that fail to occur in
a 100 million word corpus. We showed that the
recreated frequencies are significantly correlated
with plausibility judgements. These results were
then extended by applying the same smoothing
techniques to adjective-noun pairs that occur in
the corpus. These recreated frequencies were sig-
nificantly correlated with the actual frequencies,
as well as with plausibility judgements.
Our results provide independent evidence for
the validity of the smoothing techniques we em-
ployed. In contrast to previous work, our evalu-
ation does not presuppose that the recreated fre-
quencies are used in a specific natural language
processing task. Rather, we established an in-
dependent criterion for the validity of smooth-
ing techniques by comparing them to plausibil-
ity judgements, which are known to correlate
with co-occurrence frequency. We also carried
out a comparison of different smoothing meth-
ods, and found that class-based smoothing outper-
forms distance-weighted averaging.4
From a practical point of view, our findings
provide a very simple account of adjective-
noun plausibility. Extending the results of
Lapata et al (1999), we confirmed that co-
occurrence frequency can be used to estimate the
plausibility of an adjective-noun pair. If no co-
occurrence counts are available from the corpus,
then counts can be recreated using the corpus and
a structured source of taxonomic knowledge (for
the class-based approach). Distance-weighted
averaging can be seen as a ?cheap? way to obtain
this sort of taxonomic knowledge. However, this
method does not draw upon semantic informa-
tion only, but is also sensitive to the syntactic
distribution of the target word. This explains the
fact that distance-weighted averaging yielded
a lower correlation with perceived plausibility
than class-based smoothing. A taxonomy like
WordNet provides a cleaner source of conceptual
information, which captures essential aspects of
the type of knowledge needed for assessing the
plausibility of an adjective-noun combination.
References
Ellen Gurman Bard, Dan Robertson, and Antonella Sorace.
1996. Magnitude estimation of linguistic acceptability.
Language, 72(1):32?68.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
and Robert L. Mercer. 1992. Class-based n-gram
models of natural language. Computational Linguistics,
18(4):467?479.
Lou Burnard, 1995. Users Guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Service.
Stephen Clark and David Weir. 2001. Class-based probabil-
ity estimation using a semantic hierarchy. In Proceedings
of the 2nd Conference of the North American Chapter
of the Association for Computational Linguistics, Pitts-
burgh, PA.
Steffan Corley, Martin Corley, Frank Keller, Matthew W.
Crocker, and Shari Trewin. 2001. Finding syntactic
4Two anonymous reviewers point out that this conclusion
only holds for an approach that computes similarity based on
adjective-noun co-occurrences. Such co-occurrences might
not reflect semantic relatedness very well, due to the idiosyn-
cratic nature of adjective-noun combinations. It is possible
that distance-weighted averaging would yield better results if
applied to other co-occurrence data (e.g., subject-verb, verb-
object), which could be expected to produce more reliable
information about semantic similarity.
structure in unparsed corpora: The Gsearch corpus query
system. Computers and the Humanities, 35(2):81?94.
Wayne Cowart. 1997. Experimental Syntax: Applying Ob-
jective Methods to Sentence Judgments. Sage Publica-
tions, Thousand Oaks, CA.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Text-
books in Linguistics. Cambridge University Press, Cam-
bridge.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence probabil-
ities. Machine Learning, 34(1):43?69.
Ralph Grishman and John Sterling. 1994. Generalizing au-
tomatically generated selectional patterns. In Proceed-
ings of the 15th International Conference on Computa-
tional Linguistics, pages 742?747, Kyoto.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 33(3):400?401.
Frank Keller, Martin Corley, Steffan Corley, Lars Konieczny,
and Amalia Todirascu. 1998. WebExp: A Java tool-
box for web-based psychological experiments. Technical
Report HCRC/TR-99, Human Communication Research
Centre, University of Edinburgh.
Maria Lapata, Scott McDonald, and Frank Keller. 1999.
Determinants of adjective-noun plausibility. In Proceed-
ings of the 9th Conference of the European Chapter of the
Association for Computational Linguistics, pages 30?36,
Bergen.
Maria Lapata. 2000. The Acquisition and Modeling of Lexi-
cal Knowledge: A Corpus-based Investigation of System-
atic Polysemy. Ph.D. thesis, University of Edinburgh.
Mark Lauer. 1995. Designing Statistical Language Learn-
ers: Experiments on Compound Nouns. Ph.D. thesis,
Macquarie University, Sydney.
Lilian Lee. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 25?32, Univer-
sity of Maryland, College Park.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduction
to WordNet: An on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proceed-
ings of the 31st Annual Meeting of the Association for
Computational Linguistics, pages 183?190, Columbus,
OH.
Philip Stuart Resnik. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
Carson T. Schu?tze. 1996. The Empirical Base of Linguis-
tics: Grammaticality Judgments and Linguistic Method-
ology. University of Chicago Press, Chicago.
Frank Smadja. 1991. Macrocoding the lexicon with co-
occurrence knowledge. In Uri Zernik, editor, Lexical Ac-
quisition: Using Online Resources to Build a Lexicon,
pages 165?189. Lawrence Erlbaum Associates, Hillsdale,
NJ.
S. S. Stevens. 1975. Psychophysics: Introduction to its Per-
ceptual, Neural, and Social Prospects. John Wiley, New
York.
Sholom M. Weiss and Casimir A. Kulikowski. 1991. Com-
puter Systems that Learn: Classification and Prediction
Methods from Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann, San Mateo, CA.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 359?366,
New York, June 2006. c?2006 Association for Computational Linguistics
Aggregation via Set Partitioning for Natural Language Generation
Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
regina@csail.mit.edu
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
The role of aggregation in natural lan-
guage generation is to combine two or
more linguistic structures into a single
sentence. The task is crucial for generat-
ing concise and readable texts. We present
an efficient algorithm for automatically
learning aggregation rules from a text and
its related database. The algorithm treats
aggregation as a set partitioning problem
and uses a global inference procedure to
find an optimal solution. Our experiments
show that this approach yields substan-
tial improvements over a clustering-based
model which relies exclusively on local
information.
1 Introduction
Aggregation is an essential component of many nat-
ural language generation systems (Reiter and Dale,
2000). The task captures a mechanism for merg-
ing together two or more linguistic structures into
a single sentence. Aggregated texts tend to be more
concise, coherent, and more readable overall (Dalia-
nis, 1999; Cheng and Mellish, 2000). Compare,
for example, sentence (2) in Table 1 and its non-
aggregated counterpart in sentences (1a)?(1d). The
difference between the fluent aggregated sentence
and its abrupt and redundant alternative is striking.
The benefits of aggregation go beyond making
texts less stilted and repetitive. Researchers in psy-
cholinguistics have shown that by eliminating re-
(1) a. Holocomb had an incompletion in the
first quarter.
b. Holocomb had another incompletion in
the first quarter.
c. Davis was among four San Francisco
defenders.
d. Holocomb threw to Davis for a leaping
catch.
(2) After two incompletions in the first quar-
ter, Holcomb found Davis among four San
Francisco defenders for a leaping catch.
Table 1: Aggregation example (in boldface) from a
corpus of football summaries
dundancy, aggregation facilitates text comprehen-
sion and recall (see Yeung (1999) and the references
therein). Furthermore, Di Eugenio et al (2005)
demonstrate that aggregation can improve learning
in the context of an intelligent tutoring application.
In existing generation systems, aggregation typi-
cally comprises two processes: semantic grouping
and sentence structuring (Wilkinson, 1995). The
first process involves partitioning semantic content
(usually the output of a content selection compo-
nent) into disjoint sets, each corresponding to a sin-
gle sentence. The second process is concerned with
syntactic or lexical decisions that affect the realiza-
tion of an aggregated sentence.
To date, this task has involved human analysis of a
domain-relevant corpus and manual development of
aggregation rules (Dalianis, 1999; Shaw, 1998). The
corpus analysis and knowledge engineering work in
such an approach is substantial, prohibitively so in
359
large domains. But since corpus data is already used
in building aggregation components, an appealing
alternative is to try and learn the rules of semantic
grouping directly from the data. Clearly, this would
greatly reduce the human effort involved and ease
porting generation systems to new domains.
In this paper, we present an automatic method for
performing the semantic grouping task. We address
the following problem: given an aligned parallel cor-
pus of sentences and their underlying semantic rep-
resentations, how can we learn grouping constraints
automatically? In our case the semantic content cor-
responds to entries from a database; however, our
algorithm could be also applied to other representa-
tions such as propositions or sentence plans.
We formalize semantic grouping as a set parti-
tioning problem, where each partition corresponds
to a sentence. The strength of our approach lies in
its ability to capture global partitioning constraints
by performing collective inference over local pair-
wise assignments. This design allows us to inte-
grate important constraints developed in symbolic
approaches into an automatic aggregation frame-
work. At a local level, pairwise constraints cap-
ture the semantic compatibility between pairs of
database entries. For example, if two entries share
multiple attributes, then they are likely to be aggre-
gated. Local constraints are learned using a binary
classifier that considers all pairwise combinations
attested in our corpus. At a global level, we search
for a semantic grouping that maximally agrees with
the pairwise preferences while simultaneously sat-
isfying constraints on the partitioning as a whole.
Global constraints, for instance, could prevent the
creation of overly long sentences, and, in general,
control the compression rate achieved during aggre-
gation. We encode the global inference task as an
integer linear program (ILP) that can be solved us-
ing standard optimization tools.
We evaluate our approach in a sports domain rep-
resented by large real-world databases containing
a wealth of interrelated facts. Our aggregation al-
gorithm model achieves an 11% F-score increase
on grouping entry pairs over a greedy clustering-
based model which does not utilize global informa-
tion for the partitioning task. Furthermore, these re-
sults demonstrate that aggregation is amenable to an
automatic treatment that does not require human in-
volvement.
In the following section, we provide an overview
of existing work on aggregation. Then, we define the
learning task and introduce our approach to content
grouping. Next, we present our experimental frame-
work and data. We conclude the paper by presenting
and discussing our results.
2 Related Work
Due to its importance in producing coherent and flu-
ent text, aggregation has been extensively studied in
the text generation community.1 Typically, semantic
grouping and sentence structuring are interleaved in
one step, thus enabling the aggregation component
to operate over a rich feature space. The common
assumption is that other parts of the generation sys-
tem are already in place during aggregation, and thus
the aggregation component has access to discourse,
syntactic, and lexical constraints.
The interplay of different constraints is usually
captured by a set of hand-crafted rules that guide
the aggregation process (Scott and de Souza, 1990;
Hovy, 1990; Dalianis, 1999; Shaw, 1998). Al-
ternatively, these rules can be learned from a cor-
pus. For instance, Walker et al (2001) propose
an overgenerate-and-rank approach to aggregation
within the context of a spoken dialog application.
Their system relies on a preference function for se-
lecting an appropriate aggregation among multiple
alternatives and assumes access to a large feature
space expressing syntactic and pragmatic features of
the input representations. The preference function
is learned from a corpus of candidate aggregations
marked with human ratings. Another approach is put
forward by Cheng and Mellish (2000) who use a ge-
netic algorithm in combination with a hand-crafted
preference function to opportunistically find a text
that satisfies aggregation and planning constraints.
Our approach differs from previous work in two
important respects. First, our ultimate goal is a gen-
eration system which can be entirely induced from
a parallel corpus of sentences and their correspond-
ing database entries. This means that our generator
will operate over more impoverished representations
than are traditionally assumed. For example we do
1The approaches are too numerous to list; we refer the inter-
ested reader to Reiter and Dale (2000) and Reape and Mellish
(1999) for comprehensive overviews.
360
Passing
PLAYER CP/AT YDS AVG TD INT
Cundiff 22/37 237 6.4 1 1
Carter 23/47 237 5.0 1 4
. . . . . . . . . . . . . . . . . .
Rushing
PLAYER REC YDS AVG LG TD
Hambrick 13 33 2.5 10 1
. . . . . . . . . . . . . . . . . .
1 (Passing (Cundiff 22/37 237 6.4 1 1))
(Passing (Carter 23/47 237 5.0 1 4))
2 (Interception (Lindell 1 52 1))
(Kicking (Lindell 3/3 100 38 1/1 10))
3 (Passing (Bledsoe 17/34 104 3.1 0 0))
4 (Passing (Carter 15/32 116 3.6 1 0))
5 (Rushing (Hambrick 13 33 2.5 10 1))
6 (Fumbles (Bledsoe 2 2 0 0 0))
Table 2: Excerpt of database and (simplified) example of aggregated entries taken from a football domain.
This fragment will give rise to 6 sentences in the final text.
not presume to know all possible ways in which our
database entries can be lexicalized, nor do we pre-
sume to know which semantic or discourse relations
exist between different entries. In this framework,
aggregation is the task of grouping semantic content
without making any decisions about sentence struc-
ture or its surface realization. Second, we strive for
an approach to the aggregation problem which is as
domain- and representation-independent as possible.
3 Problem Formulation
We formulate aggregation as a supervised partition-
ing task, where the goal is to find a clustering of
input items that maximizes a global utility func-
tion. The input to the model consists of a set E
of database entries selected by a content planner.
The output of the model is a partition S = {Si} of
nonempty subsets such that each element of E ap-
pears in exactly one subset.2 In the context of aggre-
gation, each partition represents entries that should
be verbalized in the same sentence. An example of a
partitioning is illustrated in the right side of Table 2
where eight entries are partitioned into six clusters.
We assume access to a relational database where
each entry has a type and a set of attributes as-
sociated with it. Table 2 (left) shows an ex-
cerpt of the database we used for our experiments.
The aggregated text in Table 2 (right) contains en-
tries of five types: Passing, Interception,
Kicking, Rushing, and Fumbles. Entries of
type Passing have six attributes ? PLAYER,
2By definition, a partitioning of a set defines an equivalence
relation which is reflexive, symmetric, and transitive.
CP/AT, YDS, AVG, TD, INT, entries of type
Interception have four attributes, and so on.
We assume the existence of a non-empty set of at-
tributes that we can use for meaningful comparison
between entities of different types. In the example
above, types Passing and Rushing share the at-
tributes PLAYER,AVG (short for average), TD (short
for touchdown) and YDS (short for yards). These are
indicated in boldface in Table 2. In Section 4.1, we
discuss how a set of shared attributes can be deter-
mined for a given database.
Our training data consists of entry sets with a
known partitioning. During testing, our task is to
infer a partitioning for an unseen set of entries.
4 Modeling
Our model is inspired by research on text aggre-
gation in the natural language generation commu-
nity (Cheng and Mellish, 2000; Shaw, 1998). A
common theme across different approaches is the
notion of similarity ? content elements described
in the same sentence should be related to each other
in some meaningful way to achieve conciseness and
coherence. Consider for instance the first cluster in
Table 2. Here, we have two entries of the same type
(i.e., Passing). Furthermore, the entries share the
same values for the attributes YDS and TD (i.e., 237
and 1). On the other hand, clusters 5 and 6 have
no attributes in common. This observation moti-
vates modeling aggregation as a binary classification
task: given a pair of entries, predict their aggrega-
tion status based on the similarity of their attributes.
Assuming a perfect classifier, pairwise assignments
361
will be consistent with each other and will therefore
yield a valid partitioning.
In reality, however, this approach may produce
globally inconsistent decisions since it treats each
pair of entries in isolation. Moreover, a pairwise
classification model cannot express general con-
straints regarding the partitioning as a whole. For
example, we may want to constrain the size of the
generated partitions and the compression rate of the
document, or the complexity of the generated sen-
tences.
To address these requirements, our approach re-
lies on global inference. Given the pairwise predic-
tions of a local classifier, our model finds a glob-
ally optimal assignment that satisfies partitioning-
level constraints. The computational challenge lies
in the complexity of such a model: we need to find
an optimal partition in an exponentially large search
space. Our approach is based on an Integer Linear
Programming (ILP) formulation which can be effec-
tively solved using standard optimization tools. ILP
models have been successfully applied in several
natural language processing tasks, including relation
extraction (Roth and Yih, 2004), semantic role label-
ing (Punyakanok et al, 2004) and the generation of
route directions (Marciniak and Strube, 2005).
In the following section, we introduce our local
pairwise model and afterward we present our global
model for partitioning.
4.1 Learning Pairwise Similarity
Our goal is to determine whether two database en-
tries should be aggregated given the similarity of
their shared attributes. We generate the training data
by considering all pairs ?ei, ej? ? E ? E, where E
is the set of all entries attested in a given document.
An entry pair forms a positive instance if its mem-
bers belong to the same partition in the training data.
For example, we will generate 8?72 unordered entry
pairs for the eight entries from the document in Ta-
ble 2. From these, only two pairs constitute positive
instances, i.e., clusters 1 and 2. All other pairs form
negative instances.
The computation of pairwise similarity is based
on the attribute set A = {Ai} shared between the
two entries in the pair. As discussed in Section 3,
the same attributes can characterize multiple entry
types, and thus form a valid basis for entry compari-
son. The shared attribute set A could be identified in
many ways. For example, using domain knowledge
or by selecting attributes that appear across multiple
types. In our experiments, we follow the second ap-
proach: we order attributes by the number of entry
types in which they appear, and select the top five3.
A pair of entries is represented by a binary fea-
ture vector {xi} in which coordinate xi indicates
whether two entries have the same value for at-
tribute i. The feature vector is further expanded by
conjuctive features that explicitly represent overlap
in values of multiple attributes up to size k. The
parameter k controls the cardinality of the maximal
conjunctive set and is optimized on the development
set.
To illustrate our feature generation process, con-
sider the pair (Passing (Quincy Carter 15/32 116 3.6
1 0)) and (Rushing (Troy Hambrick 13 33 2.5 10 1))
from Table 2. Assuming A = {Player,Yds,TD}
and k = 2, the similarity between the two en-
tries will be expressed by six features, three rep-
resenting overlap in individual attributes and three
representing overlap when considering pairs of at-
tributes. The resulting feature vector has the form
?0, 0, 1, 0, 0, 0?.
Once we define a mapping from database entries
to features, we employ a machine learning algorithm
to induce a classifier on the feature vectors generated
from the training documents. In our experiments, we
used a publicly available maximum entropy classi-
fier4 for this task.
4.2 Partitioning with ILP
Given the pairwise predictions of the local classifier,
we wish to find a valid global partitioning for the
entries in a single document. We thus model the in-
teraction between all pairwise aggregation decisions
as an optimization problem.
Let c?ei,ej? be the probability of seeing entry pair
?ei, ej? aggregated (as computed by the pairwise
classifier). Our goal is to find an assignment that
maximizes the sum of pairwise scores and forms a
valid partitioning. We represent an assignment us-
ing a set of indicator variables x?ei,ej? that are set
3Selecting a larger number of attributes for representing sim-
ilarity would result in considerably sparser feature vectors.
4The software can be downloaded from http://www.
isi.edu/?hdaume/megam/index.html.
362
to 1 if ?ei, ej? is aggregated, and 0 otherwise. The
score of a global assignment is the sum of its pair-
wise scores:
?
?ei,ej??E?E
c?ei,ej?x?ei,ej?+(1?c?ei,ej?)(1?x?ei,ej?)
(1)
Our inference task is solved by maximizing the
overall score of pairs in a given document:
argmax
?
?ei,ej??E?E
c?ei,ej?x?ei,ej?+(1?c?ei,ej?)(1?x?ei ,ej?)
(2)
subject to:
x?ei,ej? ? {0, 1} ? ei, ej ? E ? E (3)
We augment this basic formulation with two types
of constraints. The first type of constraint ensures
that pairwise assignments lead to a consistent parti-
tioning, while the second type expresses global con-
straints on partitioning.
Transitivity Constraints We place constraints
that enforce transitivity in the label assignment: if
x?ei,ej? = 1 and x?ej ,ek? = 1, then x?ei,ek? = 1.
A pairwise assignment that satisfies this constraint
defines an equivalence relation, and thus yields a
unique partitioning of input entries (Cormen et al,
1992).
We implement transitivity constraints by intro-
ducing for every triple ei, ej , ek (i 6= j 6= k) an
inequality of the following form:
x?ei,ek? ? x?ei,ej? + x?ej ,ek? ? 1 (4)
If both x?ei,ej? and x?ej ,ek? are set to one, then
x?ei,ek? also has to be one. Otherwise, x?ei,ek? can
be either 1 or 0.
Global Constraints We also want to consider
global document properties that influence aggrega-
tion. For example, documents with many database
entries are likely to exhibit different compression
rates during aggregation when compared to docu-
ments that contain only a few.
Our first global constraint controls the number
of aggregated sentences in the document. This is
achieved by limiting the number of entry pairs with
positive labels for each document:
?
?ei,ej??E?E
x?ei,ej? ? m (5)
Notice that the number m is not known in ad-
vance. However, we can estimate this parameter
from our development data by considering docu-
ments of similar size (as measured by the number
of corresponding entry pairs.) For example, texts
with thousand entry pairs contain on average 70 pos-
itive labels, while documents with 200 pairs have
around 20 positive labels. Therefore, we set m sep-
arately for every document by taking the average
number of positive labels observed in the develop-
ment data for the document size in question.
The second set of constraints controls the length
of the generated sentences. We expect that there is
an upper limit on the number of pairs that can be
clustered together. This restriction can be expressed
in the following form:
? ei
?
ej?E
x?ei,ej? ? k (6)
This constraint ensures that there can be at most k
positively labeled pairs for any entry ei. In our
corpus, for instance, at most nine entries can be
aggregated in a sentence. Again k is estimated
from the development data by taking into account
the average number of positively labeled pairs for
every entry type (see Table 2). We therefore
indirectly capture the fact that some entry types
(e.g., Passing) are more likely to be aggregated
than others (e.g., Kicking).
Solving the ILP In general, solving an integer lin-
ear program is NP-hard (Cormen et al, 1992). For-
tunately, there exist several strategies for solving
ILPs. In our study, we employed lp solve, an ef-
ficient Mixed Integer Programming solver5 which
implements the Branch-and-Bound algorithm. We
generate and solve an ILP for every document we
wish to aggregate. Documents of average size (ap-
proximately 350 entry pairs) take under 30 minutes
on a 450 MHz Pentium III machine.
5The software is available from http://www.
geocities.com/lpsolve/
363
5 Evaluation Set-up
The model presented in the previous section was
evaluated in the context of generating summary re-
ports for American football games. In this section
we describe the corpus used in our experiments, our
procedure for estimating the parameters of our mod-
els, and the baseline method used for comparison
with our approach.
Data For training and testing our algorithm, we
employed a corpus of football game summaries col-
lected by Barzilay and Lapata (2005). The corpus
contains 468 game summaries from the official site
of the American National Football League6 (NFL).
Each summary has an associated database contain-
ing statistics about individual players and events. In
total, the corpus contains 73,400 database entries,
7.1% of which are verbalized; each entry is charac-
terized by a type and a set of attributes (see Table 2).
Database entries are automatically aligned with their
corresponding sentences in the game summaries by
a procedure that considers anchor overlap between
entity attributes and sentence tokens. Although the
alignment procedure is relatively accurate, there is
unavoidably some noise in the data.
The distribution of database entries per sentence
is shown in Figure 1. As can be seen, most aggre-
gated sentences correspond to two or three database
entries. Each game summary contained 14.3 entries
and 9.1 sentences on average. The training and test
data were generated as described in Section 4.1. We
used 96,434 instances (300 summaries) for training,
59,082 instances (68 summaries) for testing, and
53,776 instances (100 summaries) for development
purposes.
Parameter Estimation As explained in Section 4,
we infer a partitioning over a set of database en-
tries in a two-stage process. We first determine how
likely all entry pairs are to be aggregated using a lo-
cal classifier, and then infer a valid global partition-
ing for all entries. The set of shared attributes A
consists of five features that capture overlap in play-
ers, time (measured by game quarters), action type,
outcome type, and number of yards. The maximum
cardinality of the set of conjunctive features is five.
6See http://www.nfl.com/scores.
Figure 1: Distribution of aggregated sentences in the
NFL corpus
Overall, our local classifier used 28 features, includ-
ing 23 conjunctive ones. The maximum entropy
classifier was trained for 100 iterations. The global
constraints for our ILP models are parametrized (see
equations (5) and (6)) by m and k which are esti-
mated separately for every test document. The val-
ues of m ranged from 2 to 130 and for k from 2 to 9.
Baseline Clustering is a natural baseline model for
our partitioning problem. In our experiments, we
a employ a single-link agglomerative clustering al-
gorithm that uses the scores returned by the maxi-
mum entropy classifier as a pairwise distance mea-
sure. Initially, the algorithm creates a separate clus-
ter for each sentence. During each iteration, the two
closest clusters are merged. Again, we do not know
in advance the appropriate number of clusters for a
given document. This number is estimated from the
training data by averaging the number of sentences
in documents of the same size.
Evaluation Measures We evaluate the perfor-
mance of the ILP and clustering models by mea-
suring F-score over pairwise label assignments. We
compute F-score individually for each document and
report the average. In addition, we compute partition
accuracy in order to determine how many sentence-
level aggregations our model predicts correctly.
364
Clustering Precision Recall F-score
Mean 57.7 66.9 58.4
Min 0.0 0.0 0.0
Max 100.0 100.0 100.0
StDev 28.2 23.9 23.1
ILP Model Precision Recall F-score
Mean 82.2 65.4 70.3
Min 37.5 28.6 40.0
Max 100.0 100.0 100.0
StDev 19.2 20.3 16.6
Table 3: Results on pairwise label assignment (pre-
cision, recall, and F-score are averaged over doc-
uments); comparison between clustering and ILP
models
6 Results
Our results are summarized in Table 3. As can
be seen, the ILP model outperforms the clustering
model by a wide margin (11.9% F-score). The two
methods yield comparable recall; however, the clus-
tering model lags considerably behind as far as pre-
cision is concerned (the difference is 24.5 %).7
Precision is more important than recall in the con-
text of our aggregation application. Incorrect aggre-
gations may have detrimental effects on the coher-
ence of the generated text. Choosing not to aggre-
gate may result in somewhat repetitive texts; how-
ever, the semantic content of the underlying text re-
mains intact. In the case of wrong aggregations, we
may group together facts that are not compatible,
and even introduce implications that are false.
We also consider how well our model performs
when evaluated on total partition accuracy. Here,
we are examining the partitioning as a whole and
ask the following question: how many clusters of
size 1, 2 . . . n did the algorithm get right? This eval-
uation measure is stricter than F-score which is com-
7Unfortunately we cannot apply standard statistical tests
such as the t-test on F-scores since they violate assumptions
about underlying normal distributions. It is not possible to use
an assumptions-free test like ?2 either, since F-score is not a
frequency-based measure. We can, however, use ?2 on pre-
cision and recall, since these measures are estimated from fre-
quency data. We thus find that the ILP model is significantly
better than the clustering model on precision (?2 = 16.39,
p < 0.01); the two models are not significantly different in
terms of recall (?2 = 0.02, p < 0.89).
Figure 2: Partition accuracy for sentences of differ-
ent size
puted over pairwise label assignments. The partition
accuracy for entry groups of varying size is shown in
Figure 2. As can be seen, in all cases the ILP outper-
forms the clustering baseline. Both models are fairly
accurate at identifying singletons, i.e., database en-
tries which are not aggregated. Performance is natu-
rally worse when considering larger clusters. Inter-
estingly, the difference between the two models be-
comes more pronounced for partition sizes 4 and 5
(see Figure 2). The ILP?s accuracy increases by 24%
for size 4 and 8% for size 5.
These results empirically validate the impor-
tance of global inference for the partitioning task.
Our formulation allows us to incorporate important
document-level constraints as well as consistency
constraints which cannot be easily represented in a
vanilla clustering model.
7 Conclusions and Future Work
In this paper we have presented a novel data-driven
method for aggregation in the context of natural lan-
guage generation. A key aspect of our approach is
the use of global inference for finding aggregations
that are maximally consistent and coherent. We have
formulated our inference problem as an integer lin-
ear program and shown experimentally that it out-
performs a baseline clustering model by a wide mar-
gin. Beyond generation, the approach holds promise
for other NLP tasks requiring the accurate partition-
ing of items into equivalence classes (e.g., corefer-
ence resolution).
365
Currently, semantic grouping is carried out in our
model sequentially. First, a local classifier learns
the similarity of entity pairs and then ILP is em-
ployed to infer a valid partitioning. Although such a
model has advantages in the face of sparse data (re-
call that we used a relatively small training corpus
of 300 documents) and delivers good performance,
it effectively decouples learning from inference. An
appealing future direction lies in integrating learning
and inference in a unified global framework. Such
a framework would allow us to incorporate global
constraints directly into the learning process.
Another important issue, not addressed in this
work, is the interaction of our aggregation method
with content selection and surface realization. Using
an ILP formulation may be an advantage here since
we could use feedback (in the form of constraints)
from other components and knowlegde sources (e.g.,
discourse relations) to improve aggregation or in-
deed the generation pipeline as a whole (Marciniak
and Strube, 2005).
Acknowledgments
The authors acknowledge the support of the National Science
Foundation (Barzilay; CAREER grant IIS-0448168 and grant
IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).
Thanks to Eli Barzilay, Michael Collins, David Karger, Frank
Keller, Yoong Keok Lee, Igor Malioutov, Johanna Moore,
Kevin Simler, Ben Snyder, Bonnie Webber and the anonymous
reviewers for helpful comments and suggestions. Any opinions,
findings, and conclusions or recommendations expressed above
are those of the authors and do not necessarily reflect the views
of the NSF or EPSRC.
References
R. Barzilay, M. Lapata. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings
of the Human Language Technology Conference and
the Conference on Empirical Methods in Natural Lan-
guage Processing, 331?338, Vancouver.
H. Cheng, C. Mellish. 2000. Capturing the interaction
between aggregation and text planning in two genera-
tion systems. In Proceedings of the 1st International
Natural Language Generation Conference, 186?193,
Mitzpe Ramon, Israel.
T. H. Cormen, C. E. Leiserson, R. L. Rivest. 1992. Into-
duction to Algorithms. The MIT Press.
H. Dalianis. 1999. Aggregation in natural language gen-
eration. Computational Intelligence, 15(4):384?414.
B. Di Eugenio, D. Fossati, D. Yu. 2005. Aggregation im-
proves learning: Experiments in natural language gen-
eration for intelligent tutoring systems. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, 50?57, Ann Arbor, MI.
E. H. Hovy. 1990. Unresolved issues in paragraph plan-
ning. In R. Dale, C. Mellish, M. Zock, eds., Cur-
rent Research in Natural Language Generation, 17?
41. Academic Press, New York.
T. Marciniak, M. Strube. 2005. Beyond the pipeline:
Discrete optimization in NLP. In Proceedings of the
Annual Conference on Computational Natural Lan-
guage Learning, 136?143, Ann Arbor, MI.
V. Punyakanok, D. Roth, W. Yih, D. Zimak. 2004. Se-
mantic role labeling via integer linear programming
inference. In Proceedings of the International Con-
ference on Computational Linguistics, 1346?1352,
Geneva, Switzerland.
M. Reape, C. Mellish. 1999. Just what is aggrega-
tion anyway? In Proceedings of the 7th European
Workshop on Natural Language Generation, 20?29,
Toulouse, France.
E. Reiter, R. Dale. 2000. Building Natural Language
Generation Systems. Cambridge University Press,
Cambridge.
D. Roth, W. Yih. 2004. A linear programming formula-
tion for global inference in natural language tasks. In
Proceedings of the Annual Conference on Computa-
tional Natural Language Learning, 1?8, Boston, MA.
D. Scott, C. S. de Souza. 1990. Getting the mes-
sage across in RST-based text generation. In R. Dale,
C. Mellish, M. Zock, eds., Current Research in Nat-
ural Language Generation, 47?73. Academic Press,
New York.
J. Shaw. 1998. Clause aggregation using linguis-
tic knowledge. In Proceedings of 9th International
Workshop on Natural Language Generation, 138?147,
Niagara-on-the-Lake, Ontario, Canada.
M. A. Walker, O. Rambow, M. Rogati. 2001. Spot:
A trainable sentence planner. In Proceedings of the
2nd Annual Meeting of the North American Chapter
of the Association for Computational Linguistics, 17?
24, Pittsburgh, PA.
J. Wilkinson. 1995. Aggregation in natural language
generation: Another look. Technical report, Computer
Science Department, University of Waterloo, 1995.
A. S. Yeung. 1999. Cognitive load and learner expertise:
Split-attention and redundancy effects in reading com-
prehension tasks with vocabulary definitions. Journal
of Experimental Education, 67(3):197?218.
366
27
28
29
30
31
32
33
34
235
236
237
238
239
240
241
242
c? 2003 Association for Computational Linguistics
A Probabilistic Account of Logical
Metonymy
Maria Lapata? Alex Lascarides?
University of Sheffield University of Edinburgh
In this article we investigate logical metonymy, that is, constructions in which the argument
of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the
book means enjoy reading the book, and easy problem means a problem that is easy to solve).
The systematic variation in the interpretation of such constructions suggests a rich and complex
theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy
typically fail to describe exhaustively all the possible interpretations, or they don?t rank those
interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic
verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking
on the set of possible interpretations. We identify the interpretations automatically by exploiting
the consistent correspondences between surface syntactic cues and meaning. We evaluate our
results against paraphrase judgments elicited experimentally from humans and show that the
model?s ranking of meanings correlates reliably with human intuitions.
1. Introduction
Much work in lexical semantics has been concerned with accounting for regular poly-
semy, that is, the regular and predictable sense alternations to which certain classes of
words are subject (Apresjan 1973). It has been argued that in some cases, the different
interpretations of these words must arise from the interaction between the semantics
of the words during syntactic composition, rather than by exhaustively listing all the
possible senses of a word in distinct lexical entries (Pustejovsky 1991). The class of
phenomena that Pustejovsky (1991, 1995) has called logical metonymy is one such
example. In the case of logical metonymy additional meaning arises for particular
verb-noun and adjective-noun combinations in a systematic way: the verb (or adjec-
tive) semantically selects for an event-type argument, which is a different semantic
type from that denoted by the noun. Nevertheless, the value of this event is pre-
dictable from the semantics of the noun. An example of verbal logical metonymy is
given in (1) and (2): (1a) usually means (1b) and (2a) usually means (2b).
(1) a. Mary finished the cigarette.
b. Mary finished smoking the cigarette.
(2) a. Mary finished her beer.
b. Mary finished drinking her beer.
? Department of Computer Science, University of Sheffield, Regent Court, 211 Portobello Street, Sheffield
S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk.
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail:
alex@inf.ed.ac.uk.
262
Computational Linguistics Volume 29, Number 2
Note how the events in these examples correspond to the purpose of the object denoted
by the noun: the purpose of a cigarette is to smoke it and the purpose of a beer is to
drink it. Similarly, (3a) means a problem that is easy to solve, (3b) means a language
that is difficult to learn, speak, or write, (3c) means a cook that cooks well, (3d) means
a soup that tastes good, (3e) means someone who programmes fast, and (3f) means a
plane that flies quickly.
(3) a. easy problem
b. difficult language
c. good cook
d. good soup
e. fast programmer
f. fast plane
The interpretations of logical metonymies can typically be rendered with a para-
phrase, as we have indicated for the above examples. Verb-nouns are paraphrased
with a progressive or infinitive VP that is the complement of the polysemous verb
(e.g., smoking in (1b)) and whose object is the NP figuring in the verb-noun combina-
tion (e.g., cigarette in (1b)). Adjective-noun combinations are usually paraphrased with
a verb modified by the adjective in question or its corresponding adverb. For example,
an easy problem is a problem that is easy to solve or a problem that one can solve easily
(see (3a)).
Logical metonymy has been extensively studied in the lexical semantics literature.
Previous approaches have focused on descriptive (Vendler 1968) or theoretical (Puste-
jovsky 1991, 1995; Briscoe, Copestake, and Boguraev 1990) accounts, on the linguistic
constraints on the phenomenon (Godard and Jayez 1993; Pustejovsky and Bouillon
1995; Copestake and Briscoe 1995; Copestake 2001), and on the influence of discourse
context on the interpretation of metonymies (Briscoe, Copestake, and Boguraev 1990;
Lascarides and Copestake 1998; Verspoor 1997). McElree et al (2001) investigated the
on-line processing of metonymic expressions; their results indicate that humans dis-
play longer reading times for sentences like (1a) than for sentences like (1b).
There are at least two challenges in providing an adequate account of logical
metonymy. The first concerns semi-productivity: There is a wealth of evidence that
metonymic constructions are partially conventionalized, and so resolving metonymy
entirely via pragmatic reasoning (e.g., by computing the purpose of the object that
is denoted by the noun according to real-world knowledge) will overgenerate the
possible interpretations (Hobbs et al 1993). For example, the logical metonymies in
(4) are odd, even though pragmatics suggests an interpretation (because real-world
knowledge assigns a purpose to the object denoted by the NP):
(4) a. ?John enjoyed the dictionary.
b. ?John enjoyed the door.
c. ?John began/enjoyed the highway.
d. ?John began the bridge.
Sentence (4a) is odd because the purpose of dictionaries is to refer to them, or to
consult them. These are (pointlike) achievements and cannot easily combine with en-
joy, which has to be true of an event with significant duration. Domain knowledge
assigns doors, highways, and bridges a particular purpose, and so the fact that the
sentences in (4b)?(4d) are odd indicates that metonymic interpretations are subject to
conventional constraints (Godard and Jayez 1993).
263
Lapata and Lascarides Logical Metonymy
The second challenge concerns the diversity of possible interpretations of metony-
mic constructions. This diversity is attested across and within metonymic construc-
tions. Metonymic verbs and adjectives are able to take on different meanings depend-
ing on their local context, namely, the noun or noun class they select as objects (in the
case of verbs) or modify (in the case of adjectives). Consider the examples in (1), in
which the meaning of the verb finish varies depending on the object it selects. Simi-
larly, the adjective good receives different interpretations when modifying the nouns
cook and soup (see (3c) and (3d)).
Although we?ve observed that some logical metonymies are odd even though
pragmatics suggests an interpretation (e.g., (4c)), Vendler (1968) acknowledges that
other logical metonymies have more than one plausible interpretation. In order to
account for the meaning of adjective-noun combinations, Vendler (1968, page 92) points
out that ?in most cases not one verb, but a family of verbs is needed?. For example,
fast scientist can mean a scientist who does experiments quickly, publishes quickly, and
so on.
Vendler (1968) further observes that the noun figuring in an adjective-noun combi-
nation is usually the subject or object of the paraphrasing verb. Although fast usually
triggers a verb-subject interpretation (see (3e) and (3f)), easy and difficult trigger verb-
object interpretations (see (3a) and (3b)). An easy problem is usually a problem that one
solves easily (so problem is the object of solve), and a difficult language is a language that
one learns, speaks, or writes with difficulty (so language is the object of learn, speak,
and write). Adjectives like good allow either verb-subject or verb-object interpretations:
a good cook is a cook who cooks well, whereas good soup is a soup that tastes good.
All of these interpretations of fast scientist, difficult language, or good soup seem highly
plausible out of context, though one interpretation may be favored over another in a
particular context. In fact, in sufficiently rich contexts, pragmatics can even override
conventional interpretations: Lascarides and Copestake (1998) suggest that (5c) means
(5d) and not (5e):
(5) a. All the office personnel took part in the company sports day last
week.
b. One of the programmers was a good athlete, but the other was
struggling to finish the courses.
c. The fast programmer came first in the 100m.
d. The programmer who runs fast came first in the 100m.
e. The programmer who programs fast came first in the 100m.
The discourse context can also ameliorate highly marked logical metonymies, such
as (4c):
(6) a. John uses two highways to get to work every morning.
b. He first takes H-280 and then H-101.
c. He always enjoys H-280,
d. but the traffic jams on H-101 frustrate him.
Arguably the most influential account of logical metonymy is Pustejovsky?s
(1991, 1995) theory of the generative lexicon. Pustejovsky avoids enumerating the
various senses for adjectives like fast and verbs like finish by exploiting a rich lexical
semantics for nouns. The lexical entry for an artifact-denoting noun includes a qualia
structure: this specifies key features of the word?s meaning that are in some sense
264
Computational Linguistics Volume 29, Number 2
derivable from real-world knowledge but are lexicalized so as to influence conven-
tional processes. The qualia structure includes a telic role (i.e., the purpose of the object
denoted by the noun) and an agentive role (i.e., the event that brought the object into
existence). Thus the lexical entry for book includes a telic role with a value equivalent
to read and an agentive role with a value equivalent to write, whereas for cigarette the
telic role is equivalent to smoke and the agentive role is equivalent to roll or manufacture.
When finish combines with an object-denoting NP, a metonymic interpretation
is constructed in which the missing information is provided by the qualia structure
of the NP. More technically, semantic composition of finish with cigarette causes the
semantic type of the noun to be coerced into its telic event (or its agentive event),
and the semantic relation corresponding to the metonymic verb (finish) predicates
over this event. This results in an interpretation of (1a) equivalent to (1b). Verbs like
begin and enjoy behave in a similar way. Enjoy the book can mean enjoy reading the
book, because of book?s telic role, or enjoy writing the book, because of book?s agentive
role. In fact, the agentive reading is less typical for book than the telic one, but for
other nouns the opposite is true. For instance, begin the tunnel can mean begin building
the tunnel, but the interpretation that is equivalent to begin going through the tunnel
is highly marked. There is also variation in the relative likelihood of interpretations
among different metonymic verbs. (We will return to this issue shortly.) The adjective-
noun combinations are treated along similar lines. Thus the logical polysemy of words
like finish and fast is not accounted for by exhaustive listing.1
In contrast to the volume of theoretical work on logical metonymy, very little
empirical work has tackled the topic. Briscoe et al (1990) investigate the presence of
verbal logical metonymies in naturally occurring text by looking into data extracted
from the Lancaster-Oslo/Bergen corpus (LOB, one million words). Verspoor (1997)
undertakes a similar study in the British National Corpus (BNC, 100 million words).
Both studies investigate how widespread the use of logical metonymy is, and how
far the interpretation for metonymic examples can be recovered from the head noun?s
qualia structure, assuming one knows what the qualia structure for any given noun is.
Neither of these studies is concerned with the automatic generation of interpretations
for logical metonymies and the determination of their likelihood.
Although conceptually elegant, Pustejovsky?s (1995) theory of the generative lex-
icon does not aim to provide an exhaustive description of the telic roles that a given
noun may have. However, these roles are crucial for interpreting verb-noun and
adjective-noun metonymies. In contrast to Vendler (1968), who acknowledges that
logical metonymies may trigger more than one interpretation (in other words, that
there may be more than one possible event associated with the noun in question),
Pustejovsky implicitly assumes that nouns or noun classes have one (perhaps default)
telic role without, however, systematically investigating the relative degree of ambigu-
ity of the various cases of logical metonymy (e.g., the out-of-context possible readings
for fast scientist suggest that fast scientist exhibits a higher degree of semantic ambiguity
than fast plane). One could conceivably represent this by the generality of the semantic
type of the telic role in the various nouns (e.g., assign the telic role of scientist a rel-
atively general type of event compared with that for plane). But this simply transfers
the problem: The degree of generality in lexical representation is highly idiosyncratic
and ideally should be acquired from linguistic evidence; furthermore, for nouns with
1 Other lexical accounts, such as Copestake and Briscoe (1995), differ from Pustejovsky?s (1995) in that
the coercion is treated as internal to the semantics of the metonymic verb or adjective rather than the
noun; motivation for this comes from copredication data, such as the acceptability of fast and intelligent
typist and John picked up and finished his beer.
265
Lapata and Lascarides Logical Metonymy
a general telic role, pragmatics would have to do ?more work? to augment the gen-
eral interpretation with a more specific one. Even in theories in which more than one
interpretation is provided (see Vendler 1968), no information is given with respect to
the relative likelihood of these interpretations.
Pustejovsky?s account also doesn?t predict the degree of variation of interpretations
for a given noun among the different metonymic verbs: for example, the fact that begin
the house is, intuitively at least, more likely to resolve to an agentive-role interpretation
(i.e., begin building the house) than a telic-role interpretation (i.e., begin living in the
house), whereas the reverse is true of enjoy the house. Ideally, we would like a model
of logical metonymy that reflects this variation in interpretation.
In this article we aim to complement the theoretical work on the interpretation
of logical metonymy by addressing the following questions: (1) Can the meanings
of metonymic adjective-noun and verb-noun combinations be acquired automatically
from corpora? (2) Can we constrain the number of interpretations of these combina-
tions by providing a ranking on the set of possible meanings? (3) Can we determine
whether a particular adjective has a preference for a verb-subject or a verb-object in-
terpretation? We provide a probabilistic model that uses distributional information
extracted from a large corpus to interpret logical metonymies automatically without
recourse to pre-existing taxonomies or manually annotated data.
The differences among the various theoretical accounts?for example, that Copes-
take and Briscoe (1995) treat type coercion as internal to the metonymic word, whereas
Pustejovsky (1995) treats it as part of the noun?do not matter for our purposes, be-
cause we aim to provide information about metonymic interpretations that is compati-
ble with either account. More specifically, we are concerned with using a real-language
corpus to acquire automatically the semantic value of the event that is part of the inter-
pretation. We abstract away from theoretical concepts such as semantic type coercion
and instead utilize co-occurrence frequencies in the corpus to predict metonymic inter-
pretations. Very roughly, we acquire a ranked set of interpretations enjoy V-ing the book
for the construction enjoy the book by estimating the probabilities that V is enjoyed and
that it is something done to books; and we estimate these probabilities on the basis of
the corpus frequencies for V?s appearing as a (verbal) complement to enjoy and for V?s
taking book as its object. Similarly, we acquire a ranked set of verb-subject interpreta-
tions of fast plane by estimating the likelihood of seeing the plane Vs and Vs quickly in
the corpus. (See Sections 2 and 3 for more details and motivation of these models.)
Our results show not only that we can predict meaning differences when the same
adjective or verb is associated with different nouns, but also that we can derive?taking
into account Vendler?s (1968) observation?a cluster of meanings for a single verb- or
adjective-noun combination. We can also predict meaning differences for a given noun
associated with different metonymic verbs and adjectives. We evaluate our results by
comparing the model?s predictions to human judgments and show that the model?s
ranking of meanings correlates reliably with human intuitions.
However, the model is limited in its scope. It is suited for the interpretation of well-
formed metonymic constructions. But it does not distinguish odd metonymies (see (4))
from acceptable ones: in both cases, paraphrases will be generated, at least in principle
(see Section 2.1 for explanation and motivation). In particular, the model does not
learn conventional constraints, such as that enjoy must take an event of duration as its
argument (Godard and Jayez 1993). However, such constraints are potentially captured
indirectly: If the above conventional constraint is right, enjoy referring to should not
be attested in the corpus, and hence according to our model it won?t be part of a
possible paraphrase for enjoy the dictionary (see Sections 2.4.2 and 2.5.3 for further
discussion). Further, since the model abstracts away from semantic type coercion, it
266
Computational Linguistics Volume 29, Number 2
does not distinguish between uses of a verb or adjective that are claimed in the lexical
semantics literature to be metonymic uses (e.g., enjoy the book, fast programmer) and
those that are claimed to be nonmetonymic uses (e.g., enjoy the marriage, fast run-time).
Again, the model of interpretation presented here will generate paraphrases for all
these cases (e.g., it will paraphrase enjoy the marriage as enjoy going to or participating
in the marriage and fast run-time as run-time that goes by or passes quickly). The model
also does not take discourse context into account; for example, it will not predict the
intuitive interpretation of (5e). Rather, it determines the most dominant meanings for
a given metonymic construction overall, across all of the instances of it in the corpus.
The remainder of this article is organized as follows: in the first part (Section 2) we
present our probabilistic model of verbal logical metonymy and describe the model
parameters. In Experiment 1 we use the model to derive the meaning paraphrases for
verb-noun combinations randomly selected from the BNC (see Section 2.3) and for-
mally evaluate our results against human intuitions. Experiment 2 demonstrates that
when compared against human judgments, our model outperforms a naive baseline
in deriving a preference ordering for the meanings of verb-noun combinations, and
Experiment 3 evaluates an extension of the basic model. In the second part (Section 3),
we focus on adjectival logical metonymy. Section 3.1 introduces our probabilistic for-
malization for polysemous metonymic adjectives and Sections 3.3?3.6 present our ex-
periments and evaluate our results. Overall, the automatically acquired model of log-
ical metonymy reliably correlates with human intuitions and also predicts the relative
degree of ambiguity and acceptability of the various metonymic constructions. In Sec-
tion 4 we discuss our results, we review related work in Section 5, and we conclude
in Section 6.
2. Metonymic Verbs
2.1 The Model
Consider the verb-noun combinations in (7) and (8). Our task is to come up with
(7b) and (8b) as appropriate interpretations for (7a) and (8a). Although the interpreta-
tions of (7a) and (8a) are relatively straightforward for English speakers, given their
general knowledge about coffees and films and the activities or events associated
with them, a probabilistic model requires detailed information about words and their
interdependencies in order to generate the right interpretation. Examples of such in-
terdependencies are verbs co-occurring with coffee (e.g., drink, make, prepare) or verbs
that are related to begin (e.g., make, realize, understand).
(7) a. John began the coffee.
b. John began drinking the coffee.
(8) a. Mary enjoyed the film.
b. Mary enjoyed watching the film.
A relatively straightforward approach to the interpretation of (7a) and (8a) would
be to extract from the corpus (via parsing) paraphrases in which the additional in-
formation (e.g., drinking and watching), which is absent from (7a) and (8a), is fleshed
out. In other words, we would like to find in the corpus sentences whose main verb is
begin followed either by the progressive VP complement drinking or by the infinitive to
drink, selecting for the NP coffee as its object. In the general case we would like to find
the activities or events related both to the verb begin and the noun coffee (e.g., drinking,
buying, making, preparing). Similarly, in order to paraphrase (8a) we need information
267
Lapata and Lascarides Logical Metonymy
about the VP complements that are associated with enjoy and can take film as their
object (e.g., watching, making, shooting).
The above paraphrase-based model is attractive given its simplicity: All we need
to do is count the co-occurrences of a verb, its complements, and their objects. The
approach is unsupervised, no manual annotation is required, and no corpus-external
resources are used. Such a model relies on the assumption that the interpretations
of (7a) and (8a) can be approximated by their usage; that is, it assumes that the likeli-
hood of uttering the metonymic construction is equal to that of uttering its interpre-
tation. However, this assumption is not borne out. Only four sentences in the BNC
are relevant for the interpretation of begin coffee (see (9)); likewise, four sentences are
relevant for the interpretation of enjoy film (see (10)).
(9) a. Siegfried bustled in, muttered a greeting and began to pour his
coffee.
b. She began to pour coffee.
c. Jenna began to serve the coffee.
d. Victor began dispensing coffee.
(10) a. I was given a good speaking part and enjoyed making the film.
b. He?s enjoying making the film.
c. Courtenay enjoyed making the film.
d. I enjoy most music and enjoy watching good films.
e. Did you enjoy acting alongside Marlon Brando in the recent
film The Freshman?
The attested sentences in (9) are misleading if they are taken as the only evidence
for the interpretation of begin coffee, for on their own they suggest that the most likely
interpretation for begin coffee is begin to pour coffee, whereas begin to serve coffee and begin
dispensing coffee are less likely, as they are attested in the corpus only once. Note that
the sentences in (9) fail to capture begin to drink coffee as a potential interpretation for
begin coffee. On the basis of the sentences in (10), enjoy making the film is the most likely
interpretation for (8a), whereas enjoy watching the film and enjoy acting in the film are
equally likely.
This finding complies with Briscoe, Copestake, and Boguraev?s (1990) results for
the LOB corpus: begin V NP is very rare when the value of V corresponds to a highly
plausible interpretation of begin NP. Indeed, one can predict that problems with finding
evidence for begin V NP will occur on the basis of Gricean principles of language
production, where the heuristic be brief (which is part of the maxim of manner) will
compel speakers to utter begin coffee as opposed to begin V coffee if V is one of the
plausible interpretations of begin coffee. Thus on the basis of this Gricean reasoning,
one might expect metonymies like (7a) and (8a) to occur with greater frequencies than
their respective paraphrases (see (7b) and (8b)). Tables 1?3 show BNC counts of verb-
noun metonymies (commonly cited in the lexical semantics literature (Pustejovsky
1995; Verspoor 1997)) and their corresponding interpretations when these are attested
in the corpus. The data in Tables 1?3 indicate that metonymic expressions are more
often attested in the BNC with NP rather than with VP complements.
The discrepancy between an interpretation and its usage could be circumvented
by using a corpus labeled explicitly with interpretation paraphrases. Lacking such a
corpus, we will sketch below an approach to the interpretation of metonymies that
retains the simplicity of the paraphrase-based account but no longer assumes a tight
correspondence between a metonymic interpretation and its usage. We present an
268
Computational Linguistics Volume 29, Number 2
Table 1
BNC frequencies for begin.
Examples begin NP begin V-ing NP
begin book 35 17
begin sandwich 4 0
begin beer 2 1
begin speech 21 4
begin solo 1 1
begin song 19 8
begin story 31 15
Table 2
BNC frequencies for enjoy.
Examples enjoy NP enjoy V-ing NP
enjoy symphony 34 30
enjoy movie 5 1
enjoy coffee 8 1
enjoy book 23 9
like movie 18 3
Table 3
BNC frequencies for want.
Examples want NP want V-ing NP
want cigarette 18 3
want beer 15 8
want job 116 60
unsupervised method that generates interpretations for verbal metonymies without
recourse to manually annotated data or taxonomic information; it requires only a
part-of-speech-tagged corpus and a partial parser.
We model the interpretation of a verbal metonymy as the joint distribution P(e, o, v)
of three variables: the metonymic verb v (e.g., enjoy), its object o (e.g., film), and the
sought-after interpretation e (e.g., making, watching, directing). By choosing the ordering
?e, v, o? for the variables e, v, and o, we can factor P(e, o, v) as follows:
P(e, o, v) = P(e) ? P(v | e) ? P(o | e, v) (11)
The probabilities P(e), P(v | e), and P(o | e, v) can be estimated using maximum likeli-
hood as follows:
P?(e) =
f (e)
N
(12)
P?(v | e) = f (v, e)
f (e)
(13)
P?(o | e, v) = f (o, e, v)
f (e, v)
(14)
269
Lapata and Lascarides Logical Metonymy
Table 4
Most frequent complements of enjoy and film.
f(enjoy, e) f(film, e)
play 44 make 176
watch 42 be 154
work with 35 see 89
read 34 watch 65
make 27 show 42
see 24 produce 29
meet 23 have 24
go to 22 use 21
use 17 do 20
take 15 get 18
Although P(e) and P(v | e) can be estimated straightforwardly from a corpus (f (e)
amounts to the number of the times a given verb e is attested, N is the number of
verbs found in the corpus (excluding modals and auxiliaries), and P(v | e) can be ob-
tained through parsing, by counting the number of times a verb v takes e as its comple-
ment), the estimation of P(o | e, v) is problematic. It presupposes that co-occurrences of
metonymic expressions and their interpretations are to be found in a given corpus, but
as we?ve seen previously, there is a discrepancy between a metonymic interpretation
and its usage. In fact, metonymies occur more frequently than their overt interpreta-
tions (expressed by the term f (o, e, v) in (14)), and the interpretations in question are not
explicitly marked in our corpus. We will therefore make the following approximation:
P(o | e, v) ? P(o | e) (15)
P?(o | e) = f (o, e)
f (e)
(16)
The rationale behind this approximation is that the likelihood of seeing a noun o as the
object of an event e is largely independent of whether e is the complement of another
verb. In other words, v is conditionally independent of e, since the likelihood of o is
(largely) determined on the basis of e and not of v. Consider again example (8a): Mary
enjoyed the film. Here, film, the object of enjoy, is more closely related to the underspeci-
fied interpretation e rather than to enjoy. For example, watching movies is more likely
than eating movies, irrespective of whether Mary enjoyed or liked watching them.
We estimate P(o | e) as shown in (16). The simplification in (15) results in a compact
model with a relatively small number of parameters that can be estimated straightfor-
wardly from the corpus in an unsupervised manner. By substituting equations (12),
(13), and (16) into (11) and simplifying the relevant terms, (11) can be rewritten as
follows:
P(e, o, v) =
f (v, e) ? f (o, e)
N ? f (e) (17)
Assume we want to generate meaning paraphrases for the verb-noun pair enjoy
film (see (8a)). Table 4 lists the most frequent events related to the verb enjoy and the
most frequent verbs that take film as their object (we describe how the frequencies
f (v, e) and f (o, e) were obtained in the following section). We can observe that seeing,
watching, making, and using are all events associated with enjoy and with film and will
270
Computational Linguistics Volume 29, Number 2
be therefore generated as likely paraphrases for the metonymic expression enjoy film
(see Table 4, in which the underlined verbs indicate common complements between
the metonymic verb and its object).
Note that the model in (17) does not represent the fact that the metonymic verb v
may have a subject. This in practice means that the model cannot distinguish between
the different readings for (18a) and (18b): in (18a) the doctor enjoyed watching the film,
whereas in (18b) the director enjoyed making or directing the film. The model in (17)
will generate the set of events that are associated with enjoying films (e.g., watching,
making, seeing, going to), ignoring the contribution of the sentential subject. We present
in Section 2.5.1 an extension of the basic model that takes sentential subjects into
account.
(18) a. The doctor enjoyed the film.
b. The director enjoyed the film.
It is important to stress that the probabilistic model outlined above is a model of the
interpretation rather than the grammaticality of metonymic expressions. In other words,
we do not assume that it can distinguish between well-formed and odd metonymic
expressions (see the examples in (4)). In fact, it will generally provide a set of interpre-
tation paraphrases, even for odd formulations. The model in (11) has no component
that corresponds to the occurrence of v and o together. Choosing the ordering ?o, v, e?
for the variables o, e, and v would result in the following derivation for P(e, o, v):
P(e, o, v) = P(o) ? P(v | o) ? P(e | o, v) (19)
The term P(v | o) in (19) explicitly takes into account the likelihood of occurrence of
the metonymic expression. This means that no interpretation will be provided for odd
metonymies like enjoy the highway as long as they are not attested in the corpus. Such a
model penalizes, however, well-formed metonymies that are not attested in the corpus.
A striking example is enjoy the ice cream, which is a plausible metonymy not attested
at all in the BNC and thus by (19) would be incorrectly assigned no interpretations.
This is because the maximum-likelihood estimate of P(v | o) relies on the co-occurrence
frequency f (v, o), which is zero for enjoy the ice cream. But the probabilistic model in (11)
will generate meaning paraphrases for metonymic verb-object pairs that have not been
attested in the corpus as long as the co-occurrence frequencies f (v, e) and f (o, e) are
available.
Finally, note that our model is ignorant with respect to the discourse context
within which a given sentence is embedded. This means that it will come up with
the same ranked set of meanings for (20b), irrespective of whether it is preceded by
sentence (20a) or (21a). The model thus does not focus on the meaning of individ-
ual corpus tokens; instead it determines the most dominant meanings for a given
verb-noun combination overall, across all of its instances in the corpus.
(20) a. Who is making the cigarettes for tomorrow?s party?
b. John finished three cigarettes.
c. John finished making three cigarettes.
(21) a. Why is the room filled with smoke?
b. John finished three cigarettes.
c. John finished smoking three cigarettes.
271
Lapata and Lascarides Logical Metonymy
2.2 Parameter Estimation
We estimated the parameters of the model outlined in the previous section from a part-
of-speech-tagged and lemmatized version of the BNC, a 100-million-word collection
of samples of written and spoken language from a wide range of sources designed to
represent current British English (Burnard 1995). The counts f (v, e) and f (o, e) (see (17))
were obtained automatically from a partially parsed version of the BNC created using
Cass (Abney 1996), a robust chunk parser designed for the shallow analysis of noisy
text. The parser?s built-in function was employed to extract tuples of verb-subjects
and verb-objects (see (22)). Although verb-subject relations are not relevant for the
present model, they are important for capturing the influence of the sentential subject
(see Section 2.5) and modeling the interpretations of polysemous adjectives (which we
discuss in Section 3).
(22) a. change situation SUBJ
b. come off heroin OBJ
c. deal with situation OBJ
(23) a. isolated people SUBJ
b. smile good SUBJ
The tuples obtained from the parser?s output are an imperfect source of informa-
tion about argument relations. Bracketing errors, as well as errors in identifying chunk
categories accurately, results in tuples whose lexical items do not stand in a verb-
argument relationship. For example, inspection of the original BNC sentences from
which the tuples in (23) were derived reveals that the verb be is missing from (23a)
and the noun smile is missing from (23b) (see the sentences in (24)).
(24) a. Wenger found that more than half the childless old people in
her study of rural Wales saw a relative, a sibling, niece, nephew
or cousin at least once a week, though in inner city London
there were more isolated old people.
b. I smiled my best smile down the line.
In order to compile a comprehensive count of verb-argument relations, we dis-
carded tuples containing verbs or nouns attested in a verb-argument relationship only
once. Instances of the verb be were also eliminated, since they contribute no semantic
information with respect to the events or activities that are possibly associated with
the noun with which the verb is combined. Particle verbs (see (22b)) were retained
only if the particle was adjacent to the verb. Verbs followed by the preposition by
and a head noun were considered instances of verb-subject relations. The verb-object
tuples also included prepositional objects (see (22c)). It was assumed that PPs adja-
cent to the verb headed by any of the prepositions in, to, for, with, on, at, from, of, into,
through, and upon were prepositional objects.2 This resulted in 737,390 distinct types of
verb-subject pairs and 1,078,053 distinct types of verb-object pairs (see Table 5, which
presents information about the tuples extracted from the corpus before and after the
filtering).
2 The POS tagging of the BNC (Leech, Garside, and Bryant 1994) distinguishes between verb particle
constructions like down in climb down the mountain and up in put up the painting, on the one hand, and
prepositions, on the other. So this allowed us to distinguish PP complements from NP ones.
272
Computational Linguistics Volume 29, Number 2
Table 5
Number of tuples extracted from the BNC.
Tokens Types
Relation Parser Filtering Tuples Verbs Nouns
SUBJ 4,759,950 4,587,762 737,390 14,178 25,900
OBJ 3,723,998 3,660,897 1,078,053 12,026 35,867
The frequency f (v, e) represents verbs taking progressive or infinitive VP comple-
ments. These were extracted from the parser?s output by looking for verbs followed by
progressive or infinitival complements (a special tag, VDG, is reserved in the BNC for
verbs in the progressive). The latter were detected by looking for verbs followed by in-
finitives (indicated by the marker to (TO0) and a verb in base form (VVI)). The examples
below illustrate the information extracted from the parser?s output for obtaining the
frequency f (v, e), which collapsed counts for progressive and infinitive complements.
(25) a. I had started to write a love-story. start write
b. She started to cook with simplicity. start cook
c. The suspect attempted to run off. attempt run off
(26) a. I am going to start writing a book. start write
b. I?ve really enjoyed working with you. enjoy work with
c. The phones began ringing off the hook. begin ring off
Note that some verbs (e.g., start) allow both an infinitival and a progressive com-
plement (see (25a) and (26a), respectively), whereas other verbs (e.g., attempt) allow
only one type of complement (see (25c)). Even for verbs that allow both types of
complements, there exist syntactic contexts in which the two complement types are in
complementary distribution: to start writing occurs 15 times in the BNC, whereas to start
to write does not occur at all. The situation is reversed for starting writing and starting
to write, for the former does not occur and the latter occurs seven times. Choosing to
focus only on one type of complement would result in a lower count for f (v, e) than
collapsing the counts observed for both types of complements.
Once we have obtained the frequencies f (v, e) and f (o, e), we can determine the
most likely interpretations for metonymic verb-noun combinations. Note that we may
choose to impose thresholds on the frequencies f (v, e) and f (o, e) (e.g., f (v, e) > 1, and
f (o, e) > 1), depending on the quality of the parsing data or the type of meaning
paraphrases we seek to discover (e.g., likely versus unlikely ones).
As an example of the paraphrases generated by our model, consider the sentences
in Table 6, which were cited as examples of logical metonymy in the lexical semantics
literature (Pustejovsky 1995; Verspoor 1997). The five most likely interpretations for
these metonymies (and their respective log-transformed probabilities) are illustrated in
Table 7. Note that the model comes up with plausible meanings, some of which overlap
with those suggested in the lexical semantics literature (underlined interpretations
indicate agreement between the model and the literature). Also, the model derives
several meanings, as opposed to the single interpretations provided in most cases in
the literature. Consider, for example, the pair begin story in Table 7. Here, not only the
interpretation tell is generated, but also write, read, retell, and recount. Another example
273
Lapata and Lascarides Logical Metonymy
Table 6
Paraphrases for verb-noun combinations taken from the literature.
John began the story ? telling (Verspoor 1997, page 189)
John began the song ? singing (Verspoor 1997, page 189)
John began the sandwich ? eating/making (Verspoor 1997, page 167)
Mary wants a job ? to have (Pustejovsky 1995, page 45)
John began the book ? reading/writing (Verspoor 1997, page 167)
Bill enjoyed Steven King?s last book ? reading (Pustejovsky 1995, page 88)
John began the cigarette ? smoking (Verspoor 1997, page 167)
Harry wants another cigarette ? to smoke (Pustejovsky 1995, page 109)
Table 7
Model-derived paraphrases for verbal metonymies, ranked in order of likelihood.
begin story begin song begin sandwich want job
tell ?16.34 sing ?15.14 bite into ?18.12 get ?14.87
write ?17.02 rehearse ?16.15 eat ?18.23 lose ?15.72
read ?17.28 write ?16.86 munch ?19.13 take ?16.40
retell ?17.45 hum ?17.45 unpack ?19.14 make ?16.52
recount ?17.80 play ?18.01 make ?19.42 create ?16.62
begin book enjoy book begin cigarette want cigarette
read ?15.49 read ?16.48 smoke ?16.92 smoke ?16.67
write ?15.52 write ?17.58 roll ?17.63 take ?18.23
appear in ?16.98 browse through ?18.56 light ?17.76 light ?18.45
publish ?17.10 look through ?19.68 take ?18.88 put ?18.51
leaf through ?17.35 publish ?19.93 twitch ?19.17 buy ?18.64
is begin song, for which the model generates the interpretations rehearse, write, hum,
and play, in addition to sing.
The model also exhibits slight variation in the interpretations for a given noun
among the different metonymic verbs (compare begin book and enjoy book and begin
cigarette and want cigarette in Table 7). This is in line with claims made in the lexical
semantics literature (Copestake and Briscoe 1995; Pustejovsky 1995; Verspoor 1997),
and it ultimately contributes to an improved performance against a ?naive baseline?
model (see Section 2.4).
In some cases, the model comes up with counterintuitive interpretations: bite into
is generated as the most likely interpretation for begin sandwich (although the latter
interpretation is not so implausible, since eating entails biting into). The model also
fails to rank have as one of the five most likely interpretations for want job (see Ta-
ble 7). The interpretations get and take are, however, relatively likely; note that they
semantically entail the desired interpretation?namely, have?as a poststate. The inter-
pretations make and create imply the act of hiring rather than finding a job. Our model
cannot distinguish between the two types of interpretations. It also cannot discover
related meanings: for example, that get and take mean have or that tell, retell, and recount
(see Table 7) mean tell. (We return to this issue in Section 4.)
In the following section we test our model against verb-noun pairs randomly
selected from the BNC and evaluate the meaning paraphrases it generates against
human judgments. We explore the linear relationship between the subjects? rankings
and the model-derived probabilities using correlation analysis.
274
Computational Linguistics Volume 29, Number 2
2.3 Experiment 1: Comparison against Human Judgments
Although there is no standard way to evaluate the paraphrases generated by the model
(there is no gold standard for comparison), a reasonable way to judge the model?s per-
formance would seem to be its degree of agreement with human paraphrase ratings.
This can be roughly measured by selecting some metonymic constructions, deriving
their paraphrase interpretations using the model outlined in Section 2.1, eliciting hu-
man judgments on these paraphrases, and then looking at how well the human ratings
correlate with the model probabilities for the same paraphrases.
In the following section we describe our method for assembling the set of exper-
imental materials and eliciting human-subject data for the metonymy paraphrasing
task. We use correlation analysis to compare the model probabilities against human
judgments and explore whether there is a linear relationship between the model-
derived likelihood of a given meaning and its perceived plausibility.
In Section 2.4.1 we introduce a naive model of verbal metonymy that does not
take the contribution of the metonymic verb into account; metonymic interpretations
(i.e., verbs) are simply expressed in terms of their conditional dependence on their
objects. We investigate the naive model?s performance against the human judgments
and the paraphrases generated by our initial model (see Section 2.4).
2.3.1 Method.
2.3.1.1 Materials and Design. From the lexical semantics literature (Pustejovsky 1995;
Verspoor 1997; McElree et al 2001) we compiled a list of 20 verbs that allow logical
metonymy. From these we randomly selected 12 verbs (attempt, begin, enjoy, finish,
expect, postpone, prefer, resist, start, survive, try, and want). The selected verbs ranged in
BNC frequency from 10.9 per million to 905.3 per million. Next, we paired each one of
them with five nouns randomly selected from the BNC. The nouns had to be attested
in the corpus as the object of the verbs in question. Recall that verb-object pairs were
identified using Abney?s (1996) chunk parser Cass (see Section 2.2 for details). From
the retrieved verb-object pairs, we removed all pairs with BNC frequency of one, as we
did not want to include verb-noun combinations that were potentially unfamiliar to
the subjects. We used the model outlined in Section 2.1 to derive meaning paraphrases
for the 60 verb-noun combinations.
Our materials selection procedure abstracts over semantic distinctions that are
made in linguistic analyses. For instance, current models of lexical semantics typically
assign verbs such as enjoy a nonmetonymic sense when they are combined with NPs
that are purely temporal or eventive in nature, as in enjoy the marriage or enjoy the lecture
(Copestake and Briscoe 1995; Verspoor 1997). This is largely because a logical form
can be constructed in such cases without the use of semantic type coercion; the event-
denoting NP itself is the argument to the predicate enjoy. We did not rule out such
nouns from our materials, however, as our evaluation was conducted on randomly
selected verb-noun pairs.
More generally, we abstract over several criteria that Verspoor (1997) used in dis-
tinguishing metonymic from nonmetonymic uses within the corpus, and we adopt
a linguistically naive approach for two reasons. First, whereas Verspoor (1997) could
deploy more refined criteria because she was hand-selecting the materials from the
corpus and was focusing only on two metonymic verbs (begin and finish), our materials
were randomly sampled and covered a wider range of metonymic constructions. And
second, paraphrases for nonmetonymic cases (e.g., that enjoy the lecture can be para-
phrased as enjoy attending the lecture or enjoy listening to the lecture) may be useful for
some potential NLP applications (see the discussion in Section 4.2), since they provide
275
Lapata and Lascarides Logical Metonymy
Table 8
Number of generated interpretations as frequency cutoff for
f (v, e) and f (o, e) is varied.
Verb-noun
f(v,e) ? 1
f(o,e) ? 1
f(v,e) ? 2
f(o,e) ? 2
f(v,e) ? 3
f(o,e) ? 3
f(v,e) ? 4
f(n,e) ? 4
finish gig 11 4 3 1
finish novel 31 11 5 3
finish project 65 20 8 6
finish room 79 25 16 10
finish video 44 16 9 6
more detailed information about meaning than would be given by a logical form that
simply features enjoy(e, x, e?), where e? is the (event) variable that denotes the lecture.
Recall from Section 2.2 that thresholding is an option for the counts f (v, e) and
f (o, e). We derived model paraphrases without employing any thresholds for these
counts. Obtaining f (v, e) from the parsed data was relatively straightforward, as there
was no structural ambiguity involved. The parser?s output was postprocessed to re-
move potentially erroneous information, so there was no reason to believe that the
frequencies f (v, e) and f (o, e) were noisy. Furthermore, recent work has shown that
omitting low-frequency tuples degrades performance for language-learning tasks such
as PP attachment (Collins and Brooks 1995; Daelemans, van den Bosch, and Zavrel
1999), grapheme-to-phoneme conversion, POS tagging, and NP chunking (Daelemans,
van den Bosch, and Zavrel 1999). For our task, employing thresholds for f (v, e) and
f (o, e) dramatically decreases the number of derived interpretations. Table 8 shows the
decrease in the number of interpretations as the cutoff for f (v, e) and f (o, e) is varied for
five verb-object pairs that were included in our experimental study. Note that discard-
ing counts occurring in the corpus only once reduces the number of interpretations by
a factor of nearly three. Furthermore, applying frequency cutoffs reduces the range of
the obtained probabilities: only likely (but not necessarily plausible) interpretations are
obtained with f (o, e) ? 4 and f (v, e) ? 4. However, one of the aims of the experiment
outlined below was to explore the quality of interpretations with varied probabilities.
Table 9 displays the 10 most likely paraphrases (and their log-transformed probabili-
ties) for finish room as the cutoff for the frequencies f (v, e) and f (o, e) is varied. Notice
that applying a cutoff of three or four eliminates plausible interpretations such as dec-
orate, wallpaper, furnish, and tidy. This may be particularly harmful for verb-noun (or
adjective-noun) combinations that allow for a wide range of interpretations (like finish
room).
We estimated the probability P(e, o, v) for each verb-noun pair by varying the term
e. In order to generate stimuli covering a wide range of paraphrases corresponding to
different degrees of likelihood, for each verb-noun combination we divided the set of
generated meanings into three ?probability bands? (high, medium, and low) of equal
size and randomly chose one interpretation from each band. This division ensured
that subjects saw a wide range of paraphrases with different degrees of likelihood.
Our experimental design consisted of two factors: verb-noun pair (Pair) and proba-
bility band (Band). The factor Pair included 60 verb-noun combinations, and the factor
Band had three levels, high, medium, and low. This yielded a total of Pair ? Band
= 60 ? 3 = 180 stimuli. In order to limit the size of the experiment, the 180 stimuli
were administered to two separate groups of subjects. The first group saw meaning
paraphrases for the verbs attempt, begin, want, enjoy, try, and expect, whereas the sec-
276
Computational Linguistics Volume 29, Number 2
Table 9
Ten most likely interpretations for finish room (with log-transformed
probabilities) as frequency threshold is varied.
f(v,e) ? 1 f(v,e) ? 2 f(v,e) ? 3 f(v,e) ? 4
f(o,e) ? 1 f(o,e) ? 2 f(o,e) ? 3 f(o,e) ? 4
decorate ?18.47 decorate ?18.47 fill ?18.88 fill ?18.88
wallpaper ?19.07 fill ?18.89 clean ?19.08 clean ?19.08
clean ?19.09 clean ?19.08 pack ?20.17 pack ?20.17
paper ?19.09 search ?20.13 make ?20.36 make ?20.36
furnish ?19.31 pack ?20.17 view ?20.78 check ?21.24
tidy ?19.92 make ?20.36 check ?21.24 use ?21.78
search ?20.13 dress ?20.55 pay ?21.53 build ?21.96
pack ?20.17 view ?20.78 use ?21.78 give ?22.29
make ?20.36 check ?21.24 build ?21.96 prepare ?22.45
view ?20.78 paint ?21.38 give ?22.29 take ?23.11
Table 10
Randomly selected example stimuli with log-transformed probabilities derived by the
model.
Probability Band
Verb-noun
High Medium Low
attempt peak climb ?20.22 claim ?23.53 include ?24.85
begin production organize ?19.09 influence ?21.98 tax ?22.79
enjoy city live in ?20.77 come to ?23.50 cut ?24.67
expect reward collect ?21.91 claim ?23.13 extend ?23.52
finish room wallpaper ?19.07 construct ?22.49 want ?24.60
postpone payment make ?21.85 arrange ?23.21 read ?25.92
prefer people talk to ?20.52 sit with ?22.75 discover ?25.26
resist song whistle ?22.12 start ?24.47 hold ?26.50
start letter write ?15.59 study ?22.70 hear ?24.50
survive course give ?22.88 make ?24.48 write ?26.27
try drug take ?17.81 grow ?22.09 hate ?23.88
want hat buy ?17.85 examine ?21.56 land on ?22.38
ond group saw paraphrases for finish, prefer, resist, start, postpone, and survive. Example
stimuli are shown in Table 10.
Each experimental item consisted of two sentences, a sentence containing a meto-
nymic construction (e.g., Peter started his dinner) and a sentence paraphrasing it (e.g.,
Peter started eating his dinner). The metonymic sentences and their paraphrases were
created by the authors as follows. The selected verb-noun pairs were converted into
simple sentences by adding a sentential subject and articles or pronouns where appro-
priate. The sentential subjects were familiar proper names (BNC corpus frequency >
30 per million) balanced for gender. All sentences were in the past tense. In the para-
phrasing sentences, the metonymy was spelled out by converting the model?s output
to a verb taking either a progressive or infinitive VP complement (e.g., started to eat or
started eating). For verbs allowing both a progressive and an infinitive VP complement,
we chose the type of complement with which the verb occurred more frequently in
the corpus. A native speaker of English other than the authors was asked to confirm
that the metonymic sentences and their paraphrases were syntactically well-formed
(items found syntactically odd were modified and retested). Examples of the experi-
277
Lapata and Lascarides Logical Metonymy
mental stimuli the subjects saw are provided in (27) and (28). A complete list of the
experimental items is given in Appendix B.
(27) a. high: Michael attempted the peak
Michael attempted to climb the peak
b. medium: Michael attempted the peak
Michael attempted to claim the peak
c. low: Michael attempted the peak
Michael attempted to include the peak
(28) a. high: Jean enjoyed the city
Jean enjoyed living in the city
b. medium: Jean enjoyed the city
Jean enjoyed coming to the city
c. low: Jean enjoyed the city
Jean enjoyed cutting the city
2.3.1.2 Procedure. The experimental paradigm was magnitude estimation (ME), a
technique standardly used in psychophysics to measure judgments of sensory stim-
uli (Stevens 1975). The ME procedure requires subjects to estimate the magnitude of
physical stimuli by assigning numerical values proportional to the stimulus magni-
tude they perceive. Highly reliable judgments can be achieved in this fashion for a
wide range of sensory modalities, such as brightness, loudness, or tactile stimulation.
The ME paradigm has been extended successfully to the psychosocial domain
(Lodge 1981), and recently Bard, Robertson, and Sorace (1996) and Cowart (1997)
showed that linguistic judgments can be elicited in the same way as judgments of
sensory or social stimuli. ME requires subjects to assign numbers to a series of linguistic
stimuli in a proportional fashion. Subjects are first exposed to a modulus item, to
which they assign an arbitrary number. All other stimuli are rated proportional to the
modulus. In this way, each subject can establish his own rating scale, thus yielding
maximally fine-grained data and avoiding the known problems with the conventional
ordinal scales for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997;
Schu?tze 1996). In particular, ME does not restrict the range of the responses. No matter
which modulus a subject chooses, he or she can subsequently assign a higher or lower
judgment by using multiples or fractions of the modulus.
In the present experiment, each subject took part in an experimental session that
lasted approximately 20 minutes. The experiment was self-paced, and response times
were recorded to allow the data to be screened for anomalies. The experiment was con-
ducted remotely over the Internet. Subjects accessed the experiment using their Web
browser, which established an Internet connection to the experimental server running
WebExp 2.1 (Keller, Corley, and Scheepers 2001), an interactive software package for
administering Web-based psychological experiments. (For a discussion of WebExp and
the validity of Web-based data, see Appendix A).
2.3.1.3 Instructions. Before participating in the actual experiment, subjects were pre-
sented with a set of instructions. The instructions explained the concept of numeric
magnitude estimation of line length. Subjects were instructed to make estimates of line
length relative to the first line they would see, the reference line. Subjects were told
to give the reference line an arbitrary number, and then assign a number to each fol-
lowing line so that it represented how long the line was in proportion to the reference
278
Computational Linguistics Volume 29, Number 2
line. Several example lines and corresponding numerical estimates were provided to
illustrate the concept of proportionality.
The subjects were instructed to judge how well a particular sentence paraphrased
another sentence, using the same technique that they had applied to judging line
length. Examples of plausible (see (29a)) and implausible (see (29b)) sentence para-
phrases were provided, together with examples of numerical estimates.
(29) a. Peter started his dinner Peter started eating his dinner
b. Peter started his dinner Peter started writing his dinner
Subjects were informed that they would initially have to assign a number to a reference
paraphrase. For each subsequent paraphrase, subjects were asked to assign a number
indicating how good or bad that paraphrase was in proportion to the reference.
Subjects were told that they could use any range of positive numbers for their
judgments, including decimals. It was stressed that there was no upper or lower limit
on the numbers that could be used (exceptions being zero or negative numbers).
Subjects were urged to use a wide range of numbers and to distinguish as many
degrees of paraphrase plausibility as possible. It was also emphasized that there were
no ?correct? answers and that subjects should base their judgments on first impressions
and not spend too much time thinking about any one paraphrase.
2.3.1.4 Demographic Questionnaire. After the instructions, a short demographic ques-
tionnaire was administered. The questionnaire asked subjects to provide their name,
e-mail, address, age, sex, handedness, academic subject or occupation, and language
region. Handedness was defined as ?the hand you prefer to use for writing?; language
region was defined as ?the place (town, federal state, country) where you learned your
first language.?
2.3.1.5 Training Phase. The training phase was meant to familiarize subjects with the
concept of numeric magnitude estimation using line lengths. Items were presented as
horizontal lines, centered in the window of the subject?s Web browser. After viewing
an item, the subject had to provide a numerical judgment via the computer keyboard.
After the subject pressed Return, the current item disappeared and the next item was
displayed. There was no opportunity to revisit previous items or change responses
once Return had been pressed. No time limit was set either for the item presentation
or for the response, although response times were recorded for later inspection.
Subjects first judged the modulus item, and then all the items in the training
set. The modulus was the same for all subjects, and it remained on the screen all
the time to facilitate comparison. Items were presented in random order, with a new
randomization being generated for each subject.
The training set contained six horizontal lines. The range of the shortest to longest
item was one to ten (that is, the longest line was ten times the length of the shortest).
The items were distributed evenly over this range, with the largest item covering the
maximal window width of the Web browser. A modulus item in the middle of the
range was provided.
2.3.1.6 Practice Phase. The practice phase enabled subjects to practice magnitude esti-
mation of verb-noun paraphrases. Presentation and response procedure was the same
as in the training phase, with linguistic stimuli being displayed instead of lines. Each
subject judged the whole set of practice items, again presented to him or her in random
order.
279
Lapata and Lascarides Logical Metonymy
The practice set consisted of eight paraphrase sentences that were representative
of the test materials. The paraphrases were based on the three probability bands and
represented a wide range of probabilities. A modulus item selected from the medium
probability band was provided.
2.3.1.7 Experimental Phase. The presentation and response procedure in the exper-
imental phase were the same as in the practice phase. Subjects were assigned to
groups at random, and a random stimulus order was generated for each subject (for
the complete list of experimental stimuli, see Appendix B). Each group of subjects
saw 90 experimental stimuli (i.e., metonymic sentences and their paraphrases). As in
the practice phase, the paraphrases were representative of the three probability bands
(high, medium, low). Again a modulus item from the medium probability band was
provided (see Appendix B). The modulus was the same for all subjects and remained
on the screen the entire time the subject was completing the task.
2.3.1.8 Subject. Sixty-three native speakers of English participated in the experiment.
The subjects were recruited over the Internet through advertisements posted to news-
groups and mailing lists. Participation was voluntary and unpaid. Subjects had to be
linguistically naive (i.e., neither linguists nor students of linguistics were allowed to
participate).
The data of two subjects were eliminated after inspection of their response times
showed that they had not completed the experiment in a realistic time frame (i.e., they
provided ratings too quickly, with average response time < 1000 ms). The data of one
subject were excluded because she was a non-native speaker of English.
This left 60 subjects for analysis. Of these, 53 subjects were right-handed, 7 left-
handed; 24 subjects were female, 36 male. The age of subjects ranged from 17 to 62;
the mean was 26.4 years.
2.3.2 Results. The data were first normalized by dividing each numerical judgment
by the modulus value that the subject had assigned to the reference sentence. This
operation created a common scale for all subjects. Then the data were transformed by
taking the decadic logarithm. This transformation ensured that the judgments were
normally distributed and is standard practice for magnitude estimation data (Bard,
Robertson, and Sorace 1996; Lodge 1981). All further analyses were conducted on the
resulting normalized, log-transformed judgments.
We performed a correlation analysis to determine whether there was a linear rela-
tion between the paraphrases generated by the model and their perceived likelihood.
This tested the hypothesis that meaning paraphrases assigned high probabilities by the
model are perceived as better paraphrases by the subjects than meaning paraphrases
assigned low probabilities. For each experimental item we computed the average of
the normalized and log-transformed subject ratings. The mean subject ratings were
then compared against the (log-transformed) probabilities assigned by the model for
the same items.
The comparison between the absolute model probabilities and the human judg-
ments yielded a Pearson correlation coefficient of .64 (p < .01, N = 174; six items were
discarded because of a coding error). The mean subject ratings and the model prob-
abilities are given in Appendix B. Appendix C presents descriptive statistics for the
model probabilities and the human judgments. The relationship between judgments
and probabilities is plotted in Figure 1.
An important question is how well humans agree in their paraphrase judgments
for verb-noun combinations. Intersubject agreement gives an upper bound for the
280
Computational Linguistics Volume 29, Number 2
-30 -25 -20 -15 -10
log-transformed model probabilities
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
m
ea
n
 s
u
bje
ct 
rat
ing
s
Figure 1
Correlation of elicited judgments and model-derived probabilities for metonymic verb-noun
pairs.
task and allows us to interpret how well the model is doing in relation to humans.
To calculate intersubject agreement, we used leave-one-out resampling. The technique
is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and has been
previously used for measuring how well humans agree on judging semantic similarity
(Resnik and Diab 2000; Resnik 1999).
For each subject group we divided the set of the subjects? responses with size m
into a set of size m ? 1 (i.e., the response data of all but one subject) and a set of size
one (i.e., the response data of a single subject). We then correlated the mean ratings
of the former set with the ratings of the latter. This was repeated m times. Since each
group had 30 subjects, we performed 30 correlation analyses and report their mean.
For the first group of subjects, the average intersubject agreement was .74 (Min = .19,
Max = .87, StdDev = .12), and for the second group it was .73 (Min = .49, Max
= .87, StdDev = .09). Our model?s agreement with the human data is not far from the
average human performance of .74.
In the following section we introduce a naive model of verbal metonymy. We
compare the naive model?s performance against the human judgments and the para-
phrases generated by our initial model. We discuss extensions of the basic model in
Section 2.5.1.
2.4 Experiment 2: Comparison against Naive Baseline
2.4.1 Naive Baseline Model. In the case of verbal metonymy a naive baseline model
can be constructed by simply taking verb-noun co-occurrence data into account, ig-
noring thus the dependencies between the polysemous verb and its progressive or
infinitival VP complements. Consider the sentence John began the book. In order to gen-
erate appropriate paraphrases for begin book, we will consider solely the verbs that take
book as their object (i.e., read, write, buy, etc.). This can be simply expressed as P(e | o),
the conditional probability of a verb e given its object o (i.e., the noun figuring in the
metonymic expression), which we estimate as follows:
P?(e | o) = f (e, o)
f (o)
(30)
281
Lapata and Lascarides Logical Metonymy
The model in (30) treats metonymic verbs as semantically empty and relies on their
object NPs to provide additional semantic information. The counts f (e, o) and f (o) can
be easily obtained from the BNC: f (o) amounts to the number of times a noun is
attested as an object, and f (e, o) are verb-object tuples extracted from the BNC using
Cass (Abney 1996) as described earlier.
2.4.2 Results. We used the naive model to calculate the likelihood of the meaning
paraphrases that were presented to the subjects (see Experiment 1). Through correla-
tion analysis we explored the linear relationship between the elicited judgments and
the naive baseline model. We further directly compared the two models: that is, our
initial, linguistically more informed model and the naive baseline.
Comparison between the probabilities generated by the naive model and the
elicited judgments yielded a Pearson correlation coefficient of .42 (p < .01, N = 174).
(Recall that our initial model yielded a correlation coefficient of .64.) We conducted a
one-tailed t-test to determine if the correlation coefficients were significantly different.
The comparison revealed that the difference between them was statistically significant
(t(171) = 1.67, p < .05), indicating that our model performs reliably better than the
naive baseline. Comparison between the two models (our initial model introduced in
Section 2.1 and the naive baseline model) yielded an intercorrelation of .46 (p < .01,
N = 174). These differences between the ?full? probabilistic model and the naive base-
line model confirm claims made in the literature: Different metonymic verbs have a
different semantic impact on the resolution of metonymy.
2.5 Experiment 3: Comparison against Norming Data
Our previous experiments focused on evaluating the plausibility of meaning para-
phrases generated by a model that does not take into account the contribution of the
sentential subject. However, properties of the subject NP appear to influence the inter-
pretation of the metonymic expression in otherwise neutral contexts, as is illustrated
in (31), in which the interpretation of enjoy the book is influenced by the sentential
subject: Authors usually write books, whereas critics usually review them.
(31) a. The critic enjoyed the book.
b. The author enjoyed the book.
In this section we present an extension of the basic model outlined in Section 2.1
that takes sentential subjects into account. We evaluate the derived paraphrases and
their likelihood again by comparison with human data. This time we compare our
model against paraphrase data generated independently by subjects that participated
in an experimental study (McElree et al 2001) that was not designed specifically to
test our model.
2.5.1 The Extended Model. We model the meaning of sentences like (31) again as the
joint distribution of the following variables: the metonymic verb v, its subject s, its
object o, and the implicit interpretation e. By choosing the ordering ?e, v, s, o?, we can
factor P(e, o, s, v) as follows:
P(e, o, s, v) = P(e) ? P(v | e) ? P(s | e, v) ? P(o | e, v, s) (32)
The terms P(e) and P(v | e) are easy to estimate from the BNC. For P(e) all we need
is a POS-tagged corpus and P(v | e) can be estimated from Cass?s output (see equa-
tions (12) and (13)). The estimation of the terms P(s | e, v) and P(o | e, v, s) is, however,
282
Computational Linguistics Volume 29, Number 2
problematic, as they rely on the frequencies f (s, e, v) and f (o, e, v, s), respectively. Re-
call that there is a discrepancy between a metonymic interpretation and its usage. As
we discussed earlier, metonymic interpretation is not overtly expressed in the corpus.
Furthermore, the only type of data available to us for the estimation of P(s | e, v) and
P(o | e, v, s) is the partially parsed BNC, which is not annotated with information re-
garding the interpretation of metonymies. This means that P(s | e, v) and P(o | e, v, s)
need to be approximated somehow. We first assume that the sentential subject s is con-
ditionally independent of the metonymic verb v; second, we assume that the sentential
object o is conditionally independent of v and s:
P(s | e, v) ? P(s | e) (33)
P(o | e, v, s) ? P(o | e) (34)
The rationale behind the approximation in (33) is that the likelihood of a noun s being
a subject of a verb e is largely independent of whether e is the complement of a
metonymic verb v. For example, authors usually write, irrespective of whether they
enjoy, dislike, start, or finish doing it. The motivation for the approximation in (34)
comes from the observation that an object is more closely related to the verb that
selects for it than a subject or a metonymic verb. We are likely to come up with book
or letter for o if we know that o is the object of read or write. Coming up with an object
for o is not so straightforward if all we know is the metonymic verb (e.g., enjoy, finish)
or its sentential subject. It is the verbs, rather than other sentential constituents, that
impose semantic restrictions on their arguments. We estimate P(s | e) and P(o | e) using
maximum likelihood:
P?(s | e) = f (s, e)
f (e)
(35)
P?(o | e) = f (o, e)
f (e)
(36)
The count f (s, e) amounts to the number of times a noun s is attested as the
subject of a verb e; f (o, e) represents the number of times a noun is attested as an
object of e. Verb-argument tuples can be easily extracted from the BNC using Cass (see
Section 2.2 for details). Table 11 illustrates the model?s performance for the metonymic
constructions in (37). The table shows only the five most likely interpretations the
model came up with for each construction. Interestingly, different interpretations are
derived for different subjects. Even though pianists and composers are semantically
related, pianists are more likely to begin playing a symphony, whereas composers
are more likely to conduct or write a symphony. Similarly, builders tend to renovate
houses and architects tend to design them.
(37) a. The composer/pianist began the symphony.
b. The author/student started the book.
c. The builder/architect started the house.
d. The secretary/boss finished the memo.
In the following section we compare the interpretations generated by the model
against paraphrases provided by humans. More specifically, we explore whether there
is a linear relationship between the frequency of an interpretation as determined in
a norming study and the probability of the same interpretation as calculated by the
model.
283
Lapata and Lascarides Logical Metonymy
Table 11
Subject-related model interpretations, ranked in order of likelihood.
begin symphony start book
composer pianist author student
write ?22.2 play ?24.20 write ?14.87 read ?16.12
conduct ?23.79 hear ?25.38 publish ?16.94 write ?16.48
hear ?25.38 give ?28.44 compile ?17.84 study ?17.59
play ?25.81 do ?29.23 read ?17.98 research ?18.86
create ?25.96 have ?30.13 sign ?18.59 translate ?17.85
start house finish memo
builder architect secretary boss
renovate ?15.43 design ?16.87 write ?19.79 draft ?20.73
build ?17.56 build ?17.20 type ?20.13 send ?21.97
demolish ?18.37 restore ?19.08 send ?20.87 sign ?22.04
dismantle ?19.69 purchase ?19.32 sign ?22.01 hand ?22.12
erect ?19.81 site ?19.73 make ?22.74 write ?22.74
2.5.2 Method. For the experiment described in this section, we used the norming data
reported in McElree et al (2001). In McElree et al?s study subjects were given sentence
fragments such as (38) and were asked to complete them. Potential completions for
fragment (38a) include writing or reading. The study consisted of 142 different sentences
similar to those shown in (38) and included 15 metonymic verbs. Thirty sentences were
constructed for each of the metonymic verbs start, begin, complete, and finish, and a total
of 22 sentences for attempt, endure, expect, enjoy, fear, master, prefer, resist, savor, survive,
and try.
(38) a. The writer finished the novel.
b. The soldier attempted the mountain.
c. The teenager finished the novel.
The completions can be used to determine interpretation preferences for the metonymic
constructions simply by counting the verbs that human subjects use to complete sen-
tences like those in (38). For example, five completions were provided by the subjects
for fragment (38b): climb, hike, scale, walk, and take. Of these climb was by far the most
likely, with 78 (out of 88) subjects generating this interpretation.3 The most likely in-
terpretations for (38a) and (38c) were, respectively, write (13 out of 28 subjects) and
read (18 out of 22).
For each of the sentences included in McElree et al?s (2001) study, we derived in-
terpretation paraphrases using the model presented in Section 2.5.1. We next compared
the interpretations common in the model and the human data.
2.5.3 Results. In Experiment 1 we evaluated the paraphrases generated by the model
by eliciting plausibility judgments from subjects and showed that our model produces
an intuitively plausible ranking of meanings. Here, we evaluate the quality of the
3 McElree et al?s (2001) linguistic materials were manually constructed and not controlled for frequency.
For example, one would expect (38b) to be relatively rare, even in a large corpus. This is true for the
BNC, in which the combination attempt mountain is not attested at all.
284
Computational Linguistics Volume 29, Number 2
produced paraphrases by directly comparing them to norming data acquired inde-
pendently of our model and the particular corpus we are using.
The comparison between (log-transformed) model probabilities and (log-trans-
formed) completion frequencies yielded a Pearson correlation coefficient of .422 (p<.01,
N = 341). We also compared the completion frequencies against interpretation proba-
bilities derived using the model presented in Section 2.3, which does not take subject-
related information into account. The comparison yielded a correlation coefficient
of .216 (p < .01, N = 341). We carried out a one-tailed t-test to determine if the differ-
ence between the two correlation coefficients is significant. The comparison revealed
that the difference is statistically significant (t(338) = 2.18, p < .05). This means that
the fit between the norming data and the model is better when the model explicitly
incorporates information about the sentential subject. The two models are, as expected,
intercorrelated (r = .264, p < .01, N = 341).
2.6 Discussion
We have demonstrated that the meanings acquired by our probabilistic model corre-
late reliably with human intuitions. These meanings go beyond the examples found
in the theoretical linguistics literature. The verb-noun combinations we interpret were
randomly sampled from a large, balanced corpus providing a rich inventory for their
meanings. We have shown that the model has four defining features: (1) It is able
to derive intuitive meanings for verb-noun combinations, (2) it generates clusters of
meanings (following Vendler?s (1968) insight), (3) it predicts variation in interpretation
among the different nouns: The same verb may carry different meanings depending
on its subject or object (compare begin book to begin house and the author began the house
to the architect began the house), and (4) it represents variation in interpretation among
the different metonymic verbs (e.g., begin book vs. enjoy book). This latter property
demonstrates that although the model does not explicitly encode linguistic constraints
for resolving metonymies, it generates interpretations that broadly capture linguistic
differences (e.g., attempt imposes different constraints on interpretation from begin or
enjoy). Furthermore, these interpretations for metonymic verb-noun pairs are discov-
ered automatically, without presupposing the existence of a predefined taxonomy or
a knowledge base.
Note that the evaluation procedure to which we subject our model is rather strict.
The derived verb-noun combinations were evaluated by subjects naive to linguistic
theory. Although verbal logical metonymy is a well-researched phenomenon in the
theoretical linguistics literature, the experimental approach advocated here is, to our
knowledge, new. Comparison between our model and human judgments yielded a re-
liable correlation of .64 when the upper bound for the task (i.e., intersubject agreement)
is on average .74. Furthermore, our model performed reliably better than a naive base-
line model, which achieved a correlation of only .42. When compared against norming
data, an extended version of our model that takes subject information into account
reached a correlation of .42. Comparison against norming data is a strict test on unseen
data that was not constructed explicitly to evaluate our model but is independently
motivated and does not take our corpus (i.e., the BNC) or our particular task into
account.
We next investigate whether such an approach generalizes to other instances of
logical metonymy by looking at adjective-noun combinations. Adjectives pose a greater
challenge for our modeling task, as they can potentially allow for a wider range of
interpretations and can exhibit preferences for a verb-subject or verb-object paraphrase
(see Section 1). Following the approach we adopted for verbal metonymy, we define
the interpretation of polysemous adjective-noun combinations as a paraphrase gener-
285
Lapata and Lascarides Logical Metonymy
ation task. We provide a probabilistic model that not only paraphrases adjective-noun
pairs (e.g., fast plane) with a related verb (e.g., fly) but also predicts whether the noun
modified by the adjective (e.g., plane) is likely to be the verbal object or subject. The
model achieves this by combining distributional information about how likely it is
for any verb to be modified by the adjective in the adjective-noun combination or its
corresponding adverb with information about how likely it is for any verb to take the
modified noun as its object or subject. We obtain quantitative information about verb-
adjective modification and verb-argument relations from the BNC and evaluate our
results by comparing the model?s predictions against human judgments. Consistent
with our results on verbal metonymy, we show that the model?s ranking of meanings
correlates reliably with human intuitions.
3. Metonymic Adjectives
3.1 The Model
Consider again the adjective-noun combinations in (39). In order to come up with the
interpretation of plane that flies quickly for fast plane, we would like to find in the corpus
a sentence whose subject is the noun plane or planes and whose main verb is fly. We
would also expect fly to be modified by the adverb fast or quickly. In the general case,
we would like to gather from the corpus sentences indicating what planes do fast.
Similarly, for the adjective-noun combination fast scientist, we would like to find in
the corpus information indicating what the activities that scientists perform fast are,
whereas for easy problem we need information about what one can do with problems
easily (e.g., one can solve problems easily) or about what problems are (e.g., easy to
solve or easy to set).
(39) a. fast plane
b. fast scientist
c. fast programmer
d. easy problem
In sum, in order to come up with a paraphrase of the meaning of an adjective-
noun combination, we need to know which verbs take the head noun as their subject or
object and are modified by an adverb corresponding to the modifying adjective. This
can be expressed as the joint probability P(a , e , n , rel), where e is the verbal predicate
modified by the adverb a (directly derived from the adjective present in the adjective-
noun combination) bearing the argument relation rel (i.e., subject or object) to the head
noun n. By choosing the ordering ?e , n , a , rel? for the variables a , e , n , and rel , we can
rewrite P(a , e , n , rel), using the chain rule, as follows:
P(a , e , n , rel) = P(e) ? P(n | e) ? P(a | e , n) ? P(rel | e , n , a) (40)
Although the terms P(e) and P(n | e) can be straightforwardly estimated from the BNC
(see (12) for P(e); P(n | e) can be obtained by counting the number of times a noun n
co-occurs with a verb e either as its subject or object), the estimation of P(a | e , n) and
P(rel | e , n , a) faces problems similar to those for metonymic verbs. Let us consider
more closely the term P(rel | e , n , a), which can be estimated as shown in (41).
P?(rel | e , n , a) = f (rel , e , n , a)
f (e , n , a)
(41)
286
Computational Linguistics Volume 29, Number 2
One way to obtain f (rel , e , n , a) would be to parse fully the corpus so as to identify
the verbs that take the head noun n as their subject or object and are modified by the
adverb a, assuming it is equally likely to find in a corpus the metonymic expression
(e.g., fast plane) and its paraphrase interpretation (i.e., plane that flies quickly). As in the
case of verb-noun metonymies, this assumption is unjustified: For the adjective-noun
combination fast plane, there are only six sentences in the entire BNC that correspond
to f (rel , e , n , a). According to the sentences in (42), the most likely interpretation for
fast plane is plane that goes fast (see examples (42a)?(42c)). The interpretations plane that
swoops in fast, plane that drops down fast, and plane that flies fast are all equally likely, since
they are attested in the corpus only once (see examples (42d)?(42f)). This is rather
unintuitive, since fast planes are more likely to fly than swoop in fast. Similar problems
affect the frequency f (e , n , a).
(42) a. The plane went so fast it left its sound behind.
b. And the plane?s going slightly faster than the Hercules or
Andover.
c. He is driven by his ambition to build a plane that goes faster
than the speed of sound.
d. Three planes swooped in, fast and low.
e. The plane was dropping down fast towards Bangkok.
f. The unarmed plane flew very fast and very high.
In default of a corpus explicitly annotated with interpretations for metonymic
adjectives, we will make the following independence assumptions:
P(a | e , n) ? P(a | e) (43)
P(rel | e , n , a) ? P(rel | e , n) (44)
The rationale behind the approximation in (43) is that the likelihood of seeing an
adverbial a modifying a verb e bearing an argument relation to a noun n is largely
independent of that specific noun. For example, flying can be carried out fast or slowly
or beautifully irrespective of whether it is a pilot or a bird who is doing the flying.
Similarly, the adverb peacefully is more related to dying than killing or injuring irrespec-
tive of who the agent of these actions is. Accordingly, we assume that the argument
relation rel is independent of whether the verb e (standing in relation rel with noun n)
is modified by an adverb a (see (44)). In other words, it is the verb e and its argument
n that determine the relation rel rather than the adjective or adverb a. Knowing that
flying is conducted slowly will not affect the likelihood of inferring a subject relation
for plane and fly. Yet we are likely to infer an object relation for plane and construct
irrespective of whether the constructing is done slowly, quickly, or automatically. We
estimate the probabilities P(e), P(n | e), P(a | e), and P(rel | e , n) as follows:
P?(e) =
f (e)
N
(45)
P?(n | e) = f (n , e)
f (e)
(46)
P?(a | e) = f (a , e)
f (e)
(47)
287
Lapata and Lascarides Logical Metonymy
Table 12
Most frequent verbs modified by the adverb fast.
f(fast,e) f(fast,e)
go 29 work 6
grow 28 grow in 6
beat 27 learn 5
run 16 happen 5
rise 14 walk 4
travel 13 think 4
move 12 keep up 4
come 11 fly 4
drive 8 fall 4
get 7 disappear 4
Table 13
Most frequent verbs taking as an argument the noun plane.
f(SUBJ,e,plane) f(OBJ,e,plane)
fly 20 catch 24
come 17 board 15
go 15 take 14
take 14 fly 13
land 9 get 12
touch 8 have 11
make 6 buy 10
arrive 6 use 8
leave 5 shoot 8
begin 5 see 7
P?(rel | e , n) = f (rel , e , n)
f (e , n)
(48)
By substituting equations (45)?(48) into (41) and simplifying the relevant terms, (41)
can be rewritten as follows:
P(a , e , n , rel) =
f (rel , e , n) ? f (a , e)
f (e) ? N (49)
Assume we want to discover a meaning paraphrase for the adjective-noun combination
fast plane. We need to find the verb e and the relation rel (i.e., subject or object) that
maximize the term P(fast , e , plane , rel). Table 12 gives a list of the most frequent verbs
modified by the adverb fast in the BNC (see the term f (a , e) in equation (49)), and
Table 13 lists the verbs for which the noun plane is the most likely object or subject
(see the term f (rel , e , n) in equation (49)). In the following section, we describe how
the frequencies f (rel , e , n), f (a , e), and f (e) were estimated from a lemmatized version
of the BNC.
Table 12 can be thought of as a list of the activities that can be fast (i.e., going,
growing, flying), whereas Table 13 specifies the events associated with the noun plane.
Despite our simplifying assumptions, the model given in (49) will come up with plau-
sible meanings for adjective-noun combinations like fast plane. Note that the verbs fly,
come, and go are most likely to take the noun plane as their subject (see Table 13). These
288
Computational Linguistics Volume 29, Number 2
verbs also denote activities that are fast (see Table 12, in which the underlined verbs
are events that are associated both with the adverb fast and the noun plane). Further
note that a subject interpretation is more likely than an object interpretation for fast
plane, since none of the verbs likely to have plane as their object are modified by the
adverb fast (compare Tables 12 and 13).
As in the case of metonymic verbs, the probabilistic model outlined above ac-
quires meanings for polysemous adjective-noun combinations in an unsupervised
manner without presupposing annotated corpora or taxonomic information. The ob-
tained meanings are not discourse-sensitive; they can be thought of as default semantic
information associated with a particular adjective-noun combination. This means that
our model is unable to predict that programmer that runs fast is a likely interpretation
for fast programmer when the latter is in a context like the one given in (5) (repeated
here as (50)).
(50) a. All the office personnel took part in the company sports day last
week.
b. One of the programmers was a good athlete, but the other was
struggling to finish the courses.
c. The fast programmer came first in the 100m.
3.2 Parameter Estimation
As in the case of verbs, the parameters of the model were estimated using a part-of-
speech-tagged and lemmatized version of the BNC. The counts f (e) and N (see (49))
reduce to the number of times a given verb is attested in the corpus. The frequency
f (rel , e , n) was obtained using Abney?s (1996) chunk parser Cass (see Section 2.2 for
details).
Generally speaking, the frequency f (a , e) represents not only a verb modified by
an adverb derived from the adjective in question (see example (51a)), but also con-
structions like the ones shown in (51b) and (51c), in which the adjective takes an
infinitival VP complement whose logical subject can be realized as a for PP (see ex-
ample (51c)). In cases of verb-adverb modification we assume access to morphological
information that specifies what counts as a valid adverb for a given adjective. In most
cases adverbs are formed by adding the suffix -ly to the base of the adjective (e.g.,
slow-ly, easy-ly). Some adjectives have identical adverbs (e.g., fast, right). Others have
idiosyncratic adverbs (e.g., the adverb of good is well). It is relatively straightforward
to develop an automatic process that maps an adjective to its corresponding adverb,
modulo exceptions and idiosyncracies; however, in the experiments described in the
following sections, this mapping was manually specified.
(51) a. comfortable chair ? a chair on which one sits comfortably
b. comfortable chair ? a chair that is comfortable to sit on
c. comfortable chair ? a chair that is comfortable for me to sit on
In cases in which the adverb does not immediately succeed the verb, the parser
is not guaranteed to produce a correct analysis, since it does not resolve structural
ambiguities. So we adopted a conservative strategy, in which to obtain the frequency
f (a , e), we looked only at instances in which the verb and the adverbial phrase mod-
ifying it were adjacent. More specifically, in cases in which the parser identified an
AdvP following a VP, we extracted the verb and the head of the AdvP (see the exam-
ples in (52)). In cases where the AdvP was not explicitly identified, we extracted the
verb and the adverb immediately following or preceding it (see the examples in (53)),
289
Lapata and Lascarides Logical Metonymy
assuming that the verb and the adverb stand in a modification relation. The examples
below illustrate the parser?s output and the information that was extracted for the
frequency f (a , e).
(52) a. [NP Oriental art] [VP came] [AdvP more slowly.]
come slowly
b. [NP The issues] [VP will not be resolved] [AdvP easily.]
resolve easily
c. [NP Arsenal] [VP had been pushed] [AdvP too hard.]
push hard
(53) a. [NP Some art historians] [VP write well] [PP about the present.]
write well
b. [NP The accidents] [VP could have been easily avoided.]
avoid easily
c. [NP A system of molecules] [VP is easily shown] [VP to stay
constant.]
show easily
d. [NP Their economy] [VP was so well run.]
run well
Adjectives with infinitival complements (see (51b) and (51c)) were extracted from
the parser?s output. We concentrated solely on adjectives immediately followed by
infinitival complements with an optionally intervening for PP (see (51c)). The adjective
and the main verb of the infinitival complement were counted as instances of the
quantity f (a , e). The examples in (54) illustrate the process.
(54) a. [NP These early experiments] [VP were easy] [VP to interpret.]
easy interpret
b. [NP It] [VP is easy] [PP for an artist] [VP to show work
independently.]
easy show
c. [NP It] [VP is easy] [VP to show] [VP how the components interact.]
easy show
Finally, the frequency f (a , e) collapsed the counts from cases in which the adjective
was followed by an infinitival complement (see the examples in (54)) and cases in
which the verb was modified by the adverb corresponding to the related adjective
(see the examples in (52)?(53)). For example, assume that we are interested in the
frequency f (easy , show). In this case, we will take into account not only sentences
(54b) and (54c), but also sentence (53b). Assuming this was the only evidence in the
corpus, the frequency f (easy , show) would be three.
Once we have obtained the frequencies f (a , e) and f (rel , e , n), we can determine
what the most likely interpretations for a given adjective-noun combination are. If
we know the interpretation preference of a given adjective (i.e., subject or object), we
may vary only the term e in P(a , n , rel , e), keeping the terms n, a, and rel constant.
Alternatively, we could acquire the interpretation preferences automatically by varying
both the terms rel and e. In Experiment 4 (see Section 3.3) we acquire both meaning
paraphrases and argument preferences for polysemous adjective-noun combinations.
In what follows we illustrate the properties of the model by applying it to a small
number of adjective-noun combinations (displayed in Table 14). The adjective-noun
290
Computational Linguistics Volume 29, Number 2
Table 14
Paraphrases for adjective-noun combinations taken from the literature.
easy problem ? a problem that is easy to solve (Vendler 1968, page 97)
easy text ? text that reads easily (Vendler 1968, page 99)
difficult language ? a language that is difficult to speak, learn, write, understand (Vendler 1968, page 99)
careful scientist ? a scientist who observes, performs, runs experiments carefully (Vendler 1968, page 92)
comfortable chair ? a chair on which one sits comfortably (Vendler 1968, page 98)
good umbrella ? an umbrella that functions well (Pustejovsky 1995, page 43)
Table 15
Object-related interpretations for adjective-noun combinations, ranked in order of likelihood.
easy problem easy text difficult language comfortable chair good umbrella
solve ?15.14 read ?17.42 understand ?17.15 sink into ?18.66 keep ?21.59
deal with ?16.12 handle ?18.79 interpret ?17.59 sit on ?19.13 wave ?21.61
identify ?16.83 use ?18.83 learn ?17.67 lounge in ?19.15 hold ?21.73
tackle ?16.92 interpret ?19.05 use ?17.79 relax in ?19.33 run for ?21.73
handle ?16.97 understand ?19.15 speak ?18.21 nestle in ?20.51 leave ?22.28
Table 16
Subject-related interpretations for adjective-noun combinations,
ranked in order of likelihood.
easy text good umbrella careful scientist
see ?19.22 cover ?23.05 calculate ?22.31
read ?19.50 proceed ?22.67
understand ?19.66 investigate ?22.78
achieve ?19.71 study ?22.90
explain ?20.40 analyze ?22.92
combinations and their respective interpretations are taken from the lexical semantics
literature (i.e., Pustejovsky (1995) and Vendler (1968)). The five most likely model-
derived paraphrases for these combinations are shown in Tables 15 and 16.
The model comes up with plausible meanings, some of which overlap with those
suggested in the lexical semantics literature (underlined interpretations indicate agree-
ment between the model and the literature). Observe that the model predicts different
meanings when the same adjective modifies different nouns and derives a cluster of
meanings for a single adjective-noun combination. An easy problem is not only a prob-
lem that is easy to solve (see Vendler?s (1968) identical interpretation in Table 14) but
also a problem that is easy to deal with, identify, tackle, and handle. The meaning of
easy problem is different from the meaning of easy text, which in turn is easy to read,
handle, interpret, and understand. The interpretations the model arrives at for difficult
language are a superset of the interpretations suggested by Vendler (1968). The model
comes up with the additional meanings language that is difficult to interpret and language
that is difficult to use. Although the meanings acquired by the model for careful scientist
do not overlap with the ones suggested by Vendler (1968), they seem intuitively plau-
sible: a careful scientist is a scientist who calculates, proceeds, investigates, studies, and
analyzes carefully. These are all possible actions associated with scientists.
The model derives subject- and object-related interpretations for good umbrella,
which is an umbrella that covers well and is good to keep, good for waving, good to
hold, good to run for, and good to leave. A subject interpretation can be also derived for
291
Lapata and Lascarides Logical Metonymy
easy text. Our parser treats inchoative and noninchoative uses of the same verb as dis-
tinct surface structures (e.g., text that one reads easily vs. text that reads easily); as a result,
read is generated as a subject and object paraphrase for easy text (compare Tables 15
and 16). The object-related interpretation is nevertheless given a higher probability
than the subject-related one, which seems intuitively correct (there is an understood but
unexpressed agent in structures like text that reads easily). In general, subject and object
interpretations are derived on the basis of verb-subject and verb-object constructions
that have been extracted from the corpus heuristically without taking into account in-
formation about theta roles, syntactic transformations (with the exception of passiviza-
tion), or diathesis alternations such as the middle or causative/inchoative alternation.
Although the model can be used to provide several interpretations for a given
adjective-noun combination, not all of these interpretations are useful or plausible
(see the subject interpretations for easy text). Also, the meanings acquired by our model
are a simplified version of the ones provided in the lexical semantics literature. An
adjective-noun combination may be paraphrased with another adjective-noun combi-
nation (e.g., a good meal is a tasty meal) or with an NP instead of an adverb (e.g., a fast
decision is a decision that takes a short amount of time). We are making the simpli-
fying assumption that a polysemous adjective-noun combination can be paraphrased
by a sentence consisting of a verb whose argument is the noun with which the adjec-
tive is in construction (cf. earlier discussion concerning nonmetonymic uses of verbs
like enjoy).
In the following section we evaluate against human judgments the meaning para-
phrases generated by the model. As in the case of verbs, the model is tested on
examples randomly sampled from the BNC, and the linear relationship between the
subjects? rankings and the model-derived probabilities is explored using correlation
analysis. In Section 3.5.1 we assess whether our model outperforms a naive baseline
in deriving interpretations for metonymic adjectives.
3.3 Experiment 4: Comparison against Human Judgments
The experimental method in Experiment 4 was the same as that in Experiment 1. Mean-
ing paraphrases for adjective-noun combinations were obtained using the model intro-
duced in Section 3.1. The model?s rankings were compared against paraphrase judg-
ments elicited experimentally from human subjects. The comparison between model
probabilities and their perceived likelihood enabled us to explore (1) whether there
is a linear relationship between the likelihood of a given meaning as derived by the
model and its perceived plausibility and (2) whether the model can be used to derive
the argument preferences for a given adjective, that is, whether the adjective is biased
toward a subject or object interpretation or whether it is equibiased.
3.3.1 Method.
3.3.1.1 Materials and Design. We chose nine adjectives according to a set of minimal
criteria and paired each adjective with 10 nouns randomly selected from the BNC. We
chose the adjectives as follows: We first compiled a list of all the polysemous adjectives
mentioned in the lexical semantics literature (Vendler 1968; Pustejovsky 1995). From
these we randomly sampled nine adjectives (difficult, easy, fast, good, hard, right, safe,
slow, and wrong). These adjectives had to be relatively unambiguous syntactically: In
fact, these nine adjectives were unambiguously tagged as ?adjectives? 98.6% of the
time, measured as the number of different part-of-speech tags assigned to the word in
the BNC. The nine selected adjectives ranged in BNC frequency from 57.6 per million
to 1,245 per million.
292
Computational Linguistics Volume 29, Number 2
Adjective-noun pairs were extracted from the parser?s output. Recall that the BNC
was parsed using Abney?s (1996) chunker Cass (see Sections 2.2 and 3.2 for details).
From the syntactic analysis provided by the parser, we extracted a table containing the
adjective and the head of the noun phrase following it. In the case of compound nouns,
we included only sequences of two nouns and considered the rightmost-occurring
noun as the head. From the retrieved adjective-noun pairs, we removed all pairs with
BNC frequency of one, as we wanted to reduce the risk of paraphrase ratings being
influenced by adjective-noun combinations unfamiliar to the subjects. Furthermore,
we excluded pairs with deverbal nouns (i.e., nouns derived from a verb) such as fast
programmer, since an interpretation can be easily arrived at for these pairs by mapping
the deverbal noun to its corresponding verb. A list of deverbal nouns was obtained
from two dictionaries, CELEX (Burnage 1990) and NOMLEX (Macleod et al 1998).
We used the model outlined in Section 3.1 to derive meaning paraphrases for the
90 adjective-noun combinations. We imposed no threshold on the frequencies f (e, a)
and f (rel , e, n). The frequency f (e, a) was obtained by mapping the adjective to its
corresponding adverb: the adjective good was mapped to the adverbs good and well,
the adjective fast was mapped to the adverb fast, easy was mapped to easily, hard was
mapped to hard, right to rightly and right, safe to safely and safe, slow to slowly and slow,
and wrong to wrongly and wrong. The adverbial function of the adjective difficult is
expressed only periphrastically (i.e., in a difficult manner, with difficulty). As a result we
obtained the frequency f (difficult , e) only on the basis of infinitival constructions (see
the examples in (54)). We estimated the probability P(a , n , rel , e) for each adjective-
noun pair by varying both the terms e and rel. We thus derived both subject-related
and object-related paraphrases for each adjective-noun pair.
For each adjective-noun combination, the set of the derived meanings was again
divided into three ?probability bands? (high, medium, and low) of equal size, and one
interpretation was selected from each band. The division into bands ensured that the
experimental stimuli represented the model?s behavior for likely and unlikely para-
phrases. We performed separate divisions for object-related and subject-related para-
phrases, resulting in a total of six interpretations for each adjective-noun combination,
as we wanted to determine whether there were differences in the model?s predictions
with respect to the argument function (i.e., object or subject) and also because we
wanted to compare experimentally derived adjective biases against model-derived bi-
ases. Example stimuli (with object-related interpretations only) are shown in Table 17
for each of the nine adjectives.
Our experimental design consisted of the factors adjective-noun pair (Pair), gram-
matical function (Func) and probability band (Band). The factor Pair included 90
adjective-noun combinations. The factor Func had two levels (subject and object),
whereas the factor Band had three levels (high, medium, and low). This yielded a
total of Pair ? Func ? Band = 90? 2? 3 = 540 stimuli. The number of the stimuli was
too large for subjects to judge in one experimental session. We limited the size of the
design by selecting a total of 270 stimuli according to the following criteria: Our initial
design created two sets of stimuli, 270 subject-related stimuli and 270 object-related
stimuli. For each set of stimuli (i.e., object- and subject-related) we randomly selected
five nouns for each of the nine adjectives, together with their corresponding interpre-
tations in the three probability bands (high, medium, low). This yielded a total of Pair
? Func ? Band = 45 ? 2 ? 3 = 270 stimuli. In this way, stimuli were created for each
adjective in both subject-related and object-related interpretations.
As in Experiment 1, the stimuli were administered to two separate subject groups
in order to limit the size of the experiment. Each group saw 135 stimuli consisting
of interpretations for all adjective-noun pairs. For the first group five adjectives were
293
Lapata and Lascarides Logical Metonymy
Table 17
Randomly selected example stimuli with log-transformed probabilities derived by the
model.
Probability BandAdjective-noun
High Medium Low
difficult customer satisfy ?20.27 help ?22.20 drive ?22.64
easy food cook ?18.94 introduce ?21.95 finish ?23.15
fast pig catch ?23.98 stop ?24.30 use ?25.66
good postcard send ?20.17 draw ?22.71 look at ?23.34
hard number remember ?20.30 use ?21.15 create ?22.69
right school apply to ?19.92 complain to ?21.48 reach ?22.90
safe drug release ?22.24 try ?23.38 start ?25.56
slow child adopt ?19.90 find ?22.50 forget ?22.79
wrong color use ?21.78 look for ?22.78 look at ?24.89
represented by object-related meanings only (difficult, easy, good, hard, slow); these adjec-
tives were presented to the second group with subject-related interpretations only. Cor-
respondingly, for the first group, four adjectives were represented by subject-related
meanings only (safe, right, wrong, fast); the second group saw these adjectives with
object-related interpretations.
Each experimental item consisted of an adjective-noun pair and a sentence para-
phrasing its meaning. Paraphrases were created by the experimenters by converting
the model?s output to a simple phrase, usually a noun modified by a relative clause. A
native speaker of English other than the authors was asked to confirm that the para-
phrases were syntactically well-formed (items found syntactically odd were modified
and retested). Example stimuli are shown in (55). A complete list of the experimental
items is given in Appendix B.
(55) a. high: difficult customer
a customer who is difficult to satisfy
b. medium: difficult customer
a customer who is difficult to help
c. low: difficult customer
a customer who is difficult to drive
(56) a. high: fast horse a horse that runs fast
b. medium: fast horse a horse that works fast
c. low: fast horse a horse that sees quickly
3.3.1.2 Procedure. The method used was magnitude estimation, with the same exper-
imental protocol as in Experiment 1.
3.3.1.3 Instructions, Demographic Questionnaire, and Training Phase. The instructions were
the same as in Experiment 1, with the exception that this time the subjects were asked
to judge how well a sentence paraphrased a particular adjective-noun combination.
The demographic questionnaire and the training phase were the same as in Experi-
ment 1.
3.3.1.4 Experimental Phase. Each subject group saw 135 metonymic sentences and their
paraphrases. A modulus item from the medium probability band was provided (see
294
Computational Linguistics Volume 29, Number 2
-26 -24 -22 -20 -18 -16
model probabilities
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
m
ea
n
 s
u
bje
ct 
rat
ing
s
Figure 2
Correlation of elicited judgments and model-derived probabilities for metonymic
adjective-noun pairs.
Appendix B). The modulus was the same for all subjects and remained on the screen
the entire time the subject was completing the task. Subjects were assigned to groups
at random, and a random stimulus order was generated for each subject.
3.3.1.5 Subjects. Sixty-five native speakers of English participated in the experiment.4
The subjects were recruited over the Internet by postings to relevant newsgroups and
mailing lists. Participation was voluntary and unpaid.
The data of one subject were eliminated after inspection of his response times
showed that he had not completed the experiment in a realistic time frame (average
response time < 1000ms). The data of four subjects were excluded because they were
non-native speakers of English.
This left 60 subjects for analysis. Of these, 54 subjects were right-handed, six left-
handed; 22 subjects were female, 38 male. The age of the subjects ranged from 18 to
54 years; the mean was 27.4 years.
3.4 Results
The data were normalized as in Experiment 1. We tested the hypothesis that para-
phrases with high probabilities are perceived as better paraphrases than paraphrases
assigned low probabilities by examining the degree to which the elicited judgments
correlate with the probabilities derived by the model. As in Experiment 1, the data
used for the judgments were the average of log-transformed and normalized subject
ratings per experimental item. The comparison between model probabilities and the
human judgments yielded a Pearson correlation coefficient of .40 (p < .01, N = 270).
Figure 2 plots the relationship between judgments and model probabilities. Descrip-
tive statistics for model probabilities and subject judgments are given in Appendix C.
In order to evaluate whether grammatical function has any effect on the relation-
ship between model-derived meaning paraphrases and human judgments, we split
the items into those that received a subject interpretation and those that received an
4 None of the participants of Experiment 1 took part in Experiment 4.
295
Lapata and Lascarides Logical Metonymy
object interpretation. A comparison between our model and human judgments yielded
a correlation of r = .53 (p < .01, N = 135) for object-related items and a correlation of
r = .21 (p < .05, N = 135) for subject-related items. Note that a weaker correlation is
obtained for subject-related interpretations. One explanation for this weaker correla-
tion could be the parser?s performance; that is, the parser may be better at extracting
verb-object tuples than verb-subject tuples. Another hypothesis (which we test below)
is that most adjectives included in the experimental stimuli have an object bias, and
therefore subject-related interpretations are generally less preferred than object-related
ones.
Using leave-one-out resampling (see Section 2.3.2 for details), we calculated how
well subjects agreed in their judgments concerning metonymic adjective-noun combi-
nations. For the first group, the average intersubject agreement was .67 (Min = .03,
Max = .82, StdDev = .14), and for the second group it was .65 (Min = .05, Max = .82,
StdDev = .14).
The elicited judgments can be further used to derive the grammatical function
preferences (i.e., subject or object) for a given adjective. In particular, we can deter-
mine the preferred interpretation for individual adjectives on the basis of the human
data and then compare these preferences against the ones produced by our model.
Argument preferences can be easily derived from the model?s output by compar-
ing subject-related and object-related paraphrases. For each adjective we gathered
the subject- and object-related interpretations derived by the model and performed
a one-way analysis of variance (ANOVA) in order to determine the significance of the
grammatical function effect.
We interpret a significant effect as bias toward a particular grammatical function.
We classify a particular adjective as object-biased if the mean of the model-derived
probabilities for the object interpretation of that adjective is significantly larger than the
mean for the subject interpretation; subject-biased adjectives are classified through a
comparable procedure, whereas adjectives for which no effect of grammatical function
is found are classified as equibiased. The effect of grammatical function was significant
for the adjectives difficult (F(1, 1806) = 8.06, p < .01), easy (F(1, 1511) = 41.16, p < .01),
hard (F(1, 1310) = 57.67, p < .01), safe (F(1, 382) = 5.42, p < .05), right (F(1, 2114) = 9.85,
p < .01), and fast (F(1, 92) = 4.38, p < .05). The effect of grammatical function was
not significant for the adjectives good (F(1, 741) = 3.95, p = .10), slow (F(1, 759) =
5.30, p = .13), and wrong (F(1, 593) = 1.66, p = .19). Table 18 shows the biases for
the nine adjectives as derived by our model. A check mark next to a grammatical
function indicates that its effect was significant in that particular instance, as well as
the direction of the bias.
Ideally, we would like to elicit argument preferences from human subjects in a
similar fashion. However, since it is impractical to elicit judgments experimentally for
all paraphrases derived by the model, we will obtain argument preferences from the
judgments based on the restricted set of experimental stimuli, under the assumption
that they correspond to a wide range of model paraphrases (i.e., they correspond
to a wide range of probabilities) and therefore are representative of the entire set of
model-derived paraphrases. This assumption is justified by the fact that items were
randomly chosen from the three probability bands (i.e., high, medium, low). For each
adjective we gathered the elicited responses pertaining to subject- and object-related
interpretations and performed an ANOVA.
The ANOVA indicated that the grammatical function effect was significant for the
adjective difficult in both by-subjects (subscript 1) and by-items (subscript 2) analyses
(F1(1, 58) = 17.98, p < .01; F2(1, 4) = 53.72, p < .01), and for the adjective easy in the
by-subjects analysis only (F1(1, 58) = 10, p < .01; F2(1, 4) = 8.48, p = .44). No effect of
296
Computational Linguistics Volume 29, Number 2
Table 18
Log-transformed model-derived and subject-based argument preferences for polysemous
adjectives.
Adjective Model Mean StdDev StdEr Subjects Mean StdDev StdEr
difficult
?
OBJ ?21.62 1.36 .04 ? OBJ .0745 .3753 .0685
SUBJ ?21.80 1.34 .05 SUBJ ?.2870 .2777 .0507
easy
?
OBJ ?21.60 1.51 .05 ? OBJ .1033 .3364 .0614
SUBJ ?22.11 1.36 .06 SUBJ ?.1437 .2308 .0421
fast OBJ ?24.20 1.27 .13 OBJ ?.3544 .2914 .0532?
SUBJ ?23.80 1.40 .14 ? SUBJ ?.1543 .4459 .0814
good OBJ ?22.12 1.28 .06 OBJ ?.0136 .3898 .0712
SUBJ ?22.27 1.10 .07 SUBJ ?.1563 .2965 .0541
hard
?
OBJ ?21.69 1.53 .06 ? OBJ .0030 .3381 .0617
SUBJ ?22.12 1.35 .06 SUBJ ?.2543 .2436 .0445
right
?
OBJ ?21.65 1.36 .04 ? OBJ ?.0054 .2462 .0450
SUBJ ?21.84 1.24 .04 SUBJ ?.2413 .4424 .0808
safe OBJ ?22.75 1.48 .10 ? OBJ .0037 .2524 .0461?
SUBJ ?22.39 1.59 .12 SUBJ ?.3399 .4269 .0779
slow OBJ ?22.49 1.53 .08 OBJ ?.3030 .4797 .0876
SUBJ ?22.32 1.50 .07 ? SUBJ ?.0946 .2357 .0430
wrong OBJ ?23.15 1.33 .08 ? OBJ ?.0358 .2477 .0452
SUBJ ?23.29 1.30 .08 SUBJ ?.2356 .3721 .0679
grammatical function was found for good (F1(1, 58) = 2.55, p = .12; F2(1, 4) = 1.01,
p = .37). The effect of grammatical function was significant for the adjective hard
in the by-subjects analysis only (F1(1, 58) = 11.436, p < .01; F2(1, 4) = 2.84, p = .17),
whereas for the adjective slow the effect was significant both by subjects and by items
(F1(1, 58) = 4.56, p < .05; F2(1, 4) = 6.94, p = .058). For safe and right the main effect
was significant in both by-subjects and by-items analyses (F1(1, 58) = 14.4, p < .0005;
F2(1, 4) = 17.76, p < .05, and F1(1, 58) = 6.51, p < .05; F2(1, 4) = 15.22, p = .018, respec-
tively). The effect of grammatical function was significant for wrong and fast only by
subjects (F1(1, 58) = 5.99, p = .05; F2(1, 4) = 4.54, p = .10, and F1(1, 58) = 4.23, p = .05;
F2(1, 4) = 4.43, p = .10). The biases for these adjectives are shown in Table 18. Check
marks again indicate instances in which the grammatical function effect was significant
(as determined from the by-subjects analyses), as well as the direction of the bias.
We expect a valid model to assign on average higher probabilities to object-related
interpretations and lower probabilities to subject-related interpretations for an object-
biased adjective; accordingly, we expect the model to assign on average higher prob-
abilities to subject-related interpretations for subject-biased adjectives. Comparison of
the biases derived from the model with ones derived from the elicited judgments
shows that the model and the humans are in agreement for all adjectives but slow,
wrong, and safe. On the basis of human judgments, slow has a subject bias and wrong
has an object bias (see Table 18). Although the model could not reproduce this result,
there was a tendency in the right direction.
Note that in our correlation analysis reported above, the elicited judgments were
compared against model-derived paraphrases without taking argument preferences
297
Lapata and Lascarides Logical Metonymy
into account. We would expect a valid model to produce intuitive meanings at least
for the interpretation that a given adjective favors. We further examined the model?s
behavior by performing separate correlation analyses for preferred and dispreferred
biases as determined previously by the ANOVAs conducted for each adjective (see
Table 18). Since the adjective good was equibiased, we included both biases (i.e., object-
related and subject-related) in both correlation analyses for that adjective. The com-
parison between our model and the human judgments yielded a Pearson correlation
coefficient of .52 (p < .01, N = 150) for the preferred interpretations and a correlation
of .23 (p < .01, N = 150) for the dispreferred interpretations. The result indicates that
our model is particularly good at deriving meanings corresponding to the argument
bias for a given adjective. However, the dispreferred interpretations also correlate sig-
nificantly with human judgments, which suggests that the model derives plausible
interpretations even in cases in which the argument bias is overridden.
In sum, the correlation analysis supports the claim that adjective-noun paraphrases
with high probability are judged more plausible than those with low probability. It also
suggests that the meaning preference ordering produced by the model is intuitively
correct, since subjects? perception of likely and unlikely meanings correlates with the
probabilities assigned by the model.
The probabilistic model evaluated here explicitly takes adjective/adverb and verb
co-occurrences into account. However, one could derive meanings for polysemous
adjective-noun combinations by concentrating solely on verb-noun relations, ignoring
thus the adjective/adverb and verb dependencies. For example, in order to interpret
the combination easy problem, we could simply take into account the types of activities
that are related to problems (i.e., solving them, giving them, etc.). This simplification
is consistent with Pustejovsky?s (1995) claim that polysemous adjectives like easy are
predicates, modifying some aspect of the head noun and more specifically the events
associated with the noun. A naive baseline model would be one that simply takes into
account the number of times the noun in the adjective-noun pair acts as the subject
or object of a given verb, ignoring the adjective completely. This raises the question
of how well such a naive model would perform at deriving meaning paraphrases for
polysemous adjective-noun combinations.
In the following section we present such a naive model of adjective-noun pol-
ysemy. We compare the model?s predictions against the elicited judgments. Using
correlation analysis we attempt to determine whether the naive model can provide an
intuitively plausible ranking of meanings (i.e., whether perceived likely and unlikely
meanings are given high and low probabilities, respectively). We further compare the
naive model to our initial model (see Section 3.1) and discuss the differences between
them.
3.5 Experiment 5: Comparison against Naive Baseline
3.5.1 Naive Baseline Model. Given an adjective-noun combination, we are interested
in finding the events most closely associated with the noun modified by the adjec-
tive. In other words we are interested in the verbs whose object or subject is the
noun appearing in the adjective-noun combination. This can be simply expressed as
P(e | rel , n), the conditional probability of a verb e given an argument-noun relation
rel , n:
P(e | rel , n) = f (e, rel , n)
f (rel , n)
(57)
The model in (57) assumes that the meaning of an adjective-noun combination
is independent of the adjective in question. Consider, for example, the adjective-noun
298
Computational Linguistics Volume 29, Number 2
pair fast plane. We need to find the verbs e and the argument relation rel that maximize
the probability P(e | rel , plane). In the case of fast plane, the verb that is most frequently
associated with planes is fly (see Table 13). Note that this model will come up with
the same probabilities for fast plane and wrong plane, since it does not take the identity
of the modifying adjective into account. We estimated the frequencies f (e, rel , n) and
f (rel , n) from verb-object and verb-subject tuples extracted from the BNC using Cass
(Abney 1996) (see Section 2.2 for details on the extraction and filtering of the argument
tuples).
3.6 Results
Using the naive model we calculated the meaning probability for each of the 270
stimuli included in Experiment 4 and explored the linear relationship between the
elicited judgments and the naive baseline model through correlation analysis. The
comparison yielded a Pearson correlation coefficient of .25 (p < .01, N = 270). Recall
that we obtained a correlation of .40 (p < .01, N = 270) when comparing our original
model to the human judgments. Not surprisingly the two models are intercorrelated
(r = .38, p < .01, N = 270). An important question is whether the difference between
the two correlation coefficients (r = .40 and r = .25) is due to chance. A one-tailed t-
test revealed that the difference between them was significant (t(267) = 2.42, p < .01).
This means that our original model (see Section 3.1) performs reliably better than the
naive baseline at deriving interpretations for metonymic adjective-noun combinations.
We further compared the naive baseline model and the human judgments sepa-
rately for subject-related and object-related items. The comparison yielded a correlation
of r = .29 (p < .01, N = 135) for object interpretations. Recall that our original model
yielded a correlation coefficient of .53 (see Section 3.4). A one-tailed t-test revealed that
the two correlation coefficients were significantly different (t(132) = 3.03, p < .01). No
correlation was found for the naive model when compared against elicited subject
interpretations (r = .09, p = .28, N = 135).
3.7 Discussion
We have demonstrated that the meanings acquired by our probabilistic model corre-
late reliably with human intuitions. Our model not only acquires clusters of meanings
(following Vendler?s (1968) insight) but furthermore can be used to obtain a tripartite
distinction of adjectives depending on the type of paraphrase they prefer: subject-
biased adjectives tend to modify nouns that act as subjects of the paraphrasing verb,
and object-biased adjectives tend to modify nouns that act as objects of the paraphras-
ing verb, whereas equibiased adjectives display no preference for either argument role.
A comparison between the argument preferences produced by the model and human
intuitions revealed that most of the adjectives we examined (six out of nine) display a
preference for an object interpretation (see Table 18), two adjectives are subject-biased
(i.e., fast, slow) and one adjective is equibiased (i.e., good).
We rigorously evaluated5 the results of our model by eliciting paraphrase judg-
ments from subjects naive to linguistic theory. Comparison between our model and
human judgments yielded a reliable correlation of .40 when the upper bound for the
task (i.e., intersubject agreement) is approximately .65. We have demonstrated that a
naive baseline model that interprets adjective-noun combinations by focusing solely
on the events associated with the noun is outperformed by a more detailed model that
5 We have not compared the model?s predictions against norming data for adjectival metonymies
primarily because such data were not available to us. To our knowledge McElree et al?s (2001) study is
the only example of a norming study on logical metonymy; however, it concentrates only on verbs.
299
Lapata and Lascarides Logical Metonymy
considers not only verb-argument relations but also adjective-verb and adverb-verb
dependencies.
Although the events associated with the different nouns are crucially important for
the meaning of polysemous adjective-noun combinations, it seems that more detailed
linguistic knowledge is needed in order to produce intuitively plausible interpreta-
tions. This is by no means surprising. As a simple example, consider the adjective-
noun pair fast horse. A variety of events are associated with the noun horse, yet only
a subset of those are likely to occur fast. The three most likely interpretations for fast
horse according to the naive model are a horse that needs something fast, a horse that gets
something fast, and a horse that does something fast. A model that uses information about
verb-adjective or verb-adverb dependencies provides a more plausible ranking: a fast
horse is a horse that runs, learns, or goes fast. A similar situation arises when one con-
siders the pair careful scientist. According to the naive model, a careful scientist is more
likely to believe, say, or make something carefully. However, none of these events is
particularly associated with the adjective careful.
Although our experiments on adjectival logical metonymy revealed a reliable cor-
relation between the model?s ranking and human intuitions, the fit between model
probabilities and elicited judgments was lower for metonymic adjectives (r = .40)
than for metonymic verbs (r = .64). One explanation for this lower degree of fit for
adjectives is that the semantic restrictions that adjectives impose on the nouns with
which they combine appear to be less strict than the ones imposed by verbs (con-
sider, for example, the adjective good, which can combine with nearly any noun). A
consequence of this is that metonymic adjectives seem to allow a wider range of in-
terpretations than verbs. This means that there will be a larger variation in subjects?
responses to the generated model paraphrases when it comes to adjectives than when
it comes to verbs, thus affecting the linear relationship between our model and the
elicited judgments. Our hypothesis is further supported by the intersubject agreement,
which is lower for metonymic adjectives than for verbs (.65 versus .74). As explained
in the previous sections, our model does not take into account the wider context
within which an adjective-noun combination is found. Precisely because of the ease
with which some adjectives combine with practically any noun, it may be the case
that more information (i.e., context) is needed in order to obtain intuitively plausible
interpretations for metonymic adjectives. The model presented here can be extended
to incorporate contextual information (e.g., intra- and extrasentential information), but
we leave this to future work.
4. General Discussion
In this article we have focused on the automatic interpretation of logical metonymy.
We have shown how meaning paraphrases for metonymic expressions can be acquired
from a large corpus and have provided a probabilistic model that derives a preference
ordering on the set of possible meanings in an unsupervised manner, without relying
on the availability of a disambiguated corpus. The proposed approach utilizes surface
syntactic analysis and distributional information that can be gleaned from a corpus
while exploiting correspondences between surface cues and meaning.
Our probabilistic model reflects linguistic observations about the nature of metony-
mic constructions: It predicts variation in interpretation for different verbs and adjec-
tives with respect to the noun for which they select and is faithful to Vendler?s (1968)
claim that metonymic expressions are usually interpreted by a cluster of meanings
instead of a single meaning. This contrasts with Pustejovsky?s (1995) approach, which
typically assigns a single reading to the metonymic construction, and with the account
300
Computational Linguistics Volume 29, Number 2
put forward by Copestake and Briscoe (1995), which assigns one interpretation con-
ventionally, albeit a default interpretation?in fact this interpretation might also be of
quite a general type (e.g., the event argument to enjoy in enjoy the pebble can be assigned
the general type act-on). Our model provides plausible alternatives to the default, and
it augments the general semantic type in the interpretation that?s assigned by these the-
ories with a plausible range of more specific values, although, in contrast to the hybrid
model of interpretation described in Copestake and Lascarides (1997), it does not pre-
dict the contexts in which a statistically dispreferred interpretation is the correct one.
Our approach can be viewed as complementary to linguistic theory: Although our
model does not identify odd metonymies in the way that a rule-based model might
(we return to this in Section 4.1), it does automatically derive a ranking of meanings,
thus distinguishing likely from unlikely interpretations. Even if linguistic theory is
able to enumerate all possible interpretations for a given adjective (note that in the
case of polysemous adjectives, we would have to take into account all nouns or noun
classes that the adjective could possibly modify), in most cases it does not indicate
which ones are likely and which ones are not. Our model fares well on both tasks.
It recasts the problem of logical metonymy in a probabilistic framework and derives
a large number of interpretations not readily available from linguistic introspection.
The information acquired from the corpus can be also used to quantify the argument
preferences of metonymic adjectives. These are only implicit in the lexical semantics
literature, in which certain adjectives are exclusively given a verb-subject or verb-object
interpretation.
4.1 Limitations and Extensions
We chose to model metonymic constructions and their meanings as joint distribu-
tions of interdependent linguistic events (e.g., verb-argument relations, verb-adverb
modification). Although linguistically informed, our approach relies on approxima-
tions and simplifying assumptions, partly motivated by the absence of corpora explic-
itly annotated with metonymic interpretations. We generate meaning paraphrases for
metonymic constructions solely from co-occurrence data without taking advantage of
taxonomic information. Despite the simplicity of this approach and its portability to
languages for which lexical semantic resources may not be available, there are certain
regularities about the derived interpretations that our model fails to detect.
Consider the interpretations produced for begin song, repeated here from Table 7:
sing, rehearse, write, hum, and play. Our model fails to capture the close correspondence
for some of these meanings. For example, hum and sing are sound emission verbs; they
additionally entail the performance or execution of the song. The verbs rehearse and
play capture only the performance aspect and can be thus considered supertypes of
hum and sing (one can play or rehearse a song by humming it, singing it, drumming
it, whistling it, etc.). The verb write, on the other hand, is neither a performance nor a
sound emission verb; it has to do with communication and creation. Another example
is comfortable chair, for which the model generates sink into, sit on, lounge in, relax in, and
nestle in (see Table 15). The verbs sink into, sit on, lounge in, and nestle in describe the
position one assumes when sitting in the chair, whereas relax in refers to the state of
the person in the chair. There is no notion of semantic proximity built into the model,
and correspondences among semantically related interpretations are not automatically
recognized.
An alternative to the knowledge-free approach advocated here is to use taxonomic
information to obtain some degree of generalization over the acquired interpretations.
Semantic classifications such as WordNet (Miller et al, 1990) or that in Levin (1993)
can be used to group the obtained verbs into semantically coherent classes. Further-
301
Lapata and Lascarides Logical Metonymy
more, the WordNet semantic hierarchy can be used to estimate directly probabilities
involving either nouns (e.g., P(rel | e, n), P(o | e)) or verbs (e.g., P(v | e), P(a | e)). Prob-
abilities can be defined in terms of senses from a semantic hierarchy by exploiting
the fact that the senses can be grouped into classes consisting of semantically similar
senses (Resnik 1993; Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). So the
probability P(book | read) can be estimated by taking into account nouns that belong
to the same semantic class as book and can be read (e.g., journals, novels, scripts) or
by focusing on verbs that are semantically related to read (e.g., interpret, communi-
cate, understand). Note that estimation of probabilities over classes rather than words
can effectively overcome data sparseness and potentially lead to better probability
estimates. Currently our models cannot estimate probabilities for word combinations
unseen in the corpus, and WordNet could be used for re-creating the frequencies of
these combinations (Lapata, Keller, and McDonald 2001; Clark and Weir 2001). How-
ever, part of our aim here was to investigate whether it is at all possible to generate
interpretations for metonymic constructions without the use of prior knowledge bases
that might bias the acquisition process in uncontrolled and idiosyncratic ways.
A related issue is the fact that our models are ignorant about the potentially differ-
ent senses of the noun in the metonymic construction. For example, the combination
fast plane may be a fast aircraft, or a fast tool, or a fast geometrical plane. Our model
derives meanings related to all three senses of the noun plane. For example, a fast plane
is not only a plane (i.e., an aircraft) that flies, lands, or travels quickly, but also a plane
(i.e., a surface) that transposes or rotates quickly and a plane (i.e., a tool) that smoothes
something quickly. However, more paraphrases are derived for the ?aircraft? sense of
plane; these paraphrases also receive a higher ranking. This is not surprising, since
the number of verbs related to the ?aircraft? sense of plane are more frequent than the
verbs related to the other two senses. In contrast to fast plane, however, efficient plane
should probably bias toward the ?tool? sense of plane, even though the ?aircraft?
sense is more frequent in the corpus. One could make the model sensitive to this by
investigating the synonyms for the various senses of plane; moreover the ?tool? sense
bias of efficient plane could also be inferred on the basis that efficient plane co-occurs
with different verbs from fast plane. There are also cases in which a model-derived
paraphrase does not provide disambiguation clues with respect to the meaning of the
noun. Consider the adjective-noun combination fast game. The model comes up with
the paraphrases game that runs fast and game that goes fast. Both paraphrases may well
refer to either the ?contest,? ?activity,? or ?prey? sense of game. Note finally that our
model can be made sensitive to word sense distinctions by taking into account noun
classes rather than word forms; this modification would allow us to apply the model
to word sense?disambiguated metonymic expressions.
In this article we have focused on the automatic interpretation of logical metonymy
without explicitly dealing with the recognition of verbs or adjectives undergoing logical
metonymy. In default of the latter study, which we plan for the future, we sketch here
briefly how our proposal can be extended to recognizing logical metonymies. A very
simple approach would be to use the proposed model to generate interpretations for
metonymic and nonmetonymic constructions. The derived paraphrases and the range
of their probabilities could be then used to quantify the degree of ?metonymic-ness?
of a given verb or adjective. One would expect that a larger number of paraphrases
would be generated for verbs or adjectives for which logical metonymy is possible. We
tested this hypothesis using a metonymic verb (i.e., enjoy) and a nonmetonymic one
(i.e., play). Enjoy is attested 5,344 times in the BNC in a verb-object relation, whereas
play is attested 12,597 times (again these numbers are based on information extracted
using Cass (Abney 1996)). Using the model presented in Section 2.1 we generated
302
Computational Linguistics Volume 29, Number 2
Table 19
Model-derived paraphrases for odd metonymies, ranked in order of likelihood.
begin dictionary begin rock begin keyboard begin highway
compile ?19.32 crunch across ?18.09 use ?20.11 obstruct ?20.40
flick through ?19.59 climb ?18.39 play ?20.44 regain ?20.79
use ?19.80 run towards ?18.70 operate ?20.56 build ?20.80
publish ?20.34 percolate through ?18.78 assemble ?20.78 use ?20.81
advance ?20.39 dissolve ?19.37 tune ?20.80 detach ?20.82
meaning paraphrases for all verb-object tuples found for enjoy and play. The model
generated 44,701 paraphrases for enjoy and 9,741 for play. Comparison between the
probabilities assigned to the interpretations for enjoy and play revealed that the para-
phrases obtained for enjoy were on average more likely (mean = ?23.53, min = ?27.02,
max = ?15.32) than those discovered for play (mean = ?24.67, min = ?28.25, max
= ?17.24). The difference was statistically significant (using an independent-samples
t-test; t(54, 440) = 2.505, p < .01). This result indicates that enjoy is more likely to un-
dergo logical metonymy than play. Another potential indicator of metonymic use is
the likelihood that a given verb or adjective will be found in a certain syntactic con-
struction. Consider the adjective blue, for which our model (see Section 3.1) does not
generate any meaning paraphrases, presumably because blue is not attested as a verb
modifier (in contrast to adjectives like easy or fast).
Such an approach could potentially predict differences in productivity among
metonymic verbs. One would expect the metonymic uses of attempt, for example, to
be much less productive than the metonymic uses of enjoy and begin. One could con-
ceivably predict this on the basis of the frequency and diversity of NPs in attempt NP
constructions that are attested in the corpus, compared with those for enjoy NP and
begin NP. Furthermore, the model generates paraphrases for attempt that are on aver-
age less likely in comparison to those generated for enjoy or begin. However, we leave
this for future work.
As argued in Section 2.1, our model cannot distinguish between well-formed and
odd metonymic constructions; in fact, it will generally provide meaning paraphrases
even for combinations that are deemed by native speakers to be odd. Consider the
examples in (58) and their interpretations in Table 19. In general the paraphrases gen-
erated for problematic data are of worse quality than those produced for well-formed
metonymies. In most cases the model will generate unavailable interpretations (see be-
gin highway, begin keyboard). Consider, however, the pair begin rock. Pustejovsky (1995)
observes that although there is no generally available interpretation for a sentence like
Mary began the rock, because of what we understand begin to require of its argument
and our knowledge of what rocks are and what you can do to them, as speakers
and hearers we tend to accommodate information into the context so as to interpret
otherwise ill-formed expressions. Our model generates meaning paraphrases that are
relatively plausible assuming different pragmatic contexts for begin rock. One can begin
climbing or running towards a rock. Someone?s footsteps can crunch across a frozen rock,
a material can percolate through a rock, and rain water can dissolve a rock.
(58) a. ?John began the dictionary.
b. ?Mary began the rock.
c. *John began a keyboard.
d. *John began the highway.
303
Lapata and Lascarides Logical Metonymy
Table 20
Descriptives for odd and well-formed metonymies.
Well-formed N Mean StdDev StdErr
begin book 534 ?21.54 1.515 .066
begin cigarette 104 ?21.83 1.613 .158
begin coffee 104 ?22.03 1.626 .159
begin story 381 ?21.73 1.493 .076
easy problem 358 ?21.14 1.606 .085
Odd N Mean StdDev StdErr
begin dictionary 76 ?22.48 1.440 .165
begin keyboard 50 ?22.40 1.337 .189
begin rock 50 ?23.55 1.376 .193
begin highway 37 ?22.52 1.337 .219
easy programmer 49 ?23.23 1.289 .184
Despite the fact that the model does not recognize odd metonymies, one would
expect low probabilities to be assigned to ungrammatical constructions. Table 20 re-
ports some descriptive statistics on well-formed (top half) and odd (bottom half)
metonymies taken from the lexical semantics literature (Verspoor 1997; Pustejovsky
1995). A higher number of interpretations is generated for well-formed metonymies.
Using an independent-samples t-test, we can compare the differences in the probabili-
ties assigned to the two types of metonymies. Take, for example, begin dictionary: the av-
erage probability of its interpretations is lower than those for begin book (t(608) = 5.07,
p < .01), begin cigarette (t(178) = 2.77, p < .01), begin coffee (t(178) = 2.1, p < .05), and
begin story (t(455) = 4.1, p < .01). Similar results are obtained when comparing be-
gin keyboard against the well-formed metonymies in Table 19: the probability of its
interpretations is on average lower than those assigned to begin book, (t(582) = 3.87,
p < .01), begin cigarette (t(152) = 2.15, p < .01), and begin story (t(429) = 3.02, p < .01).
The difference between begin coffee and begin keyboard is not statistically significant
(t(152) = 1.39, p = 0.167). However, the mean for begin coffee is slightly higher than
that for begin keyboard. Although here we focus on verbs, similar comparisons can be
applied to adjectives. As shown in Table 20, the probabilities for easy programmer are
on average lower than those for easy problem (t(405) = 8.74, p < .01).
Finally, recall from Section 2.1 that on the basis of Gricean reasoning, one would
expect to find in a corpus well-formed metonymies more often than their paraphrases
(see Tables 1?3). Following this line of reasoning, one might expect for conventionally
odd metonymies the opposite situation, that is, to find the paraphrases more often
than the metonymies proper. We tested this hypothesis for some examples cited in
the literature (Verspoor 1997; Pustejovsky 1995) by examining whether paraphrases
corresponding to odd metonymies are attested in the BNC as VP complements. We
found plausible paraphrases in the BNC for almost all verb-noun pairs illustrated
in Table 21. This suggests that the corpus data relating to the odd and well-formed
examples are largely compliant with the Gricean predictions. Corpus co-occurrences
of verb-noun combinations and their paraphrases could be exploited in creating a
system aimed at quantifying the grammaticality of metonymic expressions. However,
it is beyond the scope of the present study to develop such a system.
304
Computational Linguistics Volume 29, Number 2
Table 21
BNC frequencies for odd metonymic expressions.
Odd begin NP begin V-ing NP
begin chair 0 9
begin tunnel 0 4
begin keyboard 0 0
begin tree 1 13
begin highway 0 2
begin film 0 7
begin nail 0 4
begin door 0 18
begin dictionary 0 3
begin rock 0 17
Table 22
Five most likely interpretations for good author and good language.
good author good language
write SUBJ ?21.81 use OBJ ?17.39
work SUBJ ?21.97 verse in OBJ ?17.89
describe SUBJ ?22.03 speak OBJ ?18.32
know OBJ ?22.05 know OBJ ?19.18
engage with OBJ ?22.23 learn OBJ ?19.36
4.2 Relevance for NLP Applications
The meaning paraphrases discovered by our model could be potentially useful for
a variety of NLP tasks. One obvious application is natural language generation. For
example, a generator that has knowledge of the fact that fast plane corresponds to a
plane that flies fast can exploit this information either to render the text shorter (in cases
in which the input representation is a sentence) or longer (in cases in which the in-
put representation is an adjective-noun pair). Information retrieval is another relevant
application. Consider a search engine faced with the query fast plane. Presumably one
would not like to obtain information about planes in general or about planes that go
down or burn fast, but rather about planes that fly or travel fast. So knowledge about
the most likely interpretations of fast plane could help rank relevant documents before
nonrelevant ones or restrict the number of documents retrieved.
Note that in the case of adjectives, it is not just the paraphrase, but also the
grammatical function, that needs to be determined. How to render an adjective-noun
combination with an object- or subject-related paraphrase can be worked out by com-
puting the biases discussed in Section 3.4. So if we know that fast has a subject bias,
we can concentrate only on the subject-related interpretations. The choice of grammat-
ical function is less straightforward in the case of equibiased adjectives. In fact, it is
possible that the interpretation for a particular adjective varies depending on the noun
it modifies. For example, a good author writes well, whereas a good language is good to
learn, hear, or study. A simple way to address this is to select the interpretations with
the highest probability. For good author and good language, the five most likely inter-
pretations (and their grammatical functions) according to the model (see Section 3.3)
are given in Table 22. As can be seen from the table, subject-related interpretations are
ranked higher for good author; the opposite is true for good language. Another possibil-
305
Lapata and Lascarides Logical Metonymy
ity for determining the grammatical function for equibiased adjectives is to compare
verb-object and verb-subject interpretations directly for a particular adjective-noun
combination. As an example, consider the following. For good author, the model pro-
duces 107 object-related paraphrases and 199 subject-related ones. Furthermore, the
subject-related probabilities are on average higher than the object-related ones, and the
difference is statistically significant (using a one-tailed t-test, t(304) = 3.26, p < .01).
For good language there are 253 object-related paraphrases and 180 subject-related ones.
The former are assigned higher probabilities than the latter, and the difference is sta-
tistically significant (t(431) = 3.80, p < .01).
Machine translation is another related application. A logical metonymy may be
acceptable in a source language but unacceptable in the target language. Consider the
example in (59): Its direct translation into German produces a semantically unaccept-
able sentence (see (60a)). In this case we need to spell out the metonymy in order to
obtain an acceptable translation for German, and our model can be used to provide
the missing information by generating meaning paraphrases. Under such an approach,
we would not translate (59) directly, but one of its paraphrases (see (60b) and (60c)).
(59) Peter attempted the peak.
(60) a. Peter hat den Gipfel versucht.
Peter has the peak attempted
?Peter attempted the peak.?
b. Peter hat den Gipfel zu besteigen versucht.
Peter has the peak to climb attempted
?Peter attempted to climb the peak.?
c. Peter hat den Gipfel zu erreichen versucht.
Peter has the peak to reach attempted
?Peter attempted to reach the peak.?
5. Related Work
In contrast to the extensive theoretical literature on the topic of logical metonymy, little
attention has been paid to the phenomenon from an empirical perspective. Briscoe,
Copestake, and Boguraev (1990) and Verspoor (1997) undertake a manual analysis of
logical metonymies found in naturally occurring text. Their results show that logical
metonymy is a relatively widespread phenomenon and that most metonymic exam-
ples can be interpreted on the basis of the head noun?s qualia structure, assuming a
theoretical framework similar to Pustejovsky?s (1991). Verspoor?s (1997) analysis of the
metonymic verbs begin and finish demonstrates that context plays a relatively small
role in the interpretation of these verbs: 95.0% of the logical metonymies for begin and
95.6% of the logical metonymies for finish can be resolved on the basis of information
provided by the noun for which the verb selects. Briscoe, Copestake, and Boguraev?s
(1990) work further suggests ways of acquiring qualia structures for nouns by com-
bining information extracted from machine-readable dictionaries and corpora. Our
probabilistic formulation of logical metonymy allows us to discover interpretations
for metonymic constructions without presupposing the existence of qualia structures.
In fact, we show that a simple statistical learner in combination with a shallow syn-
tactic analyzer yields relatively intuitive results, considering the simplifications and
approximations in the system.
306
Computational Linguistics Volume 29, Number 2
Perhaps more relevant to the work presented here are previous approaches to the
automatic interpretation of general metonymy (Lakoff and Johnson 1980; Nunberg
1995). This is slightly different from logical metonymy, in that the examples aren?t
usually analyzed in terms of semantic type coercion. But the phenomena are closely
related. Generally speaking an expression A is considered a metonymy if A deviates
from its literal denotation in that it stands for an entity B that is not expressed explicitly
but is conceptually related to A via a contiguity relation r (Markert and Hahn 1997).
A typical example of general metonymy is given in (61): in (61a) the bottle stands for
its content (i.e., the liquid in the bottle), and in (61b) Shakespeare stands for his works.
The contiguity relation r between the bottle and its liquid is Container for Contents; for
Shakespeare and his works the contiguity relation is Producer for Product.
(61) a. Denise drank the bottle.
b. Peter read Shakespeare.
Previous approaches to processing metonymy typically rely heavily on the avail-
ability of manually constructed knowledge bases or semantic networks (Fass 1991;
Inverson and Helmreich 1992; Bouaud, Bachimont, and Zweigenbaum 1996; Hobbs et
al. 1993). Furthermore, most implementations either contain no evaluation (Fass 1991;
Inverson and Helmreich 1992; Hobbs et al 1993) or report results on the development
data (Bouaud, Bachimont, and Zweigenbaum 1996).
The approach put forward by Utiyama, Murata, and Isahara (2000) is perhaps the
most comparable to our own work. Utiyama et al describe a statistical approach to
the interpretation of general metonymies for Japanese. Utiyama et al?s algorithm in-
terprets verb-object metonymies by generating the entities for which the object stands.
These entities are ranked using a statistical measure. Given an expression like (62),
nouns related to Shakespeare are extracted from the corpus (e.g., si ?poem?, tyosyo ?writ-
ings?, sakuhin ?works?) and ranked according to their likelihood. Two types of syntactic
relations are used as cues for the interpretation of metonymic expressions: (1) the noun
phrase A no B, roughly corresponding to the English B of A, where A is the noun figur-
ing in the metonymic expression (e.g., Shakespeare in (62)) and B is the noun it stands
for (e.g., sakuhin ?works?), and (2) nouns co-occurring with the target noun (e.g., Shake-
speare) within the target sentence.
(62) Shakespeare wo yomu
Shakespeare ACC read
?read Shakespeare?
Given a metonymy of the form A R V, the appropriateness of a noun B as an
interpretation of A is defined as
LQ(B | A, R, V) =
P(B | A, Q)P(R, V | B)
P(R, V)
(63)
where V is the verb in the metonymic expression, A is its object, R is A?s case marker
(e.g., wo (accusative)), B is the noun A stands for, and Q is the relation Q bears to
A (e.g., no). The probabilities in (63) are estimated from a large, morphologically
analyzed Japanese corpus of newspaper texts (approximately 153 million words). A
Japanese thesaurus is used for the estimation of the term P(R, V | B) when the fre-
quency f (R, V, B) is zero (see Utiyama, Murata, and Isahara (2000) for the derivation
307
Lapata and Lascarides Logical Metonymy
and estimation of (63)). Utiyama et al?s approach is tested on 75 metonymies taken
from the literature. It achieves a precision of 70.6% as measured by one of the au-
thors according to the following criterion: A metonymic interpretation was considered
correct if it made sense in some context.
Our approach is conceptually similar to that of Utiyama, Murata, and Isahara
(2000). Metonymies are interpreted using corpora as the inventory of the missing
information. In contrast to Utiyama et al, we use no information external to the cor-
pus (e.g., a thesaurus); sparse-data problems are tackled via independence assump-
tions, and syntactic information is obtained through shallow text analysis (Utiyama,
Murata, and Isahara (2000) rely on morphological analysis to provide cues for syn-
tactic information). The most striking difference, however, between our work and
Utiyama et al?s is methodological. Their evaluation is subjective and limited to ex-
amples taken from the literature. The appropriateness of their statistical measure
(see (63)) is not explored, and it is not clear whether it can derive an intuitively
plausible ranking of interpretations or whether it can extend to examples found in
naturally occurring text. We test our probabilistic formulation of logical metonymy
against a variety of examples taken from the corpus, and the derived interpretations
are evaluated objectively using standard experimental methodology. Furthermore, the
appropriateness of the proposed model is evaluated via comparisons to a naive base-
line.
6. Conclusions
In this article we proposed a statistical approach to logical metonymy. We acquired the
meanings of metonymic constructions from a large corpus and introduced a probabilis-
tic model that provides a ranking on the set of possible interpretations. We identified
semantic information automatically by exploiting the consistent correspondences be-
tween surface syntactic cues and meaning.
We evaluated our results against paraphrase judgments elicited experimentally
from subjects naive to linguistic theory and showed that the model?s ranking of
meanings correlates reliably with human intuitions. Comparison between our model
and human judgments yields a reliable correlation of .64 for verb-noun combinations
and .40 for adjective-noun pairs. Furthermore, our model performs reliably better than
a naive baseline model, which achieves only a correlation of .42 in the case of verbs
and .25 in the case of adjectives.
Our approach combined insights from linguistic theory (i.e., Pustejovsky?s (1995)
theory of qualia structure and Vendler?s (1968) observations) with corpus-based ac-
quisition techniques, probabilistic modeling, and experimental evaluation. Our results
empirically tested the validity of linguistic generalizations and extended their cov-
erage. Furthermore, in agreement with other lexical acquisition studies (Merlo and
Stevenson 2001; Barzilay and McKeown 2001; Siegel and McKeown 2000; Light 1996;
McCarthy 2000; Rooth et al 1999), we showed that it is possible to extract semantic
information from corpora even if they are not semantically annotated in any way.
Appendix A. The WebExp Software Package
As part of the evaluation of the probabilistic models presented in this article, we
conducted two psycholinguistic experiments. These experiments were administered
308
Computational Linguistics Volume 29, Number 2
using WebExp (Keller, Corley, and Scheepers 2001), a software package designed for
conducting psycholinguistic studies over the Web.6
WebExp is a set of Java classes for conducting psycholinguistic experiments over
the World Wide Web. The software consists of two modules: the WebExp server, which
is a stand-alone Java application, and the WebExp client, which is implemented as a
Java applet. The server application runs on the Web server that hosts the experiment
and waits for client applets to connect to it. It issues experimental materials to clients
and records participants? responses. The client applet is typically embedded into a
Web page that contains the instructions for the experiment. When a participant starts
the experiment, the WebExp client will download the experimental materials from
the WebExp server and administer them to the participant. After the experiment is
completed, it will send the participants? responses to the server, along with other
participant-specific data.
As Java is a full-fledged programming language, it gives the Web designer max-
imal control over the interactive features of a Web site. WebExp makes use of this
flexibility to keep the experimental procedure as constant as possible across partici-
pants. An important aspect is that the sequence in which the experimental items are
administered is fixed for each participant: The participant does not have the ability to
go back to previous stimuli and to inspect or change previous responses. (If the partic-
ipant hits the Back button on the browser, the experiment will terminate.) WebExp also
provides timings of participant responses by measuring the response onset time and
the completion time for each answer. The studies reported in this article make no direct
use of these timings. Nevertheless, the timings were useful for screening the responses
for anomalies, that is, to eliminate the data for subjects who responded too quickly
(and thus probably did not complete the experiment in a serious fashion) or those who
responded too slowly (and thus were probably distracted while doing the experiment).
WebExp automatically tests the response timings against upper and lower limits pro-
vided by the experimenter and excludes participants whose timings are anomalous.
Further manual checks can be carried out on the response timings later on.
Apart from providing response timing, WebExp also offers a set of safeguards
that are meant to ensure the authenticity of the participants taking part and exclude
participants from participating more than once:
E-mail Address. Each participant has to provide his or her e-mail address. An automatic
plausibility check is conducted on the address to ensure that it is syntactically valid.
If the address is valid, then WebExp sends an e-mail to the address at the end of
the experiment (the e-mail typically contains a message thanking the participant for
taking part). If the e-mail is returned as undeliverable, the experimenter is effectively
informed that a participant is likely to have used a fake identity and has the option
of excluding that participant?s responses from further analysis.
Personal Data. Before the experiment proper commences, each participant has to fill
in a short questionnaire supplying name, age, sex, handedness, and language back-
ground. These data allow manual plausibility checks so that participants who provide
implausible answers can be eliminated from the data set.
6 The WebExp software package is distributed free of charge for noncommercial purposes. Information
on how to obtain the latest version is available at http://www.hcrc.ed.ac.uk/web exp/. A central entry
page for all experiments using WebExp can be found at http://www.language-experiments.org/.
309
Lapata and Lascarides Logical Metonymy
Responses. A manual inspection of the responses allows the experimenter to detect
participants who have misunderstood the instructions or who have responded in an
anomalous fashion (e.g., by giving the same response to every item).
Connection Data. The software also logs data related to the participant?s Web connec-
tion. This includes the Internet address of his machine and the operating system and
browser he uses. This information (in addition to the e-mail address) is valuable in
detecting participants who take part more than once.
In addition to making it possible to administer experiments over the Web, WebExp
can also be used in a conventional laboratory setting. In such a setting, WebExp has
the advantage of being platform independent (as it is implemented in Java); that
is, it will run on any computer that is connected to the Internet and runs a Web
browser. Comparisons of experimental data obtained from Internet experiments (using
WebExp) and their laboratory-based counterparts (Keller and Asudeh 2002; Keller and
Alexopoulou 2001; Corley and Scheepers 2002) revealed high correlations between the
two types of data sets; the comparisons also demonstrated that the same main effects
are obtained from Web-based and laboratory-based experiments.
Appendix B. Materials
B.1. Experiment 1
The following is a list of the materials used in Experiment 1. The modulus is shown
in (64). The verb-noun pairs and their selected interpretations (Interpr) are illustrated
in Table 23. In addition, the table shows the mean ratings (Rtg) for each paraphrase ac-
cording to the subjects? responses and their probability (Prob) according to the model.
(64) David finished a course David finished writing a course
B.2. Experiment 4
The experimental item in (65) was presented as the modulus in Experiment 4. The
experimental materials are shown in Tables 24 and 25.
(65) hard substance a substance that is hard to alter
310
Computational Linguistics Volume 29, Number 2
Table 23
Materials for Experiment 1, with mean ratings and model probabilities.
High Medium Low
Verb-noun Interpr Rtg Prob Interpr Rtg Prob Interpr Rtg Prob
attempt definition analyze ?0.087 ?21.44 recall ?0.0338 ?22.84 support ?0.1571 ?23.87
attempt peak climb 0.2646 ?20.22 claim ?0.0900 ?23.53 include ?0.4450 ?24.85
attempt question reply to 0.1416 ?18.96 set ?0.2666 ?21.63 stick to ?0.3168 ?22.55
attempt smile give 0.2490 ?19.43 rehearse ?0.1976 ?22.21 look at ?0.4722 ?23.74
attempt walk take 0.1807 ?19.81 schedule ?0.1649 ?22.71 lead ?0.1863 ?23.85
begin game play 0.2798 ?15.11 modify ?0.3403 ?21.52 command ?0.2415 ?23.46
begin photograph develop 0.0816 ?21.11 test ?0.2147 ?22.49 spot ?0.4054 ?23.69
begin production organize 0.0502 ?19.09 influence ?0.2329 ?21.98 tax ?0.4367 ?22.78
begin test take 0.2699 ?17.97 examine ?0.1424 ?21.78 assist ?0.3440 ?24.11
begin theory formulate 0.2142 ?18.28 present 0.1314 ?21.54 assess ?0.1356 ?22.40
enjoy book read 0.2891 ?16.48 discuss ?0.1515 ?23.33 build ?0.5404 ?25.52
enjoy city live in 0.2028 ?20.77 come to 0.1842 ?23.50 cut ?0.6957 ?24.67
enjoy concert listen to 0.2779 ?20.91 throw ?0.2442 ?23.61 make ?0.2571 ?24.97
enjoy dish cook ?0.1223 ?20.21 choose ?0.2373 ?24.61 bring ?0.3385 ?25.33
enjoy story write ?0.0731 ?19.08 learn ?0.0887 ?23.50 choose ?0.2607 ?24.61
expect order hear 0.2087 ?20.29 read 0.0628 ?22.92 prepare ?0.3634 ?23.25
expect poetry see 0.1100 ?20.43 learn ?0.0601 ?22.81 prove ?0.4985 ?25.00
expect reply get 0.2696 ?20.23 listen to 0.1178 ?23.48 share ?0.2725 ?23.77
expect reward collect 0.2721 ?21.91 claim 0.1950 ?23.13 extend ?0.3743 ?23.52
expect supper eat 0.2487 ?21.27 start 0.0285 ?23.20 seek ?0.3526 ?23.91
finish gig play 0.2628 ?20.34 plan ?0.1780 ?24.47 use ?0.5341 ?25.69
finish novel translate 0.0474 ?21.81 examine ?0.1323 ?24.01 take ?0.4375 ?25.53
finish project work on 0.3113 ?18.79 study 0.0679 ?24.31 sell ?0.1692 ?25.05
finish room wallpaper 0.1497 ?19.07 construct 0.0444 ?22.48 show ?0.6305 ?24.59
finish video watch 0.3165 ?22.37 analyze ?0.0482 ?24.33 describe ?0.1718 ?25.26
postpone bill debate ?0.0401 ?22.38 give ?0.2028 ?25.36 think ?0.6238 ?27.92
postpone decision make 0.3297 ?20.38 publish ?0.1087 ?24.03 live ?0.4984 ?25.56
postpone payment make 0.2745 ?21.85 arrange 0.0166 ?23.21 read ?0.4675 ?25.91
postpone question hear ?0.1974 ?23.45 assess ?0.0795 ?24.70 go to ?0.5383 ?24.94
postpone trial go to ?0.1110 ?23.49 make ?0.1425 ?25.50 hear 0.0336 ?25.75
prefer bike ride 0.2956 ?20.64 mount ?0.1617 ?22.95 go for ?0.2119 ?24.49
prefer film go to 0.2456 ?21.63 develop ?0.1861 ?23.07 identify ?0.4657 ?24.69
prefer gas use 0.3078 ?20.28 measure ?0.2903 ?23.88 encourage ?0.5074 ?25.33
prefer people talk with 0.1235 ?20.52 sit with 0.0283 ?22.75 discover ?0.2687 ?25.26
prefer river swim in 0.0936 ?19.53 sail 0.0433 ?22.93 marry ?0.7269 ?24.13
resist argument contest ?0.0126 ?22.66 continue 0.0224 ?24.43 draw ?0.4551 ?25.51
resist invitation accept 0.2497 ?21.56 leave ?0.3620 ?25.10 offer ?0.5301 ?26.29
resist pressure take ?0.1481 ?22.67 make ?0.3602 ?24.98 see ?0.4382 ?25.22
resist proposal work on ?0.0597 ?23.56 take on 0.1286 ?24.50 call ?0.2906 ?25.99
resist song whistle ?0.0013 ?22.11 start ?0.1551 ?24.47 hold ?0.5691 ?26.50
start experiment implement 0.1744 ?21.57 study 0.0184 ?22.70 need ?0.5299 ?24.09
start letter write 0.3142 ?15.59 study ?0.0877 ?22.70 hear ?0.4526 ?24.50
start treatment receive 0.2888 ?19.53 follow 0.1933 ?22.45 assess ?0.2536 ?24.01
survive course give 0.0164 ?22.87 make ?0.1426 ?24.48 write ?0.1458 ?26.27
survive journey make 0.2719 ?22.31 take 0.2324 ?24.43 claim ?0.5388 ?25.84
survive problem create ?0.2697 ?21.12 indicate ?0.3267 ?23.01 confirm ?0.3625 ?25.08
survive scandal experience 0.2200 ?24.61 create ?0.2115 ?26.02 take ?0.3068 ?27.33
survive wound receive 0.2012 ?24.49 produce ?0.4361 ?26.32 see ?0.2668 ?26.97
try drug take 0.1077 ?17.81 grow ?0.2516 ?22.09 hate ?0.4777 ?23.88
try light turn 0.2397 ?18.10 reach for ?0.1163 ?21.23 come with ?0.5310 ?23.93
try shampoo use 0.1404 ?20.09 pack ?0.3496 ?21.56 like ?0.2581 ?24.56
try sport get into 0.1379 ?19.65 encourage ?0.2378 ?21.09 consider ?0.2223 ?22.82
try vegetable eat 0.2068 ?19.64 chop ?0.0780 ?21.38 compare ?0.3248 ?22.38
want bed lay on 0.1154 ?19.17 reserve 0.0369 ?21.24 settle in 0.0015 ?22.26
want hat buy 0.2560 ?17.84 examine ?0.2127 ?21.56 land on ?0.6379 ?22.38
want man marry 0.0826 ?15.50 torment ?0.2764 ?20.99 assess ?0.2510 ?22.22
want money make 0.1578 ?15.16 handle ?0.0929 ?20.91 represent ?0.4031 ?22.92
want program produce ?0.1980 ?18.86 teach ?0.1743 ?20.94 hate ?0.6788 ?22.63
311
Lapata and Lascarides Logical Metonymy
Table 24
Materials for Experiment 4, with mean ratings (object interpretations).
High Medium Low
Adjective-noun Interpr Rtg Prob Interpr Rtg Prob Interpr Rtg Prob
difficult consequence cope with 0.3834 ?18.61 analyze 0.0270 ?20.43 refer to ?0.3444 ?24.68
difficult customer satisfy 0.4854 ?20.27 help 0.3228 ?22.20 drive ?0.4932 ?22.64
difficult friend live with 0.2291 ?19.11 approach 0.0798 ?21.75 miss ?0.5572 ?23.04
difficult group work with 0.3066 ?18.64 teach 0.2081 ?21.00 respond to 0.1097 ?22.66
difficult hour endure 0.3387 ?21.17 complete ?0.1386 ?21.83 enjoy 0.1600 ?23.18
easy comparison make 0.4041 ?17.73 discuss ?0.0901 ?22.09 come to 0.3670 ?23.03
easy food cook 0.2375 ?18.93 introduce ?0.3673 ?21.94 finish ?0.1052 ?23.15
easy habit get into 0.2592 ?17.48 explain ?0.2877 ?21.79 support ?0.0523 ?23.70
easy point score 0.3255 ?18.77 answer 0.1198 ?20.65 know ?0.0307 ?23.43
easy task perform 0.4154 ?17.56 manage 0.3094 ?21.30 begin 0.0455 ?22.48
fast device drive ?0.0908 ?22.35 make 0.2948 ?23.65 see ?0.4817 ?25.00
fast launch stop ?0.5438 ?23.79 make 0.0075 ?25.19 see ?0.3963 ?25.84
fast pig catch ?0.5596 ?23.98 stop 0.6285 ?24.30 use ?0.5350 ?25.66
fast rhythm beat 0.0736 ?20.46 feel ?0.1911 ?25.24 make ?0.1296 ?25.60
fast town protect ?0.5896 ?23.66 make ?0.4564 ?23.90 use ?0.4996 ?25.66
good climate grow up in 0.2343 ?19.65 play in ?0.6498 ?22.18 experience ?0.4842 ?23.20
good documentation use 0.2549 ?21.89 produce ?0.2374 ?22.38 include 0.1110 ?23.73
good garment wear 0.2343 ?20.23 draw ?0.6498 ?22.71 measure ?0.4842 ?23.16
good language know 0.2188 ?19.18 reinforce ?0.1383 ?22.48 encourage ?0.0418 ?22.81
good postcard send 0.1540 ?20.17 draw ?0.3248 ?22.71 look at 0.2677 ?23.34
hard logic understand 0.2508 ?18.96 express 0.0980 ?22.76 impose ?0.2398 ?23.11
hard number remember 0.0326 ?20.30 use ?0.3428 ?21.14 create ?0.4122 ?22.69
hard path walk 0.2414 ?21.08 maintain 0.0343 ?21.64 explore 0.1830 ?23.01
hard problem solve 0.4683 ?15.92 express 0.0257 ?21.26 admit ?0.2913 ?23.39
hard war fight 0.2380 ?17.31 get through 0.2968 ?21.32 enjoy ?0.5381 ?23.18
slow child adopt ?0.5028 ?19.72 find ?0.7045 ?22.52 forget ?0.6153 ?23.93
slow hand grasp ?0.5082 ?18.03 win 0.2524 ?22.07 produce ?0.3360 ?22.52
slow meal provide ?0.0540 ?19.55 begin 0.2546 ?21.14 bring ?0.3965 ?23.29
slow minute take ?0.1396 ?19.48 fill 0.1131 ?22.06 meet ?0.6083 ?23.30
slow progress make 0.3617 ?18.50 bring ?0.1519 ?22.64 give ?0.2700 ?24.89
safe building use 0.1436 ?20.83 arrive at ?0.1640 ?23.55 come in 0.0306 ?23.96
safe drug release 0.1503 ?23.23 try 0.1930 ?23.62 start 0.1614 ?24.31
safe house go to 0.2139 ?20.87 get ?0.3438 ?22.41 make ?0.3490 ?23.19
safe speed arrive at ?0.0242 ?23.55 keep 0.1498 ?23.59 allow 0.2093 ?25.04
safe system operate 0.2431 ?19.78 move ?0.2363 ?22.85 start 0.0013 ?24.60
right accent speak in 0.1732 ?19.90 know ?0.1223 ?22.50 hear 0.0946 ?22.79
right book read 0.1938 ?18.89 lend ?0.0188 ?22.60 suggest 0.0946 ?24.90
right school apply to 0.2189 ?21.76 complain to ?0.3736 ?22.82 reach ?0.2756 ?23.69
right structure build ?0.1084 ?19.88 teach ?0.1084 ?22.76 support ?0.0505 ?24.92
right uniform wear 0.1990 ?19.79 provide ?0.1084 ?24.09 look at ?0.0505 ?25.24
wrong author accuse ?0.1925 ?21.90 read 0.0450 ?24.09 consider 0.0653 ?24.50
wrong color use 0.2366 ?21.78 look for 0.0587 ?22.78 look at ?0.1907 ?24.89
wrong note give 0.0222 ?22.29 keep 0.2014 ?22.64 accept ?0.1462 ?24.16
wrong post assume ?0.3000 ?20.10 make 0.2579 ?23.81 consider ?0.0466 ?24.50
wrong strategy adopt 0.2804 ?19.86 encourage 0.1937 ?23.51 look for 0.0135 ?24.39
312
Computational Linguistics Volume 29, Number 2
Table 25
Materials for Experiment 4, with mean ratings (subject interpretations).
High Medium Low
Adjective-noun Interpr Rtg Prob Interpr Rtg Prob Interpr Rtg Prob
difficult customer buy ?0.2682 ?20.19 pick ?0.2050 ?22.58 begin ?0.3560 ?24.83
difficult friend explain ?0.4658 ?20.16 neglect ?0.5274 ?21.75 enjoy ?0.4711 ?23.59
difficult passage read 0.1668 ?20.46 speak ?0.3600 ?22.62 appear ?0.4030 ?23.78
difficult piece read 0.1052 ?20.30 survive ?0.5080 ?22.28 continue ?0.1006 ?23.89
difficult spell break ?0.3047 ?19.80 create ?0.2412 ?22.81 start ?0.3661 ?23.50
easy car start ?0.1652 ?21.01 move ?0.2401 ?21.42 close ?0.5750 ?23.94
easy change occur 0.1999 ?20.32 prove 0.2332 ?21.57 sit ?0.0932 ?23.27
easy food cook 0.0443 ?20.28 change ?0.6046 ?22.25 form ?0.3918 ?22.96
easy habit develop 0.1099 ?21.43 start 0.1156 ?23.21 appear ?0.3490 ?24.70
easy task fit ?0.3882 ?20.77 end ?0.0982 ?22.90 continue 0.0474 ?24.11
fast device go 0.2638 ?22.80 come ?0.3652 ?23.91 add ?0.2219 ?24.68
fast horse run 0.4594 ?20.78 work 0.0025 ?22.98 add ?0.5901 ?24.64
fast lady walk ?0.0261 ?22.31 work ?0.0716 ?23.90 see ?0.4816 ?25.15
fast pig run 0.2081 ?22.57 come ?0.1807 ?23.91 get ?0.2764 ?24.75
fast town grow ?0.3601 ?18.66 spread ?0.3289 ?22.84 sell ?0.3462 ?24.33
good ad read 0.1248 ?22.39 sell 0.2154 ?22.72 run ?0.0832 ?22.78
good climate change ?0.3748 ?21.30 improve ?0.3312 ?22.57 begin ?0.4093 ?23.36
good egg look ?0.0581 ?21.79 develop ?0.2457 ?22.02 appear 0.1149 ?24.01
good light work ?0.0022 ?19.42 spread ?0.2023 ?21.75 increase ?0.4349 ?23.82
good show run 0.0787 ?21.07 continue ?0.0798 ?22.79 die ?0.6569 ?23.79
hard fish bite ?0.3583 ?20.39 pull ?0.2579 ?22.53 appear ?0.2568 ?22.79
hard logic get ?0.1211 ?21.90 sell ?0.4533 ?22.18 go ?0.4388 ?24.59
hard substance keep 0.0227 ?21.28 remain 0.0978 ?22.93 seem 0.1971 ?23.03
hard toilet flush ?0.1796 ?20.46 look ?0.3465 ?23.77 start ?0.6835 ?24.60
hard war break out ?0.4969 ?16.94 grow ?0.2792 ?22.21 increase ?0.2602 ?23.60
safe building approach ?0.6815 ?22.87 stay 0.0852 ?23.09 start ?0.5152 ?23.53
safe drug come ?0.3802 ?19.41 play ?0.5562 ?22.24 try ?0.3126 ?22.45
safe man eat ?0.5434 ?21.23 ignore ?0.6673 ?21.89 agree ?0.6509 ?23.16
safe speed go ?0.3116 ?16.26 leave ?0.6136 ?20.24 remain 0.1267 ?22.87
safe system operate 0.3697 ?19.92 continue 0.0374 ?21.48 think ?0.4845 ?22.90
slow child react 0.1485 ?22.66 adapt 0.1556 ?23.21 express ?0.0256 ?24.60
slow hand move 0.0738 ?22.24 draw 0.0039 ?23.38 work ?0.0346 ?25.56
slow meal go 0.1237 ?20.57 run ?0.1474 ?21.47 become ?0.2802 ?23.17
slow minute pass 0.2717 ?23.17 start ?0.4423 ?23.49 win ?0.6709 ?24.64
slow sleep come ?0.0671 ?19.56 follow ?0.3108 ?22.56 seem ?0.2169 ?24.85
right accent go ?0.1727 ?18.54 sound 0.1928 ?21.14 fall ?0.3926 ?22.76
right book read ?0.2429 ?19.50 feel ?0.1027 ?21.74 discuss ?0.2195 ?23.61
right character live ?0.2505 ?22.65 set ?0.4063 ?23.14 feel ?0.1651 ?23.92
right people vote ?0.4541 ?21.70 eat ?0.5921 ?22.81 answer ?0.0992 ?24.68
right school teach 0.0159 ?20.91 start ?0.3466 ?24.03 stand ?0.3839 ?24.88
wrong author go ?0.4348 ?20.59 think ?0.4128 ?22.96 read ?0.5542 ?24.50
wrong business think ?0.4018 ?23.15 spend ?0.4416 ?23.85 hope ?0.5608 ?24.18
wrong color go 0.1846 ?20.93 show ?0.0819 ?23.54 seem 0.2869 ?24.18
wrong note conclude ?0.2575 ?21.24 show ?0.3480 ?23.87 tell ?0.2732 ?24.45
wrong policy encourage ?0.0401 ?21.71 identify ?0.2167 ?23.56 accept 0.0183 ?23.76
313
Lapata and Lascarides Logical Metonymy
Appendix C. Descriptive Statistics
Table 26 displays the descriptive statistics for the model probabilities and the subject
ratings for Experiments 1 and 4.
Table 26
Descriptives for model probabilities and subject ratings.
Rank Mean StdDev StdErr Min Max
Model probabilities, Experiment 1
High ?21.62 2.59 0.26 ?24.62 ?15.11
Medium ?22.65 2.19 0.22 ?26.32 ?20.90
Low ?23.23 2.28 0.22 ?27.92 ?22.22
Subject ratings, Experiment 1
High 0.1449 0.2355 0.0309 ?0.2697 0.3297
Medium ?0.1055 0.2447 0.0321 ?0.4361 0.2324
Low ?0.3848 0.2728 0.0358 ?0.7269 0.0336
Model probabilities, Experiment 4
High ?20.49 1.71 0.18 ?23.99 ?15.93
Medium ?22.62 0.99 0.10 ?25.24 ?20.24
Low ?23.91 0.86 0.18 ?25.85 ?22.46
Subject ratings, Experiment 4
High ?0.0005 0.2974 0.0384 ?0.68 0.49
Medium ?0.1754 0.3284 0.0424 ?0.70 0.31
Low ?0.2298 0.3279 0.0423 ?0.68 0.37
Acknowledgments
This work was supported by ESRC grant
number R000237772 (Data Intensive
Semantics and Pragmatics) and the DFG
(Gottfried Wilhelm Leibniz Award to
Manfred Pinkal). Alex Lascarides is
supported by an ESRC research fellowship.
Thanks to Brian McElree and Matt Traxler
for making available to us the results of
their norming study and to Ann Copestake,
Frank Keller, Scott McDonald, Manfred
Pinkal, Owen Rambow, and three
anonymous reviewers for valuable
comments.
References
Abney, Steve. 1996. Partial parsing via
finite-state cascades. In John Carroll,
editor, Workshop on Robust Parsing, Prague.
European Summer School in Logic,
Language and Information, pages 8?15.
Apresjan, J. D. 1973. Regular polysemy.
Linguistics, 142:5?32.
Bard, Ellen Gurman, Dan Robertson, and
Antonella Sorace. 1996. Magnitude
estimation of linguistic acceptability.
Language, 72(1):32?68.
Barzilay, Regina and Kathy McKeown. 2001.
Extracting paraphrases from a parallel
corpus. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, Toulouse, France, pages 50?57.
Bouaud, Jacques, Bruno Bachimont, and
Pierre Zweigenbaum. 1996. Processing
metonymy: A domain-model heuristic
graph traversal approach. In Proceedings of
the 16th International Conference on
Computational Linguistics, Copenhagen,
Denmark, pages 137?142.
Briscoe, Ted, Ann Copestake, and Bran
Boguraev. 1990. Enjoy the paper: Lexical
semantics via lexicology. In Proceedings of
the 13th International Conference on
Computational Linguistics, Helsinki,
Finland, pages 42?47.
Burnage, Gavin. 1990. Celex?A guide for
users. Technical report, Centre for Lexical
Information, University of Nijmegen,
Nijmegen, the Netherlands.
Burnard, Lou. 1995. The Users Reference Guide
for the British National Corpus. British
314
Computational Linguistics Volume 29, Number 2
National Corpus Consortium, Oxford
University Computing Service, Oxford,
U.K.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
Second Conference of the North American
Chapter of the Association for Computational
Linguistics, Pittsburgh, Pennsylvania.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment through
a backed-off model. In David Yarowsky
and Kenneth W. Church, editors,
Proceedings of the Third Workshop on Very
Large Corpora, Cambridge, Massachusetts,
pages 27?38.
Copestake, A. 2001. The semi-generative
lexicon: Limits on lexical productivity. In
Proceedings of the First International
Workshop on Generative Approaches to the
Lexicon, Geneva.
Copestake, A. and E. J. Briscoe. 1995.
Semi-productive polysemy and sense
extension. Journal of Semantics, 12(1):15?67.
Copestake,A. and A. Lascarides. 1997. Inte-
grating symbolic and statistical represen-
tations: The lexicon pragmatics interface.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
and the Eighth Meeting of the European
Chapter for the Association for Computational
Linguistics, Madrid, pages 136?143.
Corley, Martin and Christoph Scheepers.
2002. Syntactic priming in English:
Evidence from response latencies.
Psychonomic Bulletin and Review,
9(1):126?131.
Cowart, Wayne. 1997. Experimental Syntax:
Applying Objective Methods to Sentence
Judgments. Sage, Thousand Oaks,
California.
Daelemans, Walter, Antal van den Bosch,
and Jakub Zavrel. 1999. Forgetting
exceptions is harmful in language
learning. Machine Learning, 34(1?3):11?43.
Fass, Dan. 1991. met*: A method for
discriminating metonymy and metaphor
by computer. Computational Linguistics,
17(1):49?90.
Godard, Danie`le and Jacques Jayez. 1993.
Towards a proper treatment of coercion
phenomena. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, Columbus,
Ohio, pages 168?177.
Hobbs, Jerry R., Martin Stickel, Douglas
Appelt, and Paul Martin. 1993.
Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Inverson, Eric and Stephen Helmreich. 1992.
An integrated approach to non-literal
phrase interpretation. Computational
Intelligence, 8(3):477?493.
Keller, Frank and Theodora Alexopoulou.
2001. Phonology competes with syntax:
Experimental evidence for the interaction
of word order and accent placement in
the realization of information structure.
Cognition, 79(3):301?371.
Keller, Frank and Ash Asudeh. 2002.
Probabilistic learning algorithms and
optimality theory. Linguistic Inquiry,
33(2):225?244.
Keller, Frank, Martin Corley, and Christoph
Scheepers. 2001. Conducting
psycholinguistic experiments over the
World Wide Web. Unpublished
manuscript, University of Edinburgh,
Edinburgh, U.K., and Saarland University,
Saarbruecken, Germany.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago.
Lapata, Maria, Frank Keller, and Scott
McDonald. 2001. Evaluating smoothing
algorithms against plausibility judgments.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics,
Toulouse, France, pages 346?353.
Lascarides, Alex and Ann Copestake. 1998.
Pragmatics and word meaning. Journal of
Linguistics, 34(2):387?414.
Leech, Geoffrey, Roger Garside, and Michael
Bryant. 1994. The tagging of the British
national corpus. In Proceedings of the 15th
International Conference on Computational
Linguistics, Kyoto, Japan, pages 622?628.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Light, Marc. 1996. Morphological Cues for
Lexical Semantics. Ph.D. thesis, University
of Rochester.
Lodge, Milton. 1981. Magnitude Scaling:
Quantitative Measurement of Opinions. Sage,
Beverly Hills, California.
Macleod, Catherine, Ralph Grishman,
Adam Meyers, Leslie Barrett, and Ruth
Reeves. 1998. Comlex: A lexicon of
nominalizations. In Proceedings of the
Eighth International Congress of the European
Association for Lexicography, Lie`ge,
Belgium, pages 187?193.
Markert, Katja and Udo Hahn. 1997. On the
interaction of metonymies and anaphora.
In Proceedings of the 15th International Joint
Conference on Artificial Intelligence, Nagoya,
Japan, pages 1010?1015.
315
Lapata and Lascarides Logical Metonymy
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal
participation in role switching
alternations. In Proceedings of the First
Conference of the North American Chapter of
the Association for Computational Linguistics,
Seattle, Washington, pages 256?263.
McElree, Brian, Matthew J. Traxler, Martin J.
Pickering, Rachel E. Seely, and Ray
Jackendoff. 2001. Reading time evidence
for enriched composition. Cognition,
(78)1:17?25.
Merlo, Paola and Susanne Stevenson. 2001.
Automatic verb classification based on
statistical distribution of argument
structure. Computational Linguistics,
27(3):373?408.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine J. Miller. 1990. Introduction to
WordNet: An on-line lexical database.
International Journal of Lexicography,
3(4):235?244.
Nunberg, Geoffrey. 1995. Transfers of
meaning. Journal of Semantics,
12(1):109?132.
Pustejovsky, James. 1991. The generative
lexicon. Computational Linguistics,
17(4):409?441.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge.
Pustejovsky, James and Pierrette Bouillon.
1995. Logical polysemy and aspectual
coercion. Journal of Semantics, 12:133?162.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Resnik, Philip. 1999. Semantic similarity in a
taxonomy: An information-based measure
and its application to problems of
ambiguity in natural language. Journal of
Artificial Intelligence Research, 95?130.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In Lila R.
Gleitman and Aravid K. Joshi, editors,
Proceedings of the 22nd Annual Conference of
the Cognitive Science Society. Erlbaum,
Mahwah, New Jersey, pages 399?404.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via em-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, College Park,
Maryland, pages 104?111.
Schu?tze, Carson T. 1996. The Empirical Base of
Linguistics: Grammaticality Judgments and
Linguistic Methodology. University of
Chicago Press, Chicago.
Siegel, Eric and Kathleen McKeown. 2000.
Learning methods to combine linguistic
indicators: Improving aspectual
classification and revealing linguistic
insights. Computational Linguistics,
26(4):595?629.
Stevens, S. Smith. 1975. Psychophysics:
Introduction to Its Perceptual, Neural, and
Social Prospects. Wiley, New York.
Utiyama, Masao, Masaki Murata, and
Hitoshi Isahara. 2000. A statistical
approach to the processing of metonymy.
In Proceedings of the 18th International
Conference on Computational Linguistics,
Saarbru?cken, Germany, pages 885?891.
Vendler, Zeno. 1968. Adjectives and
Nominalizations. Mouton, The Hague, the
Netherlands.
Verspoor, Cornelia Maria. 1997. Contextually-
Dependent Lexical Semantics. Ph.D. thesis,
University of Edinburgh, Edinburgh, U.K.
Weiss, Sholom M. and Casimir A.
Kulikowski. 1991. Computer Systems that
Learn: Classification and Prediction Methods
from Statistics, Neural Nets, Machine
Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, California.
c? 2003 Association for Computational Linguistics
Using the Web to Obtain Frequencies for
Unseen Bigrams
Frank Keller? Mirella Lapata?
University of Edinburgh University of Sheffield
This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen
in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun,
and verb-object bigrams from the Web by querying a search engine. We evaluate this method
by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a
reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correla-
tion between Web frequencies and frequencies recreated using class-based smoothing; (d) a good
performance of Web frequencies in a pseudodisambiguation task.
1. Introduction
In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP
algorithms are typically optimized, tested, and compared on fairly small data sets (cor-
pora with millions of words), even though data sets several orders of magnitude larger
are available, at least for some NLP tasks. Banko and Brill (2001a, 2001b) experiment
with context-sensitive spelling correction, a task for which large amounts of data can
be obtained straightforwardly, as no manual annotation is required. They demonstrate
that the learning algorithms typically used for spelling correction benefit significantly
from larger training sets, and that their performance shows no sign of reaching an
asymptote as the size of the training set increases.
Arguably, the largest data set that is available for NLP is the Web,1 which currently
consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide
enormous potential for training NLP algorithms, if Banko and Brill?s (2001a, 2001b)
findings for spelling corrections generalize; potential applications include tasks that
involve word n-grams and simple surface syntax. There is a small body of existing re-
search that tries to harness the potential of the Web for NLP. Grefenstette and Nioche
(2000) and Jones and Ghani (2000) use the Web to generate corpora for languages
for which electronic resources are scarce, and Resnik (1999) describes a method for
mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and
Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001)
proposes a method for resolving PP attachment ambiguities based on Web data, Mark-
ert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora,
? School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk
? Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK.
E-mail: mlap@dcs.shef.ac.uk
1 A reviewer points out that information providers such as Lexis Nexis ?http://www.lexisnexis.com/?
might have databases that are even larger than the Web. Lexis Nexis provides full-text access to news
sources (including newspapers, wire services, and broadcast transcripts) and legal data (including case
law, codes, regulations, legal news, and law reviews).
2 This is the number of pages indexed by Google in December 2002, as estimated by Search Engine
Showdown ?http://www.searchengineshowdown.com/?.
460
Computational Linguistics Volume 29, Number 3
and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language
modeling.
A particularly interesting application is proposed by Grefenstette (1998), who uses
the Web for example-based machine translation. His task is to translate compounds
from French into English, with corpus evidence serving as a filter for candidate transla-
tions. An example is the French compound groupe de travail. There are five translations
of groupe and three translations for travail (in the dictionary that Grefenstette [1998]
is using), resulting in 15 possible candidate translations. Only one of them, namely,
work group, has a high corpus frequency, which makes it likely that this is the correct
translation into English. Grefenstette (1998) observes that this approach suffers from
an acute data sparseness problem if the counts are obtained from a conventional cor-
pus. However, as Grefenstette (1998) demonstrates, this problem can be overcome by
obtaining counts through Web searches, instead of relying on a corpus. Grefenstette
(1998) therefore effectively uses the Web as a way of obtaining counts for compounds
that are sparse in a given corpus.
Although this is an important initial result, it raises the question of the generality
of the proposed approach to overcoming data sparseness. It remains to be shown that
Web counts are generally useful for approximating data that are sparse or unseen
in a given corpus. It seems possible, for instance, that Grefenstette?s (1998) results
are limited to his particular task (filtering potential translations) or to his particular
linguistic phenomenon (noun-noun compounds). Another potential problem is the fact
that Web counts are far more noisy than counts obtained from a well-edited, carefully
balanced corpus. The effect of this noise on the usefulness of the Web counts is largely
unexplored.
Zhu and Rosenfeld (2001) use Web-based n-gram counts for language modeling.
They obtain a standard language model from a 103-million-word corpus and employ
Web-based counts to interpolate unreliable trigram estimates. They compare their in-
terpolated model against a baseline trigram language model (without interpolation)
and show that the interpolated model yields an absolute reduction in word error rate
of .93% over the baseline. Zhu and Rosenfeld?s (2001) results demonstrate that the
Web can be a source of data for language modeling. It is not clear, however, whether
their result carries over to tasks that employ linguistically meaningful word sequences
(e.g., head-modifier pairs or predicate-argument tuples) rather than simply adjacent
words. Furthermore, Zhu and Rosenfeld (2001) do not undertake any studies that eval-
uate Web frequencies directly (i.e., without a task such as language modeling). This
could be done, for instance, by comparing Web frequencies to corpus frequencies, or
to frequencies re-created by smoothing techniques.
The aim of the present article is to generalize Grefenstette?s (1998) and Zhu and
Rosenfeld?s (2001) findings by testing the hypothesis that the Web can be employed
to obtain frequencies for bigrams that are unseen in a given corpus. Instead of hav-
ing a particular task in mind (which would introduce a sampling bias), we rely on
sets of bigrams that are randomly selected from a corpus. We use a Web-based ap-
proach for bigrams that encode meaningful syntactic relations and obtain Web fre-
quencies not only for noun-noun bigrams, but also for adjective-noun and verb-object
bigrams. We thus explore whether this approach generalizes to different predicate-
argument combinations. We evaluate our Web counts in four ways: (a) comparison
with actual corpus frequencies from two different corpora, (b) comparison with human
plausibility judgments, (c) comparison with frequencies re-created using class-based
smoothing, and (d) performance in a pseudodisambiguation task on data sets from the
literature.
461
Keller and Lapata Web Frequencies for Unseen Bigrams
2. Obtaining Frequencies from the Web
2.1 Sampling Bigrams from the BNC
The data sets used in the present experiment were obtained from the British National
Corpus (BNC) (see Burnard [1995]). The BNC is a large, synchronic corpus, consisting
of 90 million words of text and 10 million words of speech. The BNC is a balanced
corpus (i.e., it was compiled so as to represent a wide range of present day British
English). The written part includes samples from newspapers, magazines, books (both
academic and fiction), letters, and school and university essays, among other kinds of
text. The spoken part consists of spontaneous conversations, recorded from volunteers
balanced by age, region, and social class. Other samples of spoken language are also
included, ranging from business or government meetings to radio shows and phone-
ins. The corpus represents many different styles and varieties and is not limited to
any particular subject field, genre, or register.
For the present study, the BNC was used to extract data for three types of predicate-
argument relations. The first type is adjective-noun bigrams, in which we assume
that the noun is the predicate that takes the adjective as its argument.3 The second
predicate-argument type we investigated is noun-noun compounds. For these, we
assume that the rightmost noun is the predicate that selects the leftmost noun as
its argument (as compound nouns are generally right-headed in English). Third, we
included verb-object bigrams, in which the verb is the predicate that selects the object
as its argument. We considered only direct NP objects; the bigram consists of the verb
and the head noun of the object. For each of the three predicate-argument relations,
we gathered two data sets, one containing seen bigrams (i.e., bigrams that occur in
the BNC) and one with unseen bigrams (i.e., bigrams that do not occur in the BNC).
For the seen adjective-noun bigrams, we used the data of Lapata, McDonald, and
Keller (1999), who compiled a set of 90 bigrams as follows. First, 30 adjectives were
randomly chosen from a part-of-speech-tagged and lemmatized version of the BNC
so that each adjective had exactly two senses according to WordNet (Miller et al 1990)
and was unambiguously tagged as ?adjective? 98.6% of the time. Lapata, McDonald,
and Keller used the part-of-speech-tagged version that is made available with the BNC
and was tagged using CLAWS4 (Leech, Garside, and Bryant 1994), a probabilistic part-
of-speech tagger, with error rate ranging from 3% to 4%. The lemmatized version of
the corpus was obtained using Karp et al?s (1992) morphological analyzer.
The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million words;
that is, they covered the whole range from fairly infrequent to highly frequent items.
Gsearch (Corley et al 2001), a chart parser that detects syntactic patterns in a tagged
corpus by exploiting a user-specified context-free grammar and a syntactic query, was
used to extract all nouns occurring in a head-modifier relationship with one of the
30 adjectives. Examples of the syntactic patterns the parser identified are given in Ta-
ble 1. In the case of adjectives modifying compound nouns, only sequences of two
nouns were included, and the rightmost-occurring noun was considered the head.
Bigrams involving proper nouns or low-frequency nouns (less than 10 per million
words) were discarded. This was necessary because the bigrams were used in exper-
iments involving native speakers (see Section 3.2), and we wanted to reduce the risk
of including words unfamiliar to the experimental subjects. For each adjective, the set
of bigrams was divided into three frequency bands based on an equal division of the
3 This assumption is disputed in the theoretical linguistics literature. For instance, Pollard and Sag (1994)
present an analysis in which there is mutual selection between the noun and the adjective.
462
Computational Linguistics Volume 29, Number 3
Table 1
Example of patterns used for the extraction of adjective-noun bigrams.
Pattern Example
A N educational material
A Adv N usual weekly classes
A N N environmental health officers
range of log-transformed co-occurrence frequencies. Then one bigram was chosen at
random from each band. This procedure ensures that the whole range of frequencies
is represented in our sample.
Lapata, Keller, and McDonald (2001) compiled a set of 90 unseen adjective-noun
bigrams using the same 30 adjectives. For each adjective, Gsearch was used to com-
pile a list of all nouns that did not co-occur in a head-modifier relationship with the
adjective. Again, proper nouns and low-frequency nouns were discarded from this
list. Then each adjective was paired with three randomly chosen nouns from its list
of non-co-occurring nouns. Examples of seen and unseen adjective-noun bigrams are
shown in Table 2.
For the present study, we applied the procedure used by Lapata, McDonald, and
Keller (1999) and Lapata, Keller, and McDonald (2001) to noun-noun bigrams and to
verb-object bigrams, creating a set of 90 seen and 90 unseen bigrams for each type
of predicate-argument relationship. More specifically, 30 nouns and 30 verbs were
chosen according to the same criteria proposed for the adjective study (i.e., minimal
sense ambiguity and unambiguous part of speech). All nouns modifying one of the
30 nouns were extracted from the BNC using a heuristic from Lauer (1995) that looks
for consecutive pairs of nouns that are neither preceded nor succeeded by another
Table 2
Example stimuli for seen and unseen adjective-noun, noun-noun, and verb-object bigrams
(with log-transformed BNC counts).
Adjective-Noun Bigrams
Adjective High Medium Low Unseen
hungry animal 1.79 pleasure 1.38 application 0 tradition, innovation, prey
guilty verdict 3.91 secret 2.56 cat 0 system, wisdom, wartime
naughty girl 2.94 dog 1.6 lunch .69 regime, rival, protocol
Noun-Noun Bigrams
High Medium Low Unseen Head Noun
process 1.14 user .95 gala 0 collection, clause, coat directory
television 1.53 satellite .95 edition 0 chain, care, vote broadcast
plasma 1.78 nylon 1.20 unit .60 fund, theology, minute membrane
Verb-Object Bigrams
Verb High Medium Low Unseen
fulfill obligation 3.87 goal 2.20 scripture .69 participant, muscle, grade
intensify problem 1.79 effect 1.10 alarm 0 score, quota, chest
choose name 3.74 law 1.61 series 1.10 lift, bride, listener
463
Keller and Lapata Web Frequencies for Unseen Bigrams
noun. Lauer?s heuristic (see (1)) effectively avoids identifying as two-word compounds
noun sequences that are part of a larger compound.
(1) C = {(w2, w3) | w1w2w3w4; w1, w4 ? N; w2, w3 ? N}
Here, w1 w2 w3 w4 denotes the occurrence of a sequence of four words and N is the
set of words tagged as nouns in the corpus. C is the set of compounds identified by
Lauer?s (1995) heuristic.
Verb-object bigrams for the 30 preselected verbs were obtained from the BNC
using Cass (Abney 1996), a robust chunk parser designed for the shallow analysis
of noisy text. The parser recognizes chunks and simplex clauses (i.e., sequences of
nonrecursive clauses) using a regular expression grammar and a part-of-speech-tagged
corpus, without attempting to resolve attachment ambiguities. It comes with a large-
scale grammar for English and a built-in tool that extracts predicate-argument tuples
out of the parse trees that Cass produces.
The parser?s output was postprocessed to remove bracketing errors and errors in
identifying chunk categories that could potentially result in bigrams whose members
do not stand in a verb-argument relationship. Tuples containing verbs or nouns at-
tested in a verb-argument relationship only once were eliminated. Particle verbs were
retained only if the particle was adjacent to the verb (e.g., come off heroin). Verbs fol-
lowed by the preposition by and a head noun were considered instances of verb-subject
relations. It was assumed that PPs adjacent to the verb headed by any of the preposi-
tions in, to, for, with, on, at, from, of, into, through, and upon were prepositional objects (see
Lapata [2001] for details on the filtering process). Only nominal heads were retained
from the objects returned by the parser. As in the adjective study, noun-noun bigrams
and verb-object bigrams with proper nouns or low-frequency nouns (less than 10 per
million words) were discarded. The sets of noun-noun and verb-object bigrams were
divided into three frequency bands, and one bigram was chosen at random from each
band.
The procedure described by Lapata, Keller, and McDonald (2001) was followed for
creating sets of unseen noun-noun and verb-object bigrams: for each noun or verb, we
compiled a list of all nouns with which it did not co-occur within a noun-noun or verb-
object bigram in the BNC. Again, Lauer?s (1995) heuristic and Abney?s (1996) partial
parser were used to identify bigrams, and proper nouns and low-frequency nouns
were excluded. For each noun and verb, three bigrams were formed by pairing it with
a noun randomly selected from the set of the non-co-occurring nouns for that noun
or verb. Table 2 lists examples for the seen and unseen noun-noun and verb-object
bigrams generated by this procedure.
The extracted bigrams are in several respects an imperfect source of information
about adjective-noun or noun-noun modification and verb-object relations. First notice
that both Gsearch and Cass detect syntactic patterns on part-of-speech-tagged corpora.
This means that parsing errors are likely to result because of tagging mistakes. Second,
even if one assumes perfect tagging, the heuristic nature of our extraction procedures
may introduce additional noise or miss bigrams for which detailed structural infor-
mation would be needed.
For instance, our method for extracting adjective-noun pairs ignores cases in which
the adjective modifies noun sequences of length greater than two. The heuristic in (1)
considers only two-word noun sequences. Abney?s (1996) chunker recognizes basic
syntactic units without resolving attachment ambiguities or recovering missing infor-
mation (such as traces resulting from the movement of constituents). Although pars-
ing is robust and fast (since unlike in traditional parsers, no global optimization takes
464
Computational Linguistics Volume 29, Number 3
place), the identified verb-argument relations are undoubtedly somewhat noisy, given
the errors inherent in the part-of-speech tagging and chunk recognition procedure.
When evaluated against manually annotated data, Abney?s (1996) parser identified
chunks with 87.9% precision and 87.1% recall. The parser further achieved a per-word
accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk
length identified correctly).
Despite their imperfect output, heuristic methods for the extraction of syntactic
relations are relatively common in statistical NLP. Several statistical models employ
frequencies obtained from the output of partial parsers and other heuristic methods;
these include models for disambiguating the attachment site of prepositional phrases
(Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns
(Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the
induction of selectional preferences (Abney and Light 1999), methods for automati-
cally clustering words according to their distribution in particular syntactic contexts
(Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994;
Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee
1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways
for obtaining bigram frequencies that are potentially useful for such models despite
the fact that some of these bigrams are identified in a heuristic manner and may be
noisy.
2.2 Sampling Bigrams from the NANTC
We also obtained corpus counts from a second corpus, the North American News
Text Corpus (NANTC). This corpus differs in several important respects from the
BNC. It is substantially larger, as it contains 350 million words of text. Also, it is not
a balanced corpus, as it contains material from only one genre, namely, news text.
However, the text originates from a variety of sources (Los Angeles Times, Washington
Post, New York Times News Syndicate, Reuters News Service, and Wall Street Journal).
Whereas the BNC covers British English, the NANTC covers American English. All
these differences mean that the NANTC provides a second, independent standard
against which to compare Web counts. At the same time the correlation found between
the counts obtained from the two corpora can serve as an upper limit for the correlation
that we can expect between corpus counts and Web counts.
The NANTC corpus was parsed using MINIPAR (Lin 1994, 2001), a broad-coverage
parser for English. MINIPAR employs a manually constructed grammar and a lexicon
derived from WordNet with the addition of proper names (130,000 entries in total).
Lexicon entries contain part-of-speech and subcategorization information. The gram-
mar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges
(i.e., types of syntactic [dependency] relationships). MINIPAR employs a distributed-
chart parsing algorithm. Instead of a single chart, each node in the grammar network
maintains a chart containing partially built structures belonging to the grammatical
category represented by the node. Grammar rules are implemented as constraints as-
sociated with the nodes and edges.
The output of MINIPAR is a dependency tree that represents the dependency
relations between words in a sentence. Table 3 shows a subset of the dependencies
MINIPAR outputs for the sentence The fat cat ate the door mat. In contrast to Gsearch
and Cass, MINIPAR produces all possible parses for a given sentence. The parses are
ranked according to the product of the probabilities of their edges, and the most likely
parse is returned. Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson
1995), a domain-independent corpus of British English, and achieved a recall of 79%
and precision of 89% on the dependency relations.
465
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 3
Examples of dependencies generated by MINIPAR for The fat cat ate the door mat.
Head Relation Modifier Description
cat N:det:Det the determiner of noun
cat N:mod:A fat adjective modifier of noun
eat V:subj:N cat subject of verb
eat V:obj:N mat object of verb
mat N:det:Det the determiner of noun
mat N:nn:N door prenominal modifier of noun
For our experiments, we concentrated solely on adjective-noun, noun-noun, and
verb object relations (denoted as N:mod:A, N:nn:N, and V:obj:N in Table 3). From the
syntactic analysis provided by the parser, we extracted all occurrences of bigrams that
were attested both in the BNC and the NANTC corpus. In this way, we obtained
NANTC frequency counts for the bigrams that we had randomly selected from the
BNC. Table 4 shows the NANTC counts for the set of seen bigrams from Table 2.
Because of the differences in the extraction methodology (chunking versus full
parsing) and the text genre (balanced corpus versus news text), we expected that
some BNC bigrams would not be attested in the NANTC corpus. More precisely, zero
frequencies were returned for 23 adjective-noun, 16 verb-noun, and 37 noun-noun
bigrams. The fact that more zero frequencies were observed for noun-noun bigrams
than for the other two types is perhaps not surprising considering the ease with which
novel compounds are created (Levi 1978). We adjusted the zero counts by setting
them to .5. This was necessary because all further analyses were carried out on log-
transformed frequencies (see Table 4).
Table 4
Log-transformed NANTC counts for seen adjective-noun, noun-noun, and verb-object bigrams.
Adjective-Noun Bigrams
Adjective High Medium Low
hungry animal .90 pleasure -.30 application .60
guilty verdict 2.82 secret .95 cat -.30
naughty girl .69 dog -.30 lunch -.30
Noun-Noun Bigrams
High Medium Low Head Noun
process - .30 user -.30 gala -.30 directory
television 2.70 satellite -.30 edition -.30 broadcast
plasma - .30 nylon 0 unit 0 membrane
Verb-Object Bigrams
Verb High Medium Low
fulfill obligation 2.38 goal 2.04 scripture -.30
intensify problem 1.20 effect .60 alarm -.30
choose name 2.25 law .90 series .48
466
Computational Linguistics Volume 29, Number 3
2.3 Obtaining Web Counts
Web counts for bigrams were obtained using a simple heuristic based on queries to the
search engines AltaVista and Google. All search terms took into account the inflectional
morphology of nouns and verbs.
The search terms for verb-object bigrams matched not only cases in which the
object was directly adjacent to the verb (e.g., fulfill obligation), but also cases in which
there was an intervening determiner (e.g., fulfill the/an obligation). The following search
terms were used for adjective-noun, noun-noun, and verb-object bigrams, respectively:
(2) "A N", where A is the adjective and N is the singular or plural form of the
noun.
(3) "N1 N2", where N1 is the singular form of the first noun and N2 is the
singular or plural form of the second noun.
(4) "V Det N", where V is the infinitive, singular present, plural present,
past, perfect, or gerund form of the verb, Det is the determiner the, the
determiner a, or the empty string, and N is the singular or plural form of
the noun.
Note that all searches were for exact matches, which means that the words in the search
terms had to be directly adjacent to score a match. This is encoded by enclosing the
search term in quotation marks. All our search terms were in lower case. We searched
the whole Web (as indexed by AltaVista and Google); that is, the queries were not
restricted to pages in English.
Based on the Web searches, we obtained bigram frequencies by adding up the
number of pages that matched the morphologically expanded forms of the search
terms (see the patterns in (2)?(4)). This process can be automated straightforwardly
using a script that generates all the search terms for a given bigram, issues an AltaVista
or Google query for each of the search terms, and then adds up the resulting number
of matches for each bigram. We applied this process to all the bigrams in our data set,
covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams (i.e., a
set of 540 bigrams in total). The queries were carried out in January 2003 (and thus
the counts are higher than those reported in Keller, Lapata, and Ourioupina [2002],
which were generated about a year earlier).
For some bigrams that were unseen in the BNC, our Web-based procedure returned
zero counts; that is, there were no matches for those bigrams in the Web searches. It
is interesting to compare the Web and NANTC with respect to zero counts: Both
data sources are larger than the BNC and hence should be able to mitigate the data
sparseness problem to a certain extent. Table 5 provides the number of zero counts for
both Web search engines and compares them to the number of bigrams that yielded no
matches in the NANTC. We observe that the Web counts are substantially less sparse
than the NANTC counts: In the worst case, there are nine bigrams for which our Web
queries returned no matches (10% of the data), whereas up to 82 bigrams were unseen
in the NANTC (91% of the data). Recall that the NANTC is 3.5 times larger than the
BNC, which does not seem to be enough to substantially mitigate data sparseness. All
further analyses were carried out on log-transformed frequencies; hence we adjusted
zero counts by setting them to .5.
Table 6 shows descriptive statistics for the bigram counts we obtained using
AltaVista and Google. For comparison, this table also provides descriptive statis-
tics for the BNC and NANTC counts (for seen bigrams only) and for the counts
467
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 5
Number of zero counts returned by queries to search engines and in the NANTC (for bigrams
unseen in BNC).
Adjective-Noun Noun-Noun Verb-Object
AltaVista 2 9 1
Google 2 5 0
NANTC 76 82 78
Table 6
Descriptive statistics for Web counts, corpus counts, and counts re-created using class-based
smoothing (log-transformed).
Adjective-Noun Noun-Noun Verb-Object
Min Max Mean SD Min Max Mean SD Min Max Mean SD
Seen Bigrams
AltaVista 1.15 5.84 3.72 1.02 .60 6.16 3.52 1.22 .48 5.86 3.42 1.13
Google 1.54 6.11 4.01 1.01 .90 6.30 3.80 1.23 .60 5.96 3.70 1.11
BNC 0 2.19 .89 .69 0 2.14 .74 .64 0 2.55 .68 .58
NANTC - .30 2.84 .84 .96 - .30 3.02 .56 .94 -.30 3.73 1.90 .98
Smoothing - .06 2.32 1.28 .51 - .70 1.71 .30 .61 -.51 2.07 .53 .57
Unseen Bigrams
AltaVista - .30 5.00 1.50 .99 - .30 3.97 1.20 1.14 - .30 3.88 1.55 1.06
Google - .30 4.11 1.79 .95 - .30 4.15 1.60 1.12 0 4.19 1.90 1.04
Smoothing - .03 2.10 1.25 .46 -1.01 1.93 .28 .66 -.70 1.95 .53 .58
Table 7
Average factor by which Web counts are larger than BNC counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista 665 691 550
Google 1,306 1,151 1,064
re-created using class-based smoothing (see Section 3.3 for details on the re-created
frequencies).
From these data, we computed the average factor by which the Web counts are
larger than the BNC counts. The results are given in Table 7 and indicate that the
AltaVista counts are between 550 and 691 times larger than the BNC counts, and that
the Google counts are between 1,064 and 1,306 times larger than the BNC counts.
As we know the size of the BNC (100 million words), we can use these figures to
estimate the number of words available on the Web: between 55.0 and 69.1 billion
words for AltaVista, and between 106.4 and 139.6 billion words for Google. These
estimates are in the same order of magnitude as Grefenstette and Nioche?s (2000)
estimate that 48.1 billion words of English are available on the Web (based on AltaVista
counts compiled in February 2000). They also agree with Zhu and Rosenfeld?s (2001)
estimate that the effective size of the Web is between 79 and 108 billion words (based
on AltaVista, Lycos, and FAST counts; no date given).
468
Computational Linguistics Volume 29, Number 3
2.4 Potential Sources of Noise in Web Counts
The method we used to retrieve Web counts is based on very simple heuristics; it is
thus inevitable that the counts generated will contain a certain amount of noise. In
this section we discuss a number of potential sources of such noise.
An obvious limitation of our method is that it relies on the page counts returned by
the search engines; we do not download the pages themselves for further processing.
Note that many of the bigrams in our sample are very frequent (up to 106 matches;
see the ?Max? columns in Table 6), hence the effort involved in downloading all pages
would be immense (though methods for downloading a representative sample could
probably be devised).
Our approach estimates Web frequencies based not on bigram counts directly, but
on page counts. In other words, it ignores the fact that a bigram can occur more than
once on a given Web page. This approximation is justified, as Zhu and Rosenfeld (2001)
demonstrated for unigrams, bigrams, and trigrams: Page counts and n-gram counts
are highly correlated on a log-log scale. This result is based on Zhu and Rosenfeld?s
queries to AltaVista, a search engine that at the time of their research returned both
the number of pages and the overall number of matches for a given query.4
Another important limitation of our approach arises from the fact that both Google
and AltaVista disregard punctuation and capitalization, even if the search term is
placed within quotation marks. This can lead to false positives, for instance, if the
match crosses a phrase boundary, such as in (5), which matches hungry prey. Other
false positives can be generated by page titles and links, such as the examples (6)
and (7) which match edition broadcast.5
(5) The lion will kill only when it?s hungry. Prey can usually sense when
lions are hunting.
(6) 10th Edition Broadcast Products Catalog (as a page title)
(7) Issue/Edition/Broadcast (as a link)
The fact that our method does not download Web pages means that no tagging, chunk-
ing, or parsing can be carried out to ensure that the matches are correct. Instead we
rely on the simple adjacency of the search terms, which is enforced by using queries
enclosed within quotation marks (see Section 2.3 for details). This means that we miss
any nonadjacent matches, even though a chunker or parser (such as the one used for
extracting BNC or NANTC bigrams) would find them. An example is an adjective-
noun bigram in which an adverbial intervenes between the adjective and the noun
(see Table 1).
Furthermore, the absence of tagging, chunking, and parsing can also generate false
positives, in particular for queries containing words with part-of-speech ambiguity.
(Recall that our bigram selection procedure ensures that the predicate word, but not
the argument word, is unambiguous in terms of its POS tagging in the BNC.) As an
example, consider process directory, which in our data set is a noun-noun bigram (see
Table 2). One of the matches returned by Google is (8), in which process is a verb.
Another example is fund membrane, which is a noun-noun bigram in our data set but
matches (9) in Google.
4 Note that this feature of AltaVista has since been discontinued; hence in the present article we had no
option but to use page counts. However, Keller, Lapata, and Ourioupina (2002) used AltaVista match
counts (instead of page counts) on the same data sets; their results agree with the ones reported in the
present article very closely.
5 Some of the examples in (5)?(9) were kindly provided by a reviewer.
469
Keller and Lapata Web Frequencies for Unseen Bigrams
(8) The global catalog server?s function is to process directory searches for
the entire forest.
(9) Green grants fund membrane technology.
Another source of noise is the fact that Google (but not AltaVista) will sometimes
return pages that do not include the search term at all. This can happen if the search
term is contained in a link to the page (but not on the page itself).
As we did not limit our Web searches to English (even though many search engines
now allow the target language for a search to be set), there is also a risk that false posi-
tives are generated by cross-linguistic homonyms, that is, by words of other languages
that are spelled in the same way as the English words in our data sets. However, this
problem is mitigated by the fact that English is by far the most common language on
the Web, as shown by Grefenstette and Nioche (2000). Also, the chance of two such
homonyms forming a valid bigram in another language is probably fairly small.
To summarize, Web counts are certainly less sparse than the counts in a corpus of a
fixed size (see Section 2.3). However, Web counts are also likely to be significantly more
noisy than counts obtained from a carefully tagged and chunked or parsed corpus, as
the examples in this section show. It is therefore essential to carry out a comprehensive
evaluation of the Web counts generated by our method. This is the topic of the next
section.
3. Evaluation
3.1 Evaluation against Corpus Frequencies
Since Web counts can be relatively noisy, as discussed in the previous section, it is
crucial to determine whether there is a reliable relationship between Web counts and
corpus counts. Once this is assured, we can explore the usefulness of Web counts
for overcoming data sparseness. We carried out a correlation analysis to determine
whether there is a linear relationship between BNC and NANTC counts and AltaVista
and Google counts. All correlation coefficients reported in this article refer to Pear-
son?s r.6 All results were obtained on log-transformed counts.7
Table 8 shows the results of correlating Web counts with corpus counts from the
BNC, the corpus from which our bigrams were sampled (see Section 2.1). A high
correlation coefficient was obtained across the board, ranging from .720 to .847 for
AltaVista counts and from .720 to .850 for Google counts. This indicates that Web
counts approximate BNC counts for the three types of bigrams under investigation.
Note that there is almost no difference between the correlations achieved using Google
and AltaVista counts.
It is important to check that these results are also valid for counts obtained from
other corpora. We therefore correlated our Web counts with the counts obtained from
NANTC, a corpus that is larger than the BNC but is drawn from a single genre,
namely, news text (see Section 2.2). The results are shown in Table 9. We find that
6 Correlation analysis is a way of measuring the degree of linear association between two variables.
Effectively, we are fitting a linear equation y = ax + b to the data; this means that the two variables x
and y (which in our case represent frequencies or judgments) can still differ by a multiplicative
constant a and an additive constant b, even if they are highly correlated.
7 It is well-known that corpus frequencies have a Zipfian distribution. Log-transforming them is a way
of normalizing the counts before applying statistical tests. We apply correlation analysis on the
log-transformed data, which is equivalent to computing a log-linear regression coefficient on the
untransformed data.
470
Computational Linguistics Volume 29, Number 3
Table 8
Correlation of BNC counts with Web counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista .847** .720** .762**
Google .850** .720** .766**
Smoothing .248* .277** .342**
*p < .05 (one-tailed). **p < .01 (one-tailed).
Table 9
Correlation of NANTC counts with Web counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista .712** .667** .788**
Google .712** .662** .787**
BNC .710** .672** .814**
Smoothing .338** .317** .263*
*p < .05 (one-tailed). **p < .01 (one-tailed).
Google and AltaVista counts also correlate significantly with NANTC counts. The
correlation coefficients range from .667 to .788 for AltaVista and from .662 to .787 for
Google. Again, there is virtually no difference between the correlations for the two
search engines. We also observe that the correlation between Web counts and BNC is
generally slightly higher than the correlation between Web counts and NANTC counts.
We carried out one-tailed t-tests to determine whether the differences in the correlation
coefficients were significant. We found that both AltaVista counts (t(87) = 3.11, p < .01)
and Google counts (t(87) = 3.21, p < .01) were significantly better correlated with
BNC counts than with NANTC counts for adjective-noun bigrams. The difference in
correlation coefficients was not significant for noun-noun and verb-object bigrams, for
either search engine.
Table 9 also shows the correlations between BNC counts and NANTC counts.
The intercorpus correlation can be regarded as an upper limit for the correlations we
can expect between counts from two corpora that differ in size and genre and that
have been obtained using different extraction methods. The correlation between Al-
taVista and Google counts and NANTC counts reached the upper limit for all three
bigram types (one-tailed t-tests found no significant differences between the correla-
tion coefficients). The correlation between BNC counts and Web counts reached the
upper limit for noun-noun and verb-object bigrams (no significant differences for either
search engine) and significantly exceeded it for adjective-noun bigrams for AltaVista
(t(87) = 3.16, p < .01) and Google (t(87) = 3.26, p < .01).
We conclude that simple heuristics (see Section 2.3) are sufficient to obtain useful
frequencies from the Web; it seems that the large amount of data available for Web
counts outweighs the associated problems (noisy, unbalanced, etc.). We found that
Web counts were highly correlated with frequencies from two different corpora. Fur-
thermore, Web counts and corpus counts are as highly correlated as counts from two
different corpora (which can be regarded as an upper bound).
Note that Tables 8 and 9 also provide the correlation coefficients obtained when
corpus frequencies are compared with frequencies that were re-created through class-
471
Keller and Lapata Web Frequencies for Unseen Bigrams
based smoothing, using the BNC as a training corpus (after removing the seen bi-
grams). This will be discussed in more detail in Section 3.3.
3.2 Evaluation against Plausibility Judgments
Previous work has demonstrated that corpus counts correlate with human plausibility
judgments for adjective-noun bigrams. This result holds both for seen bigrams (La-
pata, McDonald, and Keller 1999) and for unseen bigrams whose counts have been
re-created using smoothing techniques (Lapata, Keller, and McDonald 2001). Based on
these findings, we decided to evaluate our Web counts on the task of predicting plausi-
bility ratings. If the Web counts for bigrams correlate with plausibility judgments, then
this indicates that the counts are valid, in the sense of being useful for predicting the
intuitive plausibility of predicate-argument pairs. The degree of correlation between
Web counts and plausibility judgments is an indicator of the quality of the Web counts
(compared to corpus counts or counts re-created using smoothing techniques).
3.2.1 Method. For seen and unseen adjective-noun bigrams, we used the two sets of
plausibility judgments collected by Lapata, McDonald, and Keller (1999) and Lapata,
Keller, and McDonald (2001), respectively. We conducted four additional experiments
to collect judgments for noun-noun and verb-object bigrams, both seen and unseen.
The experimental method was the same for all six experiments.
Materials. The experimental stimuli were based on the six sets of seen or unseen
bigrams extracted from the BNC as described in Section 2.1 (adjective-noun, noun-
noun, and verb-object bigrams). In the adjective-noun and noun-noun cases, the stimuli
consisted simply of the bigrams. In the verb-object case, the bigrams were embedded
in a short sentence to make them more natural: A proper-noun subject was added.
Procedure. The experimental paradigm was magnitude estimation (ME), a tech-
nique standardly used in psychophysics to measure judgments of sensory stimuli
(Stevens 1975), which Bard, Robertson, and Sorace (1996) and Cowart (1997) have ap-
plied to the elicitation of linguistic judgments. The ME procedure requires subjects to
estimate the magnitude of physical stimuli by assigning numerical values proportional
to the stimulus magnitude they perceive. In contrast to the five- or seven-point scale
conventionally used to measure human intuitions, ME employs an interval scale and
therefore produces data for which parametric inferential statistics are valid.
ME requires subjects to assign numbers to a series of linguistic stimuli in a propor-
tional fashion. Subjects are first exposed to a modulus item, to which they assign an
arbitrary number. All other stimuli are rated proportional to the modulus. In this way,
each subject can establish his or her own rating scale, thus yielding maximally fine-
graded data and avoiding the known problems with the conventional ordinal scales
for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Schu?tze 1996).
The experiments reported in this article were carried out using the WebExp soft-
ware package (Keller et al 1998). A series of previous studies has shown that data
obtained using WebExp closely replicate results obtained in a controlled laboratory set-
ting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou
2001), coreference judgments (Keller and Asudeh 2001), and sentence completions
(Corley and Scheepers 2002).
In the present experiments, subjects were presented with bigram pairs and were
asked to rate the degree of plausibility proportional to a modulus item. They first saw a
set of instructions that explained the ME technique and the judgment task. The concept
of plausibility was not defined, but examples of plausible and implausible bigrams
were given (different examples for each stimulus set). Then subjects were asked to
fill in a questionnaire with basic demographic information. The experiment proper
472
Computational Linguistics Volume 29, Number 3
Table 10
Descriptive statistics for plausibility judgments (log-transformed). N is the number of subjects
used in each experiment.
Adjective-Noun Bigrams Noun-Noun Bigrams Verb-Object Bigrams
N Min Max Mean SD N Min Max Mean SD N Min Max Mean SD
Seen 30 -.85 .11 -.13 .22 25 -.15 .69 .40 .21 27 -.52 .45 .12 .24
Unseen 41 -.56 .37 -.07 .20 25 -.49 .52 -.01 .23 21 -.51 .28 -.16 .22
consisted of three phases: (1) a calibration phase, designed to familiarize subjects with
the task, in which they had to estimate the length of five horizontal lines; (2) a practice
phase, in which subjects judged the plausibility of eight bigrams (similar to the ones
in the stimulus set); (3) the main experiment, in which each subject judged one of the
six stimulus sets (90 bigrams). The stimuli were presented in random order, with a
new randomization being generated for each subject.
Subjects. A separate experiment was conducted for each set of stimuli. The number
of subjects per experiment is shown in Table 10 (in the column labeled N). All sub-
jects were self-reported native speakers of English; they were recruited by postings to
newsgroups and mailing lists. Participation was voluntary and unpaid.
WebExp collects by-item response time data; subjects whose response times were
very short or very long were excluded from the sample, as they are unlikely to have
completed the experiment adequately. We also excluded the data of subjects who had
participated more than once in the same experiment, based on their demographic data
and on their Internet connection data, which is logged by WebExp.
3.2.2 Results and Discussion. The experimental data were normalized by dividing
each numerical judgment by the modulus value that the subject had assigned to the
reference sentence. This operation creates a common scale for all subjects. Then the
data were transformed by taking the decadic logarithm. This transformation ensures
that the judgments are normally distributed and is standard practice for magnitude
estimation data (Bard, Robertson, and Sorace 1996; Cowart 1997; Stevens 1975). All
further analyses were conducted on the normalized, log-transformed judgments.
Table 10 shows the descriptive statistics for all six judgment experiments: the
original experiments by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and
McDonald (2001) for adjective-noun bigrams, and our new ones for noun-noun and
verb-object bigrams.
We used correlation analysis to compare corpus counts and Web counts with plau-
sibility judgments. Table 11 (top half) lists the correlation coefficients that were ob-
tained when correlating log-transformed Web counts (AltaVista and Google) and cor-
pus counts (BNC and NANTC) with mean plausibility judgments for seen adjective-
noun, noun-noun, and verb-object bigrams. The results show that both AltaVista and
Google counts correlate well with plausibility judgments for seen bigrams. The corre-
lation coefficient for AltaVista ranges from .641 to .700; for Google, it ranges from .624
to .692. The correlations for the two search engines are very similar, which is also what
we found in Section 3.1 for the correlations between Web counts and corpus counts.
Note that the Web counts consistently achieve a higher correlation with the judg-
ments than the BNC counts, which range from .488 to .569. We carried out a series
of one-tailed t-tests to determine whether the differences between the correlation co-
efficients for the Web counts and the correlation coefficients for the BNC counts were
significant. For the adjective-noun bigrams, the AltaVista coefficient was significantly
473
Keller and Lapata Web Frequencies for Unseen Bigrams
higher than the BNC coefficient (t(87) = 1.76, p < .05), whereas the difference between
the Google coefficient and the BNC coefficient failed to reach significance. For the
noun-noun bigrams, both the AltaVista and the Google coefficients were significantly
higher than the BNC coefficient (t(87) = 3.11, p < .01 and t(87) = 2.95, p < .01).
Also, for the verb-object bigrams, both the AltaVista coefficient and the Google coef-
ficient were significantly higher than the BNC coefficient (t(87) = 2.64, p < .01 and
t(87) = 2.32, p < .05).
A similar picture was observed for the NANTC counts. Again, the Web counts
outperformed the corpus counts in predicting plausibility. For the adjective-noun bi-
grams, both the AltaVista and the Google coefficient were significantly higher than the
NANTC coefficient (t(87) = 1.97, p < .05; t(87) = 1.81, p < .05). For the noun-noun bi-
grams, the AltaVista coefficient was higher than the NANTC coefficient (t(87) = 1.64,
p < .05), but the Google coefficient was not significantly different from the NANTC co-
efficient. For verb-object bigrams, the difference was significant for both search engines
(t(87) = 2.74, p < .01; t(87) = 2.38, p < .01).
In sum, for all three types of bigrams, the correlation coefficients achieved with
AltaVista were significantly higher than the ones achieved by either the BNC or the
NANTC. Google counts outperformed corpus counts for all bigrams with the exception
of adjective-noun counts from the BNC and noun-noun counts from the NANTC.
The bottom panel of Table 11 shows the correlation coefficients obtained by com-
paring log-transformed judgments with log-transformed Web counts for unseen adjec-
tive-noun, noun-noun, and verb-object bigrams. We observe that the Web counts con-
sistently show a significant correlation with the judgments, with the coefficient rang-
ing from .480 to .578 for AltaVista counts and from .473 to .595 for the Google counts.
Table 11 also provides the correlations between plausibility judgments and counts
re-created using class-based smoothing, which we will discuss in Section 3.3.
An important question is how well humans agree when judging the plausibility of
adjective-noun, noun-noun, and verb-noun bigrams. Intersubject agreement gives an
upper bound for the task and allows us to interpret how well our Web-based method
performs in relation to humans. To calculate intersubject agreement we used leave-
Table 11
Correlation of plausibility judgments with Web counts, corpus counts, and counts re-created
using class-based smoothing. ?Agreement? refers to the intersubject agreement on the
judgment task.
Adjective-Noun Noun-Noun Verb-Object
Seen Bigrams
AltaVista .650** .700** .641**
Google .641** .692** .624**
BNC .569** .517** .488**
NANTC .526** .597** .491**
Smoothing .329** .318** .223*
Agreement .630** .641** .604**
Unseen Bigrams
AltaVista .480** .578** .551**
Google .473** .595** .520**
Smoothing .342** .372** .298**
Agreement .550** .570** .640**
*p < .05 (one-tailed). **p < .01 (one-tailed).
474
Computational Linguistics Volume 29, Number 3
one-out resampling. This technique is a special case of n-fold cross-validation (Weiss
and Kulikowski 1991) and has been previously used for measuring how well humans
agree in judging semantic similarity (Resnik 1999, 2000).
For each subject group, we divided the set of the subjects? responses with size n
into a set of size n? 1 (i.e., the response data of all but one subject) and a set of size 1
(i.e., the response data of a single subject). We then correlated the mean ratings of the
former set with the ratings of the latter. This was repeated n times (see the number
of participants in Table 6); the mean of the correlation coefficients for the seen and
unseen bigrams is shown in Table 11 in the rows labeled ?Agreement.?
For both seen and unseen bigrams, we found no significant difference between the
upper bound (intersubject agreement) and the correlation coefficients obtained using
either AltaVista or Google counts. This finding holds for all three types of bigrams. The
same picture emerged for the BNC and NANTC counts: These correlation coefficients
were not significantly different from the upper limit, for all three types of bigrams,
both for seen and for unseen bigrams.
To conclude, our evaluation demonstrated that Web counts reliably predict human
plausibility judgments, both for seen and for unseen predicate-argument bigrams.
AltaVista counts for seen bigrams are a better predictor of human judgments than
BNC and NANTC counts. These results show that our heuristic method yields valid
frequencies; the simplifications we made in obtaining the Web counts (see Section 2.3),
as well as the fact that Web data are noisy (see Section 2.4), seem to be outweighed
by the fact that the Web is up to a thousand times larger than the BNC.
3.3 Evaluation against Class-Based Smoothing
The evaluation in the last two sections established that Web counts are useful for
approximating corpus counts and for predicting plausibility judgments. As a further
step in our evaluation, we correlated Web counts with counts re-created by applying
a class-based smoothing method to the BNC.
We re-created co-occurrence frequencies for predicate-argument bigrams using a
simplified version of Resnik?s (1993) selectional association measure proposed by La-
pata, Keller, and McDonald (2001). In a nutshell, this measure replaces Resnik?s (1993)
information-theoretic approach with a simpler measure that makes no assumptions
with respect to the contribution of a semantic class to the total quantity of information
provided by the predicate about the semantic classes of its argument. It simply substi-
tutes the argument occurring in the predicate-argument bigram with the concept by
which it is represented in the WordNet taxonomy. Predicate-argument co-occurrence
frequency is estimated by counting the number of times the concept corresponding
to the argument is observed to co-occur with the predicate in the corpus. Because
a given word is not always represented by a single class in the taxonomy (i.e., the
argument co-occurring with a predicate can generally be the realization of one of
several conceptual classes), Lapata, Keller, and McDonald (2001) constructed the fre-
quency counts for a predicate-argument bigram for each conceptual class by dividing
the contribution from the argument by the number of classes to which it belongs.
They demonstrate that the counts re-created using this smoothing technique correlate
significantly with plausibility judgments for adjective-noun bigrams. They also show
that this class-based approach outperforms distance-weighted averaging (Dagan, Lee,
and Pereira 1999), a smoothing method that re-creates unseen word co-occurrences on
the basis of distributional similarity (without relying on a predefined taxonomy), in
predicting plausibility.
In the current study, we used the smoothing technique of Lapata, Keller, and
McDonald (2001) to re-create not only adjective-noun bigrams, but also noun-noun
475
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 12
Correlation of counts re-created using class-based smoothing with Web counts.
Adjective-Noun Noun-Noun Verb-Object
Seen Bigrams
AltaVista .344** .362** .361**
Google .330** .343** .349**
Unseen Bigrams
AltaVista .439** .386** .412**
Google .444** .421** .397**
*p < .05 (one-tailed). **p < .01 (one-tailed).
and verb-object bigrams. As already mentioned in Section 2.1, it was assumed that the
noun is the predicate in adjective-noun bigrams; for noun-noun bigrams, we treated
the right noun as the predicate, and for verb-object bigrams, we treated the verb as the
predicate. We applied Lapata, Keller, and McDonald?s (2001) technique to the unseen
bigrams for all three bigram types. We also used it on the seen bigrams, which we
were able to treat as unseen by removing all instances of the bigrams from the training
corpus.
To test the claim that Web frequencies can be used to overcome data sparseness,
we correlated the frequencies re-created using class-based smoothing on the BNC with
the frequencies obtained from the Web. The correlation coefficients for both seen and
unseen bigrams are shown in Table 12. In all cases, a significant correlation between
Web counts and re-created counts is obtained. For seen bigrams, the correlation coef-
ficient ranged from .344 to .362 for AltaVista counts and from .330 to .349 for Google
counts. For unseen bigrams, the correlations were somewhat higher, ranging from .386
to .439 for AltaVista counts and from .397 to .444 for Google counts. For both seen
and unseen bigrams, there was only a very small difference between the correlation
coefficients obtained with the two search engines.
It is also interesting to compare the performance of class-based smoothing and Web
counts on the task of predicting plausibility judgments. The correlation coefficients are
listed in Table 11. The re-created frequencies are correlated significantly with all three
types of bigrams, both for seen and unseen bigrams. For the seen bigrams, we found
that the correlation coefficients obtained using smoothed counts were significantly
lower than the upper bound for all three types of bigrams (t(87) = 3.01, p < .01;
t(87) = 3.23, p < .01; t(87) = 3.43, p < .01). This result also held for the unseen
bigrams: The correlations obtained using smoothing were significantly lower than the
upper bound for all three types of bigrams (t(87) = 1.86, p < .05; t(87) = 1.97, p < .05;
t(87) = 3.36, p < .01).
Recall that the correlation coefficients obtained using the Web counts were not
found to be significantly different from the upper bound, which indicates that Web
counts are better predictors of plausibility than smoothed counts. This fact was con-
firmed by further significance testing: For seen bigrams, we found that the AltaVista
correlation coefficients were significantly higher than correlation coefficients obtained
using smoothing, for all three types of bigrams (t(87) = 3.31, p < .01; t(87) = 4.11,
p < .01; t(87) = 4.32, p < .01). This also held for Google counts (t(87) = 3.16, p < .01;
t(87) = 4.02, p < .01; t(87) = 4.03, p < .01). For unseen bigrams, the AltaVista coef-
ficients and the coefficients obtained using smoothing were not significantly different
476
Computational Linguistics Volume 29, Number 3
for adjective-noun bigrams, but the difference reached significance for noun-noun and
verb-object bigrams (t(87) = 2.08, p < .05; t(87) = 2.53, p < .01). For Google counts,
the difference was again not significant for adjective-noun bigrams, but it reached sig-
nificance for noun-noun and verb-object bigrams (t(87) = 2.34, p < .05; t(87) = 2.15,
p < .05).
Finally, we conducted a small study to investigate the validity of the counts that
were re-created using class-based smoothing. We correlated the re-created counts for
the seen bigrams with their actual BNC and NANTC frequencies. The correlation
coefficients are reported in Tables 8 and 9. We found that the correlation between re-
created counts and corpus counts was significant for all three types of bigrams, for
both corpora. This demonstrates that the smoothing technique we employed generates
realistic corpus counts, in the sense that the re-created counts are correlated with
the actual counts. However, the correlation coefficients obtained using Web counts
were always substantially higher than those obtained using smoothed counts. These
differences were significant for the BNC counts for AltaVista (t(87) = 8.38, p < .01;
t(87) = 5.00, p < .01; t(87) = 5.03, p < .01) and Google (t(87) = 8.35, p < .01;
t(87) = 5.00, p < .01; t(87) = 5.03, p < .01). They were also significant for the NANTC
counts for AltaVista (t(87) = 4.12, p < .01; t(87) = 3.72, p < .01; t(87) = 6.58, p < .01)
and Google (t(87) = 4.08, p < .01; t(87) = 3.06, p < .01; t(87) = 6.47, p < .01).
To summarize, the results presented in this section indicate that Web counts are
indeed a valid way of obtaining counts for bigrams that are unseen in a given corpus:
They correlate reliably with counts re-created using class-based smoothing. For seen
bigrams, we found that Web counts correlate with counts that were re-created using
smoothing techniques (after removing the seen bigrams from the training corpus). For
the task of predicting plausibility judgments, we were able to show that Web counts
outperform re-created counts, both for seen and for unseen bigrams. Finally, we found
that Web counts for seen bigrams correlate better than re-created counts with the real
corpus counts.
It is beyond the scope of the present study to undertake a full comparison between
Web counts and frequencies re-created using all available smoothing techniques (and
all available taxonomies that might be used for class-based smoothing). The smooth-
ing method discussed above is simply one type of class-based smoothing. Other, more
sophisticated class-based methods do away with the simplifying assumption that the
argument co-occurring with a given predicate (adjective, noun, verb) is distributed
evenly across its conceptual classes and attempt to find the right level of generalization
in a concept hierarchy, by discounting, for example, the contribution of very general
classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing ap-
proaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman
and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word com-
binations by exploiting only corpus-internal evidence, without relying on taxonomic
information. Our goal was to demonstrate that frequencies retrieved from the Web
are a viable alternative to conventional smoothing methods when data are sparse; we
do not claim that our Web-based method is necessarily superior to smoothing or that
it should be generally preferred over smoothing methods. However, the next section
will present a small-scale study that compares the performance of several smoothing
techniques with the performance of Web counts on a standard task from the literature.
3.4 Pseudodisambiguation
In the smoothing literature, re-created frequencies are typically evaluated using pseu-
dodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999;
Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al 1999).
477
Keller and Lapata Web Frequencies for Unseen Bigrams
The aim of the pseudodisambiguation task is to decide whether a given algorithm
re-creates frequencies that make it possible to distinguish between seen and unseen
bigrams in a given corpus. A set of pseudobigrams is constructed according to a set
of criteria (detailed below) that ensure that they are unattested in the training corpus.
Then the seen bigrams are removed from the training data, and the smoothing method
is used to re-create the frequencies of both the seen bigrams and the pseudobigrams.
The smoothing method is then evaluated by comparing the frequencies it re-creates
for both types of bigrams.
We evaluated our Web counts by applying the pseudodisambiguation procedure
that Rooth et al (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2001)
employed for evaluating re-created verb-object bigram counts. In this procedure, the
noun n from a verb-object bigram (v, n) that is seen in a given corpus is paired with a
randomly chosen verb v? that does not take n as its object within the corpus. This results
in an unseen verb-object bigram (v?, n). The seen bigram is now treated as unseen (i.e.,
all of its occurrences are removed from the training corpus), and the frequencies of
both the seen and the unseen bigram are re-created (using smoothing, or Web counts,
in our case). The task is then to decide which of the two verbs v and v? takes the
noun n as its object. For this, the re-created bigram frequency is used: The bigram
with the higher re-created frequency (or probability) is taken to be the seen bigram.
If this bigram is really the seen one, then the disambiguation is correct. The overall
percentage of correct disambiguations is a measure of the quality of the re-created
frequencies (or probabilities). In the following, we will first describe in some detail
the experiments that Rooth et al (1999) and Clark and Weir (2001) conducted. We
will then discuss how we replicated their experiments using the Web as an alternative
smoothing method.
Rooth et al (1999) used pseudodisambiguation to evaluate a class-based model
that is derived from unlabeled data using the expectation maximization (EM) algo-
rithm. From a data set of 1,280,712 (v, n) pairs (obtained from the BNC using Carroll
and Rooth?s [1998] parser), they randomly selected 3,000 pairs, with each pair con-
taining a fairly frequent verb and noun (only verbs and nouns that occurred between
30 and 3,000 times in the data were considered). For each pair (v, n) a fairly frequent
verb v? was randomly chosen such that the pair (v?, n) did not occur in the data set.
Given the set of (v, n, v?) triples (a total of 1,337), the task was to decide whether (v, n)
or (v?, n) was the correct (i.e., seen) pair by comparing the probabilities P(n|v) and
P(n|v?). The probabilities were re-created using Rooth et al?s (1999) EM-based clus-
tering model on a training set from which all seen pairs (v, n) had been removed. An
accuracy of 80% on the pseudodisambiguation task was achieved (see Table 13).
Prescher, Riezler, and Rooth (2000) evaluated Rooth et al?s (1999) EM-based clus-
tering model again using pseudodisambiguation, but on a separate data set using a
Table 13
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using EM-based clustering (Rooth et al 1999).
Data Set N AltaVista AltaVista Rooth et al
Conditional Probability Joint Probability
Subject 717 71.2 68.5 ?
Objects 620 85.2 77.5 ?
Subjects and objects 1,337 77.7 72.7 80.0
478
Computational Linguistics Volume 29, Number 3
Table 14
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using EM-based clustering (Prescher, Riezler, and Rooth 2000).
Data Set N AltaVista AltaVista VA Model VO Model
Conditional Probability Joint Probability
Subjects 159 66.7 59.1 ? ?
Objects 139 70.5 66.2 ? ?
Subjects and objects 298 68.5 62.4 79.0 88.3
slightly different method for constructing the pseudobigrams. They used a set of 298
(v, n, n?) BNC triples in which (v, n) was chosen as in Rooth et al (1999) but paired with
a randomly chosen noun n?. Given the set of (v, n, n?) triples, the task was to decide
whether (v, n) or (v, n?) was the correct pair in each triple. Prescher, Riezler, and Rooth
(2000) reported pseudodisambiguation results with two clustering models: (1) Rooth
et al?s (1999) clustering approach, which models the semantic fit between a verb and
its argument (VA model), and (2) a refined version of this approach that models only
the fit between a verb and its object (VO model), disregarding other arguments of the
verb. The results of the two models on the pseudodisambiguation task are shown in
Table 14.
At this point, it is important to note that neither Rooth et al (1999) nor Prescher,
Riezler, and Rooth (2000) used pseudodisambiguation for the final evaluation of their
models. Rather, the performance on the pseudodisambiguation task was used to op-
timize the model parameters. The results in Tables 13 and 14 show the pseudodisam-
biguation performance achieved for the best parameter settings. In other words, these
results were obtained on the development set (i.e., on the same data set that was used
to optimize the parameters), not on a completely unseen test set. This procedure is
well-justified in the context of Rooth et al?s (1999) and Prescher, Riezler, and Rooth?s
(2000) work, which aimed at building models of lexical semantics, not of pseudodis-
ambiguation. Therefore, they carried out their final evaluations on unseen test sets for
the tasks of lexicon induction (Rooth et al 1999) and target language disambiguation
(Prescher, Riezler, and Rooth 2000), once the model parameters had been fixed using
the pseudodisambiguation development set.8
Clark and Weir (2002) use a setting similar to that of Rooth et al (1999) and
Prescher, Riezler, and Rooth (2000); here pseudodisambiguation is employed to evalu-
ate the performance of a class-based probability estimation method. In order to address
the problem of estimating conditional probabilities in the face of sparse data, Clark
and Weir (2002) define probabilities in terms of classes in a semantic hierarchy and
propose hypothesis testing as a means of determining a suitable level of generalization
in the hierarchy. Clark and Weir (2002) report pseudodisambiguation results on two
data sets, with an experimental setup similar to that of Rooth et al (1999). For the first
data set, 3,000 pairs were randomly chosen from 1.3 million (v, n) tuples extracted
from the BNC (using the parser of Briscoe and Carroll [1997]). The selected pairs con-
8 Stefan Riezler (personal communication, 2003) points out that the main variance in Rooth et al?s (1999)
pseudodisambiguation results comes from the class cardinality parameter (start values account for only
2% of the performance, and iterations do not seem to make a difference at all). Figure 3 of Rooth et al
(1999) shows that a performance of more than 75% is obtained for every reasonable choice of classes.
This indicates that a ?proper? pseudodisambiguation setting with separate development and test data
would have resulted in a similar choice of class cardinality and thus achieved the same 80%
performance that is cited in Table 13.
479
Keller and Lapata Web Frequencies for Unseen Bigrams
Table 15
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using class-based smoothing (Clark and Weir 2002).
Data Set N AltaVista AltaVista Clark and Li and Resnik
Conditional Joint Probability Weir Abe
Probability
Objects (low frequency) 3000 83.9 81.1 72.4 62.9 62.6
Objects (high frequency) 3000 87.7 85.3 73.9 68.3 63.9
tained relatively frequent verbs (occurring between 500 and 5,000 times in the data).
The data sets were constructed as proposed by Rooth et al (1999). The procedure for
creating the second data set was identical, but this time only verbs that occurred be-
tween 100 and 1,000 times were considered. Clark and Weir (2002) further compared
their approach with Resnik?s (1993) selectional association model and Li and Abe?s
(1998) tree cut model on the same data sets. These methods are directly comparable,
as they can be used for class-based probability estimation and address the question
of how to find a suitable level of generalization in a hierarchy (i.e., WordNet). The
results of the three methods used on the two data sets are shown in Table 15.
We employed the same pseudodisambiguation method to test whether Web-based
frequencies can be used for distinguishing between seen and artificially constructed
unseen bigrams. We obtained the data sets of Rooth et al (1999), Prescher, Riezler,
and Rooth (2000), and Clark and Weir (2002) described above. Given a set of (v, n, v?)
triples, the task was to decide whether (v, n) or (v?, n) was the correct pair. We obtained
AltaVista counts for f (v, n), f (v?, n), f (v), and f (v?) as described in Section 2.3.9 Then
we used two models for pseudodisambiguation: the joint probability model compared
the joint probability estimates f (v, n) and f (v?, n) and predicted that the bigram with
the highest estimate is the seen one. The conditional probability model compared the
conditional probability estimates f (v, n)/f (v) and f (v?, n)/f (v?) and again selected as
the seen bigram the one with the highest estimate (in both cases, ties were resolved
by choosing at random).10 The same two models were used to perform pseudodisam-
biguation for the (v, n, n?) triples, where we have to choose between (v, n) and (v, n?).
Here, the probability estimates f (v, n) and f (v, n?) were used for the joint probability
model, and f (v, n)/f (n) and f (v, n?)/f (n?) for the conditional probability model.
The results for Rooth et al?s (1999) data set are given in Table 13. The conditional
probability model achieves a performance of 71.2% correct for subjects and 85.2% cor-
rect for objects. The performance on the whole data set is 77.7%, which is below the
performance of 80.0% reported by Rooth et al (1999). However, the difference is not
found to be significant using a chi-square test comparing the number of correct and
incorrect classifications (?2(1) = 2.02, p = .16). The joint probability model performs
consistently worse than the conditional probability model: It achieves an overall accu-
racy of 72.7%, which is significantly lower than the accuracy of the Rooth et al (1999)
model (?2(1) = 19.50, p < .01).
9 We used only AltaVista counts, as there was virtually no difference between AltaVista and Google
counts in our previous evaluations (see Sections 3.1?3.3). Google allows only 1,000 queries per day (for
registered users), which makes it time-consuming to obtain large numbers of Google counts. AltaVista
has no such restriction.
10 The probability estimates are P(a, b) = f (a, b)/N and P(a|b) = f (a, b)/f (a) for the joint probability and
the conditional probability, respectively. However, the corpus size N can be ignored, as it is constant.
480
Computational Linguistics Volume 29, Number 3
A similar picture emerges with regard to Prescher, Riezler, and Rooth?s (2000) data
set (see Table 14). The conditional probability model achieves an accuracy of 66.7%
for subjects and 70.5% for objects. The combined performance of 68.5% is significantly
lower than the performance of both the VA model (?2(1) = 7.78, p < .01) and the
VO model (?2(1) = 33.28, p < .01) reported by Prescher, Riezler, and Rooth (2000).
Again, the joint probability model performs worse than the conditional probability
model, achieving an overall accuracy of 62.4%.
We also applied our Web-based method to the pseudodisambiguation data set of
Clark and Weir (2002). Here, the conditional probability model reached a performance
of 83.9% correct on the low-frequency data set. This is significantly higher than the
highest performance of 72.4% reported by Clark and Weir (2002) on the same data
set (?2(1) = 115.50, p < .01). The joint probability model performs worse than the
conditional model, at 81.1%. However, this is still significantly better than the best
result of Clark and Weir (2002) (?2(1) = 63.14, p < .01). The same pattern is observed
for the high-frequency data set, on which the conditional probability model achieves
87.7% correct and thus significantly outperforms Clark and Weir (2002), who obtained
73.9% (?2(1) = 283.73, p < .01). The joint probability model achieved 85.3% on this data
set, also significantly outperforming Clark and Weir (2002) (?2(1) = 119.35, p < .01).
To summarize, we demonstrated that the simple Web-based approach proposed
in this article yields results for pseudodisambiguation that outperform class-based
smoothing techniques, such as the ones proposed by Resnik (1993), Li and Abe (1998),
and Clark and Weir (2002). We were also able to show that a Web-based approach is
able to achieve the same performance as an EM-based smoothing model proposed by
Rooth et al (1999). However, the Web-based approach was not able to outperform the
more sophisticated EM-based model of Prescher, Riezler, and Rooth (2000). Another
result we obtained is that Web-based models that use conditional probabilities (where
unigram frequencies are used to normalize the bigram frequencies) generally outper-
form a more simple-minded approach that relies directly on bigram frequencies for
pseudodisambiguation.
There are a number of reasons why our results regarding pseudodisambiguation
have to be treated with some caution. First of all, the two smoothing methods (i.e.,
EM-based clustering and class-based probability estimation using WordNet) were not
evaluated on the same data set, and therefore the two results are not directly compa-
rable. For instance, Clark and Weir?s (2002) data set is substantially less noisy than
Rooth et al?s (1999) and Prescher, Riezler, and Rooth?s (2000), as it contains only words
and nouns that occur in WordNet. Furthermore, Stephen Clark (personal communica-
tion, 2003) points out that WordNet-based approaches are at a disadvantage when it
comes to pseudodisambiguation. Pseudodisambiguation assumes that the correct pair
is unseen in the training data; this makes the task deliberately hard, because some of
the pairs might be frequent enough that reliable corpus counts can be obtained with-
out having to use WordNet (using WordNet is likely to be more noisy than using the
actual counts). Another problem with WordNet-based approaches is that they offer
no systematic treatment of word sense ambiguity, which puts them at a disadvan-
tage with respect to approaches that do not rely on a predefined inventory of word
senses.
Finally, recall that the results for the EM-based approaches in Tables 13 and 14 were
obtained on the development set (as pseudodisambiguation was used as a means of
parameter tuning by Rooth et al [1999] and Prescher, Riezler, and Rooth [2000]). It is
possible that this fact inflates the performance values for the EM-based approaches
(but see note 8).
481
Keller and Lapata Web Frequencies for Unseen Bigrams
4. Conclusions
This article explored a novel approach to overcoming data sparseness. If a bigram
is unseen in a given corpus, conventional approaches re-create its frequency using
techniques such as back-off, linear interpolation, class-based smoothing or distance-
weighted averaging (see Dagan, Lee, and Pereira [1999] and Lee [1999] for overviews).
The approach proposed here does not re-create the missing counts but instead retrieves
them from a corpus that is much larger (but also much more noisy) than any existing
corpus: it launches queries to a search engine in order to determine how often the
bigram occurs on the Web.
We systematically investigated the validity of this approach by using it to obtain
frequencies for predicate-argument bigrams (adjective-noun, noun-noun, and verb-
object bigrams). We first applied the approach to seen bigrams randomly sampled from
the BNC. We found that the counts obtained from the Web are highly correlated with
the counts obtained from the BNC. We then obtained bigram counts from NANTC,
a corpus that is substantially larger than the BNC. Again, we found that Web counts
are highly correlated with corpus counts. This indicates that Web queries can generate
frequencies that are comparable to the ones obtained from a balanced, carefully edited
corpus such as the BNC, but also from a large news text corpus such as NANTC.
Secondly, we performed an evaluation that used the Web frequencies to predict
human plausibility judgments for predicate-argument bigrams. The results show that
Web counts correlate reliably with judgments, for all three types of predicate-argument
bigrams tested, both seen and unseen. For the seen bigrams, we showed that the Web
frequencies correlate better with judged plausibility than corpus frequencies.
To substantiate the claim that the Web counts can be used to overcome data sparse-
ness, we compared bigram counts obtained from the Web with bigram counts re-
created using a class-based smoothing technique (a variant of the one proposed by
Resnik [1993]). We found that Web frequencies and re-created frequencies are reliably
correlated, and that Web frequencies are better at predicting plausibility judgments
than smoothed frequencies. This holds both for unseen bigrams and for seen bigrams
that are treated as unseen by omitting them from the training corpus.
Finally, we tested the performance of our frequencies in a standard pseudodis-
ambiguation task. We applied our method to three data sets from the literature. The
results show that Web counts outperform counts re-created using a number of class-
based smoothing techniques. However, counts re-created using an EM-based smooth-
ing approach yielded better pseudodisambiguation performance than Web counts.
To summarize, we have proposed a simple heuristic method for obtaining bigram
counts from the Web. Using four different types of evaluation, we demonstrated that
this simple heuristic method is sufficient to obtain valid frequency estimates. It seems
that the large amount of data available outweighs the problems associated with using
the Web as a corpus (such as the fact that it is noisy and unbalanced).
A number of questions arise for future research: (1) Are Web frequencies suitable
for probabilistic modeling, in particular since Web counts are not perfectly normalized,
as Zhu and Rosenfeld (2001) have shown? (2) How can existing smoothing methods
benefit from Web counts? (3) How do the results reported in this article carry over
to languages other than English (for which a much smaller amount of Web data are
available)? (4) What is the effect of the noise introduced by our heuristic approach?
The last question could be assessed by reproducing our results using a snapshot of
the Web, from which argument relations can be extracted more accurately using POS
tagging and chunking techniques.
482
Computational Linguistics Volume 29, Number 3
Finally, it will be crucial to test the usefulness of Web-based frequencies for realistic
NLP tasks. Preliminary results are reported by Lapata and Keller (2003), who use Web
counts successfully for a range of NLP tasks, including candidate selection for ma-
chine translation, context-sensitive spelling correction, bracketing and interpretation
of compounds, adjective ordering, and PP attachment.
Acknowledgments
This work was conducted while both
authors were at the Department of
Computational Linguistics, Saarland
University, Saarbru?cken. The work was
inspired by a talk that Gregory Grefenstette
gave in Saarbru?cken in 2001 about his
research on using the Web as a corpus. The
present article is an extended and revised
version of Keller, Lapata, and Ourioupina
(2002). Stephen Clark and Stefan Riezler
provided valuable comments on this
research. We are also grateful to four
anonymous reviewers for Computational
Linguistics; their feedback helped to
substantially improve the present article.
Special thanks are due to Stephen Clark and
Detlef Prescher for making their
pseudodisambiguation data sets available.
References
Abney, Steve. 1996. Partial parsing via
finite-state cascades. In John Carroll,
editor, Workshop on Robust Parsing Eighth
European Summer School in Logic, Language
and Information, pages 8?15, Prague, Czech
Republic.
Abney, Steve and Marc Light. 1999. Hiding
a semantic class hierarchy in a Markov
model. In Andrew Kehler and Andreas
Stolcke, editors, Proceedings of the ACL
Workshop on Unsupervised Learning in
Natural Language Processing, pages 1?8,
College Park, MD.
Agirre, Eneko and David Martinez. 2000.
Exploring automatic word sense
disambiguation with decision lists and the
Web. In Proceedings of the 18th International
Conference on Computational Linguistics,
pages 11?19, Saarbru?cken, Germany.
Banko, Michele and Eric Brill. 2001a.
Mitigating the paucity-of-data problem:
Exploring the effect of training corpus
size on classifier performance for natural
language processing. In James Allan,
editor, Proceedings of the First International
Conference on Human Language Technology
Research. Morgan Kaufmann, San
Francisco.
Banko, Michele and Eric Brill. 2001b. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics and the
10th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 26?33, Toulouse, France.
Bard, Ellen Gurman, Dan Robertson, and
Antonella Sorace. 1996. Magnitude
estimation of linguistic acceptability.
Language, 72(1):32?68.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC.
Burnard, Lou, editor, 1995. Users Reference
Guide, British National Corpus. British
National Corpus Consortium, Oxford
University Computing Services, Oxford,
England.
Carroll, Glenn and Mats Rooth. 1998.
Valence induction with a head-lexicalized
PCFG. In Nancy Ide and Atro Voutilainen,
editors, Proceedings of the Third Conference
on Empirical Methods in Natural Language
Processing, pages 36?45, Granada, Spain.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
Second Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 95?102, Pittsburgh, PA.
Clark, Stephen and David Weir. 2002.
Class-based probability estimation using a
semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Corley, Martin and Christoph Scheepers.
2002. Syntactic priming in English
sentence production: Categorical and
latency evidence from an Internet-based
study. Psychonomic Bulletin and Review,
9(1):126?131.
Corley, Steffan, Martin Corley, Frank Keller,
Matthew W. Crocker, and Shari Trewin.
2001. Finding syntactic structure in
unparsed corpora: The Gsearch corpus
query system. Computers and the
Humanities, 35(2):81?94.
Cowart, Wayne. 1997. Experimental Syntax:
Applying Objective Methods to Sentence
Judgments. Sage, Thousand Oaks, CA.
Curran, James. 2002. Scaling context space.
In Proceedings of the 40th Annual Meeting of
483
Keller and Lapata Web Frequencies for Unseen Bigrams
the Association for Computational Linguistics,
pages 231?238, Philadelphia.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1999. Similarity-based models of
word cooccurrence probabilities. Machine
Learning, 34(1):43?69.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic, Boston.
Grefenstette, Gregory. 1998. The World
Wide Web as a resource for
example-based machine translation tasks.
In Proceedings of the ASLIB Conference on
Translating and the Computer, London.
Grefenstette, Gregory and Jean Nioche.
2000. Estimation of English and
non-English language use on the WWW.
In Proceedings of the RIAO Conference on
Content-Based Multimedia Information
Access, pages 237?246, Paris.
Grishman, Ralph and John Sterling. 1994.
Generalizing automatically generated
selectional patterns. In Proceedings of the
15th International Conference on
Computational Linguistics, pages 742?747,
Kyoto, Japan.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Jones, Rosie and Rayid Ghani. 2000.
Automatically building a corpus for a
minority language from the Web. In
Proceedings of the Student Research Workshop
at the 38th Annual Meeting of the Association
for Computational Linguistics, pages 29?36,
Hong Kong.
Karp, Daniel, Yves Schabes, Martin Zaidel,
and Dania Egedi. 1992. A freely available
wide coverage morphological analyzer for
English. In Proceedings of the 14th
International Conference on Computational
Linguistics, pages 950?954, Nantes, France.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 33(3):400?401.
Keller, Frank and Theodora Alexopoulou.
2001. Phonology competes with syntax:
Experimental evidence for the interaction
of word order and accent placement in
the realization of information structure.
Cognition, 79(3):301?372.
Keller, Frank and Ash Asudeh. 2001.
Constraints on linguistic coreference:
Structural vs. pragmatic factors. In
Johanna D. Moore and Keith Stenning,
editors, Proceedings of the 23rd Annual
Conference of the Cognitive Science Society,
pages 483?488. Erlbaum, Mahwah, NJ.
Keller, Frank, Martin Corley, Steffan Corley,
Lars Konieczny, and Amalia Todirascu.
1998. WebExp: A Java toolbox for
Web-based psychological experiments.
Technical Report HCRC/TR-99, Human
Communication Research Centre,
University of Edinburgh.
Keller, Frank, Maria Lapata, and Olga
Ourioupina. 2002. Using the Web to
overcome data sparseness. In Jan Hajic?
and Yuji Matsumoto, editors, Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages
230?237, Philadelphia.
Lapata, Maria. 2001. A corpus-based
account of regular polysemy: The case of
context-sensitive adjectives. In Proceedings
of the Second Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 63?70,
Pittsburgh, PA.
Lapata, Maria. 2002. The disambiguation of
nominalizations. Computational Linguistics,
28(3):357?388.
Lapata, Maria and Frank Keller. 2003.
Evaluating the performance of
unsupervised Web-based models for a
range of NLP tasks. Unpublished
manuscript, University of Sheffield,
Sheffield, England, and University of
Edinburgh, Edinburgh, Scotland.
Lapata, Maria, Frank Keller, and Scott
McDonald. 2001. Evaluating smoothing
algorithms against plausibility judgments.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
and the 10th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 346?353, Toulouse,
France.
Lapata, Maria, Scott McDonald, and Frank
Keller. 1999. Determinants of
adjective-noun plausibility. In Proceedings
of the Ninth Conference of the European
Chapter of the Association for Computational
Linguistics, pages 30?36, Bergen, Norway.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on
Compound Nouns. Ph.D. thesis, Macquarie
University, Sydney, Australia.
Lee, Lilian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 25?32, College Park,
MD.
Leech, Geoffrey, Roger Garside, and Michael
Bryant. 1994. The tagging of the British
national corpus. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 622?628, Kyoto, Japan.
Levi, Judith N. 1978. The Syntax and
Semantics of Complex Nominals. Academic
484
Computational Linguistics Volume 29, Number 3
Press, New York.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1994. PRINCIPAR?An
efficient broad-coverage, principle-based
parser. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 482?488, Kyoto, Japan.
Lin, Dekang. 1998. Dependency-based
evaluation of MINIPAR. In Proceedings of
the LREC Workshop on the Evaluation of
Parsing Systems, pages 48?56, Granada,
Spain.
Lin, Dekang. 2001. LaTaT: Language and
text analysis tools. In Proceedings of the
First International Conference on Human
Language Technology Research. Morgan
Kaufmann, San Francisco.
Markert, Katja, Malvina Nissim, and
Natalia N. Modjeska. 2003. Using the Web
for nominal anaphora resolution. In
Proceedings of the EACL Workshop on the
Computational Treatment of Anaphora,
Budapest, pages 39?46.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal
participation in role switching
alternations. In Proceedings of the First
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 256?263, Seattle, WA.
Mihalcea, Rada and Dan Moldovan. 1999. A
method for word sense disambiguation of
unrestricted text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 152?158,
College Park, MD.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine J. Miller. 1990. Introduction to
WordNet: An on-line lexical database.
International Journal of Lexicography,
3(4):235?244.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
Columbus, OH.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Prescher, Detlef, Stefan Riezler, and Mats
Rooth. 2000. Using a probabilistic
class-based lexicon for lexical ambiguity
resolution. In Proceedings of the 18th
International Conference on Computational
Linguistics, pages 649?655, Saarbru?cken,
Germany.
Ratnaparkhi, Adwait. 1998. Unsupervised
statistical models for prepositional phrase
attachment. In Proceedings of the
17th International Conference on
Computational Linguistics and 36th Annual
Meeting of the Association for Computational
Linguistics, pages 1079?1085, Montre?al.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Resnik, Philip. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 527?534,
College Park, MD.
Resnik, Philip. 2000. Measuring verb
similarity. In Lila R. Gleitman and
Aravid K. Joshi, editors, Proceedings of the
22nd Annual Conference of the Cognitive
Science Society, pages 399?404. Erlbaum,
Mahwah, NJ.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
College Park, MD.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Oxford University Press,
Oxford.
Schu?tze, Carson T. 1996. The Empirical Base of
Linguistics: Grammaticality Judgments and
Linguistic Methodology. University of
Chicago Press, Chicago.
Stevens, S. S. 1975. Psychophysics:
Introduction to its Perceptual, Neural, and
Social Prospects. John Wiley, New York.
Volk, Martin. 2001. Exploiting the WWW as
a corpus to resolve PP attachment
ambiguities. In Paul Rayson, Andrew
Wilson, Tony McEnery, Andrew Hardie,
and Shereen Khoja, editors, Proceedings of
the Corpus Linguistics Conference, pages
601?606, Lancaster, England.
Weiss, Sholom M. and Casimir A.
Kulikowski. 1991. Computer Systems That
Learn: Classification and Prediction Methods
from Statistics, Neural Nets, Machine
Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
Zhu, Xiaojin and Ronald Rosenfeld. 2001.
Improving trigram language modeling
with the World Wide Web. In Proceedings
of the International Conference on Acoustics
Speech and Signal Processing, Salt Lake City,
UT.
c? 2004 Association for Computational Linguistics
Verb Class Disambiguation Using
Informative Priors
Mirella Lapata? Chris Brew?
University of Sheffield Ohio State University
Levin?s (1993) study of verb classes is a widely used resource for lexical semantics. In her frame-
work, some verbs, such as give, exhibit no class ambiguity. But other verbs, such as write, have
several alternative classes. We extend Levin?s inventory to a simple statistical model of verb class
ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the
use of a disambiguated corpus. We additionally show that these preferences are useful as priors
for a verb sense disambiguator.
1. Introduction
Much research in lexical semantics has concentrated on the relation between verbs and
their arguments. Many scholars hypothesize that the behavior of a verb, particularly
with respect to the expression and interpretation of its arguments, is to a large extent
determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993;
Pinker 1989; Green 1974; Gropen et al 1989; Fillmore 1965). The correspondence be-
tween verbal meaning and syntax has been extensively studied in Levin (1993), which
argues that verbs which display the same diathesis alternations?alternations in the
realization of their argument structure?can be assumed to share certain meaning
components and to form a semantically coherent class.
The converse of this assumption is that verb behavior (i.e., participation in diathe-
sis alternations) can be used to provide clues about aspects of meaning, which in turn
can be exploited to characterize verb senses (referred to as classes in Levin?s [1993] ter-
minology). A major advantage of this approach is that criteria for assigning senses can
be more concrete than is traditionally assumed in lexicographic work (e.g., WordNet or
machine-readable dictionaries) concerned with sense distinctions (Palmer 2000). As an
example consider sentences (1)?(4), taken from Levin. Examples (1) and (2) illustrate
the dative and benefactive alternations, respectively. Dative verbs alternate between
the prepositional frame ?NP1 V NP2 to NP3? (see (1a)) and the double-object frame
?NP1 V NP2 NP3? (see (1b)), whereas benefactive verbs alternate between the double-
object frame (see (2a)) and the prepositional frame ?NP1 V NP2 for NP3? (see (2b)).
To decide whether a verb is benefactive or dative it suffices to test the acceptability
of the for and to frames. Verbs undergoing the conative alternation can be attested
either as transitive or as intransitive with a prepositional phrase headed by the word
at.1 The role filled by the object of the transitive variant is shared by the noun phrase
complement of at in the intransitive variant (see (3)). This example makes explicit that
class assignment depends not only on syntactic facts but also on judgments about
? Department of Computer Science, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK. E-mail:
mlap@dcs.shef.ac.uk.
? Department of Linguistics, Oxley Hall,1712 Neil Avenue, Columbus, OH. E-mail: cbrew@ling.ohio-
state.edu.
1 At is the most likely choice, but for some conative verbs the preposition is instead on or onto.
46
Computational Linguistics Volume 30, Number 1
semantic roles. Similarly, the possessor object alternation involves a possessor and a
possessed attribute that can be manifested either as the verbal object or as the object
of a prepositional phrase headed by for (see (4)).
(1) a. Bill sold a car to Tom.
b. Bill sold Tom a car.
(2) a. Martha carved the baby a toy.
b. Martha carved a toy for the baby.
(3) a. Paula hit the fence.
b. Paula hit at the fence.
(4) a. I admired his honesty.
b. I admired him for his honesty.
Observation of the semantic and syntactic behavior of pay and give reveals that
they pattern with sell in licensing the dative alternation. These verbs are all members
of the Give class. Verbs like make and build behave similarly to carve in licensing the
benefactive alternation and are members of the class of Build verbs. The verbs beat,
kick, and hit undergo the conative alternation; they are all members of the Hit verb
class. By grouping together verbs that pattern together with respect to diathesis alter-
nations, Levin (1993) defines approximately 200 verb classes, which she argues reflect
important semantic regularities. These analyses (and many similar ones by Levin and
her successors) rely primarily on straightforward syntactic and syntactico-semantic cri-
teria. To adopt this approach is to accept some limitations on the reach of our analyses,
since not all semantically interesting differences will have the appropriate reflexes in
syntax. Nevertheless, the emphasis on concretely available observables makes Levin?s
methodology a good candidate for automation (Palmer 2000).
Therefore, Levin?s (1993) classification has formed the basis for many efforts that
aim to acquire lexical semantic information from corpora. These exploit syntactic cues,
or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte
im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin?s classifi-
cation (in conjunction with other lexical resources) to create dictionaries that express
the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosen-
zweig, and Palmer 1997; Dorr and Jones 1996). Levin?s inventory of verbs and classes
has been also useful for applications such as machine translation (Dorr 1997; Palmer
and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin
2000), and document classification (Klavans and Kan 1998).
Although the classification provides a general framework for describing verbal
meaning, it says only which verb meanings are possible, staying silent on the relative
likelihoods of the different meanings. The inventory captures systematic regularities
in the meaning of words and phrases but falls short of providing a probabilistic model
of these regularities. Such a model would be useful in applications that need to resolve
ambiguity in the presence of multiple and conflicting probabilistic constraints.
More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists the
semantic classes and diathesis alternations. The mapping between verbs and classes
is not one to one. Of the 3,024 verbs which she covers, 784 are listed as having more
than one class. Even though Levin?s monosemous verbs outnumber her polysemous
verbs by a factor of nearly four to one, the total frequency of the former (4,252,715)
47
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 1
Polysemous verbs according to Levin.
Classes Verbs BNC frequency
1 2,239 4,252,715
2 536 2,325,982
3 173 738,854
4 43 395,212
5 23 222,747
6 7 272,669
7 2 26,123
10 1 4,427
Figure 1
Relation between number of classes and alternations.
is comparable to the total frequency of the latter (3,986,014). This means that close
to half of the cases processed by a semantic tagger would manifest some degree of
ambiguity. The frequencies are detailed in Table 1 and were compiled from a lemma-
tized version of the British National Corpus (BNC) (Burnard 1995). Furthermore, as
shown in Figure 1, the level of ambiguity increases in tandem with the number of
alternations licensed by a given verb. Consider, for example, verbs participating in
one alternation only: Of these, 90.4% have one semantic class, 8.6% have two classes,
0.7% have three classes, and 0.3% have four classes. In contrast, of the verbs licensing
six different alternations, 14% have one class, 17% have two classes, 12.4% have three
classes, 53.6% have four classes, 2% have six classes, and 1% have seven classes. As
ambiguity increases, so does the availability and potential utility of information about
diathesis alternations.
Palmer (2000) and Dang et al (1998) argue that syntactic frames and verb classes
are useful for developing principled classifications of verbs. We go beyond this, show-
ing that they can also be of assistance in disambiguation. Consider, for instance, the
verb serve, which is a member of four Levin classes: Give, Fit, Masquerade, and
Fulfilling. Each of these classes can in turn license four distinct syntactic frames.
48
Computational Linguistics Volume 30, Number 1
As shown in the examples2 below, in (5a) serve appears ditransitively and belongs to
the semantic class of Give verbs, in (5b) it occurs transitively and is a member of the
class of Fit verbs, in (5c) it takes the predicative complement as minister of the interior
and is a member of the class of Masquerade verbs. Finally, in sentence (5d) serve is
a Fulfilling verb and takes two complements, a noun phrase (an apprenticeship) and
a prepositional phrase headed by to (to a still-life photographer). In the case of verbs like
serve, we can guess their semantic class solely on the basis of the frame with which
they appear.
(5) a. I?m desperately trying to find a venue for the reception which can
serve our guests an authentic Italian meal. NP1 V NP2 NP3
b. The airline serves 164 destinations in over 75 countries. NP1 V NP2
c. Jean-Antoine Chaptal was a brilliant chemist and technocrat who
served Napoleon as minister of the interior from 1800 to 1805. NP1 V NP2
as NP3
d. Before her brief exposure to pop stardom, she served an
apprenticeship to a still-life photographer. NP1 V NP2 to NP3
But sometimes we do not have the syntactic information that would provide cues
for semantic disambiguation. Consider example (6). The verb write is a member of three
Levin classes, two of which (Message Transfer, Performance) take the double-
object frame. In this case, we have the choice between theMessage Transfer reading
(see (6a)) and the Performance reading (see (6b)). The same situation arises with the
verb toast, which is listed as a Prepare verb and a Judgment verb; both these classes
license the prepositional frame ?NP1 V NP2 for NP3.? In sentence (7a) the preferred
reading is that of Prepare rather than that of Judgment (see sentence (7b)). The verb
study is ambiguous among three classes when attested in the transitive frame: Learn
(see example (8a)), Sight (see example (8b)), and Assessment (see example (8c)). The
verb convey, when attested in the prepositional frame ?NP1 V NP2 to NP3,? can be
ambiguous between the Say class (see example (9a)) and the Send class (see exam-
ple (9b)). In order to correctly decide the semantic class for a given ambiguous verb,
we would need not only detailed semantic information about the verb?s arguments,
but also a considerable amount of world knowledge. Admittedly, selectional restric-
tions are sufficient for distinguishing (7a) from (7b) (one normally heats up inanimate
entities and salutes animate ones), but selectional restrictions alone are probably not
enough to disambiguate (6a) from (6b), since both letter and screenplay are likely to be
described as written material. Rather, we need fine-grained world knowledge: Both
scripts and letters can be written for someone: only letters can be written to someone.
(6) a. A solicitor wrote him a letter at the airport.
b. I want you to write me a screenplay called ?The Trip.?
(7) a. He sat by the fire and toasted a piece of bread for himself.
b. We all toasted Nigel for his recovery.
2 Unless otherwise stated, our example sentences were taken (possibly in simplified form) from the BNC.
49
Lapata and Brew Verb Class Disambiguation Using Informative Priors
(8) a. Chapman studied medicine at Cambridge.
b. Romanov studied the old man carefully, looking for some sign that he
knew exactly what had been awaiting him at the bank.
c. The alliance will also study the possibility of providing service to other
high-volume products, such as IBM and multi-vendor workstations.
(9) a. By conveying the news to her sister, she would convey by implication
something of her own anxiety.
b. The judge signed the committal warrant and the police conveyed Mr.
Butler to prison, giving the warrant to the governor.
This need for world knowledge (or at least a convenient way of approximating
this knowledge) is not an isolated phenomenon but manifests itself across a variety
of classes and frames (e.g., double object, transitive, prepositional frame: see exam-
ples (6)?(9)). We have argued that the concreteness of Levin-style verb classes is an
advantage, but this advantage would be compromised if we tried to fold too much
world knowledge into the classification. We do not do this. Instead, Section 5 of the
current article describes disambiguation experiments in which our probabilistic Levin
classes are used in tandem with proxies for appropriate world knowledge.
It is important to point out that Levin?s (1993) classification is not intended as an
exhaustive description of English verbs, their meanings, and their likelihood. Many
other classifications could have been built using the same principles. A different group-
ing might, for example, have occurred if finer or coarser semantic distinctions were
taken into account (see Merlo and Stevenson [2001] and Dang, Rosenzweig, and Palmer
[1997] for alternative classifications) or if the containment of ambiguity was one of the
classification objectives. As pointed out by Kipper, Dang, and Palmer (2000), Levin
classes exhibit inconsistencies, and verbs are listed in multiple classes, some of which
have conflicting sets of syntactic frames. This means that some ambiguities may also
arise as a result of accidental errors or inconsistencies. The classification was created
not with computational uses in mind, but for human readers, so it has not been nec-
essary to remedy all the errors and omissions that might cause trouble for machines.
Similar issues arise in almost all efforts to make use of preexisting lexical resources for
computational purposes (Briscoe and Carroll 1997), so none of the above comments
should be taken as criticisms of Levin?s achievement.
The objective of this article is to show how to train and use a probabilistic version
of Levin?s classification in verb sense disambiguation. We treat errors and inconsis-
tencies in the classification as noise. Although all our tests have used Levin?s classes
and the British National Corpus, the method itself depends neither on the details of
Levin?s classification nor on parochial facts about the English language. Our future
work will include tests on other languages, other classifications, and other corpora.
The model developed in this article takes as input a partially parsed corpus and
generates, for each combination of a verb and its syntactic frame, a probability distri-
bution over the available verb classes. The corpus itself does not have to be labeled
with classes. This makes it feasible to use large corpora. Our model is not immediately
useful for disambiguation, because it cannot discriminate among different occurrences
of the same verb and frame, but it can (as we show in Section 5) be used as a prior
in a full disambiguation system that does take appropriate account of context. The
model relies on several gross simplifications; it does not take selectional restrictions,
discourse, or pragmatic information into account but is demonstrably superior to sim-
pler priors that make no use of subcategorization.
50
Computational Linguistics Volume 30, Number 1
The remainder of this article is organized as follows. In Section 2 we describe the
probabilistic model and the estimation of the various model parameters. In Sections 3
and 4 we report on the results of two experiments that use the model to derive the
dominant class for polysemous verbs. Sections 5 and 6 discuss our verb class disam-
biguation experiments. We base our results on the BNC, a 100-million-word collection
of samples of written and spoken language from a wide range of sources designed
to represent a wide cross-section of current British English, both spoken and writ-
ten (Burnard 1995). We discuss our results in Section 7 and review related work in
Section 8.
2. The Prior Model
Consider again the sentences in (6). Assuming that we more often write something to
someone rather than for someone, we would like to derive Message Transfer as
the prevalent class for write rather than Performance. We view the choice of a class
for a polysemous verb in a given frame as maximizing the joint probability P(c, f , v),
where v is a verb subcategorizing for the frame f and inhabiting more than one Levin
class c:
P(c, f , v) = P(v) ? P(f |v) ? P(c|v, f ) (10)
Although the terms P(v) and P(f |v) can be estimated from the BNC (P(v) reduces
to the number of times a verb is attested in the corpus, and P(f |v) can be obtained
through parsing), the estimation of P(c|v, f ) is somewhat problematic, since it relies
on the frequency F(c, v, f ). The latter could be obtained straightforwardly if we had
access to a parsed corpus annotated with subcategorization and semantic-class infor-
mation. Lacking such a corpus we will assume that the semantic class determines the
subcategorization patterns of its members independently of their identity (see (11)):
P(c|v, f ) ? P(c|f ) (11)
The independence assumption is a simplification of Levin?s (1993) hypothesis that the
argument structure of a given verb is a direct reflection of its meaning. The rationale
behind the approximation in (11) is that since class formation is determined on the
basis of diathesis alternations, it is the differences in subcategorization structure, rather
than the identity of the individual verbs, that determine class likelihood. For example,
if we know that some verb subcategorizes for the double object and the prepositional
?NP1 V NP2 to NP3? frames, we can guess that it is a member of the Give class or
the Message Transfer class without knowing whether this verb is give, write, or tell.
Note that the approximation in (11) assumes that verbs of the same class uniformly
subcategorize (or not) for a given frame. This is evidently not true for all classes of
verbs. For example, all Give verbs undergo the dative diathesis alternation, and there-
fore we would expect them to be attested in both the double-object and prepositional
frame, but only a subset of Create verbs undergo the benefactive alternation. For
example, the verb invent is a Create verb and can be attested only in the benefactive
prepositional frame (I will invent a tool for you versus ?I will invent you a tool; see Levin
[1993] for details). By applying Bayes? law we write P(c|f ) as
P(c|f ) = P(f |c) ? P(c)
P(f )
(12)
By substituting (12) into (10), we can write P(c, f , v) as
P(c, f , v) =
P(v) ? P(f |v) ? P(f |c) ? P(c)
P(f )
(13)
51
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 2
Estimation of model parameters.
(a) P?(v) =
F(v)
?
i
F(vi)
(b) P?(f |v) = F(f , v)
F(v)
(c) P?(f |c) = F(f , c)
F(c)
(d) P?(c) =
F(c)
?
i
F(ci)
(e) P?(f ) =
F(f )
?
i
F(fi)
(f) F(f , c) =
?
i
F(c, f , vi)
(g) F(c) =
?
i
F(vi, c) (h) F(v, c) = F(v) ? P(c|v)
It is easy to obtain P(v) from the lemmatized BNC (see (a) in Table 2). In order to es-
timate the probability P(f |v), we need to know how many times a verb is attested with
a given frame. We acquired Levin-compatible subcategorization frames from the BNC
after performing a coarse-grained mapping between Levin?s frame descriptions and
surface syntactic patterns without preserving detailed semantic information about ar-
gument structure and thematic roles. This resulted in 80 frame types that were grossly
compatible with Levin. We used Gsearch (Corley et al 2001), a tool that facilitates the
search of arbitrary part-of-speech-tagged corpora for shallow syntactic patterns based
on a user-specified context-free grammar and a syntactic query. We specified a chunk
grammar for recognizing the verbal complex, NPs, and PPs and used Gsearch to ex-
tract tokens matching the frames specified in Levin. We discarded all frames with a
frequency smaller than five, as they were likely to be unreliable given our heuristic
approach. The frame probability P(f ) (see the denominator in (13) and equation (e)
in Table 2) was also estimated on the basis of the Levin-compatible subcategorization
frames that were acquired from the BNC.
We cannot read off P(f |c) in (13) directly from the corpus, because the corpus
is not annotated with verb classes. Nevertheless Levin?s (1993) classification records
the syntactic frames that are licensed by a given verb class (for example, Give verbs
license the double object and the ?NP1 V NP2 to NP3? frame) and also the number and
type of classes a given verb exhibits (e.g., write inhabits two classes, Performance
and Message Transfer). Furthermore, we know how many times a given verb is
attested with a certain frame in the corpus, as we have acquired Levin-compatible
frames from the BNC (see (b) in Table 2). We first explain how we obtain F(f , c), which
we rewrite as the sum of all occurrences of verbs v that are members of class c and
are attested in the corpus with frame f (see (c) and (f) in Table 2).
For monosemous verbs the count F(c, f , v) reduces to the number of times these
verbs have been attested in the corpus with a certain frame. For polysemous verbs,
we additionally need to know the class in which they were attested in the corpus.
Note that we don?t necessarily need an annotated corpus for class-ambiguous verbs
whose classes license distinct frames (see example (5)), provided that we have extracted
verb frames relatively accurately. For genuinely ambiguous verbs (i.e., verbs licensed
by classes that take the same frame), given that we don?t have access to a corpus
annotated with verb class information, we distribute the frequency of the verb and its
frame evenly across its semantic classes:
F(c, f , v) =
F(f , v)
|classes(v, f )| (14)
Here F(f , v) is the co-occurrence frequency of a verb and its frame and |classes(v, f )|
is the number of classes verb v is a member of when found with frame f . The joint
52
Computational Linguistics Volume 30, Number 1
Table 3
Estimation of F(c, f , v) and F(v, c).
Give F(Give, NPVNPNP, v) F(Give, NPVNPtoNP, v) F(v,Give)
feed
98
2
40
2
3, 263
4
give 25, 705 7, 502 126, 894
lend 343 648 2, 650
rent
6
2
10
1, 060
2
pass
181
3
256
3
19, 459
4
serve 85
58
2
15, 457
4
frequency of a class and its frame F(f , c) is then the sum of all verbs that are mem-
bers of the class c and are attested with frame f in the corpus (see (f) in Table 2).
Table 3 shows the estimation of the frequency F(c, f , v) for six verbs that are mem-
bers of the Give class. Consider for example feed, which is a member of four classes:
Give, Gorge, Feeding, and Fit. Of these classes, only Feeding and Give license
the double-object and prepositional frames. This is why the co-occurrence frequency
of feed with these frames is divided by two. The verb serve inhabits four classes. The
double-object frame is licensed by the Give class, whereas the prepositional frame
is additionally licensed by the Fulfilling class, and therefore the co-occurrence fre-
quency F(NPVNPtoNP, serve) is equally distributed between these two classes. This
is clearly a simplification, since one would expect F(c, f , v) to vary for different verb
classes. However, note that according to this estimation, F(f , c) will vary across frames
reflecting differences in the likelihood of a class being attested with a certain frame.
Both terms P(f |c) and P(c) in (13) rely on the class frequency F(c) (see (c) and (d)
in Table 2). We rewrite F(c) as the sum of all verbs attested in the corpus with class c
(see (g) in Table 2). For monosemous verbs the estimate of F(v, c) reduces to the count
of the verb in the corpus. Once again we cannot estimate F(v, c) for polysemous verbs
directly. The task would be straightforward if we had a corpus of verbs, each labeled
explicitly with class information. All we have is the overall frequency of a given verb
in the BNC and the number of classes it is a member of according to Levin (1993).
Since polysemous verbs can generally be the realization of more than one semantic
class, counts of semantic classes can be constructed by dividing the contribution from
the verb by the number of classes it belongs to (Resnik 1993; Lauer 1995). We rewrite
the frequency F(v, c) as shown in (h) in Table 2 and approximate P(c|v), the true
distribution of the verb and its classes, as follows:
P(c|v) ? F(v)|classes(v)| (15)
Here, F(v) is the number of times the verb v was observed in the corpus and |classes(v)|
is the number of classes c it belongs to. For example, in order to estimate the frequency
of the class Give, we consider all verbs that are listed as members of this class in Levin
(1993). The class contains thirteen verbs, among which six are polysemous. We will
obtain F(Give) by taking into account the verb frequency of the monosemous verbs
(|classes(v)| is one in this case) as well as distributing the frequency of the polyse-
mous verbs among their classes. For example, feed inhabits the classes Give, Gorge,
53
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Feeding, and Fit and occurs in the corpus 3,263 times. We will increment the count
of F(Give) by 3,2634 . Table 3 illustrates the estimation of F(v, c) for six members of the
Give class. The total frequency of the class is obtained by summing over the individual
values of F(v, c) (see equation (g) in Table 2).
The approach in (15) relies on the simplifying assumption that the frequency of
a verb is distributed evenly across its semantic classes. This is clearly not true for all
verbs. Consider, for example, the verb rent, which inhabits classes Give (Frank rented
Peter his room) and Get (I rented my flat for my sister). Intuitively speaking, the Give
sense of rent is more frequent than the Get sense, however, this is not taken into
account in (15), primarily because we do not know the true distribution of the classes
for rent. An alternative to (15) is to distribute the verb frequency unequally among verb
classes. Even though we don?t know how likely classes are in relation to a particular
verb, we can approximate how likely classes are in general on the basis of their size
(i.e., number of verbs that are members of each class). So then we can distribute a
verb?s frequency unequally, according to class size. This time we approximate P(c|v)
(see (h) in Table 2) by P(c|amb class), the probability of class c given the ambiguity
class3 amb class. The latter represents the set of classes a verb might inhabit:
F(v, c) ? F(v) ? P(c|amb class) (16)
We collapse verbs into ambiguity classes in order to reduce the number of parameters
that must be estimated; we certainly lose information, but the approximation makes it
easier to get reliable estimates from limited data. We simply approximate P(c|amb class)
using a heuristic based on class size:
P(c|amb class) ? |c|?
c ? amb class
|c| (17)
For each class we recorded the number of its members after discarding verbs
whose frequency was less than one per million in the BNC. This gave us a first ap-
proximation of the size of each class. We then computed, for each polysemous verb,
the total size of the classes of which it was a member. We calculated P(c|amb class) by
dividing the former by the latter (see equation (17)). We obtained the class frequency
F(c) by multiplying P(c|amb class) by the observed frequency of the verb in the BNC
(see equation (16)). As an example, consider again F(Give), which is calculated by
summing over all verbs that are members of this class (see (g) in Table 2). In order
to add the contribution of the verb feed, we need to distribute its corpus frequency
among the classes Give, Gorge, Feed, and Fit. The respective P(c|amb class) values
for these classes are 1538 ,
8
38 ,
3
38 , and
12
38 . By multiplying these by the frequency of feed in
the BNC (3,263), we obtain the values of F(v, c) given in Table 4. Only the frequency
F(feed,Give) is relevant for F(Give).
The estimation process just described involves at least one gross simplification,
since P(c|amb class) is calculated without reference to the identity of the verb in ques-
tion. For any two verbs that fall into the same set of classes, P(c|amb class) will be the
same, even though one or both may be atypical in its distribution across the classes.
Furthermore, the estimation tends to favor large classes, again irrespectively of the
identity of the verb in question. For example, the verb carry has three classes, Carry,
3 Our use of ambiguity classes is inspired by a similar use in hidden Markov model?based
part-of-speech tagging (Kupiec 1992).
54
Computational Linguistics Volume 30, Number 1
Table 4
Estimation of F(v, c) for the verb feed.
c |c| P(c|amb class) F(v, c)
Give 15 .39 1,272.57
Gorge 8 .21 685.23
Feed 3 .08 261.04
Fit 12 .32 1,044.16
Table 5
Ten most frequent classes using equal distribution of verb frequencies.
c F(c)
Characterize 601,647.4
Get 514,308.0
Say 450,444.6
Conjecture 390,618.4
Future Having 369,229.3
Declare 264,923.6
Amuse 258,857.9
Directed Motion 252,775.6
Message Transfer 248,238.7
Give 208,884.1
Table 6
Ten most frequent classes using unequal distribution of verb frequencies.
c F(c)
Get 453,843.6
Say 447,044.2
Characterize 404,734.2
Conjecture 382,193.8
Future Having 370,717.7
Declare 285,431.7
Directed Motion 255,821.6
Pocket 247,392.7
Amuse 205,729.4
Give 197,828.8
Fit, and Cost. Intuitively speaking, the Carry class is the most frequent (e.g., Smok-
ing can impair the blood which carries oxygen to the brain; I carry sugar lumps around with me).
However, since the Fit class (e.g., Thameslink presently carries 20,000 passengers daily)
is larger than the Carry class, it will be given a higher probability (.45 versus .4).
Our estimation scheme is clearly a simplification, but it is an empirical question how
much it matters. Tables 5 and 6 show the ten most frequent classes as estimated us-
ing (15) and (16). We explore the contribution of the two estimation schemes for P(c)
in Experiments 1 and 2.
The probabilities P(f |c) and P(f |v) will be unreliable when the frequencies F(f , v)
and F(f , c) are small and will be undefined when the frequencies are zero. Following
Hindle and Rooth (1993), we smooth the observed frequencies as shown in Table 7.
When F(f , v) is zero, the estimate used is proportional to the average F(f ,V)F(V) across
55
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 7
Smoothed estimates.
(a) P(f |v) ?
F(f , v) + F(f ,V)F(V)
F(v) + 1
(b) F(f , V) =
?
i
F( f , vi)
(c) P(f |c) ?
F(f , c) + Ff ,C)F(C)
F(c) + 1
(d) F(C) =
?
i
F( f , ci)
all verbs. Similarly, when F(f , c) is zero, our estimate is proportional to the average
F(f ,C)
F(C) across all classes. We do not claim that this scheme is perfect, but any deficien-
cies it may have are almost certainly masked by the effects of approximations and
simplifications elsewhere in the system.
We evaluated the performance of the model on all verbs listed in Levin (1993) that
are polysemous (i.e., members of more than one class) and take frames characteristic
of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and
Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less
well-known conative and possessor object alternations (see examples (1)?(4)). All four
alternations seem fairly productive; that is, a large number of verbs undergo these
alternations, according to Levin. A large number of classes license the frames that
are relevant for these alternations and the verbs that inhabit these classes are likely
to exhibit class ambiguity: 20 classes license the double object frame, 22 license the
prepositional frame ?NP1 V NP2 to NP3,? 17 classes license the benefactive ?NP1 V
NP2 for NP3? frame, 118 (out of 200) classes license the transitive frame, and 15 classes
license the conative ?NP1 V at NP2? frame.
In Experiment 1 we use the model to test the hypothesis that subcategorization
information can be used to disambiguate polysemous verbs. In particular, we concen-
trate on verbs like serve (see example (5)) that can be disambiguated solely on the
basis of their frame. In Experiment 2 we focus on verbs that are genuinely ambiguous;
that is, they inhabit a single frame and yet can be members of more than one seman-
tic class (e.g., write, study, see examples (6)?(9)). In this case, we use the probabilistic
model to assign a probability to each class the verb inhabits. The class with the highest
probability represents the dominant meaning for a given verb.
3. Experiment 1: Using Subcategorization to Resolve Verb Class Ambiguity
3.1 Method
In this experiment we focused solely on verbs whose meaning can be potentially
disambiguated by taking into account their subcategorization frame. A model that
performs badly on this task cannot be expected to produce any meaningful results for
genuinely ambiguous verbs.
We considered 128 verbs with the double-object frame (2.72 average class ambi-
guity), 101 verbs with the prepositional frame ?NP1 V NP2 to NP3? (2.59 average
class ambiguity), 113 verbs with the frame ?NP1 V NP2 for NP3? (2.63 average class
ambiguity), 42 verbs with the frame ?NP1 V at NP3? (3.05 average class ambiguity),
and 39 verbs with the transitive frame (2.28 average class ambiguity). The task was the
following: Given that we know the frame of a given verb, can we predict its semantic
class? In other words by varying the class c in the term P(c, f , v), we are trying to see
whether the class that maximizes it is the one predicted by the lexical semantics and
56
Computational Linguistics Volume 30, Number 1
Table 8
Model accuracy using equal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 60.9% 93.8%
NP1 V NP to NP3 63.3% 95.0%
NP1 V NP for NP3 63.6% 98.2%
NP1 V at NP2 2.4% 83.3%
NP1 V NP2 43.6% 87.2%
Combined 55.8% 93.9%
Table 9
Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 62.5% 93.8%
NP1 V NP to NP3 67.3% 95.0%
NP1 V NP for NP3 66.4% 98.2%
NP1 V at NP2 2.4% 85.7%
NP1 V NP2 41.0% 84.6%
Combined 56.7% 93.9%
the argument structure of the verb in question. The model?s responses were evaluated
against Levin?s (1993) classification. The model?s performance was considered correct
if it agreed with Levin in assigning a verb to an appropriate class given a particular
frame. Recall from Section 2 that we proposed two approaches for the estimation of
the class probability P(c). We explore the influence of P(c) by obtaining two sets of
results corresponding to the two estimation schemes.
3.2 Results
The model?s accuracy is shown in Tables 8 and 9. The results in Table 8 were ob-
tained using the estimation scheme for P(c) that relies on the even distribution of
the frequency of a verb across its semantic classes (see equation (15)). The results
in Table 9 were obtained using an alternative scheme that distributes verb frequency
unequally among verb classes by taking class size into account (see equation (16)).
As mentioned in Section 3.1, the results were based on comparison of the model?s
performance against Levin?s (1993) classification. We also compared the results to the
baseline of choosing the most likely class P(c) (without taking subcategorization in-
formation into account). The latter was determined on the basis of the approximations
described in Section 2 (see equation (9) in Table 2, as well as equations (15), (16),
and (17)).
The model achieved an accuracy of 93.9% using either type of estimation for P(c).
It also outperformed the baseline by 38.1% (see Table 8) and 37.2% (see Table 9). One
might expect an accuracy of 100%, since these verbs can be disambiguated solely on the
basis of their frame. However, the performance of our model achieves a lesser accuracy,
mainly because of the way we estimate the terms P(c) and P(f |c): We overemphasize
the importance of class information without taking into account how individual verbs
distribute across classes. Furthermore, we rely on frame frequencies acquired from the
BNC, using shallow syntactic analysis, which means that the correspondence between
57
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Levin?s (1993) frames and our acquired frames is not one to one. Except for the fact
that our frames do not preserve much of the linguistic information detailed Levin,
the number of frames acquired for a given verb can be a subset or superset of the
frames available in Levin. Note that the two estimation schemes yield comparable
performances. This is a positive result given the importance of P(c) in the estimation
of P(c, f , v).
A more demanding task for our probabilistic model will be with genuinely am-
biguous verbs (i.e., verbs for which the mapping between meaning and subcatego-
rization is not one to one). Although native speakers may have intuitions about the
dominant interpretation for a given verb, this information is entirely absent from Levin
(1993) and from the corpus on which our model is trained. In Experiment 2 we show
how our model can be used to recover this information.
4. Experiment 2: Using Corpus Distributions to Derive Verb Class Preferences
4.1 Method
We evaluated the performance of our model on 67 genuinely ambiguous verbs, that
is, verbs that inhabit a single frame and can be members of more than one seman-
tic class (e.g., write). These verbs are listed in Levin (1993) and undergo the dative,
benefactive, conative, and possessor object alternations. As in Experiment 1, we con-
sidered verbs with the double-object frame (3.27 average class ambiguity), verbs with
the frame ?NP1 V NP2 to NP3? (2.94 average class ambiguity), verbs with the frame
?NP1 V NP2 for NP3? (2.42 average class ambiguity), verbs with the frame ?NP1 V
at NP3? (2.71 average class ambiguity), and transitive verbs (2.77 average class am-
biguity). The model?s predictions were compared against manually annotated data
that was used only for testing purposes. The model was trained without access to a
disambiguated corpus. More specifically, corpus tokens characteristic of the verb and
frame in question were randomly sampled from the BNC and annotated with class
information so as to derive the true distribution of the verb?s classes in a particular
frame. We describe the verb selection procedure as follows.
Given the restriction that these verbs be semantically ambiguous in a specific
syntactic frame, we could not simply sample from the entire BNC, since this would
decrease the chances of finding the verb in the frame we are interested in. Instead,
a stratified sample was used: For all class-ambiguous verbs, tokens were randomly
sampled from the parsed data used for the acquisition of verb frame frequencies. The
model was evaluated on verbs for which a reliable sample could be obtained. This
meant that verbs had to have a frame frequency larger than 50. For verbs exceeding
this threshold 100 tokens were randomly selected and annotated with verb class infor-
mation. For verbs with frame frequency less than 100 and more than 50, no sampling
took place; the entire set of tokens was manually annotated. This selection procedure
resulted in 14 verbs with the double-object frame, 16 verbs with the frame ?NP1 V
NP2 to NP3,? 2 verbs with the frame ?NP1 V NP2 for NP3,? 1 verb with the frame
?NP1 V at NP3,? and 80 verbs with the transitive frame. From the transitive verbs
we further randomly selected 34 verbs; these were manually annotated and used for
evaluating the model?s performance.4
The selected tokens were annotated with class information by two judges, both
linguistics graduate students. The classes were taken from Levin (1993) and augmented
4 Although the model can yield predictions for any number of verbs, evaluation could not be performed
for all 80 verbs, as to perform such evaluation, our judges would have had to annotate 8,000 corpus
tokens.
58
Computational Linguistics Volume 30, Number 1
with the class Other, which was reserved for either corpus tokens that had the wrong
frame or those for which the classes in question were not applicable. The judges were
given annotation guidelines (for each verb) but no prior training (for details on the
annotation study see Lapata [2001]). The annotation provided a gold standard for
evaluating the model?s performance and enabled us to test whether humans agree
on the class annotation task. We measured the judges? agreement on the annotation
task using the kappa coefficient (Cohen 1960). In general, the agreement on the class
annotation task was good, with kappa values ranging from .66 to 1.00 (the mean kappa
was .80, SD = .09).
4.2 Results
We counted the performance of our model as correct if it agreed with the ?most pre-
ferred,? that is, the most frequent, verb class, as determined in the manually annotated
corpus sample by taking the average of the responses of both judges. As an example,
consider the verb feed, which in the double-object frame is ambiguous between the
classes Feed and Give. According to the model, Feed is the most likely class for feed.
Out of 100 instances of the verb feed in the double-object frame, 61 were manually
assigned the Feed class, 32 were assigned the Give class, and 6 were parsing mis-
takes (and therefore assigned the class Other). In this case the model?s outcome is
considered correct given that the corpus tokens also reveal a preference for the Feed
(i.e., the Feed instances outnumber the Give ones).
As in Experiment 1, we explored the influence of the parameter P(c) on the model?s
performance by obtaining two sets of results corresponding to the two estimation
schemes discussed in Section 2. The model?s accuracy is shown in Tables 10 and 11.
The results in Table 10 were obtained using the estimation scheme for P(c) that
relies on the even distribution of a verb?s frequency across its semantic classes (see
Table 10
Model accuracy using equal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 50.0% 78.6%
NP1 V NP to NP3 43.8% 68.8%
NP1 V NP for NP3 00.0% 100.0%
NP1 V at NP2 100.0% 100.0%
NP1 V NP2 47.1% 73.5%
Combined 46.2% 74.6%
Table 11
Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 50.0% 78.6%
NP1 V NP to NP3 43.8% 75.0%
NP1 V NP for NP3 00.0% 100.0%
NP1 V at NP2 100.0% 100.0%
NP1 V NP2 47.1% 67.6%
Combined 46.2% 73.1%
59
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 12
Semantic preferences for verbs with the double-object frame.
Verb Class K
call
?
Dub Get Other
93 -7.59 3 -8.12 4 .82
cook Build Prepare Other
28 -11.68 33 -11.50 1 1.00
declare
?
Declare Ref. Appear. Other
35 -10.51 18 -12.18 5 .89
feed
?
Feed Give Other
61 -10.63 32 -12.16 6 .73
find
?
Declare Get Other
36 -7.69 47 -7.43 17 .70
leave
?
Get Fulfill F. Have Other
6 -7.91 14 -10.40 56 ?7.66 23 .67
make
?
Build Dub Other
21 -7.25 66 -6.13 13 .79
pass
?
Give Send Throw Other
81 -8.84 0 -8.96 0 ?9.98 19 .93
save
?
Bill Get Other
24 -9.74 62 -9.59 14 .74
shoot Throw Get Other
91 -10.94 0 -9.99 5 1.00
take Bring-Take Perform Other
15 -7.02 40 -7.38 45 .77
write
?
Msg. Trans. Perform Other
54 -8.79 19 -9.05 18 .85
equation (15)). The results in Table 11 were obtained using a scheme that distributes
verb frequency unequally among verb classes by taking class size into account (see
equation (16)). As in Experiment 1, the results were compared to a simple baseline that
defaults to the most likely class without taking verb frame information into account
(see equation (g) in Table 2 as well as equations (15), (16), and (17)).
The model achieved an accuracy of 74.6% using the estimation scheme of equal
distribution and a accuracy of 73.1% using the estimation scheme of unequal distribu-
tion. The difference between the two estimation schemes is not statistically significant
( ?2(67) = 2.17, p = .84). Table 12 gives the distribution of classes for 12 polysemous
verbs taking the double-object frame as obtained from the manual annotation of corpus
tokens together with interannotator agreement (?). We also give the (log-transformed)
probabilities of these classes as derived by the model.5 The presence of the symbol
?
indicates that the model?s class preference for a given verb agrees with its distribution
in the corpus. The absence of
?
indicates disagreement. For the comparison shown in
Table 12, model class preferences were derived using the equal-distribution estimation
scheme for P(c) (see equation (15)).
As shown in Table 12, the model?s predictions are generally borne out in the corpus
data. Misclassifications are due mainly to the fact that the model does not take verb
class dependencies into account. Consider, for example, the verb cook. According to the
model the most likely class for cook is Build. Although it may generally be the case
that Build verbs (e.g., make, assemble, build) are more frequent than Prepare verbs
5 No probabilities are given for the Other class; this is not a Levin class, however, it was used by the
annotators, mainly to indicate parsing errors.
60
Computational Linguistics Volume 30, Number 1
(e.g., bake, roast, boil), the situation is reversed for cook. The same is true for the verb
shoot, which when attested in the double-object frame is more likely to be a Throw
verb (Jamie shot Mary a glance) rather than a Get verb (I will shoot you two birds).
Notice that our model is not context sensitive; that is, it does not derive class
rankings tailored to specific verbs, primarily because this information is not readily
available in the corpus, as explained in Section 2. However, we have effectively built a
prior model of the joint distribution of verbs, their classes, and their syntactic frames
that can be useful for disambiguating polysemous verbs in context. We describe our
class disambiguation experiments as follows.
5. Class Disambiguation
In the previous sections we focused on deriving a model of the distribution of Levin
classes without relying on annotated data and showed that this model infers the right
class for genuinely ambiguous verbs 74.6% of the time without taking the local context
of their occurrence into account. An obvious question is whether this information is
useful for disambiguating tokens rather than types. In the following we report on a
disambiguation experiment that takes advantage of this prior information.
Word sense disambiguation is often cast as a problem in supervised learning,
where a disambiguator is induced from a corpus of manually sense-tagged text. The
context within which the ambiguous word occurs is typically represented by a set
of linguistically motivated features from which a learning algorithm induces a repre-
sentative model that performs the disambiguation. A variety of classifiers have been
employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews),
the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classi-
fiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and
Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for
our experiments, as it is a very convenient framework for incorporating prior knowl-
edge and studying its influence on the classification task. In Section 5.1 we describe
a basic naive Bayesian classifier and show how it can be extended with informative
priors. In Section 5.2 we discuss the types of contextual features we use. We report on
our experimental results in Section 6.
5.1 Naive Bayes Classification
A naive Bayesian classifier assumes that all the feature variables representing a prob-
lem are conditionally independent, given the value of the classification variable. In
word sense disambiguation, the features (a1, a2, . . . , an) represent the context surround-
ing the ambiguous word, and the classification variable c is the sense (Levin class in
our case) of the ambiguous word in this particular context. Within a naive Bayes
approach, the probability of the class c given its context can be expressed as
P(c|ai) =
P(c)
n
?
i=1
P(ai|c)
P(ai)
(18)
where P(ai|c) is the probability that a test example is of class c given the contextual
features ai. Since the denominator P(ai) is constant for all classes c, the problem reduces
to finding the class c with the maximum value for the numerator:
P(c|ai) ? P(c)
n
?
i=1
P(ai|c) (19)
61
Lapata and Brew Verb Class Disambiguation Using Informative Priors
If we choose the prior P(c) to be uniform (P(c) = 1|c| for all c ? C), (19) can be
further simplified to
P(c|a) ?
n
?
i=1
P(ai|c) (20)
Assuming a uniform prior, a basic naive Bayesian classifier is as follows:
? c =
n
?
i=1
P(ai|c) (21)
Note, however, that we developed in the previous section two types of non-
uniform prior models. The first model derives P(c) heuristically from the BNC, ig-
noring the identity of the polysemous verb and its subcategorization profile, and the
second model estimates the class distribution P(c, v, f ) by taking the frame distribu-
tion into account. So, the naive Bayesian classifier in (21) can be extended with a
nonuniform prior:
? c = P(c)
n
?
i=1
P(ai|c) (22)
? c = P(c, v, f )
n
?
i=1
P(ai|c, f , v) (23)
where P(c) is estimated as shown in (d)?(g) in Table 2 and P(c, v, f ), the prior for
each class c corresponding to verb v in frame f , is estimated as explained in Sec-
tion 2 (see (13)). As before, ai are the contextual features. The probabilities P(ai|c)
and P(ai|c, f , v) can be estimated from the training data simply by counting the co-
occurrence of feature ai with class c (for (22)) or the co-occurrence of ai with class
c, verb v, and frame f (for (23)). For features that have zero counts, we use add-k
smoothing (Johnson 1932), where k is a number less than one.
5.2 Feature Space
As is common in word sense disambiguation studies, we experimented with two types
of context representations, collocations and co-occurrences. Co-occurrences simply in-
dicate whether a given word occurs within some number of words to the left or right
of an ambiguous word. In this case the contextual features are binary and represent
the presence or absence of a particular word in the current or preceding sentence. We
used four types of context in our experiments: left context (i.e., words occurring to
the left of the ambiguous word), right context (i.e., words occurring to the right of the
ambiguous word), the current sentence (i.e., words surrounding the ambiguous word),
and the current sentence together with its immediately preceding sentence. Punctua-
tion and capitalization were removed from the windows of context; noncontent words
were included. The context words were represented as lemmas or parts of speech.
Collocations are words that are frequently adjacent to the word to be disam-
biguated. We considered 12 types of collocations. Examples of collocations for the
verb write are illustrated in Table 13. The L columns in the table indicate the number
of words to the left of the ambiguous words, and the R columns, the number of words
to the right. So for example, the collocation 1L3R represents one word to the left and
three words to the right of the ambiguous word. Collocations again were represented
as lemmas (see Table 13) or parts of speech.
62
Computational Linguistics Volume 30, Number 1
Table 13
Features for collocations.
L R Example L R Example
0 1 write you 1 1 can write you
1 0 can write 1 2 can write you a
0 2 write you a 2 1 I can write you
2 0 I can write 1 3 can write you a story
0 3 write you a story 3 1 perhaps I can write you
3 0 perhaps I can write 2 4 I can write you a story sunshine
6. Experiment 3: Disambiguating Polysemous Verbs
6.1 Method
We tested the performance of our naive Bayesian classifiers on the 67 genuinely am-
biguous verbs on which the prior models were tested. Recall that these models were
trained without access to a disambiguated corpus, which was used only to determine
for a given verb and its frame its most likely meaning overall (i.e., across the corpus)
instead of focusing on the meaning of individual corpus tokens. The same corpus
was used for the disambiguation of individual tokens, excluding tokens assigned the
class Other. The naive Bayes classifiers were trained and tested using 10-fold cross-
validation on a set of 5,002 examples. These were representative of the frames ?NP1
V NP2,? ?NP1 V NP2 NP3,? ?NP1 V NP2 to NP3,? and ?NP1 V NP2 for NP3.? The
frame ?NP1 V at NP2? was excluded from our disambiguation experiments as it was
represented solely by the verb kick (50 instances).
In this study we compare a naive Bayesian classifier that relies on a uniform
prior (see (20)) against two classifiers that make use of nonuniform prior models:
The classifier in (22) effectively uses as prior the baseline model P(c) from Section 2,
whereas the classifier in (23) relies on the more informative model P(c, f , v). As a
baseline for the disambiguation task, we simply assign the most common class in the
training data to every instance in the test data, ignoring context and any form of prior
information (Pedersen 2001; Gale, Church, and Yarowsky 1992a). We also report an
upper bound on disambiguation performance by measuring how well human judges
agree with one another (percentage agreement) on the class assignment task. Recall
from Section 4.1 that our corpus was annotated by two judges with Levin-compatible
verb classes.
6.2 Results
The results of our class disambiguation experiments are summarized in Figures 2?5.
In order to investigate differences among different frames, we show how the naive
Bayesian classifiers perform for each frame individually. Figures 2?5 (x-axis) also re-
veal the influence of collocational features of different sizes (see Table 13) on the
classification task. Panel (b) in the figures presents the classifiers? accuracy when the
collocational features are encoded as lemmas; in panel (c) of the figures, the context
is represented as parts of speech, whereas in panel (a) of the figures, the context is
represented by both lemmas and parts of speech.
As can be seen in the figures, the naive Bayesian classifier with our informa-
tive prior (P(c, f , v), IPrior in Figures 2?5) generally outperforms the baseline prior
(P(c), BPrior in Figures 2?5), the uniform prior (UPrior in Figures 2?5), and the base-
line (Baseline in Figures 2?5) for all frames. Good performances are attained with
63
Lapata and Brew Verb Class Disambiguation Using Informative Priors
(a) (b) (c)
Figure 2
Word sense disambiguation accuracy for ?NP1 V NP2? frame.
(a) (b) (c)
Figure 3
Word sense disambiguation accuracy for ?NP1 V NP2 NP3? frame.
(a) (b) (c)
Figure 4
Word sense disambiguation accuracy for ?NP1 V NP2 to NP3? frame.
lemmas, parts of speech, and combination of the two. The naive Bayesian classifier
(IPrior) reaches the upper bound (UpBound in Figures 2?5) for the ditransitive frames
?NP1 V NP2 NP3,? ?NP1 V NP2 to NP3,? and ?NP1 V NP2 for NP3.?
The best accuracy (87.8%) for the transitive frame is achieved with the collocational
features 0L2R, 1L2R, and 1L3R (see Figures 2(a)?(c)). For the double-object frame, the
highest accuracy (90.8%) is obtained with features 0LR1 and 0L3R (see Figures 3(b)
and 3(c)). Similarly, for the ditransitive ?NP1 V NP2 to NP3? frame, the features 0L3R
and 0L1R yield the best accuracies (88.8%, see Figures 4(a)?(c)). Finally, for the ?NP1 V
NP2 for NP3? frame, accuracy (94.4%) is generally good for most features when an
informative prior is used. In fact, neither the uniform prior nor the baseline P(c)
outperforms the baseline for this frame.
64
Computational Linguistics Volume 30, Number 1
(a) (b) (c)
Figure 5
Word sense disambiguation accuracy for ?NP1 V NP2 for NP3? frame.
The context encoding (lemmas versus parts of speech) does not seem to have a
great influence on the disambiguation performance. Good accuracies are obtained with
either parts of speech or lemmas; combination of the two does not yield better results.
The classifier with the informative prior P(c, f , v) outperforms the baseline prior
P(c) and the uniform prior also when co-occurrences are used. However, the co-
occurrences never outperform the collocational features, for all four types of context.
The classifiers (regardless of the type of prior being used) never beat the baseline for
the frames ?NP1 V NP2? and ?NP1 V NP2 to NP3?. Accuracies above the baseline
are achieved for the frames ?NP1 V NP2 NP3? and ?NP1 V NP2 for NP3? when
an informative prior is used. Detailed results are summarized in the Appendix. Co-
occurrences and windows of large sizes traditionally work well for topical distinctions
(Gale, Church, and Yarowsky 1992b). Levin classes, however, typically capture differ-
ences in argument structure, that is, the types of objects or subjects that verbs select
for. Argument structure is approximated by our collocational features. For example, a
verb often taking a reflexive pronoun as its object is more likely to be a Reflexive
Verb of Appearance than a verb that never subcategorizes for a reflexive object.
There is not enough variability among the wider contexts surrounding a polysemous
verb to inform the class-disambiguation task, as the Levin classes often do not cross
topical boundaries.
7. Discussion
In this article, we have presented a probabilistic model of verb class ambiguity based
on Levin?s (1993) semantic classification. Our results show that subcategorization in-
formation acquired automatically from corpora provides important cues for verb class
disambiguation (Experiment 1). In the absence of subcategorization cues, corpus-based
distributions and quantitative approximations of linguistic concepts can be used to de-
rive a preference ordering on the set of verbal meanings (Experiment 2). The semantic
preferences that we have generated can be thought of as default semantic knowledge,
to be used in the absence of any explicit contextual or lexical semantic information to
the contrary (see Table 12). We have also shown that these preferences are useful for
disambiguating polysemous verbs within their local contexts of occurrence (Experi-
ment 3).
The approach is promising in that it achieves satisfactory results with a simple
model that has a straightforward interpretation in a Bayesian framework and does not
65
Lapata and Brew Verb Class Disambiguation Using Informative Priors
rely on the availability of annotated data. The model?s parameters are estimated using
simple distributions that can be extracted easily from corpora. Our model achieved an
accuracy of 93.9% (over a baseline of 56.7%) on the class disambiguation task (Exper-
iment 1) and an accuracy of 74.6% (over a baseline of 46.2%) on the task of deriving
dominant verb classes (Experiment 2). Our disambiguation experiments reveal that
this default semantic knowledge, when incorporated as a prior in a naive Bayes classi-
fier, outperforms the uniform prior and the baseline of always defaulting to the most
frequent class (Experiment 3). In fact, for three out of the four frames under study,
our classifier with the informative prior achieved upper-bound performance.
Although our results are promising, it remains to be shown that they generalize
across frames and alternations. Four types of alternations were investigated in this
study. However, Levin lists 79 alternations and approximately 200 classes. Although
distributions for different class/frame combinations can easily be derived automati-
cally, it remains to be shown that these distributions are useful for all verbs, frames,
and classes. Also note that the models described in the previous sections crucially rely
on the acquisition of relatively accurate frames from the corpus. It is a matter of future
work to examine how the quality of the acquired frames influences the disambiguation
task. Also, the assumption that the semantic class determines the subcategorization
patterns of its class members independently of their identity may not be harmless for
all classes and frames.
Although our original aim was to develop a probabilistic framework that exploits
Levin?s (1993) linguistic classification and the systematic correspondence between syn-
tax and semantics, a limitation of the model is that it cannot infer class information for
verbs not listed in Levin. For these verbs, P(c), and hence P(c, f , v), will be zero. Recent
work in computational linguistics (e.g., Schu?tze 1998) and cognitive psychology (e.g.,
Landauer and Dumais 1997) has shown that large corpora implicitly contain semantic
information, which can be extracted and manipulated in the form of co-occurrence
vectors. One possible approach would be to compute the centroid (geometric mean)
of the vectors of all members of a semantic class. Given an unknown verb (i.e., a verb
not listed in Levin), we can decide its semantic class by comparing its semantic vector
to the centroids of all semantic classes. For example, we could determine class mem-
bership on the basis of the distance to the closest centroid representing a semantic
class (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit). Another
approach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles
1991) to find similarities (via synonymy) between unknown verbs and verbs listed in
Levin. Once we have chosen a class for an unknown verb, we are entitled to assume
that it will share the broad syntactic and semantic properties of that class.
8. Related Work
Levin?s (1993) seminal study on diathesis alternations and verb semantic classes has
recently influenced work in dictionary creation (Dorr 1997; Dang et al 1998; Dorr
and Jones 1996) and notably lexicon acquisition on the basis of the assumption that
verbal meaning can be gleaned from corpora using cues pertaining to syntactic struc-
ture (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000).
Previous work in word sense disambiguation has not tackled explicitly the ambiguity
problems arising from Levin?s classification, although methods for deriving informa-
tive priors in an unsupervised manner have been proposed by Ciaramita and Johnson
(2000) and Chao and Dyer (2000) within the context of noun and adjective sense dis-
ambiguation, respectively. In this section we review related work on classification and
lexicon acquisition and compare it to our own work.
66
Computational Linguistics Volume 30, Number 1
Dang et al (1998) observe that verbs in Levin?s (1993) database are listed in more
than one class. The precise meaning of this ambiguity is left open to interpretation
in Levin, as it may indicate that the verb has more than one sense or that one sense
(i.e., class) is primary and the alternations for this class should take precedence over
the alternations for the other classes for which the verb is listed. Dang et al augment
Levin?s semantic classes with a set of ?intersective? classes that are created by grouping
together sets of existing classes that share a minimum of three overlapping members.
Intersective classes are more fine-grained than the original Levin classes and exhibit
more-coherent sets of syntactic frames and associated semantic components. Dang et
al. further argue that intersective classes are more compatible with WordNet than the
broader Levin classes and thus make it possible to attribute the semantic components
and associated sets of syntactic frames to specific WordNet senses as well, thereby
enriching the WordNet representation and providing explicit criteria for word sense
disambiguation.
Most statistical approaches, including ours, treat verbal-meaning assignment as a
semantic classification task. The underlying question is the following: How can corpus
information be exploited in deriving the semantic class for a given verb? Despite the
unifying theme of using corpora and corpus distributions for the acquisition task, the
approaches differ in the inventory of classes they employ, in the methodology used
for inferring semantic classes, and in the specific assumptions concerning the verbs to
be classified (e.g., can they be polysemous or not).
Merlo and Stevenson (2001) use grammatical features (acquired from corpora) to
classify verbs into three semantic classes: unergative, unaccusative, and object drop.
These classes are abstractions of Levin?s (1993) classes and as a result yield a coarser
classification. For example, object-drop verbs comprise a variety of Levin classes such
as Gesture verbs, Caring verbs, Load verbs, Push-Pull verbs, Meet verbs, So-
cial Interaction verbs, andAmuse verbs. Unergative, unaccusative, and object-drop
verbs have identical subcategorization patterns (i.e., they alternate between the tran-
sitive and intransitive frame), yet distinct argument structures, and therefore differ in
the thematic roles they assign to their arguments. For example, when attested in the
intransitive frame, the subject of an object-drop verb is an agent, whereas the subject
of an unaccusative verb is a theme. Under the assumption that differences in thematic
role assignment uniquely identify semantic classes, numeric approximations of argu-
ment structure are derived from corpora and used in a machine-learning paradigm to
place verbs in their semantic classes. The approach is evaluated on 59 verbs manually
selected from Levin (20 unergatives, 20 object drops, and 19 unaccusatives). It is as-
sumed that these verbs are monosemous, that is, they can be ergative, unergative, or
object drop. A decision-tree learner achieves an accuracy of 69.8% on the classification
task over a chance baseline of 34%.
Schulte im Walde (2000) uses subcategorization information and selectional re-
strictions to cluster verbs into Levin (1993)?compatible semantic classes. Subcatego-
rization frames are induced from the BNC using a robust statistical parser (Carroll and
Rooth 1998). The selectional restrictions are acquired using Resnik?s (1993) information-
theoretic measure of selectional association, which combines distributional and taxo-
nomic information (e.g., WordNet) to formalize how well a predicate associates with
a given argument. Two sets of experiments are run to evaluate the contribution of se-
lectional restrictions using two types of clustering algorithms: iterative clustering and
latent-class clustering (see Schulte im Walde [2000] for details). The approach is evalu-
ated on 153 verbs taken from Levin, 53 of which are polysemous (i.e., belong to more
than one class). The size of the derived clusters is restricted to four verbs and compared
to Levin: Verbs are classified correctly if they are members of a nonsingleton cluster
67
Lapata and Brew Verb Class Disambiguation Using Informative Priors
that is a subset of a Levin class. Polysemous verbs can be assigned to distinct clusters
only using the latent-class clustering method. The best results achieve a recall of 36%
and a precision of 61% (over a baseline of 5%, calculated as the number of randomly
created clusters that are subsets of a Levin class) using subcategorization information
only and iterative clustering. Inclusion of information about selectional restrictions
yields a lower accuracy of 38% (with a recall of 20%), again using iterative clustering.
Dorr and Jones (1996) use Levin?s (1993) classification to show that there is a pre-
dictable relationship between verbal meaning and syntactic behavior. They create a
database of Levin verb classes and the sentences exemplifying them (including both
positive and negative examples, i.e., examples marked with asterisks). A parser is used
to extract basic syntactic patterns for each semantic class. These patterns form the syn-
tactic signature of the class. Dorr and Jones show that 97.9% of the semantic classes can
be identified uniquely by their syntactic signatures. Grouping verbs (instead of classes)
with identical signatures to form a semantic class yields a 6.3% overlap with Levin
classes. Dorr and Jones?s results are somewhat difficult to interpret, since in practice
information about a verb and its syntactic signature is not available, and it is pre-
cisely this information that is crucial for classifying verbs into Levin classes. Schulte
im Walde?s study and our own study show that acquisition of syntactic signatures
(i.e., subcategorization frames) from corpora is feasible; however, these acquired sig-
natures are not necessarily compatible with Levin and in most cases will depart from
those derived by Dorr and Jones, as negative examples are not available in real corpora.
Ciaramita and Johnson (2000) propose an unsupervised Bayesian model for dis-
ambiguating verbal objects that uses WordNet?s inventory of senses. For each verb
the model creates a Bayesian network whose architecture is determined by WordNet?s
hierarchy and whose parameters are estimated from a list of verb-object pairs found in
a corpus. A common problem for unsupervised models trained on verb-object tuples
is that the objects can belong to more than one semantic class. The class ambiguity
problem is commonly resolved by considering each observation of an object as evi-
dence for each of the classes the word belongs to. The formalization of the problem in
terms of Bayesian networks allows the contribution of different senses to be weighted
via explaining away (Pearl 1988): If A is a hyponym of B and C is a hyponym of B,
and B is true, then finding that C is true makes A less likely.
Prior knowledge about the likelihoods of concepts is hand coded in the network
according to the following principles: (1) It is unlikely that any given class will be
a priori selected for; (2) if a class is selected, then its hyponyms are also likely to be
selected; (3) a word is likely as the object of a verb, if at least one of its classes is selected
for. Likely and unlikely here correspond to numbers that sum up to to one. Ciaramita
and Johnson show that their model outperforms other word sense disambiguation
approaches that do not make use of prior knowledge.
Chao and Dyer (2000) propose a method for the disambiguation of polysemous
adjectives in adjective-noun combinations that also uses Bayesian networks and Word-
Net?s taxonomic information. Prior knowledge about the likelihood of different senses
or semantic classes is derived heuristically by submitting queries (e.g., great hurricane)
to the AltaVista search engine and extrapolating from the number of returned doc-
uments the frequency of the adjective-noun pair (see Mihalcea and Moldovan [1998]
for details of this technique). For each polysemous adjective-noun combination, the
synonyms representative of each sense are retrieved from WordNet (e.g., {great, large,
big} vs. {great, neat, good}). Queries are submitted to AltaVista for each synonym-noun
pair; the number of documents returned is used then as an estimate of how likely
the different adjective senses are. Chao and Dyer obtain better results when prior
knowledge is factored into their Bayesian network.
68
Computational Linguistics Volume 30, Number 1
Our work focuses on the ambiguity inherently present in Levin?s (1993) classifi-
cation. The problem is ignored by Merlo and Stevenson (2001), who focus only on
monosemous verbs. Polysemous verbs are included in Schulte im Walde?s (2000) ex-
periments: The clustering approach can go so far as to identify more than one class
for a given verb without, however, providing information about its dominant class.
We recast Levin?s classification in a statistical framework and show in agreement with
Merlo and Stevenson and Schulte im Walde that corpus-based distributions provide
important information for semantic classification, especially in the case of polysemous
verbs whose meaning cannot be easily inferred from the immediate surrounding con-
text (i.e., subcategorization). We additionally show that the derived model is useful
not only for determining the most likely overall class for a given verb (i.e., across the
corpus), but also for disambiguating polysemous verb tokens in context.
Like Schulte im Walde (2000), our approach relies on subcategorization frames
extracted from the BNC (although using a different methodology). We employ Levin?s
inventory of semantic classes, arriving at a finer-grained classification than Merlo and
Stevenson (2001). In contrast to Schulte im Walde, we do not attempt to discover Levin
classes from corpora; instead, we exploit Levin?s classification and corpus frequencies
in order to derive a distribution of verbs, classes, and their frames that is not known a
priori but is approximated using simplifications. Our approach is not particularly tied
to Levin?s exact classification. We have presented in this article a general framework
that could be extended to related classifications such as the semantic hierarchy pro-
posed by Dang et al (1998). In fact the latter may be more appropriate than Levin?s
original classification for our disambiguation experiments, as it is based on a tighter
correspondence between syntactic frames and semantic components and contains links
to the WordNet taxonomy.
Prior knowledge with regard to the likelihood of polysemous verb classes is ac-
quired automatically in an unsupervised manner by combining corpus frequencies
estimated from the BNC and information inherent in Levin. The models proposed by
Chao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicable
to Levin?s classification, as the latter is not a hierarchy (and therefore not a DAG) and
cannot be straightforwardly mapped into a Bayesian network. However, in agreement
with Chao and Dyer and Ciaramita and Johnson, we show that prior knowledge about
class preferences improves word sense disambiguation performance.
Unlike Schulte im Walde (2000) and Merlo and Stevenson (2001), we ignore infor-
mation about the arguments of a given verb in the form of either selectional restric-
tions or argument structure while building our prior models. The latter information is,
however, indirectly taken into account in our disambiguation experiments: The verbs?
arguments are features for our naive Bayesian classifiers. Such information can be also
incorporated into the prior model in the form of conditional probabilities, where the
verb is, for example, conditioned on the thematic role of its arguments if this is known
(see Gildea and Jurafsky [2000] for a method that automatically labels thematic roles).
Unlike Stevenson and Merlo, Schulte im Walde, and Dorr and Jones (1996), we pro-
vide a general probabilistic model that assigns a probability to each class of a given
verb by calculating the probability of a complex expression in terms of the probability
of simpler expressions that compose it. We further show that this model is useful for
disambiguating polysemous verbs in context.
Appendix: Disambiguation Results with Co-occurrences
Figures 6?9 show the performances of our naive Bayesian classifier when co-occurrences
are used as features. We experimented with four types of context: left context (Left),
69
Lapata and Brew Verb Class Disambiguation Using Informative Priors
right context (Right), sentential context (Sentence), and the sentence within which
the ambiguous verb is found together with its immediately preceding sentence
(PSentence). The context was encoded as lemmas or parts of speech.
(a) (b)
Figure 6
Word sense disambiguation accuracy for ?NP1 V NP2? frame.
(a) (b)
Figure 7
Word sense disambiguation accuracy for ?NP1 V NP2 NP3? frame.
70
Computational Linguistics Volume 30, Number 1
(a) (b)
Figure 8
Word sense disambiguation accuracy for ?NP1 V to NP2 NP3? frame.
(a) (b)
Figure 9
Word sense disambiguation accuracy for ?NP1 V for NP2 NP3? frame.
71
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Acknowledgments
Mirella Lapata was supported by ESRC
grant number R000237772. Thanks to Frank
Keller, Alex Lascarides, Katja Markert, Paola
Merlo, Sabine Schulte im Walde, Stacey
Bailey, Markus Dickinson, Anna Feldman,
Anton Rytting, and two anonymous
reviewers for valuable comments.
References
Boguraev, Branimir K. and Ted Briscoe. 1989.
Utilising the LDOCE grammar codes. In
Ted Briscoe and Branimir K. Boguraev,
editors, Computational Lexicography for
Natural Language Processing. Longman,
London, pages 85?116.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC.
Briscoe, Ted and Ann Copestake. 1999.
Lexical rules in constraint-based grammar.
Computational Linguistics, 25(4):487?526.
Burnard, Lou, 1995. The Users Reference Guide
for the British National Corpus. British
National Corpus Consortium, Oxford
University Computing Service.
Carroll, Glenn and Mats Rooth. 1998.
Valence induction with a head-lexicalized
PCFG. In Nancy Ide and Atro Voutilainen,
editors, Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 36?45, Granada, Spain.
Chao, Gerald and Michael G. Dyer. 2000.
Word sense disambiguation of adjectives
using probabilistic networks. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 152?158, Saarbru?cken, Germany.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional restrictions with
Bayesian networks. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 187?193,
Saarbru?cken, Germany.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:37?46.
Corley, Steffan, Martin Corley, Frank Keller,
Matthew W. Crocker, and Shari Trewin.
2001. Finding syntactic structure in
unparsed corpora: The Gsearch corpus
query system. Computers and the
Humanities, 35(2):81?94.
Cucerzan, Silviu and David Yarowsky. 2002.
Augmented mixture models for lexical
disambiguation. In Jan Hajic? and Yuji
Matsumoto, editors, Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 33?40,
Philadelphia, PA.
Dang, Hoa Trang, Karin Kipper, Martha
Palmer, and Joseph Rosenzweig. 1998.
Investigating regular sense extensions
based on intersective Levin classes. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 293?299,
Montre?al, Que?bec, Canada.
Dang, Hoa Trang, Joseph Rosenzweig, and
Martha Palmer. 1997. Associating
semantic components with intersective
Levin classes. In Proceedings of the First
AMTA SIG-IL Workshop on Interlinguas,
pages 1?8, San Diego, CA.
Dorr, Bonnie J. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation.
Machine Translation, 12(4):371?322.
Dorr, Bonnie J. and Doug Jones. 1996. Role
of word sense disambiguation in lexical
acquisition: Predicting semantics from
syntactic cues. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 322?327, Copenhagen,
Denmark.
Duda, Richard O. and Peter E. Hart. 1973.
Pattern Classification and Scene Analysis.
Wiley, New York.
Fillmore, Charles. 1965. Indirect Object
Constructions and the Ordering of
Transformations. Mouton, The Hague.
Gale, William, Kenneth W. Church, and
David Yarowsky. 1992a. Estimating upper
and lower bounds on the performance of
word-sense disambiguation programs. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics,
pages 249?256, Columbus, OH.
Gale, William A., Kenneth W. Church, and
David Yarowsky. 1992b. A method for
disambiguating word senses in a large
corpus. Computers and the Humanities,
26(5?6):415?439.
Gildea, Daniel and Daniel Jurafsky. 2000.
Automatic labelling of semantic roles. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
Hong Kong.
Goldberg, Adele. 1995. Constructions.
University of Chicago Press, Chicago.
Green, Georgia. 1974. Semantics and Syntactic
Regularity. Indiana University Press,
Bloomington.
Gropen, Jess, Steven Pinker,
Michelle Hollander, Richard M. Goldberg,
and Ronald Wilson. 1989. The learnability
and acquisition of the dative alternation.
72
Computational Linguistics Volume 30, Number 1
Language, 65(2):203?257.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Ide, Nancy and Jean Ve?ronis. 1998.
Introduction to the special issue on word
sense disambiguation: The state of the art.
Computational Linguistics, 24(1):1?40.
Jackendoff, Ray. 1983. Semantics and
Cognition. MIT Press, Cambridge, MA.
Johnson, William E. 1932. Probability: The
deductive and inductive problems. Mind,
49:409?423.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction of a
verb lexicon. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 691?696, Austin, TX.
Klavans, Judith and Min-Yen Kan. 1998. Role
of verbs in document analysis. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 680?688,
Montre?al, Que?bec, Canada.
Kupiec, Julian. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language, 6(3):225?242.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato?s problem: The
latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211?240.
Lapata, Maria. 1999. Acquiring lexical
generalizations from corpora: A case study
for diathesis alternations. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 397?404,
College Park, MD.
Lapata, Maria. 2001. The Acquisition and
Modeling of Lexical Knowledge: A
Corpus-Based Investigation of Systematic
Polysemy. Ph.D. thesis, University of
Edinburgh.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on Compound
Nouns. Ph.D. thesis, Macquarie University,
Sydney, Australia.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levow, Gina-Anne, Bonnie Dorr, and
Dekang Lin. 2000. Construction of
Chinese-English semantic hierarchy for
information retrieval. Technical report,
University of Maryland, College Park.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In
Proceedings of the First North American
Annual Meeting of the Association for
Computational Linguistics, pages 256?263,
Seattle, WA.
Merlo, Paola and Susanne Stevenson. 2001.
Automatic verb classification based on
statistical distribution of argument
structure. Computational Linguistics,
27(3):373?408.
Mihalcea, Rada and Dan Moldovan. 1998.
Word sense disambiguation based on
semantic density. In Sanda Harabagiu,
editor, Proceedings of COLING/ACL
Workshop on Usage of WordNet in Natural
Language Processing, pages 16?22,
Montre?al, Que?bec, Canada.
Miller, George A. and William G. Charles.
1991. Contextual correlates of semantic
similarity. Language and Cognitive Processes,
6(1):1?28.
Mooney, Raymond J. 1996. Comparative
experiments on disambiguating word
senses: An illustration of the role of bias
in machine learning. In Eric Brill and
Kenneth Church, editors, Proceedings of the
First Conference on Empirical Methods in
Natural Language Processing, pages 82?91,
Philadelphia, PA.
Ng, Hwee Tou. 1997. Exemplar-based word
sense disambiguation: Some recent
improvements. In Claire Cardie and
Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods
in Natural Language Processing, pages
208?216, Providence, RI.
Palmer, Martha. 2000. Consistent criteria for
sense distinctions. Computers and the
Humanities, 34(1?2):217?222.
Palmer, Martha and Zhibiao Wu. 1995. Verb
semantics for English-Chinese translation.
Machine Translation, 10:59?92.
Patel, Malti, John A. Bullinaria, and
Joseph P. Levy. 1998. Extracting semantic
representations from large text corpora. In
John A. Bullinaria, D. W. Glasspool, and
G. Houghton, editors, Proceedings of the
Fourth Workshop on Neural Computation and
Psychology, pages 199?212. Springer,
Berlin.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems. Morgan Kaufmann,
San Mateo, CA.
Pedersen, Ted. 2000. A simple approach to
building ensembles of naive Bayesian
classifiers for word sense disambiguation.
In Proceedings of the First North American
Annual Meeting of the Association for
Computational Linguistics, pages 63?69,
Seattle, WA.
Pedersen, Ted. 2001. A decision tree of
bigrams is an accurate predictor of word
sense. In Proceedings of the Second North
73
Lapata and Brew Verb Class Disambiguation Using Informative Priors
American Annual Meeting of the Association
for Computational Linguistics, pages 63?69,
Pittsburgh, PA.
Pedersen, Ted and Rebecca Bruce. 1998.
Knowledge lean word-sense
disambiguation. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 800?805, Madison, WI.
Pinker, Steven. 1989. Learnability and
Cognition: The Acquisition of Argument
Structure. MIT Press, Cambridge, MA.
Resnik, Philip Stuart. 1993. Selection and
Information: A Class-Based Approach to Lexical
Relationships. Ph.D. thesis, University of
Pennsylvania.
Schulte im Walde, Sabine. 2000. Clustering
verbs semantically according to their
alternation behaviour. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 747?753,
Saarbru?cken, Germany.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
Stede, Manfred. 1998. A generative
perspective on verb alternations.
Computational Linguistics, 24(3):401?430.
Talmy, Leonard. 1985. Lexicalisation
patterns: Semantic structure in lexical
forms. In Timothy Shopen, editor,
Language Typology and Syntactic Description,
III: Grammatical Categories and the Lexicon.
Cambrige University Press, Cambridge,
pages 57?149.
Yarowsky, David. 1994. Decision lists for
lexical ambiuguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics, pages 88?95, Las Cruces, NM.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 189?196, Cambridge,
MA.
The Web as a Baseline: Evaluating the Performance of
Unsupervised Web-based Models for a Range of NLP Tasks
Mirella Lapata
Department of Computer Science
University of Sheffield
211 Portobello St., Sheffield S1 4DP
mlap@dcs.shef.ac.uk
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Pl., Edinburgh EH8 9LW
keller@inf.ed.ac.uk
Abstract
Previous work demonstrated that web counts
can be used to approximate bigram frequen-
cies, and thus should be useful for a wide va-
riety of NLP tasks. So far, only two gener-
ation tasks (candidate selection for machine
translation and confusion-set disambiguation)
have been tested using web-scale data sets. The
present paper investigates if these results gener-
alize to tasks covering both syntax and seman-
tics, both generation and analysis, and a larger
range of n-grams. For the majority of tasks, we
find that simple, unsupervised models perform
better when n-gram frequencies are obtained
from the web rather than from a large corpus.
However, in most cases, web-based models fail
to outperform more sophisticated state-of-the-
art models trained on small corpora. We ar-
gue that web-based models should therefore be
used as a baseline for, rather than an alternative
to, standard models.
1 Introduction
Keller and Lapata (2003) investigated the validity of web
counts for a range of predicate-argument bigrams (verb-
object, adjective-noun, and noun-noun bigrams). They
presented a simple method for retrieving bigram counts
from the web by querying a search engine and demon-
strated that web counts (a) correlate with frequencies ob-
tained from a carefully edited, balanced corpus such as
the 100M words British National Corpus (BNC), (b) cor-
relate with frequencies recreated using smoothing meth-
ods in the case of unseen bigrams, (c) reliably predict hu-
man plausibility judgments, and (d) yield state-of-the-art
performance on pseudo-disambiguation tasks.
Keller and Lapata?s (2003) results suggest that web-
based frequencies can be a viable alternative to bigram
frequencies obtained from smaller corpora or recreated
using smoothing. However, they do not demonstrate that
realistic NLP tasks can benefit from web counts. In or-
der to show this, web counts would have to be applied to
a diverse range of NLP tasks, both syntactic and seman-
Task n POS Ling Type
MT candidate select. 1,2 V, N Sem Generation
Spelling correction 1,2,3 Any Syn/Sem Generation
Adjective ordering 1,2 Adj Sem Generation
Compound bracketing 1,2 N Syn Analysis
Compound interpret. 1,2,3 N, P Sem Analysis
Countability detection 1,2 N, Det Sem Analysis
Table 1: Overview of the tasks investigated in this paper
(n: size of n-gram; POS: parts of speech; Ling: linguistic
knowledge; Type: type of task)
tic, involving analysis (e.g., disambiguation) and gener-
ation (e.g., selection among competing outputs). Also, it
remains to be shown that the web-based approach scales
up to larger n-grams (e.g., trigrams), and to combinations
of different parts of speech (Keller and Lapata 2003 only
tested bigrams involving nouns, verbs, and adjectives).
Another important question is whether web-based meth-
ods, which are by definition unsupervised, can be com-
petitive alternatives to supervised approaches used for
most tasks in the literature.
This paper aims to address these questions. We start by
using web counts for two generation tasks for which the
use of large data sets has shown promising results: (a) tar-
get language candidate selection for machine translation
(Grefenstette, 1998) and (b) context sensitive spelling
correction (Banko and Brill, 2001a,b). Then we investi-
gate the generality of the web-based approach by apply-
ing it to a range of analysis and generations tasks, involv-
ing both syntactic and semantic knowledge: (c) ordering
of prenominal adjectives, (d) compound noun bracketing,
(e) compound noun interpretation, and (f) noun count-
ability detection. Table 1 gives an overview of these tasks
and their properties.
In all cases, we propose a simple, unsupervised n-gram
based model whose parameters are estimated using web
counts. We compare this model both against a baseline
(same model, but parameters estimated on the BNC) and
against state-of-the-art models from the literature, which
are either supervised (i.e., use annotated training data) or
unsupervised but rely on taxonomies to recreate missing
counts.
2 Method
Following Keller and Lapata (2003), web counts for n-
grams were obtained using a simple heuristic based on
queries to the search engine Altavista.1 In this approach,
the web count for a given n-gram is simply the number of
hits (pages) returned by the search engine for the queries
generated for this n-gram. Three different types of queries
were used for the NLP tasks in the present paper:
Literal queries use the quoted n-gram directly as a
search term for Altavista (e.g., the bigram history changes
expands to the query "history changes").
Near queries use Altavista?s NEAR operator to ex-
pand the n-gram; a NEAR b means that a has to oc-
cur in the same ten word window as b; the window is
treated as a bag of words (e.g., history changes expands
to "history" NEAR "changes").
Inflected queries are performed by expanding an
n-gram into all its morphological forms. These forms
are then submitted as literal queries, and the result-
ing hits are summed up (e.g., history changes ex-
pands to "history change", "histories change",
"history changed", etc.). John Carroll?s suite of mor-
phological tools (morpha, morphg, and ana) was used
to generate inflected forms of verbs and nouns.2 In cer-
tain cases (detailed below), determiners were inserted be-
fore nouns in order to make it possible to recognize sim-
ple NPs. This insertion was limited to a/an, the, and the
empty determiner (for bare plurals).
All queries (other than the ones using the NEAR oper-
ator) were performed as exact matches (using quotation
marks in Altavista). All search terms were submitted to
the search engine in lower case. If a query consists of a
single, highly frequent word (such as the), Altavista will
return an error message. In these cases, we set the web
count to a large constant (108). This problem is limited
to unigrams, which were used in some of the models de-
tailed below. Sometimes the search engine fails to return
a hit for a given n-gram (for any of its morphological vari-
ants). We smooth zero counts by setting them to .5.
For all tasks, the web-based models are compared
against identical models whose parameters were esti-
mated from the BNC (Burnard, 1995). The BNC is a
static 100M word corpus of British English, which is
about 1000 times smaller than the web (Keller and La-
pata, 2003). Comparing the performance of the same
model on the web and on the BNC allows us to assess
how much improvement can be expected simply by using
a larger data set. The BNC counts were retrieved using
the Gsearch corpus query tool (Corley et al, 2001); the
morphological query expansion was the same as for web
queries; the NEAR operator was simulated by assuming
a window of five words to the left and five to the right.
1We did not use Google counts, as Google limits the number
of queries to 1000 per day, which makes the process of retriev-
ing a large number of web counts very time consuming.
2The tools can be downloaded from http://www.cogs.
susx.ac.uk/lab/nlp/carroll/morph.html.
# best model on development set
? 6 ? (not) sign. different from best BNC model on test set
? 6 ? (not) sign. different from baseline
? 6 ? (not) sign. different from best model in the literature
Table 2: Meaning of diacritics indicating statistical sig-
nificance (?2 tests)
Gsearch was used to search solely for adjacent words; no
POS information was incorporated in the queries, and no
parsing was performed.
For all of our tasks, we have to select either the best of
several possible models or the best parameter setting for
a single model. We therefore require a separate develop-
ment set. This was achieved by using the gold standard
data set from the literature for a given task and randomly
dividing it into a development set and a test set (of equal
size). We report the test set performance for all models
for a given task, and indicate which model shows optimal
performance on the development set (marked by a ?#? in
all subsequent tables). We then compare the test set per-
formance of this optimal model to the performance of the
models reported in the literature. It is important to note
that the figures taken from the literature were typically
obtained on the whole gold standard data set, and hence
may differ from the performance on our test set. We work
on the assumption that such differences are negligible.
We use ?2 tests to determine whether the performance
of the best web model on the test set is significantly differ-
ent from that of the best BNC model. We also determine
whether both models differ significantly from the base-
line and from the best model in the literature. A set of
diacritics is used to indicate significance throughout this
paper, see Table 2.
3 Candidate Selection for Machine
Translation
Target word selection is a generation task that occurs in
machine translation (MT). A word in a source language
can often be translated into different words in the target
language and the choice of the appropriate translation de-
pends on a variety of semantic and pragmatic factors. The
task is illustrated in (1) where there are five translation al-
ternatives for the German noun Geschichte listed in curly
brackets, the first being the correct one.
(1) a. Die Geschichte a?ndert sich, nicht jedoch die
Geographie.
b. {History, story, tale, saga, strip} changes but
geography does not.
Statistical approaches to target word selection rely on
bilingual lexica to provide all possible translations of
words in the source language. Once the set of translation
candidates is generated, statistical information gathered
from target language corpora is used to select the most
appropriate alternative (Dagan and Itai, 1994). The task is
somewhat simplified by Grefenstette (1998) and Prescher
et al (2000) who do not produce a translation of the en-
tire sentence. Instead, they focus on specific syntactic re-
lations. Grefenstette translates compounds from German
and Spanish into English, and uses BNC frequencies as
a filter for candidate translations. He observes that this
approach suffers from an acute data sparseness problem
and goes on to obtain counts for candidate compounds
through web searches, thus achieving a translation accu-
racy of 86?87%.
Prescher et al (2000) concentrate on verbs and their
objects. Assuming that the target language translation of
the verb is known, they select from the candidate transla-
tions the noun that is semantically most compatible with
the verb. The semantic fit between a verb and its argument
is modeled using a class-based lexicon that is derived
from unlabeled data using the expectation maximization
algorithm (verb-argument model). Prescher et al also
propose a refined version of this approach that only mod-
els the fit between a verb and its object (verb-object
model), disregarding other arguments of the verb. The
two models are trained on the BNC and evaluated against
two corpora of 1,340 and 814 bilingual sentence pairs,
with an average of 8.63 and 2.83 translations for the ob-
ject noun, respectively. Table 4 lists Prescher et al?s re-
sults for the two corpora and for both models together
with a random baseline (select a target noun at random)
and a frequency baseline (select the most frequent target
noun).
Grefenstette?s (1998) evaluation was restricted to com-
pounds that are listed in a dictionary. These com-
pounds are presumably well-established and fairly fre-
quent, which makes it easy to obtain reliable web fre-
quencies. We wanted to test if the web-based approach
extends from lexicalized compounds to productive syn-
tactic units for which dictionary entries do not exist. We
therefore performed our evaluation using Prescher et al?s
(2000) test set of verb-object pairs. Web counts were re-
trieved for all possible verb-object translations; the most
likely one was selected using either co-occurrence fre-
quency ( f (v,n)) or conditional probability ( f (v,n)/ f (n)).
The web counts were gathered using inflected queries in-
volving the verb, a determiner, and the object (see Sec-
tion 2). Table 3 compares the web-based models against
the BNC models. For both the high ambiguity and the
low ambiguity data set, we find that the performance
of the best Altavista model is not significantly different
from that of the best BNC model. Table 4 compares our
simple, unsupervised methods with the two sophisticated
class-based models discussed above. The results show
that there is no significant difference in performance be-
tween the best model reported in the literature and the
best Altavista or the best BNC model. However, both
models significantly outperform the baseline. This holds
for both the high and low ambiguity data sets.
Altavista BNC
high low high low
Model ambig ambig ambig ambig
f (v,n) 45.74 68.73#6 ? 45.89# 70.06#
f (v,n)/ f (n) 45.16#6 ? 64.96 46.18 66.07
Table 3: Performance of Altavista counts and BNC counts
for candidate selection for MT (data from Prescher et al
2000)
high low
Model ambig ambig
Random baseline 14.20 45.90
Frequency baseline 31.90 45.50
Prescher et al (2000): verb-argument 43.30 61.50
Best Altavista 45.16?6 ? 68.73?6 ?
Best BNC 45.89?6 ? 70.06?6 ?
Prescher et al (2000): verb-object 49.40 68.20
Table 4: Performance comparison with the literature for
candidate selection for MT
4 Context-sensitive Spelling Correction
Context-sensitive spelling correction is the task of cor-
recting spelling errors that result in valid words. Such
a spelling error is illustrated in (4) where principal was
typed when principle was intended.
(2) Introduction of the dialogue principal proved strik-
ingly effective.
The task can be viewed as generation task, as it consists
of choosing between alternative surface realizations of a
word. This choice is typically modeled by confusion sets
such as {principal, principle} or {then, than} under the
assumption that each word in the set could be mistakenly
typed when another word in the set was intended. The
task is to infer which word in a confusion set is the cor-
rect one in a given context. This choice can be either syn-
tactic (as for {then, than}) or semantic (as for {principal,
principle}).
A number of machine learning methods have been pro-
posed for context-sensitive spelling correction. These in-
clude a variety of Bayesian classifiers (Golding, 1995;
Golding and Schabes, 1996), decision lists (Golding,
1995) transformation-based learning (Mangu and Brill,
1997), Latent Semantic Analysis (LSA) (Jones and
Martin, 1997), multiplicative weight update algorithms
(Golding and Roth, 1999), and augmented mixture mod-
els (Cucerzan and Yarowsky, 2002). Despite their differ-
ences, most approaches use two types of features: context
words and collocations. Context word features record the
presence of a word within a fixed window around the tar-
get word (bag of words); collocational features capture
the syntactic environment of the target word and are usu-
ally represented by a small number of words and/or part-
of-speech tags to the left or right of the target word.
The results obtained by a variety of classification meth-
ods are given in Table 6. All methods use either the full
set or a subset of 18 confusion sets originally gathered by
Golding (1995). Most methods are trained and tested on
Model Alta BNC Model Alta BNC
f (t) 72.98 70.00 f (w1, t,w2)/ f (t) 87.77 76.33
f (w1, t) 84.40 83.02 f (w1,w2, t)/ f (t) 86.27 74.47
f (t,w1) 84.89 82.74 f (t,w2,w2)/ f (t) 84.94 74.23
f (w1, t,w2) 89.24#*77.13 f (w1, t,w2)/ f (w1, t) 80.70 73.69
f (w1,w2, t) 87.13 74.89 f (w1, t,w2)/ f (t,w2) 82.24 75.10
f (t,w1,w2) 84.68 75.08 f (w1,w2, t)/ f (w2, t) 72.11 69.28
f (w1, t)/ f (t) 82.81 77.84 f (t,w1,w2)/ f (t,w1) 75.65 72.57
f (t,w1)/ f (t) 77.49 80.71#
Table 5: Performance of Altavista counts and BNC
counts for context sensitive spelling correction (data from
Cucerzan and Yarowsky 2002)
Model Accuracy
Baseline BNC 70.00
Baseline Altavista 72.98
Best BNC 80.71??
Golding (1995) 81.40
Jones and Martin (1997) 84.26
Best Altavista 89.24??
Golding and Schabes (1996) 89.82
Mangu and Brill (1997) 92.79
Cucerzan and Yarowsky (2002) 92.20
Golding and Roth (1999) 94.23
Table 6: Performance comparison with the literature for
context sensitive spelling correction
the Brown corpus, using 80% for training and 20% for
testing.3
We devised a simple, unsupervised method for
performing spelling correction using web counts.
The method takes into account collocational features,
i.e., words that are adjacent to the target word. For each
word in the confusion set, we used the web to estimate
how frequently it co-occurs with a word or a pair of words
immediately to its left or right. Disambiguation is then
performed by selecting the word in the confusion set with
the highest co-occurrence frequency or probability. The
web counts were retrieved using literal queries (see Sec-
tion 2). Ties are resolved by comparing the unigram fre-
quencies of the words in the confusion set and defaulting
to the word with the highest one. Table 5 shows the types
of collocations we considered and their corresponding ac-
curacy. The baseline ( f (t)) in Table 5 was obtained by
always choosing the most frequent unigram in the confu-
sion set. We used the same test set (2056 tokens from the
Brown corpus) and confusion sets as Golding and Sch-
abes (1996), Mangu and Brill (1997), and Cucerzan and
Yarowsky (2002).
Table 5 shows that the best result (89.24%) for the web-
based approach is obtained with a context of one word
to the left and one word to the right of the target word
( f (w1, t,w2)). The BNC-based models perform consis-
tently worse than the web-based models with the excep-
tion of f (t,w1)/t; the best Altavista model performs sig-
nificantly better than the best BNC model. Table 6 shows
3An exception is Golding (1995), who uses the entire Brown
corpus for training (1M words) and 3/4 of the Wall Street Jour-
nal corpus (Marcus et al, 1993) for testing.
that both the best Altavista model and the best BNC
model outperform their respective baselines. A compari-
son with the literature shows that the best Altavista model
outperforms Golding (1995), Jones and Martin (1997)
and performs similar to Golding and Schabes (1996). The
highest accuracy on the task is achieved by the class of
multiplicative weight-update algorithms such as Winnow
(Golding and Roth, 1999). Both the best BNC model and
the best Altavista model perform significantly worse than
this model. Note that Golding and Roth (1999) use al-
gorithms that can handle large numbers of features and
are robust to noise. Our method uses a very small feature
set, it relies only on co-occurrence frequencies and does
not have access to POS information (the latter has been
shown to have an improvement on confusion sets whose
words belong to different parts of speech). An advantage
of our method is that it can be used for a large number
of confusion sets without relying on the availability of
training data.
5 Ordering of Prenominal Adjectives
The ordering of prenominal modifiers is important for
natural language generation systems where the text must
be both fluent and grammatical. For example, the se-
quence big fat Greek wedding is perfectly acceptable,
whereas fat Greek big wedding sounds odd. The ordering
of prenominal adjectives has sparked a great deal of the-
oretical debate (see Shaw and Hatzivassiloglou 1999 for
an overview) and efforts have concentrated on defining
rules based on semantic criteria that account for different
orders (e.g., age ? color, value ? dimension).
Data intensive approaches to the ordering problem rely
on corpora for gathering evidence for the likelihood of
different orders. They rest on the hypothesis that the rel-
ative order of premodifiers is fixed, and independent of
context and the noun being modified. The simplest strat-
egy is what Shaw and Hatzivassiloglou (1999) call di-
rect evidence. Given an adjective pair {a,b}, they count
how many times ?a,b? and ?b,a? appear in the corpus and
choose the pair with the highest frequency.
Unfortunately the direct evidence method performs
poorly when a given order is unseen in the training
data. To compensate for this, Shaw and Hatzivassiloglou
(1999) propose to compute the transitive closure of the
ordering relation: if a ? c and c ? b, then a ? b. Mal-
ouf (2000) further proposes a back-off bigram model
of adjective pairs for choosing among alternative orders
(P(?a,b?|{a,b}) vs. P(?b,a?|{a,b})). He also proposes
positional probabilities as a means of estimating how
likely it is for a given adjective a to appear first in a se-
quence by looking at each pair in the training data that
contains the adjective a and recording its position. Fi-
nally, he uses memory-based learning as a means to en-
code morphological and semantic similarities among dif-
ferent adjective orders. Each adjective pair ab is encoded
as a vector of 16 features (the last eight characters of a
and the last eight characters of b) and a class (?a,b? or
Model Altavista BNC
f (a1,a2) : f (a2,a1) 89.6#?6 ? 80.4#?
f (a1,a2)/ f (a2) : f (a2,a1)/ f (a1) 83.2 77.0
f (a1,a2)/ f (a1) : f (a2,a1)/ f (a2) 80.2 80.6
Malouf (2000): memory-based ? 91.0
Table 7: Performance of Altavista counts and BNC counts
for adjective ordering (data from Malouf 2000)
?b,a?).
Malouf (2000) extracted 263,838 individual pairs of
adjectives from the BNC which he randomly partitioned
into test (10%) and training data (90%) and evaluated
all the above methods for ordering prenominal adjec-
tives. His results showed that a memory-based classi-
fier that uses morphological information as well as po-
sitional probabilities as features outperforms all other
methods (see Table 7). For the ordering task we restricted
ourselves to the direct evidence strategy which simply
chooses the adjective order with the highest frequency
or probability (see Table 7). Web counts were obtained
by submitting literal queries to Altavista (see Section 2).
We used the same 263,838 adjective pairs that Malouf ex-
tracted from the BNC. These were randomly partitioned
into a training (90%) and test corpus (10%). The test
corpus contained 26,271 adjective pairs. Given that sub-
mitting 26,271 queries to Altavista would be fairly time-
consuming, a random sample of 1000 sequences was ob-
tained from the test corpus and the web frequencies of
these pairs were retrieved. The best Altavista model sig-
nificantly outperformed the best BNC model, as indicated
in Table 7. We also found that there was no significant
difference between the best Altavista model and the best
model reported by Malouf, a supervised method using
positional probability estimates from the BNC and mor-
phological variants.
6 Bracketing of Compound Nouns
The first analysis task we consider is the syntactic disam-
biguation of compound nouns, which has received a fair
amount of attention in the NLP literature (Pustejovsky
et al, 1993; Resnik, 1993; Lauer, 1995). The task can
be summarized as follows: given a three word compound
n1 n3 n3, determine the correct binary bracketing of the
word sequence (see (3) for an example).
(3) a. [[backup compiler] disk]
b. [backup [compiler disk]]
Previous approaches typically compare different brack-
etings and choose the most likely one. The adjacency
model compares [n1 n2] against [n2 n3] and adopts a right
branching analysis if [n2 n3] is more likely than [n1 n2].
The dependency model compares [n1 n2] against [n1 n3]
and adopts a right branching analysis if [n1 n3] is more
likely than [n1 n2].
The simplest model of compound noun disambiguation
compares the frequencies of the two competing analyses
and opts for the most frequent one (Pustejovsky et al,
Model Alta BNC
Baseline 63.93 63.93
f (n1,n2) : f (n2,n3) 77.86 66.39
f (n1,n2) : f (n1,n3) 78.68#? 65.57
f (n1,n2)/ f (n1) : f (n2,n3)/ f (n2) 68.85 65.57
f (n1,n2)/ f (n2) : f (n2,n3)/ f (n3) 70.49 63.11
f (n1,n2)/ f (n2) : f (n1,n3)/ f (n3) 80.32 66.39
f (n1,n2) : f (n2,n3) (NEAR) 68.03 63.11
f (n1,n2) : f (n1,n3) (NEAR) 71.31 67.21
f (n1,n2)/ f (n1) : f (n2,n3)/ f (n2) (NEAR) 61.47 62.29
f (n1,n2)/ f (n2) : f (n2,n3)/ f (n3) (NEAR) 65.57 57.37
f (n1,n2)/ f (n2) : f (n1,n3)/ f (n3) (NEAR) 75.40 68.03#
Table 8: Performance of Altavista counts and BNC counts
for compound bracketing (data from Lauer 1995)
Model Accuracy
Baseline 63.93
Best BNC 68.036 ??
Lauer (1995): adjacency 68.90
Lauer (1995): dependency 77.50
Best Altavista 78.68?6 ?
Lauer (1995): tuned 80.70
Upper bound 81.50
Table 9: Performance comparison with the literature for
compound bracketing
1993). Lauer (1995) proposes an unsupervised method
for estimating the frequencies of the competing brack-
etings based on a taxonomy or a thesaurus. He uses a
probability ratio to compare the probability of the left-
branching analysis to that of the right-branching (see (4)
for the dependency model and (5) for the adjacency
model).
Rdep =
?
ti?cats(wi)
P(t1 ? t2)P(t2 ? t3)
?
ti?cats(wi)
P(t1 ? t3)P(t2 ? t3)
(4)
Radj =
?
ti?cats(wi)
P(t1 ? t2)
?
ti?cats(wi)
P(t2 ? t3)
(5)
Here t1, t2 and t3 are conceptual categories in the taxon-
omy or thesaurus, and the nouns w1 . . .wi are members of
these categories. The estimation of probabilities over con-
cepts (rather than words) reduces the number of model
parameters and effectively decreases the amount of train-
ing data required. The probability P(t1 ? t2) denotes the
modification of a category t2 by a category t1.
Lauer (1995) tested both the adjacency and de-
pendency models on 244 compounds extracted from
Grolier?s encyclopedia, a corpus of 8 million words. Fre-
quencies for the two models were obtained from the same
corpus and from Roget?s thesaurus (version 1911) by
counting pairs of nouns that are either strictly adjacent or
co-occur within a window of a fixed size (e.g., two, three,
fifty, or hundred words). The majority of the bracketings
in our test set were left-branching, yielding a baseline
of 63.93% (see Table 9). Lauer?s best results (77.50%)
were obtained with the dependency model and a training
scheme which takes strictly adjacent nouns into account.
Performance increased further by 3.2% when POS tags
were taken into account. The results for this tuned model
are also given in Table 9. Finally, Lauer conducted an ex-
periment with human judges to assess the upper bound
for the bracketing task. An average accuracy of 81.50%
was obtained.
We replicated Lauer?s (1995) results for compound
noun bracketing using the same test set. We compared
the performance of the adjacency and dependency mod-
els (see (4) and (5)), but instead of relying on a corpus and
a thesaurus, we estimated the relevant probabilities us-
ing web counts. The latter were obtained using inflected
queries (see Section 2) and Altavista?s NEAR operator.
Ties were resolved by defaulting to the most frequent
analysis (i.e., left-branching). To gauge the performance
of the web-based models we compared them against their
BNC-based alternatives; the performance of the best Al-
tavista model was significantly higher than that of the
best BNC model (see Table 8). A comparison with the
literature (see Table 9) shows that the best BNC model
fails to significantly outperform the baseline, and it per-
forms significantly worse than the best model in the liter-
ature (Lauer?s tuned model). The best Altavista model, on
the other hand, is not significantly different from Lauer?s
tuned model and significantly outperforms the baseline.
Hence we achieve the same performance as Lauer with-
out recourse to a predefined taxonomy or a thesaurus.
7 Interpretation of Compound Nouns
The second analysis task we consider is the semantic
interpretation of compound nouns. Most previous ap-
proaches to this problem have focused on the interpre-
tation of two word compounds whose nouns are related
via a basic set of semantic relations (e.g., CAUSE relates
onion tears, FOR relates pet spray). The majority of pro-
posals are symbolic and therefore limited to a specific
domain due to the large effort involved in hand-coding
semantic information (see Lauer 1995 for an extensive
overview).
Lauer (1995) is the first to propose and evaluate an un-
supervised probabilistic model of compound noun inter-
pretation for domain independent text. By recasting the
interpretation problem in terms of paraphrasing, Lauer
assumes that the semantic relations of compound heads
and modifiers can be expressed via prepositions that (in
contrast to abstract semantic relations) can be found in a
corpus. For example, in order to interpret war story, one
needs to find in a corpus related paraphrases: story about
the war, story of the war, story in the war, etc. Lauer uses
eight prepositions for the paraphrasing task (of, for, in,
at, on, from, with, about). A simple model of compound
noun paraphrasing is shown in (6):
p? = argmax
p
P(p|n1,n2)(6)
Lauer (1995) points out that the above model contains
one parameter for every triple ?p,n1,n2?, and as a result
Model Altavista BNC
f (n1, p) f (p,n2) 50.71 27.85#
f (n1, p,n2) 55.71#* 11.42
f (n1, p) f (p,n2)/ f (p) 47.14 26.42
f (n1, p,n2)/ f (p) 55.00 10.71
Table 10: Performance of Altavista counts and BNC
counts for compound interpretation (data from Lauer
1995)
Model Accuracy
Best BNC 27.856 ??
Lauer (1995): concept-based 28.00
Baseline 33.00
Lauer (1995): word-based 40.00
Best Altavista 55.71??
Table 11: Performance comparison with the literature for
compound interpretation
hundreds of millions of training instances would be nec-
essary. As an alternative to (6), he proposes the model
in (7) which combines the probability of the modifier
given a certain preposition with the probability of the
head given the same preposition, and assumes that these
two probabilities are independent.
p? = argmax
p ?
t1 ? cats(n1)
t2 ? cats(n2)
P(t1|p)P(t2|p)(7)
Here, t1 and t2 represent concepts in Roget?s thesaurus.
Lauer (1995) also experimented with a lexicalized ver-
sion of (7) where probabilities are calculated on the basis
of word (rather than concept) frequencies which Lauer
obtained from Grolier?s encyclopedia heuristically via
pattern matching.
Lauer (1995) tested the model in (7) on 282 com-
pounds that he selected randomly from Grolier?s encyclo-
pedia and annotated with their paraphrasing prepositions.
The preposition of accounted for 33% of the paraphrases
in this data set (see Baseline in Table 11). The concept-
based model (see (7)) achieved an accuracy of 28% on
this test set, whereas its lexicalized version reached an
accuracy of 40% (see Table 11).
We attempted the interpretation task with the lexi-
calized version of the bigram model (see (7)), but also
tried the more data intensive trigram model (see (6)),
again in its lexicalized form. Furthermore, we experi-
mented with several conditional and unconditional vari-
ants of (7) and (6). Co-occurrence frequencies were es-
timated from the web using inflected queries (see Sec-
tion 2). Determiners were inserted before nouns result-
ing in queries of the type story/stories about and
about the/a/0 war/wars for the compound war story.
As shown in Table 10, the best performance was ob-
tained using the web-based trigram model ( f (n1, p,n2));
it significantly outperformed the best BNC model. The
comparison with the literature in Table 11 showed that
the best Altavista model significantly outperformed both
the baseline and the best model in the literature (Lauer?s
word-based model). The BNC model, on the other hand,
Altavista BNC
Model Count Uncount Count Uncount
f (n) 87.01 90.13 87.32# 90.39#
f (det,n) 88.38#6 ? 91.22#6 ? 51.01 50.23
f (det,n)/ f (n) 83.19 85.38 50.95 50.23
Backoff 87.01 89.80 ? ?
Table 12: Performance of Altavista counts and BNC
counts for noun countability detection (data from Bald-
win and Bond 2003)
achieved a performance that is not significantly different
from the baseline, and significantly worse than Lauer?s
best model.
8 Noun Countability Detection
The next analysis task that we consider is the problem
of determining the countability of nouns. Countability is
the semantic property that determines whether a noun can
occur in singular and plural forms, and affects the range
of permissible modifiers. In English, nouns are typically
either countable (e.g., one dog, two dogs) or uncountable
(e.g., some peace, *one peace, *two peaces).
Baldwin and Bond (2003) propose a method for auto-
matically learning the countability of English nouns from
the BNC. They obtain information about noun countabil-
ity by merging lexical entries from COMLEX (Grishman
et al, 1994) and the ALTJ/E Japanese-to-English seman-
tic transfer dictionary (Ikehara et al, 1991). Words are
classified into four classes: countable, uncountable, bi-
partite (e.g., trousers), and plural only (e.g., goods). A
memory-based classifier is used to learn the four-way dis-
tinction on the basis of several linguistically motivated
features such as: number of the head noun, number of the
modifier, subject-verb agreement, plural determiners.
We devised unsupervised models for the countability
learning task and evaluated their performance on Bald-
win and Bond?s (2003) test data. We concentrated solely
on countable and uncountable nouns, as they account
for the vast majority of the data. Four models were
tested: (a) compare the frequency of the singular and
plural forms of the noun; (b) compare the frequency of
determiner-noun pairs that are characteristic of countable
or uncountable nouns; the determiners used were many
for countable and much for uncountable ones; (c) same
as model (b), but the det-noun frequencies are normalized
by the frequency of the noun; (d) backoff: try to make
a decision using det-noun frequencies; if these are too
sparse, back off to singular/plural frequencies.
Unigram and bigram frequencies were estimated from
the web using literal queries; for models (a)?(c) a thresh-
old parameter was optimized on the development set (this
parameter determines the ratio of singular/plural frequen-
cies or det-noun frequencies above which a noun was
considered as countable). For model (b), an additional
backoff parameter was used, specifying the minimum fre-
quency that triggers backoff.
The models and their performance on the test set are
Model Count Uncount
Baseline 74.60 78.30
Best BNC 87.32?? 90.39??
Best Altavista 88.38?? 91.22??
Baldwin and Bond (2003) 93.90 95.20
Table 13: Performance comparison with the literature for
noun countability detection
listed in Table 12. The best Altavista model is the condi-
tional det-noun model ( f (det,n)/ f (n)), which achieves
88.38% on countable and 91.22% on uncountable nouns.
On the BNC, the simple unigram model performs best. Its
performance is not statistically different from that of the
best Altavista model. Note that for the BNC models, data
sparseness means the det-noun models perform poorly,
which is why the backoff model was not attempted here.
Table 13 shows that both the Altavista model and BNC
model significantly outperform the baseline (relative fre-
quency of the majority class on the gold-standard data).
The comparison with the literature shows that both the
Altavista and the BNC model perform significantly worse
than the best model proposed by Baldwin and Bond
(2003); this is a supervised model that uses many more
features than just singular/plural frequency and det-noun
frequency.
9 Conclusions
We showed that simple, unsupervised models using web
counts can be devised for a variety of NLP tasks. The
tasks were selected so that they cover both syntax and se-
mantics, both generation and analysis, and a wider range
of n-grams than have been previously used.
For all but two tasks (candidate selection for MT and
noun countability detection) we found that simple, un-
supervised models perform significantly better when n-
gram frequencies are obtained from the web rather than
from a standard large corpus. This result is consistent
with Keller and Lapata?s (2003) findings that the web
yields better counts than the BNC. The reason for this
seems to be that the web is much larger than the BNC
(about 1000 times); the size seems to compensate for
the fact that simple heuristics were used to obtain web
counts, and for the noise inherent in web data.
Our results were less encouraging when it comes to
comparisons with state-of-the-art models. We found that
in all but one case, web-based models fail to significantly
outperform the state of the art. The exception was com-
pound noun interpretation, for which the Altavista model
was significantly better than the Lauer?s (1995) model.
For three tasks (candidate selection for MT, adjective or-
dering, and compound noun bracketing), we found that
the performance of the web-based models was not signif-
icantly different from the performance of the best models
reported in the literature.
Note that for all the tasks we investigated, the best
performance in the literature was obtained by supervised
models that have access not only to simple bigram or tri-
gram frequencies, but also to linguistic information such
as part-of-speech tags, semantic restrictions, or context
(or a thesaurus, in the case of Lauer?s models). When un-
supervised web-based models are compared against su-
pervised methods that employ a wide variety of features,
we observe that having access to linguistic information
makes up for the lack of vast amounts of data.
Our results therefore indicate that large data sets such
as those obtained from the web are not the panacea that
they are claimed to be (at least implicitly) by authors
such as Grefenstette (1998) and Keller and Lapata (2003).
Rather, in our opinion, web-based models should be used
as a new baseline for NLP tasks. The web baseline indi-
cates how much can be achieved with a simple, unsuper-
vised model based on n-grams with access to a huge data
set. This baseline is more realistic than baselines obtained
from standard corpora; it is generally harder to beat, as
our comparisons with the BNC baseline throughout this
paper have shown.
Note that for certain tasks, the performance of a web
baseline model might actually be sufficient, so that the ef-
fort of constructing a sophisticated supervised model and
annotating the necessary training data can be avoided.
Another possibility that needs further investigation is the
combination of web-based models with supervised meth-
ods. This can be done with ensemble learning methods
or simply by using web-based frequencies (or probabil-
ities) as features (in addition to linguistically motivated
features) to train supervised classifiers.
Acknowledgments
We are grateful to Tim Baldwin, Silviu Cucerzan, Mark
Lauer, Rob Malouf, Detelef Prescher, and Adwait Ratna-
parkhi for making their data sets available.
References
Baldwin, Timothy and Francis Bond. 2003. Learning the count-
ability of English nouns from corpus data. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo, Japan, pages 463?470.
Banko, Michele and Eric Brill. 2001a. Mitigating the paucity-
of-data problem: Exploring the effect of training corpus size
on classifier performance for natural language processing.
In James Allan, editor, Proceedings of the 1st International
Conference on Human Language Technology Research. Mor-
gan Kaufmann, San Francisco.
Banko, Michele and Eric Brill. 2001b. Scaling to very very
large corpora for natural language disambiguation. In Pro-
ceedings of the 39th Annual Meeting of the Association for
Computational Linguistics. Toulouse, France.
Burnard, Lou. 1995. The Users Reference Guide for the British
National Corpus. British National Corpus Consortium, Ox-
ford University Computing Service.
Corley, Steffan, Martin Corley, Frank Keller, Matthew W.
Crocker, and Shari Trewin. 2001. Finding syntactic struc-
ture in unparsed corpora: The Gsearch corpus query system.
Computers and the Humanities 35(2):81?94.
Cucerzan, Silviu and David Yarowsky. 2002. Augmented mix-
ture models for lexical disambiguation. In Jan Hajic? and Yuji
Matsumoto, editors, Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing. Philadel-
phia, PA, pages 33?40.
Dagan, Ido and Alon Itai. 1994. Machine translation diver-
gences: A formal description and proposed solution. Com-
putational Linguistics 20(4):563?597.
Golding, Andrew R. 1995. A Bayesian hybrid method for
context-sensitive spelling correction. In David Yarowsky and
Kenneth W. Church, editors, Proceedings of the 3rd Work-
shop on Very Large Corpora. Cambridge, MA, pages 39?53.
Golding, Andrew R. and Dan Roth. 1999. A winnow-based
approach to context sensitive spelling correction. Machine
Learning 34(1?3):1?25.
Golding, Andrew R. and Yves Schabes. 1996. Combin-
ing trigram-based and feature-based methods for context-
sensitive spelling correction. In Proceedings of the 34th An-
nual Meeting of the Association for Computational Linguis-
tics. Santa Cruz, CA, pages 71?78.
Grefenstette, Gregory. 1998. The World Wide Web as a resource
for example-based machine translation tasks. In Proceedings
of the ASLIB Conference on Translating and the Computer.
London.
Grishman, Ralph, Catherine Macleod, and Adam Meyers. 1994.
COMLEX syntax: Building a computational lexicon. In Pro-
ceedings of the 15th International Conference on Computa-
tional Linguistics. Kyoto, Japan, pages 268?272.
Ikehara, Satoru, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
effects of new methods in ALT-J/E. In Proceedings of the
Third Machine Translation Summit. Washington, DC, pages
101?106.
Jones, Michael P. and James H. Martin. 1997. Contextual
spelling correction using latent semantic analysis. In Pro-
ceedings of the 5th Conference on Applied Natural Language
Processing. Washington, DC, pages 166?173.
Keller, Frank and Mirella Lapata. 2003. Using the web to obtain
frequencies for unseen bigrams. Computational Linguistics
29(3):459?484.
Lauer, Mark. 1995. Corpus statistics meet the noun compound:
Some empirical results. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguistics.
Cambridge, MA, pages 47?54.
Malouf, Robert. 2000. The order of prenominal adjectives in
natural language generation. In Proceedings of the 38th An-
nual Meeting of the Association for Computational Linguis-
tics. Hong Kong, pages 85?92.
Mangu, Lidia and Eric Brill. 1997. Automatic rule acquisi-
tion of spelling correction. In Proceedings of the 14th Inter-
national Conference on Machine Learning. Nashville, Ten-
nessee, pages 187?194.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn treebank. Computational Linguistics
19(2):313?330.
Prescher, Detlef, Stefan Riezler, and Mats Rooth. 2000. Using
a probabilistic class-based lexicon for lexical ambiguity res-
olution. In Proceedings of the 18th International Conference
on Computational Linguistics. Saarbru?cken, Germany, pages
649?655.
Pustejovsky, James, Sabine Bergler, and Peter Anick. 1993.
Lexical semantic techniques for corpus analysis. Computa-
tional Linguistics 19(3):331?358.
Resnik, Philip Stuart. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Ph.D. the-
sis, University of Pennsylvania.
Shaw, James and Vassilis Hatzivassiloglou. 1999. Ordering
among premodifiers. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics.
College Park, MD, pages 135?143.
Inferring Sentence-internal Temporal Relations
Mirella Lapata
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
mlap@dcs.shef.ac.uk
Alex Lascarides
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
alex@inf.ed.ac.uk
Abstract
In this paper we propose a data intensive ap-
proach for inferring sentence-internal tempo-
ral relations, which relies on a simple prob-
abilistic model and assumes no manual cod-
ing. We explore various combinations of fea-
tures, and evaluate performance against a gold-
standard corpus and human subjects perform-
ing the same task. The best model achieves
70.7% accuracy in inferring the temporal rela-
tion between two clauses and 97.4% accuracy
in ordering them, assuming that the temporal
relation is known.
1 Introduction
The ability to identify and analyse temporal information
is crucial for a variety of practical NLP applications such
as information extraction, question answering, and sum-
marisation. In multidocument summarisation, informa-
tion must be extracted, potentially fused, and synthesised
into a meaningful text. Knowledge about the temporal or-
der of events is important for determining what content
should be communicated (interpretation) but also for cor-
rectly merging and presenting information (generation).
In question answering one would like to find out when a
particular event occurred (e.g., When did X resign?) but
also to obtain information about how events relate to each
other (e.g., Did X resign before Y?).
Although temporal relations and their interaction
with discourse relations (e.g., Parallel, Result) have re-
ceived much attention in linguistics (Kamp and Reyle,
1993; Webber, 1991; Asher and Lascarides, 2003), the
automatic interpretation of events and their temporal re-
lations is beyond the capabilities of current open-domain
NLP systems. While corpus-based methods have acceler-
ated progress in other areas of NLP, they have yet to make
a substantial impact on the processing of temporal infor-
mation. This is partly due to the absence of readily avail-
able corpora annotated with temporal information, al-
though efforts are underway to develop treebanks marked
with temporal relations (Katz and Arosio, 2001) and de-
vise annotation schemes that are suitable for coding tem-
poral relations (Ferro et al, 2000; Setzer and Gaizauskas,
2001). Absolute temporal information has received some
attention (Wilson et al, 2001; Schilder and Habel, 2001;
Wiebe et al, 1998) and systems have been developed for
identifying and assigning referents to time expressions.
Although the treatment of time expressions is an im-
portant first step towards the automatic handling of tem-
poral phenomena, much temporal information is not ab-
solute but relative and not overtly expressed but implicit.
Consider the examples in (1) taken from Katz and Arosio
(2001). Native speakers can infer that John first met and
then kissed the girl and that he first left the party and then
walked home, even though there are no overt markers sig-
nalling the temporal order of the described events.
(1) a. John kissed the girl he met at a party.
b. Leaving the party, John walked home.
c. He remembered talking to her and asking her for her
name.
In this paper we describe a data intensive approach
that automatically captures information pertaining to the
temporal order and relations of events like the ones illus-
trated in (1). Of course trying to acquire temporal infor-
mation from a corpus that is not annotated with tempo-
ral relations, tense, or aspect seems rather futile. How-
ever, sometimes there are overt markers for temporal re-
lations, the conjunctions before, after, while, and when
being the most obvious, that make relational information
about events explicit:
(2) a. Leonard Shane, 65 years old, held the post of presi-
dent before William Shane, 37, was elected to it last
year.
b. The results were announced after the market closed.
c. Investors in most markets sat out while awaiting the
U.S. trade figures.
It is precisely this type of data that we will exploit for
making predictions about the order in which events oc-
curred when there are no obvious markers signalling tem-
poral ordering. We will assess the feasibility of such an
approach by initially focusing on sentence-internal tem-
poral relations. We will obtain sentences like the ones
shown in (2), where a main clause is connected to a sub-
ordinate clause with a temporal marker and we will de-
velop a probabilistic framework where the temporal re-
lations will be learned by gathering informative features
from the two clauses. This framework can then be used
for interpretation in cases where overt temporal markers
are absent (see the examples in (1)).
Practical NLP applications such as text summarisa-
tion and question answering place increasing demands not
only on the analysis but also on the generation of temporal
relations. For instance, non-extractive summarisers that
generate sentences by fusing together sentence fragments
(e.g., Barzilay 2003) must be able to determine whether or
not to include an overt temporal marker in the generated
text, where the marker should be placed, and what lexical
item should be used. We assess how appropriate our ap-
proach is when faced with the information fusion task of
determining the appropriate ordering among a temporal
marker and two clauses. We infer probabilistically which
of the two clauses is introduced by the marker, and effec-
tively learn to distinguish between main and subordinate
clauses.
2 The Model
Given a main clause and a subordinate clause attached to
it, our task is to infer the temporal marker linking the two
clauses. Formally, P
 
SM  t j  SS  represents the probability
that a marker t j relates a main clause SM and a subordinate
clause SS. We aim to identify which marker t j in the set
of possible markers T maximises P
 
SM  t j  SS  :
t  argmax
t j  T
P
 
SM  t j  SS 
 argmax
t j  T
P
 
SM  P
 
t j  SM  P
 
SS  SM  t j 
(3)
We ignore the term P
 
SM  in (3) as it is a constant and use
Bayes? Rule to derive P
 
SM  t j  from P
 
t j  SM  :
t  argmax
t j  T
P
 
t j  SM  P
 
SS  SM  t j 
 argmax
t j  T
P
 
t j  P
 
SM  t j  P
 
SS  SM  t j 
(4)
We will further assume that the likelihood of the
subordinate clause SS is conditionally independent of the
main clause SM (i.e., P
 
SS  SM  t j 	 P
 
SS  t j  ). The as-
sumption is clearly a simplification but makes the estima-
tion of the probabilities P
 
SM  t j  and P
 
SS  t j  more reli-
able in the face of sparse data.
t 

argmax
t j  T
P
 
t j  P
 
SM  t j  P
 
SS  t j (5)
SM and SS are vectors of features a 
 M  1 a 
 M  n and
a 
 S  1 a 
 S  n characteristic of the propositions occurring
with the marker t j (our features are described in detail
in Section 3.2). By making the simplifying assumption
that these features are conditionally independent given
the temporal marker, the probability of observing the con-
junctions a 
 M  1 a 
 M  n and a 
 S  1 a 
 S  n is:
t   argmax
t j  T
P
 
t j  ?
i
P
 
a 
 M  i  t j  P
 
a 
 S  i  t j (6)
We effectively treat the temporal interpretation prob-
lem as a disambiguation task. From the (confusion) set T
of temporal markers  after, before, while, when, as, once,
until, since  , we select the one that maximises (6). We
compiled a list of temporal markers from Quirk et al
(1985). Markers with corpus frequency less than 10 per
million were excluded from our confusion set (see Sec-
tion 3.1 for a description of our corpus).
The model in (6) is simplistic in that the relation-
ships between the features across the clauses are not cap-
tured directly. However, if two values of these features
for the main and subordinate clauses co-occur frequently
with a particular marker, then the conditional probabil-
ity of these features on that marker will approximate the
right biases. Also note that some of these markers are am-
biguous with respect to their meaning: one sense of while
denotes overlap, another contrast; since can indicate a se-
quence of events in which the main clause occurs after
the subordinate clause or cause, as indicates overlap or
cause, and when can denote overlap, a sequence of events,
or contrast. Our model selects the appropriate markers on
the basis of distributional evidence while being agnostic
to their specific meaning when they are ambiguous.
For the sentence fusion task, the identity of the two
clauses is unknown, and our task is to infer which clause
contains the marker. This can be expressed as:
p   argmax
p

M  S 
P
 
t
 ?
i
P
 
a 
 p  i  t  P
 
a 
 p  i  t (7)
where p is generally speaking a sentence fragment to be
realised as a main or subordinate clause (  p  S

p  M 
or  p  M

p  S  ), and t is the temporal marker linking
the two clauses.
We can estimate the parameters for the models in (6)
and (7) from a parsed corpus. We first identify clauses in a
hypotactic relation, i.e., main clauses of which the subor-
dinate clause is a constituent. Next, in the training phase,
we estimate the probabilities P
 
a 
 M  i  t j  and P
 
a 
 S  i  t j 
by simply counting the occurrence of the features a 
 M  i
and a 
 S  i with marker t. For features with zero counts, we
use add-k smoothing (Johnson, 1932), where k is a small
number less than one. In the testing phase, all occurrences
of the relevant temporal markers are removed for the in-
terpretation task and the model must decide which mem-
ber of the confusion set to choose. For the sentence fu-
sion task, it is the temporal order of the two clauses that
is unknown and must be inferred. A similar approach has
been advocated for the interpretation of discourse rela-
tions by Marcu and Echihabi (2002). They train a set of
naive Bayes classifiers on a large corpus (in the order of
40 M sentences) representative of four rhetorical relations
using word bigrams as features. The discourse relations
are read off from explicit discourse markers thus avoid-
ing time consuming hand coding. Apart from the fact that
we present an alternative model, our work differs from
Marcu and Echihabi (2002) in two important ways. First
we explore the contribution of linguistic information to
the inference task using considerably smaller data sets
and secondly apply the proposed model to a generation
task, namely information fusion.
3 Parameter Estimation
3.1 Data Extraction
Subordinate clauses (and their main clause counterparts)
were extracted from the BLLIP corpus (30 M words), a
Treebank-style, machine-parsed version of the Wall Street
Journal (WSJ, years 1987?89) which was produced using
Charniak?s (2000) parser. From the extracted clauses we
estimate the features described in Section 3.2.
We first traverse the tree top-down until we iden-
tify the tree node bearing the subordinate clause label
we are interested in and extract the subtree it dominates.
Assuming we want to extract after subordinate clauses,
this would be the subtree dominated by SBAR-TMP in
Figure 1 indicated by the arrow pointing down. Having
found the subordinate clause, we proceed to extract the
main clause by traversing the tree upwards and identify-
ing the S node immediately dominating the subordinate
clause node (see the arrow pointing up in Figure 1). In
cases where the subordinate clause is sentence initial, we
first identify the SBAR-TMP node and extract the subtree
dominated by it, and then traverse the tree downwards in
order to extract the S-tree immediately dominating it.
For the experiments described here we focus solely
on subordinate clauses immediately dominated by S, thus
ignoring cases where nouns are related to clauses via a
temporal marker. Note also that there can be more than
one main clause that qualify as attachment sites for a sub-
ordinate clause. In Figure 1 the subordinate clause after
the sale is completed can be attached either to said or
will loose. We are relying on the parser for providing rel-
atively accurate information about attachment sites, but
unavoidably there is some noise in the data.
3.2 Model Features
A number of knowledge sources are involved in infer-
ring temporal ordering including tense, aspect, tempo-
ral adverbials, lexical semantic information, and world
knowledge (Asher and Lascarides, 2003). By selecting
features that represent, albeit indirectly and imperfectly,
(S1 (S (NP (DT The) (NN company))
(VP (VBD said)
(S (NP (NNS employees))
(VP (MD will)
(VP (VB lose)
(NP (PRP their) (NNS jobs))
(SBAR-TMP (IN after)
(S (NP (DT the) (NN sale))
(VP (AUX is) (VP (VBN completed)))
))))))))
Figure 1: Extraction of main and subordinate clause from
parse tree
these knowledge sources, we aim to empirically assess
their contribution to the temporal inference task. Below
we introduce our features and provide the motivation be-
hind their selection.
Temporal Signature (T) It is well known that ver-
bal tense and aspect impose constraints on the tempo-
ral order of events but also on the choice of temporal
markers. These constraints are perhaps best illustrated in
the system of Dorr and Gaasterland (1995) who exam-
ine how inherent (i.e., states and events) and non-inherent
(i.e., progressive, perfective) aspectual features interact
with the time stamps of the eventualities in order to gen-
erate clauses and the markers that relate them.
Although we can?t infer inherent aspectual features
from verb surface form (for this we would need a dic-
tionary of verbs and their aspectual classes together with
a process that infers the aspectual class in a given con-
text), we can extract non-inherent features from our parse
trees. We first identify verb complexes including modals
and auxiliaries and then classify tensed and non-tensed
expressions along the following dimensions: finiteness,
non-finiteness, modality, aspect, voice, and polarity. The
values of these features are shown in Table 1. The features
finiteness and non-finiteness are mutually exclusive.
Verbal complexes were identified from the parse
trees heuristically by devising a set of 30 patterns that
search for sequencies of auxiliaries and verbs. From the
parser output verbs were classified as passive or active by
building a set of 10 passive identifying patterns requiring
both a passive auxiliary (some form of be and get) and a
past participle.
To illustrate with an example, consider again the
parse tree in Figure 1. We identify the verbal groups
will lose and is completed from the main and subordi-
nate clause respectively. The former is mapped to the fea-
tures  present, future, imperfective, active, affirmative  ,
whereas the latter is mapped to  present, /0, imperfective,
passive, affirmative  , where /0 indicates the absence of a
modal. In Table 2 we show the relative frequencies in
our corpus for finiteness (FIN), past tense (PAST), active
voice (ACT), and negation (NEG) for main and subordi-
nate clauses conjoined with the markers once and since.
FINITE =  past, present 
NON-FINITE =  infinitive, ing-form, en-form 
MODALITY =  /0, future, ability, possibility, obligation 
ASPECT =  imperfective, perfective, progressive 
VOICE =  active, passive 
NEGATION =  affimative, negative 
Table 1: Temporal signatures
Feature onceM onceS sinceM sinceS
FIN 0.69 0.72 0.75 0.79
PAST 0.28 0.34 0.35 0.71
ACT 0.87 0.51 0.85 0.81
MOD 0.22 0.02 0.07 0.05
NEG 0.97 0.98 0.95 0.97
Table 2: Relative frequency counts for temporal features
As can be seen there are differences in the distribution
of counts between main and subordinate clauses for the
same and different markers. For instance, the past tense is
more frequent in since than once subordinate clauses and
modal verbs are more often attested in since main clauses
when compared with once main clauses. Also, once main
clauses are more likely to be active, whereas once subor-
dinate clauses can be either active or passive.
Verb Identity (V) Investigations into the interpreta-
tion of narrative discourse have shown that specific lexical
information plays an important role in determing tempo-
ral interpretation (e.g., Asher and Lascarides 2003). For
example, the fact that verbs like push can cause move-
ment of the patient and verbs like fall describe the move-
ment of their subject can be used to predict that the dis-
course (8) is interpreted as the pushing causing the falling,
making the linear order of the events mismatch their tem-
poral order.
(8) Max fell. John pushed him.
We operationalise lexical relationships among verbs
in our data by counting their occurrence in main and sub-
ordinate clauses from a lemmatised version of the BLLIP
corpus. Verbs were extracted from the parse trees con-
taining main and subordinate clauses. Consider again the
tree in Figure 1. Here, we identify lose and complete,
without preserving information about tense or passivisa-
tion which is explictly represented in our temporal sig-
natures. Table 3 lists the most frequent verbs attested in
main (VerbM) and subordinate (VerbS) clauses conjoined
with the temporal markers after, as, before, once, since,
until, when, and while (TMark in Table 3).
Verb Class (VW, VL) The verb identity feature does
not capture meaning regularities concerning the types of
verbs entering in temporal relations. For example, in Ta-
ble 3 sell and pay are possession verbs, say and announce
are communication verbs, and come and rise are motion
verbs. We use a semantic classification for obtaining some
TMark VerbM VerbS NounN NounS AdjM AdjS
after sell leave year company last new
as come acquire market dollar recent previous
before say announce time year long new
once become complete stock place more new
since rise expect company month first last
until protect pay president year new next
when make sell year year last last
while wait complete chairman plan first other
Table 3: Verb, noun, and adjective occurrences in main
and subordinate clauses
degree of generalisation over the extracted verb occur-
rences. We experimented with WordNet (Fellbaum, 1998)
and the verb classification proposed by Levin (1993).
Verbs in WordNet are classified in 15 general se-
mantic domains (e.g., verbs of change, verbs of cogni-
tion, etc.). We mapped the verbs occurring in main and
subordinate clauses to these very general semantic cate-
gories (feature VW). Ambiguous verbs in WordNet will
correspond to more than one semantic class. We resolve
ambiguity heuristically by always defaulting to the verb?s
prime sense and selecting the semantic domain for this
sense. In cases where a verb is not listed in WordNet we
default to its lemmatised form.
Levin (1993) focuses on the relation between verbs
and their arguments and hypothesizes that verbs which
behave similarly with respect to the expression and inter-
pretation of their arguments share certain meaning com-
ponents and can therefore be organised into semanti-
cally coherent classes (200 in total). Asher and Lascarides
(2003) argue that these classes provide important infor-
mation for identifying semantic relationships between
clauses. Verbs in our data were mapped into their corre-
sponding Levin classes (feature VL); polysemous verbs
were disambiguated by the method proposed in Lapata
and Brew (1999). Again, for verbs not included in Levin,
the lemmatised verb form is used.
Noun Identity (N) It is not only verbs, but also nouns
that can provide important information about the semantic
relation between two clauses (see Asher and Lascarides
2003 for detailed motivation). In our domain for example,
the noun share is found in main clauses typically preced-
ing the noun market which is often found in subordinate
clauses. Table 3 shows the most frequently attested nouns
(excluding proper names) in main (NounM) and subordi-
nate (NounS) clauses for each temporal marker. Notice
that time denoting nouns (e.g., year, month) are quite fre-
quent in this data set.
Nouns were extracted from a lemmatised version
of the parser?s output. In Figure 1 the nouns employ-
ees, jobs and sales are relevant for the Noun feature.
In cases of noun compounds, only the compound head
(i.e., rightmost noun) was taken into account. A small set
of rules was used to identify organisations (e.g., United
Laboratories Inc.), person names (e.g., Jose Y. Campos),
and locations (e.g., New England) which were subse-
quently substituted by the general categories person,
organisation, and location.
Noun Class (NW). As in the case of verbs, nouns
were also represented by broad semantic classes from the
WordNet taxonomy. Nouns in WordNet do not form a
single hierarchy; instead they are partitioned according
to a set of semantic primitives into 25 semantic classes
(e.g., nouns of cognition, events, plants, substances, etc.),
which are treated as the unique beginners of separate
hierarchies. The nouns extracted from the parser were
mapped to WordNet classes. Ambiguity was handled in
the same way as for verbs.
Adjective (A) Our motivation for including adjec-
tives in our feature set is twofold. First, we hypothesise
that temporal adjectives will be frequent in subordinate
clauses introduced by strictly temporal markers such as
before, after, and until and therefore may provide clues
for the marker interpretation task. Secondly, similarly to
verbs and nouns, adjectives carry important lexical infor-
mation that can be used for inferring the semantic relation
that holds between two clauses. For example, antonyms
can often provide clues about the temporal sequence of
two events (see incoming and outgoing in (9)).
(9) The incoming president delivered his inaugural speech.
The outgoing president resigned last week.
As with verbs and nouns, adjectives were extracted
from the parser?s output. The most frequent adjectives in
main (AdjM) and subordinate (AdjS) clauses are given in
Table 3.
Syntactic Signature (S) The syntactic differences in
main and subordinate clauses are captured by the syntac-
tic signature feature. The feature can be viewed as a mea-
sure of tree complexity, as it encodes for each main and
subordinate clause the number of NPs, VPs, PPs, ADJPs,
and ADVPs it contains. The feature can be easily read
off from the parse tree. The syntactic signature for the
main clause in Figure 1 is [NP:2 VP:2 ADJP:0 ADVP:0
PP:0] and for the subordinate clause [NP:1 VP:1 ADJP:0
ADVP:0 PP:0]. The most frequent syntactic signature
for main clauses is [NP:2 VP:1 PP:0 ADJP:0 ADVP:0];
subordinate clauses typically contain an adverbial phrase
[NP:2 VP:1 ADJP:0 ADVP:1 PP:0].
Argument Signature (R) This feature captures the
argument structure profile of main and subordinate
clauses. It applies only to verbs and encodes whether a
verb has a direct or indirect object, whether it is modified
by a preposition or an adverbial. As with syntactic signa-
ture, this feature was read from the main and subordinate
clause parse-trees. The parsed version of the BLLIP cor-
pus contains information about subjects. NPs whose near-
est ancestor was a VP were identified as objects. Modifi-
cation relations were recovered from the parse trees by
finding all PPs and ADVPs immediately dominated by a
VP. In Figure 1 the argument signature of the main clause
is [SUBJ,OBJ] and for the subordinate it is [OBJ].
Position (P) This feature simply records the position
of the two clauses in the parse tree, i.e., whether the sub-
ordinate clause precedes or follows the main clause. The
majority of the main clauses in our data are sentence in-
titial (80.8%). However, there are differences among in-
dividual markers. For example, once clauses are equally
frequent in both positions. 30% of the when clauses are
sentence intitial whereas 90% of the after clauses are
found in the second position.
In the following sections we describe our experi-
ments with the model introduced in Section 2. We first
investigate the model?s accuracy on the temporal interpre-
tation and fusion tasks (Experiment 1) and then describe a
study with humans (Experiment 2). The latter enables us
to examine in more depth the model?s classification accu-
racy when compared to human judges.
4 Experiment 1: Interpretation and Fusion
4.1 Method
The model was trained on main and subordinate clauses
extracted from the BLLIP corpus as detailed in Sec-
tion 3.1. We obtained 83,810 main-subordinate pairs.
These were randomly partitioned into training (80%), de-
velopment (10%) and test data (10%). Eighty randomly
selected pairs from the test data were reserved for the hu-
man study reported in Experiment 2. We performed pa-
rameter tuning on the development set; all our results are
reported on the unseen test set, unless otherwise stated.
4.2 Results
In order to assess the impact of our features on the inter-
pretation task, the feature space was exhaustively evalu-
ated on the development set. We have nine features, which
results in 9! 9  kConstructing Semantic Space Models from Parsed Corpora
Sebastian Pad?
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbr?cken, Germany
pado@coli.uni-sb.de
Mirella Lapata
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
mlap@dcs.shef.ac.uk
Abstract
Traditional vector-based models use word
co-occurrence counts from large corpora
to represent lexical meaning. In this pa-
per we present a novel approach for con-
structing semantic spaces that takes syn-
tactic relations into account. We introduce
a formalisation for this class of models
and evaluate their adequacy on two mod-
elling tasks: semantic priming and auto-
matic discrimination of lexical relations.
1 Introduction
Vector-based models of word co-occurrence have
proved a useful representational framework for a
variety of natural language processing (NLP) tasks
such as word sense discrimination (Sch?tze, 1998),
text segmentation (Choi et al, 2001), contextual
spelling correction (Jones and Martin, 1997), auto-
matic thesaurus extraction (Grefenstette, 1994), and
notably information retrieval (Salton et al, 1975).
Vector-based representations of lexical meaning
have been also popular in cognitive science and
figure prominently in a variety of modelling stud-
ies ranging from similarity judgements (McDonald,
2000) to semantic priming (Lund and Burgess, 1996;
Lowe and McDonald, 2000) and text comprehension
(Landauer and Dumais, 1997).
In this approach semantic information is extracted
from large bodies of text under the assumption that
the context surrounding a given word provides im-
portant information about its meaning. The semantic
properties of words are represented by vectors that
are constructed from the observed distributional pat-
terns of co-occurrence of their neighbouring words.
Co-occurrence information is typically collected in
a frequency matrix, where each row corresponds to
a unique target word and each column represents its
linguistic context.
Contexts are defined as a small number of words
surrounding the target word (Lund and Burgess,
1996; Lowe and McDonald, 2000) or as entire para-
graphs, even documents (Landauer and Dumais,
1997). Context is typically treated as a set of
unordered words, although in some cases syntac-
tic information is taken into account (Lin, 1998;
Grefenstette, 1994; Lee, 1999). A word can be
thus viewed as a point in an n-dimensional semantic
space. The semantic similarity between words can
be then mathematically computed by measuring the
distance between points in the semantic space using
a metric such as cosine or Euclidean distance.
In the variants of vector-based models where no
linguistic knowledge is used, differences among
parts of speech for the same word (e.g., to drink
vs. a drink ) are not taken into account in the con-
struction of the semantic space, although in some
cases word lexemes are used rather than word sur-
face forms (Lowe and McDonald, 2000; McDonald,
2000). Minimal assumptions are made with respect
to syntactic dependencies among words. In fact it is
assumed that all context words within a certain dis-
tance from the target word are semantically relevant.
The lack of syntactic information makes the build-
ing of semantic space models relatively straightfor-
ward and language independent (all that is needed is
a corpus of written or spoken text). However, this
entails that contextual information contributes indis-
criminately to a word?s meaning.
Some studies have tried to incorporate syntactic
information into vector-based models. In this view,
the semantic space is constructed from words that
bear a syntactic relationship to the target word of in-
terest. This makes semantic spaces more flexible,
different types of contexts can be selected and words
do not have to physically co-occur to be considered
contextually relevant. However, existing models ei-
ther concentrate on specific relations for construct-
ing the semantic space such as objects (e.g., Lee,
1999) or collapse all types of syntactic relations
available for a given target word (Grefenstette, 1994;
Lin, 1998). Although syntactic information is now
used to select a word?s appropriate contexts, this in-
formation is not explicitly captured in the contexts
themselves (which are still represented by words)
and is therefore not amenable to further processing.
A commonly raised criticism for both types of se-
mantic space models (i.e., word-based and syntax-
based) concerns the notion of semantic similarity.
Proximity between two words in the semantic space
cannot indicate the nature of the lexical relations be-
tween them. Distributionally similar words can be
antonyms, synonyms, hyponyms or in some cases
semantically unrelated. This limits the application
of semantic space models for NLP tasks which re-
quire distinguishing between lexical relations.
In this paper we generalise semantic space models
by proposing a flexible conceptualisation of context
which is parametrisable in terms of syntactic rela-
tions. We develop a general framework for vector-
based models which can be optimised for different
tasks. Our framework allows the construction of se-
mantic space to take place over words or syntactic
relations thus bridging the distance between word-
based and syntax-based models. Furthermore, we
show how our model can incorporate well-defined,
informative contexts in a principled way which re-
tains information about the syntactic relations avail-
able for a given target word.
We first evaluate our model on semantic prim-
ing, a phenomenon that has received much attention
in computational psycholinguistics and is typically
modelled using word-based semantic spaces. We
next conduct a study that shows that our model is
sensitive to different types of lexical relations.
2 Dependency-based Vector Space Models
Once we move away from words as the basic con-
text unit, the issue of representation of syntactic in-
formation becomes pertinent. Information about the
dependency relations between words abstracts over
word order and can be considered as an intermediate
layer between surface syntax and semantics. More
Det
a
N
lorry
Aux
might
V
carry
A
sweet
N
apples
subj
det
aux
obj
mo
d
Figure 1: A dependency parse of a short sentence
formally, dependencies are asymmetric binary rela-
tionships between a head and a modifier (Tesni?re,
1959). The structure of a sentence can be repre-
sented by a set of dependency relationships that form
a tree as shown in Figure 1. Here the head of the sen-
tence is the verb carry which is in turn modified by
its subject lorry and its object apples.
It is the dependencies in Figure 1 that will form
the context over which the semantic space will be
constructed. The construction mechanism sets out
by identifying the local context of a target word,
which is a subset of all dependency paths starting
from it. The paths consist of the dependency edges
of the tree labelled with dependency relations such
as subj, obj, or aux (see Figure 1). The paths can be
ranked by a path value function which gives differ-
ent weight to different dependency types (for exam-
ple, it can be argued that subjects and objects convey
more semantic information than determiners). Tar-
get words are then represented in terms of syntactic
features which form the dimensions of the seman-
tic space. Paths are mapped to features by the path
equivalence relation and the appropriate cells in the
matrix are incremented.
2.1 Definition of Semantic Space
We assume the semantic space formalisation pro-
posed by Lowe (2001). A semantic space is a matrix
whose rows correspond to target words and columns
to dimensions which Lowe calls basis elements:
Definition 1. A Semantic Space Model is a matrix
K = B?T , where bi ? B denotes the basis element
of column i, t j ? T denotes the target word of row j,
and Ki j the cell (i, j).
T is the set of words for which the matrix con-
tains representations; this can be either word types
or word tokens. In this paper, we assume that co-
occurrence counts are constructed over word types,
but the framework can be easily adapted to represent
word tokens instead.
In traditional semantic spaces, the cells Ki j of
the matrix correspond to word co-occurrence counts.
This is no longer the case for dependency-based
models. In the following we explain how co-
occurrence counts are constructed.
2.2 Building the Context
The first step in constructing a semantic space from
a large collection of dependency relations is to con-
struct a word?s local context.
Definition 2. The dependency parse p of a sentence
s is an undirected graph p(s) = (Vp,Ep). The set of
nodes corresponds to words of the sentence: Vp =
{w1, . . . ,wn}. The set of edges is Ep ?Vp ?Vp.
Definition 3. A class q is a three-tuple consisting
of a POS-tag, a relation, and another POS-tag. We
write Q for the set of all classes Cat ?R?Cat. For
each parse p, the labelling function Lp : Ep ? Q as-
signs a class to every edge of the parse.
In Figure 1, the labelling function labels the left-
most edge as Lp((a, lorry)) = ?Det,det,N?. Note that
Det represents the POS-tag ?determiner? and det the
dependency relation ?determiner?.
In traditional models, the target words are sur-
rounded by context words. In a dependency-based
model, the target words are surrounded by depen-
dency paths.
Definition 4. A path ? is an ordered tuple of edges
?e1, . . . ,en? ? Enp so that
? i : (ei?1 = (v1,v2) ? ei = (v3,v4)) ? v2 = v3
Definition 5. A path anchored at a word w is a path
?e1, . . . ,en? so that e1 = (v1,v2) and w = v1. Write
?w for the set of all paths over Ep anchored at w.
In words, a path is a tuple of connected edges in
a parse graph and it is anchored at w if it starts at w.
In Figure 1, the set of paths anchored at lorry 1 is:
{?(lorry,carry)?,?(lorry,carry),(carry,apples)?,
?(lorry,a)?,?(lorry,carry),(carry,might)?, . . .}
The local context of a word is the set or a subset of
its anchored paths. The class information can always
be recovered by means of the labelling function.
Definition 6. A local context of a word w from a
sentence s is a subset of the anchored paths at w. A
function c : W ? 2?w which assigns a local context
to a word is called a context specification function.
1For the sake of brevity, we only show paths up to length 2.
The context specification function allows to elim-
inate paths on the basis of their classes. For exam-
ple, it is possible to eliminate all paths from the set
of anchored paths but those which contain immedi-
ate subject and direct object relations. This can be
formalised as:
c(w) = {? ? ?w |? = ?e??
(Lp(e) = ?V,obj,N??Lp(e) = ?V,subj,N?)}
In Figure 1, the labels of the two edges which
form paths of length 1 and conform to this context
specification are marked in boldface. Notice that the
local context of lorry contains only one anchored
path (c(lorry) = {?(lorry,carry)?}).
2.3 Quantifying the Context
The second step in the construction of the
dependency-based semantic models is to specify the
relative importance of different paths. Linguistic in-
formation can be incorporated into our framework
through the path value function.
Definition 7. The path value function v assigns a
real number to a path: v : ? ? R.
For instance, the path value function could pe-
nalise longer paths for only expressing indirect re-
lationships between words. An example of a length-
based path value function is v(?) = 1
n
where ? =
?e1, . . . ,en?. This function assigns a value of 1 to the
one path from c(lorry) and fractions to longer paths.
Once the value of all paths in the local context
is determined, the dimensions of the space must be
specified. Unlike word-based models, our contexts
contain syntactic information and dimensions can
be defined in terms of syntactic features. The path
equivalence relation combines functionally equiva-
lent dependency paths that share a syntactic feature
into equivalence classes.
Definition 8. Let ? be the path equivalence relation
on ?. The partition induced by this equivalence re-
lation is the set of basis elements B.
For example, it is possible to combine all paths
which end at the same word: A path which starts
at wi and ends at w j, irrespectively of its length and
class, will be the co-occurrence of wi and w j. This
word-based equivalence function can be defined in
the following manner:
?(v1,v2), . . . ,(vn?1,vn)? ? ?(v?1,v?2), . . . ,(v?m?1,v?m)?
iff vn = v?m
This means that in Figure 1 the set of basis elements
is the set of words at which paths end. Although co-
occurrence counts are constructed over words like in
traditional semantic space models, it is only words
which stand in a syntactic relationship to the target
that are taken into account.
Once the value of all paths in the local context
is determined, the local observed frequency for the
co-occurrence of a basis element b with the target
word w is just the sum of values of all paths ? in
this context which express the basis element b. The
global observed frequency is the sum of the local
observed frequencies for all occurrences of a target
word type t and is therefore a measure for the co-
occurrence of t and b over the whole corpus.
Definition 9. Global observed frequency:
?f (b, t) = ?
w?W (t)
?
??C(w)???b
v(?)
As Lowe (2001) notes, raw frequency counts are
likely to give misleading results. Due to the Zip-
fian distribution of word types, words occurring
with similar frequencies will be judged more similar
than they actually are. A lexical association func-
tion can be used to explicitly factor out chance co-
occurrences.
Definition 10. Write A for the lexical association
function which computes the value of a cell of the
matrix from a co-occurrence frequency:
Ki j = A( ?f (bi, t j))
3 Evaluation
3.1 Parameter Settings
All our experiments were conducted on the British
National Corpus (BNC), a 100 million word col-
lection of samples of written and spoken language
(Burnard, 1995). We used Lin?s (1998) broad cover-
age dependency parser MINIPAR to obtain a parsed
version of the corpus. MINIPAR employs a man-
ually constructed grammar and a lexicon derived
from WordNet with the addition of proper names
(130,000 entries in total). Lexicon entries con-
tain part-of-speech and subcategorization informa-
tion. The grammar is represented as a network of
35 nodes (i.e., grammatical categories) and 59 edges
(i.e., types of syntactic (dependency) relationships).
MINIPAR uses a distributed chart parsing algorithm.
Grammar rules are implemented as constraints asso-
ciated with the nodes and edges.
Cosine distance cos(~x,~y) = ?i xiyi?
?i x2i
?
?i y2i
Skew divergence s?(~x,~y) = ?i xi log xi?xi+(1??)yi
Figure 2: Distance measures
The dependency-based semantic space was con-
structed with the word-based path equivalence func-
tion from Section 2.3. As basis elements for our se-
mantic space the 1000 most frequent words in the
BNC were used. Each element of the resulting vec-
tor was replaced with its log-likelihood value (see
Definition 10 in Section 2.3) which can be consid-
ered as an estimate of how surprising or distinctive
a co-occurrence pair is (Dunning, 1993).
We experimented with a variety of distance mea-
sures such as cosine, Euclidean distance, L1 norm,
Jaccard?s coefficient, Kullback-Leibler divergence
and the Skew divergence (see Lee 1999 for an
overview). We obtained the best results for co-
sine (Experiment 1) and Skew divergence (Experi-
ment 2). The two measures are shown in Figure 2.
The Skew divergence represents a generalisation of
the Kullback-Leibler divergence and was proposed
by Lee (1999) as a linguistically motivated distance
measure. We use a value of ? = .99.
We explored in detail the influence of different
types and sizes of context by varying the context
specification and path value functions. Contexts
were defined over a set of 23 most frequent depen-
dency relations which accounted for half of the de-
pendency edges found in our corpus. From these,
we constructed four context specification functions:
(a) minimum contexts containing paths of length 1
(in Figure 1 sweet and carry are the minimum con-
text for apples), (b) np context adds dependency in-
formation relevant for noun compounds to minimum
context, (c) wide takes into account paths of length
longer than 1 that represent meaningful linguistic re-
lations such as argument structure, but also prepo-
sitional phrases and embedded clauses (in Figure 1
the wide context of apples is sweet, carry, lorry, and
might ), and (d) maximum combined all of the above
into a rich context representation.
Four path valuation functions were used: (a) plain
assigns the same value to every path, (b) length
assigns a value inversely proportional to a path?s
length, (c) oblique ranks paths according to the
obliqueness hierarchy of grammatical relations
(Keenan and Comrie, 1977), and (d) oblength
context specification path value function
1 minimum plain
2 minimum oblique
3 np plain
4 np length
5 np oblique
6 np oblength
7 wide plain
8 wide length
9 wide oblique
10 wide oblength
11 maximum plain
12 maximum length
13 maximum oblique
14 maximum oblength
Table 1: The fourteen models
combines length and oblique . The resulting 14
parametrisations are shown in Table 1. Length-
based and length-neutral path value functions are
collapsed for the minimum context specification
since it only considers paths of length 1.
We further compare in Experiments 1 and 2 our
dependency-based model against a state-of-the-art
vector-based model where context is defined as a
?bag of words?. Note that considerable latitude is
allowed in setting parameters for vector-based mod-
els. In order to allow a fair comparison, we se-
lected parameters for the traditional model that have
been considered optimal in the literature (Patel et al,
1998), namely a symmetric 10 word window and
the most frequent 500 content words from the BNC
as dimensions. These parameters were similar to
those used by Lowe and McDonald (2000) (symmet-
ric 10 word window and 536 content words). Again
the log-likelihood score is used to factor out chance
co-occurrences.
3.2 Experiment 1: Priming
A large number of modelling studies in psycholin-
guistics have focused on simulating semantic prim-
ing studies. The semantic priming paradigm pro-
vides a natural test bed for semantic space models
as it concentrates on the semantic similarity or dis-
similarity between a prime and its target, and it is
precisely this type of lexical relations that vector-
based models capture.
In this experiment we focus on Balota and Lorch?s
(1986) mediated priming study. In semantic priming
transient presentation of a prime word like tiger di-
rectly facilitates pronunciation or lexical decision on
a target word like lion. Mediated priming extends
this paradigm by additionally allowing indirectly re-
lated words as primes ? like stripes, which is only
related to lion by means of the intermediate concept
tiger. Balota and Lorch (1986) obtained small medi-
ated priming effects for pronunciation tasks but not
for lexical decision. For the pronunciation task, re-
action times were reduced significantly for both di-
rect and mediated primes, however the effect was
larger for direct primes.
There are at least two semantic space simulations
that attempt to shed light on the mediated priming
effect. Lowe and McDonald (2000) replicated both
the direct and mediated priming effects, whereas
Livesay and Burgess (1997) could only replicate di-
rect priming. In their study, mediated primes were
farther from their targets than unrelated words.
3.2.1 Materials and Design
Materials were taken form Balota and Lorch
(1986). They consist of 48 target words, each paired
with a related and a mediated prime (e.g., lion-tiger-
stripes). Each related-mediated prime tuple was
paired with an unrelated control randomly selected
from the complement set of related primes.
3.2.2 Procedure
One stimulus was removed as it had a low cor-
pus frequency (less than 100), which meant that
the resulting vector would be unreliable. We con-
structed vectors from the BNC for all stimuli with
the dependency-based models and the traditional
model, using the parametrisations given in Sec-
tion 3.1 and cosine as a distance measure. We calcu-
lated the distance in semantic space between targets
and their direct primes (TarDirP), targets and their
mediated primes (TarMedP), targets and their unre-
lated controls (TarUnC) for both models.
3.2.3 Results
We carried out a one-way Analysis of Variance
(ANOVA) with the distance as dependent variable
(TarDirP, TarMedP, TarUnC). Recall from Table 1
that we experimented with fourteen different con-
text definitions. A reliable effect of distance was
observed for all models (p < .001). We used the
?2 statistic to calculate the amount of variance ac-
counted for by the different models. Figure 3 plots
?2 against the different contexts. The best result
was obtained for model 7 which accounts for 23.1%
of the variance (F(2,140) = 20.576, p < .001) and
corresponds to the wide context specification and
the plain path value function. A reliable distance
effect was also observed for the traditional vector-
based model (F(2,138) = 9.384, p < .001).
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 1  2  3  4  5  6  7  8  9  10  11  12  13  14
e
ta
 s
qu
ar
ed
model
TarDirP -- TarMedP -- TarUnC
TarDirP -- TarUnC
 TarMedP -- TarUnC
Figure 3: ?2 scores for mediated priming materials
Model TarDirP ? TarUnC TarMedP ? TarUnC
Model 7 F = 25.290 (p < .001) F = .001 (p = .790)
Traditional F = 12.185 (p = .001) F = .172 (p = .680)
L & McD F = 24.105 (p < .001) F = 13.107 (p < .001)
Table 2: Size of direct and mediated priming effects
Pairwise ANOVAs were further performed to ex-
amine the size of the direct and mediated priming ef-
fects individually (see Table 2). There was a reliable
direct priming effect (F(1,94) = 25.290, p < .001)
but we failed to find a reliable mediated priming
effect (F(1,93) = .001, p = .790). A reliable di-
rect priming effect (F(1,92) = 12.185, p = .001)
but no mediated priming effect was also obtained for
the traditional vector-based model. We used the ?2
statistic to compare the effect sizes obtained for the
dependency-based and traditional model. The best
dependency-based model accounted for 23.1% of
the variance, whereas the traditional model ac-
counted for 12.2% (see also Table 2).
Our results indicate that dependency-based mod-
els are able to model direct priming across a wide
range of parameters. Our results also show that
larger contexts (see models 7 and 11 in Figure 3) are
more informative than smaller contexts (see mod-
els 1 and 3 in Figure 3), but note that the wide con-
text specification performed better than maximum. At
least for mediated priming, a uniform path value as
assigned by the plain path value function outper-
forms all other functions (see Figure 3).
Neither our dependency-based model nor the tra-
ditional model were able to replicate the mediated
priming effect reported by Lowe and McDonald
(2000) (see L & McD in Table 2). This may be
due to differences in lemmatisation of the BNC,
the parametrisations of the model or the choice of
context words (Lowe and McDonald use a spe-
cial procedure to identify ?reliable? context words).
Our results also differ from Livesay and Burgess
(1997) who found that mediated primes were fur-
ther from their targets than unrelated controls, us-
ing however a model and corpus different from the
ones we employed for our comparative studies. In
the dependency-based model, mediated primes were
virtually indistinguishable from unrelated words.
In sum, our results indicate that a model which
takes syntactic information into account outper-
forms a traditional vector-based model which sim-
ply relies on word occurrences. Our model is able
to reproduce the well-established direct priming ef-
fect but not the more controversial mediated prim-
ing effect. Our results point to the need for further
comparative studies among semantic space models
where variables such as corpus choice and size as
well as preprocessing (e.g., lemmatisation, tokeni-
sation) are controlled for.
3.3 Experiment 2: Encoding of Relations
In this experiment we examine whether dependency-
based models construct a semantic space that encap-
sulates different lexical relations. More specifically,
we will assess whether word pairs capturing differ-
ent types of semantic relations (e.g., hyponymy, syn-
onymy) can be distinguished in terms of their dis-
tances in the semantic space.
3.3.1 Materials and Design
Our experimental materials were taken from
Hodgson (1991) who in an attempt to investigate
which types of lexical relations induce priming col-
lected a set of 142 word pairs exemplifying the fol-
lowing semantic relations: (a) synonymy (words
with the same meaning, value and worth ), (b) su-
perordination and subordination (one word is an in-
stance of the kind expressed by the other word, pain
and sensation), (c) category coordination (words
which express two instances of a common super-
ordinate concept, truck and train), (d) antonymy
(words with opposite meaning, friend and enemy),
(e) conceptual association (the first word subjects
produce in free association given the other word,
leash and dog), and (f) phrasal association (words
which co-occur in phrases private and property).
The pairs were selected to be unambiguous exam-
ples of the relation type they instantiate and were
matched for frequency. The pairs cover a wide range
of parts of speech, like adjectives, verbs, and nouns.
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 1  2  3  4  5  6  7  8  9  10  11  12  13  14
e
ta
 s
qu
ar
ed
model
Hodgson skew divergence
Figure 4: ?2 scores for the Hodgson materials
Mean PA SUP CO ANT SYN
CA 16.25 ? ? ? ?
PA 15.13 ? ?
SUP 11.04
CO 10.45
ANT 10.07
SYN 8.87
Table 3: Mean skew divergences and Tukey test re-
sults for model 7
3.3.2 Procedure
As in Experiment 1, six words with low fre-
quencies (less than 100) were removed from the
materials. Vectors were computed for the re-
maining 278 words for both the traditional and
the dependency-based models, again with the
parametrisations detailed in Section 3.1. We calcu-
lated the semantic distance for every word pair, this
time using Skew divergence as distance measure.
3.3.3 Results
We carried out an ANOVA with the lexical rela-
tion as factor and the distance as dependent variable.
The lexical relation factor had six levels, namely the
relations detailed in Section 3.3.1. We found no ef-
fect of semantic distance for the traditional semantic
space model (F(5,141) = 1.481, p = .200). The ?2
statistic revealed that only 5.2% of the variance was
accounted for. On the other hand, a reliable effect
of distance was observed for all dependency-based
models (p < .001). Model 7 (wide context specifi-
cation and plain path value function) accounted for
the highest amount of variance in our data (20.3%).
Our results can be seen in Figure 4.
We examined whether there are any significant
differences among the six relations using Post-hoc
Tukey tests. The pairwise comparisons for model 7
are given in Table 3. The mean distances for concep-
tual associates (CA), phrasal associates (PA), super-
ordinates/subordinates (SUP), category coordinates
(CO), antonyms (ANT), and synonyms (SYN) are
also shown in Table 3. There is no significant differ-
ence between PA and CA, although SUP, CO, ANT,
and SYN, are all significantly different from CA (see
Table 3, where ? indicates statistical significance,
a = .05). Furthermore, ANT and SYN are signifi-
cantly different from PA.
Kilgarriff and Yallop (2000) point out that man-
ually constructed taxonomies or thesauri are typ-
ically organised according to synonymy and hy-
ponymy for nouns and verbs and antonymy for ad-
jectives. They further argue that for automatically
constructed thesauri similar words are words that
either co-occur with each other or with the same
words. The relations SYN, SUP, CO, and ANT can be
thought of as representing taxonomy-related knowl-
edge, whereas CA and PA correspond to the word
clusters found in automatically constructed thesauri.
In fact an ANOVA reveals that the distinction be-
tween these two classes of relations can be made
reliably (F(1,136) = 15.347, p < .001), after col-
lapsing SYN, SUP, CO, and ANT into one class and
CA and PA into another.
Our results suggest that dependency-based vector
space models can, at least to a certain degree, dis-
tinguish among different types of lexical relations,
while this seems to be more difficult for traditional
semantic space models. The Tukey test revealed that
category coordination is reliably distinguished from
all other relations and that phrasal association is re-
liably different from antonymy and synonymy. Tax-
onomy related relations (e.g., synonymy, antonymy,
hyponymy) can be reliably distinguished from con-
ceptual and phrasal association. However, no reli-
able differences were found between closely associ-
ated relations such as antonymy and synonymy.
Our results further indicate that context encoding
plays an important role in discriminating lexical re-
lations. As in Experiment 1 our best results were
obtained with the wide context specification. Also,
weighting schemes such as the obliqueness hierar-
chy length again decreased the model?s performance
(see conditions 2, 5, 9, and 13 in Figure 4), show-
ing that dependency relations contribute equally to
the representation of a word?s meaning. This points
to the fact that rich context encodings with a wide
range of dependency relations are promising for cap-
turing lexical semantic distinctions. However, the
performance for maximum context specification was
lower, which indicates that collapsing all depen-
dency relations is not the optimal method, at least
for the tasks attempted here.
4 Discussion
In this paper we presented a novel semantic space
model that enriches traditional vector-based models
with syntactic information. The model is highly gen-
eral and can be optimised for different tasks. It ex-
tends prior work on syntax-based models (Grefen-
stette, 1994; Lin, 1998), by providing a general
framework for defining context so that a large num-
ber of syntactic relations can be used in the construc-
tion of the semantic space.
Our approach differs from Lin (1998) in three
important ways: (a) by introducing dependency
paths we can capture non-immediate relationships
between words (i.e., between subjects and objects),
whereas Lin considers only local context (depen-
dency edges in our terminology); the semantic
space is therefore constructed solely from isolated
head/modifier pairs and their inter-dependencies are
not taken into account; (b) Lin creates the semantic
space from the set of dependency edges that are rel-
evant for a given word; by introducing dependency
labels and the path value function we can selectively
weight the importance of different labels (e.g., sub-
ject, object, modifier) and parametrize the space ac-
cordingly for different tasks; (c) considerable flexi-
bility is allowed in our formulation for selecting the
dimensions of the semantic space; the latter can be
words (see the leaves in Figure 1), parts of speech
or dependency edges; in Lin?s approach, it is only
dependency edges (features in his terminology) that
form the dimensions of the semantic space.
Experiment 1 revealed that the dependency-based
model adequately simulates semantic priming. Ex-
periment 2 showed that a model that relies on rich
context specifications can reliably distinguish be-
tween different types of lexical relations. Our re-
sults indicate that a number of NLP tasks could
potentially benefit from dependency-based models.
These are particularly relevant for word sense dis-
crimination, automatic thesaurus construction, auto-
matic clustering and in general similarity-based ap-
proaches to NLP.
References
Balota, David A. and Robert Lorch, Jr. 1986. Depth of au-
tomatic spreading activation: Mediated priming effects in
pronunciation but not in lexical decision. Journal of Ex-
perimental Psychology: Learning, Memory and Cognition
12(3):336?45.
Burnard, Lou. 1995. Users Guide for the British National Cor-
pus. British National Corpus Consortium, Oxford University
Computing Service.
Choi, Freddy, Peter Wiemer-Hastings, and Johanna Moore.
2001. Latent Semantic Analysis for text segmentation. In
Proceedings of EMNLP 2001. Seattle, WA.
Dunning, Ted. 1993. Accurate methods for the statistics of sur-
prise and coincidence. Computational Linguistics 19:61?74.
Grefenstette, Gregory. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers.
Hodgson, James M. 1991. Informational constraints on pre-
lexical priming. Language and Cognitive Processes 6:169?
205.
Jones, Michael P. and James H. Martin. 1997. Contextual
spelling correction using Latent Semantic Analysis. In Pro-
ceedings of the ANLP 97.
Keenan, E. and B. Comrie. 1977. Noun phrase accessibility and
universal grammar. Linguistic Inquiry (8):62?100.
Kilgarriff, Adam and Colin Yallop. 2000. What?s in a thesaurus.
In Proceedings of LREC 2000. pages 1371?1379.
Landauer, T. and S. Dumais. 1997. A solution to Platos prob-
lem: the latent semantic analysis theory of acquisition, in-
duction, and representation of knowledge. Psychological Re-
view 104(2):211?240.
Lee, Lillian. 1999. Measures of distributional similarity. In
Proceedings of ACL ?99. pages 25?32.
Lin, Dekang. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL 1998. Montr?al,
Canada, pages 768?511.
Lin, Dekang. 2001. LaTaT: Language and text analysis tools.
In J. Allan, editor, Proceedings of HLT 2001. Morgan Kauf-
mann, San Francisco.
Livesay, K. and C. Burgess. 1997. Mediated priming in high-
dimensional meaning space: What is "mediated" in mediated
priming? In Proceedings of COGSCI 1997. Lawrence Erl-
baum Associates.
Lowe, Will. 2001. Towards a theory of semantic space. In Pro-
ceedings of COGSCI 2001. Lawrence Erlbaum Associates,
pages 576?81.
Lowe, Will and Scott McDonald. 2000. The direct route: Medi-
ated priming in semantic space. In Proceedings of COGSCI
2000. Lawrence Erlbaum Associates, pages 675?80.
Lund, Kevin and Curt Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments, and Computers
28:203?8.
McDonald, Scott. 2000. Environmental Determinants of Lexical
Processing Effort. Ph.D. thesis, University of Edinburgh.
Patel, Malti, John A. Bullinaria, and Joseph P. Levy. 1998. Ex-
tracting semantic representations from large text corpora. In
Proceedings of the 4th Neural Computation and Psychology
Workshop. London, pages 199?212.
Salton, G, A Wang, and C Yang. 1975. A vector-space model
for information retrieval. Journal of the American Society
for Information Science 18(613?620).
Sch?tze, Hinrich. 1998. Automatic word sense discrimination.
Computational Linguistics 24(1):97?124.
Tesni?re, Lucien. 1959. Elements de syntaxe structurale.
Klincksieck, Paris.
Probabilistic Text Structuring: Experiments with Sentence Ordering
Mirella Lapata
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
mlap@dcs.shef.ac.uk
Abstract
Ordering information is a critical task for
natural language generation applications.
In this paper we propose an approach to
information ordering that is particularly
suited for text-to-text generation. We de-
scribe a model that learns constraints on
sentence order from a corpus of domain-
specific texts and an algorithm that yields
the most likely order among several al-
ternatives. We evaluate the automatically
generated orderings against authored texts
from our corpus and against human sub-
jects that are asked to mimic the model?s
task. We also assess the appropriateness of
such a model for multidocument summa-
rization.
1 Introduction
Structuring a set of facts into a coherent text is a
non-trivial task which has received much attention
in the area of concept-to-text generation (see Reiter
and Dale 2000 for an overview). The structured text
is typically assumed to be a tree (i.e., to have a hier-
archical structure) whose leaves express the content
being communicated and whose nodes specify how
this content is grouped via rhetorical or discourse re-
lations (e.g., contrast, sequence, elaboration).
For domains with large numbers of facts and
rhetorical relations, there can be more than one pos-
sible tree representing the intended content. These
different trees will be realized as texts with different
sentence orders or even paragraph orders and differ-
ent levels of coherence. Finding the tree that yields
the best possible text is effectively a search prob-
lem. One way to address it is by narrowing down
the search space either exhaustively or heuristically.
Marcu (1997) argues that global coherence can be
achieved if constraints on local coherence are sat-
isfied. The latter are operationalized as weights on
the ordering and adjacency of facts and are derived
from a corpus of naturally occurring texts. A con-
straint satisfaction algorithm is used to find the tree
with maximal weights from the space of all possi-
ble trees. Mellish et al (1998) advocate stochastic
search as an alternative to exhaustively examining
the search space. Rather than requiring a global op-
timum to be found, they use a genetic algorithm to
select a tree that is coherent enough for people to
understand (local optimum).
The problem of finding an acceptable order-
ing does not arise solely in concept-to-text gener-
ation but also in the emerging field of text-to-text
generation (Barzilay, 2003). Examples of applica-
tions that require some form of text structuring, are
single- and multidocument summarization as well as
question answering. Note that these applications do
not typically assume rich semantic knowledge orga-
nized in tree-like structures or communicative goals
as is often the case in concept-to-text generation. Al-
though in single document summarization the posi-
tion of a sentence in a document can provide cues
with respect to its ordering in the summary, this is
not the case in multidocument summarization where
sentences are selected from different documents and
must be somehow ordered so as to produce a coher-
ent summary (Barzilay et al, 2002). Answering a
question may also involve the extraction, potentially
summarization, and ordering of information across
multiple information sources.
Barzilay et al (2002) address the problem of
information ordering in multidocument summariza-
tion and show that naive ordering algorithms such
as majority ordering (selects most frequent orders
across input documents) and chronological ordering
(orders facts according to publication date) do not
always yield coherent summaries although the latter
produces good results when the information is event-
based. Barzilay et al further conduct a study where
subjects are asked to produce a coherent text from
the output of a multidocument summarizer. Their re-
sults reveal that although the generated orders differ
from subject to subject, topically related sentences
always appear together. Based on the human study
they propose an algorithm that first identifies top-
ically related groups of sentences and then orders
them according to chronological information.
In this paper we introduce an unsupervised
probabilistic model for text structuring that learns
ordering constraints from a large corpus. The model
operates on sentences rather than facts in a knowl-
edge base and is potentially useful for text-to-text
generation applications. For example, it can be used
to order the sentences obtained from a multidocu-
ment summarizer or a question answering system.
Sentences are represented by a set of informative
features (e.g., a verb and its subject, a noun and its
modifier) that can be automatically extracted from
the corpus without recourse to manual annotation.
The model learns which sequences of features
are likely to co-occur and makes predictions con-
cerning preferred orderings. Local coherence is thus
operationalized by sentence proximity in the train-
ing corpus. Global coherence is obtained by greedily
searching through the space of possible orders. As in
the case of Mellish et al (1998) we construct an ac-
ceptable ordering rather than the best possible one.
We propose an automatic method of evaluating the
orders generated by our model by measuring close-
ness or distance from the gold standard, a collection
of orders produced by humans.
The remainder of this paper is organized as fol-
lows. Section 2 introduces our model and an algo-
rithm for producing a possible order. Section 3 de-
scribes our corpus and the estimation of the model
parameters. Our experiments are detailed in Sec-
tion 4. We conclude with a discussion in Section 5.
2 Learning to Order
Given a collection of texts from a particular domain,
our task is to learn constraints on the ordering of
their sentences. In the training phase our model will
learn these constraints from adjacent sentences rep-
resented by a set of informative features. In the test-
ing phase, given a set of unseen sentences, we will
rely on our prior experience of how sentences are
usually ordered for choosing the most likely order-
ing.
2.1 The Model
We express the probability of a text made up of sen-
tences S1 . . .Sn as shown in (1). According to (1), the
task of predicting the next sentence is dependent on
its n? i previous sentences.
P(T ) = P(S1 . . .Sn)
= P(S1)P(S2jS1)P(S3jS1,S2) . . .P(SnjS1 ... Sn?1)
=
n
?
i=1
P(SnjS1 . . .Sn?i)
(1)
We will simplify (1) by assuming that the prob-
ability of any given sentence is determined only by
its previous sentence:
P(T ) = P(S1)P(S2jS1)P(S3jS2) . . .P(SnjSn?1)
=
n
?
i=1
P(SijSi?1)
(2)
This is a somewhat simplistic attempt at cap-
turing Marcu?s (1997) local coherence constraints as
well as Barzilay et al?s (2002) observations about
topical relatedness. While this is clearly a naive view
of text coherence, our model has some notion of the
types of sentences that typically go together, even
though it is agnostic about the specific rhetorical re-
lations that glue sentences into a coherent text. Also
note that the simplification in (2) will make the es-
timation of the probabilities P(SijSi?1) more reli-
able in the face of sparse data. Of course estimat-
ing P(SijSi?1) would be impossible if Si and Si?1
were actual sentences. It is unlikely to find the ex-
act same sentence repeated several times in a corpus.
What we can find and count is the number of times
a given structure or word appears in the corpus. We
will therefore estimate P(SijSi?1) from features that
express its structure and content (these features are
described in detail in Section 3):
P(SijSi?1) =
P(ha
hi,1i,ahi,2i . . .ahi,niijhahi?1,1i,ahi?1,2i . . .ahi?1,mii)
(3)
where ha
hi,1i,ahi,2i . . .ahi,nii are features relevant for
sentence Si and ha
hi?1,1i,ahi?1,2i . . .ahi?1,mii for sen-
tence Si?1. We will assume that these features are
independent and that P(SijSi?1) can be estimated
from the pairs in the Cartesian product defined
over the features expressing sentences Si and Si?1:
(a
hi, ji,ahi?1,ki) 2 Si Si?1. Under these assumptions
P(SijSi?1) can be written as follows:
P(SijSi?1) = P(a
hi,1ijahi?1,1i) . . .P(ahi,nijahi?1,mi)
= ?
(a
hi, ji,ahi?1,ki)2SiSi?1
P(a
hi, jijahi?1,ki)
(4)
Assuming that the features are independent
again makes parameter estimation easier. The Carte-
sian product over the features in Si and Si?1 is an at-
tempt to capture inter-sentential dependencies. Since
S1 : a b c d
S2 : e f g
S3 : h i
Figure 1: Example of probability estimation
we don?t know a priori what the important feature
combinations are, we are considering all possible
combinations over two sentences. This will admit-
tedly introduce some noise, given that some depen-
dencies will be spurious, but the model can be easily
retrained for different domains for which different
feature combinations will be important. The proba-
bility P(a
hi, jijahi?1,ki) is estimated as:
P(a
hi, jijahi?1,ki) =
f (a
hi, ji,ahi?1,ki)
?
a
hi, ji
f (a
hi, ji,ahi?1,ki)
(5)
where f (a
hi, ji,ahi?1,ki) is the number of times fea-
ture a
hi, ji is preceded by feature ahi?1,ki in the
corpus. The denominator expresses the number of
times a
hi?1,ki is attested in the corpus (preceded
by any feature). The probabilities P(a
hi, jijahi?1,ki)
will be unreliable when the frequency estimates for
f (a
hi, ji,ahi?1,ki) are small, and undefined in cases
where the feature combinations are unattested in the
corpus. We therefore smooth the observed frequen-
cies using back-off smoothing (Katz, 1987).
To illustrate with an example consider the text
in Figure 1 which has three sentences S1, S2, S3,
each represented by their respective features denoted
by letters. The probability P(S3jS2) will be calcu-
lated by taking the product of P(hje), P(hj f ), P(hjg),
P(ije), P(ij f ), and P(ijg). To obtain P(hje), we need
f (h,e) and f (e) which can be estimated in Figure 1
by counting the number of edges connecting e and
h and the number of edges starting from e, respec-
tively. So, P(hje) will be 0.16 given that f (h,e) is
one and f (e) is six (see the normalization in (5)).
2.2 Determining an Order
Once we have collected the counts for our features
we can determine the order for a new text that
we haven?t encountered before, since some of the
features representing its sentences will be familiar.
Given a text with N sentences there are N! possi-
ble orders. The set of orders can be represented as a
complete graph, where the set of vertices V is equal
to the set of sentences S and each edge u ! v has
a weight, the probability P(ujv). Cohen et al (1999)
START






H
H
H
H
H
H
S1
(0.2)
H
S2
S3
S3
S2
S2
(0.3)


H
H
S1(0.006)
S3
S3(0.02)
S1
S3
(0.05)
H
S2
S1
S1
S2
Figure 2: Finding an order for a three sentence text
show that the problem of finding an optimal ordering
through a directed weighted graph is NP-complete.
Fortunately, they propose a simple greedy algorithm
that provides an approximate solution which can be
easily modified for our task (see also Barzilay et al
2002).
The algorithm starts by assigning each vertex
v 2 V a probability. Recall that in our case vertices
are sentences and their probabilities can be calcu-
lated by taking the product of the probabilities of
their features. The greedy algorithm then picks the
node with the highest probability and orders it ahead
of the other nodes. The selected node and its incident
edges are deleted from the graph. Each remaining
node is now assigned the conditional probability of
seeing this node given the previously selected node
(see (4)). The node which yields the highest condi-
tional probability is selected and ordered ahead. The
process is repeated until the graph is empty.
As an example consider again a three sentence
text. We illustrate the search for a path through the
graph in Figure 2. First we calculate which of the
three sentences S1, S2, and S3 is most likely to start
the text (during training we record which sentences
appear in the beginning of each text). Assuming that
P(S2jSTART) is the highest, we will order S2 first,
and ignore the nodes headed by S1 and S3. We next
compare the probabilities P(S1jS2) and P(S3jS2).
Since P(S3jS2) is more likely than P(S1jS2), we or-
der S3 after S2 and stop, returning the order S2, S3,
and S1. As can be seen in Figure 2 for each vertex
we keep track of the most probable edge that ends in
that vertex, thus setting th beam search width to one.
Note, that equation (4) would assign lower and
lower probabilities to sentences with large numbers
of features. Since we need to compare sentence pairs
with varied numbers of features, we will normalize
the conditional probabilities P(SijSi?1) by the num-
ber feature of pairs that form the Cartesian product
over Si and Si?1.
1. Laidlaw Transportation Ltd. said shareholders will be asked at its Dec. 7 annual meeting to approve a change of name to
Laidlaw Inc.
2. The company said its existing name hasn?t represented its businesses since the 1984 sale of its trucking operations.
3. Laidlaw is a waste management and school-bus operator, in which Canadian Pacific Ltd. has a 47% voting interest.
Figure 3: A text from the BLLIP corpus
3 Parameter Estimation
The model in Section 2.1 was trained on the BLLIP
corpus (30 M words), a collection of texts from the
Wall Street Journal (years 1987-89). The corpus con-
tains 98,732 stories. The average story length is 19.2
sentences. 71.30% of the texts in the corpus are less
than 50 sentences long. An example of the texts in
this newswire corpus is shown in Figure 3.
The corpus is distributed in a Treebank-
style machine-parsed version which was produced
with Charniak?s (2000) parser. The parser is a
?maximum-entropy inspired? probabilistic gener-
ative model. It achieves 90.1% average preci-
sion/recall for sentences with maximum length 40
and 89.5% for sentences with maximum length 100
when trained and tested on the standard sections
of the Wall Street Journal Treebank (Marcus et al,
1993).
We also obtained a dependency-style version
of the corpus using MINIPAR (Lin, 1998) a broad
coverage parser for English which employs a manu-
ally constructed grammar and a lexicon derived from
WordNet with an additional dictionary of proper
names (130,000 entries in total). The grammar is
represented as a network of 35 nodes (i.e., grammat-
ical categories) and 59 edges (i.e., types of syntactic
(dependency) relations). The output of MINIPAR is a
dependency graph which represents the dependency
relations between words in a sentence (see Table 1
for an example). Lin (1998) evaluated the parser on
the SUSANNE corpus (Sampson, 1996), a domain in-
dependent corpus of British English, and achieved a
recall of 79% and precision of 89% on the depen-
dency relations.
From the two different parsed versions of the
BLLIP corpus the following features were extracted:
Verbs. Investigations into the interpretation of nar-
rative discourse (Asher and Lascarides, 2003) have
shown that specific lexical information (e.g., verbs,
adjectives) plays an important role in determining
the discourse relations between propositions. Al-
though we don?t have an explicit model of rhetorical
relations and their effects on sentence ordering, we
capture the lexical inter-dependencies between sen-
tences by focusing on verbs and their precedence re-
lationships in the corpus.
From the Treebank parses we extracted the
verbs contained in each sentence. We obtained
two versions of this feature: (a) a lemmatized ver-
sion where verbs were reduced to their base forms
and (b) a non-lemmatized version which preserved
tense-related information; more specifically, verbal
complexes (e.g., I will have been going) were iden-
tified from the parse trees heuristically by devis-
ing a set of 30 patterns that search for sequences
of modals, auxiliaries and verbs. This is an attempt
at capturing temporal coherence by encoding se-
quences of events and their morphology which in-
directly indicates their tense.
To give an example consider the text in Fig-
ure 3. For the lemmatized version, sentence (1) will
be represented by say, will, be, ask, and approve; for
the tensed version, the relevant features will be said,
will be asked, and to approve.
Nouns. Centering Theory (CT, Grosz et al 1995)
is an entity-based theory of local coherence, which
claims that certain entities mentioned in an utterance
are more central than others and that this property
constrains a speaker?s use of certain referring ex-
pressions. The principles underlying CT (e.g., conti-
nuity, salience) are of interest to concept-to-text gen-
eration as they offer an entity-based model of text
and sentence planning which is particularly suited
for descriptional genres (Kibble and Power, 2000).
We operationalize entity-based coherence for
text-to-text generation by simply keeping track of
the nouns attested in a sentence without however
taking personal pronouns into account. This simpli-
fication is reasonable if one has text-to-text genera-
tion mind. In multidocument summarization for ex-
ample, sentences are extracted from different docu-
ments; the referents of the pronouns attested in these
sentences are typically not known and in some cases
identical pronouns may refer to different entities. So
making use of noun-pronoun or pronoun-pronoun
co-occurrences will be uninformative or in fact mis-
leading.
We extracted nouns from a lemmatized version
of the Treebank-style parsed corpus. In cases of noun
compounds, only the compound head (i.e., rightmost
noun) was taken into account. A small set of rules
was used to identify organizations (e.g., United Lab-
oratories Inc.), person names (e.g., Jose Y. Cam-
pos), and locations (e.g., New England) spanning
more than one word. These were grouped together
and were also given the general categories person,
organization, and location. The model backs off
to these categories when unknown person names, lo-
cations, and organizations are encountered. Dates,
years, months and numbers were substituted by the
categories date, year, month, and number.
In sentence (1) (see Figure 3) we identify
the nouns Laidlaw Transportation Ltd., shareholder,
Dec 7, meeting, change, name and Laidlaw Inc. In
sentence (2) the relevant nouns are company, name,
business, 1984, sale, and operation.
Dependencies. Note that the noun and verb fea-
tures do not capture the structure of the sentences
to be ordered. This is important for our domain, as
texts seem to be rather formulaic and similar syn-
tactic structures are often used (e.g., direct and in-
direct speech, restrictive relative clauses, predicative
structures). In this domain companies typically say
things, and texts often begin with a statement of what
a company or an individual has said (see sentence (1)
in Figure 3). Furthermore, companies and individu-
als are described with certain attributes (persons can
be presidents or governors, companies are bankrupt
or manufacturers, etc.) that can give clues for infer-
ring coherence.
The dependencies were obtained from the out-
put of MINIPAR. Some of the dependencies for sen-
tence (2) from Figure 3 are shown in Table 1. The
dependencies capture structural as well lexical infor-
mation. They are represented as triples, consisting of
a head (leftmost element, e.g., say, name), a modi-
fier (rightmost element, e.g., company, its) and a re-
lation (e.g., subject (V:subj:N), object (V:obj:N),
modifier (N:mod:A)).
For efficiency reasons we focused on triples
whose dependency relations (e.g., V:subj:N) were
attested in the corpus with frequency larger than
one per million. We further looked at how individ-
ual types of relations contribute to the ordering task.
More specifically we experimented with dependen-
cies relating to verbs (49 types), nouns (52 types),
verbs and nouns (101 types) (see Table 1 for exam-
ples). We also ran a version of our model with all
types of relations, including adjectives, adverbs and
Verb Noun
say V:subj:N company name N:gen:N its
represent V:subj:N name name N:mod:A existing
represent V:have:have have business N:gen:N its
represent V:obj:N business business N:mod:Prep since
company N:det:Det the
Table 1: Dependencies for sentence (2) in Figure 3
A B C D E F G H I J
Model 1 1 2 3 4 5 6 7 8 9 10
Model 2 2 1 5 3 4 6 7 9 8 10
Model 3 10 2 3 4 5 6 7 8 9 1
Table 2: Example of rankings for a 10 sentence text
prepositions (147 types in total).
4 Experiments
In this section we describe our experiments with the
model and the features introduced in the previous
sections. We first evaluate the model by attempting
to reproduce the structure of unseen texts from the
BLLIP corpus, i.e., the corpus on which the model
is trained on. We next obtain an upper bound for the
task by conducting a sentence ordering experiment
with humans and comparing the model against the
human data. Finally, we assess whether this model
can be used for multi-document summarization us-
ing data from Barzilay et al (2002). But before we
outline the details of our experiments we discuss our
choice of metric for comparing different orders.
4.1 Evaluation Metric
Our task is to produce an ordering for the sentences
of a given text. We can think of the sentences as
objects for which a ranking must be produced. Ta-
ble 2 gives an example of a text containing 10 sen-
tences (A?J) and the orders (i.e., rankings) produced
by three hypothetical models.
A number of metrics can be used to measure
the distance between two rankings such as Spear-
man?s correlation coefficient for ranked data, Cayley
distance, or Kendall?s ? (see Lebanon and Lafferty
2002 for details). Kendall?s ? is based on the number
of inversions in the rankings and is defined in (6):
(6) ? = 1? 2(number of inversions)
N(N ?1)/2
where N is the number of objects (i.e., sentences)
being ranked and inversions are the number of in-
terchanges of consecutive elements necessary to ar-
range them in their natural order. If we think in terms
of permutations, then ? can be interpreted as the min-
imum number of adjacent transpositions needed to
bring one order to the other. In Table 2 the number
of inversions can be calculated by counting the num-
ber of intersections of the lines. The metric ranges
from ?1 (inverse ranks) to 1 (identical ranks). The ?
for Model 1 and Model 2 in Table 2 is .822.
Kendall?s ? seems particularly appropriate for
the tasks considered in this paper. The metric is sen-
sitive to the fact that some sentences may be always
ordered next to each other even though their absolute
orders might differ. It also penalizes inverse rank-
ings. Comparison between Model 1 and Model 3
would give a ? of 0.244 even though the orders be-
tween the two models are identical modulo the be-
ginning and the end. This seems appropriate given
that flipping the introduction in a document with the
conclusions seriously disrupts coherence.
4.2 Experiment 1: Ordering Newswire Texts
The model from Section 2.1 was trained on the
BLLIP corpus and tested on 20 held-out randomly
selected unseen texts (average length 15.3). We also
used 20 randomly chosen texts (disjoint from the
test data) for development purposes (average length
16.2). All our results are reported on the test set.
The input to the the greedy algorithm (see Sec-
tion 2.2) was a text with a randomized sentence or-
dering. The ordered output was compared against
the original authored text using ?. Table 3 gives the
average ? (T ) for all 20 test texts when the fol-
lowing features are used: lemmatized verbs (VL),
tensed verbs (VT ), lemmatized nouns (NL), lem-
matized verbs and nouns (VLNL), tensed verbs and
lemmatized nouns (VT NL), verb-related dependen-
cies (VD), noun-related dependencies (ND), verb and
noun dependencies (VDND), and all available de-
pendencies (AD). For comparison we also report the
naive baseline of generating a random oder (BR). As
can be seen from Table 3 the best performing fea-
tures are NL and VDND. This is not surprising given
that NL encapsulates notions of entity-based coher-
ence, which is relatively important for our domain. A
lot of texts are about a particular entity (company or
individual) and their properties. The feature VDND
subsumes several other features and does expectedly
better: it captures entity-based coherence, the inter-
relations among verbs, the structure of sentences and
also preserves information about argument structure
(who is doing what to whom). The distance between
the orders produced by the model and the original
texts increases when all types of dependencies are
Feature T StdDev Min Max
BR .35 .09 .17 .47
VL .44 .24 .17 .93
VT .46 .21 .17 .80
NL .54 .16 .18 .76
VLNL .46 .12 .18 .61
VT NL .49 .17 .21 .86
VD .51 .17 .10 .83
ND .45 .17 .10 .67
VDND .57 .12 .62 .83
AD .48 .17 .10 .83
Table 3: Comparison between original BLLIP texts
and model generated variants
taken into account. The feature space becomes too
big, there are too many spurious feature pairs, and
the model can?t distinguish informative from non-
informative features.
We carried out a one-way Analysis of Vari-
ance (ANOVA) to examine the effect of different fea-
ture types. The ANOVA revealed a reliable effect
of feature type (F(9,171) = 3.31; p < 0.01). We
performed Post-hoc Tukey tests to further examine
whether there are any significant differences among
the different features and between our model and
the baseline. We found out that NL, VT NL, VD, and
VDND are significantly better than BR (? = 0.01),
whereas NL and VDND are not significantly differ-
ent from each other. However, they are significantly
better than all other features (? = 0.05).
4.3 Experiment 2: Human Evaluation
In this experiment we compare our model?s perfor-
mance against human judges. Twelve texts were ran-
domly selected from the 20 texts in our test data. The
texts were presented to subjects with the order of
their sentences scrambled. Participants were asked
to reorder the sentences so as to produce a coherent
text. Each participant saw three texts randomly cho-
sen from the pool of 12 texts. A random order of sen-
tences was generated for every text the participants
saw. Sentences were presented verbatim, pronouns
and connectives were retained in order to make or-
dering feasible. Notice that this information is absent
from the features the model takes into account.
The study was conducted remotely over the In-
ternet using a variant of Barzilay et al?s (2002) soft-
ware. Subjects first saw a set of instructions that ex-
plained the task, and had to fill in a short question-
naire including basic demographic information. The
experiment was completed by 137 volunteers (ap-
proximately 33 per text), all native speakers of En-
glish. Subjects were recruited via postings to local
Feature T StdDev Min Max
VL .45 .16 .10 .90
VT .46 .18 .10 .90
NL .51 .14 .10 .90
VLNL .44 .14 .18 .61
VT NL .49 .18 .21 .86
VD .47 .14 .10 .93
ND .46 .15 .10 .86
VDND .55 .15 .10 .90
AD .48 .16 .10 .83
HH .58 .08 .26 .75
Table 4: Comparison between orderings produced by
humans and the model on BLLIP texts
Features T StdDev Min Max
BR .43 .13 .19 .97
NL .48 .16 .21 .86
VDND .56 .13 .32 .86
HH .60 .17 ?1 .98
Table 5: Comparison between orderings produced by
humans and the model on multidocument summaries
Usenet newsgroups.
Table 4 reports pairwise ? averaged over
12 texts for all participants (HH) and the average ?
between the model and each of the subjects for all
features used in Experiment 1. The average distance
in the orderings produced by our subjects is .58. The
distance between the humans and the best features
is .51 for NL and .55 for VDND. An ANOVA yielded
a significant effect of feature type (F(9,99) = 5.213;
p < 0.01). Post-hoc Tukey tests revealed that VL,
VT , VD, ND, AD, VLNL, and VT NL perform sig-
nificantly worse than HH (? = 0.01), whereas NL
and VDND are not significantly different from HH
(? = 0.01). This is in agreement with Experiment 1
and points to the importance of lexical and structural
information for the ordering task.
4.4 Experiment 3: Summarization
Barzilay et al (2002) collected a corpus of multiple
orderings in order to study what makes an order co-
hesive. Their goal was to improve the ordering strat-
egy of MULTIGEN (McKeown et al, 1999) a mul-
tidocument summarization system that operates on
news articles describing the same event. MULTIGEN
identifies text units that convey similar information
across documents and clusters them into themes.
Each theme is next syntactically analysed into pred-
icate argument structures; the structures that are re-
peated often enough are chosen to be included into
the summary. A language generation system outputs
a sentence (per theme) from the selected predicate
argument structures.
Barzilay et al (2002) collected ten sets of arti-
cles each consisting of two to three articles reporting
the same event and simulated MULTIGEN by man-
ually selecting the sentences to be included in the
final summary. This way they ensured that order-
ings were not influenced by mistakes their system
could have made. Explicit references and connec-
tives were removed from the sentences so as not to
reveal clues about the sentence ordering. Ten sub-
jects provided orders for each summary which had
an average length of 8.8.
We simulated the participants? task by using the
model from Section 2.1 to produce an order for each
candidate summary1. We then compared the differ-
ences in the orderings generated by the model and
participants using the best performing features from
Experiment 2 (i.e., NL and VDND). Note that the
model was trained on the BLLIP corpus, whereas the
sentences to be ordered were taken from news arti-
cles describing the same event. Not only were the
news articles unseen but also their syntactic struc-
ture was unfamiliar to the model. The results are
shown in table 5, again average pairwise ? is re-
ported. We also give the naive baseline of choosing
a random order (BR). The average distance in the
orderings produced by Barzilay et al?s (2002) par-
ticipants is .60. The distance between the humans
and NL is .48 whereas the average distance between
VDND and the humans is .56. An ANOVA yielded a
significant effect of feature type (F(3,27) = 15.25;
p < 0.01). Post-hoc Tukey tests showed that VDND
was significantly better than BR, but NL wasn?t. The
difference between VDND and HH was not signifi-
cant.
Although NL performed adequately in Experi-
ments 1 and 2, it failed to outperform the baseline in
the summarization task. This may be due to the fact
that entity-based coherence is not as important as
temporal coherence for the news articles summaries.
Recall that the summaries describe events across
documents. This information is captured more ad-
equately by VDND and not by NL that only keeps a
record of the entities in the sentence.
5 Discussion
In this paper we proposed a data intensive approach
to text coherence where constraints on sentence or-
dering are learned from a corpus of domain-specific
1The summaries as well as the human data are available from
http://www.cs.columbia.edu/?noemie/ordering/.
texts. We experimented with different feature encod-
ings and showed that lexical and syntactic informa-
tion is important for the ordering task. Our results
indicate that the model can successfully generate or-
ders for texts taken from the corpus on which it is
trained. The model also compares favorably with hu-
man performance on a single- and multiple docu-
ment ordering task.
Our model operates on the surface level rather
than the logical form and is therefore suitable for
text-to-text generation systems; it acquires ordering
constraints automatically, and can be easily ported to
different domains and text genres. The model is par-
ticularly relevant for multidocument summarization
since it could provide an alternative to chronolog-
ical ordering especially for documents where pub-
lication date information is unavailable or uninfor-
mative (e.g., all documents have the same date). We
proposed Kendall?s ? as an automated method for
evaluating the generated orders.
There are a number of issues that must be ad-
dressed in future work. So far our evaluation metric
measures order similarities or dissimilarities. This
enables us to assess the importance of particular
feature combinations automatically and to evaluate
whether the model and the search algorithm gener-
ate potentially acceptable orders without having to
run comprehension experiments each time. Such ex-
periments however are crucial for determining how
coherent the generated texts are and whether they
convey the same semantic content as the originally
authored texts. For multidocument summarization
comparisons between our model and alternative or-
dering strategies are important if we want to pursue
this approach further.
Several improvements can take place with re-
spect to the model. An obvious question is whether
a trigram model performs better than the model
presented here. The greedy algorithm implements
a search procedure with a beam of width one. In
the future we plan to experiment with larger widths
(e.g., two or three) and also take into account fea-
tures that express semantic similarities across docu-
ments either by relying on WordNet or on automatic
clustering methods.
Acknowledgments
The author was supported by EPSRC grant number R40036. We
are grateful to Regina Barzilay and Noemie Elhadad for making
available their software and for providing valuable comments
on this work. Thanks also to Stephen Clark, Nikiforos Kara-
manis, Frank Keller, Alex Lascarides, Katja Markert, and Miles
Osborne for helpful comments and suggestions.
References
Asher, Nicholas and Alex Lascarides. 2003. Logics of Conver-
sation. Cambridge University Press.
Barzilay, Regina. 2003. Information Fusion for Multi-
Document Summarization: Praphrasing and Generation.
Ph.D. thesis, Columbia University.
Barzilay, Regina, Noemie Elhadad, and Kathleen R. McKeown.
2002. Inferring strategies for sentence ordering in multidoc-
ument news summarization. Journal of Artificial Intelligence
Research 17:35?55.
Charniak, Eugene. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics.
Seattle, WA, pages 132?139.
Cohen, William W., Robert E. Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial Intelli-
gence Research 10:243?270.
Grosz, Barbara, Aravind Joshi, , and Scott Weinstein. 1995.
Centering: A framework for modeling the local coherence
of discourse. Computational Linguistics 21(2):203?225.
Katz, Slava M. 1987. Estimation of probabilities from sparse
data for the language model component of a speech recog-
nizer. IEEE Transactions on Acoustics Speech and Signal
Processing 33(3):400?401.
Kibble, Rodger and Richard Power. 2000. An integrated frame-
work for text planning and pronominalisation. In In Pro-
ceedings of the 1st International Conference on Natural Lan-
guage Generation. Mitzpe Ramon, Israel, pages 77?84.
Lebanon, Guy and John Lafferty. 2002. Combining rankings
using conditional probability models on permutations. In
C. Sammut and A. Hoffmann, editors, In Proceedings of the
19th International Conference on Machine Learning. Mor-
gan Kaufmann Publishers, San Francisco, CA.
Lin, Dekang. 1998. Dependency-based evaluation of MINIPAR.
In In Proceedings on of the LREC Workshop on the Evalua-
tion of Parsing Systems. Granada, pages 48?56.
Marcu, Daniel. 1997. From local to global coherence: A
bottom-up approach to text planning. In In Proceedings of
the 14th National Conference on Artificial Intelligence. Prov-
idence, Rhode Island, pages 629?635.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics
19(2):313?330.
McKeown, Kathleen R., Judith L. Klavans, Vasileios Hatzivas-
siloglou, Regina Barzilay, and Eleazar Eskin. 1999. Towards
multidocument summarization by reformulation: Progress
and prospects. In Proceedings of the 16th National Confer-
ence on Artificial Intelligence. Orlando, FL, pages 453?459.
Mellish, Chris, Alistair Knott, Jon Oberlander, and Mick O?
Donnell. 1998. Experiments using stochastic search for text
planning. In In Proceedings of the 9th International Work-
shop on Natural Language Generation. Ontario, Canada,
pages 98?107.
Reiter, Ehud and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press,
Cambridge.
Sampson, Geoffrey. 1996. English for the Computer. Oxford
University Press.
Automatic Paragraph Identification:
A Study across Languages and Domains
Caroline Sporleder
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
csporled@inf.ed.ac.uk
Mirella Lapata
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
mlap@dcs.shef.ac.uk
Abstract
In this paper we investigate whether paragraphs can
be identified automatically in different languages
and domains. We propose a machine learning ap-
proach which exploits textual and discourse cues
and we assess how well humans perform on this
task. Our best models achieve an accuracy that is
significantly higher than the best baseline and, for
most data sets, comes to within 6% of human per-
formance.
1 Introduction
Written texts are usually broken up into sentences
and paragraphs. Sentence splitting is a necessary
pre-processing step for a number of Natural Lan-
guage Processing (NLP) tasks including part-of-
speech tagging and parsing. Since sentence-final
punctuation can be ambiguous (e.g., a period can
also be used in an abbreviation as well as to mark
the end of a sentence), the task is not trivial and has
consequently attracted a lot of attention (e.g., Rey-
nar and Ratnaparkhi (1997)). In contrast, there
has been virtually no previous research on inferring
paragraph boundaries automatically. One reason for
this is that paragraph boundaries are usually marked
unambiguously by a new line and extra white space.
However, a number of applications could bene-
fit from a paragraph detection mechanism. Text-
to-text generation applications such as single- and
multidocument summarisation as well as text sim-
plification usually take naturally occurring texts as
input and transform them into new texts satisfying
specific constraints (e.g., length, style, language).
The output texts do not always preserve the struc-
ture and editing conventions of the original text.
In summarisation, for example, sentences are typ-
ically extracted verbatim and concatenated to form
a summary. Insertion of paragraph breaks could im-
prove the readability of the summaries by indicating
topic shifts and providing visual targets to the reader
(Stark, 1988).
Machine translation is another application for
which automatic paragraph detection is relevant.
Current systems deal with paragraph boundary in-
sertion in the target language simply by preserv-
ing the boundaries from the source language. How-
ever, there is evidence for cross-linguistic variation
in paragraph formation and placement, particularly
for languages that are not closely related such as En-
glish and Chinese (Zhu, 1999). So, a paragraph in-
sertion mechanism that is specific to the target lan-
guage, instead of one that relies solely on the source
language, may yield more readable texts.
Paragraph boundary detection is also relevant for
speech-to-text applications. The output of auto-
matic speech recognition systems is usually raw
text without any punctuation or paragraph breaks.
This naturally makes the text very hard to read,
which can cause processing difficulties, especially
if speech recognition is used to provide deaf stu-
dents with real-time transcripts of lectures. Further-
more, sometimes the output of a speech recogniser
needs to be processed automatically by applications
such as information extraction or summarisation.
Most of these applications (e.g., Christensen et al,
(2004)) port techniques developed for written texts
to spoken texts and therefore require input that is
punctuated and broken into paragraphs. While there
has been some research on finding sentence bound-
aries in spoken text (Stevenson and Gaizauskas,
2000), there has been little research on determining
paragraph boundaries.1
If paragraph boundaries were mainly an aes-
thetic device for visually breaking up long texts into
smaller chunks, as has previously been suggested
(see Longacre (1979)), paragraph boundaries could
be easily inserted by splitting a text into several
equal-size segments. Psycho-linguistic research,
however, indicates that paragraph boundaries are
not purely aesthetic. For example, Stark (1988)
1There has been research on using phonetic cues to segment
speech into ?acoustic paragraphs? (Hauptmann and Smith,
1995). However, these do not necessarily correspond to written
paragraphs. But even if they did, textual cues could comple-
ment phonetic information to identify paragraphs.
asked her subjects to reinstate paragraph bound-
aries into fiction texts from which all boundaries
had been removed and found that humans are able
to do so with an accuracy that is higher than would
be expected by chance. Crucially, she also found
that (a) individual subjects did not make all their
paragraphs the same length and (b) paragraphs in
the original text whose length deviated significantly
from the average paragraph length were still iden-
tified correctly by a large proportion of subjects.
These results show that people are often able to
identify paragraphs correctly even if they are excep-
tionally short or long without defaulting to a simple
template of average paragraph length.
Human agreement on the task suggests that the
text itself provides cues for paragraph insertion,
even though there is some disagreement over which
specific cues are used by humans (see Stark (1988)).
Possible cues include repeated content words, pro-
noun coreference, paragraph length, and local se-
mantic connectedness.
In this paper, we investigate whether it is possi-
ble to exploit some of these textual cues together
with syntactic and discourse related information to
determine paragraph boundaries automatically. We
treat paragraph boundary identification as a classi-
fication task and examine whether the difficulty of
the task and the utility of individual textual cues
varies across languages and across domains. We
also assess human performance on the same task
and whether it differs across domains.
2 Related Work
Previous work has focused extensively on the task
of automatic text segmentation whose primary goal
is to divide individual texts into sub-topics. De-
spite their differences, most methods are unsuper-
vised and typically rely on the distribution of words
in a given text to provide cues for topic segmenta-
tion.2 Hearst?s (1997) TextTiling algorithm, for ex-
ample, determines sub-topic boundaries on the basis
of term overlap in adjacent text blocks. In more re-
cent work, Utiyama and Isahara (2001) combine a
statistical segmentation model with a graph search
algorithm to find the segmentation with the maxi-
mum probability. Beeferman et al (1999) use su-
pervised learning methods to infer boundaries be-
tween texts. They employ language models to de-
tect topic shifts and combine them with cue word
features.
2Due to lack of space we do not describe previous work in
text segmentation here in detail; we refer the reader to Utiyama
and Isahara (2001) and Pevzener and Hearst (2002) for a com-
prehensive overview.
Our work differs from these previous approaches
in that paragraphs do not always correspond to sub-
topics. While topic shifts often correspond to para-
graph breaks, not all paragraph breaks indicate a
topic change. Breaks between paragraphs are often
inserted for other (not very well understood) reasons
(see Stark (1988)). Therefore, the segment granular-
ity is more fine-grained for paragraphs than for top-
ics. An important advantage for methods developed
for paragraph detection (as opposed to those de-
veloped for text-segmentation) is that training data
is readily available, since paragraph boundaries are
usually unambiguously marked in texts. Hence, su-
pervised methods are ?cheap? for this task.
3 Our Approach
3.1 Corpora
Our study focused on three languages: English,
German, and Greek. These languages differ in
terms of word order (fixed in English, semi-free in
German, fairly flexible in Greek). Greek and Ger-
man also have richer morphology than English. Ad-
ditionally, Greek has a non-Latin writing system.
For each language we created corpora represen-
tative of three domains: fiction, news, and parlia-
mentary proceedings. Previous work on the role of
paragraph markings (Stark, 1988) has focused ex-
clusively on fiction texts, and has shown that hu-
mans can identify paragraph boundaries in this do-
main reliably. It therefore seemed natural to test our
automatic method on a domain for which the task
has been shown to be feasible. We selected news
texts since most summarisation methods today fo-
cus on this domain and we can therefore explore the
relevance of our approach for this application. Fi-
nally, parliamentary proceedings are transcripts of
speech, and we can examine whether a method that
relies solely on textual cues is also useful for spoken
texts.
For English, we used the whole Hansard section
of the BNC, as our corpus of parliamentary proceed-
ings. We then created a fiction corpus of similar size
by randomly selecting prose files from the fiction
part of the BNC. In the same way a news corpus
was created from the Penn Treebank.
For German, we used the prose part of Project
Gutenberg?s e-book collection3 as our fiction corpus
and the complete Frankfurter Rundschau part of the
ECI corpus4 as our news corpus. The corpus of par-
liamentary proceedings was obtained by randomly
3http://www.gutenberg.net/ For copyright reasons,
this web site mainly contains books published before 1923.
4http://www.elsnet.org/eci.html
fiction news parliament
English 1,140,000 1,156,000 1,156,000
German 2,500,000 4,100,000 3,400,000
Greek 563,000 1,500,000 1,500,000
Table 1: Number of words per corpus
selecting a subset of the German section from the
Europarl corpus (Koehn, 2002).
For Greek, a fiction corpus was compiled from
the ECI corpus by selecting all prose files that con-
tained paragraph markings. Our news corpus was
downloaded from the WWW site of the Modern
Greek newspaper Eleftherotypia and consists of fi-
nancial news from the period of 2001?2002. A cor-
pus of parliamentary proceedings was again created
by randomly selecting a subset of the Greek section
of the Europarl corpus (Koehn, 2002).
Parts of the data were further pre-processed to
insert sentence boundaries. We trained a publicly
available sentence splitter (Reynar and Ratnaparkhi,
1997) on a small manually annotated sample (1,000
sentences per domain per language) and applied it
to our corpora. Table 1 shows the corpus sizes. All
corpora were split into training (72%), development
(24%) and test set (4%).
3.2 Machine Learning
We used BoosTexter (Schapire and Singer, 2000) as
our machine learning system. BoosTexter was orig-
inally developed for text categorisation and com-
bines a boosting algorithm with simple decision
rules. For all domains and languages our training
examples were sentences. Class labels encoded for
each sentence whether it was starting a paragraph or
not.
The features we used fall broadly into three dif-
ferent areas: non-syntactic features, language mod-
elling features and syntactic features. The latter
were only applied to English as we did not have suit-
able parsers for German and Greek.
The values of our features are numeric, boolean
or ?text?. BoosTexter applies unigram models when
forming classification hypotheses for features with
?text? values. These can be simply words or anno-
tations such as part-of-speech tags.
We deliberately did not include anaphora-based
features. While anaphors can help determine para-
graph boundaries (paragraph initial sentences tend
to contain few or no anaphors), anaphora structure
is dependent on paragraph structure rather than the
other way round. Hence, in applications which ma-
nipulate texts and thereby potentially ?mess-up? the
anaphora structure (e.g., multi-document summari-
sation), anaphors are not a reliable cue for paragraph
identification.5
3.2.1 Non-syntactic Features
Distance (Ds, Dw): These features encode the dis-
tance of the current sentence from the previous para-
graph break. We measured distance in terms of the
number of intervening sentences (Ds) as well as in
terms of the number of intervening words (Dw). If
paragraph breaks were driven purely by aesthetics
one would expect this feature to be among the most
successful ones.6
Sentence Length (Length): This feature encodes
the number of words in the current sentence. Aver-
age sentence length is known to vary with text posi-
tion (Genzel and Charniak, 2003) and it is possible
that it also varies with paragraph position.
Relative Position (Pos): The relative position of a
sentence in the text is calculated by dividing the cur-
rent sentence number by the number of sentences
in the text. The motivation for this feature is that
paragraph length may vary with text position. For
example, it is possible that paragraphs at the begin-
ning and end of a text are shorter than paragraphs
in the middle and hence a paragraph break is more
likely at the two former text positions.
Quotes (Quotep, Quotec, Quotei): These features
encode whether the previous or current sentence
contain a quotation (Quotep and Quotec, respec-
tively) and whether the current sentence contin-
ues a quotation that started in a preceding sentence
(Quotei). The presence of quotations can provide
cues for speaker turns, which are often signalled by
paragraph breaks.
Final Punctuation (FinPun): This feature keeps
track of the final punctuation mark of the previous
sentence. Some punctuation marks may provide
hints as to whether a break should be introduced.
For example, in the news domain, where there is
hardly any dialogue, if the previous sentence ended
in a question mark, it is likely that the current sen-
tence supplies an answer to this question, thus mak-
ing a paragraph break improbable.
Words (W1, W2, W3, Wall ): These text-valued fea-
tures encode the words in the sentence. Wall takes
the complete sentence as its value. W1, W2 and W3
encode the first word, the first two words and the
first three words, respectively.
5This is also true for some of the other features we use
(e.g., sentence length) but not quite to the same extent.
6One could also use the history of class labels assigned to
previous sentences as a feature (as in part-of-speech tagging);
however, we leave this to future research.
3.2.2 Language Modelling Features
Our motivation for including language modelling
features stems from Genzel and Charniak?s (2003)
work where they show that the word entropy rate is
lower for paragraph initial sentences than for non-
paragraph initial ones. We therefore decided to ex-
amine whether word entropy rate is a useful feature
for the paragraph prediction task. Using the train-
ing set for each language and domain, we created
language models with the CMU language modelling
toolkit (Clarkson and Rosenfeld, 1997). We exper-
imented with language models of variable length
(i.e., 1?5) and estimated two features: the prob-
ability of a given sentence according to the lan-
guage model (LMp) and the per-word entropy rate
(LMpwe). The latter was estimated by dividing the
sentence probability as assigned by the language
model by the number of sentence words (see Genzel
and Charniak (2003)).
We additionally experimented with character
level n-gram models. Such models are defined over
a relatively small vocabulary and can be easily con-
structed for any language without pre-processing.
Character level n-gram models have been applied
to the problem of authorship attribution and ob-
tained state-of-the art results (Peng et al, 2003).
If some characters are more often attested in para-
graph starting sentences (e.g., ?A? or ?T?), then we
expect these sentences to have a higher probability
compared to non-paragraph starting ones. Again,
we used the CMU toolkit for building the character
level n-gram models. We experimented with mod-
els whose length varied from 2 to 8 and estimated
the probability assigned to a sentence according to
the character level model (CMp).
3.2.3 Syntactic Features
For the English data we also used several features
encoding syntactic complexity. Genzel and Char-
niak (2003) suggested that the syntactic complex-
ity of sentences varies with their position in a para-
graph. Roughly speaking, paragraph initial sen-
tences are less complex. Hence, complexity mea-
sures may be a good indicator of paragraph bound-
aries. To estimate complexity, we parsed the texts
with Charniak?s (2001) parser and implemented the
following features:
Parsed: This feature states whether the current
sentence could be parsed. While this is not a real
measure of syntactic complexity it is probably cor-
related with it.
Number of phrases (nums, numvp, numnp, numpp):
These features measure syntactic complexity in
terms of the number of S, VP, NP, and PP con-
stituents in the parse tree.
Signature (Sign, Signp): These text-valued fea-
tures encode the sequence of part-of-speech tags in
the current sentence. Sign only encodes word tags,
while Signp also includes punctuation tags.
Children of Top-Level Nodes (Childrs1, Childrs):
These text-valued features encode the top-level
complexity of a parse tree: Childrs1 takes as its
value the sequence of syntactic labels of the children
of the S1-node (i.e., the root of the parse tree), while
Childrs encodes the syntactic labels of the children
of the highest S-node(s). For example, Childrs1 may
encode that the sentence consists of one clause and
Childrs may encode that this clause consists of an
NP, a VP, and a PP.
Branching Factor (Branchs, Branchvp, Branchnp,
Branchpp): These features express the average
number of children of a given non-terminal con-
stituent (cf. Genzel and Charniak (2003)). We com-
pute the branching factor for S, VP, NP, and PP con-
stituents.
Tree Depth: We define tree depth as the average
length of a path (from root node to leaf node).
Cue Words (Cues, Cuem, Cuee): These features
are not strictly syntactic but rather discourse-based.
They encode discourse cues (such as because) at
the start (Cues), in the middle (Cuem) and at the
end (Cuee) of the sentence, where ?start? is the first
word, ?end? the last one, and everything else is
?middle?. We keep track of all cue word occur-
rences, without attempting to distinguish between
their syntactic and discourse usages.
For English, there are extensive lists of discourse
cues (we used Knott (1996)), but such lists are not
widely available for German and Greek. Hence, we
only used this feature on the English data.
4 Experiments
BoosTexter is parametrised with respect to the num-
ber of training iterations. In all our experiments,
this parameter was optimised on the development
set; BoosTexter was initially trained for 500 itera-
tions, and then re-trained with the number of itera-
tions that led to the lowest error rate on the devel-
opment set. Throughout this paper all results are re-
ported on the unseen test set and were obtained us-
ing models optimised on the development set. We
report the models? accuracy at predicting the right
label (i.e., paragraph starting or not) for each sen-
tence.
English German Greek
feature fiction news parl. fiction news parl. fiction news parl.
Bd 60.16 51.73 59.50 65.44 59.03 58.26 59.00 52.85 66.48
Bm 71.04 51.44 69.38 75.75 68.24 66.17 67.57 53.99 76.25
Dists 71.07 57.74 54.02 75.80 68.25 66.23 67.69 57.94 76.30
Distw 71.02 63.08 65.64 75.80 67.70 67.20 68.31 59.76 76.30
Length 72.08 56.11 68.45 75.75 72.55 67.10 67.52 60.84 76.55
Position 71.04 49.18 38.71 75.68 68.05 66.35 67.57 56.52 76.35
Quotep 80.84 56.25 30.62 72.97 68.24 66.23 72.80 58.00 76.30
Quotec 80.64 54.95 31.00 72.35 68.24 66.17 71.03 53.99 76.25
Quotei 71.04 51.44 30.62 75.75 68.24 66.17 67.57 53.99 76.25
FinPun 72.08 54.18 71.75 73.15 76.36 69.53 73.33 59.86 76.55
W1 72.96 57.74 82.05 75.43 73.87 75.25 67.05 67.41 76.81
W2 73.47 58.51 80.62 75.80 74.77 76.74 66.37 68.22 78.48
W3 73.68 59.90 80.73 75.60 74.50 76.79 67.63 67.88 78.43
Wall 73.99 61.78 75.40 75.60 73.03 76.20 67.78 67.88 77.26
BestLMp 72.83 55.96 69.66 75.93 71.39 67.40 67.57 61.64 76.50
BestLMpwe 72.16 52.21 69.88 75.90 69.24 66.98 67.83 56.29 76.40
BestCMp 72.70 57.36 69.49 75.88 73.37 67.53 67.68 61.68 76.51
allns lcm 82.45   70.77   82.71   76.55

  79.28   79.17   78.03   76.31   79.35  
Table 2: Accuracy of non-syntactic and language modelling features on test set
4.1 The Influence of Non-syntactic Features
In the first set of experiments, we ran BoosTexter
on all 9 corpora using non-syntactic and language
modelling features. To evaluate the contribution of
individual features to the classification task, we built
one-feature classifiers in addition to a classifier that
combined all features. Table 2 shows the test set
classification accuracy of the individual features and
their combination (allns lcms). The length of the lan-
guage and character models was optimised on the
development set. The test set accuracy of the opti-
mised models is shown as BestLMp and BestLMpwe
(language models) and BestCMp (character mod-
els).7 The results for the three best performing one-
feature classifiers and the combined classifier are
shown in boldface.
BoosTexter?s classification accuracy was further
compared against two baselines. A distance-based
baseline (Bd) was obtained by hypothesising a para-
graph break after every d sentences. We estimated d
in the training data by counting the average number
of sentences between two paragraphs. Our second
baseline, Bm, defaults to the majority class, i.e., as-
sumes that the text does not have paragraph breaks.
For all languages and domains, the combined
models perform better than the best baseline. In or-
der to determine whether this difference is signifi-
cant, we applied ?2 tests. The diacritic   (   ) in Ta-
7Which language and character models perform best varies
slightly across corpora but no clear trends emerge.
ble 2 indicates whether a given model is (not) sig-
nificantly different from the best baseline. Signifi-
cant results are achieved across the board with the
exception of German fiction. We believe the rea-
son for this lies in the corpus itself, as it is very
heterogeneous, containing texts whose publication
date ranges from 1766 to 1999 and which exhibit a
wide variation in style and orthography. This makes
it difficult for any given model to reliably identify
paragraph boundaries in all texts.
In general, the best performing features vary
across domains but not languages. Word features
(W1?W3, Wall ) yield the best classification accura-
cies for news and parliamentary domains, whereas
for fiction, quotes and punctuation seem more use-
ful. The only exception is the German fiction cor-
pus, which consists mainly of 19th century texts.
These contain less direct speech than the two fic-
tion corpora for English and Greek (which contain
contemporary texts). Furthermore, while examples
of direct speech in the English corpus often involve
short dialogues, where a paragraph boundary is in-
troduced after each speaker turn, the German cor-
pus contains virtually no dialogues and examples of
direct speech are usually embedded in a longer nar-
rative and not surrounded by paragraph breaks.
Note that the distance in words from the previ-
ous paragraph boundary (Distw) is a good indicator
for a paragraph break in the English news domain.
However, this feature is less useful for the other two
languages. An explanation might be that the En-
glish news corpus is very homogeneous (i.e., it con-
tains articles that not only have similar content but
are also structurally alike). The Greek news cor-
pus is relatively homogeneous; it mainly contains
financial news articles but also some interviews, so
there is greater variation in paragraph length, which
means that the distance feature is overtaken by the
word-based features. Finally, the German news cor-
pus is highly heterogeneous, containing not only
news stories but also weather forecasts, sports re-
sults and cinema listings. This leads to a large vari-
ation in paragraph length, which in turn means that
the distance feature performs worse than the best
baseline.
The heterogeneity of the German news corpus
may also explain another difference: while the fi-
nal punctuation of the previous sentence (FinPun)
is among the less useful features for English and
Greek (albeit still outperforming the baseline), it
is the best performing feature for German. The
German news corpus contains many ?sentences?
that end in atypical end-of-sentence markers such
as semi-colons (which are found often in cinema
listings). Atypical markers will often not occur
before paragraph breaks, whereas typical markers
will. This fact renders final punctuation a better
predictor of paragraph breaks in the German corpus
than in the other two corpora.
The language models behave similarly across do-
mains and languages. With the exception of the
news domain, they do not seem to be able to out-
perform the majority baseline by more than 1%.
The word entropy rate yields the worst performance,
whereas character-based models perform as well as
word-based models. In general, our results show
that language modelling features are not particularly
useful for this task.
4.2 The Influence of Syntactic Features
Our second set of experiments concentrated solely
on the English data and investigated the useful-
ness of the syntactic features (see Table 3). Again,
we created one-feature classifiers and a classifier
that combined all features, i.e., language and char-
acter models, non-syntactic, and syntactic features
(allns lcm syn). Table 3 also repeats the performance
of the two baselines (Bd and Bm) and the combined
non-syntactic models (allns lcm). The accuracies of
the three best performing one-feature models and
the combined model are again shown in boldface.
As can be seen, syntactic features do not con-
tribute very much to the overall performance. They
only increase the accuracy by about 1%. A ?2 test
English
feature fiction news parl.
Bd 60.16 51.73 59.50
Bm 71.04 51.44 69.38
Cues 71.48 51.49 40.64
Cuem 70.97 54.28 59.03
Cuee 71.04 51.78 31.61
Parse 71.04 51.88 30.62
Nums 71.04 53.56 69.05
Numvp 71.04 54.18 70.59
Numnp 71.77 56.11 68.94
Numpp 71.04 53.61 64.98
Numad jp 71.04 51.11 42.62
Numadvp 71.04 52.40 47.96
Sign 75.39 57.02 67.95
Signp 75.49 59.18 70.76
Childrs1 71.69 55.87 79.35
Childrs 75.34 55.53 79.52
Branchs 71.35 55.82 69.11
Branchvp 71.33 53.46 70.48
Branchnp 71.77 56.11 33.09
Branchpp 71.04 51.44 30.62
TreeDepth 72.57 54.04 69.00
allns lcm 82.45 70.77 82.71
allns lcm syn 82.91  
 ? 71.83    ? 83.92    ?
Table 3: Syntactic features on English test data
revealed that the difference between allns lcm and
allns lcm syn is not statistically significant (indicated
by
 ? in Table 3) for any of the three domains.
The syntactic features seem to be less domain de-
pendent than the non-syntactic ones. In general, the
part-of-speech signature features (Sign, Signp) are
a good predictor, followed by the syntactic labels
of the children of the top nodes (Childrs, Childrs1).
The number of NPs (Numnp) and their branching
factor (Branchnp) are also good indicators for some
domains, particularly the news domain. This is
plausible since paragraph initial sentences in the
Wall Street Journal often contain named entities,
such as company names, which are parsed as flat
NPs, i.e., have a relatively high branching factor.
4.3 The Effect of Training Size
Finally, we examined the effect of the size of the
training data on the learner?s classification accuracy.
We conducted our experiments solely on the English
data, however we expect the results to generalise to
German and Greek. From each English training set
we created ten progressively smaller data sets, the
first being identical to the original set, the second
containing 9/10 of sentences in the original train-
6.8 13.6 20.4 27.2 33.9 40.8 47.6 54.4 61.1 67.970
80
90
100
 
A
cc
ur
ac
y 
(%
)
Fiction
3.9 7.8 11.7 15.6 19.5 23.4 27.3 31.20 35.1 39.360
70
80
90
100
A
cc
ur
ac
y 
(%
)
News
4.5 9.1 13.6 18.1 22.6 27.2 31.7 36.2 40.7 45.4
Thousand instances of training data
70
80
90
100
A
cc
ur
ac
y 
(%
)
Parliament
Figure 1: Learning Curves for English
Kappa % Agr
fiction .72 88.58
news .47 77.45
parl. .76 88.50
Table 4: Human agreement on the paragraph identi-
fication task
ing set, the third containing 8/10, etc. The training
instances in each data set were selected randomly.
BoosTexter was trained on each of these sets (using
all features), as described previously, and tested on
the test set.
Figure 1 shows the learning curves obtained this
way. The curves are more or less flat, i.e., increas-
ing the amount of training data does not have a large
effect on the performance of the model. Further-
more, even the smallest of our training sets is big
enough to outperform the best baseline. Hence, it is
possible to do well on this task even with less train-
ing data. This is important, given that for spoken
texts, paragraph boundaries may have to be obtained
by manual annotation. The learning curves indicate
that relatively modest effort would be required to
obtain training data were it not freely available.
4.4 Human Evaluation
We established an upper bound against which our
automatic methods could be compared by conduct-
ing an experiment that assessed how well humans
agree on identifying paragraph boundaries. Five
participants were given three English texts (one
from each domain), selected randomly from the test
corpus. Each text consisted of approximately a tenth
of the original test set (i.e., 200?400 sentences).
The participants were asked to insert paragraph
breaks wherever it seemed appropriate to them. No
other instructions were given, as we wanted to see
whether they could independently perform the task
without any specific knowledge regarding the do-
mains and their paragraphing conventions.
We measured the agreement of the judges using
the Kappa coefficient (Siegel and Castellan, 1988)
but also report percentage agreement to facilitate
comparison with our models. In all cases, we com-
pute pairwise agreements and report the mean. Our
results are shown in Table 4.
As can be seen, participants tend to agree with
each other on the task. The least agreement is ob-
served for the news domain. This is somewhat ex-
pected as the Wall Street Journal texts are rather dif-
ficult to process for non-experts. Also remember,
that our subjects were given no instructions or train-
ing. In all cases our models yield an accuracy lower
than the human agreement. For the fiction domain
the best model is 5.67% lower than the upper bound,
for the news domain it is 5.62% and for the parlia-
ment domain it is 5.42% (see Tables 4 and 3).
5 Conclusion
In this paper, we investigated whether it is possible
to predict paragraph boundaries automatically using
a supervised approach which exploits textual, syn-
tactic and discourse cues. We achieved accuracies
between 71.83% and 83.92%. These were in all but
one case significantly higher than the best baseline.
We conducted our study in three different do-
mains and languages and found that the best fea-
tures for the news and parliamentary proceedings
domains are based on word co-occurrence, whereas
features that exploit punctuation are better predic-
tors for the fiction domain. Models which incor-
porate syntactic and discourse cue features do not
lead to significant improvements over models that
do not. This means that paragraph boundaries can
be predicted by relying on low-level, language in-
dependent features. The task is therefore feasible
even for languages for which parsers or cue word
lists are not readily available.
We also experimented with training sets of differ-
ent sizes and found that more training data does not
necessarily lead to significantly better results and
that it is possible to beat the best baseline comfort-
ably even with a relatively small training set.
Finally, we examined how well humans do on
this task. Our results indicate that humans achieve
an average accuracy of about 77.45% to 88.58%,
where some domains seem to be easier than others.
Our models achieved accuracies of within 6% of hu-
man performance.
In the future, we plan to apply our model to new
domains (e.g., broadcast news or scientific papers),
to non-Indo-European languages such as Arabic and
Chinese, and to machine generated texts.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation.
Machine Learning, 34(1/3):177?210.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of the 39th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 116?123, Toulouse.
Heidi Christensen, Bala Kolluru, Yoshi Gotoh, and
Steve Renals. 2004. From text summarisation to
style-specific summarisation for broadcast news.
In Proceedings of the European Conference on
Information Retrieval, Sunderland.
Philip Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling. In Proceedings
of ESCA EuroSpeech?97, pages 2707?2710,
Rhodes.
Dmitriy Genzel and Eugene Charniak. 2003. Vari-
ation of entropy and parse trees of sentences as
a function of the sentence number. In Proceed-
ings of Empirical Methods in Natural Language
Processing, pages 65?72, Sapporo.
Alexander G. Hauptmann and Michael A. Smith.
1995. Text, speech and vision for video segmen-
tation: The informedia project. In Proceedings of
the AAAI Fall Symposium, Computational Mod-
els for Integrating Language and Vision, Cam-
bridge, MA.
Marti A. Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Compu-
tational Linguistics, 23(1):33?64.
Alistair Knott. 1996. A Data-Driven Methodol-
ogy for Motivating a Set of Coherence Rela-
tions. Ph.D. thesis, Department of Artificial In-
telligence, University of Edinburgh.
Philipp Koehn. 2002. Europarl: A multilin-
gual corpus for evaluation of machine trans-
lation. Unpublished Draft, http://www.isi.
edu/?koehn/publications/europarl.ps.
R. E. Longacre. 1979. The paragraph as a gram-
matical unit. Syntax and Semantics, 12:115?134.
Fuchun Peng, Dale Schuurmans, Vlado Keselj, and
Shaojun Wang. 2003. Language independent
authorship attribution using character level lan-
guage models. In Proceedings of the 11th Con-
ference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 267?
274, Budapest.
Lev Pevzner and Marti Hearst. 2002. A critique
and improvement of an evaluation metric for
text segmentation. Computational Linguistics,
28(1o):19?36.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997.
A maximum entropy approach to identifying sen-
tence boundaries. In Proceedings of the Fifth
Conference on Applied Natural Language Pro-
cessing, Washington, DC.
Robert E. Schapire and Yoram Singer. 2000. Boos-
texter: A boosting-based system for text catego-
rization. Machine Learning, 39(2/3):135?168.
Sidney Siegel and N. John Castellan. 1988. Non
Parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New York.
Heather A. Stark. 1988. What do paragraph mark-
ings do? Discourse Processes, 11:275?303.
Mark Stevenson and Robert Gaizauskas. 2000. Ex-
periments on sentence boundary detection. In
Proceedings of the 6th Applied Natural Langauge
Processing Conference, pages 84?89, Seattle,
WA.
Masao Utiyama and Hitoshi Isahara. 2001. A sta-
tistical model for domain-independent text seg-
mentation. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 491?498, Toulouse.
Chunshen Zhu. 1999. Ut once more: The sentence
as the key functional unit of translation. Meta,
44(3):429?447.
Coling 2010: Poster Volume, pages 250?258,
Beijing, August 2010
Topic models for meaning similarity in context
Georgiana Dinu
Dept. of Computational Linguistics
Saarland University
dinu@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
Recent work on distributional methods for
similarity focuses on using the context
in which a target word occurs to derive
context-sensitive similarity computations.
In this paper we present a method for com-
puting similarity which builds vector rep-
resentations for words in context by mod-
eling senses as latent variables in a large
corpus. We apply this to the Lexical Sub-
stitution Task and we show that our model
significantly outperforms typical distribu-
tional methods.
1 Introduction
Distributional methods for word similarity ((Lan-
dauer and Dumais, 1997), (Schuetze, 1998)) are
based on co-occurrence statistics extracted from
large amounts of text. Typically, each word is
assigned a representation as a point in a high-
dimensional space, where the dimensions rep-
resent contextual features such as co-occurring
words. Following this, meaning relatedness
scores are computed by using various similarity
measures on the vector representations.
One of the major issues that all distributional
methods have to face is sense ambiguity. Since
vector representations reflect mixtures of uses ad-
ditional methods have to be employed in order to
capture specific meanings of a word in context.
Consider the occurrence of verb shed in the fol-
lowing SemEval 2007 Lexical Substitution Task
(McCarthy and Navigli, 2007) example:
Cats in the latent phase only have the virus internally ,
but feel normal and do not shed the virus to other cats and
the environment .
Human participants in this task provided words
such as transmit and spread as good substitutes
for shed in this context, however a vector space
representation of shed will not capture this infre-
quent sense.
For these reasons, recent work on distributional
methods for similarity such as (Mitchell and La-
pata, 2008) (Erk and Pado?, 2008) (Thater et al,
2009) focuses on using the context in which a tar-
get word occurs to derive context-sensitive simi-
larity computations.
In this paper we present a method for comput-
ing similarity which builds vector representations
for words in context. Most distributional methods
so far extract representations from large texts, and
only as a follow-on step they either 1) alter these
in order to reflect a disambiguated word (such
as (Erk and Pado?, 2008)) or 2) directly asses the
appropriateness of a similarity judgment, given a
specific context (such as (Pantel et al, 2007)). Our
approach differs from this as we assume ambigu-
ity of words at the, initial, acquisition step, by en-
coding senses of words as a hidden variable in the
text we process.
In this paper we focus on a particular distribu-
tional representation inspired by (Lin and Pantel,
2001a) and induce context-sensitive similarity be-
tween phrases represented as paths in dependency
graphs. It is inspired by recent work on topic mod-
els and it deals with sense-ambiguity in a natural
manner by modeling senses as latent variables in
a large corpus. We apply this to the Lexical Sub-
stitution Task and we show that our model outper-
forms the (Lin and Pantel, 2001a) method by in-
ducing context-appropriate similarity judgments.
250
2 Related work
Discovery of Inference Rules from Text (DIRT)
A popular distributional method for meaning re-
latedness is the DIRT algorithm for extracting in-
ference rules (Lin and Pantel, 2001a). In this al-
gorithm a pattern is a noun-ending path in a de-
pendency graph and the goal is to acquire pairs of
patterns for which entailment holds (in at least one
direction) such as (X solve Y, X find solution to Y).
The method can be seen a particular instance
of a vector space. Each pattern is represented by
the sets of its left hand side (X) and right hand
side (Y) noun fillers in a large corpus. Two pat-
terns are compared in the X-filler space, and cor-
respondingly in the Y-filler space by using the Lin
similarity measure:
simLin(v, w) =
?
i?I(v)?I(w)(vi + wi)?
i?I(v) vi +
?
l?I(w)wi
where values in v and w are point-wise mutual
information, and I(?) gives the indices of positive
values in a vector.
The final similarity score between two patterns
is obtained by multiplying the X and Y similarity
scores. Table 1 shows a fragment of a DIRT-like
vector space.
.. case problem ..
(X solve Y, Y) .. 6.1 4.4 ..
(X settle Y, Y) .. 5.2 5.9 ..
Table 1: DIRT-like vector representation in the Y-filler
space. The values represent mutual information.
Further on, this similarity method is used for
the task of paraphrasing. A total set of patterns
is extracted from a large corpus and each of them
can be paraphrased by returning its most similar
patterns, according to the similarity score. Al-
though relatively accurate1, it has been noted (Lin
and Pantel, 2001b) that the paraphrases extracted
this way reflect, as expected, various meanings,
and that a context-sensitive representation would
be appropriate.
1Precision is estimated to lie around 50% for the most
confident paraphrases
Context-sensitive extensions of DIRT (Pantel
et al, 2007) and (Basili et al, 2007) focus on mak-
ing DIRT rules context-sensitive by attaching ap-
propriate semantic classes to the X and Y slots of
an inference rule. For this purpose, the initial step
in their methods is to acquire an inference rule
database, using the DIRT algorithm. Following
this, given an inference rule, they identify seman-
tic classes for the X and Y fillers which make the
application of the rule appropriate. For this (Pan-
tel et al, 2007) build a set of semantic classes us-
ing WordNet in one case and CBC clustering al-
gorithm in the other; for each rule, they use the
overlap of the fillers found in the input corpus as
an indicator of the correct semantic classes. The
same idea is used in (Basili et al, 2007) where,
this time, the X and Y fillers are clustered for each
rule individually; these nouns are clustered us-
ing an LSA-vector representation extracted from
a large corpus.
(Connor and Roth, 2007) take a slightly differ-
ent approach as they attempt to classify the con-
text of a rule as appropriate or not, again using
the overlap of fillers as an indicator. They all
show improvement over DIRT by evaluating on
occurrences of rules in context which are anno-
tated as correct/incorrect by human participants.
On a common data set (Pantel et al, 2007) and
(Basili et al, 2007) achieve significant improve-
ments over DIRT at 95% confidence level when
employing the clustering methods. (Szpektor et
al., 2008) propose a general framework for these
methods and show that some of these settings ob-
tain significant (level 0.01) improvements over the
DIRT algorithm on data derived from the ACE
2005 event detection task.
Related work on topic models Topic models
have been previously used for semantic tasks.
Work such as (Cai et al, 2007) or (Boyd-Graber et
al., 2007) use the document-level topics extracted
with Latent Dirichlet Allocation (LDA) as indi-
cators of meanings for word sense disambigua-
tion. More related to our work are (Brody and
Lapata, 2009) or (Toutanova and Johnson, 2008)
who use LDA-based models which induce latent
variables from task-specific data rather than from
simple documents.
251
(Brody and Lapata, 2009) apply such a model
for word sense induction on a set of 35 target
nouns. They assume senses as latent variables and
context features as observations; unlike our model
they induce local senses specific to every target
word by estimating separate models with the final
goal of explicitly inducing word senses.
(Toutanova and Johnson, 2008) use an LDA-
based model for semi-supervised part-of-speech
tagging. They build a word context model in
which each token involves: generating a distri-
bution over tags, sampling a tag, and finally gen-
erating context words according to a tag-specific
word distribution (context words are observa-
tions). Their model achieves highest performance
when combined with a ambiguity class compo-
nent which uses a dictionary for possible tags of
target words.
Both these papers show improvements over
state-of-the-art systems for their tasks.
3 Generative model for similarity in
context
We develop a method for computing similarity of
patterns in context, i.e. patterns with instantiated
X and Y values. We do not enhance the repre-
sentation of an inference rule with sense (context-
appropriateness) information but rather focus on
the task of assigning similarity scores to such pairs
of instantiated patterns. Unlike previous work, we
do not employ any other additional resources, in-
vestigating this way whether structurally richer in-
formation can be learned from the same input co-
occurrence matrix as the original DIRT method.
Our model, as well as the DIRT algorithm,
uses context information extracted from large
corpora to learn similarities between patterns;
however ideally we would like to learn contex-
tual preferences (or, in general, some form of
sense-disambiguation) for these patterns. This is
achieved in our model by assuming an intermedi-
ate layer consisting of meanings (senses): the con-
text surrounding a pattern is indicative of mean-
ings, and preference for some meanings gives the
characterization of a pattern.
For this we use a generative model inspired
by Latent Dirichlet Allocation (Blei et al, 2003)
(Griffiths and Steyvers, 2004) which is success-
X solve Y
we-X:122, country-X:89, government-X:82,
it-X:69,..., problem-Y:1088, issue-Y:134,
crisis-Y:99, dispute-Y:78,...
Table 2: Fragments of the document associated
to X solve Y. we-X: 122 indicates that X solve Y
occurs 122 times with we as an X filler.
fully employed for modeling collections of doc-
uments and the underlying topics which occur in
them. The statistical model is characterized by the
following distributions:
wi|zi, ?zi Discrete(?zi)
?z Dirichlet(?)
zi|?p Discrete(?p)
?p Dirichlet(?)
?p is the distribution over meanings associated
to a pattern p and ?z is the distribution over words
associated to a meaning z. The occurrence of
each filler word wi with a pattern p, is then gener-
ated by sampling 1) a meaning conditioned on the
meaning distribution associated to p: zi|?p and 2)
a word conditioned on the word distribution asso-
ciated to the meaning zi: wi|zi, ?zi . ?p and ?z are
assumed to be Dirichlet distributions with param-
eters ? and ?.
The set of context words (X and Y fillers) oc-
curring with a pattern p form the document (in
LDA terms) associated to a pattern p. Table 2 lists
a fragment of the document associated to pattern
X solve Y. These are built simply by listing for
each pattern, occurrence counts with specific filler
words. Since we want our model to differentiate
between X and Y fillers, words occurring as fillers
are made disjoint by adding a corresponding suf-
fix.
The total set of such documents extracted from
a large corpus is then used for estimating the
model. We use Gibbs sampling2 and the result
is a set of samples from P (z|w) (i.e. mean-
ing assignments for each occurring filler word)
from which ?p (pattern-meaning distributions)
and ?z(meaning-word distributions) can be esti-
mated.
Our model has the advantage that, once these
2http://gibbslda.sourceforge.net/
252
distributions are estimated, given a pattern p and a
context wn, in-context vector representations can
be built in a straightforward manner.
Meaning representation in-context Let K be
the assumed number of meanings, (z1, ..., zK).
We associate to a pattern in context (p,wn), the
K-dimensional vector containing for each mean-
ing zi (i : 1..K), the probability of zi, conditioned
on pattern p and context word wn:
vec(p, wn) = (P (z1|wn, p), ..., P (zK |wn, p))
(1)
where,
P (zi|wn, p) =
P (zi, p)P (wn|zi)
?Ki=1P (zi, p)P (wn|zi)
(2)
This is the probability that wn is generated by
meaning zi conditioned on p, therefore, the proba-
bility that pattern p has meaning zi in context wn,
exactly the concept we want to model.
Meaning representation out-of-context We
can also associate to pattern p an out-of-context
vector representation: the K-dimensional vector
representing its distribution over meanings:
vec(p) = (P (z1|p), ..., P (zK |p)) (3)
This can be seen as a dimensionality reduction
method, since we bring vector representations to a
lower dimensional space over (ideally) meaning-
ful concepts.
From the generative model we obtain the de-
sired distributions P (zi|p) = ?pi and P (wn|zi) =
?zin .3
Computing similarity between patterns The
similarity between patterns occurring with X and
Y filler-words is computed following (Lin and
Pantel, 2001a) by multiplying the similarities ob-
tained separately in the X and Y spaces.:
sim((wX1, p1, wY 1)(wX2, p2, wY 2)) =
sim(vec(p1, wX1), vec(p2, wX2))?
sim(vec(p1, wY 1), vec(p2, wY 2))
(4)
3For similarity in context, we use the conditional P (zi|p)
instead of the joint P (zi, p) which is computationally equiv-
alent for the paraphrasing setting.
we subj???? make dobj????statement
we subj???? give dobj????statement good
we subj???? prepare dobj????statement bad
Table 3: Development set: good/bad substitutes
for we subj???? make dobj????statement
Out-of-context similarity is defined in a straight-
forward manner:
sim(p1, p2) = sim(vec(p1, ), vec(p2)) (5)
4 Evaluation setup
In this paper we evaluate our model on
computing similarities between pairs of the
type (X, pattern, Y ), (X, pattern?, Y ) where
two different patterns are compared in identical
contexts. For this we use the Semeval Lexical
Substitution dataset, which requires human par-
ticipants to provide substitutes for a set of target
words occurring in different contexts. This sec-
tion describes the evaluation methodology for this
data as well as the automatically generated data
set we use for development.
Development set For finding good model pa-
rameters, we use the SemCor corpus providing
text in which all content words are tagged with
WordNet 1.6 senses. We used this data in the fol-
lowing manner: We parse the text using Stanford
parser and extract occurrences of triples (X, pat-
tern, Y). Given these triples we generate good and
bad substitutes for them: the good substitutes are
generated by replacing the words occurring in the
patterns with sense-appropriate synonyms, while
bad ones are obtained by substitution with syn-
onyms corresponding to the rest of the senses (the
wrong senses). The synonyms are extracted from
WordNet 1.6 synsets using the sense annotation
present in the text.
For evaluation we feed the models pairs of in-
stantiated patterns. One of them is the original
phrase encountered in the data, and the other one
is a good/bad substitute for it. Table 3 shows an
example of the data.
We evaluate the output of a system by requir-
ing that, for each instance, every good substitute
is scored more similar to the original phrase than
253
every bad substitute. This leads to an accuracy
score which can be compared against a random
baseline of 50%.
The data set obtained is far from being a very
reliable resource for the task of lexical substitu-
tion, however this method of generating data has
the advantage of producing a large number of in-
stances which can be easily acquired from any
sense-annotated data set. In our experiments we
use the Brown2 fragment from which we extract
over 3000 instances of patterns in context.
Lexical substitution task The Lexical Substitu-
tion Task (McCarthy and Navigli, 2007) presents
5 annotators with a set of target words, each in
different context sentences. The task requires
the participants to provide appropriate substitute
words for each occurrence of the target words.
We use this data similarly to (Erk and Pado?,
2008) and (Thater et al, 2009) and for each target
word, we pool together all the substitutes given
for all context sentences. Similarly to the Sem-
Cor data, we do not use the entire sentence as a
context as we extract only patterns containing tar-
get words together with their X and Y fillers. The
models assign similarity scores to each candidate
by comparing them to the pattern occurring in the
original sentence. A ranked list of candidates is
obtained which in turn is compared with the sub-
stitutes provided by the participants. Table 4 gives
an example of this data set (for each substitute we
list the number of participants providing it).
To evaluate the performance of a model we em-
ploy two similarity measures, which capture dif-
ferent aspects of the task. Kendall ? rank coeffi-
cient measures the correlation between two ranks;
since the gold ranking is usually only a partial or-
der, we use ?b which makes adjustments for ties.
We employ a second evaluation measure: Gener-
alized Average Precision (Kishida, 2005). This is
a measure inspired from information retrieval and
has been previously used for evaluating this task
(Thater et al, 2009). It evaluates a system on its
ability to retrieve correct substitutes using the gold
ranking together with the associated confidence
scores. The confidence scores are in turn deter-
mined by the number of people providing each
substitute.
pattern human substitutes
study subj???? shed dobj????light throw 3, reveal 2,
shine 1
cat subj???? shed dobj????virus spread 2, pass 2,
transmit 2, emit 1
Table 4: Lexical substitution data set: target verb
shed
5 Experiments
5.1 Model selection
The data we use to estimate our models is ex-
tracted from a GigaWord fragment containing ap-
proximately 100 million tokens. We parse the
text with Stanford dependency parser to obtain de-
pendency graphs from which we extract paths to-
gether with counts of their left and right fillers.
We extract paths containing at most four words,
including the two noun anchors. Furthermore
we impose a frequency threshold on patterns and
words, leading us to a collection of?80 000 paths,
with filler nouns over a vocabulary of ?40 000
words.
We estimate a total number of 20 models. We
set ? = 0.01 as previous work (Wang et al, 2009)
reports good results with this value. For parame-
ter ? we test 4 settings: ?1 = 2K and ?4 = 50Kwhich are reported in the literature as good ((Por-
teous et al, 2008) and (Griffiths and Steyvers,
2004)), as well as 2 intermediate values: ?2 = 5Kand ?3 = 10K . We test a set of 5 K values:
{800, 1000, 1200, 1400, 1600}. These are chosen
to be large since they represent the global set of
meanings shared by all the patterns in the collec-
tion.
As vector similarity measure we test scalar
product (sp), which in our model is interpreted
as the probability that two patterns share a com-
mon meaning. Additionally we test cosine (cos)
similarity and inverse Jensen-Shannon (JS) diver-
gence, which is a popular measure for comparing
probability distributions:
JSD(v, w) = 12KLD(v|m) +
1
2KLD(w|m)
with m = 12(v + w) and KLD the stan-dard Kullback-Leibler divergence: KLD(v|w) =
?ivilog( viwi ).
254
We perform both in-context (using eq. (4))
as well as out-of-context computations (eq. (5)).
Similarly to previous work (Erk and Pado?, 2008),
we observe that comparing a contextualized repre-
sentation against a non-contextualized one brings
significant improvements over comparing two
representations in context. We assume this is spe-
cific to the type of data we work with, in which
two patterns are compared in an identical context,
rather than across different contexts; we therefore
compute context-sensitive similarities by contex-
tualizing just the target word.
Number of topics Although the parameters
cover relatively large ranges the models perform
surprisingly similar across different ? and K val-
ues, as well as across all three similarity measures.
For sp similarity, the accuracy scores we obtain
are in the range [56.5-59.5] with a average devi-
ation from the mean of just 0.8%; similar figures
are obtained using the other similarity measures.
Figure 1 plots the average of the accuracy scores
using sp as similarity measure, across different
number of topics. A small preference for higher
K values is observed, all models performing con-
sistently good at 1200, 1400 and 1600 topics.
Figure 1: Average accuracy across the 5 K values.
Mixture models This leads us to attempting a
very simple mixture model, which computes the
similarity score between two patterns as the aver-
age similarity obtained across a number of mod-
els. For each ? setting, we mix models across the
three best topic numbers: {1200, 1400, 1600}. In
Figure 2 we plot this mixture model together with
the three single ones, at each ? value. It can be
Figure 2: Mixture model {1200, 1400, 1600}
(bold) vs. the three individual models, across the
4 ? values.
noticed that the mixture model improves over all
three single models for three out of the four ? val-
ues.
In-context vs. out-of-context computations
Further on we compare in-context versus out-of-
context computations. The similarity measures
exhibit significant differences in regard to this as-
pect. In Figure 3 we plot in-context vs. out-of-
context computations using scalar product (left)
and JS (right) with the mixture model previously
defined, plotted at different ? values. For sp
in-context computations significantly outperform
out-of-context ones and the two intermediate al-
pha values seem to be the best. However for JS
similarity the out-of-context computations are sig-
nificantly better and a clear preference for smaller
? values can be observed.
Finally, on the test data, we use the following
models (where GMmixt/sing,sim stands for a mix-
ture or single model with similarity measure sim):
? GMmixt,sp/cos
mixt({1200, 1400, 1600}x{?2, ?3})
? GMmixt,js
mixt({1200, 1400, 1600}x{?1, ?2})
? GMsing,sp: (1600, ?2)
? GMsing,cos/js: (1200, ?1)
The mixture models are build based on the ob-
servations previously made while the single mod-
255
Model In-context Out-of-context
GMmixt,sp 59.89 58.68
GMmixt,cos 59.50 58.67
GMmixt,js 59.73 60.68
GMsing,sp 59.48 58.86
GMsing,cos 59.43 57.87
GMsing,js 58.65 59.36
Table 5: Accuracy results on development set
els are the best performing ones, for each similar-
ity measure. The accuracy scores obtained with
these models are given in Table 5. Mixture models
generally outperform single ones and in-context
computations outperform out-of-context ones for
sp and cos. The best results on the development
set are however achieved by out-of-context mod-
els using JS as similarity measure.
Figure 3: In-context (bold) vs. out-of-context
computations across the 4 ? values using scalar
product (left) and JS (right)
5.2 Results
Table 6 shows the results for the Lexical Substitu-
tion data set. We use the subset of the data con-
taining sentences in which the target word is part
of a syntactic path which is present in the total col-
lection of patterns. This leads to a set containing
165 instances of patterns in context, most of these
containing target verbs.
Since sp and cos measures perform very sim-
ilarly we only list results with cosine similarity
measure. In addition to the models with settings
determined on the development set, we also test
a very simple mixture model: GMmixt?all,sim.
This simply averages over all 20 configurations
and its purpose is to investigate the necessity of a
carefully selected mixture model.
It can be noticed that all GM mixture mod-
els outperform DIRT, which is reflected in both
Model ?b GAP
Random 0.0 34.91
DIRT 14.53 48.06
GMmixt,cos 22.35 52.04
GMmixt,js 18.17 50.80
GMmixt?all,cos 20.42 51.13
GMmixt?all,js 19.03 51.15
GMsing,cos 15.10 48.20
GMsing,js 14.17 47.97
Table 6: Results on Lexical Substitution data
similarity measures. Notably the very simple
model which averages all the configurations im-
plemented is surprisingly performant. Using ran-
domized significance testing we obtained that
GMmixt,cos is significantly better than DIRT at p
level 1e-03 on both GAP and ?b. GMmixt?all,cos
outperforms DIRT at level 0.05.
In terms of similarity measures, the observa-
tions made on the development set hold, as for
the in-context computations cos and sp outper-
form JS. However, unlike on the development
data, the single models perform much worse than
the mixture ones which can indicate that the de-
velopment set is not perfectly suited for choosing
model parameters.
Out-of-context computations for all models and
all similarity measures are significantly outper-
formed, leading to scores in ranges [11-14] ?b and
[45-48] GAP.
In Table 7 we list the rankings produced by
three models for the target word shed in con-
text virus obj??? shed prep???? to pobj???? cat. As it
can be observed, the model performing context-
sensitive computations GMmixt,cos-in-context re-
turns a better ranking in comparison to theDIRT
and GMmixt,cos-out-of-context models.
6 Conclusion
We have addressed the task of computing meaning
similarity in context using distributional methods.
The specific representation we use follows (Lin
and Pantel, 2001a): we extract patterns (paths
in dependency trees which connect two nouns)
and we use the co-occurrence with these nouns
to build high-dimensional vectors. Using this data
256
virus obj??? shed prep???? to pobj???? cat
GMmixt,cos GMmixt,cos DIRT GOLD
in-context out-of-context
lose lose drop pass 2
drop drop lose spread 2
transmit relinquish give transmit 2
spread reveal transmit
pass pass spread
relinquish throw reveal
reveal spread relinquish
throw transmit throw
give give pass
Table 7: Ranks returned for virus obj??? shed prep???? to pobj???? cat
we develop a principled method to induce context-
sensitive representations by modeling the mean-
ing of a pattern as a latent variable in the input
corpus. We apply this model to the task of Lex-
ical Substitution and we show it allows the com-
putation of context-sensitive similarities; it signif-
icantly outperforms the original method, while us-
ing the exact same input data.
In future work, we plan to use our model for
generating paraphrases for patterns occurring in
context, a scenario closer to real applications than
out-of-context paraphrasing.
Finally, a formulation of our model in a typical
bag-of-words semantic space for word similarity
can be employed in a wider range of applications
and will allow comparison with other methods for
building context-sensitive vector representations.
7 Acknowledgments
This work was partially supported by DFG (IRTG
715).
References
Basili, Roberto, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
In Proceedings of RANLP 2007, Borovets, Bulgaria.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Boyd-Graber, Jordan, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Empirical Methods in Natural Language
Processing.
Brody, Samuel and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL ?09: Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 103?111, Morristown, NJ, USA. Association
for Computational Linguistics.
Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml:improving word sense disambiguation us-
ing topic features. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249?252, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Connor, Michael and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised classi-
fier. In ECML ?07: Proceedings of the 18th Euro-
pean conference on Machine Learning, pages 104?
115, Berlin, Heidelberg. Springer-Verlag.
Erk, Katrin and Sabastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP 2008, Waikiki, Honolulu,
Hawaii.
Griffiths, T. L. and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101(Suppl. 1):5228?5235, April.
Kishida, Kazuaki. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent seman-
tic analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Lin, Dekang and Patrick Pantel. 2001a. DIRT ? Dis-
covery of Inference Rules from Text. In Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD-01), San Francisco, CA.
257
Lin, Dekang and Patrick Pantel. 2001b. Discovery of
inference rules for question-answering. Nat. Lang.
Eng., 7(4):343?360.
McCarthy, D. and R. Navigli. 2007. SemEval-2007
Task 10: English Lexical Substitution Task. In Pro-
ceedings of SemEval, pages 48?53, Prague.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Pantel, Patrick, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Rochester, New
York.
Porteous, Ian, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling.
2008. Fast collapsed gibbs sampling for latent
dirichlet alocation. In KDD ?08: Proceeding of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 569?
577, New York, NY, USA. ACM.
Schuetze, Hinrich. 1998. Automatic word sense dis-
crimination. Journal of Computational Linguistics,
24:97?123.
Szpektor, Idan, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT, pages 683?691, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Thater, Stefan, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of TextInfer ACL 2009.
Toutanova, Kristina and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In Platt, J.C., D. Koller,
Y. Singer, and S. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. MIT Press, Cambridge, MA.
Wang, Yi, Hongjie Bai, Matt Stanton, Wen-Yen Chen,
and Edward Y. Chang. 2009. Plda: Parallel latent
dirichlet alocation for large-scale applications. In
Proc. of 5th International Conference on Algorith-
mic Aspects in Information and Management.
258
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513?523,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Title Generation with Quasi-Synchronous Grammar
Kristian Woodsend, Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
Edinburgh EH8 9AB, United Kingdom
k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The task of selecting information and render-
ing it appropriately appears in multiple con-
texts in summarization. In this paper we
present a model that simultaneously optimizes
selection and rendering preferences. The
model operates over a phrase-based represen-
tation of the source document which we ob-
tain by merging PCFG parse trees and depen-
dency graphs. Selection preferences for in-
dividual phrases are learned discriminatively,
while a quasi-synchronous grammar (Smith
and Eisner, 2006) captures rendering prefer-
ences such as paraphrases and compressions.
Based on an integer linear programming for-
mulation, the model learns to generate sum-
maries that satisfy both types of preferences,
while ensuring that length, topic coverage and
grammar constraints are met. Experiments on
headline and image caption generation show
that our method obtains state-of-the-art per-
formance using essentially the same model for
both tasks without any major modifications.
1 Introduction
Summarization is the process of condensing a source
text into a shorter version while preserving its infor-
mation content. Humans summarize on a daily ba-
sis and effortlessly, yet the automatic production of
high-quality summaries remains a challenge.
Most work today focuses on extractive summa-
rization, where a summary is created by identifying
and subsequently concatenating the most important
sentences in a document. The advantage of this ap-
proach is that it does not require a great deal of lin-
guistic analysis to generate grammatical sentences,
assuming the source document was well written.
Unfortunately, extracts generated this way are often
documents of low readability and text quality, and
contain much redundant information. The concise-
ness can be improved when sentence extraction is
interfaced with sentence compression, where words
and clauses are deleted based on rules typically op-
erating over parsed input (Jing, 2000; Daume? III
and Marcu, 2002; Lin, 2003; Daume? III, 2006; Zajic
et al, 2007; Martins and Smith, 2009).
An alternative abstractive or ?bottom-up? ap-
proach involves identifying high-interest words and
phrases in the source text, and combining them into
new sentences guided by a language model (Banko
et al, 2000; Soricut and Marcu, 2007). This ap-
proach has the potential to work well, breaking out
of the single-sentence paradigm. Unfortunately, the
resulting summaries are not always coherent ? indi-
vidual constituent phrases are often combined with-
out any semantic constraints ? or grammatical be-
yond the n-gram horizon imposed by the language
model.
Constituent deletion and recombination are
merely two of the many rewrite operations profes-
sional editors and abstractors employ when creating
summaries (Jing, 2002). Additional operations in-
clude truncating sentences, aggregating them, and
paraphrasing at word or syntax level. Furthermore,
professionals write summaries in a task-specific
style. News headlines for example are typically
short (three to six words), written in the present
tense and active voice, and often leave out forms of
the verb be. There are also different ways of writing
a headline either directly by stating what the docu-
513
ment is about or indirectly by raising a question in
the reader?s mind, which the document answers.
The automatic generation of summaries similar to
those produced by human abstractors is challenging
because of the many constraints imposed by the task:
the summary must be maximally informative and
minimally redundant, grammatical, coherent, adhere
to a pre-specified length and stylistic conventions.
Importantly, these constraints are conflicting; the
deletion of certain phrases may avoid redundancy
but result in ungrammatical output and information
loss.
In this paper we propose a model for summariza-
tion that attempts to capture and optimize these con-
straints jointly. We learn both how to select the
most important information (the content), and how
to render it appropriately (the style). Selection pref-
erences are learned discriminatively, while a quasi-
synchronous grammar (QG, Smith and Eisner 2006)
captures rendering preferences such as paraphrases
and compressions. The entire solution space of
possible extractions and QG-generated paraphrases
is searched efficiently through use of integer lin-
ear programming. The ILP framework allows us to
model naturally as constraints, additional require-
ments such as sentence length, overall summary
length, topic coverage and, importantly, grammati-
cality.
We argue that QG is attractive for describ-
ing rewrite operations common in summarization.
Rather than assuming a strictly synchronous struc-
ture over the source and target sentences, QG iden-
tifies a ?sloppy? alignment of parse trees assuming
that the target tree is in some way ?inspired by? the
source tree. A key insight in our approach is to
formulate the summarization problem at the phrase
level: both QG rules and information extraction op-
erate over individual phrases rather than (as is the
norm) sentences. At this smaller unit level, QG
rules become more widely applicable and compres-
sion falls naturally because only phrases deemed im-
portant should appear in the summary.
We evaluate the proposed model on headline gen-
eration and the related task of image caption gen-
eration. However, there is nothing inherent in our
formulation that is specific to those two tasks; it is
possible for the model to generate longer or shorter
summaries, for a single or multiple documents. Ex-
perimental results show that our method obtains
state-of-the-art performance, both in terms of gram-
maticality and informativeness for both tasks using
the same summarization model.
2 Related work
Much effort in automatic summarization has been
devoted to sentence extraction which is often for-
malized as a classification task (Kupiec et al, 1995).
Given appropriately annotated training data, a bi-
nary classifier learns to predict for each document
sentence if it is worth extracting. A few previ-
ous approaches have attempted to interface sentence
compression with summarization. A straightforward
way to achieve this is by adopting a two-stage ar-
chitecture (e.g., Lin 2003) where the sentences are
first extracted and then compressed or the other way
round.
Other work implements a joint model where
words are deleted and sentences selected from a doc-
ument simultaneously (Daume? III and Marcu, 2002;
Martins and Smith, 2009; Woodsend and Lapata,
2010). ILP models have also been developed for
sentence rather than document compression (Clarke
and Lapata, 2008). Dras (1999) discusses the appli-
cation of ILP to reluctant paraphrasing, i.e., the task
of choosing between paraphrases while conforming
to length, readability, or style constraints. Again,
the aim is to rewrite text without, however, con-
tent selection. Rewrite operations other than dele-
tion tend to be hand-crafted and domain specific
(Jing and McKeown, 2000). Notable exceptions are
Cohn and Lapata (2008) and Zhao et al (2009) who
present a model that can both compress and para-
phrase individual sentences without however gener-
ating document-level summaries.
Headline generation is a well-studied task within
single-document summarization, due to its promi-
nence in the DUC-03 and DUC-04 evaluation com-
petitions.1 Many approaches identify the most infor-
mative sentence in a given document (typically the
first sentence for the news genre) and subsequently
apply a form of sentence compression such that
the headline meets some length requirement (Dorr
1Approaches to headline generation are too numerous to list
in detail; see the proceedings of DUC-03 and DUC-04 for an
overview.
514
et al, 2003). The compressed sentence may also be
?padded? with important content words or phrases
to ensure that the topic of the document is covered
(Zajic et al, 2004). Other work generates headlines
in a bottom-up fashion starting from important, indi-
vidual words and phrases, that are glued together to
create a fluent sentence. For example, Banko et al
(2000) draw inspiration from Machine Translation
and generate headlines using statistical models for
content selection and sentence realization.
Relatively little work has focused on caption gen-
eration, a task related to headline generation. The
aim here is to create a short, title-like description of
an image embedded in a news article. Like head-
lines, captions have to be short and informative. In
addition, a good caption must clearly identify the
subject of the picture and establish its relevance to
the article. Feng and Lapata (2010a) develop ex-
tractive and abstractive caption generation models
that operate over the output of a probabilistic im-
age annotation model that preprocesses the pictures
and suggests keywords to describe their content.
Their best model is an extension of Banko et al?s
(2000) word-based model for headline generation to
phrases.
Our own work develops an ILP-based summariza-
tion model with rewrite operations that are not lim-
ited to deletion, are defined over phrases, and en-
coded in quasi-synchronous grammar. The QG for-
malism has been previously applied to parser adap-
tation and projection (Smith and Eisner, 2009), para-
phrase identification (Das and Smith, 2009), and
question answering (Wang et al, 2007); however
the use of QG in summarization is novel to our
knowledge. Unlike most synchronous grammar for-
malisms, QG does not posit a strict isomorphism be-
tween a source sentence and its target translation; it
only loosely links the syntactic structure of the two,
and is therefore well suited to describing the rela-
tionship between a document and its abstract. We
propose an ILP formulation which not only allows
to efficiently search through the space of many QG
rules but also to incorporate constraints relating to
content, style, and the task at hand.
3 Modeling
There are three components to our model. Content
selection is performed discriminatively; an SVM
learns which information in the source document
should be in the summary, and gives a real-valued
salience score for each phrase. QG rules are used
to generate compressions and paraphrases of the
source sentences. An ILP model combines the out-
put of these two components into an output sum-
mary, while optimizing content selection and surface
realization preferences jointly.
3.1 Document Representation
Our model operates on documents annotated with
syntactic information which we obtain by parsing
every sentence twice, once with a phrase structure
parser and once with a dependency parser. The out-
put from the two representations is combined into a
single data structure, by mapping the dependencies
to the edges of the phrase structure tree. The proce-
dure is described in detail in Woodsend and Lapata
(2010). However, we do not merge the leaf nodes
into phrases here, but keep the full tree structure,
as we will apply compression to phrases through
the QG. In our experiments, we obtain this com-
bined representation from the output of the Stan-
ford parser (Klein and Manning, 2003) but any other
broadly similar parser could be used instead.
3.2 Quasi-synchronous grammar
Given an input sentence S1 or its parse tree T1, the
QG constructs a monolingual grammar for parsing,
or generating, the possible translation (or here, para-
phrase) trees T2. A grammar node in the target tree
T2 is modeled on a subset of nodes in the source tree,
with a rather loose alignment between the trees.
In our approach, the process of learning the gram-
mar is unsupervised. Each sentence of the source
document is compared to each sentence in the target
document ? headline or caption, depending on the
task. Using the combined PCFG-dependency tree
representation described above, we build up a list of
leaf node alignments based on lexical identity, after
stemming and removing stop words. We align direct
parent nodes where more than one child node aligns.
A grammar rule is created if the all the nodes in the
target tree can be explained using nodes from the
515
(a) NP
NNP/nn
Saudi
JJ/amod
dissident
NNP/nn
Osama
NNP/nn
bin
NNP/?
Laden
NP
NNP/nn
bin
NNP/?
Laden
(b) PP/prep in
IN/?
in
DT/det
the
JJ/amod
disputed
NN/?
territory
PP/prep of
IN/?
of
NNP/nn
East
NNP/?
Timor
PP/prep in
IN/?
in
NNP/nn
East
NNP/?
Timor
(c) NP/dobj
DT/det
the
NN/?
extradition
PP/prep of
IN/?
of
NNP/nn
Kurdish
NN/nn
leader
NNP/?
Ocalan
NP/dobj
NP/poss
Ocalan?s
NN/?
extradition
Figure 1: Examples of QG alignments between
source node (left) and target node (right). (a) align-
ment of child nodes, involving compression through
deletion; (b) rewriting involving child and grand-
child nodes; (c) reordering of child nodes (with fur-
ther compression through applying other QG rules
on children). Nodes bear phrase and dependency la-
bels. Dotted lines show alignments in the grammar
between source and target child nodes. Examples
are taken from the QG rules discovered in the DUC-
03 data set of headlines.
source; this helps to improve the quality in what is
inherently a noisy process. Finally, QG rules are cre-
ated from aligned nodes above the leaf node level,
recording the phrase and dependency label of nodes,
and the alignment of child nodes.
Unlike previous work involving QG which has
used dependency graphs exclusively (e.g., Wang
et al 2007; Das and Smith 2009), our approach op-
erates over a combined PCFG-dependency represen-
tation. As a result, some configurations in Smith and
Eisner (2006) are not so relevant here ? instead,
we found that deletions, reorderings, flattening of
nodes, and the addition of text elements were im-
CHOICE/?
PP/prep in
IN/?
in
DT/det
the
JJ/amod
disputed
NN/?
territory
PP/prep of
IN/?
of
NNP/nn
East
NNP/?
Timor
PP/prep in
IN/?
in
NNP/nn
East
NNP/?
Timor
Figure 2: Alternative paraphrases are represented as
a CHOICE sub-tree.
portant operations for the grammar.
Figure 1 shows some example alignments that are
captured by the QG, with the source node on the
left and the target node on the right. Leaf nodes
have their original text, while other nodes have a
combined phrase and dependency label that they ob-
tain in the merged representation described in Sec-
tion 3.1 above (e.g., NP/dobj is a noun phrase and a
direct object, NNP/nn is a proper noun and a nomi-
nal modifier, whereas NN/? is a head noun). Align-
ments between the children are shown by dotted
lines. In Figure 1(a), some child nodes are aligned
while others are not present in the target tree. This
type of rule is common in our training data, and typ-
ically arises from the compression of names in noun
phrases. Another frequent compression, shown in
Figure 1(b), is flattening the tree structure by in-
corporating grand-child elements at the child level.
Figure 1(c) shows a rule involving the reordering
of child nodes, and where additional rules are ap-
plied recursively to achieve further compression and
a transformation in the phrase constituency.
Paraphrases are created from source sentence
parse trees by applying suitable rules recursively.
Suitable rules have matching structure in terms of
phrase and dependency label, for both the parent and
child nodes. Additionally, the proposed paraphrase
sub-tree must be suitable for the target tree being
created (i.e., the root node of the paraphrase must
match the phrase and dependency label of the corre-
sponding node in the target tree). Where more than
one paraphrase is possible, the alternatives are incor-
porated into the target parse tree under a CHOICE
node, as is shown in Figure 2. Note that unlike pre-
516
vious QG approaches, we do not use the probability
model proposed by Smith and Eisner (2006); instead
the QG is used to represent rewrite operations, and
we simply record a frequency count for how often
each rule is encountered in the training data.
3.3 ILP model
The objective of our model is to create the most in-
formative text possible, subject to constraints which
can be tailored to the specific task. These relate to
sentence length, overall summary length, the inclu-
sion of specific topics, and grammaticality. These
constraints are global in their scope, and cannot be
adequately satisfied by optimizing each one of them
individually. Our approach therefore uses an ILP
formulation which will provide a globally optimal
solution, and which can be efficiently solved using
standard optimization tools. Specifically, the model
selects phrases and paraphrases from which to form
the output sentence. Here, we focus on a single
sentence as this is most appropriate for title gener-
ation. However, multi-sentence output can be easily
generated by setting a summary length constraint.
The model operates over the merged phrase struc-
ture trees described in Section 3.1, augmented with
paraphrase choice nodes such as shown in Figure 2
rather than raw text.
Let S be the set of sentences in a document, P be
the set of phrases, and Ps ? P be the set of phrases
in each sentence s ? S . Let the sets Di ? P , ?i ? P
capture the phrase dependency information for each
phrase i, where each set Di contains the phrases that
depend on the presence of i. In a similar fashion,
C ? P is the set of choice nodes throughout the doc-
ument, which represent nodes in the tree where more
than one QG rule can be applied; Ci ? P , i ? C are
the sets of phrases that are direct children of each
choice node, in other words they are the individual
alternative paraphrases. Let li be the length of each
phrase i, in tokens.
For caption generation, the model has as addi-
tional input a list of tags (keywords drawn from the
source document) that correspond to the image, and
we refer to this set of tags as T . Pt ? P is the set of
phrases containing the tag t ? T . We use the proba-
bilistic image annotation model of Feng and Lapata
(2010a) to generate the list of keywords. The lat-
ter highlight the objects depicted in the image and
should be in all likelihood included in the caption.
The model is cast as an integer linear program:
max
x ?
i?P
( fi +?gi)xi (1a)
s.t. ?
i?P
lixi ? Lmax (1b)
?
i?P
lixi ? Lmin (1c)
?
i?Pt ,t?T
xi ? Tmin (1d)
x j? xi ?i ? P , j ?Di (1e)
?
j?Ci
x j = xi ?i ? C , j ? Ci (1f)
xi? ys ?s ? S , i ? Ps (1g)
?
s?S
ys ? NS (1h)
xi ? {0,1} ?i ? P (1i)
ys ? {0,1} ?s ? S . (1j)
A vector of binary variables x? {0,1}|P | indicates
if each phrase is to be part of the output. The vector
of auxiliary binary variables y ? {0,1}|S | indicates
from which sentences the chosen phrases come, see
Equation (1g).
Our objective function (1a) is the weighted sum of
two components for each phrase: a salience score,
and a measure of how frequently the QG rule was
seen in the training data. Let fi denote the salience
score for phrase i, determined by the machine learn-
ing algorithm. We apply a paraphrase penalty gi to
each phrase,
gi = log
(
nr
Nr
)
,
where nr is a count of the number of times this par-
ticular QG rule r was seen in the training data, and
Nr is the number of times all suitable rules for this
phrase node were seen. If no suitable rules exist,
we set gi = 0. The intuition here is that common
paraphrases should be more trustworthy, and thus
are given a smaller penalty than rare ones. Para-
phrase penalties are weighted by the constant param-
eter ?. which controls the amount of paraphrasing
we allow in the output. The objective function is
the sum of the salience scores and paraphrase penal-
ties of all the phrases chosen to form the output of a
given document, subject to the constraints in Equa-
517
tions (1b)?(1j). The latter provide a natural way of
describing the requirements the output must meet.
Constraints (1b) and (1c) ensure that the gener-
ated output stays within the acceptable length range
of (Lmin,Lmax) tokens. Equation (1d) is a set-
covering constraint, requiring that at least Tmin words
in T appear in the output. This is important where
we want to focus on some aspect of the source doc-
ument, for instance on the subject of an image.
Constraint (1e) ensures that the phrase dependen-
cies are respected and thus enforces grammatical
correctness. Phrases that depend on phrase i are con-
tained in the set Di. Variable xi is true, and therefore
phrase i will be included, if any of its dependents
x j ?Di are true. The phrase dependency constraints,
contained in the set Di and enforced by (1e), are the
result of three principles based on the typed depen-
dency information:
1. Where the QG provides alternative para-
phrases, it makes sense of course to select only
one. This is controlled by constraint (1f), and
by placing all paraphrases in the set Di for the
choice node i.
2. Where there are no applicable QG rules to
guide the model, in general we require all child
nodes j of the current node i to be included in
the summary if node i is included. As excep-
tions, we allow the subtree represented by node
j to be deleted if the dependency label for the
connecting edge i? j is of type advcl (adver-
bial clause) or some form of conj (conjunction).
3. In general, we force the parent node p of the
current node i to be included in the output if i
is, resulting in all ancestors up to the root node
being included. We allow a break, and the sub-
tree at i to be used as a stand-alone sentence, if
the PCFG parser has marked i with an S (sen-
tence) label.
Constraint (1g) tells the ILP to output a sentence if
one of its constituent phrases is chosen. Finally, (1h)
limits the output to a maximum of NS sentences.
4 Experimental Set-up
As mentioned earlier we evaluated the performance
of our model on two title generation tasks, namely
headline and caption generation. In this section we
give details on the corpora and grammars we used,
model parameters and features. We also describe the
baselines used for comparison with our approach,
and explain how system output was evaluated.
Training We obtained phrase-based salience
scores using a supervised machine learning algo-
rithm. For the headline generation task, the full
DUC-03 (Task 1) corpus was used for training;
it contains 500 documents and 4 headline-style
summaries per document. For the captions, training
data was gathered from the CNN news website.2
We used 200 documents and their corresponding
captions. Sentences were first tokenized to separate
words and punctuation, and then parsed to obtain
phrases and dependencies as described in Section 3
using the Stanford parser (Klein and Manning,
2003). Document phrases were marked as positive
or negative automatically. If there was a unigram
overlap (excluding stop words) between the phrase
and any of the original title or caption, we marked
this phrase with a positive label. Non-overlapping
phrases were given negative labels.
Our feature set comprised surface features such as
sentence and paragraph position information, POS
tags, and whether high-scoring tf.idf words were
present in the phrase. Additionally, the caption train-
ing set contained features for unigram and bigram
overlap with the title. We learned the feature weights
with a linear SVM, using the software SVM-OOPS
(Woodsend and Gondzio, 2009). This tool gave us
directly the feature weights as well as support vec-
tor values, and it allowed different penalties to be
applied to positive and negative misclassifications,
enabling us to compensate for the unbalanced data
set. The penalty hyper-parameters chosen were the
ones that gave the best F-scores, using 10-fold vali-
dation.
For each of the two tasks, QG rules were extracted
from the same data used to train the SVM, resulting
in 2,910 distinct rules for headlines and 2,757 rules
for the captions. Table 1 shows that for both tasks,
the majority of rules apply to PP and NP phrases.
Both tasks involve considerable compression, but
the proportions of the rewrite operations involved in-
dicate differences in style between them. Compared
2See http://edition.cnn.com/.
518
Label Prop?n Proportion for Label
of set Unmod Del Ins Re-ord
PP 40% 5% 93% 12% 6%
NP 31% 5% 87% 14% 7%
S 20% 1% 96% 15% 7%
SBAR 6% 4% 95% 28% 6%
(a) Headlines
Label Prop?n Proportion for Label
of set Unmod Del Ins Re-ord
PP 30% 17% 81% 7% 4%
NP 29% 17% 76% 11% 3%
S 27% 10% 84% 16% 6%
SBAR 10% 13% 80% 16% 3%
(b) Captions
Table 1: QG rules generated for (a) headline and
(b) caption tasks (top 4 labels shown). The columns
show label of root node, proportion of the full rule-
set, then the proportions of rules for this label in-
volving no modification, deletions, insertions and
re-orderings.
to headlines, captions involve slightly less deletion
and a higher proportion of the phrases are unmod-
ified. The QG learning mechanism also discovers
more alignments between source sentences and cap-
tions than it does for the headline task.
Title generation For the headline generation task,
we evaluated our model on a testing partition from
the DUC-04 corpus (75 documents, Task 1). For the
caption task, we used the test set (240 documents)
described in Feng and Lapata (2010a). Their corpus
was downloaded from the BBC news site and con-
tains documents, images, and their captions.3
We created and solved an ILP for each docu-
ment. For each phrase, features were extracted and
salience scores calculated from the feature weights
determined through SVM training. The distance
from the SVM hyperplane represents the salience
score. Parameters for the ILP models for the two
tasks are shown in Table 2. The ? parameter was
set to 0.2 to ensure that paraphrases were included;
other parameters were chosen to capture the prop-
3Available from http://homepages.inf.ed.ac.uk/
s0677528/data.html.
Parameter Headlines Captions
Min length Lmin 8 8
Max length Lmax 16 20
Min keywords Tmin 0 2
Max sentences NS 5 1
Paraphrase ? 0.2 0.1
Table 2: ILP model parameters for the two tasks.
erties seen in the majority of the training set. Note
the maximum number of sentences allowed to form
a headline is set to 5 as some of the headlines in the
DUC dataset contained multiple sentences.
To solve the ILP model we used the ZIB Opti-
mization Suite software (Achterberg, 2007; Koch,
2004). The solution was converted into a sentence
by removing nodes not chosen from the tree rep-
resentation, then concatenating the remaining leaf
nodes in order.
Model Comparison For the headline task, we
compared our model to the DUC-04 standard base-
line of the first sentence, truncated at the first word
boundary after 75 characters; and the output of the
Topiary system (Zajic et al, 2004), which came top
in almost all measures in the DUC-04 evaluation.
In order to generate a headline, Topiary first com-
presses the lead sentence using linguistically moti-
vated heuristics and then enhances it with topic key-
words. For the captions, we compared our model
against the highest-scoring document sentence ac-
cording to the SVM and against the probabilistic
model presented in Feng and Lapata (2010a). The
latter estimates the probability of a phrase appear-
ing in the caption given the same phrase appearing
in the corresponding document and uses a language
model to select among many different surface real-
izations. The language model is adapted with prob-
abilities from an image annotation model (Feng and
Lapata, 2010b).
Evaluation We evaluated the quality of the head-
lines using ROUGE (Lin and Hovy, 2003). The
DUC-04 dataset provides four reference head-
lines per document. We report unigram overlap
(ROUGE-1) and bigram overlap (ROUGE-2) as a
means of assessing informativeness, and the longest
common subsequence (ROUGE-L) as a means of as-
519
sessing fluency. Original DUC-04 ROUGE parame-
ters were used. We also use ROUGE to evaluate the
automatic captions with the original BBC captions
as reference.
In addition, we evaluated the generated headlines
by eliciting human judgments. Participants were
presented with a news article and its correspond-
ing headline and were asked to rate the latter along
two dimensions: informativeness (does the headline
capture the article?s most important information?),
and grammaticality (is it fluent and easy to under-
stand?). The subjects used a seven point rating scale;
an ideal system would receive high numbers for
both measures. We randomly selected twelve docu-
ments from the test set and generated headlines with
our model. We also included the output of Topiary
and the human written DUC-04 headlines as a gold
standard. We thus obtained ratings for 48 (12 ? 4)
document-highlights pairs.
We elicited judgments for the generated captions
in a similar fashion. Participants were presented
with a document, an associated image, and its cap-
tion, and asked to rate the latter (using a 1?7 rating
scale) with respect to grammaticality and informa-
tiveness (does it describe succinctly the content of
the image and document?). Again, we randomly se-
lected 12 document-image pairs from the test set and
generated captions for them using the highest scor-
ing document sentence according to the SVM, our
ILP-based model, and the output of Feng and Lap-
ata?s (2010a) system. We also included the original
BBC captions as an upper bound. Both studies were
conducted over the Internet using WebExp (Keller
et al, 2009). 80 unpaid volunteers rated the head-
lines and 65 the captions, all self reported native En-
glish speakers.
5 Results
We report results on the headline generation task in
Figure 3, with ROUGE-1, ROUGE-2 and ROUGE-
L. In ROUGE-1 and ROUGE-L measures, the best
scores are obtained by the Topiary system, slightly
better than the lead sentence baseline, while for
ROUGE-2 the ordering is reversed. Our model does
not outperform the lead sentence or Topiary. Note
that the 95% confidence level intervals reported by
ROUGE are so large that no results are statistically
Lead The chances for a new, strictly secular government in
Turkey faded Wednesday.
Topiary TURKEY YILMAZ PARTY ECEVIT chances strictly
secular government faded.
ILP Bulent Ecevit needs Turkey?s two-center right parties to
hammer together secular coalition.
DUC Chance for new, secular, Turkish government fades; what
will Ecevit do now?
Source Premier-designate Bulent Ecevit needs Turkey?s two-
center right parties to hammer together a secular coali-
tion, but Tansu Ciller, the ex-premier who commands 99
votes in parliament, rebuffed him Wednesday.
Lead U.S. President Bill Clinton won South Korea?s support
Saturday for confronting.
Topiary NUCLEAR U.S. President Bill Clinton won for con-
fronting North Korea.
ILP North Koreans have denied construction site has nuclear
purpose.
DUC U.S. warns N. Korea not to waste chance for peace over
alleged nuclear site.
Source The North Koreans have denied the underground con-
struction site has any nuclear purpose, and it has de-
manded a dlrs 300 million payment for proving that.
Lead By only one vote, the center-left prime minister of Italy,
Romano Prodi.
Topiary PRODI By only one vote center left prime minister and
toppled from power.
ILP Political system changes, Italy is condemned to political
instability.
DUC Prodi loses confidence vote; will stay as caretaker until
new government.
Source ?Unless the Italian political system changes, Italy is con-
demned to political instability,? said Sergio Romano, a
former diplomat and political science professor.
Table 3: Example headline output.
F&L The former paramedic training officer stood at the next
general election.
ILP The majority are now believing that war in Iraq was
wrong.
BBC L/Cpl Thomas Keys was shot 18 times, his inquest heard.
Source The majority of people in this country are now believing
that the war in Iraq was wrong, and I do believe we will
get support.
F&L The state government of Victoria take as those tests for
cannabis.
ILP Police in Victoria have begun randomly testing drivers for
the drug ecstasy.
BBC Police say drugs like Ecstasy can be as dangerous as al-
cohol for drivers.
Source Police in the Australian state of Victoria have begun ran-
domly testing drivers for the drug ecstasy.
F&L The US Government Professor Holdren called for more
than a year.
ILP ?We are experiencing dangerous human disruption of
global climate,? Professor Holdren said.
BBC Sea levels could rise by 4m over the coming century, he
warns.
Source ?We are experiencing dangerous human disruption of the
global climate and we?re going to experience more,? Pro-
fessor Holdren said.
Table 4: Example caption output.
520
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
Lead-1 Topiary ILP
Sc
or
e
Rouge-1
Rouge-L
Rouge-2
Figure 3: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the DUC-04 headlines for our ILP model,
the lead sentence baseline and Topiary.
 0
 0.05
 0.1
 0.15
 0.2
SVM F&L ILP
Sc
or
e
Rouge-1
Rouge-L
Rouge-2
Figure 4: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the BBC captions for our ILP model, the
sentence baseline chosen by the SVM, and Feng and
Lapata?s (2010) model.
significant. We also investigated using an ILP model
with just the QG rules or just dependency label in-
formation (see constraint (1e) in Section 3.3). Both
settings gave less compressed output, and the result-
ing ROUGE scores were lower on all measures. The
ROUGE results for the caption generation task fol-
low a similar pattern (see Figure 4). Our model is
slightly better than the best sentence baseline but
performs worse than Feng and Lapata (2010a). Ta-
bles 3 and 4 show example output for the ILP model
and the baselines on the headline and caption tasks
respectively. In the tables, Source refers to the sen-
tence chosen by the ILP, but before any paraphrasing
is applied. We can see that deletion rules dominate,
and a more compressive style of paraphrasing has
been learned for the headline task.
The results of our human evaluation study for
the DUC-04 headlines are summarized in Table 5.
Means differences were compared using a Post-hoc
Model Grammaticality Importance
Lead-1 4.95 3.30
Topiary 3.03 3.43
ILP 5.36 4.94
Reference 5.12 5.17
Table 5: Average human ratings of DUC-04 head-
lines, for our ILP model, the lead sentence baseline,
the output of Topiary and the human-written refer-
ence.
Model Grammaticality Importance
SVM 5.24 5.01
F&L 4.42 4.74
ILP 5.49 5.25
Reference 5.61 5.18
Table 6: Average human ratings of captions, for
our ILP model, the sentence baseline chosen by the
SVM, Feng and Lapata?s (2010) model and the ref-
erence BBC caption.
Tukey test. The headlines created by our model
were considered significantly more important and
more grammatical than those of the Topiary sys-
tem (? < 0.01), despite the better overlap of Topi-
ary with the reference headlines as indicated in the
Rouge results above. Compared to the lead sentence
of the article (the DUC-04 baseline), our model was
also rated significantly higher in terms of importance
(? < 0.01) but not grammaticality.
Table 6 summarizes the results of our second
judgment elicitation study. The captions generated
by our model are significantly more grammatical
than those of Feng and Lapata (2010a) (? < 0.01).
The SVM, ILP model and reference captions do not
differ significantly in terms of grammaticality. In
terms of importance, the ILP model is significantly
better than the SVM (? < 0.01) and Feng and Lap-
ata (? < 0.01) and comparable to the reference.
The human ratings are more favorable to our
model than ROUGE for both tasks. There are two
reasons for this. Firstly, the model is not bi-
ased towards selecting the lead sentence as a head-
line/caption and is disadvantaged in ROUGE evalua-
tions as professional abstractors often reuse the lead
or parts of it to create a title. Secondly, the model
often generates an appropriate title that is lexically
521
distinct from the reference even though it expresses
similar meaning.
6 Conclusions
In this paper we proposed a joint content se-
lection and surface realization model for single-
document summarization. The model operates over
a syntax-rich representation of the source docu-
ment and learns which phrases should be in the
summary. Content selection preferences are cou-
pled with a quasi-synchronous grammar whose rules
encode surface realization preferences (e.g., para-
phrases and compressions). Both types of prefer-
ences are optimized simultaneously in an integer lin-
ear program subject to grammaticality, length and
coverage constraints. Importantly, the QG allows
the model to adapt to the writing and stylistic con-
ventions of different tasks. The results of our hu-
man studies show that our system creates grammati-
cal and informative summaries whilst outperforming
several competitive baselines.
The model itself is relatively simple and achieves
good performance without any task-specific modifi-
cation. One potential stumbling block may be the
availability of parallel data for acquiring the QG.
The Internet provides a large repository of news
documents with headlines, images and captions. In
some cases news articles are even accompanied with
?story highlights? which could be used as training
data for longer summaries.4 For other domains ob-
taining such data may be more difficult. However,
our experiments have shown that relatively small
parallel corpora (in the range of 200?500 pairs) suf-
fice to learn many of the writing conventions for a
given task.
In the future, we plan to explore how to inte-
grate more sophisticated QG rules in the generation
process. Currently we consider deletions, reorder-
ings and insertions. Ideally, we would also like to
model arbitrary substitutions between words but also
larger constituents (e.g., subclauses, sentence aggre-
gation). Beyond summarization, we would also like
to apply our model to other generation tasks, such as
paraphrasing and text simplification.
4On-line CNN news articles are prefaced by story
highlights?three or four short sentences that are written by hu-
mans and give a brief overview of the article.
Acknowledgments We are grateful to David Chi-
ang and Noah Smith for their input on earlier ver-
sions of this work. We would also like to thank
Andreas Grothey and members of ICCS at the
School of Informatics for valuable discussions and
comments. We acknowledge the support of EP-
SRC through project grants EP/F055765/1 and
GR/T04540/01.
References
Achterberg, Tobias. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universita?t Berlin.
Banko, Michele, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In Proceedings of the 38th ACL. Hong
Kong, pages 318?325.
Clarke, James and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research 31:399?429.
Cohn, Trevor and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd COLING. Manchester, UK, pages 137?144.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the ACL-IJCNLP.
Suntec, Singapore, pages 468?476.
Daume? III, Hal. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th ACL. Philadelphia, PA, pages 449?456.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 2003 Text Summarization Workshop and Doc-
ument Understanding Conference. Edmondon, Al-
berta, pages 1?8.
Dras, Mark. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text.. Ph.D. thesis, Macquarie
University.
Feng, Yansong and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, Uppsala, Sweden, pages 1239?1249.
Feng, Yansong and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Pro-
522
ceedings of the NAACL HLT . Association for Com-
putational Linguistics, Los Angeles, California, pages
831?839.
Jing, Hongyan. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th ANLP.
Seattle, WA, pages 310?315.
Jing, Hongyan. 2002. Using hidden Markov modeling to
decompose human-written summaries. Computational
Linguistics 28(4):527?544.
Jing, Hongyan and Kathleen McKeown. 2000. Cut
and paste summarization. In Proceedings of the 1st
NAACL. Seattle, WA, pages 178?185.
Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of web experi-
ments: A case study using the WebExp software pack-
age. Behavior Research Methods 41(1):1?12.
Klein, Dan and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st ACL.
Sapporo, Japan, pages 423?430.
Koch, Thorsten. 2004. Rapid Mathematical Prototyping.
Ph.D. thesis, Technische Universita?t Berlin.
Kupiec, Julian, Jan O. Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Proceed-
ings of SIGIR-95. Seattle, WA, pages 68?73.
Lin, Chin-Yew. 2003. Improving summarization perfor-
mance by sentence compression ? a pilot study. In
Proceedings of the 6th International Workshop on In-
formation Retrieval with Asian Languages. Sapporo,
Japan, pages 1?8.
Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT NAACL. Edmonton,
Canada, pages 71?78.
Martins, Andre? and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing. Boulder, Colorado, pages 1?9.
Smith, David and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings on the Workshop on Sta-
tistical Machine Translation. Association for Compu-
tational Linguistics, New York City, pages 23?30.
Smith, David A. and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the EMNLP. Suntec, Sin-
gapore, pages 822?831.
Soricut, R. and D. Marcu. 2007. Abstractive head-
line generation using WIDL-expressions. Information
Processing and Management 43(6):1536?1548. Text
Summarization.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
EMNLP-CoNLL. Prague, Czech Republic, pages 22?
32.
Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector ma-
chine training. Computational Optimization and Ap-
plications Published online.
Woodsend, Kristian and Mirella Lapata. 2010. Automatic
generation of story highlights. In Sandra Carberry and
Stephen Clark, editors, Proceedings of the 48th ACL.
Uppsala, Sweden, pages 565?574.
Zajic, David, Bonnie Dorr, and Richard Schwartz. 2004.
BBN/UMD at DUC-2004: Topiary. In Proceedings
of the NAACL Workshop on Document Understanding.
Boston, MA, pages 112?119.
Zajic, David, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing Management Special Is-
sue on Summarization 43(6):1549?1570.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP. Suntec, Singapore, pages 834?842.
523
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162?1172,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Measuring Distributional Similarity in Context
Georgiana Dinu
Department of Computational Linguistics
Saarland University
Saarbru?cken, Germany
dinu@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
The computation of meaning similarity as
operationalized by vector-based models has
found widespread use in many tasks ranging
from the acquisition of synonyms and para-
phrases to word sense disambiguation and tex-
tual entailment. Vector-based models are typ-
ically directed at representing words in isola-
tion and thus best suited for measuring simi-
larity out of context. In his paper we propose
a probabilistic framework for measuring sim-
ilarity in context. Central to our approach is
the intuition that word meaning is represented
as a probability distribution over a set of la-
tent senses and is modulated by context. Ex-
perimental results on lexical substitution and
word similarity show that our algorithm out-
performs previously proposed models.
1 Introduction
The computation of meaning similarity as op-
erationalized by vector-based models has found
widespread use in many tasks within natural lan-
guage processing (NLP). These range from the ac-
quisition of synonyms (Grefenstette, 1994; Lin,
1998) and paraphrases (Lin and Pantel, 2001) to
word sense disambiguation (Schuetze, 1998), tex-
tual entailment (Clarke, 2009), and notably informa-
tion retrieval (Salton et al, 1975).
The popularity of vector-based models lies in
their unsupervised nature and ease of computation.
In their simplest incarnation, these models repre-
sent the meaning of each word as a point in a
high-dimensional space, where each component cor-
responds to some co-occurring contextual element
(Landauer and Dumais, 1997; McDonald, 2000;
Lund and Burgess, 1996). The advantage of taking
such a geometric approach is that the similarity of
word meanings can be easily quantified by measur-
ing their distance in the vector space, or the cosine
of the angle between them.
Vector-based models do not explicitly identify the
different senses of words and consequently repre-
sent their meaning invariably (i.e., irrespective of co-
occurring context). Consider for example the adjec-
tive heavy which we may associate with the gen-
eral meaning of ?dense? or ?massive?. However,
when attested in context, heavy may refer to an over-
weight person (e.g., She is short and heavy but she
has a heart of gold.) or an excessive cannabis user
(e.g., Some heavy users develop a psychological de-
pendence on cannabis.).
Recent work addresses this issue indirectly with
the development of specialized models that repre-
sent word meaning in context (Mitchell and Lap-
ata, 2008; Erk and Pado?, 2008; Thater et al, 2009).
These methods first extract typical co-occurrence
vectors representing a mixture of senses and then use
vector operations to either obtain contextualized rep-
resentations of a target word (Erk and Pado?, 2008)
or a representation for a set of words (Mitchell and
Lapata, 2009).
In this paper we propose a probabilistic frame-
work for representing word meaning and measuring
similarity in context. We model the meaning of iso-
lated words as a probability distribution over a set of
latent senses. This distribution reflects the a priori,
out-of-context likelihood of each sense. Because
sense ambiguity is taken into account directly in the
1162
vector construction process, contextualized meaning
can be modeled naturally as a change in the origi-
nal sense distribution. We evaluate our approach on
word similarity (Finkelstein et al, 2002) and lexical
substitution (McCarthy and Navigli, 2007) and show
improvements over competitive baselines.
In the remainder of this paper we give a brief
overview of related work, emphasizing vector-based
approaches that compute word meaning in context
(Section 2). Next, we present our probabilistic
framework and different instantiations thereof (Sec-
tions 3 and 4). Finally, we discuss our experimental
results (Sections 5 and 6) and conclude the paper
with future work.
2 Related work
Vector composition methods construct representa-
tions that go beyond individual words (e.g., for
phrases or sentences) and thus by default obtain
word meanings in context. Mitchell and Lapata
(2008) investigate several vector composition op-
erations for representing short sentences (consist-
ing of intransitive verbs and their subjects). They
show that models performing point-wise multiplica-
tion of component vectors outperform earlier pro-
posals based on vector addition (Landauer and Du-
mais, 1997; Kintsch, 2001). They argue that multi-
plication approximates the intersection of the mean-
ing of two vectors, whereas addition their union.
Mitchell and Lapata (2009) further show that their
models yield improvements in language modeling.
Erk and Pado? (2008) employ selectional prefer-
ences to contextualize occurrences of target words.
For example, the meaning of a verb in the presence
of its object is modeled as the multiplication of the
verb?s vector with the vector capturing the inverse
selectional preferences of the object; the latter are
computed as the centroid of the verbs that occur
with this object. Thater et al (2009) improve on this
model by representing verbs in a second order space,
while the representation for objects remains first or-
der. The meaning of a verb boils down to restricting
its vector to the features active in the argument noun
(i.e., dimensions with value larger than zero).
More recently, Reisinger and Mooney (2010)
present a method that uses clustering to pro-
duce multiple sense-specific vectors for each word.
Specifically, a word?s contexts are clustered to pro-
duce groups of similar context vectors. An aver-
age prototype vector is then computed separately
for each cluster, producing a set of vectors for each
word. These cluster vectors can be used to determine
the semantic similarity of both isolated words and
words in context. In the second case, the distance
between prototypes is weighted by the probability
that the context belongs to the prototype?s cluster.
Erk and Pado? (2010) propose an exemplar-based
model for capturing word meaning in context. In
contrast to the prototype-based approach, no cluster-
ing takes place, it is assumed that there are as many
senses as there are instances. The meaning of a word
in context is the set of exemplars most similar to it.
Unlike Reisinger and Mooney (2010) and Erk and
Pado? (2010) our model is probabilistic (we repre-
sent word meaning as a distribution over a set of la-
tent senses), which makes it easy to integrate and
combine with other systems via mixture or product
models. More importantly, our approach is concep-
tually simpler as we use a single vector representa-
tion for isolated words as well as for words in con-
text. A word?s different meanings are simply mod-
eled as changes in its sense distribution. We should
also point out that our approach is not tied to a spe-
cific sense induction method and can be used with
different variants of vector-space models.
3 Meaning Representation in Context
In this section we first describe how we represent
the meaning of individual words and then move on
to discuss our model of inducing meaning represen-
tations in context.
Observed Representations Most vector space
models in the literature perform computations on
a co-occurrence matrix where each row repre-
sents a target word, each column a document or
another neighboring word, and each entry their
co-occurrence frequency. The raw counts are typ-
ically mapped into the components of a vector in
some space using for example conditional probabil-
ity, the log-likelihood ratio or tf-idf weighting. Un-
der this representation, the similarity of word mean-
ings can be easily quantified by measuring their dis-
tance in the vector space, the cosine of the angle be-
tween them, or their scalar product.
1163
Our model assumes the same type of input data,
namely a co-occurrence matrix, where rows corre-
spond to target words and columns to context fea-
tures (e.g., co-occurring neighbors). Throughout
this paper we will use the notation ti with i : 1..I
to refer to a target word and cj with j : 1..J to refer
to context features. A cell (i, j) in the matrix rep-
resents the frequency of occurrence of target ti with
context feature cj over a corpus.
Meaning Representation over Latent Senses
We further assume that the target words ti i : 1...I
found in a corpus share a global set of meanings
or senses Z = {zk|k : 1...K}. And therefore the
meaning of individual target words can be described
as a distribution over this set of senses. More for-
mally, a target ti is represented by the following vec-
tor:
v(ti) = (P(z1|ti), ...,P(zK|ti)) (1)
where component P (z1|ti) is the probability of
sense z1 given target word ti, component P (z2|ti)
the probability of sense z2 given ti and so on.
The intuition behind such a representation is that
a target word can be described by a set of core mean-
ings and by the frequency with which these are at-
tested. Note that the representation in (1) is not
fixed but parametrized with respect to an input cor-
pus (i.e., it only reflects word usage as attested in
that corpus). The senses z1 . . . zK are latent and can
be seen as a means of reducing the dimensionality
of the original co-occurrence matrix.
Analogously, we can represent the meaning of a
target word given a context feature as:
v(ti, cj) = (P(z1|ti, cj), ...,P(zK|ti, cj)) (2)
Here, target ti is again represented as a distribution
over senses, but is now modulated by a specific con-
text cj which reflects actual word usage. This distri-
bution is more ?focused? compared to (1); the con-
text helps disambiguate the meaning of the target
word, and as a result fewer senses will share most
of the probability mass.
In order to create the context-aware representa-
tions defined in (2) we must estimate the proba-
bilities P (zk|ti, cj) which can be factorized as the
product of P (ti, zk), the joint probability of target ti
and latent sense zk, and P (cj |zk, ti), the conditional
probability of context cj given target ti and sense zk:
P (zk|ti, cj) =
P (ti, zk)P (cj |zk, ti)
?
k P (ti, zk)P (cj |zk, ti)
(3)
Problematically, the term P (cj |zk, ti) is difficult to
estimate since it implies learning a total number of
K ? I J-dimensional distributions. We will there-
fore make the simplifying assumption that target
words ti and context features cj are conditionally in-
dependent given sense zk:
P (zk|ti, cj) ?
P (zk|ti)P (cj |zk)
?
k P (zk|ti)P (cj |zk)
(4)
Although not true in general, the assumption is rela-
tively weak. We do not assume that words and con-
text features occur independently of each other, but
only that they are generated independently given an
assigned meaning. A variety of latent variable mod-
els can be used to obtain senses z1 . . . zK and es-
timate the distributions P (zk|ti) and P (cj |zk); we
give specific examples in Section 4.
Note that we abuse terminology here, as the
senses our models obtain are not lexicographic
meaning distinctions. Rather, they denote coarse-
grained senses or more generally topics attested in
the document collections our model is trained on.
Furthermore, the senses are not word-specific but
global (i.e., shared across all words) and modulated
either within or out of context probabilistically via
estimating P (zk|ti, cj) and P (zk|ti), respectively.
4 Parametrizations
The general framework outlined above can be
parametrized with respect to the input co-occurrence
matrix and the algorithm employed for inducing the
latent structure. Considerable latitude is available
when creating the co-occurrence matrix, especially
when defining its columns, i.e., the linguistic con-
texts a target word is attested with. These con-
texts can be a small number of words surrounding
the target word (Lund and Burgess, 1996; Lowe
and McDonald, 2000), entire paragraphs, documents
(Salton et al, 1975; Landauer and Dumais, 1997)
or even syntactic dependencies (Grefenstette, 1994;
Lin, 1998; Pado? and Lapata, 2007).
1164
Analogously, a number of probabilistic models
can be employed to induce the latent senses. Ex-
amples include Probabilistic Latent Semantic Anal-
ysis (PLSA, Hofmann (2001)), Probabilistic Prin-
cipal Components Analysis (Tipping and Bishop,
1999), non-negative matrix factorization (NMF, Lee
and Seung (2000)), and latent Dirichlet alocation
(LDA, Blei et al (2003)). We give a more detailed
description of the latter two models as we employ
them in our experiments.
Non-negative Matrix Factorization Non-
negative matrix factorization algorithms approx-
imate a non-negative input matrix V by two
non-negative factors W and H , under a given
loss function. W and H are reduced-dimensional
matrices and their product can be regarded as a
compressed form of the data in V :
VI,J ?WI,KHK,J (5)
where W is a basis vector matrix and H is an en-
coded matrix of the basis vectors in equation (5).
Several loss functions are possible, such as mean
squared error and Kullback-Leibler (KL) diver-
gence. In keeping with the formulation in Sec-
tion 3 we opt for a probabilistic interpretation of
NMF (Gaussier and Goutte, 2005; Ding et al, 2008)
and thus minimize the KL divergence between WH
and V .
min
?
i,j
(Vi,j log
Vi,j
WHi,j
? Vi,j +WHi,j) (6)
Specifically, we interpret matrix V as
Vij = P (ti, cj), and matrices W and H as P (ti, zk)
and P (cj |zk), respectively. We can also ob-
tain the following more detailed factorization:
P (ti, cj) =
?
k P (ti)P (zk|ti)P (cj |zk).
Le WH denote the factors in a NMF decom-
position of an input matrix V and B be a diag-
onal matrix with Bkk =
?
j Hkj . B
?1H gives a
row-normalized version of H . Similarly, given
matrix WB, we can define a diagonal matrix A,
with Aii =
?
k(WB)ik. A
?1WB row-normalizes
matrix WB. The factorization WH can now be re-
written as:
WH=AA?1WBB?1H=A(A?1WB)(B?1H)
which allows us to interpret A as P (ti), A?1WB
as P (zk|ti) and B?1H as P (cj |zk). These interpre-
tations are valid since the rows of A?1WB and of
B?1H sum to 1, matrix A is diagonal with trace 1
because elements in WH sum to 1, and all entries
are non-negative.
Latent Dirichlet Allocation LDA (Blei et al,
2003) is a probabilistic model of text generation.
Each document d is modeled as a distribution
over K topics, which are themselves characterized
by distributions over words. The individual words
in a document are generated by repeatedly sampling
a topic according to the topic distribution and then
sampling a single word from the chosen topic.
More formally, we first draw the mixing propor-
tion over topics ?d from a Dirichlet prior with pa-
rameters ?. Next, for each of the Nd words wdn in
document d, a topic zdn is first drawn from a multi-
nomial distribution with parameters ?dn. The prob-
ability of a word token w taking on value i given
that topic z = j is parametrized using a matrix ?
with bij = P (w = i|z = j). Integrating out ?d?s
and zdn?s, gives P (D|?, ?), the probability of a cor-
pus (or document collection):
M?
d=1
?
P (?d|?)
?
?
Nd?
n=1
?
zdn
P (zdn|?d)P (wdn|zdn, ?)
?
?d?d
The central computational problem in topic
modeling is to obtain the posterior distri-
bution P (?, z|w, ?, ?) of the hidden vari-
ables z = (z1, z2, . . . , zN ). given a docu-
ment w = (w1, w2, . . . , wN ). Although this
distribution is intractable in general, a variety
of approximate inference algorithms have been
proposed in the literature. We adopt the Gibbs
sampling procedure discussed in Griffiths and
Steyvers (2004). In this model, P (w = i|z = j) is
also a Dirichlet mixture (denoted ?) with symmetric
priors (denoted ?).
We use LDA to induce senses of target words
based on context words, and therefore each row ti
in the input matrix transforms into a document. The
frequency of ti occurring with context feature cj is
the number of times word cj is encountered in the
?document? associated with ti. We train the LDA
model on this data to obtain the ? and ? distribu-
1165
tions. ? gives the sense distributions of each tar-
get ti: ?ik = P (zk|ti) and ? the context-word dis-
tribution for each sense zk: ?kj = P (cj |zk).
5 Experimental Set-up
In this section we discuss the experiments we per-
formed in order to evaluate our model. We describe
the tasks on which it was applied, the corpora used
for model training and our evaluation methodology.
Tasks The probabilistic model presented in Sec-
tion 3 represents words via a set of induced senses.
We experimented with two types of semantic space
based on NMF and LDA and optimized parameters
for these models on a word similarity task. The
latter involves judging the similarity sim(ti, t?i) =
sim(v(ti), v(t?i)) of words ti and t
?
i out of context,
where v(ti) and v(t?i) are obtained from the output of
NMF or LDA, respectively. In our experiments we
used the data set of Finkelstein et al (2002). It con-
tains 353 pairs of words and their similarity scores
as perceived by human subjects.
The contextualized representations were next
evaluated on lexical substitution (McCarthy and
Navigli, 2007). The task requires systems to find
appropriate substitutes for target words occurring in
context. Typically, systems are given a set of substi-
tutes, and must produce a ranking such that appro-
priate substitutes are assigned a higher rank com-
pared to non-appropriate ones. We made use of the
SemEval 2007 Lexical Substitution Task benchmark
data set. It contains 200 target words, namely nouns,
verbs, adjectives and adverbs, each of which occurs
in 10 distinct sentential contexts. The total set con-
tains 2,000 sentences. Five human annotators were
asked to provide substitutes for these target words.
Table 1 gives an example of the adjective still and
its substitutes.
Following Erk and Pado? (2008), we pool together
the total set of substitutes for each target word.
Then, for each instance the model has to produce a
ranking for the total substitute set. We rank the can-
didate substitutes based on the similarity of the con-
textualized target and the out-of-context substitute,
sim(v(ti, cj), v(t?i)), where ti is the target word, cj a
context word and t?i a substitute. Contextualizing
just one of the words brings higher discriminative
power to the model rather than performing compar-
Sentences Substitutes
It is important to apply the
herbicide on a still day, be-
cause spray drift can kill
non-target plants.
calm (5) not-windy (1)
windless (1)
A movie is a visual docu-
ment comprised of a series
of still images.
motionless (3) unmov-
ing (2) fixed (1) sta-
tionary (1) static (1)
Table 1: Lexical substitution data example for the adjec-
tive still ; numbers in parentheses indicate the frequency
of the substitute.
isons with the target and its substitute embedded in
an identical context (see also Thater et al (2010) for
a similar observation).
Model Training All the models we experimented
with use identical input data, i.e., a bag-of-words
matrix extracted from the GigaWord collection of
news text. Rows in this matrix are target words and
columns are their co-occurring neighbors, within a
symmetric window of size 5. As context words, we
used a vocabulary of the 3,000 most frequent words
in the corpus.1
We implemented the classical NMF factorization
algorithm described in Lee and Seung (2000). The
input matrix was normalized so that all elements
summed to 1. We experimented with four dimen-
sions K: [600 ? 1000] with step size 200. We ran
the algorithm for 150 iterations to obtain factors W
and H which we further processes as described in
Section 4 to obtain the desired probability distribu-
tions. Since the only parameter of the NMF model
is the factorization dimension K, we performed two
independent runs with each K value and averaged
their predictions.
The parameters for the LDA model are the num-
ber of topicsK and Dirichlet priors ? and ?. We ex-
perimented with topics K: [600? 1400], again with
step size 200. We fixed ? to 0.01 and tested two val-
ues for ?: 2K (Porteous et al, 2008) and
50
K (Griffiths
and Steyvers, 2004). We used Gibbs sampling on
the ?document collection? obtained from the input
matrix and estimated the sense distributions as de-
scribed in Section 4. We ran the chains for 1000 iter-
1The GigaWord corpus contains 1.7B words; we scale down
all the counts by a factor of 70 to speed up the computation of
the LDA models. All models use this reduced size input data.
1166
ations and averaged over five iterations [600?1000]
at lag 100 (we observed no topic drift).
We measured similarity using the scalar prod-
uct, cosine, and inverse Jensen-Shannon (IJS) diver-
gence (see (7), (8), and (9), respectively):
sp(v, w) =< v,w >=
?
i
viwi (7)
cos(v, w) =
?v, w?
||v|| ||w||
(8)
IJS(v, w) =
1
JS(v,w)
(9)
JS(v,w) =
1
2
KL(v|m) +
1
2
KL(w|m) (10)
where m is a shorthand for 12(v + w) and
KL the Kullback-Leibler divergence, KL(v|w) =
?
i vilog(
vi
wi
).
Among the above similarity measures, the scalar
product has the most straightforward interpretation
as the probability of two targets sharing a common
meaning (i.e., the sum over all possible meanings).
The scalar product assigns 1 to a pair of identi-
cal vectors if and only if P (zi) = 1 for some i
and P (zj) = 0,?j 6= i. Thus, only fully disam-
biguated words receive a score of 1. Beyond similar-
ity, the measure also reflects how ?focused? the dis-
tributions in question are, as very ambiguous words
are unlikely to receive high scalar product values.
Given a set of context words, we contextualize the
target using one context word at a time and compute
the overall similarity score by multiplying the indi-
vidual scores.
Baselines Our baseline models for measuring sim-
ilarity out of context are Latent Semantic Analysis
(Landauer and Dumais, 1997) and a simple seman-
tic space without any dimensionality reduction.
For LSA, we computed the U?V SVD decompo-
sition of the original matrix to rank k = 1000. Any
decomposition of lower rank can be obtained from
this by setting rows and columns to 0. We evaluated
decompositions to ranks K: [200 ? 1000], at each
100 step. Similarity computations were performed
in the lower rank approximation matrix U?V , as
originally proposed in Deerwester et al (1990), and
in matrix U which maps the words into the concept
space. It is common to compute SVD decomposi-
tions on matrices to which prior weighting schemes
have been applied. We experimented with tf-idf
weighting and line normalization.
Our second baseline, the simple semantic space,
was based on the original input matrix on which
we applied several weighting schemes such as point-
wise mutual information, tf-idf, and line normaliza-
tion. Again, we measured similarity using cosine,
scalar product and inverse JS divergence. In addi-
tion, we also experimented with Lin?s (1998) simi-
larity measure:
lin(v, w) =
?
i?I(v)?I(w)(vi + wi)
?
i?I(v) vi +
?
l?I(w)wi
(11)
where the values in v and w are point-wise mutual
information, and I(?) gives the indices of positive
values in a vector.
Our baselines for contextualized similarity were
vector addition and vector multiplication which
we performed using the simple semantic space
(Mitchell and Lapata, 2008) and dimensionality
reduced representations obtained from NMF and
LDA. To create a ranking of the candidate substi-
tutes we compose the vector of the target with its
context and compare it with each substitute vector.
Given a set of context words, we contextualize the
target using each context word at a time and multi-
ply the individual scores.
Evaluation Method For the word similarity task
we used correlation analysis to examine the rela-
tionship between the human ratings and their cor-
responding vector-based similarity values. We re-
port Spearman?s ? correlations between the simi-
larity values provided by the models and the mean
participant similarity ratings in the Finkelstein et al
(2002) data set. For the lexical substitution task, we
compare the system ranking with the gold standard
ranking using Kendall?s ?b rank correlation (which is
adjusted for tied ranks). For all contextualized mod-
els we defined the context of a target word as the
words occurring within a symmetric context window
of size 5. We assess differences between models us-
ing stratified shuffling (Yeh, 2000).2
2Given two system outputs, the null hypothesis (i.e., that
the two predictions are indistinguishable) is tested by randomly
mixing the individual instances (in our case sentences) of the
two outputs. We ran a standard number of 10000 iterations.
1167
Model Spearman ?
SVS 38.35
LSA 49.43
NMF 52.99
LDA 53.39
LSAMIX 49.76
NMFMIX 51.62
LDAMIX 51.97
Table 2: Results on out of context word similarity using
a simple co-occurrence based vector space model (SVS),
latent semantic analysis, non-negative matrix factoriza-
tion and latent Dirichlet alocation as individual models
with the best parameter setting (LSA, NMF, LDA) and as
mixtures (LSAMIX, NMFMIX, LDAMIX).
6 Results
Word Similarity Our results on word similar-
ity are summarized in Table 2. The simple co-
occurrence based vector space (SVS) performed best
with tf-idf weighting and the cosine similarity mea-
sure. With regard to LSA, we obtained best re-
sults with initial line normalization of the matrix,
K = 600 dimensions, and the scalar product sim-
ilarity measure while performing computations in
matrix U . Both NMF and LDA models are generally
better with a larger number of senses. NMF yields
best performance with K = 1000 dimensions and
the scalar product similarity measure. The best LDA
model also uses the scalar product, has K = 1200
topics, and ? set to 50K .
Following Reisinger and Mooney (2010), we also
evaluated mixture models that combine the output
of models with varying parameter settings. For both
NMF and LDA we averaged the similarity scores re-
turned by all runs. For comparison, we also present
an LSA mixture model over the (best) middle in-
terval K values. As can be seen, the LSA model
improves slightly, whereas NMF and LDA perform
worse than their best individual models.3 Overall,
we observe that NMF and LDA yield significantly
(p < 0.01) better correlations than LSA and the sim-
3It is difficult to relate our results to Reisinger and Mooney
(2010), due to differences in the training data and the vector rep-
resentations it gives rise to. As a comparison, a baseline config-
uration with tf-idf weighting and the cosine similarity measure
yields a correlation of 0.38 with our data and 0.49 in Reisinger
and Mooney (2010).
Model Kendall?s ?b
SVS 11.05
Add-SVS 12.74
Add-NMF 12.85
Add-LDA 12.33
Mult-SVS 14.41
Mult-NMF 13.20
Mult-LDA 12.90
Cont-NMF 14.95
Cont-LDA 13.71
Cont-NMFMIX 16.01
Cont-LDAMIX 15.53
Table 3: Results on lexical substitution using a simple
semantic space model (SVS), additive and multiplicative
compositional models with vector representations based
on co-occurrences (Add-SVS, Mult-SVS), NMF (Add-
NMF, Mult-NMF), and LDA (Add-LDA, Mult-LDA) and
contextualized models based on NMF and LDA with the
best parameter setting (Cont-NMF, Cont-LDA) and as
mixtures (Cont-NMFMIX, Cont-LDAMIX).
ple semantic space, both as individual models and as
mixtures.
Lexical Substitution Our results on lexical sub-
stitution are shown in Table 3. As a baseline we
also report the performance of the simple semantic
space that does not use any contextual information.
This model returns the same ranking of the substi-
tute candidates for each instance, based solely on
their similarity with the target word. This is a rel-
atively competitive baseline as observed by Erk and
Pado? (2008) and Thater et al (2009).
We report results with contextualized NMF and
LDA as individual models (the best word similar-
ity settings) and as mixtures (as described above).
These are in turn compared against additive and
multiplicative compositional models. We imple-
mented an additive model with pmi weighting and
Lin?s similarity measure which is defined in an ad-
ditive fashion. The multiplicative model uses tf-
idf weighting and cosine similarity, which involves
multiplication of vector components. Other combi-
nations of weighting schemes and similarity mea-
sures delivered significantly lower results. We also
report results for these models when using the NMF
and LDA reduced representations.
1168
Model Adv Adj Noun Verb
SVS 22.47 14.38 09.52 7.98
Add-SVS 22.79 14.56 11.59 10.00
Mult-SVS 22.85 16.37 13.59 11.60
Cont-NMFMIX 26.13 17.10 15.16 14.18
Cont-LDAMIX 21.21 16.00 16.31 13.67
Table 4: Results on lexical substitution for different parts
of speech with a simple semantic space model (SVS), two
compositional models (Add-SVS, Mult-SVS), and con-
textualized mixture models with NMF and LDA (Cont-
NMFMIX, Cont-LDAMIX), using Kendall?s ?b correlation
coefficient.
All models significantly (p < 0.01) outperform
the context agnostic simple semantic space (see
SVS in Table 3). Mixture NMF and LDA mod-
els are significantly better than all variants of com-
positional models (p < 0.01); the individual mod-
els are numerically better, however the difference
is not statistically significant. We also find that the
multiplicative model using a simple semantic space
(Mult-SVS) is the best performing compositional
model, thus corroborating the results of Mitchell and
Lapata (2009). Interestingly, dimensionality compo-
sitional models. This indicates that the better results
we obtain are due to the probabilistic formulation of
our contextualized model as a whole rather than the
use of NMF or LDA. Finally, we observe that the
Cont-NMF model is slightly better than Cont-LDA,
however the difference is not statistically significant.
To allow comparison with previous results re-
ported on this data set, we also used the General-
ized Average Precision (GAP, Kishida (2005)) as an
evaluation measure. GAP takes into account the or-
der of candidates ranked correctly by a hypothetical
system, whereas average precision is only sensitive
to their relative position. The best performing mod-
els are Cont-NMFMIX and Cont-LDAMIX obtaining
a GAP of 42.7% and 42.9%, respectively. Erk and
Pado? (2010) report a GAP of 38.6% on this data set
with their best model.
Table 4 shows how the models perform across dif-
ferent parts of speech. While verbs and nouns seem
to be most difficult, we observe higher gains from
the use of contextualized models. Cont-LDAMIX
obtains approximately 7% absolute gain for nouns
and Cont-NMFMIX approximately 6% for verbs. All
Senses Word Distributions
TRAFFIC (0.18) road, traffic, highway, route, bridge
MUSIC (0.04) music, song, rock, band, dance, play
FAN (0.04) crowd, fan, people, wave, cheer, street
VEHICLE (0.04) car, truck, bus, train, driver, vehicle
Table 5: Induced senses of jam and five most likely words
given these senses using an LDA model; sense probabili-
ties are shown in parentheses.
contextualized models obtain smaller improvements
for adjectives. For adverbs most models do not im-
prove over the no-context setting, with the exception
Cont-NMFMIX.
Finally, we also qualitatively examined how the
context words influence the sense distributions of
target words using examples from the lexical sub-
stitution dataset and the output of an individual
Cont-LDA model. In many cases, a target word
starts with a distribution spread over a larger number
of senses, while a context word shifts this distribu-
tion to one majority sense. Consider, for instance,
the target noun jam in the following sentence:
(1) With their transcendent, improvisational jams
and Mayan-inspired sense of a higher, meta-
physical purpose, the band?s music delivers a
spiritual sustenance that has earned them a very
devoted core following.
Table 5 shows the out-of-context senses activated
for jam together with the five most likely words as-
sociated with them.4 Sense probabilities are also
shown in parentheses. As can be seen, initially two
traffic-related and two music-related senses are acti-
vated, however with low probabilities. In the pres-
ence of the context word band, we obtain a much
more ?focused? distribution, in which the MUSIC
sense has 0.88 probability. The system ranks riff
and gig as the most likely two substitutes for jam.
The gold annotation also lists session as a possible
substitute.
In a large number of cases, the target is only par-
tially disambiguated by a context word and this is
also reflected in the resulting distribution. An ex-
4Sense names are provided by the authors in an attempt to
best describe the clusters (i.e., topics for LDA) to which words
are assigned.
1169
ample is the word bug which initially has a distribu-
tion triggering the SOFTWARE (0.09, computer, soft-
ware, microsoft, windows) and DISEASE (0.06, dis-
ease, aids, virus, cause) senses. In the context of
client, bug remains ambiguous between the senses
SECRET-AGENCY (0.34, agent, secret, intelligence,
FBI) ) and SOFTWARE (0.29):
(2) We wanted to give our client more than just a
list of bugs and an invoice ? we wanted to
provide an audit trail of our work along with
meaningful productivity metrics.
There are also cases where the contextualized dis-
tributions are not correct, especially when senses are
domain specific. An example is the word function
occurring in its mathematical sense with the context
word distribution. However, the senses that are trig-
gered by this pair all relate to the ?service? sense of
function. This is a consequence of the newspaper
corpus we use, in which the mathematical sense of
function is rare. We also see several cases where
the target word and one of the context words are as-
signed senses that are locally correct, but invalid in
the larger context. In the following example:
(3) Check the shoulders so it hangs well, stops at
hips or below, and make sure the pants are long
enough.
The pair (check, shoulder) triggers senses IN-
JURY (0.81, injury, left, knee, shoulder) and
BALL-SPORTS (0.10, ball, shot, hit, throw). How-
ever, the sentential context ascribes a meaning that
is neither related to injury nor sports. This suggests
that our models could benefit from more principled
context feature aggregation.
Generally, verbs are not as good context words
as nouns. To give an example, we often encounter
the pair (let, know), used in the common ?inform?
meaning. The senses we obtain for this pair, are,
however, rather uninformative general verb classes:
{see, know, think, do} (0.57) and {go, say, do,
can} (0.20). This type of error can be eliminated in
a space where context features are designed to best
reflect the properties of the target words.
7 Conclusions
In this paper we have presented a general frame-
work for computing similarity in context. Key in this
framework is the representation of word meaning as
a distribution over a set of global senses where con-
textualized meaning is modeled as a change in this
distribution. The approach is conceptually simple,
the same vector representation is used for isolated
words and words in context without being tied to a
specific sense induction method or type of semantic
space.
We have illustrated two instantiations of this
framework using non-negative matrix factorization
and latent Dirichlet alocation for inducing the la-
tent structure, and shown experimentally that they
outperform previously proposed methods for mea-
suring similarity in context. Furthermore, both of
them benefit from mixing model predictions over a
set of different parameter choices, thus making pa-
rameter tuning redundant.
The directions for future work are many and var-
ied. Conceptually, we have defined our model in an
asymmetric fashion, i.e., by stipulating a difference
between target words and contextual features. How-
ever, in practice, we used vector representations that
do not distinguish the two: target words and con-
textual features are both words. This choice was
made to facilitate comparisons with the popular bag-
of-words vector space models. However, differen-
tiating target from context representations may be
beneficial particularly when the similarity compu-
tations are embedded within specific tasks such as
the acquisition of paraphrases, the recognition of en-
tailment relations, and thesaurus construction. Also
note that our model currently contextualizes target
words with respect to individual contexts. Ideally,
we would like to compute the collective influence of
several context words on the target. We plan to fur-
ther investigate how to select or to better aggregate
the entire set of features extracted from a context.
Acknowledgments The authors acknowledge the
support of the DFG (Dinu; International Re-
search Training Group ?Language Technology and
Cognitive Systems?) and EPSRC (Lapata; grant
GR/T04540/01).
1170
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 112?119, Athens, Greece.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas, and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American So-
ciety for Information Science, 41:391?407.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Katrin Erk and Sabastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Honolulu, Hawaii.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92?
97, Uppsala, Sweden.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th Annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, New York, NY.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Machine Learn-
ing, 41(2):177?196.
Walter Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211?240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In NIPS,
pages 556?562.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):342?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768?774, Montre?al, Canada.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceedings
of the 22nd Annual Conference of the Cognitive Sci-
ence Society, pages 675?680, Philadelphia, PA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. In
Proceedings of SemEval, pages 48?53, Prague, Czech
Republic.
Scott McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 430?439, Suntec, Singa-
pore.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent Dirichlet alo-
cation. In Proceeding of the 14th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 569?577, New York, NY.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 109?
117, Los Angeles, California.
G Salton, A Wang, and C Yang. 1975. A vector-space
model for information retrieval. Journal of the Ameri-
can Society for Information Science, 18:613?620.
1171
Hinrich Schuetze. 1998. Automatic word sense discrim-
ination. Journal of Computational Linguistics, 24:97?
123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948?957, Uppsala,
Sweden.
Michael E. Tipping and Chris M. Bishop. 1999. Prob-
abilistic principal component analysis. Journal of the
Royal Statistical Society, Series B, 61:611?622.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics,
pages 947?953, Saarbru?cken, Germany.
1172
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 409?420,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning to Simplify Sentences with Quasi-Synchronous Grammar and
Integer Programming
Kristian Woodsend and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
k.woodsend@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Text simplification aims to rewrite text into
simpler versions, and thus make information
accessible to a broader audience. Most pre-
vious work simplifies sentences using hand-
crafted rules aimed at splitting long sentences,
or substitutes difficult words using a prede-
fined dictionary. This paper presents a data-
driven model based on quasi-synchronous
grammar, a formalism that can naturally
capture structural mismatches and complex
rewrite operations. We describe how such a
grammar can be induced from Wikipedia and
propose an integer linear programming model
for selecting the most appropriate simplifica-
tion from the space of possible rewrites gen-
erated by the grammar. We show experimen-
tally that our method creates simplifications
that significantly reduce the reading difficulty
of the input, while maintaining grammaticality
and preserving its meaning.
1 Introduction
Sentence simplification is perhaps one of the oldest
text rewriting problems. Given a source sentence,
the goal is to create a grammatical target that is
easier to read with simpler vocabulary and syntac-
tic structure. An example is shown in Table 1 in-
volving a broad spectrum of rewrite operations such
as deletion, substitution, insertion, and reordering.
The popularity of the simplification task stems from
its potential relevance to various applications. Ex-
amples include the development of reading aids for
people with aphasia (Carroll et al, 1999), non-native
Also contributing to the firmness in copper, the an-
alyst noted, was a report by Chicago purchasing
agents, which precedes the full purchasing agents re-
port that is due out today and gives an indication of
what the full report might hold.
Also contributing to the firmness in copper, the an-
alyst noted, was a report by Chicago purchasing
agents. The Chicago report precedes the full purchas-
ing agents report. The Chicago report gives an indica-
tion of what the full report might hold. The full report
is due out today.
Table 1: Example of a source sentence (top) and its sim-
plification (bottom).
speakers (Siddharthan, 2003) and more generally in-
dividuals with low literacy (Watanabe et al, 2009).
A simplification component could be also used as
a preprocessing step to improve the performance
of parsers (Chandrasekar et al, 1996), summarizers
(Beigman Klebanov et al, 2004) and semantic role
labelers (Vickrey and Koller, 2008).
Simplification is related to, but different from
paraphrase extraction (Barzilay, 2003). We must not
only have access to paraphrases (i.e., rewrite rules),
but also be able to combine them to generate new
text, in a simpler language. The task is also dis-
tinct from sentence compression as it aims to ren-
der a sentence more accessible while preserving its
meaning. On the contrary, compression unavoidably
leads to some information loss as it creates shorter
sentences without necessarily reducing complexity.
In fact, one of the commonest simplification oper-
ations is sentence splitting which usually produces
longer rather than shorter output! Moreover, mod-
409
els developed for sentence compression have been
mostly designed with one rewrite operation in mind,
namely word deletion, and are thus unable to model
consistent syntactic effects such as reordering, sen-
tence splitting, changes in non-terminal categories,
and lexical substitution (but see Cohn and Lapata
2008 and Zhao et al 2009 for notable exceptions).
In this paper we propose a sentence simplification
model that is able to handle structural mismatches
and complex rewriting operations. Our approach is
based on quasi-synchronous grammar (QG, Smith
and Eisner 2006), a formalism that is well suited for
text rewriting. Rather than postulating a strictly syn-
chronous structure over the source and target sen-
tences, QG identifies a ?sloppy? alignment of parse
trees assuming that the target tree is in some way
?inspired by? the source tree. Specifically, our model
is formulated as an integer linear program and uses
QG to capture the space of all possible rewrites.
Given a source tree, it finds the best target tree li-
censed by the grammar subject to constraints such
as sentence length and reading ease. Our model is
conceptually simple and computationally efficient.
Furthermore, it finds globally optimal simplifica-
tions without resorting to heuristics or approxima-
tions during the decoding process.
Contrary to most previous approaches (see the
discussion in Section 2) which rely heavily on
hand-crafted rules, our model learns simplifi-
cation rewrites automatically from examples of
source-target sentences. Our work joins others in us-
ing Wikipedia to extract data appropriate for model
training (Yamangil and Nelken, 2008; Yatskar et al,
2010; Zhu et al, 2010). Advantageously, the Sim-
ple English Wikipedia (henceforth SimpleEW) pro-
vides a large repository of simplified language; it
uses fewer words and simpler grammar than the or-
dinary English Wikipedia (henceforth MainEW) and
is aimed at non-native English speakers, children,
translators, people with learning disabilities or low
reading proficiency. We exploit Wikipedia and cre-
ate a (parallel) simplification corpus in two ways:
by aligning MainEW sentences to their SimpleEW
counterparts, and by extracting training instances
from SimpleEW revision histories, thus leveraging
Wikipedia?s collaborative editing process.
Our experimental results demonstrate that a sim-
plification model can be learned from Wikipedia
data alone without any manual effort. Perhaps un-
surprisingly, the quality of the QG grammar rules
greatly improves when these are learned from re-
vision histories which are less noisy than sentence
alignments. When compared against current state-
of-the-art methods (Zhu et al, 2010) our model
yields significantly simpler output that is both gram-
matical and meaning preserving.
2 Related Work
Sentence simplification has attracted a great deal
of attention due to its potential impact on society.
The literature is rife with attempts to simplify text
using mostly hand-crafted syntactic rules aimed at
splitting long and complicated sentences into sev-
eral simpler ones (Carroll et al, 1999; Chandrasekar
et al, 1996; Siddharthan, 2004; Vickrey and Koller,
2008). Other work focuses on lexical simplifications
and substitutes difficult words by more common
WordNet synonyms or paraphrases found in a pre-
defined dictionary (Devlin, 1999; Inui et al, 2003;
Kaji et al, 2002).
More recently, Yatskar et al (2010) explore
data-driven methods to learn lexical simplifications
from Wikipedia revision histories. A key idea in
their work is to utilize SimpleEW edits, while rec-
ognizing that these may serve other functions, such
as vandalism removal or introduction of new con-
tent. Zhu et al (2010) also use Wikipedia to learn
a sentence simplification model which is able to
perform four rewrite operations, namely substitu-
tion, reordering, splitting, and deletion. Inspired
by syntax-based SMT (Yamada and Knight, 2001),
their model consists of three components: a lan-
guage model P(s) whose role is to guarantee that the
simplification output is grammatical, a direct trans-
lation model P(s|c) capturing the probability that the
target sentence s is a simpler version of the source c,
and a decoder which searches for the simplifica-
tion s which maximizes P(s)P(s|c). The translation
model is the product of the aforementioned four
rewrite operations whose probabilities are estimated
from a parallel corpus of MainEW and SimpleEW
sentences using an expectation maximization algo-
rithm. Their decoder translates sentences into sim-
pler alternatives by greedily selecting the branch in
the source tree with the highest probability.
410
Our own work formulates sentence simplification
in the framework of Quasi-synchronous grammar
(QG, Smith and Eisner 2006). QG allows to describe
non-isomorphic tree pairs (the grammar rules can
comprise trees of arbitrary depth, and fragments can
be mapped) and is thus suited to text-rewriting tasks
which typically involve a number of local modifi-
cations to the input text. We use quasi-synchronous
grammar to learn a wide range of rewrite opera-
tions capturing both lexical and structural simplifi-
cations naturally without any additional rule engi-
neering. In contrast to Yatskar et al (2010) and Zhu
et al (2010), simplification operations (e.g., substi-
tution or splitting) are not modeled explicitly; in-
stead, we leave it up to our grammar extraction algo-
rithm to learn appropriate rules that reflect the train-
ing data. Compared to Zhu et al, our model is con-
ceptually simpler and more general. The proposed
ILP formulation not only allows to efficiently search
through the space of many QG rules but also to in-
corporate constraints relating to grammaticality and
the task at hand without the added computational
cost of integrating a language model. Furthermore,
our learning framework is not limited to simplifi-
cation and could be easily adapted to other rewrit-
ing tasks. Indeed, the QG formalism has been pre-
viously applied to parser adaptation and projection
(Smith and Eisner, 2009), paraphrase identification
(Das and Smith, 2009), question answering (Wang
et al, 2007), and title generation (Woodsend et al,
2010).
Finally, our work relates to a large body of recent
literature on Wikipedia and its potential for a wide
range of NLP tasks. Beyond text rewriting, examples
include semantic relatedness (Ponzetto and Strube,
2007), information extraction (Wu and Weld, 2010),
ontology induction (Nastase and Strube, 2008), and
the automatic creation of overview articles (Sauper
and Barzilay, 2009).
3 Sentence Simplification Model
Our model takes a single sentence as input and cre-
ates a version that is simpler to read. This may
involve rendering syntactically complex structures
simpler (e.g., through sentence splitting), or sub-
stituting rare words with more common words or
phrases (e.g., such that a second language learner
may be familiar with), or deleting elements of the
original text in order to produce a relatively sim-
pler and shallower syntactic structure. In addition,
the output must be grammatical and coherent. These
constraints are global in their scope, and cannot be
adequately satisfied by optimizing each one of them
individually. Our approach therefore uses an ILP
formulation which will provide a globally optimal
solution. Given an input sentence, our model decon-
structs it into component phrases and clauses, each
of which is simplified (lexically and structurally)
through QG rewrite rules. We generate all possible
simplifications for a given input and use the ILP to
find the best target subject to grammaticality con-
straints. In what follows we first detail how we ex-
tract QG rewrite rules as these form the backbone of
our model and then formulate the ILP proper.
3.1 Quasi-synchronous Grammar
Phrase alignment Our model operates on indi-
vidual sentences annotated with syntactic informa-
tion i.e., phrase structure trees. In our experiments,
we obtain this information from the Stanford parser
(Klein and Manning, 2003) but any other broadly
similar parser could be used instead. Given an input
sentence S1 or its parse tree T1, the QG constructs
a monolingual grammar for parsing, or generating,
possible translation trees T2. A grammar node in the
target tree T2 is modeled on a subset of nodes in the
source tree, with a rather loose alignment between
the trees.
We take aligned sentence pairs represented as
phrase structure trees and build up a list of leaf node
alignments based on lexical identity. We align direct
parent nodes where more than one child node aligns.
QG rules are created from aligned nodes above the
leaf node level if the all the nodes in the target tree
can be explained using nodes from the source. This
helps to improve the quality in what is inherently a
noisy process, and it is largely responsible for a rel-
atively small resulting grammar (see Table 2). Ex-
amples of phrase alignments (indicated with dotted
lines) are shown in Figure 1.
Syntactic simplification rules Each QG rule de-
scribes the transformations required from source to
target phrase sub-trees. It allows child (and possi-
bly grand-child) constituents to be deleted or re-
411
ST
.
.
VP
VP 2
NP 4
NNP
Mary
VBD 3
met
ADVP
RB
afterwards
CC
and
VP 1
NP
NN
dog
PRP$
his
VBD
walked
NP
NNP
Smith
NNP
John
ST (main)
.
.
VP 1
NP
NN
dog
PRP$
his
VBD
walked
NP
NNP
Smith
NNP
John
ST (aux)
.
.
VP 2
ADVP
RB
later
NP 4
NNP
Mary
VBD 3
met
NP
PRP
He
Rule involving lexical substitution:
?VP, VP? ? ?[ADVP [RB afterwards] VBD 3 NP 4 ], [VBD 3 NP 4 ADVP [RB later]]?
Rule for splitting into main constituent and auxiliary sentence:
?VP, VP, ST? ? ?[VP 1 and VP 2 ], [VP 1 ], [NP [PRP He] VP 2 .]?
Figure 1: A source sentence (upper tree) is split into two sentences. Dotted lines show word alignments, while boxed
subscripts show aligned nodes used to form QG rules. Below, two QG rules learned from this data.
ordered, and for nodes to be flattened. In addition,
we allow insertion of punctuation and some func-
tion words, identified by a small set of POS tags. To
distinguish sentences proper (which have final punc-
tuation) from clauses, we modify the output of the
parser, changing the root sentence parse tag from S
to ST (a ?top-level sentence?); this allows clauses to
be extracted and rewritten as stand-alone sentences.
Lexical simplification rules Lexical substitutions
are an important part of simplification. We learn
them from aligned sub-trees, in the same way as
described above for syntax rules, by allowing a
small number of lexical substitutions to be present
in the rules, and provided they do not include proper
nouns. The resulting QG rules could be applied
by matching the syntax of the whole sub-tree sur-
rounding the substitution, but this approach is overly
restrictive and suffers from data sparsity. Indeed,
Yatskar et al (2010) learn lexical simplifications
without taking syntactic context into account. We
therefore add a post-processing stage to the learning
process. For rules where the syntactic structures of
the source and target sub-trees match, and the only
difference is a lexical substitution, we construct a
more general rule by extracting the words and cor-
responding POS tags involved in the substitution.
Then at the generation stage, identifying suitable
rules depends only on the substitution words, rather
than the surrounding syntactic context. An example
of a lexical substitution rule is shown in Figure 1.
Sentence splitting rules Another important sim-
plification technique is to split syntactically compli-
cated sentences into several shorter ones. To learn
QG rules for this operation, the source sentence is
aligned with two consecutive target sentences.
Rather than expecting to discover a split point in
the source sentence, we attempt to identify a node
in the source parse tree that contributes to both of
the two target sentences. Our intuition is that one
of the target sentences will follow the general syn-
tactic structure of the source sentence. We designate
this as the main sentence. A node in the source sen-
tence parse tree will be aligned with a (similar but
simpler) node in the main target sentence, but at the
same time it will fully explain the other target sen-
tence, which we term the auxiliary sentence. It is
412
possible for the auxiliary sentence to come before or
after the main sentence. In the learning procedure,
we try both possible orderings, and record the order
in any QG rules successfully produced.
The resulting QG rule is a tuple of three phrase
structure elements: the source node, the node in the
target main sentence (the top level of this node is
typically the same as that of the source node), and
the phrase structure of the entire auxiliary sentence.1
In addition, there is a flag to indicate if the auxiliary
sentence comes before or after the main sentence.
This formalism is able to capture the operations re-
quired to split sentences containing coordinate or
subordinate clauses, parenthetical content, relative
clauses and apposition. An example of a sentence
splitting rule is illustrated in Figure 1.
3.2 ILP-based Generation
We cast the problem of finding a suitable target sim-
plification given a source sentence as an integer lin-
ear program (ILP). Specifically, simplified text is
created from source sentence parse trees by identi-
fying and applying QG grammar rules. These will
have matching structure and may also require lexical
matching (shown using italics in the example rules
in Figure 1). The generation process starts at the root
node of the parse tree, applying QG rules to sub-
trees until leaf nodes are reached. We do not use the
Bayesian probability model proposed by Smith and
Eisner (2006) to identify the best sequence of sim-
plification rules. Instead, where there is more than
one matching rule, and so more than one simplifi-
cation is possible, the alternatives are all generated
and incorporated into the target phrase structure tree.
The ILP model operates over this phrase structure
tree and selects the phrase nodes from which to form
the target output.
Applying the QG rules on the source sentence
generates a number of auxiliary sentences. Let S be
this set of sentences. Let P be the set of nodes in the
phrase structure trees of the auxiliary sentences, and
Ps ? P be the set of nodes in each sentence s ? S .
Let the sets Di ? P , ?i ? P capture the phrase de-
pendency information for each node i, where each
set Di contains the nodes that depend on the pres-
1Note that the target component comprises the second and
third elements as a pair, and variables from the source compo-
nent are split between them.
ence of i. In a similar fashion, the sets Ai? S , ?i?P
capture the indices of any auxiliary sentences that
depend on the presence of node i. C ? P is the set
of nodes involving a choice of alternative simplifi-
cations (nodes in the tree where more than one QG
rewrite rule can be applied, as mentioned above);
Ci ? P , i ? C are the sets of nodes that are direct
children of each such node, in other words they are
the individual simplifications. Let l(w)i be the length
of each node i in words, and l(sy)i its length in syl-
lables. As we shall see below counts of words and
syllables are important cues in assessing readability.
The model is cast as an binary integer linear
program. A vector of binary decision variables
x ? {0,1}|P | indicates if each node is to be part of
the output. A vector of auxiliary binary variables
y ? {0,1}|S | indicates which (auxiliary) sentences
have been chosen.
maxx ?i?P gixi +hw +hsy (1a)
s.t. x j? xi ?i ? P , j ?Di (1b)
xi? ys ?i ? P ,s ? Ai (1c)
xi? ys ?s ? S , i ? Ps (1d)
?
j?Ci
x j = xi ?i ? C , j ? Ci (1e)
?
s?S
yi ? 1 (1f)
xi ? {0,1} ?i ? P (1g)
ys ? {0,1} ?s ? S . (1h)
Our objective function, given in Equation (1a),
is the summation of local and global compo-
nents. Each phrase is locally given a rewrite
penalty gi, where common lexical substitutions,
rewrites and simplifications are penalized less (as
we trust them more), compared to rarer QG rules.
The penalty is a simple log-probability measure,
gi = log
(
nrNr
)
, where nr is the number of times the
QG rule r was seen in the training data, and Nr
the number of times all suitable rules for this
phrase node were seen. If no suitable rules exist, we
set gi = 0.
The other two components of the objective,
hw and hsy, are global in nature, and guide the ILP
413
towards simpler language. They draw inspiration
from existing measures of readability (the ease with
which a document can be read and understood).
The primary aim of readability formulas is to assess
whether texts or books are suitable for students at
particular grade levels or ages (see Mitchell 1985 for
an overview). Intuitively, texts ought to be simpler if
they correspond to low reading levels. A commonly
used reading level measure is the Flesch-Kincaid
Grade Level (FKGL) index which estimates read-
ability as a combination of the average number of
syllables per word and the average number of words
per sentence. Unfortunately, this measure is non-
linear2 and cannot be incorporated directly into the
objective of the ILP. Instead, we propose a linear ap-
proximation. We provide the ILP with targets for the
average number of words per sentence (wps), and
syllables per word (spw). hw(x,y) then measures the
number of words below this target level that the ILP
has achieved:
hw(x,y) = wps??
i?S
yi??
i?P
l(w)i xi.
When positive, this indicates that sentences are
shorter than target, and contributes positively to the
readability objective whilst encouraging the appli-
cation of sentence splitting and deletion-based QG
rules. Similarly, hsy(x,y) measures the number of
syllables below that expected, from the target aver-
age and the number of words the ILP has chosen:
hsy(x) = spw??
i?P
l(w)i xi??
i?P
l(sy)i xi.
This component of the objective encourages the
deletion or lexical substitution of complex words.
We can use the two target parameters (wps and spw)
to control how much simplification the ILP should
apply.
Constraint (1b) enforces grammatical correctness
by ensuring that the phrase dependencies are re-
spected and the resulting structure is a tree. Phrases
that depend on phrase i are contained in the set Di.
Variable xi is true, and therefore phrase i will be
included in the target output, if any of its depen-
dents x j ?Di are true.3 Constraint (1c) links main
2FKGL = 0.39
( total wordstotal sentences
)
+1.8
( total syllables
total words
)
?15.59
3Constraints (1b), (1c) and (1d) are shown as dependencies
for clarity, but they were implemented as inequalities in the ILP.
phrases to auxiliary sentences, so that the latter can
only be included in the output if the main phrase
has also been chosen. This helps to control coher-
ence within the output text. Despite seeming similar
to (1c), the role of constraint (1d) is quite different.
It links phrase variables x to sentence variables y, to
ensure the logical integrity of the model is correct.
Where the QG provides alternative simplifications,
it makes sense of course to select only one. This is
controlled by constraint (1e), and by placing all al-
ternatives in the set Di for the node i.
With these constraints alone, and faced with a
source sentence that is particularly difficult to sim-
plify, it is possible for the ILP solver to return a ?triv-
ial? solution of no output at all, as all other avail-
able solutions result in a negative objective value.
It is therefore necessary to impose a global mini-
mum output constraint (1f). In combination with the
dependency relations in (1c), this constraint ensures
that at least an element of the root sentence is present
in the output. Global maximum length constraints
are a frequently occurring aspect of ILP models used
in NLP applications. We decided not to incorporate
any such constraints into our model, as we did not
want to place limitations on the simplification of
original content.
4 Experimental Setup
In this section we present our experimental setup
for assessing the performance of the simplification
model described above. We give details on the cor-
pora and grammars we used, model parameters, the
systems used for comparison with our approach, and
explain how the output was evaluated.
Grammar Extraction QG rules were learned
from revision histories and an aligned simplifica-
tion corpus, which we obtained from snapshots4 of
MainEW and SimpleEW. Wiki-related mark-up and
meta-information was removed to extract the plain
text from the articles.
SimpleEW revisions not only simplify the text of
existing articles, they may also introduce new con-
tent, vandalize or remove vandalism, or perform nu-
merous automatic ?house-keeping? modifications.
4The snapshots for MainEW (enwiki) and SimpleEW (sim-
plewiki dated 2010-09-16 and 2010-09-13, respectively (both
available from http://download.wikimedia.org/).
414
Corpora Syntactic Lexical Splitting
Revision 316 269 184
Aligned 312 96 254
Table 2: Number of QG rules extracted (after removing
singletons) from revision-based and aligned corpora.
We identified suitable revisions for simplification by
selecting those where the author had mentioned a
keyword (such as simple, clarification or grammar)
in the revision comments. Each selected revision
was compared to the previous version. Because the
entire article is stored at each revision, we needed to
identify and align modified sentences. We first iden-
tified modified sections using the Unix diff pro-
gram, and then individual sentences within the sec-
tions were aligned using the program dwdiff5. This
resulted in 14,831 paired sentences. With regard to
the aligned simplification corpus, we paired 15,000
articles from SimpleEW and MainEW following the
language link within the snapshot files. Within the
paired articles, we identified aligned sentences us-
ing macro alignment (at paragraph level) then mi-
cro alignment (at sentence level), using tf.idf scores
to measure similarity (Barzilay and Elhadad, 2003;
Nelken and Schieber, 2006).
All source-target sentences (resulting from revi-
sions or alignments) were parsed with the Stanford
parser (Klein and Manning, 2003) in order to la-
bel the text with syntactic information. QG rules
were created by aligning nodes in these sentences
as described earlier. A breakdown of the number
and type of rules we obtained from the revision
and aligned corpora (after removing rules appear-
ing only once) is given in Table 2. Examples of the
most frequently learned QG rules are shown in Ta-
ble 3. Rules (1)?(3) involve syntactic simplification
and rules (4)?(6) involve sentence splitting. Exam-
ples of common lexical simplifications found by our
grammar are: ?discovered? ? ?found?, ?defeated?
? ?won against?, ?may refer to?? ?could mean?,
?original?? ?first?, ?requires?? ?needs?.
Sentence generation We generated simplified
versions of MainEW sentences. For each (parsed)
source sentence, we created and solved an ILP (see
Equation (1)) parametrized as follows: the number
5http://os.ghalkes.nl/dwdiff.html
1. ?S, ST? ? ?[NP 1 VP 2 ], [NP 1 VP 2 .]?
2. ?S, ST? ? ?[VP 1 ], [This VP 1 .]?
3. ?NP, ST? ? ?[NP 1 , NP 2 ], [NP 1 was VP 2 .]?
4. ?ST, ST, ST? ? ?[S 1 , and S 2 ], [ST 1 ], [ST 2 ]?
5. ?ST, ST, ST? ? ?[S 1 : S 2 ], [ST 1 ], [ST 2 ]?
6. ?ST, ST, ST? ? ?[S 1 , but S 2 ], [ST 1 ], [ST 2 ]?
Table 3: Examples of QG rules involving syntactic sim-
plification (1)?(3) and sentence division (4)?(6). The lat-
ter are shown as the tuple ?source, target, aux?. The trans-
form of nodes from S to ST (for example) rely on the
application of syntactic simplification rules rules. Boxed
subscripts show aligned nodes.
of target words per sentence (wps) was set to 8, and
syllables per word (spw) to 1.5. These two param-
eters were empirically tuned on the training set. To
solve the ILP model we used the ZIB Optimization
Suite software (Achterberg, 2007; Koch, 2004). The
solution was converted into a sentence by removing
nodes not chosen from the tree representation, then
concatenating the remaining leaf nodes in order.
Evaluation We evaluated our model on the same
dataset used in Zhu et al (2010), an aligned cor-
pus of MainEW and SimpleEW sentences. The cor-
pus contains 100/131 source/target sentences and
was created automatically. Sentences from this cor-
pus (and their revisions) were excluded from train-
ing. We evaluated two versions of our model, one
with rewrite rules acquired from revision histories
of simplified documents and another one with rules
extracted from MainEW-SimpleEW aligned sen-
tences. These models were compared against Zhu
et al (2010)6 who also learn simplification rules
from Wikipedia, and a simple baseline that uses
solely lexical simplifications7 provided by the Sim-
pleEW editor ?SpencerK? (Spencer Kelly). An obvi-
ous idea would be to treat sentence simplification as
an English-to-English translation problem and use
an off-the-shelf system like Moses8 for the task.
However, we refrained from doing so as Zhu et al
(2010) show that Moses performs poorly, it cannot
model rewrite operations that split sentences or drop
words and in most cases generates output identical
6We are grateful to Zhemin Zhu for providing us with his
test set and the output of his system.
7http://www.spencerwaterbed.com/soft/simple/
8http://www.statmt.org/moses/
415
MainEW Wonder has recorded several critically acclaimed albums and hit singles, and writes and produces songs
for many of his label mates and outside artists as well.
Zhu et alWonder has recorded several praised albums and writes and produces songs. Many of his label mates
and outside artists as well.
AlignILP Wonder has recorded several critically acclaimed albums and hit singles. He produces songs for many
of his label mates and outside artists as well. He writes.
RevILP Wonder has recorded many critically acclaimed albums and hit singles. He writes. He makes songs for
many of his label mates and outside artists as well.
SimpleEW He has recorded 23 albums and many hit singles, and written and produced songs for many of his label
mates and other artists as well.
MainEW The London journeys In 1790, Prince Nikolaus died and was succeeded by a thoroughly unmusical
prince who dismissed the entire musical establishment and put Haydn on a pension.
Zhu et alThe London journeys in 1790, prince Nikolaus died and was succeeds by a son became prince. A son
became prince told the entire musical start and put he on a pension.
AlignILP The London journeys In 1790, Prince Nikolaus died. He was succeeded by a thoroughly unmusical
prince. He dismissed the entire musical establishment. He put Haydn on a pension.
RevILP The London journeys In 1790, Prince Nikolaus died. He was succeeded by a thoroughly unmusical
prince. He dismissed the whole musical establishment. He put Haydn on a pension.
SimpleEW The London journeys In 1790, Prince Nikolaus died and his son became prince. Haydn was put on a
pension.
Table 4: Example simplifications produced by the systems in this paper (RevILP, AlignILP) and Zhu et al?s (2010)
model, compared to real Wikipedia text (MainEW: input source, SimpleEW: simplified target).
to the source.
We evaluated model output in two ways, using au-
tomatic evaluation measures and human judgments.
Intuitively, readability measures ought to be suit-
able for assessing the output of simplification sys-
tems. We report results with the well-known Flesch-
Kincaid Grade Level index (FKGL). Experiments
with other readability measures such as the Flesch
Reading Ease and the Coleman-Liau index obtained
similar results. In addition, we also assessed how the
system output differed from the human SimpleEW
gold standard by computing BLEU (Papineni et al,
2002) and TERp (Snover et al, 2009). Both mea-
sures are commonly used to automatically evaluate
the quality of machine translation output. BLEU9
scores the target output by counting n-gram matches
with the reference, whereas TERp is similar to word
error rate, the only difference being that it allows
shifts and thus can account for word order differ-
ences. TERp also allows for stem, synonym, and
paraphrase substitutions which are common rewrite
operations in simplification.
In line with previous work on text rewriting
(e.g., Knight and Marcu 2002) we also evaluated
9We calculated single-reference BLEU using the mteval-
v13a script (with the default settings).
system output by eliciting human judgments. We
conducted three experiments. In the first experi-
ment participants were presented with a source sen-
tence and its target simplification and asked to rate
whether the latter was easier to read compared to the
source. In the second experiment, they were asked
to rate the grammaticality of the simplified output.
In the third experiment, they judged how well the
simplification preserved the meaning of the source.
In all experiments participants used a five point rat-
ing scale where a high number indicates better per-
formance. We randomly selected and automatically
simplified 64 sentences from Zhu et al?s (2010) test
corpus using the four models described above. We
also included gold standard simplifications. Our ma-
terials thus consisted of 320 (64 ? 5) source-target
sentences.10 We collected ratings from 45 unpaid
volunteers, all self reported native English speakers.
The studies were conducted over the Internet using
a custom built web interface. Examples of our ex-
perimental items are given in Table 4 (we omit the
output of SpencerK as this is broadly similar to the
source sentence, modulo lexical substitutions).
10A Latin square design ensured that subjects did not see two
different simplifications of the same sentence.
416
Models FKGL BLEU TERP
MainEW 15.12 ? ?
SimpleEW 11.25 ? ?
SpencerK 14.67 0.47 0.51
Zhu et al9.41 0.38 0.59
RevILP 10.92 0.42 0.60
AlignILP 12.36 0.34 0.85
Table 5: Model performance using automatic evaluation
measures.
5 Results
The results of our automatic evaluation are summa-
rized in Table 5. The first column reports the FKGL
readability index of the source sentences (MainEW),
of their target simplifications (SimpleEW) and the
output of four models: a simple baseline that re-
lies on lexical substitution (SpencerK), Zhu et al?s
(2010) model, and two versions of our model, one
trained on revision histories (RevILP) and another
one trained on the MainEW-SimpleEW aligned cor-
pus (AlignILP). As can be seen, the source sentences
have the highest reading level. Zhu et al?s system
has the lowest reading level followed by our own
models and SpencerK. All models are significantly11
different in reading level from SimpleEW with the
exception of RevILP (using a one-way ANOVA with
post-hoc Tukey HSD tests). SpencerK is not signif-
icantly different in readability from MainEW; Re-
vILP is significantly different from Zhu et al and
AlignILP. In sum, these results indicate that RevILP
is the closest to SimpleEW and that the provenance
of the QG rules has an impact on the model?s perfor-
mance.
Table 5 also shows BLEU and TERp scores with
SimpleEW as the reference. These scores can be
used to examine how close to the gold standard our
models are. SpencerK has the highest BLEU and
lowest TERp scores.12 This is expected as this base-
line performs only a very limited type of rewriting,
namely lexical substitution. AlignILP is most differ-
ent from the reference, followed by Zhu et al (2010)
and RevILP. Taken together these results indicate
11All significance differences reported throughout this paper
are with a level less than 0.01.
12The perfect BLEU score is one and the perfect TERp score
is zero.
Models Simplicity Grammaticality Meaning
SimpleEW 3.74 4.89 4.41
SpencerK 1.41 4.87 4.84
Zhu et al2.92 3.43 3.44
RevILP 3.64 4.55 4.19
AlignILP 2.69 4.03 3.98
Table 6: Average human ratings for gold standard Sim-
pleEW sentences, a simple baseline (SpencerK) based on
lexical substitution, Zhu et al?s 2010 model, and two ver-
sions of our ILP model (RevILP and AlignILP).
Zhu et alAlignILP RevILP SimpleEW
SpencerK 2?4 2?4 24 24
Zhu et al?4 2?4 2?4
AlignILP 2?N 2?4
RevILP N
Table 7: 2/: is/not sig. diff. wrt simplicity; ?/: is/not
sig. diff. wrt grammaticality; 4/N: is/not sig. diff. wrt
meaning.
that the ILP models perform a fair amount of rewrit-
ing without simply rehashing the source sentence.
We now turn to the results of our judgment elic-
itation study. Table 6 reports the average ratings
for Simplicity (is the target sentence simpler than
the source?), Grammaticality (is the target sentence
grammatical?), and Meaning (does the target pre-
serve the meaning of the source?). With regard to
simplicity, our participants perceive the gold stan-
dard (SimpleEW) to be the simplest, followed by
RevILP, Zhu et al and AlignILP. SpencerK is the
least simple model and the most grammatical one
as lexical substitutions do not change the structure
of the sentence. Interestingly, RevILP and AlignILP
are also rated highly with regard to grammaticality.
Zhu et al (2010) is the least grammatical model.
Finally, RevILP preserves the meaning of the tar-
get as well as SimpleEW, whereas Zhu et al yields
the most distortions. Again SpencerK is rated highly
amongst the other models as it is does not substan-
tially simplify and thus change the meaning of the
source.
Table 7 reports on pairwise comparisons between
all models and their statistical significance (again us-
ing a one-way ANOVA with post-hoc Tukey HSD
tests). RevILP is not significantly different from
SimpleEW on any dimension (Simplicity, Grammat-
417
Original story: There was once a sweet little maid who lived with her father and mother in a pretty little
cottage at the edge of the village. At the further end of the wood was another pretty cottage and in it lived
her grandmother. Everybody loved this little girl, her grandmother perhaps loved her most of all and gave
her a great many pretty things. Once she gave her a red cloak with a hood which she always wore, so people
called her Little Red Riding Hood.
Generated simplification: There was once a sweet little maid. She lived with her father and mother in
a pretty little cottage at the edge of the village. At the further end of the wood it lived her grandmother.
Everybody loved this little girl. Her grandmother perhaps loved her most of all. She gave her a great many
pretty things. Once she gave her a red cloak with a hood, so persons called her Little Red Riding Hood.
Table 8: Excerpt of Little Red Riding Hood simplified by the RevILP model. Modifications to the original story are
highlighted in italics.
icality, Meaning), whereas Zhu et al differs signif-
icantly from RevILP and SimpleEW on all dimen-
sions. It is also significantly different from Alig-
nILP in terms of grammaticality and meaning but
not simplicity. RevILP is significantly more simple
and grammatical than AlignILP but performs com-
parably with respect to preserving the meaning of
the source.
In sum, our results show that RevILP is the best
performing model. It creates sentences that are sim-
ple, grammatical and adhere to the meaning of
the source. The QG rules obtained from the revi-
sion histories produce better output compared to the
aligned corpus. As revision histories are created by
Wikipedia contributors, they tend to be a more ac-
curate data source than aligned sentences which are
obtained via an automatic and unavoidably noisy
procedure. Our results also show that a more gen-
eral model not restricted to specific rewrite opera-
tions like Zhu et al (2010) obtains superior results
and has better coverage.
We also wanted to see whether a simplification
model trained on Wikipedia could be applied to an-
other domain. To this end, we used RevILP to sim-
plify five children stories from the Gutenburg13 col-
lection. The model simplified one sentence at a time
and was ran with the Wikipedia settings without any
modification. The mean FKGL on the simplified sto-
ries was 3.78. compared to 7.04 for the original ones.
An example of our system?s output on Little Red
Riding Hood is shown in Table 8.
Possible extensions and improvements to the cur-
rent model are many and varied. We have presented
an all-purpose simplification model without a target
13http://www.gutenberg.org
audience or application in mind. An interesting re-
search direction would be to simplify text accord-
ing to readability levels or text genres (e.g., news-
paper vs literary text). We could do this by incorpo-
rating readability-specific constraints to the ILP or
by changing the objective function (e.g., by favoring
more domain-specific rules). Finally, we would like
to extend the current model so as to simplify entire
documents both in terms of style and content.
Acknowledgments We are grateful to Lillian Lee
whose invited talk at CoNLL-2010 inspired this re-
search. We would also like to thank the members of
the Probabilistic Models of Language group at the
School of Informatics for valuable discussions and
comments. We acknowledge the support of EPSRC
through project grant EP/F055765/1.
References
Achterberg, Tobias. 2007. Constraint Integer Pro-
gramming. Ph.D. thesis, Technische Universita?t
Berlin.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization: Paraphrasing
and Generation. Ph.D. thesis, Columbia Univer-
sity.
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Sapporo, Japan, pages 25?32.
Beigman Klebanov, Beata, Kevin Knight, and
Daniel Marcu. 2004. Text simplification for
information-seeking applications. In Proceed-
ings of Ontologies, Dabases, and Applications of
Semantics (ODBASE) International Conference.
418
Springer, Agia Napa, Cyprus, volume 3290 of
Lecture Notes in Computer Science, pages 735?
747.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and John Tait.
1999. Simplifying text for language-impaired
readers. In Proceedings of the 9th Conference of
the European Chapter of the ACL. Bergen, Nor-
way, pages 269?270.
Chandrasekar, Raman, Christine Doran, and Ban-
galore Srinivas. 1996. Motivations and meth-
ods for text simplification. In Proceedings of the
16th International Conference on Computational
Linguistics. Copenhagen, Denmark, pages 1041?
1044.
Cohn, Trevor and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Pro-
ceedings of the 22nd International Conference
on Computational Linguistics. Manchester, UK,
pages 137?144.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the ACL-IJCNLP.
Suntec, Singapore, pages 468?476.
Devlin, Siobhan. 1999. Simplifying Natural Lan-
guage for Aphasic Readers. Ph.D. thesis, Univer-
sity of Sunderland.
Inui, Kentaro, Atsushi Fujita, Tetsuro Takahashi,
Ryu Iida, and Tomoya Iwakura. 2003. Text sim-
plification for reading assistance: A project note.
In Proceedings of the Second International Work-
shop on Paraphrasing. Association for Computa-
tional Linguistics, Sapporo, Japan, pages 9?16.
Kaji, Nobuhiro, Daisuke Kawahara, Sadao Kuro-
hashi, and Satoshi Sato. 2002. Verb paraphrase
based on case frame alignment. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics. Association for Compu-
tational Linguistics, Philadelphia, Pennsylvania,
USA, pages 215?222.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting of the Association of
Computational Linguistics. Sapporo, Japan, pages
423?430.
Knight, Kevin and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artificial
Intelligence 139(1):91?107.
Koch, Thorsten. 2004. Rapid Mathematical Pro-
totyping. Ph.D. thesis, Technische Universita?t
Berlin.
Mitchell, James V. 1985. The Ninth Mental Mea-
surements Year-book. University of Nebraska
Press, Lincoln, Nebraska.
Nastase, Vivi and Michael Strube. 2008. Decoding
Wikipedia categories for knowledge acquisition.
In Proceedings of the 23rd Conference on Artifi-
cial Intelligence. pages 1219?1224.
Nelken, Rani and Stuart Schieber. 2006. Towards
robust context-sensitive sentence alignment for
monolingual corpora. In Proceedings of the 11th
Conference of the European Chapter of the As-
sociation for Computational Linguistics. Trento,
Italy, pages 161?168.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th ACL. Philadelphia, PA, pages
311?318.
Ponzetto, Simone Paolo and Michael Strube. 2007.
Knowledge derived from Wikipedia for comput-
ing semantic relatedness. Journal of Artificial In-
telligence Research 30:181?212.
Sauper, Christina and Regina Barzilay. 2009. Au-
tomatically generating Wikipedia articles: A
structure-aware approach. In Proceedings of the
Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the
AFNLP. Association for Computational Linguis-
tics, Suntec, Singapore, pages 208?216.
Siddharthan, Advaith. 2003. Syntactic Simplifica-
tion and Text Cohesion. Ph.D. thesis, University
of Cambridge, University of Cambridge.
Siddharthan, Advaith. 2004. Syntactic simplifica-
tion and text cohesion. in research on language
and computation. Research on Language and
Computation 4(1):77?109.
Smith, David and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft pro-
jection of syntactic dependencies. In Proceedings
on the Workshop on Statistical Machine Transla-
419
tion. Association for Computational Linguistics,
New York City, pages 23?30.
Smith, David A. and Jason Eisner. 2009. Parser
adaptation and projection with quasi-synchronous
grammar features. In Proceedings of the EMNLP.
Suntec, Singapore, pages 822?831.
Snover, Matthew, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or HTER? Exploring different human judgments
with a tunable MT metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion. Athens, Greece, pages 259?268.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT . Association for Com-
putational Linguistics, Columbus, Ohio, pages
344?352.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings
of the EMNLP-CoNLL. Prague, Czech Republic,
pages 22?32.
Watanabe, Willian Massami, Arnaldo Candido Ju-
nior, Vin??cius Rodriguez de Uze?da, Renata Pon-
tin de Mattos Fortes, Thiago Alexandre Salgueiro
Pardo, and Sandra Maria Alu?sio. 2009. Facilita:
reading assistance for low-literacy readers. In
Proceedings of the 27th ACM International Con-
ference on Design of Communication. Blooming-
ton, IN.
Woodsend, Kristian, Yansong Feng, and Mirella
Lapata. 2010. Title generation with quasi-
synchronous grammar. In Proceedings of the
2010 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Com-
putational Linguistics, Cambridge, MA, pages
513?523.
Wu, Fei and Daniel S. Weld. 2010. Open infor-
mation extraction using Wikipedia. In Proceed-
ings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics. Association
for Computational Linguistics, Uppsala, Sweden,
pages 118?127.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceed-
ings of 39th Annual Meeting of the Association
for Computational Linguistics. Toulouse, France,
pages 523?530.
Yamangil, Elif and Rani Nelken. 2008. Mining
Wikipedia revision histories for improving sen-
tence compression. In Proceedings of ACL-08:
HLT, Short Papers. Association for Computa-
tional Linguistics, Columbus, Ohio, pages 137?
140.
Yatskar, Mark, Bo Pang, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2010. For
the sake of simplicity: Unsupervised extrac-
tion of lexical simplifications from Wikipedia.
In Proceedings of the Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics. pages 365?368.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li.
2009. Application-driven statistical paraphrase
generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP. Singapore,
pages 834?842.
Zhu, Zhemin, Delphine Bernhard, and Iryna
Gurevych. 2010. A monolingual tree-based trans-
lation model for sentence simplification. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics. Beijing, China, pages
1353?1361.
420
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320?1331,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Semantic Role Induction with Graph Partitioning
Joel Lang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we present a method for unsuper-
vised semantic role induction which we for-
malize as a graph partitioning problem. Ar-
gument instances of a verb are represented as
vertices in a graph whose edge weights quan-
tify their role-semantic similarity. Graph par-
titioning is realized with an algorithm that it-
eratively assigns vertices to clusters based on
the cluster assignments of neighboring ver-
tices. Our method is algorithmically and con-
ceptually simple, especially with respect to
how problem-specific knowledge is incorpo-
rated into the model. Experimental results on
the CoNLL 2008 benchmark dataset demon-
strate that our model is competitive with other
unsupervised approaches in terms of F1 whilst
attaining significantly higher cluster purity.
1 Introduction
Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the se-
mantic relations that hold between a predicate and
its arguments (e.g., ?who? did ?what? to ?whom?,
?when?, ?where?, and ?how?) abstracting over sur-
face syntactic configurations.
In the example sentences below, window occu-
pies different syntactic positions ? it is the object of
broke in sentences (1a,b), and the subject in (1c) ?
while bearing the same semantic role, i.e., the phys-
ical object affected by the breaking event. Analo-
gously, ball is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).
(1) a. [Jim]A0 broke the [window]A1 with a
[ball]A2.
b. The [ball]A2 broke the [window]A1.
c. The [window]A1 broke [last night]TMP.
The semantic roles in the examples are labeled in
the style of PropBank (Palmer et al, 2005), a broad-
coverage human-annotated corpus of semantic roles
and their syntactic realizations. Under the Prop-
Bank annotation framework (which we will assume
throughout this paper) each predicate is associated
with a set of core roles (named A0, A1, A2, and so
on) whose interpretations are specific to that predi-
cate1 and a set of adjunct roles such as location or
time whose interpretation is common across predi-
cates (e.g., last night in sentence (1c)).
The availability of PropBank and related re-
sources (e.g., FrameNet; Ruppenhofer et al (2006))
has sparked the development of great many seman-
tic role labeling systems most of which conceptu-
alize the task as a supervised learning problem and
rely on role-annotated data for model training. Most
of these systems implement a two-stage architec-
ture consisting of argument identification (determin-
ing the arguments of the verbal predicate) and ar-
gument classification (labeling these arguments with
semantic roles). Despite being relatively shallow, se-
1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).
1320
mantic role analysis has the potential of benefiting a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al, 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al, 2005).
Current approaches have high performance ? a
system will recall around 81% of the arguments cor-
rectly and 95% of those will be assigned a cor-
rect semantic role (see Ma`rquez et al (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al, 2008). Unfortu-
nately, the reliance on role-annotated data which is
expensive and time-consuming to produce for every
language and domain, presents a major bottleneck to
the widespread application of semantic role labeling.
In this paper we argue that unsupervised meth-
ods offer a promising yet challenging alternative. If
successful, such methods could lead to significant
savings in terms of annotation effort and ultimately
yield more portable semantic role labelers that re-
quire overall less engineering effort. Our approach
formalizes semantic role induction as a graph parti-
tioning problem. Given a verbal predicate, it con-
structs a weighted graph whose vertices correspond
to argument instances of the verb and whose edge
weights quantify the similarity between these in-
stances. The graph is partitioned into vertex clus-
ters representing semantic roles using a variant of
Chinese Whispers, a graph-clustering algorithm pro-
posed by Biemann (2006). The algorithm iteratively
assigns cluster labels to graph vertices by greedily
choosing the most common label amongst the neigh-
bors of the vertex being updated. Beyond extend-
ing Chinese Whispers to the semantic role induc-
tion task, we also show how it can be understood
as a type of Gibbs sampling when our graph is inter-
preted as a Markov random field.
Experimental results on the CoNLL 2008 bench-
mark dataset demonstrate that our method, de-
spite its simplicity, improves upon competitive ap-
proaches in terms of F1 and achieves significantly
higher cluster purity.
2 Related Work
Although the bulk of previous work on semantic role
labeling has primarily focused on supervised meth-
ods (Ma`rquez et al, 2008), a few semi-supervised
and unsupervised approaches have been proposed
in the literature. The majority of semi-supervised
models have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(Fu?rstenau and Lapata, 2009) or across different lan-
guages (Pado? and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) pro-
pose to increase the coverage of PropBank to un-
seen verbs by finding syntactically similar (labeled)
verbs and using their annotations as surrogate train-
ing data.
Swier and Stevenson (2004) were the first to intro-
duce an unsupervised semantic role labeling system.
Their algorithm induces role labels following a boot-
strapping scheme where the set of labeled instances
is iteratively expanded using a classifier trained on
previously labeled instances. Their method starts
with a dataset containing no role annotations at all,
but crucially relies on VerbNet (Kipper et al, 2000)
for identifying the arguments of predicates and mak-
ing initial role assignments. VerbNet is a manually
constructed lexicon of verb classes each of which is
explicitly associated with argument realization and
semantic role specifications.
Subsequent work has focused on unsupervised
methods for argument identification and classifica-
tion. Abend et al (2009) recognize the arguments of
predicates by relying solely on part of speech anno-
tations whereas Abend and Rappoport (2010) distin-
guish between core and adjunct roles, using an unsu-
pervised parser and part-of-speech tagger. Grenager
and Manning (2006) address the role induction prob-
lem and propose a directed graphical model which
relates a verb, its semantic roles, and their possible
syntactic realizations. Latent variables represent the
semantic roles of arguments and role induction cor-
responds to inferring the state of these latent vari-
ables.
Following up on this work, Lang and Lapata
(2010) formulate role induction as the process of de-
1321
tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that takes
advantage of the close relationship between syntac-
tic functions and semantic roles. More recently,
Lang and Lapata (2011) propose a clustering algo-
rithm which first splits the argument instances of
a verb into fine-grained clusters based on syntac-
tic cues and then executes a series of merge steps
(mainly) based on lexical cues. The split phase cre-
ates a large number of small clusters with high purity
but low collocation, i.e., while the instances in a par-
ticular cluster typically belong to the same role the
instances for a particular role are commonly scat-
tered amongst many clusters. The subsequent merge
phase conflates clusters with the same role in order
to increase collocation.
Like Grenager and Manning (2006) and Lang
and Lapata (2010; 2011), this paper describes an
unsupervised method for semantic role induction,
i.e., one that does not require any role annotated data
or additional semantic resources for training. Con-
trary to these previous approaches, we conceptualize
role induction in a novel way, as a graph partitioning
problem. Our method is simple, computationally ef-
ficient, and does not rely on hidden variables. More-
over, the graph-based representation for verbs and
their arguments affords greater modeling flexibility.
A wide range of methods exist for finding partitions
in graphs (Schaeffer, 2007), besides Chinese Whis-
pers (Biemann, 2006), which could be easily applied
to the semantic role induction problem. However,
we leave this to future work.
Graph-based methods are popular in natural lan-
guage processing, especially with unsupervised
learning problems (Chen and Ji, 2010). The Chinese
Whispers algorithm itself (Biemann, 2006) has been
previously applied to several tasks including word
sense induction (Klapaftis and M., 2010) and unsu-
pervised part-of-speech tagging (Christodoulopou-
los et al, 2010). The same algorithm is also de-
scribed in Abney (2007, pp. 146-147) under the
name ?clustering by propagation?. The term makes
explicit the algorithm?s connection to label propa-
gation, a general framework2 for semi-supervised
learning (Zhu et al, 2003) with applications to
machine translation (Alexandrescu and Kirchhoff,
2009), information extraction (Talukdar and Pereira,
2010) and structured part-of-speech tagging (Sub-
ramanya et al, 2010). The basic idea behind la-
bel propagation is to represent labeled and unlabeled
instances as vertices in an undirected graph with
edges whose weights express similarity (and possi-
bly dissimilarity) between the instances. Label in-
formation is then propagated between the vertices
in such a way that similar instances tend to be as-
signed the same label. Analogously, Chinese Whis-
pers works by propagating cluster membership in-
formation along the edges of a graph, even though
the graph does not contain any human-labeled in-
stance vertices.
3 Problem Setting
We adopt the standard architecture of supervised se-
mantic role labeling systems where argument identi-
fication and argument classification are treated sep-
arately. Our role labeler is fully unsupervised with
respect to both tasks ? it does not rely on any role
annotated data or semantic resources. However, our
system does not learn from raw text. In common
with most semantic role labeling research, we as-
sume that the input is syntactically analyzed in the
form of dependency trees.
We view argument identification as a syntactic
processing step that can be largely undertaken deter-
ministically through structural analysis of the depen-
dency tree. We therefore use a small set of rules to
detect arguments with high precision and recall (see
Section 4). Argument classification is more chal-
lenging and must take into account syntactic as well
as lexical-semantic information. Both types of in-
formation are incorporated into our model through
a similarity function that assigns similarity scores
to pairs of argument instances. Following previous
work (Lang and Lapata, 2010; Grenager and Man-
ning, 2006), our system outputs verb-specific roles
by grouping argument instances into clusters and la-
beling each argument instance with an identifier cor-
2For example, Haffari and Sarkar (2007) use label propa-
gation to analyze other semi-supervised algorithms such as the
Yarowsky (1995) algorithm.
1322
responding to the cluster it has been assigned to.
Such identifiers are similar to PropBank-style core
labels (e.g., A0, A1).
4 Argument Identification
Supervised semantic role labelers often employ a
classifier in order to decide for each node in the
parse tree whether or not it represents a semantic
argument. Nodes classified as arguments are then
assigned a semantic role. In the unsupervised set-
ting, we slightly reformulate argument identification
as the task of discarding as many non-semantic ar-
guments as possible. This means that the argument
identification component does not make a final posi-
tive decision for any of the argument candidates; in-
stead, a final decision is only made in the subsequent
argument classification stage.
We discard or select argument candidates us-
ing the set of rules developed in Lang and Lap-
ata (2011). These are mainly based on the parts
of speech and syntactic relations encountered when
traversing a dependency tree from the predicate
node to the argument node. For each candidate,
rules are considered in a prespecified order and the
first matching rule is applied. When evaluated on
its own, the argument identification component ob-
tained 88.1% precision (percentage of semantic ar-
guments out of those identified) and 87.9% recall
(percentage of identified arguments out of all gold
arguments).
5 Argument Classification
After identifying likely arguments for each verb,
the next step is to infer a label for each argument
instance. Since we aim to induce verb-specific
roles (see Section 3), we construct an undirected,
weighted graph for each verb. Vertices corre-
spond to verb argument instances and edge weights
quantify the similarities between them. This
argument-instance graph is then partitioned into
clusters of vertices representing semantic roles and
each argument instance is assigned a label that indi-
cates the cluster it belongs to. In what follows we
first describe how the graph is constructed and then
provide the details of our graph partitioning algo-
rithm.
CA
E D
B
0.4 0.1
0.8 ?1
1
0.3
0.2 0.7
Figure 1: Simplified example of an argument-instance
graph. All pairs of vertices with non-zero similarity are
connected through edges that are weighted with a simi-
larity score ?(vi,v j). Upon updating the label for a vertex
all neighboring vertices propagate their label to the vertex
being updated. The score for each label is determined by
summing together the weighted votes for that label and
the label with the maximal score is chosen.
5.1 Graph Construction
For each verb we construct an undirected, weighted
graph G = (V,E,?) with vertices V , edges E, and
edge weight function ? as follows. Each argu-
ment instance in the corpus that belongs to the
verb is added as a vertex. Then, for each possi-
ble pair of vertices (vi,v j) we compute a weight
?(vi,v j) ? R according to the function ?. If the
weight is non-zero, an undirected edge e = (vi,v j)
with weight ?(vi,v j) is added to the graph. The func-
tion ? quantifies the similarity or dissimilarity be-
tween instances; positive values indicate that roles
are likely to be the same, negative values indicate
that roles are likely to differ, and zero values indicate
that there is no evidence for either case. Our simi-
larity function is symmetric, i.e., ?(vi,v j) = ?(v j,vi)
and permits negative values (see Section 5.4 for a
detailed description).
Figure 1 shows an example of a graph for a verb
with five argument instances (vertices A?E). Edges
are drawn between pairs of vertices with non-zero
similarity values. For instance, vertex D is con-
nected to vertex A with weight 0.2, to vertex E
with 1, and vertex C with?1. Since edges are drawn
between all pairs of vertices with non-zero simi-
larity, the resulting graphs tend to be densely con-
nected, which for large datasets may be prohibitively
1323
inefficient. A solution would be to sample a subset
from all possible pairs, but we did not make use of
any kind of edge pruning in our experiments.
5.2 Graph Partitioning
Graph partitioning is realized with a variant of Chi-
nese Whispers (Biemann, 2006) whose details are
given below. In addition, we discuss how our algo-
rithm relates to other graph-based models in order to
help provide a better theoretical understanding.
We assume each vertex vi is assigned a label
li ? {1 . . .L} indicating the cluster it belongs to. Ini-
tially, each vertex belongs to its own cluster, i.e., we
let the number of clusters L = |V | and set li? i.
Given this initial vertex labeling, the algorithm pro-
ceeds by iteratively updating the label for each ver-
tex. The update is based on the labels of neighbor-
ing vertices and reflects their similarity to the vertex
being updated. Intuitively, each neighboring vertex
votes for the cluster it is currently assigned to, where
the strength of the vote is determined by the similar-
ity (i.e., edge weight) to the vertex being updated.
The label li of vertex vi is thus updated according to
the following equation:
li? arg maxl?{1...L} ?v j?Ni(l)?(vi,v j) (2)
where Ni(l) = {v j|(vi,v j) ? E ? l = l j} denotes the
set of vi?s neighbors with label l. In other words,
for each label we compute a score by summing to-
gether the weights of edges to neighboring vertices
with that label and select the label with the maximal
score. Note that negative edges decrease the score
for a particular label, thus demoting the label.
Consider again Figure 1. Assume we wish to up-
date vertex A. In addition, assume that B and E are
currently assigned the same label (i.e., they belong
to the same cluster) whereas C and D are each in
different clusters. The score for cluster {B,E} is
0.4+0.8 = 1.2, the score for cluster {C} is 0.3 and
the score for cluster {D} is 0.2. We would thus as-
sign A to cluster {B,E} as it has the highest score.
The algorithm is run for several iterations. At
each iteration it passes over all vertices, and the up-
date order of the vertices is chosen randomly. As
the updates proceed, labels can disappear from the
graph, whereby the number of clusters decreases.
Empirically, we observe that for sufficiently many
iterations the algorithm converges to a fixed labeling
or oscillates between labelings that differ only in a
few vertices. The result of the algorithm is a hard
partitioning of the given graph, where the number of
clusters is determined automatically.
5.3 Propagation Prioritization
We make one important modification to the basic al-
gorithm described so far based on the intuition that
higher scores for a label indicate more reliable prop-
agations. More precisely, when updating vertex vi to
label l we define the confidence of the update as the
average similarity to neighbors with label l:
con f (li? l) = 1|Ni(l)| ?v j?Ni(l)?(vi,v j) (3)
We can then prioritize high-confidence updates by
setting a threshold ? and allowing only updates with
confidence greater or equal to ?. The threshold is
initially set to 1 (i.e., the maximal possible confi-
dence) and then lowered by some small constant ?
after each iteration until it reaches a minimum ?min,
at which point the algorithm terminates. This im-
proves the resulting clustering, since it promotes
reliable updates in earlier phases of the algorithm
which in turn has a positive effect on successive up-
dates.
5.4 Argument-Instance Similarity
As described earlier, the edge weights in our graph
are similarity scores, with positive values indicating
similarity and negative values indicating dissimilar-
ity. Determining the similarity function ? without
access to labeled training data poses a major diffi-
culty which we resolve by relying on prior linguis-
tic knowledge. Specifically, we measure the sim-
ilarity of argument instances based on three sim-
ple and intuitive criteria: (1) whether the instances
are lexically similar; (2) whether the instances oc-
cur in the same syntactic position; and (3) whether
the instances occur in the same frame (i.e., are argu-
ments in the same clause). The same criteria were
used in (Lang and Lapata, 2011) and shown effec-
tive in quantifying role-semantic similarity between
clusters of argument instances. Lexical and syntac-
tic similarity are scored through functions lex(vi,v j)
and syn(vi,v j) with range [?1,1], whereas the third
criterion enters the scoring function directly:
1324
?(vi,v j)=
{
?? if vi and v j are in same frame (4)
?lex(vi,v j)+(1??)syn(vi,v j) otherwise.
The first case in the function expresses a com-
mon linguistic assumption, i.e., that two argument
instances vi and v j occurring in the same frame can-
not have the same semantic role. The function im-
plements this constraint by returning??.3 The syn-
tactic similarity function s(vi,v j) indicates whether
two argument instances occur in a similar syntactic
position. We define syntactic positions through four
cues: the relation of the argument head word to its
governor, verb voice (active/passive), the linear po-
sition of the argument relative to the verb (left/right)
and the preposition used for realizing the argument
(if any). The score is S4 where S is the number of cueswhich agree, i.e., have the same value. The syntac-
tic score is set to zero when the governor relation
of the arguments is not the same. Lexical similar-
ity l(vi,v j) is measured in terms of the cosine of the
angle between vectors hi and h j representing the ar-
gument head words:
lex(vi,v j) = cos(hi,h j) = hi?h j?hi??h j? (5)
We obtain hi and h j from a simple semantic space
model (Turney and Pantel, 2010) which requires no
supervision (Section 6 describes the details of the
model used in our experiments).
Our similarity function weights the contribution
of syntax vs. semantics equally, i.e., ? is set to 0.5.
This reflects the linguistic intuition that lexical and
syntactic information are roughly of equal impor-
tance.
5.5 Relation to Other Models
This section briefly points out some connections to
related models. The averaging procedure used for
updating the graph vertices (Equation 2) appears in
some form in most label propagation algorithms (see
Talukdar (2010) for details). Label propagation al-
gorithms are commonly interpreted as random walks
3Formally, ? has range ran(?) = [?1,1] ? {??} and for
x ? ran(?) we define x+(??) =??. This means that the over-
all score computed for a label (Equation 2) is ?? if one of the
summands is ??.
2?
3 3
1
Figure 2: The update rule (Equation 2) can be under-
stood as choosing a minimal edge-cut, thereby greedily
maximizing intra-cluster similarity and minimizing inter-
cluster similarity. Assuming equal weight for all edges
above, label 3 is chosen for the vertex being updated such
that the sum of weights of edges crossing the cut is mini-
mal.
on graphs. In our case such an interpretation is
not directly possible due to the presence of negative
edge weights. This could be changed by transform-
ing the edge weights onto a non-negative scale, but
we find the current setup more expedient for model-
ing dissimilarity.
Our model could be also transformed into a prob-
abilistic graphical model that specifies a distribution
over vertex labels. In the transformed model each
vertex corresponds to a random variable over labels
and edges are associated with binary potential func-
tions over vertex-pairs. Let 1(vi = v j) denote an in-
dicator function which takes value 1 iff. li = l j and
value 0, otherwise. Then pairwise potentials can be
defined in terms of the original edge weights4 as
?(vi,v j) = exp(1(vi = v j)?(vi,v j)). A Gibbs sam-
pler used to sample from the distribution of the
resulting pairwise Markov random field (Bishop,
2006; Wainwright and Jordan, 2008) would employ
almost the same update procedure as in Equation 2,
the difference being that labels would be sampled
according to their probabilities, rather than chosen
deterministically based on scores.
A third way of understanding the update rule
is as a heuristic for maximizing intra-cluster sim-
ilarity and minimizing inter-cluster similarity. By
4Including weights with value zero and thus connecting all
vertex pairs.
1325
assigning the label with maximal score to vi, we
greedily maximize the sum of intra-cluster edge
weights while minimizing the sum of inter-cluster
edge weights, i.e., the weight of the edge-cut. This
is illustrated in Figure 2. Cut-based methods are
a common method in graph clustering (Schaeffer,
2007) and are also used for inference in pairwise
Markov random fields like the one described in the
previous paragraph (Boykov et al, 2001).
Note that while it would be possible to transform
our model into a model with a formal probabilistic
interpretation (either as a graph random walk or as a
probabilistic graphical model) this would not change
the non-empirical nature of the similarity function,
which is unavoidable in the unsupervised setting and
is also common in the semi-supervised methods dis-
cussed in Section 2.
6 Experimental Setup
In this section we describe how we assessed the
performance of our model. We discuss the dataset
on which our experiments were carried out, explain
how our system?s output was evaluated and present
the methods used for comparison with our approach.
Data We compared the output of our model
against the PropBank gold standard annotations con-
tained in the CoNLL 2008 shared task dataset (Sur-
deanu et al, 2008). The latter was taken from the
Wall Street Journal portion of the Penn Treebank
and converted into a dependency format (Surdeanu
et al, 2008). In addition to gold standard depen-
dency parses, the dataset alo contains automatic
parses obtained from the MaltParser (Nivre et al,
2007). The dataset provides annotations for ver-
bal and nominal predicate-argument constructions,
but we only considered the former, following previ-
ous work on semantic role labeling (Ma`rquez et al,
2008). All the experiments described in this paper
use the CoNLL 2008 training dataset.
Evaluation Metrics For each verb, we determine
the extent to which argument instances in the clus-
ters share the same gold standard role (purity) and
the extent to which a particular gold standard role is
assigned to a single cluster (collocation).
More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the
percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, G j the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can be
then written as:
PU = 1N ?i maxj |G j ?Ci| (6)
Collocation is defined as follows. For each gold
role, we determine the cluster with the largest num-
ber of instances for that role (the role?s primary clus-
ter) and then compute the percentage of instances
that belong to the primary cluster for each gold role:
CO = 1N ?j maxi |G j ?Ci| (7)
Per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the
micro-average obtained by weighting the scores for
individual verbs proportionately to the number of in-
stances for that verb.
Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:
F1 = 2?CO?PUCO+PU (8)
Model Parameters Recall that our algorithm pri-
oritizes updates with confidence higher than a
threshold ?. Initially, ? is set to 1 and its value
decreases at each iteration by a small constant ?
which we set to 0.0025. The algorithm terminates
when a minimum confidence ?min is reached. While
choosing a value for ? is straightforward ? it sim-
ply has to be a small fraction of the maximal pos-
sible confidence ? specifying ?min on the basis of
objective prior knowledge is less so. And although
a human judge could determine the optimal termina-
tion point based on several criteria such as clustering
quality or the number of clusters, we used a develop-
ment set instead for the sake of reproducibility and
comparability. Specifically, we optimized ?min on
the CoNLL test set and obtained best results with
?min = 13 . This value was used for all our experi-ments and was also kept fixed for all verbs. Impor-
tantly, the development set was not used for any kind
of supervised training.
1326
Syntactic Function Latent Logistic Split-Merge Graph Partitioning
PU CO F1 PU CO F1 PU CO F1 PU CO F1
auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2 82.5 68.8 75.0
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9 84.0 73.5 78.4
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3 87.4 65.9 75.2
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1 88.6 70.7 78.6
Table 1: Evaluation of the output of our graph partitioning algorithm compared to our previous models and a baseline
that assigns arguments to clusters based on their syntactic function.
0 10 20 30 40 50 60 70Average number of clusters per verb60
70
80
90
100
Clus
ter p
urity
 (%)
Figure 3: Purity (vertical axis) against average number
of clusters per verb (horizontal axis) on the auto/auto
dataset.
Recall that one of the components in our simi-
larity function is lexical similarity which we mea-
sure using a vector-based model (see Section 5.4).
We created such a model from the Google N-Grams
corpus (Brants and Franz, 2006) using a context
window of two words on both sides of the target
word and co-occurrence frequencies as vector com-
ponents (no weighting was applied). The large size
of this corpus allows us to use bigram frequencies,
rather than frequencies of individual words and to
distinguish between left and right bigrams. We used
randomized algorithms (Ravichandran et al, 2005)
to build the semantic space efficiently.
Comparison Models We compared our graph par-
titioning algorithm against three competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as a point of com-
parison by other unsupervised semantic role induc-
tion systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.
0 100 200 300 400 500Number of iterations0
20
40
60
80
F1 sc
ore (%
)
Syntactic FunctionGraph Partitioning
Figure 4: F1 (vertical axis) against number of iterations
(horizontal axis) on the auto/auto dataset.
Our implementation allocates up to N = 21 clusters5
for each verb, one for each of the 20 most frequent
syntactic functions and a default cluster for all other
functions. We also compared our approach to Lang
and Lapata (2010) using the same model settings
(with 10 latent variables) and feature set proposed
in that paper. Finally, our third comparison model
is Lang and Lapata?s (2011) split-merge clustering
algorithm. Again we used the same parameters and
number of clusters (on average 10 per verb). Our
graph partitioning method uses identical cues for as-
sessing role-semantic similarity as the method de-
scribed in Lang and Lapata (2011).
7 Results
Our results are summarized in Table 1. We report
cluster purity (PU), collocation (CO) and their har-
monic mean (F1) for the baseline (Syntactic Func-
tion), our two previous models (the Latent Logistic
classifier and Split-Merge) and the graph partition-
ing algorithm on four datasets. These result from the
combination of automatic parses with automatically
identified arguments (auto/auto), gold parses with
5This is the number of gold standard roles.
1327
Syntactic Function
PU 91.4 68.6 45.1 59.7 62.4 61.9 63.5 75.9 76.7 69.6 63.1 53.7
CO 91.3 71.9 56.0 68.4 72.7 76.8 65.6 79.7 76.0 63.8 73.4 58.9
F1 91.4 70.2 49.9 63.7 67.1 68.6 64.5 77.7 76.3 66.6 67.9 56.2
Graph Partitioning
PU 95.6 83.5 72.3 75.4 83.3 84.4 74.8 84.8 89.5 83.0 73.2 66.3
CO 89.1 62.7 42.1 64.2 56.2 66.3 57.2 73.2 64.1 54.3 66.0 57.7
F1 92.2 71.6 53.2 69.4 67.1 74.3 64.8 78.5 74.7 65.7 69.4 61.7
Verb say make go increase know tell consider acquire meet send open break
Freq 15238 4250 2109 1392 983 911 753 704 574 506 482 246
Table 2: Clustering results for individual verbs on the auto/auto dataset with our graph partitioning algorithm and the
syntactic function baseline; the scores were taken from a single run.
automatic arguments (gold/auto), automatic parses
with gold arguments (auto/gold) and gold parses
with gold arguments (gold/gold). Table 1 reports
averages across multiple runs. This was necessary
in order to ensure that the results of our randomized
graph partitioning algorithm are stable.6 The argu-
ments for the auto/auto and gold/auto datasets were
identified using the rules described in Lang and Lap-
ata (2011) (see Section 4). Bold-face is used to high-
light the best performing system under each measure
(PU, CO, or F1) on each dataset.
Compared to the Syntactic Function baseline,
the Graph Partitioning algorithm has higher F1 on
the auto/auto and auto/gold datasets but lags be-
hind by 0.5 points on the gold/auto dataset and
by 0.9 points on the gold/gold dataset. It attains
highest purity on all datasets except for gold/gold,
where it is 0.1 points below Split-Merge. When con-
sidering F1 in conjunction with purity and colloca-
tion, we observe that Graph Partitioning can attain
higher purity than the comparison models by trading
off collocation. If we were to hand label the clusters
output by our system, purity would correspond to the
quality of the resulting labeling, while collocation
would determine the labeling effort. The relation-
ship is illustrated more explicitly in Figure 3, which
plots purity against the average number of clusters
per verb on the auto/auto dataset. As the algorithm
6For example, on the auto/auto dataset and over 10 runs,
the standard deviation in F1 was 0.11 points in collocation 0.16
points and in purity 0.08 points. The worst F1 was 0.20 points
below the average, the worst collocation was 0.32 points be-
low the average and the worst purity was 0.17 points below the
average.
proceeds the number of clusters is reduced which
results in a decrease of purity. The latter decreases
more rapidly once the number of 20 clusters per verb
is reached. This is accompanied by a decreasing
tradeoff ratio between collocation and purity: at this
stage decreasing purity by one point increases collo-
cation by roughly one point, whereas in earlier itera-
tions a decrease of purity by one point goes together
with several points increase in collocation. This is
most likely due to the fact that the number of gold
standard classes is around 20.
Figure 4 shows the complete learning curve of our
graph partitioning method on the auto/auto dataset
(F1 is plotted against the number of iterations).
The algorithm naturally terminates at iteration 266
(when ?min = 1/3), but we have also plotted itera-
tions beyond that point. Since lower values of ? per-
mit unreliable propagations, F1 eventually falls be-
low the baseline (see Section 5.2). The importance
of our propagation prioritization mechanism is fur-
ther underlined by the fact that when it is not em-
ployed (i.e., when using the vanilla Chinese Whis-
pers algorithm without any modifications), it per-
forms substantially worse than the comparison mod-
els. On the auto/auto dataset, F1 converges to 59.1
(purity is 55.5 and collocation 63.2) within 10 itera-
tions.
Finally, Table 2 shows how performance varies
across verbs. We report results for the Syntac-
tic Function baseline and Graph Partitioning on the
auto/auto dataset for 12 verbs. These were selected
so as to exhibit varied occurrence frequencies and
alternation patterns. As can be seen, the macro-
1328
scopic result ? increase in F1 and purity ? also
holds across verbs.
8 Conclusions
In this paper we described an unsupervised method
for semantic role induction, in which argument-
instance graphs are partitioned into clusters repre-
senting semantic roles. The approach is conceptu-
ally and algorithmically simple and novel in its for-
malization of role induction as a graph partitioning
problem. We believe this constitutes an interesting
alternative for two reasons. Firstly, eliciting and
encoding problem-specific knowledge in the form
of instance-wise similarity judgments can be easier
than encoding it into model structure e.g., by mak-
ing statistical independence assumptions or assump-
tions about latent structure. Secondly, the approach
is general and amenable to other graph partitioning
algorithms and relates to well-known graph-based
semi-supervised learning methods.
The similarity function in this paper is by neces-
sity rudimentary, since it cannot be estimated from
data. Nevertheless, the resulting system attains com-
petitive F1 and notably higher purity than the com-
parison models. Arguably, performance could be
improved by developing a better similarity function.
Therefore, in the future we intend to investigate how
our system performs in a weakly supervised setting,
where the similarity function is estimated from a
small amount of labeled instances, since this would
allow us to incorporate richer syntactic features and
result in more precise similarity scores.
Acknowledgments We are grateful to Charles
Sutton for his valuable feedback on this work. The
authors acknowledge the support of EPSRC (grant
GR/T04540/01).
References
O. Abend and A. Rappoport. 2010. Fully unsupervised
core-adjunct argument classification. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 226?236, Uppsala,
Sweden.
O. Abend, R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing, pages 28?36, Singapore.
S. Abney. 2007. Semisupervised Learning for Computa-
tional Linguistics. Chapman & Hall/CRC.
A. Alexandrescu and K. Kirchhoff. 2009. Graph-based
learning for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
119?127, Boulder, Colorado.
C. Biemann. 2006. Chinese Whispers: an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the First Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City.
C. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast Ap-
proximate Energy Minimization via Graph Cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 23(11):1222?1239.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia.
Z. Chen and H. Ji. 2010. Graph-based clustering for
computational linguistics: A survey. In Proceedings of
TextGraphs-5 - 2010 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 1?9, Up-
psala, Sweden.
C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 575?584, Cambridge, MA.
D. Dowty. 1991. Thematic Proto Roles and Argument
Selection. Language, 67(3):547?619.
H. Fu?rstenau and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 11?20, Singa-
pore.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245?288.
A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192?199, Prague, Czech Republic.
T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1?8, Sydney, Australia.
1329
G. Haffari and A. Sarkar. 2007. Analysis of Semi-
Supervised Learning with the Yarowsky Algorithm. In
Proceedings of the 23rd Conference on Uncertainty in
Artificial Intelligence, Vancouver, BC.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691?696. AAAI Press / The MIT Press.
I. Klapaftis and Suresh M. 2010. Word sense induction
& disambiguation using hierarchical random graphs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 745?
755, Cambridge, MA.
J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939?
947, Los Angeles, California.
J. Lang and M. Lapata. 2011. Unsupervised Semantic
Role Induction via Split-Merge Clustering. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, Portland, Oregon.
To appear in.
L. Ma`rquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145?
159.
G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95?135.
S. Pado? and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307?340.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289?310.
D. Ravichandran, P. Pantel, and E. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Function for High Speed Noun Clustering.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, page 622629,
Ann Arbor, Michigan.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice, version 1.3. Technical report, In-
ternational Computer Science Institute, Berkeley, CA,
USA.
S. Schaeffer. 2007. Graph clustering. Computer Science
Review, 1(1):27?64.
D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12?21,
Prague, Czech Republic.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Effi-
cient graph-based semi-supervised learning of struc-
tured tagging models. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 167?176, Cambridge, MA.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8?15, Sapporo, Japan.
M. Surdeanu, R. Johansson, A. Meyers, and L. Ma`rquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159?177, Manchester,
England.
R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95?102, Barcelona, Spain.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1473?1481, Uppsala, Sweden.
P. Talukdar. 2010. Graph-Based Weakly Supervised
Methods for Information Extraction & Integration.
Ph.D. thesis, CIS Department, University of Pennsyl-
vania.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
M. Wainwright and M. Jordan. 2008. Graphical Mod-
els, Exponential Families, and Variational Inference.
Foundations and Trends in Machine Learning, 1(1-
2):1?305.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North
1330
American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13?16, Boulder, Colorado.
D. Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
Supervised Learning Using Gaussian Fields and Har-
monic Functions. In Proceedings of the 20th Interna-
tional Conference on Machine Learning, Washington,
DC.
1331
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 233?243, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Multiple Aspect Summarization Using Integer Linear Programming
Kristian Woodsend and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
k.woodsend@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Multi-document summarization involves
many aspects of content selection and sur-
face realization. The summaries must be
informative, succinct, grammatical, and obey
stylistic writing conventions. We present a
method where such individual aspects are
learned separately from data (without any
hand-engineering) but optimized jointly
using an integer linear programme. The
ILP framework allows us to combine the
decisions of the expert learners and to select
and rewrite source content through a mixture
of objective setting, soft and hard constraints.
Experimental results on the TAC-08 data set
show that our model achieves state-of-the-art
performance using ROUGE and signifi-
cantly improves the informativeness of the
summaries.
1 Introduction
Automatic summarization has enjoyed wide popu-
larity in natural language processing (see the pro-
ceedings of the Document Understanding and Text
Analysis conferences) due to its potential for prac-
tical applications but also because it incorporates
many important aspects of both natural language un-
derstanding and generation. Of the many summa-
rization paradigms that have been identified over the
years (see Sparck Jones (1999) and Mani (2001) for
comprehensive overviews), multi-document sum-
marization ? the task of producing summaries from
clusters of thematically related documents ? has
consistently attracted attention.
Despite considerable research effort, the auto-
matic generation of multi-document summaries that
resemble those written by humans remains chal-
lenging. This is primarily due to the task itself
which is complex and subject to several constraints:
the summary must be maximally informative and
minimally redundant, grammatical, coherent, adhere
to a pre-specified length and stylistic conventions.
An ideal model would learn to output summaries
that simultaneously meet alhese constraints from
data (i.e., document clusters and their correspond-
ing summaries). This global inference problem is,
however, hard ? the solution space is large and the
lack of easily accessible datasets an obstacle to joint
learning. It is thus no surprise that previous work has
focused on specific aspects of joint learning.
Initial global formulations of the multi-document
summarization task focused on extractive summa-
rization and used approximate greedy algorithms for
finding the sentences of the summary. Goldstein et
al. (2000) search for the set of sentences that are
both relevant and non-redundant, whereas Filatova
and Hatzivassiloglou (2004) model multi-document
summarization as an instance of the maximum cov-
erage set problem.1 More recent work improves on
the search problem by considering exact solutions
and permits a limited amount of rewriting. McDon-
ald (2007) proposes an integer linear programming
formulation that maximizes the sum of relevance
scores of the selected sentences penalized by the
1Given C, a finite set of weighted elements, a collection T of
subsets of C, and an integer k, find those k sets that maximize the
total number of elements in the union of T ?s members (Hochba,
1997).
233
sum of redundancy scores of all pairs of selected
sentences. Gillick et al2008) develop an exact so-
lution for a model similar to Filatova and Hatzivas-
siloglou (2004) under the assumption that the value
of a summary is the sum of values of the unique con-
cepts (approximated by bigrams) it contains. Subse-
quent work (Gillick et al2009; Berg-Kirkpatrick et
al., 2011) extends this model to allow sentence com-
pression in the form of word or constituent deletion.
In this paper we propose a model for multi-
document summarization that attempts to cover
many different aspects of the task such as content se-
lection, surface realization, paraphrasing, and stylis-
tic conventions. These aspects are learned separately
using specific ?expert? predictors, but are optimized
jointly using an integer linear programming model
(ILP) to generate the output summary.2 All experts
are learned from data without requiring additional
annotation over and above the summaries written
for each document cluster. Our predictors include
the use of unique bigram information to model con-
tent and avoid redundancy, positional information to
model important and poor locations of content, and
language modeling to capture stylistic conventions.
Learning each predictor separately gives better gen-
eralization, while the ILP framework allows us to
combine the decisions of the expert learners through
the use of objectives, hard and soft constraints.
The experts work collaboratively to rewrite the
content using rules extracted from document clusters
and model summaries. We adopt the synchronous
tree substitution grammar (STSG) formalism (Eis-
ner, 2003) which can model non-isomorphic tree
structures (the grammar rules can comprise trees of
arbitrary depth) and is thus suited to text-rewriting
tasks which typically involve a number of local mod-
ifications to the input text. Specifically, we pro-
pose quasi-synchronous tree substitution grammar
(QTSG) as a flexible formalism to learn general tree-
edits from loosely-aligned phrase structure trees.
We evaluate our model on the 100-word ?non-
2Our task is standard multi-document summarization and
should not be confused with ?guided? summarization where
system and human summarizers are given a list of important
aspects to cover in the summary. Our usage of the term aspects
broadly refers to the different types of constraints (e.g., relating
to content or style) a summary must meet, but these are learned
rather than specified in advance.
update? summarization task as defined in the the
Text Analysis Conference (TAC 2008). Experimen-
tal results show that our method obtains perfor-
mance comparable and in some cases superior to
state-of-the-art, in terms of ROUGE and human rat-
ings of summary grammaticality and informative-
ness. Importantly, there is nothing inherent in our
model that is specific to this particular summariza-
tion task. As all of the different experts are learned
from data, it could easily adapt to other summariza-
tion styles or conventions as needed.
2 Related work
Recent years have seen increased interest in global
inference methods for summarization. ILP-based
models have been developed for several subtasks
ranging from sentence compression (Clarke and La-
pata, 2008), to single- and multi-document sum-
marization (McDonald, 2007; Martins and Smith,
2009; Gillick and Favre, 2009; Woodsend and Lap-
ata, 2010; Berg-Kirkpatrick et al2011), and head-
line generation (Deshpande et al2007; Wood-
send et al2010). Most of these approaches are ei-
ther purely extractive or implement a single rewrite
operation, namely word deletion. Although it is
well-known that hand-written summaries often ex-
hibit additional edits and sentence recombinations
(Jing, 2002), the challenges involved in acquiring
the rewrite rules, interfacing them with inference,
and ensuring grammatical output make the develop-
ment of abstractive models non-trivial.
Our work is closest to Gillick et al2008) who
also develop an ILP model for multi-document sum-
marization. A key assumption in their model which
we also follow is that input documents contain a
variety of concepts, each of which are allocated a
value, and the goal of a good summary is to max-
imize the sum of these values subject to the length
constraint. The authors use bigrams as concepts and
their frequency in the input documents as a proxy
for their value. This model can also perform sen-
tence compression (see also Gillick et al2009)),
however, the deletion rules are hand-coded. Berg-
Kirkpatrick et al2011) build on this work by re-
casting it as a structured prediction problem. They
essentially combine the same bigram content scor-
ing system with features relating to the parse tree
234
which they learn using a maximum-margin SVM
trained on annotated gold-standard compressions.
Our multi-document summarization model jointly
optimizes different aspects of the task involving both
content selection and surface realization. Each indi-
vidual aspect has its own dedicated expert, which we
argue is advantageous as it renders inference simpler
and affords flexibility (e.g., additional aspects can be
incorporated into the model or trained separately on
different datasets). Our work differs from Gillick et
al. (2009) and Berg-Kirkpatrick et al2011) in three
important respects. Firstly, we develop a genuinely
abstractive model that is not limited to deletion.
Our rewrite rules are encoded in quasi-synchronous
tree substitution grammar and learned automatically
from source documents and their summaries. Un-
like previous applications of STSG to sentence com-
pression (Cohn and Lapata, 2009; Cohn and Lap-
ata, 2008) our quasi-synchronous TSG does not at-
tempt to learn the complete translation from source
to target sentence; it only loosely links the syntactic
structure of the two (Smith and Eisner, 2006), and
is therefore well suited to describing the relation-
ship between documents and their abstracts. Sec-
ondly, our content selection component extends to
features beyond the bigram horizon, as we learn to
identify important concepts based on syntactic and
positional information. We also learn which words
are unlikely to appear in a summary. Thirdly, unlike
Berg-Kirkpatrick et al2011) our model does not
try to learn all the parameters (e.g., content, rewrite
rules, style) of the summarization problem jointly;
although decoupling learning from inference is per-
haps less elegant from a modeling perspective, the
learning process is more robust and reliable.
3 Modeling
There are many aspects to producing a good sum-
mary of multiple documents. The important con-
tent needs to be captured, typically key facts in
each individual document, and information seen
across the cluster. Stylistic features may be differ-
ent in the summary from original documents. For
instance, summaries tend to use more concise lan-
guage, sources are not attributed as they are in news
articles, and relative dates are not included. In addi-
tion, the summary must be fluent, coherent, and re-
spect a pre-specified maximum length requirement.
We present an approach where elements of all the
above considerations are learned from training data
by separate dedicated components, and then com-
bined in an integer linear programme. Content se-
lection is performed partly through identifying the
most salient topics (bigrams); an additional compo-
nent learns to identify which information from the
source documents should be in the summary based
on positional information. Meanwhile, in terms of
surface realization, a language model identifies the
words that should not be in the output summaries,
whereas a separate component learns to exclude
sentences that are poor candidates for summaries.
QTSG rules, learned from the training corpus, are
used to generate alternative compressions and para-
phrases of the source sentences, in the style suit-
able for the summaries. Finally, an ILP model com-
bines the output of these components into a sum-
mary, jointly optimizing content selection and sur-
face realization preferences, and providing the flexi-
bility to treat some components as soft while others
as hard constraints.
3.1 Document Representation
Given an input sentence, our approach deconstructs
it into component phrases and clauses, typical of a
phrase structure parser. In our experiments, we ob-
tain this representation from the output of the Stan-
ford parser (Klein and Manning, 2003) but any other
broadly similar parser could be used instead. Nodes
in the parse tree represent points where QTSG rules
can be applied (and paraphrases generated), and they
also represent decision points for the ILP. In the fol-
lowing, we will refer to these decision nodes as the
set N , and decisions for each node using the binary
variable zi, i ?N .
3.2 Content Selection Using Bigrams
We follow Gillick et al2008) in modeling the infor-
mation content of the summary as the weighted sum
of the individual information units it contains. We
represent information units as the set of bigrams B
seen in the source documents. The weight w of each
bigram is calculated from the number of source doc-
uments where the bigram was seen. The summary is
thus given the score fB(z), i.e., the weighted sum of
235
its information units:
fB(z) = ?
j?B
w jb j (1)
where w j is the weight of concept j, b j a binary vari-
able to indicate if concept j is present in the sum-
mary, and j ? B .
Importantly, each information unit is counted only
once; this encourages wide coverage of the source
documents, and removes any drive towards redun-
dant information without actively discouraging it,
contrary to other global formulations where redun-
dancy measures form part of the objective (McDon-
ald, 2007). The counting mechanism is achieved by
linking the variables z indicating nodes in the parse
tree and b indicating bigrams:
b j ? ?
i?N : j?Bi
zi ? j ? B (2)
where Bi ? B is the subset of bigrams that are con-
tained in node i. A drawback of the global nature
of this counting mechanism, however, is that it can-
not be integrated with local features such as those
described below; our approach takes local features
into account but these are weighted by other compo-
nents.
3.3 Content Selection Using Salience
The bigram approach is a powerful method for
identifying important concepts within the document
cluster. It works particularly well in the sentence ex-
traction paradigm. However, additional elements are
known to be good predictors of important informa-
tion. Examples include the position of a sentence
in the document (e.g., first sentences often con-
tain salient information), whether it contains proper
nouns, numbers, pronouns, mentions of money, and
so on. We decided to learn which of these elements
(represented as nodes in the parse tree) are infor-
mative from training data. Specifically, sentences
in the cluster documents were aligned to sentences
from corresponding human summaries. Alignment
was based rather simply on identifying the sentence
pairs with the highest number of overlapping bi-
grams, without compensating for sentence length, or
matching the sequence of information in the sum-
maries and source documents (Nelken and Schieber,
Weight Feature
1.21 From first sentence in document
0.73 Contains proper nouns
0.68 Contains nouns
0.57 From first paragraph
0.53 From first three sentences
0.51 Contains numbers
-0.50 Contains pronouns
0.32 Contains money
Table 1: Weights and features of SVM that predicts the
salience of summary content. Negative weights indicate
information that should not be included in the summary.
2006). Matched sentences in the source documents
were given positive labels, while unaligned sen-
tences were given negative labels. These labels were
then propagated to phrase structure nodes.
We trained an SVM on this data (tree nodes and
their labels) using surface features that do not over-
lap with bigram information: sentence and para-
graph position, POS-tag information. Table 1 shows
the most important features learned by the model as
predictors of salient content.
The summary can be given a salience score fS (z)
using the raw SVM prediction scores of the individ-
ual parse tree nodes:
fS (z) = ?
i?N
(?(i) ??)zi (3)
where ?(i) is the feature vector for node i, and ? the
weights learned by the SVM.
3.4 Surface Realization Using Style
Some sentences in the source documents will make
poor summary sentences, despite the information
they contain, and therefore contrary to the predic-
tions of the content selection indicators described
above. This may be because the source sentence is
very short, or is expressed as a quotation, or con-
tains many pronouns that will not be resolved when
the sentence is extracted.
Our idea is to learn which sentences are poor from
a stylistic perspective using again aligned training
data. We train a second SVM on the aligned sen-
tences and their labels using surface features at the
sentence level, such as sentence length and POS-tag
information. The most important features learned by
236
Weight Feature
-1.04 Word count less than 10
-0.83 Word count less than 20
-0.30 Question
-0.30 Quotation
-0.14 Personal pronouns
Table 2: Weights and features of SVM that predicts poor
candidate sentences.
the model as predictors of poor sentences, and the
weights assigned to them, are shown in Table 2.
The predictions of the SVM are incorporated into
the ILP as a hard constraint, by forcing all parse tree
nodes within those sentences predicted as poor (the
set N ?) to be zero:
zi = 0 ?i ?N ?. (4)
3.5 Surface Realization Using Lexical
Preferences
Human-written summaries differ from the source
news articles in a number of ways. They delete ex-
traneous information, merge material from several
sentences, employ paraphrases and syntactic trans-
formations, change the order of the source sentences
and replace phrases or clauses with more general
or specific descriptions. We could attempt to learn
the ?language of summaries? with a language model
which we could then use to guide the generation
process (e.g., by producing maximally probable out-
put). Aside from the logistics of gathering training
data large enough to provide robust estimates, we
believe that a more compelling approach is to focus
on the words that are unlikely to appear in the sum-
mary despite appearing in the source documents.
A comparison of the language models generated
from the source documents and model summaries,
even at the unigram level, is revealing. Table 3 shows
lexemes that appear in both source and summary
documents, but where the likelihood of the lexeme
appearing in the summary is much less than that
of it appearing the document, taking into account
that the summary is much shorter anyway. The fi-
nal column shows the log10-ratio (L(w)) between
the two probabilities. We can see that least prob-
able words are those that correspond to attribut-
ing information sources (e.g., said, told, according
Lexeme w Source Summary L(w)
count count
say 5670 88 -1.63
go 638 11 -1.52
last 616 9 -1.69
get 543 15 -1.05
tell 512 8 -1.62
come 488 12 -1.17
know 404 9 -1.27
monday 391 8 -1.35
think 382 7 -1.46
next 239 7 -0.99
spokesman 197 4 -1.36
Table 3: Counts of lexemes in the source news articles and
summaries, and measure of the ratio of their probabilities
(for most common lexemes with ratio <?0.95).
to, spokesman), dates described relatively (e.g., last
Monday), and events that are in the process of hap-
pening (e.g., coming, going).
As the amount of training data tends to be lim-
ited ? there are usually only a few human-written
summaries available per document cluster ? we
use a unigram language model, but conceivably a
longer-range n-gram could be employed in the same
vein. We incorporate preferences about summary
language into the model as a soft constraint. The
log-ratio values fLR (z) are included in the objective
and defined at the tree node level:
fLR (z) = ?
i?N
?
w?Wi
L(w)zi (5)
where L(w), w ?Wi is the log-ratio value for an in-
dividual word w:
L(w) = log10
Psrc(w)
Psum(w)
,
Psrc(w) and Psum(w) are the probabilities of word w
appearing in the source and summary documents re-
spectively, and Wi is the set of words at parse tree
node i. Importantly, we include only those those lex-
emes with negative L(w) values. This guides the
model away from the kind of phrases described
above, but not towards any particular language pref-
erences.
237
3.6 Quasi-synchronous Tree Substitution
Grammar
Rewrite rules involving substitutions, deletions and
reorderings are captured in our model using a quasi-
synchronous tree substitution grammar. Given an in-
put (source) sentence S1 or its parse tree T1, the
QTSG contains rules for generating possible trans-
lation trees T2. A grammar node in the target tree T2
is modeled on a subset of nodes in the source tree,
with a rather loose alignment between the trees.
We extract QTSG rules from aligned source
and summary sentence pairs represented by their
phrase structure trees. Our algorithm builds up a
list of leaf node alignments based on lexical iden-
tity. Direct parent nodes are aligned where more
than one child node aligns. This quasi-synchronous
?bottom-up? process gives us better ability to match
non-isomorphic structures. We do not assume an
alignment between source and target root nodes, nor
do we require a surjective alignment of all target
nodes to the source tree. QTSG rules are then cre-
ated from aligned nodes above the leaf node level if
all the nodes in the target tree can be explained us-
ing nodes from the source. Individual rewrite rules
describe the mapping of source tree fragments into
target tree fragments, and so the grammar represents
the space of valid target trees that can be produced
from a given source tree (Eisner, 2003; Cohn and
Lapata, 2009).
Examples of the most frequent QTSG rules
learned by the above process are shown in Figure 1.
Many of the rules relate to the compression of noun
phrases through deletion, and examples are shown
in the upper box. Others capture the compression of
verb phrases (middle box). An important rewrite op-
eration is the abstraction of a sentence from a more
complex source sentence, adding final punctuation if
necessary (lower box).
At generation, paraphrases are created from
source sentence parse trees by identifying and ap-
plying QTSG rules with matching structure. The
transduction process starts at the root node of the
parse tree, applying QTSG rules to sub-trees un-
til leaf nodes are reached. Note that we do not use
the Bayesian probability model normally associated
with quasi-synchronous grammars (Smith and Eis-
ner, 2006); instead, we ask the QTSG to provide
?NP, NP? ? ?[NP 1 PP], [NP 1 ]?
?NP, NP? ? ?[NP 1 VP], [NP 1 ]?
?NP, NP? ? ?[NP 1 SBAR], [NP 1 ]?
?NP, NP? ? ?[NP 1 , NP ,], [NP 1 ]?
?NP, NP? ? ?[NP 1 CC NP], [NP 1 ]?
?NP, NP? ? ?[NNP NNP 1 ], [NNP 1 ]?
?NP, NP? ? ?[DT 1 JJ NN 2 ], [DT 1 NN 2 ]?
?VP, VP? ? ?[VP 1 CC VP], [VP 1 ]?
?VP, VP? ? ?[VP CC VP 1 ], [VP 1 ]?
?VP, VP? ? ?[VP 1 , CC VP], [VP 1 ]?
?S, S? ? ?[NP 1 VP 2 ], [NP 1 VP 2 .]?
?S, S? ? ?[ADVP , NP 1 VP 2 .], [NP 1 VP 2 .]?
Figure 1: Examples of most frequently learned QTSG
rules. Boxed subscripts show aligned nodes.
paraphrases that are acceptable rather than probable,
and generate all paraphrases licensed by the QTSG.
The alternative paraphrases are incorporated into
the target phrase structure tree as choices that the
ILP can make. We use the set C ? N to be the
set of nodes where a choice of paraphrases is avail-
able, and Ci ?N , i ? C to be the actual paraphrases
of i. Where there are alternatives, it makes sense of
course to select only one, which we implement using
the constraint:
?
j?Ci
z j = zi ?i ? C , j ? Ci (6)
More generally, we need to constrain the output to
ensure that a parse tree structure is maintained. For
each node i ?N , the set Di ?N contains the list of
dependent nodes (both ancestors and descendants)
of node i, so that each set Di contains the nodes that
depend on the presence of i. We introduce a con-
straint to force node i to be present if any of its de-
pendent nodes are chosen:
z j? zi ?i ?N , j ?Di (7)
3.7 The ILP Objective
The model we propose for generating a multi-
document summary is expressed as an integer linear
programme and incorporates the content selection
and surface realization preferences, as well as the
238
soft and hard constraints described in the preceding
sections. The objective of the optimization problem
is to maximize the score contributed by the various
elements of content selection ( fB(z) and fS (z)) and
soft surface realization constraints ( fLR (z)) :
max
z
fB(z)+ fS (z)+ fLR (z) (8)
This objective is subject to the constraints (2), (4),
(6), and (7) that represent hard constraint decisions,
or maintain the logical integrity of the model. An
overall length constraint completes the model:
?
i?N
lizi ? lmax (9)
where li is the number of words generated by choos-
ing node i, and lmax is the global word length limit.
Note that the scores in the objective are for each
tree node and not each sentence. This affords the
model flexibility: the content selection elements are
generally not competing with each other to give a
decision on a sentence (see McDonald (2007)). In-
stead, components are marking positive and nega-
tive nodes. The ILP is implicitly searching the gram-
mar rules for ways to rewrite the sentence, with the
aim of including the salient nodes while removing
negative-scoring nodes (deleting them increases the
score of the node to zero). Figure 2 shows an exam-
ple of a source sentence where the bigram, salience
and language preference components of the ILP
work together to score nodes in the parse tree. The
nodes NP 1 , VP 3 and VP 4 all have positive scores,
while ?said Tuesday? is negative. As a rewrite pos-
sibility, the rewrite rule shown bottom left is avail-
able, which will remove the negative node. Further
rewrite rules allow VP 2 to be compressed. The out-
put actually generated by the model used sub-trees
(b) and (d) ? the final text is included in Table 6.
4 Experimental Set-up
Data Our model was evaluated on the TAC non-
update multi-document summarization task which
involves generating a 100-word-limited summary
from a cluster of 10 related input documents; ad-
ditionally, TAC provides a set of four model sum-
maries for each cluster, written by human experts.
We used the 44 document clusters from TAC-2009
as training data, to learn the different elements of
the model. The 48 document clusters of TAC-2008
were reserved for the generation of test summaries.3
Training The two components described in Sec-
tions 3.3 and 3.4 were trained using binary SVM
classifiers, with labels inferred automatically via
alignment. The salience classifier was trained on
102,754 node instances (16,042 positive and 86,712
negative). The style classifier was trained on 20,443
sentence instances (2,083 positive and 18,360 neg-
ative). We learned the feature weights with a linear
SVM, using the software SVM-OOPS (Woodsend
and Gondzio, 2009). Because of the high compres-
sion rate in this task, sentence alignment leads to an
unbalanced data set. We compensated for this by us-
ing different SVM hyper-parameters C+ and C? as
the loss multiplier for misclassification of positive
and negative training samples respectively. SVM
hyper-parameters were chosen that gave the high-
est F1 values using 10-fold cross-validation. The
salience SVM obtained a precision of 0.28 and re-
call of 0.43. Precision for the style SVM was 0.20
and recall 0.63, respectively. The classifiers on their
own would thus not be great predictors of salience
or style, but in practice they were useful for break-
ing ties in bigram scores.
Aligned sentences from the training data were
also used to learn the quasi-synchronous tree sub-
stitution grammar, using the process described in
Section 3.6. Rules seen fewer than 3 times were re-
moved, resulting in a total of 339 QTSG rules. Two
unigram language models (see Section 3.5) were
trained on the source articles and summaries, respec-
tively. Their probabilities were compared to give the
word list shown in Table 3. We removed words with
a source count less than 50, providing a list of 60 lex-
emes. The resulting integer linear programmes were
solved using SCIP,4 and it took 55 seconds on aver-
age to read in and solve a document cluster problem.
Evaluation We compared our model against two
systems. As a baseline, we used the ICSI-1 extrac-
tive system (Gillick et al2008) which is also based
on ILP and was highly ranked in the TAC-2008
evaluation. We also compared against the ?learned
phrase compression? system of Berg-Kirkpatrick et
3This split follows Berg-Kirkpatrick et al2011).
4http://scip.zib.de/
239
(a) S
.
.
VP
said Tuesday
NP
a top space
official
,
,
S
VP 2
SBAR 4
if its maiden unmanned spacecraft Chandrayaan-1,
slated to be launched by 2008, is successful in
mapping the lunar surface
VP 3
will launch more mis-
sions to the moon
NP 1
India
(b) S
.VP 2NP 1
(c) VP 2
VP 3
(d) VP 2
SBAR 4VP 3
?S, S? ?
?[NP 1 VP 2 ], [NP 1 VP 2 .]?
?VP 2 , VP 2 ? ?
?[VP 3 SBAR], [VP 3 ]?
?VP 2 , VP 2 ? ?
?[VP 3 SBAR 4 ], [VP 3 SBAR 4 ]?
Figure 2: Sentence representation provided to the ILP. (a) The source sentence representation (child nodes condensed
for space reasons). Bigrams are shown in bold, slanted text indicates phrases with high salience scores fS , while said
Tuesday is penalized by fLR . Alternative sub-trees (b), (c) and (d) are created using QTSG rules (dashed lines). The
output sentence (see Table 6) was generated from sub-trees (b) and (d).
al. (2011) (henceforth B-K), which has the highest
reported ROUGE scores that we are aware of.5 In
addition to the full model described in Section 3, we
also produced outputs where each of the five compo-
nents described in Sections 3.2?3.6 were removed,
to assess their individual contribution.
We evaluated the output summaries in two ways,
using automatic measures and human judgements.
Automatic evaluation was performed with ROUGE
(Lin and Hovy, 2003) using TAC-2008 parame-
ter settings. We report bigram overlap (ROUGE-2)
and skip-bigram (ROUGE-SU4) recall values. We
also used Translation Edit Rate (TER, Snover et al
(2006)) to examine the systems? rewrite potential.
TER is defined as the minimum number of edits
(insertions, deletions, substitutions, and shifts) re-
quired to change the system output so that it exactly
matches a reference (here, the reference is the most
closely aligning source sentence). The perfect TER
score is 0, however note that it can be higher than 1
due to insertions.
Our judgement elicitation study was conducted
as follows. We randomly selected ten document
5We are grateful to Taylor Berg-Kirkpatrick for making his
system output available to us.
clusters from the test set and generated summaries
with our model (and its lesser variations). We also
included the corresponding ICSI-1 and B-K sum-
maries, and one randomly-selected model summary.
The study was conducted over the Internet using
Mechanical Turk and was completed by 54 volun-
teers, all self reported native English speakers. Par-
ticipants were first asked to read the documents in
each cluster. Next, they were asked a few compre-
hension questions to ensure they had understood and
processed the documents. Finally, they were pre-
sented with a summary and asked to rate it along
two dimensions: grammaticality (is the summary
fluent and grammatical?), and informativeness (are
the main topics captured in the summary?). The sub-
jects used a 1?5 rating scale, with half-points al-
lowed. Participants who declared themselves as non-
native English speakers, did not answer the compre-
hension questions correctly or took only a few min-
utes to complete the task were eliminated.
5 Results
Our results are summarized in Table 4. Let us first
discuss those obtained using ROUGE-2 (2-R) and
ROUGE-SU4 (SU4-R) recall values. As can be seen
240
Models ROUGE TER (%) Sentences
2-R SU4-R Ins Del Sub Shift Count CR (%) Mod (%)
ICSI-1 11.03 13.96 ? ? ? ? 200 ? ?
B-K 11.71 14.47 0.2 26.2 2.3 0.4 216 74.0 63.9
MA-ILP 11.37 14.47 0.7 11.6 5.3 0.6 191 89.1 61.8
ILP w/o bigrams 9.24 12.66 0.8 15.4 11.8 1.2 205 85.4 80.0
ILP w/o salience 11.38 14.71 1.1 19.1 12.0 1.3 233 82.1 92.3
ILP w/o style 11.83 15.09 1.4 17.4 18.9 1.7 271 84.1 86.3
ILP w/o log-ratio 11.41 14.70 1.2 16.9 12.5 1.5 223 84.3 90.1
ILP w/o QTSG 10.32 13.68 0 0 0 0 163 100.0 0
Table 4: Performance of the multiple-aspect ILP model against comparison systems using ROUGE and the four com-
ponents of TER (insertion, deletion, substitution, shifts). In the lower section, performance of our model without (w/o)
each component in turn. The final columns show the number of source sentences, the average compression ratio, and
the proportion of sentences modified.
from the upper section of Table 4, the systems incor-
porating some form of rewriting gain slightly higher
ROUGE scores than ICSI-1. The multiple aspects
ILP system (MA-ILP) yields ROUGE scores simi-
lar to B-K, despite performing rewriting operations
which increase the scope for error and without re-
quiring any hand-crafted compression rules or man-
ually annotated training data. Indeed, the outputs of
the two systems are not significantly different under
ROUGE (using a paired t-test, p > 0.5).
In the lower section of Table 4, we show the per-
formance of our model when each of the contribut-
ing components described in Section 3 are removed.
Clearly the bigram content indicators are an impor-
tant element for the ROUGE scores, as their removal
yields a reduction of 2.46 points (see the row ILP
w/o bigrams in Table 4). The model without QTSG
rules (ILP w/o QTSG) is effectively limited to sen-
tence extraction, and removing rewrite rules also
lowers ROUGE scores to levels similar to ICSI-1.
ROUGE scores are increased by allowing the model
to select ?poor quality? sentences (ILP w/o style),
higher indeed than those of the B-K system. The
inclusion of non-summary language (ILP w/o log-
ratio) does not affect ROUGE scores to the same ex-
tent that bigrams and QTSG do.
Table 4 includes a break-down of the systems?
rewrite operations as measured by TER. We also
show the number of source sentences (Count), the
average compression ratio (CR %) and the propor-
tion of sentences modified (Mod %) by each system.
As can be seen, MA-ILP draws on fewer sentences,
Models Grammar Inform
ICSI-1 4.68 2.55
B-K 4.40 2.70
MA-ILP 4.68 3.90
ILP w/o style 3.30 2.67
Gold 4.90 4.75
Table 5: Mean ratings on system output output.
performs less deletion and more rewriting than B-K.
The number of deletions increases when individual
ILP components are removed and so does the num-
ber of substitutions. All the subsystems are more ag-
gressive in their rewriting than when used in com-
bination (higher TER, higher compression rate and
a larger number of sentences are modified). Expect-
edly, when removing the QTSG rules, the ILP is lim-
ited to a pure extractive system (last row in Table 4).
The results of our human evaluation study are
shown in Table 5. We elicited grammaticality and in-
formativeness ratings for a randomly selected model
summary, ICSI-1, B-K, the multiple aspect ILP
(MA-ILP), and the ILP w/o style which we in-
cluded in this study as it performed best under
ROUGE. ICSI-1, B-K, and MA-ILP are rated highly
on the grammaticality dimension. MA-ILP is in-
distinguishable from the sentence extraction sys-
tem (ICSI-1). Both systems are significantly more
grammatical than B-K (?< 0.05, using a Post-hoc
Tukey test). Notice that summaries created by the
ILP w/o style are rated poorly by humans, contrary
to ROUGE. The style component stops very short
241
Florida?s Governor Jeb Bush asked the US
Supreme Court to intervene to keep a comatose
woman alive, over the wishes of her husband,
who wants to disconnect the feeding tube that
has sustained her for 14 years. Her husband,
Michael Schiavo, and her parents, Robert and
Mary Schindler, have conflicts of interest that pre-
vent them from fairly deciding whether to keep
her alive. Some doctors have testified that Terri
Schiavo is in a persistent vegetative state with
no hope for recovery. The state House in Florida
passed a bill Thursday to extend life support for a
brain-damaged woman.
The space agencies of India and France signed an
agreement to cooperate in launching a satellite in
four years that will help make climate predictions
more accurate. The Indian Space Research Orga-
nization (ISRO) has short-listed experiments from
five nations including the United States, Britain
and Germany, for a slot on India?s unmanned
moon mission Chandrayaan-1 to be undertaken
by 2006-2007, the Press Trust of India (PTI) re-
ported Monday. India will launch more missions
to the moon if its maiden unmanned spacecraft
Chandrayaan-1, slated to be launched by 2008, is
successful in mapping the lunar surface.
Table 6: Example summaries generated by the multiple
aspects model (MA-ILP).
sentences and quotations from being included in the
summary even if they have quite high bigram or
content scores. Without it, the model tends to gen-
erate summaries that are fragmentary and lacking
proper context, resulting in lower grammaticality
(and informativeness) when judged by humans. The
MA-ILP system obtains the highest rating with re-
spect to information content. It is significantly better
(?< 0.05) than ICSI-1 and B-K. This is not entirely
surprising as our model includes additional content
selection elements over and above the bigram units.
There is still a significant gap from all systems to the
gold-standard human-authored summaries. Example
output summaries of the full ILP model are shown in
Table 6.
Overall, we obtain best results when considering
the contributions from the individual model experts
collectively. This suggests that additional improve-
ments could be obtained with more experts. It is also
possible that optimizing the relative weightings of
experts in the ILP objective would improve output.
The TER analysis shows that the experts have a tem-
pering effect on each other, resulting in less aggres-
sive, but qualitatively better, rewriting than when
used individually. Generally, experts work together
to shape an output sentence, but they can also com-
pete. In the future, we also plan to test the ability
of the model to adapt to other multi-document sum-
marization tasks, where the location of summary in-
formation is not as regular as it is in news articles.
We would also like interface our model with sen-
tence ordering and more generally with some notion
of the coherence of the generated summary.
Acknowledgments We are grateful to Micha El-
sner for his input on earlier versions of this work.
We would also like to thank members of the ILCC
at the School of Informatics for valuable discus-
sions and comments. We acknowledge the support
of EPSRC through project grants EP/I032916/1 and
EP/I017127/1.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481?490, Portland, Ore-
gon.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 137?144, Manchester, UK.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Pawan Deshpande, Regina Barzilay, and David Karger.
2007. Randomized decoding for selection-and-
ordering problems. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
242
Proceedings of the Main Conference, pages 444?451,
Rochester, New York.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
ACL Interactive Poster/Demonstration Sessions, pages
205?208, Sapporo, Japan.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the 20th
International Conference on Computational Linguis-
tics, pages 397?403, Geneva, Switzerland.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18, Boulder, Colorado.
Dan Gillick, Benoit Favre, and Dilek Hakkani-tu?r. 2008.
The ICSI summarization system at TAC 2008. In Pro-
ceedings of the Text Analysis Conference.
Dan Gillick, Benoit Favre, Dilek Hakkani-tu?r, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
ICSI/UTD summarization system at TAC 2009. In
Proceedings of the Text Analysis Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization
by sentence extraction. In Proceedings of the 2000
NAACL?ANLP Workshop on Automatic Summariza-
tion, pages 40?48, Seattle, Washington.
Dorit S. Hochba. 1997. Approximating covering
and packing problems: Set cover, vertex cover, in-
dependent set, and related problems. In Dorit S.
Hochba, editor, Approximation Algorithms for NP-
Hard Problems, pages 94?143. PWS Publishing Com-
pany, Boston, MA.
Honyang Jing. 2002. Using Hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?544.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st As-
sociation for Computational Linguistics, pages 423?
430, Sapporo, Japan.
Chin-Yew Lin and Eduard H. Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT?NAACL, pages 71?
78, Edmonton, Canada.
Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Pub Co.
Andre? Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing, pages 1?9, Boulder, Colorado.
Ryan McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceed-
ings of the 29th European conference on IR Research,
pages 557?564, Rome, Italy.
Rani Nelken and Stuart Schieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 161?168, Trento, Italy.
David Smith and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of Workshop on Statis-
tical Machine Translation, pages 23?30, NYC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231, Cambridge.
Karen Sparck Jones. 1999. Automatic summarizing:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1?33. MIT Press, Cambridge.
Kristian Woodsend and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector ma-
chine training. Computational Optimization and Ap-
plications.
Kristian Woodsend and Mirella Lapata. 2010. Automatic
generation of story highlights. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 565?574, Uppsala, Sweden.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, pages
513?523, Cambridge, MA.
243
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 546?556, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Comparison of Vector-based Representations for Semantic Composition
William Blacoe and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
w.b.blacoe@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we address the problem of
modeling compositional meaning for phrases
and sentences using distributional methods.
We experiment with several possible com-
binations of representation and composition,
exhibiting varying degrees of sophistication.
Some are shallow while others operate over
syntactic structure, rely on parameter learn-
ing, or require access to very large corpora.
We find that shallow approaches are as good
as more computationally intensive alternatives
with regards to two particular tests: (1) phrase
similarity and (2) paraphrase detection. The
sizes of the involved training corpora and the
generated vectors are not as important as the
fit between the meaning representation and
compositional method.
1 Introduction
Distributional models of semantics have seen con-
siderable success at simulating a wide range of be-
havioral data in tasks involving semantic cognition
and also in practical applications. For example, they
have been used to model judgments of semantic sim-
ilarity (McDonald, 2000) and association (Denhire
and Lemaire, 2004; Griffiths et al2007) and have
been shown to achieve human level performance
on synonymy tests (Landauer and Dumais, 1997;
Griffiths et al2007) such as those included in the
Test of English as a Foreign Language (TOEFL).
This ability has been put to practical use in numer-
ous natural language processing tasks such as au-
tomatic thesaurus extraction (Grefenstette, 1994),
word sense discrimination (Schu?tze, 1998), lan-
guage modeling (Bellegarda, 2000), and the iden-
tification of analogical relations (Turney, 2006).
While much research has been directed at the
most effective ways of constructing representations
for individual words, there has been far less con-
sensus regarding the representation of larger con-
structions such as phrases and sentences. The prob-
lem has received some attention in the connection-
ist literature, particularly in response to criticisms of
the ability of connectionist representations to handle
complex structures (Smolensky, 1990; Plate, 1995).
More recently, several proposals have been put for-
ward for computing the meaning of word combina-
tions in vector spaces. This renewed interest is partly
due to the popularity of distributional methods and
their application potential to tasks that require an un-
derstanding of larger phrases or complete sentences.
For example, Mitchell and Lapata (2010) intro-
duce a general framework for studying vector com-
position, which they formulate as a function f of
two vectors u and v. Different composition mod-
els arise, depending on how f is chosen. Assuming
that composition is a linear function of the Cartesian
product of u and v allows to specify additive mod-
els which are by far the most common method of
vector combination in the literature (Landauer and
Dumais, 1997; Foltz et al1998; Kintsch, 2001).
Alternatively, assuming that composition is a linear
function of the tensor product of u and v, gives rise
to models based on multiplication.
One of the most sophisticated proposals for se-
mantic composition is that of Clark et al2008) and
the more recent implementation of Grefenstette and
546
Sadrzadeh (2011a). Using techniques from logic,
category theory, and quantum information they de-
velop a compositional distributional semantics that
brings type-logical and distributional vector space
models together. In their framework, words belong
to different type-based categories and different cate-
gories exist in different dimensional spaces. The cat-
egory of a word is decided by the number and type of
adjoints (arguments) it can take and the composition
of a sentence results in a vector which exists in sen-
tential space. Verbs, adjectives and adverbs act as re-
lational functions, are represented by matrices, and
modify the properties of nouns, that are represented
by vectors (see also Baroni and Zamparelli (2010)
for a proposal similar in spirit). Clarke (2012) intro-
duces context-theoretic semantics, a general frame-
work for combining vector representations, based on
a mathematical theory of meaning as context, and
shows that it can be used to describe a variety of
models including that of Clark et al2008).
Socher et al2011a) and Socher et al2011b)
present a framework based on recursive neural net-
works that learns vector space representations for
multi-word phrases and sentences. The network is
given a list of word vectors as input and a binary
tree representing their syntactic structure. Then, it
computes an n-dimensional representation p of two
n-dimensional children and the process is repeated
at every parent node until a representation for a full
tree is constructed. Parent representations are com-
puted essentially by concatenating the representa-
tions of their children. During training, the model
tries to minimize the reconstruction errors between
the n-dimensional parent vectors and those repre-
senting their children. This model can also compute
compositional representations when the tree struc-
ture is not given, e.g., by greedily inferring a binary
tree.
Although the type of function used for vector
composition has attracted much attention, relatively
less emphasis has been placed on the basic distri-
butional representations on which the composition
functions operate. In this paper, we examine three
types of distributional representation of increasing
sophistication and their effect on semantic composi-
tion. These include a simple semantic space, where
a word?s vector represents its co-occurrence with
neighboring words (Mitchell and Lapata, 2010),
a syntax-aware space based on weighted distribu-
tional tuples that encode typed co-occurrence rela-
tions among words (Baroni and Lenci, 2010), and
word embeddings computed with a neural language
model (Bengio, 2001; Collobert and Weston, 2008).
Word embeddings are distributed representations,
low-dimensional and real-valued. Each dimension
of the embedding represents a latent feature of the
word, hopefully capturing useful syntactic and se-
mantic properties.
Using these representations, we construct several
compositional models, based on addition, multipli-
cation, and recursive neural networks. We assess
the effectiveness of these models using two evalua-
tion protocols. The first one involves modeling sim-
ilarity judgments for short phrases gathered in hu-
man experiments (Mitchell and Lapata, 2010). The
second one is paraphrase detection, i.e., the task of
examining two sentences and determining whether
they have the same meaning (Socher et al2011a).
We find that shallow approaches are as good as
more computationally intensive alternatives. They
achieve considerable semantic expressivity without
any learning, sophisticated linguistic processing, or
access to very large corpora.
Our contributions in this work are three-fold: an
empirical comparison of a broad range of composi-
tional models, some of which are introduced here for
the first time; the use of an evaluation methodology
that takes into account the full spectrum of compo-
sitionality from phrases to sentences; and the em-
pirical finding that relatively simple compositional
models can be used to perform competitively on the
paraphrase detection and phrase similarity tasks.
2 Modeling
The elementary objects that we operate on are vec-
tors associated with words. We instantiate these
word representations following three distinct seman-
tic space models which we describe in Section 2.1
below. Analogously, in Section 2.2 we consider
three methods of vector composition, i.e., how a
phrase or a sentence can be represented as a vector
using the vectors of its constituent words. Combin-
ing different vector representations and composition
methods gives rise to several compositional models
whose performance we evaluate in Sections 3 and 4.
547
2.1 Word Representations
For all of our experiments we employ column vec-
tors from a Cartesian, finitely-dimensional space.
The dimensionality will depend on the source of
the vectors involved. Similarly, the component val-
ues inside each source?s vectors are not to be inter-
preted in the same manner. Nonetheless, they have
in common that they originate from distributive cor-
pus statistics.
Co-occurence-based Semantic Space Word
meaning is commonly represented in a high-
dimensional space, where each component corre-
sponds to some contextual element in which the
word is found. The contextual elements can be
words themselves, or larger linguistic units such as
sentences or documents, or even more complex lin-
guistic representations such as the argument slots of
predicates. A semantic space that is often employed
in studying compositionality across a variety of
tasks (Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011a) uses a context window of five
words on either side of the target word, and 2,000
vector dimensions. These are the common context
words in the British National Corpus (BNC), a
corpus of about 100 million tokens. Their values
are set to the ratio of the probability of the context
word given the target word to the probability of the
context word overall.
More formally, let us consider the BNC as a set of
sentences:
BNC = {Sen(BNC)1 , ...,Sen
(BNC)
nBNC } (1)
where the i-th sentence is a sequence of words
Seni = (w
(i)
1 , ...,w
(i)
ni ) from the BNC?s vocabulary
VocBNC. Then f reqw is the amount of times
that each word w ? VocBNC appears in the BNC.
Mitchell and Lapata (2010) collect the M most
frequent non-stoplist words in the set ctxttop =
{w(top)1 , ...,w
(top)
M } and let them consitute the word
vectors? dimensions. Each dimension?s value is ob-
tained from a co-occurrence count:
coCountw[ j] =
nBNC
?
i=1
ni
?
t=1
(2)
|{k ? [t?5; t +5] |w(i)t = w, w
(i)
k = w
(top)
j }|
for w?VocBNC and j = 1, ...,M. Using these counts,
they define word vectors component-wise.
wdVec(rp)w [ j] =
p(w(top)j |w)
p(w(top)j )
= (3)
coCountw[ j]
f reqw
?
totalCount
f req
w(top)j
for j = 1, ...,M, where totalCount is the total num-
ber of words in the BNC.
This space is relatively simple, it has few param-
eters, requires no preprocessing other than tokeniza-
tion and involves no syntactic information or param-
eter learning. Despite its simplicity, it is a good start-
ing point for studying representations for composi-
tional models as a baseline against which to evaluate
more elaborate models.
Neural Language Model Another perhaps less
well-known approach to meaning representation is
to represent words as continuous vectors of param-
eters. Such word vectors can be obtained with an
unsupervised neural language model (NLM, Bengio
(2001); Collobert and Weston (2008)) which jointly
learns an embedding of words into a vector space
and uses these vectors to predict how likely a word
is, given its context.
We induced word embeddings with Collobert
and Weston (2008)?s neural language model. The
model is discriminative and non-probabilistic. Each
word i ? D (the vocabulary) is embedded into a
d-dimensional space using a lookup table LTW (?):
LTW (i) =Wi (4)
where W ? Rd?|D| is a matrix of parameters to be
learned. Wi ? Rd is the i-th column of W and d is
the word vector size to be chosen by the user. The
parameters W are automatically trained during the
learning process using backpropagation.
Specifically, at each training update, the model
reads an n-gram x = (w1, . . . ,wn) from the cor-
pus. The n-gram is paired with a corrupted n-gram
x? = (w1, . . . , w?n) where w?n 6= wn is chosen uniformly
from the vocabulary. The model concatenates the
learned embeddings of the n words and predicts a
score for the n-gram sequence using the learned em-
beddings as features. The training criterion is that
548
n-grams that are present in the training corpus must
have a score at least some margin higher than the
corrupted n-grams. The model learns via gradient
descent over the neural network parameters and the
embedding lookup table. Word vectors are stored in
a word embedding matrix which captures syntactic
and semantic information from co-occurrence statis-
tics. As these representations are learned, albeit in
an unsupervised manner, one would hope that they
capture word meanings more succinctly, compared
to the simpler distributional representations that are
merely based on co-occurrence.
We trained the neural language model on the
BNC. We optimized the model?s parameters on a
word similarity task using 4% of the BNC as de-
velopment data. Specifically, we used WordSim353,
a benchmark dataset (Finkelstein et al2001), con-
sisting of relatedness judgments (on a scale of 0
to 10) for 353 word pairs. We experimented with
vectors of varying dimensionality (ranging from 50
to 200, with a step size of 50). The size of the target
word?s context window was 2, 3 and 4 in turn. The
rate at which embeddings were learned ranged from
3.4? 10?10 to 6.7? 10?10 to 10?9. We ran each
training process for 1.1?108 to 2.7?108 iterations
(ca. 2 days). We obtained the best results with 50
dimensions, a context window of size 4, and a em-
bedding learning rate of 10?9. The NLM with these
parameters was then trained for 1.51?109 iterations
(ca. 2 weeks).
Figure 1 illustrates a two-dimensional projection
of the embeddings for the 500 most common words
in the BNC. We only show two out of the actual
50 dimensions involved, but one can already begin
to see clusterings of a syntactic and semantic na-
ture. In one corner, for example, we encounter a
grouping of possessive pronouns together with the
possessive clitic ?s. The singular ones my, her and
his are closely positioned, as are the plural ones our,
your and their. Also, there is a clustering of socio-
political terms, such as international, country, na-
tional, government, and council.
DistributionalMemory Tensor Baroni and Lenci
(2010) present Distributional Memory, a general-
ized framework for distributional semantics from
which several special-purpose models can be de-
rived. In their framework distributional information
Figure 1: A two-dimensional projection of the word em-
beddings we trained on the BNC using Turian et al
(2010) implementation of the NLM. Two small sections
have been blown up to a legible scale. They show exam-
ples of syntactic and semantic clustering, respectively.
word w link l co-word v value c
1950s-n of essence-n 2.4880
1950s-n during bring-v 16.4636
Anyone-n nmod reaction-n 1.2161
American-n coord-1 athlete-n 5.6485
American-j nmod wasp-n 3.4945
American-n such as-1 country-n 14.4269
American-n sbj tr build-v 23.1014
Table 1: Example entries in Baroni and Lenci (2010)?s
tensor
is extracted from the corpus once, in the form of a
set of weighted word-link-word tuples arranged into
a third-order tensor. Different matrices are then gen-
erated from the tensor, and their rows and columns
give rise to different semantic spaces appropriate for
capturing different semantic problems. In this way,
the same distributional information can be shared
across tasks such as word similarity or analogical
learning.
More formally, Baroni and Lenci (2010) con-
struct a 3-dimensional tensor T assigning a value c
to instances of word pairs w,v and a connecting
link-word l. This representation operates over a
dependency-parsed corpus and the scores c are ob-
tained via counting the occurrences of tuples, and
weighting the raw counts by mutual information.
Table 1 presents examples of tensor entries. These
were taken from a distributional memory tensor1
1Available at http://clic.cimec.unitn.it/dm/.
549
frequency link l co-word v
17059 obj include-v
16713 obj use-v
16573 obj call-v
16475 obj see-v
15962 obj make-v
15707 nmod-1 other-j
15554 nmod-1 new-j
15224 obj find-v
15221 nmod-1 more-j
14715 nmod-1 first-j
14348 obj give-v
Table 2: The 11 most frequent contexts in Baroni and
Lenci (2010)?s tensor (v and j represent verbs and adjec-
tives, respectively).
that Baroni and Lenci obtained via preprocessing
several corpora: the web-derived ukWac corpus of
about 1.915 billion words, a mid-2009 dump of
the English Wikipedia containing about 820 million
words, and the BNC.
Extracting a 3-dimensional tensor from the BNC
alone would create very sparse representations.
We therefore extract so-called word-fibres, essen-
tially projections onto a lower-dimensional sub-
space, from the same tensor Baroni and Lenci (2010)
collectively derived from the 3 billion word corpus
just described (henceforth 3-BWC). We view the
3-dimensional tensor
T = {(w(T )1 , l
(T )
1 ,v
(T )
1 ,c
(T )
1 ), ...} (5)
as a mapping which assigns each target word w a
non-zero value c, given the context (l,v). All word-
context combinations not listed in T are implicitly
assigned a zero value.
Now we consider two possible approaches for
obtaining vectors, depending on their application.
First, we let the D most frequent contexts
ctxtD = {(l1,v1), ...,(lD,vD)} (6)
constitute the D dimensions that each word vec-
tor will have. Table 2 shows the 11 contexts (l,v)
that appear most frequently in T . Thus, each target
word?s vector is defined component-wise as:
wdVecw[ j] =
{
c, if (w, l j,v j,c) ? T
0, otherwise
(7)
for j = 1, ...,D. This approach is used when a fixed
vector dimensionality is necessary.
A more dynamic approach is possible when very
few words w1, ...,wn are involved in a test. Their
representations can then have a denser format, that
is, with no zero-valued components. For this we
identify the set of contexts common to the words in-
volved,
ctxtdyn = {(l
(dyn)
1 ,v
(dyn)
1 ),(l
(dyn)
2 ,v
(dyn)
2 ), ...} (8)
= {(l,v) |(wi, l,v,c) ? T,c ? R, i = 1, ...,n}
Each context (l,v) again constitutes a vector dimen-
sion. The dimensionality varies strongly depend-
ing on the selection of words, but if n does not ex-
ceed 4, the dimensionality |ctxtdyn| will typically be
substantial enough. In this approach, each word?s
vector consists of the values c found along with that
word and its context in the tensor.
wdVecwi [ j] = c, (9)
where (wi, l
(dyn)
j ,v
(dyn)
j ,c)? T , for j = 1, ..., |ctxtdyn|.
2.2 Composition Methods
In our experiments we compose word vectors to cre-
ate representations for phrase vectors and sentence
vectors. The phrases we are interested in consist of
two words each: an adjective and a noun like black
hair, a compound noun made up of two nouns such
as oil industry, or a verbal phrase with a transitive
verb and an object noun, e.g., pour tea.
Conceiving of a phrase phr = (w1,w2) as a binary
tuple of words, we obtain its vector from its words?
vectors either by addition:
phrVec(w1,w2) = wdVecw1 +wdVecw2 (10)
or by point-wise multiplication:
phrVec(w1,w2) = wdVecw1wdVecw2 (11)
In the same way we acquire a vector senVeci rep-
resenting a sentence Seni = (w
(i)
1 , ...,w
(i)
ni ) from the
vectors for w1, ...,wni . We simply sum the existing
word vectors, that is, vectors obtained via the respec-
tive corpus for words that are not on our stoplist:
senVec(+)i [ j] = ?
k=1,...,ni
wdVecwk exists
wdVecwk [ j] (12)
550
And do the same with point-wise multiplication:
senVec()i [ j] = ?
k=1,...,ni
wdVecwk exists
wdVecwk [ j] (13)
The multiplication model in (13) can be seen as an
instantiation of the categorical compositional frame-
work put forward by Clark et al2008). In fact,
a variety of multiplication-based models can be de-
rived from this framework; and comparisons against
component-wise multiplication on phrase similar-
ity tasks yield comparable results (Grefenstette
and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh,
2011b). We thus opt for the model (13) as an exam-
ple of compositional models based on multiplication
due to its good performance across a variety of tasks,
including language modeling and prediction of read-
ing difficulty (Mitchell, 2011).
Our third method, for creating phrase and sen-
tence vectors alike, is the application of Socher et
al. (2011a)?s model. They use the Stanford parser
(Klein and Manning, 2003) to create a binary parse
tree for each input phrase or sentence. This tree is
then used as the basis for a deep recursive autoen-
coder (RAE). The aim is to construct a vector rep-
resentation for the tree?s root bottom-up where the
leaves contain word vectors. The latter can in the-
ory be provided by any type of semantic space, how-
ever Socher et alse word embeddings provided by
the neural language model (Collobert and Weston,
2008).
Given the binary tree input structure, the model
computes parent representations p from their chil-
dren (c1,c2) using a standard neural network layer:
p = f (W (1)[c1;c2]+b
(1)), (14)
where [c1;c2] is the concatenation of the two chil-
dren, f is an element-wise activation function such
as tanh, b is a bias term, and W ? Rn?2n is an en-
coding matrix that we want to learn during training.
One way of assessing how well p represents its di-
rect children is to decode their vectors in a recon-
struction layer:
[c?1;c
?
2] = f (W
(2)p+b(2)) (15)
During training, the goal is to minimize the re-
construction errors of all input pairs at nontermi-
nal nodes p in a given parse tree by computing the
square of the Euclidean distance between the origi-
nal input and its reconstruction:
Erec([c1;c2]) =
1
2
|[c1;c2]? [c
?
1;c
?
2]|
2 (16)
Socher et al2011a) extend the standard re-
cursive autoencoder sketched above in two ways.
Firstly, they present an unfolding autoencoder that
tries to reconstruct all leaf nodes underneath each
node rather than only its direct children. And sec-
ondly, instead of transforming the two children di-
rectly into a parent p, they introduce another hidden
layer inbetween.
We obtained three compositional models per rep-
resentation resulting in nine compositional mod-
els overall. Plugging different representations into
the additive and multiplicative models is relatively
straightforward. The RAE can also be used with
arbitrary word vectors. Socher et al2011a) ob-
tain best results with 100-dimensional vectors which
we also used in our experiments. NLM vectors
were trained with this dimensionality on the BNC
for 7.9? 108 iterations (with window size 4 and an
embedding learning rate of 10?9). We constructed
a simple distributional space with M = 100 dimen-
sions, i.e., those connected to the 100 most frequent
co-occurrence words. In the case of vectors obtained
from Baroni and Lenci (2010)?s DM tensor, we dif-
ferentiated between phrases and sentences, due to
the disparate amount of words contained in them
(see Section 2.1). To represent phrases, we used
vectors of dynamic dimensionality, since these form
a richer and denser representation. The sentences
considered in Section 4 are too large for this ap-
proach and all word vectors must be members of
the same vector space. Hence, these sentence vec-
tors have fixed dimensionality D = 100, consisting
of the ?most significant? 100 dimensions, i.e., those
reflecting the 100 most frequent contexts.
3 Experiment 1: Phrase Similarity
Our first experiment focused on modeling similarity
judgments for short phrases gathered in human ex-
periments. Distributional representations of individ-
ual words are commonly evaluated on tasks based
on their ability to model semantic similarity rela-
tions, e.g., synonymy or priming. Thus, it seems
appropriate to evaluate phrase representations in a
551
dim. c.m. Adj-N N-N V-Obj
2000 + 0.37 0.38 0.28
SDS
2000  0.48 0.50 0.35
(BNC)
100 RAE 0.31 0.30 0.28
vary + 0.37 0.30 0.29
DM
vary  0.21 0.37 0.33
(3-BWC)
100 RAE 0.25 0.26 0.09
50 + 0.28 0.26 0.24
NLM
50  0.26 0.22 0.18
(BNC)
100 RAE 0.19 0.24 0.28
Table 3: Correlation coefficients of model predictions
with subject similarity ratings (Spearman?s ?); columns
show dimensionality: fixed or varying (see Section 2.1),
composition method: + is additive vector composition,
 is component-wise multiplicative vector composition,
RAE is Socher et al2011a)?s recursive auto-encoder.
similar manner. Specifically, we used the dataset
from Mitchell and Lapata (2010) which contains
similarity judgments for adjective-noun, noun-noun
and verb-object phrases, respectively.2 Each item is
a phrase pair phr1, phr2 which has a human rating
from 1 (very low similarity) to 7 (very high similar-
ity).
Using the composition models described above,
we compute the cosine similarity of phr1 and phr2:
phrSimphr1,phr2 =
phrVecphr1 ? phrVecphr2
|phrVecphr1 |? |phrVecphr2 |
(17)
Model similarities were evaluated against the human
similarity ratings using Spearman?s ? correlation co-
efficient.
Table 3 summarizes the performance of the vari-
ous models on the phrase similarity dataset. Rows
in the table correspond to different vector repre-
sentations: the simple distributional semantic space
(SDS) from Mitchell and Lapata (2010), Baroni and
Lenci?s (2010) distributional memory tensor (DM)
and the neural language model (NLM), for each
phrase combination: adjective noun (Adj-N), noun-
noun (N-N) and verb object (V-Obj). For each
phrase type we report results for each compositional
model, namely additive (+), multiplicative () and
recursive autoencoder (RAE). The table also shows
2The dataset is publicly available from http:
//homepages.inf.ed.ac.uk/s0453356/share
the dimensionality of the input vectors next to the
vector representation.
As can be seen, for SDS the best performing
model is multiplication, as it is mostly for DM. With
regard to NLM, vector addition yields overall better
results. In general, neither DM or NLM in any com-
positional configuration are able to outperform SDS
with multiplication. All models in Table 3 are sig-
nificantly correlated with the human similarity judg-
ments (p < 0.01). Spearman?s ? differences of 0.3
or more are significant at the 0.01 level, using a t-
test (Cohen and Cohen, 1983).
4 Experiment 2: Paraphrase Detection
Although the phrase similarity task gives a fairly
direct insight into semantic similarity and compo-
sitional representations, it is somewhat limited in
scope as it only considers two-word constructions
rather than naturally occurring sentences. Ideally,
we would like to augment our evaluation with a task
which is based on large quantities of natural data and
for which vector composition has practical conse-
quences. For these reasons, we used the Microsoft
Research Paraphrase Corpus (MSRPC) introduced
by Dolan et al2004). The corpus consists of sen-
tence pairs Seni1 ,Seni2 and labels indicating whether
they are in a paraphrase relationship or not. The vec-
tor representations obtained from our various com-
positional models were used as features for the para-
phrase classification task.
The MSRPC dataset contains 5,801 sentence
pairs, we used the standard split of 4,076 training
pairs (67.5% of which are paraphrases) and 1,725
test pairs (66.5% of which are paraphrases). In order
to judge whether two sentences have the same mean-
ing we employ Fan et al2008)?s liblinear classifier.
For each of our three vector sources and three differ-
ent compositional methods, we create the following
features: (a) a vector representing the pair of input
sentences either via concatenation (?con?) or sub-
traction (?sub?); (b) a vector encoding which words
appear therein (?enc?); and (c) a vector made up of
the following four other pieces of information: the
cosine similarity of the sentence vectors, the length
of Seni1 , the length of Seni2 , and the unigram overlap
among the two sentences.
In order to encode which words appear in
552
NLM DM SDS
(BNC) (3-BWC) (BNC)
+ 69.04 73.51 72.93
(con, other) (other) (other)
 67.83 67.54 73.04
(sub, other) (other) (other)
RAE 70.26 68.29 69.10
(con, other) (sub, other) (con, other)
Table 4: Paraphrase classification accuracy in %. In-
cluded features are in parentheses: ?con? is sentence vec-
tor concatenation, ?sub? is sentence vector subtraction,
?other? stands for 4 other features (see Section 4)
each sentence and how often, we define a vec-
tor wdCounti for sentence Seni and enumerate all
words occuring in the MSRPC:
VocMSRPC = {w
(MSRPC)
1 , ...,w
(MSRPC)
nMSRPC } (18)
giving the word count vectors nMSRPC dimensions.
Thus the k-th component of wdCounti is the fre-
quency with which the word w(MSRPC)k appears in
Seni = (w
(i)
1 , ...,w
(i)
ni ):
wdCounti[k] = |{ j ? [1;ni] |w
(MSRPC)
k = w
(i)
j }| (19)
for k = 1, ...,nMSRPC. Even though nMSRPC may be
large, the computer files storing our feature vectors
do not explode in size because wdCount contains
many zeros and the classifier allows a sparse nota-
tion of (non-zero) feature values.
Regarding the last four features, we measured the
similarity between sentences the same way as we did
with phrases in section 3.
senSimi1,i2 =
senVeci1 ? senVeci2
|senVeci1 |? |senVeci2 |
(20)
Note that this is the cosine of the angle between
senVeci1 and senVeci2 . This enables us to observe
the similarity or dissimilarity of two sentences inde-
pendent of their sentence length. Even though each
contained word increases or decreases the norm of
the resulting sentence vector, this does not distort
the overall similarity value, due to normalization.
The lengths of Seni1 and Seni2 are simply the
number of words they contain. The unigram over-
lap feature value may be viewed as the cardinal-
NLM DM SDS
(BNC) (3-BWC) (BNC)
+ 81.00 82.16 80.76
(con, other) (other) (other)
 80.41 80.18 82.33
(sub, other) (other) (other)
RAE 81.28 80.43 80.68
(con, other) (sub, other) (con, other)
Table 5: Paraphrase classification F1-score in %. The
involved features are exactly the same as in Table 4.
ity of the intersection of each sentence?s multiset-
bag-of-words. The latter is encoded in the already-
introduced wdCount vectors. Therefore,
uniOverlapi1,i2 =
nMSRPC
?
k=1
min
s=1,2
{wdCountis [k]} (21)
In order to establish which features work best for
each representation and composition method, we ex-
haustively explored all combinations on a develop-
ment set (20% of the original MSRPC training set).
Tables 4 (accuracy) and 5 (F1) show our results on
the test set with the best feature combinations for
each model (shown in parentheses). Each row cor-
responds to a different type of composition and each
column to a different word representation model.
As can be seen, the distributional memory (DM)
is the best performing representation for the addi-
tive composition model. The neural language model
(NLM) gives best results for the recursive autoen-
coder (RAE), although the other two representations
come close. And finally the simple distributional
semantic space (SDS) works best with multiplica-
tion. Also note that the best performing models,
namely DM with addition and SDS with multipli-
cation, use a basic feature space consisting only of
the cosine similarity of the composed sentence vec-
tors, the length of the two sentences involved, and
their unigram word overlap.
Although our intention was to use the paraphrase
detection task as a test-bed for evaluating composi-
tional models rather than achieving state-of-the-art
results, Table 6 compares our approach against pre-
vious work on the same task and dataset. Initial re-
search concentrated on individual words rather than
sentential representations. Several approaches used
553
Model Acc. F1
Baseline 66.5 79.9
Mihalcea et al2006) 70.3 81.3
Rus et al2008) 70.6 80.5
Qiu et al2006) 72.0 81.6
Islam and Inkpen (2007) 72.6 81.3
Mitchell and Lapata (2010) () 73.0 82.3
Baroni and Lenci (2010) (+) 73.5 82.2
Fernando and Stevenson (2008) 74.1 82.4
Wan et al2006) 75.6 83.0
Das and Smith (2009) 76.1 82.7
Socher et al2011a) 76.8 83.6
Table 6: Overview of results on the MSRCP (test corpus).
Accuracy differences of 3.3 or more are significant at the
0.01 level (using the ?2 statistic).
WordNet in conjunction with distributional similar-
ity in an attempt to detect meaning conveyed by syn-
onymous words (Islam and Inkpen, 2007; Mihalcea
et al2006; Fernando and Stevenson, 2008). More
recently, the addition of syntactic features based
on dependency parse trees (Wan et al2006; Das
and Smith, 2009) has been shown to substantially
boost performance. The model of Das and Smith
(2009), for example, uses quasi-synchronous depen-
dency grammar to model the structure of the sen-
tences involved in the comparison and their corre-
spondences. Socher et al2011a) obtain an accu-
racy that is higher than previously published results.
This model is more sophisticated than the one we
used in our experiments (see Table 4 and 5). Rather
than using the output of the RAE as features for the
classifier, it applies dynamic pooling, a procedure
that takes a similarity matrix as input (e.g., created
by sentences with differing lengths) and maps it to
a matrix of fixed size that represents more faithfully
the global similarity structure.3
Overall, we observe that our own models do as
well as some of the models that employ WordNet
and more sophisticated syntactic features. With re-
gard to F1, we are comparable with Das and Smith
(2009) and Socher et al2011a) without using elab-
orate features, or any additional manipulations over
and above the output of the composition functions
3Without dynamic pooling, their model yields an accuracy
of 74.2.
which if added could increase performance.
5 Discussion
In this paper we systematically compared three types
of distributional representation and their effect on
semantic composition. Our comparisons involved
a simple distributional semantic space (Mitchell
and Lapata, 2010), word embeddings computed
with a neural language model (Collobert and We-
ston, 2008) and a representation based on weighted
word-link-word tuples arranged into a third-order
tensor (Baroni and Lenci, 2010). These represen-
tations vary in many respects: the amount of pre-
processing and linguistic information involved (the
third-order tensor computes semantic representa-
tions over parsed corpora), whether the semantic
space is the by-product of a learning process (in the
neural language model the parameters of the lookup
table must be learned), and data requirements (the
third-order tensor involves processing billions of
words). These representations served as input to
three composition methods involving addition, mul-
tiplication and a deep recursive autoencoder. Again
these methods differ in terms of how they imple-
ment compositionality: addition and multiplication
are commutative and associative operations and thus
ignore word order and, more generally, syntactic
structure. In contrast, the recursive autoencoder is
syntax-aware as it operates over a parse tree. How-
ever, the composed representations must be learned
with a neural network.
We evaluated nine models on the complementary
tasks of phrase similarity and paraphrase detection.
The former task simplifies the challenge of find-
ing an adequate method of composition and places
more emphasis on the representation, whereas the
latter poses, in a sense, the ultimate challenge for
composition models. It involves entire sentences
exhibiting varied syntactic constructions and in the
limit involves genuine natural language undertand-
ing. Across both tasks our results deliver a consis-
tent message: simple is best. Despite being in the-
ory more expressive, the representations obtained by
the neural language model and the third-order ten-
sor cannot match the simple semantic space on the
phrase similarity task. In this task syntax-oblivious
composition models are superior to the more sophis-
554
ticated recursive autoencoder. The latter performs
better on the paraphrase detection task when its out-
put is fed to a classifier. The simple semantic space
may not take word order or sentence structure into
account, but nevertheless achieves considerable se-
mantic expressivity: it is on par with the third-order
tensor without having access to as much data (3 bil-
lion words) or a syntactically parsed corpus.
What do these findings tell us about the future of
compositional models for distributional semantics?
The problem of finding the right methods of vec-
tor composition cannot be pursued independent of
the choice of lexical representation. Having tested
many model combinations, we argue that in a good
model of distributive semantics representation and
composition must go hand in hand, i.e., they must
be mutually learned.
Acknowledgments We are grateful to Jeff
Mitchell for his help with the re-implementation
of his models. Thanks to Frank Keller and Micha
Elsner for their input on earlier versions of this work
and to Richard Socher for technical assistance. We
acknowledge the support of EPSRC through project
grant EP/I032916/1.
References
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?
721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA.
Jerome R. Bellegarda. 2000. Exploiting latent seman-
tic information in statistical language modeling. Pro-
ceedings of the Institute of of Electrical and Electron-
ics Engineers, 88(8):1279?1296.
Yoshua Bengio. 2001. Neural net language models.
Scholarpedia, 3(1):3881.
Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.
2008. A compositional distributional model of mean-
ing. In Proceedings of the 2nd Quantum Interaction
Symposium, pages 133?140, Oxford, UK.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41?71.
J Cohen and P Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
the 25th International Conference on Machine Learn-
ing, pages 160?167, New York, NY. ACM.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 468?476, Suntec,
Singapore.
G. Denhire and B. Lemaire. 2004. A computational
model of children?s semantic memory. In Proceedings
of the 26th Annual Meeting of the Cognitive Science
Society, pages 297?302, Mahwah, NJ. Lawrence Erl-
baum Associates.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 350?356, Geneva,
Switzerland. COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Samuel Fernando and Mark Stevenson. 2008. A seman-
tic similarity approach to paraphrase detection. Tech-
nology.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the concept
revisited. In WWW, pages 406?414.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with la-
tent semantic analysis. Discourse Process, 15:285?
307.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 1394?1404, Edinburgh,
Scotland.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b.
Experimenting with transitive verbs in a DisCoCat. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
62?66, Edinburgh, UK.
555
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Aminul Islam and Diana Inkpen. 2007. Semantic sim-
ilarity of short texts. In Proceedings of the Interna-
tional Conference on Recent Advances in Natural Lan-
guage Processing, Borovets, Bulgaria.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: the latent semantic analysis theory
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104(2):211?240.
Scott McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava,
2006. Corpus-based and knowledge-based measures
of text semantic similarity, pages 775?780. AAAI
Press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
38(8):1388?1429.
Jeff Mitchell. 2011. Composition in distributional mod-
els of semantics. Ph.D. thesis, University of Edin-
burgh.
Tony A. Plate. 1995. Holographic reduced repre-
sentations. IEEE Transactions on Neural Networks,
6(3):623?641.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 18?26, Sydney, Australia.
Vasile Rus, Philip M. McCarthy, Mihai C. Lintean,
Danielle S. McNamara, and Arthur C. Graesser. 2008.
Paraphrase identification with lexico-syntactic graph
subsumption. In David Wilson and H. Chad Lane, ed-
itors, Florida Artificial Intelligence Research Society
Conference, pages 201?206. AAAI Press.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?124.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46:159?
216.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011a.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in Neu-
ral Information Processing Systems 24, pages 801?
809. Granada, Spain.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161, Edinburgh, Scot-
land.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384?394, Uppsala, Sweden.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
?para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop 2006,
pages 131?138, Sydney, Australia.
556
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1423?1433, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Grounded Models of Semantic Representation
Carina Silberer and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
c.silberer@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
A popular tradition of studying semantic rep-
resentation has been driven by the assump-
tion that word meaning can be learned from
the linguistic environment, despite ample ev-
idence suggesting that language is grounded
in perception and action. In this paper we
present a comparative study of models that
represent word meaning based on linguistic
and perceptual data. Linguistic information is
approximated by naturally occurring corpora
and sensorimotor experience by feature norms
(i.e., attributes native speakers consider impor-
tant in describing the meaning of a word). The
models differ in terms of the mechanisms by
which they integrate the two modalities. Ex-
perimental results show that a closer corre-
spondence to human data can be obtained by
uncovering latent information shared among
the textual and perceptual modalities rather
than arriving at semantic knowledge by con-
catenating the two.
1 Introduction
Distributional models of lexical semantics have seen
considerable success at accounting for a wide range
of behavioral data in tasks involving semantic cog-
nition (Landauer and Dumais, 1997; Griffiths et
al., 2007). These models have also enjoyed last-
ing popularity in natural language processing. Ex-
amples involve information retrieval (Salton et al
1975), word sense discrimination (Schu?tze, 1998),
text segmentation (Choi et al 2001), and numerous
studies of lexicon acquisition (Grefenstette, 1994;
Lin, 1998). Despite their widespread use, distribu-
tional models have been criticized as ?disembodied?
in that they learn exclusively from linguistic infor-
mation but are not grounded in perception and ac-
tion (Perfetti, 1998; Barsalou, 1999; Glenberg and
Kaschak, 2002).
This lack of grounding contrasts with many ex-
perimental studies suggesting that word meaning is
acquired not only from exposure to the linguistic
environment but also from our interaction with the
physical world (Landau et al 1998; Bornstein et al
2004). Beyond language acquisition, there is consid-
erable evidence across both behavioral experiments
and neuroimaging studies that the perceptual asso-
ciates of words play an important role in language
processing (for a review see Barsalou (2008)).
It is thus no surprise that recent years have wit-
nessed the emergence of perceptually grounded dis-
tributional models. An important question in the for-
mulation of such models concerns the provenance
of perceptual information. A few models use fea-
ture norms as a proxy for sensorimotor experience
(Howell et al 2005; Andrews et al 2009; Steyvers,
2010; Johns and Jones, 2012). These are obtained
by asking native speakers to write down attributes
they consider important in describing the meaning
of a word. The attributes represent perceived phys-
ical and functional properties associated with the
referents of words. For example, apples are typi-
cally green or red, round, shiny, smooth, crunchy,
tasty, and so on; dogs have four legs and bark,
whereas chairs are used for sitting. Other models fo-
cus solely on the visual modality under the assump-
tion that it represents a major source of data from
1423
which humans can learn semantic representations
of both linguistic and non-linguistic communicative
actions (Regier, 1996). For example, Feng and Lap-
ata (2010) learn semantic representations from cor-
pora of texts paired with naturally co-occurring im-
ages (e.g., news articles and their associated pic-
tures), whereas Bruni et al(2011) learn textual and
visual representations independently from distinct
data sources.
Aside from the type of data used to capture per-
ceptual information, another important issue con-
cerns how the two modalities (perceptual and tex-
tual) are integrated. A simple solution would be
to learn both modalities independently (Bruni et al
2011) or to infer one modality by means of the other
(Johns and Jones, 2012) and to arrive at a grounded
representation simply by concatenating the two. An
alternative is to learn from both modalities jointly
(Andrews et al 2009; Feng and Lapata, 2010;
Steyvers, 2010). According to this view, seman-
tic knowledge is gained by simultaneously learning
from the statistical structure within each modality
assuming both data sources have been generated by
a shared set of meanings or topics.
In this paper we undertake the first comparative
study of perceptually grounded distributional mod-
els. We examine three models with different as-
sumptions regarding the integration of perceptual
and linguistic data. The first model, originally pro-
posed by Andrews et al(2009), is an extension of
latent Dirichlet alcation (LDA, Blei et al(2003)).
It simultaneously considers the distribution of words
across contexts in a text corpus and the distribu-
tion of words across perceptual features and extracts
joint information from both data sources. Our sec-
ond model is based on Johns and Jones (2012) who
represent the meaning of a word as the concatena-
tion of its textual and its perceptual vector. Interest-
ingly, their model allows to infer a perceptual vector
for words without feature norms, simply by taking
into account similar words for which perceptual in-
formation is available.
Finally, we propose Canonical Correlation Anal-
ysis (Hotelling, 1936; Hardoon et al 2004) as our
third model. CCA is a data analysis and dimen-
sionality reduction method similar to PCA. While
PCA deals with only one data space, CCA is a tech-
nique for joint dimensionality reduction across two
Features table dog apple
has 4 legs .28 .60 0
used for eating .50 0 0
a pet 0 .40 0
is brown 0 0 0
is crunchy 0 0 .58
is round .22 0 .42
has fangs 0 0 0
Table 1: Feature norms for the nouns table, dog, and
apple shown as a distribution.
(or more) spaces that provide heterogeneous repre-
sentations of the same objects. The assumption is
that the representations in these two spaces contain
some joint information that is reflected in correla-
tions between them.
In all three models we use feature norms as a
proxy for perceptual information. Despite their
shortcomings (e.g., they often cover a small frac-
tion of the vocabulary of an adult speaker due to
the effort involved in eliciting them), feature norms
provide detailed knowledge about meaning repre-
sentations and are a useful starting point for study-
ing the integration of perceptual and textual infor-
mation without being susceptible to the effects of
noise, e.g., coming from image processing. In other
words, feature norms can serve as an upper bound
of what can be achieved when integrating detailed
perceptual information with vanilla text-based dis-
tributional models.
Our experimental results demonstrate that joint
models give a better fit to human word similarity and
association data than a model that considers only
one data source, or the simple concatenation of the
two sources.
2 Perceptually Grounded Models
In this study we examine semantic representation
models that rely on linguistic and perceptual data.
The linguistic environment is approximated by cor-
pora such as the British National Corpus (BNC).
As mentioned earlier, we resort to feature norms
as proxy for perceptual information. In our exper-
iments, we relied on the norming study of McRae et
al. (2005), in which a large number of human par-
ticipants were presented with a series of words and
1424
asked to list relevant features of the words? refer-
ents. Table 1 presents examples of features partici-
pants listed for the nouns apple, dog, and table. The
number of participants listing a certain feature for a
word can be used to compute a probability distribu-
tion over features given the word:
P( fk|w) =
f ( fk,w)
F
?
m=1
f ( fm,w)
(1)
where f ( fk,w) is the number of participants who
listed feature fk for word w and F is the total number
of features.
In the remainder of this section we will describe
our models and how they arrive at an integrated per-
ceptual and linguistic representation.
2.1 Feature-topic Model
Andrews et al(2009) present an extension of LDA
(Blei et al 2003) where words in documents as well
as their associated features are treated as observed
variables that are explained by a generative process.
The underlying training data consists of a corpus D
where each document is represented by words and
their frequency of occurrence within the document.
In addition, those words of a document that are also
included in the feature norms are paired with one of
their features, where a feature is sampled according
to the feature distribution given that word. For ex-
ample, suppose a document d j consists of the sen-
tence Mix in the apple, celery, raisins, and apple
juice. Suppose further that all content words ex-
cept of mix and juice are included in the feature
norms. Then, a representation for d j is mix:1, ap-
ple;is red:2, celery;has leaves:1, raisin;is edible:1,
juice:1.
The plate diagram in Figure 1 illustrates the
graphical model in detail. Each document d j
in D is generated by a mixture of components
{x1, ...,xc, ...,xC} ? C ; a component xc comprises a
latent discourse topic coupled with a feature clus-
ter originating from the feature norms. A dis-
course topic belonging to xc, in turn, is a distribu-
tion ?c ? ?= {?1, ...,?C} over words, and a feature
cluster is a distribution ?c ? ?= {?1, ...,?C} over
features.
In order to create document d j, a distribution pi j
over components is sampled from a Dirichlet distri-
pi
x
? w, f ?
?
? ?
?xc ? C ?i ? {1, ...,n j}
? j ? {1, ...,D}
?xc ? C
Figure 1: Feature-topic model. The components x ji of a
document d j are sampled from pi j. For each xc = x ji, a
word w ji is drawn from distribution ?c and a feature f ji is
drawn from distribution ?c.
bution parametrized by ?. To generate each word
w ji ? {w j1, ...,w jn j}, a component xc = x ji is drawn
from pi j; w ji is then drawn from the corresponding
distribution ?c. If w ji is in the feature norms, it is
coupled with a feature f ji which is correspondingly
drawn from ?c. A symmetric Dirichlet prior with
hyperparameters ? and ? is placed on ? and ?, re-
spectively. The probability of the corpus D is de-
fined as:
P((w? f )1:D|?,?,?) =
D
?
j=1
?
dpi j
n j
?
i=1
P(pi j|?)
C
?
c=1
P(w ji|x ji = xc,?)P( f ji|x ji = xc,?)P(x ji = xc|pi j)
(2)
where D is the number of documents and C the
predefined number of components. Computing the
posterior distribution P(?,?,?,?,?|(w ? f )1:D) of
the hidden variables given the data is generally in-
tractable:
P(?,?,?,?,?|(w? f )1:D) ? P((w? f )1:D|?,?,?)
P(?|?)P(?|?)P(?)P(?)P(?)
(3)
Equation (3) may be approximated using the Gibbs
1425
[ x1 x2 x12 ... x28 x75 x107 x119 x125 x148 x182 ... x266 x326 x349 x350
apple 3e-5 3e-5 0 . . . 5e-4 9e-4 .09 .002 7.6e-5 2e-4 .003 . . . 0 0 3e-6 0
]
Figure 2: Example of the representation of the meaning of apple with the model of (Andrews et al 2009) .
[ ... d16 ... d322 ... d2469 d2470 ... dD
apple . . . 1 . . . 1 . . . 0 1 . . . 0
][ a f ruit has f angs is crunchy ... is yellow is red is green is round
0 0 0 . . . 0 0 0 0
]
[ ... d16 ... d322 ... d2469 d2470 ... dD
apple . . . 1 . . . 1 . . . 0 1 . . . 0
][ a f ruit has f angs is crunchy ... is yellow is red is green is round
.006 1.8e-5 8e-4 . . . .004 .004 .006 .02
]
Figure 3: Example representation for apple before (first row) and after (second row) applying the perceptual inference
method of Johns and Jones (2012).
sampling procedure described in Andrews et al
(2009).
Inducing feature-topic components from a docu-
ment collection D with the extended LDA model
just described gives two sets of parameters: word
probabilities given components PW (wi|X = xc) for
wi, i = 1, ...,N, and feature probabilities given com-
ponents PF( fk|X = xc) for fk, k= 1, ...,F . For exam-
ple, most of the probability mass of component x107
would be reserved for the words apple, fruit, lemon,
orange, tree and the features is red, tastes sweet,
is round and so on.
Word meaning in this model is represented by the
distribution PX |W over the learned components (see
Figure 2 for an example). Assuming a uniform dis-
tribution over components xc in D , PX |W can be ap-
proximated as:
PX=xc|W=wi =
P(wi|xc)P(xc)
P(wi)
?
P(wi|xc)
C
?
l=1
P(wi|xl)
(4)
where C is the total number of components. The
model can be also used to infer features for words
that were not originally included in the feature
norms. The probability distribution PF |W over fea-
tures given a word wi is simply inferred by summing
over all components xc for each feature fk:
PF( fk|W = wi) =
C
?
c=1
P( fk|xc)P(xc|wi) (5)
2.2 Global Similarity Model
Johns and Jones (2012) propose an approach for
generating perceptual representations for words by
means of global lexical similarity. Their model does
not place so much emphasis on the integration of
perceptual and linguistic information, rather its main
focus is on inducing perceptual representations for
words with no perceptual correlates. Their idea is to
assume that lexically similar words also share per-
ceptual features and hence it should be possible to
transfer perceptual information onto words that have
none from their linguistically similar neighbors.
Let T ? {1,0}N?D denote a binary term-
document matrix, where each cell records the pres-
ence or absence of a term in a document. Let
P ? [0,1]N?F denote a perceptual matrix, represent-
ing a probability distribution over features for each
word (see Table 1). A word?s meaning is repre-
sented by the concatenation of its textual and per-
ceptual vectors (see Figure 3). If a word has not
been normed, its perceptual vector will be all zeros.
Johns and Jones (2012) propose a two-step estima-
tion process for words without perceptual vectors.
Initially, a perceptual vector is constructed based on
the word?s weighted similarity to other words that
have non-zero perceptual vectors:
pin f =
N
?
i=1
ti ? sim(ti,p)? (6)
where p is the representation of a word with a tex-
tual vector but an empty perceptual vector, ts are
composite representations consisting of textual and
perceptual vectors, sim is a measure of distributional
similarity such as cosine, ? a weighting parameter,
and pin f the resulting inferred representation of the
word. The process is repeated a second time, so
as to incorporate the inferred perceptual vector in
the computation of the inferred vectors of all other
words. An example of this inference procedure is
illustrated in Figure 3.
1426
[ ... d16 ... d322 ... d2470 ... dD
apple . . . .006 . . . .003 . . . .1e-6 . . . 0
][ a f ruit has f angs is crunchy ... is yellow is red is green is round
.13 0 .06 . . . .04 .14 .09 .04
]
[ k1 k2 k3 ... k409 k410
apple ?.003 ?.01 .002 . . . ?.002 ?.01
][ k1 k2 k3 ... k409 k410
.008 ?.03 ?.008 . . . ?.02 ?.07
]
Figure 4: Example representation for apple before (first row) and after (second row) applying CCA.
2.3 Canonical Correlation Analysis
Our third model uses Canonical Correlation Analy-
sis (CCA, Hardoon et al(2004)) to learn a joint se-
mantic representation from the textual and percep-
tual views. Given two random variables x and y
(or two sets of vectors), CCA can be seen as de-
termining two sets of basis vectors in such a way,
that the correlation between the projections of the
variables onto these bases is mutually maximized
(Borga, 2001). In effect, the representation-specific
details pertaining to the two views of the same phe-
nomenon are discarded and the underlying hidden
factors responsible for the correlation are revealed.
In our case the linguistic view is represented by a
term-document matrix, T ? RN?D, containing infor-
mation about the occurrence of each word in each
document. The perceptual view is captured by a
perceptual matrix, P ? [0,1]N?F , representing words
as a probability distribution over normed features.
CCA is concerned with describing linear dependen-
cies between two sets of variables of relatively low
dimensionality. Since the correlation between the
linguistic and perceptual views may exist in some
nonlinear relationship, we used a kernelized version
of CCA (Hardoon et al 2004) which first projects
the data into a higher-dimensional feature space and
then performs CCA in this new feature space. The
two kernel matrices are KT = TT ? and KP = PP?.
After applying CCA we obtain two matrices pro-
jected onto L basis vectors, Ct ? RN?L, resulting
from the projection of the textual matrix T onto the
new basis and Cp ?RN?L, resulting from the projec-
tion of the corresponding perceptual feature matrix.
The meaning of a word can thus be represented by
its projected textual vector in CT , its projected per-
ceptual vector in CP or their concatenation. Figure 4
shows an example of the textual and perceptual vec-
tors for the word apple which were used as input for
CCA (first row) and their new representation after
the projection onto new basis vectors (second row).
The CCA model as sketched above will only ob-
tain full representations for words with perceptual
features available. One solution would be to apply
the method from Johns and Jones (2012) to infer the
perceptual vectors and then perform CCA on the in-
ferred vectors. Another approach which we assess
experimentally (see Section 4) is to create a percep-
tual vector for a word that has none from its k-most
(textually) similar neighbors, simply by taking the
average of their perceptual vectors. This inference
procedure can be applied to the original vectors or
the projected vectors in CT and CP, respectively,
once CCA has taken place.
2.4 Discussion
Johns and Jones (2012) primarily present a model of
perceptual inference, where textual data is used to
infer perceptual information for words not included
in feature norms. There is no means in this model to
obtain a joint representation resulting from the mu-
tual influence of the perceptual and textual views.
As shown in the example in Figure 3 the textual
vector on the left-hand side does not undergo any
transformation whatsoever. The generative model
put forward by Andrews et al(2009) learns meaning
representations by simultaneously considering doc-
uments and features. Rather than simply adding per-
ceptual information to textual data it integrates both
modalities jointly in a single representation which
is desirable, at least from a cognitive perspective.
It is unlikely that we have separate representations
for different aspects of word meaning (Rogers et al
2004). Similarly to Johns and Jones (2012), An-
drews et al (2009) feature-topic model can also
infer perceptual representations for words that have
none. The inference is performed automatically in
an implicit manner during component induction.
In CCA, textual and perceptual data represent two
different views of the same objects and the model
operates on these views directly without combining
or manipulating any of them a priori. Instead, the
combination of the two modalities is realized via
1427
correlating the linear relationships between them. A
drawback of the model lies in the need of additional
methods for inferring perceptual representations for
words not available in feature norms.
3 Experimental Setup
Data All our experiments used a lemmatized ver-
sion of the British National Corpus (BNC) as a
source of textual information. The feature norms of
McRae et al(2005) were used as a proxy for percep-
tual information. The BNC comprises 4,049 texts
totalling approximately 100 million words. McRae
et als feature norms consist of 541 words and 2,526
features; 824 of these features occur with at least two
different words.
Evaluation Tasks Our evaluation experiments
compared the models discussed above on three
tasks. Two of them have been previously used
to evaluate semantic representation models, namely
word association and word similarity. In order
to simulate word association, we used the human
norms collected by (Nelson et al 1998).1 These
were established by presenting a large number of
participants with a cue word (e.g., rice) and ask-
ing them to name an associate word in response
(e.g., Chinese, wedding, food, white). For each cue
word, the norms provide a set of associates and the
frequencies with which they were named. We can
thus compute the probability distribution over asso-
ciates for each cue. Analogously, we can estimate
the degree of similarity between a cue and its as-
sociates using our models (see the following sec-
tion for details on the similarity measures we em-
ployed). The norms contain 63,619 unique normed
cue-associate pairs in total. Of these, 25,968 pairs
were covered by all models and 520 appeared in
McRae et als (2005) norms. Using correlation anal-
ysis, we examined the degree of linear relationship
between the human cue-associate probabilities and
the automatically derived similarity values.
Our word similarity experiments used the
WordSimilarity-353 test collection (Finkelstein et
al., 2002)2 which consists of relatedness judgments
1Available at http://www.usf.edu/Freeassociation.
2Available at http://www.cs.technion.ac.il/?gabr/
resources/data/wordsim353/.
for word pairs. For each pair, a similarity judg-
ment (on a scale of 0 to 10) was elicited from 13 or
16 human subjects (e.g., tiger-cat are very similar,
whereas delay?racism are not). The average rating
for each pair represents an estimate of the perceived
similarity of the two words. The task varies slightly
from word association. Here, participants are asked
to rate perceived similarity rather than to generate
the first word that came to mind in response to a cue
word. The collection contains similarity ratings for
353 word pairs. Of these, 76 pairs appeared in our
corpus and 3 in McRae et als (2005) norms. Again,
we evaluated how well model produced similarities
correlate with human ratings. Throughout this paper
we report correlation coefficients using Pearson?s r.
Our third task assessed the models? ability to in-
fer perceptual vectors for words that have none. To
do this, we conducted 10-fold cross-validation on
McRae et als (2005) norms. We treated the per-
ceptual vectors in each test fold as unseen, and used
the data in the corresponding training fold together
with the models presented in Section 2 to infer them.
Then, for each word, we examined how close the in-
ferred vector was to the actual one, via correlation
analysis.
Model Parameters The feature-topic model has a
few parameters that must be instantiated. These in-
clude, C, the number of predefined components and
the priors ?, ?, and ?. Following Andrews et al
(2009), the components C were set to 350.3 A vague
inverse gamma prior was placed on ?, ?, and ?.4 To
measure word similarity within this model, we adopt
Griffiths et als (2007) definition. The underlying
idea is that word association can be expressed as a
conditional distribution. If we have seen word w1,
then we can determine the probability that w2 will
be also generated by computing P(w2|w1). Assum-
ing that both w1 and w2 came from a single compo-
nent, P(w2|w1) can be estimated as:
P(w2|w1) =
C
?
c=1
P(w2|xc)P(xc|w1)
P(xc|w1) ? P(w1|xc)P(xc)
(7)
3As we explain in Section 4 the feature-topic model was
compared to a vanilla LDA model trained on the BNC only.
For that model, C was set to 250.
4That is P(?) = exp(? 1? )?
?2.
1428
where P(xc) is uniform, a single component xc is
sampled from the distribution P(xc|w1), and an over-
all estimate is obtained by averaging over all C com-
ponents.
Johns and Jones? (2012) model uses binary tex-
tual vectors to represent word meaning. If the word
is present in a given document, that vector element
is coded as one; if it is absent, it is coded as zero.
We built a binary term-document matrix from the
BNC over 14,000 lemmas. The value of the similar-
ity weighting parameter ? was set to the same values
reported by Johns and Jones (?1=3 for Step 1 and
?2 = 13 for Step 2).
For the CCA model, we represented the textual
view with a term-document co-occurrence matrix.
Matrix cells were set to their tf-idf values.5 The tex-
tual and perceptual matrices were projected onto 410
vectors. As mentioned in Section 2.3, CCA does not
naturally lend itself to inferring perceptual vectors,
yet a perceptual vector for a word can be created
from its k-nearest neighbors. We inferred a percep-
tual vector by averaging over the perceptual vectors
of the word?s k most similar words; textual similarity
between two words was measured using the cosine
of the angle of the two vectors representing them.
To find the optimal value for k, we used one third of
Nelson?s (1998) cues as development set. The high-
est correlation was achieved with k = 2 when the
perceptual vectors were created prior to CCA and
k = 8 when they were inferred on the projected tex-
tual and perceptual matrices.
4 Results
Our experiments were designed to answer three
questions: (1) Does the integration of perceptual and
textual information yield a better fit with behavioral
data compared to a model that considers only one
data source? (2) What is the best way to integrate
the two modalities, e.g., via simple concatenation or
jointly? (3) How accurately can we approximate the
perceptual information when the latter is absent?
To answer the first question, we assessed the mod-
els? performance when textual and perceptual infor-
mation are both available. The results in Table 2
are thus computed on the subset of Nelson?s (1998)
5Experiments with a binarized version of the term-document
matrix consistently performed worse.
Models Modality Pearson?s r
Feature-topic +t +p .35
Feature-topic +t ?p .12
Feature-topic ?t +p .22
Global similarity +t +p .23
Global similarity +t ?p .11
Global similarity ?t +p .22
CCA +t +p .32
CCA +t ?p .14
CCA ?t +p .29
Upper Bound ? .91
Table 2: Performance of feature-topic, global similarity,
and CCA models on a subset of the Nelson et al(1998)
norms when taking into account the textual and percep-
tual modalities on their own (+t?p and ?t+p) and in
combination (+t+p). All correlation coefficients are sta-
tistically significant (p < 0.01).
norms (520 cue-associate pairs) that also appeared in
McRae et al(2005) and for which a perceptual vec-
tor was present. The table shows different instanti-
ations of the three models depending on the type of
modality taken into account: textual, perceptual or
both.
As can be seen, Andrews et als (2009) feature-
topic model provides a better fit with the association
data when both modalities are taken into account
(+t+p). A vanilla LDA model constructed solely
on the BNC (+t?p) or McRae et als (2005) fea-
ture norms (?t+p) yields substantially lower corre-
lations. We observe a similar pattern with Johns and
Jones? (2012) global similarity model. Concatena-
tion of perceptual and textual vectors yields the best
fit with the norming data, relying on perceptual in-
formation alone (?t+p) comes close, whereas tex-
tual information on its own seems to have a weaker
effect (+t?p).6 The CCA model takes perceptual
and textual information as input in order to find a
projection onto basis vectors that are maximally cor-
related. Although by definition the CCA model must
operate on the two views, we can nevertheless iso-
late the contribution of each modality by considering
the vectors resulting from the projection of the tex-
6In this evaluation setting, the model does not infer any per-
ceptual representations; perceptual vectors are taken directly
from McRae et al(2005).
1429
tual matrix (+t?p), the perceptual matrix (?t+p) or
their concatenation (+t+p). We obtain best results
with the latter representation; again we observe that
the perceptual information is more dominant.
Overall we find that the feature-topic model and
CCA perform best. In fact the correlations achieved
by the two models do not differ significantly, us-
ing a t-test (Cohen and Cohen, 1983). The per-
formance of the global similarity model is signifi-
cantly worse than the feature-topic model and CCA
(p < 0.01). Recall that the feature-topic model
(+t+p) represents words as distributions over com-
ponents, whereas the global similarity model sim-
ply concatenates the textual and perceptual vectors.
The same input is also given to CCA which in turn
attempts to interpret the data by inferring common
relationships between the two views. In sum, we
can conclude that the higher correlation with human
judgments indicates that integrating textual and per-
ceptual modalities jointly is preferable to concatena-
tion.
However, note that all models in Table 2 fall
short of the human upper bound which we mea-
sured by calculating the reliability of Nelson et als
(1998) norms. Reliability estimates the likelihood
of a similarly-composed group of participants pre-
sented with the same task under the same circum-
stances producing identical results. We split the col-
lected cue-associate pairs randomly into two halves
and computed the correlation between them; this
correlation was averaged across 200 random splits.
These correlations were adjusted by applying the
Spearman-Brown prediction formula (Voorspoels et
al., 2008).
The results in Table 2 are computed on a small
fraction of Nelson et als (1998) norms. One might
even argue that the comparison is slightly unfair as
the global similarity model is more geared towards
inferring perceptual vectors rather than integrating
the two modalities in the best possible way. To gain
a better understanding of the models? behavior and
to allow comparisons on a larger dataset and more
equal footing, we also report results on the entire
dataset (20,556 cue-associate pairs).7 This entails
that the models will infer perceptual vectors for the
7This excludes the data used as development set for tuning
the k-nearest neighbors for CCA.
Models Pearson?s r
Feature-topic .15
Global similarity .03
Global similarity CCA .12
k-NN CCA .11
CCA k-NN .12
Upper Bound .96
Table 3: Performance of the feature-topic, global simi-
larityand CCA models on the Nelson et al(1998) norms
(entire dataset). All correlation coefficients are statisti-
cally significant (p < 0.01).
words that are not attested in McRae et als norms.
Recall from Section 2.3 that CCA does not have
a dedicated inference mechanism. We thus experi-
mented with three options (a) interfacing the infer-
ence method of Johns and Jones (2012) with CCA
(global similarity  CCA) (b) creating a percep-
tual vector from the words? k-nearest neighbors be-
fore (k-NN  CCA) or (c) after CCA takes place
(CCA k-NN).
Our results are summarized in Table 3. The up-
per bound was estimated in the same fashion as for
the smaller dataset. Despite being statistically sig-
nificant (p < 0.01), the correlation coefficients are
lower. This is hardly surprising as perceptual infor-
mation is approximate and in several cases likely to
be wrong. Interestingly, we observe similar mod-
eling trends, irrespective of whether the models are
performing perceptual inference or not. The feature-
topic model achieves the best fit with the data, fol-
lowed by CCA. The inference method here does not
seem to have much of an impact: CCA  k-NN
does as well as global similarity  CCA. This is
perhaps expected as the inference procedure adopted
by Johns and Jones (2012) is a generalization of our
k-nearest neighbor approach. The global similarity
model performs worst; we conjecture that this is due
to the way semantic information is integrated rather
than the inference method itself. CCA works with
similar input, yet achieves better correlations with
the human data, due to its ability to represent the
commonalities shared by the two modalities. Taken
together the results in Tables 2 and 3 provide an an-
swer to our second question. Models that capture la-
tent information shared between the two modalities
1430
Models Pearson?s r
Feature-topic .17
Global similarity .25
Global similarity CCA .21
k-NN CCA .19
CCA k-NN .13
Table 4: Mean correlation coefficients between origi-
nal and inferred feature vectors in McRae et als (2005)
norms.
create more accurate semantic representations com-
pared to simply treating the two as independent data
sources.
In order to isolate the influence of the inference
method from the resulting semantic representation
we evaluated the inferred perceptual vectors on their
own by computing their correlation with the original
feature distributions in McRae et als (2005) norms.
The correlation coefficients are reported in Table 4
and were computed by averaging the coefficients ob-
tained for individual words. Here, the global simi-
larity model achieves the highest correlation, and for
a good reason. It is the only model with an empha-
sis on inference, the other two models do not have
such a dedicated mechanism. CCA has in fact none,
whereas in the feature-topic model the inference of
missing perceptual information is a by-product of
the generative process. The results in Table 4 indi-
cate that the perceptual vectors are not reconstructed
very accurately (the highest correlation coefficient
is .25) and that better inference mechanisms are re-
quired for perceptual information to have a positive
impact on semantic representation.
In Table 5 we examine the models? performance
on semantic similarity rather than association using
the WordSimilarity-353 dataset (Finkelstein et al
2002). The models were evaluated on 76 word pairs
that appeared in the BNC. We inferred the percep-
tual vectors for 51 words. We computed the upper
bound using the reliability method described ear-
lier. Again, the joint models achieve better results
than the simple concatenation model. The feature-
topic and CCA models perform comparably, with
the global similarity model lagging substantially be-
hind. In sum, our results indicate that the issue
of how to best integrate the two modalities has a
Models Pearson?s r
Feature-topic .35
Global similarity .08
Global similarity CCA .38
k-NN CCA .39
CCA k-NN .28
Upper Bound .98
Table 5: Model performance on predicting word similar-
ity. All correlation coefficients are statistically significant
(p < 0.01), except for the global similarity model.
greater impact on the resulting semantic representa-
tions compared to the mechanism by which missing
perceptual information is inferred.
5 Conclusions
In this paper, we have presented a comparative study
of semantic representation models which compute
word meaning on the basis of linguistic and per-
ceptual information. The models differ in terms
of the mechanisms by which they integrate the two
modalities. In the feature-topic model (Andrews et
al., 2009), the textual and perceptual views are in-
tegrated via a set of latent components that are in-
ferred from the joint distribution of textual words
and perceptual features. The model based on Canon-
ical Correlation Analysis (Hardoon et al 2004) in-
tegrates the two views by deriving a consensus rep-
resentation based on the correlation between the lin-
guistic and perceptual modalities. Johns and Jones?
(2012) similarity-based model simply concatentates
the two representations. In addition, it uses the lin-
guistic representations of words to infer perceptual
information when the latter is absent.
Experiments on word association and similarity
show that all models benefit from the integration of
perceptual data. We also find that joint models are
superior as they obtain a closer fit with human judg-
ments compared with an approach that simply con-
catenates the two views. We have also examined
how these models perform on the perceptual infer-
ence task which has implications for the wider appli-
cability of grounded semantic representation mod-
els. Johns and Jones? (2012) inference mechanism
goes some way towards reconstructing the informa-
tion contained in the feature norms, however, further
1431
work is needed to achieve representations accurate
enough to be useful in semantic tasks.
In this paper we have used McRae et als (2005)
norms without any extensive feature engineering
other than applying a frequency cut-off. In the fu-
ture we plan to experiment with feature selection
methods in an attempt to represent perceptual in-
formation more succinctly. For example, it may
be that different features are appropriate for differ-
ent word classes (e.g., color versus event denoting
nouns). Although feature norms are a useful first ap-
proximation of perceptual data, the effort involved in
eliciting them limits the scope of any computational
model based on normed data. A natural avenue for
future work would be to develop semantic represen-
tation models that exploit perceptual data that is both
naturally occurring and easily accessible (e.g., im-
ages, physical simulations).
Acknowledgments We are grateful to Brendan
Johns for his help with the re-implementation of his
model. Thanks to Frank Keller and Michael Roth for
their input on earlier versions of this work, Ioannis
Konstas for his help with the final version, and mem-
bers of the ILCC at the School of Informatics for
valuable discussions and comments. We acknowl-
edge the support of EPSRC through project grant
EP/I032916/1.
References
M. Andrews, G. Vigliocco, and D. Vinson. 2009. Inte-
grating Experiential and Distributional Data to Learn
Semantic Representations. Psychological Review,
116(3):463?498.
Lawrence Barsalou. 1999. Perceptual Symbol Systems.
Behavioral and Brain Sciences, 22:577?609.
Lawrence W. Barsalou. 2008. Grounded Cognition. An-
nual Review of Psychology, 59:617?845.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
Magnus Borga. 2001. Canonical Correlation - a Tutorial,
January.
M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.
Park, and L. Pascual. 2004. Cross-linguistic Analy-
sis of Vocabulary in Young Children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development, 75(4):1115?1139.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional Semantics from Text and Images. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
22?32, Edinburgh, UK, July. Association for Compu-
tational Linguistics.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 6th EMNLP, pages
109?117, Seattle, WA.
J Cohen and P Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
Yansong Feng and Mirella Lapata. 2010. Visual Infor-
mation in Semantic Representation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 91?99, Los Ange-
les, California, June. Association for Computational
Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing Search in Context: The Con-
cept Revisited. ACMTransactions on Information Sys-
tems, 20(1):116?131, January.
Arthur M. Glenberg and Michael P. Kaschak. 2002.
Grounding Language in Action. Psychonomic Bulletin
and Review, 9(3):558?565.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum. 2007.
Topics in Semantic Representation. Psychological Re-
view, 114(2):211?244.
David R. Hardoon, Sandor R. Szedmak, and John R.
Shawe-Taylor. 2004. Canonical Correlation Analysis:
An Overview with Application to Learning Methods.
Neural Computation, 16(12):2639?2664.
H Hotelling. 1936. Relations between Two Sets of Vari-
ates. Biometrika, 28:312?377.
Steve R. Howell, Damian Jankowicz, and Suzanna
Becker. 2005. A Model of Grounded Language Ac-
quisition: Sensorimotor Features Improve Lexical and
Grammatical Learning. Journal of Memory and Lan-
guage, 53(2), 258-276, 53(2):258?276.
Brendan T. Johns and Michael N. Jones. 2012. Percep-
tual Inference through Global Lexical Similarity. Top-
ics in Cognitive Science, 4(1):103?120.
B. Landau, L. Smith, and S. Jones. 1998. Object Percep-
tion and Object Naming in Early Development. Trends
in Cognitive Science, 27:19?24.
T. Landauer and S. T. Dumais. 1997. A Solution to
Plato?s Problem: the Latent Semantic Analysis The-
ory of Acquisition, Induction, and Representation of
Knowledge. Psychological Review, 104(2):211?240.
1432
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768?774, Montre?al, Canada.
K. McRae, G. S. Cree, M. S. Seidenberg, and C. McNor-
gan. 2005. Semantic Feature Production Norms for a
Large Set of Living and Nonliving Things. Behavior
Research Methods, 37(4):547?559, November.
D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998.
The University of South Florida Word Association,
Rhyme, and Word Fragment Norms.
C. Perfetti. 1998. The Limits of Co-occurrence: Tools
and Theories in Language Research. Discourse Pro-
cesses, (25):363?377.
Terry Regier. 1996. The Human Semantic Potential.
MIT Press, Cambridge, MA.
T. T. Rogers, M. A. Lambon Ralph, P. Garrard, S. Bozeat,
J. L. McClelland, J. R. Hodges, and K. Patterson.
2004. Structure and Deterioration of Semantic Mem-
ory: A Neuropsychological and Computational Inves-
tigation. Psychological Review, 111(1):205?235.
G Salton, A Wang, and C Yang. 1975. A Vector-space
Model for Information Retrieval. Journal of the Amer-
ican Society for Information Science, 18:613?620.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?124.
Mark Steyvers. 2010. Combining Feature Norms and
Text Data with Topic Models. Acta Psychologica,
133(3):234?342.
Wouter Voorspoels, Wolf Vanpaemel, and Gert Storms.
2008. Exemplars and Prototypes in Natural Language
Concepts: A Typicality-based Evaluation. Psycho-
nomic Bulletin & Review, 15:630?637.
1433
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 415?425,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Unsupervised Relation Extraction with General Domain Knowledge
Oier Lopez de Lacalle1,2 and Mirella Lapata1
1Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB
2IKERBASQUE, Basque Foundation for Science, Bilbao, Spain
oier.lopezdelacalle@ehu.es, mlap@inf.ed.ac.uk
Abstract
In this paper we present an unsupervised ap-
proach to relational information extraction.
Our model partitions tuples representing an
observed syntactic relationship between two
named entities (e.g., ?X was born in Y?
and ?X is from Y?) into clusters correspond-
ing to underlying semantic relation types
(e.g., BornIn, Located). Our approach incor-
porates general domain knowledge which we
encode as First Order Logic rules and auto-
matically combine with a topic model devel-
oped specifically for the relation extraction
task. Evaluation results on the ACE 2007
English Relation Detection and Categoriza-
tion (RDC) task show that our model outper-
forms competitive unsupervised approaches
by a wide margin and is able to produce clus-
ters shaped by both the data and the rules.
1 Introduction
Information extraction (IE) is becoming increas-
ingly useful as a form of shallow semantic analy-
sis. Learning relational facts from text is one of the
core tasks of IE and has applications in a variety
of fields including summarization, question answer-
ing, and information retrieval. Previous work (Sur-
deanu and Ciaramita, 2007; Culotta and Sorensen,
2004; Zhou et al, 2007) has traditionally relied on
extensive human involvement (e.g., hand-annotated
training instances, manual pattern extraction rules,
hand-picked seeds). Standard supervised techniques
can yield high performance when large amounts
of hand-labeled data are available for a fixed in-
ventory of relation types (e.g., Employment, Lo-
cated), however, extraction systems do not easily
generalize beyond their training domains and often
must be re-engineered for each application. Un-
supervised approaches offer a promising alternative
which could lead to significant resource savings and
more portable extraction systems.
It therefore comes as no surprise that latent topic
analysis methods have been used for a variety of
IE tasks. Yao et al (2011), for example, propose
a series of topic models which perform relation
discovery by clustering tuples representing an ob-
served syntactic relationship between two named en-
tities (e.g., ?X was born in Y? and ?X is from Y?).
The clusters correspond to semantic relations whose
number or type is not known in advance. Their mod-
els depart from standard Latent Dirichlet Allocation
(Blei et al, 2003) in that a document consists of re-
lation tuples rather than individual words; moreover,
tuples have features each of which is generated in-
dependently from a hidden relation (e.g., the words
corresponding to the first and second entities, the
type and order of the named entities). Since these
features are local, they cannot capture more global
constraints pertaining to the relation extraction task.
Such constraints may take the form of restrictions
on which tuples should be clustered together or
not. For instance, different types of named entities
may be indicative of different relations (ORG-LOC
entities often express a Location relation whereas
PER-PER entities express Business or Family rela-
tions) and thus tuples bearing these entities should
not be grouped together. Another example are tuples
with identical or similar features which intuitively
should be clustered together.
In this paper, we propose an unsupervised ap-
proach to relation extraction which does not re-
415
quire any relation-specific training data and allows
to incorporate global constraints general express-
ing domain knowledge. We encode domain knowl-
edge as First Order Logic (FOL) rules and automati-
cally integrate them with a topic model to produce
clusters shaped by the data and the constraints at
hand. Specifically, we extend the Fold-all (First-
Order Logic latent Dirichlet Allocation) framework
(Andrzejewski et al, 2011) to the relation extraction
task, explain how to incorporate meaningful con-
straints, and develop a scalable inference technique.
In the presence of multiple candidate relation de-
compositions for a given corpus, domain knowledge
can steer the model towards relations which are best
aligned with user and task modeling goals. We also
argue that a general mechanism for encoding addi-
tional modeling assumptions and side information
can lessen the need for ?custom? relation extraction
model variants. Experimental results on the ACE-
2007 Relation Detection and Categorization (RDC)
dataset show that our model outperforms competi-
tive unsupervised approaches by a wide margin and
is able to uncover meaningful relations with only
two general rule types.
Our contributions in this work are three-fold: a
new model that modifies the Fold-all framework and
extends it to the relation extraction task; a new for-
malization of the logic rules applicable to topic mod-
els defined over a rich set of features; and a proposal
for mining the logic rules automatically from a cor-
pus contrary to Andrzejewski et al (2011) who em-
ploy manually crafted seeds.
2 Related Work
A variety of learning paradigms have been applied
to relation extraction. As mentioned earlier, super-
vised methods have been shown to perform well in
this task. The reliance on manual annotation, which
is expensive to produce and thus limited in quantity,
has provided the impetus for semi-supervised and
purely unsupervised approaches. Semi-supervised
methods use a small number of seed instances or
patterns (per relation) to launch an iterative train-
ing process (Riloff and Jones, 1999; Agichtein and
Gravano, 2000; Bunescu and Mooney, 2007; Pan-
tel and Pennacchiotti, 2006). The seeds are used
to extract a new set of patterns from a large cor-
pus, which are then used to extract more instances,
and so on. Unsupervised relation extraction meth-
ods are not limited to a predefined set of target
relations, but discover all types of relations found
in the text. The relations represent clusters over
strings of words (Banko et al, 2007; Hasegawa et
al., 2004), syntactic patterns between entities (Yao
et al, 2011; Shinyama and Sekine, 2006), or logical
expressions (Poon and Domingos, 2009). Another
learning paradigm is distant supervision which does
not require labeled data but instead access to a rela-
tional database such as Freebase (Mintz et al, 2009).
The idea is to take entities that appear in some rela-
tion in the database, find the sentences that express
the relation in an unlabeled corpus, and use them to
train a relation classifier.
Our own work adds an additional approach into
the mix. We use a topic model to infer an arbi-
trary number of relations between named entities.
Although we do not have access to relation-specific
information (either as a relational database or manu-
ally annotated data), we impose task-specific con-
straints which inject domain knowledge into the
learning algorithm. We thus alleviate known prob-
lems with the interpretability of the clusters obtained
from topic models and are able to guide our model
towards reasonable relations. Andrzejewski et al
(2011) show how to integrate First-Order Logic with
vanilla LDA. We extend their formulation to relation
tuples rather than individual words. Our model gen-
erates a corpus of entity tuples which are in turn rep-
resented by features and uses automatically acquired
FOL rules. The idea of integrating topic modeling
with FOL builds on research in probabilistic logic
modeling such as Markov Logic Networks (Richard-
son and Domingos, 2006). Schoenmackers et al
(2010) learn Horn clauses from web-scale text with
aim of finding answers to a user?s query. Our work
is complementary to theirs. We could make use of
their rules to discover more accurate relations.
The general goal of assisting the learner in re-
covering the ?correct? clustering by supplying ad-
ditional domain knowledge is not new. Gondek and
Hofmann (2004) supply a known clustering they do
not want the learner to return, whereas Wagstaff
et al (2001) use pairwise labels for items indicat-
ing whether they belong in the same cluster. These
methods combine domain knowledge with statistical
learning in order to improve performance with re-
416
spect to the true target clustering. Although, the tar-
get labels are not available in our case, we are able to
show that the inclusion of domain knowledge yields
clustering improvements.
3 Learning Setting
Our relation extraction task broadly adheres to the
ACE specification guidelines. Our aim is to detect
and characterize the semantic relations between
two named entities. The input to our model is a
corpus of documents, where each document is a
bag of relation tuples which can be obtained from
the output of any dependency parser. Each tuple
represents a syntactic relationship between two
named entity (NE) mentions, and consists of three
components: the dependency path between the
two mentions, the source NE, and the target NE. A
dependency path is the concatenation of dependency
edges and nodes along a path in the dependency
tree. For example, the sentence ?George Bush
traveled to France on Thursday for a summit.?
would yield the tuple [SOURCE:George Bush(PER),
PATH:?nsubj?traveled?prep?to?pobj?,
DES:France(LOC)]. The tuple here expresses the
relation Located, however our model does not
observe any relation labels during training. The
model assigns tuples to clusters, corresponding to
an underlying relation type. Each tuple instance can
be then labeled with an identifier corresponding to
the cluster (aka relation) it has been assigned to.
4 Modeling Framework
Our model builds on the work of Yao et al (2011)
who develop a series of generative probabilistic
models for relation extraction. Specifically, we ex-
tend their relational LDA model by interfacing it
with FOL-rules. In the following, we first describe
their approach in more detail and then present our
extensions and modifications.
4.1 Relational LDA
Relational LDA is an extension to LDA with a sim-
ilar generative story. LDA models each document
as a mixture of topics, which are in turn character-
ized as distributions over words. In relational LDA,
each document is a mixture of relations over tuples
representing syntactic relations between two named
entities. The relation tuples are in turn generated a
by set of features drawn independently from the un-
derlying relation distribution.
More technically, a multinomial distribution over
relations ?di is drawn from a Dirichlet prior
(? ? Dir(?)) at the document level. Relation tuples
are generated from a multinomial distribution ?di
(zi|?di ? Mult(?di)) and are represented with k fea-
tures. Each feature is drawn (independently) from
a multinomial distribution selected by the relation
assigned to tuple i (fik|zi, ?zi ? Mult(?zi)). Rela-
tions are drawn from a Dirichlet prior (? ? Dir(?)).
In other words, each tuple in a document is assigned
a hidden relation (z = z1...zN ); each relation is
represented by a multinomial distribution over fea-
tures ?r (Dirichlet prior ?). ?r is a vector with F
dimensions each corresponding to a feature. Fi-
nally, documents (j = 1...D) are associated with a
multinomial distribution ?j over relations (Dirichlet
prior ?). ?j is a vector with R dimensions, one for
each relation.
Figure 1 represents relational LDA model as a an
undirected graphical model or factor graph (Bishop,
2006), ignoring for the moment the factor which
connects the d, z, f1...k and o variables. Directed
graphical models can be converted into undirected
ones by adding edges between co-parents (Koller
and Friedman, 2009). Each clique in the graph de-
fines a potential function which replaces the condi-
tional probabilities in the directed graph. Each max-
imal clique is associated with a special factor node
(the black squares) and clique members are con-
nected to that factor. The probability of any specific
configuration is calculated by multiplying the poten-
tial functions and normalizing them. We adopt the
factor graph representation as is it convenient for in-
troducing logic rules into the model. The joint prob-
ability of the model given the priors and the docu-
ments (P (p, z, ?, ?|?, ?,d)) is equivalent to:
R?
r
p(?r|?)
D?
j
p(?j |?)
N?
i
?di(zi)
?
k?pi
?zi(fk) (1)
where ?di(zi) is the zi-th element in the vector ?di
and ?zi(fk) is fk-th feature in the ?zi vector. Vari-
able pi is the i-th tuple containing k features. The
parameters of the latent variables (e.g., ?, ?) are
typically estimated using an approximate inference
algorithm such as Gibbs Sampling (Griffiths and
Steyvers, 2004).
417
Figure 1: Relational LDA as a factor graph. Filled
circles represent observed variables, empty circles are
associated with latent variables or model hyperparame-
ters, and plates indicate repeating structures. The black
squares are the factor nodes and are associated with the
potential functions corresponding to conditional indepen-
dence among the variables. The model observes D doc-
uments (d) consisting of N tuples (p), each represented
by a set of features f1,f2 . . . fk. z represents the relation
type assignment to a tuple, ? is the relation type propor-
tion for a given document, and ? the relation type dis-
tribution over the features. The logic factor (indicated
with the arrow) connects the KB with the relational LDA
model. Variable o is an observed variable which contains
the side information expressed in FOL.
As shown in Figure 1, the observed variables are
represented by filled circles. In our case, our model
sees the corpus (p, d), where d is the variable rep-
resenting the document and the tuples (p) are repre-
sented by a set of features f1,f2 . . . fk in the graph.
Empty circles are associated with latent variables to
be estimated: z represents the relation type assign-
ment to the tuple, ? is the relation type proportion
for the given document, and ? is the relation type
distribution over the features.
The features representing the tuples tap onto se-
mantic information expressed by different surface
forms and are an important part of the model. We
use a subset of the features proposed in Yao et al
(2011) which we briefly describe below:
SOURCE This feature corresponds to the first en-
tity mention of the tuple. In the sentence George
Bush traveled to France on Thursday for a summit.,
the value of this feature would be George Bush .
Value Predicate Description
zi = r Z(i, r) Latent relation type
fk = v F(k, v) feature of relation tuple
pi = i P(i, fk) tuple i contains feature fk
di = j D(i, j) observed document
Table 1: Logical variables for Relational LDA. The vari-
able i ranges over tuples in the corpus (i = [1 . . . N ]),
and k over features in the corpus (k = [1 . . . F ]).
DEST The feature corresponds to the second entity
mention and its value would be France in the previ-
ous example.
NEPAIR The feature indicates the type and order
of two entity mentions in the tuple. This would
be PER-ORG in our example.
PATH This feature refers to the dependency
path between two entity mentions. In our
sentence, the value of the feature would be
PATH:?nsubj?traveled?prep?to?pobj?.
TRIGGER Finally, trigger features are content
words occurring in the dependency path. The path
PATH:?nsubj?traveled?prep?to?pobj? con-
tains only one trigger word, namely traveled. The
intuition behind this feature is that paths sharing the
same set of trigger words should be grouped in the
same cluster.
4.2 First Order Logic and Relational LDA
We next couple relational LDA with global con-
straints, which we express using FOL rules. We
begin by representing relational LDA as a Markov
Logic Network (Richardson and Domingos, 2006).
We define a logical predicate for each model vari-
able. For example, assigned relation variable
(Z(i, r)) is true if zi = r and false otherwise. Table 1
shows the mapping of model variables onto logical
predicates. Logical rules are encoded in the form of
a weighted FOL knowledge base (KB) which is then
converted into Conjunctive Normal form:
KB = {(?1, ?1), ..., (?L, ?L)} (2)
The KB consists of L pairs, where each ?l rep-
resents a FOL rule and ?l ? 0 its weight. Rules
are soft preferences rather than hard constraints;
the weights represent the importance of ?l and are
418
set manually by the domain expert. The KB is
tied to the probabilistic model via its groundings
in the corpus. For each FOL rule ?l, let G(?l) be
the set of groundings, each mapping the free vari-
ables in ?l to a specific value. For example, in the
rule ?i, j, p : F(i, Obama) ? F(j,WhiteHouse) ?
P(p, i) ? P(p, j) ? Z(p, r)1, G consists of all the
rules where the free variables i, j and p are instanti-
ated. At grounding time, we parse the corpus search-
ing for the tuples that satisfy the logic rules and store
the indices of the tuples that ground the rule. The
stored indices are used to set ?l to a specific value.
For the (Obama, White House) example above, G
consists of F propositional rules for each observed
feature, where i ? [1 . . . F ]. For each grounding
(g ? G(?l)) we define an indicator function:
1g(z,p,d,o) =
?
??
??
1, if g is true under
z and p,d,o
0, otherwise
(3)
where z are relation assignments to tuples, p is the
set of features in tuples, d are documents, and o
the side information encoded in FOL. Contrary to
Andrzejewski et al (2011), we need to ground the
rules while taking into account if the feature speci-
fied in the rule is expressed by any tuple or the spe-
cific given tuple, since we are assigning relations to
tuples, and not directly to words.
Next, we define a Markov Random Field (MRF)
which combines relational LDA with the FOL
knowledge base. The MRF is defined over latent
relation tuple assignments z, relation feature multi-
nomials ?, and relation document multinomials ?
(the feature set, document, and external informa-
tion o are observed). Under this model the con-
ditional probability P (z, ?, ?|?, ?,p,d,o,KB) is
proportional to:
exp
?
?
L?
l
?
g?G(?l)
?l1g(z,p,d,o)
?
??
R?
r
p(?r|?)
D?
j
p(?j |?)
N?
i
?di(zi)
?
k?pi
?zi(fk)
(4)
The first term in Equation (4) corresponds to the
logic factor in Figure 1 that groups variables d, z,
1This rule translates as ?every tuple containing Obama and
White House as features should be in relation cluster r?.
f1, f2, . . . fk and o. The remaining terms in Equa-
tion (4) refer to relational LDA. The goal of the
model is to estimate the most likely ? and ? for the
given observed state. As z can not be marginalized
out, we proceed with MAP estimation of (z, ?, ?),
maximizing the log of the probability as in Andrze-
jewski et al (2011):
argmax
z,?,?
L?
l
?
g?G(?l)
?l1g(z,p,d,o)+
R?
r
log p(?r|?)+
N?
i
log ?di(zi)
?
k?pi
?zi(fk)
(5)
Once the parameters of the model are estimated
(see Section 4.3 for details), we use the ? proba-
bility distribution to assign a relation to a new test
tuple. We select the relation that maximizes the
probability argmaxr
?k
i P (fi|?r) where f1 . . . fk
are features representing the tuple and r the relation
index.
4.3 Inference
Exact inference is intractable for both relational
LDA and MLN models. In order to infer the most
likely multinomial parameters ? and ?, we applied
the Alternating Optimization with Mirror Descent
algorithm introduced in Andrzejewski et al (2011).
The algorithm alternates between optimizing the
multinomial parameters (?, ?), whilst holding the re-
lation assignments (z) fixed, and vice-versa. At each
iteration, the algorithm first finds the optimal (?, ?)
for a fixed z as the MAP estimate of the Dirichlet
posterior:
?r(f) ? nrf + ? ? 1 (6)
?j(r) ? njr + ?? 1 (7)
where nrf is the number of times feature f is
assigned to relation r in relation assignments z,
and njr is the number of times relation r is assigned
to document j. Next, z is optimized while keeping ?
and ? fixed. This step is divided into two parts. The
algorithm first deals with all zi which appear only in
trivial groundings, i.e., groundings whose indicator
functions 1g are not affected by the latent relation
assignment z. As zi only appears in the last term of
419
Equation (5), the algorithm needs only optimize the
following term:
zi = argmax
r=1...R
?di(r)
?
k?pi
?zi(fk) (8)
The second part deals with the remaining zi that ap-
pear in non-trivial groundings in the first term of
Equation (5). We follow Andrzejewski et al (2011)
in relaxing (5) into a continuous optimization prob-
lem and refer the reader to their paper for a more
in depth treatment. Suffice it to say that once the
binary variables zir ? {0, 1} are relaxed to contin-
uous values zir ? [0, 1], it is possible to introduce
the relational LDA term in the equation and com-
pute the gradient using the Entropic Mirror Descent
Algorithm (Beck and Teboulle, 2003):
argmax
z?[0,1]|KB|
L?
l
?
g?G(?l)
?l1g(z)+
?
i,r
zir log ?di(r)
?
k?pi
?zi(fk)
s.t zir ? 0 ,
?
i,r
zir = 1
(9)
In every iteration the approximation algorithm
randomly samples a term from the objective func-
tion (Equation (9)). The sampled term can be
a particular ground rule g or the relational LDA
term (
?
r zir log ?di(r)
?
k?pi
?zi(fk)) for some
uniformly sampled index i. The sampling of the
terms is weighted according to the rule weight (?l)
and the grounded value (G(?l)) in the case of logic
rules, and the size of corpus in tuples (|zKB|) for re-
lational LDA. Once we choose term f and take the
gradient, we can apply the Entropic Mirror Descent
update:
zir ?
zir exp(?Ozirf)?
r? zir? exp(?Ozir?f)
(10)
Finally, zi is recovered by rounding to argmaxr zir.
The main advantage of this approach is that it re-
quires only a means to sample groundings g for each
rule?l, and can avoid fully grounding the FOL rules.
4.4 Logic Rules
Our model assigns relations to tuples rather than top-
ics to words. Since our tuples are described in terms
of features our logic rules must reflect this too. For
our experiments we defined two very general types
of rules described below.
Must-link Tuple The motivation behind this rule
is that tuples which share features probably express
the same underlying relation. The rule must spec-
ify which feature has to be shared for the tuples
to be clustered together. For example, the rule be-
low states that tuples containing the dependency
path PATH:?appos?president?prep?of?pobj?
should go in the same cluster:
?i, j, k : F(i, PATH:is the president of ) ? P(j, fi)
?P(k, fi)? ?Z(j, t) ? Z(k, r)
Cannot-link Tuple We also define rules prohibit-
ing tuples to be clustered together because they do
not share any features. For example, tuples with
ORG-LOC entities, probably express a Location re-
lation and should not be clustered together with
PER-PER tuples, which in all likelihood express a
different relationship (e.g., Family). The rule below
expresses this constraint:
?i, j, k, l : F(i, NEPAIR:PER-PER)
?F(j, NEPAIR:ORG-LOC)
?P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
The specification of the first order logic rules is
an integral part of the model. The rules express
knowledge about the task at hand, the domain in-
volved, and the way the relation extraction problem
is modeled (i.e., tuples expressed as features). So
far, we have abstractly formulated the rules without
explaining how they are specifically instantiated in
our model. We could write them down by hand after
inspecting some data or through consultation with a
domain expert. Instead, we obtain logic rules au-
tomatically from a corpus following the procedure
described in Section 5.
5 Experimental Setup
Data We trained our model on the New York
Times (years 2000?2007) corpus created by Yao et
al. (2011). The corpus contains approximately 2M
entity tuples. The latter were extracted from
428K documents. After post-processing (tokeniza-
tion, sentence-splitting, and part-of-speech tagging),
420
Must-link Tuple
F(i, NEPAIR:PER-PER, TRIGGER:wife) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
F(i, NEPAIR:PER-LOC, TRIGGER:die) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
F(i, PATH:?nsubj?die?prep?in?pobj?) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
F(i, SOURCE:Kobe, DEST:Lakers) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
Cannot-link Tuple
F(i, NEPAIR:ORG-LOC) ? F(j, NEPAIR:PER-PER) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
F(i, NEPAIR:LOC-LOC) ? F(j, TRIGGER:president) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
F(i, NEPAIR:PER-LOC) ? F(j, TRIGER:member) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
F(i, NEPAIR:PER-PER) ? F(j, TRIGER:sell) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
Table 2: Examples of automatically extracted Must-link and Cannot-link tuple rules.
named entities were automatically recognized and
labeled with PER, ORG, LOC, and MISC (Finkel
et al, 2005). Dependency paths for each pair of
named entity mentions were extracted from the out-
put of the MaltParser (Nivre et al, 2004). In our
experiments, we discarded tuples with paths longer
than 10 edges (Lin and Pantel, 2001). We evalu-
ated our model on the test partition of the ACE 2007
(English) RDC dataset which is labeled with gold
standard entity mentions and their relations. There
are six general relation types and 18 subtypes. We
used 25% of the ACE training partition as a devel-
opment set for parameter tuning.
Logic Rule Extraction We automatically ex-
tracted logic rules from the New York Times
(NYT) corpus as follows. The intuition behind
Must-link rules is that tuples with common features
should cluster together. Although we do not know
which features would yield the best rules, we
naively assume that good features are frequently
co-occurring features. Using the log-likelihood
ratio (Dunning, 1993), we first discarded low
confidence feature co-occurrences (p < 0.05). Two
features co-occur if they are both found within
the same sentence. We then sorted the remaining
co-occurrences by their frequency and retained the
N -best ones. We only considered unigram and
bigram features since higher-order ones tend to
be sparse. An example of a bigram feature would
be (PATH:?nsubj?grow?prep?in?pobj?,
DEST:Chicago).
The main intuition behind Cannot-link rules is
that tuples without any common features should
not cluster together. So, if two features never
co-occur, they probably express different relations.
For every unigram and bigram feature in the re-
spective N -best list, we find the features it does
not co-occur with in the NYT corpus. For ex-
ample, NEPAIR:PER?LOC does not co-occur with
DEST:Yankees and the bigram DEST:United Na-
tions, NEPAIR:PER?ORG does not co-occur with
SOURCE:Mr. Bush, NEPAIR:PER?LOC. Cannot-
link rules are then based on such non-co-occurring
feature pairs.
We optimized N empirically on the development
set. We experimented with values ranging from 20
to 500. We obtained 20 Must-link rules for coarse-
grained relations and 400 rules for their subtypes.
We extracted 1,814 Cannot-link rules for general re-
lations (N = 50) and 34,522 rules for subtypes
(N = 400). The number of features involved in the
Must-link rules was 25 for coarse-grained relations
and 422 for fine-grained relations. For Cannot-link
rules, 62 features were involved in coarse-grained
relations and 422 in fine-grained relations.
Examples of the rules we extracted are shown in
Table 2. The first rule in the upper half of the ta-
ble states that tuples must cluster together if their
source and target entities are PER and contain the
trigger word wife in their dependency path. The sec-
ond rule is similar, the source entity here is PER,
the target LOC and the trigger word is die. Ac-
cording to the third rule, tuples featuring the path
PATH:?nsubj?die?prep?in?pobj? should be
in the same cluster. The fourth rule forces tuples
whose source entity is Kobe and target entity is Lak-
ers to cluster together. The second half of the table
illustrates Cannot-link tuple rules. The first rule pre-
vents tuples with ORG-LOC entities to cluster to-
421
gether with PER-PER tuples. The second rule states
that we cannot link LOC-LOC tuples with those
whose trigger word is president, and so on.
Parameter Tuning Our framework has several
parameters that must be adjusted for an optimal clus-
tering solution. These include the hyperparame-
ters ? and ? as well as the number of clusters. In
addition, we have to assign a weight to each FOL
rule grounding. An exhaustive search on the hy-
perparameters and rule weights is not possible. We
therefore followed a step-wise approximation proce-
dure. First, we find the best ? and ? values, whilst
varying the number of clusters. Once we have the
best hyperparameters for each clustering, we set the
weights for the FOL rules. We varied the number
of relations from 5 to 50. We experimented with ?
values in the range of [0.05 ? 0.5] and ? values in
the range of [0.05 ? 0.5]. These values were opti-
mized separately for coarse- and fine-grained rela-
tions. Table 3 shows the optimal number of clusters
for different model variants and relation types.
The FOL weights can also make a difference in
the final output; the bigger the weight the more
times the rule will be sampled in the Mirror Descent
algorithm. We experimented with two weighting
schemes: (a) we gave a weight of 1 or 0.5 to each
rule grounding and (b) we scaled the weights so as
to make their contribution comparable to relational
LDA. We obtained best results on the development
set with the former scheme.
Baselines We compared our FOL relational LDA
model against standard LDA (Blei et al, 2003) and
relational LDA without the FOL component. In the
case of standard LDA, we estimated topics (rela-
tions) over words, and used the context of the en-
tity mentions pairs as a bag of words feature to se-
lect the most likely cluster at test time. Parameters
for LDA and relational LDA were optimized follow-
ing the same parameter tuning procedure described
above.
We also compared our model against the unsuper-
vised method introduced in Hasegawa et al (2004).
Their key idea is to cluster pairs of co-occurring
named entities according to the similarity of their
surrounding contexts. Following their approach, we
measured context similarity using the vector space
model and the cosine metric and grouped NE pairs
into clusters using a complete linkage hierarchical
clustering algorithm. We adopted the same parame-
ter values as detailed in their paper (e.g., cosine sim-
ilarity threshold, length of context vectors). At test
time, instances were assigned to the relation cluster
most similar to them (according to the cosine mea-
sure).
Evaluation We evaluated the clusters obtained by
our model and the comparison systems using the Fs-
core measure introduced in the SemEval 2007 task
(Agirre and Soroa, 2007); it is the harmonic mean
of precision and recall defined as the number of cor-
rect members of a cluster divided by the number of
items in the cluster and the number of items in the
gold-standard class, respectively.
6 Results
Our results are summarized in Table 3 which reports
Fscore for (Hasegawa et al, 2004), LDA, relational
LDA (RelLDA), and our model with the FOL com-
ponent. To assess the impact of the rules on the
clustering, we conducted several rule ablation stud-
ies. We thus present results with a model that in-
cludes both Must-link and Cannot-link tuple rules
(CLT+MLT), and models that include either Must-
link (MLT) or Cannot-link (CLT) rules but not both.
We show the performance of these models with the
entire feature set (see (ALL) in the table) and with a
subset consisting solely of NE pair related features
(see (NEPAIR) in the table). We report results against
coarse- and fine-grained relations (6 and 18 relation
types in ACE, respectively). The table shows the
optimal number of relation clusters (in parentheses)
per model and relation type.
We also wanted to examine the quality of the logic
rules. Recall that we learn these heuristically from
the NYT corpus. We thus trained an additional vari-
ant of our model with rules extracted from the ACE
training set (75%) which contains relation annota-
tions. The extraction procedure was similar to the
unsupervised case, save that the relation types were
known and thus informative features could be mined
more reliably. For Must-link rules, we extracted un-
igram and bigram feature frequencies for each re-
lation type and applied TF-IDF weighting in order
to discover the most discriminative ones. We cre-
ated logic rules for the 10 best feature combinations
in each relation type. Regarding Cannot-link rules,
we enumerated the features (unigrams and bigrams)
422
Model Subtype Type
HASEGAWA 26.1 (12) 34.7 (12)
LDA 23.4 (10) 29.0 (5)
RelLDA 30.4 (40) 38.6 (5)
U-MLT (ALL) 36.6 (10) 48.0 (5)
U-CLT (ALL) 30.5 (5) 39.3 (5)
U-CLT+MLT (ALL) 29.8 (5) 42.0 (5)
U-MLT (NEPAIR) 36.5 (10) 47.2 (5)
U-CLT (NEPAIR) 28.8 (50) 40.5 (5)
U-CLT+MLT (NEPAIR) 30.9 (10) 41.5 (5)
S-MLT (ALL) 37.0 (10) 47.0 (5)
S-CLT (ALL) 31.4 (50) 40.9 (5)
S-CLT+MLT (ALL) 32.3 (10) 42.5 (5)
S-MLT (NEPAIR) 37.0 (10) 47.6 (10)
S-CLT (NEPAIR) 31.4 (10) 40.1 (5)
S-CLT+MLT (NEPAIR) 37.1 (10) 46.0 (5)
Table 3: Model performance on the ACE 2007 test set
using Fscore. Results are shown for six main relation
types and their subtypes (18 in total). (ALL) models con-
tain rules extracted from the entire feature set. For (NE-
PAIR) models, rules were extracted from NEPAIR-related
features only. Prefix U- denotes models that use unsu-
pervised rules; prefix S- highlights models using super-
vised rules. The optimal number of relations per model
is shown in parentheses.
that did not co-occur in any relation type and applied
TF-IDF weighting. Again, we created rules for the
10 most discriminative features. We defined rules
over the entire feature set (466 Must-link and 26,074
Cannot-link rules) and a subset containing only NE
pairs. In Table 3, prefixes S- and U- indicate model
variants with supervised and unsupervised rules, re-
spectively.
Our results show that standard LDA is not suit-
able for relation extraction. The obtained clusters
are not informative enough to induce semantic re-
lations, whereas RelLDA yields substantially bet-
ter Fscores. This is not entirely surprising, given
that RelLDA is a relation extraction specific model.
Hasegawa et al?s (2004) model lies somewhere in
the middle between LDA and RelLDA. The com-
bination of RelLDA with automatically extracted
FOL rules improves over RelLDA across the board
(see the U- models in Table 3). MLT rules deliver
the largest improvement for both coarse and fine-
grained relation types. In general, CLT models per-
form worse as well as models using both types of
rules (MLT+CLT). The inferior performance of the
rule combination may be due to the fact that MLT
and CLT rules contain conflicting information and
to a certain extent cancel each other out. The use
of many rules might also negatively impact infer-
ence, i.e., discriminative rules are sampled less and
cannot influence the model towards a better solu-
tion. Restricting the number of features and rules
to named entity pairs only incurs a negligible drop
in performance. This is good news for scaling pur-
poses, since a small number of rules can greatly
speed-up inference. Interestingly, model variants
which use supervised FOL rules (see the prefix S-
in Table 3) perform on par with unsupervised mod-
els. Again, MLT rules perform best in the super-
vised case, whereas CLT rules marginally improve
over RelLDA.
We assessed whether differences in performance
are statistically significant (p < 0.05) using boot-
strap resampling (Noreen, 1989). All models across
all relation types are significantly better than LDA
and Hasegawa et al (2004). FOL-based models per-
form significantly better than RelLDA, with the ex-
ception of all CLT models and U-CLT+MLT (ALL).
MLT models are significantly better than any other
rule-based model, except those that only use NE-
PAIR features. We also measured whether differ-
ent models agree on their topic assignments using
Cohen?s Kappa.2 RelLDA agrees least with MLT
models and most with CLT models (i.e., ? = 0.50
for U-MLT (ALL) and ? = 0.65 for U-CLT (ALL)).
This suggests that the CLT rules do not affect the
output of RelLDA as much as MLT ones. Examples
of relation clusters discovered by the U-MLT (ALL)
model are shown in Table 4.
A last note on parameter selection. Our experi-
ments explored the parameter space extensively in
order to examine any interactions between the in-
duced relations and the logic rules. For most model
variants inferring subtype relations, the preferred
number of clusters is 10. For coarse-grained rela-
tions, the optimal number of clusters is five. Over-
all, we found that the quality of the output is highly
correlated with the quality of the logic rules and that
a few good rules are more important than the opti-
mal number of clusters. We consider these findings
robust enough to apply across domains and datasets.
2For all comparison models the number of relation clusters
was set to 10.
423
SOURCE PATH DEST
Republican president of Senate
Senate director of Yankees
House professor at Republican
Bush chairman of Congress
Democrat spokesman for House
Mr. Bush executive of Mets
Democrats director at U. of California
Republican analyst at United Nations
E
m
pl
oy
m
en
t
SOURCE PATH DEST
Yankees defeat World Series
Mets win Olympic
United States beat World Cup
Giants play Yankees
Jets win Super Bowl
Nets lose Olympics
Knicks sign Mets
Rangers victory over Giants
Sp
or
ts
Table 4: Clusters discovered by the U-MLT (ALL) model
indicating employment- and sports-type relations. For the
sake of readability, we do not display the syntactic depen-
dencies between words in a path.
7 Conclusions
In this paper we presented a new model for unsu-
pervised relation extraction which operates over tu-
ples representing a syntactic relationship between
two named entities. Our model clusters such tuples
into underlying semantic relations (e.g., Located,
Family) by incorporating general domain knowledge
which we encode as First Order Logic rules. Specif-
ically, we combine a topic model developed for the
relation extraction task with domain relevant rules,
and present an algorithm for estimating the param-
eters of this model. Evaluation results on the ACE
2007 (English) RDC task show that our model out-
performs competitive unsupervised approaches by a
wide margin and is able to produce clusters shaped
by both the data and the rules.
In the future, we would like to explore additional
types of rules such as seed rules, which would as-
sign tuples complying with the ?seed? information
to distinct relations. Aside from devising new rule
types, an obvious next step would be to explore dif-
ferent ways of extracting the rule set based on differ-
ent criteria (e.g., the most general versus most spe-
cific rules). Also note that in the current framework
rule weights are set manually by the domain expert.
An appealing direction would be to learn these auto-
matically e.g., via a procedure that optimizes some
clustering objective. Finally, it should be interesting
to use some form of distant supervision (Mintz et al,
2009) either as a means of obtaining useful rules or
to discard potentially noisy or uninformative rules.
Acknowledgments
We gratefully acknowledge financial support from
the Department of Education, Universities and Re-
search of the Basque Government (BFI-2011-442).
We also thank Limin Yao and Sebastian Riedel for
sharing their corpus with us and the members of the
Probabilistic Models reading group at the University
of Edinburgh for helpful feedback.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries, pages 85?94, San Antonio,
Texas.
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 7?12,
Prague, Czech Republic.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and Ben
Recht. 2011. A framework for incorporating general
domain knowledge into latent Dirichlet alocation us-
ing first-order logic. In Proceedings of the 22nd In-
ternational Joint Conference on Artificial Intelligence,
pages 1171?1177, Barcelona, Spain.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, Hyderabad, India.
Amir Beck and Marc Teboulle. 2003. Mirror de-
scent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters,
31(3):167?175.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting
424
of the Association of Computational Linguistics, pages
576?583, Prague, Czech Republic.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics, Main Volume, pages 423?429, Barcelona,
Spain.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363?370, Ann Arbor, Michigan.
David Gondek and Thomas Hofmann. 2004. Non-
redundant data clustering. In IEEE International Con-
ference on Data Mining, pages 75?82. IEEE Computer
Society.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(1):5228?5235.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 415?422, Barcelona, Spain.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discovery
of inference rules from text. In Proceedings of the 7th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 323?328, San
Francisco, California.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 1003?1011,
Suntec, Singapore.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of the 8th Conference on Computational Natural Lan-
guage Learning, pages 49?56, Boston, Massachusetts.
Eric W. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley and
Sons Inc.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 113?120, Sydney, Aus-
tralia.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10, Suntec, Singapore.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62(1?2):107?136.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction. In Proceedings of the
16th International Joint Conference on Artificial Intel-
ligence, pages 474?479, Stockholm, Sweden.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order Horn clauses
from web text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1088?1098, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
pages 304?311, New York City, USA.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Ro-
bust information extration with perceptrons. In Pro-
ceedings of the NIST 2007 Automatic Content Extrac-
tion Workshop.
Kiri Wagstaff, Claire Cardie, C Rogers, and S Schro?dl.
2001. Constrained k-means clustering with back-
ground knowledge. In International Conference on
Machine Learning, pages 577?584. Morgan Kauf-
mann.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1456?1466, Edinburgh, Scotland, UK.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoM-
ing Zhu. 2007. Tree kernel-based relation extraction
with context-sensitive structured parse tree informa-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 728?736, Prague, Czech Republic.
425
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503?1514,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Inducing Document Plans for Concept-to-text Generation
Ioannis Konstas and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
ikonstas@inf.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In a language generation system, a content
planner selects which elements must be in-
cluded in the output text and the ordering be-
tween them. Recent empirical approaches per-
form content selection without any ordering
and have thus no means to ensure that the out-
put is coherent. In this paper we focus on
the problem of generating text from a database
and present a trainable end-to-end generation
system that includes both content selection
and ordering. Content plans are represented
intuitively by a set of grammar rules that op-
erate on the document level and are acquired
automatically from training data. We de-
velop two approaches: the first one is inspired
from Rhetorical Structure Theory and repre-
sents the document as a tree of discourse re-
lations between database records; the second
one requires little linguistic sophistication and
uses tree structures to represent global patterns
of database record sequences within a doc-
ument. Experimental evaluation on two do-
mains yields considerable improvements over
the state of the art for both approaches.
1 Introduction
Concept-to-text generation broadly refers to the task
of automatically producing textual output from non-
linguistic input (Reiter and Dale, 2000). Depend-
ing on the application and the domain at hand, the
input may assume various representations including
databases, expert system knowledge bases, simula-
tions of physical systems, or formal meaning rep-
resentations. Generation systems typically follow
a pipeline architecture consisting of three compo-
nents: content planning (selecting and ordering the
parts of the input to be mentioned in the output text),
sentence planning (determining the structure and
lexical content of individual sentences), and surface
realization (verbalizing the chosen content in natu-
ral language). Traditionally, these components are
hand-engineered in order to ensure output of high
quality.
More recently there has been growing interest
in the application of learning methods because of
their promise to make generation more robust and
adaptable. Examples include learning which con-
tent should be present in a document (Duboue and
McKeown, 2002; Barzilay and Lapata, 2005), how it
should be aligned to utterances (Liang et al, 2009),
and how to select a sentence plan among many al-
ternatives (Stent et al, 2004). Beyond isolated com-
ponents, a few approaches have emerged that tackle
concept-to-text generation end-to-end. Due to the
complexity of the task, most models simplify the
generation process, e.g., by treating sentence plan-
ning and surface realization as one component (An-
geli et al, 2010), by implementing content selection
without any document planning (Konstas and Lap-
ata, 2012; Angeli et al, 2010; Kim and Mooney,
2010), or by eliminating content planning entirely
(Belz, 2008; Wong and Mooney, 2007).
In this paper we present a trainable end-to-end
generation system that captures all components of
the traditional pipeline, including document plan-
ning. Rather than breaking up the generation pro-
cess into a sequence of local decisions, each learned
separately (Reiter et al, 2005; Belz, 2008; Chen and
Mooney, 2008; Kim and Mooney, 2010), our model
performs content planning (i.e., document planning
and content selection), sentence planning (i.e., lex-
1503
Database Records
temp(time:6-21, min:9, mean:15, max:21)
wind-spd(time:6-21, min:15, mean:20, max:30)
sky-cover(time:6-9, percent:25-50)
sky-cover(time:9-12, percent:50-75)
wind-dir(time:6-21, mode:SSE)
gust(time:6-21, min:20, mean:30, max:40)
Output Text
Cloudy, with a high around 20. South southeast wind
between 15 and 30 mph. Gusts as high as 40 mph.
(a) WEATHERGOV
Database Records
desktop(cmd:lclick, name:start, type:button)
start(cmd:lclick, name:settings, type:button)
start-target(cmd:lclick, name:control panel, type:button)
win-target(cmd:dblclick, name:users and passwords, type:item)
contMenu(cmd:lclick, name:advanced, type:tab)
action-contMenu(cmd:lclick, name:advanced, type:button)
Output Text
Click start, point to settings, and then click control panel. Double-
click users and passwords. On the advanced tab, click advanced.
(b) WINHELP
Figure 1: Database records and corresponding text for (a) weather forecasting and (b) Windows trou-
bleshooting. Each record has a type (e.g., win-target), and a set of fields. Each field has a value, which
can be categorical (in typewriter), an integer (in bold), or a literal string (in italics).
icalization of input entries), and surface realization
jointly. We focus on the problem of generating text
from a database. The input to our model is a set of
database records and collocated descriptions, exam-
ples of which are shown in Figure 1.
Given this input, we define a probabilistic
context-free grammar (PCFG) that captures the
structure of the database and how it can be verbal-
ized. Specifically, we extend the model of Kon-
stas and Lapata (2012) which also uses a PCFG to
perform content selection and surface realization,
but does not capture any aspect of document plan-
ning. We represent content plans with grammar
rules which operate on the document level and are
embedded on top of the original PCFG. We essen-
tially learn a discourse grammar following two ap-
proaches. The first one is linguistically naive but
applicable to multiple languages and domains; it ex-
tracts rules representing global patterns of record
sequences within a sentence and among sentences
from a training corpus. The second approach learns
document plans based on Rhetorical Structure The-
ory (RST; Mann and Thomson, 1988); it therefore
has a solid linguistic foundation, but is resource in-
tensive as it assumes access to a text-level discourse
parser.
We learn document plans automatically using
both representations and develop a tractable decod-
ing algorithm for finding the best output, i.e., deriva-
tion in our grammar. To the best of our knowledge,
this is the first data-driven model to incorporate doc-
ument planning in a joint end-to-end system. Exper-
imental evaluation on the WEATHERGOV (Liang et
al., 2009) and WINHELP (Branavan et al, 2009) do-
mains shows that our approach improves over Kon-
stas and Lapata (2012) by a wide margin.
2 Related Work
Content planning is a fundamental component in a
natural generation system. Not only does it deter-
mine which information-bearing units to talk about,
but also arranges them into a structure that cre-
ates coherent output. It is therefore not surpris-
ing that many content planners have been based
on theories of discourse coherence (Hovy, 1993;
Scott and de Souza, 1990). Other work has re-
lied on generic planners (Dale, 1988) or schemas
(Duboue and McKeown, 2002). In all cases, con-
tent plans are created manually, sometimes through
corpus analysis. A few researchers recognize that
this top-down approach to planning is too inflexible
and adopt a generate-and-rank architecture instead
(Mellish et al, 1998; Karamanis, 2003; Kibble and
Power, 2004). The idea is to produce a large set
of candidate plans and select the best one according
to a ranking function. The latter is typically devel-
oped manually taking into account constraints relat-
ing to discourse coherence and the semantics of the
domain.
Duboue and McKeown (2001) present perhaps
the first empirical approach to content planning.
They use techniques from computational biology
to learn the basic patterns contained within a plan
and the ordering among them. Duboue and McK-
eown (2002) learn a tree-like planner from an
aligned corpus of semantic inputs and correspond-
ing human-authored outputs using evolutionary al-
1504
gorithms. More recent data-driven work focuses on
end-to-end systems rather than individual compo-
nents, however without taking document planning
into account. For example, Kim and Mooney (2010)
first define a generative model similar to Liang et
al. (2009) that selects which database records to
talk about and then use an existing surface real-
izer (Wong and Mooney, 2007) to render the cho-
sen records in natural language. Their content plan-
ner has no notion of coherence. Angeli et al (2010)
adopt a more unified approach that builds on top of
the alignment model of Liang et al (2009). They
break record selection into a series of locally coher-
ent decisions, by first deciding on what records to
talk about. Each choice is based on a history of
previous decisions, which is encoded in the form
of discriminative features in a log-linear model.
Analogously, they choose fields for each record,
and finally verbalize the input using automatically
extracted domain-specific templates from training
data.
Konstas and Lapata (2012) propose a joint model,
which recasts content selection and surface realiza-
tion into a parsing problem. Their model optimizes
the choice of records, fields and words simultane-
ously, however they still select and order records lo-
cally. We replace their content selection mechanism
(which is based on a simple markovized chaining of
records) with global document representations. A
plan in our model is identified either as a sequence
of sentences, each containing a sequence of records,
or as a tree where the internal nodes denote dis-
course information and the leaf nodes correspond to
records.
3 Problem Formulation
The generator takes as input a set of database
records d and outputs a text g that verbalizes some
of these records. Each record token ri ? d, with
1 ? i ? |d|, has a type ri.t and a set of fields f as-
sociated with it. Fields have different values f .v and
types f .t (i.e., integer, categorical, or literal strings).
For example, in Figure 1b, win-target is a record
type with three fields: cmd (denotes the action the
user must perform on an object on their screen,
e.g., left-click), name (denotes the name of the ob-
ject), and type (denotes the type of the object). The
values of these fields are dblclick, users and pass-
words, and item; name is a literal string, the rest are
Grammar Rules
1. S? R(start)
2. R(ri.t)? FS(r j,start) R(r j.t) | FS(r j,start)
3. FS(r,r. fi)? F(r,r. f j) FS(r,r. f j) | F(r,r. f j)
4. F(r,r. f )?W(r,r. f ) F(r,r. f ) |W(r,r. f )
5. W(r,r. f )? ? | g( f .v)
Figure 2: Grammar G of the original model. Paren-
theses denote features, and impose constraints on the
grammar.
categorical.
During training, our algorithm is given a corpus
consisting of several scenarios, i.e., database records
paired with texts w (see Figure 1). For each sce-
nario, the model first decides on a global document
plan, i.e., it selects which types of records belong to
each sentence (or phrase) and how these sentences
(or phrases) should be ordered. Then it selects ap-
propriate record tokens for each type and progres-
sively chooses the most relevant fields; then, based
on the values of the fields, it generates the final text,
word by word.
4 Original Model
Our work builds on the model developed by Kon-
stas and Lapata (2012). The latter is essentially
a PCFG which captures both the structure of the
input database and the way it renders into natural
language. This grammar-based approach lends it-
self well to the incorporation of document planning
which has traditionally assumed tree-like represen-
tations. We first briefly describe the original model
and then present our extensions in Section 5.
Grammar Grammar G in Figure 2 defines a set
of non-recursive CFG rewrite rules that capture the
structure of the database, i.e., the relationship be-
tween records, records and fields, fields and words.
These rules are domain-independent and could be
applied to any database provided it follows the same
structure. Non-terminal symbols are in capitals, the
terminal symbol ? corresponds to the vocabulary of
the training set and g( f .v) is a function which gener-
ates integers given the field value f .v. Note that all
non-terminals have features (in parentheses) which
1505
act as constraints and impose non-recursion (e.g., in
rule (2) i 6= j, so that a record cannot emit itself).
Rule (1) defines the expansion from the start sym-
bol S to the first record R of type start. The rules
in (2) implement content selection, by choosing ap-
propriate records from the database and generating
a sequence. R(ri.t) is the source record, R(r j.t) is
the target record and FS(r j.start) is a place-holder
symbol for the set of fields of record token r j. This
method is locally optimal, since it only keeps track
of the previous type of record for each re-write. The
rules in (3) conclude content selection on the field
level, i.e., after we have chosen a record, we select
and order the corresponding fields. Finally, the rules
in (4) and (5) correspond to surface realization. The
former rule binarizes the sequence of words emitted
by a particular field r. f in an attempt to capture local
dependencies between words, such as multi-word
expressions (e.g., right click, radio button). The lat-
ter rule defines the emission of words and integer
numbers1, given a field type and its value. Note that
the original model lexicalizes field values of cate-
gorical and integer type only.
Training The rules of grammar G are associated
with weights that are learned using the EM algo-
rithm (Dempster et al, 1977). During training, the
records, fields and values of database d and the
words w from the associated text are observed, and
the model learns the mapping between them. Notice
that we use w to denote the gold-standard text and
g to refer to the words generated by the model. The
mapping between the database and the observed text
is unknown and thus the weights of the rules define
a hidden correspondence h between records, fields
and their values.
Decoding Given a trained grammar G and an in-
put scenario from a database d, the model generates
text by finding the most likely derivation, i.e., se-
quence of rewrite rules for the input. Although re-
sembling parsing, the generation task is subtly dif-
ferent. In parsing, we observe a string of words and
our goal is to find the most probable syntactic struc-
ture, i.e., hidden correspondence h?. In generation,
1The function g( f .v) : Z? Z, generates an integer in the
following six ways (Liang et al, 2009): identical, rounding
up/down to a multiple of 5, rounding off a multiple of 5 and
adding or subtracting some noise modelled by a geometric dis-
tribution.
however, the string is not observed; instead, we must
find the best text g?, by maximizing both over h and g,
where g = g1 . . .gN is a sequence of words licensed
by G. More formally:
g? = f
(
argmax
g,h
P
(
(g,h)
))
(1)
where f is a function that takes as input a derivation
tree (g,h) and returns g?. Konstas and Lapata (2012)
use a modified version of the CYK parser (Kasami,
1965; Younger, 1967) to find g?. Specifically, they
intersect grammar G with a n-gram language model
and calculate the most probable generation g? as:
g? = f
(
argmax
g,h
p(g) ? p(g,h |d)
)
(2)
where p(g,h |d) is the decoding likelihood for a se-
quence of words g = g1 . . .gN of length N and the
hidden correspondence h that emits it, i.e., the likeli-
hood of the grammar for a given database input sce-
nario d. p(g) is a measure of the quality of each out-
put and is provided by the n-gram language model.
5 Extensions
In this section we extend the model of Konstas and
Lapata (2012) by developing two more sophisticated
content selection approaches which are informed by
a global plan of the document to be generated.
5.1 Planning with Record Sequences
Grammar Our key idea is to replace the content
selection mechanism of the original model with a
document plan which essentially defines a gram-
mar on record types. We split a document into
sentences, each terminated by a full-stop. Then a
sentence is further split into a sequence of record
types. Contrary to the original model, we observe a
complete sequence2 of record types, split into sen-
tences. This way we learn domain-specific pat-
terns of frequently occurring record type sequences
among the sentences of a document, as well as more
local structures within a sentence. We thus substitute
rules (1)?(2) in Figure 2 with sub-grammar GRSE
based on record type sequences:
Definition 1 (GRSE grammar)
GRSE = {?R, NRSE , PRSE , D}
2Note that a sequence is different from a permutation, as we
may allow repetitions or omissions of certain record types.
1506
where ?R is a set of terminal symbols R(r.t), and
NRSE is a set of non-terminal symbols:
NRSE = {D, SENT}
where D represents the start symbol and SENT a
sequence of records. PRSE is a set of production rules
of the form:
(a) D? SENT (ti, . . . , t j) . . . SENT (tl, . . . , tm)
(b) SENT (ti, . . . , t j)? R(ra.ti) . . . R(rk.t j) ?
where t is a record type, ti, t j, tl and tm may overlap
and ra, rk are record tokens of type ti and t j respec-
tively. The corresponding weights for the production
rules PRSE are:
Definition 2 (GRSE weights)
(a) p(ti, . . . , t j, . . . tl, . . . , tm | D)
(b) p(ti) ? ... ? p(t j) = 1|s(ti)| ? . . . ?
1
|s(t j)|
where s(t) is a function that returns the set of records
with type t (Liang et al, 2009).
Rule (a) defines the expansion from the start sym-
bol D to a sequence of sentences, each represented
by the non-terminal SENT . Similarly to the original
grammar G, we employ the use of features (in paren-
theses) to denote a sequence of record types. The
same record types may recur in different sentences,
but not in the same one. The weight of rule (a) is
simply the joint probability of all the record types
present, ordered and segmented appropriately into
sentences in the document, given the start symbol.
Once record types have been selected (on a per
sentence basis) we move on to rule (b) which de-
scribes how each non-terminal SENT expands to
an ordered sequence of records R, as they are ob-
served within a sentence (see the terminal sym-
bol ?.? at the end of the rule). Notice that a record
type ti may correspond to several record tokens ra.
Rules (3)?(5) in grammar G make decisions on these
tokens based on the overall content of the database
and the field/value selection. The weight of this
rule is the product of the weights of each record
type. This is set to the uniform distribution over
{1, ..., |s(t)|} for record type t, where |s(t)| is the
number of records with that type.
Figure 3d shows an example tree for the database
input in Figure 1b, using GRSE and assuming that the
alignments between records and text are given. The
top level of the tree refers to the sequence of record
types as they are observed in the text. The first sen-
tence contains three records with types ?desktop?,
?start? and ?start-target?, each corresponding to the
textual segments click start, point to settings, and
then click control panel. The next level on the tree,
denotes the choice of record tokens for each sen-
tence, provided that we have decided on the choice
and order of their types (see Figure 3b). In Fig-
ure 3d, the bottom-left sub-tree corresponds to the
choice of the first three records of Figure 1b.
Training A straightforward way to train the ex-
tended model would be to embed the parameters of
GRSE in the original model and then run the EM al-
gorithm using inside-outside at the E-step. Unfortu-
nately, this method will induce a prohibitively large
search space. Rule (a) enumerates all possible com-
binations of record type sequences and the number
grows exponentially even for a few record types and
a small sequence size. To tackle this problem, we ex-
tracted rules for GRSE from the training data, based
on the assumption that there will be far fewer unique
sequences of record types per dataset than exhaus-
tively enumerating all possibilities.
For each scenario, we obtain a word-by-word
alignment between the database records and the cor-
responding text. In our experiments we used Liang
et al?s (2009) unsupervised model, however any
other semi- or fully supervised method could be
used. As we show in Section 7, the quality of the
alignment inevitably correlates with the quality of
the extracted grammar and the decoder?s output. We
then map the aligned record tokens to their corre-
sponding types, merge adjacent words with the same
type and segment on punctuation (see Figure 3b).
Next, we create the corresponding tree according to
GRSE (Figure 3d) and binarize it. We experimented
both with left and right binarization and adhered to
the latter, as it obtained a more compact set of rules.
Finally, we collectively count the rule weights on the
resulting treebank and extract a rule set, discarding
rules with frequency less than three.
Using the extracted (weighted) GRSE rules, we run
the EM algorithm via inside-outside and learn the
weights for the remaining rules in G. Decoding re-
mains the same as in Konstas and Lapata (2012);
the only requirement is that the extracted grammar
remains binarized in order to guarantee the cubic
1507
desktop1
Click start,
start1
point to settings,
start-target1
and then click control panel.
win-target1
Double-click users and passwords.
contMenu1
On the advanced tab ,p
action-contMenu1
click advanced.
(a) Record token alignments
[
desktop start start-target?win-target?contMenu action-contMenu?
]
(b) Record type segmentation
[Click start,]desktop1.t [point to settings, ]start1.t [and then
click control panel.]start?target1.t [Double-click users and
passwords.]win?target1.t [On the advanced tab,]contMenu1.t [click
advanced.]action?contMenu1.t
(c) Segmentation of text into EDUs
D
SENT(c, a-c)
R(a-c1.t)R(c1.t)
SENT(w-t)
R(w-t1.t)
SENT(d, s, s-t)
R(s-t1.t)R(s1.t)R(d1.t)
(d) Document plan using the GRSE grammar
D
Elaboration[N][S]
Elaboration[N][S]
R(a-c1.t)R(c1.t)
Elaboration[N][S]
R(w-t1.t)Elaboration[N][S]
Joint[N][N]
R(s-t1.t)R(s1.t)
R(d1.t)
(e) Document plan using the GRST grammar
Figure 3: Grammar extraction example from the WINHELP domain using GRSE and GRST . For GRSE , we take
the alignments of records on words and map them to their corresponding types (a); we then segment record
types into sentences (b); and finally, create a tree using grammar GRSE (c). For GRST , we segment the text
into EDUs based on the records they align to (d) and output the discourse tree (omitted here for brevity?s
sake); we build the document plan once we substitute the EDUs with their corresponding record types (e).
bound of the Viterbi search algorithm. Note that the
original grammar is limited to the generation of cat-
egorical and integer values. We extend it to support
the generation of strings. The following rule adds a
simple verbatim lexicalization for string values:
W(r,r. f )? gen str( f .v, i)
gen str( f .v, i) : V ?V, f .v ?V
where V is the set of words for the fields of type
string, and gen str is a function that takes the value
of a string-typed field f .v, and the position i in
the string, and generates the corresponding word at
that position. For example, gen str(users and pass-
words, 3) = passwords. The weight of this rule is set
to 1.
5.2 Planning with Rhetorical Structure Theory
Grammar RST (Mann and Thompson, 1988) is a
theory of text organization which provides a frame-
work for analyzing text. A basic tenet of the the-
ory is that a text consists of hierarchically organized
text spans or elementary discourse units (EDUs) that
stand in various relations to one another (e.g., Elab-
oration, Attribution). These ?rhetorical relations?
hold between two adjacent parts of the text, where
typically, one part is ?nuclear? and one a ?satellite?.
An analysis of a text consists in identifying the re-
lations holding between successively larger parts of
the text, yielding a natural hierarchical description
of the rhetorical organization of the text. From its
very inception, RST was conceived as a way to char-
acterize text and textual relations for the purpose of
text generation.
In order to create a RST-inspired document plan
for our input (i.e., database records paired with
texts), we make the following assumption: each
record corresponds to a unique non-overlapping
span in the collocated text, and can be therefore
mapped to an EDU. Assuming the text has been seg-
mented and aligned to a sequence of records, we
can create a discourse tree with record types (in
place of their corresponding EDUs) as leaf nodes.
Again, we define a sub-grammar GRST which re-
places rules (1)?(2) from Figure 2:
1508
Definition 3 (GRST grammar)
GRST = {?R, NRST , PRST , D}
where ?R is the alphabet of leaf nodes as de-
fined in Section 5.1, NRST is a set of non-terminals
corresponding to rhetorical relations augmented
with nucleus-satellite information (e.g., Elabora-
tion[N][S] stands for the elaboration relation be-
tween the nucleus EDU left-adjoining with the satel-
lite EDU), PRST is the set of production rules of the
form PRST ? NRST ?{NRST ??R}?{NRST ??R} as-
sociated with a weight for each rule, and D ? NRST
is the root symbol. Figure 3e gives the discourse tree
for the database input of Figure 1b, using GRST .
Training In order to obtain the weighted produc-
tions of GRST , we use an existing state-of-the-art dis-
course parser3 (Feng and Hirst, 2012) trained on the
RST-DT corpus (Carlson et al, 2001). The latter
contains a selection of 385 Wall Street Journal arti-
cles which have been annotated using the framework
of RST and an inventory of 78 rhetorical relations,
classified into 18 coarse-grained categories (Carl-
son and Marcu, 2001). Figure 4 gives a comparison
of the distribution of relations extracted for the two
datasets we used, against the gold-standard annota-
tion of RST-DT. The statistics for the RST-DT cor-
pus are taken from Williams and Power (2008). The
relative frequencies of relations on both datasets fol-
low closely the distribution of those in RST-DT, thus
empirically supporting the application of the RST
framework to our data.
We segment each document in our training set
into EDUs based on the record-to-text alignments
given by the model of Liang et al (2009) (see Fig-
ure 3c). We then run the discourse parser on the
resulting EDUs, and retrieve the corresponding dis-
course tree; the internal nodes are labelled with
one of the RST relations. Finally, we replace the
leaf EDUs with their respective terminal symbols
R(r.t) ? ?R (Figure 3e) and collect the resulting
grammar productions; their weights are calculated
via maximum likelihood estimation based on their
collective counts in the parse trees licensed by GRST .
Training and decoding of the extended generation
model (after we embed GRST in the original gram-
mar G) is performed identically to Section 5.1.
3Publicly available from http://www.cs.toronto.edu/
?weifeng/software.html.
6 Experimental Design
Data Since our aim was to evaluate the planning
component of our model, we used datasets whose
documents are at least a few sentences long. Specif-
ically, we generated weather forecasts and trou-
bleshooting guides for an operating system. For
the first domain (henceforth WEATHERGOV) we used
the dataset of Liang et al (2009), which consists
of 29,528 weather scenarios for 3,753 major US
cities (collected over four days). The database has
12 record types, each scenario contains on average
36 records, 5.8 out of which are mentioned in the
text. A document has 29.3 words and is four sen-
tences long. The vocabulary is 345 words. We used
25,000 scenarios from WEATHERGOV for training,
1,000 scenarios for development and 3,528 scenar-
ios for testing.
For the second domain (henceforth WINHELP) we
used the dataset of Branavan et al (2009), which
consists of 128 scenarios. These are articles from
Microsoft?s Help and Support website4 and contain
step-by-step instructions on how to perform tasks on
the Windows 2000 operating system. In its original
format, the database provides a semantic representa-
tion of the textual guide, i.e., it represents the user?s
actions on the operating system?s UI. We semi-
automatically converted this representation into a
schema of records, fields and values, following the
conventions adopted in Branavan et al (2009).5 The
final database has 13 record types. Each scenario has
9.2 records and each document 51.92 words with 4.3
sentences. The vocabulary is 629 words. We per-
formed 10-fold cross-validation on the entire dataset
for training and testing. Compared to WEATHER-
GOV, WINHELP documents are longer with a larger
vocabulary. More importantly, due to the nature of
the domain, i.e., giving instructions, content selec-
tion is critical not only in terms of what to say but
also in what order.
Grammar Extraction and Parameter Setting
We obtained alignments between database records
and textual segments for both domains and gram-
mars (GRSE and GRST ) using the unsupervised model
of Liang et al (2009). On WEATHERGOV, we ex-
tracted a GRSE grammar with 663 rules (after bi-
4support.microsoft.com
5The dataset can be downloaded from http://homepages.
inf.ed.ac.uk/ikonstas/index.php?page=resources
1509
E
la
bo
ra
ti
on
A
tt
ri
bu
ti
on
Jo
in
t
C
on
tr
as
t
E
xp
la
na
ti
on
B
ac
kg
ro
un
d
E
na
bl
em
en
t
C
au
se
E
va
lu
at
io
n
C
om
pa
ri
so
n
C
on
di
ti
on
To
pi
c-
C
om
m
en
t
Te
m
po
ra
l
E
xp
la
na
ti
on
S
um
m
ar
y
To
pi
c
C
ha
ng
e
0
20
40
60 RST-DT
WEATHERGOV
WINHELP
Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power,
2008).
narization). The WINHELP dataset is considerably
smaller, and as a result the procedure described in
Section 5.1 yields a very sparse grammar. To al-
leviate this, we horizontally markovized the right-
hand side of each rule (Collins, 1999; Klein and
Manning, 2003).6 After markovization, we obtained
a GRSE grammar with 516 rules. On WEATHERGOV,
we extracted 434 rules for GRST . On WINHELP we
could not follow the horizontal markovization pro-
cedure, since the discourse trees are already bina-
rized. Instead, we performed vertical markovization,
i.e., annotated each non-terminal with their parent
node (Johnson, 1998) and obtained a GRST grammar
with 419 rules. The model of Konstas and Lapata
(2012) has two parameters, namely the number of
k-best lists to keep in each derivation, and the or-
der of the language model. We tuned k experimen-
tally on the development set and obtained best re-
sults with 60 for WEATHERGOV and 120 for WIN-
HELP. We used a trigram model for both domains,
trained on each training set.
Evaluation We compared two configurations of
our system, one with a content planning compo-
nent based on record type sequences (GRSE) and
6When horizontally markovizing, we can encode an arbi-
trary amount of context in the intermediate non-terminals that
result from this process; in our case we store h=1 horizontal
siblings plus the mother left-hand side (LHS) non-terminal, in
order to uniquely identify the Markov chain. For example,
A? B C D becomes A? B ?A . . .B?, ?A . . .B? ? C ?A . . .C?,
?A . . .C? ? D.
another one based on RST (GRST ). In both cases
content plans were extracted from (noisy) unsuper-
vised alignments. As a baseline, we used the orig-
inal model of Konstas and Lapata (2012). We also
compared our model to Angeli et al?s system (2010),
which is state of the art on WEATHERGOV.
System output was evaluated automatically, using
the BLEU modified precision score (Papineni et al,
2002) with the human-written text as reference. In
addition, we evaluated the generated text by eliciting
human judgments. Participants were presented with
a scenario and its corresponding verbalization and
were asked to rate the latter along three dimensions:
fluency (is the text grammatical?), semantic correct-
ness (does the meaning conveyed by the text corre-
spond to the database input?) and coherence (is the
text comprehensible and logically structured?). Par-
ticipants used a five point rating scale where a high
number indicates better performance. We randomly
selected 12 documents from the test set (for each do-
main) and produced output with the system of Kon-
stas and Lapata (2012) (henceforth K&L), our two
models using GRSE and GRST , respectively, and An-
geli et al (2010) (henceforth ANGELI). We also in-
cluded the original text (HUMAN) as gold-standard.
We obtained ratings for 60 (12 ? 5) scenario-text
pairs for each domain. Examples of the documents
shown to the participants are given in Table 1.
The study was conducted over the Internet us-
1510
WEATHERGOV WINHELP
G
R
SE
Showers before noon. Cloudy, with a high near
38. Southwest wind between 3 and 8 mph.
Chance of precipitation is 55 %.
Right-click my network places, and then click prop-
erties. Right-click local area connection, and click
properties. Click to select the file and printer sharing
for Microsoft networks, and then click ok.
G
R
ST
Showers likely. Mostly cloudy, with a high around
38. South wind between 1 and 8 mph. Chance of
precipitation is 55 %.
Right-click my network places, and then click proper-
ties. Right-click local area connection. Click file and
printer sharing for Microsoft networks, and click ok.
K
&
L
A chance of showers. Otherwise, cloudy, with a
high near 38. Southwest wind between 3 and 8
mph.
Right-click my network places, click properties.
Right-click local area connection. Click to select the
file and printer sharing for Microsoft networks, and
then click ok.
A
N
G
E
L
I A chance of rain or drizzle after 9am. Mostly
cloudy, with a high near 38. Southwest wind be-
tween 3 and 8 mph. Chance of precipitation is 50
%.
Right-click my network places, and then click prop-
erties on the tools menu, and then click proper-
ties. Right-click local area connection, and then click
properties. Click file and printer sharing for Microsoft
networks, and then click ok.
H
U
M
A
N A 50 percent chance of showers. Cloudy, with a
high near 38. Southwest wind between 3 and 6
mph.
Right-click my network places, and then click proper-
ties. Right-click local area connection, and then click
properties. Click to select the file and printer sharing
for Microsoft networks check box. Click ok.
Table 1: Human-authored text and system output on WEATHERGOV and WINHELP.
ing Amazon Mechanical Turk7, and involved 200
volunteers (100 for WEATHERGOV, and 100 for
WINHELP), all self reported native English speak-
ers. For WINHELP, we made sure participants were
computer-literate and familiar with the Windows op-
erating system by administering a short question-
naire prior to the experiment.
7 Results
The results of the automatic evaluation are summa-
rized in Table 2. Overall, our models outperform
K&L?s system by a wide margin on both datasets.
The two content planners (GRSE and GRST ) perform
comparably in terms of BLEU. This suggests that
document plans induced solely from data are of sim-
ilar quality to those informed by RST. This is an
encouraging result given that RST-style discourse
parsers are currently available only for English. AN-
GELI performs better on WEATHERGOV possibly due
to better output quality on the surface level. Their
system defines trigger patterns that specifically lexi-
calize record fields containing numbers. In contrast,
on WINHELP it is difficult to explicitly specify such
patterns, as none of the record fields are numeric; as
a result their system performs poorly compared to
7https://www.mturk.com
the other models.
To assess the impact of the alignment on the
content planner, we also extracted GRSE from
cleaner alignments which we obtained automat-
ically via human-crafted heuristics for each do-
main. The heuristics performed mostly anchor
matching between database records and words in the
text (e.g., the value Lkly of the field rainChance,
matches with the string rain likely in the text).
Using these alignments, GRSE obtained a BLEU
score of 39.23 on WEATHERGOV and 41.35 on WIN-
HELP. These results indicate that improved align-
ments would lead to more accurate grammar rules.
WEATHERGOV seems more sensitive to the align-
ments than WINHELP. This is probably because
the dataset shows more structural variations in the
choice of record types at the document level, and
therefore the grammar extracted from the unsuper-
vised alignments is noisier. Unfortunately, perform-
ing this kind of analysis for GRST would require gold
standard segmentation of our training corpus into
EDUs which we neither have nor can easily approx-
imate via heuristics.
The results of our human evaluation study are
shown in Table 3. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
1511
Model WEATHERGOV WINHELP
GRSE 35.60 40.92
GRST 36.54 40.65
K&L 33.70 38.26
ANGELI 38.40 32.21
Table 2: Automatic evaluation of system output us-
ing BLEU-4.
WEATHERGOV WINHELP
Model FL SC CO FL SC CO
GRSE 4.25 3.75 4.18 3.59 3.21 3.35
GRST 4.10 3.68 4.10 3.45 3.29 3.22
K&L 3.73 3.25 3.59 3.27 2.97 2.93
ANGELI 3.90 3.44 3.82 3.44 2.79 2.97
HUMAN 4.22 3.72 4.11 4.20 4.41 4.25
Table 3: Mean ratings for fluency (FL), semantic
correctness (SC) and coherence (CO) on system out-
put elicited by humans.
type (GRSE , GRST , K&L, ANGELI, and HUMAN) on
fluency, semantic correctness and coherence ratings.
Means differences of 0.2 or more are significant at
the 0.05 level using a post-hoc Tukey test. Interest-
ingly, we observe that document planning improves
system output overall, not only in terms of coher-
ence. Across all dimensions our models are per-
ceived better than K&L and ANGELI. As far as co-
herence is concerned, the two content planners are
rated comparably (differences in the means are not
significant). Both GRSE and GRST are significantly
better than the comparison systems (ANGELI and
K&L). Table 1 illustrates examples of system out-
put along with the gold standard content selection
for reference, for the WEATHERGOV and WINHELP
domains, respectively.
In sum, we observe that integrating document
planning either via GRSE or GRST boosts perfor-
mance. Document plans induced from record
sequences exhibit similar performance, compared
to those generated using expert-derived linguistic
knowledge. Our systems are consistently better than
K&L both in terms of automatic and human eval-
uation and are close or better than the supervised
model of Angeli et al (2010). We also show that
feeding the system with a grammar of better qual-
ity can achieve state-of-the-art performance, without
further changes to the model.
8 Conclusions
In this paper, we have proposed an end-to-end sys-
tem that generates text from database input and cap-
tures all components of the traditional generation
pipeline, including document planning. Document
plans are induced automatically from training data
and are represented intuitively by PCFG rules cap-
turing the structure of the database and the way it
renders to text. We proposed two complementary
approaches to inducing content planners. In a first
linguistically naive approach, a document is mod-
elled as a sequence of sentences and each sentence
as a sequence of records. Our second approach
draws inspiration from Rhetorical Structure Theory
(Mann and Thomson, 1988) and represents a docu-
ment as a tree with intermediate nodes correspond-
ing to discourse relations, and leaf nodes to database
records.
Experiments with both approaches demonstrate
improvements over models that do not incorporate
document planning. In the future, we would like to
tackle more challenging domains, such as NFL re-
caps, financial articles and biographies (Howald et
al., 2013; Schilder et al, 2013). Our models could
also benefit from the development of more sophis-
ticated planners either via grammar refinement or
more expressive grammar formalisms (Cohn et al,
2010).
Acknowledgments
We are grateful to Percy Liang and Gabor Angeli
for providing us with their code and data. Thanks to
Giorgio Satta and Charles Sutton for helpful com-
ments and suggestions. We also thank the members
of the Probabilistic Models reading group at the Uni-
versity of Edinburgh for useful feedback.
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502?512, Cambridge, MA.
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive content selection for concept-to-text generation.
In Proceedings of Human Language Technology and
1512
Empirical Methods in Natural Language Processing,
pages 331?338, Vancouver, British Columbia.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 14(4):431?455.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
82?90, Suntec, Singapore.
L. Carlson and D. Marcu. 2001. Discourse tagging ref-
erence manual. Technical report, Univ. of Southern
California / Information Sciences Institute.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue - Volume 16, SIGDIAL ?01, pages 1?10,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128?135, Helsinki, Finland.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, 11(November):3053?
3096.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Robert Dale. 1988. Generating referring expressions in
a domain of objects and processes. Ph.D. thesis, Uni-
versity of Edinburgh.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society, se-
ries B, 39(1):1?38.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In Proceedings of the 39th An-
nual Meeting on Association for Computational Lin-
guistics, pages 172?179.
Pablo A. Duboue and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Pro-
ceedings of International Natural Language Genera-
tion, pages 89?96, Ramapo Mountains, NY.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 60?68, Jeju
Island, Korea.
Eduard Hovy. 1993. Automated discourse generation
using discourse structure relations. Artificial Intelli-
gence, 63:341?385.
Blake Howald, Ravikumar Kondadadi, and Frank
Schilder. 2013. Domain adaptable semantic clustering
in statistical nlg. In Proceedings of the 10th Interna-
tional Conference on Computational Semantics (IWCS
2013) ? Long Papers, pages 143?154, Potsdam, Ger-
many, March. Association for Computational Linguis-
tics.
Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632, December.
Nikiforos Karamanis. 2003. Entity Coherence for De-
scriptive Text Structuring. Ph.D. thesis, University of
Edinburgh.
Tadao Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Lab, Bedford, MA.
Rodger Kibble and Richard Power. 2004. Optimising
referential coherence in text generation. Computa-
tional Linguistics, 30(4):401?416.
Joohyun Kim and Raymond Mooney. 2010. Generative
alignment and semantic parsing for learning from am-
biguous supervision. In Proceedings of the 23rd Con-
ference on Computational Linguistics, pages 543?551,
Beijing, China.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics Morristown, NJ, USA.
Ioannis Konstas and Mirella Lapata. 2012. Unsupervised
concept-to-text generation with hypergraphs. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 752?
761, Montre?al, Canada.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 91?99, Suntec, Singapore.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
William C. Mann and Sandra A. Thomson. 1988.
Rhetorical structure theory. Text, 8(3):243?281.
1513
Chris Mellish, Alisdair Knott, Jon Oberlander, and Mick
O?Donnell. 1998. Experiments using stochastic
search for text planning. In Proceedings of Interna-
tional Natural Language Generation, pages 98?107,
New Brunswick, NJ.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian
Davy. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Frank Schilder, Blake Howald, and Ravi Kondadadi.
2013. Gennext: A consolidated domain adaptable nlg
system. In Proceedings of the 14th European Work-
shop on Natural Language Generation, pages 178?
182, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Donia Scott and Clarisse Sieckenius de Souza. 1990.
Getting the message across in RST-based text gener-
ation. In Robert Dale, Chris Mellish, and Michael
Zock, editors, Current Research in Natural Language
Generation, pages 47?73. Academic Press, New York.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of Association for Computational Linguis-
tics, pages 79?86, Barcelona, Spain.
Sandra Williams and Richard Power. 2008. Deriving
rhetorical complexity data from the rst-dt corpus. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), May.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?179, Rochester, NY.
Daniel H Younger. 1967. Recognition and parsing for
context-free languages in time n3. Information and
Control, 10(2):189?208.
1514
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301?312,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incremental Semantic Role Labeling with Tree Adjoining Grammar
Ioannis Konstas
?
, Frank Keller
?
, Vera Demberg
?
and Mirella Lapata
?
?: Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
{ikonstas,keller,mlap}@inf.ed.ac.uk
?: Cluster of Excellence Multimodal Computing and Interaction,
Saarland University
vera@coli.uni-saarland.de
Abstract
We introduce the task of incremental se-
mantic role labeling (iSRL), in which se-
mantic roles are assigned to incomplete
input (sentence prefixes). iSRL is the
semantic equivalent of incremental pars-
ing, and is useful for language model-
ing, sentence completion, machine trans-
lation, and psycholinguistic modeling. We
propose an iSRL system that combines
an incremental TAG parser with a seman-
tically enriched lexicon, a role propaga-
tion algorithm, and a cascade of classi-
fiers. Our approach achieves an SRL F-
score of 78.38% on the standard CoNLL
2009 dataset. It substantially outper-
forms a strong baseline that combines
gold-standard syntactic dependencies with
heuristic role assignment, as well as a
baseline based on Nivre?s incremental de-
pendency parser.
1 Introduction
Humans are able to assign semantic roles such as
agent, patient, and theme to an incoming sentence
before it is complete, i.e., they incrementally build
up a partial semantic representation of a sentence
prefix. As an example, consider:
(1) The athlete realized [her
goals]
PATIENT/THEME
were out of reach.
When reaching the noun phrase her goals, the hu-
man language processor is faced with a semantic
role ambiguity: her goals can either be the PA-
TIENT of the verb realize, or it can be the THEME
of a subsequent verb that has not been encoun-
tered yet. Experimental evidence shows that the
human language processor initially prefers the PA-
TIENT role, but switches its preference to the
theme role when it reaches the subordinate verb
were. Such semantic garden paths occur because
human language processing occurs word-by-word,
and are well attested in the psycholinguistic litera-
ture (e.g., Pickering et al., 2000).
Computational systems for performing seman-
tic role labeling (SRL), on the other hand, proceed
non-incrementally. They require the whole sen-
tence (typically together with its complete syntac-
tic structure) as input and assign all semantic roles
at once. The reason for this is that most features
used by current SRL systems are defined globally,
and cannot be computed on sentence prefixes.
In this paper, we propose incremental SRL
(iSRL) as a new computational task that mimics
human semantic role assignment. The aim of an
iSRL system is to determine semantic roles while
the input unfolds: given a sentence prefix and its
partial syntactic structure (typically generated by
an incremental parser), we need to (a) identify
which words in the input participate in the seman-
tic roles as arguments and predicates (the task of
role identification), and (b) assign correct seman-
tic labels to these predicate/argument pairs (the
task of role labeling). Performing these two tasks
incrementally is substantially harder than doing it
non-incrementally, as the processor needs to com-
mit to a role assignment on the basis of incom-
plete syntactic and semantic information. As an
example, take (1): on reaching athlete, the proces-
sor should assign this word the AGENT role, even
though it has not seen the corresponding predicate
yet. Similarly, upon reaching realized, the pro-
cessor can complete the AGENT role, but it should
also predict that this verb also has a PATIENT role,
even though it has not yet encountered the argu-
ment that fills this role. A system that performs
SRL in a fully incremental fashion therefore needs
to be able to assign incomplete semantic roles,
unlike existing full-sentence SRL models.
The uses of incremental SRL mirror the applica-
tions of incremental parsing: iSRL models can be
used in language modeling to assign better string
probabilities, in sentence completion systems to
301
provide semantically informed completions, in
any real time application systems, such as dia-
log processing, and to incrementalize applications
such as machine translation (e.g., in speech-to-
speech MT). Crucially, any comprehensive model
of human language understanding needs to com-
bine an incremental parser with an incremental se-
mantic processor (Pad?o et al., 2009; Keller, 2010).
The present work takes inspiration from the
psycholinguistic modeling literature by proposing
an iSRL system that is built on top of a cogni-
tively motivated incremental parser, viz., the Psy-
cholinguistically Motivated Tree Adjoining Gram-
mar parser of Demberg et al. (2013). This parser
includes a predictive component, i.e., it predicts
syntactic structure for upcoming input during in-
cremental processing. This makes PLTAG par-
ticularly suitable for iSRL, allowing it to predict
incomplete semantic roles as the input string un-
folds. Competing approaches, such as iSRL based
on an incremental dependency parser, do not share
this advantage, as we will discuss in Section 4.3.
2 Related Work
Most SRL systems to date conceptualize seman-
tic role labeling as a supervised learning prob-
lem and rely on role-annotated data for model
training. Existing models often implement a
two-stage architecture in which role identification
and role labeling are performed in sequence. Su-
pervised methods deliver reasonably good perfor-
mance with F-scores in the low eighties on stan-
dard test collections for English (M`arquez et al.,
2008; Bj?orkelund et al., 2009).
Current approaches rely primarily on syntactic
features (such as path features) in order to iden-
tify and label roles. This has been a mixed bless-
ing as the path from an argument to the predi-
cate can be very informative but is often quite
complicated, and depends on the syntactic formal-
ism used. Many paths through the parse tree are
likely to occur infrequently (or not at all), result-
ing in very sparse information for the classifier to
learn from. Moreover, as we will discuss in Sec-
tion 4.4, such path information is not always avail-
able when the input is processed incrementally.
There is previous SRL work employing Tree Ad-
joining Grammar, albeit in a non-incremental set-
ting, as a means to reduce the sparsity of syntax-
based features. Liu and Sarkar (2007) extract a
rich feature set from TAG derivations and demon-
strate that this improves SRL performance.
In contrast to incremental parsing, incremental
semantic role labeling is a novel task. Our model
builds on an incremental Tree Adjoining Gram-
mar parser (Demberg et al., 2013) which predicts
the syntactic structure of upcoming input. This al-
lows us to perform incremental parsing and incre-
mental SRL in tandem, exploiting the predictive
component of the parser to assign (potentially in-
complete) semantic roles on a word-by-word ba-
sis. Similar to work on incremental parsing that
evaluates incomplete trees (Sangati and Keller,
2013), we evaluate the incomplete semantic struc-
tures produced by our model.
3 Psycholinguistically Motivated TAG
Demberg et al. (2013) introduce Psycholin-
guistically Motivated Tree Adjoining Grammar
(PLTAG), a grammar formalism that extends stan-
dard TAG (Joshi and Schabes, 1992) in order to
enable incremental parsing. Standard TAG as-
sumes a lexicon of elementary trees, each of
which contains at least one lexical item as an an-
chor and at most one leaf node as a foot node,
marked with A?. All other leaves are marked with
A? and are called substitution nodes. Elementary
trees that contain a foot node are called auxiliary
trees; those that do not are called initial trees. Ex-
amples for TAG elementary trees are given in Fig-
ure 1a?c.
To derive a TAG parse for a sentence, we start
with the elementary tree of the head of the sen-
tence and integrate the elementary trees of the
other lexical items of the sentence using two oper-
ations: adjunction at an internal node and substi-
tution at a substitution node (the node at which the
operation applies is the integration point). Stan-
dard TAG derivations are not guaranteed to be in-
cremental, as adjunction can happen anywhere in
a sentence, possibly violating left-to-right process-
ing order. PLTAG addresses this limitation by in-
troducing prediction trees, elementary trees with-
out a lexical anchor. These can be used to predict
syntactic structure anchored by words that appear
later in an incremental derivation. The use of pre-
diction trees ensures that fully connected prefix
trees can be built for every prefix of the input sen-
tence.
Each node in a prediction tree carries mark-
ers to indicate that this node was predicted, rather
than being anchored by the current sentence pre-
fix. An example is Figure 1d, which contains a
prediction tree with marker ?1?. In PLTAG, mark-
ers are eliminated through a new operation called
verification, which matches them with the nodes
302
(a) NP
NNS
Banks
(b) S
VP
VB
open
NP?
(c) VP
VP*AP
RB
rarely
(d) S
1
VP
1
1
NP
1
?
Figure 1: PLTAG lexicon entries: (a) and (b) ini-
tial trees, (c) auxiliary tree, (d) prediction tree.
a
S
 B? 
 C? 
a
S
B
 C? 
b
a
S
 B? 
C
c
(a) valid (b) invalid
Figure 3: The current fringe (dashed line) indi-
cates where valid substitutions can occur. Other
substitutions result in an invalid prefix tree.
of non-predictive elementary trees. An example
of a PLTAG derivation is given in Figure 2. In
step 1, a prediction tree is introduced through sub-
stitution, which then allows the adjunction of an
adverb in step 2. Step 3 involves the verification
of the marker introduced by the prediction tree
against the elementary tree for open.
In order to efficiently parse PLTAG, Demberg
et al. (2013) introduce the concept of fringes.
Fringes capture the fact that in an incremental
derivation, a prefix tree can only be combined with
an elementary tree at a limited set of nodes. For
instance, the prefix tree in Figure 3 has two substi-
tution nodes, for B and C. However, only substi-
tution into B leads to a valid new prefix tree; if we
substitute into C, we obtain the tree in Figure 3b,
which is not a valid prefix tree (i.e., it represents a
non-incremental derivation).
The parsing algorithm proposed by Demberg
et al. (2013) exploits fringes to tabulate interme-
diate results. It manipulates a chart in which each
cell (i, f ) contains all the prefix trees whose first
i leaves are the first i words and whose current
fringe is f . To extend the prefix trees for i to
the prefix trees for i+ 1, the algorithm retrieves
all current fringes f such that the chart has entries
in the cell (i, f ). For each such fringe, it needs
to determine the elementary trees in the lexicon
that can be combined with f using substitution or
adjunction. In spite of the large size of a typi-
cal TAG lexicon, this can be done efficiently, as
it only requires matching the current fringes. For
each match, the parser then computes the new pre-
Banks refused to open today
A0
A1
A1
AM-TMP
nsbj aux
xcomp
tmod
?A0,Banks,refused?
?A1,to,refused?
?A1,Banks,open?
?AM-TMP,today,open?
Figure 4: Syntactic dependency graph with se-
mantic role annotation and the accompanying se-
mantic triples, for Banks refused to open today.
fix trees and its new current fringe f
?
and enters it
into cell (i+1, f
?
).
Demberg et al. (2013) convert the Penn Tree-
bank (Marcus et al., 1993) into TAG for-
mat by enriching it with head information and
argument/modifier information from Propbank
(Palmer et al., 2005). This makes it possible
to decompose the Treebank trees into elementary
trees as proposed by Xia et al. (2000). Predic-
tion trees can be learned from the converted Tree-
bank by calculating the connection path (Mazzei
et al., 2007) at each word in a tree. Intuitively,
a prediction tree for word w
n
contains the struc-
ture that is necessary to connect w
n
to the prefix
tree w
1
. . .w
n?1
, but is not part of any of the ele-
mentary trees of w
1
. . .w
n?1
. Using this lexicon, a
probabilistic model over PLTAG operations can be
estimated following Chiang (2000).
4 Model
4.1 Problem Formulation
In a typical semantic role labeling scenario, the
goal is to first identify words that are predicates
in the sentence and then identify and label all the
arguments for each predicate. This translates into
spotting specific words in a sentence that repre-
sent the predicate?s arguments, and assigning pre-
defined semantic role labels to them. Note that in
this work we focus on verb predicates only. The
output of a semantic role labeler is a set of seman-
tic dependency triples ?l,a, p?, with l ? R , and
a, p ? w, where R is a set of semantic role labels
denoting a specific relationship between a predi-
cate and an argument (e.g., ARG0, ARG1, ARGM
in Propbank), w is the list of words in the sentence,
l denotes a specific role label, a the argument, and
p the predicate. An example is shown in Figure 4.
As discussed in the introduction, standard se-
mantic role labelers make their decisions based on
evidence from the whole sentence. In contrast, our
aim is to assign semantic roles incrementally, i.e.,
303
NP
NNS
Banks
S
1
VP
1
1
NP
NNS
Banks
S
1
VP
1
VP
1
AP
RB
rarely
NP
NNS
Banks
S
VP
VP
VB
open
AP
RB
rarely
NP
NNS
Banks
1. subst
2. adj
3. verif
Figure 2: Incremental parse for Banks rarely open using the operations substitution (with a prediction
tree), adjunction, and verification.
we want to produce a set of (potentially incom-
plete) semantic dependency triples for each prefix
of the input sentence. Note that not every word
is an argument to a predicate, therefore the set of
triples will not necessarily change at every input
word. Furthermore, the triples themselves may
be incomplete, as either the predicate or the argu-
ment may not have been observed yet (predicate-
incomplete or argument-incomplete triples).
Our iSRL system relies on PLTAG, using a se-
mantically augmented lexicon. We parse an in-
put sentence incrementally, applying a novel in-
cremental role propagation algorithm (IRPA) that
creates or updates existing semantic triple candi-
dates whenever an elementary (or prediction) tree
containing role information is attached to the ex-
isting prefix tree. As soon as a triple is completed
we apply a two-stage classification process, that
first identifies whether the predicate/argument pair
is a good candidate, and then disambiguates role
labels in case there is more than one candidate.
4.2 Semantic Role Lexicon
Recall that Propbank is used to construct the
PLTAG treebank, in order to distinguish between
arguments and modifiers, which result in elemen-
tary trees with substitution nodes, and auxiliary
trees, i.e., trees with a foot node, respectively (see
Figure 1). Conveniently, we can use the same in-
formation to also enrich the extracted lexicon with
the semantic role annotations, following the pro-
cess described by Sayeed and Demberg (2013).
1
For arguments, annotations are retained on the
substitution node in the parental tree, while for
modifiers, the role annotation is displayed on the
foot node of the auxiliary tree. Note that we dis-
play role annotation on traces that are leaf nodes,
1
Contrary to Sayeed and Demberg (2013) we put role la-
bel annotations for PPs on the preposition rather than their
NP child, following of the CoNLL 2005 shared task (Carreras
and M`arquez, 2005).
which enables us to recover long-range dependen-
cies (third and fifth tree in Figure 5a). Likewise,
we annotate prediction trees with semantic roles,
which enables our system to predict upcoming in-
complete triples.
Our annotation procedure unavoidably intro-
duces some role ambiguity, especially for fre-
quently occurring trees. This can give rise to two
problems when we generate semantic triples incre-
mentally: IRPA tends to create many spurious can-
didate semantic triples for elementary trees that
correspond to high frequency words (e.g., preposi-
tions or modals). Secondly, a semantic triple may
be identified correctly but is assigned several role
labels. (See the elementary tree for refuse in Fig-
ure 5a.) We address these issues by applying clas-
sifiers for role label disambiguation at every pars-
ing operation (substitution, adjunction, or verifica-
tion), as detailed in Section 4.4.
4.3 Incremental Role Propagation Algorithm
The main idea behind IRPA is to create or up-
date existing semantic triples as soon as there is
available role information during parsing. Our al-
gorithm (lines 1?6 in Algorithm 1) is applied af-
ter every PLTAG parsing operation, i.e., when an
elementary or prediction tree T is adjoined to a
particular integration point node pi
ip
of the prefix
tree of the sentence, via substitution or adjunction
(lines 3?4).
2
In case an elementary tree T
v
verifies
a prediction tree T
pr
(lines 5?6), the same method-
ology applies, the only difference being that we
have to tackle multiple integration point nodes
T
pr,ip
, one for each prediction marker of T
pr
that
matches the corresponding nodes in T
v
.
For simplicity of presentation, we will use a
concrete example, see Figure 5. Figure 5a shows
the lexicon entries for the words of the sentence
2
Prediction tree T
pr
in our algorithm is only used during
verification, so it set to nil for substitution and adjunction op-
erations.
304
Banks refused to open. Naturally, some nodes in
the lexicon trees might have multiple candidate
role labels. For example, the substitution NP node
of the second tree takes two labels, namely A0
and A1. These stem from different role signatures
when the same elementary tree occurs in differ-
ent contexts during training (A1 only on the NP;
A0 on the NP and A1 on S). For simplicity?s sake,
we collapse different signatures, and let a classi-
fier labeller to disambiguate such cases (see Sec-
tion 4.4).
Algorithm 1 Incremental Role Propagation Alg.
1: procedure IRPA(pi
ip
, T , T
pr
)
2: ??? . ? is a dictionary of (pi
ip
, ?l,a, p?) pairs
3: if parser operation is substitution or adjunction then
4: CREATE-TRIPLES(pi
ip
, T )
5: else if parser operation is verification then
6: CREATE-TRIPLES-VERIF(pi
ip
, T , T
pr
)
return set of triples ?l,a, p? for prefix tree pi
7: procedure CREATE-TRIPLES(pi
ip
, T )
8: if HAS-ROLES(pi
ip
) then
9: UPDATE-TRIPLE(pi
ip
, T )
10: else if HAS-ROLES(T ) then
11: T
ip
? substitution or foot node of T
12: ADD-TRIPLE(pi
ip
, T
ip
, T )
13: for all remaining nodes n ? T with roles do
14: ADD-TRIPLE(pi
ip
, n, T ) . incomplete triples
15: procedure CREATE-TRIPLES-VERIF(pi
ip
, T
v
, T
pr
)
16: if HAS-ROLES(T
v
) then
17: anchor? lexeme of T
v
18: for all T
ip
? node in T
v
with role do
19: T
pr,ip
? matching node of T
ip
in T
pr
20: CREATE-TRIPLES(T
pr,ip
, T
v
)
. Process the rest of covered nodes in T
pr
with roles
21: for all remaining T
pr,ip
? node in T
pr
with role do
22: UPDATE-TRIPLE(T
pr,ip
, T
pr
)
23: function UPDATE-TRIPLE(pi
ip
, T )
24: dep? FIND-INCOMPLETE(?, T
ip
)
25: anchor? lexeme of T
26: if anchor of T is predicate then
27: SET-PREDICATE(dep, anchor)
28: else if anchor of T is argument then
29: SET-ARGUMENT(dep, anchor)
return dep
30: procedure ADD-TRIPLE(pi
ip
, T
ip
, T )
31: dep? ?[roles of T
ip
], nil, nil?
32: anchor? lexeme of T
33: if anchor of T is predicate then
34: SET-PREDICATE(dep, anchor)
35: SET-ARGUMENT(dep, head of pi
ip
)
36: else if anchor of T is argument then
37: if T is auxiliary then . adjunction
38: SET-ARGUMENT(dep, anchor)
39: else . substitution: arg is head of prefix tree
40: SET-ARGUMENT(dep, head of T
ip
)
41: pred? find dep ? ? with matching pi
ip
42: SET-PREDICATE(dep, pred)
43: ?? (pi
ip
, dep)
Once we process Banks, the prefix tree becomes
the lexical entry for this word, see the first col-
umn of Figure 5b. Next, we process refused:
the parser substitutes the prefix tree into the ele-
mentary tree T of refused;
3
the integration point
pi
ip
on the prefix tree is the topmost NP. Since
the operation is a substitution (line 3), we create
triples between T and pi
ip
via CREATE-TRIPLES
(lines 7?12). pi
ip
does not have any role infor-
mation (line 8), so we proceed to add a new se-
mantic triple between the role-labeled integration
point T
ip
, i.e., substitution NP node of T , and pi
ip
,
via ADD-TRIPLE (lines 30?43). First, we create
an incomplete semantic triple with all roles from
T
ip
(line 31). Then we set the predicate to the an-
chor of T to be the word refused, and the argu-
ment to be the head word of the prefix tree, Banks
(lines 34?35). Note that predicate identification is
a trivial task based on part-of-speech information
in the elementary tree.
4
Then, we add the pair (NP? ?{A0,A1},Banks,
refused?) to a dictionary (line 43). Storing the in-
tegration point along with the semantic triple is
essential, to be able to recover incomplete triples
in later stages of the algorithm. Finally, we re-
peat this process for all remaining nodes on T that
have roles, in our example the substitution node S
(lines 13?14). This outputs an incomplete triple,
?{A1},nil,refused?.
Next, the parser decides to substitute a predic-
tion tree (third tree in Figure 5a) into the substitu-
tion node S of the prefix tree. Since the integration
point is on the prefix tree and has role information
(line 8), the corresponding triple should already be
present in our dictionary. Upon retrieving it, we
set the nil argument to the anchor of the incoming
tree. Since it is a prediction tree, we set it to the
root of the tree, namely S
2
(phrase labels in triples
are denoted by italics), but mark the triple as yet
incomplete. This distinction allows us to fill in the
correct lexical information once it becomes avail-
able, i.e, when the tree gets verified. We also add
an incomplete triple for the trace t in the subject
position of the prediction tree, as described above.
Note that this triple contains multiple roles; this is
expected given that prediction trees are unlexical-
ized and occur in a wide variety of contexts.
When the next verb arrives, the parser success-
fully verifies it against the embedded prediction
3
PLTAG parsing operations can occur in two ways: An
elementary tree can be substituted into the substitution node
of the prefix tree, or the prefix tree can be substituted into a
node of an elementary tree. The same holds for adjunction.
4
Most predicates can be identified as anchors of non-
modifier auxiliary trees. However, there are exceptions to
this rule, i.e., modifier auxiliary trees and non-modifier non-
auxiliary trees being also verbs in our lexicon, hence the use
of the more reliable POS tags.
305
IRPA MaltParser
Banks ? ?
refused ?{A0,A1},Banks,refused?,
?A1,S
2
,refused?,
?{A0,A1,A2},t,nil?
?A0,Banks,refused?
to ? ?
open ?A1,to,refused?,
?A1,Banks,open?
?A1,to,refused?,
?A0,Banks,open?
today ?AM-TMP,today,open? ?AM-TMP,today,open?
Table 1: Complete and incomplete semantic triple
generation, comparing IRPA and a system that
maps gold-standard role labels onto MaltParser in-
cremental dependencies for Figure 4.
tree within the prefix tree (last step of Figure 5b).
Our algorithm first cycles through all nodes that
match between the verification tree T
v
and the pre-
diction tree T
pr
and will complete or create new
triples via CREATE-TRIPLES (lines 18?20). In
our example, the second semantic triple gets com-
pleted by replacing S
2
with the head of the sub-
tree rooted in S. Normally, this would be the verb
open, but in this case the verb is followed by the
infinitive marker to, hence we heuristically set it
to be the argument of the triple instead, following
Carreras and M`arquez (2005). For the last triple,
we set the predicate to the anchor of T
v
open, and
now are able to remove the excess role labels A0
and A2. This illustrated how the lexicalized veri-
fication tree disambiguates the semantic informa-
tion stored in the prediction tree. Finally, trace t is
set to the closest NP head that is below the same
phrase subtree, in this case Banks. Note that Banks
is part of two triples as shown in the last tree of
Figure 5b: it is either an A0 or an A1 for refused
and an A1 for open.
We are able to create incomplete semantic
triples after the prediction of the upcoming verb at
step 2, as shown in Figure 5b. This is not possible
using an incremental dependency parser such as
MaltParser (Nivre et al., 2007) that lacks a predic-
tive component. Table 1 illustrates this by compar-
ing the output of IRPA for Figure 5b with the out-
put of a baseline system that maps role labels onto
the syntactic dependencies in Figure 4, generated
incrementally by MaltParser (see Section 5.3 for
a description of the MaltParser baseline). Malt-
Parser has to wait for the verb open before out-
putting the relevant semantic triples. In contrast,
IRPA outputs incomplete triples as soon as the in-
formation is available, and later on updates its de-
cision. (MaltParser also incorrectly assigns A0 for
the Banks?open pair.)
4.4 Argument Identification and Role Label
Disambiguation
IRPA produces semantic triples for every role an-
notation present in the lexicon entries, which will
often overgenerate role information. Furthermore,
some triples have more than one role label at-
tached to them. During verification, we are able to
filter out the majority of labels in the correspond-
ing prediction trees; However, most triples are cre-
ated via substitution and adjunction.
In order to address these problems we adhere to
the following classification and ranking strategy:
after each semantic triple gets completed, we per-
form a binary classification that evaluates its suit-
ability as a whole, given bilexical and syntactic in-
formation. If the triple is identified as a good can-
didate, then we perform multi-class classification
over role labels: we feed the same bilexical and
syntactic information to a logistic classifier, and
get a ranked list of labels. We then use this list to
re-rank the existing ambiguous role labels in the
semantic triple, and output the top scoring ones.
The identifier is a binary L2-loss support vec-
tor classifier, and the role disambiguator an L2-
regularized logistic regression classifier, both im-
plemented using the efficient LIBLINEAR frame-
work of Fan et al. (2008). The features used are
based on Bj?orkelund et al. (2009) and Liu and
Sarkar (2007), and are listed in Table 2.
The bilexical features are: predicate POS tag,
predicate lemma, argument word form, argument
POS tag, and position. The latter indicates the po-
sition of the argument relative to the predicate, i.e.,
before, on, or after. The syntactic features are:
the predicate and argument elementary trees with-
out the anchors (to avoid sparsity), the category of
the integration point node on the prefix tree where
the elementary tree of the argument attaches to,
an alphabetically ordered set of the categories of
the fringe nodes of the prefix tree after attaching
the argument tree, and the path of PLTAG opera-
tions applied between the argument and the pred-
icate. Note that most of the original features used
by Bj?orkelund et al. (2009) and others are not ap-
plicable in our context, as they exploit information
that is not accessible incrementally. For example,
sibling information to the right of the word is not
available. Furthermore, our PLTAG parser does
not compute syntactic dependencies, hence these
cannot serve as features (and in any case not all
dependencies are available incrementally, see Fig-
ure 4). To counterbalance this, we use local syn-
tactic information stored in the fringe of the pre-
306
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP?
{A0,A1}
S
2
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VP?TO
to
S
VP
VB
open
NP
t
{A1}
(a) Lexicon entries
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
VP
2
VB
2
2
TO
to
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
VP
VP
VB
open
TO
to
{A1}
NP
t
VP
VBD
refused
NP
NNS
Banks
{A0,A1}/{A1}
1. subst 2. subst
3. adj
4. verif
1. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,nil,refused?
2. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,S
2
,refused?
NP ? ?{A0,A1,A2},t,nil?
3. ?
4. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,to,refused?
NP ? ?A1,Banks,open?
(b) Incremental parsing using PLTAG and incremental propagation of roles
Figure 5: Incremental Role Propagation Algorithm application for the sentence Banks refused to open.
Bilexical Syntactic
PredPOS PredElemTree
PredLemma ArgElemTree
ArgWord IntegrationPoint
ArgPOS PrefixFringe
Position OperationPath
Table 2: Features for argument identification and
role label disambiguation.
fix tree. We also store the series of operations ap-
plied by our parser between argument and predi-
cate, in an effort to emulate the effect of recover-
ing longer-range patterns.
5 Experimental Design
5.1 PLTAG and Classifier Training
We extracted the semantically-enriched lexicon
and trained the PLTAG parser by converting the
Wall Street Journal part of Penn Treebank to
PLTAG format. We used Propbank to retrieve
semantic role annotation, as described in Sec-
tion 4.2. We trained the PLTAG parser according
to Demberg et al. (2013) and evaluated the parser
on section 23, on sentences with 40 words or less,
given gold POS tags for each word, and achieved
a labeled bracket F
1
score of 79.41.
In order to train the argument identification and
role label disambiguation classifiers, we used the
English portion of the CoNLL 2009 Shared Task
(Haji?c et al., 2009; Surdeanu et al., 2008). It
consists of the Penn Treebank, automatically con-
verted to dependencies following Johansson and
307
Nugues (2007), accompanied by semantic role la-
bel annotation for every argument pair. The latter
is converted from Propbank based on Carreras and
M`arquez (2005). We extracted the bilexical fea-
tures for the classifiers directly from the gold stan-
dard annotation of the training set. The syntactic
features were obtained as follows: for every sen-
tence in the training set we applied IRPA using the
trained PLTAG parser, with gold standard lexicon
entries for each word of the input sentence. This
ensures near perfect parsing accuracy. Then for
each semantic triple predicted incrementally, we
extracted the relevant syntactic information in or-
der to construct training vectors. If the identified
predicate-argument pair was in the gold standard
then we assigned a positive label for the identifi-
cation classifier, otherwise we flagged it as nega-
tive. For those pairs that are not identified by IRPA
but exist in the gold standard (false negatives), we
extracted syntactic information from already iden-
tified similar triples, as follows: We first look for
correctly identified arguments, wrongly attached
to a different predicate and re-create the triple with
correct predicate/argument information. If no ar-
gument is found, we then pick the argument in the
list of identified arguments for a correct predicate
with the same POS-tag as the gold-standard argu-
ment. In the case of the role label disambigua-
tion classifier we just assign the gold label for ev-
ery correctly identified pair, and ignore the (possi-
bly ambiguous) predicted one. After tuning on the
development set, the argument identifier achieved
an accuracy of 92.18%, and the role label disam-
biguation classifier, 82.37%.
5.2 Evaluation
The focus of this paper is to build a system that is
able to output semantic role labels for predicate-
argument pairs incrementally, as soon as they be-
come available. In order to properly evaluate such
a system, we need to measure its performance in-
crementally. We propose two different cumulative
scores for assessing the (possibly incomplete) se-
mantic triples that have been created so far, as the
input is processed from left to right, per word. The
first metric is called Unlabeled Prediction Score
(UPS) and gets updated for every identified argu-
ment or predicate, even if the corresponding se-
mantic triple is incomplete. Note that UPS does
not take into account the role label, it only mea-
sures predicate and argument identification. In this
respect it is analogous to unlabeled dependency
accuracy reported in the parsing literature. We ex-
pect a model that is able to predict semantic roles
to achieve an improved UPS result compared to a
system that does not do prediction, as illustrated in
Table 1. Our second score, Combined Incremental
SRL Score (CISS), measures the identification of
complete semantic role triples (i.e., correct predi-
cate, predicate sense, argument, and role label) per
word; by the end of the sentence, CISS coincides
with standard combined SRL accuracy, as reported
in CoNLL 2009 SRL-only task. This score is anal-
ogous to labeled dependency accuracy in parsing.
Note that conventional SRL systems such as
Bj?orkelund et al. (2009) typically assume gold
standard syntactic information. In order to emu-
late this, we give our parser gold standard lexicon
entries for each word in the test set; these contain
all possible roles observed in the training set for
a given elementary tree (and all possible senses
for each predicate). This way the parser achieves
a syntactic parsing F
1
score of 94.24, thus ensur-
ing the errors of our system can be attributed to
IRPA and the classifiers. Also note that we evalu-
ate on verb predicates only, therefore trivially re-
ducing the task of predicate identification to the
simple heuristic of looking for words in the sen-
tence with a verb-related POS tag and excluding
auxiliaries and modals. Likewise, predicate sense
disambiguation on verbs presumably is trivial, as
we observed almost no ambiguity of senses among
lexicon entries of the same verb (we adhered to a
simple majority baseline, by picking the most fre-
quent sense, given the lexeme of the verb, in the
few ambiguous cases). It seems that the syntactic
information held in the elementary trees discrimi-
nates well among different senses.
5.3 System Comparison
We evaluated three configurations of our system.
The first configuration (iSRL) uses all seman-
tic roles for each PLTAG lexicon entry, applies
the PLTAG parser, IRPA, and both classifiers to
perform identification and disambiguation, as de-
scribed in Section 4. The second one (Majority-
Baseline), solves the problem of argument identifi-
cation and role disambiguation without the classi-
fiers. For the former we employ a set of heuristics
according to Lang and Lapata (2014), that rely on
gold syntactic dependency information, sourced
from CoNLL input. For the latter, we choose the
most frequent role given the gold standard depen-
dency relation label for the particular argument.
Note that dependencies have been produced in
view of the whole sentence and not incrementally.
308
System Prec Rec F1
iSRL-Oracle 91.00 80.26 85.29
iSRL 81.48 75.51 78.38
Majority-Baseline 71.05 58.10 63.92
Malt-Baseline 60.90 46.14 52.50
Table 3: Full-sentence combined SRL score
This gives the baseline a considerable advantage
especially in case of longer range dependencies.
The third configuration (iSRL-Oracle), is identical
to iSRL, but uses the gold standard roles for each
PLTAG lexicon entry, and thus provides an upper-
bound for our methodology. Finally, we evalu-
ated against Malt-Baseline, a variant of Majority-
Baseline that uses the MaltParser of Nivre et al.
(2007) to provide labeled syntactic dependencies
MaltParser is a state-of-the-art shift-reduce depen-
dency parser which uses an incremental algorithm.
Following Beuck et al. (2011), we modified the
parser to provide intermediate output at each word
by emitting the current state of the dependency
graph before each shift step. We trained Malt-
Parser using the arc-eager algorithm (which out-
performed the other parsing algorithms available
with MaltParser) on the CoNLL dataset, achiev-
ing a labeled dependency accuracy of 89.66% on
section 23.
6 Results
Figures 6 and 7 show the results on the incremen-
tal SRL task. We plot the F
1
for Unlabeled Predic-
tion Score (UPS) and Combined Incremental SRL
Score (CISS) per word, separately for sentences
of lengths 10, 20, 30, and 40 words. The task gets
harder with increasing sentence length, hence we
can only meaningfully compare the average scores
for sentence of the same length. (This approach
was proposed by Sangati and Keller 2013 for eval-
uating the performance of incremental parsers.)
The UPS results in Figure 6 clearly show that
our system (iSRL) outperforms both baselines
on unlabeled argument and predicate prediction,
across all four sentence lengths. Furthermore,
we note that the iSRL system achieves a near-
constant performance for all sentence prefixes.
Our PLTAG-based prediction/verification archi-
tecture allows us to correctly predict incomplete
semantic role triples, even at the beginning of the
sentence. Both baselines perform worse than the
iSRL system in general. Moreover, the Malt-
Baseline performs badly on the initial sentence
prefixes (up to word 10), presumably as it does
not benefit from syntactic prediction, and thus can-
not generate incomplete triples early in the sen-
tence, as illustrated in Table 1. The Majority-
Baseline also does not do prediction, but it has ac-
cess to gold-standard syntactic dependencies, and
thus outperforms the Malt-Baseline on initial sen-
tence prefixes. Note that due to prediction, our
system tends to over-generate incomplete triples
in the beginning of sentences, compared to non-
incremental output, which may inflate UPS for
the first words. However, this cancels out later
in the sentence if triples are correctly completed;
failure to do so would decrease UPS. The near-
constant performance of our output illustrates this
phenomenon. Finally, the iSRL-Oracle outper-
forms all other systems, as it benefits from correct
role labels and correct PLTAG syntax, thus provid-
ing an upper limit on performance.
The CISS results in Figure 7 present a simi-
lar picture. Again, the iSRL system outperforms
both baselines at all sentence lengths. In addition,
it shows particularly strong performance (almost
at the level of the iSRL-Oracle) at the beginning
of the sentence. This presumably is due to the
fact that our system uses prediction and is able to
identify correct semantic role triples earlier in the
sentence. The baselines also show higher perfor-
mance early in the sentence, but to a lesser degree.
Table 3 reports traditional combined SRL scores
for full sentences over all sentence lengths, as
defined for the CoNLL task. Our iSRL system
outperforms the Majority-Baseline by almost 15
points, and the Malt-Baseline by 25 points. It re-
mains seven points below the iSRL-Oracle upper
limit.
Finally, in order to test the effect of syntactic
parsing on our system, we also experimented with
a variant of our iSRL system that utilizes all lex-
icon entries for each word in the test set. This is
similar to performing the CoNLL 2009 joint task,
which is designed for systems that carry out both
syntactic parsing and semantic role labeling. This
variant achieved a full sentence F-score of 68.0%,
i.e., around 10 points lower than our iSRL system.
This drop in score correlates with the difference
in syntactic parsing F-score between the two ver-
sions of PLTAG parser (94.24 versus 79.41), and
is expected given the high ambiguity of the lex-
icon entries for each word. Note, however, that
the full-parsing version of our system still outper-
forms Malt-Baseline by 15 points.
309
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 6: Unlabeled Prediction Score (UPS)
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 7: Combined iSRL Score (CISS)
7 Conclusions
In this paper, we introduced the new task of incre-
mental semantic role labeling and proposed a sys-
tem that solves this task by combining an incre-
mental TAG parser with a semantically enriched
lexicon, a role propagation algorithm, and a cas-
cade of classifiers. This system achieved a full-
sentence SRL F-score of 78.38% on the standard
CoNLL dataset. Not only is the full-sentence
score considerably higher than the Majority-
Baseline (which is a strong baseline, as it uses
gold-standard syntactic dependencies), but we
also observe that our iSRL system performs well
incrementally, i.e., it predicts both complete and
incomplete semantic role triples correctly early on
in the sentence. We attributed this to the fact that
our TAG-based architecture makes it possible to
predict upcoming syntactic structure together with
the corresponding semantic roles.
Acknowledgments
EPSRC support through grant EP/I032916/1 ?An
integrated model of syntactic and semantic predic-
tion in human language processing? to FK and ML
is gratefully acknowledged.
References
Beuck, Niels, Arne Khn, and Wolfgang Menzel.
2011. Incremental parsing and the evaluation
of partial dependency analyses. In Proceedings
of the 1st International Conference on Depen-
dency Linguistics. Depling 2011.
Bj?orkelund, Anders, Love Hafdell, and Pierre
Nugues. 2009. Multilingual semantic role la-
beling. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language
Learning: Shared Task. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
CoNLL ?09, pages 43?48.
Carreras, Xavier and Llu??s M`arquez. 2005. Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Con-
ference on Computational Natural Language
Learning. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, CONLL ?05,
pages 152?164.
Chiang, David. 2000. Statistical parsing with
an automatically-extracted tree adjoining gram-
mar. In Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics.
pages 456?463.
Demberg, Vera, Frank Keller, and Alexander
Koller. 2013. Incremental, predictive pars-
ing with psycholinguistically motivated tree-
adjoining grammar. Computational Linguistics
39(4):1025?1066.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
310
blinear: A library for large linear classification.
Journal of Machine Learning Research 9:1871?
1874.
Haji?c, Jan, Massimiliano Ciaramita, Richard Jo-
hansson, Daisuke Kawahara, Maria Ant`onia
Mart??, Llu??s M`arquez, Adam Meyers, Joakim
Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel
Stra?n?ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multi-
ple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language
Learning (CoNLL-2009), June 4-5. Boulder,
Colorado, USA.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion
for english. In Joakim Nivre, Heiki-Jaan
Kalep, Kadri Muischnek, and Mare Koit, edi-
tors, NODALIDA 2007 Proceedings. University
of Tartu, pages 105?112.
Joshi, Aravind K. and Yves Schabes. 1992. Tree
adjoining grammars and lexicalized grammars.
In Maurice Nivat and Andreas Podelski, editors,
Tree Automata and Languages, North-Holland,
Amsterdam, pages 409?432.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, Companion Vol-
ume: Short Papers. Uppsala, pages 60?67.
Lang, Joel and Mirella Lapata. 2014. Similarity-
driven semantic role induction via graph par-
titioning. Computational Linguistics Accepted
pages 1?62. To appear.
Liu, Yudong and Anoop Sarkar. 2007. Experimen-
tal evaluation of LTAG-based features for se-
mantic role labeling. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Prague, Czech Republic, pages 590?599.
Marcus, Mitchell P., Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a large
annotated corpus of english: The penn treebank.
Computational Linguistics 19(2):313?330.
M`arquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic Role Labeling: An Introduction to
the Special Issue. Computational Linguistics
34(2):145?159.
Mazzei, Alessandro, Vincenzo Lombardo, and
Patrick Sturt. 2007. Dynamic TAG and lexi-
cal dependencies. Research on Language and
Computation 5:309?332.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Sve-
toslav Marinov, and Erwin Marsi. 2007. Malt-
parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13:95?135.
Pad?o, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. The proposition bank: An anno-
tated corpus of semantic roles. Computational
Linguistics 31(1):71?106.
Pickering, Martin J., Matthew J. Traxler, and
Matthew W. Crocker. 2000. Ambiguity reso-
lution in sentence processing: Evidence against
frequency-based accounts. Journal of Memory
and Language 43(3):447?475.
Sangati, Federico and Frank Keller. 2013. In-
cremental tree substitution grammar for pars-
ing and word prediction. Transactions of
the Association for Computational Linguistics
1(May):111?124.
Sayeed, Asad and Vera Demberg. 2013. The se-
mantic augmentation of a psycholinguistically-
motivated syntactic formalism. In Proceed-
ings of the Fourth Annual Workshop on Cog-
nitive Modeling and Computational Linguistics
(CMCL). Association for Computational Lin-
guistics, Sofia, Bulgaria, pages 57?65.
Surdeanu, Mihai, Richard Johansson, Adam Mey-
ers, Llu??s M`arquez, and Joakim Nivre. 2008.
The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL-
2008).
Witten, Ian H. and Timothy C. Bell. 1991. The
zero-frequency problem: estimating the proba-
bilities of novel events in adaptive text compres-
sion. Information Theory, IEEE Transactions
on 37(4):1085?1094.
Xia, Fei, Martha Palmer, and Aravind Joshi. 2000.
A uniform method of grammar extraction and
its applications. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
311
Natural Language Processing and Very Large
Corpora. pages 53?62.
312
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670?680,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Chinese Poetry Generation with Recurrent Neural Networks
Xingxing Zhang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
x.zhang@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
We propose a model for Chinese poem
generation based on recurrent neural net-
works which we argue is ideally suited to
capturing poetic content and form. Our
generator jointly performs content selec-
tion (?what to say?) and surface realization
(?how to say?) by learning representations
of individual characters, and their com-
binations into one or more lines as well
as how these mutually reinforce and con-
strain each other. Poem lines are gener-
ated incrementally by taking into account
the entire history of what has been gen-
erated so far rather than the limited hori-
zon imposed by the previous line or lexical
n-grams. Experimental results show that
our model outperforms competitive Chi-
nese poetry generation systems using both
automatic and manual evaluation methods.
1 Introduction
Classical poems are a significant part of China?s
cultural heritage. Their popularity manifests itself
in many aspects of everyday life, e.g., as a means
of expressing personal emotion, political views,
or communicating messages at festive occasions
as well as funerals. Amongst the many differ-
ent types of classical Chinese poetry, quatrain and
regulated verse are perhaps the best-known ones.
Both types of poem must meet a set of structural,
phonological, and semantic requirements, render-
ing their composition a formidable task left to the
very best scholars.
An example of a quatrain is shown in Table 1.
Quatrains have four lines, each five or seven char-
acters long. Characters in turn follow specific
phonological patterns, within each line and across
lines. For instance, the final characters in the sec-
ond, fourth and (optionally) first line must rhyme,
??
Missing You
?????? (* Z P P Z)
Red berries born in the warm southland.
?????? (P P Z Z P)
How many branches flush in the spring?
? ????? (* P P Z Z)
Take home an armful, for my sake,
?????? (* Z Z P P)
As a symbol of our love.
Table 1: An example of a 5-char quatrain ex-
hibiting one of the most popular tonal patterns.
The tone of each character is shown at the end of
each line (within parentheses); P and Z are short-
hands for Ping and Ze tones, respectively; * indi-
cates that the tone is not fixed and can be either.
Rhyming characters are shown in boldface.
whereas there are no rhyming constraints for the
third line. Moreover, poems must follow a pre-
scribed tonal pattern. In traditional Chinese, ev-
ery character has one tone, Ping (level tone) or Ze
(downward tone). The poem in Table 1 exempli-
fies one of the most popular tonal patterns (Wang,
2002). Besides adhering to the above formal crite-
ria, poems must exhibit concise and accurate use
of language, engage the reader/hearer, stimulate
their imagination, and bring out their feelings.
In this paper we are concerned with generat-
ing traditional Chinese poems automatically. Al-
though computers are no substitute for poetic cre-
ativity, they can analyze very large online text
repositories of poems, extract statistical patterns,
maintain them in memory and use them to gen-
erate many possible variants. Furthermore, while
amateur poets may struggle to remember and ap-
ply formal tonal and structural constraints, it is rel-
atively straightforward for the machine to check
670
whether a candidate poem conforms to these re-
quirements. Poetry generation has received a fair
amount of attention over the past years (see the
discussion in Section 2), with dozens of computa-
tional systems written to produce poems of vary-
ing sophistication. Beyond the long-term goal of
building an autonomous intelligent system capa-
ble of creating meaningful poems, there are po-
tential short-term applications for computer gen-
erated poetry in the ever growing industry of elec-
tronic entertainment and interactive fiction as well
as in education. An assistive environment for
poem composition could allow teachers and stu-
dents to create poems subject to their require-
ments, and enhance their writing experience.
We propose a model for Chinese poem genera-
tion based on recurrent neural networks. Our gen-
erator jointly performs content selection (?what
to say?) and surface realization (?how to say?).
Given a large collection of poems, we learn repre-
sentations of individual characters, and their com-
binations into one or more lines as well as how
these mutually reinforce and constrain each other.
Our model generates lines in a poem probabilis-
tically: it estimates the probability of the current
line given the probability of all previously gener-
ated lines. We use a recurrent neural network to
learn the representations of the lines generated so
far which in turn serve as input to a recurrent lan-
guage model (Mikolov et al., 2010; Mikolov et al.,
2011b; Mikolov et al., 2011a) which generates the
current line. In contrast to previous approaches
(Greene et al., 2010; Jiang and Zhou, 2008), our
generator makes no Markov assumptions about the
dependencies of the words within a line and across
lines.
We evaluate our approach on the task of qua-
train generation (see Table 1 for a human-written
example). Experimental results show that our
model outperforms competitive Chinese poetry
generation systems using both automatic and man-
ual evaluation methods.
2 Related Work
Automated poetry generation has been a popular
research topic over the past decades (see Colton
et al. (2012) and the references therein). Most ap-
proaches employ templates to construct poems ac-
cording to a set of constraints (e.g., rhyme, me-
ter, stress, word frequency) in combination with
corpus-based and lexicographic resources. For
example, the Haiku poem generator presented in
Wu et al. (2009) and Tosa et al. (2008) produces
poems by expanding user queries with rules ex-
tracted from a corpus and additional lexical re-
sources. Netzer et al. (2009) generate Haiku
with Word Association Norms, Agirrezabal et
al. (2013) compose Basque poems using patterns
based on parts of speech and WordNet (Fellbaum,
1998), and Oliveira (2012) presents a generation
algorithm for Portuguese which leverages seman-
tic and grammar templates.
A second line of research uses genetic algo-
rithms for poem generation (Manurung, 2003;
Manurung et al., 2012; Zhou et al., 2010). Ma-
nurung et al. (2012) argue that at a basic level
all (machine-generated) poems must satisfy the
constraints of grammaticality (i.e., a poem must
syntactically well-formed), meaningfulness (i.e., a
poem must convey a message that is meaningful
under some interpretation) and poeticness (i.e., a
poem must exhibit features that distinguishes it
from non-poetic text, e.g., metre). Their model
generates several candidate poems and then uses
stochastic search to find those which are grammat-
ical, meaningful, and poetic.
A third line of research draws inspiration from
statistical machine translation (SMT) and re-
lated text-generation applications such as sum-
marization. Greene et al. (2010) infer meters
(stressed/unstressed syllable sequences) from a
corpus of poetic texts which they subsequently
use for generation together with a cascade of
weighted finite-state transducers interpolated with
IBM Model 1. Jiang and Zhou (2008) generate
Chinese couplets (two line poems) using a phrase-
based SMT approach which translates the first line
to the second line. He et al. (2012) extend this al-
gorithm to generate four-line quatrains by sequen-
tially translating the current line from the previous
one. Yan et al. (2013) generate Chinese quatrains
based on a query-focused summarization frame-
work. Their system takes a few keywords as input
and retrieves the most relevant poems from a cor-
pus collection. The retrieved poems are segmented
into their constituent terms which are then grouped
into clusters. Poems are generated by iteratively
selecting terms from clusters subject to phonolog-
ical, structural, and coherence constraints.
Our approach departs from previous work in
two important respects. Firstly, we model the tasks
of surface realization and content selection jointly
671
?(spring)
??(lute)
?(drunk)
Keywords
ShiXueHanYing
spring
lute drunk
? ? ? ? ?
? ? ? ? ?
...
Candidate lines
Line 1Line 2Line 3Line 4
First line
generation
Next line
generation
Figure 1: Poem generation with keywords spring, lute, and drunk. The keywords are expanded into
phrases using a poetic taxonomy. Phrases are then used to generate the first line. Following lines are
generated by taking into account the representations of all previously generated lines.
using recurrent neural networks. Structural, se-
mantic, and coherence constraints are captured
naturally in our framework, through learning the
representations of individual characters and their
combinations. Secondly, generation proceeds by
taking into account multi-sentential context rather
than the immediately preceding sentence. Our
work joins others in using continuous representa-
tions to express the meaning of words and phrases
(Socher et al., 2012; Mikolov et al., 2013) and
how these may be combined in a language mod-
eling context (Mikolov and Zweig, 2012). More
recently, continuous translation models based on
recurrent neural networks have been proposed as
a means to map a sentence from the source lan-
guage to sentences in the target language (Auli
et al., 2013; Kalchbrenner and Blunsom, 2013).
These models are evaluated on the task of rescor-
ing n-best lists of translations. We use neural net-
works more directly to perform the actual poem
generation task.
3 The Poem Generator
As common in previous work (Yan et al., 2013;
He et al., 2012) we assume that our generator op-
erates in an interactive context. Specifically, the
user supplies keywords (e.g., spring, lute, drunk )
highlighting the main concepts around which the
poem will revolve. As illustrated in Figure 1, our
generator expands these keywords into a set of re-
lated phrases. We assume the keywords are re-
stricted to those attested in the ShiXueHanYing po-
etic phrase taxonomy (He et al., 2012; Yan et al.,
2013). The latter contains 1,016 manual clusters
of phrases (Liu, 1735); each cluster is labeled with
a keyword id describing general poem-worthy top-
ics. The generator creates the first line of the poem
based on these keywords. Subsequent lines are
generated based on all previously generated lines,
subject to phonological (e.g., admissible tonal pat-
terns) and structural constraints (e.g., whether the
quatrain is five or seven characters long).
To create the first line, we select all phrases
corresponding to the user?s keywords and gener-
ate all possible combinations satisfying the tonal
pattern constraints. We use a language model to
rank the generated candidates and select the best-
ranked one as the first line in the poem. In im-
plementation, we employ a character-based recur-
rent neural network language model (Mikolov et
al., 2010) interpolated with a Kneser-Ney trigram
and find the n-best candidates with a stack de-
coder (see Section 3.5 for details). We then gen-
erate the second line based on the first one, the
third line based on the first two lines, and so on.
Our generation model computes the probability
of line S
i+1
= w
1
,w
2
, . . . ,w
m
, given all previously
generated lines S
1:i
(i? 1) as:
P(S
i+1
|S
1:i
) =
m?1
?
j=1
P(w
j+1
|w
1: j
,S
1:i
) (1)
Equation (1), decomposes P(S
i+1
|S
1:i
) as the prod-
uct of the probability of each character w
j
in
the current line given all previously generated
characters w
1: j?1
and lines S
1:i
. This means
that P(S
i+1
|S
1:i
) is sensitive to previously gener-
ated content and currently generated characters.
The estimation of the term P(w
j+1
|w
1: j
,S
1:i
)
lies at the heart of our model. We learn repre-
sentations for S
1:i
, the context generated so far,
using a recurrent neural network whose output
672
serves as input to a second recurrent neural net-
work used to estimate P(w
j+1
|w
1: j
,S
1:i
). Figure 2
illustrates the generation process for the ( j+ 1)th
character w
j+1
in the (i + 1)th line S
i+1
. First,
lines S
1:i
are converted into vectors v
1:i
with a
convolutional sentence model (CSM; described in
Section 3.1). Next, a recurrent context model
(RCM; see Section 3.2) takes v
1:i
as input and
outputs u
j
i
, the representation needed for gener-
ating w
j+1
? S
i+1
. Finally, u
1
i
,u
2
i
, . . . ,u
j
i
and the
first j characters w
1: j
in line S
i+1
serve as input to
a recurrent generation model (RGM) which esti-
mates P(w
j+1
= k|w
1: j
,S
1:i
) with k ?V , the prob-
ability distribution of the ( j + 1)th character over
all words in the vocabulary V . More formally, to
estimate P(w
j+1
|w
1: j
,S
1:i
) in Equation (1), we ap-
ply the following procedure:
v
i
= CSM(S
i
) (2a)
u
j
i
= RCM(v
1:i
, j) (2b)
P(w
j+1
|w
1: j
,S
1:i
) = RGM(w
1: j+1
,u
1: j
i
) (2c)
We obtain the probability of the (i + 1)th sen-
tence P(S
i+1
|S
1:i
), by running the RGM in (2c)
above m? 1 times (see also Equation (1)). In the
following, we describe how the different compo-
nents of our model are obtained.
3.1 Convolutional Sentence Model (CSM)
The CSM converts a poem line into a vector. In
principle, any model that produces vector-based
representations of phrases or sentences could be
used (Mitchell and Lapata, 2010; Socher et al.,
2012). We opted for the convolutional sentence
model proposed in Kalchbrenner and Blunsom
(2013) as it is n-gram based and does not make
use of any parsing, POS-tagging or segmentation
tools which are not available for Chinese poems.
Their model computes a continuous representation
for a sentence by sequentially merging neighbor-
ing vectors (see Figure 3).
Let V denote the character vocabulary in our
corpus; L ? R
q?|V |
denotes a character embed-
ding matrix whose columns correspond to char-
acter vectors (q represents the hidden unit size).
Such vectors can be initialized randomly or ob-
tained via a training procedure (Mikolov et al.,
2013). Let w denote a character with index k;
e(w) ?R
|V |?1
is a vector with zero in all positions
except e(w)
k
= 1; T
l
? R
q?N
l
is the sentence rep-
resentation in the lth layer, where N
l
is the num-
ber of columns in the lth layer (N
l
= 1 in the
v
i
u
j
i
h
i
h
i?1
u
k
i
(k 6= j)
RCM
1-of-N encoding of
w
j
=(0,. . . ,1,. . . ,0)
r
j
r
j?1
P(w
j+1
|w
1: j
,S
1:i
)
RGM
Figure 2: Generation of the ( j + 1)th charac-
ter w
j+1
in the (i + 1)th line S
i+1
. The recur-
rent context model (RCM) takes i lines as in-
put (represented by vectors v
1
, . . . ,v
i
) and cre-
ates context vectors for the recurrent generation
model (RGM). The RGM estimates the probabil-
ity P(w
j+1
|w
1: j
,S
1:i
).
top layer); C
l,n
? R
q?n
is an array of weight ma-
trices which compress neighboring n columns in
the lth layer to one column in the (l + 1)th layer.
Given a sentence S = w
1
,w
2
, . . . ,w
m
, the first layer
is represented as:
T
1
= [L ? e(w
1
),L ? e(w
2
), . . . ,L ? e(w
m
)]
N
1
= m
(3)
The (l +1)th layer is then computed as follows:
T
l+1
:, j
= ?(
n
?
i=1
T
l
:, j+i?1
C
l,n
:,i
)
N
l+1
= N
l
?n+1
1? j ? N
l+1
(4)
where T
l
is the representation of the previous
layer l, C
l,n
a weight matrix,  element-wise vec-
tor product, and ? a non-linear function. We com-
press two neighboring vectors in the first two lay-
ers and three neighboring vectors in the remaining
layers. Specifically, for quatrains with seven char-
acters, we use C
1,2
, C
2,2
, C
3,3
, C
4,3
to merge vec-
tors in each layer (see Figure 3); and for quatrains
with five characters we use C
1,2
, C
2,2
, C
3,3
.
673
? ? ? ? ? ? ?
Far off I watch the waterfall plunge to the
long river.
C
1,2
C
2,2
C
3,3
C
4,3
Figure 3: Convolutional sentence model for 7-char
quatrain. The first layer has seven vectors, one
for each character. Two neighboring vectors are
merged to one vector in the second layer with
weight matrix C
1,2
. In other layers, either two or
three neighboring vectors are merged.
3.2 Recurrent Context Model (RCM)
The RCM takes as input the vectors representing
the i lines generated so far and reduces them to a
single context vector which is then used to gener-
ate the next character (see Figure 2). We compress
the i previous lines to one vector (the hidden layer)
and then decode the compressed vector to different
character positions in the current line. The output
layer consists thus of several vectors (one for each
position) connected together. This way, different
aspects of the context modulate the generation of
different characters.
Let v
1
, . . . ,v
i
(v
i
?R
q?1
) denote the vectors of
the previous i lines; h
i
? R
q?1
is their compressed
representation (hidden layer) which is obtained
with matrix M ? R
q?2q
; matrix U
j
decodes h
i
to
u
j
i
? R
q?1
in the (i+ 1)th line. The computation
of the RCM proceeds as follows:
h
0
= 0
h
i
= ?(M ?
[
v
i
h
i?1
]
)
u
j
i
= ?(U
j
?h
i
) 1? j ? m?1
(5)
where ? is a non-linear function such as sigmoid
and m the line length. Advantageously, lines in
classical Chinese poems have a fixed length of five
or seven characters. Therefore, the output layer of
the recurrent context model only needs two weight
matrices (one for each length) and the number of
parameters still remains tractable.
3.3 Recurrent Generation Model (RGM)
As shown in Figure 2, the RGM estimates the
probability distribution of the next character (over
the entire vocabulary) by taking into account the
context vector provided by the RCM and the
1-of-N encoding of the previous character. The
RGM is essentially a recurrent neural network lan-
guage model (Mikolov et al., 2010) with an aux-
iliary input layer, i.e., the context vector from
the RCM. Similar strategies for encoding addi-
tional information have been adopted in related
language modeling and machine translation work
(Mikolov and Zweig, 2012; Kalchbrenner and
Blunsom, 2013; Auli et al., 2013).
Let S
i+1
= w
1
,w
2
, . . . ,w
m
denote the line
to be generated. The RGM must esti-
mate P(w
j+1
|w
1: j
,S
1:i
), however, since the first
i lines have been encoded in the context vector u
j
i
,
we compute P(w
j+1
|w
1: j
,u
j
i
) instead. Therefore,
the probability P(S
i+1
|S
1:i
) becomes:
P(S
i+1
|S
1:i
) =
m?1
?
j=1
P(w
j+1
|w
1: j
,u
j
i
)
(6)
Let |V | denote the size of the character vocabu-
lary. The RGM is specified by a number of ma-
trices. Matrix H ? R
q?q
(where q represents the
hidden unit size) transforms the context vector to
a hidden representation; matrix X ? R
q?|V |
trans-
forms a character to a hidden representation, ma-
trix R ? R
q?q
implements the recurrent transfor-
mation and matrix Y ? R
|V |?q
decodes the hidden
representation to weights for all words in the vo-
cabulary. Let w denote a character with index k
in V ; e(w) ? R
|V |?1
represents a vector with zero
in all positions except e(w)
k
= 1, r
j
is the hidden
layer of the RGM at step j, and y
j+1
the output of
the RGM, again at step j. The RGM proceeds as
follows:
r
0
= 0 (7a)
r
j
= ?(R ? r
j?1
+X ? e(w
j
)+H ?u
j
i
) (7b)
y
j+1
= Y ? r
j
(7c)
where ? is a nonlinear function (e.g., sigmoid).
674
The probability of the ( j+1)th word given the
previous j words and the previous i lines is esti-
mated by a softmax function:
P(w
j+1
= k|w
1: j
,u
j
i
) =
exp(y
j+1,k
)
?
|V |
k=1
exp(y
j+1,k
)
(8)
We obtain P(S
i+1
|S
1:i
) by multiplying all the terms
in the right hand-side of Equation (6).
3.4 Training
The objective for training is the cross entropy er-
rors of the predicted character distribution and the
actual character distribution in our corpus. An
l
2
regularization term is also added to the objec-
tive. The model is trained with back propagation
through time (Rumelhart et al., 1988) with sen-
tence length being the time step. The objective
is minimized by stochastic gradient descent. Dur-
ing training, the cross entropy error in the output
layer of the RGM is back-propagated to its hid-
den and input layers, then to the RCM and finally
to the CSM. The same number of hidden units
(q = 200) is used throughout (i.e., in the RGM,
RCM, and CSM). In our experiments all param-
eters were initialized randomly, with the excep-
tion of the word embedding matrix in the CSM
which was initialized with word2vec embeddings
(Mikolov et al., 2013) obtained from our poem
corpus (see Section 4 for details on the data we
used).
To speed up training, we employed word-
classing (Mikolov et al., 2011b). To compute the
probability of a character, we estimate the proba-
bility of its class and then multiply it by the proba-
bility of the character conditioned on the class. In
our experiments we used 82 (square root of |V |)
classes which we obtained by applying hierarchi-
cal clustering on character embeddings. This strat-
egy outperformed better known frequency-based
classing methods (Zweig and Makarychev, 2013)
on our task.
Our poem generator models content selection
and lexical choice and their interaction, but does
not have a strong notion of local coherence,
as manifested in poetically felicitous line-to-line
transitions. In contrast, machine translation mod-
els (Jiang and Zhou, 2008) have been particu-
larly successful at generating adjacent lines (cou-
plets). To enhance coherence, we thus interpolate
our model with two machine translation features
(i.e., inverted phrase translation model feature and
inverted lexical weight feature). Also note, that
in our model surface generation depends on the
last observed character and the state of the hidden
layer before this observation. This way, there is no
explicitly defined context, and history is captured
implicitly by the recurrent nature of the model.
This can be problematic for our texts which must
obey certain stylistic conventions and sound po-
etic. In default of a better way of incorporating
poeticness into our model, we further interpolate it
with a language model feature (i.e., a Kneser-Ney
trigram model).
Throughout our experiments, we use the
RNNLM toolkit to train the character-based recur-
rent neural network language model (Mikolov et
al., 2010). Kneser-Ney n-grams were trained with
KenLM (Heafield, 2011).
3.5 Decoding
Our decoder is a stack decoder similar to Koehn
et al. (2003). In addition, it implements the tonal
pattern and rhyming constraints necessary for gen-
erating well-formed Chinese quatrains. Once the
first line in a poem is generated, its tonal pattern
is determined. During decoding, phrases violat-
ing this pattern are ignored. As discussed in Sec-
tion 1, the final characters of the second and the
fourth lines must rhyme. We thus remove during
decoding fourth lines whose final characters do not
rhyme with the second line. Finally, we use MERT
training (Och, 2003) to learn feature weights for
the decoder.
4 Experimental Design
Data We created a corpus of classical Chinese
poems by collating several online resources: Tang
Poems, Song Poems, Song Ci, Ming Poems, Qing
Poems, and Tai Poems. The corpus consists
of 284,899 poems in total. 78,859 of these are
quatrains and were used for training and evalu-
ating our model.
1
Table 2 shows the different
partitions of this dataset (POEMLM) into train-
ing (QTRAIN)
2
, validation (QVALID) and testing
(QTEST). Half of the poems in QVALID and
QTEST are 5-char quatrains and the other half
are 7-char quatrains. All poems except QVALID
1
The data used in our experiments can be downloaded
from http://homepages.inf.ed.ac.uk/mlap/index.
php?page=resources.
2
Singleton characters in QTRAIN (6,773 in total) were re-
placed by <R> to reduce data sparsity.
675
Poems Lines Characters
QTRAIN 74,809 299,236 2,004,460
QVALID 2,000 8,000 48,000
QTEST 2,050 8,200 49,200
POEMLM 280,849 2,711,034 15,624,283
Table 2: Dataset partitions of our poem corpus.
and QTEST were used for training the character-
based language models (see row POEMLM in Ta-
ble 2). We also trained word2vec embeddings on
POEMLM. In our experiments, we generated qua-
trains following the eight most popular tonal pat-
terns according to Wang (2002).
Perplexity Evaluation Evaluation of machine-
generated poetry is a notoriously difficult task.
Our evaluation studies were designed to assess
Manurung et al.?s (2012) criteria of grammatical-
ity, meaningfulness, and poeticness. As a san-
ity check, we first measured the perplexity of our
model with respect to the goldstandard. Intu-
itively, a better model should assign larger proba-
bility (and therefore lower perplexity) to goldstan-
dard poems.
BLEU-based Evaluation We also used BLEU
to evaluate our model?s ability to generate the sec-
ond, third and fourth line given previous goldstan-
dard lines. A problematic aspect of this evalu-
ation is the need for human-authored references
(for a partially generated poem) which we do not
have. We obtain references automatically follow-
ing the method proposed in He et al. (2012). The
main idea is that if two lines share a similar topic,
the lines following them can be each other?s ref-
erences. Let A and B denote two adjacent lines
in a poem, with B following A. Similarly, let line
B
?
follow line A
?
in another poem. If lines A and
A
?
share some keywords in the same cluster in the
Shixuehanying taxonomy, then B and B
?
can be
used as references for both A and A
?
. We use this
algorithm on the Tang Poems section of our corpus
to build references for poems in the QVALID and
QTEST data sets. Poems in QVALID (with auto-
generated references) were used for MERT train-
ing and Poems in QTEST (with auto-generated ref-
erences) were used for BLEU evaluation.
Human Evaluation Finally, we also evaluated
the generated poems by eliciting human judg-
Models Perplexity
KN5 172
RNNLM 145
RNNPG 93
Table 3: Perplexities for different models.
ments. Specifically, we invited 30 experts
3
on
Chinese poetry to assess the output of our gen-
erator (and comparison systems). These experts
were asked to rate the poems using a 1?5 scale on
four dimensions: fluency (is the poem grammati-
cal and syntactically well-formed?), coherence (is
the poem thematically and logically structured?),
meaningfulness (does the poem convey a mean-
ingful message to the reader?) and poeticness
(does the text display the features of a poem?).
We also asked our participants to evaluate system
outputs by ranking the generated poems relative to
each other as a way of determining overall poem
quality (Callison-Burch et al., 2012).
Participants rated the output of our model and
three comparison systems. These included He et
al.?s (2012) SMT-based model (SMT), Yan et al.?s
(2013) summarization-based system (SUM), and
a random baseline which creates poems by ran-
domly selecting phrases from the Shixuehanying
taxonomy given some keywords as input. We
also included human written poems whose content
matched the input keywords. All systems were
provided with the same keywords (i.e., the same
cluster names in the ShiXueHanYing taxonomy).
In order to compare all models on equal footing,
we randomly sampled 30 sets of keywords (with
three keywords in each set) and generated 30 qua-
trains for each system according to two lengths,
namely 5-char and 7-char. Overall, we obtained
ratings for 300 (5?30?2) poems.
5 Results
The results of our perplexity evaluation are sum-
marized in Table 3. We compare our RNN-based
poem generator (RNNPG) against Mikolov?s
(2010) recurrent neural network language model
(RNNLM) and a 5-gram language model with
Kneser-Ney smoothing (KN5). All models were
trained on QTRAIN and tuned on QVALID. The
perplexities were computed on QTEST. Note that
3
27 participants were professional or amateur poets and
three were Chinese literature students who had taken at least
one class on Chinese poetry composition.
676
Models
1? 2 2? 3 3? 4 Average
5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char
SMT 0.0559 0.0906 0.0410 0.1837 0.0547 0.1804 0.0505 0.1516
RNNPG 0.0561 0.1868 0.0515 0.2102 0.0572 0.1800 0.0549 0.1923
Table 4: BLEU-2 scores on 5-char and 7-char quatrains. Given i goldstandard lines, BLEU-2 scores are
computed for the next (i+1)th lines.
Models
Fluency Coherence Meaning Poeticness Rank
5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char
Random 2.52 2.18 2.22 2.16 2.02 1.93 1.77 1.71 0.31 0.26
SUM 1.97 1.91 2.08 2.33 1.84 1.98 1.66 1.73 0.25 0.22
SMT 2.81 3.01 2.47 2.76 2.33 2.73 2.08 2.36 0.43 0.53
RNNPG 4.01
**
3.44
*
3.18
**
3.12
*
3.20
**
3.02 2.80
**
2.68
*
0.73
**
0.64
*
Human 4.31
+
4.19
++
3.81
++
4.00
++
3.61
+
3.91
++
3.29
++
3.49
++
0.79 0.84
++
Table 5: Mean ratings elicited by humans on 5-char and 7-char quatrains. Diacritics
**
(p < 0.01)
and
*
(p < 0.05) indicate our model (RNNPG) is significantly better than all other systems except Human.
Diacritics
++
(p < 0.01) and
+
(p < 0.05) indicate Human is significantly better than all other systems.
the RNNPG estimates the probability of a poem
line given at least one previous line. Therefore, the
probability of a quatrain assigned by the RNNPG
is the probability of the last three lines. For a fair
comparison, RNNLM and KN5 only leverage the
last three lines of each poem during training, vali-
dation and testing. The results in Table 3 indicate
that the generation ability of the RNNPG is better
than KN5 and RNNLM. Note that this perplexity-
style evaluation is not possible for models which
cannot produce probabilities for gold standard po-
ems. For this reason, other related poem gener-
ators (Yan et al., 2013; He et al., 2012) are not
included in the table.
The results of our evaluation using BLEU-2 are
summarized in Table 4. Here, we compare our
system against the SMT-based poem generation
model of He et al. (2012).
4
Their system is a
linear combination of two translation models (one
with five features and another one with six). Our
model uses three of their features, namely the in-
verted phrase translation model feature, the lexical
weight feature, and a Kneser-Ney trigram feature.
Unfortunately, it is not possible to evaluate Yan
et al.?s (2013) summarization-based system with
BLEU, as it creates poems as a whole and there is
no obvious way to generate next lines with their
4
Our re-implementation of their system delivered very
similar scores to He et al. (2012). For example, we ob-
tained an average BLEU-1 of 0.167 for 5-char quatrains and
0.428 for 7-char quatrains compared to their reported scores
of 0.141 and 0.380, respectively.
algorithm. The BLEU scores in Table 4 indicate
that, given the same context lines, the RNNPG is
better than SMT at generating what to say next.
BLEU scores should be, however, viewed with
some degree of caution. Aside from being an ap-
proximation of human judgment (Callison-Burch
et al., 2012), BLEU might be unnecessarily con-
servative for poem composition which by its very
nature is a creative endeavor.
The results of our human evaluation study are
shown in Table 5. Each column reports mean rat-
ings for a different dimension (e.g., fluency, co-
herence). Ratings for 5-char and 7-char quatrains
are shown separately. The last column reports
rank scores for each system (Callison-Burch et al.,
2012). In a ranked list of N items (N = 5 here), the
score of the ith ranked item is
(N?i)
(N?1)
. The numer-
ator indicates how many times a systems won in
pairwise comparisons, while the denominator nor-
malizes the score.
With respect to 5-char quatrains, RNNPG is
significantly better than Random, SUM and SMT
on fluency, coherence, meaningfulness, poeticness
and ranking scores (using a t-test). On all dimen-
sions, human-authored poems are rated as signif-
icantly better than machine-generated ones, with
the exception of overall ranking. Here, the dif-
ference between RNNPG and Human is not sig-
nificant. We obtain similar results with 7-char
quatrains. In general, RNNPG seems to perform
better on the shorter poems. The mean ratings
677
?????, ???????,
Egrets stood, peeping fishes. Budding branches are full of romance.
?????. ???????.
Water was still, reflecting mountains. Plum blossoms are invisible but adorable.
?????, ???????,
The wind went down by nightfall, With the east wind comes Spring.
?????. ???????.
as the moon came up by the tower. Where on earth do I come from?
Table 6: Example output produced by our model (RNNPG).
are higher and the improvements over other sys-
tems are larger. Also notice, that the score mar-
gins between the human- and machine-written po-
ems become larger for 7-char quatrains. This in-
dicates that the composition of 7-char quatrains is
more difficult compared to 5-char quatrains. Ta-
ble 6 shows two example poems (5-char and 7-
char) produced by our model which received high
scores with respect to poeticness.
Interestingly, poems generated by SUM
5
are
given ratings similar to Random. In fact SUM
is slightly worse (although not significantly) than
Random on all dimensions, with the exception of
coherence. In the human study reported in Yan et
al. (2013), SUM is slightly better than SMT. There
are several reasons for this discrepancy. We used
a more balanced experimental design: all systems
generated poems from the same keywords which
were randomly chosen. We used a larger dataset
to train the SMT model compared to Yan et al.
(284,899 poems vs 61,960). The Random baseline
is not a straw-man; it selects phrases from a taxon-
omy of meaningful clusters edited by humans and
closely related to the input keywords.
6 Conclusions
In this paper we have presented a model for Chi-
nese poem generation based on recurrent neural
networks. Our model jointly performs content se-
lection and surface realization by learning repre-
sentations of individual characters and their com-
binations within and across poem lines. Previous
work on poetry generation has mostly leveraged
contextual information of limited length (e.g., one
sentence). In contrast, we introduced two recur-
rent neural networks (the recurrent context model
and recurrent generation model) which naturally
5
We made a good-faith effort to re-implement their poem
generation system. We are grateful to Rui Yan for his help
and technical advice.
capture multi-sentential content. Experimental re-
sults show that our model yields high quality po-
ems compared to the state of the art. Perhaps un-
surprisingly, our human evaluation study revealed
that machine-generated poems lag behind human-
generated ones. It is worth bearing in mind that
poetry composition is a formidable task for hu-
mans, let alone machines. And that the poems
against which our output was compared have been
written by some of the most famous poets in Chi-
nese history!
Avenues for future work are many and varied.
We would like to generate poems across differ-
ent languages and genres (e.g., Engish sonnets or
Japanese haiku). We would also like to make the
model more sensitive to line-to-line transitions and
stylistic conventions by changing its training ob-
jective to a combination of cross-entropy error and
BLEU score. Finally, we hope that some of the
work described here might be of relevance to other
generation tasks such as summarization, concept-
to-text generation, and machine translation.
Acknowledgments
We would like to thank Eva Halser for valuable
discussions on the machine translation baseline.
We are grateful to the 30 Chinese poetry experts
for participating in our rating study. Thanks to
Gujing Lu, Chu Liu, and Yibo Wang for their help
with translating the poems in Table 6 and Table 1.
References
Manex Agirrezabal, Bertol Arrieta, Aitzol Astigarraga,
and Mans Hulden. 2013. POS-Tag Based Po-
etry Generation with WordNet. In Proceedings of
the 14th European Workshop on Natural Language
Generation, pages 162?166, Sofia, Bulgaria.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
678
Modeling with Recurrent Neural Networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full-FACE Poetry Generation. In Proceedings of the
International Conference on Computational Cre-
ativity, pages 95?102, Dublin, Ireland.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic Analysis of Rhythmic Poetry with
Applications to Generation and Translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 524?
533, Cambridge, MA.
Jing He, Ming Zhou, and Long Jiang. 2012. Gener-
ating Chinese Classical Poems with Statistical Ma-
chine Translation Models. In Proceedings of the
26th AAAI Conference on Artificial Intelligence,
pages 1650?1656, Toronto, Canada.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Long Jiang and Ming Zhou. 2008. Generating Chinese
Couplets using a Statistical MT Approach. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 377?384, Manch-
ester, UK, August.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
Washington.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54, Edmonton, Canada.
Wenwei Liu. 1735. ShiXueHanYing.
Ruli Manurung, Graeme Ritchie, and Henry Thomp-
son. 2012. Using Genetic Algorithms to Create
Meaningful Poetic Text. Journal of Experimental
Theoretical Artificial Intelligence, 24(1):43?64.
Ruli Manurung. 2003. An Evolutionary Algorithm Ap-
proach to Poetry Generation. Ph.D. thesis, Univer-
sity of Edinburgh.
Tomas Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Lan-
guage Model. In Proceedings of 2012 IEEE Work-
shop on Spoken Language Technology, pages 234?
239, Miami, Florida.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proceedings of INTERSPEECH, pages 1045?1048,
Makuhari, Japan.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011a. Strategies
for Training Large Scale Neural Network Language
Models. In Proceedings of ASRU 2011, pages 196?
201, Hilton Waikoloa Village, Big Island, Hawaii,
US.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of Recurrent Neural Network Language
Model. In Proceedings of the 2011 IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing, pages 5528?5531, Prague, Czech Re-
public.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Advances in Neural Information Process-
ing Systems, pages 3111?3119, Lake Tahoe, Nevada,
United States.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1439.
Yael Netzer, David Gabay, Yoav Goldberg, and
Michael Elhadad. 2009. Gaiku: Generating Haiku
with Word Associations Norms. In Proceedings of
the Workshop on Computational Approaches to Lin-
guistic Creativity, pages 32?39, Boulder, Colorado.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Hugo Gonc?alo Oliveira. 2012. PoeTryMe: a Versa-
tile Platform for Poetry Generation. Computational
Creativity, Concept Invention, and General Intelli-
gence, 1:21.
David Rumelhart, Geoffrey Hinton, and Ronald
Williams. 1988. Learning Representations by Back-
propagating Errors. MIT Press, Cambridge, MA,
USA.
679
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1201?1211, Jeju Island, Korea.
Naoko Tosa, Hideto Obara, and Michihiko Minoh.
2008. Hitch Haiku: An Interactive Supporting Sys-
tem for Composing Haiku Poem How I Learned to
Love the Bomb: Defcon and the Ethics of Com-
puter Games. In Proceedings of the 7th Inter-
national Conference on Entertainment Computing,
pages 209?216, Pittsburgh, PA.
Li Wang. 2002. A Summary of Rhyming Constraints
of Chinese Poems (Shi Ci Ge Lv Gai Yao). Beijing
Press, 2002.
Xiaofeng Wu, Naoko Tosa, and Ryohei Nakatsu. 2009.
New Hitch Haiku: An Interactive Renku Poem
Composition Supporting Tool Applied for Sightsee-
ing Navigation System. In Proceedings of the 8th
International Conference on Entertainment Com-
puting, pages 191?196, Paris, France.
Rui Yan, Han Jiang, Mirella Lapata, Shou-De Lin,
Xueqiang Lv, and Xiaoming Li. 2013. I, Poet:
Automatic Chinese Poetry Composition Through a
Generative Summarization Framework Under Con-
strained Optimization. In Proceedings of the 23rd
International Joint Conference on Artificial Intelli-
gence, pages 2197?2203, Beijing, China.
Cheng-Le Zhou, Wei You, and Xiaojun Ding. 2010.
Genetic Algorithm and its Implementation of Au-
tomatic Generation of Chinese SongCi. Journal of
Software, pages 427?437.
Geoffrey Zweig and Konstantin Makarychev. 2013.
Speed Regularization and Optimality in Word Class-
ing. In Proceedings of the 2014 IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 8237?8241, Florence, Italy.
680
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 249?258,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Incremental Bayesian Learning of Semantic Categories
Lea Frermann and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
l.frermann@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Models of category learning have been ex-
tensively studied in cognitive science and
primarily tested on perceptual abstractions
or artificial stimuli. In this paper we focus
on categories acquired from natural lan-
guage stimuli, that is words (e.g., chair is
a member of the FURNITURE category).
We present a Bayesian model which, un-
like previous work, learns both categories
and their features in a single process. Our
model employs particle filters, a sequential
Monte Carlo method commonly used for
approximate probabilistic inference in an
incremental setting. Comparison against
a state-of-the-art graph-based approach re-
veals that our model learns qualitatively
better categories and demonstrates cogni-
tive plausibility during learning.
1 Introduction
Considerable psychological research has shown
that people reason about novel objects they en-
counter by identifying the category to which these
objects belong and extrapolating from their past
experiences with other members of that category
(Smith and Medin, 1981). Categorization is a clas-
sic problem in cognitive science, underlying a va-
riety of common mental tasks including percep-
tion, learning, and the use of language.
Given its fundamental nature, categorization
has been extensively studied both experimentally
and in simulations. Indeed, numerous models ex-
ist as to how humans categorize objects ranging
from strict prototypes (categories are represented
by a single idealized member which embodies
their core properties; e.g., Reed 1972) to full ex-
emplar models (categories are represented by a list
of previously encountered members; e.g., Nosof-
sky 1988) and combinations of the two (e.g., Grif-
fiths et al. 2007). A common feature across dif-
ferent studies is the use of stimuli involving real-
world objects (e.g., children?s toys; Starkey 1981),
perceptual abstractions (e.g., photographs of ani-
mals; Quinn and Eimas 1996), or artificial ones
(e.g., binary strings, dot patterns or geometric
shapes; Medin and Schaffer 1978; Posner and
Keele 1968; Bomba and Siqueland 1983). Most
existing models focus on adult categorization, in
which it is assumed that a large number of cate-
gories have already been learnt (but see Anderson
1991 and Griffiths et al. 2007 for exceptions).
In this work we focus on categories acquired
from natural language stimuli (i.e., words) and
investigate how the statistics of the linguistic en-
vironment (as approximated by large corpora) in-
fluence category formation (e.g., chair and ta-
ble are FURNITURE whereas peach and apple are
FRUIT
1
). The idea of modeling categories using
words as a stand-in for their referents has been
previously used to explore categorization-related
phenomena such as semantic priming (Cree et al.,
1999) and typicality rating (Voorspoels et al.,
2008), to evaluate prototype and exemplar mod-
els (Storms et al., 2000), and to simulate early lan-
guage category acquisition (Fountain and Lapata,
2011). The idea of using naturalistic corpora has
received little attention. Most existing studies use
feature norms as a proxy for people?s representa-
tion of semantic concepts. In a typical procedure,
participants are presented with a word and asked to
generate the most relevant features or attributes for
its referent concept. The most notable collection
of feature norms is probably the multi-year project
of McRae et al. (2005), which obtained features
for a set of 541 common English nouns.
Our approach replaces feature norms with rep-
resentations derived from words? contexts in cor-
pora. While this is an impoverished view of how
categories are acquired ? it is clear that they are
learnt through exposure to the linguistic environ-
ment and the physical world ? perceptual infor-
1
Throughout this paper we will use small caps to denote
CATEGORIES and italics for their members.
249
mation relevant for extracting semantic categories
is to a large extent redundantly encoded in linguis-
tic experience (Riordan and Jones, 2011). Besides,
there are known difficulties with feature norms
such as the small number of words for which these
can be obtained, the quality of the attributes, and
variability in the way people generate them (see
Zeigenfuse and Lee 2010 for details). Focusing
on natural language categories allows us to build
categorization models with theoretically unlimited
scope.
To this end, we present a probabilistic Bayesian
model of category acquisition based on the key
idea that learners can adaptively form category
representations that capture the structure ex-
pressed in the observed data. We model category
induction as two interrelated sub-problems: (a) the
acquisition of features that discriminate among
categories, and (b) the grouping of concepts into
categories based on those features. An important
modeling question concerns the exact mechanism
with which categories are learned. To maintain
cognitive plausibility, we develop an incremental
learning algorithm. Incrementality is a central as-
pect of human learning which takes place sequen-
tially and over time. Humans are capable of deal-
ing with a situation even if only partial information
is available. They adaptively learn as new infor-
mation is presented and locally update their inter-
nal knowledge state without systematically revis-
ing everything known about the situation at hand.
Memory and processing limitations also explain
why humans must learn incrementally. It is not
possible to store and have easy access to all the in-
formation one has been exposed to. It seems likely
that people store the most prominent facts and gen-
eralizations, which they modify on they fly when
new facts become available.
Our model learns categories using a particle fil-
ter, a Markov Chain Monte Carlo (MCMC) in-
ference mechanism which sequentially integrates
newly observed data and can be thus viewed as a
plausible proxy for human learning. Experimental
results show that the incremental learner obtains
meaningful categories which outperform the state
of the art whilst at the same time acquiring seman-
tic representations of words and their features.
2 Related Work
The problem of category induction has achieved
much attention in the cognitive science literature.
Incremental category learning was pioneered by
Anderson (1991) who develops a non-parametric
model able to induce categories from abstract
stimuli represented by binary features. Sanborn
et al. (2006) present a fully Bayesian adaptation of
Anderson?s original model, which yields a better
fit with behavioral data. A separate line of work
examines the cognitive characteristics of category
acquisition as well as the processes of generalizing
and generating new categories and exemplars (Jern
and Kemp, 2013; Kemp et al., 2012). The above
models are conceptually similar to ours. How-
ever, they were developed with adult categoriza-
tion in mind, and use rather simplistic categories
representing toy-domains. It is therefore not clear
whether they generalize to arbitrary stimuli and
data sizes. We aim to show that it is possible to ac-
quire natural language categories on a larger scale
purely from linguistic context.
Our model is loosely related to Bayesian mod-
els of word sense induction (Brody and Lapata,
2009; Yao and Durme, 2011). We also assume
that local linguistic context can provide important
cues for word meaning and by extension category
membership. However, the above models focus
on performance optimization and learn in an ideal
batch mode, while incorporating various kinds of
additional features such as part of speech tags or
dependencies. In contrast, we develop a cogni-
tively plausible (early) language learning model
and show that categories can be acquired purely
from context, as well as in an incremental fashion.
From a modeling perspective, we learn cate-
gories incrementally using a particle filtering al-
gorithm (Doucet et al., 2001). Particle filters are
a family of sequential Monte Carlo algorithms
which update the state space of a probabilistic
model with newly encountered information. They
have been successfully applied to natural lan-
guage acquisition tasks such as word segmentation
(Borschinger and Johnson, 2011), or sentence pro-
cessing (Levy et al., 2009). Sanborn et al. (2006)
also use particle filters for small-scale categoriza-
tion experiments with artificial stimuli. To the best
of our knowledge, we present the first particle fil-
tering algorithm for large-scale category acquisi-
tion from natural text.
Our work is closest to Fountain and Lapata
(2011) who also develop a model for inducing nat-
ural language categories. Specifically, they pro-
pose an incremental version of Chinese Whispers
(Biemann, 2006), a randomized graph-clustering
algorithm. The latter takes as input a graph which
is constructed from corpus-based co-occurrence
statistics and produces a hard clustering over the
nodes in the graph. Contrary to our model, they
treat the tasks of inferring a semantic representa-
250
wt
w
c
z
?
?
?
?
?
?
Mult Mult
Mult
Dir Dir
Dir
n
d
k
k
Figure 1: Plate diagram representation of the
BayesCat model.
tion for concepts and their class membership as
two separate processes. This allows to experi-
ment with different ways of initializing the co-
occurrence matrix (e.g., from bags of words or
a dependency parsed corpus), however at the ex-
pense of cognitive plausibility. It is unlikely that
humans have two entirely separate mechanisms
for learning the meaning of words and their cat-
egories. We formulate a more expressive model
within a probabilistic framework which captures
the meaning of words, their similarity, and the pre-
dictive power of their linguistic contexts.
3 The BayesCat Model
In this section we present our Bayesian model of
category induction (BayesCat for short). The input
to the model is natural language text, and its final
output is a set of clusters representing categories
of semantic concepts found in the input data. Like
many other semantic models, BayesCat is inspired
by the distributional hypothesis which states that
a word?s meaning is predictable from its context
(Harris, 1954). By extension, we also assume that
contextual information can be used to character-
ize general semantic categories. Accordingly, the
input to our model is a corpus of documents, each
defined as a target word t centered in a fixed-length
context window:
[c
?n
... c
?1
t c
1
... c
n
] (1)
We assume that there exists one global distribu-
tion over categories from which all documents are
generated. Each document is assigned a category
label, based on two types of features: the docu-
ment?s target word and its context words, which
are modeled through separate category-specific
distributions. We argue that it is important to dis-
tinguish between these features, since words be-
longing to the same category do not necessarily
co-occur, but tend to occur in the same contexts.
For example, the words polar bear and anteater
Draw distribution over categories ?? Dir(?)
for category k do
Draw target word distribution ?
k
?Dir(?)
Draw context word distribution ?
k
?
Dir(?)
for Document d do
Draw category z
d
?Mult(?)
Draw target word w
d
t
?Mult(?
z
d
)
for context position n = {1..N} do
Draw context word w
d,n
c
?Mult(?
z
d
)
Figure 2: The generative process of the BayesCat
model.
are both members of the category ANIMAL. How-
ever, they rarely co-occur (in fact, a cursory search
using Google yields only three matches for the
query ?polar bear * anteater?). Nevertheless, we
would expect to observe both words in similar
contexts since both animals eat, sleep, hunt, have
fur, four legs, and so on. This distinction con-
trasts our category acquisition task from the clas-
sical task of topic inference.
Figure 1 presents a plate diagram of the
BayesCat model; an overview of the generative
process is given in Figure 2. We first draw a global
category distribution ? from the Dirichlet distribu-
tion with parameter ?. Next, for each category k,
we draw a distribution over target words ?
k
from a
Dirichlet with parameter ? and a distribution over
context words ?
k
from a Dirichlet with parame-
ter ?. For each document d, we draw a category z
d
,
then a target word, and N context words from the
category-specific distributions ?
z
d
and ?
z
d
, respec-
tively.
4 Learning
Our goal is to infer the joint distribution of
all hidden model parameters, and observable
data W . Since we use conjugate prior distributions
throughout the model, this joint distribution can be
simplified to:
P(W,Z,?,?,?;?,?,?) ?
?
k
?(N
k
+?
k
)
?(
?
k
N
k
+?
k
)
?
K
?
k=1
?
r
?(N
k
r
+?
r
)
?(
?
r
N
k
r
+?
r
)
?
K
?
k=1
?
s
?(N
k
s
+ ?
s
)
?(
?
s
N
k
s
+ ?
s
)
, (2)
where r and s iterate over the target and context
word vocabulary, respectively, and the distribu-
251
tions ?,?, and ? are integrated out and implic-
itly captured by the corresponding co-occurrence
counts N
?
?
. ?() denotes the Gamma function, a
generalization of the factorial to real numbers.
Since exact inference of the parameters of the
BayesCat model is intractable, we use sampling-
based approximate inference. Specifically, we
present two learning algorithms, namely a Gibbs
sampler and a particle filter.
The Gibbs Sampler Gibbs sampling is a well-
established approximate learning algorithm, based
on Markov Chain Monte Carlo methods (Geman
and Geman, 1984). It operates in batch-mode by
repeatedly iterating through all data points (doc-
uments in our case) and assigning the currently
sampled document d a category z
d
conditioned on
the current labelings of all other documents z
?d
:
z
d
? P(z
d
|z
?d
,W
?d
;?,?,?), (3)
using equation (2) but ignoring information
from the currently sampled document in all co-
occurrence counts.
The Gibbs sampler can be seen as an ideal
learner, which can view and revise any relevant
information at any time during learning. From a
cognitive perspective, this setting is implausible,
since a human language learner encounters train-
ing data incrementally and does not systematically
revisit previous learning decisions. Particle filters
are a class of incremental, or sequential, Monte
Carlo methods which can be used to model aspects
of the language learning process more naturally.
The Particle Filter Intuitively, a particle fil-
ter (henceforth PF) entertains a fixed set of
N weighted hypotheses (particles) based on pre-
vious training examples. Figure 3 shows an
overview of the particle filtering learning proce-
dure. At first, every particle of the PF is initialized
from a base distribution P
0
(Initialization). Then a
single iteration over the input data y is performed,
during which the posterior distribution of each
data point y
t
under all current particles is com-
puted given information from all previously en-
countered data points y
t?1
(Sampling/Prediction).
Crucially, each update is conditioned only on the
previous model state z
t?1
, which results in a con-
stant state space despite an increasing amount of
available data. A common problem with PF al-
gorithms is weight degeneration, i.e., one particle
tends to accumulate most of the weight. To avoid
this problem, at regular intervals the set of parti-
cles is resampled in order to discard particles with
for particle p do . Initialization
Initialize randomly or from z
0
p
? p
0
(z)
for observation t do
for particle n do . Sampling/Prediction
P
n
(z
t
n
|y
t
)? p(z
t
n
|z
t?1
n
,?)P(y
t
|z
t
n
,y
t?1
)
z
t
?Mult({P
n
(z
t
n
)}
N
i=1
) . Resampling
Figure 3: The particle filtering procedure.
low probability and to ensure that the sample is
representative of the state space at any time (Re-
sampling).
This general algorithm can be straightforwardly
adapted to our learning problem (Griffiths et al.,
2011; Fearnhead, 2004). Each observation corre-
sponds to a document, which needs to be assigned
a category. To begin with, we assign the first ob-
served document to category 0 in all particles (Ini-
tialization). Then, we iterate once over the remain-
ing documents. For each particle n, we compute
a probability distribution over K categories based
on the simplified posterior distribution as defined
in equation (2) (Sampling/Prediction), with co-
occurrence counts based on the information from
all previously encountered documents. Thus, we
obtain a distribution over N ?K possible assign-
ments. From this distribution we sample with
replacement N new particles, assign the current
document to the corresponding category (Resam-
pling), and proceed to the next input document.
5 Experimental Setup
The goal of our experimental evaluation is to as-
sess the quality of the inferred clusters by compar-
ison to a gold standard and an existing graph-based
model of category acquisition. In addition, we are
interested in the incremental version of the model,
whether it is able to learn meaningful categories
and how these change over time. In the following,
we give details on the corpora we used, describe
how model parameters were selected, and explain
our evaluation procedure.
5.1 Data
All our experiments were conducted on a lem-
matized version of the British National Corpus
(BNC). The corpus was further preprocessed by
removing stopwords and infrequent words (occur-
ring less than 800 times in the BNC).
The model output was evaluated against a gold
standard set of categories which was created by
collating the resources developed by Fountain and
252
Lapata (2010) and Vinson and Vigliocco (2008).
Both datasets contain a classification of nouns into
(possibly multiple) semantic categories produced
by human participants. We therefore assume that
they represent psychologically salient categories
which the cognitive system is in principle capable
of acquiring. After merging the two resources, and
removing duplicates we obtained 42 semantic cat-
egories for 555 nouns. We split this gold standard
into a development (41 categories, 492 nouns) and
a test set (16 categories, 196 nouns).
2
The input to our model consists of short chunks
of text, namely a target word centered in a sym-
metric context window of five words (see (1)).
In our experiments, the set of target words corre-
sponds to the set of nouns in the evaluation dataset.
Target word mentions and their context are ex-
tracted from the BNC.
5.2 Parameters for the BayesCat Model
We optimized the hyperparameters of the
BayesCat model on the development set.
For the particle filter, the optimal values are
? = 0.7,? = 0.1,? = 0.1. We used the same
values for the Gibbs Sampler since it proved
insensitive to hyperparameter variations. We run
the Gibbs sampler for 200 iterations
3
and report
results averaged over 10 runs. For the PF, we set
the number of particles to 500, and report final
scores averaged over 10 runs. For evaluation,
we take the clustering from the particle with the
highest weight
4
.
5.3 Model Comparison
Chinese Whispers We compared our approach
with Fountain and Lapata (2011) who present a
non-parametric graph-based model for category
acquisition. Their algorithm incrementally con-
structs a graph from co-occurrence counts of tar-
get words and their contexts (they use a symmetric
context window of five words). Target words con-
stitute the nodes of the graph, their co-occurrences
are transformed into a vector of positive PMI val-
ues, and graph edges correspond to the cosine sim-
ilarity between the PMI-vectors representing any
two nodes. They use Chinese Whispers (Biemann,
2006) to partition a graph into categories.
2
The dataset is available from www.frermann.de/data.
3
We checked for convergence on the development set.
4
While in theory particles should be averaged, we found
that eventually they became highly similar ? a common
problem known as sample impoverishment, which we plan to
tackle in the future. Nevertheless, diversity among particles
is present in the initial learning phase, when uncertainty is
greatest, so the model still benefits from multiple hypotheses.
We replicated the bag-of-words model pre-
sented in Fountain and Lapata (2011) and assessed
its performance on our training corpora and test
sets. The scores we report are averaged over 10
runs.
Chinese Whispers can only make hard cluster-
ing decisions, whereas the BayesCat model re-
turns a soft clustering of target nouns. In order
to be able to compare the two models, we con-
vert the soft clusters to hard clusters by assign-
ing each target word w to category c such that
cat(w) = max
c
P(w|c) ?P(c|w).
LDA We also compared our model to a standard
topic model, namely Latent Dirichlet Allocation
(LDA; Blei et al. 2003). LDA assumes that a docu-
ment is generated from an individual mixture over
topics, and each topic is associated with one word
distribution. We trained a batch version of LDA
using input identical to our model and the Mallet
toolkit (McCallum, 2002).
Chinese Whispers is a parameter-free algorithm
and thus determines the number of clusters auto-
matically. While the Bayesian models presented
here are parametric in that an upper bound for the
potential number of categories needs to be speci-
fied, the models themselves decide on the specific
value of this number. We set the upper bound of
categories to 100 for LDA as well as the batch and
incremental version of the BayesCat model.
5.4 Evaluation Metrics
Our aim is to learn a set of clusters each of which
corresponds to one gold category, i.e., it contains
all and only members of that gold category. We
report evaluation scores based on three metrics
which measure this tradeoff. Since in unsuper-
vised clustering the cluster IDs are meaningless,
all evaluation metrics involve a mapping from in-
duced clusters to gold categories. The first two
metrics described below perform a cluster-based
mapping and are thus not ideal for assessing the
output of soft clustering algorithms. The third
metric performs an item-based mapping and can
be directly used to evaluate soft clusters.
Purity/Collocation are based on member over-
lap between induced clusters and gold classes
(Lang and Lapata, 2011). Purity measures the de-
gree to which each cluster contains instances that
share the same gold class, while collocation mea-
sures the degree to which instances with the same
gold class are assigned to a single cluster. We re-
port the harmonic mean of purity and collocation
253
as a single measure of clustering quality.
V-Measure is the harmonic mean between
homogeneity and collocation (Rosenberg and
Hirschberg, 2007). Like purity, V-Measure
performs cluster-based comparisons but is an
entropy-based method. It measures the condi-
tional entropy of a cluster given a class, and vice
versa.
Cluster-F1 is an item-based evaluation metric
which we propose drawing inspiration from the
supervised metric presented in Agirre and Soroa
(2007). Cluster-F1 maps each target word type to
a gold cluster based on its soft class membership,
and is thus appropriate for evaluation of soft clus-
tering output. We first create a K?G soft map-
ping matrix M from each induced category k
i
to
gold classes g
j
from P(g
j
|k
i
). We then map each
target word type to a gold class by multiplying
its probability distribution over soft clusters with
the mapping matrix M , and taking the maximum
value. Finally, we compute standard precision, re-
call and F1 between the mapped system categories
and the gold classes.
6 Results
Our experiments are designed to answer three
questions: (1) How do the induced categories
fare against gold standard categories? (2) Are
there performance differences between BayesCat
and Chinese Whispers, given that the two models
adopt distinct mechanisms for representing lexical
meaning and learning semantic categories? (3) Is
our incremental learning mechanism cognitively
plausible? In other words, does the quality of the
induced clusters improve over time and how do the
learnt categories differ from the output of an ideal
batch learner?
Clustering performance for the batch BayesCat
model (BC-Batch), its incremental version
(BC-Inc), Chinese Whispers (CW), and LDA
is shown in Table 1. Comparison of the two
incremental models, namely BC-Inc and CW,
shows that our model outperforms CW under
all evaluation metrics both on the test and the
development set. Our BC models perform at
least as well as LDA, despite the more complex
learning objective. Recall that LDA does not learn
category specific features. BC-Batch performs
best overall, however this is not surprising. The
BayesCat model learnt in batch mode uses a Gibbs
sampler which can be viewed as an ideal learner
with access to the entire training data at any time,
and the ability to systematically revise previous
decisions. This puts the incremental variant at a
disadvantage since the particle filter encounters
the data incrementally and never resamples
previously seen documents. Nevertheless, as
shown in Table 1 BC-Inc?s performance is very
close to BC-Batch. BC-Inc outperforms the Gibbs
sampler in the PC-F1 metric, because it achieves
higher collocation scores. Inspection of the output
reveals that the Gibbs sampler induces larger clus-
ters compared to the particle filter (as well as less
distinct clusters). Although the general pattern of
results is the same on the development and test
sets, absolute scores for all systems are higher on
the test set. This is expected, since the test set
contains less categories with a smaller number of
exemplars and more accurate clusterings can be
thus achieved (on average) more easily.
Figure 4 displays the learning curves produced
by CW and BC-Inc under the PC-F1 (left) and
Cluster-F1 (right) evaluation metrics. Under
PC-F1, CW produces a very steep initial learning
curve which quickly flattens off, whereas no learn-
ing curve emerges for CW under Cluster-F1. The
BayesCat model exhibits more discernible learn-
ing curves under both metrics. We also observe
that learning curves for CW indicate much more
variance during learning compared to BC-Inc, ir-
respectively of the evaluation metric being used.
Figure 4b shows learning curves for BC-Inc when
its output classes are interpreted in two ways,
i.e., as soft or hard clusters. Interestingly, the two
curves have a similar shape which points to the
usefulness of Cluster-F1 as an evaluation metric
for both types of clusters.
In order to better understand the differences in
the learning process between CW and BC-Inc we
tracked the evolution of clusterings over time, as
well as the variance across cluster sizes at each
point in time. The results are plotted in Figure 5.
The top part of the figure compares the number
of clusters learnt by the two models. We see that
the number of clusters inferred by CW drops over
time, but is closer to the number of clusters present
in the gold standard. The final number of clus-
ters inferred by CW is 26, whereas PF-Inc infers
90 clusters (there are 41 gold classes). The mid-
dle plot shows the variance in cluster size induced
at any time by CW which is by orders of magni-
tude higher than the variance observed in the out-
put of BayesCat (bottom plot). More importantly,
the variance in BayesCat resembles the variance
present in the gold standard much more closely.
The clusterings learnt by CW tend to consist of
254
Development Set Test Set
Metric LDA CW BC-Inc BC-Batch LDA CW BC-Inc BC-Batch
PC-F1 (Hard) 0.283 0.211 0.283 0.261 0.446 0.380 0.503 0.413
V-Measure (Hard) 0.399 0.143 0.383 0.428 0.572 0.220 0.567 0.606
Cluster-F1 (Hard) 0.416 0.301 0.386 0.447 0.521 0.443 0.671 0.693
Cluster-F1 (Soft) 0.387 ? 0.484 0.523 0.665 ? 0.644 0.689
Table 1: Evaluation of model output against a gold standard. Results are reported for the BayesCat model
trained incrementally (BC-Inc) and in batch mode (BC-Batch), and Chinese Whispers (CW). The type
of clusters being evaluated is shown within parentheses.
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
PC
-F1
Number of encountered Documents
CW (Hard)
BC-Inc (Hard)
(a)
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Cl
ust
er-
F1
Number of encountered Documents
CW (Hard)
BC-Inc (Hard)
BC-Inc (Soft)
(b)
Figure 4: Learning curves for BC-Inc and CW based on PC-F1 (left), and Cluster-F1 (right). The type
of clusters being evaluated is shown within parentheses. Results are reported on the development set.
few very large clusters and a large number of very
small (mostly singleton) clusters. Although some
of the bigger clusters are meaningful, the overall
structure of clusterings does not faithfully repre-
sent the gold standard.
Finally, note that in contrast to CW and LDA,
the BayesCat model learns not only how to in-
duce clusters of target words, but also informa-
tion about their category-specific contexts. Table 2
presents examples of the learnt categories together
with their most likely contexts. For example, one
of the categories our model discovers corresponds
to BUILDINGS. Some of the context words or fea-
tures relating to buildings refer to their location
(e.g., city, road, hill, north, park), architectural
style (e.g., modern, period, estate), and material
(e.g., stone).
7 Discussion
In this paper we have presented a Bayesian model
of category acquisition. Our model learns to group
concepts into categories as well as their features
(i.e., context words associated with them). Cat-
egory learning is performed incrementally, using
a particle filtering algorithm which is a natural
choice for modeling sequential aspects of lan-
guage learning.
We now return to our initial questions and sum-
marize our findings. Firstly, we observe that
our incremental model learns plausible linguistic
categories when compared against the gold stan-
dard. Secondly, these categories are qualitatively
better when evaluated against Chinese Whispers,
a closely related graph-based incremental algo-
rithm. Thirdly, analysis of the model?s output
shows that it simulates category learning in two
important ways, it consistently improves over time
and can additionally acquire category features.
Overall, our model has a more cognitively plau-
sible learning mechanism compared to CW, and
is more expressive, as it can simulate both cat-
egory and feature learning. Although CW ulti-
mately yields some meaningful categories, it does
not acquire any knowledge pertaining to their fea-
tures. This is somewhat unrealistic given that hu-
mans are good at inferring missing features for
255
 0
 20
 40
 60
 80
 100
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Nu
mb
er o
f C
lus
ters
Number of encountered Documents
BC-IncCWGold
 0
 100
 200
 300
 400
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Va
rian
ce
Number of encountered Documents
CWGold
 3
 6
 9
 12
 15
 18
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Va
rian
ce
Number of encountered Documents
BC-IncGold
Figure 5: Number of clusters over time (top).
Cluster size variance for CW (middle) and BC-Inc
(bottom). Results shown on the development set.
unknown categories (Anderson, 1991). It is also
symptomatic of the nature of the algorithm which
does not have an explicit learning mechanism.
Each node in the graph iteratively adopts (in ran-
dom order) the strongest class in its neighborhood
(i.e., the set of nodes with which it shares an edge).
We also showed that LDA is less appropriate for
the category learning task on account of its for-
mulation which does not allow to simultaneously
acquire clusters and their features.
There are several options for improving our
model. The learning mechanism presented here
is the most basic of particle methods. A common
problem in particle filtering is sample impoverish-
ment, i.e., particles become highly similar after a
few iterations, and do not optimally represent the
sample space. More involved resampling methods
such as stratified sampling or residual resampling,
have been shown to alleviate this problem (Douc,
2005).
From a cognitive perspective, the most obvious
weakness of our algorithm is its strict incremen-
tality. While our model simulates human mem-
BUILDINGS
wall, bridge, building, cottage, gate, house, train,
bus, stone, chapel, brick, cathedral
plan, include, park, city, stone, building, ho-
tel, lead, road, hill, north, modern, visit, main,
period, cathedral, estate, complete, site, owner,
parish
WEAPONS
shotgun, pistol, knife, crowbar, gun, sledgeham-
mer, baton, bullet, motorcycle, van, ambulance
injure, ira, jail, yesterday, arrest, stolen, fire, of-
ficer, gun, police victim, hospital, steal, crash,
murder, incident, driver, accident, hit
INSTRUMENTS
tuba, drum, harmonica, bagpipe, harp, violin,
saxophone, rock, piano, banjo, guitar, flute, harp-
sichord, trumpet, rocker, clarinet, stereo, cello,
accordion
amp, orchestra, sound, electric, string, sing,
song, drum, piano, condition, album, instrument,
guitar, band, bass, music
Table 2: Examples of categories induced by the in-
cremental BayesCat model (upper row), together
with their most likely context words (lower row).
ory restrictions and uncertainty by learning based
on a limited number of current knowledge states
(i.e., particles), it never reconsiders past catego-
rization decisions. In many linguistic tasks, how-
ever, learners revisit past decisions (Frazier and
Rayner, 1982) and intuitively we would expect
categories to change based on novel evidence, es-
pecially in the early learning phase. In fixed-lag
smoothing, a particle smoothing variant, model
updates include systematic revision of a fixed set
of previous observations in the light of newly en-
countered evidence (Briers et al., 2010). Based
on this framework, we will investigate different
schemes for informed sequential learning.
Finally, we would like to compare the model?s
predictions against behavioral data, and exam-
ine more thoroughly how categories and features
evolve over time.
Acknowledgments We would like to thank
Charles Sutton and members of the ILCC at the
School of Informatics for their valuable feedback.
We acknowledge the support of EPSRC through
project grant EP/I037415/1.
256
References
Agirre, Eneko and Aitor Soroa. 2007. Semeval-
2007 task 02: Evaluating word sense induc-
tion and discrimination systems. In Proceedings
of the 4th International Workshop on Semantic
Evaluations. Prague, Czech Republic, pages 7?
12.
Anderson, John R. 1991. The adaptive nature of
human categorization. Psychological Review
98:409?429.
Biemann, Chris. 2006. Chinese Whispers - an effi-
cient graph clustering algorithm and its applica-
tion to natural language processing problems. In
Proceedings of TextGraphs: the 1st Workshop
on Graph Based Methods for Natural Language
Processing. New York City, pages 73?80.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet allocation. Journal
of Machine Learning Research 3:993?1022.
Bomba, Paul C. and Eimas R. Siqueland. 1983.
The nature and structure of infant form cate-
gories. Journal of Experimental Child Psychol-
ogy 35:294?328.
Borschinger, Benjamin and Mark Johnson. 2011.
A particle filter algorithm for Bayesian word
segmentation. In Proceedings of the Aus-
tralasian Language Technology Association
Workshop. Canberra, Australia, pages 10?18.
Briers, Mark, Arnaud Doucet, and Simon Maskell.
2010. Smoothing algorithms for state-space
models. Annals of the Institute of Statistical
Mathematics 62(1):61?89.
Brody, Samuel and Mirella Lapata. 2009.
Bayesian word sense induction. In Proceedings
of the 12th Conference of the European Chapter
of the ACL. Athens, Greece, pages 103?111.
Cree, George S., Ken McRae, and Chris McNor-
gan. 1999. An attractor model of lexical con-
ceptual processing: Simulating semantic prim-
ing. Cognitive Science 23(3):371?414.
Douc, Randal. 2005. Comparison of resampling
schemes for particle filtering. In 4th Interna-
tional Symposium on Image and Signal Pro-
cessing and Analysis. Zagreb, Croatia, pages
64?69.
Doucet, Arnaud, Nando de Freitas, and Neil Gor-
don. 2001. Sequential Monte Carlo Methods in
Practice. Springer, New York.
Fearnhead, Paul. 2004. Particle filters for mix-
ture models with an unknown number of com-
ponents. Statistics and Computing 14(1):11?21.
Fountain, Trevor and Mirella Lapata. 2010. Mean-
ing representation in natural language catego-
rization. In Proceedings of the 32nd Annual
Conference of the Cognitive Science Society.
Portland, Oregon, pages 1916?1921.
Fountain, Trevor and Mirella Lapata. 2011. In-
cremental models of natural language category
acquisition. In Proceedings of the 33nd An-
nual Conference of the Cognitive Science Soci-
ety. Boston, Massachusetts, pages 255?260.
Frazier, Lyn and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology 14(2):178?210.
Geman, Stuart and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions and the
Bayesian restoration of images. IEEE Trans-
actions on Pattern Analysis and Machine Intel-
ligence 6(6):721?741.
Griffiths, Thomas L., Kevin R. Canini, Adam N.
Sanborn, and Daniel J. Navarro. 2007. Unifying
rational models of categorization via the hierar-
chical Dirichlet process. In Proceedings of the
29th Annual Conference of the Cognitive Sci-
ence Society. Nashville, Tennessee, pages 323?
328.
Griffiths, Thomas L., Adam N. Sanborn, Kevin R.
Canini, John D. Navarro, and Joshua B. Tenen-
baum. 2011. Nonparametric Bayesian mod-
els of categorization. In Emmanuel M. Pothos
and Andy J. Wills, editors, Formal Approaches
in Categorization, Cambridge University Press,
pages 173?198.
Harris, Zellig. 1954. Distributional structure.
Word 10(23):146?162.
Jern, Alan and Charles Kemp. 2013. A proba-
bilistic account of exemplar and category gen-
eration. Cognitive Psychology 66:85?125.
Kemp, Charles, Patrick Shafto, and Joshua B.
Tenenbaum. 2012. An integrated account of
generalization across objects and features. Cog-
nitive Psychology 64:35?75.
Lang, Joel and Mirella Lapata. 2011. Unsuper-
vised semantic role induction with graph par-
titioning. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Edinburgh, Scotland, UK.,
pages 1320?1331.
Levy, Roger P., Florencia Reali, and Thomas L.
Griffiths. 2009. Modeling the effects of mem-
257
ory on human online sentence processing with
particle filters. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances
in Neural Information Processing Systems 21,
pages 937?944.
McCallum, Andrew Kachites. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
McRae, Ken, George S. Cree, Mark S. Seidenberg,
and Chris McNorgan. 2005. Semantic feature
production norms for a large set of living and
nonliving things. Behavioral Research Methods
37(4):547?59.
Medin, Douglas L. and Marguerite M. Schaffer.
1978. Context theory of classification learning.
Psychological Review 85(3):207?238.
Nosofsky, Robert M. 1988. Exemplar-based
accounts of relations between classification,
recognition, and typicality. Journal of Exper-
imental Psychology: Learning, Memory, and
Cognition 14:700?708.
Posner, Michael I. and Steven W. Keele. 1968. On
the genesis of abstract ideas. Journal of Exper-
imental Psychology 21:367?379.
Quinn, Paul C. and Peter D. Eimas. 1996. Percep-
tual cues that permit categorical differentiation
of animal species by infants. Journal of Exper-
imental Child Psychology 63:189?211.
Reed, Stephen K. 1972. Pattern recognition and
categorization. Cognitive psychology 3(3):382?
407.
Riordan, Brian and Michael N. Jones. 2011. Re-
dundancy in perceptual and linguistic experi-
ence: Comparing feature-based and distribu-
tional models of semantic representation. Top-
ics in Cognitive Science 3(2):303?345.
Rosenberg, Andrew and Julia Hirschberg. 2007.
V-measure: A conditional entropy-based ex-
ternal cluster evaluation measure. In Proceed-
ings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing
and Computational Natural Language Learn-
ing. Prague, Czech Republic, pages 410?420.
Sanborn, Adam N., Thomas L. Griffiths, and
Daniel J. Navarro. 2006. A more rational model
of categorization. In Proceedings of the 28th
Annual Conference of the Cognitive Science So-
ciety. Vancouver, Canada, pages 726?731.
Smith, Edward E. and Douglas L. Medin. 1981.
Categories and Concepts. Harvard University
Press, Cambridge, MA, USA.
Starkey, David. 1981. The origins of concept for-
mation: Object sorting and object preference in
early infancy. Child Development pages 489?
497.
Storms, Gert, Paul De Boeck, and Wim Ruts.
2000. Prototype and exemplar-based informa-
tion in natural language categories. Journal of
Memory and Language 42:51?73.
Vinson, David and Gabriella Vigliocco. 2008. Se-
mantic feature production norms for a large set
of objects and events. Behavior Research Meth-
ods 40(1):183?190.
Voorspoels, Wouter, Wolf Vanpaemel, and Gert
Storms. 2008. Exemplars and prototypes in
natural language concepts: A typicality-based
evaluation. Psychonomic Bulletin & Review
15(3):630?637.
Yao, Xuchen and Benjamin Van Durme. 2011.
Nonparametric Bayesian word sense induc-
tion. In Proceedings of TextGraphs-6: Graph-
based Methods for Natural Language Process-
ing. Portland, Oregon, pages 10?14.
Zeigenfuse, Matthew D. and Michael D. Lee.
2010. Finding the features that represent stim-
uli. Acta Psychologica 133(3):283?295.
258
Dependency-Based Construction of Semantic
Space Models
Sebastian Pad??
Saarland University
Mirella Lapata??
University of Edinburgh
Traditionally, vector-based semantic space models use word co-occurrence counts from large
corpora to represent lexical meaning. In this article we present a novel framework for construct-
ing semantic spaces that takes syntactic relations into account. We introduce a formalization for
this class of models, which allows linguistic knowledge to guide the construction process. We
evaluate our framework on a range of tasks relevant for cognitive science and natural language
processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases,
our framework obtains results that are comparable or superior to the state of the art.
1. Introduction
Vector space models of word co-occurrence have proved a useful framework for repre-
senting lexical meaning in a variety of natural language processing (NLP) tasks, such
as word sense discrimination (Sch?tze 1998) and ranking (McCarthy et al 2004), text
segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correc-
tion (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin
1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models
have also been popular in cognitive science and figure prominently in several studies
simulating human behavior. Examples include similarity judgments (McDonald 2000),
semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and
McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and
Dumais 1997; Foltz, Kintsch, and Landauer 1998).
The popularity of vector-based models in both fields lies in their ability to repre-
sent word meaning simply by using distributional statistics. The central assumption
here is that the context surrounding a given word provides important information
about its meaning (Harris 1968). The semantic properties of words are captured in a
multi-dimensional space by vectors that are constructed from large bodies of text by
observing the distributional patterns of co-occurrence with their neighboring words.
Co-occurrence information is typically collected in a frequency matrix, where each row
corresponds to a unique word, commonly referred to as ?target word,? and each column
represents a given linguistic context. The semantic similarity between any two words
? Computational Linguistics, P.O. Box 15 11 50, 66041 Saarbr?cken, Germany. E-mail: pado@coli.uni-sb.de.
?? School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 20 December 2004; revised submission received: 26 September 2006;
accepted for publication: 23 November 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 2
can then be quantified directly using a distance measure such as cosine or Euclidean
distance.
Contexts are defined as a small number of words surrounding the target word
(Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs?even
documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997). Latent Semantic
Analysis (LSA; Landauer and Dumais 1997) is an example of a document-based vector
space model that is commonly used in information retrieval and cognitive science. Each
target word t is represented by a k element vector of paragraphs p1...k and the value
of each vector element is a function of the number of times t occurs in pi. In contrast,
the Hyperspace Analogue to Language model (HAL; Lund and Burgess 1996) creates
a word-based semantic space: each target word t is represented by a k element vector,
whose dimensions correspond to context words c1...k. The value of each vector element
is a function of the number of times each ci occurs within a window of size n before or
after t in a large corpus.
In their simplest incarnation, semantic space models treat context as a set of un-
ordered words, without even taking parts of speech into account (e.g., to drink and
a drink are represented by a single vector). In fact, with the exception of function words
(e.g., the, down), which are often removed, it is often assumed that all context words
within a certain distance from the target word are semantically relevant. Because no
linguistic knowledge is taken into account, the construction of semantic space models is
straightforward and language-independent?all that is needed is a segmented corpus
of written or spoken text.
However, the assumption that contextual information contributes indiscriminately
to a word?s meaning is clearly a simplification. There is ample evidence demonstrating
that syntactic relations across and within sentences are crucial for sentence and dis-
course processing (West and Stanovich 1986; Neville et al 1991; Fodor 1995; Miltsakaki
2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994). Fur-
thermore, much research in lexical semantics hypothesizes that the behavior of words,
particularly with respect to the expression and interpretation of their arguments, is to a
large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983;
Talmy 1985; Gropen et al 1989; Pinker 1989; Levin 1993; Goldberg 1995).
It is therefore not surprising that there have been efforts to enrich vector-based
models with morpho-syntactic information. Extensions range from part of speech tag-
ging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analy-
sis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin
1998a). In these semantic space models, contexts are defined over words bearing a
syntactic relationship to the target words of interest. This makes semantic spaces more
flexible; different types of contexts can be selected; words do not have to co-occur within
a small, fixed word window; and word order or argument structure differences can be
naturally mirrored in the semantic space.
This article proposes a general framework for semantic space models which concep-
tualizes context in terms of syntactic relations. We introduce an algorithm for construct-
ing semantic space models from texts annotated with syntactic information (specifi-
cally dependency relations) and illustrate how different model classes can be derived
from this linguistically rich representation. Our guiding hypothesis is that syntactic
structure in general and argument structure in particular is a close reflection of lexical
meaning (Levin 1993). We thus model meaning by quantifying the degree to which
words are attested in similar syntactic environments. The expressive power of our
framework stems from three novel parameters which guide model construction. The
first parameter determines which types of syntactic structures contribute towards the
162
Pad? and Lapata Dependency-Based Semantic Spaces
representation of lexical meaning. The second parameter allows us to weigh the relative
importance of different syntactic relations. Finally, the third parameter determines how
the semantic space is actually represented, for instance as co-occurrences of words with
other words, words with parts of speech, or words with argument relations (e.g., subject,
object).
We evaluate our framework on tasks relevant for cognitive science and NLP. We
start by simulating semantic priming, a phenomenon that has received much atten-
tion in computational psycholinguistics and is typically modeled using word-based
semantic spaces (Landauer and Dumais 1997; McDonald and Brew 2004). We next
consider the problem of recognizing synonyms by selecting an appropriate synonym
for a target word from a set of semantically related candidate words. Specifically, we
evaluate the performance of our model on synonym questions from the Test of English
as a Foreign Language (TOEFL). These are routinely used as a testbed for assessing
how well vector-based models capture lexical knowledge (Landauer and Dumais 1997;
Turney 2001; Sahlgren 2006). Our final experiment concentrates on unsupervised word
sense disambiguation (WSD), thereby exploring the potential of the proposed frame-
work for NLP applications requiring large scale semantic processing. We automatically
infer predominant senses in untagged text by incorporating our syntax-based semantic
spaces into the modeling paradigm proposed by McCarthy et al (2004). In all cases, we
show that our framework consistently outperforms word-based models yielding results
that are comparable or superior to state of the art.
Our contributions are threefold: a novel framework for semantic spaces that in-
corporates syntactic information in the form of dependency relations and generalizes
previous syntax-based vector-based models; an application of this framework to a wide
range of tasks relevant to cognitive modeling and NLP; and an empirical comparison of
our dependency-based models against state-of-the-art word-based models.
In Section 2, we give a brief overview of existing word-based and syntax-based
models. In Section 3, we present our modeling framework and relate it to previous work.
Section 4 discusses the parameter settings for our experiments. Section 5 details our
priming experiment, Section 6 presents our study on the TOEFL synonymy task, and
Section 7 describes our sense-ranking experiment. Discussion of our results and future
work concludes the article (Section 8).
2. Overview of Semantic Space Models
2.1 Word-Based and Syntax-Based Models
To facilitate comparisons with our framework, we begin with a brief overview of exist-
ing semantic space models. We describe traditional word-based co-occurrence models
as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy
and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994)
and Lin (1998a).
Lowe (2001) defines a semantic space model as a quadruple ?B, A, S, V?. B is the set
b1...D of basis elements, the dimensions of the space. B can be a set of words (Lund and
Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows
2003) or words with a syntactic relation such as subject or object (Lin 1998a). Usually,
the dimensionality of the matrix is restricted to a relatively small number. A popular
choice is the k most frequent words (minus the stop words) in a corpus, typically 100?
2,000 (McDonald 2000; Levy and Bullinaria 2001). A is a lexical association function
163
Computational Linguistics Volume 33, Number 2
applied to the co-occurrence frequency of target word t with basis element b so that each
word is represented by a vector v = ?A( f (t, b1)), A( f (t, b2)), . . . , A( f (t, bn))?. If A is the
identity function, the raw frequencies are used. Functions such as mutual information
or the log-likelihood ratio are often applied to factor out co-occurrences due to chance.
S is a similarity measure that maps pairs of vectors onto a continuous-valued scale of
contextual similarity. V is an optional transformation that reduces the dimensionality
of the semantic space. Singular value decomposition (SVD; Berry, Dumais, and O?Brien
1994; Golub and Loan 1989) is commonly used for this purpose. SVD can be thought
of as a means of inferring latent structure in distributional data, while making sparse
matrices more informative. For the rest of this article, we will ignore V and other
statistical transformations and concentrate primarily on ways of inducing structure
from grammatical and syntactic information.
To illustrate this definition, we construct a word-based semantic space for the target
words T = {lorry, carry, sweet, fruit}, using as our corpus the following sentence: A lorry
might carry sweet apples. For a word-based space, we might use the basis elements
B = {lorry, might, carry, sweet, apples}, a symmetric window of size 2, and identity as
the association function A. Each target word ti ? T will then be represented by a five-
dimensional row vector, and the value of each vector element will record the number
of times each basis element bi ? B occurs within a window of two words to the left and
two words to the right of the target word ti. The co-occurrence matrix that we obtain
according to these specifications is shown in Figure 1. A variety of distance measures
can be used to compute the similarity S between two target words (see Lee [1999] for an
overview), the cosine being the most popular:
simcos(x,y ) =
n
?
i=1
xiyi
?
n
?
i=1
x2i
?
n
?
i=1
y2i
(1)
Syntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mere
co-occurrence by capturing syntactic relationships between words such as subject?verb
or modifier?noun, irrespectively of whether they are physically adjacent or not. The
basis elements are generally assumed to be tuples (r, w) where w is a word occurring in
relation type r with a target word t. The relations typically reflect argument structure
(e.g., subject, object, indirect object) or modification (e.g., adjective?noun, noun?noun)
and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999;
Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran
2004). The basis elements (r, w) are treated as a single unit and are often called attributes
(Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a).
Figure 1
Word-based semantic space (symmetric window size 2).
164
Pad? and Lapata Dependency-Based Semantic Spaces
Figure 2 shows a syntax-based semantic space in the manner of Grefenstette (1994),
using the basis elements (subj,lorry), (aux,might), (mod,sweet), and (obj,apples). The
binary association function A records whether the target word possesses the feature
(denoted by x in Figure 2) or not. Because the cells of the matrix do not contain
numerical values, a similarity measure that is appropriate for categorical values must be
chosen. Grefenstette (1994) uses a weighted version of Jaccard?s coefficient, a measure
of association commonly employed in information retrieval (Salton and McGill 1983).
Assuming Attr(t) is the set of basis elements co-occurring with t, Jaccard?s coefficient is
defined as:
simJacc(t1, t2) =
Attr(t1) ? Attr(t2)
Attr(t1) ? Attr(t2)
(2)
Lin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the
matrix cells represent the number of times a target word t co-occurs with basis element
(r, w), as shown in Figure 3. He proposes an information theoretic similarity measure
based on the distribution of target words and basis elements:
simlin(t1, t2) =
?
(r,w)?T(t1 )?T(t2)
I(t1, r, w) + I(t2, r, w)
?
(r,w)?T(t1 )
I(t1, r, w) +
?
(r,w)?T(t2 )
I(t2, r, w)
(3)
where I(t, r, w) is the mutual information between t and r, w and T(t) is the set of basis
elements (r, w) such that I(t, r, w) is positive and
I(t, r, w) = log
P(t, r, w)P(r)
P(w, r)P(t, r)
= log
P(w|r, t)
P(w|r) (4)
Figure 2
Grefenstette?s (1994) semantic space.
Figure 3
Lin?s (1988a) semantic space.
165
Computational Linguistics Volume 33, Number 2
2.2 Discussion
Because syntax-based models capture more linguistic structure than word-based mod-
els, they should at least in theory provide more informative representations of word
meaning. Unfortunately, comparisons between the two types of models have been few
and far between in the literature. Furthermore, the potential of syntax-based models
has not been fully realized since most previous approaches limit themselves to a specific
model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002).
This section discusses these issues in more detail and sketches how we plan to address
them.
Modeling of syntactic context. All existing syntax-based semantic space models we are
aware of incorporate syntactic information in a rather limited fashion. For example,
the construction of the space is either based on all relations (Grefenstette 1994; Lin
1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction between
different relations. Even in cases where many relations are used (Lin 1998a; Lin and
Pantel 2001), only direct relations are taken into account, ignoring potentially important
co-occurrence patterns between, for instance, the subject and the object of a verb, or
between a verb and its non-local argument (e.g., in control structures).
Comparison between model classes. Syntax-based vector space models have been used in
NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefen-
stette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation
discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and
Carroll 2003). Comparisons between word-based and syntax-based models on the same
task are rare, and the effect of syntactic knowledge has not been rigorously investigated
or quantified. The few studies on this topic reveal an inconclusive picture. On the one
hand, Grefenstette compared the performance of the two classes of models on the task
of automatic thesaurus extraction and found that a syntactically enhanced model gave
significantly better results over a simple word co-occurrence model. A replication of
Grefenstette?s study with a more sophisticated parser (Curran and Moens 2002) re-
vealed that additional syntactic information yields further improvements. On the other
hand, attempts to generate more meaningful indexing terms for information retrieval
(IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et
al. 2002) have been largely unsuccessful. Experimental results show minimal differences
in retrieval effectiveness at a substantially greater processing cost (see Voorhees [1999]
for details).
Impact on cognitive modeling. Despite their widespread use in NLP, syntax-based seman-
tic spaces have attracted little attention in cognitive science and computational psy-
cholinguistics. Wiemer-Hastings and Zipitria (2001) construct a semantic space similar
to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters
in an intelligent tutoring context. Their results, however, show that the tagged LSA space
yields worse performance than a word-based model. Kanejiya, Kumar, and Prasad
(2003) attempt to capture syntactic context in a shallow manner by enhancing target
words with the parts-of-speech of their immediately preceding words. They argue that
this representation can provide useful information for the upcoming target words, as is
often the case in language modeling and left-to-right parsing. They employ a document-
based semantic space, which they submit to SVD and subsequently compare against an
LSA model that contains no syntactic information, again in the context of an intelligent
166
Pad? and Lapata Dependency-Based Semantic Spaces
tutoring system. Their results indicate that the syntactically enhanced model has better
coverage than the LSA model (i.e., it is able to evaluate more student answers), although
it displays a lower correlation with human raters than raw LSA.
In this article, we argue the case for investigating dependency-based semantic space
models in more depth. We provide a general definition for these models, which incor-
porates a wider range of syntactic relations than previously considered and subsumes
existing syntax-based and word-based models. In order to demonstrate the scope of
our framework, we evaluate our models on tasks popular in both cognitive science and
NLP. Furthermore, in all cases we report comparisons against state of the art word-
based models and show that the additional processing cost incurred by syntax-based
models is worthwhile.
3. A General Framework for Semantic Space Models
Once we move away from words as the basic context unit, the issue of representa-
tion of syntactic information becomes pertinent. An ideal syntactic formalism should
abstract over surface word order, mirror semantic relationships as closely as possible,
and incorporate word-based information in addition to syntactic analysis. It should be
also applicable to different languages. These requirements point towards dependency
grammar, which can be considered as an intermediate layer between surface syntax and
semantics. More formally, dependency relations are asymmetric binary relationships
between a head and a modifier (Tesni?re 1959). The structure of a sentence is analyzed
as a directed graph whose nodes correspond to words. The graph?s edges correspond
to dependency relationships and each edge is labeled with a specific relationship type
(e.g., subject, object).
The dependency analysis for the sentence A lorry might carry sweet apples is given
in Figure 4. On the left, the sentence is represented as a graph. The sentence head is the
main verb carry which is modified by its subject lorry, its object apples and the auxiliary
might. The subject and object are modified respectively by a determiner (a) and an ad-
jective (sweet). On the right side of Figure 4, an adjacency matrix notation is used. Edges
in the graph are represented as triples of a dependent word (e.g., lorry), a dependency
label (e.g., [N, subj, V]), and a head word (e.g., carry). The dependency label consists of
the part of speech of the modifier (capitalized, e.g., N) , the dependency relation itself
(in lower case, e.g., subj), and the part of speech of the head (also capitalized, e.g., V).
It is combinations of dependencies like the ones in Figure 4 that will form the
context over which the semantic space will be constructed. We base our discussion
Figure 4
A dependency analysis of the sentence A lorry might carry sweet apples as parse tree (left) and set
of head-relation-modifier triples (right).
167
Computational Linguistics Volume 33, Number 2
and experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin
1998a, 2001). However, there is nothing inherent in our formalization that restricts us to
this particular parser. Any other parser with broadly similar dependency output (e.g.,
Briscoe and Carroll 2002) could serve our purposes.
In the remainder of this section, we first give a non-technical description of our
algorithm for the construction of semantic spaces. Then, we proceed to discuss each con-
struction step (context selection, basis mapping, and quantification of co-occurrences)
in more detail. Finally, we show how our framework subsumes existing models. Table 1
lists the notation we use in the rest of the article.
3.1 The Construction Algorithm
Our algorithm for creating semantic space models is summarized in Figure 5. Central
to the construction process is the notion of paths, namely sequences of dependency
edges extracted from the dependency parse of a sentence (we define paths formally in
Section 3.2). Consider again the graph in Figure 4. Besides individual edges (i.e., paths
of length 1), it contains several longer paths, such as the path between lorry and
sweet (?lorry, carry, apples, sweet?), the path between a and carry (?a, lorry, carry?), the path
between lorry and carry (?lorry, carry?), and so forth. The usage of paths allows us to
represent direct and indirect relationships between words and gives rise to three novel
parameters:
1. The context selection function cont(t) determines which paths in the
graph contribute towards the representation of target word t. For example,
we may choose to consider only paths of length 1, or paths with
length ? 3. The function is effectively a syntax-based generalization of the
traditional ?window size? parameter.
2. The path value function v assigns weights to paths, thus allowing
linguistic knowledge to influence the construction of the space. For
Table 1
Summary of notation.
b ? B Basis element
t ? T Target word type
W(t) Set of tokens of target type t
M[t][b] ? R Cell of semantic space matrix for target word t and basis element b
? Dependency path (in a given dependency tree)
? Set of all undirected paths
?s Set of all undirected paths in sentence s
?t Set of all undirected paths in a sentence anchored at word t
start(?), end(?) First and last node of an undirected path
Cat Set of POS categories (for given parser)
R Set of dependency relations (for given parser)
l : ?? (Cat ? R ? Cat)? Edge (sequence) labeling function
cont : T ? 2?s Local context selection function (subset of paths in sentence s)
? : ?? B Basis element mapping function
v : ?? R Path value function
A : R4 ? R Lexical association function
168
Pad? and Lapata Dependency-Based Semantic Spaces
Figure 5
Algorithm for construction of semantic space.
instance, it can be used to discount longer paths, or give more weight to
paths containing subjects and objects as opposed to determiners or
modifiers.
3. The basis mapping function ? creates the dimensions of the semantic
space. Although paths themselves could serve as dimensions, the
resulting co-occurrence matrix would be overly sparse (this is especially
true for lexicalized paths whose number can become unwieldy when
parsing a large corpus). For this reason, the basis elements forming
the dimensions of the space are defined independently from the path
construction. The basis mapping function maps paths onto basis elements
by collapsing paths deemed functionally equivalent. For instance, we
may consider paths carrying the same dependency relations, or paths
ending in the same word, as equivalent. We thus disassociate the
definition of context entities (paths) from the dimensions of the final
space (basis elements).
As discussed in Section 2, the main difference among variants of semantic space models
lies in the specification of basis elements B. By treating the dependency paths as distinct
from the basis elements, we obtain a general framework for vector-based models that
can be parametrized for different tasks and allows for the construction of spaces with
basis elements consisting of words, syntactic entities, or combinations of both. This
flexibility, in conjunction with the context selection and path value functions, allows
our model to subsume both traditional word-based and syntax-based models (see
Section 3.6 for more details).
3.2 Step 1: Building the Context
The first step in constructing a semantic space from a large collection of dependency
relations is to define an appropriate syntactic context for the target words of interest.
We define contexts as anchored paths, that is, paths in a dependency graph that start
at a particular target word t. Our assumption is that the set of paths anchored at
t is a superset of the paths that can contribute relevant distributional information
about t.
169
Computational Linguistics Volume 33, Number 2
Definition 1. The dependency parse p of a sentence s is a directed graph ps = (Vs, Es),
where Es ? Vs ? Vs. The nodes v ? Vs are labeled with individual words wi. For
simplicity, we use nodes and their labels interchangeably, and the set of nodes
corresponds to the words of the sentence: Vs = {w1, . . . , wn}. Each edge e ? Es bears
a label l : Es ? Cat ? R ? Cat where Cat belongs to a set of POS tags and R to a set of
dependency relations. We assume that this set is finite and parser-specific.1 We write
edge labels in square brackets. [Det,det,N] and [N,subj,V] are examples for labels
provided by MINIPAR (see Figure 4, right-hand side).
We are now ready to define paths in our dependency graph, save one important
issue: Should we confine ourselves to directed paths or perhaps disregard the direction
of the edges? In a dependency graph, directed paths can only capture the relationship
between a head and its (potentially transitive) dependents (e.g., carry and sweet in
Figure 4). This excludes informative contexts representing, for instance, the relationship
between the subject and the object of a predicate (e.g., lorry and apples in Figure 4). Our
intuition is therefore that directed paths would limit the context too severely. In the
following, we assume undirected paths.
Definition 2. An (undirected) path ? is an ordered tuple of nodes ?v0, . . . , vn? ? V?s for
some sentence s that meets the following two constraints:
? i : (vi?1, vi) ? Es ? (vi, vi?1) ? Es (connectedness)
? i? j : i = j ? vi = vj (cycle-freeness)
In the rest of the article, we use the term path as a shorthand for undirected path.
Definition 3. A path ? is anchored at a word t iff start(?) = t. We write ?t ? ?s for the
set of all paths anchored at t in sentence s.
As an example, the set of paths anchored at lorry in Figure 4 is:
{?lorry, carry?, ?lorry, a?, (two paths of length 1)
?lorry, carry, apples?, ?lorry, carry, might?, (two paths of length 2)
?lorry, carry, apples, sweet?} (one path of length 3)
Definition 4. The context selection function cont : W ? 2?t assigns to a word t a subset
of the paths anchored at t. We call this subset the syntactic context of t.
The context selection function allows direct control over the type of linguistic informa-
tion represented in the semantic space. In traditional vector-based models, the context
1 For the sake of simplicity, we use R without a subscript to denote the set of dependency relations
provided by MINIPAR. We utilize subscripts to distinguish between general sets (e.g., E for the set of all
conceivable edges) and sentence-specific sets (e.g., Es for the set of edges in the parse tree of sentence s).
170
Pad? and Lapata Dependency-Based Semantic Spaces
selection function does not take any syntactic information into account: All paths ? are
selected for which the absolute difference (abs) between the positions (pos) of the anchor
start(?) and the end word end(?) does not exceed the window size k:
cont(t) = {? ? ?t | abs(pos(start(?)) ? pos(end(?))) ? k} (5)
The dependency-based models proposed by Grefenstette (1994) and Lin (1998a) con-
sider minimal syntactic contexts in the form of individual dependency relations,
namely, dependency paths of length 1:
cont(t) = {? ? ?t | ||?|| = 1} (6)
The context selection function as defined herein permits the elimination of paths from
the semantic space on the basis of linguistic or other information. For example, it can be
argued that subjects and objects convey more semantic information than determiners
or auxiliaries. We can thus limit our context to the set of all anchored paths consisting
exclusively of subject or object dependencies:
cont(t) = {? ? ?t | l(?) ? {[V, subj, N], [V, obj, N]}?} (7)
When this context specification function is applied to the dependency graph in Figure 4,
only the edges shown in boxes are retained. The context of lorry is thus reduced to
two paths: ?lorry, carry? (length 1) and ?lorry, carry, apples? (length 2). The paths ?lorry, a?,
?lorry, carry, might?, and ?lorry, carry, apples, sweet? are omitted because their label se-
quences (such as [N,det,Det] for ?lorry, a?) are disallowed by (7).
3.3 Step 2: Basis Mapping
The second step in the construction of our semantic space model is to specify its
dimensions, the basis elements, following Lowe?s (2001) terminology.
Definition 5. The basis mapping function ? : ?? B maps paths onto basis elements.
By dissociating dependency paths and basis elements in this way, we decouple the
observed syntactic context from its representation in the final semantic space. The
basis mapping allows us to exploit underlying relationships among different paths:
Two paths which are (in some sense) equivalent can be mapped onto the same basis
element. The function effectively introduces a partitioning of paths into equivalence
classes ?labeled? by basis elements, thus offering more flexibility in defining the basis
elements themselves.
Traditional co-occurrence models use a word-based basis mapping. This means that all
paths ending at word w are mapped onto the basis element w, resulting in a semantic
space with context words as basis elements (recall that all paths in the local context start
at the target word):
?(?) = end(?) (8)
171
Computational Linguistics Volume 33, Number 2
A word-based mapping is also possible when paths are defined over dependency
graphs. As an example consider the paths anchored at lorry in Figure 4. Using (8), these
paths are mapped to the following basis elements:
?lorry, carry? carry
?lorry, a? a
?lorry, carry, apples? apples
?lorry, carry, might? might
?lorry, carry, apples, sweet? sweet
A different mapping is used in Grefenstette (1994) and Lin (1998a), who consider only
paths of length 1. In their case, paths are mapped onto pairs representing a dependency
relation r and the end word w (see the discussion in Section 2):
?(?) = (r, end(?)) where ||?|| = 1 ? ?r? = l(?) (9)
Any plausible and computationally feasible function can be used as basis mapping.
However, in this article we restrict ourselves to models which use a word-based ba-
sis mapping. The resulting spaces are similar to traditional word-based spaces?both
use sets of context words?which allows for direct comparisons between our models
and word-based alternatives. Crucially, our models differ from traditional models in
the more general treatment of (syntactic) context: Only paths in the syntactic context,
and not surface co-occurrences, contribute towards counts in the matrix. The context
selection function supports inference over classes of basis elements (which in previous
models would have been considered distinct) as well as fine-grained control over the
types of relationships that enter into the space construction.
3.4 Step 3: Quantifying Syntactic Co-occurrence
The last step in the construction of the dependency-based semantic models is to specify
the relative importance (i.e., value) of different paths:
Definition 6. The path value function v assigns a real number to a path: v : ?? R.
Traditional models do not exploit this possibility, thus giving equal weight to all paths:
vplain(?) = 1 (10)
The path value function provides additional flexibility for incorporating linguistic in-
formation into our framework. Even if two paths are mapped onto the same basis
element (by the basis mapping), the path value function can weigh their respective
contributions differently. For instance, it could discount longer paths that express
indirect relationships between words. An example of such a length-based path value
172
Pad? and Lapata Dependency-Based Semantic Spaces
function is given in Equation (11). It assigns a value of 1 to paths of length 1 and fractions
to longer paths:
vlength(?) = 1||?|| (11)
A more linguistically informed path value function can be defined by taking into
account the obliqueness hierarchy of grammatical relations (Keenan and Comrie 1977).
According to this hierarchy subjects are more salient than objects, which in turn are
more salient than obliques (e.g., prepositional phrases). And obliques are more salient
than genitives. We thus define a linear relation-based weighting scheme that ranks paths
according to their most salient grammatical function, without considering their length:
vgram?rel(?) =
?
?
?
?
?
?
?
?
?
?
?
?
?
5 if subj ? l(?)
4 if obj ? l(?)
3 if obl ? l(?)
2 if gen ? l(?)
1 else
(12)
The path value function assigns a numerical value to each path forming the syntac-
tic context of a token t. We can next define the local co-occurrence frequency between t
and a basis element b as the sum of the path values v(?) for all paths ? ? cont(t) that are
mapped onto b. Because our semantic space construction algorithm operates over word
types, we sum the local co-occurrence frequencies for all instances of a target word type t
(written as W(t)) to obtain its global co-occurrence frequency. The latter is a measure
of the co-occurrence of t and b over the entire corpus.
Definition 7. The global co-occurrence frequency of a basis element b and a target t is
computed by the function f : B ? T ? R defined by
f (b, t) =
?
w?W(t)
?
??cont(w)??(?)=b
v(?)
The global co-occurrence frequency f (b, t) could be used directly as the matrix value
M[b][t]. However, as Lowe (2001) notes, raw counts are likely to give misleading results.
This is due to the non-uniform distribution of words in corpora which will introduce a
frequency bias so that words with similar frequency will be judged more similar than
they actually are. It is therefore advisable to use a lexical association function A to factor
out chance co-occurrences explicitly.
Our definition allows an arbitrary choice of lexical association function (see
Manning and Sch?tze [1999] for an overview). In our experiments, we follow Lowe
and McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993).
We can visualize the computation using a two-by-two contingency table whose four
cells correspond to four events (Kilgarriff 2001):
t ? t
b k l
? b m n
173
Computational Linguistics Volume 33, Number 2
The top left cell records the frequency k with which t and b co-occur (i.e., k corresponds
to raw frequency counts). The top right cell l records how many times b is attested with
any word other than t, the bottom left cell m represents the frequency of any word other
than b with t, and the bottom right cell n records the frequency of pairs involving neither
b nor t. The function G2 : R4 ? R is defined as
G2(k, l, m, n) = 2(k log k + l log l + m log m + n log n
? (k + l) log(k + l) ? (k + m) log(k + m)
? (l + n) log(l + n) ? (m + n) log(m + n)
+ (k + l + m + n) log(k + l + m + n))
(13)
A naive implementation of the log-likelihood ratio would keep track of all four events
for each pair (t, b); this strategy would require updating the entire matrix for each
path and would render the construction of the space prohibitively expensive. This
can be avoided by computing only k = f (t, b), the global co-occurrence frequency,
and using the marginal frequencies of paths and targets to estimate l, m and n as
follows:
l =
?
t
f (t, b) ? k m =
?
b
f (t, b) ? k n =
?
b
?
t
f (t, b) ? (k + l + m) (14)
For example, l can be computed as the total value of all paths in the corpus that are
mapped onto b minus the value of those paths that are anchored at t.
3.5 Definition of Semantic Space
Our framework of semantic space models can now be formally specified by extending
Lowe?s (2001) definition from Section 2:
Definition 8. A semantic space is a tuple ?B, T, M, S, A, cont,?, v?. B is the set of
basis elements, T the set of target words, and M is the matrix M = B ? T. We write
M[tj][bi] ? R for the matrix cell (i, j). S : T ? T ? R is the similarity measure, and
A : R4 ? R is the lexical association function. Our additional parameters are the
content selection function cont : T ? 2?s, the basis mapping function ? : ?? B, and
the path value function v : ?? R.
Note that the set of target words T can contain either word types or word tokens. In
the preceding definitions, we have assumed that co-occurrence counts are constructed
over word types, however the framework can be also used to represent word tokens. In
this case, each set of target tokens contains exactly one word (W(t) = {t}), and the outer
summation step in Definition 7 trivially does not apply. We work with type-based spaces
in the rest of this article. The use of tokens may be appropriate for other applications
such as word sense discrimination (Sch?tze 1998).
We can now construct a semantic space that illustrates our framework. Consider
again the sentence A lorry might carry sweet apples. According to Definition 8, in
order to construct vectors for the target words T = {lorry, might, carry, sweet, fruit}, we
174
Pad? and Lapata Dependency-Based Semantic Spaces
must provide a context selection function, a basis mapping function, and a path value
function. The space resulting from a context selection function which considers exclu-
sively subject and object dependencies (see Equation (7)), a word-based basis mapping
function (see Equation (8)), and a length-based path value function (see Equation (11)),
is shown in Figure 6.
3.6 Discussion
We have proposed a general framework for semantic space models which operates
on dependency relations and allows linguistic knowledge to inform the construction
of the semantic space. The framework is highly flexible: Depending on the context
selection and basis mapping functions, semantic spaces can be constructed over words,
words and parts of speech, syntactic relations, or combinations of words and syntactic
relations. This flexibility unavoidably increases the parameter space of our models,
because there is a potentially large number of context selection or path value functions
for which semantic spaces can be constructed.
At the same time, this allows us to subsume existing semantic space models in
our framework, and facilitates comparisons across different kinds of spaces (compare
Figures 1, 3, and 6). Our space is sparser than the word-based space in Figure 1,
due to the choice of a more selective context specification function (see Equations (5)
and (7)). However, this is expected because our main motivation is to distinguish
between informative and uninformative syntactico-semantic relations. Using a minimal
context selection function results in a space that contains indisputably valid semantic
relations, excluding potentially noisy relations like the one between might and sweet. By
adding richer linguistic information to the context selection function, the space can be
expanded in a principled manner. In comparison with previous syntax-based models,
which only use direct dependency relations (see Equation (6)), our dependency-based
space additionally represents indirect semantic relations (e.g., between lorry and apples).
A smaller parameter space could have resulted from collapsing the context selection
and path value functions into one parameter, for example, by defining context selection
directly as a function from (anchored) paths to their path values, and thus assigning a
value of zero to all paths ? ? cont(t). However, we refrained from doing this for two
reasons, a methodological one and a technical one. On the methodological side, we
believe that it makes sense to keep the two concepts of context selection and context
weighting distinct. The separation allows us to experiment with different path value
functions while keeping the set of paths resulting from context selection constant.
On the technical side, the two functions are easier to specify declaratively when kept
separately. Also, a separate context selection function can be used to efficiently isolate
relevant context paths without having to compute the values for all anchored paths.
Figure 6
A dependency-based semantic space using context selection function (7), basis mapping
function (8), and path value function (11).
175
Computational Linguistics Volume 33, Number 2
The context selection function operates over a subset of dependency paths that are
anchored, cycle-free, and connected. These three preconditions on paths are meant to
reflect linguistic properties of reasonable syntactic contexts while at the same time they
guarantee the efficient construction of the semantic space. Anchoredness ensures that
all paths are semantically connected to the target; this also means that the search space
can be limited to paths starting at the target word. Cycle-freeness and connectedness
exclude linguistically meaningless paths such as paths of infinite length (cycles) or paths
consisting of several unconnected fragments. These properties guarantee that context
paths can be created incrementally, and that construction terminates.
3.7 Runtime and Implementation
Our implementation uses path templates to encode the context selection function (see
Appendix A for more details). The runtime of the semantic space construction algorithm
presented in Section 3 is O(maxg ? |cont| ? t), where maxg is the maximal degree of a node
in the grammar, |cont| the number of path templates used for context selection, and t the
number of target tokens in the corpus. This assumes that ?(?) and v(?) can be computed
in constant time, which is warranted in practice because most linguistically interesting
paths will be of limited length (in our study, all paths have a length of at most 4). The
linear runtime in the size of the corpus provides a theoretical guarantee that the method
is applicable to large corpora such as the British National Corpus (BNC).
A Java implementation of the framework presented in this article is available un-
der the GNU General Public License from http://www.coli.uni-saarland.de/?pado/
dv/dv.html. The system can create dependency spaces from the output of MINIPAR (Lin
1998b, 2001). We also provide an interface for integrating other parsers. The distribution
includes a set of prespecified parameter settings, namely the word-based basis mapping
function, and the path value and context selection functions used in our experiments.
4. Experimental Setup
In this section, we describe the corpus and parser chosen for our experiments. We also
discuss our parameter and model choice procedure, and introduce the baseline word-
based model which we use for comparison with our approach. Our experiments are
then presented in Sections 5?7.
4.1 Corpus and Parser
All our experiments were conducted on the British National Corpus (BNC), a 100-
million word collection of samples of written and spoken English (Burnard 1995). The
corpus represents a wide range of British English, including samples from newspapers,
magazines, books (both academic and fiction), letters, essays, as well as spontaneous
conversations, business or government meetings, radio shows, and phone-ins. The BNC
has been used extensively in building vector space models for many tasks relevant
for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and
Brew 2004) and NLP (McCarthy et al 2004; Weeds 2003; Widdows 2003).
In order to construct dependency spaces, the BNC was parsed with MINIPAR,
version 0.5 (Lin 1998b, 2001), a wide-coverage dependency parser. MINIPAR employs a
manually constructed grammar and a lexicon derived from WordNet with the addition
of proper names (130,000 entries in total). Lexicon entries contain part-of-speech and
176
Pad? and Lapata Dependency-Based Semantic Spaces
subcategorization information. The grammar is represented as a network of 35 nodes
(i.e., grammatical categories) and 59 edges (i.e., types of dependency relationships).
MINIPAR uses a distributed chart parsing algorithm. Grammar rules are implemented
as constraints associated with the nodes and edges. When evaluated on the SUSANNE
corpus (Sampson 1995), the parser achieved a precision of 89% and a recall of 79% in
identifying labeled dependencies (Lin 1998b).
4.2 Model Selection
The construction of semantic space models involves a large number of parameters: the
dimensions of the space, the size and type of the employed context, and the choice of
similarity function. A number of studies (Patel, Bullinaria, and Levy 1998; Levy and
Bullinaria 2001; McDonald 2000) have explored the parameter space for word-based
models in detail, using evaluation benchmarks such as human similarity judgments or
synonymy choice tests. The motivation behind such studies is to identify parameters
or parameter classes that yield consistently good performance across tasks. To avoid
overfitting, exploration of the parameter space is typically performed on a development
data set different from the test data (McDonald 2000).
The benchmark data set collected by Rubenstein and Goodenough (1965) is rou-
tinely used in NLP and cognitive science for development purposes?for example, for
evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and
Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vec-
tor space models (McDonald 2000). It consists of 65 noun-pairs ranging from highly
synonymous (gem-jewel ) to semantically unrelated (noon-string). For each pair, a sim-
ilarity judgment (on a scale of 0 to 4) was elicited from human subjects. The average
rating for each pair represents an estimate of the perceived similarity of the two words.
Correlation analysis is often used to examine the degree of linear relationship between
the human ratings and the corresponding automatically derived similarity values.
Following previous work, we explored the parameter space of our dependency
models on the Rubenstein and Goodenough (1965) data set. The best performing model
was then used in all our subsequent experiments. We expect a dependency model
optimized on the semantic similarity task to perform well across other related lexical
tasks, which incorporate semantic similarity either directly or indirectly. This is true
for all tasks reported in this article, namely priming (Experiment 1), inferring whether
two words are synonyms (Experiment 2), and acquiring predominant word senses
(Experiment 3). Some performance gains could be expected, if parameter optimization
took place separately for each task. However, such a strategy would unavoidably lead
to overfitting, especially because our data sets are generally small (see Experiments 1
and 2).
We next detail how parameters were instantiated in our dependency models with
an emphasis on the influence of the context selection and path value functions.
Parameters. Dependency contexts were defined over a set of 14 dependency relations,
each of which occurred more than 500,000 times in the BNC and which in total ac-
counted for about 76 million of the 88 million dependency relations found in the corpus.
These relations are: amod (adjective modifier), comp1 (first complement), conj (coordina-
tion), fc (finite complement), gen (genitive noun modifier), i (the relationship between
a main clause and a complement clause), lex-mod (lexical modifier), mod (modifier), nn
(noun-noun modifier), obj (object of a verb), pcomp-n (nominal complement of preposi-
tions), rel (relative clause), s (surface subject), and subj (subject of a verb). From these,
177
Computational Linguistics Volume 33, Number 2
we constructed three context selection functions (fully described in Appendix A), which
we implemented as parser-specific templates (one template per non-lexical dependency
path):
 minimum contexts contain paths of length 1 (27 templates; in Figure 4 sweet
and carry are the minimum context for apples). This definition of syntactic
context considers only direct relations and corresponds to local verbal
predicate-argument structure.
 medium contexts add to minimum contexts dependency paths which model
the internal structure of noun phrases (length ? 3; 59 templates). In
particular, the medium context covers phenomena such as coordination,
genitive constructions, noun compounds, and different kinds of
modification.
 maximum contexts combine all templates defined over the 14 dependency
relations described above into a rich context representation (length ? 4;
123 templates).
The context specification functions were combined with the three path value functions
introduced in Section 3:
 plain (vplain, see Equation (10)) assigns the same value (namely 1) to every
path. It is the simplest path value function and assumes that all paths are
equally important.
 length (vlength, see Equation (11)) implements a length-based weighting
scheme: It assigns each path a value inversely proportional to its length,
thus giving more weight to shorter paths corresponding to more direct
relationships.
 gram-rel (vgram-rel, see Equation (12)) uses the obliqueness hierarchy
(Keenan and Comrie 1977) to rank paths according to the salience of their
grammatical relations. Specifically, each path is assigned the value of its
most salient grammatical relation (subjects are more salient than objects,
which are more salient than other noun phrases).
The combination of the three context selection and three path value functions yields
nine model instantiations.2 To facilitate comparisons with traditional semantic space
models, we used a word-based basis mapping function (see Equation (8)) and the log-
likelihood score (see Equation (13)) as our lexical association function. We also created
semantic spaces with different dimensions, using the 500, 1,000, and 2,000 most frequent
basis elements obtained from the BNC. Finally, we experimented with a variety of
similarity measures: cosine, Euclidean distance, L1 norm, Jaccard?s coefficient, Kullback-
Leibler divergence, skew divergence, and Lin?s (1998a) measure.3
2 Because the minimum context selection only considers paths of length 1, the combinations minimum-plain
and minimum-length are identical.
3 The original specification of Lin?s distance measure (Equation (3)) assumes relation?word pairs as basis
elements. Because we work with a word-based basis mapping, we use a simplified version, where
I(t, r, w) reduces to I(t, w) = log P(t,w)P(t)P(w) .
178
Pad? and Lapata Dependency-Based Semantic Spaces
Results. The effects of different parameters on modeling semantic similarity (using
Rubenstein and Goodenough?s [1965] data set) are illustrated in Tables 2 and 3. We
report the Pearson product moment correlation (?Pearson?s r?) between human ratings
of similarity and vector-based similarity. Rubenstein and Goodenough report an inter-
subject correlation of r = 0.85 on the rating task. The latter can be considered an upper
bound for what can be expected from computational models. For the sake of brevity, we
only report results with 2,000 basis elements, because we found that models with fewer
dimensions (e.g., 500 and 1,000) generally obtained worse performance. Lin?s (1998a)
similarity measure uniformly outperformed all other measures by a large margin. For
comparison, we also give the results we obtained with the cosine similarity measure
(see Table 2).
As can be seen, the gram-rel path value function performs generally worse than
length or plain. We suspect that this function is, at least in its present form, too selective,
giving a low weight to a large number of possibly informative paths without subjects
or objects. A similar result is reported in Henderson et al (2002), who find that using
the obliqueness hierarchy to isolate important index terms in an information retrieval
task degrades performance. The use of the less fine-grained length path value function
delivers better results for the medium and maximum context configurations (see Table 3).
Finally, we observe that the medium context yields the best overall performance. Within
the currently explored parameter space, medium appears to strike the best balance: It
includes some dependency paths beyond length one (corresponding to informative
indirect relations), but also avoids very long and infrequent contexts which could
potentially lead to overly sparse representations. In sum, the best dependency-based
model uses the medium content selection and length path value functions, 2,000 basis
elements, and Lin?s (1998a) similarity measure. This model will be used for our subse-
quent experiments without additional parameter tuning. We will refer to this model as
the optimal dependency-based model.
Table 2
Correlations (Pearson?s r) between elicited similarity and dependency models using the cosine
distance, 2,000 basis elements, and the log-likelihood association function.
?
?
?
?
?
?
?
?
?
Context
Path plain length gram-rel
minimum 0.45 0.45 0.43
medium 0.45 0.45 0.44
maximum 0.47 0.46 0.45
Table 3
Correlations (Pearson?s r) between elicited similarity and dependency models using Lin?s
(1998a) similarity measure, 2,000 basis elements and the log-likelihood association function.
?
?
?
?
?
?
?
?
?
Context
Path plain length gram-rel
minimum 0.58 0.58 0.58
medium 0.60 0.62 0.59
maximum 0.56 0.59 0.55
179
Computational Linguistics Volume 33, Number 2
4.3 Baseline Model
Our experiments will compare the optimal dependency model just described against a
state-of-the art word-based vector space model commonly used in the literature. The
latter employs a ?bag of words? definition of context (see Equation (5)), uses words
as basis elements, and assumes that all words are given equal weight. In order to
allow a fair comparison, we trained the word-based model on the same corpus as
the dependency-based model (the complete BNC) and selected parameters that have
been considered ?optimal? in the literature (Patel, Bullinaria, and Levy 1998; Lowe and
McDonald 2000; McDonald 2000). Specifically, we built a word-based model with a
symmetric 10 word window as context and the most frequent 500 content words from
the BNC as dimensions.4 We used log-likelihood as our lexical association function and
the cosine similarity measure5 as our distance measure.
5. Experiment 1: Single-Word Priming
A large number of modeling studies in psycholinguistics have focused on simulating
semantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000;
McDonald 2000; McDonald and Brew 2004). The semantic priming paradigm provides a
natural test bed for semantic space models, as it concentrates on the semantic similarity
or dissimilarity between words, and it is precisely this type of lexical relation that
vector-based models should capture. If dependency-based models indeed represent
more linguistic knowledge, they should be able to model semantic priming better than
traditional word-based models.
In this experiment, we focus on Hodgson?s (1991) single-word lexical priming study.
In single-word semantic priming, the transient presentation of a prime word like tiger
directly facilitates pronunciation or lexical decision on a target word like lion: responses
are usually faster and more accurate when the prime is semantically related to the
target than when it is unrelated. Hodgson set out to investigate which types of lexical
relations induce priming. He collected a set of 144 word pairs exemplifying six different
lexical relations: (a) synonymy (words with the same meaning, e.g., value and worth);
(b) superordination and subordination (one word is an instance of the kind expressed by
the other word, e.g., pain and sensation); (c) category coordination (words which express
two instances of a common superordinate concept, e.g., truck and train); (d) antonymy
(words with opposite meaning, e.g., friend and enemy); (e) conceptual association (the
first word subjects produce in free association given the other word, e.g., leash and dog);
and (f) phrasal association (words which co-occur in phrases, e.g., private and property).
The pairs covered the most prevalent parts of speech (adjectives, verbs, and nouns); they
were selected to be unambiguous examples of the relation type they instantiate and
were matched for frequency. Hogdson found equivalent priming effects (i.e., reduced
reading times) for all six types of lexical relation, indicating that priming was not
restricted to particular types of prime?target relation.
The priming effects reported in Hodgson (1991) have recently been modeled by
McDonald and Brew (2004), using an incremental vector-based model of contextual
4 Increasing the dimensions of the space to 1,000 and 2,000 degraded performance. Smaller context
windows did not yield performance gains either.
5 We repeated all experiments for the word-based model with Lin?s (1998a) distance measure, obtaining
consistently worse results.
180
Pad? and Lapata Dependency-Based Semantic Spaces
facilitation. Their ICE model (short for Incremental Construction of Semantic Expec-
tations) simulates the difference in effort between processing a target word preceded
by a related prime and processing the same target preceded by an unrelated prime.
This is achieved by quantifying the ability of the distributional characteristics of the
prime word to predict the distributional properties of the target. The prime word is
represented by a vector of probabilities which reflects the likely location in semantic
space of the upcoming word. When the target word is observed, the representation
is updated using a Bayesian inference mechanism to reflect the newly arrived infor-
mation. McDonald and Brew use a traditional semantic space that takes only word
co-occurrences into account and is defined over the 500 most frequent words of the
spoken portion of the BNC. They measure distance in semantic space using relative
entropy (also known as Kullback?Leibler divergence) and successfully model the data
by predicting that the distance should be lower for related prime-target pairs than for
unrelated prime?target pairs.
5.1 Method
In this experiment we follow McDonald and Brew?s (2004) methodology in simulating
semantic priming. However, because our primary focus is on the representation of the
semantic space, we do not adopt their incremental model of semantic processing. We
simply model reading time for prime?target pairs by distance in the semantic space,
without making explicit predictions about upcoming words.
From the 143 prime?target pairs listed in Hodgson (1991) (one synonymy pair is
missing in the original data set), seven pairs containing at least one low-frequency word
(less than 100 occurrences in the BNC) were removed to avoid creating vectors with
unreliable counts.6 We constructed a dependency-based model with the parameters that
yielded best performance on our development set (see Section 4.2) and a baseline word-
based model (see Section 4.3). Each prime?target pair was represented by two vectors
(one corresponding to the prime and one corresponding to the target).
These prime?target pairs form the items in this experiment. The independent vari-
ables (i.e., the variables directly manipulated by Hodgson [1991] in his original experi-
ment) are (1) the type of Lexical Relation (antonyms, synonyms, conceptual associates,
phrasal associates, category coordinates, superordinate-subordinates), and (2) the Prime
(related, unrelated). The dependent variable (i.e., the quantity being measured) is the
distance between the vector space representations of the prime and the target. The
priming effect is simulated by comparing the distances between Related and Unrelated
prime?target pairs. Because the original materials do not provide Unrelated primes, we
emulated the unrelated pairs as described in McDonald and Brew (2004), by using the
average distance of a target to all other primes of the same relation.
We test two hypotheses: first, that our dependency-based model can simulate se-
mantic priming. Failure to do so would indicate that our model is deficient because it
cannot capture basic semantic relatedness, a notion underlying many tasks in cognitive
science and NLP. Second, we predict that the dependency-based model will be better at
simulating priming than a traditional word-based one.
6 Low-frequency words are deemed to produce high variance vectors because the co-occurrence counts
needed to determine M[t][b] will be unreliable (see McDonald [2000] for further evidence). Variance can
be decreased by providing more data or by smoothing; however, we leave this to future work.
181
Computational Linguistics Volume 33, Number 2
5.2 Results
We carried out a two-way analysis of variance (ANOVA) on the simulated priming
data generated by the optimal dependency-based and the baseline word-based model.
The factors were the two independent variables introduced herein, namely Lexical
Relation (six levels) and Prime (two levels). A reliable Prime effect was observed for
the dependency-based model (F(1, 129) = 182.46, MSE = 0.93, p < 0.01): the distance
between a target and its Related prime was significantly smaller than between a tar-
get and an Unrelated prime. We also observed a reliable Prime effect for the tradi-
tional word-based model that did not use any syntactic information (F(1, 129) = 106.69,
MSE = 2.92, p < 0.01). There was no main effect of Lexical Relation for either model
(F(5, 129) < 1).
The fact that the analysis of variance has produced a significant F for the two models
only indicates that there are differences between the Related and Unrelated prime-target
means that cannot be attributed to error. Ideally, we would like to compare the two
models, for example, by quantifying the magnitude of the Prime effect. Eta-squared
(?2) is a statistic7 often used to measure the strength of an experimental effect (Howell
2002). It is analogous to r2 in correlation analysis and represents how much of the
overall variability in the dependent variable (in our case distance in semantic space)
can be explained or accounted for by the independent variable (i.e., Prime). The use of
?2 allowed us to perform comparisons between models (the higher the ?2, the better the
model). The Prime effect size was greater for the dependency model, which obtained an
?2 of 0.332 compared to the word-based model whose ?2 was 0.284. In other words, the
dependency model accounted for 33.2% of the variance, whereas the word-based model
accounted for 28.4%.
To establish whether the priming effect observed by the dependency model holds
across all relations, we next conducted separate ANOVAS for each type of Lexical Rela-
tion. The ANOVAS revealed reliable priming effects for all six relations. Table 4 shows the
mean distance values for each relation in the Related and Unrelated condition and the
Prime Effect size for the dependency model. The latter was estimated as the difference
in distance values between related and unrelated prime-target pairs (asterisks indicate
whether the difference is statistically significant, according to a two-tailed paired t-test).
For comparison, we also report the Prime Effect size that McDonald and Brew (2004)
obtained in their simulation.
To summarize, our results indicate that a semantic space model defined over depen-
dency relations simulates direct priming across a wide range of lexical relations. Fur-
thermore, our model obtained a priming effect that is not only reliable but also greater
in magnitude than the one obtained by a traditional word-based model. Although we
used a less sophisticated model than McDonald and Brew (2004), without an update
procedure and an explicit computation of expectations, we obtained priming effects
across all relations. In fact, we consider the two models complementary. McDonald and
Brew?s model could straightforwardly incorporate syntax-based semantic spaces like
the ones defined in this article.
We next examine synonymy, a single lexical relation, in more detail and assess
whether the proposed dependency model can reliably distinguish synonyms from non-
synonyms. This capability may be exploited to automatically generate corpus-based
7 Eta-squared is defined as ?2 =
SSeffect
SStotal
where SSeffect is the variance (sum of squares) created by one
particular effect (Prime in our case) and SStotal is the variance of all observations together.
182
Pad? and Lapata Dependency-Based Semantic Spaces
Table 4
Mean distance values for Related and Unrelated prime?target pairs; Prime Effect size
(= Related ? Unrelated) for the dependency model and ICE.
Lexical Relation N Related Unrelated Effect Effect
(dependency) (ICE)
Synonymy 23 0.267 0.102 0.165** 0.063
Superordination 21 0.227 0.121 0.106** 0.067
Category coordination 23 0.256 0.119 0.137** 0.074
Antonymy 24 0.292 0.127 0.165** 0.097
Conceptual association 23 0.204 0.121 0.083** 0.086
Phrasal association 22 0.146 0.103 0.043** 0.058
**p < 0.01 (2-tailed)
thesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications
that utilize semantic similarity. Examples include contextual spelling correction (Jones
and Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and question
answering (Lin and Pantel 2001).
6. Experiment 2: Detecting Synonymy
The Test of English as a Foreign Language (TOEFL) is commonly used as a benchmark
for comparing the merits of different similarity models. The test is designed to assess
non-native speakers? knowledge of English. It consists of multiple-choice questions,
each involving a target word embedded in a sentence and four potential synonyms.
The task is to identify the real synonym. An example is shown below where crossroads
is the real synonym for intersection.
You will find the office at the main intersection.
(a) place (b) crossroads (c) roundabout (d) building
Landauer and Dumais (1997) were the first to propose the TOEFL items as a test for
lexical semantic similarity. Their LSA model achieved an accuracy of 64.4% on 80 items,
a performance comparable to the average score attained by non-native speakers taking
the test. Sahlgren (2006) uses Random Indexing, a method comparable to LSA, to
represent the meaning of words and reports a 75.0% accuracy on the same TOEFL items.
It should be noted that both Landauer and Dumais and Sahlgren report results on seen
data, that is, parameters are optimized on the entire data set until performance has
peaked.
Rather than assuming that similar words tend to occur in similar contexts, Turney
(2001) and Higgins (2004) propose models that capitalize on the collocational nature of
semantically related words. Two words are considered similar if they tend to occur near
each other. Turney uses pointwise mutual information (PMI) to measure the similarity
between a target word and each of its candidate synonyms. Co-occurrence frequencies
are retrieved from the Web using an information retrieval (IR) engine:
SimilarityPMI?IR(w1, w2) =
P(w1, w2)
P(w1)P(w2)
? hits(w1 NEAR w2)
hits(w1)hits(w2)
(15)
183
Computational Linguistics Volume 33, Number 2
where P(w1, w2) is estimated by the number of hits (i.e., number of documents) returned
by the IR engine (Turney used AltaVista) when submitting a query with the NEAR
operator.8 The PMI-IR model obtained an accuracy of 72.5% on the TOEFL data set.
Higgins (2004) proposes a modification to Equation (15): He dispenses with the
NEAR operator by concentrating on word pairs that are strictly adjacent:
SimilarityLC?IR(w1, w2) =
min(hits(w1, w2), hits(w2, w1))
hits(w1)hits(w2)
(16)
Note that Equation (16) takes the minimum number of hits for the two possible orders
w1, w2 and w2, w1 in an attempt to rule out the effects of collocations and part-of-speech
ambiguities. The LC-IR (local-context information retrieval) model outperformed PMI-
IR, achieving an accuracy of 81.3% on the TOEFL items.
6.1 Method
For this experiment, we used the TOEFL benchmark data set9 (80 items). We com-
pared our optimal dependency-based model against the baseline word-based model.
We would also like to compare the vector-based models against Turney?s (2001) and
Higgins? (2004) collocational models. Ideally, such a comparison should take place on
the same corpus. Unfortunately, downloading and parsing a snapshot of the whole Web
is outside the scope of the present article. Instead, we assessed the performance of these
models on the BNC, using a search engine which simulated AltaVista. Specifically, we
indexed the BNC using Glimpse (Manber and Wu 1994), a fast and flexible indexing and
query system.10 Glimpse supports approximate and exact matching, Boolean queries,
wild cards, regular expressions, and many other options.
For the PMI-IR model, we estimated hits(w1 NEAR w2) by retrieving and counting
the number of documents containing w1 and w2 or w2 and w1 in the same sentence.
The target w1 and its candidate synonym w2 did not have to be adjacent, but the
number of the intervening words was bounded by the length of the sentence. The
frequencies hits(w1) and hits(w2) were estimated similarly by counting the number of
documents in which w1 and w2 occurred. Ties were resolved by randomly selecting
one of the candidate synonyms. The BNC proved too small a corpus for the LC-
IR model, which relies on w1 and w2 occurring in directly adjacent positions. This
is not a problem when frequencies are obtained from Web-scale corpora, but in our
case most queries retrieved no documents at all (96.6% of hits(w1, w2) and 95% of
hits(w2, w1) were zero). We thus report only the performance of the PMI-IR model on
the BNC.
The models performed a decision task similar to TOEFL test takers: They had to
decide which one of the four alternatives was synonymous with the target word. For
the vector-based models, we computed the distance between the vector representing
the candidate word and each of the candidate synonyms, and selected the candidate
with the smallest distance. Analogously, the candidate with the largest PMI-IR value
was chosen for Turney?s (2001) model. Accuracy was measured as the percentage of
8 The NEAR operator constrains the search to documents that contain w1 and w2 within ten words of one
another, in either order.
9 The items were kindly provided to us by Thomas Landauer.
10 The software can be downloaded from http://webglimpse.net/download.php.
184
Pad? and Lapata Dependency-Based Semantic Spaces
Table 5
Comparison of different models on the TOEFL synonymy task.
Model Corpus Accuracy (%)
Random baseline ? 25.0
Word-based space BNC 61.3?
Dependency space BNC 73.0?*
PMI-IR BNC 61.3?
PMI-IR Web 72.5?*
LC-IR Web 81.3?*
?Significantly better than random guessing.
*Significantly better than word-based vector model.
right decisions the model made. We also report the accuracy of a naive baseline model
which guesses synonyms at random.
In this experiment, we aim to show that the superior performance of the depen-
dency model carries over to a different task and data set. We are further interested to see
whether linguistic information (represented in our case by dependency paths) makes
up for the vast amounts of data required by the collocational models. We therefore
compare directly previously proposed Web-based similarity models with BNC-based
vector space models.
6.2 Results
Our results11 are summarized in Table 5. We used a ?2 test to determine whether the
differences in accuracy are statistically significant. Not surprisingly, all models are sig-
nificantly better than random guessing (p < 0.01). The dependency model significantly
outperforms the word-based model and PMI-IR when the latter uses BNC frequencies
(p < 0.05). PMI-IR performs comparably to our model when using Web frequencies.
The Web-based LC-IR numerically outperforms the dependency model, however the
difference is not statistically significant on the TOEFL data set (p < 1). Expectedly, Web-
based PMI-IR and LC-IR are significantly better than the word-based vector model and
the BNC-based PMI-IR (p < 0.05).
Our results show that the dependency-based model retains its advantage over the
word-based model on the synonymy detection task. On the BNC, it also outperforms
the collocation-based PMI-IR. Our interpretation is that the conceptually simpler collo-
cation models suffer from data sparseness, whereas the dependency model can profit
from the additional distributional information it incorporates. It is a matter of future
work to examine whether dependency models can carry over their advantage to larger
corpora.
Our following experiment applies the dependency space introduced in this article
to word sense disambiguation (WSD), a task which has received much attention in NLP
and is ultimately important for document understanding.
11 We omit LSA (Landauer and Dumais 1997) and Random indexing (Sahlgren 2006) from our comparison,
because these models were not evaluated on unseen data.
185
Computational Linguistics Volume 33, Number 2
7. Experiment 3: Sense Ranking
The ability to identify the intended reading of a polysemous word (the word sense)
in context is crucial for accomplishing many NLP tasks. Examples include lexicon
acquisition, discourse parsing, or metonymy resolution. Applications such as question
answering or machine translation could also benefit from large scale word sense disam-
biguation (WSD).
Given the importance of WSD for basic NLP tasks and multilingual applications,
a variety of approaches have been proposed for disambiguating word senses. To date,
most accurate WSD systems are supervised and rely on the availability of training data
(see Yarowsky and Florian [2002], Mihalcea and Edmonds [2004], and the references
therein). Although supervised methods typically achieve better performance than their
unsupervised alternatives, their applicability is limited to those words for which sense-
labeled data exists, and their accuracy is strongly correlated with the amount of labeled
data available. Furthermore, if the distribution of senses is skewed, as is often the case,
the simple heuristic of choosing the most common or predominant sense in the training
data (henceforth ?the first sense heuristic?) delivers results competitive with supervised
approaches based on local context (Hoste et al 2002).
Obtaining the first sense heuristic via annotation is obviously costly and time
consuming. More importantly, one would expect that a word?s first sense varies across
domains and text genres (the word court in legal documents will most likely mean
?tribunal? rather than ?yard?). Therefore, manual annotation must be redone for most
new languages, domains, and sense inventories. McCarthy et al (2004) show that the
annotation bottleneck can be avoided by inferring the first sense heuristic automatically
from raw text. They argue that, even though the first sense heuristic is not a WSD
method in itself, it can be usefully combined with context-based disambiguation meth-
ods in order to alleviate the data requirements for WSD. Their method builds on the
observation that a word?s distributionally similar neighbors often provide cues about its
senses. In their model, sense ranking is equivalent to quantifying the degree of similarity
between each neighbor and each sense description of a polysemous word. The sense
most similar to the neighbors is the first sense.
McCarthy et al?s (2004) approach crucially relies on the quality of the set of neigh-
bors to acquire more or less accurate first senses. In this experiment, we examine
whether the dependency-based models discussed in this article can be used for the sense
ranking task, thereby assessing their potential for practical NLP tasks. The aims of our
experiment are twofold: (1) to investigate whether our dependency-based framework
can be used to acquire distributionally similar words that differ in quality from those
obtained with word-based models and (2) to observe their impact on WSD. We first de-
scribe McCarthy et al?s sense-ranking model, which forms the basis of our experiments,
and then detail our methodology and results.
7.1 The Sense-Ranking Model
Let w be a word, N(w) = {n1, n2, . . . , nk} the set of the k most similar words to w, and
S(w) = {ws1, ws2, . . .wsn} the set of senses for w. McCarthy et al?s (2004) model assigns
each sense wsi a ?predominant sense score? PS(wsi) as follows:
PS(wsi) =
?
nj?N(w)
simdistr(w, nj) ?
simsem(wsi, nj)
?
ws
i??S(w)
simsem(wsi? , nj)
(17)
186
Pad? and Lapata Dependency-Based Semantic Spaces
where
simsem(wsi, nj) = max
wsx?S(nj )
simWN(wsi, wsx) (18)
The predominant sense of w is simply the one with the largest PS(wsi), that is, the sense
that is maximally similar to its neighbors nj ? N(w) according to Equations (17) and (18).
This sense ranking model has four free parameters: (1) the semantic space over
which distributionally similar words are acquired, (2) the measure of distributional
similarity (simdistr), (3) the number of neighbors taken into account (k), and (4) the
measure of sense similarity (simWN). The PS score combines distributional similarity
and sense similarity, taking into account both lexical knowledge gathered from corpora
and the organization and structure of the lexical resource that provides the sense inven-
tory. A large number of sense similarity measures have been developed for WordNet
and WordNet-like taxonomies. These vary from simple edge-counting (Rada, Mili, and
Bicknell 1989) to attempts to factor in peculiarities of the network structure by consid-
ering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow
1998), and density (Agirre and Rigau 1996). A number of hybrid approaches have also
been proposed that combine WordNet with corpus statistics (Resnik 1995; Jiang and
Conrath 1997).
McCarthy et al (2004) use their ranking model to automatically infer the first senses
of all nouns attested in SemCor, a subset of the Brown corpus containing 23,346 lemmas
annotated with senses according to WordNet 1.6. They acquire distributionally similar
words from a large collection of dependency relations obtained from the written part
of the BNC (90 million words) using Briscoe and Carroll?s (2002) parser. Their model
considers solely dependency paths of length one (see context selection function (5)),
and is restricted to a small set of dependency relations (verb?subject, verb?object,
noun?noun, and adjective?noun). They employ a basis mapping function that maps
paths to (r, w) tuples (see Equation (9)) and Lin?s information-theoretic similarity
measure (see Equation (3)). They obtained a type-level accuracy of 54% (a random
baseline achieved 32%) at recovering the most prevalent sense (using 50 neighbors
and either Lesk?s [1986] or Jiang and Conrath?s [1997] measures). They also used a
token disambiguator that always defaults to the automatically acquired first sense and
obtained a token-level disambiguation accuracy of 48% for Lesk (50 neighbors) and
46% for Jiang and Conrath (50 neighbors). Their baseline for this task was 24%.
7.2 Method
We replicated McCarthy et al?s (2004) study using our optimal dependency-based
model (medium context selection, length path value functions, 2,000 basis elements, Lin?s
[1998a] similarity measure, and the log-likelihood association function) and the baseline
word-based model. We used Equation (17) to find the first sense for all polysemous
nouns in SemCor (according to WordNet 1.6). Following McCarthy et al, we only
considered polysemous nouns attested in SemCor with a frequency > 2, and in our
parsed version of the BNC with a frequency ? 10. The total number of nouns after
applying the frequency cutoffs was 2,75012 and the average sense ambiguity was 4.55
12 McCarthy et al (2004) use 2,595 nouns. The slight variation is due to the different parsers employed in
the two studies. Recall that we obtain dependency relations using MINIPAR (Lin 1998b), whereas
McCarthy et al employ Briscoe and Carroll?s (2002) parser.
187
Computational Linguistics Volume 33, Number 2
(the most ambiguous word had 30 senses, and least ambiguous 2). For each one of the
2,750 nouns, we generated the set of its distributionally similar neighbors from the set
of the nouns in the intersection between the BNC and WordNet (15,656 in total).
We did not experiment in detail with WordNet-based similarity measures or with
the number of distributionally similar neighbors required for the computation of the
prevalence score. McCarthy et al (2004) undertook a thorough comparison and ob-
tained best results with 50 neighbors using Lesk?s (1986) and Jiang and Conrath?s (1997)
measures. They argue that the latter measure is more efficient for large scale WSD and
use it exclusively in all subsequent work (McCarthy et al 2004; Koeling, McCarthy, and
Carroll 2005). We thus adopted the parameters that McCarthy et al found to be optimal,
namely 50 neighbors and Jiang and Conrath?s similarity measure, which we will briefly
describe.
Jiang and Conrath?s (1997) measure estimates the similarity between two word
senses by combining taxonomic information with corpus data. It is based on the notion
of information content (IC) of a WordNet synset s. IC is defined as the negative log-
likelihood of s:
IC(s) = ? log p(s) (19)
Jiang and Conrath define a distance measure that combines IC with edge counting
by taking into account local density, node depth, and link type. They introduce two
parameters, ? and ?, that control the influence of node depth and density, respectively.
Setting ? to zero and ? to one, their measure simplifies to:
Djcn(s1, s2) = log p(s1) + log p(s2) ? 2 ? log p(lso(s1, s2)) (20)
where lso(s1, s2) is the lowest super-ordinate (most specific common subsumer) of
synsets (that is, senses) s1 and s2. We used the WordNet Similarity Package (Pedersen,
Patwardhan, and Michelizzi 2004), which provides an implementation of Jiang and
Conrath?s (1997) measure (version 0.06).13 We re-estimated the IC counts from the BNC
because those provided with the package are derived from the manually annotated
SemCor and would positively bias our results.
We replicated McCarthy et al?s (2004) procedure for evaluating the acquired pre-
dominant sense against the manually annotated SemCor. We use the following notation
to describe our evaluation measures: W is the set of all word types (|W| = 2, 570) and
Wps is the set of word types with a predominant sense, that is, with a sense that is more
frequent than the second sense in SemCor (|Wps| = 2, 338). S(w) is the set of WordNet
senses for word type w, and T(w) the set of all tokens of w. Finally, we use pssc(w) and
psr(w) to refer to the predominant sense of word w according to SemCor and the sense
ranking model, respectively, and sensesc(t) to denote the sense annotated in SemCor for
a particular token t.
We first evaluate our models performance on the sense ranking task (Accsr),
namely, on identifying the predominant sense for a word type, if one exists:
Accsr =
|{w ? Wps | pssc(w) = psr(w)}|
|Wps|
(21)
13 The package is publicly available from http://www.d.umn.edu/?tpederse/similarity.html.
188
Pad? and Lapata Dependency-Based Semantic Spaces
A baseline for the sense ranking task can be easily defined by selecting a sense at ran-
dom for each word type from its sense inventory and assuming that this is the first sense:
Randomsr = 1|Wps|
?
w ?Wps
1
|S(w)| (22)
Like McCarthy et al (2004), we also assessed the word sense disambiguation potential
(Accwsd) of the automatically acquired first senses for each word token. We assigned
the predominant sense (according to the ranking model) to every noun token, without
taking its context into account, and measured the ratio of tokens for which the first
sense given by the ranking model is identical to the SemCor gold standard sense:
Accwsd =
?
w?W
|{t ? T(w) | psr(w) = sensesc(t)}|
?
w?W
|T(w)| (23)
A baseline disambiguator can be defined by assigning a random sense to each token:
Randomwsd = 1?
w?W
|T(w)|
?
w?W
|T(w)| 1|S(w)| (24)
7.3 Results
Table 6 shows the results for the optimal dependency-based model, the random base-
line, the baseline word-based model, and McCarthy et al?s (2004) state of the art model.
As an upper bound, we report WSD accuracy when defaulting to the first (i.e., most
frequent) sense provided by SemCor. All models use 50 nearest neighbors and Jiang and
Conrath?s (1997) WordNet-based semantic similarity measure. As far as distributional
similarity is concerned, our dependency model employs Lin?s (1998a) measure and
so do McCarthy et al, whereas the traditional word co-occurrence model uses cosine.
Our model differs from McCarthy et al in the context selection, path value, and basis
mapping functions (see the subsequent discussion). We used a ?2 test to determine if
the differences in performance are statistically significant. Note that we have a slightly
different set of nouns from McCarthy et al (2004); this is due to the use of a different
Table 6
Results on sense ranking and WSD tasks, using 50 neighbors and the Jiang and Conrath (1995)
distance measure.
Models Accsr Accwsd
Random baseline 31.0 25.4
Word-based space 49.3? 49.9?$
Dependency space 54.3?* 54.3?* $
McCarthy et al 54.0?* 46.0?
Upper bound ? 67.0
?Significantly better than random baseline.
*Significantly better than word-based model.
$Significantly better than McCarthy et al
189
Computational Linguistics Volume 33, Number 2
parser and a larger corpus. We work on the assumption that this difference is negligible.
We use a set of diacritics to denote statistical significance, explanations for which are
provided in Table 6.
We first consider the predominant sense acquisition task (Accsr). Table 6 shows
that all models significantly outperform the random baseline (p < 0.01). Furthermore,
both the dependency-based model and McCarthy et al (2004) significantly outperform
the word-based model. The two dependency models yield comparable performances
(p < 1). For the WSD task, we also observe that all models significantly outperform
the random baseline (p < 0.01). Our dependency model significantly outperforms the
word-based model and McCarthy et al (p < 0.01). The word-based model performs
significantly better than McCarthy et al (p < 0.01). All models expectedly perform
worse than the upper bound (p < 0.01).
An interesting observation is that our dependency model outperforms McCarthy
et al (2004) by a large margin (8.3%) on the WSD task, whereas the two models yield
comparable performances on sense ranking. Also, the word-based model performs
significantly better than McCarthy et al on WSD, while it is significantly worse than
McCarthy et al in sense ranking. This indicates that the words for which each model
delivers the first sense correctly are different. Indeed, inspection of the first sense
assignments reveals that McCarthy et al and our dependency model have only 35.7%
nouns in common for which they predict the first sense correctly. McCarthy et al
has 34.8% nouns in common with the word-based model, which in turn has 40.3%
nouns in common with our dependency model.
To follow up on this observation, we investigated how ambiguity and word fre-
quency influence the performance of our ranking model. In theory, an automatically
acquired sense ranker should have a good accuracy on all ambiguous words in order
to do well on WSD. However, in practice the sense ranker?s performance depends
crucially on its ability to correctly predict the first sense for highly frequent and highly
ambiguous words. An additional complicating factor is the sense distribution of the
words in question. For words whose sense distributions are not particularly skewed,
getting the first sense wrong will not be entirely detrimental as long as the WSD method
misclassifies relatively frequent senses as predominant.
Take, for example, the word corner, which is attested 61 times in Semcor and has
11 senses according to WordNet 1.6. Among these, sense 1 is found seventeen times,
sense 2 fifteen, sense 3 ten, and sense 4 nine (all other senses have considerably smaller
frequencies). Now suppose that the sense ranking method wrongly identifies sense 2
as the predominant sense for corner. Using this sense, our WSD system will correctly
disambiguate 24.6% of the instances of corner in Semcor, despite the fact that it will not
receive any credit for identifying the first sense. Note that the right first sense would
yield only slightly better accuracy (i.e., 27.4%).
We grouped all ambiguous noun tokens in SemCor into five frequency bands (fre-
quencies were estimated from the BNC as it constitutes a larger sample of English than
Semcor). Table 7 illustrates our models? sense ranking and WSD accuracy according to
these bands; we also list the average sense ambiguity and number of word types for each
band. As can be seen, our dependency model obtains consistently good performance on
both tasks, even in the high ambiguity bands (Bands 1,000?5,000 and 5,000+, highlighted
in Table 7). The obtained accuracies are well above the baseline of choosing a sense at
random (for example, an average ambiguity of 8.3 in the 5000+ band corresponds to
a random baseline of 12% in the sense ranking task). This is not entirely surprising;
frequent words are represented by more reliable vectors. As a result, the acquired
neighbors are of higher quality, which counteracts the increased ambiguity.
190
Pad? and Lapata Dependency-Based Semantic Spaces
The results in Table 7 furthermore reveal that WSD performance exceeds sense
ranking accuracy in high-frequency bands (most notably in Band 1,000?5,000), which
seems counterintuitive. This effect can be explained by taking into account the observed
sense frequencies and the types of errors introduced by our model in these bands. The
distribution of senses in the high-frequency bands tends to be less skewed, at least ac-
cording to Semcor (82% of nouns in Band 1,000?5,000 and 65% in Band 5,000+ have a
first sense with frequency <50). Our model?s mistakes are often ?near misses?, that is,
the first and second sense ranks are flipped. Specifically, near misses are observed for
25% of the noun types in Band 1,000?5,000, and 15% in Band 5,000+. Now, for nouns
with non-skewed sense distributions, disambiguating with the second sense will boost
WSD accuracy even though this is not the case for sense ranking (see the previous
discussion).
Our results show that semantic space models defined according to the framework
presented in this article can be successfully used for the automatic acquisition of first
senses from raw text. We obtained results similar to McCarthy et al (2004) on the sense
ranking task and demonstrated that our model performs significantly better on WSD.
Furthermore, it outperformed a word-based semantic space on both tasks. Our model
differs from McCarthy et al in three important ways: (a) following our terminology,
they use a semantic space with the minimum context selection (paths of length one)
and plain path value (no path weighting) functions, whereas our model employs the
medium content selection and length path value functions; (b) their space is constructed
over a limited set of dependency paths, namely subject, object, and adjective-noun
modification relations, whereas our model uses a wider range of relations including
information about tense (for example, whether a complement is finite or not), relativi-
sation, etc. (see Section 4.2 for details); and (c) their basis mapping function maps paths
to tuples whereas we employ a word-based function and restrict the dimensions of the
space to the 2,000 most frequent elements (McCarthy et al do not employ any cutoffs).
Furthermore, they used a slightly smaller corpus (only the written part of the BNC,
amounting to 90% of the total corpus) and a different parser (Briscoe and Carroll 2002).
Although replicating our study with Briscoe and Carroll?s parser (2002) is outside of
the scope of this article, we should note that the two parsers yield comparable perform-
ances and employ a similar inventory of dependency relations (see Curran (2004) for
more discussion). We thus suspect that differences in WSD accuracy cannot be uniquely
attributed to parser performance. We can, however, assess whether the difference is due
to corpus size by examining its effect on the performance of our model. If it is indeed
sensitive to corpus size, we would expect a relatively large drop in performance when
Table 7
Sense ranking and WSD accuracy for the dependency-based model as word frequency and
average sense ambiguity are varied.
FBand AvgAmbig Types accsr accwsd
<50 3.29 174 0.53 0.46
50?200 3.60 489 0.54 0.49
200?1,000 4.29 1,014 0.57 0.54
1,000?5,000 5.65 583 0.51 0.57
5,000+ 8.32 78 0.50 0.51
FBand = frequency band; AvgAmbig = average WordNet sense ambiguity within frequency
band; Types = number of noun types within frequency band.
191
Computational Linguistics Volume 33, Number 2
our semantic space is built on smaller corpora. We randomized the order of sentences
in the BNC and constructed semantic spaces on data sets progressively increasing in
size: The first space was constructed from 5% of the BNC, the next from 10%, and so
on. We tested each model on the SemCor data (see Section 7.2). Figure 7 shows the
resulting learning curves. When the dependency model is constructed on 5% of the
BNC, it delivers a WSD accuracy of 51%, which eventually increases to 54.3% when
the entire corpus is used. This result indicates that the model performs well when
trained on a small corpus and that its good performance cannot be attributed solely
to corpus size. However, it also suggests that a large increase in corpus size is necessary
to obtain substantial improvements with the present sense ranking strategy, which
uses distributional similarity as a corrective for taxonomy-based similarity: Accuracy
increases by approximately 4% when our corpus size increases by a factor of 20.
We believe that the differences in performance between the two models are largely
due to differences in the basis mapping function. Because McCarthy et al (2004) use all
available basis elements, their semantic space grows linearly with vocabulary (i.e., cor-
pus) size. Each target word is represented by a set of ?features??relation?word pairs
with a non-zero occurrence frequency?which may vary widely between target words.
In contrast, our model defines a modest number of basis elements (2,000) which are
shared between all target words. The resulting representation is a vector space which is
less sparse and the resulting neighbors capture more succinctly the semantic properties
of words. Additional evidence comes from the performance of the word-based model,
which also uses a word basis mapping function and a fixed number of dimensions (500
words). Although this model does not incorporate syntactic information in any way,
it manages to outperform McCarthy et al on the WSD task. In sum, we attribute the
superior performance of the vector-based model to two key factors: low dimensionality
Figure 7
Learning curve for the dependency-based model on a randomized version for the BNC: accuracy
of predominant sense acquisition (solid) and WSD (dashed) with varying corpus size.
192
Pad? and Lapata Dependency-Based Semantic Spaces
(as seen by the comparison to McCarthy et al) and the incorporation of linguistic
knowledge (as seen by the comparison to the word-based model).
8. General Discussion
In this article, we presented a general framework for the construction of semantic space
models. The framework operates on paths of dependency relations, allowing linguistic
knowledge to guide the construction of semantic spaces. It extends previous work
on traditional word-based semantic space models as well as syntax-based models by
providing a principled way for defining the context and the dimensions of the semantic
space. More specifically, we isolated three important parameters of space construction:
the context selection function, the basis mapping function, and the path value function.
In combination, these three functions determine which paths (e.g., local or distant),
dimensions (e.g., words, parts of speech or word?relation tuples), and dependency
relations (e.g., subjects, objects) contribute towards the construction of a semantic space.
We evaluated our framework on tasks relevant for NLP and cognitive science and
compared it against state-of-the-art models. Experiment 1 revealed that semantic space
models defined over dependency relations adequately simulate semantic priming. Ex-
periments 2 and 3 examined the usefulness of our framework for NLP: we used our
model to detect synonymy relations and to automatically acquire prevalent senses for
polysemous words. In all cases, syntactically enriched models outperformed traditional
word-based models that did not take account of syntax.
Our strategy in the present study was to define a small number of generic parame-
terizations, evaluate the resulting models on a development set, and select a broadly
optimal model for testing on unseen data. Therefore, our models were not specifically
tuned for the tasks at hand and we have only explored a relatively small subset of the
parameter space. Our examination of different parameter combinations in Section 4.2
revealed that medium syntactic contents yield consistently better performance when
combined with a path value function that penalizes longer paths (length). An important
avenue for future work concerns defining more fine-grained path value functions. Our
results show that a path value function inspired by the obliqueness hierarchy delivers
worse results than the linguistically naive length function. Alternatively, we could define
a function that combines gram-rel with length, or more generally learn a weighting
scheme for paths by optimizing some objective function.
Our experiments concentrated on spaces that used solely a basis mapping function
that maps dependency paths to words. It should also be interesting to experiment with
different types of basis mapping functions. For example, we could experiment with
more coarse-grained functions based on parts-of-speech or more fine-grained ones such
as the relation?word pairs used by McCarthy et al (2004). We would also like to observe
the impact of singular value decomposition (SVD) on our semantic spaces along the
lines of Kanejiya, Kumar, and Prasad?s (2003) cognitive modeling work. They use SVD
to reduce the dimensionality of a semantic space that uses (word, part-of-speech) pairs
as basis elements, obtaining better coverage compared with an LSA space constructed
over word co-occurrences. Further studies must examine the effect of parser quality on
the obtained co-occurrences, and the influence of the chosen similarity measure.
We have just scratched the surface of the possibilities for the framework discussed
in this article. The potential applications are many and varied both for cognitive science
and NLP. Our syntactically enriched models retain the simplicity of word co-occurrence
models while allowing for the role of syntactic structure to influence the representa-
tion of the semantic space. The resulting vectors have a higher degree of linguistic
193
Computational Linguistics Volume 33, Number 2
plausibility?it is not mere lexical association that accounts for the meaning of words
but rather their lexical and syntactic dependencies. Arguably, this property holds great
promise for languages less configurational than English. A prediction that we intend to
test in the future is that syntax-based semantic space models should be able to represent
meaning more adequately than traditional word-based models for languages that allow
constituent scrambling (e.g., German) or have free word order (e.g., Czech).
It remains to be seen whether our models can capture the wide range of data that
traditional and LSA-based models have accounted for. Possible future experiments in-
clude mediated priming (Lowe and McDonald 2000) and multiple priming (McDonald
and Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherence
rating (Foltz, Kintsch, and Landauer 1998). A number of NLP tasks could also benefit
from the framework presented in this article. Examples include word sense discrimi-
nation (Lin 1998a; Sch?tze 1998), automatic thesaurus construction (Grefenstette 1994;
Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general
similarity-based approaches to NLP.
Appendix A. Context Selection Functions
In what follows we present the context selection functions we used in our experiments.
These are encoded as non-lexicalized path templates and are distributed as part of the
software package that implements our dependency-based semantic space framework
(see Section 3.7 for details). Each context selection function cont is represented by a
set of path templates, Temp(cont). Each path template directly corresponds to a path
label sequence. Path templates are denoted by a comma-separated sequence of one or
more edge labels; each edge label is a colon-separated triple POS1:relation:POS2 (see
Definition 1). The semantics of a set of path templates Temp(c) is as follows: For a target
word t and a context selection function c, the context of t consists of all paths ?t (i.e., all
paths anchored at t) so that there is a path template temp ? Temp(c) that matches the
label sequence l(?t).
Minimum:
A:amod:V
A:mod:A
A:mod:A
A:mod:N
A:mod:Prep
A:mod:V
A:subj:N
N:conj:N
N:gen:N
N:mod:A
N:mod:Prep
N:nn:N
N:obj:V
N:pcomp-n:Prep
N:subj:A
N:subj:N
N:subj:V
(null):lex-mod:V
Prep:mod:A
Prep:mod:N
Prep:mod:V
Prep:pcomp-n:N
V:amod:A
V:lex-mod:(null)
V:mod:A
V:mod:Prep
V:obj:N
V:subj:N
Medium contains all minimum
templates and:
A:mod:N,N:lex-mod:(null)
A:mod:N,N:nn:N
A:subj:N,N:lex-mod:(null)
A:subj:N,N:nn:N
N:conj:N,N:lex-mod:(null)
N:conj:N,N:nn:N
N:gen:N,N:lex-mod:(null)
N:gen:N,N:nn:N
N:nn:N,N:conj:N
N:nn:N,N:conj:N,N:nn:N
N:nn:N,N:gen:N
194
Pad? and Lapata Dependency-Based Semantic Spaces
N:nn:N,N:gen:N,N:nn:N
N:nn:N,N:mod:A
N:nn:N,N:mod:Pred
N:nn:N,N:obj:V
N:nn:N,N:subj:A
N:nn:N,N:subj:V
(null):lex-mod:N,N:conj:N
(null):lex-mod:N,N:conj:N,
N:lex-mod:(null)
(null):lex-mod:N,N:gen:N
(null):lex-mod:N,N:gen:N,
N:lex-mod:(null)
(null):lex-mod:N,N:mod:A
(null):lex-mod:N,N:mod:Pred
(null):lex-mod:N,N:obj:V
(null):lex-mod:N,N:subj:A
(null):lex-mod:N,N:subj:V
Prep:mod:N,N:lex-mod:(null)
Prep:mod:N,N:nn:N
V:obj:N,N:lex-mod:(null)
V:obj:N,N:nn:N
V:subj:N,N:lex-mod:(null)
V:subj:N,N:nn:N
Maximum contains all medium
templates and:
A:mod:A,A:mod:N,N:lex-mod:(null)
A:mod:A,A:mod:N,N:nn:N
A:mod:Prep,Prep:pcomp-n:N,
N:lex-mod:(null)
N:mod:Prep,Prep:pcomp-n:N,
N:lex-mod:(null)
N:mod:Prep,Prep:pcomp-n:N,
N:nn:N
N:nn:N,N:mod:A,A:mod:A
N:nn:N,N:mod:Prep,Prep:pcomp-n:N
N:nn:N,N:mod:Prep,Prep:pcomp-n:N,
N:nn:N
N:nn:N,N:obj:V,V:subj:N
N:nn:N,N:obj:V,V:subj:N,N:nn:N
N:nn:N,N:pcomp-n:Prep
N:nn:N,N:pcomp-n:Prep,Prep:mod:N
N:nn:N,N:pcomp-n:Prep,Prep:mod:N,
N:nn:N
N:nn:N,N:subj:V,V:obj:N
N:nn:N,N:subj:V,V:obj:N,N:nn:N
N:nn:N,V:s:C,C:fc:V
N:obj:V,V:subj:N,N:lex-mod:(null)
N:obj:V,V:subj:N,N:nn:N
N:pcomp-n:Prep,Prep:mod:N,
N:lex-mod:(null)
N:pcomp-n:Prep,Prep:mod:N,N:nn:N
N:subj:V,V:obj:N,N:lex-mod:(null)
N:subj:V,V:obj:N,N:nn:N
(null):lex-mod:N,N:mod:A,A:mod:A
(null):lex-mod:N,N:mod:Prep,
Prep:pcomp-n:N
(null):lex-mod:N,N:mod:Prep,
Prep:pcomp-n:N,N:lex-mod:(null)
(null):lex-mod:N,N:obj:V,V:subj:N
(null):lex-mod:N,N:obj:V,
V:subj:N,N:lex-mod:(null)
(null):lex-mod:N,N:pcomp-n:Pred,
Prep:mod:A
(null):lex-mod:N,N:pcomp-n:Prep
(null):lex-mod:N,N:pcomp-n:Prep,
Prep:mod:N
(null):lex-mod:N,N:pcomp-n:Prep,
Prep:mod:N,N:lex-mod:(null)
(null):lex-mod:N,N:pcomp-n:Prep,
Prep:mod:V
(null):lex-mod:N,N:rel:C,C:i:V
(null):lex-mod:N,N:subj:V,V:obj:N
(null):lex-mod:N,N:subj:V,V:obj:N,
N:lex-mod:(null)
(null):lex-mod:N,V:s:C,C:fc:V
Prep:pcomp-n:N,N:lex-mod:(null)
Prep:pcomp-n:N,N:nn:N
V:fc:C,C:s:N,N:lex-mod:(null)
V:fc:C,C:s:N,N:nn:N
V:i:C,C:rel:N,N:lex-mod:(null)
V:mod:Prep,Prep:pcomp-n:N,
N:lex-mod:(null)
Acknowledgments
We are grateful to Diana McCarthy for
providing us with the results of her system
on our data. We are also grateful to four
anonymous reviewers for Computational
Linguistics whose feedback helped to
substantially improve the present article. We
also thank Colin Bannard, Gemma Boleda,
Amit Dubey, Katrin Erk, Frank Keller, Ulrike
Pad?, and Caroline Sporleder for useful
comments and suggestions. A preliminary
version of this work was published in the
proceedings of ACL 2003; we thank the
anonymous reviewers of that paper for their
comments.
References
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings
of the 16th International Conference on
Computational Linguistics, pages 16?22,
Copenhagen, Denmark.
Banerjee, Satanjeev and Ted Pedersen. 2003.
Extended gloss overlaps as a measure
of semantic relatedness. In Proceedings
of the 18th International Joint Conference
on Artificial Intelligence, pages 805?810,
Acapulco, Mexico.
Bannard, Colin, Timothy Baldwin, and
Alex Lascarides. 2003. A statistical
195
Computational Linguistics Volume 33, Number 2
approach to the semantics of
verb-particles. In Proceedings of the ACL
Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment,
pages 65?72, Sapporo, Japan.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization:
Paraphrasing and Generation.Ph.D. thesis,
Columbia University, New York.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O?Brien. 1994. Using linear
algebra for intelligent information
retrieval. SIAM Review, 37(4):573?595.
Briscoe, Ted and John Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 1499?1504, Las Palmas,
Canary Islands.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in WordNet:
An experimental, application-oriented
evaluation of five measures. In Proceedings
of ACL Workshop on WordNet and
Other Lexical Resources, pages 29?34,
Pittsburgh, PA.
Burnard, Lou, 1995. Users Guide for the British
National Corpus. British National Corpus
Consortium, Oxford University
Computing Service, Oxford, UK.
Choi, Freddy, Peter Wiemer-Hastings,
and Johanna Moore. 2001. Latent
Semantic Analysis for text segmentation.
In Proceedings of the 6th Conference on
Empirical Methods in Natural Language
Processing, pages 109?117, Seattle, WA.
Curran, James R. 2004. From Distributional
to Semantic Similarity. Ph.D. thesis,
University of Edinburgh.
Curran, James R. and Marc Moens. 2002.
Scaling context space. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 231?238,
Philadelphia, PA.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Erkan, G?nes and Dragomir R. Radev.
2004. Lexrank: Graph-based centrality as
salience in text summarization. Journal of
Artificial Intelligence Research, 22:457?479.
Fillmore, Charles. 1965. Indirect Object
Constructions and the Ordering of
Transformations. Mouton, The Hague.
Fodor, Janet Dean. 1995. Comprehending
sentence structure. In Lila R. Gleitman and
Mark Liberman, editors, Invitation to
Cognitive Science, volume 1. MIT Press,
Cambridge, MA, pages 209?246.
Foltz, Peter W., Walter Kintsch, and
Thomas K. Landauer. 1998. The
measurement of textual coherence
with latent semantic analysis. Discourse
Process, 15:285?307.
Goldberg, Adele. 1995. Constructions.
Chicago University Press, Chicago.
Golub, Gene H. and Charles F. Van Loan.
1989. Matrix Computations. Johns Hopkins
Series in the Mathematical Sciences. Johns
Hopkins University Press, Baltimore,
3rd edition.
Green, Georgia. 1974. Semantics and Syntactic
Regularity. Indiana University Press,
Bloomington.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht.
Gropen, Jess, Steven Pinker, Michelle
Hollander, Richard Goldberg, and
Ronald Wilson. 1989. The learnability
and acquisition of the dative alternation.
Language, 65(2):203?257.
Harris, Zellig. 1968. Mathematical Structures
of Language. Wiley, New York.
Henderson, James, Paola Merlo, Ivan Petroff,
and Gerold Schneider. 2002. Using
syntactic analysis to increase efficiency in
visualizing text collections. In Proceedings
of the 19th International Conference on
Computational Linguistics, pages 335?341,
Taipei, Taiwan.
Higgins, Derrick. 2004. Which statistics
reflect semantics? Rethinking synonymy
and word similarity. In Proceedings of the
International Conference on Linguistic
Evidence, pages 265?284, T?bingen,
Germany.
Hirst, Graeme and David St-Onge. 1998.
Lexical chains as representations of
context for the detection and correction of
malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA,
pages 305?332.
Hodgson, James M. 1991. Informational
constraints on pre-lexical priming.
Language and Cognitive Processes, 6:169?205.
Hoste, V?ronique, Iris Hendrickx, Walter
Daelemans, and Antal van den Bosch.
2002. Parameter optimization for
machine-learning of word sense
disambiguation. Language Engineering,
8(4):311?325.
Howell, David C. 2002. Statistical Methods
for Psychology. Duxbury, Pacific Grove,
CA, 5th edition.
Jackendoff, Ray. 1983. Semantic and Cognition.
The MIT Press, Cambridge, MA.
196
Pad? and Lapata Dependency-Based Semantic Spaces
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the 10th International
Conference on Research in Computational
Linguistics, pages 19?33, Taipei, Taiwan.
Jones, Michael P. and James H. Martin.
1997. Contextual spelling correction using
Latent Semantic Analysis. In Proceedings
of the 5th Conference on Applied Natural
Language Processing, pages 166?173,
Washington, DC.
Kanejiya, Dharmendra, Arun Kumar,
and Surendra Prasad. 2003. Automatic
evaluation of students? answers using
syntactically enhanced LSA. In Proceedings
of the HLT-NAACL Workshop on Building
Educational Applications Using Natural
Language Processing, pages 53?60,
Edmonton, Canada.
Keenan, Edward and Bernard Comrie. 1977.
Noun phrase accessibility and universal
grammar. Linguistic Inquiry, 8:62?100.
Kilgarriff, Adam. 2001. Comparing corpora.
International Journal of Corpus Linguistics,
6(1):97?133.
Koeling, Rob, Diana McCarthy, and John
Carroll. 2005. Domain-specific sense
distributions and predominant sense
acquisition. In Proceedings of the Joint
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 419?426,
Vancouver, Canada.
Landauer, Thomas and Susan T. Dumais.
1997. A solution to Plato?s problem:
The latent semantic analysis theory of
acquisition, induction, and representation
of knowledge. Psychological Review,
104(2):211?240.
Leacock, Claudia and Martin Chodorow.
1998. Combining local context and
WordNet similarity for word sense
identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA,
pages 265?283.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 25?32, College Park, MA.
Lesk, Michael. 1986. Automatic sense
disambiguation: How to tell a pine cone
from an ice cream cone. In Proceedings of
the 1986 Special Interest Group in
Documentation, pages 24?26, New York.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levy, Joseph P. and John A. Bullinaria. 2001.
Learning lexical properties from word
usage patterns. In Robert M. French and
Jacques P. Sougn?, editors, Connectionist
Models of Learning, Development, and
Evolution. Springer, pages 273?282.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words. In
Proceedings of the Joint Annual Meeting
of the Association for Computational
Linguistics and International Conference on
Computational Linguistics, pages 768?774,
Montr?al, Canada.
Lin, Dekang. 1998b. Dependency-based
evaluation of MINIPAR. In Proceedings
of the LREC Workshop on the Evaluation
of Parsing Systems, pages 234?241,
Granada, Spain.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics,
pages 317?324, College Park, MA.
Lin, Dekang. 2001. LaTaT: Language and text
analysis tools. In Proceedings of the 1st
Human Language Technology Conference,
pages 222?227, San Francisco, CA.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):342?360.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of
the 23rd Annual Conference of the
Cognitive Science Society, pages 576?581,
Edinburgh, UK.
Lowe, Will and Scott McDonald. 2000. The
direct route: Mediated priming in semantic
space. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society,
pages 675?680, Philadelphia, PA.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Manber, Udi and Sun Wu. 1994. GLIMPSE: a
tool to search through entire file systems.
In Proceedings of the USENIX Winter 1994
Technical Conference, pages 23?32, San
Francisco, CA.
Manning, Chris and Hinrich Sch?tze. 1999.
Foundations of Statistical Natural Language
Processing. MIT Press, Cambridge, MA.
McCarthy, Diana, Bill Keller, and John
Carroll. 2003. Detecting a continuum
of compositionality in phrasal verbs.
In Proceedings of the ACL Workshop
on Multiword Expressions: Analysis,
197
Computational Linguistics Volume 33, Number 2
Acquisition and Treatment, pages 73?80,
Sapporo, Japan.
McCarthy, Diana, Rob Koeling, Julie
Weeds, and John Carroll. 2004. Finding
predominant senses in untagged text. In
Proceedings of the 42th Annual Meeting of the
Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
McDonald, Scott. 2000. Environmental
Determinants of Lexical Processing Effort.
Ph.D. thesis, University of Edinburgh.
McDonald, Scott and Chris Brew. 2004. A
distributional model of semantic context
effects in lexical processing. In Proceedings
of the 42th Annual Meeting of the Association
for Computational Linguistics, pages 17?24,
Barcelona, Spain.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text,
Barcelona, Spain.
Miltsakaki, Eleni. 2003. The Syntax-Discourse
Interface: Effects of the Main-Subordinate
Distinction on Attention Structure. Ph.D.
thesis, University of Pennsylvania.
Morris, Robin K. 1994. Lexical and
message-level sentence context effects
on fixation times in reading. Journal of
Experimental Psychology: Learning,
Memory, and Cognition, (20):92?103.
Neville, Helen, Janet L. Nichol, Andrew
Barss, Kenneth I. Forster, and Merrill F.
Garrett. 1991. Syntactically based sentence
prosessing classes: Evidence form
event-related brain potentials. Journal of
Cognitive Neuroscience, 3:151?165.
Patel, Malti, John A. Bullinaria, and
Joseph P. Levy. 1998. Extracting
semantic representations from large
text corpora. In Proceedings of the
4th Neural Computation and Psychology
Workshop: Connectionist Representations,
pages 199?212, London.
Pedersen, Ted, Siddharth Patwardhan,
and Jason Michelizzi. 2004.
WordNet::Similarity?measuring the
relatedness of concepts. In Proceedings of
the Joint Human Language Technology
Conference and Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 38?41,
Boston, MA.
Pinker, Steven. 1989. Learnability and
Cognition: The Acquisition of Argument
Structure. The MIT Press, Cambridge, MA.
Rada, Roy, Hafedh Mili, and Ellen Bicknell.
1989. Development and application of a
metric on semantic nets. IEEE Transactions
on Systems, Man, and Cybernetics,
19(1):17?30.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity.
In Proceedings of 14th International Joint
Conference on Artificial Intelligence,
pages 448?453, Montr?al, Canada.
Rubenstein, Herbert and John B.
Goodenough. 1965. Contextual correlates
of synonymy. Communications of the ACM,
8(10):627?633.
Sahlgren, Magnus. 2006. The Word-Space
Model: Using Distributional Analysis to
Represent Syntagmatic and Paradigmatic
Relations Between Words in
High-Dimensional Vector Spaces. Ph.D.
thesis, Stockholm University.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Salton, Gerard and Maria Smith. 1989. On
the application of syntactic methodologies
in automatic text indexing. In Proceedings
of the 12th ACM SIGIR Conference,
pages 137?150, Cambridge, MA.
Salton, G., A. Wang, and C. Yang.
1975. A vector-space model for
information retrieval. Journal of the
American Society for Information Science,
18:613?620.
Sampson, Geoffrey R. 1995. English for the
Computer. Oxford University Press,
Oxford.
Sch?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
Strzalkowski, Tomek, editor. 1999. Natural
Language Information Retrieval. Kluwer
Academic Publishers, Dordrecht.
Talmy, L. 1985. Lexicalisation patterns:
Semantic structure in lexical forms.
In T. Shopen, editor, Language
Typology and Syntactic Description III:
Grammatical Categories and the Lexicon.
Cambrige University Press, Cambridge,
pages 57?149.
Tesni?re, Lucien. 1959. Elements de syntaxe
structurale. Klincksieck, Paris.
Turney, Peter D. 2001. Mining the Web
for synonyms: PMI-IR versus LSA
on TOEFL. In Proceedings of the 12th
European Conference on Machine
Learning, pages 491?502, Freiburg,
Germany.
Voorhees, Ellen M. 1999. Natural language
processing and information retrieval.
In 2nd School on Information Extraction
(SCIE99), pages 32?48, Frascati (Rome),
Italy.
198
Pad? and Lapata Dependency-Based Semantic Spaces
Weeds, Julie. 2003. Measures and Applications
of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex, UK.
West, R. F. and K. E. Stanovich. 1986. Robust
effects of syntactic structure on visual
word processing. Journal of Memory and
Cognition, 14:104?112.
Widdows, Dominic. 2003. Unsupervised
methods for developing taxonomies by
combining syntactic and statistical
information. In Proceedings of the Joint
Human Language Technology Conference
and Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 197?204, Edmonton,
Canada.
Wiemer-Hastings, Peter and Iraide
Zipitria. 2001. Rules for syntax, vectors
for semantics. In Proceedings of the
23rd Annual Conference of the Cognitive
Science Society, pages 1140?1145,
Edinburgh, UK.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation across
diverse parameter spaces. Natural
Language Engineering, 9(4):293?310.
199

Discourse Constraints for
Document Compression
James Clarke?
University of Illinois at
Urbana-Champaign
Mirella Lapata??
University of Edinburgh
Sentence compression holds promise for many applications ranging from summarization to
subtitle generation. The task is typically performed on isolated sentences without taking the
surrounding context into account, even though most applications would operate over entire
documents. In this article we present a discourse-informed model which is capable of producing
document compressions that are coherent and informative. Our model is inspired by theories of
local coherence and formulated within the framework of integer linear programming. Experimen-
tal results show significant improvements over a state-of-the-art discourse agnostic approach.
1. Introduction
Recent years have witnessed increasing interest in sentence compression. The task
encompasses automatic methods for shortening sentences with minimal information
loss while preserving their grammaticality. The popularity of sentence compression is
largely due to its relevance for applications. Summarization is a case in point here. Most
summarizers to date aim to produce informative summaries at a given compression
rate. If we can have a compression component that reduces sentences to a minimal
length and still retains the most important content, then we should be able to pack more
information content into a fixed size summary. In other words, sentence compression
would allow summarizers to increase the overall amount of information extracted
without increasing the summary length (Lin 2003; Zajic et al 2007). It could also be
used as a post-processing step in order to render summaries more coherent and less
repetitive (Mani, Gates, and Bloedorn 1999).
Beyond summarization, a sentence compression module could be used to display
text on small screen devices such as PDAs (Corston-Oliver 2001) or as a reading aid
for the blind (Grefenstette 1998). Sentence compression could also benefit information
retrieval by eliminating extraneous information from the documents indexed by the
? Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N Goodwin Ave,
Urbana, IL 61801, USA. E-mail: clarkeje@illinois.edu.
?? School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK.
E-mail: mlap@inf.ed.ac.uk.
Submission received: 10 September 2008; revised submission received: 27 October 2009; accepted for
publication: 6 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
retrieval engine. This way it would be possible to store less information in the index
without dramatically affecting retrieval performance (Olivers and Dolan 1999).
In theory, sentence compression may involve several rewrite operations such as
deletion, substitution, insertion, and word reordering. In practice, however, the task
is commonly defined as a word deletion problem: Given an input sentence of words
x = x1, x2, . . . , xn, the aim is to produce a compression by removing any subset of these
words (Knight and Marcu 2002). Many sentence compression models aim to learn dele-
tion rules from a parsed parallel corpus of source sentences and their target compres-
sions (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007;
Cohn and Lapata 2009). For example, Knight and Marcu (2002) learn a synchronous
context-free grammar (Aho and Ullman 1969) from such a corpus. The grammar rules
have weights (essentially probabilities estimated using maximum likelihood) and are
used to find the best compression from the set of all possible compressions for a given
sentence. Other approaches exploit syntactic information without making explicit use
of a parallel grammar?for example, by learning which words or constituents to delete
from a parse tree (Riezler et al 2003; Nguyen et al 2004; McDonald 2006; Clarke and
Lapata 2008).
Despite differences in formulation and training requirements (some approaches
require a parallel corpus, whereas others do not), existing models are similar in that
they compress sentences in isolation without taking their surrounding context into
account. This is in marked contrast with common practice in summarization. Pro-
fessional abstractors often rely on contextual cues while creating summaries (Endres-
Niggemeyer 1998). This is true of automatic summarization systems too, which consider
the position of a sentence in a document and how it relates to its surrounding sentences
(Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and
Moens 2002). Determining which information is important in a sentence is not merely
a function of its syntactic position (e.g., deleting the verb or the subject of a sentence is
less likely). A variety of contextual factors can play a role, such as the discourse topic,
whether the sentence introduces new entities or events that have not been mentioned
before, or the reader?s background knowledge.
A sentence-centric view of compression is also at odds with most relevant appli-
cations which aim to create a shorter document rather than a single sentence. The
resulting document must not only be grammatical but also coherent if it is to function as
a replacement for the original. However, this cannot be guaranteed without knowledge
of how the discourse progresses from sentence to sentence. To give a simple example, a
contextually aware compression system could drop a word or phrase from the current
sentence, simply because it is not mentioned anywhere else in the document and is
therefore deemed unimportant. Or it could decide to retain it for the sake of topic
continuity.
In this article we are interested in creating a compression model that is appropriate
for both documents and sentences. Luckily, a variety of discourse theories have been
developed over the years (e.g., Mann and Thompson, 1988; Grosz, Weinstein, and Joshi
1995; Halliday and Hasan 1976) and have found application in summarization (Barzilay
and Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generation
applications (Scott and de Souza 1990; Kibble and Power 2004). In creating a context-
sensitive compression model we are faced with three important questions: (1) Which
type of discourse information is useful for compression? (2) Is it amenable to automatic
processing (there is little hope for interfacing our compression model with applications
if discourse-level cues cannot be identified robustly)? and (3) How are sentence- and
document-based information best integrated in a unified modeling framework?
412
Clarke and Lapata Discourse Constraints for Document Compression
In building our compression model we borrow insights from two popular models
of discourse, Centering Theory (Grosz, Weinstein, and Joshi 1995) and lexical chains
(Morris and Hirst 1991). Both approaches capture local coherence?the way adjacent
sentences bind together to form a larger discourse. They also both share the view that
discourse coherence revolves around discourse entities and the way they are intro-
duced and discussed. We first automatically augment our documents with annotations
pertaining to centering and lexical chains, which we subsequently use to inform our
compression model. The latter is an extension of the integer linear programming for-
mulation proposed by Clarke and Lapata (2008). In a nutshell, sentence compression is
modeled as an optimization problem. Given a long sentence, a compression is formed
by retaining the words that maximize a scoring function coupled with a small number
of constraints ensuring that the resulting output is grammatical. The constraints are en-
coded as linear inequalities whose solution is found using integer linear programming
(ILP; Winston and Venkataramanan 2003; Vanderbei 2001). Discourse-level information
can be straightforwardly incorporated by slightly changing the compression objective?
we now wish to compress entire documents rather than isolated sentences?and aug-
menting the constraint set with discourse-specific constraints. We use our model to
compress whole documents (rather than sentences sequentially) and evaluate whether
the resulting text is understandable and informative using a question-answering task.
We show that our method yields significant improvements over discourse agnostic
state-of-the-art compression models (McDonald 2006; Clarke and Lapata 2008).
The remainder of this article is organized as follows. Section 2 provides an overview
of related work. In Section 3 we present the ILP framework and compression model we
employ in our experiments. We introduce our discourse-related extensions in Sections 4
and 5. Section 6 discusses our experimental set-up and evaluation methodology. Our
results are presented in Section 7. Discussion of future work concludes the paper.
2. Related Work
Sentence compression has been extensively studied across different modeling para-
digms and has received both generative and discriminative formulations. Most gen-
erative approaches (Knight and Marcu 2002; Turner and Charniak 2005; Galley and
McKeown 2007) are instantiations of the noisy-channel model, whereas discriminative
formulations include decision-tree learning (Knight and Marcu 2002), maximum en-
tropy (Riezler et al 2003), support vector machines (Nguyen et al 2004), and large-
margin learning (McDonald 2006; Cohn and Lapata 2009). These models are trained
on a parallel corpus and learn either which constituents to delete or which words to
place adjacently in the compression output. Relatively few approaches dispense with
the parallel corpus and generate compressions in an unsupervised manner using either
a scoring function (Hori and Furui 2004; Clarke and Lapata 2008) or compression rules
that are approximated from a non-parallel corpus such as the Penn Treebank (Turner
and Charniak 2005).
The majority of sentence compression approaches only look at sentences in isolation
without taking into account any discourse information. However, there are two notable
exceptions. Jing (2000) uses information from the local context as evidence for and
against the removal of phrases during sentence compression. The idea here is that
words or phrases which have more links to the surrounding context are more indicative
of its topic, and thus should not be dropped. The topic is not explicitly identified;
instead the importance of each phrase is determined by the number of lexical links
within the local context. A link is created between two words if they are repetitions,
413
Computational Linguistics Volume 36, Number 3
morphologically related, or associated in WordNet (Fellbaum 1998) through a lexical
relation (e.g., hyponymy, synonymy). Links have weights?for example, repetition is
considered more important than hypernymy. Each word is assigned a context weight
based on the number of links to the local context and the importance of each relation
type. Phrases are scored by the sum of their children?s context scores. The decision to
drop a phrase is influenced by several factors, besides the local context, such as the
phrase?s grammatical role and previous evidence from a parallel corpus.
Daume? III and Marcu (2002) generalize sentence compression to document com-
pression. Given a document D = w1, w2, . . . , wn the goal is to produce a summary, S, by
dropping any subset of words from D. Their system uses the discourse structure of a
document and the syntactic structure of each of its sentences in order to decide which
words to drop. Specifically, they extend Knight and Marcu?s (2002) noisy-channel model
so that it can be applied to entire documents. In its simpler sentence compression instan-
tiation, the noisy-channel model has two components, a language model and a channel
model, both of which act on probabilistic context-free grammar (PCFG) representations.
Daume? III and Marcu define a noisy-channel model over syntax and discourse trees.
Following Rhetorical Structure Theory (RST; Mann and Thompson 1988), they represent
documents by trees whose leaves correspond to elementary discourse units (edus) and
whose nodes specify how these and larger units (e.g., multi-sentence segments) are
linked to each other by rhetorical relations (e.g., Contrast, Elaboration). Discourse units
are further characterized in terms of their text importance: nuclei denote central seg-
ments, whereas satellites denote peripheral ones. Their model therefore learns not only
which syntactic constituents to drop but also which discourse units are unimportant.
While Daume? III and Marcu (2002) present a hybrid summarizer that can simulta-
neously delete words and sentences from a document, the majority of summarization
systems to date simply select and present to the user the most important sentences in
a text (see Mani [2001] for a comprehensive overview of the methods used to achieve
this). Discourse-level information plays a prominent role here as the overall document
organization can indicate whether a sentence should be included in the summary. A
variety of approaches have focused on cohesion (Halliday and Hasan 1976) and the
way it is expressed in discourse. The term broadly describes a variety of linguistic
devices responsible for making the elements of a text appear unified or connected.
Examples include word repetition, anaphora, ellipsis, and the use of synonyms or
superordinates. The underlying assumption is that sentences connected to many other
sentences are likely to carry salient information and should therefore be included
in the summary (Sjorochod?ko 1972). In exploiting cohesion for summarization, it is
necessary to somehow represent cohesive ties. For instance, Boguraev and Kennedy
(1997) represent cohesion in terms of anaphoric relations, whereas Barzilay and Elhadad
(1997) operationalize cohesion via lexical chains?sequences of related words spanning
a topical unit (Morris and Hirst 1991). Besides repetition, they also examine semantic
relations based on synonymy, antonymy, hypernymy, and holonymy (we discuss their
approach in more detail in Section 4.1).
Other approaches characterize the document in terms of discourse structure
and rhetorical relations. Documents are commonly represented as trees (Mann and
Thompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Carlson et al 2001)
and the position of a sentence in a tree is indicative of its importance. To give an ex-
ample, Marcu (2000) proposes a summarization algorithm based on RST. Assuming
that nuclei are more salient than satellites, the importance of sentential or clausal units
can be determined based on tree depth. Alternatively, discourse structure can be repre-
sented as a graph (Wolf and Gibson 2004) and sentence importance is determined in
414
Clarke and Lapata Discourse Constraints for Document Compression
graph-theoretic terms, by using graph connectivity measures such as in-degree or
PageRank (Brin and Page 1998). Although a great deal of research in summarization
has focused on global properties of discourse structure, there is evidence that local
coherence may also be useful without the added complexity of computing discourse
representations. (Unfortunately, discourse parsers have yet to achieve levels of perfor-
mance comparable to syntactic parsers.) Teufel and Moens (2002) identify discourse
relations on a sentence-by-sentence basis without presupposing an explicit discourse
structure. Inspired by Centering Theory (Grosz, Weinstein, and Joshi 1995)?a theory
of local discourse structure that models the interaction of referential continuity and
salience of discourse entities?Ora?san (2003) proposes a summarization algorithm that
extracts sentences with at least one entity in common. The idea here is that summaries
containing sentences referring to the same entity will be more coherent. Other work
has relied on centering not so much to create summaries but to assess whether they are
readable (Barzilay and Lapata 2008).
Our approach differs from previous sentence compression approaches in three
key respects. First, we present a compression model that is contextually aware; decisions
on whether to remove or retain a word (or phrase) are informed by its discourse prop-
erties (e.g., whether it introduces a new topic, or whether it is semantically related to the
previous sentence). Unlike Jing (2000) we explicitly identify topically important words
and assume specific representations of discourse structure. Secondly, in contrast to
Daume? III and Marcu (2002) and other summarization work, we adopt a less global
and more shallow representation of discourse based on Centering Theory and lexical
chains. One of our aims is to exploit discourse features that can be computed efficiently
and relatively cheaply. Thirdly, our compression model can be applied to isolated
sentences as well as to entire documents. We claim the latter is more in the spirit of real-
world applications where the goal is to generate a condensed and coherent text. Unlike
Daume? III and Marcu (2002) our model can delete words but not sentences, although it
could be used to compress documents of any type, even summaries.
3. The Compression Model
Our model is an extension of the approach put forward in Clarke and Lapata (2008)
where they formulate sentence compression as an optimization problem. Given a long
sentence, a compression is created by retaining the words that maximize a scoring func-
tion. The latter is essentially a language model coupled with a few constraints ensuring
that the resulting output is grammatical. The language model and the constraints are
encoded as linear inequalities whose solution is found using ILP.1
Their model is a good point of departure for studying document-based compres-
sion. As it does not require a parallel corpus, it can be ported across domains and
text genres, while delivering state-of-the-art results (see Clarke and Lapata [2008] for
details). Importantly, discourse-level information can be easily incorporated in two
ways: Firstly, by applying the compression objective to entire documents rather than
individual sentences; and secondly, by augmenting the constraint set with discourse-
related information. This is not the case for other approaches (e.g., those based on
the noisy channel model) where compression is modeled by grammar rules indicating
which constituents to delete in a syntactic context. Moreover, ILP delivers a globally
1 It is outside the scope of this article to provide an introduction to ILP. We refer the interested reader to
Winston and Venkataramanan (2003) and Vanderbei (2001) for comprehensive overviews.
415
Computational Linguistics Volume 36, Number 3
optimal solution by searching over the entire compression space2 without employing
heuristics or approximations during decoding (see Turner and Charniak [2005] and
McDonald [2006] for examples).
Besides sentence compression, the ILP modeling framework has been applied to
a wide range of natural language processing tasks demonstrating improvements over
more traditional methods. Examples include reluctant paraphrasing (Dras 1997), rela-
tion extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al 2004),
concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006),
dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and
coreference resolution (Denis and Baldridge 2007).
In the following we describe Clarke and Lapata?s (2008) model in more detail.
Sections 4?5 present our extensions and modifications.
3.1 Language Model
Let x = x0, x1, x2, . . . , xn denote a source sentence for which we wish to generate a target
compression. We use x0 to denote the ?start? token. We introduce a decision variable
for each word in the source and constrain it to be binary; a value of 0 represents a word
being dropped, whereas a value of 1 includes the word in the target compression. Let:
?i =
{
1 if xi is in the compression
0 otherwise
?i ? [1 . . . n]
A trigram language model forms the backbone of the compression model. The language
model is formulated as an integer linear program with the introduction of extra decision
variables indicating which word sequences should be retained or dropped from the
compression. Let:
?i =
{
1 if xi starts the compression
0 otherwise
?i ? [1 . . . n]
?ij =
?
?
?
1 if sequence xi, xj ends
the compression ?i ? [0 . . . n ? 1]
0 otherwise ?j ? [i + 1 . . . n]
?ijk =
?
?
?
1 if sequence xi, xj, xk ?i ? [0 . . . n ? 2]
is in the compression ?j ? [i + 1 . . . n ? 1]
0 otherwise ?k ? [j + 1 . . . n]
The objective function is expressed in Equation (1). It is the sum of all possible trigrams
multiplied by the appropriate decision variable where n is the length of the sentence
(note all probabilities throughout this paper are log-transformed). The objective func-
tion also includes a significance score I(xi) for each word xi multiplied by the decision
2 For a sentence of length n, there are 2n compressions.
416
Clarke and Lapata Discourse Constraints for Document Compression
variable for that word (see the first summation term in Equation (1)). This score high-
lights important content words in a sentence and is defined in Section 3.2.
max z =
n
?
i=1
?i ? ?I(xi) +
n
?
i=1
?i ? P(xi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k=j+1
?ijk ? P(xk|xi, xj)
+
n?1
?
i=0
n
?
j=i+1
?ij ? P(end|xi, xj)
??min ? ? ? ?max ? ? (1)
Note that we add a weighting factor, ?, to the objective, in order to counterbalance the
importance of the language model and the significance score.
The final component of our objective function, ? ? ?, relates to the compression rate.
As we explain shortly (Equations (7) and (8)) the compressions our model generates
are subject to a prespecified compression rate. For instance we may wish to create com-
pressions at a minimum rate of 40% and maximum rate of 70%. The compression rate
constraint can be violated with a penalty, ?, which applies to each word. ?min counts the
number of words under the compression rate and ?max the number of words over the
compression rate. Thus, the more the output violates the compression rate, the larger
the penalty will be. In other words, the term ?min ? ? ? ?max ? ? acts as a soft constraint
providing a means to guide the compression towards the desired rate. The violation
penalty ? is tuned experimentally and may vary depending on the desired compression
rate or application.
The objective function in Equation (1) allows any combination of trigrams to be
selected. As a result, invalid trigram sequences (e.g., two or more trigrams containing
the ?end? token) could appear in the target compression. We avoid this situation by
introducing sequential constraints (on the decision variables ?i, ?ijk, ?i, and ?ij) that
restrict the set of allowable trigram combinations.
Constraint 1. Exactly one word can begin a sentence.
n
?
i=1
?i = 1 (2)
Constraint 2. If a word is included in the sentence it must either start the sentence or be
preceded by two other words or one other word and the ?start? token x0.
?k ? ?k ?
k?2
?
i=0
k?1
?
j=i+1
?ijk = 0 (3)
?k : k ? [1 . . . n]
417
Computational Linguistics Volume 36, Number 3
Constraint 3. If a word is included in the sentence it must either be preceded by one word
and followed by another or it must be preceded by one word and end the sentence.
?j ?
j?1
?
i=0
n
?
k=j+1
?ijk ?
j?1
?
i=0
?ij = 0 (4)
?j : j ? [1 . . . n]
Constraint 4. If a word is in the sentence it must be followed by two words or followed
by one word and then the end of the sentence or it must be preceded by one word and
end the sentence.
?i ?
n?1
?
j=i+1
n
?
k=j+1
?ijk ?
n
?
j=i+1
?ij ?
i?1
?
h=0
?hi = 0 (5)
?i : i ? [1 . . . n]
Constraint 5. Exactly one word pair can end the sentence.
n?1
?
i=0
n
?
j=i+1
?ij = 1 (6)
Note that Equations (2)?(6) are merely well-formedness constraints and differ from the
compression-specific constraints which we discuss subsequently. Any language model
formulated as an ILP would require similar constraints.
Compression rate constraints. Depending on the application or the task at hand, we
may require that the compressions fall within a specific compression rate. We assume
here that our model is given a compression rate range, cmin% ? cmax%, and create two
constraints that penalize compressions which do not fall within this range:
n
?
i=0
?i + ?min ? cmin ? n (7)
n
?
i=0
?i ? ?max ? cmax ? n (8)
Here, ?i is still a decision variable for each word, n is the number of words in the
sentence, ? is the number of words over or under the compression rate, and cmin and
cmax are the limits of the range.
3.2 Significance Score
The significance score is an attempt at capturing the gist of a sentence. The score has
two components which correspond to document and sentence importance, respectively.
Given a sentence and its syntactic parse, we define I(xi) as:
I(xi) = fi log
Fa
Fi
? lN (9)
418
Clarke and Lapata Discourse Constraints for Document Compression
where xi is a topic word, fi is xi?s document frequency, Fi its corpus frequency, and Fa the
sum of all topic words in the corpus; l is the number of clause constituents above xi, and
N is the deepest level of clause embedding in the parse.
The first term in Equation (9) is similar to tf ? idf ; it highlights words that are
important in the document and should therefore not be dropped. The score is not
applied indiscriminately to all words in a sentence but solely to topic-related words,
which are approximated by nouns and verbs. This is offset by the importance of these
words in the specific sentence being compressed. Intuitively, in a sentence with multiply
nested clauses, more deeply embedded clauses tend to carry more semantic content.
This is illustrated in Figure 1, which depicts the clause embedding for the sentence Mr
Field has said he will resign if he is not reselected, a move which could divide the party nationally.
Here, the most important information is conveyed by clauses S3 (he will resign) and S4
(if he is not reselected), which are embedded. Accordingly, we should give more weight
to words found in these clauses than in the main clause (S1 in Figure 1). A simple way
to enforce this is to give clauses weight proportional to the level of embedding (see the
second term in Equation (9)). Therefore in Figure 1, the term lN is 1.0 (4/4) for clause S4,
0.75 (3/4) for clause S3, and so on. Individual words inherit their weight from their
clauses. We obtain syntactic information in our experiments from RASP (Briscoe and
Carroll 2002), a domain-independent, robust parsing system for English. However, any
other parser with broadly similar output (e.g., Lin 2001) could also serve our purposes.
Note that the significance score in Equation (9) does not weight differentially the
contribution of tf ? idf versus level of embedding. Although we found in our exper-
iments that the latter term was as important as tf ? idf in producing meaningful com-
pressions, there may be applications or data sets where the contribution of the two terms
varies. This could be easily remedied by introducing a weighting factor.
3.3 Sentential Constraints
In its original formulation, the model also contains a small number of sentence-level
constraints. Their aim is to preserve the meaning and structure of the original sentence
as much as possible. The majority of constraints revolve around modification and
Figure 1
The clause embedding of the sentence Mr Field has said he will resign if he is not reselected, a move
which could divide the party nationally; nested boxes correspond to nested clauses.
419
Computational Linguistics Volume 36, Number 3
argument structure and are defined over parse trees or grammatical relations which
as mentioned earlier we extract from RASP.
Modifier Constraints. Modifier constraints ensure that relationships between head words
and their modifiers remain grammatical in the compression:
?i ? ?j ? 0 (10)
?i, j : xj ? xi?s ncmods
?i ? ?j ? 0 (11)
?i, j : xj ? xi?s detmods
Equation (10) guarantees that if we include a non-clausal modifier3 (ncmod) in the
compression (such as an adjective or a noun) then the head of the modifier must also be
included; this is repeated for determiners (detmod) in Equation (11).
Other modifier constraints ensure the meaning of the source sentence is preserved
in the compression. For example, Equation (12) enforces not in the compression when
the head is included. A similar constraint is added for possessive modifiers (e.g., his,
our), including genitives (e.g., John?s gift), as shown in Equation (13).
?i ? ?j = 0 (12)
?i, j : xj ? xi?s ncmods ? xj = not
?i ? ?j = 0 (13)
?i, j : xj ? xi?s possessive mods
Argument Structure Constraints. Argument structure constraints make sure that the re-
sulting compression has a canonical argument structure. The first constraint (Equa-
tion (14)) ensures that if a verb is present in the compression then so are its arguments,
and if any of the arguments are included in the compression then the verb must also be
included.
?i ? ?j = 0 (14)
?i, j : xj ? subject/object of verb xi
Another constraint forces the compression to contain at least one verb provided the
source sentence contains one as well:
?
i:xi?verbs
?i ? 1 (15)
3 Clausal modifiers (cmod) are adjuncts modifying entire clauses. In the example he ate the cake because he
was hungry, the because-clause is a modifier of the sentence he ate the cake.
420
Clarke and Lapata Discourse Constraints for Document Compression
Other constraints apply to prepositional phrases and subordinate clauses and force the
introducing term (i.e., the preposition, or subordinator) to be included in the compres-
sion if any word from within the syntactic constituent is also included:
?i ? ?j ? 0 (16)
?i, j : xj ? PP/SUB ? xi starts PP/SUB
By subordinator (SUB) we mean wh-words (e.g., who, which, how, where), the word that,
and subordinating conjunctions (e.g., after, although, because). The reverse is also true?
that is, if the introducing term is included, at least one other word from the syntactic
constituent should also be included.
?
i:xi?PP/SUB
?i ? ?j ? 0 (17)
?j : xj starts PP/SUB
All the constraints described thus far are mostly syntactic. They operate over
parse trees or dependency graphs. In the following sections we present our discourse-
specific constraints. But first we discuss how we represent and automatically detect
discourse-related information.
4. Discourse Representation
Obtaining an appropriate representation of discourse is the first step toward creating a
compression model that exploits document-level information. Our goal is to annotate
documents automatically with discourse-level information which will subsequently be
used to inform our compression procedure. As mentioned in Section 2 previous summa-
rization work has mainly focused on cohesion (Sjorochod?ko 1972; Barzilay and Elhadad
1997) or global discourse structure (Marcu 2000; Daume? III and Marcu 2002). We also
opt for a cohesion-based representation of discourse operationalized by lexical chains
(Morris and Hirst 1991). Computing global discourse structure robustly and accurately
is far from trivial. For example, Daume? III and Marcu (2002) employ an RST parser4
but find that it produces noisy output for documents containing longer sentences.
We therefore focus on the less ambitious task of characterizing local coherence?the
way adjacent sentences bind together to form a larger discourse. Although it does
not explicitly capture long distance relationships between sentences, local coherence is
still an important prerequisite for maintaining global coherence. Specifically, we turn
to Centering Theory (Grosz, Weinstein, and Joshi 1995) and adopt an entity-based
representation of discourse.
In the following sections we briefly introduce lexical chains and centering and
describe our algorithms for obtaining discourse annotations.
4 This is the decision-based parser described in Marcu (2000); it achieves an F1 of 38.2 for the identification
of elementary discourse units, 50.0 for hierarchical spans, 39.9 for nuclearity, and 23.4 for relation
assignment.
421
Computational Linguistics Volume 36, Number 3
4.1 Lexical Chains
Lexical cohesion refers to the degree of semantic relatedness observed among lexical
items in a document. The term was coined by Halliday and Hasan (1976), who observed
that coherent documents tend to have more related terms or phrases than incoherent
ones. A number of linguistic devices can be used to signal cohesion; these range from
repetition, to synonymy, hyponymy, and meronymy. Lexical chains are a representation
of lexical cohesion as sequences of semantically related words (Morris and Hirst 1991).
There is a close relationship between discourse structure and cohesion. Related words
tend to co-occur within the same discourse. Thus, cohesion is a surface indicator of
discourse structure and can be identified through lexical chains.
Lexical chains provide a useful means for describing the topic flow in discourse.
For example, a document containing the chain {house, home, loft, house} will proba-
bly describe a situation involving a house. Documents often have multiple topics (or
themes) and consequently will contain many different lexical chains. Some of these
topics will be peripheral and thus represented by short chains whereas main topics
will correspond to dense longer chains. Words participating in the latter chains are
important for our compression task?they reveal what the document is about?and in
all likelihood should not be deleted.
Barzilay and Elhadad (1997) describe a technique for building lexical chains for
extractive text summarization. In their approach chains of semantically related expres-
sions are used to select sentences for inclusion in a summary. Their algorithm uses
WordNet (Fellbaum 1998) to build chains of nouns (and noun compounds). Nouns
are considered related if they are repetitions or linked in WordNet via synonymy,
antonymy, hypernymy, and holonymy. Computing lexical chains would be relatively
straightforward if each word was always represented by a single sense. However, due
to the high level of polysemy inherent in WordNet, algorithms developed for computing
lexical chains must adopt some strategy for disambiguating word senses. For example,
Hirst and St-Onge (1998) greedily disambiguate a word as soon as it is encountered by
selecting the sense most strongly related to existing chain members, whereas Barzilay
and Elhadad (1997) consider all possible alternatives of word senses and then choose
the best one among them.
Once created, lexical chains can serve to highlight which document sentences are
more topical, and should therefore be included in a summary. Barzilay and Elhadad
(1997) rank their chains heuristically by a score based on their length and homogeneity.
They generate summaries by extracting sentences corresponding to strong chains, that
is, chains whose score is two standard deviations above the average score. Analogously,
we also wish to determine which lexical chains indicate the most prevalent discourse
topics. Our assumption is that terms belonging to these chains are indicative of the
document?s main focus and should therefore be retained in the compressed output.
Barzilay and Elhadad?s (1997) scoring function aims to identify sentences (for inclusion
in a summary) that have a high concentration of chain members. In contrast, we are
interested in chains that span several sentences. We thus score chains according to the
number of sentences their terms occur in. For example, the hypothetical chain {house3,
home3, loft3, house5} (where wordi denotes word occurring in sentence i) would be given a
score of two as the terms occur only in two sentences. We assume that a chain signals a
prevalent discourse topic if it occurs throughout more sentences than the average chain.
The scoring algorithm is outlined more formally as:
1. Compute the lexical chains for the document.
422
Clarke and Lapata Discourse Constraints for Document Compression
2. Score(Chain) = Sentences(Chain).
3. Discard chains for which Score(Chain) < Average(Score).
4. Mark terms from the remaining chains as being the focus of the document.
We use the method of Galley and McKeown (2003) to compute lexical chains for
each document.5 It improves on Barzilay and Elhadad?s (1997) original algorithm by
providing better word sense disambiguation and linear runtime. The algorithm pro-
ceeds in three steps. Initially, a graph is built representing all possible interpretations
of the document under consideration. The text is processed sequentially, comparing
each word against all words previously read. If a relation exists between the senses of
the current word and any possible sense of a previous word, a connection is formed
between the appropriate words and senses. The strength of the connection is a function
of the type of relationship and of the distance between the words in the text (in terms
of words, sentences, and paragraphs). Words are represented as nodes in the graph and
semantic relations as weighted edges. The relations considered by Galley and McKeown
(2003) are all first-order WordNet relations, with the addition of siblings?two words
are considered siblings if they are both hyponyms of the same hypernym. Next, all
occurrences of a given word are collected together. For each sense of a target word,
the strength of all connections involving that sense are summed, giving that sense a
unified score. The sense with the highest unified score is chosen as the correct sense
for the target word. Lastly, the lexical chains are constructed by collecting same sense
words into the same chain.
Figure 2 illustrates the lexical chains created by our algorithm for three documents
(taken from our test set). Chains are shown in oval boxes; members of the same chain
have the same index. The algorithm identifies three chains in the first document: {flow,
rate}, {today, day, yesterday}, and {miles, ft}. In the second document the chains are {body}
and {month, night}, and in the third {policeman, police}, {woman, woman, boyfriend, man}.
As can be seen, members of a chain represent a shared concept (e.g., ?time?, ?linear
unit?, or ?person?). In some cases important topics are missed. For instance, in the first
document no chains were created with the words lava or debris. The second document
is about Mrs Allan and contains many references to her. However, because Mrs Allan is
not listed in WordNet it is not possible to create any chains for this word or any of its
coreferents (e.g., she, her). A similar problem is observed in the third document where
Anderson is not included in any chain even though he is one of the main protagonists
throughout the text. We next turn to Centering Theory as a means of identifying which
entities are prominent in a document.
4.2 Centering Theory
Centering Theory (Grosz, Weinstein, and Joshi 1995) is an entity-orientated theory of
local coherence and salience. One of the main ideas underlying centering is that certain
entities mentioned in an utterance are more central than others. This in turn imposes
constraints on the use of referring expressions and in particular on the use of pronouns.
The theory begins by assuming that a discourse is broken into ?utterances.? These
can be phrases, clauses, sentences, or even paragraphs. At any point in discourse,
some entities are considered more salient than others, and are expected to exhibit
5 The software is available from http://www1.cs.columbia.edu/nlp/tools.cgi.
423
Computational Linguistics Volume 36, Number 3
Figure 2
Excerpts of documents from our test set with discourse annotations. Centers are in double boxes;
terms occurring in lexical chains are in oval boxes. Words with the same subscript are members
of the same chain (e.g., police, policeman).
different properties. Specifically, although each utterance may contain several entities, it
is assumed that a single entity is ?centered,? thereby representing the current discourse
focus. One of the main claims underlying centering is that discourse segments in which
successive utterances contain common centers are more coherent than segments where
the center repeatedly changes.
Each utterance Uj in a discourse has a list of forward-looking centers, Cf (Uj), and
a unique backward-looking center, Cb(Uj). Cf (Uj) represents a ranking of the entities
invoked by Uj according to their salience. Thus, some entities in the discourse are
deemed more important than others. The Cb of the current utterance Uj is the highest-
ranked element in Cf (Uj?1) that is also in Uj. (Centering hypothesizes that the Cb is
likely to be realized as a pronoun.) Entities are commonly ranked in terms of their
grammatical function, namely, subjects are ranked more highly than objects, which are
more highly ranked than the rest (Grosz, Weinstein, and Joshi 1995). The Cb links Uj to
the previous discourse, but it does so locally since Cb(Uj) is chosen from Uj?1.
Centering formalizes fluctuations in topic continuity in terms of transitions be-
tween adjacent utterances. Grosz, Weinstein, and Joshi (1995) distinguish between three
424
Clarke and Lapata Discourse Constraints for Document Compression
types of transitions. In CONTINUE transitions, Cb(Uj) = Cb(Uj?1) and Cb(Uj) is the
most highly ranked element entity in Uj. In RETAIN transitions Cb(Uj) = Cb(Uj?1) but
Cb(Uj) is not the most highly ranked element entity in Uj. And in SHIFT transitions
Cb(Uj) = Cb(Uj?1). These transitions are ordered: CONTINUEs are preferred over RE-
TAINs, which are preferred over SHIFTs. And discourses with many CONTINUE transi-
tions are considered more coherent than those which repeatedly SHIFT from one center
to the other.
We demonstrate these concepts in passages (1a)?(1c) taken from Walker, Joshi, and
Prince (1998).
(1) a. Jeff helped Dick wash the car.
CF(Jeff, Dick, car)
b. He washed the windows as Dick waxed the car.
CF(Jeff, Dick, car)
CB=Jeff
c. He soaped a pane.
CF(Jeff, pane)
CB=Jeff
Here, the first utterance does not have a backward-looking center but has three forward-
looking centers Jeff, Dick, and car. To determine the backward-looking center of (1b) we
find the highest ranked entity among the forward-looking centers in (1a) which also
occurs in (1b). This is Jeff as it is the subject (and thus most salient entity) in (1a) and
present (as a pronoun) in (1b). The same procedure is applied for utterance (1c). Also
note that (1a) and (1b) are linked via a CONTINUE transition. The same is true for (1b)
and (1c).
For the purposes of our document compression application, we are not so much
interested in characterizing our texts in terms of entity transitions. Because they are all
written by humans, we can assume they are more or less coherent. Nonetheless, identi-
fying the centers in discourse seems important. These will indicate what the document
is about, who the main protagonists are, and how the discourse focus progresses. We
would probably not want to delete entities functioning as backward-looking centers.
As Centering is primarily a linguistic theory rather than a computational one,
it is not explicitly stated how the concepts of ?utterance,? ?entities,? and ?ranking?
are instantiated. A great deal of research has been devoted to fleshing these out and
many different instantiations have been developed in the literature (see Poesio et al
[2004] for details). In our case, the instantiation will have a bearing on the reliability
of the algorithm to detect centers. If the parameters are too specific then it may not be
possible to accurately determine the center for a given utterance. Because our aim is
to identify centers in discourse automatically, our parameter choice is driven by two
considerations: robustness and ease of computation.
We therefore follow previous work (e.g., Miltsakaki and Kukich 2000) in assuming
that the unit of an utterance is the sentence (i.e., a main clause with accompanying
subordinate and adjunct clauses). This is a simplistic view of an utterance; however it
is in line with our compression task, which also operates over sentences. We determine
which entities are invoked by a sentence using two methods. First, we perform named
entity identification and coreference resolution on each document using LingPipe,6 a
6 LingPipe can be downloaded from http://alias-i.com/lingpipe/.
425
Computational Linguistics Volume 36, Number 3
publicly available system. Named entities are not the only type of entity to occur in our
data, thus to ensure a high entity recall we add named entities and all remaining nouns7
to the Cf list. Entity matching between sentences is required to determine the Cb of a sen-
tence. This is done using the named entity?s unique identifier (as provided by LingPipe)
or by the entity?s surface form in the case of nouns not classified as named entities.
We follow Grosz, Weinstein, and Joshi (1995) in ranking entities according to their
grammatical roles; subjects are ranked more highly than objects, which are in turn
ranked higher than other grammatical roles; ties are broken using left-to-right ordering
of the grammatical roles in the sentence (Tetreault 2001). We identify grammatical roles
using RASP (Briscoe and Carroll 2002). Formally, our centering algorithm is as follows
(where Uj corresponds to sentence j):
1. Extract entities from Uj.
2. Create Cf (Uj) by ranking the entities in Uj according to their grammatical
role (subjects > objects > others, ties broken using left-to-right word order
of Uj).
3. Find the highest ranked entity in Cf (Uj?1) which occurs in Cf (Uj); set the
entity to be Cb(Uj).
This procedure involves several automatic steps (named entity recognition, coreference
resolution, and identification of grammatical roles) and will unavoidably produce some
noisy annotations. There is no guarantee, therefore, that the right Cb will be identified or
that all sentences will be marked with a Cb. The latter situation also occurs in passages
that contain abrupt changes in topic. In such cases, none of the entities realized in Uj will
occur in Cf (Uj?1). Hopefully, lexical chains will come to the rescue here as an alternative
means of capturing local content within a document.
Figure 2 shows the centers (in double boxes) identified by our algorithm. In the first
document lava and debris are marked as centers, in the second document Mrs Allan (and
its coreferents), and in the third one Peter Anderson and allotment. When comparing the
annotations produced by centering and the lexical chains, we observe that they tend
to be complementary. Proper nouns that lexical chains miss out on are often identi-
fied by centering. When the latter fails, due to errors in coreference resolution or the
identification of grammatical relations, lexical chains can be more robust because only
WordNet is required for their computation. As an example consider the third document
in Figure 2. Here, lexical chains provide a better insight into the text. Were we to rely
solely on centering, we would obtain annotations only for two entities, namely, Peter
Anderson and allotment.
5. The Discourse-Inspired Compression Model
We now turn our attention to incorporating discourse information into our compression
model. Before compression takes place, all documents are processed using the center-
ing and lexical chain algorithms described earlier. In each sentence we annotate the
center Cb(Uj) if one exists. Words (or phrases) that are present in the current sentence
and function as the center in the next sentence Cb(Uj+1) are also flagged. Finally, words
7 As determined by the word?s part-of-speech tag.
426
Clarke and Lapata Discourse Constraints for Document Compression
are marked if they are part of a prevalent (high scoring) chain. Provided with this
additional knowledge our model takes a (sentence-separated) source document as input
and generates a compressed version by applying sentence-level and discourse-level
constraints to the entire document rather than to each sentence sequentially. In our
earlier formulation of the compression task (Clarke and Lapata 2008), we create and
solve an ILP for every sentence, whereas now an ILP is solved for each document.
This makes sense from a discourse perspective as compression decisions are not made
independently of each other. Also note that this latter formulation brings compression
closer to summarization as we can manipulate the document compression rate directly,
for example, by adding a constraint that forces the target document to be less than b to-
kens. This allows the model to choose how much to compress each individual sentence
without requiring that they all have the same compression rate. Accordingly, we modify
our objective function by introducing a sum over all sentences (assuming l sentences are
present in the document) and adding an additional index g to each decision variable to
track the sentence it came from:
max z =
l
?
g=1
[ ng
?
i=1
?g,i ? ?I(xg,i) +
ng
?
i=1
?g,i ? P(xg,i|start)
+
ng?2
?
i=1
ng?1
?
j=i+1
ng
?
k=j+1
?g,ijk ? P(xg,k|xg,i, xg,j)
+
ng?1
?
i=0
ng
?
j=i+1
?g,ij ? P(end|xg,i, xg,j)
?
?
??min ? ? ? ?max ? ? (18)
We also modify the compression rate soft constraint to act over the whole document
rather than sentences. This allows some sentences to violate the compression rate with-
out incurring a penalty, provided the compression rate of the document falls within the
specified range.
Document Compression Rate Constraints. We wish to penalize compressions which do not
fall within a desired compression rate range (cmin% ? cmax%).
l
?
g=1
ng
?
i=0
?g,i + ?min ? cmin ?
l
?
g=1
ng (19)
l
?
g=1
ng
?
i=0
ng
?
i=0
?g,i ? ?max ? cmax ?
l
?
g=1
ng (20)
Besides the new objective function and compression rate constraints, the model
makes use of all the sentence-level constraints introduced in Section 3.3, but is crucially
enhanced with three discourse constraints explained in the following.
427
Computational Linguistics Volume 36, Number 3
5.1 Discourse Constraints
Our first goal to is preserve the focus of each sentence. If the center, Cb, is identified in
the source sentence it must be retained in the target compression. If present, the entity
realized as the Cb in the following sentence should also be retained to ensure the focus
is preserved from one sentence to the next. Such a condition is easily captured with the
following ILP constraint:
?i = 1 (21)
?i : xi ? {Cb(Uj), Cb(Uj+1)}
As an example, consider the first discourse in Figure 2. The constraints generated from
Equation (21) will require the compression to retain lava in the first two sentences and
debris in the second and third sentences.
As mentioned in the previous section, the centering algorithm relies on NLP tech-
nology that is not 100% accurate (named entity detection, parsing, and coreference
resolution). Therefore, the algorithm can only approximate the center for each sen-
tence and in some cases fails to identify any centers at all. Lexical chains provide a
complementary annotation of the topic or theme of the document using information
which is not restricted to adjacent sentences. Recall that once chains are created, they
are scored, and chains with scores less than the average are discarded. We consider all
remaining lexical chains as topical and require that words in these be retained in the
compression.
?i = 1 (22)
?i : xi ? document topical lexical chain
Consider again the first text in Figure 2. Here, flow and rate are members of the same
chain (marked with subscript 1). According to constraint (22) both words must be
included in the compressed document. In the third document the words relating to
?police? (police, policeman) and ?people? (woman, boyfriend, man) also would be retained
in the compression.
Our final discourse constraint concerns pronouns. Specifically, we force per-
sonal pronouns (whose antecedent may not always be identified) to be included in the
compression.
?i = 1 (23)
?i : xi ? personal pronouns
The constraints just described ensure that the compressed document will retain
the discourse flow of the source document and will preserve terms indicative of
important topics. Document compression aside, the discourse constraints will also
benefit sentence-level compression. They provide our model, which so far relied on
syntactic evidence and surface level document characteristics (i.e., word frequencies),
additional evidence for retaining (discourse) relevant words.
428
Clarke and Lapata Discourse Constraints for Document Compression
5.2 Applying the Constraints
As explained earlier we apply the model and the constraints to each document. In our
earlier sentence-based formulation, a significance score (see Section 3.2) was used to
highlight which nouns and verbs should be included in the compression. As far as
nouns are concerned, our discourse constraints perform a similar task. Thus, when a
sentence contains discourse annotations, we are inclined to trust them more and only
calculate the significance score for verbs.
During development it was observed that applying all discourse constraints si-
multaneously (see Equations (21)?(23)) results in relatively long compressions. To
counteract this, we employ these constraints using a back-off strategy that relies on
progressively less reliable information. Our back-off model works as follows: If center-
ing information is present, we apply the appropriate constraints (Equation (21)). If no
centers are present, we back off to the lexical chain information using Equation (22), and
in the absence of the latter we back off to the pronoun constraint (Equation (23)). Finally,
if discourse information is entirely absent from the sentence, we default to the sig-
nificance score. Sentential constraints are applied throughout irrespective of discourse
constraints. We determined this ordering (i.e., centering first, then lexical chains, and
then pronouns) on the development set. Centering tends to be more precise, whereas
lexical chains have high recall but lower precision in terms of identifying which entities
are in focus and should therefore not be dropped. In our test data (see Section 6 for
details), the centering constraint was used in 68.6% of the sentences. The model backed
off to lexical chains for 13.7% of the test sentences, whereas the pronoun constraint
was applied in 8.5%. Finally, the noun and verb significance score was used on the
remaining 9.2%. Examples of our system?s output for the texts in Figure 2 are given in
Figure 3.
6. Experimental Set-up
In this section we present our experimental set-up for assessing the performance of
the compression model. We describe the compression corpus used in our study, briefly
introduce the model used for comparison with our approach, and explain how system
output was evaluated.
6.1 Compression Corpus
Previous work on sentence compression has used almost exclusively the Ziff-Davis cor-
pus, a compression corpus derived automatically from document?abstract pairs (Knight
and Marcu 2002). Unfortunately, this corpus is not suitable for our purposes because it
consists of isolated sentences taken from several different documents. We thus created
a document-based compression corpus manually. Specifically, annotators were pre-
sented with one document at a time and asked to compress sentences sequentially
by removing tokens. They were free to remove any words they deemed superfluous,
provided their deletions (a) preserved the most important information in the source sen-
tence, and (b) ensured the compressed sentence remained grammatical. If they wished,
they could leave a sentence uncompressed. They were not allowed to delete whole
sentences even if they believed they contained no information content with respect to
the story, as this would blur the task with summarization. Following these guidelines,
429
Computational Linguistics Volume 36, Number 3
Figure 3
Compression output on excerpts from Figure 2 using the discourse model. Words that are
dropped are striken out.
the annotators created compressions for 82 stories (1,629 sentences) from the BNC and
the LA Times and Washington Post.8 Forty-eight (48) documents (962 sentences) were
used for training, 3 for development (63 sentences), and 31 for testing (604 sentences).
6.2 Comparison with State-of-the-Art
The discourse-based compression model was evaluated against our earlier sentence-
based ILP model (without the discourse constraints). In addition, we compared our ap-
proach against a state-of-the-art model which does not take discourse-level information
into account, does not use ILP, and is sentence-based. We give a brief description in the
following, and refer the interested reader to McDonald (2006) for details.
McDonald (2006) formalizes sentence compression as a classification task in a dis-
criminative large-margin learning framework: Pairs of words from the source sentence
are classified as being adjacent or not in the target compression. Let x = x1, . . . , xn
denote a source sentence with a target compression y = y1, . . . , ym where each yj oc-
curs in x. The function L(yi) ? {1 . . . n} maps word yi in the target to the index of
the word in the source, x (subject to the constraint that L(yi) < L(yi+1)). McDonald
defines the score of a compression y for a sentence x as the dot product between
8 The corpus is available from http://homepages.inf.ed.ac.uk/s0460084/data/.
430
Clarke and Lapata Discourse Constraints for Document Compression
a high-dimensional feature representation over bigrams and a corresponding weight
vector:
s(x, y) =
|y|
?
j=2
w ? f(x, L(yj?1), L(yj)) (24)
Decoding in this framework amounts to finding the combination of bigrams that maxi-
mize the scoring function in Equation (24). The maximization is solved using dynamic
programming (see McDonald [2006] for details).
The model parameters are estimated using the Margin Infused Relaxed Algorithm
(MIRA; Crammer and Singer 2003), a discriminative large-margin online learning tech-
nique. This algorithm learns by compressing each sentence and comparing the result
with the gold standard. The weights are updated so that the score of the correct com-
pression (the gold standard) is greater than the score of all other compressions by a
margin proportional to their loss. The loss function is the number of words falsely re-
tained or dropped in the incorrect compression relative to the gold standard. McDonald
employs a rich feature set defined over words, parts of speech, phrase structure trees,
and dependencies. These are gathered over adjacent words in the compression and the
words in between which were dropped.
It is important to note that McDonald (2006) is not a straw-man system. It achieves
highly competitive performance compared with Knight and Marcu?s (2002) noisy-
channel and decision-tree models. Due to its discriminative nature, the model is able
to use a large feature set and to optimize compression accuracy directly. In other words,
McDonald?s model has a head start against our own model which does not utilize a
large parallel corpus and has only a few constraints. The comparison of the two systems
allows us to establish that we have a competitive state-of-the-art system, even without
discourse constraints.
We trained McDonald?s (2006) model on the full training set (48 documents, 962
sentences). Our implementation used an identical feature set, the only difference being
that our phrase structure and dependency features were extracted from the output
of Roark?s (2001) parser. McDonald uses Charniak?s (2000) parser, which performs
comparably. We also employed a slightly modified loss function to encourage compres-
sion on our data set. McDonald?s results were reported on the Ziff-Davis corpus. The
language model required for the ILP system was trained on 80 million tokens from the
English GigaWord corpus (LDC2007T07) using the SRI Language Modeling Toolkit with
Kneser-Ney discounting. The significance score was calculated on 80 million tokens
from the same corpus. The ILP model presented in Equation (1) implements a weighted
combination of the significance score with a language model. The weight was tuned
on the development set which consisted of three source documents and their target
compressions. Our optimization procedure used Powell?s method (Press et al 1992) and
a loss function based on the grammatical relations F1 between the gold standard and
system output. The optimal weight was approximately 9.0. Note that the development
set was the only source of parallel data our model had access to.
In order to compare all three models (sentence-based ILP, discourse-based ILP, and
McDonald [2006]) on an equal footing, we ensured that their compression rates were
similar. To do this, we first run McDonald?s model on our data and then set the com-
pression rate for our ILP models so that it is comparable to his output. This can be done
relatively straightforwardly by adjusting the compression rate range soft constraint. In
our experiments we set the minimum compression rate to 57%, the upper rate to 62%,
431
Computational Linguistics Volume 36, Number 3
and the violation penalty (?) to ?99. In practice, the soft constraint controlling the
compression rate can be removed or specifically tuned to suit the application.
6.3 Evaluation
Previous studies evaluate the well-formedness of automatically generated compres-
sions out of context. The target sentences are typically rated by naive subjects on two
dimensions, grammaticality and importance (Knight and Marcu 2002). Automatic eval-
uation measures have also been proposed. Riezler et al (2003) compare the grammatical
relations found in the system output against those found in a gold standard using F1.
Although F1 conflates grammaticality and importance into a single score, it neverthe-
less has been shown to correlate reliably with human judgments (Clarke and Lapata
2006).
The aims of our evaluation study were twofold. Firstly, we wanted to examine
whether our discourse constraints improve the compressions for individual sentences.
There is no hope for generating shorter documents if the compressed sentences are
either too wordy or too ungrammatical. Secondly and more importantly, our goal was
to evaluate the compressed documents as a whole by examining whether they are
readable and the degree to which they retain key information when compared to the
originals. We evaluated sentence-based compressions automatically using F1 and the
grammatical relations annotations provided by RASP (Briscoe and Carroll 2002). This
parser is suited to the compression task as it provides parses for both full sentences
and sentence fragments and is generally robust enough to analyze semi-grammatical
sentences. We computed F1 over all the relations provided by RASP (e.g., subject,
direct/indirect object, modifier; 17 in total). We compared the output of our discourse
system on the test set (31 documents, 604 sentences) against the sentence-based ILP
model and McDonald (2006).
Our document-level evaluation was motivated by two questions: (1) Are the com-
pressed documents readable? and (2) How much key information is preserved between
the source document and its target compression? The readability of a document is
fairly straightforward to measure by asking participants to provide a rating (e.g., on a
seven-point scale). Measuring how much information is preserved in the compressed
document is more involved. Under the assumption that the target document is to
function as a replacement for the source, we can measure the extent to which the
compressed version can be used to find answers for questions which have been derived
from the source and are representative of its core content. We thus created questions
from the source and then determined whether it was possible to find their answers by
reading the compressed target. The more questions a hypothetical compression system
can answer, the better it is at compressing the document as a whole.
A question-answering (Q&A) paradigm has been used previously to evaluate
summaries and text compression. Morris, Kasper, and Adams (1992) performed one
of the first Q&A evaluations to investigate the degree to which documents could be
summarized before reading comprehension diminished. Their corpus consisted of four
passages randomly selected from a set of sample Graduate Management Aptitude Test
(GMAT) reading comprehension tests. The texts covered a range of topics including
medieval literature, 18th-century Japan, minority-operated businesses, and Florentine
art. Accompanying each text were eight multiple-choice questions, each containing
five possible answers. The questions were provided by the Educational Testing Service
and were designed to measure the subjects? reading comprehension. Subjects were
432
Clarke and Lapata Discourse Constraints for Document Compression
given various textual treatments: the full text, a human-authored abstract, three system-
generated extracts, and a final treatment where merely the questions were presented
without any text. The questions-only treatment was used as a control to investigate if
subjects could answer questions without any source material. Subjects were instructed
to read the passage (if provided) and answer the multiple choice questions.
The advantage of using standardized tests, such as the GMAT reading compre-
hension test, is that Q&A pairs are provided along with a method for scoring answers
(the correct answer is one among five possible choices). However, our corpora do not
contain ready prepared Q&A pairs; thus we require a methodology for constructing
questions and their answers and scoring documents against the answers. One such
methodology is presented in the TIPSTER Text Summarization Evaluation (SUMMAC;
Mani et al 2002). SUMMAC was concerned with producing summaries tailored to
specific topics. The Q&A task involved an evaluation where a topic-related summary
for a document was evaluated in terms of its ?informativeness,? namely, the degree
to which it contained answers found in the source document to a set of topic-related
questions. For each topic (three in total), 30 relevant documents were chosen to generate
a single summary. One annotator per topic came up with no more than five questions
relating to the obligatory aspects of the topic. An obligatory aspect of a topic was
defined as information that must be present in the document for the document to be
relevant to the topic. The annotators then created an answer key for their topic by
annotating the passages and phrases from the documents which provided the answers
to the questions. In the SUMMAC evaluation, the annotator for each topic was tasked
with scoring the system summaries. Scoring involved comparing the summaries against
the answer key (annotated passages from the source documents) while judging whether
the summary provided a Correct, Partially Correct, or Missing answer. If a summary con-
tained an answer key and sufficient context the summary was deemed correct; however,
summaries would be considered partially correct if the answer key was present but with
insufficient context. If context was completely missing, misleading, or the answer key
was absent then the summary was judged missing.
Our methodology for constructing Q&A pairs and for scoring documents is in-
spired by the SUMMAC evaluation exercise (Mani et al 2002). Rather than creating
questions for document sets (or topics) our questions were derived from individual
documents. Two annotators were independently instructed to read the documents from
our (test) corpus and create Q&A pairs. Each annotator drafted no more than ten
questions and answers per document, related to its content. Annotators were asked
to create fact-based questions which required an unambiguous answer; these were
typically who, what, where, when, and how?style questions. The purpose of using two
annotators per document was to allow annotators to compare and revise their Q&A
pairs; this process was repeated until a common agreed-upon set of questions was
reached. Revisions typically involved merging and simplifying questions to make them
clearer, and in some cases splitting a question into multiple questions. Documents for
which too few questions were agreed upon and for which the questions and answers
were too ambiguous were removed. This left an evaluation set of six documents with
between five to eight concise questions per document. Figure 4 shows a document from
our test set and the questions and answers our annotators created for it.
For scoring our documents we adopt a more objective method than SUMMAC.
Instead of asking the annotator who constructed the questions to check the document
compressions for the answers, we ask naive participants to read the compressed doc-
uments and answer the questions as best as they can. During evaluation, the source
document is not shown to our subjects; thus, if the compression is difficult to read, the
433
Computational Linguistics Volume 36, Number 3
Figure 4
Example document from our test set and questions with answer key created for this document.
participants have no point of reference to help them understand the compression. This
is a departure from previous evaluations within text generation tasks, where the source
text is available at judgment time; in our case only the system output is available.
The document-based evaluation was conducted remotely over the Internet using
a custom-built Web interface. Upon loading the Web interface, participants were pre-
sented with a set of instructions that explained the Q&A task and provided examples.
434
Clarke and Lapata Discourse Constraints for Document Compression
Table 1
Compression results: compression rate and relation-based F1.
Model CompR Precision Recall F1
McDonald 60.1% 43.9% 36.5%? 37.9%?
Sentence ILP 62.1% 40.7%? 39.4%? 39.0%?
Discourse ILP 61.0% 46.2% 44.2% 42.2%
Gold Standard 70.3% ?? ?? ??
? Significantly different from Discourse ILP (p < 0.01 using the Wilcoxon test).
Subjects were first asked to read the compressed document and then rate its readability
on a seven-point scale where 7 = excellent, and 1 = terrible. Next, questions were
presented one at a time (the order being is defined by the annotators) and participants
were encouraged to consult the document for the answer. Answers were written directly
into a text field on the Web interface which allowed free-form text to be submitted. Once
a participant provided an answer and confirmed the answer, the interface locked the
answer to ensure it was not modified later. This was necessary because later questions
could reveal information which would help answer previous questions.
We elicited answers for six documents in four compression conditions: gold stan-
dard, using the ILP sentence-based model, the ILP discourse model, and McDonald?s
(2006) model. A Latin square design was used to prevent participants from seeing
multiple treatments (compressions) of the same document thus removing any learning
effect. A total of 116 unpaid volunteers completed the experiment. They were recruited
through student mailing lists and the Language Experiments Web site.9 The answers
provided by our subjects were scored against an answer key. A correct answer was
marked with a score of one, and zero otherwise. In cases where two answers were
required, a score of 0.5 was awarded to each correct answer. The score for a compressed
document is the average of its question scores. All subsequent tests and comparisons
are performed on the document score.
7. Results
We first assessed the compressions produced by the two ILP models (Discourse and
Sentence) and McDonald (2006) on a sentence-by-sentence basis. Table 1 shows the
compression rates (CompR) for the three systems and evaluates the quality of their
output using grammatical relations F1. As can be seen, all three systems produce
comparable compression rates. The Discourse ILP compressions are slightly longer than
McDonald?s (2006) (61.0% vs. 60.1%) and slightly shorter than the Sentence ILP model
(61.0% vs. 62.1%). The Discourse ILP model is significantly better than McDonald (2006)
and Sentence ILP in terms of F1, indicating that discourse-level information is generally
helpful. All three systems could use further improvement, as inter-annotator agreement
on this data yields an F1 of 65.8% (Clarke 2008).
Let us now consider the results of our document-based evaluation. Table 2 shows
the mean readability ratings obtained for each system and the percentage of questions
answered correctly. We used an analysis of variance (ANOVA) to examine the effect
9 Available at http://www.language-experiments.org.
435
Computational Linguistics Volume 36, Number 3
Table 2
Human evaluation results: average readability ratings and average percentage of questions
answered correctly.
Model Readability Q&A (%)
McDonald 2.52? 51.42??
Sentence ILP 2.76? 52.35??
Discourse ILP 3.10? 71.38?
Gold Standard 5.41? 85.48?
? Significantly different from Gold Standard.
? Significantly different from Discourse ILP.
of compression type (McDonald, Sentence ILP, Discourse ILP, Gold Standard). The
ANOVA revealed a reliable effect on both readability and Q&A. Post hoc Tukey tests
showed that McDonald and the two ILP models do not differ significantly in terms
of readability. However, they are all significantly less readable than the gold standard
(? < 0.01). For the Q&A task, we observe that our system is significantly better than
McDonald (? < 0.01) and Sentence ILP (? < 0.01), but significantly worse than the gold
standard (? < 0.05). McDonald and Sentence ILP yield comparable performance (their
difference is not statistically significant).
These results indicate that the automatic systems lag behind the human gold stan-
dard in terms of readability. When reading entire documents, subjects are less tolerant
of ungrammatical constructions. We also find out that, despite relatively low readability,
the documents are overall understandable. The discourse-based model generates more
informative documents?the number of questions answered correctly increases by 19%
in comparison to McDonald and Sentence ILP. This is an encouraging result suggesting
that there are advantages in developing compression models that exploit discourse-
level information information.
Figure 5 shows the output of the ILP systems (Discourse and Sentence) on two
test documents. Words that are dropped have been stricken out. As can be seen, the
two systems produce different compressions, and the discourse-based output is more
coherent. This is corroborated by the readability results where the discourse ILP model
received the highest rating. Also note that some of the compressions produced by the
sentence-based model distort the meaning of the original text, presumably leading the
reader to make wrong inferences. For example, in the second document (Sentence ILP
version) one infers that the victim was urged to report the incident. Moreover, important
information is often omitted, for example, that the victim was indeed raped or that the
strike would be damaging not only to the company but also to its staff (see the Sentence
ILP version in the first document).
8. Conclusions and Future Work
In this article we proposed a novel method for automatic sentence compression. Central
in our approach is the use of discourse-level information, which we argue is an impor-
tant prerequisite for document (as opposed to sentence) compression. Our model uses
integer linear programming for inferring globally optimal compressions in the presence
of linguistically motivated constraints. Our discourse constraints aim to capture local
coherence and are inspired by Centering Theory and lexical chains. We showed that our
436
Clarke and Lapata Discourse Constraints for Document Compression
Improvements in certain allowances were made, described as divisive by
the unions, but the company has refused to compromise on a reduction in
the shorter working week. Ford dismissed an immediate meeting with the
unions but did not rule out talks after Christmas. It said that a strike would
be damaging to the company and to its staff. Production closed down at Ford
last night for the Christmas period. Plants will open again on January 2.D
is
co
u
rs
e
IL
P
Improvements in certain allowances were made, described as divisive by
the unions, but the company has refused to compromise on a reduction in
the shorter working week. Ford dismissed an immediate meeting with the
unions but did not rule out talks after Christmas. It said that a strike would
be damaging to the company and to its staff. Production closed down at Ford
last night for the Christmas period. Plants will open again on January 2.S
en
te
n
ce
IL
P
He threatened her by forcing his truncheon under her chin and then raped
her. She said he only refrained from inserting his truncheon into her, after she
begged him not to. Afterwards he told her not to report the incident because
he could have her ?nicked? for soliciting. She did not report it because she
did not think she would be believed. Police investigated after an anonymous
report.D
is
co
u
rs
e
IL
P
He threatened her by forcing his truncheon under her chin and then raped
her. She said he only refrained from inserting his truncheon into her, after she
begged him not to. Afterwards he told her not to report the incident because
he could have her ?nicked? for soliciting . She did not report it because she
did not think she would be believed. Police investigated after an anonymous
report.S
en
te
n
ce
IL
P
Figure 5
Output of Discourse and Sentence ILP systems on two test documents. Words that are stricken
out have been dropped.
model can be successfully employed to produce compressed documents that preserve
most of the original core content.
Our results confirm the conventional wisdom that discourse-level information is
helpful in summarization. We also show that this type of information can be identified
robustly in free text. Our experiments focused primarily on local discourse structure us-
ing two complementary representations. Centering tends to produce more annotations
since it tries to identify a center in every sentence. Lexical chains tend to provide more
general information, such as the major topics in a document. Due to their approximate
nature, there is no one representation that is uniquely suited to the compression task.
Rather, it is the synergy between lexical chains and centering that brings improvements.
The discourse annotations proposed here are not specific to our model. They could
be easily translated into features and incorporated into discriminative modeling par-
adigms (e.g., Nguyen et al 2004; McDonald 2006; Cohn and Lapata 2009). The same
is true for the Q&A evaluation paradigm employed in our experiments. It could be
straightforwardly adapted to assess the information content of shorter summaries and
potentially used to perform large-scale comparisons within and across systems.
Our approach differs from most summarization work in that our summaries are
fairly long. However, we believe this is the first step to understanding how com-
pression can help summarization. An obvious extension would be to interface our
437
Computational Linguistics Volume 36, Number 3
compression model with sentence extraction (see Martins and Smith [2009] for an ILP
formulation of a model that jointly performs sentence extraction and compression,
without, however, taking discourse level information into account). The discourse
annotations can help guide the extraction method into selecting topically related sen-
tences which can consequently be compressed together. More generally, formulating the
summarization process in the ILP framework outlined here would allow the integration
of varied and sometimes conflicting constraints during summary generation. Examples
include the summary length, and whether it is coherent, grammatical, or repetitive. Ad-
ditional flexibility can be introduced by changing some of the constraints from hard to
soft (as we did with the compression rate constraints), although determining the penalty
for constraint violation manually using prior knowledge is a non-trivial task (Chang,
Ratinov, and Roth 2007) and automatically learning the constraint penalty results in a
harder learning problem. Importantly, under the ILP formulation such constraints can
be explicitly encoded and applied during inference while finding a globally optimal
solution.
Acknowledgments
We are grateful to Ryan McDonald for his
help with the re-implementation of his
system, and our annotators Vasilis Karaiskos
and Sarah Luger. Thanks to Alex Lascarides,
Sebastian Riedel, and Bonnie Webber for
insightful comments and suggestions, and to
the anonymous referees whose feedback
helped to substantially improve the present
article. Lapata acknowledges the support of
EPSRC (grant GR/T04540/01).
References
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Computer and System
Sciences, 3:37?56.
Barzilay, R. and M. Elhadad. 1997. Using
lexical chains for text summarization. In
Proceedings of the ACL-97 Intelligent Scalable
Text Summarization Workshop, pages 10?17,
Madrid.
Barzilay, Regina and Mirella Lapata. 2006.
Aggregation via set partitioning for
natural language generation. In Proceedings
of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 359?366, New York, NY.
Barzilay, Regina and Mirella Lapata. 2008.
Modeling local coherence: An entity-based
approach. Computational Linguistics,
34(1):1?34.
Boguraev, Branimir and Chris Kennedy.
1997. Salience-based content
characterization of text documents. In
Proceedings of the ACL?97/EACL?97
Workshop on Intelligent Scalable Text
Summarization, pages 2?9, Madrid.
Brin, Sergey and Michael Page. 1998.
Anatomy of a large-scale hypertextual
Web search engine. In Proceedings of the
7th Conference on World Wide Web,
pages 107?117, Brisbane.
Briscoe, E. J. and J. Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC?2002), pages 1499?1504,
Las Palmas.
Carlson, Lynn, John M. Conroy, Daniel
Marcu, Dianne P. O?Leary, Mary E.
Okurowski, and Anthony Taylor. 2001. An
empirical study on the relation between
abstracts, extracts, and the discourse
structure of texts. In Proceedings of the
DUC-2001 Workshop on Text Summarization,
New Orleans, LA.
Chang, Ming-Wei, Lev Ratinov, and Dan
Roth. 2007. Guiding semi-supervision with
constraint-driven learning. In Proceedings
of the 22nd International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 280?287, Prague.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st North American Annual Meeting of the
Association for Computational Linguistics,
pages 132?139, Seattle, WA.
Clarke, James. 2008. Global Inference for
Sentence Compression: An Integer Linear
Programming Approach. Ph.D. thesis,
University of Edinburgh.
Clarke, James and Mirella Lapata. 2006.
Models for sentence compression: A
comparison across domains, training
requirements and evaluation measures.
In Proceedings of the 21st International
438
Clarke and Lapata Discourse Constraints for Document Compression
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 377?384,
Sydney.
Clarke, James and Mirella Lapata. 2008.
Global inference for sentence compression:
An integer linear programming approach.
Journal of Artificial Intelligence Research,
31:399?429.
Cohn, Trevor and Mirella Lapata. 2009.
Sentence compression as tree transduction.
Journal of Artificial Intelligence Research,
34:637?674.
Corston-Oliver, Simon. 2001. Text
compaction for display on very small
screens. In Proceedings of the NAACL
Workshop on Automatic Summarization,
pages 89?98, Pittsburgh, PA.
Corston-Oliver, Simon H. 1998. Computing
representations of the structure of written
discourse. Technical Report MSR-TR-98-15,
Microsoft Research, Redmond, WA.
Crammer, Koby and Yoram Singer. 2003.
Ultraconservative online algorithms for
multiclass problems. Journal of Machine
Learning Research, 3:951?991.
Daume? III, Hal and Daniel Marcu. 2002.
A noisy-channel model for document
compression. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 449?456,
Philadelphia, PA.
Denis, Pascal and Jason Baldridge. 2007.
Joint determination of anaphoricity and
coreference resolution using integer
programming. In Proceedings of Human
Language Technologies 2007: The Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 236?243, Rochester, NY.
Dras, Mark. 1997. Reluctant paraphrase:
Textual restructuring under an
optimisation model. In Proceedings of the
Fifth Biannual Meeting of the Pacific
Association for Computational Linguistics,
pages 98?104, Ohme.
Endres-Niggemeyer, Brigitte. 1998.
Summarising Information. Springer, Berlin.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Database. MIT Press,
Cambridge, MA.
Galley, Michel and Kathleen McKeown.
2003. Improving word sense disambiguation
in lexical chaining. In Proceedings of 18th
International Joint Conference on Artificial
Intelligence (IJCAI?03), pages 1486?1488,
Acapulco, Mexico.
Galley, Michel and Kathleen McKeown.
2007. Lexicalized Markov grammars for
sentence compression. In Proceedings of
Human Language Technologies 2007: The
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 180?187, Rochester, NY.
Grefenstette, Gregory. 1998. Producing
Intelligent Telegraphic Text Reduction to
Provide an Audio Scanning Service for the
Blind. In Proceedings of the AAAI Symposium
on Intelligent Text Summarization,
pages 111?117, Stanford, CA.
Grosz, Barbara J., Scott Weinstein, and
Aravind K. Joshi. 1995. Centering: a
framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203?225.
Halliday, M. A. K. and Ruqaiya Hasan.
1976. Cohesion in English. Longman,
London.
Hirst, Graeme and David St-Onge. 1998.
Lexical chains as representations of
context for the detection and correction
of malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Database.
MIT Press, Cambridge, MA,
pages 305?332.
Hori, Chiori and Sadaoki Furui. 2004.
Speech summarization: An approach
through word extraction and a method
for evaluation. IEICE Transactions on
Information and Systems, E87-D(1):15?25, 1.
Jing, Hongyan. 2000. Sentence reduction
for automatic text summarization. In
Proceedings of the 6th conference on
Applied Natural Language Processing,
pages 310?315, Seattle, WA.
Kibble, Rodger and Richard Power. 2004.
Optimising referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Knight, Kevin and Daniel Marcu. 2002.
Summarization beyond sentence
extraction: a probabilistic approach to
sentence compression. Artificial Intelligence,
139(1):91?107.
Kupiec, Julian, Jan O. Pedersen, and Francine
Chen. 1995. A trainable document
summarizer. In Proceedings of SIGIR-95,
pages 68?73, Seattle, WA.
Lin, Chin-Yew. 2003. Improving
summarization performance by sentence
compression?A pilot study. In Proceedings
of the 6th International Workshop on
Information Retrieval with Asian Languages,
pages 1?8, Sapporo.
Lin, Dekang. 2001. LaTaT: Language and text
analysis tools. In Proceedings of the first
Human Language Technology Conference,
pages 222?227, San Francisco, CA.
439
Computational Linguistics Volume 36, Number 3
Mani, Inderjeet. 2001. Automatic
Summarization. John Benjamins,
Amsterdam.
Mani, Inderjeet, The?re`se Firmin, David
House, Gary Klein, Beth Sundheim, and
Lynette Hirschman. 2002. The TIPSTER
SUMMAC Text Summarization
Evaluation. Natural Language Engineering,
8:43?68.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 558?565,
College Park, MD.
Mann, William C. and Sandra A. Thompson.
1988. Rhetorical structure theory: Toward a
functional theory of text organization. Text,
8(3):243?281.
Marciniak, Tomasz and Michael Strube.
2005. Beyond the pipeline: Discrete
optimization in NLP. In Proceedings of
the Ninth Conference on Computational
Natural Language Learning (CoNLL?2005),
pages 136?143, Ann Arbor, MI.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization. The
MIT Press, Cambridge, MA.
Martins, Andre? and Noah A. Smith. 2009.
Summarization with a joint model for
sentence extraction and compression. In
Proceedings of the Workshop on Integer Linear
Programming for Natural Language
Processing, pages 1?9, Boulder, CO.
Martins, Andre?, Noah Smith, and Eric Xing.
2009. Concise integer linear programming
formulations for dependency parsing. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 342?350, Suntec.
McDonald, Ryan. 2006. Discriminative
sentence compression with soft syntactic
constraints. In Proceedings of the
11th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 297?304, Trento.
Miltsakaki, Eleni and Karen Kukich. 2000.
The role of centering theory?s rough-shift
in the teaching and evaluation of writing
skills. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 408?415, Hong Kong.
Morris, A., G. Kasper, and D. Adams. 1992.
The effects and limitations of automated
text condensing on reading
comprehension performance. Information
Systems Research, 3(1):17?35.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 17(1):21?48.
Nguyen, Minh Le, Akira Shimazu, Susumu
Horiguchi, Tu Bao Ho, and Masaru
Fukushi. 2004. Probabilistic sentence
reduction using support vector machines.
In Proceedings of the 20th International
Conference on Computational Linguistics,
pages 743?749, Geneva.
Olivers, S. H. and W. B. Dolan. 1999. Less is
more; eliminating index terms from
subordinate clauses. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 349?356,
College Park, MD.
Ono, Kenji, Kazuo Sumita, and Seiji Miike.
1994. Abstract generation based on
rhetorical structure extraction. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 344?348, Kyoto.
Ora?san, Constantin. 2003. An evolutionary
approach for improving the quality of
automatic summaries. In ACL Workshop on
Multilingual Summarization and Question
Answering, pages 37?45, Sapporo, Japan.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: a parametric theory and
its instantiations. Computational Linguistics,
30(3):309?363.
Press, William H., Saul A. Teukolsky,
William T. Vetterling, and Brian P.
Flannery. 1992. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge
University Press, Cambridge, UK.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings of the
20th International Conference on
Computational Linguistics, pages 1346?1352,
Geneva.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 129?137, Sydney.
Riezler, Stefan, Tracy H. King, Richard
Crouch, and Annie Zaenen. 2003.
Statistical sentence condensation using
ambiguity packing and stochastic
disambiguation methods for
lexical-functional grammar. In Proceedings
of the 2003 Human Language Technology
Conference of the North American Chapter of
440
Clarke and Lapata Discourse Constraints for Document Compression
the Association for Computational Linguistics,
pages 118?125, Edmonton.
Roark, Brian. 2001. Probabilistic top?down
parsing and language modeling.
Computational Linguistics, 27(2):249?276.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 1?8, Boston, MA.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press,
New York, pages 47?73.
Sjorochod?ko, E. F. 1972. Adaptive method
for automatic abstracting and indexing. In
Information Processing 71: Proceedings of the
IFIP Congress 71, pages 1179?1182,
Amsterdam.
Tetreault, Joel R. 2001. A corpus-based
evaluation of centering and pronoun
resolution. Computational Linguistics,
27(4):507?520.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles?
Experiments with relevance and rhetorical
status. Computational Linguistics,
28(4):409?446.
Turner, Jenine and Eugene Charniak.
2005. Supervised and unsupervised
learning for sentence compression. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics, pages 290?297, Ann
Arbor, MI.
Vanderbei, Robert J. 2001. Linear
Programming: Foundations and Extensions.
Kluwer Academic Publishers, Boston,
2nd edition.
Walker, Marilyn, Aravind Joshi, and
Ellen Prince. 1998. Centering in
naturally occurring discourse: An
overview. In Centering Theory in
Discourse. Oxford University Press,
Oxford, pages 1?28.
Winston, Wayne L. and Munirpallam
Venkataramanan. 2003. Introduction to
Mathematical Programming. Brooks/Cole,
Independence, KY.
Wolf, Florian and Edward Gibson. 2004.
Paragraph-, word-, and coherence-based
approaches to sentence ranking: A
comparison of algorithm and human
performance. In Proceedings of the
42nd Meeting of the Association for
Computational Linguistics, pages 383?390,
Barcelona.
Zajic, David, Bonnie J. Dorr, Jimmy J. Lin,
and Richard M. Schwartz. 2007.
Multi-candidate reduction: Sentence
compression as a tool for document
summarization tasks. Information
Processing and Management,
43(6):1549?1570.
441

Semi-Supervised Semantic Role Labeling
via Structural Alignment
Hagen Fu?rstenau?
Columbia University
Mirella Lapata??
University of Edinburgh
Large-scale annotated corpora are a prerequisite to developing high-performance semantic role
labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and
may not be representative. Our work aims to reduce the annotation effort involved in creating
resources for semantic role labeling via semi-supervised learning. The key idea of our approach
is to find novel instances for classifier training based on their similarity to manually labeled seed
instances. The underlying assumption is that sentences that are similar in their lexical material
and syntactic structure are likely to share a frame semantic analysis. We formalize the detection of
similar sentences and the projection of role annotations as a graph alignment problem, which we
solve exactly using integer linear programming. Experimental results on semantic role labeling
show that the automatic annotations produced by our method improve performance over using
hand-labeled instances alone.
1. Introduction
Recent years have seen growing interest in the shallow semantic analysis of natural
language text. The term is most commonly used to refer to the automatic identification
and labeling of the semantic roles conveyed by sentential constituents (Gildea and
Jurafsky 2002). Semantic roles themselves have a long-standing tradition in linguistic
theory, dating back to the seminal work of Fillmore (1968). They describe the relations
that hold between a predicate and its arguments, abstracting over surface syntactic
configurations. Consider the following example sentences:
(1) a. The burglar broke the window with a hammer.
b. A hammer broke the window.
c. The window broke.
? Center for Computational Learning Systems, Columbia University, 475 Riverside Drive, Suite 850,
New York, NY 10115, USA. E-mail: hagen@ccls.columbia.edu.
(The work reported in this paper was carried out while the author was at Saarland University, Germany.)
?? School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK.
E-mail: mlap@inf.ed.ac.uk.
Submission received: 30 August 2010; revised submission received: 29 April 2011; accepted for publication:
14 June 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
Here, the phrase the window occupies different syntactic positions?it is the object of
break in sentences (1a) and (1b), and the subject in (1c)?and yet bears the same semantic
role denoting the affected physical object of the breaking event. Analogously, hammer is
the instrument of break both when attested with a prepositional phrase in (1a) and as
a subject in (1b). The examples represent diathesis alternations1 (Levin 1993), namely,
regular variations in the syntactic expressions of semantic roles, and their computational
treatment is one of the main challenges faced by automatic semantic role labelers.
Several theories of semantic roles have been proposed in the literature, differing
primarily in the number and type of roles they postulate. These range from Fillmore?s
(1968) small set of universal roles (e.g., Agentive, Instrumental, Dative) to individual
roles for each predicate (Palmer, Gildea, and Kingsbury 2005). Frame semantic theory
(Fillmore, Johnson, and Petruck 2003) occupies the middle ground by postulating
situations (or frames) that can be evoked by different predicates. In this case, roles
are not specific to predicates but to frames, and therefore ought to generalize among
semantically related predicates. As an example, consider the sentences in Example (2):
(2) a. [Lee]Agent [punched]CAUSE HARM [John]Victim [in the eye]Body part.
b. [A falling rock]Cause [crushed]CAUSE HARM [my ankle]Body part.
c. [She]Agent [slapped]CAUSE HARM [him]Victim [hard]Degree [for his
change of mood]Reason.
d. [Rachel]Agent [injured]CAUSE HARM [her friend]Victim [by closing
the car door on his left hand]Means.
Here, the verbs punch, crush, slap, and injure are all frame evoking elements (FEEs),
that is, they evoke the CAUSE HARM frame, which in turn exhibits the frame-specific
(or ?core?) roles Agent, Victim, Body part, and Cause, and the more general (?non-core?)
roles Degree, Reason, and Means. A frame may be evoked by different lexical items,
which may in turn inhabit several frames. For instance, the verb crush may also evoke
the GRINDING frame, and slap the IMPACT frame.
The creation of resources that document the realization of semantic roles in
example sentences such as FrameNet (Fillmore, Johnson, and Petruck 2003) and
PropBank (Palmer, Gildea, and Kingsbury 2005) has greatly facilitated the develop-
ment of learning algorithms capable of automatically analyzing the role semantic struc-
ture of input sentences. Moreover, the shallow semantic analysis produced by existing
systems has been shown to benefit a wide spectrum of applications ranging from
information extraction (Surdeanu et al 2003) and question answering (Shen and Lapata
2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al 2005).
Most semantic role labeling (SRL) systems to date conceptualize the task as
a supervised learning problem and rely on role-annotated data for model training.
Supervised methods deliver reasonably good performance2 (F1 measures in the low
80s on standard test collections for English); however, the reliance on labeled training
data, which is both difficult and highly expensive to produce, presents a major obstacle
to the widespread application of semantic role labeling across different languages and
text genres. And although nowadays corpora with semantic role annotations exist in
1 Sentences (1a) and (1b) illustrate the instrument subject alternation and sentences (1a) and (1c) illustrate
the causative/inchoative alternation.
2 We refer the interested reader to the reports on the SemEval-2007 shared task (Baker, Ellsworth, and Erk
2007) for an overview of the state of the art.
136
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
other languages (e.g., German, Spanish, Catalan, Chinese, Korean), they tend to be
smaller than their English equivalents and of limited value for modeling purposes.
It is also important to note that the performance of supervised systems degrades
considerably (by 10%) on out-of-domain data even within English, a language for which
two major annotated corpora are available (Pradhan, Ward, and Martin 2008). And this
is without taking unseen events into account, which unavoidably affect coverage. The
latter is especially an issue for FrameNet (version 1.3) which is still under development,
despite being a relatively large resource?it contains almost 140,000 annotated sentences
for a total of 502 frames, which are evoked by over 5,000 different lexical units. Coverage
issues involve not only lexical units but also missing frames and incompletely exempli-
fied semantic roles.
In this article, we attempt to alleviate some of these problems by using semi-
supervised methods that make use of a small number of manually labeled training
instances and a large number of unlabeled instances. Whereas manually labeled data are
expensive to create, unlabeled data are often readily available in large quantities. Our
approach aims to improve the performance of a supervised SRL system by enlarging
its training set with automatically inferred annotations of unlabeled sentences. The key
idea of our approach is to find novel instances for classifier training based on their simi-
larity to manually labeled seed instances. The underlying assumption is that sentences
that are similar in their lexical material and syntactic structure are likely to share a frame
semantic analysis. The annotation of an unlabeled sentence can therefore be inferred from
a sufficiently similar labeled sentence. For example, given the labeled sentence (3) and
the unlabeled sentence (4), we wish to recognize that they are lexically and structurally
similar; and infer that thumped also evokes the IMPACT frame, whereas the rest of his body
and against the front of the cage represent the Impactor and Impactee roles, respectively.
(3) [His back]Impactor [thudded]IMPACT [against the wall]Impactee.
(4) The rest of his body thumped against the front of the cage.
We formalize the detection of similar sentences and the projection of role annota-
tions in graph-theoretic terms by conceptualizing the similarity between labeled and
unlabeled sentences as a graph alignment problem. Specifically, we represent sentences
as dependency graphs and seek an optimal (structural) alignment between them. Given
this alignment, we then project annotations from the labeled onto the unlabeled sen-
tence. Graphs are scored using a function based on lexical and syntactic similarity which
allows us to identify alternations like those presented in Example (1) and more generally
to obtain training instances with novel structure and lexical material. We obtain the best
scoring graph alignment using integer linear programming, a general-purpose exact
optimization framework. Importantly, our approach is not tied to a particular SRL
system. We obtain additional annotations irrespective of the architecture or implemen-
tation details of the supervised role labeler that uses them. This renders our approach
portable across learning paradigms, languages, and domains.
After discussing related work (Section 2), we describe the details of our semi-
supervised method (Section 3) and then move on to evaluate its performance (Section 4).
We conduct two sets of experiments using data from the FrameNet corpus: In Section 5,
we apply our method to increase the training data for known predicates, that is, words
for which some seed annotations already exist. In Section 6, we focus on the comple-
mentary task of creating training instances for unknown predicates, that is, words that
do not occur in the FrameNet corpus at all. Section 7 concludes the article.
137
Computational Linguistics Volume 38, Number 1
2. Related Work
The lack of annotated data presents an obstacle to developing many natural language
applications, especially for resource-poor languages. It is therefore not surprising that
previous efforts to reduce the need for semantic role annotation have focused primarily
on languages other than English.
Annotation projection is a popular framework for transferring semantic role anno-
tations from one language to another while exploiting the translational and structural
equivalences present in parallel corpora. The idea here is to leverage the existing En-
glish FrameNet and rely on word or constituent alignments to automatically create an
annotated corpus in a new language. Pado? and Lapata (2009) transfer semantic role
annotations from English onto German and Johansson and Nugues (2006) from English
onto Swedish. A different strategy is presented in Fung and Chen (2004), where English
FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for
Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates
instantiating these concepts are found in a monolingual corpus and their arguments are
labeled with FrameNet roles.
Other work attempts to alleviate the data requirements for semantic role labeling
within the same language either by increasing the coverage of existing resources or by
inducing role annotations from unlabeled data. Swier and Stevenson (2004) propose
a method for bootstrapping a semantic role labeler. Given a verb instance, they first
select a frame from VerbNet, a semantic role resource akin to FrameNet and PropBank,
and label each argument slot with sets of possible roles. Their algorithm then proceeds
iteratively by first making initial unambiguous role assignments, and then successively
updating a probability model on which future assignments are based. Gordon and
Swanson (2007) attempt to increase the coverage of PropBank. Their approach leverages
existing annotations to handle novel verbs. Rather than annotating new sentences that
contain novel verbs, they find syntactically similar verbs and use their annotations as
surrogate training data.
Much recent work has focused on increasing the coverage of FrameNet, either
by generalizing semantic roles across different frames or by determining the frame
membership of unknown predicates. Matsubayashi, Okazaki, and Tsujii (2009) propose
to exploit the relations between semantic roles in an attempt to overcome the scarcity
of frame-specific role annotations. They propose several ways of grouping roles into
classes based on the FrameNet role hierarchy, human-understandable descriptors of
roles, selectional restrictions, and a FrameNet to VerbNet role mapping. They show that
transforming this information into feature functions and incorporating it into super-
vised learning improves role classification considerably.
The task of relating known frames to unknown predicates is addressed primarily by
resorting to WordNet (Fellbaum 1998). For example, Burchardt, Erk, and Frank (2005)
apply a word sense disambiguation system to annotate predicates with a WordNet sense
and hyponyms of these predicates are then assumed to evoke the same frame. Johansson
and Nugues (2007b) treat this problem as an instance of supervised classification. Using
a feature representation based also on WordNet, they learn a classifier for each frame,
which decides whether an unseen word belongs to the frame or not. Pennacchiotti
et al (2008) create ?distributional profiles? for frames. The meaning of each frame is
represented by a vector, which is the (weighted) centroid of the vectors representing
the predicates that can evoke it. Unknown predicates are then assigned to the most
similar frame. They also propose a WordNet-based model that computes the similarity
between the synsets representing an unknown predicate and those activated by the
138
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
predicates of a frame (see Section 6 for details). Das et al (2010) represent a departure
from the WordNet-based approaches in their use of a latent variable model to allow for
the disambiguation of unknown predicates.
Unsupervised approaches to SRL have been few and far between. Abend, Reichart,
and Rappoport (2009) propose an algorithm that identifies the arguments of predicates
by relying only on part-of-speech annotations, without, however, assigning their se-
mantic roles. In contrast, Grenager and Manning (2006) focus on role induction which
they formalize as probabilistic inference in a Bayesian network. Their model defines
a joint probability distribution over a verb, its semantic roles, and possible syntactic
realizations. More recently, Lang and Lapata (2010) formulate the role induction prob-
lem as one of detecting alternations and finding a canonical syntactic form for them.
Their model extends the logistic classifier with hidden variables and is trained on parsed
output which is used as a noisy target for learning.
Our own work aims to reduce but not entirely eliminate the annotation effort
involved in semantic role labeling. We thus assume that a small number of manual an-
notations is initially available. Our algorithm augments these with unlabeled examples
whose roles are inferred automatically. We apply our method in a monolingual setting,
and thus do not project annotations between languages but within the same language.
Importantly, we acquire new training instances for both known and unknown pred-
icates. Previous proposals extend FrameNet with novel predicates without inducing
annotations that exemplify their usage. We represent labeled and unlabeled instances
as graphs, and seek to find a globally optimal alignment between their nodes, subject to
semantic and structural constraints. Finding similar labeled and unlabeled sentences is
reminiscent of paraphrase identification (Qiu, Kan, and Chua 2006; Wan et al 2006; Das
and Smith 2009; Chang et al 2010), the task of determining whether one sentence is a
paraphrase of another. The sentences we identify are not strictly speaking paraphrases
(even if the two predicates are similar their arguments often are not); however, the
idea of modeling the correspondence structure (or alignment) between parts of the
two sentences is also present in the paraphrase identification work (Das and Smith
2009; Chang et al 2010). Besides machine translation (Matusov, Zens, and Ney 2004;
Taskar, Lacoste-Julien, and Klein 2005), methods based on graph alignments have been
previously employed for the recognition of semantic entailments (Haghighi, Ng, and
Manning 2005; de Marneffe et al 2007), where an optimization problem similar to
ours is solved using approximate techniques (our method is exact) and an alignment
scoring function is learned from annotated data (our scoring function does not require
extensive supervision). On a related note, de Salvo Braz et al (2005) model entail-
ments via a subsumption algorithm that operates over concept graphs representing
a source S and target T sentence and uses integer linear programming to prove that
S  T.
3. Method
In this section we describe the general idea behind our semi-supervised algorithm and
then move on to present our specific implementation. Given a set L of sentences labeled
with FrameNet frames and roles (the seed corpus) and a (much larger) set U of unla-
beled sentences (the expansion corpus), we wish to automatically create a set X ? U
of novel annotated instances. Algorithm 1 describes our approach, which consists of
two parts. In the labeling stage, annotations are proposed for every unlabeled sentence
(lines 1?20), and in the selection stage, instances with high quality annotations are
chosen to make up the final new corpus (lines 21?26).
139
Computational Linguistics Volume 38, Number 1
In the labeling stage, (almost) every unlabeled sentence u ? U receives an annota-
tion via projection from the seed l? ? L most similar to it. In theory, this means that each
unlabeled sentence u is compared with each labeled seed l. In practice, however, we
reduce the number of comparisons by requiring that u and l have identical or at least
similar FEEs. This process will yield many sentences for every seed with annotations
of varying quality. In default of a better way of distilling high-quality annotations, we
use similarity as our criterion in the selection stage. From the annotations originating
from a particular seed, we therefore collect the k instances with the highest similarity
values. Our selection procedure is guided by the seeds available rather than the corpus
from which unlabeled sentences are extracted. This is intended, as the seeds can be
used to create a balanced training set or one that exemplifies difficult or rare training
instances.
In the remainder of this section, we present the labeling stage of our algorithm in
more detail. Section 3.1 formally introduces the notion of semantically labeled depen-
dency graphs and defines the subgraphs M and N representing relevant predicate?
argument structures. Section 3.2 formalizes alignments as mappings between graph
nodes and defines our similarity score as a function on alignments between labeled
and unlabeled dependency graphs. Section 3.3 formulates an integer linear program
140
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
(ILP) for finding optimal alignments, and Section 3.4 presents an efficient algorithm
for solving this ILP. Finally, Section 3.5 describes how annotations are projected from
labeled onto unlabeled graphs.
3.1 Semantically Labeled Dependency Graphs
Seed sentences labeled with role-semantic annotations are represented by dependency
graphs. The latter capture grammatical relations between words via directed edges
from syntactic heads to their dependents (e.g., from a verb to its subject or from a
noun to a modifying adjective). Edges can be labeled to indicate the type of head?
dependent relationship (e.g., subject, object, modifier). In our case, dependency graphs
are further augmented with FrameNet annotations corresponding to the FEE and its
semantic roles.
A dependency graph of the sentence Old Herkimer blinked his eye and nodded wisely
is shown in Figure 1. Nodes are indicated by rectangles and dependencies by edges
(arrows). Solid arrows represent syntactic dependencies (e.g., subject, object), and
dashed arrows correspond to FrameNet annotations. Here, blink evokes the frame
Body movement, Herkimer bears the role Agent, and eye the role Body part.
Unfortunately, FrameNet annotations have not been created with dependency
graphs in mind. FEEs and roles are marked as substrings and contain limited syntac-
tic information, distinguishing only the grammatical functions ?external argument,?
?object,? and ?dependent? for the arguments of verbal FEEs. To obtain dependency
graphs with semantic annotations like the one shown in Figure 1, we parse the sentences
in the seed corpus with a dependency parser and compare the FrameNet annotations
(substrings) to the nodes of the dependency graph. For the FEE, we simply look for a
graph node that coincides with the word marked by FrameNet. Analogously, we map
Figure 1
Dependency graph with semantic annotations for the sentence Old Herkimer blinked his eye and
nodded wisely (taken from the FrameNet corpus). Nodes in the alignment domain are indicated
by double frames. Labels in italics denote frame roles, and grammatical roles are rendered in
small capitals. Annotations are only shown for the predicate blink, which evokes the frame
Body Movement.
141
Computational Linguistics Volume 38, Number 1
role annotations onto the graph by finding a node with a yield equal to the marked
substring, that is, a node that (together with its dominated nodes) represents the words
of the role. Our experiments make use of the dependency graphs produced by RASP
(Briscoe, Carroll, and Watson 2006), although there is nothing inherent in our approach
that assumes this specific parser. Any other dependency parser with broadly similar
output could be used instead.
Searching for nodes representing the FEE and its semantic roles may in some cases
yield no match. There are two reasons for this?parser errors and role annotations vio-
lating syntactic structure. We address this problem heuristically: If no perfect match is
found, the closest match is determined based on the number of mismatching characters
in the string. We thus compute a mismatch score for the FEE and each role. To make
allowances for parser errors, we compute these scores for the n-best parses produced
by the dependency parser and retain the dependency graph with the lowest mismatch.
This mapping procedure is more thoroughly discussed in Fu?rstenau (2008).
Each sentence in the seed corpus contains annotations for a predicate and its se-
mantic roles. A complex sentence (with many subordinate clauses) will be represented
by a large dependency graph, with only a small subgraph corresponding to these
annotations. Our method for computing alignments between graphs only considers
subgraphs with nodes belonging to the predicate-argument structure in question. This
allows us to compare graphs in a computationally efficient manner as many irrelevant
alignments are discarded, although admittedly the entire graph may provide useful
contextual clues to the labeling problem.
We are now ready to define the alignment domain M of a labeled dependency
graph. Let p be a node (i.e., word) in the graph corresponding to the FEE. If there are
no mismatches between semantic and syntactic arguments, we expect all roles in the
graph to be instantiated by syntactic dependents of p. Although this is often the case, it
does not always hold?for example, because of the way the dependency parser analyzes
raising, control, or coordination structures. We therefore cannot simply define M as
the set of direct dependents of the predicate, but also have to consider complex paths
between p and role-bearing nodes. An example is given in Figure 1, where the role Agent
is filled by a node that is not dominated by the FEE blink; instead, it is connected to blink
by the complex path (CONJ?1, SUBJ). For a given sentence, we build the set of all such
complex paths to any role-bearing node and also include all nodes connected to p by
one of these paths. We thus define the subgraph M to contain:
i. the predicate node p
ii. all direct dependents of p, except auxiliaries
iii. all nodes on complex paths from p to any role-bearing node
iv. single direct dependents of any preposition or conjunction node which is
in (ii) or end-point of a complex path covered in (iii)
In Figure 1 the nodes in the alignment domain are indicated by double frames.
In an unlabeled dependency graph we similarly identify the alignment range as the
subgraph corresponding to the predicate?argument structure of a target predicate. As
we do not have any frame semantic analysis for the unlabeled sentence, however, we
cannot determine a set of complex paths. We could ignore complex paths altogether and
thus introduce a substantial asymmetry into the comparison between a labeled and an
unlabeled sentence, as unlabeled sentences would be assumed to be structurally simpler
142
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
than labeled ones. This assumption will often be wrong and moreover introduce a bias
towards simpler structures for the new annotations. To avoid this, we reuse the set of
complex paths from the labeled sentence. Although this is not ideal either (it makes
the comparison asymmetrically dependent on the annotation of the labeled sentence)
it allows us to compare labeled and unlabeled sentences on a more equal footing. We
therefore define the alignment range N in exact analogy to the alignment domain M, the
only exception being that complex paths to role-bearing nodes are determined by the
labeled partner in the comparison.
3.2 Scoring Graph Alignments
We conceptualize the similarity between subgraphs representing predicate?argument
structures as an alignment problem. Specifically, we seek to find an optimal alignment
between the alignment domain M of a labeled graph and the alignment range N of
an unlabeled sentence. Alignments are scored using a similarity measure that takes
syntactic and lexical information into account.
We formalize the alignment between M and N as a partial injective function from
M to N, that is, a function ? : M ? N ? {} where ?(x) = ?(x?) =  implies x = x?.
Here,  denotes a special empty value. We say that x ? M is aligned to x? ? N by ?, iff
?(x) = x?. Correspondingly, a node x ? M with ?(x) =  or a node x? ? N that is not the
image of any x ? M is called unaligned. Figure 2 shows an example of an alignment
Figure 2
The dotted arrows show aligned nodes in the graphs for the two sentences His back thudded
against the wall and The rest of his body thumped against the front of the cage (graph edges are also
aligned to each other). The nodes in the alignment domain and alignment range are indicated by
double frames.
143
Computational Linguistics Volume 38, Number 1
between a labeled and an unlabeled dependency graph for the predicates thud and
thump.
Each alignment ? between M and N receives a score, the weighted sum of the lexical
similarity between nodes (lex) and syntactic similarity between edges (syn):
score(?) := 1
C
?
?
?
?
?
x?M
?(x)=
lex (x,?(x)) + ? ?
?
(x1,x2 )?E(M)
(?(x1),?(x2 ))?E(N)
syn
(
rx1x2 , r
?(x1)
?(x2)
)
?
?
?
?
(1)
Here, E(M) and E(N) denote the sets of graph edges between the nodes of M and N,
respectively, while rx1x2 is the label of the edge (x1, x2), that is, the grammatical relation
between these two nodes.
Equation (1) introduces a normalizing factor C whose purpose is to render similarity
scores of different pairs of sentences comparable. Without normalization, it would be
easier to achieve high similarity to a complex predicate?argument structure than a
simpler one, which is counter-intuitive. This can be seen from the fact that the self-
similarity of a sentence (i.e., the similarity of a sentence to itself) depends on the number
of nodes in M. Assuming that the maximal value for lex and syn is 1 for identical
words and grammatical relations, self-similarity is then |M|+ ?|E(M)| and constitutes
an upper bound for the similarity between any two sentences. We could use this term
to normalize the similarity score. However, this would only account for unaligned or
badly aligned nodes and edges in the labeled sentence while ignoring the unlabeled
partner. To obtain a symmetric normalization factor we therefore define:
C :=
?
|M| ? |N|+ ?
?
|E(M)| ? |E(N)| (2)
C is now symmetric in the two sentences and when introduced in equation (1) leads to
self-similarities of 1:
score(?self) =
1
?
|M|2 + ?
?
E(M)2
(|M| ? 1 + ? ? |E(M)| ? 1) = 1 (3)
Notice that our formulation uses the same score for finding whether there exists
an alignment and for evaluating its quality. Consequently, our algorithm will attempt
to construct an alignment even if there is none, that is, in cases where the similarity
between labeled and unlabeled sentences is low. Our approach is to filter out erroneous
alignments by considering only the k nearest neighbors of each seed. Alternatively, we
could first establish valid alignments and then score them; we leave this to future work.
The employed score is the weighted combination of lexical and syntactic similarity. In
our experiments we use cosine similarity in a vector space model of co-occurrence statis-
tics for lex and define syn as a binary function reflecting the identity of grammatical
relations (see Section 4 for details). Other measures based on WordNet (e.g., Budanitsky
and Hirst 2001) or finer grammatical distinctions are also possible.
3.3 ILP Formulation
We define the similarity of two predicate?argument structures as the maximum score
of any alignment ? between them. Intuitively, the alignment score corresponds to the
amount of changes required to transform one graph into the other. High scores indicate
144
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
high similarity and thus minimal changes. We do not need to formalize such changes,
although it would be possible to describe them in terms of substitutions, deletions,
and insertions. For our purposes, the alignment scores themselves can be used to
indicate whether two graphs are substantially similar to warrant projection of the frame
semantic annotations. We do this by finding an optimal alignment, that is, an alignment
with the highest score as defined in Equation (1).
To solve this optimization problem efficiently, we recast it as an integer linear pro-
gram (ILP). The ILP modeling framework has been recently applied to a wide range of
natural language processing tasks, demonstrating improvements over more traditional
optimization methods. Examples include reluctant paraphrasing (Dras 1999), relation
extraction (Roth and tau Yih 2004), semantic role labeling (Punyakanok et al 2004),
concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), de-
pendency parsing (Riedel and Clarke 2006), sentence compression (Clarke and Lapata
2008), and coreference resolution (Denis and Baldridge 2007). Importantly, the ILP
approach3 delivers a globally optimal solution by searching over the entire alignment
space without employing heuristics or approximations (see de Marneffe et al [2007]
and Haghighi, Ng, and Manning [2005]). Furthermore, an ILP-based formulation seems
well-suited to our problem because the domain of the optimization, namely, the set of
partial injective functions from M to N, is discrete. We define arbitrary linear orders on
the sets M and N, writing M = {n1, . . . , nm} and N = {n?1, . . . , n?n} and then introduce
binary indicator variables xij to represent an alignment ?:
xij :=
{
1 if ?(ni) = n?j
0 else
(4)
Each alignment ? thus corresponds to a distinct configuration of xij values. In order
to ensure that the latter describe a partial injective function, we enforce the following
constraints:
1. ?j :
?
1?i?m xij ? 1 (Each node in N is aligned to at most one node in M.)
2. ?i :
?
1?j?n xij ? 1 (Each node in M is aligned to at most one node in N.)
We can now write Equation (1) in terms of the variables xij (which capture exactly the
same information as the function ?):
score(x) = 1
C
?
?
?
?
?
1?i?m
1?j?n
lex
(
ni, n
?
j
)
xij + ? ?
?
1?i,k?m
1?j,l?n
syn
(
rnink , r
n?j
n?l
)
xijxkl
?
?
?
?
(5)
Note that Equations (1) and (5) are summations of the same terms.4 However,
Equation (5) is not linear in the variables xij as it contains products of the form xijxkl.
3 It is outside the scope of this article to provide an introduction to ILP. We refer the interested reader to
Winston and Venkataramanan (2003) and Vanderbei (2001) for comprehensive overviews.
4 For convenience, we define rn1n2 =  if there is no relation between n1 and n2, and assume that syn is 0 if
either of its arguments is .
145
Computational Linguistics Volume 38, Number 1
This can be remedied through the introduction of another set of binary variables yijkl
subject to additional constraints ensuring that yijkl = xijxkl:
3. ?i,j,k,l : yijkl ? xij
4. ?i,j,k,l : yijkl ? xkl
5. ?i,j,k,l : yijkl ? xij + xkl ? 1
We also want to make sure that the FEE of the labeled sentence is aligned to the
target predicate of the unlabeled sentence. We express this with the following con-
straint, assuming that the FEE and the target predicate are represented by n1 and n?1,
respectively:
6. x11 = 1
We therefore have to solve an ILP in the mn + m2n2 variables xij and yijkl, subject to
m + n + 3m2n2 + 1 constraints (see constraints (1)?(6)), with the objective function:
score(x, y) = 1
C
?
?
?
?
?
1?i?m
1?j?n
lex
(
ni, n
?
j
)
xij + ? ?
?
1?i,k?m
1?j,l?n
syn
(
rnink , r
n?j
n?l
)
yijkl
?
?
?
?
(6)
Exact optimization for the general ILP problem is NP-hard (Cormen, Leiserson, and
Rivest 1992). ILPs with a totally unimodular constraint matrix5 are solvable efficiently,
using polynomial time algorithms. In this special case, it can be shown that the optimal
solution to the linear program is integral. Unfortunately, our ILP falls outside this class
due to the relatively complex structure of our constraints. This can be easily seen when
considering the three constraints x11 + x12 + ? ? ?+ x1m ? 1, ?x11 + y1112 ? 0 and ?x12 +
y1112 ? 0. The coefficients of the three variables x11, x12, and y1112 in these constraints
make up the matrix
?
?
1 1 0
?1 0 1
0 ?1 1
?
?
The determinant of this matrix is 2 and therefore the complete coefficient matrix of the
ILP has a quadratic submatrix with a determinant that is not 0 or ?1, which means
that it is not totally unimodular. Indeed, it has been shown that the structural matching
problem is NP-hard (Klau 2009).
3.4 Solving the ILP
There are various techniques for finding the optimal solution of an ILP, such as ap-
proximation with error bounds (Klau 2009) or application of the branch-and-bound
5 A matrix A is totally unimodular if every square sub-matrix of A has its determinant equal to 0, +1, or ?1.
146
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
algorithm (Land and Doig 1960). The latter allows for solving an ILP exactly and signif-
icantly faster than by naive enumeration. It does this by relaxing the integer constraints
and solving the resulting LP problem, known as the LP relaxation. If the solution of
the LP relaxation is integral, then it is the optimal solution. Otherwise, the resulting
solution provides an upper bound on the solution for the ILP. The algorithm proceeds
by creating two new sub-problems based on the non-integer solution for one variable
at a time. These are solved and the process repeats until the optimal integer solution
is found. Our alignment problem has only binary variables and is thus an instance of
a ?pure? 0?1 ILP. For such problems, implicit enumeration can be used to simplify
both the braching and bounding components of the branch-and-bound process and
to determine efficiently when a node is infeasible. This is achieved by systematically
evaluating all possible solutions, without, however, explicitly solving a potentially large
number of LPs derived from the relaxation.
To obtain a solution for the ILP in Section 3.3, we could have used any solver that
implements the standard branch-and-bound algorithm. To speed up computation time,
we have instead modified the branch-and-bound algorithm so as to take into account
the special structure of our graph alignment problem. Our own algorithm follows the
principles of branch-and-bound but avoids explicit representation of the variables yijkl,
performs early checks of the constraints on the variables xij on branching, and takes
into account some of the constraints on the variables yijkl for the estimation of lower
and therefore more efficient bounds. In the following, we first describe our modified
algorithm and then assess its runtime in comparison to a publicly available solver.
Algorithm 2 shows how to find an optimal alignment ?? with score s? in pseu-
docode. ?0 and ?1 denote partial solutions, while completions are built in ?. syn? is the
maximum possible value of syn, that is, syn? = 1 for a binary measure. We initialize ??
with the trivial solution which aligns n1 to n?1 and leaves all other nodes unaligned.
6
This gives a score of lex(n1, n?1). To find better solutions we start with an initial partial
alignment ?0, which contains only the mapping n1 ? n?1 and leaves the alignments of
all other n ? M unspecified. (Note that this is different from the complete alignment
?? which specifies those nodes as unaligned: n ? .) As in the general branch-and-
bound algorithm, the space of all alignments is searched recursively by branching on the
alignment decision for each remaining node. A branch is left as soon as an upper bound
on the achievable score indicates that the current best solution cannot be improved
within this branch.
Given a partial alignment ?0 (the initial or any subsequent one) defined on some
subset of M, we estimate a suitable bound by extending ?0 to a complete function ? on
all nodes in M: Each of the remaining nodes is aligned to its partner in N maximizing lex.
If no positive value can be found for lex, the node is defined as unaligned. We then
define the bound s as the score of ?0 together with the lexical scores of the newly created
alignments and a hypothetical syntactic score which assumes that each of the newly
considered edges is aligned perfectly, that is, with the maximum value syn? attainable
by syn. (This is a lower bound than the one a naive application of the branch-and-bound
algorithm would compute.)
Of course, ? need not fulfill the constraints of the ILP and s need not be an attainable
score. It is, however, an upper bound for the score of any valid alignment. If it is not
6 In the description of the algorithm, we use the more intuitive notation ni ? n?j to indicate that ni is
aligned to n?j . Note, however, that this could be equivalently formulated in terms of the ILP variables
(i.e., xij = 1), and our algorithm still broadly follows the branch-and-bound procedure for ILPs.
147
Computational Linguistics Volume 38, Number 1
greater than the current best score s?, we leave the current branch. Otherwise, we check
if ? is a valid alignment with score s, that is, if it satisfies the constraints of the ILP
and s is its score (which means that the assumptions of perfect syntactic scores were
justified). If this is the case, we have a new current optimum and do not need to follow
the current branch any more either. If, however, the bound s is greater than the current
optimum s?, but ? violates some constraints or does not achieve a score of s because it
contains imperfect syntactic alignments, we have to branch on the decision of how to
extend ?0 by an additional alignment link. We consider the next node with unspecified
alignment and recursively apply the algorithm to extensions of ?0. Each extension ?1
aligns this node to a partner in N that has thus far been left unaligned. (This simple
148
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
check of constraint (1), which extends the general branch-and-bound algorithm, avoids
recursion into branches that cannot contain any valid solutions.) The partial score s1
corresponding to ?1 is computed by taking into account the consequences of the new
alignment to the lexical and syntactic scores.
We found this algorithm to be very effective in solving the ILPs arising in our
experiments. While its worst case performance is still exponential in the number of
aligned nodes and edges, it almost always finds the optimum within a relatively small
number of iterations of the outer loop (line 4 in Figure 2). This is also due to the fact that
the alignment domain and range are typically not very large. In a realistic application of
our method, 70% of the ILPs were solvable with less than 100 iterations, 93% with less
than 1,000 iterations, 98.6% with less than 10,000 iterations, and 99.95% with less than
1,000,000 iterations. As the remaining 0.05% of the ILPs may still take an inordinate
amount of time, we abort the search at this point. In this case, it is highly likely that the
alignment domain and range are large and any resulting alignment would be overly
specific and thus not very useful. Aborting at 1,000,000 iterations is also preferable to a
time-out based on processing time, as it makes the result deterministic and independent
of the specific implementation and hardware. All expansion sets in the experiments
described in Sections 5 and 6 were computable within hours on modern hardware and
under moderate parallelization, which is trivial to implement over the instances of the
unlabeled corpus.
Because our branch-and-bound algorithm performs exact optimization, it could be
replaced by any other exact solution algorithm, without affecting our results. To assess
its runtime performance further, we compared it to the publicly available lp solve7
solver which can handle integer variables via the branch-and-bound algorithm. We
sampled 100 alignment problems for each problem size (measured in number of nodes
in the alignment domain) and determined the average runtime of our algorithm and
lp solve. (The latter was run with the option -time, which excludes CPU time spent
on input parsing). Figure 3 shows how the average time required to solve an ILP
varies with the problem size. As can be seen, our algorithm is about one order of
magnitude more efficient than the implementation of the general-purpose branch-and-
bound algorithm.
3.5 Annotation Projection
Given a labeled graph l, an unlabeled graph u, and an optimal alignment ? between
them, it is relatively straightforward to project frame and role information from one to
the other. As described in Section 3.1, frame names are associated with the nodes of their
FEEs and role names with the nodes of their role filler heads. By definition, all of these
nodes are in the alignment range M. It is therefore natural to label ?(x) ? N with the
role carried by x for each role-bearing node x ? M. The only complicating factor is that
we have allowed unaligned nodes, that is, nodes with ?(x) = . Although this is useful
for ignoring irrelevant nodes in M, we must decide how to treat these when they are
role-bearing (note that FEEs are always aligned by constraint (6), so frame names can
always be projected).
A possible solution would be to only project roles on nodes x with ?(x) = , so
that roles associated with unaligned nodes do not show up in the inferred annotation.
Unfortunately, allowing such partial projections introduces a systematic bias in favor
7 Version 5.5, available at http://lpsolve.sourceforge.net/.
149
Computational Linguistics Volume 38, Number 1
Figure 3
Average time required to solve an ILP as a function of the size of the alignment domain.
of simpler structures. When these new instances are used as a training set for a role
labeler, they will bias the classifier towards under-annotating roles and thus decrease
performance. We therefore do not want to allow partial projections and demand that
?(x) =  for all role-bearing nodes x.
We could incorporate this additional constraint into the ILP by finding a (lower scor-
ing) solution that satisfies it. However, there is no theoretical justification for favoring a
lower ranking alignment over the optimal one only because of projection requirements.
If lexical and structural measures tell us that a certain alignment is best, we should
not dismiss this information, but rather take the contradiction between the optimal
alignment and the frame semantic (non-)projectability to indicate that l is not suitable
for inferring a labeling of u. There are several possible reasons for this, ranging from
idiosyncratic annotations to parser or pre-processing errors. We therefore do not discard
the optimal alignment in favor of a lower scoring one, but rather dismiss the seed l as a
source of information for inferring a labeling on u. This reflects our precision-oriented
approach: If u does not find a better partner among the other seeds, it will be discarded
as unsuitable for the expansion set.
4. Experimental Set-up
In this section, we describe the data and supervised semantic role labeler used in our
experiments and explain how the free parameters of our method were instantiated. We
then move on to present two experiments that evaluate our semi-supervised method.
4.1 Data
In our experiments, we use various subsets of the English FrameNet corpus (version 1.3;
Fillmore, Johnson, and Petruck 2003) as seed sets for our semi-supervised method and
as test sets in our evaluation. We only consider sentences with verbal FEEs (60,666 in
total). Furthermore, we always assume that an oracle identifies the verbal predicate, so
recognition of the FEE is not part of our evaluation. Unlabeled sentences for expansion
150
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Table 1
Features used by the frame classifier. Example values for the annotated graph in Figure 1 are
given in parentheses.
Feature Type Description and example value
target lemma atomic lemma of the target node (blink)
frames set frames that can be evoked by the target verb
({BODY MOVEMENT})
voice binary voice of the target node (active)
parent word set lemma of the parents of the target node ({and})
parent POS set part of speech of the parents of the target node ({CC})
rel to parent set grammatical relations between the target node and its
parents ({CONJ})
parent has obj binary whether any parents have an outgoing ?object?
relation (no)
dsubcat atomic subcategorization frame, the multi-set of all outgoing
relations of the target node (DOBJ)
child word set set lemma of the children of the target node ({eye})
child dep set set outgoing relations of the target node ({DOBJ})
child word dep set set pair (lemma, relation) for the children of the target
node ({(eye, DOBJ)})
were taken from the British National Corpus (BNC), excluding sentences with manual
annotations in FrameNet. The BNC is considerably larger compared with FrameNet,
approximately by a factor of 100. Dependency graphs were produced with RASP
(Briscoe, Carroll, and Watson 2006). Frame semantic annotations for labeled sentences
were merged with their dependency-based representations as described in Section 3.1.
Sentences for which this was not possible (mismatch score greater than 0) were excluded
from the seed set, but retained in the test sets to allow for unbiased evaluation. For unla-
beled BNC sentences, we used an existing RASP-parsed version of the BNC (Andersen
et al 2008).
4.2 Supervised SRL System
A natural way of evaluating the proposed semi-supervised method is by comparing
two instantiations of a supervised SRL system, one that is trained solely on FrameNet
annotations and one that also uses the additional training instances produced by our
algorithm. We will henceforth use the term unexpanded to refer to the corpus (and sys-
tem trained on it) that contains only human-annotated instances, and accordingly, the
term expanded to describe the corpus (and system) resulting from the application of our
method or any other semi-supervised approach that obtains training instances automat-
ically. As our approach is based on dependency graphs, we employed a dependency-
based SRL system for evaluation.8
We thus implemented a supervised SRL system based on the features proposed
by Johansson and Nugues (2007a). Many of these features have been found useful in
a number of previous SRL systems, and can be traced back to the seminal work of
Gildea and Jurafsky (2002). Our own implementation uses the features listed in Tables 1
and 2 for frame labeling and role labeling, respectively. Atomic features are converted
8 Semantic role labelers that take advantage of dependency information perform comparably to those that
rely on phrase structure trees (Johansson 2008).
151
Computational Linguistics Volume 38, Number 1
Table 2
Features used by the role classifiers. Example values for the Body part role of the annotated
graph in Figure 1 are given in parentheses.
Feature Type Description and example value
target lemma atomic lemma of the FEE (blink)
target POS atomic part of speech of the FEE (VVD)
roles set roles that can feature in the given frame ({Agent, Body part,
Addressee, ...})
voice binary voice of the FEE (active)
parent word set lemma of the parents of the FEE ({and})
parent POS set part of speech of the parents of the FEE ({CC})
rel to parent set grammatical relation between the FEE and its parents ({CONJ})
parent has obj binary whether any parents have an outgoing ?object? relation (no)
dsubcat atomic subcategorization frame, multi-set of all outgoing relations
of the FEE (DOBJ)
child dep set set outgoing relations of the FEE ({DOBJ})
arg word atomic lemma of the argument (eye)
arg POS atomic part of speech of the argument (NN1)
position atomic position of the argument (before, on, or after) in the sentence,
relative to the FEE (after)
left word atomic lemma of the word to the left of the argument in the sentence (his)
left POS atomic part of speech of the word to the left of the argument in the sentence
(APP$)
right word atomic lemma of the word to the right of the argument in the sentence (and)
right POS atomic part of speech of the word to the right of the argument in the
sentence (CC)
path atomic path of grammatical relations between FEE and argument (DOBJ)
function set relations between argument and its heads ({DOBJ})
into binary features of the SVM by 1-of-k coding, and for set features each possible set
element is represented by its own binary feature. (Features pertaining to parent nodes
are set features as we do not require our dependency graphs to be trees and a node
can therefore have more than one parent.) We followed a classical pipeline architecture,
first predicting a frame name for a given lexical unit, then identifying role-bearing
dependency graph nodes, and finally labeling these nodes with specific roles. All three
classification stages were implemented as support vector machines, using LIBLINEAR
(Fan et al 2008). The frame classifier is trained on instances of all available predicates,
while individual role classifiers are trained for each frame. The one-vs-one strategy
(Friedman 1996) was employed for multi-classification.
We evaluate the performance of the SRL system on a test set in terms of frame accu-
racy and role labeling F1. The former is simply the relative number of correctly identified
frame names. The latter is based on the familiar measure of labeled F1 (the harmonic
mean of labeled precision and recall). When a frame is labeled incorrectly, however, we
assume that its roles are also misclassified. This is in agreement with the notion of frame-
specific roles. Moreover, it allows us to compare the performance of different classifiers,
which would not be possible if we evaluated role labeling performance on changing test
sets, such as the set of only those sentences with correct frame predictions.
The misclassification penalty C for the SVM was optimized on a small training set
consisting of five annotated sentences per predicate randomly sampled from FrameNet.
We varied C for the frame classification, role recognition, and role classification SVMs
152
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
between 0.01 and 10.0 and measured F1 on a test set consisting of 10% of FrameNet (see
Section 5.1). For frame and role classification, we did not observe significant changes
in F1 and therefore maintained the default of C = 1.0. For role recognition, we obtained
best performance with C = 0.1 (F1 was 38.78% compared to 38.04% with the default
C = 1), which we subsequently used for all our experiments. All other SVM parameters
were left at their default values.
4.3 Lexical and Syntactic Similarity
Our definition of the lexical similarity measure, lex, uses a vector space model of word
co-occurrence which we created from a lemmatized version of the BNC. Specifically, we
created a semantic space with a context window of five words on either side of the target
word and the most common 2,000 context words as vector dimensions. Their values
were set to the ratio of the probability of the context word given the target word to the
probability of the context word overall. Previous work shows that this configuration is
optimal for measuring word similarity (Mitchell and Lapata 2010; Bullinaria and Levy
2007). In our specific setting, lex is then simply the cosine of the angle between the
vectors representing any two words.9
For the syntactic measure syn we chose the simplest definition possible: syn(r, r?) is 1
if r and r? denote the same grammatical relation (r = r?), and 0 otherwise. We also con-
sidered more sophisticated definitions based on different degrees of similarity between
grammatical relations, but were not able to find parameters performing consistently
better than this simple approach.
A crucial parameter in the formulation of our similarity score (see Equation (1)) is
the relative weight ? of syntactic compared to lexical similarity. Intuitively, both types
of information should be taken into account, as favoring one over the other may yield
sentences with either similar structure or similar words, but entirely different meaning.
This suggests that ? should be neither very small nor very large and will ultimately also
depend on the specific measures used for lex and syn.
We optimized ? on a development set using F1 score as the objective function.
Specifically, we used a random sample of 20% of the FrameNet instances as seed
corpus and expanded it with instances from the BNC using different values for ?. For
each seed sentence, the most similar neighbor was selected (i.e., k = 1). We evaluated
performance of the role labeler enhanced with automatic annotations on a test set
consisting of another random 10% of the FrameNet instances. (These development and
test sets were not used in any of the subsequent experiments.) The parameter ? ranges
between 0 (using only lexical information) and ? (using only syntactic information).
We therefore performed a grid search on a logarithmic scale, varying log? between ?3
and 3 with steps of size 0.2. We also computed performance in the extreme cases of
log? = ??.
Figure 4 shows the results of the tuning procedure. With the exception of ? = ??
(i.e., ignoring syntactic information) all expansions of the seed corpus lead to better
role labelers in terms of F1. Furthermore, extreme values of ? are clearly not as good as
values that take both types of information into account. The optimal value according
to this tuning experiment is log? = ?0.6. Finer tuning of the parameter will most
9 Experiments with off-the-shelf WordNet-based similarity measures did not yield performance superior to
the cosine measure (see Fu?rstenau [2011] for details).
153
Computational Linguistics Volume 38, Number 1
Figure 4
Performance of our method on the development set for different values of the ? parameter. The
baseline is the performance of a semantic role labeler trained on the seed set.
likely not yield improvements, as the differences in F1 are already relatively small.
We therefore set ? = e?0.6 ? 0.55 for all further experiments. This means that lex is
weighted approximately twice as strongly as syn.
5. Experiment 1: Known Verbs
In this section, we describe a first set of experiments with the aim of automatically
creating novel annotation instances for SRL training. We assume that a small number
of manually labeled instances are available and apply our method to obtain more
annotations for the FEEs attested in the seed corpus. The FEE of the labeled sentence and
the target verb of the unlabeled sentence are presumed identical. However, we waive
this restriction in Experiment 2, where we acquire annotations for unknown FEEs, that
is, predicates for which no manual annotations are available.
5.1 Method
We applied our expansion method to seed corpora of different sizes. A random sample
of 60% of the FrameNet instances was used as training set and 10% as test set (the
remaining 30% were used as development set for tuning the ? parameter). The training
set was reduced in size by randomly choosing between 1 and 10 annotated instances
per FEE. These reduced sets are our seed corpora. We first trained the supervised SRL
system on each of these seed corpora. Next, we used our expansion method to add the
k nearest neighbors of each seed instance to the training corpus, with k ranging from 1
to 6, and retrained the SRL classifiers.
We also compared our approach to self-training by selecting k sentences from the
unlabeled corpus, labeling them with the baseline classifier trained on the unexpanded
corpus (instead of applying our projection method), and then adding these to the
training corpus and retraining the classifier. Specifically, we employed three variants of
self-training. Firstly, unlabeled sentences were selected for each seed sentence randomly,
the only constraint being that both sentences feature the same FEE.
154
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Secondly, new instances were chosen according to a sentence similarity measure
shown to be highly competitive on a paraphrase recognition task (Achananuparp, Hu,
and Shen 2008). We used the measure proposed in Malik, Subramaniam, and Kaushik
(2007), which is a simpler variant of a sentence similarity measure originally described
in Mihalcea, Corley, and Strapparava (2006). Given two sentences or more generally text
segments Ti and Tj, their similarity is determined as follows:
sim(Ti, Tj) =
?
w?Ti
maxSim(w, Tj) +
?
w?Tj
maxSim(w, Ti)
|Ti|+ |Tj|
(7)
where maxSim(w, Tj) is the maximum similarity score between the word w in Ti and any
word in Tj with the same part of speech (i.e., noun, verb, adjective). A large number of
measures have been proposed in the literature for identifying word-to-word similarities
using corpus-based information, a taxonomy such as WordNet (Fellbaum 1998) or a
combination of both (see Budanitsky and Hirst [2001] for an overview). Here, we use
cosine similarity and the vector space model defined in Section 4.3.
Our third variant of self-training identified new instances according to our own
measure (see Section 4.3), which incorporates both lexical and syntactic similarity. The
different self-training settings allow us to assess the extent to which the success of
our method depends simply on the increase of the training data, the definition of the
sentence similarity measure, the alignment algorithm for annotation projection, or their
combination.
5.2 Results
Our results are summarized in Figure 5 (and documented exhaustively in the Ap-
pendix). Here, we only consider role labeling performance, that is, we use gold-standard
Figure 5
Role labeling F1 obtained by expanding seed corpora of different sizes: The dotted lines show
performance of unexpanded classifiers trained on two to six seed instances per verb. Each
solid line starts from such a baseline at k = 0 and for k > 0 shows the performance obtained
by adding the k nearest neighbors of each seed to the respective baseline corpus.
155
Computational Linguistics Volume 38, Number 1
frames of the test set and evaluate the role recognition and classification stages of the
classifiers. (Frame labeling accuracy will be evaluated in the following section.) The
dotted lines show the performance of unexpanded classifiers trained on two to six seed
instances per verb. The solid lines show the performance of our expanded classifiers
when the k nearest neighbors (of each seed instance) are added to the training set. So, to
give a concrete example, the unexpanded classifier trained on a corpus with two seeds
per verb yields an F1 of 35.94%. When the single nearest neighbors are added, F1 in-
creases to 36.63%, when the two nearest neighbors are added, F1 increases to 37.00%,
and so on.
As can be seen in Figure 5, most expansions lead to improved SRL performance. All
improvements for 1 ? k ? 5 are statistically significant (at p < 0.05 and p < 0.001) as
determined by stratified shuffling (Noreen 1989; see the Appendix for details). The only
exception is k = 5 for two seeds per FEE. We obtain largest improvements when k ranges
between 2 and 4, with a decline in performance for higher values of k. This illustrates the
trade-off between acquiring many novel annotations and inevitably introducing noise.
For progressively less similar neighbors, the positive effect of the former is out-weighted
by the detrimental effect of the latter.
It is also interesting to observe that automatically generated instances often have
a positive effect on role labeling performance similar to, or even larger than, manually
labeled instances. For example, the corpus with two seeds per FEE, expanded by two,
three or four nearest neighbors, leads to better performance than the corpus with three
manually labeled seeds; and an expanded version of the five seeds/FEE corpus closes
60% of the gap to the six seeds/FEE corpus. Generally, the positive effect of our expan-
sion method is largest for corpora with only a few seed instances per FEE. The results in
Figure 5 may seem low, especially with respect to the state of the art (see the discussion
in Section 1). Bear in mind, however, that the semantic role labeler is trained on a small
fraction of the available annotated data. This fits well with its intended application to
minimize annotation effort when creating resources for new languages or adapting to
new domains.
Figure 6 shows the results of self-training. Dotted lines again denote the perfor-
mance of unexpanded classifiers trained on seed corpora of different sizes (ranging
from two to five seeds per verb). The solid lines show the performance of these clas-
sifiers expanded with k neighbors. Figures 6(a)?6(c) correspond to different methods
for selecting the k-best sentences to add to the seed corpus (i.e., randomly, according
to the similarity function presented in Malik, Subramaniam, and Kaushik (2007), and
our own similarity measure that takes both syntactic and semantic information into
account). In all cases we observe that self-training cannot improve upon the baseline
classifier. Randomly selecting new sentences yields the lowest F1 scores, followed by
Malik, Subramaniam, and Kaushik and our own measure. Figure 6(d) compares the
three self-training methods in the five seeds per verb setting. These results indicate that
the ability to improve labeling performance is not merely due to selecting sentences
similar to the seeds. In other words, the graph alignment algorithm is worth the added
work as the projection of annotations contributes to achieving better SRL results.
To gain a better understanding of the quality of the annotations inferred by our
system, we further analyzed a small sample. Specifically, we randomly selected 100 seed
instances from the FrameNet corpus, and used 59,566 instances as the unlabeled ex-
pansion corpus, treating their gold standard annotations as unseen (the remaining
1,000 instances were held out as a separate test set, as discussed subsequently). Seed
and expansion corpora were thus proportionately similar to those used in our main
experiments (where seed instances in the range of [2,092?16,595] were complemented
156
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Figure 6
Role labeling F1 with self-training; dotted lines show the performance of unexpanded classifiers
trained on two to five seed instances per verb. Each solid line starts from such a baseline at k = 0
and for k > 0 shows the performance obtained by adding k sentences with the same FEE to the
respective baseline corpus.
with approximately 6 million unlabeled BNC sentences). For each of the 100 seeds,
we projected annotations to their nearest neighbors according to our algorithm, and
compared their quality to the held-out gold standard. Figure 7 reports labeled F1 for
the sets of d-th neighbors. Unlike the neighbors used in our previous experiments, these
are mutually exclusive. In other words, the set for d = 1 includes only the first most
similar neighbors, for d = 2 the second most similar neighbors, and so on. As expected,
we observe decreasing quality for more distant neighbors, falling from 44.24% for d = 1
to 20.53% for d = 12.
Next, we examined how the quality of the novel annotations impacts the semi-
supervised learning task when these are used as additional training data. As in our
previous experiments, we trained the system on the 100 seed sentences alone to obtain
an ?unexpanded? baseline and on several ?expanded? versions containing the seeds
and one of the d = 1, . . . , 12 sets. The resulting role labeling systems were evaluated
on the 1,000 held-out test sentences mentioned previously. As shown in Figure 7,
performance increases for intermediate values of d and then progressively decreases
for larger values. The performance of the expanded classifiers corresponds closely to
the quality of the projected annotations (or lack thereof). We observe substantial gains
157
Computational Linguistics Volume 38, Number 1
for the sets d = 1, . . . , 6 compared to the baseline role labeler. The latter achieves an F1
of 9.06% which increases to 12.82% for d = 1 neighbors, to 11.61% for d = 2 neighbors,
and so on. In general, improvements in semantic role labeling occur when the projected
annotations maintain an F1 quality in the range of [40?30%]. When F1 drops below 30%,
improvements are relatively small and finally disappear.
We also manually inspected the projected annotations in the set of first neighbors
(i.e., d = 1). Of these, 33.3% matched the gold standard exactly, 55.5% received the right
frame but showed one or more role labeling errors, and 11.1% were labeled with an
incorrect frame. We further analyzed sentences with incorrect roles and found that
for 22.5% of them this was caused by parser errors, whereas another 42.5% could not
have received a correct annotation in the first place by any alignment, because there was
no node in the dependency graph whose yield exactly corresponded to the annotated
substring of the gold standard. This was again due to parser errors or to FrameNet
specific idiosyncrasies (e.g., the fact that roles may span more than one constituent).
For 35.0% of these sentences, the incorrect roles were genuine failures of our projection
algorithm. Some of these failures are due to subtle role distinctions (e.g., Partner1
and Partners for the frame FORMING RELATIONSHIPS), whereas others require detailed
linguistic knowledge which the parser does not capture either by mistake or by design.
For example, seed sentences without overtly realized subjects (such as imperatives) can
lead to incomplete annotations, missing on the Agent role.
In total, we found that parser errors contributed to 45.8% of the erroneous annota-
tions. The remaining errors range from minor problems, which could be fixed by more
careful preprocessing or more linguistically aware features in the similarity function,
to subtle distinctions in the FrameNet annotation, which are not easily addressed by
computational methods. As parsing errors are the main source of projection errors, one
would expect improvements in semantic role labeling with more accurate parsers. We
leave a detailed comparison of dependency parsers and their influence on our method
to future work, however. Moreover, our results demonstrate that some mileage can be
Figure 7
Evaluation of annotations projected onto the d-th neighbors of 100 randomly chosen seed
sentences. The quality of the novel annotations is evaluated directly against held-out
gold-standard data, and indirectly when these are used as training data for an SRL system.
In both cases, we measure labeled F1.
158
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
gained from annotation projection in spite of parser noise. In fact, comparison with
self-training indicates that annotation projection is a major contributor to performance
improvements.
6. Experiment 2: Unknown Verbs
In this section, we describe a second set of experiments, where our method is ap-
plied to acquire novel instances for unknown FEEs, that is, predicates for which no
manually labeled instances are available. Unknown predicates present a major obstacle
to existing supervised SRL systems. Labeling performance on such predicates is typi-
cally poor due to the lack of specific training material for learning (Baker, Ellsworth,
and Erk 2007).
6.1 Method
To simulate frame and role labeling for unknown FEEs, we divided the set of verbal
FEEs in FrameNet into two sets, namely, ?known? and ?unknown.? All annotations of
verbs marked as ?unknown? made up the test set, and the annotations for the ?known?
verbs were the seed corpus (in both cases excluding the 30% of FrameNet used as
development set). To get a balanced division, we sorted all verbal predicates by their
number of annotated sentences and marked every fifth verb in the resulting list (i.e., 20%
of the verbs) as ?unknown,? the rest as ?known.? We used our expansion algorithm to
automatically produce labeled instances for unknown verbs, selecting the most similar
neighbor of each seed sentence (k = 1). Then we trained the SRL system on both the
seeds and the new annotations and tested it on the held-out instances of the ?unknown?
verbs.
6.2 Frame Candidates
So far we have made the simplifying assumption (see Experiment 1, Section 5) that the
FEE of the labeled sentence and the target verb of the unlabeled sentence are identical.
This assumption is not strictly necessary in our framework; however, it reduces com-
putational effort and ensures precision that is higher than would be expected when
comparing arbitrary pairs of verbs. When acquiring novel instances for unseen FEEs, it
is no longer possible to consider identical verbs. The vast majority of seeds, however,
will be inappropriate for a given unlabeled sentence, because their predicates relate to
different situations. So, in order to maintain high precision, and to make expansions
computationally feasible, we must first identify the seeds that might be relevant for a
sentence featuring an unknown predicate. In the following, we propose two methods
for determining frame candidates for an unknown verb, one using vector-based simi-
larity and one that takes WordNet information into account. As we shall see, WordNet-
based similarity yields significantly better results, but its application is restricted to
languages or domains with similar resources.
6.2.1 Vector-based Method. To associate unknown FEEs with known frames, Pennacchiotti
et al (2008) make use of a simple co-occurrence-based semantic space similar to the one
we used to define the lexical measure lex. They represent each FEE v by a vector v and
159
Computational Linguistics Volume 38, Number 1
then compute a vector representation f for a frame f as the weighted centroid of the
vectors of all words evoking it:
f =
?
v?f
wvfv (8)
The weight wvf is operationalized as the relative frequency of v among the FEEs evok-
ing f , counted over the corpus used in building the vector space. The (cosine) similarity
between the unknown target v0 and each frame vector f produces an ordering of frames,
the n-best of which are considered frame candidates.
simV(v0, f ) = cos
(
v0,f
)
(9)
6.2.2 WordNet-based Method. In addition to the vector-based approach, Pennacchiotti
et al (2008) propose a method that is based on WordNet (Fellbaum 1998) and treats
nouns, verbs, and adjectives differently. Given a frame and an unknown verb v0, they
count the number of FEEs that are co-hyponyms of v0 in WordNet. If the number of
co-hyponyms exceeds a threshold ?,10 then the frame is considered a candidate for v0.
In our experiments, we found this method to perform poorly. This suggests that
the improvements reported in Pennacchiotti et al (2008) are due to their more refined
treatment of nouns, which are not considered in our set-up. We thus follow the basic
idea of measuring relatedness between an unknown verb v0 and the set of lexical
units of a frame, and propose a measure based on counts of synonyms, hypernyms,
hyponyms, and co-hyponyms in WordNet. We define:
simW (v0, f ) =
?
v?F
r(v0, v) (10)
where r(v, v?) is 1 if v and v? are synonyms, 12 if one is a hypernym of the other,
1
4 if
they are co-hyponyms, and 0 otherwise. These numbers were chosen heuristically to
represent different degrees of relatedness in WordNet. Relations more distant than co-
hyponymy did not improve performance, as the verb hierarchy in WordNet is shallow.
It therefore seems unlikely that much could be gained by refining the measure r, for
example, by incorporating traditional WordNet similarity measures (e.g., Budanitsky
and Hirst 2001).
6.2.3 Method Comparison. To evaluate which of the methods just described performs best,
we used a leave-one-out procedure over the FrameNet predicates marked as ?known?
in our experimental set-up. Specifically, we set aside one predicate at a time and use
all remaining predicates to predict its frame candidates. The resulting candidates are
then compared to the true frames evoked by the predicate. (We do not consider ?un-
known? predicates here as these are reserved for evaluating the expansion method as a
whole.) For the vector-based method we also explore an unweighted variant, setting all
wvf = 1.
Evaluation results are summarized in Figure 8, which shows the proportion of
predicates for which at least one frame candidate is among the true evokable frames
10 Set to ? = 2 according to personal communication.
160
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Figure 8
Frame labeling accuracy out of n frame candidates; open circles indicate vector-based similarity;
black circles indicate WordNet-based similarity.
(when considering up to 10 best candidates).11 As can be seen, performance increases
by a large margin when unweighted centroids are considered instead of weighted ones.
Apparently, the stabilizing effect of the centroid computation, which allows common
meaning aspects of the predicates to reinforce each other and reduces the effect of spuri-
ous word senses, is more pronounced when all predicates are weighted equally. Figure 8
also shows that a WordNet-based approach that takes into account various kinds of
semantic relations is superior to vector-based methods and to Pennachiotti et al?s (2008)
original proposal based only on co-hyponyms. All subsequent experiments will identify
frame candidates using our WordNet-based definition (Equation (10)).
6.3 Results
Evaluation results of our approach on unknown verbs are summarized in Figure 9.
Frame labeling accuracy is shown in Figure 9(a) and role labeling performance in
Figure 9(b).
As far as frame labeling accuracy is concerned, we compare a semantic role labeler
trained on additional annotations produced by our method against a baseline classifier
trained on known verbs only. Both expanded and unexpanded classifiers choose frames
from the same sets of candidates, which is also the set of frames that the expansion
algorithm is considering. We could have let the unexpanded classifier select among the
entire set of FrameNet frames (more than 500 in total). This would perform poorly,
however, and our evaluation would conflate the effect of additional training material
with the effect of restricting the set of possible frame predictions to likely candidates.
11 Note that although our evaluation is similar to Pennacchiotti et al (2008) the numbers are not strictly
comparable due to differences in the test sets, as well as the fact that they consider FEEs across parts of
speech (not only verbs) and omit infrequent predicates.
161
Computational Linguistics Volume 38, Number 1
Figure 9
Frame labeling accuracy (a) and role labeling performance (b); comparison between unexpanded
and expanded classifiers and random baseline; frame candidates selected based on WordNet.
We also show the accuracy of a simple baseline labeler, which chooses one of the k
candidate frames at random.
As illustrated in Figure 9(a), both expanded and unexpanded classifiers outperform
the random baseline by a wide margin. This indicates that the SRL system is indeed able
to generalize to unknown predicates, even without specific training data. The expanded
classifier is in turn consistently better than the unexpanded one for all numbers of
frame candidates (x axis). The case where only one frame candidate (k = 1) is considered
deserves a special mention. Here, a predicate is assigned the frame most similar to it,
irrespectively of its sentential context. In other words, all instances of the predicate
are assigned the same frame, without any attempt at disambiguation. In this case,
both expanded and unexpanded classifiers obtain the same performance. Although
the unexpanded classifier does not improve over and above this type-based frame
labeling approach, however, the expanded classifier yields significantly better results
for two candidates (p < 0.01 with McNemar?s test). This means that the additional
162
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
training material enables the classifier to successfully favor lower scoring candidates
over higher-scoring ones based on sentential context.
Figure 9(b) shows our results for the role labeling task. We again compare ex-
panded and unexpanded classifiers. Note that there is no obvious random baseline
for the complex task of predicting role spans and their labels, however. Again, we
observe that the expanded classifier outperforms the unexpanded one, save the arti-
ficial case of one candidate where it yields slightly lower results. In this configuration,
our expansion framework cannot account for FEEs that are polysemous by selecting
among different frames, and as a result role labeling performance is compromised.
For two candidates the expanded classifier yields significantly better results than this
token-based approach (p < 0.05 with stratified shuffling). For three, four, and five can-
didates, performance is also numerically better, but the results do not reach statistical
significance. This shows that the expanded classifier is not only able to correctly select
lower scoring frame candidates for unknown verbs, but also to accurately label their
roles. The overall scale of our F1 scores might seem low. This is due to both the
difficulty of the task of predicting fine-grained sense distinctions for verbs without
specific training data, and the comprehensive evaluation measure, which takes into
account all three stages of the SRL system: frame labeling, role recognition, and role
classification.
Incidentally, we should point out that similar tendencies are observed when using
vector-based similarity for identifying the frame candidates. Although overall classi-
fier performance is worse, results are qualitatively similar: The expanded classifiers
outperform the unexpanded ones, and obtain best frame accuracy and labeled F1
with two candidates. Performance also significantly improves compared to selecting a
frame randomly or defaulting to the first candidate (we summarize these results in the
Appendix).
7. Conclusions
We have presented a novel semi-supervised approach for reducing the annotation effort
involved in creating resources for semantic role labeling. Our method automatically
produces training instances from an unlabeled corpus. The key idea is to project an-
notations from labeled sentences onto similar unlabeled ones. We formalize the projec-
tion task as a graph alignment problem. Specifically, we optimize alignments between
dependency graphs under an objective function that takes both lexical and structural
similarity into account. The optimization problem is solved exactly by an integer linear
program.
Experimental results show that the additional training instances produced by our
method significantly improve role labeling performance of a supervised SRL system on
predicates for which only a few or no manually labeled training instances are available.
In the latter case, we first determine suitable frame candidates, improving over similar
methods proposed in the literature. Comparison with a self-training approach shows
that the improvements attained with our method are not merely a side effect of addi-
tional training data. Rather, by identifying sentences that are structurally and lexically
similar to the labeled seeds we are able to acquire qualitatively novel annotations.
Our experiments make use of relatively simple similarity measures, which could be
improved in future work. Incorporating a notion of selectional preferences would allow
for finer-grained distinctions in computing argument similarities. Analogously, our
definition of syntactic similarity could be refined by considering grammar formalisms
163
Computational Linguistics Volume 38, Number 1
with richer syntactic categories such as Combinatory Categorial Grammar (Steedman
2000).
Possible extensions to the work presented in this article are many and varied.
For example, we could combine our approach with cross-lingual annotation projection
(Johansson and Nugues 2006; Pado? and Lapata 2009). For languages without any role
semantic resources, initial annotations could be obtained by cross-lingual projection and
then extended with our semi-supervised method. Another application of our frame-
work would be in domain adaptation, where a supervised model is trained on a seed
corpus, and then unlabeled data from a target domain is used to select new instances
and thus train a new semantic role labeler for the given domain. As our algorithm
produces novel annotated sentences, it could also be used to reduce annotation effort
by offering automatically labeled sentences to humans to inspect and correct. The
experiments presented here are limited to verbal categories and focus solely on English.
In the future, we would like to examine whether our approach generalizes to other
syntactic categories such as nouns, adjectives, and prepositions. An obvious extension
also involves experiments with other languages. Experiments on the SALSA corpus
(Burchardt et al 2006) show that similar improvements can be obtained for German
(Fu?rstenau 2011).
Finally, the general formulation of our expansion framework allows its application
to other tasks. Deschacht and Moens (2009) adapt our approach to augment subsets
of the PropBank corpus and observe improvements over a supervised system for a
small seed corpus. They also show that defining the lexical similarity measure in terms
of Jensen?Shannon divergence instead of cosine similarity can additionally improve
performance. Another possibility would be to employ our framework for the acquisition
of paraphrases, for example, by extending the multiple-sequence alignment approach
of Barzilay and Lee (2003) with our notion of graph alignments. Finally, it would be
interesting to investigate how to reduce the dependency on full syntactic analyses, for
example, by employing shallow parsers or chunkers.
Appendix: Detailed Experimental Results
In this appendix, we give complete results for the expansion experiments discussed in
Sections 5 and 6. Asterisks in the tables indicate levels of significance. For simplicity, we
only present two levels of significance, p < 0.05 with a single asterisk (*) and p < 0.001
with two asterisks (**). Significance tests for exact match and frame labeling accuracy
were performed using McNemar?s test. We used stratified shuffling Noreen (1989) to
examine whether differences in labeled F1 were significant.12
Experiments on Known Predicates. The following table shows the performance of ex-
panded classifiers when [1?6] automatically generated nearest neighbors (NN) are
added to seed corpora containing [1?10] manually labeled sentences per verb. We report
precision (Prec), recall (Rec), their harmonic mean (F1), and exact match (ExMatch; the
proportion of sentences that receive entirely correct frame and role annotations). Some
of these results were visualized in Figure 5.
12 We used the sigf tool (Pado? 2006).
164
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Training set Size Prec (%) Rec (%) F1 (%) ExMatch (%)
1 seed/verb 2, 092 40.74 23.69 29.96 6.38
+ 1-NN 3, 297 40.52 24.23 30.33 6.81 *
+ 2-NN 4, 481 40.29 24.99 30.85 * 6.97 *
+ 3-NN 5, 649 39.52 25.02 30.64 * 7.35 **
+ 4-NN 6, 803 39.52 25.39 30.92 * 7.30 **
+ 5-NN 7, 947 39.04 25.34 30.73 * 7.12 *
+ 6-NN 9, 076 38.40 25.16 30.40 6.89
2 seeds/verb 4, 105 45.22 29.81 35.94 9.40
+ 1-NN 6, 500 45.09 30.84 36.63 * 10.19 **
+ 2-NN 8, 850 44.82 31.50 37.00 ** 10.32 **
+ 3-NN 11, 157 44.65 31.85 37.18 ** 10.32 **
+ 4-NN 13, 423 43.99 31.94 37.01 ** 10.15 *
+ 5-NN 15, 652 42.64 31.23 36.05 9.73
+ 6-NN 17, 846 42.57 31.36 36.11 9.63
3 seeds/verb 6, 021 45.03 31.29 36.92 9.81
+ 1-NN 9, 492 44.78 32.45 37.63 * 10.35 *
+ 2-NN 12, 874 44.15 32.69 37.57 * 10.37 *
+ 3-NN 16, 179 43.90 33.00 37.68 * 10.68 *
+ 4-NN 19, 424 43.60 33.36 37.80 * 10.35
+ 5-NN 22, 609 43.15 33.26 37.56 * 10.50 *
+ 6-NN 25, 734 42.72 33.17 37.34 10.45 *
4 seeds/verb 7, 823 44.42 32.21 37.35 9.48
+ 1-NN 12, 321 44.45 33.31 38.09 * 10.20 **
+ 2-NN 16, 688 44.26 34.13 38.54 ** 10.40 **
+ 3-NN 20, 944 43.71 34.20 38.37 ** 10.72 **
+ 4-NN 25, 098 43.37 34.35 38.34 ** 10.57 **
+ 5-NN 29, 166 43.25 34.45 38.35 * 10.67 **
+ 6-NN 33, 142 42.48 34.24 37.92 10.40 *
5 seeds/verb 9, 515 45.45 33.81 38.78 10.35
+ 1-NN 15, 026 45.47 34.90 39.49 * 10.95 *
+ 2-NN 20, 363 45.03 35.39 39.63 * 11.42 **
+ 3-NN 25, 533 44.56 35.51 39.53 * 11.56 **
+ 4-NN 30, 576 44.44 35.78 39.64 * 11.70 **
+ 5-NN 35, 494 44.22 35.94 39.65 * 11.72 **
+ 6-NN 40, 286 43.74 35.83 39.39 * 11.49 **
6 seeds/verb 11, 105 46.50 35.44 40.22 10.95
+ 1-NN 17, 553 46.05 36.11 40.48 11.56 *
+ 2-NN 23, 779 45.71 36.67 40.70 12.07 **
+ 3-NN 29, 787 45.16 36.83 40.57 11.92 **
+ 4-NN 35, 623 44.82 36.92 40.49 11.80 *
+ 5-NN 41, 310 44.60 36.91 40.40 12.13 **
+ 6-NN 46, 851 44.07 36.86 40.14 12.02 **
8 seeds/verb 13, 999 47.60 37.29 41.82 12.25
+ 1-NN 22, 115 47.08 37.71 41.88 12.48
+ 2-NN 29, 907 46.45 38.01 41.81 12.64
+ 3-NN 37, 400 46.01 38.11 41.69 12.69
+ 4-NN 44, 656 45.55 38.12 41.51 12.78
+ 5-NN 51, 705 45.53 38.38 41.65 13.22 *
+ 6-NN 58, 562 45.00 38.24 41.34 13.34 **
10 seeds/verb 16, 595 48.97 39.02 43.43 13.73
+ 1-NN 26, 180 48.24 39.55 43.47 14.01
+ 2-NN 35, 336 47.11 39.32 42.86 13.80
+ 3-NN 44, 113 46.69 39.45 42.77 13.85
+ 4-NN 52, 602 46.18 39.31 42.47 13.63
+ 5-NN 60, 827 46.22 39.76 42.75 13.68
+ 6-NN 68, 791 45.69 39.58 42.42 13.95
165
Computational Linguistics Volume 38, Number 1
Experiments on Unknown Predicates. In the following, we show the performance of un-
expanded and expanded classifiers when selecting among [1?5] frame candidates gen-
erated by the WordNet-based method. We report frame labeling accuracy, role labeling
performance, and exact match scores. Asterisks indicate that the expanded classifier is
significantly better than an unexpanded classifier choosing among the same number
of candidates. For frame labeling accuracy, we additionally provide the results of the
random baseline and an upper bound, which always chooses the correct frame if it is
among the candidates. Some of these results were shown in Figure 9.
Frame labeling accuracy (%)
Candidates Random Unexpanded Expanded Upper bound
1 45.50 45.50 45.50 45.50
2 29.61 41.24 46.89 ** 59.23
3 22.20 36.02 44.82 ** 66.60
4 17.31 28.75 44.75 ** 69.23
5 14.45 26.56 43.58 ** 72.25
Unexpanded (%) Expanded (%)
Candidates Prec Rec F1 ExMatch Prec Rec F1 ExMatch
1 24.77 18.94 21.47 6.54 23.61 18.72 20.88 6.56
2 22.52 17.63 19.78 5.87 24.60 20.05 22.09 ** 7.02 **
3 19.52 15.20 17.09 5.04 24.23 19.79 21.79 ** 7.24 **
4 16.18 12.31 13.98 4.02 24.59 20.09 22.11 ** 7.26 **
5 14.78 11.27 12.78 3.77 24.12 19.70 21.69 ** 7.44 **
For two candidates, the expanded classifier also performs significantly better than the
best unexpanded classifier (i.e., the one given only one candidate) in terms of frame
labeling accuracy, F1, and exact match (p < 0.05). In terms of exact match, it also
performs significantly better for three candidates (p < 0.05), four candidates (p < 0.05),
and five candidates (p < 0.001).
Vector-based frame candidates. The following graphs show the performance of the un-
expanded and expanded classifiers when frame candidates are selected by the vector-
based method. The expanded classifiers significantly outperform the unexpanded ones
in terms of frame labeling accuracy and role labeling F1 for [2?5] candidates (p < 0.001).
For two candidates, frame labeling accuracy and role labeling F1 also significantly
improve compared with the type-based approach of always choosing the first candidate
(p < 0.001). For three candidates performance is significantly better only in terms of
frame labeling accuracy (p < 0.05) but not F1.
166
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
a.
10
15
20
25
30
35
40
1 2 3 4 5
Accuracy
frame candidates
unexpanded classifier
 




expanded classifier






random baseline





b.
10
11
12
13
14
15
16
17
18
19
1 2 3 4 5
F1
frame candidates
unexpanded classifier





 expanded classifier


 


Acknowledgments
We are grateful to the anonymous referees,
whose feedback helped to substantially
improve this article. Special thanks are due
to Richard Johansson for his help with
the re-implementation of his semantic role
labeler and Manfred Pinkal for insightful
comments and suggestions. We acknowledge
the support of EPSRC (Lapata; grant
GR/T04540/01) and DFG (Fu?rstenau;
IRTG 715 and project PI 154/9-3).
References
Abend, Omri, Roi Reichart, and Ari
Rappoport. 2009. Unsupervised argument
identification for semantic role labeling.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural
Language Processing of the AFNLP,
pages 28?36, Singapore.
Achananuparp, Palakorn, Xiaohua Hu, and
Xiajiong Shen. 2008. The evaluation of
167
Computational Linguistics Volume 38, Number 1
sentence similarity measures.
In Proceedings of the 10th International
Conference on Data Warehousing and
Knowledge Discovery, pages 305?316,
Turin.
Andersen, ?istein E., Julien Nioche,
Ted Briscoe, and John Carroll. 2008.
The BNC parsed with RASP4UIMA.
In Proceedings of LREC, pages 865?869,
Marrakech.
Baker, Collin F., Michael Ellsworth, and
Katrin Erk. 2007. SemEval-2007 Task 19:
Frame Semantic structure extraction.
In Proceedings of the 4th International
Workshop on Semantic Evaluations,
pages 99?104, Prague.
Barzilay, Regina and Mirella Lapata. 2006.
Aggregation via set partitioning for
natural language generation. In Proceedings
of the Human Language Technology
Conference of the NAACL, pages 359?366,
New York, NY.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: An unsupervised
approach using multiple-sequence
alignment. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 16?23,
Edmonton.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release
of the RASP system. In Proceedings
of the COLING/ACL 2006 Interactive
Presentation Sessions, pages 77?80,
Sydney.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in WordNet:
An experimental, application-oriented
evaluation of five measures. In Proceedings
of the ACL Workshop on WordNet and
other Lexical Resources, pages 29?34,
Pittsburgh, PA.
Bullinaria, John A. and Joseph P. Levy.
2007. Extracting semantic representations
from word co-occurrence statistics:
A computational study. Behavior
Research Methods, 39:510?526.
Burchardt, Aljoscha, Katrin Erk, and
Anette Frank. 2005. A WordNet detour
to FrameNet. In Proceedings of the GLDV
GermaNet II Workshop, pages 408?421,
Bonn.
Burchardt, Aljoscha, Katrin Erk, Anette
Frank, Andrea Kowalski, Sebastian Pado?,
and Manfred Pinkal. 2006. The SALSA
corpus: A German corpus resource for
lexical semantics. In Proceedings of LREC,
pages 969?974, Genoa.
Chang, Ming-Wei, Dan Goldwasser,
Dan Roth, and Vivek Srikumar. 2010.
Discriminative learning over constrained
latent representations. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 429?437, Los Angeles, CA.
Clarke, James and Mirella Lapata. 2008.
Global inference for sentence compression:
An integer linear programming approach.
Journal of Artificial Intelligence Research,
31:273?381.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1992. Introduction
to Algorithms. The MIT Press,
Cambridge, MA.
Das, Dipanjan, Nathan Schneider,
Desai Chen, and Noah A. Smith. 2010.
Probabilistic Frame-Semantic parsing.
In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 948?956,
Los Angeles, CA.
Das, Dipanjan and Noah A. Smith. 2009.
Paraphrase identification as probabilistic
quasi-synchronous recognition. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 468?476, Singapore.
de Marneffe, Marie-Catherine, Trond
Grenager, Bill MacCartney, Daniel Cer,
Daniel Ramage, Chloe? Kiddon, and
Christopher D. Manning. 2007. Aligning
semantic graphs for textual inference
and machine reading. In AAAI Spring
Symposium at Stanford.
de Salvo Braz, Rodrigo, Roxana Girju,
Vasin Punyakanok, Dan Roth, and Mark
Sammons. 2005. An inference model for
semantic entailment in natural language.
In Proceedings of the 20th National Conference
on Artificial Intelligence, pages 1043?1049,
Pittsburgh, PA.
Denis, Pascal and Jason Baldridge. 2007.
Joint determination of anaphoricity and
coreference resolution using integer
programming. In Human Language
Technologies 2007: The Conference of the
North American Chapter of the Association
for Computational Linguistics; Proceedings
of the Main Conference, pages 236?243,
Rochester, NY.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the Latent Words Language
168
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Model. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 21?29, Singapore.
Dras, Mark. 1999. Tree Adjoining Grammar
and the Reluctant Paraphrasing of Text.
Ph.D. thesis, Macquarie University,
Sydney.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A library for large linear
classification. Journal of Machine Learning
Research, 9:1871?1874.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Database. MIT Press,
Cambridge, MA.
Fillmore, Charles J. 1968. The case for case.
In Emmon Bach and Robert T. Harms,
editors, Universals in Linguistic Theory.
Holt, Rinehart & Winston, New York,
NY, pages 1?88.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16:235?250.
Friedman, Jerome H. 1996. Another
approach to polychotomous classification.
Technical report, Department of Statistics,
Stanford University.
Fung, Pascale and Benfeng Chen. 2004.
BiFrameNet: Bilingual Frame Semantics
resource construction by cross-lingual
induction. In Proceedings of COLING 2004,
pages 931?937, Geneva.
Fu?rstenau, Hagen. 2008. Enriching frame
semantic resources with dependency
graphs. In Proceedings of LREC,
pages 1478?1484, Marrakech.
Fu?rstenau, Hagen. 2011. Semi-supervised
Semantic Role Labeling via Graph
Alignment, volume 32 of Saarbru?cken
Dissertations in Computational Linguistics
and Language Technology. German Research
Center for Artificial Intelligence and
Saarland University, Saarbru?cken,
Germany.
Gildea, Daniel and Dan Jurafsky. 2002.
Automatic labeling of semantic
roles. Computational Linguistics,
28(3):245?288.
Gordon, Andrew and Reid Swanson. 2007.
Generalizing semantic role annotations
across syntactically similar verbs.
In Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 192?199, Prague.
Grenager, Trond and Christopher D.
Manning. 2006. Unsupervised discovery of
a statistical verb lexicon. In Proceedings of
the 2006 Conference on Empirical Methods in
Natural Language Processing, pages 1?8,
Sydney.
Haghighi, Aria D., Andrew Y. Ng, and
Christopher D. Manning. 2005. Robust
textual inference via graph matching.
In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 387?394, Vancouver.
Johansson, Richard. 2008. Dependency-based
Semantic Analysis of Natural-language Text.
Ph.D. thesis, Department of Computer
Science, Lund University, Sweden.
Johansson, Richard and Pierre Nugues.
2006. A FrameNet-based semantic role
labeler for Swedish. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 436?443, Sydney.
Johansson, Richard and Pierre Nugues.
2007a. Syntactic representations
considered for frame-semantic analysis.
In Proceedings of the Sixth International
Workshop on Treebanks and Linguistic
Theories (TLT 2007), Bergen.
Johansson, Richard and Pierre Nugues.
2007b. Using WordNet to extend
FrameNet coverage. In Proceedings of the
NODALIDA-2007 Workshop FRAME
2007: Building Frame Semantics Resources
for Scandinavian and Baltic Languages,
pages 27?30, Tartu.
Klau, Gunnar W. 2009. A new graph-based
method for pairwise global network
alignment. BMC Bioinformatics,
10 (Suppl 1):S59.
Land, Ailsa H. and Alison G. Doig. 1960.
An automatic method for solving discrete
programming problems. Econometrica,
28(3):497?520.
Lang, Joel and Mirella Lapata. 2010.
Unsupervised induction of semantic
roles. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 939?947,
Los Angeles, CA.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press.
Malik, Rahul, L. Venkata Subramaniam,
and Saroj Kaushik. 2007. Automatically
selecting answer templates to respond
to customer emails. In Proceedings of the
20th International Joint Conference on
Artificial Intelligence, pages 1659?1664,
Hyderabad.
Marciniak, Tomasz and Michael Strube.
2005. Beyond the pipeline: Discrete
optimization in NLP. In Proceedings of
169
Computational Linguistics Volume 38, Number 1
the 9th Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 136?143, Ann Arbor, MI.
Matsubayashi, Yuichiroh, Naoaki Okazaki,
and Jun?ichi Tsujii. 2009. A comparative
study on generalization of semantic
roles in FrameNet. In Proceedings of
the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 19?27,
Singapore.
Matusov, Evgeny, Richard Zens, and
Hermann Ney. 2004. Symmetric word
alignments for statistical matching
translation. In Proceedings of COLING
2004, pages 219?225, Geneva.
Melli, Gabor, Yang Wang, Yudong Liu,
Mehdi M. Kashani, Zhongmin Shi,
Baohua Gu, Anoop Sarkar, and Fred
Popowich. 2005. Description of SQUASH,
the SFU question answering summary
handler for the DUC-2005 summarization
task. In Proceedings of the HLT-EMNLP
Document Understanding Workshop,
Vancouver.
Mihalcea, Rada, Courtney Corley, and
Carlo Strapparava. 2006. Corpus-based
and knowledge-based measures of
text semantic similarity. In Proceedings
of the 21st National Conference on
Artificial Intelligence, pages 775?780,
Boston, MA.
Mitchell, Jeff and Mirella Lapata. 2010.
Composition in distributional models
of semantics. Cognitive Science,
(34):1388?1429.
Noreen, Eric. 1989. Computer-intensive
Methods for Testing Hypotheses: An
Introduction. New York, Wiley.
Pado?, Sebastian, 2006. User?s guide to
sigf: Significance testing by approximate
randomization. Available at:
www.nlpado.de/?sebastian/
software/sigf.shtml.
Pado?, Sebastian and Mirella Lapata.
2009. Cross-lingual annotation
projection for semantic roles. Journal
of Artificial Intelligence Research,
36:307?340.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Pennacchiotti, Marco, Diego De Cao, Roberto
Basili, Danilo Croce, and Michael Roth.
2008. Automatic induction of FrameNet
lexical units. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 457?465,
Honolulu, HI.
Pradhan, Sameer S., Wayne Ward, and
James H. Martin. 2008. Towards robust
semantic role labeling. Computational
Linguistics, 34(2):289?310.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings of COLING 2004,
pages 1346?1352, Geneva.
Qiu, Long, Min-Yen Kan, and Tat-Seng
Chua. 2006. Paraphrase recognition via
dissimilarity significance classification.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 18?26, Sydney.
Riedel, Sebastian and James Clarke.
2006. Incremental integer linear
programming for non-projective
dependency parsing. In Proceedings
of the 2006 Conference on Empirical
Methods in Natural Language Processing,
pages 129?137, Sydney.
Roth, Dan and Wen tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 1?8, Boston, MA.
Shen, Dan and Mirella Lapata. 2007. Using
semantic roles to improve question
answering. In Proceedings of the 2007
Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 12?21, Prague.
Steedman, Mark. 2000. The Syntactic Process.
The MIT Press, Cambridge, MA.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate?argument structures for
information extraction. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 8?15,
Sapporo.
Swier, Robert S. and Suzanne Stevenson.
2004. Unsupervised semantic role
labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing, pages 95?102,
Bacelona.
Taskar, Ben, Simon Lacoste-Julien, and
Dan Klein. 2005. A discriminative
matching approach to word alignment.
In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 73?80, Vancouver.
170
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Vanderbei, Robert J. 2001. Linear
Programming: Foundations and Extensions.
Berlin, Springer.
Wan, Stephen, Mark Dras, Robert Dale,
and Ce?cile Paris. 2006. Using
dependency-based features to take
the ?para-farce? out of paraphrase.
In Proceedings of the 2006 Australasian
Language Technology Workshop,
pages 131?138, Sydney.
Winston, Wayne L. and Munirpallam
Venkataramanan. 2003. Introduction to
Mathematical Programming: Applications
and Algorithms (4th edition). Pacific Grove,
CA, Duxbury Press.
Wu, Dekai and Pascale Fung. 2009. Semantic
roles for SMT: A hybrid two-pass model.
In Proceedings of HLT-NAACL, Companion
Volume: Short Papers, pages 13?16,
Boulder, CO.
171
Similarity-Driven Semantic Role Induction
via Graph Partitioning
Joel Lang?
University of Geneva
Mirella Lapata??
University of Edinburgh
As in many natural language processing tasks, data-driven models based on supervised learning
have become the method of choice for semantic role labeling. These models are guaranteed to
perform well when given sufficient amount of labeled training data. Producing this data is
costly and time-consuming, however, thus raising the question of whether unsupervised methods
offer a viable alternative. The working hypothesis of this article is that semantic roles can
be induced without human supervision from a corpus of syntactically parsed sentences based
on three linguistic principles: (1) arguments in the same syntactic position (within a specific
linking) bear the same semantic role, (2) arguments within a clause bear a unique role, and
(3) clusters representing the same semantic role should be more or less lexically and distribu-
tionally equivalent. We present a method that implements these principles and formalizes the
task as a graph partitioning problem, whereby argument instances of a verb are represented as
vertices in a graph whose edges express similarities between these instances. The graph consists
of multiple edge layers, each one capturing a different aspect of argument-instance similarity,
and we develop extensions of standard clustering algorithms for partitioning such multi-layer
graphs. Experiments for English and German demonstrate that our approach is able to induce
semantic role clusters that are consistently better than a strong baseline and are competitive with
the state of the art.
1. Introduction
Recent years have seen increased interest in the shallow semantic analysis of natural
language text. The term is often used to describe the automatic identification and
labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky
2002). Semantic roles describe the relations that hold between a predicate and its
arguments (e.g., ?who? did ?what? to ?whom?, ?when?, ?where?, and ?how?)
abstracting over surface syntactic configurations. This type of semantic information
? Department of Computer Science, University of Geneva, 7 route de Drize, 1227 Carouge, Switzerland,
E-mail: Joel.Lang@unige.ch.
?? Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh,
10 Crichton Street, EH8 9AB, E-mail: mlap@inf.ed.ac.uk.
Submission received: 26 December 2012; revised version received: 19 September 2013; accepted for
publication: 20 November 2013.
doi:10.1162/COLI a 00195
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
is shallow but relatively straightforward to infer automatically and useful for the
development of broad-coverage, domain-independent language understanding
systems. Indeed, the analysis produced by existing semantic role labelers has been
shown to benefit a wide spectrum of applications ranging from information extraction
(Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine
translation (Wu and Fung 2009) and summarization (Melli et al. 2005).
In the example sentences below, window occupies different syntactic positions?it is
the object of broke in sentences (1a,b), and the subject in (1c). In all instances, it bears the
same semantic role, that is, the patient or physical object affected by the breaking event.
Analogously, ball is the instrument of break both when realized as a prepositional phrase
in (1a) and as a subject in (1b).
(1) a. [Jim]A0 broke the [window]A1 with a [ball]A2.
b. The [ball]A2 broke the [window]A1.
c. The [window]A1 broke [last night]TMP.
Also notice that all three instances of break in Example (1) have apparently similar
surface syntax with a subject and a noun directly following the predicate. However,
in sentence (1a) the subject of break expresses the agent role, in (1b) it expresses the
instrument role, and in (1c) the patient role.
The examples illustrate the fact that predicates can license several alternate map-
pings or linkings between their semantic roles and their syntactic realization. Pairs of
linkings allowed by a single predicate are often called diathesis alternations (Levin
1993). Sentence pair (1a,b) is an example of the instrument subject alternation, and
pair (1b,c) illustrates the causative alternation. Resolving the mapping between the
syntactic dependents of a predicate (e.g., subject, object) and the semantic roles that they
each express is one of the major challenges faced by semantic role labelers.
The semantic roles in the examples are labeled in the style of PropBank (Palmer,
Gildea, and Kingsbury 2005), a broad-coverage human-annotated corpus of semantic
roles and their syntactic realizations. Under the PropBank annotation framework each
predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose
interpretations are specific to that predicate1 and a set of adjunct roles such as location or
time whose interpretation is common across predicates (e.g., last night in sentence (1c)).
The availability of PropBank and related resources (e.g., FrameNet; Ruppenhofer et al.
2006) has sparked the development of a variety semantic role labeling systems, most
of which conceptualize the task as a supervised learning problem and rely on role-
annotated data for model training. Most of these systems implement a two-stage ar-
chitecture consisting of argument identification (determining the arguments of the
verbal predicate) and argument classification (labeling these arguments with semantic
roles). Current approaches deliver reasonably good performance?a system will recall
around 81% of the arguments correctly and 95% of those will be assigned a correct
semantic role (see Ma`rquez et al. [2008] for details), although only on languages and
domains for which large amounts of role-annotated training data are available.
Unfortunately, the reliance on labeled data, which is both difficult and expensive
to produce, presents a major obstacle to the widespread application of semantic role
labeling across different languages and text genres. Although corpora with semantic
1 More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient
in the sense of Dowty (1991).
634
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
role annotations exist nowadays in other languages (e.g., German, Spanish, Catalan,
Chinese, Korean), they tend to be smaller than their English equivalents and of limited
value for modeling purposes. Even within English, a language for which two major
annotated corpora are available, systems trained on PropBank demonstrate a marked
decrease in performance (approximately by 10%) when tested on out-of-domain data
(Pradhan, Ward, and Martin 2008). The data requirements for supervised systems and
the current paucity of such data has given impetus to the development of unsupervised
methods that learn from unlabeled data. If successful, unsupervised approaches could
lead to significant resource savings and the development of semantic role labelers
that require less engineering effort. Besides being interesting on their own right, from
a theoretical and linguistic perspective, unsupervised methods can provide valuable
features for downstream (supervised) processing and serve as a preprocessing step for
applications that require broad coverage understanding. In this article we study the
potential of unsupervised methods for semantic role labeling. As in the supervised case,
we decompose the problem into an argument identification step and an argument clas-
sification step. Our work primarily focuses on argument classification, which we term
role induction, because there is no predefined set of semantic roles in the unsupervised
case, and these must be induced from data. The goal is to assign argument instances to
clusters such that each cluster contains arguments corresponding to a specific semantic
role and each role corresponds to exactly one cluster.
Unsupervised learning is known to be challenging for many natural language
processing problems and role induction is no exception. Firstly, it is difficult to define
a learning objective function whose optimization will yield an accurate model. This
contrasts with the supervised setting, where the objective function can directly reflect
training error (i.e., some estimate of the mismatch between model output and the gold
standard) and the model can be tuned to replicate human output for a given input under
mathematical guarantees regarding the accuracy of the trained model. Secondly, it is
also more difficult to incorporate rich feature sets into an unsupervised model (Berg-
Kirkpatrick et al. 2010). Unless we explicitly know exactly how features interact, more
features may not necessarily lead to a more accurate model and may even decrease
performance. In the supervised setting, feature interactions relevant for a particular
learning task can be determined to a large extent automatically and thus a large number
of them can be included even if their significance is not clear a priori.
The lack of an extensional definition (in the form of training examples) of the target
concept makes a strong case for the development of unsupervised methods that use
problem specific prior knowledge. The idea is to derive a strong inductive bias (Gordon
and Desjardins 1995) based on this prior knowledge that will guide the learning towards
the correct target concept. For semantic role induction, we propose to build on the
following linguistic principles:
1. Semantic roles are unique within a particular frame.
2. Arguments occurring in a specific syntactic position within a specific linking
all bear the same semantic role.
3. The (asymptotic) distribution over argument heads is the same for two
clusters that represent the same semantic role.
We hypothesize that these three principles are, at least in theory, sufficient for
inducing high-quality semantic role clusters. A challenge, of course, lies in adequately
operationalizing them so that they guide the unsupervised learner towards meaningful
635
Computational Linguistics Volume 40, Number 3
solutions. The approach taken in this article translates these principles into estimates of
similarity (or dissimilarity) between argument instances and/or clusters of argument
instances. Principle (1) states that argument instances occurring in the same frame
(i.e., clause) cannot bear the same semantic role, and are thus dissimilar. From Prin-
ciple (2) it follows that arguments occurring in the same syntactic position within the
same linking can be considered similar (leaving aside for the moment the difficulty of
representing linkings through syntactic cues observable in a corpus). Principle (3) states
that two clusters of instances containing similar distributions over head words should
be considered similar.
Based on these similarity estimates we construct a graph whose vertices represent
argument instances and whose edges express similarities between these instances. The
graphs consist of multiple edge layers, each capturing one particular type of argument-
instance similarity. For example, one layer will be used to represent whether argument
instances occur in the same frame, and another layer will represent whether two argu-
ments have a similar head word, and so on. Given this graph representation of the data,
we formalize role induction as the problem of partitioning the graph into clusters of sim-
ilar vertices. We present two algorithms for partitioning multi-layer graphs, which are
adaptations of standard graph partitioning algorithms to the multi-layer setting. The al-
gorithms differ in the way they exploit the similarity information encoded in the graph.
The first one is based on agglomeration, where two clusters containing similar instances
are grouped into a larger cluster. The second one is based on propagation, where role-
label information is transferred from one cluster to another based on their similarity.
To understand how the aforementioned principles might allow us to handle the
ambiguity stemming from alternate linkings, consider again Example (1). The most
important thing to note is that, whereas the subject position is ambiguous with respect
to the semantic roles it can express (it can be A0, A1, or A2), we can resolve the
ambiguity by exploiting overt syntactic cues of the underlying linking. For example,
the predicate break is transitive in sentences (1a) and (1b), and intransitive in sentence
(1c). Thus, by taking into account the argument?s syntactic position and the predicate?s
transitivity, we can guess that the semantic role expressed by the subject in sentence (1c)
is different from the roles expressed by the subjects in sentences (1a,b). Now consider
the more difficult case of distinguishing between the subjects in sentences (1a) and (1b).
One linking cue that could help here is the prepositional phrase in sentence (1a), which
results in a syntactic frame different from sentence (1b). Were the prepositional phrase
omitted, we would attempt to disambiguate the linkings by resorting to lexical-semantic
cues (e.g., by taking into account whether the subject is animate). In sum, if we encode
sufficiently many linking cues, then the resulting fine-grained syntactic information will
discriminate ambiguous semantic roles. In cases where syntactic cues are not discerning
enough, we can exploit lexical information and group arguments together based on
their lexical content.
The remainder of this article is structured as follows. Section 2 provides an overview
of unsupervised methods for semantic role labeling. Sections 3 and 4 present the details
of our method, that is, how the graphs are constructed and partitioned. Role induction
experiments in English and German are described in sections 5 and 6, respectively.
Discussion of future work concludes in section 7.
2. Related Work
The bulk of previous work on semantic role labeling has focused on supervised methods
(Ma`rquez et al. 2008), although a few semi-supervised and unsupervised approaches
636
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
have been proposed. The majority of semi-supervised models have been developed
within a framework known as annotation projection. The idea is to combine labeled
and unlabeled data by projecting annotations from a labeled source sentence onto
an unlabeled target sentence within the same language (Fu?rstenau and Lapata 2009)
or across different languages (Pado? and Lapata 2009). Beyond annotation projection,
Gordon and Swanson (2007) propose to increase the coverage of PropBank to unseen
verbs by finding syntactically similar (labeled) verbs and using their annotations as
surrogate training data.
Swier and Stevenson (2004) were the first to introduce an unsupervised se-
mantic role labeling system. Their algorithm induces role labels following a boot-
strapping scheme where the set of labeled instances is iteratively expanded using
a classifier trained on previously labeled instances. Their method starts with a data
set containing no role annotations at all, but crucially relies on VerbNet (Kipper,
Dang, and Palmer 2000) for identifying the arguments of predicates and making
initial role assignments. VerbNet is a manually constructed lexicon of verb classes,
each of which is explicitly associated with argument realization and semantic role
specifications.
In this article we will not assume the availability of any role-semantic resources,
although we do assume that sentences are syntactically analyzed. There have been
two main approaches to role induction from parsed data. Under the first approach,
semantic roles are modeled as latent variables in a (directed) graphical model that
relates a verb, its semantic roles, and their possible syntactic realizations (Grenager
and Manning 2006). Role induction here corresponds to inferring the state of the
latent variables representing the semantic roles of arguments. Following up on this
work, Lang and Lapata (2010) reformulate role induction as the process of detecting
alternations and finding a canonical syntactic form for them. Verbal arguments are
then assigned roles, according to their position in this canonical form, because each
position references a specific role. Their model extends the logistic classifier with
hidden variables and is trained in a manner that takes advantage of the close re-
lationship between syntactic functions and semantic roles. More recently, Garg and
Henderson (2012) extend the latent-variable approach by modeling the sequential order
of roles.
The second approach is similarity-driven and based on clustering. Lang and Lapata
(2011a) propose an algorithm that first splits the set of all argument instances of a verb
according to their syntactic position within a particular linking and then iteratively
merges clusters. A different clusstering algorithm is adopted in Lang and Lapata
(2011b). Specifically, they induce semantic roles via graph partitioning: Each vertex
in the graph corresponds to an argument instance and edges represent a heuristically
defined measure of their lexical and syntactic similarity. The similarity-driven approach
has been recently adopted by Titov and Klementiev (2012a), who propose a Bayesian
clustering algorithm based on the Chinese Restaurant Process. In addition, they present
a method that shares linking preferences across verbs using a distance-dependent
Chinese Restaurant Process prior which encourages similar verbs to have similar
linking preferences. Titov and Klementiev (2012b) further introduce the use of multi-
lingual data for improving role induction.
There has also been work on unsupervised methods for argument identification.
Abend, Reichart, and Rappoport (2009) devise a method for recognizing the arguments
of predicates that relies solely on part of speech annotations, whereas Abend and
Rappoport (2010a) distinguish between core and adjunct roles, using an unsupervised
parser and part-of-speech tagger. More generally, shallow semantic representations
637
Computational Linguistics Volume 40, Number 3
induced from syntactic information are commonly used in lexicon acquisition and
information extraction tasks. For example, Lin and Pantel (2001) cluster syntactic re-
lations between pairs of words as expressed by parse tree paths into semantic relations
by exploiting lexical distributional similarity. Although not compatible with PropBank
or semantic roles as such, Poon and Domingos (2009) and Titov and Klementiev (2011)
also induce semantic information from dependency parses and apply it to a question
answering task for the biomedical domain. Another example is the work by Gamallo,
Agustini, and Lopes (2005), who cluster similar syntactic positions in order to develop
models of selectional preferences to be used for word sense induction and the resolution
of attachment ambiguities.
The work described here unifies the two clustering methods presented in Lang and
Lapata (2011a and 2011b) by reformulating them as graph partitioning algorithms. It
also extends them by utilizing multi-layer graphs which separate the similarities be-
tween instances on different features (e.g., part-of-speech, argument head) into different
layers. This has the advantage that similarity scores on individual features do not have
to be eagerly combined into a similarity score between instances. Instead, one can first
aggregate the similarity scores on each feature layer between two clusters and then
combine them into a similarity score between clusters. This is more robust, as the feature-
wise similarity scores between clusters can be computed in a principled way and the
heuristic combination step is deferred to the end (see Section 4 for details). Besides
providing a general modeling framework for semantic role induction, we discuss in
detail the linguistic principles guiding our modeling choices and assess their applica-
bility across languages. Specifically, we show that the framework presented here (and
the aforementioned principles) can be readily applied to English and German with
identical parametrizations for both languages and without fundamentally changing
the underlying model features, despite major syntactic differences between the two
languages.
3. Graph Construction
We begin by explaining how we construct a graph that represents verbs and their
arguments. Next, we describe how edge weights are computed?these translate to
similarity scores between argument instances?and then move on to provide the details
of our graph-partitioning algorithms.
As mentioned earlier, we formalize semantic role induction as a clustering problem.
Clustering algorithms (see Jain, Murty, and Flynn [1999] for an overview) commonly
take a matrix of pairwise similarity scores between instances as input and produce
a set of output clusters, often satisfying some explicitly defined optimality criterion.
The success or failure of the clustering approach is closely tied to the adequacy of
the employed similarity function for the task at hand. The graph partitioning view
of clustering (see Schaeffer [2007] for a detailed treatment) arises when instances are
represented as the vertices of a graph and the similarity matrix is interpreted as the
weight matrix of the graph. For semantic role induction, a straightforward application
of clustering would be to construct a graph for each verbal predicate such that vertices
correspond to argument instances of the verb and edge weights quantify the similarity
between these instances.
Lang and Lapata (2011b) hand-craft an instance similarity function by taking into
account different features such as the argument head or its syntactic position. Defin-
ing an appropriate instance-wise similarity function is nevertheless problematic as
638
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Figure 1
A multi-layer graph consists of multiple edge layers, one for each similarity feature. Multi-layer
graph partitioning algorithms exploit this representation by computing separate similarity
scores between clusters for each feature layer and then combining them into a single overall
similarity score. This is advantageous over single-layer graph partitioning because it avoids
eagerly combining the similarity scores for individual features into a heuristic instance-wise
similarity score.
weights have to be chosen heuristically. Instead, we will represent similarities with
respect to different features on separate edge layers in the graph. For example, one
layer will represent the similarity between the head words of arguments and another
one will represent the similarity between pars of speech. So, given M features, the
graph will consist of M layers, one for each feature. Edge weights on a particular
layer quantify the similarity between the instances with respect to that feature. This
is illustrated in Figure 1 for two argument instances and three features. Formally, a
multi-layer graph is defined as a pair (V, {E1, . . . , EM}) consisting of vertices V and
a set of edge layers Ef for f = 1 . . .M. The set of vertices V = {v1, . . . , vN} consists
of all N argument instances for a particular verb. The edge layer Ef for feature f
is constructed by connecting all vertex-pairs with non-zero similarity with respect
to f :
Ef = {(vi, vj) ? V ? V|?f (vi, vj) = 0}. (2)
where ?f (vi, vj) is a similarity function for feature f , whose form will be discussed in the
next section. Each edge (vi, vj) ? Ef in layer f is weighted by ?f (vi, vj).
3.1 Feature Similarity Functions
Similarities for a specific feature f are measured with a function ?f (vi, vj) which assigns
a [?1, 1] value to any pair of instances (vi, vj). We assume similarities are measured on
an interval scale?that is, while sums, differences, and averages of the values of some
similarity function ?f express meaningful quantities, products and ratios do not. More-
over, the values of two distinct similarity functions cannot necessarily be meaningfully
compared without rescaling. Positive similarity values indicate that the semantic roles
are likely to be the same, negative values indicate that roles are likely to differ, and zero
values indicate that there is no evidence for either case. The magnitude of ?f expresses
the degree of confidence in the similarity judgment, with extreme values (i.e., ?1 and 1)
indicating maximal confidence.
In our model, we simply use indicator functions which output either 1 or ?1 iff
feature values are equal and 0 otherwise. Specifically, we define four feature similarity
functions that we derive from the principles discussed in Section 1. Our similarity
functions are based on the following features: the argument head words and their parts
639
Computational Linguistics Volume 40, Number 3
of speech,2 the frame constraint, and the syntactic position within a particular linking.
We measure lexical and part-of-speech similarity as follows:
?lex(vi, vj) =
{
1 if vlexi = v
lex
j
0 otherwise
?pos(vi, vj) =
{
1 if vposi = v
pos
j
0 otherwise.
(3)
The constraint that two argument instances vi and vj occurring in the same frame
cannot have the same semantic role is captured by the following similarity function:
?frame(vi, vj) =
{
?1 if vframei = v
frame
j
0 otherwise.
(4)
Finally, we also measure syntactic similarity through an indicator function
?syn(vi, vj), which assumes value 1 if two instances occur in the same syntactic position
within the same linking:
?syn(vi, vj) =
{
1 if vsyni = v
syn
j
0 otherwise.
(5)
The syntactic position of an argument is directly given by the parse tree and can
be encoded, for example, by the full path from predicate to argument head, or for
practical purposes, in order to reduce sparsity, simply through the relation governing
the argument head and its linear position relative to the predicate (left or right). In
contrast, linkings are not directly observed, but we can resort to overt syntactic cues
as a proxy. Examples include the verb?s voice (active/passive), whether it is transitive,
the part-of-speech of the subject, and so on. We argue that in principle, if sufficiently
many cues are taken into account, they will capture one particular linking, although
there may be several encodings for the same linking. Note that syntactic similarity is
not used to construct another graph layer; rather, it will be used for deriving initial
clusters of instances, as we explain in Section 4.1.
4. Graph Partitioning
The graph partitioning problem consists of finding a set of clusters {c1, . . . , cS} that
form a partition of the vertex-set, namely, ?ici = V and ci ? cj = ? for all i = j, such that
(ideally) each cluster contains argument instances of only one particular semantic role,
and the instances for a particular role are all assigned to one and the same cluster. In the
following sections we provide two algorithms for multi-layer graph partitioning, based
on standard clustering algorithms for single-layer graphs. Both algorithms operate
on the same graph but differ in terms of the underlying clustering mechanism they
use. The first algorithm is an adaptation of agglomerative clustering (Jain, Murty, and
Flynn 1999) to the multi-layer setting: Starting from an initial clustering, the algorithm
2 We include parts of speech as a simple means of alleviating the sparsity of head words.
640
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
iteratively merges vertex clusters in order to arrive at increasingly accurate representa-
tions of semantic roles. Rather than greedily merging clusters, our second algorithm is
based on propagating cluster membership information among the set of initial clusters
(Abney 2007).
4.1 Agglomerative Graph Partitioning
The agglomerative algorithm induces clusters in a bottom?up manner starting from an
initial cluster assignment that we will subsequently discuss in detail. Our initialization
results in a clustering that has high purity but low collocation, that is, argument
instances in each cluster tend to belong to the same role but argument instances of
a particular role are scattered among many clusters.3 The algorithm then improves
collocation by iteratively merging pairs of clusters. The agglomeration procedure is
described in Algorithm 1. As can be seen, pairs of clusters are merged iteratively until
a termination criterion is met. The decision of which cluster pair to merge at each
step is made by scoring a set of candidate cluster pairs and choosing the highest one
(line 5). The scoring function s(ci, cj? ) quantifies how likely two clusters are to contain
arguments of the same role. A key question is how to define this scoring function on
the basis of the underlying graph representation, that is, with reference to the instance
similarities expressed by the edges. In order to collect evidence for or against a merge,
we take into account the connectivity of a cluster pair at each feature layer of the graph.
This crucially involves aggregating over all edges that connect the two clusters, and
allows us to infer a cluster-level similarity score from the individual instance-level
similarities encoded in the edges. The evidence collected at each layer is then combined
together in order to arrive at an overall decision (see Figure 1 for an illustration).
3 We define the terms purity and collocation more formally in Section 5.4.
641
Computational Linguistics Volume 40, Number 3
Although it would be possible to enumerate and score all possible cluster pairs
at each step, we apply a more efficient and effective procedure in which the set of
candidates consists of pairs formed by combining a fixed cluster ci with all clusters c?j
larger than ci. This requires comparing only O(|C|) rather than O(|C|2) scores and, more
importantly, it favors merges between large clusters whose score can be computed more
reliably. As mentioned earlier, our scoring function implements an averaging procedure
over the instances contained in the clusters, and thus yields less noisy scores when
clusters are large (i.e., contain many instances). This prioritization promotes reliable
merges over less reliable ones in the earlier phases of the algorithm with a positive
effect on merges in the later phases. Moreover, by keeping ci fixed, we only require that
scores s(ci, x) and s(ci, z) are comparable (i.e., where one cluster is argument in both
scores), rather than comparisons between arbitrary cluster pairs (e.g., s(w, x) and s(y, z)).
In the following, we will provide details on the initialization of the algorithm and the
computation of the similarity scoring function.
A standard agglomerative clustering algorithm forms clusters bottom?up by ini-
tially placing each item of interest in its own cluster. In our case, initializing the algo-
rithm with as many clusters as argument instances would result in a clustering with
maximal purity and minimal collocation. There are two reasons that justify a more
sophisticated initialization procedure for our problem. Firstly, the scoring function we
use is more reliable for larger clusters than for smaller clusters (see the subsequent
discussion). In fact, the standard initialization that creates clusters with a single instance
would not yield useful results as our scoring function crucially relies on initial clusters
containing several instances on average. Secondly, the similarity scores for different
features are not directly comparable. Recall from Section 3.1 that we introduced different
types of similarities based on the arguments? head words (?lex), parts-of-speech (?pos),
syntactic positions (?syn), and frame constraints (?frame). As discussed earlier, engineer-
ing a scoring function that integrates these into a single score without resorting to
heuristic judgments on how to weight them poses a major challenge. In particular, it
is difficult to weight the contribution of the two forms of positive evidence given by
lexical and syntactic similarity. This motivates the idea of using syntactic similarity
for initialization, and lexical similarity (as well as the frame constraint) for scoring.
This separation avoids the difficulty of defining the exact interaction between the two.
Specifically, we obtain an initial clustering by grouping together all instances which
occur in the same syntactic position within a linking?that is, all pairs (vi, vj) for which
?syn(vi, vj) = 1 are grouped into the same cluster, assuming that arguments occurring in
a specific syntactic position under a specific linking share the same role.
We specify the syntactic position of an argument using four cues: the verb?s voice
(active/passive), the argument?s linear position relative to the predicate (left/right),
the syntactic relation of the argument to its governor (e.g., subject or object), and the
preposition used for realizing the argument (if any). Each argument is assigned a four-
tuple consisting of these cues and two syntactic positions are assumed equal iff they
agree on all cues.
Whereas the similarity functions defined in Section 3.1 measure role-semantic
similarity between instances on a particular feature, the scoring function measures role-
semantic similarity between clusters. Naturally, the similarity between two clusters is
defined in terms of the similarities of the instances contained in the clusters. This
involves two aggregation stages. Initially, instance similarities are aggregated in each
feature layer, resulting in an aggregate score for each feature. These layer-specific scores
are then integrated into a single score, which quantifies the overall similarity between
the two clusters (see Figure 1).
642
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
An obvious way to determine the similarity between two clusters (with respect to
a particular feature f ) would be to analyze their connectivity. For example, we could
use edge density (Schaeffer 2007) to average over the weights of edges between two
clusters. However, edge density is an inappropriate measure of similarity in our case,
because we cannot assume that arbitrary pairs of instances are similar with respect to
a particular feature, even if two clusters represent the same semantic role. Consider for
example lexical similarity: Most head words will not agree (even within a cluster) and
therefore averaging between all pairs would yield low scores, regardless of whether
the clusters represent the same role or not. Analogously, the vast majority of instance
pairs from any two clusters will belong to different frames, and thus averaging over
all possible pairs of instances would not yield indicative scores.
We therefore adopt an averaging procedure which finds, for each instance in one
cluster, the instance in the other cluster that is maximally similar or dissimilar and
averages over the scores of these alignments:
sf (ck, cl) =
1
Nk + Nl
?
?
?
vi?ck
abs max
vj?cl
?f (vi, vj) +
?
vj?cl
abs max
vi?ck
?f (vi, vj)
?
? (6)
Here, abs max is a functional that returns the extreme value of its argument, either
positive or negative: abs maxx?X g(x) = g(arg maxx?X |g(x)|). Note that the alignments
are unconstrained in the sense that va ? ck can be aligned to vb ? cl in the first term
of Equation (6), while vb can be aligned to some other instance in the second term.
Moreover, alignments in each term are many-to-one, namely, multiple instances from ck
can be aligned to the same vb ? cl in the first term and likewise in the second term. This
means that score aggregation does not reflect the distributional properties of clusters
(e.g., the frequency of head words in each cluster). Consider for example two clusters
with an identical set of head words. Because many-to-one alignments are allowed, each
instance can be aligned with maximal score to some other instance regardless of the
frequencies of these words.
As an alternative, we also use the well-known cosine similarity function?although
only for the features based on argument head words (lex) and parts of speech (pos):
sf (ck, cl) =
x fk ? x
f
l
?x fk ??x
f
l ?
. (7)
Here x fk and x
f
l are vector representations of the cluster containing as components
the occurrence frequencies of a particular value of the feature f (i.e., lex and pos in
our case). Another solution would be to enforce one-to-one alignments and redefine
Equation (6) as the optimal bipartite matching between the two clusters. Although
this solution adheres to the graph formulation (in contrast to Equation (7)) we see no
theoretical reason that makes it superior to cosine similarity. Moreover, its computation
would require cubic runtime in the number of vertices using the Hungarian algorithm
(Munkres 1957), which is prohibitively slow for sufficiently large clusters.
Layer-specific similarity scores must be combined into an overall cluster similar-
ity score. Because similarity scores and their aggregates for different features are not
directly comparable, their combination through summation would require weighting
each layer score according to its relative strength. Due to the difficulty of specifying
these weights without access to labeled training data, we propose an alternative scheme
643
Computational Linguistics Volume 40, Number 3
that is based on the distinction between positive and negative evidence. Negative
evidence is used to rule out a merge, whereas positive evidence provided by the lexical
score is used to score merges that have not yet been ruled out:
s(ck, cl) =
?
?
?
?
?
?
?
?1 if sframe(ck, cl) < ?
?1 if spos(ck, cl) < ?
slex(ck, cl) if slex(ck, cl) > ?
0 otherwise.
(8)
When the part-of-speech similarity is below a certain threshold ?, or when clause-level
constraints are satisfied to a lesser extent than threshold ?, the score takes value ?1
and the merge is ruled out. If the merge is not ruled out, the lexical similarity score
determines the magnitude of the overall score, provided that it is above threshold ?.
Otherwise, the function returns 0, indicating that neither strong positive nor negative
evidence is available. The cluster-similarity scoring function can be viewed as the
decision function of a binary classifier for deciding on whether to merge a particular
pair of clusters. The classifier is informed by the similarity scores for each feature
layer and outputs a confidence-weighted decision (positive/negative), where the sign
sgn(?f (vi, vj)) indicates the decision and the absolute value |?f (vi, vj)| quantifies confi-
dence. The scoring function in Equation (8) essentially implements a simple decision list
classifier, whose decision rules are sequentially inspected from top to bottom, applying
the first matching rule.
Although our definition avoids weighting, it has introduced threshold parame-
ters ?, ?, and ? that we need to somehow estimate. We propose a scheme in which
parameters ? and ? are iteratively adjusted, and ?, the threshold determining the
extent to which the frame constraints can be violated, is kept fixed. We heuristically set
? to ?0.05, based on the intuition that in principle frame constraints must be satisfied
although in practice, due to noise we expect a small number of violations (i.e., at most
5% of instances can violate the constraint). Parameters ? and ? are initially set to their
maximal value 1, thereby ruling out all merges except those with maximal confidence.
The parameters then decrease iteratively according to a routine whose pseudo-code
is specified in Algorithm 2. The parameter ? decreases at each iteration by a small
amount (0.025) until it reaches  = 0.025, at which point its value is reset to 1.0 and ?
is discounted by a factor close to one (0.9). This is repeated until ? falls below , upon
which the algorithm terminates.
644
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Runtime Analysis. As described in the previous section, Algorithm 1 stops when the
threshold ? falls below some small value . Both ? and ? iteratively decrease based on
a fixed scheme. The outer loop and starting in line 1 is therefore computed in constant
time T. Each pass through the inner loop starting at line 4 iterates over O(|C|) clusters
and for each one of them a score with O(|C|) other clusters is computed. Assume that
fi denotes the fraction of all V instances in cluster ci, namely, fiV = |ci| and
?
|C|
i=1 fi = 1.
Then, overall, the number of instance-wise similarities we need to evaluate is at most
O(|V|2):
|C|
?
i=1
|C|
?
j=i+1
(fi|V|)(fj|V|) =
1
2
|C|
?
i=1
|C|
?
j=1
(fi|V|)(fj|V|) ?
1
2
|C|
?
i=1
(fi|V|)
2
?|V|2
|C|
?
i=1
|C|
?
j=1
fifj = |V|
2
|C|
?
i=1
fi
|C|
?
j=1
fj = |V|
2
The total runtime in terms of the input is therefore O(T ? |V|2). Although this could be
prohibitively inefficient for large data sets, we did not observe long runtimes in our
experiments. Various optimizations are conceivable?for example, the cluster similarity
scores in line 5 of Algorithm 1 can be cached such that they only need to be recomputed
when a cluster changes (i.e., it is merged with another cluster).
4.2 Multi-Layer Label Propagation
Our second graph partitioning algorithm is based on the idea of propagating cluster
membership information along the edges of a graph, subsequently referred to as propa-
gation graph. As we explain in more detail subsequently, compared with agglomerative
clustering, this algorithm in principle is less prone to making false greedy decisions
that cannot be later revoked. Moreover, it has lower runtime and thus scales better to
larger data sets.
The propagation graph is created by collapsing vertices of the initial multi-layer
graph. Vertices in the propagation graph represent an atomic set of instances of the
original graph, that is, a group of instances that are always assigned to the same
cluster. For our induction problem, the vertices of the propagation graph correspond
to the initial clusters of the agglomerative algorithm discussed in Section 4.1. More
formally, let ai ? A denote the i-th vertex of the propagation graph, which references
an atomic cluster of vertices {vi1 . . . viNi} of the original graph that occur in the same
syntactic position within the same linking. Because each vertex of the propagation graph
corresponds to a cluster of vertices in the original graph, the edges of the propagation
graph can be defined in terms of the edges between these vertices in the original graph.
We reuse Equations (6) and (7) to define the edge weights of the propagation graph
as aggregates over the edge weights in the original graph. For each feature layer we
define the set of edges as:
Bf = {(ai, aj) ? A ? A|sf (ai, aj) = 0} (9)
Each edge (ai, aj) ? Bf in layer f is accordingly weighted by sf (ai, aj). Each vertex ai
is associated with a label li, indicating the partition that ai and all the vertices in the
original graph that have been collapsed into ai belongs to.
645
Computational Linguistics Volume 40, Number 3
Note that the label propagation algorithm is informed by the same similarity func-
tions as agglomerative clustering and uses an identical initialization procedure but pro-
vides an alternative means of cluster inference. Initially, each vertex of the propagation
graph belongs to its own cluster, that is, we let the number of clusters L = |A| and set
li ? i. Given this initial vertex labeling, the algorithm proceeds by iteratively updating
the label for each vertex (lines 4?10 in Algorithm 3). This crucially relies on a scoring
procedure in which a score s(l) is computed for each possible label l. We discuss the
details of the scoring procedure below.
The label scoring procedure required in line 5 of Algorithm 3 has parallels to the
cluster pair scoring procedure of the agglomerative algorithm. It also consists of two
stages: Initially, evidence is collected independently on each feature layer by computing
label score aggregates with respect to each feature and then these feature scores are
combined in order to arrive at an overall score.
Assume we are updating vertex ai. The first step is to compute the score for each
feature f and each label l:
sf (l) =
?
aj?Ni(l)
sf (ai, aj) (10)
where Ni(l) = {aj|(ai, aj) ? Bf ? l = lj ? |aj| > |ai|} denotes the set of ai?s neighbors with
label l that are larger than ai. Intuitively, each neighboring vertex votes for the cluster it
is currently assigned to, where the strength of the vote is determined by the similarity
to the vertex (i.e., edge weight) being updated. The votes of all (larger) neighboring
vertices are counted together, resulting in a score for each possible label. The condition
of including only larger vertices for computing the score is analogous to the prioriti-
zation mechanism of the agglomerative algorithm (only merges with larger clusters are
considered for a given candidate cluster). We impose this restriction for the same reason,
namely, that scores for larger clusters are more reliable.
Given the scores sf (l) for a particular label l on each layer f , our goal then is to com-
bine them into a single overall score s(l) for the label. As in agglomerative partitioning,
combining these scores through summation is not possible without ?guessing? their
weights, and therefore we use a sequential combination instead:
s(l) =
?
?
?
?
?
?
?
?1 if sframe(l) < ?
?1 if spos(l) < ?
slex(l) if slex(l) > ?
0 otherwise.
(11)
Analogously to Equation (8), negative evidence that stems from part-of-speech informa-
tion or frame constraints can veto a propagation, whereas positive evidence stemming
from argument head words can promote a propagation. If neither strong evidence
(positive or negative) is available, the label is assigned a zero score. Note that the
scoring function has three parameters with an identical interpretation to those in the
scoring function of the agglomerative algorithm. The threshold update that takes place
in line 11 of Algorithm 3 is therefore the same as the one described in Section 4.1 for
the agglomerative algorithm.
We now analyze the runtime of our algorithm. Let T denote the number of iterations
of the outer loop starting at line 1 of Algorithm 3. The inner loop starting at line 4 iterates
over |A| clusters and for each one of them it has to evaluate at most |A| neighboring
646
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
nodes. Additionally, there are the one-time costs of computing the similarities between
atomic clusters which take O(|V|2) time. The total runtime is therefore O(T|A|2 + |V|2).
Because |A|2 << |V|2, label propagation is substantially faster than agglomerative
clustering.
4.3 Relationship to Single-Layer Graph Partitioning
Clustering algorithms typically assume instance-wise similarities as input (i.e., single-
layer graphs). For our role induction problem, this would require a heuristically defined
similarity function that combines the similarities on individual features into a single
similarity score between instances. In other words, we would collapse the multiple
graph layers into a single layer and then partition the resulting single-layer graph
according to a standard clustering algorithm. A main difference between the two ap-
proaches is the order in which similarities are aggregated: Whereas multi-layer graph
partitioning aggregates similarities on each feature layer first and then combines them
into an overall cluster-wise similarity score, in the single-layer case feature similarities
are eagerly combined into an overall instance-wise similarity score and then aggregated.
Thus, in the multi-layer setting, aggregation can be done in a principled way by con-
sidering the individual feature layers in isolation. For large clusters the resulting scores
for each feature layer will provide reliable evidence for or against a merge. Combining
these cluster-wise similarity scores is much less error-prone than the eager combination
at the instance-level used by the single-layer approach. We experimentally confirm this
intuition (see Section 5.5) by comparing against the single-layer partitioning algorithm
presented in Lang and Lapata (2011b).
5. Role Induction Experiments on English
We adopt the general architecture of supervised semantic role labeling systems where
argument identification and argument classification are treated separately. Our role
labeler is fully unsupervised with respect to both tasks?it does not rely on any role
647
Computational Linguistics Volume 40, Number 3
annotated data or semantic resources. However, our system does not learn from raw
text. In common with most semantic role labeling research, we assume that the input is
syntactically analyzed. Our approach is not tied to a specific syntactic representation?
both constituent- and dependency-based representations can be used. The bulk of
our experiments focus on English data and a dependency-based representation that
simplifies argument identification considerably and is consistent with the CoNLL 2008
benchmark data set used for evaluation in our experiments. To show that our method
can be applied to other languages and across varying syntactic representations, we
also report experiments on German using a constituent-based representation (see
Section 6).
Given the parse of a sentence, our system identifies argument instances and as-
signs them to clusters. Thereafter, argument instances can be labeled with an identifier
corresponding to the cluster they have been assigned to, similar to PropBank core labels
(e.g., A0, A1). We view argument identification as a syntactic processing step that can be
largely undertaken deterministically through analysis of the syntactic tree. We therefore
use a small set of rules to detect arguments with high precision and recall. In the follow-
ing, we first describe the data set (Section 5.1) on which our experiments were carried
out. Next, we present the argument identification component of our system (Section 5.2)
and the method used for comparison with our approach. Finally, we explain how system
output was evaluated (Section 5.4).
5.1 Data
For evaluation purposes, we ran our method on the CoNLL 2008 shared task data set
(Surdeanu et al. 2008), which provides PropBank style gold standard annotations. As
our algorithm induces verb-specific roles, PropBank annotations are a natural choice of
gold standard for our problem. The data set contains annotations for verbal and nominal
predicate-argument constructions, but we only considered the former. The CoNLL data
set was taken from the Wall Street Journal portion of the Penn Treebank and converted
into a dependency format (Surdeanu et al. 2008). Input sentences are represented in
the dependency syntax specified by the CoNLL 2008 shared task (see Figure 2 for an
example). In addition to gold standard dependency parses, the data set also contains
automatic parses obtained from the MaltParser (Nivre et al. 2007), which we will use
as an alternative in our experiments in order to assess the impact of parse quality. For
each argument only the head word is annotated with the corresponding semantic role,
rather than the whole constituent. We assume that argument heads are content words
(e.g., the head of a prepositional phrase is the nominal head rather than the preposition).
We do not treat split arguments or co-referential arguments (e.g., in relative clauses).
Specifically, we ignore arguments with roles preceded by the C- or R- prefix in the
gold standard. All argument lemmas were normalized to lower case; we also replaced
numerical quantities with a placeholder; to further reduce data sparsity, we identified
Figure 2
A sample dependency parse with dependency labels SBJ (subject), OBJ (object), NMOD
(nominal modifier), OPRD (object predicative complement), PRD (predicative complement),
and IM (infinitive marker).
648
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Table 1
Argument identification rules for English.
1. Discard a candidate if it is a coordinating conjunction or punctuation.
2. Discard a candidate if the path of relations from predicate to candidate ends with
coordination, subordination, etc. (see Appendix A for the full list of relations).
3. Keep a candidate if it is the closest subject (governed by the subject-relation) to the left of
a predicate and the relations from predicate p to the governor g of the candidate are all
upward-leading (directed as g ? p).
4. Discard a candidate if the path between the predicate and the candidate, excluding the
last relation, contains a subject relation, adjectival modifier relation, etc. (see Appendix A
for the full list of relations).
5. Discard a candidate if it is an auxiliary verb.
6. Keep a candidate if it is directly connected to the predicate.
7. Keep a candidate if the path from predicate to candidate leads along several verbal nodes
(verb chain) and ends with an arbitrary relation.
8. Discard all remaining candidates.
the head of proper noun phrases heuristically as the most frequent lemma contained in
the phrase.
5.2 Argument Identification
In the supervised setting, a classifier is used in order to decide for each node in the parse
tree whether it represents a semantic argument or not. Nodes classified as arguments
are then assigned a semantic role. In the unsupervised setting, we slightly reformulate
argument identification as the task of discarding as many non-semantic arguments as
possible. This means that the argument identification component does not make a final
positive decision for any of the argument candidates; instead, this decision is deferred to
role induction.4 We assume here that predicate identification is a precursor to argument
identification and can be done relatively straightforwardly based on part-of-speech
information.
The rules given in Table 1 are used to discard or select argument candidates for
English. They primarily take into account the parts of speech and the syntactic relations
encountered when traversing the dependency tree from predicate to argument. A priori,
all words in a sentence are considered argument candidates for a given predicate. Then,
for each candidate, the rules are inspected sequentially and the first matching rule is
applied. We will exemplify how the argument identification component works for the
predicate expect in the sentence The company said it expects its sales to remain steady whose
parse tree is shown in Figure 2. Initially, all words except the predicate itself are treated
as argument candidates. Then, the rules from Table 1 are applied as follows. Firstly,
the words the and to are discarded based on their part of speech (Rule 1); then, remain
is discarded because the path ends with the relation IM and said is discarded as the
4 A few supervised systems implement a similar definition (Koomen et al. 2005), although in most cases
the argument identification component makes a final positive or negative decision regarding the status of
an argument candidate.
649
Computational Linguistics Volume 40, Number 3
path ends with an upward-leading OBJ relation (Rule 2). Rule 3 matches to it, which is
therefore added as a candidate. Next, steady is discarded because there is a downward-
leading OPRD relation along the path and the words company and its are also discarded
because of the OBJ relations along the path (Rule 4). Rule 5 does not apply but the word
sales is kept as a likely argument (Rule 6). Finally, Rule 7 does not apply, because there
are no candidates left.
On the CoNLL 2008 training set, our argument identification rules obtain a pre-
cision of 87.0% and a recall of 92.1% on gold standard parses. On automatic parses,
precision is 79.3% and recall 84.8%. Here, precision measures the percentage of selected
arguments that are actual semantic arguments, and recall measures the percentage of
actual arguments that are not filtered out.
Grenager and Manning (2006) also devise rules for argument identification, un-
fortunately without providing any details on their implementation. More recently, at-
tempts have been made to identify arguments without relying on a treebank-trained
parser (Abend and Rappoport 2010b; Abend, Reichart, and Rappoport 2009). The idea
is to combine a part-of-speech tagger and an unsupervised parser in order to identify
constituents. Likely arguments can be in turn identified based on a set of rules and
the degree of collocation with the predicate. Perhaps unsurprisingly, this method does
not match the quality of a rule-based component operating over trees produced by a
supervised parser.
5.3 Baseline Method for Semantic Role Induction
The linking between semantic roles and syntactic positions is not arbitrary; specific
semantic roles tend to map onto specific syntactic positions such as subject or object
(Levin and Rappaport 2005; Merlo and Stevenson 2001). We further illustrate this
observation in Table 2, which shows how often individual semantic roles map onto
certain syntactic positions. The latter are simply defined as the relations governing the
argument. The frequencies in the table were obtained from the CoNLL 2008 data set and
are aggregates across predicates. As can be seen, semantic roles often approximately
correspond to a single syntactic position. For example, A0 is commonly mapped onto
subject (SBJ), whereas A1 is often realized as object (OBJ).
This motivates a baseline that directly assigns instances to clusters according to
their syntactic position. The pseudo-code is given in Algorithm 4. For each verb we
allocate N = 22 clusters (the maximal number of gold standard clusters together with a
default cluster). Apart from the default cluster, each cluster is associated with a syntactic
position and all instances occurring in that position are mapped into the cluster. Despite
being relatively simple, this baseline has been previously used as a point of comparison
by other unsupervised semantic role labeling systems (Grenager and Manning 2006;
Lang and Lapata 2010) and shown difficult to outperform. This is partly due to the fact
that almost two thirds of the PropBank arguments are either A0 or A1. Identifying these
two roles correctly is therefore the most important distinction to make, and because this
can be largely achieved on the basis of the arguments? syntactic position (see Table 2),
the baseline yields high scores.
5.4 Evaluation
In this section we describe how we assess the quality of a role induction method that
assigns labels to units that have been identified as likely arguments. We also discuss
how we measure whether differences in model performance are statistically significant.
650
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Table 2
Contingency table between syntactic position and semantic roles. Only the eight most frequent
syntactic positions and their labels are listed (i.e., SBJ (Subject), OBJ (Object), ADV (Adverbial),
TMP (Temporal), PMOD (Preposition and its child), OPRD (Object complement), LOC
(Location), DIR (Direction)). Counts were obtained from the CoNLL 2008 training data set using
gold standard parses. The marginals in the right-most column include all syntactic positions
(not only the eight most frequent ones). Boldface highlights the most frequent role per syntactic
position (e.g., SBJ is frequently A0, OBJ is A1).
SBJ OBJ ADV TMP PMOD OPRD LOC DIR Total
A0 50,473 3,350 145 4 2,464 28 12 0 60,398
A1 18,090 50,986 3,207 45 4,819 3,489 118 170 83,535
A2 1,344 2,741 6,413 74 774 2,440 606 800 19,585
A3 88 254 1,208 37 116 114 63 940 3,359
A4 6 20 351 7 79 34 28 2,089 2,687
A5 0 0 19 0 1 3 0 28 67
AA 10 1 0 0 1 0 0 0 13
ADV 7 46 7,364 33 55 31 103 2 8,070
CAU 3 6 215 14 5 0 8 0 1,178
DIR 0 3 304 2 5 1 19 639 1,123
DIS 0 3 3,326 47 2 0 15 0 4,823
EXT 1 6 418 0 6 3 23 4 621
LOC 18 32 358 15 127 2 5,076 9 5,831
MNR 7 54 2,285 22 59 36 154 6 6,238
MOD 9 2,130 77 22 69 3 6 0 9,030
NEG 0 0 3,078 39 0 0 0 0 3,172
PNC 1 11 458 4 4 292 8 4 2,231
PRD 0 2 41 0 0 11 2 0 66
PRT 0 0 0 0 0 0 0 0 2
REC 0 5 8 0 0 0 0 0 14
TMP 14 93 969 14,465 141 1 42 15 16,086
Total 70,071 59,744 30,248 14,830 8,730 6,488 6,285 4,706 228,129
Arguments are labeled based on the cluster they have been assigned to, which
means that in contrast to the supervised setting we cannot verify the correctness of
these labels directly (e.g., by comparing them to the gold standard). Instead, we will
look at the induced clusters as a whole and assess their quality in terms of how well
they reflect the assumed gold standard. Specifically, for each verb, we determine the
extent to which argument instances in the clusters share the same gold standard role
(purity) and the extent to which a particular gold standard role is assigned to a single
cluster (collocation).
More formally, for each group of verb-specific clusters we measure cluster purity
as the percentage of instances belonging to the majority gold class in their respective
cluster. Let N denote the total number of instances, Gj the set of instances belonging to
the j-th gold class, and Ci the set of instances belonging to the i-th cluster. Purity can be
then written as
PU = 1N
?
i
max
j
|Gj ? Ci| (12)
Collocation is the inverse of purity (van Rijsbergen 1974) and defined as follows.
For each gold role, we determine the cluster with the largest number of instances for
651
Computational Linguistics Volume 40, Number 3
that role (the role?s primary cluster) and then compute the percentage of instances that
belong to the primary cluster for each gold role:
CO = 1N
?
j
max
i
|Gj ? Ci| (13)
Per-verb scores are aggregated into an overall score by averaging over all verbs. We
use the micro-average obtained by weighting the scores for individual verbs propor-
tionately to the number of instances for that verb. Finally, we use the harmonic mean of
purity and collocation as a single measure of clustering quality:
F1 = 2?CO?PU
CO + PU
(14)
Purity and collocation measure essentially the same data traits as precision and
recall, which in the context of clustering are, however, defined on pairs of instances
(Manning, Raghavan, and Schu?tze 2008), which makes them a bit harder to grasp
intuitively. We therefore prefer purity and collocation, arguing that these should be
assessed in combination or together with F1 because they can be traded off against each
other. Purity can be trivially maximized by mapping each instance into its own cluster,
and collocation can be trivially maximized by mapping all instances into a single cluster.
Although it is desirable to report performance with a single score such as F1, it
is equally important to assess how purity and collocation contribute to this score. In
particular, if a hypothetical system were to be used for automatically annotating data,
low collocation would result in higher annotation effort and low purity would result in
lower data quality. Therefore high purity is imperative for an effective system whereas
652
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
high collocation contributes to efficient data labeling. For assessing our methods we
therefore introduce the following terminology. If a model attains higher purity than
the baseline, we will say that it is adequate, because it induced roles that adequately
represent semantic roles. If a model attains higher F1 than the baseline, we will say
that it is non-trivial, because it strikes a tradeoff between collocation and purity that is
non-trivial. Our goal then is to find models that are both adequate and non-trivial.
In order to assess whether differences in performance between two models are
statistically significant, we used a sign test. Specifically, we obtained a series of score
pairs by testing two methods on a subsample of the test data. Each subsample corre-
sponds to a random selection of M = 2, 000. We consider the resulting samples to be
?sufficiently? independent to obtain indicative results from the test. As null hypothesis
(H0) we assume that a model m attains scores equal to another model b. Under H0 the
probability that model m outperforms model b on a particular test set is 12 . The random
variable S counting the number of times that scorem > scoreb in a sample of N score pairs
is binomially distributed:
S =
N
?
i=1
1[score(i)m > score
(i)
b ] Bin(
1
2 , N) (15)
We can therefore use S as our test statistic and reject the null hypothesis H0 if S >> N2 .
5.5 Results
Our results are summarized in Tables 3?5, which report cluster purity (PU), collocation
(CO), and their harmonic mean (F1) for the baseline and our two multi-layer graph
partitioning algorithms. We present scores on four data sets that result from the combi-
nation of automatic parses with automatically identified arguments (auto/auto), gold
parses with automatic arguments (gold/auto), automatic parses with gold arguments
(auto/gold), and gold parses with gold arguments (gold/gold). We show how per-
formance varies for our methods when measuring cluster similarity in the two ways
described above: (a) by finding for each instance in one cluster the instance in the
other cluster that is maximally similar or dissimilar and averaging over the scores of
these alignments (avgmax) and (b) by using cosine similarity (see Section 4.1). We also
report results for the single-layer algorithm proposed in Lang and Lapata (2011b).5
Given a verbal predicate, they construct a single-layer graph whose edge weights
express instance-wise similarities directly. The graph is partitioned into vertex clusters
representing semantic roles using a variant of Chinese Whispers, a graph clustering
algorithm proposed by Biemann (2006). The algorithm iteratively assigns cluster labels
to graph vertices by greedily choosing the most common label among the neighbors of
the vertex being updated.
Both agglomerative partitioning and multi-layered label propagation algorithms
systematically achieve higher F1 scores than the baseline?that is, induce non-trivial
clusterings and more adequate semantic roles (by attaining higher purity). For exam-
ple, on the auto/auto data set, the agglomerative algorithm using cosine similarity
5 The results in Table 5 differ slightly from those published in Lang and Lapata (2011b). This is due to a
small change in the preprocessing of the data. For all English experiments reported here, we removed
arguments with R- and C- role prefixes and replaced numbers with a placeholder.
653
Computational Linguistics Volume 40, Number 3
Table 3
Results for agglomerative partitioning (for avgmax and cosine similarity). F1 improvements
over the baseline are statistically significant in all settings (q < 0.001). Boldface highlights the
best performing system according to purity, collocation, and F1.
Parse/Arg Agglomerative
Baseline avgmax cosine
PU CO F1 PU CO F1 PU CO F1
auto/auto 68.3 72.1 70.1 75.3 69.2 72.1 75.5 69.5 72.4
gold/auto 74.9 78.5 76.6 80.3 73.8 76.9 80.7 74.0 77.2
auto/gold 77.0 71.5 74.1 84.9 70.8 77.2 85.6 71.9 78.1
gold/gold 81.6 78.1 79.8 87.4 75.3 80.9 87.9 75.6 81.3
Table 4
Results for multi-layered label propagation (for avgmax and cosine similarity). F1 improvements
over the baseline are statistically significant in all settings (q < 0.001). Boldface highlights the
best performing system according to purity, collocation, and F1.
Parse/Arg Multi-Layer Label Propagation
Baseline avgmax cosine
PU CO F1 PU CO F1 PU CO F1
auto/auto 68.3 72.1 70.1 73.8 70.3 72.0 74.0 70.3 72.1
gold/auto 74.9 78.5 76.6 78.8 74.3 76.5 79.2 74.3 76.7
auto/gold 77.0 71.5 74.1 82.9 72.8 77.5 83.6 73.1 78.0
gold/gold 81.6 78.1 79.8 85.6 75.8 80.4 86.3 76.1 80.9
Table 5
Results for single-layered label propagation using a heuristic similarity function.
F1 improvements over the baseline are statistically significant (q < 0.001) in the auto/gold
and gold/gold settings. Boldface highlights the best performing system according to purity,
collocation, and F1.
Parse/Arg Baseline Label Propagation
PU CO F1 PU CO F1
auto/auto 68.3 72.1 70.1 70.1 70.4 70.2
gold/auto 74.9 78.5 76.6 76.4 77.2 76.8
auto/gold 77.0 71.5 74.1 79.6 72.6 75.9
gold/gold 81.6 78.1 79.8 83.7 78.2 80.9
increases F1 by 2.3 points over the baseline and by 7.2 points in terms of purity. This
increase in purity is achieved by trading off against collocation, although in a favorable
ratio as indicated by the overall higher F1. All improvements over the baseline are
statistically significant (q < 0.001 according to the test described in Section 5.4). In
general, we observe that cosine similarity outperforms avgmax similarity. We conjecture
that cosine is a more appropriate measure of cluster similarity for features where it
is beneficial to capture the distributional similarity of clusters. The two algorithms
perform comparably?differences in F1 are not statistically significant (except in the
gold/auto setting). Nevertheless, agglomerative partitioning obtains higher purity and
F1 than label propagation. The latter trades off more purity and in return obtains
higher collocation. The single-layer method is inferior to the multi-layer algorithms,
654
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
in particular because it is less robust to noise, as demonstrated by the markedly worse
results on automatic parses. On the auto/auto data set the single-layered algorithm is on
a par with the baseline and marginally outperforms it on the auto/gold and gold/gold
configurations.
To help put our results in context, we compare our methods with Titov
and Klementiev?s (2012a) Bayesian clustering models. They report results on the
CoNLL 2008 data sets with two model variants, a factored model that models each verb
independently and a coupled model where model parameters are shared across verbs.
In an attempt to reduce the sparsity of the argument fillers, they also present variants
of the factored and coupled models where the argument heads have been replaced by
lexical cluster ids stemming from Brown et al.?s (1992) clustering algorithm on the RCV1
corpus. In Table 6 we follow Titov and Klementiev (2012a) and show results on the
gold/gold and gold/auto settings. As can be seen, both the agglomerative clustering
and label propagation perform comparably to their coupled model, even though they
do not implement any specific mechanism for sharing clustering preferences across
verbs. Versions of their models that use Brown word clusters (i.e., Factored+Br and
Coupled+Br) yield overall best results. We expect this type of preprocessing to also
increase the performance of our models, however we leave this to future work. Finally,
we should point out that Titov and Klementiev (2012a) do not cluster adjunct-like
modifier arguments that are already explicitly represented in syntax (e.g., TMP, LOC,
DIR). Thus, their Coupled+Mods model is most comparable to ours in terms of the
clustering objective as it treats both core and adjunct arguments and does not make
use of the Brown clustering. Table 6 shows the performance of Coupled+Mods on the
gold/gold setting only because auto/gold results are not reported.
We further examined the output of the baseline and our best performing model
in order to better understand where the performance gains are coming from. Table 7
shows how the two approaches differ when it comes to individual roles. We observe
that the agglomerative clustering algorithm performs better than the baseline on all
core roles. There are some adjunct roles for which the baseline obtains a higher F1.
This is not surprising because the parser directly outputs certain labels such as LOC
and TMP which results in high baseline scores for these roles. A word of caution is
necessary here since core roles are defined individually for each verb and need not have
a uniform corpus-wide interpretation. Thus, conflating per-role scores across verbs is
only meaningful to the extent that these labels actually signify the same role (which is
mostly true for A0 and A1). Furthermore, the purity scores we provide in this context
are averages over the clusters for which the specified role is the majority role.
Table 6
Semantic role induction with graph partitioning and Bayesian clustering.
Model gold/gold auto/gold
PU CO F1 PU CO F1
Baseline 81.6 78.1 79.8 77.0 71.5 74.1
Agglomerative 87.9 75.6 81.3 85.6 71.9 78.1
Multi-Layer LP 86.3 76.1 80.9 83.6 73.1 78.0
Factored 88.1 77.1 82.2 85.1 71.8 77.9
Coupled 89.3 76.6 82.5 86.7 71.2 78.2
Coupled+Mods 89.2 74.0 80.9 ? ? ?
Factored+Br 86.8 78.8 82.6 83.8 74.1 78.6
Coupled+Br 88.7 78.1 83.0 86.2 72.7 78.8
655
Computational Linguistics Volume 40, Number 3
Table 7
Results for individual roles on the auto/auto data set; comparison between the baseline and the
agglomerative clustering algorithm with the cosine similarity function. Boldface highlights the
best performing system according to purity, collocation, and F1.
Role Freq Baseline Agglomerative
PU CO F1 PU CO F1
A0 49,956 68.2 89.6 77.5 71.1 90.0 79.4
A1 72,032 77.5 75.2 76.3 80.7 76.9 78.7
A2 16,795 65.7 71.4 68.4 79.1 68.3 73.3
A3 2,860 45.4 81.8 58.4 71.7 80.1 75.7
A4 2,471 61.6 86.1 71.8 81.6 85.1 83.3
A5 44 46.4 59.1 52.0 92.5 84.1 88.1
AA 9 46.7 100.0 63.6 50.0 100.0 66.7
ADV 5,824 33.8 86.3 48.6 67.7 41.9 51.8
CAU 878 67.5 79.3 72.9 81.5 73.9 77.5
DIR 811 51.5 71.6 59.9 66.9 58.9 62.7
DIS 3,022 36.1 90.4 51.6 57.5 75.7 65.3
EXT 536 46.9 91.0 61.9 70.2 92.2 79.7
LOC 4,481 65.1 76.5 70.4 74.2 58.4 65.3
MNR 5,066 62.0 64.6 63.3 84.3 48.3 61.5
MOD 8,064 80.2 44.1 56.9 90.3 89.3 89.8
NEG 2,952 38.7 98.6 55.6 53.5 98.7 69.4
PNC 1,682 67.9 71.8 69.8 77.8 70.6 74.1
PRD 56 39.1 92.9 55.1 80.4 85.7 83.0
REC 9 25.0 100.0 40.0 75.0 100.0 85.7
TMP 12,928 71.1 78.7 74.7 73.1 43.1 54.2
NONE 49,663 57.1 47.3 51.8 71.6 44.8 55.1
We further investigated the degree to which the baseline and the agglomera-
tive clustering algorithm agree in their role assignments. The overall mean overlap
was 46.03%. Figure 3a shows the percentage of verbs for which the baseline and our
algorithm have no, some, or complete overlap. We discretized overlap into 10 bins of
equal size ranging from 0 to 100. We observe that the role assignments produced by
the two methods have nothing in common for approximately 13.6% verbs, whereas
assignments are identical for 18.1% verbs. Aside from these two bins (see 0 and 100
in Figure 3), a large number of verbs seems to exhibit overlap in the range of 40?60%.
Figure 3b shows how the overlap in the cluster assignments varies with verb frequency.
Perhaps unsurprisingly, we can see that overlap is higher for least frequent and there-
fore less ambiguous verbs. In general, although the two methods have some degree
of overlap, agglomerative clustering does indeed manage to change and improve the
original role assignments of the baseline.
An interesting question concerns precisely the type of changes affected by the
agglomerative clustering algorithm over the assignments of the baseline. To be able
to characterize these changes we first examined the consistency of the role assignments
created by the two algorithms. Specifically, we would expect a verb-argument pair to be
mostly assigned to the same cluster (i.e., an argument to bear the same role label for the
same verb). Of course this is not a hard constraint as arguments and predicates can be
ambiguous and their roles may vary in specific syntactic configurations and contexts.
To give an idea of an upper bound, in our gold standard, an argument instance of the
same verb bears on average 2.23 distinct roles. For comparison, the baseline creates
(on average) 2.9 role clusters for an argument, whereas agglomerative clustering yields
more consistent assignments, with an average of 2.34 role clusters per argument.
656
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Figure 3
Role assignment overlap between the baseline and agglomerative clustering on the auto/
auto data set. Figure 3a shows the percentage of verbs with no overlap (0%), 10% overlap,
20% overlap, 30% overlap, and so on. Figure 3b shows how role overlap varies with verb
frequency. Results are reported on the auto/auto data set.
We further grouped the verbs in our data set into different bins according to their
polysemy and allowable argument realizations. Specifically, we followed Levin?s (1993)
taxonomy and grouped verbs according to the number of semantic classes they inhabit
(e.g., one, two, and so on). We also binned verbs according to the number of alternations
they exhibit. To give an example, the verb donate is a member of the CONTRIBUTE class
and participates in the causative/inchoative and dative alternations, whereas the verb
shower is a member of four classes (i.e., SPRAY/LOAD, PELT, DRESS, and WEATHER) and
participates in the understood reflexive object and spray/load alternations. Figures 4a,b
show the overlap in role assignments between the baseline and agglomerative clus-
tering and how it varies according to verb class ambiguity and argument structure;
figures 4c,d illustrate the same for role assignments and their consistency. As can be
seen, there is less overlap between the two methods when the verbs in question are
more polysemous (Figures 4a) or exhibit more variation in their argument structure
(Figure 4b). As far as consistency in role assignments is concerned, agglomerative
clustering appears overall more consistent than the baseline. As expected, the mean
657
Computational Linguistics Volume 40, Number 3
role assignment is slightly higher for polysemous verbs because differences in meaning
manifest themselves in different argument realizations.
Figure 5 shows how purity, collocation, and F1 vary across alternations and verb
classes. Perhaps unsurprisingly, performance is generally better for least ambiguous
verbs exhibiting a small number of alternations. In general, agglomerative clustering
achieves higher purity across the board whereas the baseline achieves higher collo-
cation. Although agglomerative clustering achieves a consistently higher F1 over the
baseline, the performance of the two algorithms converges for the most polysemous
verbs (i.e., those inhabiting more than six semantic classes; see Figure 5f). Interestingly,
also note that F1 is comparable for verbs with less varied argument structure (i.e., verbs
inhabiting one alternation; see Figure 5c). For such verbs the performance gap between
the baseline and the agglomerative algorithm is narrower both in terms of purity and
collocation. Overall, we observe that agglomerative clustering is able to change some of
the role assignments of the baseline for verbs exhibiting a good degree of alternations
and polysemy.
Table 8 reports results for 12 individual verbs for the best performing method
(i.e., agglomerative partitioning using cosine similarity) on the auto/auto data set.
These verbs were selected so as to exhibit varied occurrence frequencies and alternation
patterns. As can be seen, the macroscopic result?higher F1 due to significantly higher
purity?seems to consistently hold also across verbs. An important exception is the verb
Figure 4
Comparison between the baseline and the agglomerative clustering algorithm in terms of
role assignment overlap (a and b) and consistency (c and d). Verbs are grouped according to
polysemy (a and c) and number of alternations (b and d). All results are reported on the
auto/auto data set.
658
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Figure 5
Comparison between the baseline and the agglomerative clustering algorithm across
alternations (a?c) and verb classes (d?f) using purity, collocation, and F1. All results are reported
on the auto/auto data set.
say, for which the baseline attains high scores due to little variation in its syntactic
realization within the corpus. Example output is given in Table 9, which shows the
five largest clusters produced by the baseline and agglomerative partitioning for the
verb increase. For each cluster we list the 10 most frequent argument head lemmas.
In this case, our method managed to induce an A0 cluster that is not present in the
top five clusters of the baseline, although the cluster also incorrectly contains some A1
arguments that stem from a false merge.
6. Role Induction Experiments on German
The applicability of our method to arbitrary languages is important from a theoretical
and practical perspective. On the one hand, linguistic theory calls for models which are
universal and generalize across languages. This is especially true for models operating
on the (frame-) semantic level, which is a generalization over surface structure and
should therefore be less language specific (Boas 2005). On the other hand, a language-
independent model can be applied to arbitrary languages, genres, and domains and
is thus of greater practical benefit. Because our approach is based on the language-
independent principles discussed in Section 1, we argue that it can easily generalize
to other languages. To test this claim, we further applied our methods to German data.
659
Computational Linguistics Volume 40, Number 3
Table 8
Results for individual verbs on the auto/auto data set; comparison between the baseline and our
agglomerative clustering algorithm with the cosine similarity function. Boldface highlights the
best performing system according to purity, collocation, and F1.
Verb Freq Baseline Agglomerative
PU CO F1 PU CO F1
say 16,698 86.7 90.8 88.7 85.8 90.4 88.0
make 4,589 63.3 71.0 67.0 66.4 71.0 68.6
go 2,331 47.3 56.0 51.3 55.7 55.3 55.5
increase 1,425 58.0 69.0 63.0 59.2 71.5 64.8
know 1,083 58.3 70.8 63.9 58.6 62.0 60.2
tell 969 59.0 76.8 66.7 71.4 68.0 69.7
consider 799 60.7 65.3 62.9 71.0 60.2 65.1
acquire 761 70.7 78.4 74.4 72.0 77.8 74.8
meet 616 70.0 72.2 71.1 78.9 68.3 73.2
send 515 68.3 67.4 67.9 75.9 64.9 70.0
open 528 55.3 67.8 60.9 61.9 55.1 58.3
break 274 51.1 59.1 54.8 62.8 55.8 59.1
Table 9
Five largest clusters created by the baseline and agglomerative partitioning for the verb increase.
Symbols $ and CD are used as placeholders for monetary units and cardinal numbers,
respectively.
Role Baseline
A0 it, sales, revenue, company, profit, rates, they, earnings, we, number
A1 number, reserves, stake, sales, costs, will, board, demand, rates, capacity
A4 $, %, CD, yen, cent, #, member, earlier, kronor, years
ADV $, not, CD, also, be, increase, greatly, month, %, thus
A2 %, $, CD, average, significantly, penny, yen, days, slightly, share
Role Agglomerative
A1 %, number, costs, sales, reserves, demand, stake, competition, pressure, size
A0 it, sales, revenue, company, profit, rates, earnings, we, they, line
A4 $, %, CD, yen, cent, member, result, #, kronor, barrels
A3 $, CD, %, yen, cent, earlier, period, #, member, quarter
TMP year, quarter, month, years, period, september, CD, week, example, instance
Although on a high-level, German clauses do not differ drastically from English
ones with respect to their frame-semantic make-up, there are differences in terms of
how frame elements are mapped onto specific positions on the linear surface structure
of a sentence, beyond any variations observed among English verbs. In general, German
places fewer constraints on word order (more precisely phrase order) and instead relies
on richer morphology to help disambiguate the grammatical functions of linguistic
units. In particular, verbal nominal arguments are marked with a grammatical case6
that directly indicates their grammatical function. Although in main declarative clauses
the inflected part of the verb has to occur in second position, German is commonly
6 German has (partially ambiguous) markers for Nominative, Accusative, Dative, and Genitive.
660
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
considered a verb-final language. This is because the verb often takes the final position
in subordinate clauses, as do infinitive verbs (Brigitta 1996).
6.1 Data
We conducted our experiments on the SALSA corpus (Burchardt et al. 2006), a lexical
resource for German, which, like FrameNet for English, associates predicates with
frames. SALSA is built as an extra annotation layer over the TIGER corpus (Brants
et al. 2002), a treebank for German consisting of approximately 40,000 sentences (700,000
tokens) of newspaper text taken from the Frankfurter Rundschau, although to date not all
predicate-argument structures have been annotated. The frame and role inventory of
SALSA was taken from FrameNet, but has been extended and adapted where necessary
due to lack of coverage and cross-lingual divergences.
The syntactic structure of a sentence is represented through a constituent tree whose
terminal nodes are tokens and non-terminal nodes are phrases (see Figure 6). In addition
to labeling each node with a constituent type such as Sentence, Noun Phrase, and Verb
Phrase, the edges between a parent and a child node are labeled according to the function
of the child within the parent constituent, for example, Accusative Object, Noun Kernel,
or Head. Edges can cross, allowing local and non-local dependencies to be encoded in a
uniform way and eliminating the need for traces. This approach has significant advan-
tages for non-configurational languages such as German, which exhibit a rich inventory
of discontinuous constituents and considerable freedom with respect to word order
(Smith 2003). Compared with the Penn TreeBank (Marcus, Santorini, and Marcinkiewicz
1993), tree structures are relatively flat. For example, the tree does not encode whether
a constituent is a verbal argument or adjunct; this information is encoded through the
edge labels instead.
The frame annotations contained in SALSA do not cover all of the predicate-
argument structures of the underlying TIGER corpus. Only a subset of around
550 predicates with approximately 18,000 occurrences in the corpus have been an-
notated. Moreover, only core roles are annotated, whereas adjunct roles are not, re-
sulting in a smaller number of arguments per predicate (1.96 on average) compared
with the CoNLL 2008 data set (2.57 on average) described in section 5.1. Because our
Figure 6
A sample parse tree for the sentence Pra?sident Jelzin verliert die Mach ans Ku?chenkabinett und
wird die Wahlen kaum gewinnen ko?nnen [translated in English as President Jelzin loses power to the
kitchen cabinet and will hardly be able to win the elections]. The parse tree contains phrase labels NP
(Noun Phrase), PP (Prepositional Phrase), VP (Verb Phrase), S (Sentence), and CS (Coordinated
Sentence). The dependency labels are NK (Noun Kernel), SB (Subject), AO (Object Accusative),
HD (Head), MO (Modifier), AC (Adpositional Case Marker), CJ (Conjunct), and OC (Clausal
Object).
661
Computational Linguistics Volume 40, Number 3
method is designed to induce verb-specific frames, we converted the SALSA frames
into PropBank-like frames by splitting each frame into several verb-specific frames and
accordingly mapping frame roles onto verb-specific roles. Our data set is comparable
to the German data set released as part of the CoNLL 2009 shared task (Hajic? et al.
2009), which was also derived from the SALSA corpus. However, we did not convert
the original constituent-based SALSA representation into dependencies, as we wanted
to assess whether our methods are also compatible with phrase structure trees.
6.2 Experimental Set-up
Although we follow the same experimental set-up as described in Section 5 for En-
glish, there are some deviations due to differences in the data sets utilized for the two
languages. Firstly, in contrast to the CoNLL 2008 data set, the SALSA data set (and
the underlying TIGER corpus) does not supply automatic parse trees and we therefore
conducted our experiments on gold parses only. Moreover, because adjunct arguments
are not annotated in SALSA, and because argument identification is not the central issue
of this work, we chose to also consider only the gold argument identification. Thus,
all our experiments for German were carried out on the gold/gold data set.
A substantial linguistic difference between the German and English data sets is the
sparsity of the argument head lemmas, which is significantly higher for German than
for English: In the CoNLL 2008 data set, the average number of distinct head lemmas
per verb is only 3.69, whereas in the SALSA data set it is 20.12. This is partly due to
the fact that the Wall Street Journal text underlying the English data is topically more
focused than the Rundschau newspaper text, which covers a broader range of news
beyond economics and politics. Moreover, noun compounding is more commonly used
in German than in English (Corston-Oliver and Gamon 2004), which leads to higher
lexical sparsity.
Data sparsity affects our method, which crucially relies on lexical similarity for
determining the role-equivalence of clusters. Therefore, we reduced the number of
syntactic cues used for cluster initialization in order to avoid creating too many small
clusters for which similarities cannot be reliably computed. Specifically, only the syn-
tactic position and function word served as cues to initialize our clusters. Note that, as
in English, the relatively small number of syntactic cues that determine the syntactic
position within a linking is a consequence of the size of our evaluation data set (which
is rather small) and not an inherent limitation of our method. On larger data sets, more
syntactic cues could and should be incorporated in order to increase performance.
In our experiments we compared the baseline introduced in Section 5.3 against
agglomerative partitioning and the label propagation algorithm using both cosine- and
avgmax-similarity. The parameters ?, ?, and ?, which determine the thresholds used
in defining overall similarity scores, were set and updated identically as for English
(i.e., these parameters can be considered language-independent).
6.3 Results
Table 10 reports results for the baseline and our role induction methods, namely, ag-
glomerative clustering and multi-layered label propagation (using the avgmax and
cosine similarity functions) on the SALSA gold/gold data set. For comparison, we
also include results on the English CoNLL-2008 gold/gold data set. As can be seen,
the baseline obtains a similar F1 for German and English, although the contributions
of purity and collocation are different for the two languages. In English, purity is
662
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Table 10
Results of agglomerative partitioning and label propagation for cosine and avgmax similarity
on German. For comparison purposes results for English on the gold/gold data set are also
tabulated. All improvements over the baseline are statistically significant at significance level
q < 0.001.
Model German English
PU CO F1 PU CO F1
Baseline 75.0 81.7 78.2 81.6 78.1 79.8
Agglomerative (avgmax) 77.6 80.8 79.2 87.3 75.3 80.9
Agglomerative (cosine) 77.6 80.8 79.2 87.9 75.6 81.3
Label Propagation (avgmax) 77.4 80.9 79.1 85.6 75.8 80.4
Label Propagation (cosine) 77.5 81.0 79.2 86.3 76.0 80.9
noticeably higher than in German, whereas collocation is higher in German. This is not
surprising when taking into account the distribution of syntactic relations governing an
argument. A few frequent relation labels absorb most of the probability mass in German
(see Figure 7b), whereas the mass is distributed more evenly among the labels in English
(Figure 7a), thus leading to higher purity but lower collocation.
Figure 7
Distribution of syntactic relations governing an argument in English and German data sets.
Only the most frequent relations are shown (a key for the English relations is given in Table 2;
in German the relations are SB (Subject), OA (Object Accusative), CJ (Conjunct), DA (Dative),
CD (Coordinator), MO (Modifier), RE (Subordinate Clause), RS (Reported Speech), OC
(Object Clausal), OP (Object Prepositional), NK (Noun Kernel), and CVC (Collocational
Verb Construction).
663
Computational Linguistics Volume 40, Number 3
Table 11
Results for individual verbs on the gold/gold SALSA data set; comparison between the baseline
and the agglomerative clustering algorithm with the cosine similarity function.
Verb Freq Baseline Agglomerative (cosine)
PU CO F1 PU CO F1
sagen [say] 2,076 96.3 89.0 92.5 97.3 97.7 97.5
wissen [know] 487 79.7 76.0 77.8 80.1 80.3 80.2
berichten [report] 438 79.5 78.3 78.9 80.0 81.3 80.7
nehmen [take] 420 49.8 70.2 58.3 51.9 72.4 60.5
verurteilen [convict] 265 70.9 83.4 76.7 70.6 81.9 75.8
erho?hen [increase] 120 58.3 70.8 64.0 70.8 73.3 72.1
schlie?sen [close] 93 40.9 72.0 52.1 53.8 78.5 63.8
brechen [break] 45 40.0 91.1 55.6 44.4 91.1 59.7
schauen [watch] 35 82.9 91.4 86.9 85.7 71.4 77.9
plazieren [place] 18 55.6 83.3 66.7 66.7 61.1 63.8
treffen [meet] 14 100.0 100.0 100.0 100.0 100.0 100.0
regnen [rain] 12 66.7 83.3 74.1 83.3 50.0 62.5
In German, our role induction algorithms improve over the baseline in terms of
F1. All four methods perform comparably and manage to strike a tradeoff between
collocation and purity that is non-trivial and represents semantic roles adequately.
Compared with English, the difference between the baseline and our algorithms is
narrower. This is because we use fewer syntactic cues for initialization in German, due
to the increased data sparsity discussed in the previous section. This also explains why
there is little variation in the collocation and purity results across methods. However,
qualitatively the tradeoff between purity and collocation is the same as for English (i.e.,
purity is increased at the cost of collocation).
Tables 11 and 12 show per-verb and per-role results, respectively, for agglomerative
clustering using cosine similarity. We report per-verb scores for a selection of 10 verbs
Table 12
Results for individual roles on gold/gold SALSA data set; comparison between the baseline and
the agglomerative clustering algorithm with the cosine similarity function.
Role Freq Baseline Agglomerative (cosine)
PU CO F1 PU CO F1
Agent 1,908 70.4 92.8 80.1 70.5 93.9 80.5
Theme 1,637 69.1 79.2 73.8 69.2 79.7 74.1
Cognizer 1,244 75.7 94.3 84.0 76.2 94.6 84.4
Entity 1,195 79.7 85.9 82.7 78.6 86.7 82.4
Content 1,136 87.2 65.2 74.6 88.7 66.8 76.2
Goal 1,071 62.0 81.0 70.2 87.0 67.2 75.9
Topic 477 85.2 69.4 76.5 86.8 58.9 70.2
Source 267 71.6 94.0 81.3 66.1 76.0 70.7
Goods 171 73.0 68.4 70.6 74.8 66.7 70.5
Buyer 121 65.0 90.1 75.5 70.4 88.4 78.4
Employee 63 50.4 98.4 66.7 50.4 98.4 66.7
Required Situation 56 60.3 78.6 68.3 52.1 82.1 63.8
Opinion 50 66.7 50.0 57.1 69.0 62.0 65.3
Leader 29 86.7 69.0 76.8 86.7 65.5 74.6
Financed 25 79.3 64.0 70.8 80.0 64.0 71.1
664
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
(see Table 12a), which in some cases are translations of the verbs used for English. With
respect to per-role scores, we make use of the fact that roles have a common meaning
across predicates (like A0 and A1 in PropBank), and report scores for a selection of
15 different roles (Table 12b) with varied occurrence frequencies. Per-verb results con-
firm that data sparsity affects performance in German. As can be seen, agglomerative
clustering outperforms the baseline on high-frequency verbs that are less affected by
sparsity, although this is not always the case on lower-frequency verbs. Analogously,
the method tends to perform better on high-frequency roles, whereas there is no clear
trend on lower-frequency roles. In contrast to English, for more than half of the verbs
the method manages to outperform the baseline in terms of both purity and collocation,
which is consistent with our macroscopic result, where the tradeoff between purity and
collocation is not as strong as for English.
The experiments show that our methods can be successfully applied to languages
other than English, thereby supporting the claim that they are based on a set of
language-independent assumptions and principles. Despite substantial differences be-
tween German and English grammar, both generally and in terms of the specific syn-
tactic representation that was used, our methods increased F1 over the baseline for both
languages and resulted in a similar tradeoff between purity and collocation. Improve-
ments were observed in spite of pronounced data sparsity in the case of German. Recall
that we had to reduce the number of syntactic initialization cues in order to be able
to obtain results on the relatively small amount of gold-standard data. We would also
like to note that porting our system to German did not require any additional feature
engineering or algorithmic changes.
7. Conclusions
In this article we described an unsupervised method for semantic role induction in
which argument-instance graphs are partitioned into clusters representing semantic
roles. A major hypothesis underlying our work has been that semantic roles can be
induced without human supervision from a corpus of syntactically parsed sentences
based on three linguistic principles : (1) arguments in the same syntactic position (within
a specific linking) bear the same semantic role, (2) arguments within a clause bear a
unique role, and (3) clusters representing the same semantic role should be more or less
lexically and distributionally equivalent. Based on these principles we have formulated
a similarity-driven model and introduced a multi-layer graph partitioning approach that
represents similarity between clusters on multiple feature layers, whose connectiv-
ity can be analyzed separately and then combined into an overall cluster-similarity
score.
Our work has challenged the established view that supervised learning is the
method of choice for the semantic role labeling task. Although the proposed unsuper-
vised models do yet achieve results comparable to their supervised counterparts, we
have been able to show that they consistently outperform the syntactic baseline across
several data sets that combine automatic and gold parses, with gold and automatic
argument identification in English and German. Our methods obtain F1 scores that are
systematically above the baseline and the purity of the induced clusters is considerably
higher, although in most cases this increase in purity is achieved by decreasing colloca-
tion. In sum, these results provide strong empirical evidence towards the soundness of
our method and the principles they are based on.
In terms of modeling, we have contributed to the body of work on similarity-
driven models by demonstrating their suitability for this problem, their effectiveness,
665
Computational Linguistics Volume 40, Number 3
and their computational efficiency. The models are based on judgments regarding
the similarity of argument instances with respect to their semantic roles. We showed
that these judgments are comparatively simple to formulate and incorporate into a
graph representation of the data. We have introduced the idea of separating different
similarity features into different graph layers, which resolves the problem faced by
many similarity-based approaches of having to heuristically define an instance-wise
similarity function and brings the advantage that cluster similarities can be computed
in a more principled way. Beyond semantic role labeling, we hope that the multi-layered
graph representation described here might be of relevance to other unsupervised prob-
lems such part-of-speech tagging or coreference resolution. The approach is general
and amenable to other graph partitioning algorithms besides agglomeration and label
propagation.
There are two forms of data sparsity that arise in the context of our work, namely,
the lexical sparsity of argument head lemmas and the sparsity of specific combinations
of linking and syntactic position. As our methods are unsupervised, the conceptually
simple solution to sparsity is to train on larger data sets. Because, with some modifica-
tions, our graph partitioning approaches could be scaled to larger data sets (in terms of
orders of magnitude), this is an obvious next step and would address both instances of
data sparsity. Firstly, it would allow us to incorporate a richer set of syntactic features
for initialization and would therefore necessarily result in initial clusterings of higher
purity. Secondly, the larger size of clusters would result in more reliable similarity
scores. Augmenting the data set would therefore almost surely increase the quality of
induced clusterings; however, we leave this to future work.
Another interesting future direction would be to eliminate the model?s reliance on a
syntactic parser that prohibits its application to languages for which parsing resources
are not available. It would therefore be worthwhile, albeit challenging, to build models
that operate on more readily available forms of syntactic analysis or even raw text. For
example, existing work (Abend and Rappoport 2010b; Abend, Reichart, and Rappoport
2009) attempts to identify arguments and distinguish them into core and adjunct ones
through unsupervised part of speech and grammar induction. As much as making our
model more unsupervised it would also be interesting to see whether some form of
weak supervision might help induce higher-quality semantic roles without incurring a
major labeling effort. The ideas conveyed in this article and the proposed methods
extend naturally to this setting: Introducing labels on some of the graph vertices would
translate into a semi-supervised graph-based learning task, akin to Zhu, Ghahramani,
and Lafferty (2003).
Appendix A. Argument Identification Rules
This appendix specifies the full set of relations used by Rules (2) and (4) of the argument
identification rules given for English in Section 5.2, Table 1. The symbols ? and ? denote
the direction of the dependency relation (upward and downward, respectively). The
dependency relations are explained in Surdeanu et al. (2008), in their Table 4.
The relations in Rule (2) from Table 1 are IM??, PRT?, COORD??, P??, OBJ?, PMOD?,
ADV?, SUB??, ROOT?, TMP?, SBJ?, OPRD?.
The relations in Rule (4) are ADV??, AMOD??, APPO??, BNF??-, CONJ??, COORD??,
DIR??, DTV??-, EXT??, EXTR??, HMOD??, IOBJ??, LGS??, LOC??, MNR??, NMOD??,
OBJ??, OPRD??, POSTHON??, PRD??, PRN??, PRP??, PRT??, PUT??, SBJ??, SUB??,
SUFFIX?? TMP??, VOC?? .
666
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Acknowledgments
We are grateful to the anonymous referees,
whose feedback helped to substantially
improve this article. We also thank the
members of the Probabilistic Models reading
group at the University of Edinburgh for
helpful discussions and comments. We
acknowledge the support of EPSRC (grant
EP/K017845/1).
References
Abend, O. and A. Rappoport. 2010a. Fully
unsupervised core-adjunct argument
classification. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 226?236,
Uppsala.
Abend, O. and A. Rappoport. 2010b. Fully
unsupervised core-adjunct argument
classification. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics, pages 226?236,
Uppsala.
Abend, O., R. Reichart, and A. Rappoport.
2009. Unsupervised argument
identification for semantic role labeling.
In Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 28?36, Suntec.
Abney, S. 2007. Semisupervised Learning for
Computational Linguistics. Chapman &
Hall/CRC.
Berg-Kirkpatrick, T., A. Bouchard-Co?te?,
J. DeNero, and D. Klein. 2010. Painless
unsupervised learning with features. In
Proceedings of the Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 582?590,
Los Angeles, CA.
Biemann, C. 2006. Chinese Whispers: An
efficient graph clustering algorithm and its
application to natural language processing
problems. In Proceedings of TextGraphs: The
First Workshop on Graph Based Methods for
Natural Language Processing, pages 73?80,
New York, NY.
Boas, H. 2005. Semantic frames as
interlingual representations for
multilingual lexical databases. International
Journal of Lexicography, 18(4):445?478.
Brants, S., S. Dipper, S. Hansen, W. Lezius,
and G. Smith. 2002. The TIGER treebank.
In Proceedings of the 1st Workshop on
Treebanks and Linguistic Theories,
pages 24?41, Sozopol.
Brigitta, H. 1996. Deutsch ist eine
V/2-Sprache mit Verbendstellung
und freier Wortfolge. In E. Lang and
G. Zifonun, editors, Deutsch U? typologisch,
pages 121?141. Walter de Gruyter.
Brown, P. F., V. J. Della Pietra, P. V. deSouza,
J. C. Lai, and R. L. Mercer. 1992. Class-
based n-gram models of natural language.
Computational Linguistics, 18(4):283?298.
Burchardt, A., K. Erk, A. Frank, A. Kowalski,
S. Pado?, and M. Pinkal. 2006. The SALSA
corpus: a German corpus resource for
lexical semantics. In Proceedings of the
International Conference on Language
Resources and Evaluation, pages 969?974,
Genoa.
Corston-Oliver, S. and M. Gamon. 2004.
Normalizing German and English
inflectional morphology to improve
statistical word alignment. In Robert
Frederking and Kathryn Taylor, editors,
Machine Translation: From Real Users to
Research, volume 3265 of Lecture Notes
in Computer Science. Springer, Berlin
Heidelberg, pages 48?57.
Dowty, D. 1991. Thematic proto roles
and argument selection. Language,
67(3):547?619.
Fu?rstenau, H. and M. Lapata. 2009. Graph
alignment for semi-supervised semantic
role labeling. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 11?20,
Singapore.
Gamallo, P., A. Agustini, and G. Lopes. 2005.
Clustering syntactic positions with similar
semantic requirements. Computational
Linguistics, 31(1):107?146.
Garg, N. and J. Henderson. 2012.
Unsupervised semantic role induction
with global role ordering. In Proceedings of
the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers), pages 145?149, Jeju Island.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Gordon, A. and R. Swanson. 2007.
Generalizing semantic role annotations
across syntactically similar verbs. In
Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 192?199, Prague.
Gordon, D. and M. Desjardins. 1995.
Evaluation and selection of biases in
machine learning. Machine Learning,
20:5?22.
Grenager, T. and C. Manning. 2006.
Unsupervised discovery of a statistical
verb lexicon. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 1?8, Sydney.
667
Computational Linguistics Volume 40, Number 3
Hajic?, J., M. Ciaramita, R. Johansson,
D. Kawahara, M. A. Mart??, L. Ma`rquez,
A. Meyers, J. Nivre, S. Pado?, J. S?te?pa?nek,
P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL 2009 shared
task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009):
Shared Task, pages 1?18, Boulder, CO.
Jain, A., M. Murty, and P. Flynn. 1999. Data
clustering: A review. ACM Computing
Surveys, 31(3):264?323.
Kipper, K., H. T. Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon.
In Proceedings of the AAAI Conference on
Artificial Intelligence, pages 691?696,
Austin, TX.
Koomen, P., V. Punyakanok, D. Roth, and
W. Yih. 2005. Generalized inference
with multiple semantic role labeling
systems. In Proceedings of the Conference on
Computational Natural Language Learning,
pages 181?184, Ann Arbor, MI.
Lang, J. and M. Lapata. 2010. Unsupervised
induction of semantic roles. In Proceedings
of the North American Chapter of the Association
for Computational Linguistics Conference,
pages 939?947, Los Angeles, CA.
Lang, J. and M. Lapata. 2011a. Unsupervised
induction of semantic roles via split-merge
clustering. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 1,117?1,126, Portland, OR.
Lang, J. and M. Lapata. 2011b. Unsupervised
semantic role induction with graph
partitioning. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing, pages 1,320?1,331, Edinburgh.
Levin, B. and M. Rappaport. 2005. Argument
Realization. Cambridge University Press.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Lin, D. and P. Pantel. 2001. Discovery of
inference rules for question-answering.
Natural Langugae Engineering, 7:343?360.
Manning, C., P. Raghavan, and H. Schu?tze.
2008. Introduction to Information Retrieval.
Cambridge University Press.
Marcus, M., B. Santorini, and
M. Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
treebank. Computational Linguistics,
19(2):313?330.
Ma`rquez, L., X. Carras, K. Litkowski,
and S. Stevenson. 2008. Semantic role
labeling: An introduction to the special
issue. Computational Linguistics,
34(2):145?159.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani,
Z. Shi, B. Gu, A. Sarkar, and F. Popowich.
2005. Description of SQUASH, the SFU
question answering summary handler
for the DUC-2005 summarization task.
In Proceedings of the Human Language
Technology Conference and the Conference
on Empirical Methods in Natural Language
Processing Document Understanding
Workshop, Vancouver.
Merlo, P. and S. Stevenson. 2001. Automatic
verb classification based on statistical
distributions of argument structure.
Computational Linguistics, 27:373?408.
Munkres, J. 1957. Algorithms for the
assignment and transportation problems.
Journal of the Society for Industrial and
Applied Mathematics, 5(1):32?38.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit,
A. Chanev, S. Ku?bler, S. Marinov, and
E. Marsi. 2007. Malt-Parser: A language-
independent system for data-driven
dependency parsing. Natural Language
Engineering, 13(2):95?135.
Pado?, S. and M. Lapata. 2009. Cross-lingual
annotation projection of semantic roles.
Journal of Artificial Intelligence Research,
36:307?340.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The Proposition Bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?106.
Poon, H. and P. Domingos. 2009.
Unsupervised semantic parsing. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 1?10, Singapore.
Pradhan, S., W. Ward, and J. Martin. 2008.
Towards robust semantic role labeling.
Computational Linguistics, 34(2):289?310.
Ruppenhofer, J., M. Ellsworth, M. Petruck,
C. Johnson, and J. Scheffczyk. 2006.
FrameNet II: Extended theory and practice,
version 1.3. Technical Report, International
Computer Science Institute, Berkeley, CA.
Schaeffer, S. 2007. Graph clustering. Computer
Science Review, 1(1):27?64.
Shen, D. and M. Lapata. 2007. Using
semantic roles to improve question
answering. In Proceedings of the 2007
Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 12?21, Prague.
Smith, G. 2003. A Brief Introduction to the
TIGER Treebank, Version 1. Technical
Report, University of Potsdam.
668
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using predicate-
argument structures for information
extraction. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics, pages 8?15, Sapporo.
Surdeanu, M., R. Johansson, A. Meyers, and
L. Ma`rquez. 2008. The CoNLL-2008 shared
task on joint parsing of syntactic and
semantic dependencies. In Proceedings of
the Conference on Natural Language Learning,
pages 159?177, Manchester.
Swier, R. and S. Stevenson. 2004.
Unsupervised semantic role labelling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 95?102, Barcelona.
Titov, I. and A. Klementiev. 2011. A
Bayesian model for unsupervised
semantic parsing. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 1,445?1,455,
Portland, OR.
Titov, I. and A. Klementiev. 2012a. A
Bayesian approach to unsupervised
semantic role induction. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational
Linguistics, pages 12?22, Avignon.
Titov, I. and A. Klementiev. 2012b.
Crosslingual induction of semantic roles.
In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers), pages 647?656,
Jeju Island.
van Rijsbergen, C. 1974. Foundation of
evaluation. Journal of Documentation,
30(4):265?374.
Wu, D. and P. Fung. 2009. Semantic roles
for SMT: A hybrid two-pass model. In
Proceedings of Human Language Technologies:
The Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, Companion Volume: Short Papers,
pages 13?16, Boulder, CO.
Zhu, X., Z. Ghahramani, and J. Lafferty.
2003. Semi-supervised learning using
Gaussian fields and harmonic functions.
In Proceedings of the International Conference
on Machine Learning, pages 912?919,
Washington, DC.
669

This article has been cited by:
1. Dan Jurafsky. 2014. Charles J. Fillmore. Computational Linguistics 40:3, 725-731. [Citation] [Full
Text] [PDF] [PDF Plus]
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 91?99,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Visual Information in Semantic Representation
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The question of how meaning might be ac-
quired by young children and represented by
adult speakers of a language is one of the most
debated topics in cognitive science. Existing
semantic representation models are primarily
amodal based on information provided by the
linguistic input despite ample evidence indi-
cating that the cognitive system is also sensi-
tive to perceptual information. In this work we
exploit the vast resource of images and associ-
ated documents available on the web and de-
velop a model of multimodal meaning repre-
sentation which is based on the linguistic and
visual context. Experimental results show that
a closer correspondence to human data can be
obtained by taking the visual modality into ac-
count.
1 Introduction
The representation and modeling of word mean-
ing has been a central problem in cognitive science
and natural language processing. Both disciplines
are concerned with how semantic knowledge is ac-
quired, organized, and ultimately used in language
processing and understanding. A popular tradition
of studying semantic representation has been driven
by the assumption that word meaning can be learned
from the linguistic environment. Words that are sim-
ilar in meaning tend to behave similarly in terms
of their distributions across different contexts. Se-
mantic spacemodels, among which Latent Semantic
Analysis (LSA, Landauer and Dumais 1997) is per-
haps known best, operationalize this idea by captur-
ing word meaning quantitatively in terms of simple
co-occurrence statistics. Each word w is represented
by a k element vector reflecting the local distribu-
tional context of w relative to k context words. More
recently, topic models have been gaining ground as
a more structured representation of word meaning.
In contrast to more standard semantic space mod-
els where word senses are conflated into a single
representation, topic models assume that words ob-
served in a corpus manifest some latent structure ?
word meaning is a probability distribution over a set
of topics (corresponding to coarse-grained senses).
Each topic is a probability distribution over words,
and the content of the topic is reflected in the words
to which it assigns high probability.
Semantic space (and topic) models are extracted
from real language corpora, and thus provide a direct
means of investigating the influence of the statistics
of language on semantic representation. They have
been successful in explaining a wide range of be-
havioral data ? examples include lexical priming,
deep dyslexia, text comprehension, synonym selec-
tion, and human similarity judgments (see Landauer
and Dumais 1997 and the references therein). They
also underlie a large number of natural language
processing (NLP) tasks including lexicon acquisi-
tion, word sense discrimination, text segmentation
and notably information retrieval. Despite their pop-
ularity, these models offer a somewhat impoverished
representation of word meaning based solely on in-
formation provided by the linguistic input.
Many experimental studies in language acquisi-
tion suggest that word meaning arises not only from
exposure to the linguistic environment but also from
our interaction with the physical world. For ex-
ample, infants are from an early age able to form
perceptually-based category representations (Quinn
et al, 1993). Perhaps unsurprisingly, words that re-
fer to concrete entities and actions are among the
first words being learned as these are directly ob-
servable in the environment (Bornstein et al, 2004).
Experimental evidence also shows that children re-
spond to categories on the basis of visual features,
e.g., they generalize object names to new objects of-
ten on the basis of similarity in shape (Landau et al,
1998) and texture (Jones et al, 1991).
In this paper we aim to develop a unified mod-
91
eling framework of word meaning that captures the
mutual dependence between the linguistic and visual
context. This is a challenging task for at least two
reasons. First, in order to emulate the environment
within which word meanings are acquired, we must
have recourse to a corpus of verbal descriptions and
their associated images. Such corpora are in short
supply compared to the large volumes of solely tex-
tual data. Secondly, our model should integrate lin-
guistic and visual information in a single representa-
tion. It is unlikely that we have separate representa-
tions for different aspects of word meaning (Rogers
et al, 2004).
We meet the first challenge by exploiting mul-
timodal corpora, namely collections of documents
that contain pictures. Although large scale corpora
with a one-to-one correspondence between words
and images are difficult to come by, datasets that
contain images and text are ubiquitous. For exam-
ple, online news documents are often accompanied
by pictures. Using this data, we develop a model
that combines textual and visual information to learn
semantic representations. We assume that images
and their surrounding text have been generated by
a shared set of latent variables or topics. Our model
follows the general rationale of topic models ? it is
based upon the idea that documents are mixtures of
topics. Importantly, our topics are inferred from the
joint distribution of textual and visual words. Our
experimental results show that a closer correspon-
dence to human word similarity and association can
be obtained by taking the visual modality into ac-
count.
2 Related Work
The bulk of previous work has focused on models of
semantic representation that are based solely on tex-
tual data. Many of these models represent words as
vectors in a high-dimensional space (e.g., Landauer
and Dumais 1997), whereas probabilistic alterna-
tives view documents as mixtures of topics, where
words are represented according to their likelihood
in each topic (e.g., Steyvers and Griffiths 2007).
Both approaches allow for the estimation of similar-
ity between words. Spatial models compare words
using distance metrics (e.g., cosine), while proba-
bilistic models measure similarity between terms ac-
cording to the degree to which they share the same
topic distributions.
Within cognitive science, the problem of how
words are grounded in perceptual representations
has attracted some attention. Previous modeling ef-
forts have been relatively small-scale, using either
artificial images, or data gathered from a few sub-
jects in the lab. Furthermore, the proposed models
work well for the tasks at hand (e.g., either word
learning or object categorization) but are not de-
signed as a general-purpose meaning representation.
For example, Yu (2005) integrates visual informa-
tion in a computational model of lexical acquisi-
tion and object categorization. The model learns a
mapping between words and visual features from
data provided by (four) subjects reading a children?s
story. In a similar vein, Roy (2002) considers the
problem of learning which words or word sequences
refer to objects in a synthetic image consisting of ten
rectangles. Andrews et al (2009) present a proba-
bilistic model that incorporates perceptual informa-
tion (indirectly) by combining distributional infor-
mation gathered from corpus data with speaker gen-
erated feature norms1 (which are also word-based).
Much work in computer vision attempts to learn
the underlying connections between visual features
and words from examples of images annotated with
description keywords. The aim here is to enhance
image-based applications (e.g., search or retrieval)
by developing models that can label images with
keywords automatically. Most methods discover
the correlations between visual features and words
by introducing latent variables. Standard latent se-
mantic analysis (LSA) and its probabilistic variant
(PLSA) have been applied to this task (Pan et al,
2004; Hofmann, 2001; Monay and Gatica-Perez,
2007). More sophisticated approaches estimate the
joint distribution of words and regional image fea-
tures, whilst treating annotation as a problem of sta-
tistical inference in a graphical model (Blei and Jor-
dan, 2003; Barnard et al, 2002).
Our own work aims to develop a model of se-
mantic representation that takes visual context into
account. We do not model explicitly the correspon-
dence of words and visual features, or learn a map-
ping between words and visual features. Rather,
we develop a multimodal representation of meaning
which is based on visual information and distribu-
tional statistics. We hypothesize that visual features
are crucial in acquiring and representing meaning
1Participants are given a series of object names and for each
object they are asked to name all the properties they can think
of that are characteristic of the object.
92
Michelle Obama fever hits the UK
In the UK on her first
visit as first lady, Michelle
Obama seems to be mak-
ing just as big an im-
pact. She has attracted as
much interest and column
inches as her husband on
this London trip; creating
a buzz with her dazzling outfits, her own schedule
of events and her own fanbase. Outside Bucking-
ham Palace, as crowds gathered in anticipation of
the Obamas? arrival, Mrs Obama?s star appeal was
apparent.
Table 1: Each article in the document collection contains
a document (the title is shown in boldface), and image
with related content.
and conversely, that linguistic information can be
useful in isolating salient visual features. Our model
extracts a semantic representation from large docu-
ment collections and their associated images without
any human involvement. Contrary to Andrews et al
(2009) we use visual features directly without rely-
ing on speaker generated norms. Furthermore, un-
like most work in image annotation, we do not em-
ploy any goldstandard data where images have been
manually labeled with their description keywords.
3 Semantic Representation Model
Much like LSA and the related topic models our
model creates semantic representations from large
document collections. Importantly, we assume that
the documents are paired with images which in turn
describe some of the document?s content. Our ex-
periments make use of news articles which are of-
ten accompanied with images illustrating events, ob-
jects or people mentioned in the text. Other datasets
with similar properties include Wikipedia entries
and their accompanying pictures, illustrated stories,
and consumer photo collections. An example news
article and its associated image is shown in Table 1
(we provide more detail on the database we used in
our experiments in Section 4).
Our model exploits the redundancy inherent in
this multimodal collection. Specifically, we assume
that the images and their surrounding text have been
generated by a shared set of topics. A potential
stumbling block here is the fact that images and
documents represent distinct modalities: images are
commonly described by a continuous feature space
(e.g., color, shape, texture; Barnard et al 2002; Blei
and Jordan 2003), whereas words are discrete. For-
tunately, we can convert the visual features from a
continuous onto a discrete space, thereby rendering
image features more like word units. In the follow-
ing we describe how we do this and then move on to
present an extension of Latent Dirichlet Allocation
(LDA, Blei and Jordan 2003), a topic model that can
be used to represent meaning as a probability distri-
bution over a set of multimodal topics. Finally, we
discuss how word similarity can be measured under
this model.
3.1 Image Processing
A large number of image processing techniques have
been developed in computer vision for extracting
meaningful features which are subsequently used
in a modeling task. For example, a common first
step to all automatic image annotation methods is
partitioning the image into regions, using either an
image segmentation algorithm (such as normalized
cuts; Shi and Malik 2000) or a fixed-grid layout
(Feng et al, 2004). In the first case the image is
represented by irregular regions (see Figure 1(a)),
whereas in the second case the image is partitioned
into smaller scale regions which are uniformly ex-
tracted from a fixed grid (see Figure 1(b)). The ob-
tained regions are further represented by a standard
set of features including color, shape, and texture.
These can be treated as continuous vectors (Blei and
Jordan, 2003) or in quantized form (Barnard et al,
2002).
Despite much progress in image segmentation,
there is currently no automatic algorithm that can
reliably divide an image into meaningful parts. Ex-
tracting features from small local regions is thus
preferable, especially for image collections that are
diverse and have low resolution (this is often the case
for news images). In our work we identify local re-
gions using a difference-of-Gaussians point detector
(see Figure 1(c)). This representation is based on de-
scriptors computed over automatically detected im-
age regions. It provides a much richer (and hopefully
more informative) feature space compared to the
alternative image representations discussed above.
For example, an image segmentation algorithm,
would extract at most 20 regions from the image
in Figure 1; uniform grid segmentation yields 143
93
(a) (b) (c)
Figure 1: Image partitioned into regions of varying granularity using (a) the normalized cut image segmentation algo-
rithm, (b) uniform grid segmentation, and (c) the SIFT point detector.
(11 ? 13) regions, whereas an average of 240 points
(depending on the image content) are detected. A
non-sparse feature representation is critical in our
case, since we usually do not have more than one
image per document.
We compute local image descriptors using the
the Scale Invariant Feature Transform (SIFT) algo-
rithm (Lowe, 1999). Importantly, SIFT descriptors
are designed to be invariant to small shifts in posi-
tion, changes in illumination, noise, and viewpoint
and can be used to perform reliable matching be-
tween different views of an object or scene (Mikola-
jczyk and Schmid, 2003; Lowe, 1999). We further
quantize the SIFT descriptors using the K-means
clustering algorithm to obtain a discrete set of vi-
sual terms (visiterms) which form our visual vo-
cabulary VocV . Each entry in this vocabulary stands
for a group of image regions which are similar
in content or appearance and assumed to origi-
nate from similar objects. More formally, each im-
age I is expressed in a bag-of-words format vector,
[v1,v2, ...,vL], where vi = n only if I has n regions
labeled with vi. Since both images and documents
in our corpus are now represented as bags-of-words,
and since we assume that the visual and textual
modalities express the same content, we can go a
step further and represent the document and its as-
sociated image as a mixture of verbal and visual
words dMix. We will then learn a topic model on this
concatenated representation of visual and textual in-
formation.
3.2 Topic Model
Latent Dirichlet Allocation (Blei et al, 2003; Grif-
fiths et al, 2007) is a probabilistic model of text gen-
eration. LDA models each document using a mix-
ture over K topics, which are in turn characterized
as distributions over words. The words in the docu-
ment are generated by repeatedly sampling a topic
according to the topic distribution, and selecting a
word given the chosen topic. Under this framework,
the problem of meaning representation is expressed
as one of statistical inference: given some data ?
textual and visual words ? infer the latent structure
from which it was generated. Word meaning is thus
modeled as a probability distribution over a set of
latent multimodal topics.
LDA can be represented as a three level hierarchi-
cal Bayesian model. Given a corpus consisting of M
documents, the generative process for a document d
is as follows. We first draw the mixing proportion
over topics ?d from a Dirichlet prior with parame-
ters ?. Next, for each of the Nd words wdn in doc-
ument d, a topic zdn is first drawn from a multino-
mial distribution with parameters ?dn. The probabil-
ity of a word token w taking on value i given that
topic z = j is parametrized using a matrix ? with
bi j = p(w = i|z = j). Integrating out ?d?s and zdn?s,
gives P(D|?,?), the probability of a corpus (or doc-
ument collection):
M
?
d=1
Z
P(?d |?)
(
Nd
?
n=1
?
zdn
P(zdn|?d)P(wdn|zdn,?)
)
d?d
The central computational problem in topic
modeling is to compute the posterior distribu-
tion P(?,z|w,?,?) of the hidden variables given
a document w = (w1,w2, . . . ,wN). Although this
distribution is intractable in general, a variety of ap-
94
proximate inference algorithms have been proposed
in the literature including variational inference
which our model adopts. Blei et al (2003) introduce
a set of variational parameters, ? and ?, and show
that a tight lower bound on the log likelihood of
the probability can be found using the following
optimization procedure:
(??,??) = argmin
?,?
D(q(?,z|?,?)||p(?,z|w,?,?))
Here, D denotes the Kullback-Leibler (KL) diver-
gence between the true posterior and the variational
distribution q(?,z|?,?) defined as: q(?,z|?,?) =
q(?|?)?Nn=1 q(zn|?n), where the Dirichlet parame-
ter ? and the multinomial parameters (?1, . . . ,?N) are
the free variational parameters. Notice that the opti-
mization of parameters (??(w),??(w)) is document-
specific (whereas ? is corpus specific).
Previous applications of LDA (e.g., to docu-
ment classification or information retrieval) typi-
cally make use of the posterior Dirichlet parame-
ters ??(w) associated with a given document. We are
not so much interested in ? as we wish to obtain a
semantic representation for a given word across doc-
uments. We therefore train the LDA model sketched
above on a corpus of multimodal documents {dMix}
consisting of both textual and visual words. We se-
lect the number of topics, K, and apply the LDA al-
gorithm to obtain the ? parameters, where ? repre-
sents the probability of a word wi given a topic z j,
p(wi|z j) = ?i j. The meaning of wi is thus extracted
from ? and is a K-element vector, whose compo-
nents correspond to the probability of wi given each
latent topic assumed to have generated the document
collection.
3.3 Similarity Measures
The ability to accurately measure the similarity or
association between two words is often used as a di-
agnostic for the psychological validity of semantic
representation models. In the topic model described
above, the similarity between two words w1 and w2
can be intuitively measured by the extent to which
they share the same topics (Griffiths et al, 2007).
For example, we may use the KL divergence to mea-
sure the difference between the distributions p and q:
D(p,q) =
K
?
j=1
p j log2
p j
q j
where p and q are shorthand for P(w1|z j)
and P(w2|z j), respectively.
The KL divergence is asymmetric and in many ap-
plications, it is preferable to apply a symmetric mea-
sure such as the Jensen Shannon (JS) divergence.
The latter measures the ?distance? between p and q
through (p+q)2 , the average of p and q:
JS(p,q) =
1
2
[
D(p,
(p+q)
2
)+D(q,
(p+q)
2
)
]
An alternative approach to expressing the similar-
ity between two words is proposed in Griffiths et al
(2007). The underlying idea is that word association
can be expressed as a conditional distribution. If we
have seen word w1, then we can determine the prob-
ability that w2 will be also generated by comput-
ing P(w2|w1). Although the LDA generative model
allows documents to contain multiple topics, here it
is assumed that both w1 and w2 came from a single
topic:
P(w2|w1) =
K
?
z=1
P(w2|z)P(z|w1)
P(z|w1) ? P(w1|z)P(z)
where p(z) is uniform, a single topic is sampled
from the distribution P(z|w1), and an overall esti-
mate is obtained by averaging over all topics K.
Griffiths et al (2007) report results on mod-
eling human association norms using exclu-
sively P(w2|w1). We are not aware of any previous
work that empirically assesses which measure is best
at capturing semantic similarity. We undertake such
an empirical comparison as it is not a priory obvious
how similarity is best modeled under a multimodal
representation.
4 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the model pre-
sented above. We give details on our training proce-
dure and parameter estimation and present the base-
line method used for comparison with our model.
Data We trained the multimodal topic model on
the corpus created in Feng and Lapata (2008). It
contains 3,361 documents that have been down-
loaded from the BBC News website.2 Each doc-
ument comes with an image that depicts some of
its content. The images are usually 203 pixels wide
2http://news.bbc.co.uk/
95
and 152 pixels high. The average document length
is 133.85 words. The corpus has 542,414 words in
total. Our experiments used a vocabulary of 6,253
textual words. These were words that occurred at
least five times in the whole corpus, excluding
stopwords. The accompanying images were prepro-
cessed as follows. We first extracted SIFT features
from each image (150 on average) which we subse-
quently quantized into a discrete set of visual terms
using K-means. As we explain below, we deter-
mined an optimal value for K experimentally.
Evaluation Our evaluation experiments compared
the multimodal topic model against a standard text-
based topic model trained on the same corpus whilst
ignoring the images. Both models were assessed on
two related tasks, that have been previously used
to evaluate semantic representation models, namely
word association and word similarity.
In order to simulate word association, we used
the human norms collected by Nelson et al (1999).3
These were established by presenting a large num-
ber of participants with a cue word (e.g., rice) and
asking them to name an associate word in response
(e.g.,Chinese, wedding, food, white). For each word,
the norms provide a set of associates and the fre-
quencies with which they were named. We can thus
compute the probability distribution over associates
for each cue. Analogously, we can estimate the de-
gree of similarity between a cue and its associates
using our model (and any of the measures in Sec-
tion 3.3). And consequently examine (using corre-
lation analysis) the degree of linear relationship be-
tween the human cue-associate probabilities and the
automatically derived similarity values. We also re-
port howmany times the word with the highest prob-
ability under the model was the first associate in the
norms. The norms contain 10,127 unique words in
total. Of these, we created semantic representations
for the 3,895 words that appeared in our corpus.
Our word similarity experiment used the Word-
Sim353 test collection (Finkelstein et al, 2002)
which consists of relatedness judgments for word
pairs. For each pair, a similarity judgment (on
a scale of 0 to 10) was elicited from human
subjects (e.g., tiger-cat are very similar, whereas
delay?racism are not). The average rating for each
pair represents an estimate of the perceived sim-
ilarity of the two words. The task varies slightly
from word association. Here, participants are asked
3http://www.usf.edu/Freeassociation.
Figure 2: Performance of multimodal topic model on pre-
dicting word association under varying topics and visual
terms (development set).
to rate perceived similarity rather than generate the
first word that came into their head in response to a
cue word. The collection contains similarity ratings
for 353 word pairs. Of these, we constructed seman-
tic representations for the 254 that appeared in our
corpus. We also evaluated how well model produced
similarities correlate with human ratings. Through-
out this paper we report correlation coefficients us-
ing Pearson?s r.
5 Experimental Results
Model Selection The multimodal topic model has
several parameters that must be instantiated. These
include the quantization of the image features, the
number of topics, the choice of similarity function,
and the values for ? and ?. We explored the pa-
rameter space on held-out data. Specifically, we fit
the parameters for the word association and similar-
ity models separately using a third of the associa-
tion norms and WordSim353 similarity judgments,
respectively. As mentioned in Section 3.1 we used
K-means to quantize the image features into a dis-
crete set of visual terms. We varied K from 250
to 2000.We also varied the number of topics from 25
to 750 for both the multimodal and text-based topic
models. The parameter ? was set to 0.1 and ? was
initialized randomly. The model was trained using
variational Bayes until convergence of its bound on
the likelihood objective. This took 1,000 iterations.
Figure 2 shows how word association perfor-
mance varies on the development set with different
numbers of topics (t) and visual terms (r) according
96
Figure 3: Performance of multimodal topic model on pre-
dicting word similarity under varying topics and visual
terms (development set).
to three similarity measures: KL divergence, JS di-
vergence, and P(w2|w1), the probability of word w2
given w1 (see Section 3.3). Figure 3 shows results on
the development set for the word similarity task. As
far as word association is concerned, we obtain best
results with P(w2|w1), 750 visual terms and 750 top-
ics (r = 0.188). On word similarity, JS performs best
with 500 visual terms and 25 topics (r = 0.374). It is
not surprising that P(w2|w1) works best for word as-
sociation. The measure expresses the associative re-
lations between words as a conditional distribution
over potential response words w2 for cue word w1.
A symmetric function is more appropriate for word
similarity as the task involves measuring the degree
to which to words share some meaning (expressed
as topics in our model) rather than whether a word is
likely to be generated as a response to another word.
These differences also lead to different parametriza-
tions of the semantic space. A rich visual term vo-
cabulary (750 terms) is needed for modeling associ-
ation as broader aspects of word meaning are taken
into account, whereas a sparser more focused repre-
sentation (with 500 visual terms and 25 overall top-
ics) is better at isolating the common semantic con-
tent between two words. We explored the parame-
ter space for the text-based topic model in a sim-
ilar fashion. On the word association task the best
correlation coefficient was achieved with 750 top-
ics and P(w2|w1) (r = 0.139). On word similarity,
the best results were obtained with 75 topics and the
JS divergence (r = 0.309).
Model Word Association Word Similarity
UpperBnd 0.400 0.545
MixLDA 0.123 0.318
TxtLDA 0.077 0.247
Table 2: Model performance on word association and
similarity (test set).
Model Comparison Table 2 summarizes our re-
sults on the test set using the optimal set of pa-
rameters as established on the development set. The
first row shows how well humans agree with each
other on the two tasks (UpperBnd). We estimated
the intersubject correlation using leave-one-out re-
sampling4 (Weiss and Kulikowski, 1991). As can
be seen, in all cases the topic model based on tex-
tual and visual modalities (MixLDA) outperforms
the model relying solely on textual information
(TxtLDA). The differences in performance are sta-
tistically significant (p < 0.05) using a t-test (Cohen
and Cohen, 1983).
Steyvers and Griffiths (2007) also predict word
association using Nelson?s norms and a state-of-the-
art LDA model. Although they do not report correla-
tions, they compute how many times the word with
the highest probability P(w2|w1) under the model
was the first associate in the human norms. Using
a considerably larger corpus (37,651 documents),
they reach an accuracy of 16.15%. Our corpus con-
tains 3,361 documents, the MixLDA model per-
forms at 14.15% and the LDA model at 13.16%. Us-
ing a vector-based model trained on the BNC corpus
(100Mwords), Washtell andMarkert (2009) report a
correlation of 0.167 on the same association data set,
whereas our model achieves a correlation of 0.123.
With respect to word similarity, Marton et al (2009)
report correlations within the range of 0.31?0.54 us-
ing different instantiations of a vector-based model
trained on the BNC with a vocabulary of 33,000
words. Our MixLDA model obtains a correlation
of 0.318 with a vocabulary five times smaller (6,253
words). Although these results are not strictly com-
parable due to the different nature and size of the
training data, they give some indication of the qual-
ity of our model in the context of other approaches
that exploit only the textual modality. Besides, our
intent is not to report the best performance possible,
4We correlated the data obtained from each participant with
the ratings obtained from all other participants and report the
average.
97
GAME, CONSOLE, XBOX, SECOND, SONY, WORLD,
TIME, JAPAN, JAPANESE, SCHUMACHER, LAP, MI-
CROSOFT, ALONSO, RACE, TITLE, WIN, GAMERS,
LAUNCH, RENAULT, MARKET
PARTY, MINISTER, BLAIR, LABOUR, PRIME, LEADER,
GOVERNMENT, TELL, BROW, MP, TONY, SIR, SECRE-
TARY, ELECTION, CONFERENCE, POLICY, NEW, WANT,
PUBLIC, SPEECH
SCHOOL, CHILD, EDUCATION, STUDENT, WORK,
PUPIL, PARENT, TEACHER, GOVERNMENT, YOUNG,
SKILL, AGE, NEED, UNIVERSITY, REPORT, LEVEL,
GOOD, HELL, NEW, SURVEY
Table 3: Most frequent words in three topics learnt from
a corpus of image-document pairs.
but to show that a model of meaning representation
is more accurate when taking visual information into
account.
Table 3 shows some examples of the topics
found by our model, which largely form coher-
ent blocks of semantically related words. In gen-
eral, we observe that the model using image fea-
tures tends to prefer words that visualize easily
(e.g., CONSOLE, XBOX). Furthermore, the visual
modality helps obtain crisper meaning distinctions.
Here, SCHUMACHER is a very probable world for
the ?game? cluster. This is because the Formula One
driver appears as a character in several video games
discussed and depicted in our corpus. For com-
parison the ?game? cluster for the text-based LDA
model contains the words: GAME, USE, INTERNET,
SITE, USE, SET, ONLINE, WEB, NETWORK, MUR-
RAY, PLAY, MATCH, GOOD, WAY, BREAK, TECH-
NOLOGY, WORK, NEW, TIME, SECOND.
We believe the model presented here works bet-
ter than a vanilla text-based topic model for at least
three reasons: (1) the visual information helps cre-
ate better clusters (i.e., conceptual representations)
which in turn are used to measure similarity or as-
sociation; these clusters themselves are amodal but
express commonalities across the visual and textual
modalities; (2) the model is also able to capture per-
ceptual correlations between words. For example,
RED is the most frequent associate for APPLE in Nel-
son?s norms. This association is captured in our vi-
sual features (pictures with apples cluster with pic-
tures showing red objects) even though RED does not
co-occur with APPLE in our data; (3) finally, even in
cases where two words are visually very different in
terms of shape or color (e.g., BANANA and APPLE),
they tend to appear in images with similar structure
(e.g., on tables, in bowls, as being held or eaten by
someone) and thus often share some common ele-
ment of meaning.
6 Conclusion
In this paper we developed a computational model
that unifies visual and linguistic representations of
word meaning. The model learns from natural lan-
guage corpora paired with images under the assump-
tion that visual terms and words are generated by
mixtures of latent topics. We have shown that a
closer correspondence to human data can be ob-
tained by explicitly taking the visual modality into
account in comparison to a model that estimates the
topic structure solely from the textual modality. Be-
yond word similarity and association, the approach
is promising for modeling word learning and cate-
gorization as well as a wide range of priming stud-
ies. Outwith cognitive science, we hope that some
of the work described here might be of relevance
to more applied tasks such as thesaurus acquisition,
word sense disambiguation, multimodal search, im-
age retrieval, and summarization.
Future improvements include developing a non-
parametric version that jointly learns how many vi-
sual terms and topics are optimal. Currently, the size
of the visual vocabulary and the number of topics
are parameters in the model, that must be tuned sep-
arately for different tasks and corpora. Another ex-
tension concerns the creation of visual terms. Our
model assumes that an image is a bag of words. The
assumption is convenient for modeling purposes, but
clearly false in the context of visual processing. Im-
age descriptors found closely to each other are likely
to represent the same object and should form one
term rather than several distinct ones (Wang and
Grimson, 2007). Taking the spatial structure among
visual words into account would yield better topics
and overall better semantic representations. Analo-
gously, we could represent documents by their syn-
tactic structure (Boyd-Graber and Blei, 2009).
References
Andrews, M., G. Vigliocco, and D. Vinson. 2009. In-
tegrating experiential and distributional data to learn
semantic representations. Psychological Review
116(3):463?498.
Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas,
D. Blei, andM. Jordan. 2002. Matching words and pic-
98
tures. Journal of Machine Learning Research 3:1107?
1135.
Blei, D. and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference. Toronto, ON, pages 127?134.
Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning Re-
search 3:993?1022.
Bornstein, M. H., L. R. Cote, S. Maital, K. Painter, S.-
Y. Park, and L. Pascual. 2004. Cross-linguistic analy-
sis of vocabulary in young children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development 75(4):1115?1139.
Boyd-Graber, J. and D. Blei. 2009. Syntactic topic
models. In Proceedings of the 22nd Conference on
Advances in Neural Information Processing Systems.
MIT, Press, Cambridge, MA, pages 185?192.
Cohen, J. and P. Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
Feng, S., V. Lavrenko, and R. Manmatha. 2004. Mul-
tiple Bernoulli relevance models for image and video
annotation. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002?1009.
Feng, Y. and M. Lapata. 2008. Automatic image annota-
tion using auxiliary text information. In Proceedings
of the ACL-08: HLT . Columbus, pages 272?280.
Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing
search in context: The concept revisited. ACM Trans-
actions on Information Systems 20(1):116?131.
Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum. 2007.
Topics in semantic representation. Psychological Re-
view 114(2):211?244.
Hofmann, T. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning
41(2):177?196.
Jones, S. S., L. B. Smith, and B. Landau. 1991. Ob-
ject properties and knowledge in early lexical learning.
Child Development (62):499?516.
Landau, B., L. Smith, and S. Jones. 1998. Object percep-
tion and object naming in early development. Trends
in Cognitive Science 27:19?24.
Landauer, T. and S. T. Dumais. 1997. A solution to
Plato?s problem: the latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review 104(2):211?240.
Lowe, D. 1999. Object recognition from local scale-
invariant features. In Proceedings of International
Conference on Computer Vision. IEEE Computer So-
ciety, pages 1150?1157.
Marton, Y., S. Mohammad, and P. Resnik. 2009. Estimat-
ing semantic distance using soft semantic constraints
in knowledge-source ? corpus hybrid models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing. Singapore, pages
775?783.
Mikolajczyk, K. and C. Schmid. 2003. A performance
evaluation of local descriptors. In Proceedings of the
9th International Conference on Computer Vision and
Pattern Recognition. Nice, France, volume 2, pages
257?263.
Monay, F. and D. Gatica-Perez. 2007. Modeling semantic
aspects for cross-media image indexing. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
29(10):1802?1817.
Nelson, D. L., C. L. McEvoy, and T.A. Schreiber. 1999.
The university of South Florida word association
norms.
Pan, J., H. Yang, P. Duygulu, and C. Faloutsos. 2004. Au-
tomatic image captioning. In Proceedings of the 2004
International Conference on Multimedia and Expo.
Taipei, pages 1987?1990.
Quinn, P., P. Eimas, and S. Rosenkrantz. 1993. Evidence
for representations of perceptually similar natural cate-
gories by 3-month and 4-month old infants. Perception
22:463?375.
Rogers, T. T., M. A. Lambon Ralph, P. Garrard,
S. Bozeat, J. L. McClelland, J. R. Hodges, and K. Pat-
terson. 2004. Structure and deterioration of semantic
memory: A neuropsychological and computational in-
vestigation. Psychological Review 111(1):205?235.
Roy, D. 2002. Learning words and syntax for a visual de-
scription task. Computer Speech and Language 16(3).
Shi, J. and J. Malik. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence 22(8):888?905.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. McNamara, S Dennis, and
W Kintsch, editors, A Handbook of Latent Semantic
Analysis, Psychology Press.
Wang, X. and E. Grimson. 2007. Spatial latent Dirichlet
allocation. In Proceedings of the 20th Conference on
Advances in Neural Information Processing Systems.
MI Press, Cambridge, MA, pages 1577?1584.
Washtell, J. and K. Markert. 2009. A comparison of win-
dowless and window-based computational association
measures as predictors of syntagmatic human associa-
tions. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing. Sin-
gapore, pages 628?637.
Weiss, S. M. and C. A. Kulikowski. 1991. Computer Sys-
tems that Learn: Classification and Prediction Meth-
ods from Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann, San Mateo,
CA.
Yu, C. 2005. The emergence of links between lexical
acquisition and object categorization: A computational
study. Connection Science 17(3):381?397.
99
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831?839,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Topic Models for Image Annotation and Text Illustration
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Image annotation, the task of automatically
generating description words for a picture,
is a key component in various image search
and retrieval applications. Creating image
databases for model development is, however,
costly and time consuming, since the key-
words must be hand-coded and the process
repeated for new collections. In this work
we exploit the vast resource of images and
documents available on the web for develop-
ing image annotation models without any hu-
man involvement. We describe a probabilistic
model based on the assumption that images
and their co-occurring textual data are gener-
ated by mixtures of latent topics. We show that
this model outperforms previously proposed
approaches when applied to image annotation
and the related task of text illustration despite
the noisy nature of our dataset.
1 Introduction
Recent years have witnessed the rapid growth of im-
age collections available for searching and browsing
over the Internet. Although image search engines are
still in their infancy, initial research suggests that the
deployed algorithms are not very accurate (Hawking
et al, 1999). Given a query, search engines retrieve
relevant pictures by analyzing the image caption (if
it exists), textual descriptions found adjacent to the
image, and other text-related factors such as the file
name of the image. However, since they do not an-
alyze the actual content of the images, search en-
gines cannot be used to retrieve pictures from unan-
notated collections. The ability to perform the an-
notation task automatically would be of significant
practical import for many image-based applications.
Besides search and retrieval, other examples include
browsing support (e.g., by clustering images into
groups that are visually and semantically coherent)
and story picturing (i.e., automatically suggesting
images to illustrate text).
Automatic image annotation is a popular task in
computer vision; a large number of approaches have
been proposed in the literature under many distinct
learning paradigms. These range from supervised
classification (Smeulders et al, 2000; Vailaya et al,
2001) to instantiations of the noisy-channel model
(Duygulu et al, 2002), to clustering (Barnard et al,
2002), and methods inspired by information retrieval
(Feng et al, 2004; Lavrenko et al, 2003). Despite
differences in application and formulation, all these
methods essentially attempt to learn the correlation
between image features and words from examples
of annotated images. The Corel database has been
extensively used as a testbed for the development
and evaluation of image annotation models. It is a
collection of stock photographs, divided into themes
(e.g., tigers, sunsets) each of which are associated
with keywords (e.g., sun, sea) that are considered
appropriate descriptors for all images belonging to
the same theme.
Unfortunately, the Corel database is not represen-
tative of the size or content of real-world image col-
lections. It has a small number of themes with many
closely related images which in turn share keyword
descriptions. It is therefore relatively easy to learn
the associations between images and keywords and
do well on annotation and retrieval tasks (Tang and
Lewis, 2007; Westerveld and de Vries, 2003). An
appealing alternative is the use of resources where
images and their annotations co-occur naturally. Ex-
amples include images found in news documents,
consumer photo collections, Wikipedia articles, il-
lustrated stories and so on. The key idea here is to
treat the words in the surrounding text as annota-
tions for the image. These annotations are undoubt-
831
edly noisy, but plenty and cost-free. Moreover, the
collateral text is often longer and more informative
in comparison to the few keywords reserved for each
image in Corel.
In this paper we propose a probabilistic image
annotation model that learns to automatically label
images under such noisy conditions. We use the
database created in Feng and Lapata (2008) which
consists of news articles, images, and their cap-
tions. Our model exploits the redundancy inherent
in this multimodal dataset by assuming that images
and their surrounding text are generated by a shared
set of latent variables or topics. Specifically, we de-
scribe documents and images by a common multi-
modal vocabulary consisting of textual words and
visual terms (visiterms). Due to polysemy and syn-
onymy many words in this vocabulary will refer to
the same underlying concept. Using Latent Dirichlet
Allocation (LDA, Blei and Jordan 2003), a proba-
bilistic model of text generation, we represent visual
and textual meaning jointly as a probability distribu-
tion over a set of topics. Our annotation model takes
these topic distributions into account while finding
the most likely keywords for an image and its asso-
ciated document. We also show how the model can
be straightforwardly modified to perform automatic
text illustration.1 The task is routinely performed by
news writers who often have to search large image
collections in order to find suitable pictures for their
text. Here, the model takes a document as input and
suggests images that match its content. Experimen-
tal results on both tasks bring improvements over
competitive models.
2 Related Work
A variety of learning methods have been applied to
the image annotation task. These generally fall un-
der two broad categories. Supervised methods de-
fine annotation as a classification task, e.g., by as-
suming a one-to-one correspondence between vo-
cabulary words and classes or by grouping several
words into a single class (see Chai and Hung 2008
for an overview). Unsupervised approaches attempt
to discover the underlying connections between vi-
sual features and words, typically by introducing
latent variables. Standard latent semantic analysis
(LSA) and its probabilistic variant (PLSA) have
been applied to this task (Hofmann, 2001; Monay
and Gatica-Perez, 2007; Pan et al, 2004). More so-
phisticated models estimate the joint distribution of
words and regional image features, whilst treating
1We use the terms ?text illustration? and ?story picturing?
interchangeably throughout the paper.
annotation as a problem of statistical inference in a
graphical model (Barnard et al, 2002; Blei and Jor-
dan, 2003; Wang et al, 2009).
Irrespectively of the underlying model or task at
hand, much work has focused how to best represent
the visual and textual modalities in order to exploit
their synergy. Several approaches attempt to render
images more word-like, by reducing the dimension-
ality of the image feature space (Bosch et al, 2008;
Fei-Fei and Perona, 2005) or by learning a single
representation for both visual and textual features
(Monay and Gatica-Perez, 2007; Zhao and Grosky,
2003).
Our own work approaches the image annotation
(and related story picturing) task from a slightly dif-
ferent angle. We train and test our model on im-
ages that contain implicit (and thus noisy) annota-
tions that have not been specifically created for our
task. On account of this, our model has access to
knowledge sources other than the image and its key-
words (i.e., the news article containing the image
we wish to annotate). In Feng and Lapata (2008)
we addressed this problem with a modified ver-
sion of the continuous relevance annotation model
(Lavrenko et al, 2003). Unlike other unsupervised
approaches where a set of latent variables is in-
troduced, each defining a joint distribution on the
space of keywords and image features, the relevance
model captures the joint probability of images and
annotated words directly, without requiring an inter-
mediate clustering stage (i.e., each annotated image
in the training set is treated as a latent variable). We
modified this model so as to exploit the informa-
tion present in the document in two ways. First, in
estimating the conditional probability of a keyword
given an image, we also considered its likelihood in
the collateral document. Secondly, we used an LDA
model (trained on the document collection) to prune
from the model?s output words that are not represen-
tative of the document?s topics.
The proposed approach differs from Feng
and Lapata (2008) in three important respects:
(a) document-based information is an integral part
of our model as we predict caption words given the
image and its accompanying document (b) LDA is
no longer a post-processing step; our model relies on
LDA to infer meaningful topics that capture the co-
occurrence of visual features and words; (c) beyond
image annotation, we show how the same frame-
work can be applied to story picturing (Joshi et al,
2006), a task which has received less attention in the
literature.
In terms of model structure, Blei and Jordan
832
(2003) andMonay and Gatica-Perez (2007) are clos-
est to our work. The first model, known as corre-
spondence LDA (CorrLDA), has been successfully
employed for modeling annotated images in the
Corel domain. CorrLDA also uses the notion of topic
to model the generation of images and their captions.
In this model, the visual modality drives the defini-
tion of the latent space to which the textual modality
is linked. The second model is based on PLSA and
learns a representation similar to ours consisting of
textual and visual features. It is also trained using
captioned images from the Corel database. We work
with noisier and larger datasets. Our model exploits
the captions accompanying the images as well as
their surrounding documents. As a result, we obtain
a similar number of textual and visual words; these
are often imbalanced in the Corel database, where
visual words are nearly 50 times more than textual
words. The different nature of our data dictates the
use of a model where the visual and textual modal-
ity are given equal importance in defining the latent
space.
3 Problem Formulation
In this section we give a brief description of the im-
age database we employ and also define the image
annotation and story picturing tasks we are attempt-
ing here. As mentioned earlier, we use the dataset
created in Feng and Lapata (2008).2 It contains
3,361 articles that have been downloaded from the
BBC News website3. Each article contains a news
image which in turn is associated with a caption.
The images are usually 203 pixels wide and 152 pix-
els high. The average caption length is 5.35 tokens,
and the average document length 133.85 tokens. The
captions vocabulary is 2,167 words and the docu-
ment vocabulary is 6,253. The vocabulary shared
between captions and documents is 2,056 words.
In contrast to the Corel database, this dataset con-
tains more complex images (with many objects) and
has a larger vocabulary (Corel?s vocabulary is ap-
proximately 300 words). An example of an abridged
database entry is shown in Figure 1.
Due to the non-standard nature of the database
we assume that the caption and news article de-
scribe the content of the image either directly or in-
directly. It also follows that we may not be able to
name all objects depicted in the image. Now, given
these constraints our goal is twofold. Firstly, we will
perform image annotation. Our model is trained on
2Available from http://homepages.inf.ed.ac.uk/
s0677528/data.html.
3http://news.bbc.co.uk/
A woman from East
Sussex who bought an
emu egg sold as a nov-
elty food item on a
farm on the Isle of
Wight has managed to
hatch it into a chick. Osborne the emu will grow
Gillian Stone, from to over 6ft tall
Bexhill, who breeds chickens, brought home three large
green emu eggs from a holiday and put them in an incu-
bator in her kitchen. Two turned out to be infertile, but
after 52 days little Osborne hatched
Table 1: Each entry in the BBC News database contains a
document, an image, and its caption (shown in boldface).
document-image-caption tuples like the one shown
in Table 1. During testing, we must infer the cap-
tion for an image. Secondly, we use the same dataset
to perform automatic text illustration. During train-
ing, the model has access to the same collection of
image-caption-document tuples. During testing, we
are given a document and must find the images that
best illustrate it.
4 Image and Document Representation
Words and images represent distinct modalities, im-
ages live in a continuous feature space, whereas
words are discrete. Yet, both modalities on some
level capture the same underlying concepts as they
are used to describe the same objects. A common
first step to all previous methods is the segmenta-
tion of the image into regions, using either a fixed-
grid layout or an image segmentation algorithm. Re-
gions are usually described by a standard set of fea-
tures including color, texture, and shape which are
treated as continuous vectors (e.g., Barnard et al
2002; Blei and Jordan 2003) or in quantized form
(e.g., Duygulu et al 2002). Through this process,
the low-level image features are made to resemble
word-like units.
Here, we go one step further and represent each
image by a bag of visual words, thereby convert-
ing visual features from a continuous onto a discrete
space. In order to do this we use the Scale Invariant
Feature Transform (SIFT) algorithm (Lowe, 1999).
The general idea behind the algorithm is to first
sample an image with the difference-of-Gaussians
point detector at different scales and locations. Im-
portantly, this detector is, to some extent, invariant to
translation, scale, rotation and illumination changes.
Each detected region is represented with a SIFT de-
scriptor which is a histogram of edge directions at
833
different locations. SIFT descriptors are quantized
using the K-means clustering algorithm to obtain a
discrete set of visual terms (visiterms) which form
our visual vocabulary Vocv. Each entry in this vo-
cabulary stands for a group of image regions which
are similar in content or appearance and assumed to
originate from similar objects. More formally, each
image I is expressed in a bag-of-words format vec-
tor, [wv1,wv2, ...,wvL ], where wvi = n only if I has n
regions labeled with vi.
Since visual and textual modalities have now the
same status?they are both represented as bags-of-
words?we can also represent any image-caption-
document tuple jointly as a mixed document dMix.
The underlying assumption is that the two modali-
ties express the same meaning which, as we explain
below, can be operationalized as a probability distri-
bution over a set of topics.
5 Modeling
Latent Dirichlet Allocation For ease of exposi-
tion, we first describe the basics of Latent Dirichlet
Allocation (LDA; Blei et al 2003), a probabilistic
model of text generation and then move on to dis-
cuss our models which make use of probabilities es-
timated by LDA.
LDA can be represented as a three level hierarchi-
cal Bayesian model. Given a corpus consisting of M
documents, Blei et al (2003) define the generative
process for a document d as follows:
1. Choose ?|?? Dir(?)
2. For n ? 1,2, ...,N :
(a) Choose topic zn|??Mult(?)
(b) Choose a word wn|zn,?1:K ?Mult(?zn)
The mixing proportion over topics ? is drawn from
a Dirichlet prior with parameters ? whose role is to
create a smoothed topic distribution. Once ? and ?
are sampled, then each document is generated ac-
cording to the topic proportions z1:K and word prob-
abilities over topics ?. The probability of a docu-
ment d in a corpus is defined as:
P(d|?,?)=
Z
?
P(?|?)
(
N
?
n=1
?
zk
P(zk|?)P(wn|zk,?)
)
d?
Computing the posterior distribution P(?,z|d,?,?)
of the hidden variables given a document is in-
tractable in general. However, a variety of approx-
imate inference algorithms have been proposed in
the literature including variational inference which
our model adopts (Blei et al, 2003). In this case,
training an LDA model on a document collection
will give two sets of parameters, the word proba-
bilities given topics, p(w|z1:K), and the topic pro-
portions given documents, P(z1:K |d). The latter are
document-specific, whereas the former represent the
set of topics learned from the document collection.
Given a trained model, it is possible to do infer-
ence on an unseen document dnew:
p(w|dnew) ?
K
?
k=1
P(w|zk)
?k
?Kj=1 ? j
(1)
where P(w|z1:K) are word probabilities over top-
ics z1:K estimated during model training, and ?1:K
are variational Dirichlet parameters obtained during
inference on the new document (and can be consid-
ered as the posteriors of topic proportions over doc-
uments).
Image Annotation In the standard image annota-
tion setting, a hypothetical model is given an image I
and a set of keywordsW , and must find the subsetWI
(WI ?W ) which appropriately describes image I:
W ?I = argmax
W
P(W |I) (2)
The keywords are usually assumed to be condition-
ally independent on each other, so Equation (2) sim-
plifies to:
W ?I = argmax
W
?
w?W
P(w|I) (3)
Each entry in our database is an image-caption-
document tuple (I,C,D). In this setting, we must
find the subset of keywords WI which appropriately
describe image I with the help of the accompanying
document D:
W ?I = argmax
Wt
P(Wt |I,D) (4)
Here, Wt denotes a set of textual words (we use the
subscript t to discriminate from the visual words
which are not part of the model?s output). We also
assume that the keywords are conditionally indepen-
dent of each other:
W ?I =argmax
Wt
P(Wt |I,D)=argmax
Wt
?
wt?Wt
P(wt |I,D) (5)
Since I and D are represented jointly as the con-
catenation of textual and visual terms, we may intu-
itively simplify the problem and use the mixed doc-
ument representation dMix directly in estimating the
conditional probabilities P(wt |I,D):
P(wt |I,D) ? P(wt |dMix) (6)
834
Substituting Equation (6) into (5) yields:
W ?I ? argmax
Wt
?
wt?Wt
P(wt |dMix) (7)
As mentioned earlier, we assume that the image and
its associated text are generated by a mixture of
latent topics which we infer using LDA. Specifi-
cally, we select the number of topics K and apply
the LDA algorithm to a corpus consisting of doc-
uments {dMix} in order to obtain the multimodal
word distributions over topics P(w|z1:K), and the
estimated posterior of the topic proportions over
documents P(z1:K |dMix). We infer the topic pro-
portions P(z1:K |dMixnew) on a new document-image
pair dMixnew approximately using Equations (1)
and (7):4
W ?I ? argmax
Wt
?
wt?Wt
P(wt |dMix) (8)
= argmax
Wt
?
wt?Wt
K
?
k=1
P(wt |zk)P(zk|dMix)
? argmax
Wt
?
wt?Wt
K
?
k=1
P(wt |zk)
?k
?Kj=1 ? j
where P(wt |zk) are obtained during training, and ?1:K
are inferred on the image-document test pair.
However, note that for an unseen image dI and ac-
companying document dD, the estimated topic pro-
portions are solely based on variational inference
which is an approximate algorithm. In order to ren-
der the model more robust, we smooth the topic pro-
portions P(z1:K |dMix) with probabilities based on a
single modality:
P?(z1:K |dMix) ? (9)
q1P(z1:K |dMix)+q2P(z1:K |dD)+q3P(z1:K |dI)
where P(z1:K |dD) and P(z1:K |dI) are inferred on dD
and dI , respectively, and q1, q2, q3 are smoothing
parameters (which we tune experimentally on held-
out data); q3 is a shorthand for (1?q1?q2).
In sum, calculating P(Wt |I,D) boils down to es-
timating the probabilities P(wt |dMix) according to
Equations (8) and (9) which we obtain using the
LDA model. We train LDA on the document col-
lection {dMix} and use inference to obtain the topic
distributions of unseen image-document pairs. In the
end, we obtain a ranked list of textual words wt , the
n-best of which are the annotations for image I.
4During training, the model has access to all three elements
(I,C,D), so the mixed document dMix is the concatenation of
the visual terms and words in the caption and document. Dur-
ing testing, the model is given an image and its accompanying
document, so dMix contains words based on I and D, but notC.
Text Illustration Previous text illustration models
are based on Corel-like databases with manual im-
age descriptions (Barnard and Forsyth, 2001; Blei
and Jordan, 2003) or instance-based learning using
complex learning schemes (Joshi et al, 2006). Here,
we present a relatively simple model, again under
the topic mixture framework.
Given a test document D and a candidate image
database I1...N with captionsC, we must find the im-
age or images which best describe the document.
We can simply compute the probability of each vi-
sual term in the vocabulary given D by marginaliz-
ing over the document topics z1:K :
P(wv|D) = ?
z1:K
P(wv|zk)P(zk|dD) (10)
where wv is a visual term and P(wv|zk) the probabil-
ity of wv given topic zk (as estimated on the training
set).
Equation (10) delivers a ranked list of visual terms
according to a given document. We could multiply
these probabilities together mirroring Equation (7),
however this is not reliable. In contrast to textual
words, for which we may infer whether they are
linguistically meaningful (e.g., by resorting to their
part of speech), there is no easy way of knowing
which visual words are important. Relying solely on
frequency is not reliable either, as frequent visiterms
may simply represent features common in all images
(e.g., most images have some white color). To avoid
a bias towards frequent but potentially irrelevant vi-
sual words, we output a fixed number of visual terms
and select the image with the highest overlap as the
correct illustration.
6 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the models pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe our fea-
tures, and present the baseline methods used for
comparison with our models.
Data We evaluated the image annotation and text
illustration tasks on the dataset described in Sec-
tion 3. Documents and captions were part-of-speech
tagged and lemmatized with Tree Tagger (Schmid,
1994). We excluded from the vocabulary low fre-
quency words (appearing fewer than five times)
and words other than nouns, verbs, and adjectives.
For the image annotation task we follow the data
split used in Feng and Lapata (2008). The training
set contains 2,881 image-caption-document tuples;
240 tuples are reserved for development and 240 for
835
testing. Our text illustration experiments, used 2,881
image-caption-document tuples for training. For the
purposes of simulating a real story picturing engine
environment, we created a large image pool of 450
image-caption pairs and tested on 300 of them.
Model Parameters For each image we ex-
tracted 150 (on average) SIFT features. These were
quantized into a discrete set of visual terms us-
ing K-means. We varied K from 100 to 2,000. We
trained the LDA topic model on the multimodal doc-
ument collection {dMix} and varied the number of
topics from 15 to 1,000. The hyperparameter ? was
initialized to 0.1; the ? probabilities were initial-
ized randomly. The maximum number of iterations
for variational inference was set to 1,000. We tuned
the smoothing parameters q1, q2, and q3 (see Equa-
tion (9)) on the development set. The best values
were q1 = 0.84, q2 = 0.12, and q3 = 0.04 (for both
tasks). As the number of visual terms and topics are
interrelated we exhaustively examined all possible
combinations on the development set. We obtained
best results on image annotation with 1,000 topics
and 750 visual terms. On text illustration the best pa-
rameters were 1,000 topics and 2,000 visual terms.
Baselines For the image annotation experiments,
we compared our model against the following base-
lines. Firstly, we trained a vanilla LDA model on
the document collection without taking the im-
ages into account. This model estimates P(wt |D) =
?Kk=1P(wt |zk)P(zk|D), the probability of textual
word wt given text document D. We assume that the
most probable words are the captions for the accom-
panying image. Our second baseline is the extended
relevance model (Feng and Lapata, 2008) that also
takes the document into account but crucially as-
sumes that the process of generating the images is
independent from the process of generating its key-
words.
We also compared our approach with two
closely related latent variable models (developed for
image-caption pairs), a PLSA-based model (Monay
and Gatica-Perez, 2007) and CorrLDA (Blei and
Jordan, 2003). The former model is an asymmet-
ric version of PLSA; it estimates the topic structure
solely from the textual modality and keeps it fixed
for the visual modality. The technique is similar to
folding-in (Hofmann, 2001), the standard PLSA pro-
cedure for inference in unseen documents and al-
lows modeling an image as a mixture of latent top-
ics that is defined only by one modality (in this
case the caption words). CorrLDA first generates
image regions from a Gaussian multinomial distri-
bution parametrized with Dirichlet priors. Then, for
each annotation word, it uniformly selects a region
from the image and generates a word according to
the topic used to generate that region. We optimized
the parameters for both models on the development
set. For CorrLDA, we followed the mean-field vari-
ational inference strategy proposed in Blei (2004).
The optimal number of topics for PLSA, was 200
(with 2000 visual terms) and for CorrLDA 50.
For the text illustration experiments, the pro-
posed model was compared with three baselines.
The first one is essentially word overlap. We se-
lect the image whose caption has the largest num-
ber of words in common with the test document.
The second one is a straightforward implementa-
tion of the vector space model (Salton and McGill,
1983) where documents and captions are repre-
sented by vectors whose components correspond to
term-document co-occurrences. We followed com-
mon practice in weighting terms by their tf-idf val-
ues, and used the cosine similarity measure to find
the image whose caption is most similar to the
test document. Our third baseline uses a text-based
LDA model to estimate document-caption similar-
ity probabilistically, through topic sharing. The im-
ages most relevant to a document are found by max-
imizing the conditional probability of the candi-
date captions C given the document dD: P(C|dD) =
?
wc?C
?Kk=1P(wc|zk)P(zk|dD) (where wc are the cap-
tion words, P(wc|zk) the conditional distribution of
each wc given a topic zk, and P(zk|dD) the condi-
tional distribution of zk given dD, the document we
wish to illustrate.
Evaluation In the image annotation task we
follow the evaluation methodology proposed in
Duygulu et al (2002). We are given an un-annotated
image I and asked to automatically produce the
n-best keywords. For all models discussed here, we
report results with the top 10 annotation words us-
ing precision, recall and F1. In the text illustration
task, we are given a test document d and a pool
of candidate images I1...N with captions C1...N . The
model is expected to find an image from the can-
didate pool that matches the test document. We use
equation (10) to output a ranked list of MI visual
terms. The image having the highest overlap with
the top 30 visual terms is selected as the illustration
for the test document. All illustration models were
evaluated using top 1 accuracy, which is the percent-
age of successfully matched image-document pairs
in the test set.
836
Model Top 10
Precision Recall F1
CorrLDA 5.33 11.80 7.36
TxtLDA 7.30 16.90 10.20
PLSA 10.26 22.60 14.12
ExtRel 14.70 27.90 19.80
MixLDA 16.30 33.10 21.60
Table 2: Automatic image annotation results.
7 Results
Our results on the image annotation task are summa-
rized in Table 2. Here, we compare our own model
(MixLDA) which is trained on both visual and tex-
tual information against an LDA model based solely
on textual information (TxtLDA), an extended ver-
sion of the Continuous Relevance model that also
exploits collateral document information (ExtRel;
Feng and Lapata 2008), a PLSA model that prior-
itizes the textual over visual modality (Monay and
Gatica-Perez, 2007), and CorrLDA (Blei and Jor-
dan, 2003) which does the opposite. We performed
significance testing on F1 using stratified shuffling
(Noreen, 1989), an instance of assumption-free ap-
proximative randomization testing.
Let us first discuss the performance of TxtLDA
and MixLDA. These two models are closely related
? they both rely on the probabilities P(wt |d) to
generate the image keywords ? save one important
difference. MixLDA uses a concatenated represen-
tation of words and visual features assuming that
the two modalities have equal importance in defin-
ing the latent space, whereas TxtLDA considers only
the textual modality. Our results show that MixLDA
outperforms TxtLDA in terms of precision (by 9%),
recall (by 16.2%). MixLDA improves F1 by 11.4%,
and the difference is significant (p < 0.01).
PLSA significantly (p < 0.01) improves upon
TxtLDA. The key difference is the visual informa-
tion which makes up (to a certain extent) for the
lack of richer textual data. Interestingly, CorrLDA
performs significantly (p < 0.01) worse than both
PLSA and TxtLDA. Recall that in CorrLDA word
topic assignments are drawn from the image regions
which are in turn drawn from a Gaussian distribu-
tion. Although this modeling choice delivers bet-
ter results on the simpler Corel dataset, it does not
seem able to capture the characteristics of our im-
ages which are noisier and more complex. More-
over, CorrLDA assumes that annotation keywords
must correspond to image regions. This assumption
is too restrictive in our setting where a single key-
TxtLDA
Afghanistan, Taleban,
soldier, British, zone,
kill, force, Microsoft,
troop, NATO
police, Burgess, time,
letter, crash, case,
death, operation,
investigation, jail
MixLDA
Afghanistan, troop,
Blair, British, NATO,
helicopter, soldier,
support, operation,
commander
Diana, police, case,
crash, Princess, re-
port, death, inquest,
Paris, Burgess
Caption
Troops need more
Chinook helicopters to
carry out operations
Princess Diana died in
a car crash in Paris in
1997
Figure 1: Annotations generated by the TxtLDA and
MixLDA models. Words in bold face indicate exact
matches. The original captions are in the last row.
word may refer to many objects or persons in an
image (e.g.,the word badminton is used to collec-
tively describe an image depicting players, shuttle-
cocks, and rackets). As an aside, it is interesting to
note, that neither PLSA nor CorrLDA achieve better
results, when modified to take the captions and asso-
ciated documents into account. PLSA scores are in
the same ballpark (see Table 2), whereas CorrLDA
performs worse, F1 decreases by 2%.
The extended relevance model improves consid-
erably upon TxtLDA, CorrLDA, and PLSA but is
significantly worse (p < 0.01) than MixLDA. On
the surface, MixLDA seems similar to ExtRel, both
models take advantage of visual and textual informa-
tion. ExtRel smooths the conditional probability of a
word given an image with the conditional probabil-
ity of the same word given the document and uses an
LDA model (trained on the document collection) to
remove non-topical keywords from the model?s out-
put. MixLDA is conceptually simpler, LDA is the
actual model rather than a post-processing step, and
exploits the synergy between visual and textual in-
formation more directly. Topics are created based on
both modalities which are treated on an equal foot-
ing. Compared to ExtRel, MixLDA improves pre-
cision by 1.6%, recall by 5.2% and the overall F1
by 1.8%.
Figure 1 illustrates examples of annotations gen-
837
Model Accuracy
TxtLDA 31.0
Overlap 31.3
VectorSpace 38.7
MixLDA 57.3
Table 3: Text Illustration results.
erated by TxtLDA and MixLDA for two images. For
comparison, we also show the goldstandard image
captions. Note that TxtLDA fails to generate any
words relating to the objects shown in the image.
It finds primarily words relating to the topics of the
associated articles such as troops and crash. On the
contrary, MixLDA is more successful at identifying
the depicted objects, since it takes visual informa-
tion into account.
Table 3 presents our results on the automatic
text illustration task. Here, we compare our mul-
timodal topic model (MixLDA) against three text-
based baselines, namely word overlap (Overlap)
a standard vector space model (VectorSpace), and
TxtLDA. We examined whether differences in per-
formance are statistically significant using a ?2 test.
As can be seen, MixLDA significantly (p < 0.01)
outperforms these models by a wide margin (accu-
racy is 57.3% for MixLDA vs. 31.0% for TxtLDA,
38.7% for the vector space model, and 31.3% for
word overlap). These results are encouraging given
the simplicity of our model. They also indicate that
substantial mileage can be gained by taking into ac-
count the visual modality.
Figure 2 shows the three best illustrations found
by MixLDA and VectorSpace (incidentally, Overlap
delivered the same ranking as VectorSpace). The im-
ages are presented in ranked order, i.e., the first im-
age was given a higher score than the second one,
etc. The document discusses Smart 1 Probe, a lunar
satellite about to end its mission by crashing onto
the moon?s surface. MixLDA identifies an image de-
picting this satellite. The second best picture is also
relevant, it resembles the moon?s surface. The Vec-
torSpace model does not find any related images, the
first one is a DNA image, the second one depicts
policemen at a crime scene and the third one Ben
Nevis, the highest mountain in the British Isles.
8 Conclusions
In this paper we have presented a probabilistic ap-
proach for automatic image annotation and text il-
lustration. Our model postulates that visual terms
and words are generated by common (hidden) top-
V
ec
to
rS
pa
ce
M
ix
L
D
A
Europe?s lunar satellite, the Smart 1 probe, is
about to end its mission by crashing onto the
Moon?s surface. It will be a spectacular end
for the robot which has spent the past 16 months
testing innovative and miniaturized space tech-
nologies. Smart 1 has also produced detailed
maps of the Moon?s chemical make-up.
Figure 2: Top-3 illustrations for document in bottom row.
ics and is trained on a dataset consisting of images
available on the Internet, their captions, and associ-
ated news articles. The annotations are implicit and
the dataset is representative of the scale, diversity,
and difficulty of real-world image collections. Over-
all, our results show that the model is robust to the
noise inherent in such data. It improves upon com-
petitive approaches that prioritize one modality over
the other or exploit them indirectly. We also show
that with minor modifications the model can be used
to automatically illustrate a document with an appro-
priate image. Our method shows promise for multi-
modal search and image retrieval and other applica-
tions which have been traditionally text-based. An
interesting future direction involves generating ac-
tual sentence descriptions rather than isolated key-
words. Another relevant application is summariza-
tion. Our results suggest that taking visual informa-
tion into account could potentially create more fo-
cused and accurate summaries.
The model presented here could be further im-
proved in two ways. Firstly, we could allow an in-
finite number of topics and develop a nonparamet-
ric version that learns how many topics are optimal.
Secondly, our model is based on word unigrams, and
in this sense takes very little linguistic knowledge
into account. Recent developments in topic model-
ing could potentially rectify this, e.g., by assuming
that each word is generated by a distribution that
combines document-specific topics and parse-tree-
specific syntactic transitions (Boyd-Graber and Blei,
2009).
838
References
Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas,
D. Blei, andM. Jordan. 2002. Matching words and pic-
tures. Journal of Machine Learning Research 3:1107?
1135.
Barnard, K. and D. Forsyth. 2001. Learning the semantics
of words and pictures. In Proceedings of the 8th Inter-
national Conference on Computer Vision. Vancouver,
BC, pages 408?415.
Blei, D. 2004. Probabilistic Models of Text and Images.
Ph.D. thesis, U.C. Berkeley, Division of Computer Sci-
ence.
Blei, D. and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference. Toronto, ON, pages 127?134.
Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research
3:993?1022.
Bosch, A., A. Zisserman, and X. Munoz. 2008. Scene
classification using a hybrid generative/discriminative
approach. IEEE Transactions on Pattern Analysis and
Machine Intelligence 30(4):712?727.
Boyd-Graber, J. and D. Blei. 2009. Syntactic topic
models. In Proceedings of the 22nd Conference on
Advances in Neural Information Processing Systems.
MIT, Press, Cambridge, MA, pages 185?192.
Chai, C. and C. Hung. 2008. Automatically annotating
images with keywords: A review of image annotation
systems. Recent Patents on Computer Science 1:55?
68.
Duygulu, P., K. Barnard, J. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary. In
Proceedings of the 7th European Conference on Com-
puter Vision. Copenhagen, Danemark, pages 97?112.
Fei-Fei, L. and P. Perona. 2005. A Bayesian hierarchi-
cal model for learning natural scene categories. In
Proceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition.
IEEE Computer Society Washington, DC, volume 2,
pages 524?531.
Feng, S., V. Lavrenko, and R. Manmatha. 2004. Mul-
tiple Bernoulli relevance models for image and video
annotation. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002?1009.
Feng, Y. and M. Lapata. 2008. Automatic image annota-
tion using auxiliary text information. In Proceedings
of ACL-08: HLT . Columbus, OH, pages 272?280.
Hawking, D., N. Craswell, P. Thistlewaite, and D. Har-
man. 1999. Results and challenges in web search eval-
uation. Computer Networks 31(11):1321?1330.
Hofmann, T. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning
41(2):177?196.
Joshi, D., J.Z. Wang, and J. Li. 2006. The story picturing
engine?a system for automatic text illustration. ACM
Transactions on Multimedia Computing, Communica-
tions, and Applications 2(1):68?89.
Lavrenko, V., R. Manmatha, and J. Jeon. 2003. A model
for learning the semantics of pictures. In Proceedings
of the 17th Conference on Advances in Neural Infor-
mation Processing Systems. MIT, Press, Cambridge,
MA.
Lowe, D. 1999. Object recognition from local scale-
invariant features. In Proceedings of International
Conference on Computer Vision. IEEE Computer So-
ciety, pages 1150?1157.
Monay, F. and D. Gatica-Perez. 2007. Modeling semantic
aspects for cross-media image indexing. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
29(10):1802?1817.
Pan, J., H. Yang, P. Duygulu, and C. Faloutsos. 2004. Au-
tomatic image captioning. In Proceedings of the 2004
International Conference on Multimedia and Expo.
Taipei, pages 1987?1990.
Salton, G. and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill, New York.
Schmid, H. 1994. Probabilistic part-of-speech tagging us-
ing decision trees. In Proceedings of the International
Conference on New Methods in Language Processing.
Manchester, UK, pages 44?49.
Smeulders, A. W., M. Worring, S. Santini, A. Gupta, and
R. Jain. 2000. Content-based image retrieval at the
end of the early years. IEEE Transactions on Pattern
Analysis and Machine Intelligence 22(12):1349?1380.
Tang, J. and P. H. Lewis. 2007. A study of quality is-
sues for image auto-annotation with the Corel data-set.
IEEE Transactions on Circuits and Systems for Video
Technology 17(3):384?389.
Vailaya, A., M. Figueiredo, A. Jain, and H. Zhang. 2001.
Image classification for content-based indexing. IEEE
Transactions on Image Processing 10:117?130.
Wang, C., D. Blei, and L. Fei-Fei. 2009. Simultaneous
image classification and annotation. In Proceedings of
CVPR. Miami, FL, pages 1903?1910.
Westerveld, T. and A. P. de Vries. 2003. Experimental
evaluation of a generative probabilistic image retrieval
model on ?easy? data. In Proceedings of the SIGIR
Multimedia Information Retrieval Workshop. Toronto,
ON.
Zhao, R. and W. I. Grosky. 2003. Video shot detection
using color anglogram and latent semantic indexing:
From contents to semantics. In B. Furht and O. Mar-
ques, editors, Handbook of Video Databases: Design
and Applications, CRC Press, pages 371?392.
839
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 939?947,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Induction of Semantic Roles
Joel Lang and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Datasets annotated with semantic roles are
an important prerequisite to developing high-
performance role labeling systems. Unfortu-
nately, the reliance on manual annotations,
which are both difficult and highly expen-
sive to produce, presents a major obstacle to
the widespread application of these systems
across different languages and text genres. In
this paper we describe a method for induc-
ing the semantic roles of verbal arguments di-
rectly from unannotated text. We formulate
the role induction problem as one of detecting
alternations and finding a canonical syntactic
form for them. Both steps are implemented in
a novel probabilistic model, a latent-variable
variant of the logistic classifier. Our method
increases the purity of the induced role clus-
ters by a wide margin over a strong baseline.
1 Introduction
Semantic role labeling (SRL, Gildea and Jurafsky
2002) is the task of automatically classifying the ar-
guments of a predicate with roles such as Agent, Pa-
tient or Location. These labels capture aspects of the
semantics of the relationship between the predicate
and the argument while abstracting over surface syn-
tactic configurations. SRL has received much atten-
tion in recent years (Surdeanu et al, 2008; Ma`rquez
et al, 2008), partly because of its potential to im-
prove applications that require broad coverage se-
mantic processing. Examples include information
extraction (Surdeanu et al, 2003), question answer-
ing (Shen and Lapata, 2007), summarization (Melli
et al, 2005), and machine translation (Wu and Fung,
2009).
Given sentences (1-a) and (1-b) as input, an SRL
system would have to identify the verb predicate
(shown in boldface), its arguments (Michael and
sandwich) and label them with semantic roles (Agent
and Patient).
(1) a. [Michael]Agent eats [a sandwich]Patient.
b. [A sandwich]Patient is eaten [by
Michael]Agent.
Here, sentence (1-b) is an alternation of (1-a).
The verbal arguments bear the same semantic role,
even though they appear in different syntactic posi-
tions: sandwich is the object of eat in sentence (1-a)
and its subject in (1-b) but it is in both instances as-
signed the role Patient. The example illustrates the
passive alternation. The latter is merely one type
of alternation, many others exist (Levin, 1993), and
their computational treatment is one of the main
challenges faced by semantic role labelers.
Most SRL systems to date conceptualize semantic
role labeling as a supervised learning problem and
rely on role-annotated data for model training. Prop-
Bank (Palmer et al, 2005) has been widely used for
the development of semantic role labelers as well as
FrameNet (Fillmore et al, 2003). Under the Prop-
Bank annotation framework (which we will assume
throughout this paper) each predicate is associated
with a set of core roles (named A0, A1, A2, and so
on) whose interpretations are specific to that pred-
icate1 and a set of adjunct roles (e.g., Location or
Time) whose interpretation is common across predi-
cates. In addition to large amounts of role-annotated
data, SRL systems often make use of a parser to ob-
tain syntactic analyses which subsequently serve as
input to a pipeline of components concerned with
1More precisely, A0 and A1 have a common interpreta-
tion across predicates as proto-agent and proto-patient (Dowty,
1991).
939
identifying predicates and their arguments (argu-
ment identification) and labeling them with semantic
roles (argument classification).
Supervised SRL methods deliver reasonably good
performance (a system will recall around 81% of the
arguments correctly and 95% of those will be as-
signed a correct semantic role; see Ma`rquez et al
2008 for details). Unfortunately, the reliance on la-
beled training data, which is both difficult and highly
expensive to produce, presents a major obstacle
to the widespread application of semantic role la-
beling across different languages and text genres.
And although corpora with semantic role annota-
tions exist nowadays in other languages (e.g., Ger-
man, Spanish, Catalan, Chinese, Korean), they tend
to be smaller than their English equivalents and of
limited value for modeling purposes. Moreover, the
performance of supervised systems degrades consid-
erably (by 10%) on out-of-domain data even within
English, a language for which two major annotated
corpora are available. Interestingly, Pradhan et al
(2008) find that the main reason for this are errors
in the assignment of semantic roles, rather than the
identification of argument boundaries. Therefore, a
mechanism for inducing the semantic roles observed
in the data without additional manual effort would
enhance the robustness of existing SRL systems and
enable their portability to languages for which anno-
tations are unavailable or sparse.
In this paper we describe an unsupervised ap-
proach to argument classification or role induction2
that does not make use of role-annotated data. Role
induction can be naturally formalized as a cluster-
ing problem where argument instances are assigned
to clusters. Ideally, each cluster should contain argu-
ments corresponding to a specific semantic role and
each role should correspond to exactly one cluster. A
key insight in our approach is that many predicates
are associated with a standard linking. A linking is
a deterministic mapping from semantic roles onto
syntactic functions such as subject, or object. Most
predicates will exhibit a standard linking, i.e., they
will be predominantly used with a specific map-
ping. Alternations occur when a different linking
is used. In sentence (1-a) the predicate eat is used
with its standard linking (the Agent role is mapped
onto the subject function and the Patient onto the
object), whereas in sentence (1-b) eat is used with
2We use the term role induction rather than argument clas-
sification for the unsupervised setting.
its passive-linking (the Patient is mapped onto sub-
ject and the Agent appears as a prepositional phrase).
When faced with such alternations, we will attempt
to determine for each argument the syntactic func-
tion it would have had, had the standard linking been
used. We will refer to this function as the arguments?
canonical function, and use the term canonicaliza-
tion to describe the process of inferring these canon-
ical functions in the case of alternations. So, in sen-
tence (1-b) the canonical functions of the arguments
by Michael and sandwich are subject and object, re-
spectively.
Since linkings are injective, i.e., no two seman-
tic roles are mapped onto the same syntactic func-
tion, the canonical function of an argument uniquely
references a specific semantic role. We define a
probabilistic model for detecting non-standard link-
ings and for canonicalization. The model specifies a
distribution p(F) over the possible canonical func-
tions F of an argument. We present an extension of
the logistic classifier with the addition of latent vari-
ables which crucially allow to learn generalizations
over varying syntactic configurations. Rather than
using manually labeled data, we train our model on
observed syntactic functions which can be obtained
automatically from a parser. These training instances
are admittedly noisy but readily available and as
we show experimentally a useful data source for
inducing semantic roles. Application of the model
to a benchmark dataset yields improvements over a
strong baseline.
2 Related Work
Much previous work on SRL relies on supervised
learning methods for both argument identification
and argument classification (see Ma`rquez et al 2008
for an overview). Most systems use manually anno-
tated resources to train separate classifiers for dif-
ferent SRL subtasks (e.g., Surdeanu et al 2008).
A few approaches adopt semi-supervised learning
methods. The idea here is to to alleviate the data
requirements for semantic role labeling by extend-
ing existing resources through the use of unlabeled
data. Swier and Stevenson (2004) induce role la-
bels with a bootstrapping scheme in which the set
of labeled instances is iteratively expanded using
a classifier trained on previously labeled instances.
Pado? and Lapata (2009) project role-semantic anno-
tations from an annotated corpus in one language
onto an unannotated corpus in another language.
And Fu?rstenau and Lapata (2009) propose a method
940
in which annotations are projected from a source
corpus onto a target corpus, however within the
same language.
Unsupervised approaches to SRL have been few
and far between. Early work on lexicon acquisition
focuses on identifying verbal alternations rather than
their linkings. This is often done in conjunction with
hand-crafted resources such as a taxonomy of possi-
ble alternations (McCarthy and Korhonen, 1998) or
WordNet (McCarthy, 2002). Lapata (1999) proposes
a corpus-based method that is less reliant on taxo-
nomic resources, however focuses only on two spe-
cific verb alternations. Other work attempts to clus-
ter verbs into semantic classes (e.g., Levin 1993) on
the basis of their alternation behavior (Schulte im
Walde and Brew, 2002).
More recently, Abend et al (2009) propose an
unsupervised algorithm for argument identifica-
tion that relies only on part-of-speech annotations,
whereas Grenager and Manning (2006) focus on
role induction which they formalize as probabilis-
tic inference in a Bayesian network. Their model
defines a joint probability distribution over the par-
ticular linking used together with a verb instance
and for each verbal argument, its lemma, syntactic
function as well as semantic role. Parameters in this
model are estimated using the EM algorithm as the
training instances include latent variables, namely
the semantic roles and linkings. To make inference
tractable they limit the set of linkings to a small
number and do not distinguish between different
types of adjuncts. Our own work also focuses on
inducing the semantic roles and the linkings used
by each verb. Our approach is conceptually sim-
pler and computationally more tractable. Our model
is a straightforward extension of the logistic classi-
fier with latent variables applied to all roles not just
coarse ones.
3 Problem Formulation
We treat role induction as a clustering problem.
The goal is to assign argument instances (i.e., spe-
cific arguments, occurring in an input sentence) into
clusters such that each cluster contains instances
with the same semantic role, and each semantic
role is found in exactly one cluster. As we as-
sume PropBank-style roles (Palmer et al, 2005),
our model will allocate a separate set of clusters for
each predicate and assign the arguments of a specific
predicate to one of the clusters associated with it.
As mentioned earlier (Section 1) a linking is a de-
A0 A1 TMP MNR
SBJ 54514 19684 15 7
OBJ 3359 51730 93 54
ADV 162 3506 976 2308
TMP 5 60 15167 22
PMOD 2466 4860 142 62
OPRD 37 5554 1 36
LOC 17 145 43 157
DIR 0 178 15 6
MNR 5 48 13 3312
PRP 9 50 11 6
LGS 2168 36 2 2
PRD 413 830 31 38
NMOD 422 388 25 59
EXT 0 20 2 12
DEP 18 150 25 65
SUB 3 84 4 2
CONJ 198 331 22 8
ROOT 62 147 84 2
64517 88616 16803 6404
Table 1: Contingency table between syntactic func-
tion and semantic role for two core roles Agent (A0)
and Patient (A1) and two adjunct roles, Time (TMP)
and Manner (MNR). Only syntactic functions occur-
ring more than 1000 times are listed. Counts were
obtained from the CoNLL 2008 training dataset us-
ing gold standard parses (the marginals in the bottom
row also include counts of unlisted co-occurrences).
terministic mapping from semantic roles onto syn-
tactic functions. Table 1 shows how frequently in-
dividual semantic roles map onto certain syntactic
functions. The frequencies were obtained from the
CoNLL 2008 dataset (see Surdeanu et al 2008 for
details) and constitute an aggregate across predi-
cates. As can be seen, there is a clear tendency for
a semantic role to be mapped onto a single syntac-
tic function. This is true across predicates and even
more so for individual predicates. For example, A0
is commonly mapped onto subject (SBJ), whereas
A1 is often realized as object (OBJ). There are two
reasons for this. Firstly, a predicate is often asso-
ciated with a standard linking which is most fre-
quently used. Secondly, the alternate linkings of a
given predicate often differ from the standard link-
ing only with respect to a few roles. Importantly, we
do not assume that a single standard linking is valid
941
for all predicates. Rather, each predicate has its own
standard linking. For example, in the standard link-
ing for the predicate fall, A1 is mapped onto subject
position, whereas in the standarad linking for eat,
A1 is mapped onto object position.
When an argument is attested with a non-standard
linking, we wish to determine the syntactic func-
tion it would have had if the standard linking had
been used. This canonical function of the argument
uniquely references a specific semantic role, i.e., the
semantic role that is mapped onto the function under
the standard linking. We can now specify an indi-
rect method for partitioning argument instances into
clusters:
1. Detect arguments that are linked in a non-
standard way (detection).
2. Determine the canonical function of these argu-
ments (canonicalization). For arguments with
standard linkings, their syntactic function cor-
responds directly to the canonical function.
3. Assign arguments to a cluster according to their
canonical function.
We distinguish between detecting non-standard link-
ings and canonicalization because in principle two
separate models could be used. In our probabilis-
tic formulation, both detection and canonicaliza-
tion rely on an estimate of the probability distribu-
tion p(F) over the canonical function F of an ar-
gument. When the most likely canonical function
differs from the observed syntactic function this in-
dicates that a non-standard linking has been used
(detection). This most likely canonical function can
be taken as the canonical function of the argument
(canonicalization).
Arguments are assigned to clusters based on
their inferred canonical function. Since we assume
predicate-specific roles, we induce a separate clus-
ter for each predicate. Given K clusters, we use the
following scheme for determining the mapping from
functions to clusters:
1. Order the functions by occurrence frequency.
2. For each of the K ? 1 most frequent functions
allocate a separate cluster.
3. Assign all remaining functions to the K-th clus-
ter.
4 Model
The detection of non-standard linkings and canon-
icalization both rely on a probabilistic model p(F)
which specifies the distribution over the canonical
functions F of an argument. As is the case with most
SRL approaches, we assume to be given a syntactic
parse of the sentence from which we can extract la-
beled dependencies, corresponding to the syntactic
functions of arguments. To train the model we ex-
ploit the fact that most observed syntactic functions
will correspond to canonical functions. This enables
us to use the parser?s output for training even though
it does not contain semantic role annotations.
Critically, the features used to determine the
canonical function must be restricted so that they
give no cues about possible alternations. If they
would, the model could learn to predict alternations,
and therefore produce output closer to the observed
syntactic rather than canonical function of an argu-
ment. To avoid this pitfall we only use features at
or below the node representing the argument head in
the parse tree apart from the predicate lemma (see
Section 5 for details).
Given these local argument features, a simple so-
lution would be to use a standard classifier such as
the logistic classifier (Berger et al, 1996) to learn
the canonical function of arguments. However, this
is problematic, because in our setting the training
and application of the classifier happen on the same
dataset. The model will over-adapt to the observed
targets (i.e., the syntactic functions) and fail to learn
appropriate canonical functions. Lexical sparsity is
a contributing factor: the parameters associated with
sparse lexical features will be unavoidably adjusted
so that they are highly indicative of the syntactic
function they occur with.
One way to improve generalization is to incor-
porate a layer of latent variables into the logistic
classifier, which mediates between inputs (features
defined over parse trees) and target (the canonical
function). As a result, inputs and target are no longer
directly connected and the information conveyed by
the features about the target must be transferred via
the latent layer. The model is shown in plate notation
in Figure 1a. Here, Xi represents the observed in-
put features, Y the observed target, and Z j the latent
variables. The number of latent variables influences
the generalization properties of the model. With too
few latent variables too little information will be
transferred via the latent variables, whereas with too
many latent variables generalization will degrade.
The model defines a probability distribution over
the target variable Y and the latent variables Z, con-
942
YZ j
M
Xi
N
Y
Z1 Z2
X2X1 X3
(a) (b)
Figure 1: The logistic classifier with latent variables
(shaded nodes) illustrated as a graphical model using
(a) plate notation and (b) in unrolled form for M = 2
and N = 3.
ditional on the input variables X :
p(y,z|x,?) =
1
P(x,?)
exp
(
?
k
?k?k(x,y,z)
)
(1)
We will assume that the latent variables Zi are bi-
nary. Each of the feature functions ?k is associated
with a parameter ?k. The partition function normal-
izes the distribution:
P(x,?) =?
y
?
z
exp
(
?
k
?k?k(x,y,z)
)
(2)
Note that this model is a special case of a conditional
random field with latent variables (Sutton and Mc-
Callum, 2007) and resembles a neural network with
one hidden layer (Bishop, 2006).
Let (c,d) denote a training set of inputs and corre-
sponding targets. The maximum-likelihood parame-
ters can then be obtained by finding the ? maximiz-
ing:
l(?) = log p(d|c)
= ?i log?z p(di,z|ci)
= ?i log
?z exp(?k ?k?k(ci,di,z))
P(ci,?)
(3)
And the gradient is given by:
(?l)k = ???k l(?)
= ?i?z p(z|di,ci)?k(ci,di,z)
??i?y,z p(y,z|ci)?k(ci,y,z)
(4)
where the first term is the conditional expected fea-
ture count and the second term is the expected fea-
ture count.
Thus far, we have written the equations in a
generic form for arbitrary conditional random fields
with latent variables (Sutton and McCallum, 2007).
In our model we have two types of pairwise suffi-
cient statistics: ?(x,z) : R?{0,1}? R, between a
single input variable and a single latent variable, and
?(y,z) : Y ?{0,1}? R, between the target and a la-
tent variable. Then, we can more specifically write
the gradient component of a parameter associated
with a sufficient statistic ?(x j,zk) as:
?
i
?
zk
p(zk|di,ci)?(ci, j,zk)??
i
?
zk
p(zk|ci)?(ci, j,zk) (5)
And the gradient component of a parameter associ-
ated with a sufficient statistic ?(y,zk) is:
?
i
?
zk
p(zk|di,ci)?(di,zk)??
i
?
y,zk
p(y,zk|ci)?(y,zk) (6)
To obtain maximum-a-posteriori parameter esti-
mates we regularize the equations. Like for the stan-
dard logistic classifier this results in an additional
term of the target function and each component
of the gradient (see Sutton and McCallum 2007).
Computing the gradient requires computation of the
marginals which can be performed efficiently using
belief propagation (Yedidia et al, 2003). Note that
due to the fact, that there are no edges between the
latent variables, the inference graph is tree structured
and therefore inference yields exact results. We use
a stochastic gradient optimization method (Bottou,
2004) to optimize the target. Optimization is likely
to result in a local maximum, as the likelihood func-
tion is not convex due to the latent variables.
5 Experimental Design
In this section we discuss the experimental design
for assessing the performance of the model de-
scribed above. We give details on the dataset, fea-
tures and evaluation measures employed and present
the baseline methods used for comparison with our
model.
943
Figure 2: Dependency graph (simplified) of a sample sentence from the corpus.
Data Our experiments were carried out on the
CoNLL 2008 (Surdeanu et al, 2008) training dataset
which contains both verbal and nominal predicates.
However, we focused solely on verbal predicates,
following most previous work on semantic role la-
beling (Ma`rquez et al, 2008). The CoNLL dataset
is taken form the Wall Street Journal portion of
the Penn Treebank corpus (Marcus et al, 1993).
Role semantic annotations are based on PropBank
and have been converted from a constituent-based
to a dependency-based representation (see Surdeanu
et al 2008). For each argument of a predicate only
the head word is annotated with the correspond-
ing semantic role, rather than the whole constituent.
In this paper we are only concerned with role in-
duction, not argument identification. Therefore, we
identify the arguments of each predicate by consult-
ing the gold standard.
The CoNLL dataset alo supplies an automatic
dependency parse of each input sentence obtained
from the MaltParser (Nivre et al, 2007). The target
and features used in our model are extracted from
these parses. Syntactic functions occurring more
than 1,000 times in the gold standard are shown
in Table 1 (for more details we refer the interested
reader to Surdeanu et al 2008). Syntactic func-
tions were further modified to include prepositions if
specified, resulting in a set of functions with which
arguments can be distinguished more precisely. This
was often the case with functions such as ADV,
TMP, LOC, etc. Also, instead of using the prepo-
sition itself as the argument head, we used the ac-
tual content word modifying the preposition. We
made no attempt to treat split arguments, namely in-
stances where the semantic argument of a predicate
has several syntactic heads. These are infrequent in
the dataset, they make up for less than 1% of all ar-
guments.
Model Setup The specific instantiation of the
model used in our experiments has 10 latent vari-
ables. With 10 binary latent variables we can en-
code 1024 different target values, which seems rea-
sonable for our set of syntactic functions which
comprises around 350 elements.
Features representing argument instances were
extracted from dependency parses like the one
shown in Figure 2. We used a relatively small feature
set consisting of: the predicate lemma, the argument
lemma, the argument part-of-speech, the preposition
involved in dependency between predicate and argu-
ment (if there is one), the lemma of left-most/right-
most child of the argument, the part-of-speech of
left-most/right-most child of argument, and a key
formed by concatenating all syntactic functions of
the argument?s children. The features for the argu-
ment maker in Figure 2 are [sell, maker, NN, ?, the,
auto, DT, NN, NMOD+NMOD]. The target for this
instance (and observed syntactic function) is SBJ.
Evaluation Evaluating the output of our model
is no different from other clustering problems. We
can therefore use well-known measures from the
clustering literature to assess the quality of our
role induction method. We first created a set of
gold-standard role labeled argument instances which
were obtained from the training partition of the
CoNLL 2008 dataset (corresponding to sections
02?21 of PropBank). We used 10 clusters for each
predicate and restricted the set of predicates to those
attested with more than 20 instances. This rules out
simple cases with only few instances relative to the
number of clusters, which trivially yield high scores.
We compared the output of our method against
the gold-standard using the following common mea-
sures. Let K denote the number of clusters, ci the set
of instances in the i-th cluster and g j the set of in-
stances having the j-th gold standard semantic role
label. Cluster purity (PU) is defined as:
PU =
1
K ?i
max
j
|ci ?g j| (7)
We also used cluster accuracy (CA, Equation 8),
944
PU CA CP CR CF1
Mic Mac Mic Mac Mic Mac Mic Mac Mic Mac
SyntFunc 73.2 75.8 82.0 80.9 67.6 65.3 55.7 50.1 61.1 56.7
LogLV 72.5 74.0 81.1 79.4 64.3 60.6 59.7 56.3 61.9 58.4
UpperBndS 94.7 96.1 96.9 97.0 97.4 97.6 90.4 100 93.7 93.8
UpperBndG 98.8 99.4 99.9 99.9 99.7 99.9 100 100 99.8 100
Table 2: Clustering results using our model (LogLV) against the baseline (SyntFunc) and upper bounds
(UpperBndS and UpperBndG).
cluster precision (CP, Equation 9), and cluster recall
(CR, Equation 9). Cluster F1 (CF1) is the harmonic
mean of precision and recall.
CA =
T P+T N
T P+FP+T N +FN
(8)
CP =
T P
T P+FP
CR =
T P
T P+FN
(9)
Here T P is the number of pairs of instances which
have the same role and are in the same cluster, T N is
the number of pairs of instances which have different
roles and are in different clusters, FP is the number
of pairs of instances with different roles in the same
cluster and FN the number of pairs of instances with
the same role in different clusters.
Baselines and Upper Bound We compared our
model against a baseline that assigns arguments to
clusters based on their syntactic function. Here, no
attempt is made to correct the roles of arguments in
non-standard linkings. We would also like to com-
pare our model against a supervised system. Unfor-
tunately, this is not possible, as we are using the des-
ignated CoNLL training set as our test set, and any
supervised system trained on this data would achieve
unfairly high scores. Therefore, we approximate the
performance of a supervised system by clustering in-
stances according to their gold standard role after
introducing some noise. Specifically, we randomly
selected 5% of the gold standard roles and mapped
them to an erroneous role. This roughly corresponds
to the clustering which would be induced by a state-
of-the-art supervised system with 95% precision. Fi-
nally, we also report the results of the true upper
bound obtained by clustering the arguments, based
on their gold standard semantic role (again using 10
clusters per verb).
6 Results
Our results are summarized in Table 2. We report
cluster purity, accuracy, precision, recall, and F1 for
our latent variable logistic classifier (LogLV) and a
baseline that assigns arguments to clusters accord-
ing to their syntactic function (SyntFunc). The table
also includes the gold standard upper bound (Up-
perBndG) and its supervised proxy (UpperBndS).
We report micro- and macro-average scores.3
Model scores are quite similar to the baseline,
which might suggest that the model is simply repli-
cating the observed data. However, this is not the
case: canonical functions differ from observed func-
tions for approximately 27% of the argument in-
stances. If the baseline treated these instances cor-
rectly, we would expect it to outperform our model.
The fact that it does not, indicates that the baseline
error rate is higher precisely on these instances. In
other words, the model can help in detecting alter-
nate linkings and thus baseline errors.
We further analyzed our model?s ability to de-
tect alternate linkings. Specifically, if we assume a
standard linking where model and observation agree
and an alternate linking where they disagree, we
obtain the following. The number of true positives
(correctly detected alternate linkings) is 27,606, the
number of false positives (incorrectly marked al-
ternations) is 32,031, the number of true negatives
(cases where the model correctly did not detect an
alternate linking) is 132,556, and the number of false
negatives (alternate linkings that the model should
have detected but did not) is 32,516.4. The analysis
shows that 46% of alternations (baseline errors) are
detected.
3Micro-averages are computed over instances while macro-
averages are computed over verbs.
4Note that the true/false positives/negatives here refer to al-
ternate linkings, not to be confused with the true/false positives
in equations (8) and (9).
945
PU CA CP CR CF1
Mic Mac Mic Mac Mic Mac Mic Mac Mic Mac
SyntFunct 73.9 77.8 82.1 81.3 68.0 66.5 55.9 50.3 61.4 57.3
LogLV 82.6 83.7 87.4 85.5 79.1 74.5 73.3 68.5 76.1 71.4
Table 3: Clustering results using our model to detect alternate linkings (LogLV) against the baseline (Synt-
Func).
We can therefore increase cluster purity by clus-
tering only those instances where the model does
not indicate an alternation. The results are shown
in Table 3. Using less instances while keeping the
number of clusters the same will by itself tend to
increase performance. To compensate for this, we
also report results for the baseline on a reduced
dataset. The latter was obtained from the origi-
nal dataset by randomly removing the same num-
ber of instances.5 By using the model to detect al-
ternations, scores improve over the baseline across
the board. We observe performance gains for pu-
rity which increases by 8.7% (micro-average; com-
pare Tables 2 and 3). F1 also improves considerably
by 13% (micro-average). These results are encour-
aging indicating that detecting alternate linkings is
an important first step towards more accurate role
induction.
We also conducted a more detailed error analysis
to gain more insight into the behavior of our model.
In most cases, alternate linkings where A1 occurs in
subject position and A0 in object position are canon-
icalized correctly (with 96% and 97% precision, re-
spectively). Half of the detected non-standard link-
ings involve adjunct roles. Here, the model has much
more difficulty with canonicalization and is success-
ful approximately 25% of the time. For example, in
the phrase occur at dawn the model canonicalizes
LOC to ADV, whereas TMP would be the correct
function. About 75% of all false negatives are due to
core roles and only 25% due to adjunct roles. Many
false negatives are due to parser errors, which are
reproduced by the model. This indicates overfitting,
and indeed many of the false negatives involve in-
frequent lexical items (e.g., juxtapose or Odyssey).
Finally, to put our evaluation results into context,
we also wanted to compare against Grenager and
Manning?s (2006) related system. A direct compar-
ison is somewhat problematic due to the use of dif-
5This was repeated several times to ensure that the results
are stable across runs.
ferent datasets and the fact that we induce labels for
all roles whereas they collapse adjunct roles to a sin-
gle role. Nevertheless, we made a good-faith effort
to evaluate our system using their evaluation setting.
Specifically, we ran our system on the same test set,
Section 23 of the Penn Treebank (annotated with
PropBank roles), using gold standard parses with six
clusters for each verb type. Our model achieves a
cluster purity score of 90.3% on this dataset com-
pared to 89.7% reported in Grenager and Manning.
7 Conclusions
In this paper we have presented a novel framework
for unsupervised role induction. We conceptualized
the induction problem as one of detecting alternate
linkings and finding their canonical syntactic form,
and formulated a novel probabilistic model that per-
forms these tasks. The model extends the logis-
tic classifier with latent variables and is trained on
parsed output which is used as a noisy target for
learning. Experimental results show promise, alter-
nations can be successfully detected and the quality
of the induced role clusters can be substantially en-
hanced.
We argue that the present model could be use-
fully employed to enhance the performance of other
models. For example, it could be used in an active
learning context to identify argument instances that
are difficult to classify for a supervised or semi-
supervised system and would presumably benefit
from additional (manual) annotation. Importantly,
the framework can incorporate different probabilis-
tic models for detection and canonicalization which
we intend to explore in the future. We also aim to
embed and test our role induction method within a
full SRL system that is also concerned with argu-
ment identification. Eventually, we also intend to re-
place the treebank-trained parser with a chunker.
946
References
Abend, O., R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of ACL-IJCNLP. Singapore,
pages 28?36.
Berger, A., S. Della Pietra, and V. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics 22(1):39?71.
Bishop, C. 2006. Pattern Recognition and Machine
Learning. Springer.
Bottou, L. 2004. Stochastic Learning. In Advanced Lec-
tures on Machine Learning, Springer Verlag, Lecture
Notes in Artificial Intelligence, pages 146?168.
Dowty, D. 1991. Thematic Proto Roles and Argument
Selection. Language 67(3):547?619.
Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International Journal
of Lexicography 16:235?250.
Fu?rstenau, H. and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of EMNLP. Singapore, pages 11?20.
Gildea, D. and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics
28(3):245?288.
Grenager, T. and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of EMNLP. Sydney, Australia, pages 1?8.
Lapata, M. 1999. Acquiring Lexical Generalizations
from Corpora: A Case Study for Diathesis Alterna-
tions. In Proceedings of the 37th ACL. pages 397?404.
Levin, B. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. The University of Chicago
Press.
Marcus, M., B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: the
Penn Treebank. Computational Linguistics 19(2):313?
330.
Ma`rquez, L., X. Carreras, K. Litkowski, and S. Steven-
son. 2008. Semantic Role Labeling: an Introduc-
tion to the Special Issue. Computational Linguistics
34(2):145?159.
McCarthy, D. 2002. Using Semantic Preferences to Iden-
tify Verbal Participation in Role Switching Alterna-
tions. In Proceedings of the 1st NAACL. Seattle, WA,
pages 256?263.
McCarthy, D. and A. Korhonen. 1998. Detecting Verbal
Participation in Diathesis Alternations. In Proceed-
ings of COLING/ACL. Montre?al, Canada, pages 1493?
1495.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Descrip-
tion of SQUASH, the SFU Question Answering Sum-
mary Handler for the DUC-2005 Summarization Task.
In Proceedings of the HLT/EMNLP Document Under-
standing Workshop. Vancouver, Canada.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering 13(2):95?135.
Pado?, S. and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research 36:307?340.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics 31(1):71?106.
Pradhan, S. S., W. Ward, and J. H. Martin. 2008. Towards
Robust Semantic Role Labeling. Computational Lin-
guistics 34(2):289?310.
Schulte im Walde, S. and C. Brew. 2002. Inducing
German Semantic Verb Classes from Purely Syntac-
tic Subcategorisation Information. In Proceedings of
the 40th ACL. Philadelphia, PA, pages 223?230.
Shen, D. and M. Lapata. 2007. Using Semantic Roles to
Improve Question Answering. In Proceedings of the
EMNLP-CoNLL. Prague, Czech Republic, pages 12?
21.
Surdeanu, M., S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st ACL.
Sapporo, Japan, pages 8?15.
Surdeanu, M., R. Johansson, A. Meyers, and L. Ma`rquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL. Manchester, England, pages
159?177.
Sutton, C. and A. McCallum. 2007. An Introduction to
Conditional Random Fields for Relational Learning.
In L. Getoor and B. Taskar, editors, Introduction to
Statistical Relational Learning, MIT Press, pages 93?
127.
Swier, R. and S. Stevenson. 2004. Unsupervised Se-
mantic Role Labelling. In Proceedings of EMNLP.
Barcelona, Spain, pages 95?102.
Wu, D. and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of
NAACL HLT 2009: Short Papers. Boulder, Colorado,
pages 13?16.
Yedidia, J., W. Freeman, and Y. Weiss. 2003. Understand-
ing Belief Propagation and its Generalizations. Mor-
gan Kaufmann Publishers Inc., pages 239?269.
947
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 466?476,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Taxonomy Induction Using Hierarchical Random Graphs
Trevor Fountain and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
t.fountain@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
This paper presents a novel approach for in-
ducing lexical taxonomies automatically from
text. We recast the learning problem as
that of inferring a hierarchy from a graph
whose nodes represent taxonomic terms and
edges their degree of relatedness. Our model
takes this graph representation as input and
fits a taxonomy to it via combination of a
maximum likelihood approach with a Monte
Carlo Sampling algorithm. Essentially, the
method works by sampling hierarchical struc-
tures with probability proportional to the like-
lihood with which they produce the input
graph. We use our model to infer a taxonomy
over 541 nouns and show that it outperforms
popular flat and hierarchical clustering algo-
rithms.
1 Introduction
The semantic knowledge encoded in lexical re-
sources such as WordNet (Fellbaum, 1998) has been
proven beneficial for several applications including
question answering (Harabgiu et al, 2003), doc-
ument classification (Hung et al, 2004), and tex-
tual entailment (Geffet and Dagan, 2005). As the
effort involved in creating such resources manu-
ally is prohibitive (cost, consistency and coverage
are often cited problems) and has to be repeated
for new languages or domains, recent years have
seen increased interest in automatic taxonomy in-
duction. The task has assumed several guises, such
as term extraction ? finding the concepts of the
taxonomy (Kozareva et al, 2008; Navigli et al,
2011), term relation discovery ? learning whether
any two terms stand in an semantic relation such as
IS-A, or PART-OF (Hearst, 1992; Berland and Char-
niak, 1999), and taxonomy construction ?- creat-
ing the taxonomy proper by organizing its terms hi-
erarchically (Kozareva and Hovy, 2010; Navigli et
al., 2011). Previous work has also focused on the
complementary task of augmenting an existing tax-
onomy with missing information (Snow et al, 2006;
Yang and Callan, 2009).
In this paper we propose an unsupervised ap-
proach to taxonomy induction. Given a corpus and
a set of terms, our algorithm jointly induces their re-
lations and their taxonomic organization. We view
taxonomy learning as an instance of the problem
of inferring a hierarchy from a network or graph.
We create this graph from unstructured text simply
by drawing an edge between distributionally sim-
ilar terms. Next, we fit a Hierarchical Random
Graph model (HRG; Clauset et al (2008)) to the
observed graph data based on maximum likelihood
methods and Markov chain Monte Carlo sampling.
The model essentially works by sampling hierarchi-
cal structures with probability proportional to the
likelihood with which they produce the input graph.
This is advantageous as it allows us to consider the
ensemble of random graphs that are statistically sim-
ilar to the original graph, and through this to de-
rive a consensus hierarchical structure from the en-
semble of sampled models. The approach differs
crucially from hierarchical clustering in that it ex-
plicitly acknowledges that most real-world networks
have many plausible hierarchical representations of
roughly equal likelihood and does not seek a sin-
gle hierarchical representation for a given network.
This feature also bodes well with the nature of lexi-
cal taxonomies: there is no uniquely correct taxon-
omy for a set of terms, rather different taxonomies
466
are likely to be appropriate for different tasks and
different taxonomization criteria.
Our contributions in this paper are three-fold: we
adapt the HRG model to the taxonomy induction
task and show that its performance is superior to al-
ternative methods based on either flat or hierarchi-
cal clustering; we analyze the requirements of the
algorithm with respect to the input graph and the
semantic representation of its nodes; and introduce
new ways of evaluating the fit of an automatically
induced taxonomy against a gold-standard. In the
following section we provide an overview of related
work. Next, we describe our HRG model in more
detail (Section 3) and present the resources and eval-
uation methodology used in our experiments (Sec-
tion 4). We conclude the paper by presenting and
discussing our results (Sections 4.1?4.4).
2 Related Work
The bulk of previous work has focused on term re-
lation discovery following essentially two method-
ological paradigms, pattern-based bootstrapping and
clustering. The former approach (Hearst, 1992;
Roark and Charniak, 1998; Berland and Charniak,
1999; Girju et al, 2003; Etzioni et al, 2005;
Kozareva et al, 2008) utilizes a few hand-crafted
seed patterns representative of taxonomic relations
(e.g., IS-A, PART-OF, SIBLING) to extract instances
from corpora. These instances are then used to ex-
tract new patterns which are in turn used to find new
instances and so on. Clustering-based approaches
have been mostly employed to discover IS-A and
SIBLING relations (Lin, 1998; Caraballo, 1999; Pan-
tel and Ravichandran, 2004). A common assump-
tion is that words are related if they occur in similar
contexts and thus clustering algorithms group words
together if they share contextual features. Most of
these algorithms aim at inducing flat clusters rather
than taxonomies, with the exception of Brown et al
(1992) whose method induces binary trees.
Contrary to the plethora of algorithms developed
for relation discovery, methods dedicated to taxon-
omy learning have been few and far between. Cara-
ballo (1999) was the first to induce a taxonomy
from a corpus using a combination of clustering and
pattern-based methods. Specifically, nouns are orga-
nized into a tree using a bottom-up clustering algo-
rithm and internal nodes of the resulting tree are la-
beled with hypernyms from the nouns clustered un-
derneath using patterns such as ?B is a kind of A?.
Kozareva et al (2008) and Navigli et al (2011)
both develop systems that create taxonomies end-
to-end, i.e., discover the terms, their relations, and
how these are hierarchically organized. The two ap-
proaches are conceptually similar: they both use the
web and pattern-based methods for finding domain-
specific terms. Additionally, in both approaches the
acquired knowledge is represented as a graph from
which a taxonomy is induced using task-specific al-
gorithms such as graph pruning, edge weighting,
and so on.
Our work also addresses taxonomy learning, how-
ever, without the term discovery step ? we assume
we are given the terms for which to create a taxon-
omy. Similarly to Kozareva et al (2008) and Nav-
igli et al (2011), our model operates over a graph
whose nodes represent terms and edges their rela-
tionships. We construct this graph from a corpus
simply by taking account of the distributional sim-
ilarity of the terms in question. Our taxonomy in-
duction algorithm is conceptually simpler and more
general; it fits a taxonomy to the observed network
data using the tools of statistical inference, combin-
ing a maximum likelihood approach with a Monte
Carlo Sampling algorithm. The technique allows us
to sample hierarchical random graphs with probabil-
ity proportional to the likelihood that they generate
the observed network. The induction algorithm can
operate over any kind of (undirected) graph, and thus
does not have to be tuned specifically for different
inputs. We should also point out that our formula-
tion of the inference problem utilizes very little cor-
pus external knowledge other than the set of input
terms, and could thus be easily applied to domains
or languages where lexical resources are scarce.
The Hierarchical Random Graph model (Clauset
et al, 2008) has been applied to construct hierarchi-
cal decompositions from three sets of network data:
a bacterial metabolic network; a food-web among
grassland species; and the network of associations
among terrorist cells. The only language-related ap-
plication we are aware of concerns word sense in-
duction. Klapaftis and Manandhar (2010) create a
graph of contexts for a polysemous target word and
use the HRG to organize them hierarchically, under
the assumption that different tree heights correspond
to different levels of sense granularity.
467
A B
C
D E
F
(a) Input graph
1.00
1.00 C
A B
1.00
E F
0.11
0.50
D
(b) Binary tree
C A B
E F
D
(c) Hierarchy
A B
C D
E F
(d) Clusters
Figure 1: Flow of information through the Hierarchical Random Graph algorithm. From a semantic net-
work (1a), the model constructs a binary tree (1b). Edges in the semantic network are then used to compute
the ? parameters for internal nodes in the tree; the maximum-likelihood-estimated ? parameter for an internal
node indicates the density of edges between its children. This tree is then resampled using the ? parameters
(1b) until the MCMC process converges, at which point it can be collapsed into a n-ary hierarchy (1c). The
same collapsing process can be also used to identify a flat clustering (1d).
A B
CA
B C A
B
C
Figure 2: Any internal node with subtrees A, B and
C can be permuted to one of two possible alter-
nate configurations. Shaded nodes represent internal
nodes which are unmodified by such permutation.
3 The Hierarchical Random Graph Model
A HRG consists of a binary tree and a set of likeli-
hood parameters, and operates on input organized
into a semantic network, an undirected graph in
which nodes represent terms and edges between
nodes indicate a relationship between pairs of terms
(Figure 1a). From this representation, the model
constructs a binary tree whose leaves correspond
to nodes in the semantic network (Figure 1b); the
model then employs a simple Markov chain Monte
Carlo (MCMC) process in order to explore the space
of possible binary trees and derives a consensus hi-
erarchical structure from the ensemble of sampled
models (Figure 1c).
3.1 Representing a Hierarchical Structure
Formally, we denote a semantic network S = (V,E),
where V = {v1,v2 . . .vn} is the set of vertices, one
per term, and E is the set of edges between terms
in which Ea,b indicates the presence of an edge be-
tween va and vb.
Given a network S, we construct a binary tree D
whose n leaves correspond to V and whose n? 1
internal nodes denote a hierarchy over V . Because
the leaves remain constant for a given S, we define
D as the set of internal nodes D = {D1,D2 . . .Dn}
and associate each edge Ea,b ? E with an internal
node Di being the lowest common parent of a,b?V .
The core assumption underlying the HRG model is
that edges in S have a non-uniform and independent
probability of existing. Each possible edge Ea,b ? E
exists with a probability ?i, where ?i is associated
with the corresponding internal node Di.
For a given internal node Di, let Li and Ri be the
number of leaves in Di?s left and right subtrees, re-
spectively; let Ei be the number of edges in E asso-
ciated with Di (colloquially, the number of edges in
S between leaves in Di?s left and right subtrees). For
each Di ? D, we can estimate the maximum likeli-
hood for the corresponding ?i as ?i =
Ei
LiRi
. The like-
lihood L(D,?|S) of a HRG over a given semantic
network S is then given by:
L(D,?|S) =
n?1
?
i=1
(?i)
Ei(1??i)
LiRi?Ei (1)
3.2 Markov Chain Monte Carlo Sampling
Given a representation for a HRG H (D,?) and a
method for estimating the likelihood of a given D
and ?, we can focus on obtaining the binary tree D
which best fits (or most plausibly explains) a given
semantic network. Because the space of possible bi-
nary trees over V is super-exponential with respect
to |V |, we employ a MCMC process to sample from
the space of binary trees. During each iteration of
468
Algorithm 1: MCMC Sampling
Compute the likelihood L(D,?) of the current1
binary tree.
Pick a random internal node Di ? D.2
Randomly permute Di according to Figure 2.3
Compute the likelihood L?(D,?) of the modified4
binary tree.
if L?(D,?) > L(D,?) then5
accept the transition;6
else7
accept with probability L?(D,?)/L(D,?)8
(i.e., standard Metropolis acceptance).
end9
Repeat;10
this process we randomly select a node within the
tree and permute it according to Figure 2. If this
permutation improves the overall likelihood of the
dendrogram we accept it as a transition, otherwise it
is accepted with a probability proportional to the de-
gree to which it decreases the overall likelihood (i.e.
standard Metropolis acceptance). This procedure is
described in more detail in Algorithm 1.
3.3 Consensus Hierarchy
Once the MCMC process has converged, the model
is left with a binary tree over the terms from the input
semantic network. As in standard hierarchical clus-
tering, however, this imposes an arbitrary structure
which may or may not correspond to the observed
data ? the tree at convergence will be similar to an
ideal tree given the graph, but may not be the most
plausible structure. Indeed, for taxonomy induction
it is quite unlikely that a binary tree will provide the
most appropriate categorization.
To avoid encoding such bias we employ a
model averaging technique to produce a consen-
sus hierarchy. For a set of binary trees sam-
pled after convergence, we first identify the set
of possible clusters encoded in the tree, e.g., the
binary tree in Figure 1b encodes the clusters
{AB,ABC,EF,D,DEF,ABCDEF}. As in Clauset et
al. (2008), each cluster instance is then weighted
according to the likelihood of the originating HRG
(Equation 1); we then sum the weights for each dis-
tinct cluster across all resampled trees and discard
those whose aggregate weight is lower than 50% of
the total observed weight. The remaining clusters
are then used to reconstruct a hierarchy in which
Algorithm 2: Flat Clusters
Let Dk be the root node of D.1
if ?k > ?? then2
output the leaves of the subtree rooted at Dk3
as a cluster
else4
repeat 2 with left and right children of Dk.5
end6
each subtree appears in the majority of trees ob-
served after the sampling process has reached con-
vergence, hence the term consensus hierarchy.
3.4 Obtaining Flat Clusters
For evaluation purposes we may want to compare
the groupings created by the HRG to a simpler non-
hierarchical clustering algorithm (see Section 4 for
details). We thus defined a method of converting the
tree produced by the HRG into a flat (hard) clus-
tering. This can be done in a relatively straightfor-
ward, principled fashion using the HRG?s ? parame-
ters. For a given H (D,?) we identify internal nodes
whose ?k likelihood is greater than the mean likeli-
hood and who possess no parent node whose ?k like-
lihood is also greater than the mean. Each such node
is the root of a densely-connected subtree; each such
subtree is then assumed to represent a single discrete
cluster of related items, where ?? = mean(?) (illus-
trated in Figure 1c). This procedure is explained in
greater detail in Algorithm 2.
4 Evaluation
Data We evaluated our taxonomy induction algo-
rithm using McRae et al?s (2005) dataset which
consists of for 541 basic level nouns (e.g., DOG
and TABLE). Each noun is associated with features
(e.g., has-legs, is-flat, and made-of-wood for TABLE)
collected from human participants in multiple stud-
ies over several years. The original norming study
does not include class labels for these nouns, how-
ever, we were able to exploit a clustering provided
by Fountain and Lapata (2010), in which a set of on-
line participants annotated each of the McRae et al
nouns with basic category labels.
The nouns and their class labels were further tax-
onomized using WordNet (Fellbaum, 1998). Specif-
ically, we first identified the full hypernym path in
WordNet for each noun in McRae et al?s (2005)
dataset, e.g., APPLE > PLANT STRUCTURE > NAT-
469
URAL OBJECT > PHYSICAL OBJECT > ENTITY (a
total of 493 concepts appear in both). These hyper-
nym paths were then combined to yield a full tax-
onomy over McRae et al?s nouns; internal nodes
having only a single child were recursively removed
to produce a final, compact taxonomy1 containing
186 semantic classes (e.g., ANIMALS, WEAPONS,
FRUITS) organized into varying levels of granular-
ity (e.g., SONGBIRDS > BIRDS >ANIMALS).
Evaluation measures Evaluation of taxonomi-
cally organized information is notoriously hard (see
Hovy (2002) for an extensive discussion on this
topic). This is due to the nature of the task which is
inherently subjective and application specific (e.g., a
dolphin can be a Mammal to a biologist, but a Fish
to a fisherman or someone visiting an aquarium).
Nevertheless, we assessed the taxonomies produced
by the HRG against the WordNet-like taxonomy de-
scribed above using two measures, one that sim-
ply evaluates the grouping of the nouns into classes
without taking account of their position in the taxon-
omy and one which evaluates the taxonomy directly.
To evaluate a flat clustering into classes we use
the F-score measure introduced in the SemEval 2007
task (Agirre and Soroa, 2007); it is the harmonic
mean of precision and recall defined as the number
of correct members of a cluster divided by the num-
ber of items in the cluster and the number of items
in the gold-standard class, respectively. Although
informative, evaluation based solely on F-score puts
the HRG model at a comparative disadvantage as the
task of taxonomy induction is significantly more dif-
ficult than simple clustering. To overcome this dis-
advantage we propose an automatic method of eval-
uating taxonomies directly by first computing the
walk distance between pairs of terms that share a
gold-standard category label within a gold-standard
and a candidate taxonomy, and then computing the
pairwise correlation between distances in each tree
(Lapointe, 1995). This captures the intuition that
a ?good? hierarchy is one in which items appearing
near one another in the gold taxonomy also appear
near one another in the induced one. It is also con-
ceptually similar to the task-based IS-A evaluation
(Snow et al, 2006) which has been traditionally used
to evaluate taxonomies.
Formally, let G = {g0,1,g0,2 . . .gn,n?1}, where ga,b
indicates the walk distance between terms a and b
1The taxonomy and flat cluster labels are available from
http://homepages.inf.ed.ac.uk/s0897549/data.
in the gold standard hierarchy. Similarly, let C =
{c0,1,c0,2 . . .cn,n?1}, where ca,b is the distance be-
tween a and b in the candidate hierarchy. The tree-
height correlation between G and C is then given
by Spearman?s ? correlation coefficient between the
two sets. All tree-height correlations reported in
our experiments were computed using the WordNet-
based gold-standard taxonomy over McRae et al?s
(2005) nouns.
Baselines We compared the HRG output against
three baselines. The first is Chinese Whispers (CW;
Biemann (2006)), a randomized graph-clustering al-
gorithm which like the HRG also takes as input a
graph with weighted edges. It produces a hard (flat)
clustering over the nodes in the graph, where the
number of clusters is determined automatically. Our
second baseline is Brown et al?s (1992) agglomer-
ative clustering algorithm that induces a mapping
from word types to classes. It starts with K classes
for the K most frequent word types and then pro-
ceeds by alternately adding the next most frequent
word to the class set and merging the two classes
which result in the least decrease in the mutual in-
formation between class bigrams. The result is a
class hierarchy with word types at the leaves. Ad-
ditionally, we compare against standard agglomer-
ative clustering (Sokal and Michener, 1958) which
produces a binary dendrogram in a bottom-up fash-
ion by recursively identifying concepts or clusters
with the highest pairwise similarity.
In the following, we present our taxonomy induc-
tion experiments (Sections 4.1?4.3). Since HRGs
provide a means of inducing a hierarchy over a
graph-based representation, which may be con-
structed in an arbitrary fashion, our experiments
were designed to investigate how the topology and
quality of the input graph influences the algorithm?s
performance. We thus report results when the se-
mantic network is created from data sources of vary-
ing quality and granularity.
4.1 Experiment 1: Taxonomy Induction from
Feature Norms
Method We first considered the case where the in-
put graph is of high semantic quality and constructed
a semantic network from the feature norms collected
by McRae et al (2005). Each noun was represented
as a vector with dimensions corresponding to the
possible features generated by participants of the
norming study; the value of a term along a dimen-
470
Method F-score Tree Correlation
HRG 0.507 0.168
CW 0.464 ?
Agglo 0.352 0.137
Table 1: Cluster F-score and tree-height correla-
tion evaluation; a semantic network constructed over
McRae et al?s (2005) nouns and features is given as
input to the algorithms.
sion was taken to be the frequency with which par-
ticipants generated the corresponding feature when
given the term. For each pair of terms an edge was
added to the semantic network if the cosine similar-
ity between their vector representations exceeded a
fixed threshold T (set to 0.15).
The resulting network was then provided as in-
put to the HRG, which was resampled until con-
vergence. The binary tree at convergence was col-
lapsed into a hierarchy over clusters using the pro-
cedure described in Section 3.4; this hierarchy was
evaluated by computing the cluster F-score between
its constituent clusters and those of a gold-standard
(human-produced) clustering. The resulting consen-
sus hierarchy was evaluated by computing the tree-
height correlation between it and the gold-standard
(WordNet-derived) hierarchy.
Results Our results are summarized in Table 1.
We only give the tree correlation for the HRG and
agglomerative methods (Agglo) as CW does not in-
duce a hierarchical clustering. In addition, we do
not compare against Brown et al (1992) as the in-
put to this algorithm is not vector-based. When
evaluated using F-score, the HRG algorithm pro-
duces better quality clusters compared to CW, in
addition to being able to organize them hierarchi-
cally. It also outperforms agglomerative clustering
by a large margin. A similar pattern emerges when
the HRG and Agglo are evaluated on tree correla-
tion. The taxonomies produced by the HRG are a
better fit against the WordNet-based gold standard;
the difference in performance is statistically signif-
icant (p < 0.01) using a t-test (Cohen and Cohen.,
1983).
4.2 Experiment 2: Taxonomy Induction from
the British National Corpus
Method The results of Experiment 1 can be con-
sidered as an upper bound of what can be achieved
by the HRG when the input graph is constructed
from highly accurate semantic information. Feature
norms provide detailed knowledge about meaning
which would be very difficult if not close to impos-
sible to obtain from a corpus. Nevertheless, it is
interesting to explore how well we can induce tax-
onomies using a lower quality semantic network.
We therefore constructed a network based on co-
occurrence statistics computed from the British Na-
tional Corpus (BNC, 2007) and provided the result-
ing semantic network as input to the HRG, CW,
and Agglo models; additionally, we employed the
algorithm of Brown et al (1992) to induce a hier-
archy over the target terms directly from the cor-
pus. Unfortunately, this algorithm requires the num-
ber of desired output clusters to be specified in ad-
vance; in all trials this parameter was set to the num-
ber of clusters in the gold-standard clustering (41),
thus providing the Brown-induced clusterings with a
slight oracle advantage.
Again, nouns were represented as vectors in se-
mantic space. We used a context window of five
words on either side of the target word and 5,000
vector components corresponding to the most fre-
quent non-stopwords in the BNC. Raw frequency
counts were transformed using pointwise mutual in-
formation (PMI). An edge was added to the seman-
tic network between a pair of nouns if their simi-
larity exceeded a predefined threshold (the same as
in Experiment 1). The similarity of two nouns was
defined as the cosine distance between their corre-
sponding vectors.
The HRG algorithm was used to produce a tax-
onomy from this network and was also compared
against Brown et al (1992). The latter induces a hi-
erarchy from a corpus directly, without the interme-
diate graph representation. All resulting taxonomies
were evaluated against gold standard flat and hierar-
chical clusterings, again as in Experiment 1.
Results Results are shown in Table 2. With regard
to flat clustering (the F-score column in the table),
the HRG has a slight advantage against CW, and
Brown et al?s (1992) algorithm (Brown). However,
differences in performance are not statistically sig-
nificant. Agglomerative clustering is the worst per-
forming method leading to a decrease in F-score of
approximately 1.5. With regard to tree correlation,
the output of the HGRG is comparable to Brown
(the difference between the two is not statistically
significant). Both algorithms are significantly better
(p < 0.01) than Agglo.
471
(a) s = 0.0 (b) s = 0.5 (c) s = 1.0
Figure 3: The original semantic network as derived from the BNC (a) and the same network re-weighted
using a flat clustering produced by CW (b). As s approaches 1.0 the network exhibits an increasingly strong
small-world property, eventually reconstructing the input clustering only (c).
Method F-score Tree Correlation
HRG 0.276 0.104
CW 0.274 ?
Brown 0.258 0.124
Agglo 0.122 0.077
Table 2: Cluster F-score and tree-height correla-
tion evaluation for taxonomies inferred over McRae
et al?s (2005) nouns; all algorithms are run on the
BNC.
Performance of the HRG is better when the se-
mantic network is based on feature norms (compare
Tables 1 and 2), both in terms of tree-height correla-
tion and F-score. This suggests that the algorithm is
highly dependent on the quality of the semantic net-
work used as input. In particular, HRGs are known
to be more appropriate for so-called small-world
networks, graphs composed of densely-connected
subgraphs with relatively sparse connections be-
tween (Klapaftis and Manandhar, 2010). Indeed, in-
spection of the semantic network produced from the
BNC (see Figure 3a) shows that our corpus-derived
graph is emphatically not a small-world graph, yet
the HRG is able to recover some taxonomic infor-
mation from such a densely-connected network.
In the following experiments we first assess the
difficulty of the taxonomy induction task to get a
feel of how well the algorithms are performing in
comparison to humans and then investigate ways of
rendering the BNC-based graph more similar to a
small-world network.
4.3 Experiment 3: Human Upper Bound
Method The previous experiments evaluated the
performance of the HRG against a gold-standard
hierarchy derived from WordNet. For any set of
concepts there will exist multiple valid taxonomies,
each representing an accurate if differing organiza-
tion of identical concepts using different criteria;
for the set of concepts used in Experiments 1?2 the
WordNet hierarchy represents merely one of many
valid hierarchies. Noting this, it is interesting to ex-
plore how well the hierarchies output by the model
fit within the set of possible, valid taxonomies over
a given set of concepts.
We thus conducted an experiment in which hu-
man participants were asked to organize words into
arbitrary hierarchies. To render the task feasible,
they were given a small subset of 12 words rather
than the full set of 541 nouns over which the HRG
operates. We first selected a sub-hierarchy of the
WordNet tree (?living things?) along with its subtrees
(e.g., ?animals?, ?plants?), and chose target concepts
from within these trees in order to produce a tax-
onomy in which some items were differentiated at
a high level (e.g., ?python? vs. ?dog?) and others at
a fine-grained level (e.g., ?lion? vs ?tiger?). The ex-
periment was conducted using Amazon Mechanical
Turk2, and involved 41 participants, all self-reported
native English speakers. No guidelines as to what
features participants were to use when organizing
these concepts were provided. Participants were pre-
sented with a web-based, graphical, mouse-driven
interface for constructing a taxonomy over the cho-
2http://mturk.com
472
Method Tree Correlation Min Max Std
HRG 0.412 -0.039 0.799 0.166
Brown 0.181 0.006 0.510 0.121
Agglo 0.274 -0.056 0.603 0.121
Agreement 0.511 -0.109 1.000 0.267
Table 3: Model performance on a subset of the
target words used in Experiments 1?2, applied to
a subset of the semantic network used in Experi-
ment 2. Instead of a WordNet-derived hierarchy,
models were evaluated against hierarchies manually
produced by participants in an online study. Tree
correlation values are means; we also report the min-
imum (Min), maximum (Max), and standard devia-
tion (Std) of the mean.
sen set of concepts.
To evaluate the HRG, along with the baselines
from Experiment 2, against the resulting hierarchies
we constructed a semantic network over the subset
of concepts using similarities derived from the BNC;
this network was a subgraph of that used in Exper-
iment 2. We compute inter-annotator agreement as
the mean pairwise tree-height correlation between
the hierarchies our participants produced. We also
report for each model the mean tree-height corre-
lation between the hierarchy it produced and those
created by human annotators.
Results As shown in Table 3, participants achieve
a mean pairwise tree correlation of 0.511. This in-
dicates that there is a fair amount of agreement with
respect to the taxonomic organization of the words
in question. The HRG comes close achieving a mean
tree correlation of 0.412, followed by Agglo, and
Brown. In general, we observe that the HRG man-
ages to produce hierarchies that resemble those gen-
erated by humans to a larger extent than competing
algorithms. The results in Table 3 also hint at the
fact that the taxonomy induction task is relatively
hard as participants do not achieve perfect agree-
ment despite the fact that they are asked to taxon-
omize only 12 words.
4.4 Experiment 4: Taxonomy Induction from a
Small-world Network
Method In Experiment 2 we hypothesized that a
small-world input graph would be more advanta-
geous for the HRG. In order to explore this further,
we imposed something of a small-world structure on
Method F-score Tree Correlation
HRG 0.276 0.104
HRG + CW 0.291 0.161
HRG + Brown 0.255 0.173
Table 4: Cluster F-score and tree-height correlation
evaluation for taxonomies inferred by the HRG us-
ing semantic network derived from the BNC and re-
weighted using CW and Brown.
the BNC semantic network, using a combination of
the baseline clustering methods evaluated in Exper-
iment 2. Specifically, we first obtain a (flat) cluster-
ing using either CW or Brown, which we then use
to re-weight the BNC graph given as input to the
HRG.3 Note that, as the clustering algorithms used
are unsupervised this procedure does not introduce
any outside supervision into the overall taxonomy
induction task.
The modified weight W?A,B between a pair of
terms A,B was computed according to Equation (2),
where s indicates the proportion of edge weight
drawn from the clustering, WA,B is the edge weight
in the original (BNC) semantic network, and CA,B is
a binary value indicating that A and B belong to the
same cluster (i.e., CA,B = 1 if A and B share a cluster;
CA,B = 0 otherwise).
W?A,B = (1? s)WA,B + sCA,B (2)
The value of the s parameter was tuned empirically
on held-out development data and set to s = 0.4 for
both CW and Brown algorithms. Each re-weighted
network was then used as input to an HRG, and
the resulting taxonomies were evaluated in the same
manner as in Experiments 1 and 2.
Results Table 4 shows results for cluster F-score
and tree-height correlation for the HRG when us-
ing a graph derived from the BNC without any
modifications, and two re-weighted versions using
the CW and Brown clustering algorithms, respec-
tively. As can been seen, re-weighting improves
tree-height correlation substantially: HRG with CW
and Brown is significantly better than HRG on its
own (p < 0.05). In the case of CW, cluster F-score
also yields a slight improvement. Interestingly,
the tree-height correlations obtained with CW and
Brown are comparable to those attained by the HRG
3We omit agglomerative clustering as it performed poorly
on the BNC, see Table 2.
473
bed cushion pillow sofabow jeans mittens veil blouse coat gown pants trousers leotards dress swimsuit shawl scarf
jacket sweater tie vest bra camisole nylons
01
23 4
Figure 4: An excerpt from a hierarchy induced by the HRG, using the BNC semantic network with Brown
re-weighting. The HRG does not provide category labels for internal nodes of the hierarchy, but subtrees
within this excerpt correspond roughly to (0) TEXTILES, (1) CLOTHING, (2) GENDERED CLOTHING, (3)
MEN?S CLOTHING, and (4) WOMEN?S CLOTHING.
when using the human-produced feature norms (dif-
ferences in correlations are not statistically signifi-
cant). An excerpt of a HRG-induced taxonomy is
shown in Figure 4.
5 Conclusions
In this paper we have presented a novel method for
automatically inducing lexical taxonomies based on
Hierarchical Random Graphs. The approach is con-
ceptually simple, taking a graph representation as
input and fitting a taxonomy via combination of a
maximum likelihood approach with a Monte Carlo
Sampling algorithm. Importantly, the approach does
not operate on corpora directly, instead it relies on
an abstract, interim representation (a semantic net-
work) which we argue is advantageous, as it allows
to easily encode additional information in the input.
Furthermore, the model presented here is largely
parameter-free, as both the input graph and the in-
ferred taxonomy are derived empirically in an unsu-
pervised manner (minimal tuning is required when
graph re-weighting is employed, the parameter s).
Our experiments have shown that both the input
semantic network and the representation of its nodes
influence the quality of the induced taxonomy. Rep-
resenting the terms of the taxonomy as vectors in a
human-produced feature space yields more coherent
semantic classes compared to a corpus-based vector
representation (see the F-score in Tables 1 and 4).
This is not surprising, as feature norms provide
more detailed and accurate knowledge about seman-
tic representations than often noisy and approxi-
mate corpus-based distributions.4 It may be possi-
4Note that as multiple participants are required to create a
representation for each word, norming studies typically involve
ble to obtain better performance when considering
more elaborate representations. We have only ex-
perimented with a simple semantic space, however
variants that utilize syntactic information (e.g., Pado?
and Lapata (2007)) may be more appropriate for the
taxonomy induction task. Our experiments have also
shown that the topology of the input semantic net-
work is critical for the success of the HRG. In partic-
ular edge re-weighting plays an important role and
generally improves performance. We have adopted
a simple method based on flat clustering; it may be
interesting to compare how this fares with more in-
volved weighting schemes such as those described
in Navigli et al (2011). Finally, we have shown that
naive participants are able to perform the taxonomy
induction task relatively reliably and that the HRG
approximates human performance on a small-scale
experiment. We have evaluated model output using
F-score and tree-height correlation which we argue
are complementary and allow to assess hierarchical
clustering more rigorously.
Avenues for future work are many and varied. Be-
sides exploring the performance of our algorithm on
more specialized domains (e.g., mathematics or ge-
ography) we would also like to create an incremen-
tal version that augments an existing taxonomy with
missing information. Additionally, the taxonomies
inferred with the HRG do not currently admit term
ambiguity which we could remedy by modifying our
technique for constructing a consensus hierarchy to
reflect the sampled frequency of observed subtrees.
a small number of items, consequently limiting the scope of any
computational model based on normed data.
474
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
pages 7?12, Prague, Czech Republic, June.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 57?64, College Park, Maryland.
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to natu-
ral language processing problems. In Proceedings of
TextGraphs: the 1st Workshop on Graph Based Meth-
ods for Natural Language Processing, pages 73?80,
New York City.
BNC. 2007. The British National Corpus, version 3
(BNC XML Edition). Distributed by Oxford Univer-
sity Computing Services on behalf of the BNC Con-
sortium.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 120?126,
College Park, Maryland.
Aaron Clauset, Christopher Moore, and M. E. J. New-
man. 2008. Hierarchical structure and the prediction
of missing links in networks. Nature, 453:98?101,
February.
J. Cohen and P. Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Erlbaum, Hillsdale, NJ.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communi-
cation). The MIT Press.
Trevor Fountain and Mirella Lapata. 2010. Meaning rep-
resentation in natural language categorization. In Stel-
lan Ohlsson and Richard Catrambone, editors, Pro-
ceedings of the 31st Annual Conference of the Cogni-
tive Science Society, pages 1916?1921, Portland, Ore-
gon. Cognitive Science Society.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 107?114,
Ann Arbor, Michigan.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 80?87, Edmonton,
Canada.
Sanda M. Harabgiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-doman textual question answering
techniques. Natural Language Engineering, 9(3):1?
38.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics, pages
539?545, Nantes, France.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tionships in ontologies. In Rebecca Green, Carol A.
Bean, and Sun Hyon Myaeng, editors, The Seman-
tics of Relationships: An Interdisciplinary Perspec-
tive, pages 91?110. Kluwer Academic Publishers, The
Netherlands.
Chihli Hung, Stefan Wermter, and Peter Smith. 2004.
Hybrid neural document clustering using guided self-
organization and wordnet. IEEE Intelligent Systems,
19(2):68?77.
Ioannis Klapaftis and Suresh Manandhar. 2010. Word
sense induction and disambiguation using hierarchical
random graphs. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 745?755, Cambridge, MA.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1482?1491, Uppsala, Sweden, July.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056, Columbus, Ohio, June.
Franc?ois-Joseph Lapointe. 1995. Comparison tests for
dendrograms: A comparative evaluation. Journal of
Classification 12:265-282, 12:265?282.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, Volume 2, pages 768?774, Mon-
treal, Quebec, Canada.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and non-living things.
475
Behavioral Research Methods Instruments & Comput-
ers, 37(4):547?559.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In Proceedings of the
22nd International Joint Conference on Artificial In-
telligence, pages 1872?1877, Barcelona, Spain.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Main Proceedings, pages 321?328,
Boston, Massachusetts.
Brian Roark and Eugene Charniak. 1998. Noun-
phrase co-occurrence statistics for semi-automatic se-
mantic lexicon construction. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, Volume 2, pages 1110?
1116, Montreal, Quebec.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 801?808, Sydney, Australia.
Robert Sokal and Charles Michener. 1958. A statistical
method for evaluating systematic relationships. Uni-
versity of Kansas Science Bulletin, 38:1409?1438.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 271?279, Suntec, Singapore.
476
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752?761,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Concept-to-text Generation with Hypergraphs
Ioannis Konstas and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
i.konstas@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Concept-to-text generation refers to the task of
automatically producing textual output from
non-linguistic input. We present a joint model
that captures content selection (?what to say?)
and surface realization (?how to say?) in
an unsupervised domain-independent fashion.
Rather than breaking up the generation pro-
cess into a sequence of local decisions, we de-
fine a probabilistic context-free grammar that
globally describes the inherent structure of the
input (a corpus of database records and text
describing some of them). We represent our
grammar compactly as a weighted hypergraph
and recast generation as the task of finding the
best derivation tree for a given input. Experi-
mental evaluation on several domains achieves
competitive results with state-of-the-art sys-
tems that use domain specific constraints, ex-
plicit feature engineering or labeled data.
1 Introduction
Concept-to-text generation broadly refers to the task
of automatically producing textual output from non-
linguistic input (Reiter and Dale, 2000). Depend-
ing on the application and the domain at hand, the
input may assume various representations includ-
ing databases of records, expert system knowledge
bases, simulations of physical systems and so on.
Figure 1 shows input examples and their correspond-
ing text for three domains, air travel, sportscasting
and weather forecast generation.
A typical concept-to-text generation system im-
plements a pipeline architecture consisting of three
core stages, namely text planning (determining the
content and structure of the target text), sentence
planning (determining the structure and lexical con-
tent of individual sentences), and surface realiza-
tion (rendering the specification chosen by the sen-
tence planner into a surface string). Traditionally,
these components are hand-engineered in order to
generate high quality text, however at the expense
of portability and scalability. It is thus no surprise
that recent years have witnessed a growing interest
in automatic methods for creating trainable genera-
tion components. Examples include learning which
database records should be present in a text (Duboue
and McKeown, 2002; Barzilay and Lapata, 2005)
and how these should be verbalized (Liang et al,
2009). Besides concentrating on isolated compo-
nents, a few approaches have emerged that tackle
concept-to-text generation end-to-end. Due to the
complexity of the task, most models simplify the
generation process, e.g., by creating output that con-
sists of a few sentences, thus obviating the need for
document planning, or by treating sentence planning
and surface realization as one component. A com-
mon modeling strategy is to break up the genera-
tion process into a sequence of local decisions, each
learned separately (Reiter et al, 2005; Belz, 2008;
Chen and Mooney, 2008; Angeli et al, 2010; Kim
and Mooney, 2010).
In this paper we describe an end-to-end gen-
eration model that performs content selection and
surface realization jointly. Given a corpus of
database records and textual descriptions (for some
of them), we define a probabilistic context-free
grammar (PCFG) that captures the structure of the
database and how it can be rendered into natural
752
Flight
From To
phoenix new york
Search
Type What
query flight
Day
Day Dep/Ar
sunday departure
List flights from phoenix to new york on sunday
Temperature
Time Min Mean Max
06:00-21:00 9 15 21
Wind Speed
Time Min Mean Max
06:00-21:00 15 20 30
Cloud Sky Cover
Time Percent (%)
06:00-09:00 25-50
09:00-12:00 50-75
Wind Direction
Time Mode
06:00-21:00 S
Cloudy, with a low around 10. South wind around 20 mph.
Pass
From To
pink3 pink7
Bad Pass
From To
pink7 purple3
Turn Over
From To
pink7 purple3
pink3 passes the ball to pink7
(b)(a)
(c)
Figure 1: Input-output examples for (a) query generation in the air travel domain, (b) weather forecast generation, and
(c) sportscasting.
language. This grammar represents a set of trees
which we encode compactly using a weighted hy-
pergraph (or packed forest), a data structure that de-
fines a probability (or weight) for each tree. Gen-
eration then boils down to finding the best deriva-
tion tree in the hypergraph which can be done effi-
ciently using the Viterbi algorithm. In order to en-
sure that our generation output is fluent, we intersect
our grammar with a language model and perform
decoding using a dynamic programming algorithm
(Huang and Chiang, 2007).
Our model is conceptually simpler than previous
approaches and encodes information about the do-
main and its structure globally, by considering the
input space simultaneously during generation. Our
only assumption is that the input must be a set of
records essentially corresponding to database-like
tables whose columns describe fields of a certain
type. Experimental evaluation on three domains ob-
tains results competitive to the state of the art with-
out using any domain specific constraints, explicit
feature engineering or labeled data.
2 Related Work
Our work is situated within the broader class of
data-driven approaches to content selection and sur-
face realization. Barzilay and Lapata (2005) focus
on the former problem which they view as an in-
stance of collective classification (Barzilay and La-
pata, 2005). Given a corpus of database records
and texts describing some of them, they learn a con-
tent selection model that simultaneously optimizes
local label assignments and their pairwise relations.
Building on this work, Liang et al (2009) present a
hierarchical hidden semi-Markov generative model
that first determines which facts to discuss and then
generates words from the predicates and arguments
of the chosen facts.
A few approaches have emerged more recently
that combine content selection and surface realiza-
tion. Kim and Mooney (2010) adopt a two-stage ap-
proach: using a generative model similar to Liang et
al. (2009), they first decide what to say and then ver-
balize the selected input with WASP?1, an existing
generation system (Wong and Mooney, 2007). In
contrast, Angeli et al (2010) propose a unified con-
tent selection and surface realization model which
also operates over the alignment output produced
by Liang et al (2009). Their model decomposes
into a sequence of discriminative local decisions.
They first determine which records in the database
to talk about, then which fields of those records
to mention, and finally which words to use to de-
scribe the chosen fields. Each of these decisions
is implemented as a log-linear model with features
learned from training data. Their surface realiza-
tion component is based on templates that are au-
tomatically extracted and smoothed with domain-
specific constraints in order to guarantee fluent out-
put. Other related work (Wong and Mooney, 2007;
Lu and Ng, 2011). has focused on generating natural
language sentences from logical form (i.e., lambda-
expressions) using mostly synchronous context-free
grammars (SCFGs).
753
Similar to Angeli et al (2010), we also present
an end-to-end system that performs content selec-
tion and surface realization. However, rather than
breaking up the generation task into a sequence of
local decisions, we optimize what to say and how
to say simultaneously. We do not learn mappings
from a logical form, but rather focus on input which
is less constrained, possibly more noisy and with a
looser structure. Our key insight is to convert the
set of database records serving as input to our gen-
erator into a PCFG that is neither hand crafted nor
domain specific but simply describes the structure
of the database. The approach is conceptually sim-
ple, does not rely on discriminative training or any
feature engineering. We represent the grammar and
its derivations compactly as a weighted hypergraph
which we intersect with a language model in order
to generate fluent output. This allows us to easily
port surface generation to different domains without
having to extract new templates or enforce domain
specific constraints.
3 Problem Formulation
We assume our generator takes as input a set of
database records d and produces text w that verbal-
izes some of these records. Each record r ? d has a
type r.t and a set of fields f associated with it. Fields
have different values f .v and types f .t (i.e., in-
teger or categorical). For example, in Figure 1b,
wind speed is a record type with four fields: time,
min, mean, and max. The values of these fields are
06:00-21:00, 15, 20, and 30, respectively; the type
of time is categorical, whereas all other fields are
integers.
During training, our algorithm is given a cor-
pus consisting of several scenarios, i.e., database
records paired with texts like those shown in Fig-
ure 1. In the weather forecast domain, a scenario cor-
responds to weather-related measurements of tem-
perature, wind, speed, and so on collected for a spe-
cific day and time (e.g., day or night). In sportscast-
ing, scenarios describe individual events in the soc-
cer game (e.g., passing or kicking the ball). In the air
travel domain, scenarios comprise of flight-related
details (e.g., origin, destination, day, time). Our goal
then is to reduce the tasks of content selection and
surface realization into a common probabilistic pars-
ing problem. We do this by abstracting the struc-
ture of the database (and accompanying texts) into
a PCFG whose probabilities are learned from train-
ing data.1 Specifically, we convert the database into
rewrite rules and represent them as a weighted di-
rected hypergraph (Gallo et al, 1993). Instead of
learning the probabilities on the PCFG, we directly
compute the weights on the hyperarcs using a dy-
namic program similar to the inside-outside algo-
rithm (Li and Eisner, 2009). During testing, we are
given a set of database records without the corre-
sponding text. Using the trained grammar we com-
pile a hypergraph specific to this test input and de-
code it approximately via cube pruning (Chiang,
2007).
The choice of the hypergraph framework is moti-
vated by at least three reasons. Firstly, hypergraphs
can be used to represent the search space of most
parsers (Klein and Manning, 2001). Secondly, they
are more efficient and faster than the common CYK
parser-based representation for PCFGs by a factor
of more than ten (Huang and Chiang, 2007). And
thirdly, the hypergraph representation allows us to
integrate an n-gram language model and perform de-
coding efficiently using k-best Viterbi search, opti-
mizing what to say and how to say at the same time.
3.1 Grammar Definition
Our model captures the inherent structure of the
database with a number of CFG rewrite rules, in
a similar way to how Liang et al (2009) define
Markov chains in the different levels of their hierar-
chical model. These rules are purely syntactic (de-
scribing the intuitive relationship between records,
records and fields, fields and corresponding words),
and could apply to any database with similar struc-
ture irrespectively of the semantics of the domain.
Our grammar is defined in Table 1 (rules (1)?(9)).
Rule weights are governed by an underlying multi-
nomial distribution and are shown in square brack-
ets. Non-terminal symbols are in capitals and de-
1An alternative would be to learn a SCFG between the
database input and the accompanying text. However, this would
involve considerable overhead in terms of alignment (as the
database and the text do not together constitute a clean parallel
corpus, but rather a noisy comparable corpus), as well as gram-
mar training and decoding using state-of-the art SMT methods,
which we manage to avoid with our simpler approach.
754
1. S? R(start) [Pr = 1]
2. R(ri.t)? FS(r j,start) R(r j.t) [P(r j.t |ri.t) ??]
3. R(ri.t)? FS(r j,start) [P(r j.t |ri.t) ??]
4. FS(r,r. fi)? F(r,r. f j) FS(r,r. f j) [P( f j | fi)]
5. FS(r,r. fi)? F(r,r. f j) [P( f j | fi)]
6. F(r,r. f )?W(r,r. f ) F(r,r. f ) [P(w |w?1,r,r. f )]
7. F(r,r. f )?W(r,r. f ) [P(w |w?1,r,r. f )]
8. W(r,r. f )? ? [P(? |r,r. f , f .t, f .v)]
9. W(r,r. f )? g( f .v)
[P(g( f .v).mode |r,r. f , f .t = int)]
Table 1: Grammar rules and their weights shown in
square brackets.
note intermediate states; the terminal symbol ?
corresponds to all words seen in the training set,
and g( f .v) is a function for generating integer num-
bers given the value of a field f . All non-terminals,
save the start symbol S, have one or more features
(shown in parentheses) that act as constraints, sim-
ilar to number and gender agreement constraints in
augmented syntactic rules.
Rule (1) denotes the expansion from the start
symbol S to record R, which has the special ?start?
record type (hence the notation R(start)). Rule (2)
defines a chain between two consecutive records,
i.e., going from a source record ri to a target r j.
Here, FS(r j,r j. f ) represents the set of fields of the
target r j, following the source record R(ri).
For example, the rule R(skyCover1.t) ?
FS(temperature1,start)R(temperature1.t) can
be interpreted as follows. Given that we have
talked about skyCover1, we will next talk about
temperature1 and thus emit its corresponding fields.
R(temperature1.t) is a non-terminal place-holder
for the continuation of the chain of records, and
start in FS is a special boundary field between
consecutive records. The weight of this rule is the
bigram probability of two records conditioned on
their record type, multiplied with a normalization
factor ?. We have also defined a null record type
i.e., a record that has no fields and acts as a
smoother for words that may not correspond to a
particular record. Rule (3) is simply an escape rule,
so that the parsing process (on the record level) can
finish.
Rule (4) is the equivalent of rule (2) at the
field level, i.e., it describes the chaining of
two consecutive fields fi and f j. Non-terminal
F(r,r. f ) refers to field f of record r. For
example, the rule FS(windSpeed1,min) ?
F(windSpeed1,max)FS(windSpeed1,max), spec-
ifies that we should talk about the field max of
record windSpeed1, after talking about the field
min. Analogously to the record level, we have also
included a special null field type for the emission
of words that do not correspond to a specific record
field. Rule (6) defines the expansion of field F to
a sequence of (binarized) words W, with a weight
equal to the bigram probability of the current word
given the previous word, the current record, and
field. This is an attempt at capturing contextual
dependencies between words over and above to
integrating a language model during decoding (see
Section 3.3).
Rules (8) and (9) define the emission of words and
integer numbers from W, given a field type and its
value. Rule (8) emits a single word from the vocabu-
lary of the training set. Its weight defines a multino-
mial distribution over all seen words, for every value
of field f , given that the field type is categorical or
the special null field. Rule (9) is identical but for
fields whose type is integer. Function g( f .v) gener-
ates an integer number given the field value, using
either of the following six ways (Liang et al, 2009):
identical to the field value, rounding up or rounding
down to a multiple of 5, rounding off to the clos-
est multiple of 5 and finally adding or subtracting
some unexplained noise.2 The weight is a multino-
mial over the six generation function modes, given
the record field f .
3.2 Hypergraph Construction
So far we have defined a probabilistic grammar
that captures the structure of a database d with
records and fields as intermediate non-terminals, and
words w (from the associated text) as terminals. Us-
ing this grammar and the CYK parsing algorithm,
we could obtain the top scoring derivation of records
and fields for a given input (i.e., a sequence of
2The noise is modeled as a geometric distribution.
755
S0,7
R0,2(start)
R0,1(start)
? ? ?
FS0,1(skyCover1,start)
R1,1(skyCover1.t)
R1,1(temp1.t)
FS0,1(temp1,start)
? ? ?
F0,1(skyCover1,%)
FS1,1(skyCover1,%)
F0,1(skyCover1,time)
FS1,1(skyCover1,time)
W0,1(skyCover1,%)
W0,1(skyCover1,time)
FS1,2(temp1,start)
R2,2(temp1.t)
FS1,2(skyCover1,start)
R2,2(skyCover1.t)
? ? ?
sunny
F1,2(temp1,min)
FS2,2(temp1,min)
W0,1(temp1,min) g0,1(min,v=10)
F1,2(temp1,max)
FS2,2(temp1,max)
W0,1(temp1,max) g0,1(max,v=20)
with
Figure 2: Partial hypergraph representation for the sentence ?Sunny with a low around 30 .? For the sake of readability,
we show a partial span on the first two words without weights on the hyperarcs.
words) as well as the optimal segmentation of the
text, provided we have a trained set of weights. The
inside-outside algorithm is commonly used for esti-
mating the weights of a PCFG. However, we first
transform the CYK parser and our grammar into
a hypergraph and then compute the weights using
inside-outside. Huang and Chiang (2005) define a
weighted directed hypergraph as follows:
Definition 1 An ordered hypergraph H is a tuple
?N,E, t,R?, where N is a finite set of nodes, E
is a finite set of hyperarcs and R is the set of
weights. Each hyperarc e ? E is a triple e =
?T (e),h(e), f (e)?, where h(e) ? N is its head node,
T (e) ? N? is a set of tail nodes and f (e) is a mono-
tonic weight function R|T (e)| to R and t ? N is a tar-
get node.
Definition 2 We impose the arity of a hyperarc to be
|e| = |T (e)| = 2, in other words, each head node is
connected with at most two tail nodes.
Given a context-free grammar G = ?N,T,P,S?
(where N is the set of variables, T the set of ter-
minals, P the set of production rules, and S ? N the
start symbol) and an input string w, we can map the
standard weighted CYK algorithm to a hypergraph
as follows. Each node [A, i, j] in the hypergraph
corresponds to non-terminal A spanning words wi
to w j of the input. Each rewrite rule A? BC in P,
with three free indices i < j < k, is mapped to
the hyperarc ?((B, i, j),(C, j,k)) ,(A, i,k), f ?, where
f = f ((B, i, j)) f ((C, j,k)) ?Pr(A? BC).3 The hy-
3Similarly, rewrite rules of type A? B are mapped to the
hyperarc ?(B, i, j),(A, i, j), f ?, with f = f ((B, i, j)) ?Pr(A? B).
pergraph can be thus viewed as a compiled lattice
of the corresponding chart graph. Figure 2 shows
an example hypergraph for a grammar defined on
database input similar to Figure (1b).
In order to learn the weights on the hyperarcs we
perform the following procedure iteratively in an
EM fashion (Li and Eisner, 2009). For each train-
ing scenario we build its hypergraph representation.
Next, we perform inference by calculating the in-
side and outside scores of the hypergraph, so as to
compute the posterior distribution over its hyperarcs
(E-step). Finally, we collectively update the posteri-
ors on the parameters-weights, i.e., rule probabilities
and emission multinomial distributions (M-step).
3.3 Decoding
In the framework outlined above, parsing an input
string w (given some learned weights) boils down
to traversing the hypergraph in a particular order.
(Note that the hypergraph should be acyclic, which
is always guaranteed by the grammar in Table 1). In
generation, our aim is to verbalize an input scenario
from a database d (see Figure 1). We thus find the
best text by maximizing:
argmax
w
P(w |d) = argmax
w
P(w) ?P(d |w) (1)
where P(d |w) is the decoding likelihood for a se-
quence of words w, P(w) is a measure of the qual-
ity of each output (given by a language model),
and P(w |d) the posterior of the best output for
database d. Note that calculating P(d |w) requires
deciding on the output length |w|. Rather than set-
756
ting w to a fixed length, we rely on a linear regres-
sion predictor that uses the counts of each record
type per scenario as features and is able to produce
variable length texts.
In order to perform decoding with an n-gram lan-
guage model, we adopt Huang and Chiang?s (2007)
dynamic-programming algorithm for SCFG-based
systems. Each node in the hypergraph is split into
a set of compound items, namely +LM items. Each
+LM item is of the form (na?b), where a and b are
boundary words of the generation string, and ? is a
place-holder symbol for an elided part of that string,
indicating a sub-generation part ranging from a to b.
An example +LM deduction of a single hyperarc of
the hypergraph in Figure 2 using bigrams is:
(2)
FS1,2(temp1,start)low : (w1,g1),
R2,2(temp1.t)around?degrees : (w2,g2)
R1,1(skyCover1.t)low?degrees : (w,g1g2)
w = w1 +w2 + ew +Plm(around | low) (3)
where w1,w2 are node weights, g1,g2 are the corre-
sponding sub-generations, ew is the weight of the hy-
perarc and w the weight of the resulting +LM item.
Plm and (na?b) are defined as in Chiang (2007) in a
generic fashion, allowing extension to an arbitrary
size of n-gram grammars.
Naive traversal of the hypergraph bottom-up
would explore all possible +LM deductions along
each hyperarc, and would increase decoding com-
plexity to an infeasible O(2nn2), assuming a trigram
model and a constant number of emissions at the ter-
minal nodes. To ensure tractability, we adopt cube
pruning, a popular approach in syntax-inspired ma-
chine translation (Chiang, 2007). The idea is to use a
beam-search over the intersection grammar coupled
with the cube-pruning heuristic. The beam limits the
number of derivations for each node, whereas cube-
pruning further limits the number of +LM items con-
sidered for inclusion in the beam. Since f (e) in Def-
inition 1 is monotonic, we can select the k-best items
without computing all possible +LM items.
Our decoder follows Huang and Chiang (2007)
but importantly differs in the treatment of leaf nodes
in the hypergraph (see rules (8) and (9)). In the
SCFG context, the Viterbi algorithm consumes ter-
minals from the source string in a bottom-up fashion
and creates sub-translations according to the CFG
rule that holds each time. In the concept-to-text
generation context, however, we do not observe the
words; instead, for each leaf node we emit the k-best
words from the underlying multinomial distribution
(see weights on rules (8) and (9)) and continue build-
ing our sub-generations bottom-up.
4 Experimental Design
Data We used our system to generate soccer com-
mentaries, weather forecasts, and spontaneous utter-
ances relevant to the air travel domain (examples
are given in Figure 1). For the first domain we
used the dataset of Chen and Mooney (2008), which
consists of 1,539 scenarios from the 2001?2004
Robocup game finals. Each scenario contains on av-
erage |d|= 2.4 records, each paired with a short sen-
tence (5.7 words). This domain has a small vocabu-
lary (214 words) and simple syntax (e.g., a transitive
verb with its subject and object). Records in this
dataset (henceforth ROBOCUP) were aligned man-
ually to their corresponding sentences (Chen and
Mooney, 2008). Given the relatively small size of
this dataset, we performed cross-validation follow-
ing previous work (Chen and Mooney, 2008; An-
geli et al, 2010). We trained our system on three
ROBOCUP games and tested on the fourth, averaging
over the four train/test splits.
For weather forecast generation, we used the
dataset of Liang et al (2009), which consists of
29,528 weather scenarios for 3,753 major US cities
(collected over four days). The vocabulary in this
domain (henceforth WEATHERGOV) is comparable
to ROBOCUP (345 words), however, the texts are
longer (|w| = 29.3) and more varied. On average,
each forecast has 4 sentences and the content selec-
tion problem is more challenging; only 5.8 out of
the 36 records per scenario are mentioned in the text
which roughly corresponds to 1.4 records per sen-
tence. We used 25,000 scenarios from WEATHER-
GOV for training, 1,000 scenarios for development
and 3,528 scenarios for testing. This is the same par-
tition used in Angeli et al (2010).
For the air travel domain we used the ATIS dataset
(Dahl et al, 1994), consisting of 5,426 scenar-
ios. These are transcriptions of spontaneous utter-
ances of users interacting with a hypothetical on-
757
WEATHERGOV ATIS ROBOCUP
1-
B
E
S
T Near 57. Near 57. Near 57. Near 57. Near
57. Near 57. Near 57. Near 57. Near 57.
Near 57. Near 57. South wind.
What what what what flights
from Denver Phoenix Pink9 to to Pink7 kicks
k-
B
E
S
T
As high as 23 mph. Chance of precipitation
is 20. Breezy, with a chance of showers.
Mostly cloudy, with a high near 57. South
wind between 3 and 9 mph.
Show me the flights from
Denver to Phoenix
Pink9 passes back to Pink7
A
N
G
E
L
I
A chance of rain or drizzle, with a high near
57. South wind between 3 and 9 mph.
Show me the flights leave
from Nashville to Phoenix
Pink9 kicks to Pink7
H
U
M
A
N
A slight chance of showers. Mostly cloudy,
with a high near 58. South wind between 3
and 9 mph, with gusts as high as 23 mph.
Chance of precipitation is 20%.
List flights from Denver to
Phoenix
Pink9 passes back to Pink7
Table 2: System output on WEATHERGOV, ATIS, and ROBOCUP (1-BEST, k-BEST, ANGELI) and corresponding
human-authored text (HUMAN).
line flight booking system. We used the dataset
introduced in Zettlemoyer and Collins (2007)4 and
automatically converted their lambda-calculus ex-
pressions to attribute-value pairs following the con-
ventions adopted by Liang et al (2009). For ex-
ample, the scenario in Figure 1(a) was initially
represented as: ?x. f light(x) ? f rom(x, phoenix) ?
to(x,new york)?day(x,sunday).5 In contrast to the
two previous datasets, ATIS has a much richer vo-
cabulary (927 words); each scenario corresponds
to a single sentence (average length is 11.2 words)
with 2.65 out of 19 record types mentioned on av-
erage. Following Zettlemoyer and Collins (2007),
we trained on 4,962 scenarios and tested on ATIS
NOV93 which contains 448 examples.
Model Parameters Our model has two parame-
ters, namely the number of k grammar derivations
considered by the decoder and the order of the
language model. We tuned k experimentally on
held-out data taken from WEATHERGOV, ROBOCUP,
and ATIS, respectively. The optimal value was k=15
for WEATHERGOV, k=25 for ROBOCUP, and k = 40
4The original corpus contains user utterances of single dia-
logue turns which would result in trivial scenarios. Zettlemoyer
and Collins (2007) concatenate all user utterances referring to
the same dialogue act, (e.g., book a flight), thus yielding more
complex scenarios with longer sentences.
5The resulting dataset and a technical report describ-
ing the mapping procedure in detail are available from
http://homepages.inf.ed.ac.uk/s0793019/index.php?
page=resources
for ATIS. For the ROBOCUP domain, we used a bi-
gram language model which was considered suffi-
cient given that the average text length is small. For
WEATHERGOV and ATIS, we used a trigram language
model.
System Comparison We evaluated two configu-
rations of our system. A baseline that uses the top
scoring derivation in each subgeneration (1-BEST)
and another version which makes better use of our
decoding algorithm and considers the best k deriva-
tions (i.e., 15 for WEATHERGOV, 40 for ATIS, and
25 for ROBOCUP). We compared our output to An-
geli et al (2010) whose approach is closest to ours
and state-of-the-art on the WEATHERGOV domain.
For ROBOCUP, we also compare against the best-
published results (Kim and Mooney, 2010).
Evaluation We evaluated system output automat-
ically, using the BLEU modified precision score
(Papineni et al, 2002) with the human-written text
as reference. In addition, we evaluated the gener-
ated text by eliciting human judgments. Participants
were presented with a scenario and its correspond-
ing verbalization and were asked to rate the latter
along two dimensions: fluency (is the text grammat-
ical and overall understandable?) and semantic cor-
rectness (does the meaning conveyed by the text cor-
respond to the database input?). The subjects used a
five point rating scale where a high number indicates
better performance. We randomly selected 12 doc-
758
ROBOCUP WEATHERGOV ATIS
System BLEU BLEU BLEU
1-BEST 10.79 8.64 11.85
k-BEST 30.90 33.70 29.30
ANGELI 28.70 38.40 26.77
KIM-MOONEY 47.27 ? ?
Table 3: BLEU scores on ROBOCUP (fixed content se-
lection), WEATHERGOV, and ATIS.
uments from the test set (for each domain) and gen-
erated output with our models (1-BEST and k-BEST)
and Angeli et al?s (2010) model (see Figure 2 for
examples of system output). We also included the
original text (HUMAN) as gold standard. We thus
obtained ratings for 48 (12 ? 4) scenario-text pairs
for each domain. The study was conducted over the
Internet using WebExp (Keller et al, 2009) and was
completed by 114 volunteers, all self reported native
English speakers.
5 Results
We conducted two experiments on the ROBOCUP do-
main. We first assessed the performance of our gen-
erator (k-BEST) on joint content selection and sur-
face realization and obtained a BLEU score of 24.88.
In comparison, the baseline?s (1-BEST) BLEU score
was 8.01. In a second experiment we forced the
generator to use the gold-standard records from the
database. This was necessary in order to compare
with previous work (Angeli et al, 2010; Kim and
Mooney, 2010).6 Our results are summarized in Ta-
ble 3. Overall, our generator performs better than
the baseline and Angeli et al (2010). We observe
a substantial increase in performance compared to
the joint content selection and surface realization
setting. This is expected as the generator is faced
with an easier task and there is less scope for error.
Our model does not outperform Kim and Mooney
(2010), however, this is not entirely surprising as
their model requires considerable more supervision
(e.g., during parameter initialization) and includes a
post-hoc re-ordering component.
6Angeli et al (2010) and Kim and Mooney (2010) fix con-
tent selection both at the record and field level. We let our gen-
erator select the appropriate fields, since these are at most two
per record type and this level of complexity can be easily tack-
led during decoding.
ROBOCUP WEATHERGOV ATIS
System F SC F SC F SC
1-BEST 2.47?? 2.33?? 1.82?? 2.05?? 2.40?? 2.46??
k-BEST 4.31? 3.96? 3.92? 3.30? 4.01 3.87
ANGELI 4.03?? 3.70?? 4.26? 3.60? 3.56?? 3.33??
HUMAN 4.47? 4.37? 4.61? 4.03? 4.10 4.01
Table 4: Mean ratings for fluency (F) and semantic cor-
rectness (SC) on system output elicited by humans on
ROBOCUP, WEATHERGOV, and ATIS (?: sig. diff. from
HUMAN; ?: sig. diff. from k-BEST.)
With regard to WEATHERGOV, our generator im-
proves over the baseline but lags behind Angeli et
al. (2010). Since our system emits words based on
a language model rather than a template, it displays
more freedom in word order and lexical choice, and
is thus penalized by BLEU when creating output that
is overly distinct from the reference. On ATIS, our
model outperforms both the baseline and Angeli et
al. This is the most challenging domain with re-
gard to surface realization with a vocabulary larger
than ROBOCUP and WEATHERGOV by factors of 2.7
and 4.3, respectively.
The results of our human evaluation study are
shown in Table 3. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
type (1-BEST, k-BEST, ANGELI, and HUMAN) on the
fluency and semantic correctness ratings. Means
differences were compared using a post-hoc Tukey
test. On ROBOCUP, our system (k-BEST) is signif-
icantly better than the baseline (1-BEST) and AN-
GELI both in terms of fluency and semantic correct-
ness (a < 0.05). On WEATHERGOV, our generator
performs comparably to ANGELI on fluency and se-
mantic correctness (the differences in the means are
not statistically significant); 1-BEST is significantly
worse than 15-BEST and ANGELI (a < 0.05). On
ATIS, k-BEST is significantly more fluent and seman-
tically correct than 1-BEST and ANGELI (a < 0.01).
There was no statistically significant difference be-
tween the output of our system and the original ATIS
sentences.
In sum, we observe that taking the k-best deriva-
tions into account boosts performance (the 1-BEST
system is consistently worse). Our model is on par
with ANGELI on WEATHERGOV but performs better
on ROBOCUP and ATIS when evaluated both auto-
759
matically and by humans. In general, a large part of
our output resembles the human text, which demon-
strates that our simple language model yields coher-
ent sentences (without any template engineering), at
least for the domains under consideration.
6 Conclusions
We have presented an end-to-end generation system
that performs both content selection and surface re-
alization. Central to our approach is the encoding
of generation as a parsing problem. We reformulate
the input (a set of database records and text describ-
ing some of them) as a PCFG and show how to find
the best derivation using the hypergraph framework.
Despite its simplicity, our model is able to obtain
performance comparable to the state of the art. We
argue that our approach is computationally efficient
and viable in practical applications. Porting the sys-
tem to a different domain is straightforward, assum-
ing a database and corresponding (unaligned) text.
As long as the database is compatible with the struc-
ture of the grammar in Table 1, we need only retrain
to obtain the weights on the hyperarcs and a domain
specific language model.
Our model takes into account the k-best deriva-
tions at decoding time, however inspection of these
shows that it often fails to select the best one. In
the future, we plan to remedy this by using forest
reranking, a technique that approximately reranks
a packed forest of exponentially many derivations
(Huang, 2008). We would also like to scale our
model to more challenging domains (e.g., product
descriptions) and to enrich our generator with some
notion of discourse planning. An interesting ques-
tion is how to extend the PCFG-based approach ad-
vocated here so as to capture discourse-level docu-
ment structure.
Acknowledgments We are grateful to Percy Liang
and Gabor Angeli for providing us with their code
and data. We would also like to thank Luke Zettle-
moyer and Tom Kwiatkowski for sharing their ATIS
dataset with us and Frank Keller for his feedback on
an earlier version of this paper.
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502?512, Cambridge, MA.
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive content selection for concept-to-text generation.
In Proceedings of Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 331?338, Vancouver, British Columbia.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 14(4):431?455.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128?135, Helsinki, Finland.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis task:
the atis-3 corpus. In Proceedings of the Workshop on
Human Language Technology, pages 43?48, Plains-
boro, NJ.
Pablo A. Duboue and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Pro-
ceedings of International Natural Language Genera-
tion, pages 89?96, Ramapo Mountains, NY.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and appli-
cations. Discrete Applied Mathematics, 42:177?201.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International Work-
shop on Parsing Technology, pages 53?64, Vancouver,
British Columbia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio.
Frank Keller, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of Web ex-
periments: A case study using the WebExp software
package. Behavior Research Methods, 41(1):1?12.
760
Joohyun Kim and Raymond Mooney. 2010. Generative
alignment and semantic parsing for learning from am-
biguous supervision. In Proceedings of the 23rd Con-
ference on Computational Linguistics, pages 543?551,
Beijing, China.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the 7th Interna-
tional Workshop on Parsing Technologies, pages 123?
134, Beijing, China.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 40?51, Suntec, Sin-
gapore.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In roceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 91?99, Suntec, Singapore.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-
to-string model for language generation from typed
lambda calculus expressions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 1611?1622, Edinburgh,
UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian
Davy. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?179, Rochester, NY.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 678?687, Prague, Czech Republic.
761
Proceedings of NAACL-HLT 2013, pages 847?857,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Quantum-Theoretic Approach to Distributional Semantics
William Blacoe1, Elham Kashefi2, Mirella Lapata1
1Institute for Language, Cognition and Computation,
2Laboratory for Foundations of Computer Science
School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB
w.b.blacoe@sms.ed.ac.uk, ekashefi@inf.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we explore the potential of
quantum theory as a formal framework for
capturing lexical meaning. We present a
novel semantic space model that is syntacti-
cally aware, takes word order into account,
and features key quantum aspects such as
superposition and entanglement. We define
a dependency-based Hilbert space and show
how to represent the meaning of words by den-
sity matrices that encode dependency neigh-
borhoods. Experiments on word similarity
and association reveal that our model achieves
results competitive with a variety of classical
models.
1 Introduction
The fields of cognitive science and natural language
processing have recently produced an ensemble of
semantic models which have an impressive track
record of replicating human behavior and enabling
real-world applications. Examples include simula-
tions of word association (Denhie`re and Lemaire,
2004; Griffiths et al, 2007), semantic priming (Lund
and Burgess, 1996; Landauer and Dumais, 1997;
Griffiths et al, 2007), categorization (Laham, 2000),
numerous studies of lexicon acquisition (Grefen-
stette, 1994; Lin, 1998), word sense discrimination
(Schu?tze, 1998), and paraphrase recognition (Socher
et al, 2011). The term ?semantic? derives from the
intuition that words seen in the context of a given
word contribute to its meaning (Firth, 1957). Al-
though the specific details of the individual models
differ, they all process a corpus of text as input and
represent words (or concepts) in a (reduced) high-
dimensional space.
In this paper, we explore the potential of quan-
tum theory as a formal framework for capturing lex-
ical meaning and modeling semantic processes such
as word similarity and association (see Section 6
for an overview of related research in this area).
We use the term quantum theory to refer to the ab-
stract mathematical foundation of quantum mechan-
ics which is not specifically tied to physics (Hughes,
1989; Isham, 1989). Quantum theory is in prin-
ciple applicable in any discipline where there is a
need to formalize uncertainty. Indeed, researchers
have been pursuing applications in areas as diverse
as economics (Baaquie, 2004), information theory
(Nielsen and Chuang, 2010), psychology (Khren-
nikov, 2010; Pothos and Busemeyer, 2012), and cog-
nitive science (Busemeyer and Bruza, 2012; Aerts,
2009; Bruza et al, 2008). But what are the features
of quantum theory which make it a promising frame-
work for modeling meaning?
Superposition, entanglement, incompatibility,
and interference are all related aspects of quantum
theory, which endow it with a unique character.1 Su-
perposition is a way of modeling uncertainty, more
so than in classical probability theory. It contains in-
formation about the potentialities of a system?s state.
An electron whose location in an atom is uncertain
can be modeled as being in a superposition of loca-
tions. Analogously, words in natural language can
have multiple meanings. In isolation, the word pen
may refer to a writing implement, an enclosure for
confining livestock, a playpen, a penitentiary or a fe-
male swan. However, when observed in the context
of the word ink the ambiguity resolves into the sense
of the word dealing with writing. The meanings of
words in a semantic space are superposed in a way
which is intuitively similar to the atom?s electron.
Entanglement concerns the relationship between
1It is outside the scope of the current paper to give a detailed
introduction on the history of quantum mechanics. We refer
the interested reader to Vedral (2006) and Kleppner and Jackiw
(2000) for comprehensive overviews.
847
systems for which it is impossible to specify a joint
probability distribution from the probability distri-
butions of their constituent parts. With regard to
word meanings, entanglement encodes (hidden) re-
lationships between concepts. The different senses
of a word ?exist in parallel? until it is observed
in some context. This reduction of ambiguity has
effects on other concepts connected via entangle-
ment. The notion of incompatibility is fundamen-
tal to quantum systems. In classical systems, it is
assumed by default that measurements are compati-
ble, that is, independent, and as a result the order in
which these take place does not matter. By contrast
in quantum theory, measurements may share (hid-
den) order-sensitive inter-dependencies and the out-
come of the first measurement can change the out-
come of the second measurement.
Interference is a feature of quantum probability
that can cause classical assumptions such as the law
of total probability to be violated. When concepts
interact their joint representation can exhibit non-
classical behavior, e.g., with regard to conjunction
and disjunction (Aerts, 2009). An often cited ex-
ample is the ?guppy effect?. Although guppy is an
example of a pet-fish it is neither a very typical pet
nor fish (Osherson and Smith, 1981).
In the following we use the rich mathematical
framework of quantum theory to model semantic in-
formation. Specifically, we show how word mean-
ings can be expressed as quantum states. A word
brings with it its own subspace which is spanned by
vectors representing its potential usages. We present
a specific implementation of a semantic space that is
syntactically aware, takes word order into account,
and features key aspects of quantum theory. We em-
pirically evaluate our model on word similarity and
association and show that it achieves results com-
petitive with a variety of classical models. We be-
gin by introducing some of the mathematical back-
ground needed for describing our approach (Sec-
tion 2). Next, we present our semantic space model
(Section 3) and our evaluation experiments (Sec-
tions 4 and 5). We conclude by discussing related
work (Section 6).
2 Preliminaries
Let c = r ei? be a complex number, expressed in po-
lar form, with absolute value r = |c| and phase ?. Its
complex conjugate c? = r e?i? has the inverse phase.
Thus, their product cc? = (r ei?)(r e?i?) = r2 is real.
2.1 Vectors
We are interested in finite-dimensional, complex-
valued vector spaces Cn with an inner product, oth-
erwise known as Hilbert space. A column vector
??? ? Cn can be written as an ordered vertical array
of its n complex-valued components, or alternatively
as a weighted sum of base vectors:
???=
?
?
?
?
?1
?2
...
?n
?
?
?
?=?1
?
?
?
?
1
0
...
0
?
?
?
?+ . . .+?n
?
?
?
?
0
...
0
1
?
?
?
? (1)
Whereas Equation (1) uses base vectors from the
standard base Bstd = {
??
b1 , ...,
??
bn}, any other set of n
orthonormal vectors serves just as well as a base for
the same space. Dirac (1939) introduced the so-
called bra-ket notation which is equally expressive
but notationally more convenient. A column vector
becomes a ket:
??? ? |??= ?1|b1?+?2|b2?+ . . .+?n|bn? (2)
and a row vector becomes a bra ??|. Transposing
a complex-valued vector or matrix (via the super-
script ???) involves complex-conjugating all compo-
nents:
|??? = ??|= ??1?b1|+?
?
2?b2|+ . . .+?
?
n?bn| (3)
The Dirac notation for the inner product ??|?? il-
lustrates the origin of the terminology ?bra-ket?.
Since Bstd?s elements are normalised and pairwise
orthogonal their inner product is:
?bi|b j?=
{
1, if i = j
0, otherwise
(4)
The inner product is also applicable to pairs of non-
base kets:
(??1 ?
?
2 ? ? ? ?
?
n)
?
?
?
?
?1
?2
...
?n
?
?
?
?? ??|??
= (?i?
?
i ?bi|)
(
? j ? j|b j?
)
= ?i, j?
?
i ? j?bi|b j?= ?i?
?
i ?i?bi|bi?
= ?i?
?
i ?i
(5)
848
Reversing the order of an inner product complex-
conjugates it:
(??|??)? = ??|?? (6)
2.2 Matrices
Matrices are sums of outer products |????|. For ex-
ample, the matrix (Mi, j)i, j can be thought of as
the weighted sum of ?base-matrices? Bi, j ? |bi??b j|,
whose components are all 0 except for a 1 in the i-th
row and j-th column. The outer product extends lin-
early to non-base kets in the following manor:
|????|= (?i?i|bi?)
(
? j ?
?
j?b j|
)
= ?i, j?i?
?
j |bi??b j|
(7)
This is analogous to the conventional multiplication:
?
?
?
?1
...
?n
?
?
?(??1 ? ? ? ?
?
n)=
?
?
?
?1??1 ? ? ? ?1?
?
n
...
. . .
...
?n??1 ? ? ? ?n?
?
n
?
?
? (8)
We will also make use of the tensor product. Its ap-
plication to kets, bras and outer products is linear:
(|a?+ |b?)?|c?= |a?? |c?+ |b?? |c?
(?a|+ ?b|)??c|= ?a|? ?c|+ ?b|? ?c|
(|a??b|+ |c??d|)?|e?? f |=
(|a?? |e?)(?b|? ? f |)+(|c?? |e?)(?d|? ? f |)
(9)
For convenience we omit ??? where no confusion
arises, e.g., |a? ? |b? = |a?|b?. When applied to
Hilbert spaces, the tensor product creates the com-
posed Hilbert space H = H1? ...?Hn whose base
kets are simply induced by the tensor product of its
subspaces? base kets:
base(H1? ...?Hn) =
{
nO
i=1
|b?i : |b?i ? base(Hi), 1? i? n
} (10)
Whereas the order of composed kets |a?|b?|c? usu-
ally suffices to identify which subket lives in which
subspace, we make this explicit by giving subkets
the same subscript as the corresponding subspace.
Thus, the order no longer matters, as in the follow-
ing inner product of composed kets:
(?a|1?b|2?c|3)(|e?3|d?1| f ?2) = ?a|d??b| f ??c|e? (11)
Definition 1. Self-adjoint Matrix
A matrix M is self-adjoint iff Mi, j = M?j,i for all i, j.
Consequently, all diagonal elements are real-valued,
and M = M? is its own transpose conjugate.
Definition 2. Density Matrix
A self-adjoint matrix M is a density matrix iff
it is positive semi-definite, i.e., ??|M|?? ? 0 for
all |?? ? Cn, and it has unit trace, i.e., Tr(M) =
?|b??B ?b|M|b?= 1.
The term ?density matrix? is synonymous with
?density operator?. Any density matrix ? can
be decomposed arbitrarily as ? = ?i pi|si??si|, the
weighted sum of sub-matrices |si??si| with pi ? R>0
and ?si|si?= 1. The pi need not sum to 1. In fact the
decomposition where the pi sum to 1 and the |si? are
mutually orthogonal is unique and is called the eigen
decomposition. Consequently Beig = {|si?}i consti-
tutes an orthonormal base, ??s so-called eigen base.
Density operators are used in quantum theory to de-
scribe the state of some system. If the system?s state
? is certain we call it a pure state and write ?= |s??s|
for some unit ket |s?. Systems whose state is uncer-
tain are described by a mixed state ? = ?i pi|si??si|
which represents an ensemble of substates or pure
states {(pi,si)}i where the system is in substate si
with probability pi. Hence, the term ?density? as in
probability density.
It is possible to normalize a density matrix
without committing to any particular decomposi-
tion. Only the trace function is required, because
norm(?) = ?/Tr(?). Definition 2 mentions what the
trace function does. However, notice that the same
result is produced for any orthonormal base B , in-
cluding ??s eigen base Beig = {|ei?}i. Even though
we do not know the content of Beig, we know that it
849
exists. So we use it to show that dividing ? by:
Tr(?) = Tr(?i pi|ei??ei|)
= ? j?e j|(?i pi|ei??ei|)|e j?
= ?i, j pi?e j|ei??ei|e j?
= ?i pi?ei|ei??ei|ei?= ?i pi
(12)
normalizes its probability distribution over eigen
kets:
?
Tr(?) =
?i pi|ei??ei|
? j p j
=
?i
pi
? j p j
|ei??ei|
(13)
3 Semantic Space Model
We represent the meaning of words by density ma-
trices. Specifically, a lexical item w is modeled as
an ensemble Uw = {(pi,ui)}i of usages ui and the
corresponding probabilities pi that w gets used ?in
the i-th manor?. A word?s usage is comprised of
distributional information about its syntactic and se-
mantic preferences, in the form of a ket |ui?. The
density matrix ?w = ?i pi|ui??ui| represents the en-
semble Uw. This section explains our method of ex-
tracting lexical density matrices from a dependency-
parsed corpus. Once density matrices have been
learned, we can predict the expected usage similar-
ity of two words as a simple function of their density
matrices. Our explication will be formally precise,
but at the same time illustrate each principle through
a toy example.
3.1 Dependency Hilbert Space
Our model learns the meaning of words from a
dependency-parsed corpus. Our experiments have
used the Stanford parser (de Marneffe and Man-
ning, 2008), however any other dependency parser
with broadly similar output could be used instead.
A word?s usage is learned from the type of depen-
dency relations it has with its immediate neighbors
in dependency graphs. Its semantic content is thus
approximated by its ?neighborhood?, i.e., its co-
occurrence frequency with neighboring words.
Neighborhoods are defined by a vocabu-
lary V = {w1, ...,wnV } of the nV most fre-
quent (non-stop) words in the corpus. Let
Rel = {sub?1,dobj?1,amod,num,poss, ...} denote
Document 1:
(1a)
the man see two angry jaguar
det subj
dobj
num
anod
(1b)
we see two angry elephant
subj
dobj
num
amod
(1c)
two elephant run
num nsubj
Document 2:
(2a)
she buy a nice new jaguar
subj
dobj
det
amod
amod
(2b)
I like my jaguar
subj
dobj
poss
Figure 1: Example dependency trees in a toy corpus. Dot-
ted arcs are ignored because they are either not connected
to the target words jaguar and elephant or because their
relation is not taken into account in constructing the se-
mantic space. Words are shown as lemmas.
a subset of all dependency relations provided by
the parser and their inverses. The choice of Rel is a
model parameter. We considered only the most fre-
quently occuring relations above a certain threshold,
which turned out to be about half of the full inven-
tory. Relation symbols with the superscript ??1?
indicate the inversion of the dependency direction
(dependent to head). All other relation symbols
have the conventional direction (head to dependent).
Hence, w
xyz
? v is equivalent to v
xyz?1
? w. We then
partition Rel into disjoint clusters of syntactically
similar relations Part = {RC1, ...,RCnPart}. For
example, we consider syntactically similar relations
which connect target words with neighbors with
the same part of speech. Each relation cluster RCk
is assigned a Hilbert space Hk whose base kets
{|w(k)j ?} j correspond to the words in V = {w j} j.
Figure 1 shows the dependency parses for a
toy corpus consisting of two documents and five
sentences. To create a density matrix for the target
words jaguar and elephant, let us assume that we
850
will consider the following relation clusters:
RC1 = {dobj?1,iobj?1,agent?1,nsubj?1, ...},
RC2 = {advmod,amod,tmod, ...} and RC3 = {nn,
appos,num,poss, ...}.
3.2 Mapping from Dependency Graphs to Kets
Next, we create kets which encode syntactic and se-
mantic relations as follows. For each occurrence of
the target word w in a dependency graph, we only
consider the subtree made up of w and the immedi-
ate neighbors connected to it via a relation in Rel.
In Figure 1, arcs from the dependency parse that we
ignore are shown as dotted. Let the subtree of in-
terest be st = {(RC1,v1), ...,(RCnPart ,vnPart )}, that is,
w is connected to vk via some relation in RCk, for
k ? {1, ...,nPart}. For any relation cluster RCk that
does not feature in the subtree, let RCk be paired with
the abstract symbol w /0 in st. This symbol represents
uncertainty about a potential RCk-neighbor.
We convert all subtrees st in the corpus for the tar-
get word w into kets |?st? ? H1? ...?HnPart . These
in turn make up the word?s density matrix ?w. Be-
fore we do so, we assign each relation cluster RCk
a complex value ?k = ei?k . The idea behind these
values is to control for how much each subtree con-
tributes to the overall density matrix. This becomes
more apparent after we formulate our method of in-
ducing usage kets and density matrices.
|?st?= ?st
O
(RCk,v)?st
|v?k, (14)
where ?st = ?(RCk,v)?st,v 6=w /0 ?k. Every RCk paired
with some neighbor v ? V induces a basic subket
|v?k ? base(Hk), i.e., a base ket of the k-th sub-
space or subsystem. All other subkets |w /0?k =
?v?V |V |
? 12 |v?k are in a uniformly weighted super-
position of all base kets. The factor |V |?
1
2 ensures
that ?w /0|w /0? = 1. The composed ket for the sub-
tree st is again weighted by the complex-valued ?st .
?st is the sum of complex values ?k = ei?k , each
with absolute value 1. Therefore, its own abso-
lute value depends highly on the relative orienta-
tion ?k among its summands: equal phases reinforce
absolute value, but the more phases are opposed
(i.e., their difference approaches pi), the more they
cancel out the sum?s absolute value. Only those ?k
contribute to this sum whose relation cluster is not
paired with w /0. The choice of the parameters ?k al-
lows us to put more weight on some combinations
(a)
??st1 | ??st2 | ??st3 |
|?st1?
|?st2?
|?st3?
6= 0 6= 0 6= 0
6= 0 6= 0 6= 0
6= 0 6= 0 6= 0
(b)
??st1 | ??st2 | ??st3 |
|?st1?
|?st2?
|?st3?
6= 0 0 0
0 6= 0 0
0 0 6= 0
Figure 2: Excerpts of density matrices that result from
the dependency subtrees st1,st2,st3. Element mi, j in row i
and column j is mi, j|?sti???st j | in Dirac notation. (a) All
three subtrees are in the same document. Thus their kets
contribute to diagonal and off-diagonal matrix elements.
(b) Each subtree is in a separate document. Therefore
their kets do not group, affecting only diagonal matrix
elements.
of dependency relations than others.
Arbitrarily choosing ?1 = pi4 , ?2 =
7pi
4 ,
and ?3 = 3pi4 renders the subtrees in Fig-
ure 1 as |?st1a? = e
ipi/4|see?1|angry?2|two?3,
|?st2a?=
?
2|buy?1(|nice?2 + |new?2)|w /0?3, |?st2b?=?
2eipi/2|like?1|w /0?2|my?3, which are relevant for
jaguar, and |?st1b? = e
ipi/4|see?1|angry?2|two?3,
|?st1c? =
?
2eipi/2|run?1|w /0?2|two?3, which are
relevant for elephant. The subscripts outside of the
subkets correspond to those of the relation clusters
RC1,RC2,RC3 chosen in Section 3.1.
In sentence 2a, jaguar has two neighbors under
RC2. Therefore the subket from H2 is a superpo-
sition of the base kets |nice?2 and |new?2. This is
a more intuitive formulation of the equivalent ap-
proach which first splits the subtree for buy nice new
jaguar into two similar subtrees for buy nice jaguar
and for buy new jaguar, and then processes them as
seperate subtrees within the same document.
3.3 Creating Lexical Density Matrices
We assume that a word?s usage is uniform through-
out the same document. In our toy corpus in Fig-
ure 1, jaguar is always the direct object of the main
verb. However, in Document 1 it is used in the an-
imal sense, whereas in Document 2 it is used in the
car sense. Even though the usage of jaguar in sen-
tence (2b) is ambiguous, we group it with that of
sentence (2a).
These considerations can all be comfortably en-
851
? jaguar = (|?st1a???st1a |+(|?st2a?+ |?st2b?)(??st2a |+ ??st2b |))/7 =
0.14|see?1|angry?2|two?3?see|1?angry|2?two|3+ 0.29|buy?1|nice?2|w /0?3?buy|1?nice|2?w /0|3+
0.29|buy?1|nice?2|w /0?3?buy|1?new|2?w /0|3+ 0.29|buy?1|new?2|w /0?3?buy|1?nice|2?w /0|3+
0.29|buy?1|new?2|w /0?3?buy|1?new|2?w /0|3+ 0.29epi/2|like?1|w /0?2|my?3?buy|1?nice|2?w /0|3+
0.29epi/2|like?1|w /0?2|my?3?buy|1?new|2?w /0|3+ 0.29e?pi/2|buy?1|nice?2|w /0?3?like|1?w /0|2?my|3+
0.29e?pi/2|buy?1|new?2|w /0?3?like|1?w /0|2?my|3+ 0.29|like?1|w /0?2|my?3?like|1?w /0|2?my|3
?elephant = ((|?st1b?+ |?st1c?)(??st1b |+ ??st1c |))/3 =
0.33|see?1|angry?2|two?3?see|1?angry|2?two|3+ 0.47epi/4|run?1|w /0?2|two?3?see|1?angry|2?two|3+
0.47e?pi/4|see?1|angry?2|two?3?run|1?w /0|2?two|3+ 0.67|run?1|w /0?2|two?3?run|1?w /0|2?two|3
Tr(? jaguar?elephant)=Tr(0.05|?st1a???st1b |+0.05e
ipi/4|?st1a???st1c |)=Tr(0.05|see?1|angry?2|two?3?see|1?angry|2?two|3+
0.07e?pi/4|see?1|angry?2|two?3?run|1?w /0|2?two|3) = ?
|b??base(H1?H2?H3)
?b|(0.05|see?1|angry?2|two?3?see|1?angry|2?two|3+
0.07e?pi/4|see?1|angry?2|two?3?run|1?w /0|2?two|3)|b?= 0.05
Figure 3: Lexical density matrices for the words jaguar and elephant and their similarity.
coded in a density matrix. This is simply gener-
ated via the outer product of our subtree kets |?st?.
For example, ?D1, jaguar = |?st1a???st1a | represents
the contribution that document D1 makes to ? jaguar.
Document D2, however, has more than one ket
relevant to ? jaguar. Due to our assumption of
document-internal uniformity of word usage, we
group D2?s subtree-kets additively: ?D2, jaguar =
(|?st2a?+ |?st2b?)(??st2a |+??st2b |). The target word?s
density matrix ?w is the normalized sum of all den-
sity matrices ?D,w obtained from each D:
?D,w =
(
?
st?STD,w
|?st?
)(
?
st?STD,w
??st |
)
(15)
where STD,w is the set of all subtrees for target
word w in document D. To illustrate the differ-
ence that this grouping makes, consider the den-
sity matrices in Figure 2. Whereas in (a) the sub-
trees st1,st2,st3 share a document, in (b) they are
from distinct documents. This grouping causes them
to not only contribute to diagonal matrix elements,
e.g., |?st2???st2 |, as in (b), but also to off diagonal
ones, e.g., |?st2???st1 |, as in (a).
Over the course of many documents the summa-
tion of all contributions, no matter how small or
large the groups are, causes ?clusters of weight?
to form, which hopefully coincide with word us-
ages. As mentioned in Section 3.2, adding complex-
valued matrix elements increases or decreases the
sum?s absolute value depending on relative phase
orientation. This makes it possible for interference
to occur. Since the same word appears in varying
contexts, the corresponding complex-valued outer
products interact upon summation. Finally, the den-
sity matrix gets normalized, i.e., divided by its trace.
This leaves the distributional information intact and
merely normalizes the probabilities. Figure 3 illus-
trates the estimation of the density matrices for the
words jaguar and elephant from the toy corpus in
Figure 1.
3.4 Usage Similarity
Decomposing the density matrix of the target
word w, ?w =?i pi|ui??ui| recovers the usage ensem-
ble Uw = {(pi,ui)}i. However, in general there are
infinitely many possible ensembles which ?w might
represent. This subsection explains our metric for
estimating the usage similarity of two words. The
math involved shows that we can avoid the question
of how to best decompose ?w.
We compute the usage similarity of two words w
and v by comparing each usage of w with each us-
age of v and weighting these similarity values with
the corresponding usage probabilities. Let ?w =
?i p
(w)
i |u
(w)
i ??u
(w)
i | and ?v = ?i p
(v)
i |u
(v)
i ??u
(v)
i |. The
similarity of some usage kets |u(w)i ? and |u
(v)
j ? is ob-
tained, as is common in the literature, by their in-
ner product ?u(w)i |u
(v)
j ?. However, as this is a com-
plex value, we multiply it with its complex conju-
gate, rendering the real value ?u(v)j |u
(w)
i ??u
(w)
i |u
(v)
j ?=
|?u(w)i |u
(v)
j ?|
2. Therefore, in total the expected simi-
larity of w and v is:
852
(16)sim(w,v) =?
i, j
p(w)i p
(v)
j ?u
(v)
j |u
(w)
i ??u
(w)
i |u
(v)
j ?
= Tr
(
?
i, j
p(w)i p
(v)
j |u
(w)
i ??u
(w)
i |u
(v)
j ??u
(v)
j |
)
=
Tr
(
(?
i
p(w)i |u
(w)
i ??u
(w)
i |)(?
j
p(v)j |u
(v)
j ??u
(v)
j |)
)
= Tr(?w?v)
We see that the similarity function simply reduces to
multiplying ?w with ?v and applying the trace func-
tion. The so-called cyclic property of the trace func-
tion (i.e., Tr(M1M2)= Tr(M2M1) for any two matri-
ces M1,M2) gives us the corollary that this particular
similarity function is symmetric.
Figure 3 (bottom) shows how to calculate the sim-
ilarity of jaguar and elephant. Only the coefficient
of the first outer product survives the tracing pro-
cess because its ket and bra are equal modulo trans-
pose conjugate. As for the second outer product,
0.05eipi/4?b|?st1a???st1c |b? is 0 for all base kets |b?.
3.5 What Does This Achieve?
We represent word meanings as described above for
several reasons. The density matrix decomposes into
usages each of which are a superposition of combi-
nations of dependents. Internally, these usages are
established automatically by way of ?clustering?.
Our model is parameterized with regard to the
phases of sub-systems (i.e., clusters of syntactic re-
lations) which allows us to make optimal use of in-
terference, as this plays a large role in the over-
all quality of representation. It is possible for a
combination of (groups of) dependents to get en-
tangled if they repeatedly appear together under the
same word, and only in that combination. If the
co-occurence of (groups of) dependents is uncorre-
lated, though, they remain unentangled. Quantum
entanglement gives our semantic structures the po-
tential for long-distance effects, once quantum mea-
surement becomes involved. This is in analogy to
the nonlocal correlation between properties of sub-
atomic particles, such as the magnetic spin of elec-
trons or the polarization of photons. Such an exten-
sion to our implementation will also uncover which
sets of measurements are order-sensitive, i.e., in-
compatible.
Our similarity metric allows two words to ?select?
each other?s usages via their pairwise inner prod-
ucts. Usage pairs with a high distributional simi-
larity roughly ?align? and then get weighted by the
probabilities of those usages. Two words are similar
if they are substitutable, that is, if they can be used
in the same syntactic environment and have the same
meaning. Hopefully, this leads to more accurate es-
timation of distributional similarity and can be used
to compute word meaning in context.
4 Experimental Setup
Data All our experiments used a dependency
parsed and lemmatized version of the British Na-
tional Corpus (BNC). As mentioned in Section 3, we
obtained dependencies from the output of the Stan-
ford parser (de Marneffe and Manning, 2008). The
BNC comprises 4,049 texts totalling approximately
100 million words.
Evaluation Tasks We evaluated our model on
word similarity and association. Both tasks are em-
ployed routinely to assess how well semantic models
predict human judgments of word relatedness. We
used the WordSim353 test collection (Finkelstein et
al., 2002) which consists of similarity judgments for
word pairs. Participants gave each pair a similar-
ity rating using a 0 to 10 scale (e.g., tiger?cat are
very similar, whereas delay?racism are not). The
average rating for each pair represents an estimate of
the perceived similarity of the two words. The col-
lection contains ratings for 437 unique words (353
pairs) all of which appeared in our corpus. Word as-
sociation is a slightly different task: Participants are
given a cue word (e.g., rice) and asked to name an
associate in response (e.g., Chinese, wedding, food,
white). We used the norms collected by Nelson et
al. (1998). We estimated the strength of association
between a cue and its associate, as the relative fre-
quency with which it was named. The norms con-
tain 9,968 unique words (70,739 pairs) out of which
9,862 were found in our corpus, excluding multi-
word expressions.
For both tasks, we used correlation analysis to ex-
amine the degree of linear relationship between hu-
man ratings and model similarity values. We report
correlation coefficients using Spearman?s rank cor-
relation coefficient.
Quantum Model Parameters The quantum
framework presented in Section 3 is quite flexible.
Depending on the choice of dependency rela-
tions Rel, dependency clusters RC j, and complex
853
values ? j = ei? j , different classes of models can be
derived. To explore these parameters, we partitioned
the WordSim353 dataset and Nelson et al?s (1998)
norms into a development and test set following
a 70?30 split. We tested 9 different intuitively
chosen relation partitions {RC1, ...,RCnPart}, cre-
ating models that considered only neighboring
heads, models that considered only neighboring
dependents, and models that considered both. For
the latter two we experimented with partitions of
one, two or three clusters. In addition to these more
coarse grained clusters, for models that included
both heads and dependents we explored a partition
with twelve clusters broadly corresponding to
objects, subjects, modifiers, auxiliaries, determiners
and so on. In all cases stopwords were not taken
into account in the construction of the semantic
space.
For each model variant we performed a grid
search over the possible phases ? j = kpi with range
k = 04 ,
1
4 , ...,
7
4 for the complex-valued ? j assigned
to the respective relation cluster RC j (see Section
3.2 for details). In general, we observed that the
choice of dependency relations and their clustering
as well as the phases assigned to each cluster greatly
influenced the semantic space. On both tasks, the
best performing model had the relation partition de-
scribed in Section 3.1. Section 5 reports our results
on the test set using this model.
Comparison Models We compared our quantum
space against three classical distributional models.
These include a simple semantic space, where a
word?s meaning is a vector of co-occurrences with
neighboring words (Mitchell and Lapata, 2010), a
syntax-aware space based on weighted distributional
triples that encode typed co-occurrence relations
among words (Baroni and Lenci, 2010) and word
embeddings computed with a neural language model
(Bengio, 2001; Collobert and Weston, 2008) For all
three models we used parameters that have been re-
ported in the literature as optimal.
Specifically, for the simple co-occurrence-based
space we follow the settings of Mitchell and Lapata
(2010): a context window of five words on either
side of the target word and 2,000 vector dimensions
(i.e., the 2000 most common context words in the
BNC). Vector components were set to the ratio of
the probability of the context word given the target
word to the probability of the context word overall.
For the neural language model, we adopted the best
Models WordSim353 Nelson Norms
SDS 0.433 0.151
DM 0.318 0.123
NLM 0.196 0.091
QM 0.535 0.185
Table 1: Performance of distributional models on Word-
Sim353 dataset and Nelson et al?s (1998) norms (test
set). Correlation coefficients are all statistically signifi-
cant (p < 0.01).
performing parameters from our earlier comparison
of different vector sources for distributional seman-
tics (Blacoe and Lapata, 2012) where we also used
the BNC for training. There we obtained best results
with 50 dimensions, a context window of size 4,
and an embedding learning rate of 10?9. Our third
comparison model uses Baroni and Lenci?s (2010)
third-order tensor2 which they obtained from a very
large dependency-parsed corpus containing approxi-
mately 2.3 billion words. Their tensor assigns a mu-
tual information score to instances of word pairs w,v
and a linking word l. We obtained vectors ??w from
the tensor following the methodology proposed in
Blacoe and Lapata (2012) using 100 (l,v) contexts
as dimensions.
5 Results
Our results are summarized in Table 1. As can
be seen, the quantum model (QM) obtains perfor-
mance superior to other better-known models such
as Mitchell and Lapata?s (2010) simple semantic
space (SDS), Baroni and Lenci?s (2010) distribu-
tional memory tensor (DM), and Collobert and We-
ston?s (2008) neural language model (NLM). Our
results on the association norms are comparable to
the state of the art (Silberer and Lapata, 2012; Grif-
fiths et al, 2007). With regard to WordSim353,
Huang et al (2012) report correlations in the range
of 0.713?0.769, however they use Wikipedia as a
training corpus and a more sophisticated version of
the NLM presented here, that takes into account
global context and performs word sense discrimi-
nation. In the future, we also plan to evaluate our
model on larger Wikipedia-scale corpora. We would
also like to model semantic composition as our ap-
proach can do this easily by taking advantage of the
notion of quantum measurement. Specifically, we
2Available at http://clic.cimec.unitn.it/dm/.
854
Models bar order
SDS pub, snack, restau-
rant, grill, coctail
form, direct, proce-
dure, plan, request
DM counter, rack, strip,
pipe, code
court, demand, form,
law, list
NLM room, pole, drink,
rail, coctail
direct, command,
plan, court, demand
QM prison, liquor, beer,
club, graph
organization, food,
law, structure,
regulation
HS drink, beer, stool, al-
cohol, grill
food, form, law, heat,
court
Table 2: Associates for bar and order ranked according to
similarity. Underlined associates overlap with the human
responses (HS).
can work out the meaning of a dependency tree by
measuring the meaning of its heads in the context of
their dependents.
Table 2 shows the five most similar associates (or-
dered from high to low) for the cues bar and order
for the quantum model and the comparison models.
We also show the human responses (HS) according
to Nelson et al?s (1998) norms. The associates gen-
erated by the quantum model correspond to several
different meanings correlated with the target. For
example, prison refers to the ?behind bars? sense
of bar, liquor and beer refer to what is consumed
or served in bars, club refers to the entertainment
function of bars, whereas graph refers to how data
is displayed in a chart.
6 Related Work
Within cognitive science the formal apparatus of
quantum theory has been used to formulate models
of cognition that are superior to those based on tra-
ditional probability theory. For example, conjunc-
tion fallacies3 (Tversky and Kahneman, 1983) have
been explained by making reference to quantum the-
ory?s context dependence of the probability assess-
ment. Violations of the sure-thing principle4 (Tver-
sky and Shafir, 1992) have been modeled in terms of
a quantum interference effect. And the asymmetry
of similarity relations has been explained by pos-
tulating that different concepts correspond to sub-
spaces of different dimensionality (Pothos and Buse-
meyer, 2012). Several approaches have drawn on
3A conjunction fallacy occurs when it is assumed that spe-
cific conditions are more probable than a single general one.
4The principle is the expectation that human behavior ought
to conform to the law of total probability
quantum theory in order to model semantic phe-
nomena such as concept combination (Bruza and
Cole, 2005), the emergence of new concepts (Aerts
and Gabora, 2005), and the human mental lexicon
(Bruza et al, 2009). Chen (2002) captures syllo-
gisms in a quantum theoretic framework; the model
takes statements like All whales are mammals and
all mammals are animals as input and outputs con-
clusions like All whales are animals.
The first attempts to connect the mathematical
basis of semantic space models with quantum the-
ory are due to Aerts and Czachor (2004) and Bruza
and Cole (2005). They respectively demonstrate
that Latent Semantic Analysis (Landauer and Du-
mais, 1997) and the Hyperspace Analog to Lan-
guage model (Lund and Burgess, 1996) are essen-
tially Hilbert space formalisms, without, however,
providing concrete ways of building these models
beyond a few hand-picked examples. Interestingly,
Bruza and Cole (2005) show how lexical operators
may be contrived from corpus co-occurrence counts,
albeit admitting to the fact that their operators do not
provide sensical eigenkets, most likely because of
the simplified method of populating the matrix from
corpus statistics. Grefenstette et al (2011) present a
model for capturing semantic composition in a quan-
tum theoretical context, although it appears to be
reducible to the classical probabilistic paradigm. It
does not make use of the unique aspects of quantum
theory (e.g., entanglement, interference, or quantum
collapse).
Our own work follows Aerts and Czachor (2004)
and Bruza and Cole (2005) in formulating a model
that exhibits important aspects of quantum theory.
Contrary to them, we present a fully-fledged seman-
tic space rather than a proof-of-concept. We obtain
quantum states (i.e., lexical representations) for each
word by taking its syntactic context into account.
Quantum states are expressed as density operators
rather than kets. While a ket can only capture one
pure state of a system, a density operator contains
an ensemble of pure states which we argue is advan-
tageous from a modeling perspective. Within this
framework, not only can we compute the meaning of
individual words but also phrases or sentences, with-
out postulating any additional operations. Compo-
sitional meaning reduces to quantum measurement
at each inner node of the (dependency) parse of the
structure in question.
855
References
Diederik Aerts and Marek Czachor. 2004. Quantum as-
pects of semantic analysis and symbolic artificial intel-
ligence. Journal of PhysicsA: Mathematical and Gen-
eral, 37:123?132.
Diederik Aerts and Liane Gabora. 2005. A state-context-
property model of concepts and their combinations
ii: A hilbert space representation. Kybernetes, 1?
2(34):192?221.
Diederik Aerts. 2009. Quantum structure in cognition.
Journal of Mathematical Psychology, 53:314?348.
Belal E. Baaquie. 2004. Quantum Finance: Path In-
tegrals and Hamiltonians for Oprions and Interest
Rates. Cambridge University Press, Cambridge.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?
721.
Yoshua Bengio. 2001. Neural net language models.
Scholarpedia, 3(1):3881.
William Blacoe and Mirella Lapata. 2012. A compari-
son of vector-based representations for semantic com-
position. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 546?556, Jeju Island, Korea, July. Association
for Computational Linguistics.
Peter D. Bruza and Richard J. Cole. 2005. Quantum
logic of semantic space: An exploratory investigation
of context effects in practical reasoning. In S. Arte-
mov, H. Barringer, S. A. d?Avila Garcez, L. C. Lamb,
and J. Woods, editors, We Will Show Them: Essays
in Honour of Dov Gabbay, volume 1, pages 339?361.
London: College Publications.
Peter D. Bruza, Kirsty Kitto, Douglas McEnvoy, and
Cathy McEnvoy. 2008. Entangling words and mean-
ing. In Second Quantum Interaction Symposium. Ox-
ford University.
Peter D. Bruza, Kirsty Kitto, Douglas Nelson, and Cathy
McEvoy. 2009. Is there something quantum-like in
the human mental lexicon? Journal of Mathematical
Psychology, 53(5):362?377.
Jerome R. Busemeyer and Peter D. Bruza. 2012. Quan-
tum Models of Cognition and Decision. Cambridge
University Press, Cambridge.
Joseph C. H. Chen. 2002. Quantum computation and
natural language processing. Ph.D. thesis, Universita?t
Hamburg.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multita sk learning. In Proceedings of
the 25th International Conference on Machine Learn-
ing, pages 160?167, New York, NY. ACM.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8, Manchester, UK.
Guy Denhie`re and Beno??t Lemaire. 2004. A compu-
tational model of children?s semantic memory. In
Proceedings of the 26th Annual Meeting of the Cog-
nitive Science Society, pages 297?302, Mahwah, NJ.
Lawrence Erlbaum Associates.
Paul A. M. Dirac. 1939. A new notation for quantum me-
chanics. Mathematical Proceedings of the Cambridge
Philosophical Society, 35:416?418.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing Search in Context: The Con-
cept Revisited. ACMTransactions on Information Sys-
tems, 20(1):116?131, January.
John R. Firth. 1957. A synopsis of linguistic theory
1930-1955. In Studies in Linguistic Analysis, pages
1?32. Philological Society, Oxford.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distribu-
tional models of meaning. Proceedings of the 9th In-
ternational Conference on Computational Semantics
(IWCS11), pages 125?134.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 873?882, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
R. I. G. Hughes. 1989. The Structure and Interpreta-
tion of Quantum Mechnics. Harvard University Press,
Cambridge, MA.
Chris J. Isham. 1989. Lectures on Quantum Theory. Sin-
gapore: World Scientific.
Andrei Y. Khrennikov. 2010. Ubiquitous Quantum
Structure: From Psychology to Finance. Springer.
Daniel Kleppner and Roman Jackiw. 2000. One hundred
years of quantum physics. Science, 289(5481):893?
898.
Darrell R. Laham. 2000. Automated Content Assess-
ment of Text Using Latent Semantic Analysis to Sim-
ulate Human Cognition. Ph.D. thesis, University of
Colorado at Boulder.
856
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: the latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211?240.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768?774, Montre?al, Canada.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments
& Computers, 28:203?208.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
38(8):1388?1429.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 1998. The University of South Florida
Word Association, Rhyme, and Word Fragment
Norms.
Michael A. Nielsen and Isaac L. Chuang. 2010. Quan-
tum Computation and Information Theory. Cambridge
University Press, Cambridge.
Daniel Osherson and Edward E. Smith. 1981. On the
adequacy of prototype theory as a theory of concepts.
Cognition, 9:35?38.
Emmanuel M. Pothos and Jerome R. Busemeyer. 2012.
Can quantum probability provide a new direction for
cognitive modeling? Behavioral and Brain Sciences.
to appear.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?124.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433, Jeju
Island, Korea. Association for Computational Linguis-
tics.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In J. Shawe-Taylor, R.S.
Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Wein-
berger, editors, Advances in Neural Information Pro-
cessing Systems 24, pages 801?809.
Amos Tversky and Daniel Kahneman. 1983. Exten-
sional versus intuitive reasoning: The conjuctive fal-
lacy in probability judgment. Psychological Review,
4(90):293?315.
Amos Tversky and Eldar Shafir. 1992. The disjunction
effect in choice under uncertainty. Psychological Sci-
ence, 3(5):305?309.
Vlatko Vedral. 2006. Introduction to Quantum Informa-
tion Science. Oxford University Press, New York.
857
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196?206,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Syntactic and Semantic Factors in Processing Difficulty:
An Integrated Measure
Jeff Mitchell, Mirella Lapata, Vera Demberg and Frank Keller
University of Edinburgh
Edinburgh, United Kingdom
jeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk,
v.demberg@ed.ac.uk, keller@inf.ed.ac.uk
Abstract
The analysis of reading times can pro-
vide insights into the processes that under-
lie language comprehension, with longer
reading times indicating greater cognitive
load. There is evidence that the language
processor is highly predictive, such that
prior context allows upcoming linguistic
material to be anticipated. Previous work
has investigated the contributions of se-
mantic and syntactic contexts in isolation,
essentially treating them as independent
factors. In this paper we analyze reading
times in terms of a single predictive mea-
sure which integrates a model of seman-
tic composition with an incremental parser
and a language model.
1 Introduction
Psycholinguists have long realized that language
comprehension is highly incremental, with readers
and listeners continuously extracting the meaning
of utterances on a word-by-word basis. As soon
as they encounter a word in a sentence, they inte-
grate it as fully as possible into a representation
of the sentence thus far (Marslen-Wilson 1973;
Konieczny 2000; Tanenhaus et al 1995; Sturt and
Lombardo 2005). Recent research suggests that
language comprehension can also be highly pre-
dictive, i.e., comprehenders are able to anticipate
upcoming linguistic material. This is beneficial as
it gives them more time to keep up with the in-
put, and predictions can be used to compensate for
problems with noise or ambiguity.
Two types of prediction have been observed in
the literature. The first type is semantic predic-
tion, as evidenced in semantic priming: a word
that is preceded by a semantically related prime
or a semantically congruous sentence fragment is
processed faster (Stanovich and West 1981; van
Berkum et al 1999; Clifton et al 2007). Another
example is argument prediction: listeners are able
to launch eye-movements to the predicted argu-
ment of a verb before having encountered it, e.g.,
they will fixate an edible object as soon as they
hear the word eat (Altmann and Kamide 1999).
The second type of prediction is syntactic predic-
tion. Comprehenders are faster at naming words
that are syntactically compatible with prior con-
text, even when they bear no semantic relationship
to the context (Wright and Garrett 1984). Another
instance of syntactic prediction has been reported
by Staub and Clifton (2006): following the word
either, readers predict or and the complement that
follows it, and process it faster compared to a con-
trol condition without either.
Thus, human language processing takes advan-
tage of the constraints imposed by the preceding
semantic and syntactic context to derive expecta-
tions about the upcoming input. Much recent work
has focused on developing computational mea-
sures of these constraints and expectations. Again,
the literature is split into syntactic and semantic
models. Probably the best known measure of syn-
tactic expectation is surprisal (Hale 2001) which
can be coarsely defined as the negative log proba-
bility of word wt given the preceding words, typ-
ically computed using a probabilistic context-free
grammar.
Modeling work on semantic constraint focuses
on the degree to which a word is related to its
preceding context. Pynte et al (2008) use La-
tent Semantic Analysis (LSA, Landauer and Du-
mais 1997) to assess the degree of contextual con-
straint exerted on a word by its context. In this
framework, word meanings are represented as vec-
tors in a high dimensional space and distance in
this space is interpreted as an index of process-
ing difficulty. Other work (McDonald and Brew
2004) models contextual constraint in information
theoretic terms. The assumption is that words
carry prior semantic expectations which are up-
dated upon seeing the next word. Expectations are
represented by a vector of probabilities which re-
flects the likely location in semantic space of the
upcoming word.
The measures discussed above are typically
computed automatically on real-language corpora
using data-driven methods and their predictions
are verified through analysis of eye-movements
that people make while reading. Ample evidence
196
(Rayner 1998) demonstrates that eye-movements
are related to the moment-to-moment cognitive ac-
tivities of readers. They also provide an accurate
temporal record of the on-line processing of nat-
ural language, and through the analysis of eye-
movement measurements (e.g., the amount of time
spent looking at a word) can give insight into the
processing difficulty involved in reading.
In this paper, we investigate a model of predic-
tion that is incremental and takes into account syn-
tactic as well as semantic constraint. The model
essentially integrates the predictions of an incre-
mental parser (Roark 2001) together with those
of a semantic space model (Mitchell and Lap-
ata 2009). The latter creates meaning representa-
tions compositionally, and therefore builds seman-
tic expectations for word sequences (e.g., phrases,
sentences, even documents) rather than isolated
words. Some existing models of sentence process-
ing integrate semantic information into a prob-
abilistic parser (Narayanan and Jurafsky 2002;
Pado? et al 2009); however, the semantic compo-
nent of these models is limited to semantic role in-
formation, rather than attempting to build a full se-
mantic representation for a sentence. Furthermore,
the models of Narayanan and Jurafsky (2002) and
Pado? et al (2009) do not explicitly model pre-
diction, but rather focus on accounting for garden
path effects. The proposed model simultaneously
captures semantic and syntactic effects in a sin-
gle measure which we empirically show is predic-
tive of processing difficulty as manifested in eye-
movements.
2 Models of Processing Difficulty
As described in Section 1, reading times provide
an insight into the various cognitive activities that
contribute to the overall processing difficulty in-
volved in comprehending a written text. To quan-
tify and understand the overall cognitive load asso-
ciated with processing a word in context, we will
break that load down into a sum of terms repre-
senting distinct computational costs (semantic and
syntactic). For example, surprisal can be thought
of as measuring the cost of dealing with unex-
pected input. When a word conforms to the lan-
guage processor?s expectations, surprisal is low,
and the cognitive load associated with processing
that input will also be low. In contrast, unexpected
words will have a high surprisal and a high cogni-
tive cost.
However, high-level syntactic and semantic fac-
tors are only one source of cognitive costs. A siz-
able proportion of the variance in reading times is
accounted for by costs associated with low-level
features of the stimuli, e.g.. relating to orthography
and eye-movement control (Rayner 1998). In ad-
dition, there may also be costs associated with the
integration of new input into an incremental rep-
resentation. Dependency Locality Theory (DLT,
Gibson 2000) is essentially a distance-based mea-
sure of the amount of processing effort required
when the head of a phrase is integrated with its
syntactic dependents. We do not consider integra-
tion costs here (as they have not been shown to
correlate reliably with reading times; see Demberg
and Keller 2008 for details) and instead focus on
the costs associated with semantic and syntactic
constraint and low-level features, which appear to
make the most substantial contributions.
In the following subsections we describe the
various features which contribute to the process-
ing costs of a word in context. We begin by look-
ing at the low-level costs and move on to con-
sider the costs associated with syntactic and se-
mantic constraint. For readers unfamiliar with the
methodology involved in modeling eye-tracking
data, we note that regression analysis (or the more
general mixed effects models) is typically used to
study the relationship between dependent and in-
dependent variables. The independent variables
are the various costs of processing effort and
the dependent variables are measurements of eye-
movements, three of which are routinely used in
the literature: first fixation duration (the duration
of the first fixation on a word regardless of whether
it is the first fixation on a word or the first of mul-
tiple fixations on the same word), first pass dura-
tion, also known as gaze duration, (the sum of all
fixations made on a word prior to looking at an-
other word), and total reading time (the sum of
all fixations on a word including refixations after
moving on to other words).
2.1 Low-level Costs
Low-level features include word frequency (more
frequent words are read faster), word length
(shorter words are read faster), and the position
of the word in the sentence (later words are read
faster). Oculomotor variables have also been
found to influence reading times. These include
previous fixation (indicating whether or not the
previous word has been fixated), launch distance
(how many characters intervene between the cur-
rent fixation and the previous fixation), and land-
ing position (which letter in the word the fixation
landed on).
Information about the sequential context of a
word can also influence reading times. Mc-
197
Donald and Shillcock (2003) show that forward
and backward transitional probabilities are pre-
dictive of first fixation and first pass durations:
the higher the transitional probability, the shorter
the fixation time. Backward transitional prob-
ability is essentially the conditional probabil-
ity of a word given its immediately preceding
word, P(wk|wk?1). Analogously, forward proba-
bility is the conditional probability of the current
word given the next word, P(wk|wk+1).
2.2 Syntactic Constraint
As mentioned earlier, surprisal (Hale 2001; Levy
2008) is one of the best known models of process-
ing difficulty associated with syntactic constraint,
and has been previously applied to the modeling of
reading times (Demberg and Keller 2008; Ferrara
Boston et al 2008; Roark et al 2009; Frank 2009).
The basic idea is that the processing costs relating
to the expectations of the language processor can
be expressed in terms of the probabilities assigned
by some form of language model to the input.
These processing costs are assumed to arise from
the change in the expectations of the language pro-
cessor as new input arrives. If we express these ex-
pectations in terms of a distribution over all possi-
ble continuations of the input seen so far, then we
can measure the magnitude of this change in terms
of the Kullback-Leibler divergence of the old dis-
tribution to the updated distribution. This measure
of processing cost for an input word, wk+1, given
the previous context, w1 . . .wk, can be expressed
straightforwardly in terms of its conditional prob-
ability as:
S =? logP(wk+1|w1 . . .wk) (1)
That is, the processing cost for a word decreases as
its probability increases, with zero processing cost
incurred for words which must appear in a given
context, as these do not result in any change in the
expectations of the language processor.
The original formulation of surprisal (Hale
2001) used a probabilistic parser to calculate these
probabilities, as the emphasis was on the process-
ing costs incurred when parsing structurally am-
biguous garden path sentences.1 Several variants
of calculating surprisal have been developed in the
literature since using different parsing strategies
1While hearing a sentence like The horse raced past the
barn fell (Bever 1970), English speakers are inclined to in-
terpreted horse as the subject of raced expecting the sentence
to end at the word barn. So upon hearing the word fell they
are forced to revise their analysis of the sentence thus far and
adopt a reduced relative reading.
(e.g., left-to-right vs. top-down, PCFGs vs de-
pendency parsing) and different degrees of lexi-
calization (see Roark et al 2009 for an overview) .
For instance, unlexicalized surprisal can be easily
derived by substituting the words in Equation (1)
with parts of speech (Demberg and Keller 2008).
Surprisal could be also defined using a vanilla
language model that does not take any structural
or grammatical information into account (Frank
2009).
2.3 Semantic Constraint
Distributional models of meaning have been com-
monly used to quantify the semantic relation be-
tween a word and its context in computational
studies of lexical processing. These models are
based on the idea that words with similar mean-
ings will be found in similar contexts. In putting
this idea into practice, the meaning of a word is
then represented as a vector in a high dimensional
space, with the vector components relating to the
strength on occurrence of that word in various
types of context. Semantic similarities are then
modeled in terms of geometric similarities within
the space.
To give a concrete example, Latent Semantic
Analysis (LSA, Landauer and Dumais 1997) cre-
ates a meaning representation for words by con-
structing a word-document co-occurrence matrix
from a large collection of documents. Each row in
the matrix represents a word, each column a doc-
ument, and each entry the frequency with which
the word appeared within that document. Because
this matrix tends to be quite large it is often trans-
formed via a singular value decomposition (Berry
et al 1995) into three component matrices: a ma-
trix of word vectors, a matrix of document vectors,
and a diagonal matrix containing singular values.
Re-multiplying these matrices together using only
the initial portions of each (corresponding to the
use of a lower dimensional spatial representation)
produces a tractable approximation to the original
matrix. In this framework, the similarity between
two words can be easily quantified, e.g., by mea-
suring the cosine of the angle of the vectors repre-
senting them.
As LSA is one the best known semantic space
models it comes as no surprise that it has been
used to analyze semantic constraint. Pynte et al
(2008) measure the similarity between the next
word and its preceding context under the assump-
tion that high similarity indicates high semantic
constraint (i.e., the word was expected) and analo-
gously low similarity indicates low semantic con-
straint (i.e., the word was unexpected). They oper-
198
ationalize preceding contexts in two ways, either
as the word immediately preceding the next word
as the sentence fragment preceding it. Sentence
fragments are represented as the average of the
words they contain independently of their order.
The model takes into account only content words,
function words are of little interest here as they can
be found in any context.
Pynte et al (2008) analyze reading times on the
French part of the Dundee corpus (Kennedy and
Pynte 2005) and find that word-level LSA similar-
ities are predictive of first fixation and first pass
durations, whereas sentence-level LSA is only
predictive of first pass duration (i.e., for a mea-
sure that includes refixation). This latter finding
is somewhat counterintuitive, one would expect
longer contexts to have an immediate effect as
they are presumably more constraining. One rea-
son why sentence-level influences are only visible
on first pass duration may be due to LSA itself,
which is syntax-blind. Another reason relates to
the way sentential context was modeled as vec-
tor addition (or averaging). The idea of averag-
ing is not very attractive from a linguistic perspec-
tive as it blends the meanings of individual words
together. Ideally, the combination of simple el-
ements onto more complex ones must allow the
construction of novel meanings which go beyond
those of the individual elements (Pinker 1994).
The only other model of semantic constraint we
are aware of is Incremental Contextual Distinc-
tiveness (ICD, McDonald 2000; McDonald and
Brew 2004). ICD assumes that words carry prior
semantic expectations which are updated upon
seeing the next word. Context is represented by
a vector of probabilities which reflects the likely
location in semantic space of the upcoming word.
When the latter is observed, the prior expecta-
tion is updated using a Bayesian inference mecha-
nism to reflect the newly arrived information. Like
LSA, ICD is based on word co-occurrence vectors,
however it does not employ singular value decom-
position, and constructs a word-word rather than a
word-document co-occurrence matrix. Although
this model has been shown to successfully simu-
late single- and multiple-word priming (McDon-
ald and Brew 2004), it failed to predict processing
costs in the Embra eye-tracking corpus (McDon-
ald and Shillcock 2003).
In this work we model semantic constraint us-
ing the representational framework put forward in
Mitchell and Lapata (2008). Their aim is not so
much to model processing difficulty, but to con-
struct vector-based meaning representations that
go beyond individual words. They introduce a
general framework for studying vector composi-
tion, which they formulate as a function f of two
vectors u and v:
h = f (u,v) (2)
where h denotes the composition of u and v. Dif-
ferent composition models arise, depending on
how f is chosen. Assuming that h is a linear func-
tion of the Cartesian product of u and v allows to
specify additive models which are by far the most
common method of vector combination in the lit-
erature:
hi = ui + vi (3)
Alternatively, we can assume that h is a linear
function of the tensor product of u and v, and thus
derive models based on multiplication:
hi = ui ? vi (4)
Mitchell and Lapata (2008) show that several ad-
ditive and multiplicative models can be formu-
lated under this framework, including the well-
known tensor products (Smolensky 1990) and cir-
cular convolution (Plate 1995). Importantly, com-
position models are not defined with a specific se-
mantic space in mind, they could easily be adapted
to LSA, or simple co-occurrence vectors, or more
sophisticated semantic representations (e.g., Grif-
fiths et al 2007), although admittedly some com-
position functions may be better suited for partic-
ular semantic spaces.
Composition models can be straightforwardly
used as predictors of processing difficulty, again
via measuring the cosine of the angle between a
vector w representing the upcoming word and a
vector h representing the words preceding it:
sim(w,h) =
w ?h
|w||h|
(5)
where h is created compositionally, via some (ad-
ditive or multiplicative) function f .
In this paper we evaluate additive and compo-
sitional models in their ability to capture seman-
tic prediction. We also examine the influence of
the underlying meaning representations by com-
paring a simple semantic space similar to Mc-
Donald (2000) against Latent Dirichlet Allocation
(Blei et al 2003; Griffiths et al 2007). Specif-
ically, the simpler space is based on word co-
occurrence counts; it constructs the vector repre-
senting a given target word, t, by identifying all the
tokens of t in a corpus and recording the counts of
context words, ci (within a specific window). The
context words, ci, are limited to a set of the n most
199
common content words and each vector compo-
nent is given by the ratio of the probability of a ci
given t to the overall probability of ci.
vi =
p(ci|t)
p(ci)
(6)
Despite its simplicity, the above semantic space
(and variants thereof) has been used to success-
fully simulate lexical priming (e.g., McDonald
2000), human judgments of semantic similarity
(Bullinaria and Levy 2007), and synonymy tests
(Pado? and Lapata 2007) such as those included in
the Test of English as Foreign Language (TOEFL).
LDA is a probabilistic topic model offering an
alternative to spatial semantic representations. It
is similar in spirit to LSA, it also operates on a
word-document co-occurrence matrix and derives
a reduced dimensionality description of words and
documents. Whereas in LSA words are repre-
sented as points in a multi-dimensional space,
LDA represents words using topics. Specifically,
each document in a corpus is modeled as a distri-
bution over K topics, which are themselves char-
acterized as distribution over words. The individ-
ual words in a document are generated by repeat-
edly sampling a topic according to the topic distri-
bution and then sampling a single word from the
chosen topic. Under this framework, word mean-
ing is represented as a probability distribution over
a set of latent topics, essentially a vector whose
dimensions correspond to topics and values to the
probability of the word given these topics. Topic
models have been recently gaining ground as a
more structured representation of word meaning
(Griffiths et al 2007; Steyvers and Griffiths 2007).
In contrast to more standard semantic space mod-
els where word senses are conflated into a single
representation, topics have an intuitive correspon-
dence to coarse-grained sense distinctions.
3 Integrating Semantic Constraint into
Surprisal
The treatment of semantic and syntactic constraint
in models of processing difficulty has been some-
what inconsistent. While surprisal is a theo-
retically well-motivated measure, formalizing the
idea of linguistic processing being highly predic-
tive in terms of probabilistic language models, the
measurement of semantic constraint in terms of
vector similarities lacks a clear motivation. More-
over, the two approaches, surprisal and similarity,
produce mathematically different types of mea-
sures. Formally, it would be preferable to have
a single approach to capturing constraint and the
obvious solution is to derive some form of seman-
tic surprisal rather than sticking with similarity.
This can be achieved by turning a vector model
of semantic similarity into a probabilistic language
model.
There are in fact a number of approaches to de-
riving language models from distributional mod-
els of semantics (e.g., Bellegarda 2000; Coccaro
and Jurafsky 1998; Gildea and Hofmann 1999).
We focus here on the model of Mitchell and La-
pata (2009) which tackles the issue of the compo-
sition of semantic vectors and also integrates the
output of an incremental parser. The core of their
model is based on the product of a trigram model
p(wn|w
n?1
n?2) and a semantic component ?(wn,h)
which determines the factor by which this proba-
bility should be scaled up or down given the prior
semantic context h:
p(wn) = p(wn|w
n?1
n?2) ??(wn,h) (7)
The factor ?(wn,h) is essentially based on a com-
parison between the vector representing the cur-
rent word wn and the vector representing the prior
history h. Varying the method for constructing
word vectors (e.g., using LDA or a simpler seman-
tic space model) and for combining them into a
representation of the prior context h (e.g., using
additive or multiplicative functions) produces dis-
tinct models of semantic composition.
The calculation of ? is then based on a weighted
dot product of the vector representing the upcom-
ing word w, with the vector representing the prior
context h:
?(w,h) =?
i
wihi p(ci) (8)
As shown in Equation (7) this semantic factor then
modulates the trigram probabilities, to take ac-
count of the effect of the semantic content outside
the n-gram window.
Mitchell and Lapata (2009) show that a com-
bined semantic-trigram language model derived
from this approach and trained on the Wall Street
Journal outperforms a baseline trigram model in
terms of perplexity on a held out set. They also
linearly interpolate this semantic language model
with the output of an incremental parser, which
computes the following probability:
p(w|h) = ?p1(w|h)+(1??)p2(w|h) (9)
where p1(w|h) is computed as in Equation (7)
and p2(w|h) is computed by the parser. Their im-
plementation uses Roark?s (2001) top-down incre-
mental parser which estimates the probability of
200
the next word based upon the previous words of
the sentence. These prefix probabilities are calcu-
lated from a grammar, by considering the likeli-
hood of seeing the next word given the possible
grammatical relations representing the prior con-
text.
Equation (9) essentially defines a language
model which combines semantic, syntactic and
n-gram structure, and Mitchell and Lapata (2009)
demonstrate that it improves further upon a se-
mantic language model in terms of perplexity. We
argue that the probabilities from this model give
us a means to model the incrementally and predic-
tivity of the language processor in a manner that
integrates both syntactic and semantic constraints.
Converting these probabilities to surprisal should
result in a single measure of the processing cost as-
sociated with semantic and syntactic expectations.
4 Method
Data The models discussed in the previous sec-
tion were evaluated against an eye-tracking cor-
pus. Specifically, we used the English portion
of the Dundee Corpus (Kennedy and Pynte 2005)
which contains 20 texts taken from The Indepen-
dent newspaper. The corpus consists of 51,502
tokens and 9,776 types in total. It is annotated
with the eye-movement records of 10 English na-
tive speakers, who each read the whole corpus.
The eye-tracking data was preprocessed following
the methodology described in Demberg and Keller
(2008). From this data, we computed total reading
time for each word in the corpus. Our statistical
analyses were based on actual reading times, and
so we only included words that were not skipped.
We also excluded words for which the previous
word had been skipped, and words on which the
normal left-to-right movement of gaze had been
interrupted, i.e., by blinks, regressions, etc. Fi-
nally, because our focus is the influence of seman-
tic context, we selected only content words whose
prior sentential context contained at least two fur-
ther content words. The resulting data set con-
sisted of 53,704 data points, which is about 10%
of the theoretically possible total.2
2The total of all words read by all subjects is 515,020.
The pre-processing recommended by Demberg and Keller?s
(2008) results in a data sets containing 436,000 data points.
Removing non-content words leaves 205,922 data points. It
only makes sense to consider words that were actually fixated
(the eye-tracking measures used are not defined on skipped
words), which leaves 162,129 data points. Following Pynte
et al (2008), we require that the previous word was fixated,
with 70,051 data points remaining. We exclude words on
which the normal left to right movement of gaze had been
interrupted, e.g., by blinks and regressions, which results in
the final total to 53,704 data points.
Model Implementation All elements of our
model were trained on the BLLIP corpus, a col-
lection of texts from the Wall Street Journal
(years 1987?89). The training corpus consisted of
38,521,346 words. We used a development cor-
pus of 50,006 words and a test corpus of similar
size. All words were converted to lowercase and
numbers were replaced with the symbol ?num?. A
vocabulary of 20,000 words was chosen and the
remaining tokens were replaced with ?unk?.
Following Mitchell and Lapata (2009), we con-
structed a simple semantic space based on co-
occurrence statistics from the BLLIP training set.
We used the 2,000 most frequent word types as
contexts and a symmetric five word window. Vec-
tor components were defined as in Equation (6).
We also trained the LDA model on BLLIP, using
the Gibb?s sampling procedure discussed in Grif-
fiths et al (2007). We experimented with different
numbers of topics on the development set (from 10
to 1,000) and report results on the test set with 100
topics. In our experiments, the hyperparameter ?
was initialized to .5, and the ? word probabilities
were initialized randomly.
We integrated our compositional models with a
trigram model which we also trained on BLLIP.
The model was built using the SRILM toolkit
(Stolcke 2002) with backoff and Kneser-Ney
smoothing. As our incremental parser we used
Roark?s (2001) parser trained on sections 2?21 of
the Penn Treebank containing 936,017 words. The
parser produces prefix probabilities for each word
of a sentence which we converted to conditional
probabilities by dividing each current probability
by the previous one.
Statistical Analysis The statistical analyses in
this paper were carried out using linear mixed
effects models (LME, Pinheiro and Bates 2000).
The latter can be thought of as generalization of
linear regression that allows the inclusion of ran-
dom factors (such as participants or items) as well
as fixed factors (e.g., word frequency). In our
analyses, we treat participant as a random factor,
which means that our models contain an intercept
term for each participant, representing the individ-
ual differences in the rates at which they read.3
We evaluated the effect of adding a factor to a
model by comparing the likelihoods of the mod-
els with and without that factor. If a ?2 test on the
3Other random factors that are appropriate for our anal-
yses are word and sentence; however, due to the large num-
ber of instances for these factors (given that the Dundee cor-
pus contains 51,502 tokens), we were not able to include
them: the model fitting algorithm we used (implemented in
the R package lme4) does not converge for such large models.
201
Factor Coefficient
Intercept ?.011
Word Length .264
Launch Distance .109
Landing Position .612
Word Frequency ?.010
Reading Time of Last Word .151
Table 1: Coefficients of the baseline LME model
for total reading time
likelihood ratio is significant, then this indicates
that the new factor significantly improves model
fit. We also experimented with adding random
slopes for participant to the model (in addition to
the random intercept); however, this either led to
non-convergence of the model fitting procedure, or
failed to result in an increase in model fit accord-
ing to the likelihood ratio test. Therefore, all mod-
els reported in the rest of this paper contain ran-
dom intercept of participants as the sole random
factor.
Rather than model raw reading times, we model
times on the log scale. This is desirable for a
number of reasons. Firstly, the raw reading times
tend to have a skew distribution and taking logs
produces something closer to normal, which is
preferable for modeling. Secondly, the regres-
sion equation makes more sense on the log scale
as the contribution of each term to raw reading
time is multiplicative rather than additive. That is,
log(t) = ?i?ixi implies t = ?i e
?ixi . In particular,
the intercept term for each participant now repre-
sents a multiplicative factor by which that partici-
pant is slower or faster.
5 Results
We computed separate mixed effects models for
three dependent variables, namely first fixation du-
ration, first pass duration, and total reading time.
We report results for total times throughout, as
the results of the other two dependent variables
are broadly similar. Our strategy was to first con-
struct a baseline model of low-level factors influ-
encing reading time, and then to take the resid-
uals from that model as the dependent variable
in subsequent analyses. In this way we removed
the effects of low-level factors before investigating
the factors associated with syntactic and semantic
constraint. This avoids problems with collinear-
ity between low-level factors and the factors we
are interested in (e.g., trigram probability is highly
correlated with word frequency). The baseline
model contained the factors word length, word fre-
Model Composition Coefficient
SSS
Additive ?.03820???
Multiplicative ?.00895???
LDA
Additive ?.02500???
Multiplicative ?.00262???
Table 2: Coefficients of LME models including
simple semantic space (SSS) or Latent Dirichlet
Allocation (LDA) as factors; ???p < .001
quency, launch distance, landing position, and the
reading time for the last fixated word, and its pa-
rameter estimates are given in Table 1. To further
reduce collinearity, we also centered all fixed fac-
tors, both in the baseline model, and in the models
fitted on the residuals that we report in the follow-
ing. Note that some intercorrelations remain be-
tween the factors, which we will discuss at the end
of Section 5.
Before investigating whether an integrated
model of semantic and syntactic constraint im-
proves the goodness of fit over the baseline, we ex-
amined the influence of semantic constraint alone.
This was necessary as compositional models have
not been previously used to model processing
difficulty. Besides, replicating Pynte et al?s
(2008) finding, we were also interested in assess-
ing whether the underlying semantic representa-
tion (simple semantic space or LDA) and com-
position function (additive versus multiplicative)
modulate reading times differentially.
We built an LME model that predicted the resid-
ual reading times of the baseline model using the
similarity scores from our composition models as
factors. We then carried out a ?2 test on the like-
lihood ratio of a model only containing the ran-
dom factor and the intercept, and a model also
containing the semantic factor (cosine similarity).
The addition of the semantic factor significantly
improves model fit for both the simple semantic
space and LDA. This result is observed for both
additive and multiplicative composition functions.
Our results are summarized in Table 2 which re-
ports the coefficients of the four LME models fit-
ted against the residuals of the baseline model, to-
gether with the p-values of the ?2 test.
Before evaluating our integrated surprisal mea-
sure, we evaluated its components individually in
order to tease their contributions apart. For ex-
ample, it may be the case that syntactic surprisal
is an overwhelmingly better predictor of reading
time than semantic surprisal, however we would
not be able to detect this by simply adding a factor
based on Equation (9) to the baseline model. The
202
Factor SSS Coef LDA Coef
? log(p) .00760??? .00760???
A
dd
? log(?) .03810??? .00622???
log(?+(1??) p2p1 ) .00953
??? .00943???
M
ul
t ? log(?) .01110??? ?.00033
log(?+(1??) p2p1 ) .00882
??? .00133
Table 3: Coefficients of nested LME models with
the components of SSS or LDA surprisal as fac-
tors; only the coefficient of the additional factor at
each step are shown
integrated surprisal measure can be written as:
S =? log(?p1 +(1??)p2) (10)
Where p2 is the incremental parser probability and
p1 is the product of the semantic component, ?,
and the trigram probability, p. This can be broken
down into the sum of two terms:
S =? log(p1)? log(?+(1??)
p2
p1
) (11)
Since the first term, ? log(p1) is itself a product it
can also be broken down further:
S =? log(p)? log(?)? log(?+(1??)
p2
p1
) (12)
Thus, to evaluate the contribution of the three
components to the integrated surprisal measure we
fitted nested LME models, i.e., we entered these
terms one at a time into a mixed effects model
and tested the significance of the improvement in
model fit for each additional term.
We again start with an LME model that only
contains the random factor and the intercept, with
the residuals of the baseline models as the depen-
dent variable. Considering the trigram model first,
we find that adding this factor to the model gives a
significant improvement in fit. Also adding the se-
mantic component (? log(?)) improves fit further,
both for additive and multiplicative composition
functions using a simple semantic space. Finally,
the addition of the parser probabilities (log(?+
(1??) p2p1 )) again improves model fit significantly.
As far as LDA is concerned, the additive model
significantly improves model fit, whereas the mul-
tiplicative one does not. These results mirror
the findings of Mitchell and Lapata (2009), who
report that a multiplicative composition function
produced the lowest perplexity for the simple se-
mantic space model, whereas an additive function
gave the best perplexity for the LDA space. Ta-
ble 3 lists the coefficients for the nested models for
Model Composition Coefficient
SSS
Additive .00804???
Multiplicative .00819???
LDA
Additive .00817???
Multiplicative .00640???
Table 4: Coefficients of LME models with inte-
grated surprisal measure (based on SSS or LDA)
as factor
all four variants of our semantic constraint mea-
sure.
Finally, we built a separate LME model where
we added the integrated surprisal measure (see
Equation (9)) to the model only containing the ran-
dom factor and the intercept (see Table 4). We
did this separately for all four versions of the in-
tegrated surprisal measure (SSS, LDA; additive,
multiplicative). We find that model fit improved
significantly all versions of integrated surprisal.
One technical issue that remains to be discussed
is collinearity, i.e., intercorrelations between the
factors in a model. The presence of collinearity
is problematic, as it can render the model fitting
procedure unstable; it can also affect the signifi-
cance of individual factors. As mentioned in Sec-
tion 4 we used two techniques to reduce collinear-
ity: residualizing and centering. Table 5 gives
an overview of the correlation coefficients for all
pairs of factors. It becomes clear that collinear-
ity has mostly been removed; there is a remaining
relationship between word length and word fre-
quency, which is expected as shorter words tend to
be more frequent. This correlation is not a prob-
lem for our analysis, as it is confined to the base-
line model. Furthermore, word frequency and tri-
gram probability are highly correlated. Again this
is expected, given that the frequencies of unigrams
and higher-level n-grams tend to be related. This
correlation is taken care of by residualizing, which
isolates the two factors: word frequency is part
of the baseline model, while trigram probability is
part of the separate models that we fit on the resid-
uals. All other correlations are small (with coeffi-
cients of .27 or less), with one exception: there is
a high correlation between the ? log(?) term and
the log(?+ (1? ?) p2p1 ) term in the multiplicative
LDA model. This collinearity issue may explain
the absence of a significant improvement in model
fit when these two terms are added to the baseline
(see Table 3).
203
Factor Len Freq ?l(p)?l(?)
Frequency ?.310
? log(p) .230?.700
S
S
S
A
dd
? log(?) .016?.120 .025
log(?+(1??) p2p1 ) .024 .036?.270 .065
S
S
S
M
ul
t ? log(?) ?.015?.110 .035
log(?+(1??) p2p1 ) .020 .028?.260 .160
L
D
A
A
dd
? log(?) ?.024?.130 .046
log(?+(1??) p2p1 ) .005 .014?.250 .030
L
D
A
M
ul
t ? log(?) ?.120 .006?.046
log(?+(1??) p2p1 )?.089?.005?.180 .740
Table 5: Intercorrelations between model factors
6 Discussion
In this paper we investigated the contributions of
syntactic and semantic constraint in modeling pro-
cessing difficulty. Our work departs from previ-
ous approaches in that we propose a single mea-
sure which integrates syntactic and semantic fac-
tors. Evaluation on an eye-tracking corpus shows
that our measure predicts reading time better than
a baseline model that captures low-level factors
in reading (word length, landing position, etc.).
Crucially, we were able to show that the semantic
component of our measure improves reading time
predictions over and above a model that includes
syntactic measures (based on a trigram model and
incremental parser). This means that semantic
costs are a significant predictor of reading time in
addition to the well-known syntactic surprisal.
An open issue is whether a single, integrated
measure (as evaluated in Table 4) fits the eye-
movement data significantly better than separate
measures for trigram, syntactic, and semantic sur-
prisal (as evaluated in Table 3. However, we are
not able to investigate this hypothesis: our ap-
proach to testing the significance of factors re-
quires nested models; the log-likelihood test (see
Section 4) is only able to establish whether adding
a factor to a model improves its fit; it cannot com-
pare models with disjunct sets of factors (such as
a model containing the integrated surprisal mea-
sure and one containing the three separate ones).
However, we would argue that a single, integrated
measure that captures human predictive process-
ing is preferable over a collection of separate mea-
sures. It is conceptually simpler (as it is more par-
simonious), and is also easier to use in applica-
tions (such as readability prediction). Finally, an
integrated measure requires less parameters; our
definition of surprisal in 12 is simply the sum of
the trigram, syntactic, and semantic components.
An LME model containing separate factors, on the
other hand, requires a coefficient for each of them,
and thus has more parameters.
In evaluating our model, we adopted a broad
coverage approach using the reading time data
from a naturalistic corpus rather than artificially
constructed experimental materials. In doing so,
we were able to compare different syntactic and
semantic costs on the same footing. Previous
analyses of semantic constraint have been con-
ducted on different eye-tracking corpora (Dundee
and Embra Corpus) and on different languages
(English, French). Moreover, comparisons of the
individual contributions of syntactic and semantic
factors were generally absent from the literature.
Our analysis showed that both of these factors can
be captured by our integrated surprisal measure
which is uniformly probabilistic and thus prefer-
able to modeling semantic and syntactic costs dis-
jointly using a mixture of probabilistic and non-
probabilistic measures.
An interesting question is which aspects of se-
mantics our model is able to capture, i.e., why
does the combination of LSA or LDA representa-
tions with an incremental parser yield a better fit of
the behavioral data. In the psycholinguistic liter-
ature, various types of semantic information have
been investigated: lexical semantics (word senses,
selectional restrictions, thematic roles), senten-
tial semantics (scope, binding), and discourse se-
mantics (coreference and coherence); see Keller
(2010) of a detailed discussion. We conjecture that
our model is mainly capturing lexical semantics
(through the vector space representation of words)
and sentential semantics (through the multiplica-
tion or addition of words). However, discourse
coreference effects (such as the ones reported by
Altmann and Steedman (1988) and much subse-
quent work) are probably not amenable to a treat-
ment in terms of vector space semantics; an ex-
plicit representation of discourse entities and co-
reference relations is required (see Dubey 2010
for a model of human sentence processing that can
handle coreference).
A key objective for future work will be to in-
vestigate models that integrate semantic constraint
with syntactic predictions more tightly. For ex-
ample, we could envisage a parser that uses se-
mantic representations to guide its search, e.g., by
pruning syntactic analyses that have a low seman-
tic probability. At the same time, the semantic
model should have access to syntactic informa-
tion, i.e., the composition of word representations
should take their syntactic relationships into ac-
count, rather than just linear order.
204
References
ACL. 2010. Proceedings of the 48th Annual Meet-
ing of the Association for Computational Lin-
guistics. Uppsala.
Altmann, Gerry T. M. and Yuki Kamide. 1999.
Incremental interpretation at verbs: Restricting
the domain of subsequent reference. Cognition
73:247?264.
Altmann, Gerry T. M. and Mark J. Steedman.
1988. Interaction with context during human
sentence processing. Cognition 30(3):191?238.
Bellegarda, Jerome R. 2000. Exploiting latent se-
mantic information in statistical language mod-
eling. Proceedings of the IEEE 88(8):1279?
1296.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O?Brien. 1995. Using linear algebra
for intelligent information retrieval. SIAM re-
view 37(4):573?595.
Bever, Thomas G. 1970. The cognitive basis for
linguistic strutures. In J. R. Hayes, editor, Cog-
nition and the Development of Language, Wi-
ley, New York, pages 279?362.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet alocation. Journal
of Machine Learning Research 3:993?1022.
Bullinaria, John A. and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study.
Behavior Research Methods 39:510?526.
Clifton, Charles, Adrian Staub, and Keith Rayner.
2007. Eye movement in reading words and sen-
tences. In R V Gompel, M Fisher, W Murray,
and R L Hill, editors, Eye Movements: A Win-
dow in Mind and Brain, Elsevier, pages 341?
372.
Coccaro, Noah and Daniel Jurafsky. 1998. To-
wards better integration of semantic predictors
in satistical language modeling. In Proceedings
of the 5th International Conference on Spoken
Language Processing. Sydney, Australia, pages
2403?2406.
Demberg, Vera and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition
101(2):193?210.
Dubey, Amit. 2010. The influence of discourse on
syntax: A psycholinguistic model of sentence
processing. In ACL.
Ferrara Boston, Marisa, John Hale, Reinhold
Kliegl, Umesh Patil, and Shravan Vasishth.
2008. Parsing costs as predictors of reading dif-
ficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Re-
search 2(1):1?12.
Frank, Stefan L. 2009. Surprisal-based compar-
ison between a symbolic and a connectionist
model of sentence processing. In Proceedings
of the 31st Annual Conference of the Cognitive
Science Society. Austin, TX, pages 139?1144.
Gibson, Edward. 2000. Dependency locality the-
ory: A distance-dased theory of linguistic com-
plexity. In Alec Marantz, Yasushi Miyashita,
and Wayne O?Neil, editors, Image, Language,
Brain: Papers from the First Mind Articulation
Project Symposium, MIT Press, Cambridge,
MA, pages 95?126.
Gildea, Daniel and Thomas Hofmann. 1999.
Topic-based language models using EM. In
Proceedings of the 6th European Conference
on Speech Communiation and Technology. Bu-
dapest, Hungary, pages 2167?2170.
Griffiths, Thomas L., Mark Steyvers, and
Joshua B. Tenenbaum. 2007. Topics in se-
mantic representation. Psychological Review
114(2):211?244.
Hale, John. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chap-
ter of the Association. Association for Compu-
tational Linguistics, Pittsburgh, PA, volume 2,
pages 159?166.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In ACL.
Kennedy, Alan and Joel Pynte. 2005. Parafoveal-
on-foveal effects in normal reading. Vision Re-
search 45:153?168.
Konieczny, Lars. 2000. Locality and parsing com-
plexity. Journal of Psycholinguistic Research
29(6):627?645.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to Plato?s problem: the latent seman-
tic analysis theory of acquisition, induction and
representation of knowledge. Psychological Re-
view 104(2):211?240.
Levy, Roger. 2008. Expectation-based syntactic
comprehension. Cognition 106(3):1126?1177.
Marslen-Wilson, William D. 1973. Linguistic
structure and speech shadowing at very short la-
tencies. Nature 244:522?523.
McDonald, Scott. 2000. Environmental Determi-
nants of Lexical Processing Effort. Ph.D. thesis,
University of Edinburgh.
205
McDonald, Scott and Chris Brew. 2004. A dis-
tributional model of semantic context effects in
lexical processing. In Proceedings of the 42th
Annual Meeting of the Association for Com-
putational Linguistics. Barcelona, Spain, pages
17?24.
McDonald, Scott A. and Richard C. Shillcock.
2003. Low-level predictive inference in read-
ing: The influence of transitional probabilities
on eye movements. Vision Research 43:1735?
1751.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-
based models of semantic composition. In Pro-
ceedings of ACL-08: HLT . Columbus, OH,
pages 236?244.
Mitchell, Jeff and Mirella Lapata. 2009. Language
models based on semantic composition. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing. Sin-
gapore, pages 430?439.
Narayanan, Srini and Daniel Jurafsky. 2002. A
Bayesian model predicts human parse prefer-
ence and reading time in sentence processing. In
Thomas G. Dietterich, Sue Becker, and Zoubin
Ghahramani, editors, Advances in Neural In-
formation Processing Systems 14. MIT Press,
Cambridge, MA, pages 59?65.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics
33(2):161?199.
Pado?, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Pinheiro, Jose C. and Douglas M. Bates.
2000. Mixed-effects Models in S and S-PLUS.
Springer, New York.
Pinker, Steven. 1994. The Language Instinct: How
the Mind Creates Language. HarperCollins,
New York.
Plate, Tony A. 1995. Holographic reduced repre-
sentations. IEEE Transactions on Neural Net-
works 6(3):623?641.
Pynte, Joel, Boris New, and Alan Kennedy. 2008.
On-line contextual influences during reading
normal text: A multiple-regression analysis. Vi-
sion Research 48:2172?2183.
Rayner, Keith. 1998. Eye movements in read-
ing and information processing: 20 years of re-
search. Psychological Bulletin 124(3):372?422.
Roark, Brian. 2001. Probabilistic top-down pars-
ing and language modeling. Computational
Linguistics 27(2):249?276.
Roark, Brian, Asaf Bachrach, Carlos Cardenas,
and Christophe Pallier. 2009. Deriving lex-
ical and syntactic expectation-based measures
for psycholinguistic modeling via incremental
top-down parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural
Language Processing. Association for Compu-
tational Linguistics, Singapore, pages 324?333.
Smolensky, Paul. 1990. Tensor product vari-
able binding and the representation of symbolic
structures in connectionist systems. Artificial
Intelligence 46:159?216.
Stanovich, Kieth E. and Richard F. West. 1981.
The effect of sentence context on ongoing word
recognition: Tests of a two-pricess theory. Jour-
nal of Experimental Psychology: Human Per-
ception and Performance 7:658?672.
Staub, Adrian and Charles Clifton. 2006. Syntac-
tic prediction in language comprehension: Evi-
dence from either . . .or. Journal of Experimen-
tal Psychology: Learning, Memory, and Cogni-
tion 32:425?436.
Steyvers, Mark and Tom Griffiths. 2007. Proba-
bilistic topic models. In T. Landauer, D. Mc-
Namara, S Dennis, and W Kintsch, editors, A
Handbook of Latent Semantic Analysis, Psy-
chology Press.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the
Internatinal Conference on Spoken Language
Processing. Denver, Colorado.
Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-
tality and connectedness. Cognitive Science
29(2):291?305.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632?1634.
van Berkum, Jos J. A., Colin M. Brown, and Peter
Hagoort. 1999. Early referential context effects
in sentence processing: Evidence from event-
related brain potentials. Journal of Memory and
Language 41:147?182.
Wright, Barton and Merrill F. Garrett. 1984. Lex-
ical decision in sentences: Effects of syntactic
structure. Memory and Cognition 12:31?45.
206
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565?574,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Generation of Story Highlights
Kristian Woodsend and Mirella Lapata
School of Informatics, University of Edinburgh
Edinburgh EH8 9AB, United Kingdom
k.woodsend@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we present a joint con-
tent selection and compression model
for single-document summarization. The
model operates over a phrase-based rep-
resentation of the source document which
we obtain by merging information from
PCFG parse trees and dependency graphs.
Using an integer linear programming for-
mulation, the model learns to select and
combine phrases subject to length, cover-
age and grammar constraints. We evalu-
ate the approach on the task of generat-
ing ?story highlights??a small number of
brief, self-contained sentences that allow
readers to quickly gather information on
news stories. Experimental results show
that the model?s output is comparable to
human-written highlights in terms of both
grammaticality and content.
1 Introduction
Summarization is the process of condensing a
source text into a shorter version while preserving
its information content. Humans summarize on
a daily basis and effortlessly, but producing high
quality summaries automatically remains a chal-
lenge. The difficulty lies primarily in the nature
of the task which is complex, must satisfy many
constraints (e.g., summary length, informative-
ness, coherence, grammaticality) and ultimately
requires wide-coverage text understanding. Since
the latter is beyond the capabilities of current NLP
technology, most work today focuses on extractive
summarization, where a summary is created sim-
ply by identifying and subsequently concatenating
the most important sentences in a document.
Without a great deal of linguistic analysis, it
is possible to create summaries for a wide range
of documents. Unfortunately, extracts are of-
ten documents of low readability and text quality
and contain much redundant information. This is
in marked contrast with hand-written summaries
which often combine several pieces of informa-
tion from the original document (Jing, 2002) and
exhibit many rewrite operations such as substitu-
tions, insertions, deletions, or reorderings.
Sentence compression is often regarded as a
promising first step towards ameliorating some of
the problems associated with extractive summa-
rization. The task is commonly expressed as a
word deletion problem. It involves creating a short
grammatical summary of a single sentence, by re-
moving elements that are considered extraneous,
while retaining the most important information
(Knight and Marcu, 2002). Interfacing extractive
summarization with a sentence compression mod-
ule could improve the conciseness of the gener-
ated summaries and render them more informative
(Jing, 2000; Lin, 2003; Zajic et al, 2007).
Despite the bulk of work on sentence compres-
sion and summarization (see Clarke and Lapata
2008 and Mani 2001 for overviews) only a handful
of approaches attempt to do both in a joint model
(Daume? III and Marcu, 2002; Daume? III, 2006;
Lin, 2003; Martins and Smith, 2009). One rea-
son for this might be the performance of sentence
compression systems which falls short of attaining
grammaticality levels of human output. For ex-
ample, Clarke and Lapata (2008) evaluate a range
of state-of-the-art compression systems across dif-
ferent domains and show that machine generated
compressions are consistently perceived as worse
than the human gold standard. Another reason is
the summarization objective itself. If our goal is
to summarize news articles, then we may be bet-
ter off selecting the first n sentences of the docu-
ment. This ?lead? baseline may err on the side of
verbosity but at least will be grammatical, and it
has indeed proved extremely hard to outperform
by more sophisticated methods (Nenkova, 2005).
In this paper we propose a model for sum-
565
marization that incorporates compression into the
task. A key insight in our approach is to formulate
summarization as a phrase rather than sentence
extraction problem. Compression falls naturally
out of this formulation as only phrases deemed
important should appear in the summary. Ob-
viously, our output summaries must meet addi-
tional requirements such as sentence length, over-
all length, topic coverage and, importantly, gram-
maticality. We combine phrase and dependency
information into a single data structure, which al-
lows us to express grammaticality as constraints
across phrase dependencies. We encode these con-
straints through the use of integer linear program-
ming (ILP), a well-studied optimization frame-
work that is able to search the entire solution space
efficiently.
We apply our model to the task of generat-
ing highlights for a single document. Examples
of CNN news articles with human-authored high-
lights are shown in Table 1. Highlights give a
brief overview of the article to allow readers to
quickly gather information on stories, and usually
appear as bullet points. Importantly, they repre-
sent the gist of the entire document and thus of-
ten differ substantially from the first n sentences
in the article (Svore et al, 2007). They are also
highly compressed, written in a telegraphic style
and thus provide an excellent testbed for models
that generate compressed summaries. Experimen-
tal results show that our model?s output is compa-
rable to hand-written highlights both in terms of
grammaticality and informativeness.
2 Related work
Much effort in automatic summarization has been
devoted to sentence extraction which is often for-
malized as a classification task (Kupiec et al,
1995). Given appropriately annotated training
data, a binary classifier learns to predict for
each document sentence if it is worth extracting.
Surface-level features are typically used to sin-
gle out important sentences. These include the
presence of certain key phrases, the position of
a sentence in the original document, the sentence
length, the words in the title, the presence of
proper nouns, etc. (Mani, 2001; Sparck Jones,
1999).
Relatively little work has focused on extraction
methods for units smaller than sentences. Jing and
McKeown (2000) first extract sentences, then re-
move redundant phrases, and use (manual) recom-
bination rules to produce coherent output. Wan
and Paris (2008) segment sentences heuristically
into clauses before extraction takes place, and
show that this improves summarization quality.
In the context of multiple-document summariza-
tion, heuristics have also been used to remove par-
enthetical information (Conroy et al, 2004; Sid-
dharthan et al, 2004). Witten et al (1999) (among
others) extract keyphrases to capture the gist of the
document, without however attempting to recon-
struct sentences or generate summaries.
A few previous approaches have attempted to
interface sentence compression with summariza-
tion. A straightforward way to achieve this is by
adopting a two-stage architecture (e.g., Lin 2003)
where the sentences are first extracted and then
compressed or the other way round. Other work
implements a joint model where words and sen-
tences are deleted simultaneously from a docu-
ment. Using a noisy-channel model, Daume? III
and Marcu (2002) exploit the discourse structure
of a document and the syntactic structure of its
sentences in order to decide which constituents to
drop but also which discourse units are unimpor-
tant. Martins and Smith (2009) formulate a joint
sentence extraction and summarization model as
an ILP. The latter optimizes an objective func-
tion consisting of two parts: an extraction com-
ponent, essentially a non-greedy variant of max-
imal marginal relevance (McDonald, 2007), and
a sentence compression component, a more com-
pact reformulation of Clarke and Lapata (2008)
based on the output of a dependency parser. Com-
pression and extraction models are trained sepa-
rately in a max-margin framework and then inter-
polated. In the context of multi-document summa-
rization, Daume? III?s (2006) vine-growth model
creates summaries incrementally, either by start-
ing a new sentence or by growing already existing
ones.
Our own work is closest to Martins and Smith
(2009). We also develop an ILP-based compres-
sion and summarization model, however, several
key differences set our approach apart. Firstly,
content selection is performed at the phrase rather
than sentence level. Secondly, the combination of
phrase and dependency information into a single
data structure is new, and important in allowing
us to express grammaticality as constraints across
phrase dependencies, rather than resorting to a lan-
566
Most blacks say MLK?s vision fulfilled, poll finds
WASHINGTON (CNN) ? More than two-thirds of African-
Americans believe Martin Luther King Jr.?s vision for race
relations has been fulfilled, a CNN poll found ? a figure up
sharply from a survey in early 2008.
The CNN-Opinion Research Corp. survey was released
Monday, a federal holiday honoring the slain civil rights
leader and a day before Barack Obama is to be sworn in as
the first black U.S. president.
The poll found 69 percent of blacks said King?s vision has
been fulfilled in the more than 45 years since his 1963 ?I have
a dream? speech ? roughly double the 34 percent who agreed
with that assessment in a similar poll taken last March.
But whites remain less optimistic, the survey found.
? 69 percent of blacks polled say Martin Luther King Jr?s
vision realized.
? Slim majority of whites say King?s vision not fulfilled.
? King gave his ?I have a dream? speech in 1963.
9/11 billboard draws flak from Florida Democrats, GOP
(CNN) ? A Florida man is using billboards with an image of
the burning World Trade Center to encourage votes for a Re-
publican presidential candidate, drawing criticism for politi-
cizing the 9/11 attacks.
?Please Don?t Vote for a Democrat? reads the type over the
picture of the twin towers after hijacked airliners hit them on
September, 11, 2001.
Mike Meehan, a St. Cloud, Florida, businessman who paid to
post the billboards in the Orlando area, said former President
Clinton should have put a stop to Osama bin Laden and al
Qaeda before 9/11. He said a Republican president would
have done so.
? Billboards use image from 9/11 to encourage GOP votes.
? 9/11 image wrong for ad, say Florida political parties.
? Floridian praises President Bush, says ex-President Clin-
ton failed to stop al Qaeda.
Table 1: Two example CNN news articles, showing the title and the first few paragraphs, and below, the
original highlights that accompanied each story.
guage model. Lastly, our model is more com-
pact, has fewer parameters, and does not require
two training procedures. Our approach bears some
resemblance to headline generation (Dorr et al,
2003; Banko et al, 2000), although we output sev-
eral sentences rather than a single one. Head-
line generation models typically extract individual
words from a document to produce a very short
summary, whereas we extract phrases and ensure
that they are combined into grammatical sentences
through our ILP constraints.
Svore et al (2007) were the first to foreground
the highlight generation task which we adopt as an
evaluation testbed for our model. Their approach
is however a purely extractive one. Using an al-
gorithm based on neural networks and third-party
resources (e.g., news query logs and Wikipedia en-
tries) they rank sentences and select the three high-
est scoring ones as story highlights. In contrast,
we aim to generate rather than extract highlights.
As a first step we focus on deleting extraneous ma-
terial, but other more sophisticated rewrite opera-
tions (e.g., Cohn and Lapata 2009) could be incor-
porated into our framework.
3 The Task
Given a document, we aim to produce three or four
short sentences covering its main topics, much like
the ?Story Highlights? accompanying the (online)
CNN news articles. CNN highlights are written by
humans; we aim to do this automatically.
Documents Highlights
Sentences 37.2 ? 39.6 3.5 ? 0.5
Tokens 795.0 ? 744.8 47.0 ? 9.6
Tokens/sentence 22.4 ? 4.2 13.3 ? 1.7
Table 2: Overview statistics on the corpus of doc-
uments and highlights (mean and standard devia-
tion). A minority of documents are transcripts of
interviews and speeches, and can be very long; this
accounts for the very large standard deviation.
Two examples of a news story and its associ-
ated highlights, are shown in Table 1. As can be
seen, the highlights are written in a compressed,
almost telegraphic manner. Articles, auxiliaries
and forms of the verb be are often deleted. Com-
pression is also achieved through paraphrasing,
e.g., substitutions and reorderings. For example,
the document sentence ?The poll found 69 percent
of blacks said King?s vision has been fulfilled.? is
rephrased in the highlight as ?69 percent of blacks
polled say Martin Luther King Jr?s vision real-
ized.?. In general, there is a fair amount of lexi-
cal overlap between document sentences and high-
lights (42.44%) but the correspondence between
document sentences and highlights is not always
one-to-one. In the first example in Table 1, the sec-
ond paragraph gives rise to two highlights. Also
note that the highlights need not form a coherent
summary, each of them is relatively stand-alone,
and there is little co-referencing between them.
567
(a)
S
S
CC
But
NP
NNS
whites
VP
VBP
remain
ADJP
RBR
less
JJ
optimistic
,
,
NP
DT
the
NN
survey
VP
VBD
found
.
.
(b)
TOP
found
optimistic
whites
ns
ub
j
remain
co
p
less
advmod
cc
om
p
survey
the
de
t
nsubj
Figure 1: An example phrase structure (a) and dependency (b) tree for the sentence ?But whites remain
less optimistic, the survey found.?.
In order to train and evaluate the model pre-
sented in the following sections we created a cor-
pus of document-highlight pairs (approximately
9,000) which we downloaded from the CNN.com
website.1 The articles were randomly sampled
from the years 2007?2009 and covered a wide
range of topics such as business, crime, health,
politics, showbiz, etc. The majority were news
articles, but the set alo contained a mixture of
editorials, commentary, interviews and reviews.
Some overview statistics of the corpus are shown
in Table 2. Overall, we observe a high degree of
compression both at the document and sentence
level. The highlights summary tends to be ten
times shorter than the corresponding article. Fur-
thermore, individual highlights have almost half
the length of document sentences.
4 Modeling
The objective of our model is to create the most in-
formative story highlights possible, subject to con-
straints relating to sentence length, overall sum-
mary length, topic coverage, and grammaticality.
These constraints are global in their scope, and
cannot be adequately satisfied by optimizing each
one of them individually. Our approach therefore
uses an ILP formulation which will provide a glob-
ally optimal solution, and which can be efficiently
solved using standard optimization tools. Specif-
ically, the model selects phrases from which to
form the highlights, and each highlight is created
from a single sentence through phrase deletion.
The model operates on parse trees augmented with
1The corpus is available from http://homepages.inf.
ed.ac.uk/mlap/resources/index.html.
dependency labels. We first describe how we ob-
tain this representation and then move on to dis-
cuss the model in more detail.
Sentence Representation We obtain syntactic
information by parsing every sentence twice, once
with a phrase structure parser and once with a
dependency parser. The phrase structure and
dependency-based representations for the sen-
tence ?But whites remain less optimistic, the sur-
vey found.? (from Table 1) are shown in Fig-
ures 1(a) and 1(b), respectively.
We then combine the output from the two
parsers, by mapping the dependencies to the edges
of the phrase structure tree in a greedy fashion,
shown in Figure 2(a). Starting at the top node of
the dependency graph, we choose a node i and a
dependency arc to node j. We locate the corre-
sponding words i and j on the phrase structure
tree, and locate their nearest shared ancestor p. We
assign the label of the dependency i? j to the first
unlabeled edge from p to j in the phrase structure
tree. Edges assigned with dependency labels are
shown as dashed lines. These edges are important
to our formulation, as they will be represented by
binary decision variables in the ILP. Further edges
from p to j, and all the edges from p to i, are
marked as fixed and shown as solid lines. In this
way we keep the correct ordering of leaf nodes.
Finally, leaf nodes are merged into parent phrases,
until each phrase node contains a minimum of two
tokens, shown in Figure 2(b). Because of this min-
imum length rule, it is possible for a merged node
to be a clause rather than a phrase, but in the sub-
sequent description we will use the term phrase
rather loosely to describe any merged leaf node.
568
(a)
S
S
CC
But
NP
NNS
whites
ns
ub
j
VP
VBP
remain
co
p
ADJP
RBR
less
adv
mod
JJ
optimistic
ccomp
,
,
NP
DT
the
de
t
NN
survey
nsubj
VP
VBD
found
.
.
(b)
S
S
But whites remain
less optimistic
cc
om
p
,
,
NP
the survey
nsubj
VBD
found .
Figure 2: Dependencies are mapped onto phrase structure tree (a) and leaf nodes are merged with parent
phrases (b).
ILP model The merged phrase structure tree,
such as shown in Figure 2(b), is the actual input to
our model. Each phrase in the document is given
a salience score. We obtain these scores from the
output of a supervised machine learning algorithm
that predicts for each phrase whether it should be
included in the highlights or not (see Section 5 for
details). Let S be the set of sentences in a docu-
ment, P be the set of phrases, and Ps ? P be the
set of phrases in each sentence s ? S . T is the set
of words with the highest tf.idf scores, and Pt ? P
is the set of phrases containing the token t ? T .
Let fi denote the salience score for phrase i, deter-
mined by the machine learning algorithm, and li is
its length in tokens.
We use a vector of binary variables x ? {0,1}|P |
to indicate if each phrase is to be within a high-
light. These are either top-level nodes in our
merged tree representation, or nodes whose edge
to the parent has a dependency label (the dashed
lines). Referring to our example in Figure 2(b), bi-
nary variables would be allocated to the top-level S
node, the child S node and the NP node. The vec-
tor of auxiliary binary variables y ? {0,1}|S | in-
dicates from which sentences the chosen phrases
come (see Equations (1i) and (1j)). Let the sets
Di ? P , ?i ? P capture the phrase dependency in-
formation for each phrase i, where each set Di
contains the phrases that depend on the presence
of i. Our objective function function is given in
Equation (1a): it is the sum of the salience scores
of all the phrases chosen to form the highlights
of a given document, subject to the constraints
in Equations (1b)?(1j). The latter provide a nat-
ural way of describing the requirements the output
must meet.
max
x ?
i?P
fixi (1a)
s.t. ?
i?P
lixi ? LT (1b)
?
i?Ps
lixi ? LMys ?s ? S (1c)
?
i?Ps
lixi ? Lmys ?s ? S (1d)
?
i?Pt
xi ? 1 ?t ? T (1e)
x j? xi ?i ? P , j ?Di (1f)
xi? ys ?s ? S , i ? Ps (1g)
?
s?S
ys ? NS (1h)
xi ? {0,1} ?i ? P (1i)
ys ? {0,1} ?s ? S . (1j)
Constraint (1b) ensures that the generated high-
lights do not exceed a total budget of LT tokens.
This constraint may vary depending on the appli-
cation or task at hand. Highlights on a small screen
device would presumably be shorter than high-
lights for news articles on the web. It is also possi-
ble to set the length of each highlight to be within
the range [Lm,LM]. Constraints (1c) and (1d) en-
force this requirement. In particular, these con-
straints stop highlights formed from sentences at
the beginning of the document (which tend to have
569
high salience scores) from being too long. Equa-
tion (1e) is a set-covering constraint, requiring that
each of the words in T appears at least once in
the highlights. We assume that words with high
tf.idf scores reveal to a certain extent what the doc-
ument is about. Constraint (1e) ensures that some
of these words will be present in the highlights.
We enforce grammatical correctness through
constraint (1f) which ensures that the phrase de-
pendencies are respected. Phrases that depend on
phrase i are contained in the set Di. Variable xi is
true, and therefore phrase i will be included, if any
of its dependents x j ?Di are true. The phrase de-
pendency constraints, contained in the set Di and
enforced by (1f), are the result of two rules based
on the typed dependency information:
1. Any child node j of the current node i,
whose connecting edge i ? j is of type
nsubj (nominal subject), nsubjpass (passive
nominal subject), dobj (direct object), pobj
(preposition object), infmod (infinitival mod-
ifier), ccomp (clausal complement), xcomp
(open clausal complement), measure (mea-
sure phrase modifier) and num (numeric
modifier) must be included if node i is in-
cluded.
2. The parent node p of the current node i must
always be included if i is, unless the edge
p? i is of type ccomp (clausal complement)
or advcl (adverbial clause), in which case it
is possible to include i without including p.
Consider again the example in Figure 2(b).
There are only two possible outputs from this sen-
tence. If the phrase ?the survey? is chosen, then
the parent node ?found? will be included, and from
our first rule the ccomp phrase must also be in-
cluded, which results in the output: ?But whites
remain less optimistic, the survey found.? If, on
the other hand, the clause ?But whites remain less
optimistic? is chosen, then due to our second rule
there is no constraint that forces the parent phrase
?found? to be included in the highlights. Without
other factors influencing the decision, this would
give the output: ?But whites remain less opti-
mistic.? We can see from this example that encod-
ing the possible outputs as decisions on branches
of the phrase structure tree provides a more com-
pact representation of many options than would be
possible with an explicit enumeration of all possi-
ble compressions. Which output is chosen (if any)
depends on the scores of the phrases involved, and
the influence of the other constraints.
Constraint (1g) tells the ILP to create a highlight
if one of its constituent phrases is chosen. Finally,
note that a maximum number of highlights NS can
be set beforehand, and (1h) limits the highlights to
this maximum.
5 Experimental Set-up
Training We obtained phrase-based salience
scores using a supervised machine learning algo-
rithm. 210 document-highlight pairs were chosen
randomly from our corpus (see Section 3). Two
annotators manually aligned the highlights and
document sentences. Specifically, each sentence
in the document was assigned one of three align-
ment labels: must be in the summary (1), could be
in the summary (2), and is not in the summary (3).
The annotators were asked to label document sen-
tences whose content was identical to the high-
lights as ?must be in the summary?, sentences
with partially overlapping content as ?could be in
the summary? and the remainder as ?should not
be in the summary?. Inter-annotator agreement
was .82 (p < 0.01, using Spearman?s ? rank corre-
lation). The mapping of sentence labels to phrase
labels was unsupervised: if the phrase came from
a sentence labeled (1), and there was a unigram
overlap (excluding stop words) between the phrase
and any of the original highlights, we marked this
phrase with a positive label. All other phrases
were marked negative.
Our feature set comprised surface features such
as sentence and paragraph position information,
POS tags, unigram and bigram overlap with the
title, and whether high-scoring tf.idf words were
present in the phrase (66 features in total). The
210 documents produced a training set of 42,684
phrases (3,334 positive and 39,350 negative). We
learned the feature weights with a linear SVM,
using the software SVM-OOPS (Woodsend and
Gondzio, 2009). This tool gave us directly the fea-
ture weights as well as support vector values, and
it allowed different penalties to be applied to pos-
itive and negative misclassifications, enabling us
to compensate for the unbalanced data set. The
penalty hyper-parameters chosen were the ones
that gave the best F-scores, using 10-fold valida-
tion.
Highlight generation We generated highlights
for a test set of 600 documents. We created and
570
solved an ILP for each document. Sentences were
first tokenized to separate words and punctuation,
then parsed to obtain phrases and dependencies as
described in Section 4 using the Stanford parser
(Klein and Manning, 2003). For each phrase, fea-
tures were extracted and salience scores calcu-
lated from the feature weights determined through
SVM training. The distance from the SVM hyper-
plane represents the salience score. The ILP model
(see Equation (1)) was parametrized as follows:
the maximum number of highlights NS was 4,
the overall limit on length LT was 75 tokens, the
length of each highlight was in the range of [8,28]
tokens, and the topic coverage set T contained the
top 5 tf.idf words. These parameters were chosen
to capture the properties seen in the majority of
the training set; they were also relaxed enough to
allow a feasible solution of the ILP model (with
hard constraints) for all the documents in the test
set. To solve the ILP model we used the ZIB Opti-
mization Suite software (Achterberg, 2007; Koch,
2004; Wunderling, 1996). The solution was con-
verted into highlights by concatenating the chosen
leaf nodes in order. The ILP problems we created
had on average 290 binary variables and 380 con-
straints. The mean solve time was 0.03 seconds.
Summarization In order to examine the gen-
erality of our model and compare with previous
work, we also evaluated our system on a vanilla
summarization task. Specifically, we used the
same model (trained on the CNN corpus) to gen-
erate summaries for the DUC-2002 corpus2. We
report results on the entire dataset and on a subset
containing 140 documents. This is the same parti-
tion used by Martins and Smith (2009) to evaluate
their ILP model.3
Baselines We compared the output of our model
to two baselines. The first one simply selects
the ?leading? three sentences from each document
(without any compression). The second baseline
is the output of a sentence-based ILP model, sim-
ilar to our own, but simpler. The model is given
in (2). The binary decision variables x ? {0,1}|S |
now represent sentences, and fi the salience score
for each sentence. The objective again is to max-
imize the total score, but now subject only to
tf.idf coverage (2b) and a limit on the number of
2http://www-nlpir.nist.gov/projects/duc/
guidelines/2002.html
3We are grateful to Andre? Martins for providing us with
details of their testing partition.
highlights (2c) which we set to 3. There are no
sentence length or grammaticality constraints, as
there is no sentence compression.
max
x ?
i?S
fixi (2a)
s.t. ?
i?St
xi ? 1 ?t ? T (2b)
?
i?S
xi ? NS (2c)
xi ? {0,1} ?i ? S . (2d)
The SVM was trained with the same features used
to obtain phrase-based salience scores, but with
sentence-level labels (labels (1) and (2) positive,
(3) negative).
Evaluation We evaluated summarization qual-
ity using ROUGE (Lin and Hovy, 2003). For the
highlight generation task, the original CNN high-
lights were used as the reference. We report un-
igram overlap (ROUGE-1) as a means of assess-
ing informativeness and the longest common sub-
sequence (ROUGE-L) as a means of assessing flu-
ency.
In addition, we evaluated the generated high-
lights by eliciting human judgments. Participants
were presented with a news article and its corre-
sponding highlights and were asked to rate the lat-
ter along three dimensions: informativeness (do
the highlights represent the article?s main topics?),
grammaticality (are they fluent?), and verbosity
(are they overly wordy and repetitive?). The sub-
jects used a seven point rating scale. An ideal
system would receive high numbers for grammat-
icality and informativeness and a low number for
verbosity. We randomly selected nine documents
from the test set and generated highlights with our
model and the sentence-based ILP baseline. We
also included the original highlights as a gold stan-
dard. We thus obtained ratings for 27 (9 ? 3)
document-highlights pairs.4 The study was con-
ducted over the Internet using WebExp (Keller
et al, 2009) and was completed by 34 volunteers,
all self reported native English speakers.
With regard to the summarization task, follow-
ing Martins and Smith (2009), we used ROUGE-1
and ROUGE-2 to evaluate our system?s output.
We also report results with ROUGE-L. Each doc-
ument in the DUC-2002 dataset is paired with
4A Latin square design ensured that subjects did not see
two different highlights of the same document.
571
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
Recall Precision
Rouge-1
F-score Recall Precision
Rouge-L
F-score
S
co
re
Leading-3
ILP sentence
ILP phrase
Figure 3: ROUGE-1 and ROUGE-L results for
phrase-based ILP model and two baselines, with
error bars showing 95% confidence levels.
a human-authored summary (approximately 100
words) which we used as reference.
6 Results
We report results on the highlight generation task
in Figure 3 with ROUGE-1 and ROUGE-L (error
bars indicate the 95% confidence interval). In
both measures, the ILP sentence baseline has the
best recall, while the ILP phrase model has the
best precision (the differences are statistically sig-
nificant). F-score is higher for the phrase-based
system but not significantly. This can be at-
tributed to the fact that the longer output of the
sentence-based model makes the recall task easier.
Average highlight lengths are shown in Table 3,
and the compression rates they represent. Our
phrase model achieves the highest compression
rates, whereas the sentence-based model tends to
select long sentences even in comparison to the
lead baseline. The sentence ILP model outper-
forms the lead baseline with respect to recall but
not precision or F-score. The phrase ILP achieves
a significantly better F-score over the lead baseline
with both ROUGE-1 and ROUGE-L.
The results of our human evaluation study are
summarized in Table 4. There was no sta-
tistically significant difference in the grammat-
icality between the highlights generated by the
phrase ILP system and the original CNN high-
lights (means differences were compared using a
Post-hoc Tukey test). The grammaticality of the
sentence ILP was significantly higher overall as
no compression took place (? < 0.05). All three
s toks/s C.R.
Articles 36.5 22.2 ? 4.0 100%
CNN highlights 3.5 13.3 ? 1.7 5.8%
ILP phrase 3.8 18.0 ? 2.9 8.4%
Leading-3 3.0 25.1 ? 7.4 9.3%
ILP sentence 3.0 31.3 ? 7.9 11.6%
Table 3: Comparison of output lengths: number
of sentences, tokens per sentence, and compres-
sion rate, for CNN articles, their highlights, the
ILP phrase model, and two baselines.
Model Grammar Importance Verbosity
CNN highlights 4.85 4.88 3.14
ILP sentence 6.41 5.47 3.97
ILP phrase 5.53 5.05 3.38
Table 4: Average human ratings for original CNN
highlights, and two ILP models.
systems performed on a similar level with respect
to importance (differences in the means were not
significant). The highlights created by the sen-
tence ILP were considered significantly more ver-
bose (? < 0.05) than those created by the phrase-
based system and the CNN abstractors. Overall,
the highlights generated by the phrase ILP model
were not significantly different from those written
by humans. They capture the same content as the
full sentences, albeit in a more succinct manner.
Table 5 shows the output of the phrase-based sys-
tem for the documents in Table 1.
Our results on the complete DUC-2002 cor-
pus are shown in Table 6. Despite the fact that
our model has not been optimized for the original
task of generating 100-word summaries?instead
it is trained on the CNN corpus, and generates
highlights?the results are comparable with the
best of the original participants5 in each of the
ROUGE measures. Our model is also significantly
better than the lead sentences baseline.
Table 7 presents our results on the same
DUC-2002 partition (140 documents) used by
Martins and Smith (2009). The phrase ILP model
achieves a significantly better F-score (for both
ROUGE-1 and ROUGE-2) over the lead baseline,
the sentence ILP model, and Martins and Smith.
We should point out that the latter model is not a
straw man. It significantly outperforms a pipeline
5The list of participants is on page 12 of the slides
available from http://duc.nist.gov/pubs/2002slides/
overview.02.pdf.
572
? More than two-thirds of African-Americans believe
Martin Luther King Jr.?s vision for race relations has
been fulfilled.
? 69 percent of blacks said King?s vision has been ful-
filled in the more than 45 years since his 1963 ?I have a
dream? speech.
? But whites remain less optimistic, the survey found.
? A Florida man is using billboards with an image of the
burning World Trade Center to encourage votes for a
Republican presidential candidate, drawing criticism.
? ?Please Don?t Vote for a Democrat? reads the type over
the picture of the twin towers.
? Mike Meehan said former President Clinton should
have put a stop to Osama bin Laden and al Qaeda be-
fore 9/11.
Table 5: Generated highlights for the stories in Ta-
ble 1 using the phrase ILP model.
Participant ROUGE-1 ROUGE-2 ROUGE-L
28 0.464 0.222 0.432
19 0.459 0.221 0.431
21 0.458 0.216 0.426
29 0.449 0.208 0.419
27 0.445 0.209 0.417
Leading-3 0.416 0.200 0.390
ILP phrase 0.454 0.213 0.428
Table 6: ROUGE results on the complete
DUC-2002 corpus, including the top 5 original
participants. For all results, the 95% confidence
interval is ?0.008.
approach that first creates extracts and then com-
presses them. Furthermore, as a standalone sen-
tence compression system it yields state of the art
performance, comparable to McDonald?s (2006)
discriminative model and superior to Hedge Trim-
mer (Zajic et al, 2007), a less sophisticated deter-
ministic system.
7 Conclusions
In this paper we proposed a joint content selection
and compression model for single-document sum-
marization. A key aspect of our approach is the
representation of content by phrases rather than
entire sentences. Salient phrases are selected to
form the summary. Grammaticality, length and
coverage requirements are encoded as constraints
in an integer linear program. Applying the model
to the generation of ?story highlights? (and sin-
gle document summaries) shows that it is a vi-
able alternative to extraction-based systems. Both
ROUGE scores and the results of our human study
ROUGE-1 ROUGE-2 ROUGE-L
Leading-3 .400 ? .018 .184 ? .015 .374 ? .017
M&S (2009) .403 ? .076 .180 ? .076 ?
ILP sentence .430 ? .014 .191 ? .015 .401 ? .014
ILP phrase .445 ? .014 .200 ? .014 .419 ? .014
Table 7: ROUGE results on DUC-2002 cor-
pus (140 documents). ?: only ROUGE-1 and
ROUGE-2 results are given in Martins and Smith
(2009).
confirm that our system manages to create sum-
maries at a high compression rate and yet maintain
the informativeness and grammaticality of a com-
petitive extractive system. The model itself is rel-
atively simple and knowledge-lean, and achieves
good performance without reference to any re-
sources outside the corpus collection.
Future extensions are many and varied. An ob-
vious next step is to examine how the model gen-
eralizes to other domains and text genres. Al-
though coherence is not so much of an issue for
highlights, it certainly plays a role when generat-
ing standard summaries. The ILP model can be
straightforwardly augmented with discourse con-
straints similar to those proposed in Clarke and
Lapata (2007). We would also like to generalize
the model to arbitrary rewrite operations, as our
results indicate that compression rates are likely
to improve with more sophisticated paraphrasing.
Acknowledgments
We would like to thank Andreas Grothey and
members of ICCS at the School of Informatics for
the valuable discussions and comments through-
out this work. We acknowledge the support of EP-
SRC through project grants EP/F055765/1 and
GR/T04540/01.
References
Achterberg, Tobias. 2007. Constraint Integer Programming.
Ph.D. thesis, Technische Universita?t Berlin.
Banko, Michele, Vibhu O. Mittal, and Michael J. Witbrock.
2000. Headline generation based on statistical translation.
In Proceedings of the 38th ACL. Hong Kong, pages 318?
325.
Clarke, James and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings of
EMNLP-CoNLL. Prague, Czech Republic, pages 1?11.
Clarke, James and Mirella Lapata. 2008. Global inference
for sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Research
31:399?429.
Cohn, Trevor and Mirella Lapata. 2009. Sentence compres-
sion as tree transduction. Journal of Artificial Intelligence
Research 34:637?674.
573
Conroy, J. M., J. D. Schlesinger, J. Goldstein, and D. P.
O?Leary. 2004. Left-brain/right-brain multi-document
summarization. In DUC 2004 Conference Proceedings.
Daume? III, Hal. 2006. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D. thesis,
University of Southern California.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of the
40th ACL. Philadelphia, PA, pages 449?456.
Dorr, Bonnie, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 2003
Workshop on Text Summarization. pages 1?8.
Jing, Hongyan. 2000. Sentence reduction for automatic text
summarization. In Proceedings of the 6th ANLP. Seattle,
WA, pages 310?315.
Jing, Hongyan. 2002. Using hidden Markov modeling to de-
compose human-written summaries. Computational Lin-
guistics 28(4):527?544.
Jing, Hongyan and Kathleen McKeown. 2000. Cut and paste
summarization. In Proceedings of the 1st NAACL. Seattle,
WA, pages 178?185.
Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of web experi-
ments: A case study using the WebExp software package.
Behavior Research Methods 41(1):1?12.
Klein, Dan and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st ACL. Sap-
poro, Japan, pages 423?430.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach to sen-
tence compression. Artificial Intelligence 139(1):91?107.
Koch, Thorsten. 2004. Rapid Mathematical Prototyping.
Ph.D. thesis, Technische Universita?t Berlin.
Kupiec, Julian, Jan O. Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proceedings of SIGIR-
95. Seattle, WA, pages 68?73.
Lin, Chin-Yew. 2003. Improving summarization performance
by sentence compression ? a pilot study. In Proceed-
ings of the 6th International Workshop on Information Re-
trieval with Asian Languages. Sapporo, Japan, pages 1?8.
Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT NAACL. Edmonton, Canada, pages
71?78.
Mani, Inderjeet. 2001. Automatic Summarization. John Ben-
jamins Pub Co.
Martins, Andre? and Noah A. Smith. 2009. Summarization
with a joint model for sentence extraction and compres-
sion. In Proceedings of the Workshop on Integer Linear
Programming for Natural Language Processing. Boulder,
Colorado, pages 1?9.
McDonald, Ryan. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy.
McDonald, Ryan. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceedings
of the 29th ECIR. Rome, Italy.
Nenkova, Ani. 2005. Automatic text summarization of
newswire: Lessons learned from the Document Under-
standing Conference. In Proceedings of the 20th AAAI.
Pittsburgh, PA, pages 1436?1441.
Siddharthan, Advaith, Ani Nenkova, and Kathleen McKe-
own. 2004. Syntactic simplification for improving con-
tent selection in multi-document summarization. In Pro-
ceedings of the 20th International Conference on Compu-
tational Linguistics (COLING 2004). pages 896?902.
Sparck Jones, Karen. 1999. Automatic summarizing: Factors
and directions. In Inderjeet Mani and Mark T. Maybury,
editors, Advances in Automatic Text Summarization, MIT
Press, Cambridge, pages 1?33.
Svore, Krysta, Lucy Vanderwende, and Christopher Burges.
2007. Enhancing single-document summarization by
combining RankNet and third-party sources. In Proceed-
ings of EMNLP-CoNLL. Prague, Czech Republic, pages
448?457.
Wan, Stephen and Ce?cile Paris. 2008. Experimenting with
clause segmentation for text summarization. In Proceed-
ings of the 1st TAC. Gaithersburg, MD.
Witten, Ian H., Gordon Paynter, Eibe Frank, Carl Gutwin, and
Craig G. Nevill-Manning. 1999. KEA: Practical automatic
keyphrase extraction. In Proceedings of the 4th ACM
International Conference on Digital Libraries. Berkeley,
CA, pages 254?255.
Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector machine
training. Computational Optimization and Applications .
Wunderling, Roland. 1996. Paralleler und objektorientierter
Simplex-Algorithmus. Ph.D. thesis, Technische Univer-
sita?t Berlin.
Zajic, David, Bonnie J. Door, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks.
Information Processing Management Special Issue on
Summarization 43(6):1549?1570.
574
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239?1249,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
How Many Words is a Picture Worth?
Automatic Caption Generation for News Images
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we tackle the problem of au-
tomatic caption generation for news im-
ages. Our approach leverages the vast re-
source of pictures available on the web
and the fact that many of them are cap-
tioned. Inspired by recent work in sum-
marization, we propose extractive and ab-
stractive caption generation models. They
both operate over the output of a proba-
bilistic image annotation model that pre-
processes the pictures and suggests key-
words to describe their content. Exper-
imental results show that an abstractive
model defined over phrases is superior to
extractive methods.
1 Introduction
Recent years have witnessed an unprecedented
growth in the amount of digital information avail-
able on the Internet. Flickr, one of the best known
photo sharing websites, hosts more than three bil-
lion images, with approximately 2.5 million im-
ages being uploaded every day.1 Many on-line
news sites like CNN, Yahoo!, and BBC publish
images with their stories and even provide photo
feeds related to current events. Browsing and find-
ing pictures in large-scale and heterogeneous col-
lections is an important problem that has attracted
much interest within information retrieval.
Many of the search engines deployed on the
web retrieve images without analyzing their con-
tent, simply by matching user queries against col-
located textual information. Examples include
meta-data (e.g., the image?s file name and for-
mat), user-annotated tags, captions, and gener-
ally text surrounding the image. As this limits
the applicability of search engines (images that
1http://www.techcrunch.com/2008/11/03/
three-billion-photos-at-flickr/
do not coincide with textual data cannot be re-
trieved), a great deal of work has focused on the
development of methods that generate description
words for a picture automatically. The literature
is littered with various attempts to learn the as-
sociations between image features and words us-
ing supervised classification (Vailaya et al, 2001;
Smeulders et al, 2000), instantiations of the noisy-
channel model (Duygulu et al, 2002), latent vari-
able models (Blei and Jordan, 2003; Barnard et al,
2002; Wang et al, 2009), and models inspired by
information retrieval (Lavrenko et al, 2003; Feng
et al, 2004).
In this paper we go one step further and gen-
erate captions for images rather than individual
keywords. Although image indexing techniques
based on keywords are popular and the method of
choice for image retrieval engines, there are good
reasons for using more linguistically meaningful
descriptions. A list of keywords is often ambigu-
ous. An image annotated with the words blue,
sky, car could depict a blue car or a blue sky,
whereas the caption ?car running under the blue
sky? would make the relations between the words
explicit. Automatic caption generation could im-
prove image retrieval by supporting longer and
more targeted queries. It could also assist journal-
ists in creating descriptions for the images associ-
ated with their articles. Beyond image retrieval, it
could increase the accessibility of the web for vi-
sually impaired (blind and partially sighted) users
who cannot access the content of many sites in
the same ways as sighted users can (Ferres et al,
2006).
We explore the feasibility of automatic caption
generation in the news domain, and create descrip-
tions for images associated with on-line articles.
Obtaining training data in this setting does not re-
quire expensive manual annotation as many ar-
ticles are published together with captioned im-
ages. Inspired by recent work in summarization,
we propose extractive and abstractive caption gen-
1239
eration models. The backbone for both approaches
is a probabilistic image annotation model that sug-
gests keywords for an image. We can then simply
identify (and rank) the sentences in the documents
that share these keywords or create a new caption
that is potentially more concise but also informa-
tive and fluent. Our abstractive model operates
over image description keywords and document
phrases. Their combination gives rise to many
caption realizations which we select probabilisti-
cally by taking into account dependency and word
order constraints. Experimental results show that
the model?s output compares favorably to hand-
written captions and is often superior to extractive
methods.
2 Related Work
Although image understanding is a popular topic
within computer vision, relatively little work has
focused on the interplay between visual and lin-
guistic information. A handful of approaches gen-
erate image descriptions automatically following
a two-stage architecture. The picture is first ana-
lyzed using image processing techniques into an
abstract representation, which is then rendered
into a natural language description with a text gen-
eration engine. A common theme across differ-
ent models is domain specificity, the use of hand-
labeled data, and reliance on background ontolog-
ical information.
For example, He?de et al (2004) generate de-
scriptions for images of objects shot in uniform
background. Their system relies on a manually
created database of objects indexed by an image
signature (e.g., color and texture) and two key-
words (the object?s name and category). Images
are first segmented into objects, their signature is
retrieved from the database, and a description is
generated using templates. Kojima et al (2002,
2008) create descriptions for human activities in
office scenes. They extract features of human mo-
tion and interleave them with a concept hierarchy
of actions to create a case frame from which a nat-
ural language sentence is generated. Yao et al
(2009) present a general framework for generating
text descriptions of image and video content based
on image parsing. Specifically, images are hierar-
chically decomposed into their constituent visual
patterns which are subsequently converted into a
semantic representation using WordNet. The im-
age parser is trained on a corpus, manually an-
notated with graphs representing image structure.
A multi-sentence description is generated using a
document planner and a surface realizer.
Within natural language processing most previ-
ous efforts have focused on generating captions to
accompany complex graphical presentations (Mit-
tal et al, 1998; Corio and Lapalme, 1999; Fas-
ciano and Lapalme, 2000; Feiner and McKeown,
1990) or on using the captions accompanying in-
formation graphics to infer their intended mes-
sage, e.g., the author?s goal to convey ostensible
increase or decrease of a quantity of interest (Elzer
et al, 2005). Little emphasis is placed on image
processing; it is assumed that the data used to cre-
ate the graphics are available, and the goal is to
enable users understand the information expressed
in them.
The task of generating captions for news im-
ages is novel to our knowledge. Instead of relying
on manual annotation or background ontological
information we exploit a multimodal database of
news articles, images, and their captions. The lat-
ter is admittedly noisy, yet can be easily obtained
from on-line sources, and contains rich informa-
tion about the entities and events depicted in the
images and their relations. Similar to previous
work, we also follow a two-stage approach. Us-
ing an image annotation model, we first describe
the picture with keywords which are subsequently
realized into a human readable sentence. The
caption generation task bears some resemblance
to headline generation (Dorr et al, 2003; Banko
et al, 2000; Jin and Hauptmann, 2002) where the
aim is to create a very short summary for a doc-
ument. Importantly, we aim to create a caption
that not only summarizes the document but is also
a faithful to the image?s content (i.e., the caption
should also mention some of the objects or indi-
viduals depicted in the image). We therefore ex-
plore extractive and abstractive models that rely
on visual information to drive the generation pro-
cess. Our approach thus differs from most work in
summarization which is solely text-based.
3 Problem Formulation
We formulate image caption generation as fol-
lows. Given an image I, and a related knowl-
edge database ?, create a natural language descrip-
tion C which captures the main content of the im-
age under ?. Specifically, in the news story sce-
nario, we will generate a caption C for an image I
and its accompanying document D. The training
data thus consists of document-image-caption tu-
1240
Thousands of Tongans have
attended the funeral of King
Taufa?ahau Tupou IV, who
died last week at the age
of 88. Representatives
from 30 foreign countries
watched as the king?s coffin
was carried by 1,000 men
to the official royal burial
ground.
King Tupou, who was 88,
died a week ago.
A Nasa satellite has doc-
umented startling changes
in Arctic sea ice cover be-
tween 2004 and 2005. The
extent of ?perennial? ice
declined by 14%, losing an
area the size of Pakistan
or Turkey. The last few
decades have seen ice cover
shrink by about 0.7% per
year.
Satellite instruments can
distinguish ?old? Arctic
ice from ?new?.
Contaminated Cadbury?s
chocolate was the most
likely cause of an outbreak
of salmonella poisoning,
the Health Protection
Agency has said. About 36
out of a total of 56 cases of
the illness reported between
March and July could be
linked to the product.
Cadbury will increase its
contamination testing levels.
A third of children in the
UK use blogs and social
network websites but two
thirds of parents do not
even know what they
are, a survey suggests.
The children?s charity
NCH said there was ?an
alarming gap? in techno-
logical knowledge between
generations.
Children were found to be
far more internet-wise than
parents.
Table 1: Each entry in the BBC News database contains a document an image, and its caption.
ples like the ones shown in Table 1. During test-
ing, we are given a document and an associated
image for which we must generate a caption.
Our experiments used the dataset created by
Feng and Lapata (2008).2 It contains 3,361 articles
downloaded from the BBC News website3 each of
which is associated with a captioned news image.
The latter is usually 203 pixels wide and 152 pix-
els high. The average caption length is 9.5 words,
the average sentence length is 20.5 words, and
the average document length 421.5 words. The
caption vocabulary is 6,180 words and the docu-
ment vocabulary is 26,795. The vocabulary shared
between captions and documents is 5,921 words.
The captions tend to use half as many words as
the document sentences, and more than 50% of the
time contain words that are not attested in the doc-
ument (even though they may be attested in the
collection).
Generating image captions is a challenging task
even for humans, let alne computers. Journalists
are given explicit instructions on how to write cap-
tions4 and laypersons do not always agree on what
a picture depicts (von Ahn and Dabbish, 2004).
Along with the title, the lead, and section head-
ings, captions are the most commonly read words
2Available from http://homepages.inf.ed.ac.uk/
s677528/data/
3http://news.bbc.co.uk/
4See http://www.theslot.com/captions.html and
http://www.thenewsmanual.net/ for tips on how to write
good captions.
in an article. A good caption must be succinct and
informative, clearly identify the subject of the pic-
ture, establish the picture?s relevance to the arti-
cle, provide context for the picture, and ultimately
draw the reader into the article. It is also worth
noting that journalists often write their own cap-
tions rather than simply extract sentences from the
document. In doing so they rely on general world
knowledge but also expertise in current affairs that
goes beyond what is described in the article or
shown in the picture.
4 Image Annotation
As mentioned earlier, our approach relies on an
image annotation model to provide description
keywords for the picture. Our experiments made
use of the probabilistic model presented in Feng
and Lapata (2010). The latter is well-suited to our
task as it has been developed with noisy, multi-
modal data sets in mind. The model is based on the
assumption that images and their surrounding text
are generated by mixtures of latent topics which
are inferred from a concatenated representation of
words and visual features.
Specifically, images are preprocessed so that
they are represented by word-like units. Lo-
cal image descriptors are computed using the
Scale Invariant Feature Transform (SIFT) algo-
rithm (Lowe, 1999). The general idea behind the
algorithm is to first sample an image with the
difference-of-Gaussians point detector at different
1241
scales and locations. Importantly, this detector is,
to some extent, invariant to translation, scale, ro-
tation and illumination changes. Each detected re-
gion is represented with a SIFT descriptor which
is a histogram of edge directions at different lo-
cations. Subsequently SIFT descriptors are quan-
tized into a discrete set of visual terms via a clus-
tering algorithm such as K-means.
The model thus works with a bag-of-words rep-
resentation and treats each article-image-caption
tuple as a single document dMix consisting of tex-
tual and visual words. Latent Dirichlet Allocation
(LDA, Blei et al 2003) is used to infer the latent
topics assumed to have generated dMix. The ba-
sic idea underlying LDA, and topic models in gen-
eral, is that each document is composed of a prob-
ability distribution over topics, where each topic
represents a probability distribution over words.
The document-topic and topic-word distributions
are learned automatically from the data and pro-
vide information about the semantic themes cov-
ered in each document and the words associated
with each semantic theme. The image annotation
model takes the topic distributions into account
when finding the most likely keywords for an im-
age and its associated document.
More formally, given an image-caption-
document tuple (I,C,D) the model finds the
subset of keywords WI (WI ? W ) which appro-
priately describe I. Assuming that keywords
are conditionally independent, and I, D are
represented jointly by dMix, the model estimates:
W ?I ? argmax
Wt
?
wt?Wt
P(wt |dMix) (1)
= argmax
Wt
?
wt?Wt
K
?
k=1
P(wt |zk)P(zk|dMix)
Wt denotes a set of description keywords (the sub-
script t is used to discriminate from the visual
words which are not part of the model?s output),
K the number of topics, P(wt |zk) the multimodal
word distributions over topics, and P(zk|dMix) the
estimated posterior of the topic proportions over
documents. Given an unseen image-document
pair and trained multimodal word distributions
over topics, it is possible to infer the posterior of
topic proportions over the new data by maximizing
the likelihood. The model delivers a ranked list of
textual words wt , the n-best of which are used as
annotations for image I.
It is important to note that the caption gener-
ation models we propose are not especially tied
to the above annotation model. Any probabilis-
tic model with broadly similar properties could
serve our purpose. Examples include PLSA-based
approaches to image annotation (e.g., Monay
and Gatica-Perez 2007) and correspondence LDA
(Blei and Jordan, 2003).
5 Extractive Caption Generation
Much work in summarization to date focuses on
sentence extraction where a summary is created
simply by identifying and subsequently concate-
nating the most important sentences in a docu-
ment. Without a great deal of linguistic analysis, it
is possible to create summaries for a wide range of
documents, independently of style, text type, and
subject matter. For our caption generation task, we
need only extract a single sentence. And our guid-
ing hypothesis is that this sentence must be max-
imally similar to the description keywords gener-
ated by the annotation model. We discuss below
different ways of operationalizing similarity.
Word Overlap Perhaps the simplest way of
measuring the similarity between image keywords
and document sentences is word overlap:
Overlap(WI,Sd) =
|WI ?Sd |
|WI ?Sd |
(2)
where WI is the set of keywords and Sd a sentence
in the document. The caption is then the sentence
that has the highest overlap with the keywords.
Cosine Similarity Word overlap is admittedly
a naive measure of similarity, based on lexical
identity. We can overcome this by representing
keywords and sentences in vector space (Salton
and McGill, 1983). The latter is a word-sentence
co-occurrence matrix where each row represents
a word, each column a sentence, and each en-
try the frequency with which the word appeared
within the sentence. More precisely matrix cells
are weighted by their tf-idf values. The similarity
of the vectors representing the keywords
??
WI and
document sentence
??
Sd can be quantified by mea-
suring the cosine of their angle:
sim(
??
WI,
??
Sd) =
??
WI ?
??
Sd
|
?????
WI||
??
Sd |
(3)
Probabilistic Similarity Recall that the back-
bone of our image annotation model is a topic
model with images and documents represented as
a probability distribution over latent topics. Un-
der this framework, the similarity between an im-
1242
age and a sentence can be broadly measured by the
extent to which they share the same topic distribu-
tions (Steyvers and Griffiths, 2007). For example,
we may use the KL divergence to measure the dif-
ference between the distributions p and q:
D(p,q) =
K
?
j=1
p j log2
p j
q j
(4)
where p and q are shorthand for the image
topic distribution PdMix and sentence topic distri-
bution PSd , respectively. When doing inference on
the document sentence, we also take its neighbor-
ing sentences into account to avoid estimating in-
accurate topic proportions on short sentences.
The KL divergence is asymmetric and in many
applications, it is preferable to apply a symmet-
ric measure such as the Jensen Shannon (JS) di-
vergence. The latter measures the ?distance? be-
tween p and q through (p+q)2 , the average of p
and q:
JS(p,q) =
1
2
[
D(p,
(p+q)
2
)+D(q,
(p+q)
2
)
]
(5)
6 Abstractive Caption Generation
Although extractive methods yield grammatical
captions and require relatively little linguistic
analysis, there are a few caveats to consider.
Firstly, there is often no single sentence in the doc-
ument that uniquely describes the image?s content.
In most cases the keywords are found in the doc-
ument but interspersed across multiple sentences.
Secondly, the selected sentences make for long
captions (sometimes longer than the average doc-
ument sentence), are not concise and overall not
as catchy as human-written captions. For these
reasons we turn to abstractive caption generation
and present models based on single words but also
phrases.
Word-based Model Our first abstractive model
builds on and extends a well-known probabilistic
model of headline generation (Banko et al, 2000).
The task is related to caption generation, the aim is
to create a short, title-like headline for a given doc-
ument, without however taking visual information
into account. Like captions, headlines have to be
catchy to attract the reader?s attention.
Banko et al (2000) propose a bag-of-words
model for headline generation. It consists of con-
tent selection and surface realization components.
Content selection is modeled as the probability of
a word appearing in the headline given the same
word appearing in the corresponding document
and is independent from other words in the head-
line. The likelihood of different surface realiza-
tions is estimated using a bigram model. They also
take the distribution of the length of the headlines
into account in an attempt to bias the model to-
wards generating concise output:
P(w1,w2, ...,wn) =
n
?
i=1
P(wi ? H|wi ? D) (6)
?P(len(H) = n)
?
n
?
i=2
P(wi|wi?1)
where wi is a word that may appear in head-
line H, D the document being summarized,
and P(len(H) = n) a headline length distribution
model.
The above model can be easily adapted to the
caption generation task. Content selection is now
the probability of a word appearing in the cap-
tion given the image and its associated document
which we obtain from the output of our image an-
notation model (see Section 4). In addition we re-
place the bigram surface realizer with a trigram:
P(w1,w2, ...,wn) =
n
?
i=1
P(wi ?C|I,D) (7)
?P(len(C) = n)
?
n
?
i=3
P(wi|wi?1,wi?2)
where C is the caption, I the image, D the accom-
panying document, and P(wi ? C|I,D) the image
annotation probability.
Despite its simplicity, the caption generation
model in (7) has a major drawback. The content
selection component will naturally tend to ignore
function words, as they are not descriptive of the
image?s content. This will seriously impact the
grammaticality of the generated captions, as there
will be no appropriate function words to glue the
content words together. One way to remedy this
is to revert to a content selection model that ig-
nores the image and simply estimates the prob-
ability of a word appearing in the caption given
the same word appearing in the document. At the
same time we modify our surface realization com-
ponent so that it takes note of the image annotation
probabilities. Specifically, we use an adaptive lan-
guage model (Kneser et al, 1997) that modifies an
1243
n-gram model with local unigram probabilities:
P(w1,w2, ...,wn) =
n
?
i=1
P(wi ?C|wi ? D) (8)
?P(len(C) = n)
?
n
?
i=3
Padap(wi|wi?1,wi?2)
where P(wi ?C|wi ?D) is the probability of wi ap-
pearing in the caption given that it appears in
the document D, and Padap(wi|wi?1,wi?2) the lan-
guage model adapted with probabilities from our
image annotation model:
Padap(w|h) =
?(w)
z(h)
Pback(w|h) (9)
?(w)? (
Padap(w)
Pback(w)
)? (10)
z(h) =?
w
?(w) ?Pback(w|h) (11)
where Pback(w|h) is the probability of w given
the history h of preceding words (i.e., the orig-
inal trigram model), Padap(w) the probability
of w according to the image annotation model,
Pback(w) the probability of w according to the orig-
inal model, and ? a scaling parameter.
Phrase-based Model The model outlined in
equation (8) will generate captions with function
words. However, there is no guarantee that these
will be compatible with their surrounding context
or that the caption will be globally coherent be-
yond the trigram horizon. To avoid these prob-
lems, we turn our attention to phrases which are
naturally associated with function words and can
potentially capture long-range dependencies.
Specifically, we obtain phrases from the out-
put of a dependency parser. A phrase is sim-
ply a head and its dependents with the exception
of verbs, where we record only the head (other-
wise, an entire sentence could be a phrase). For
example, from the first sentence in Table 1 (first
row, left document) we would extract the phrases:
thousands of Tongans, attended, the funeral, King
Taufa?ahau Tupou IV, last week, at the age, died,
and so on. We only consider dependencies whose
heads are nouns, verbs, and prepositions, as these
constitute 80% of all dependencies attested in our
caption data. We define a bag-of-phrases model
for caption generation by modifying the content
selection and caption length components in equa-
tion (8) as follows:
P(?1,?2, ...,?m) ?
m
?
j=1
P(? j ?C|? j ? D) (12)
?P(len(C) =
m
?
j=1
len(? j))
?
?mj=1 len(? j)
?
i=3
Padap(wi|wi?1,wi?2)
Here, P(? j ?C|? j ? D) models the probability of
phrase ? j appearing in the caption given that it also
appears in the document and is estimated as:
P(? j ?C|? j ? D) = ?
w j?? j
P(w j ?C|w j ? D) (13)
where w j is a word in the phrase ? j.
One problem with the models discussed thus
far is that words or phrases are independent of
each other. It is up to the trigram model to en-
force coarse ordering constraints. These may be
sufficient when considering isolated words, but
phrases are longer and their combinations are sub-
ject to structural constraints that are not captured
by sequence models. We therefore attempt to take
phrase attachment constraints into account by es-
timating the probability of phrase ? j attaching to
the right of phrase ?i as:
P(? j|?i)= ?
wi??i
?
w j?? j
p(w j|wi) (14)
=
1
2 ?wi??i
?
w j?? j
{
f (wi,w j)
f (wi,?)
+
f (wi,w j)
f (?,w j)
}
where p(w j|wi) is the probability of a phrase con-
taining word w j appearing to the right of a phrase
containing word wi, f (wi,w j) indicates the num-
ber of times wi and w j are adjacent, f (wi,?) is
the number of times wi appears on the left of any
phrase, and f (?,wi) the number of times it ap-
pears on the right.5
After integrating the attachment probabilities
into equation (12), the caption generation model
becomes:
P(?1,?2, ...,?m)?
m
?
j=1
P(? j ?C|? j ? D) (15)
?
m
?
j=2
P(? j|? j?1)
?P(len(C) = ?mj=1 len(? j))
??
m
?
j=1
len(? j)
i=3 Padap(wi|wi?1,wi?2)
5Equation (14) is smoothed to avoid zero probabilities.
1244
On the one hand, the model in equation (15) takes
long distance dependency constraints into ac-
count, and has some notion of syntactic structure
through the use of attachment probabilities. On
the other hand, it has a primitive notion of caption
length estimated by P(len(C) = ?mj=1 len(? j)) and
will therefore generate captions of the same
(phrase) length. Ideally, we would like the model
to vary the length of its output depending on the
chosen context. However, we leave this to future
work.
Search To generate a caption it is neces-
sary to find the sequence of words that maxi-
mizes P(w1,w2, ...,wn) for the word-based model
(equation (8)) and P(?1,?2, ...,?m) for the
phrase-based model (equation (15)). We rewrite
both probabilities as the weighted sum of their log
form components and use beam search to find a
near-optimal sequence. Note that we can make
search more efficient by reducing the size of the
document D. Using one of the models from Sec-
tion 5, we may rank its sentences in terms of
their relevance to the image keywords and con-
sider only the n-best ones. Alternatively, we could
consider the single most relevant sentence together
with its surrounding context under the assumption
that neighboring sentences are about the same or
similar topics.
7 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the caption gen-
eration models presented above. We give details
on our training procedure, parameter estimation,
and present the baseline methods used for com-
parison with our models.
Data All our experiments were conducted on
the corpus created by Feng and Lapata (2008),
following their original partition of the data
(2,881 image-caption-document tuples for train-
ing, 240 tuples for development and 240 for test-
ing). Documents and captions were parsed with
the Stanford parser (Klein and Manning, 2003) in
order to obtain dependencies for the phrase-based
abstractive model.
Model Parameters For the image annotation
model we extracted 150 (on average) SIFT fea-
tures which were quantized into 750 visual
terms. The underlying topic model was trained
with 1,000 topics using only content words
(i.e., nouns, verbs, and adjectives) that appeared
no less than five times in the corpus. For all
models discussed here (extractive and abstractive)
we report results with the 15 best annotation key-
words. For the abstractive models, we used a
trigram model trained with the SRI toolkit on a
newswire corpus consisting of BBC and Yahoo!
news documents (6.9 M words). The attachment
probabilities (see equation (14)) were estimated
from the same corpus. We tuned the caption
length parameter on the development set using a
range of [5,14] tokens for the word-based model
and [2,5] phrases for the phrase-based model. Fol-
lowing Banko et al (2000), we approximated the
length distribution with a Gaussian. The scaling
parameter ? for the adaptive language model was
also tuned on the development set using a range
of [0.5,0.9]. We report results with ? set to 0.5.
For the abstractive models the beam size was set
to 500 (with at least 50 states for the word-based
model). For the phrase-based model, we also ex-
perimented with reducing the search scope, ei-
ther by considering only the n most similar sen-
tences to the keywords (range [2,10]), or simply
the single most similar sentence and its neighbors
(range [2,5]). The former method delivered better
results with 10 sentences (and the KL divergence
similarity function).
Evaluation We evaluated the performance of
our models automatically, and also by eliciting hu-
man judgments. Our automatic evaluation was
based on Translation Edit Rate (TER, Snover et al
2006), a measure commonly used to evaluate the
quality of machine translation output. TER is de-
fined as the minimum number of edits a human
would have to perform to change the system out-
put so that it exactly matches a reference transla-
tion. In our case, the original captions written by
the BBC journalists were used as reference:
TER(E,Er) =
Ins+Del+Sub+Shft
Nr
(16)
where E is the hypothetical system output, Er the
reference caption, and Nr the reference length.
The number of possible edits include insertions
(Ins), deletions (Del), substitutions (Sub) and
shifts (Shft). TER is similar to word error rate,
the only difference being that it allows shifts. A
shift moves a contiguous sequence to a different
location within the the same system output and is
counted as a single edit. The perfect TER score
is 0, however note that it can be higher than 1 due
to insertions. The minimum translation edit align-
1245
Model TER AvgLen
Lead sentence 2.12? 21.0
Word Overlap 2.46?? 24.3
Cosine 2.26? 22.0
KL Divergence 1.77?? 18.4
JS Divergence 1.77?? 18.6
Abstract Words 1.11?? 10.0
Abstract Phrases 1.06?? 10.1
Table 2: TER results for extractive, abstractive
models, and lead sentence baseline; ?: sig. dif-
ferent from lead sentence; ?: sig. different from
KL and JS divergence.
ment is usually found through beam search. We
used TER to compare the output of our extractive
and abstractive models and also for parameter tun-
ing (see the discussion above).
In our human evaluation study participants were
presented with a document, an associated image,
and its caption, and asked to rate the latter on two
dimensions: grammaticality (is the sentence flu-
ent or word salad?) and relevance (does it de-
scribe succinctly the content of the image and doc-
ument?). We used a 1?7 rating scale, participants
were encouraged to give high ratings to captions
that were grammatical and appropriate descrip-
tions of the image given the accompanying docu-
ment. We randomly selected 12 document-image
pairs from the test set and generated captions for
them using the best extractive system, and two ab-
stractive systems (word-based and phrase-based).
We also included the original human-authored
caption as an upper bound. We collected ratings
from 23 unpaid volunteers, all self reported native
English speakers. The study was conducted over
the Internet.
8 Results
Table 2 reports our results on the test set us-
ing TER. We compare four extractive models
based on word overlap, cosine similarity, and two
probabilistic similarity measures, namely KL and
JS divergence and two abstractive models based
on words (see equation (8)) and phrases (see equa-
tion (15)). We also include a simple baseline that
selects the first document sentence as a caption
and show the average caption length (AvgLen) for
each model. We examined whether performance
differences among models are statistically signifi-
cant, using the Wilcoxon test.
Model Grammaticality Relevance
KL Divergence 6.42?? 4.10??
Abstract Words 2.08? 3.20?
Abstract Phrases 4.80? 4.96?
Gold Standard 6.39?? 5.55?
Table 3: Mean ratings on caption output elicited
by humans; ?: sig. different from word-
based abstractive system; ?: sig. different from
phrase-based abstractive system.
As can be seen the probabilistic models (KL and
JS divergence) outperform word overlap and co-
sine similarity (all differences are statistically sig-
nificant, p < 0.01).6 They make use of the same
topic model as the image annotation model, and
are thus able to select sentences that cover com-
mon content. They are also significantly better
than the lead sentence which is a competitive base-
line. It is well known that news articles are written
so that the lead contains the most important infor-
mation in a story.7 This is an encouraging result
as it highlights the importance of the visual infor-
mation for the caption generation task. In general,
word overlap is the worst performing model which
is not unexpected as it does not take any lexical
variation into account. Cosine is slightly better
but not significantly different from the lead sen-
tence. The abstractive models obtain the best TER
scores overall, however they generate shorter cap-
tions in comparison to the other models (closer to
the length of the gold standard) and as a result TER
treats them favorably, simply because the number
of edits is less. For this reason we turn to the re-
sults of our judgment elicitation study which as-
sesses in more detail the quality of the generated
captions.
Recall that participants judge the system out-
put on two dimensions, grammaticality and rele-
vance. Table 3 reports mean ratings for the out-
put of the extractive system (based on the KL di-
vergence), the two abstractive systems, and the
human-authored gold standard caption. We per-
formed an Analysis of Variance (ANOVA) to ex-
amine the effect of system type on the generation
task. Post-hot Tukey tests were carried out on the
mean of the ratings shown in Table 3 (for gram-
maticality and relevance).
6We also note that mean length differences are not signif-
icant among these models.
7As a rule of thumb the lead should answer most or all of
the five W?s (who, what, when, where, why).
1246
G: King Tupou, who was 88, died a week ago.
KL: Last year, thousands of Tongans took part in unprece-
dented demonstrations to demand greater democracy
and public ownership of key national assets.
AW : King Toupou IV died at the age of Tongans last week.
AP: King Toupou IV died at the age of 88 last week.
G: Cadbury will increase its contamination testing levels.
KL: Contaminated Cadbury?s chocolate was the most
likely cause of an outbreak of salmonella poisoning,
the Health Protection Agency has said.
AW : Purely dairy milk buttons Easter had agreed to work
has caused.
AP: The 105g dairy milk buttons Easter egg affected by
the recall.
G: Satellite instruments can distinguish ?old? Arctic ice
from ?new?.
KL: So a planet with less ice warms faster, potentially turn-
ing the projected impacts of global warming into real-
ity sooner than anticipated.
AW : Dr less winds through ice cover all over long time
when.
AP: The area of the Arctic covered in Arctic sea ice cover.
G: Children were found to be far more internet-wise than
parents.
KL: That?s where parents come in.
AW : The survey found a third of children are about mobile
phones.
AP: The survey found a third of children in the driving
seat.
Table 4: Captions written by humans (G) and gen-
erated by extractive (KL), word-based abstractive
(AW ), and phrase-based extractive (AP systems).
The word-based system yields the least gram-
matical output. It is significantly worse than the
phrase-based abstractive system (? < 0.01), the
extractive system (? < 0.01), and the gold stan-
dard (? < 0.01). Unsurprisingly, the phrase-based
system is significantly less grammatical than the
gold standard and the extractive system, whereas
the latter is perceived as equally grammatical as
the gold standard (the difference in the means is
not significant). With regard to relevance, the
word-based system is significantly worse than the
phrase-based system, the extractive system, and
the gold-standard. Interestingly, the phrase-based
system performs on the same level with the hu-
man gold standard (the difference in the means is
not significant) and significantly better than the ex-
tractive system. Overall, the captions generated by
the phrase-based system, capture the same content
as the human-authored captions, even though they
tend to be less grammatical. Examples of system
output for the image-document pairs shown in Ta-
ble 1 are given in Table 4 (the first row corresponds
to the left picture (top row) in Table 1, the second
row to the right picture, and so on).
9 Conclusions
We have presented extractive and abstractive mod-
els that generate image captions for news articles.
A key aspect of our approach is to allow both
the visual and textual modalities to influence the
generation task. This is achieved through an im-
age annotation model that characterizes pictures
in terms of description keywords that are subse-
quently used to guide the caption generation pro-
cess. Our results show that the visual information
plays an important role in content selection. Sim-
ply extracting a sentence from the document often
yields an inferior caption. Our experiments also
show that a probabilistic abstractive model defined
over phrases yields promising results. It generates
captions that are more grammatical than a closely
related word-based system and manages to capture
the gist of the image (and document) as well as the
captions written by journalists.
Future extensions are many and varied. Rather
than adopting a two-stage approach, where the im-
age processing and caption generation are carried
out sequentially, a more general model should in-
tegrate the two steps in a unified framework. In-
deed, an avenue for future work would be to de-
fine a phrase-based model for both image annota-
tion and caption generation. We also believe that
our approach would benefit from more detailed
linguistic and non-linguistic information. For in-
stance, we could experiment with features related
to document structure such as titles, headings, and
sections of articles and also exploit syntactic infor-
mation more directly. The latter is currently used
in the phrase-based model by taking attachment
probabilities into account. We could, however, im-
prove grammaticality more globally by generating
a well-formed tree (or dependency graph).
References
Banko, Michel, Vibhu O. Mittal, and Micheael J.
Witbrock. 2000. Headline generation based on
statistical translation. In Proceedings of the 38th
Annual Meeting on Association for Computa-
tional Linguistics. Hong Kong, pages 318?325.
Barnard, Kobus, Pinar Duygulu, David Forsyth,
Nando de Freitas, David Blei, and Michael
Jordan. 2002. Matching words and pictures.
Journal of Machine Learning Research 3:1107?
1135.
Blei, David and Michael Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th An-
1247
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval. Toronto, ON, pages 127?134.
Blei, David, Andrew Ng, and Michael Jordan.
2003. Latent Dirichlet alocation. Journal of
Machine Learning Research 3:993?1022.
Corio, Marc and Guy Lapalme. 1999. Generation
of texts for information graphics. In Proceed-
ings of the 7th European Workshop on Natural
Language Generation. Toulouse, France, pages
49?58.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim ap-
proach to headline generation. In Proceed-
ings of the HLT-NAACL 2003 Workshop on Text
Summarization. Edmonton, Canada, pages 1?8.
Duygulu, Pinar, Kobus Barnard, Nando de Freitas,
and David Forsyth. 2002. Object recognition as
machine translation: Learning a lexicon for a
fixed image vocabulary. In Proceedings of the
7th European Conference on Computer Vision.
Copenhagen, Denmark, pages 97?112.
Elzer, Stephanie, Sandra Carberry, Ingrid Zuker-
man, Daniel Chester, Nancy Green, , and Seniz
Demir. 2005. A probabilistic framework for rec-
ognizing intention in information graphics. In
Proceedings of the 19th International Confer-
ence on Artificial Intelligence. Edinburgh, Scot-
land, pages 1042?1047.
Fasciano, Massimo and Guy Lapalme. 2000. In-
tentions in the coordinated generation of graph-
ics and text from tabular data. Knowledge In-
formation Systems 2(3):310?339.
Feiner, Steven and Kathleen McKeown. 1990. Co-
ordinating text and graphics in explanation gen-
eration. In Proceedings of National Conference
on Artificial Intelligence. Boston, MA, pages
442?449.
Feng, Shaolei Feng, Victor Lavrenko, and R Man-
matha. 2004. Multiple Bernoulli relevance
models for image and video annotation. In
Proceedings of the International Conference
on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002?1009.
Feng, Yansong and Mirella Lapata. 2008. Au-
tomatic image annotation using auxiliary text
information. In Proceedings of the 46th An-
nual Meeting of the Association of Computa-
tional Linguistics: Human Language Technolo-
gies. Columbus, OH, pages 272?280.
Feng, Yansong and Mirella Lapata. 2010. Topic
models for image annotation and text illustra-
tion. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics. Los
Angeles, LA.
Ferres, Leo, Avi Parush, Shelley Roberts, and
Gitte Lindgaard. 2006. Helping people with
visual impairments gain access to graphical in-
formation through natural language: The graph
system. In Proceedings of 11th International
Conference on Computers Helping People with
Special Needs. Linz, Austria, pages 1122?1130.
He?de, Patrick, Pierre Allain Moe?llic, Joe?l Bour-
geoys, Magali Joint, and Corinne Thomas.
2004. Automatic generation of natural lan-
guage descriptions for images. In Proceed-
ings of Computer-Assisted Information Re-
trieval (Recherche d?Information et ses Appli-
cations Ordinateur) (RIAO). Avignon, France.
Jin, Rong and Alexander G. Hauptmann. 2002. A
new probabilistic model for title generation. In
Proceedings of the 19th International Confer-
ence on Computational linguistics. Taipei, Tai-
wan, pages 1?7.
Klein, Dan and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. In Proceedings
of the 41st Annual Meeting of the Association
of Computational Linguistics. Sapporo, Japan,
pages 423?430.
Kneser, Reinhard, Jochen Peters, and Dietrich
Klakow. 1997. Language model adaptation
using dynamic marginals. In Proceedings of
5th European Conference on Speech Commu-
nication and Technology. Rhodes, Greece, vol-
ume 4, pages 1971?1974.
Kojima, Atsuhiro, Mamoru Takaya, Shigeki Aoki,
Takao Miyamoto, and Kunio Fukunaga. 2008.
Recognition and textual description of human
activities by mobile robot. In Proceedings of
the 3rd International Conference on Innova-
tive Computing Information and Control. IEEE
Computer Society, Washington, DC, pages 53?
56.
Kojima, Atsuhiro, Takeshi Tamura, and Kunio
Fukunaga. 2002. Natural language description
of human activities from video images based
on concept hierarchy of actions. International
Journal of Computer Vision 50(2):171?184.
Lavrenko, Victor, R. Manmatha, and Jiwoon Jeon.
2003. A model for learning the semantics of
1248
pictures. In Proceedings of the 16th Conference
on Advances in Neural Information Processing
Systems. Vancouver, BC.
Lowe, David G. 1999. Object recognition from
local scale-invariant features. In Proceedings of
International Conference on Computer Vision.
IEEE Computer Society, pages 1150?1157.
Mittal, Vibhu O., Johanna D. Moore, Giuseppe
Carenini, and Steven Roth. 1998. Describing
complex charts in natural language: A caption
generation system. Computational Linguistics
24:431?468.
Monay, Florent and Daniel Gatica-Perez. 2007.
Modeling semantic aspects for cross-media
image indexing. IEEE Transactions on
Pattern Analysis and Machine Intelligence
29(10):1802?1817.
Salton, Gerard and M.J. McGill. 1983. In-
troduction to Modern Information Retrieval.
McGraw-Hill, New York.
Smeulders, Arnols W.M., Marcel Worring, Si-
mone Santini, Amarnath Gupta, and Ramesh
Jain. 2000. Content-based image retrieval at
the end of the early years. IEEE Transactions
on Pattern Analysis and Machine Intelligence
22(12):1349?1380.
Snover, Matthew, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Trans-
lation in the Americas. Cambridge, pages 223?
231.
Steyvers, Mark and Tom Griffiths. 2007. Proba-
bilistic topic models. In T. Landauer, D. Mc-
Namara, S Dennis, and W Kintsch, editors, A
Handbook of Latent Semantic Analysis, Psy-
chology Press.
Vailaya, Aditya, Ma?rio A. T. Figueiredo, Anil K.
Jain, and Hong-Jiang Zhang. 2001. Image clas-
sification for content-based indexing. IEEE
Transactions on Image Processing 10:117?130.
von Ahn, Luis and Laura Dabbish. 2004. Labeling
images with a computer game. In ACM Confer-
ence on Human Factors in Computing Systems.
New York, NY, pages 319?326.
Wang, Chong, David Blei, and Li Fei-Fei. 2009.
Simultaneous image classification and annota-
tion. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recog-
nition. Miami, FL, pages 1903?1910.
Yao, Benjamin, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song chun Zhu. 2009. I2t: Image pars-
ing to text description. Proceedings of IEEE (in-
vited for the special issue on Internet Vision) .
1249
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1562?1572,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Plot Induction and Evolutionary Search for Story Generation
Neil McIntyre and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
n.d.mcintyre@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we develop a story genera-
tor that leverages knowledge inherent in
corpora without requiring extensive man-
ual involvement. A key feature in our ap-
proach is the reliance on a story planner
which we acquire automatically by record-
ing events, their participants, and their
precedence relationships in a training cor-
pus. Contrary to previous work our system
does not follow a generate-and-rank archi-
tecture. Instead, we employ evolutionary
search techniques to explore the space of
possible stories which we argue are well
suited to the story generation task. Experi-
ments on generating simple children?s sto-
ries show that our system outperforms pre-
vious data-driven approaches.
1 Introduction
Computer story generation has met with fasci-
nation since the early days of artificial intelli-
gence. Indeed, over the years, several genera-
tors have been developed capable of creating sto-
ries that resemble human output. To name only
a few, TALE-SPIN (Meehan, 1977) generates sto-
ries through problem solving, MINSTREL (Turner,
1992) relies on an episodic memory scheme, es-
sentially a repository of previous hand-coded sto-
ries, to solve the problems in the current story,
and MAKEBELIEVE (Liu and Singh, 2002) uses
commonsense knowledge to generate short stories
from an initial seed story (supplied by the user). A
large body of more recent work views story gener-
ation as a form of agent-based planning (Swartjes
and Theune, 2008; Pizzi et al, 2007). The agents
act as characters with a list of goals. They form
plans of action and try to fulfill them. Interesting
stories emerge as plans interact and cause failures
and possible replanning.
The broader appeal of computational story gen-
eration lies in its application potential. Examples
include the entertainment industry and the devel-
opment of tools that produce large numbers of
plots automatically that might provide inspiration
to professional screen writers (Agudo et al, 2004);
rendering video games more interesting by allow-
ing the plot to adapt dynamically to the players?
actions (Barros and Musse, 2007); and assisting
teachers to create or personalize stories for their
students (Riedl and Young, 2004).
A major stumbling block for the widespread use
of computational story generators is their reliance
on expensive, manually created resources. A typi-
cal story generator will make use of a knowledge
base for providing detailed domain-specific infor-
mation about the characters and objects involved
in the story and their relations. It will also have a
story planner that specifies how these characters
interact, what their goals are and how their ac-
tions result in different story plots. Finally, a sen-
tence planner (coupled with a surface realizer) will
render an abstract story specification into natural
language text. Traditionally, most of this knowl-
edge is created by hand, and the effort must be re-
peated for new domains, new characters and plot
elements.
Fortunately, recent work in natural language
processing has taken significant steps towards de-
veloping algorithms that learn some of this knowl-
edge automatically from natural language cor-
pora. Chambers and Jurafsky (2009, 2008) pro-
pose an unsupervised method for learning narra-
tive schemas, chains of events whose arguments
are filled with participant semantic roles defined
over words. An example schema is {X arrest, X
charge, X raid, X seize, X confiscate, X detain, X
deport}, where X stands for the argument types
{police, agent, authority, government}. Their ap-
proach relies on the intuition that in a coherent
text events that are about the same participants are
1562
likely to be part of the same story or narrative.
Their model extracts narrative chains, essentially
events that share argument slots and merges them
into schemas. The latter could be used to construct
or enrich the knowledge base of a story generator.
In McIntyre and Lapata (2009) we presented a
story generator that leverages knowledge inherent
in corpora without requiring extensive manual in-
volvement. The generator operates over predicate-
argument and predicate-predicate co-occurrence
tuples gathered from training data. These are used
to produce a large set of candidate stories which
are subsequently ranked based on their interest-
ingness and coherence. The approach is unusual
in that it does not involve an explicit story plan-
ning component. Stories are created stochastically
by selecting entities and the events they are most
frequently attested with.
In this work we develop a story generator that
is also data-driven but crucially relies on a story
planner for creating meaningful stories. Inspired
by Chambers and Jurafsky (2009) we acquire story
plots automatically by recording events, their par-
ticipants, and their precedence relationships as at-
tested in a training corpus. Entities give rise to
different potential plots which in turn generate
multiple stories. Contrary to our previous work
(McIntyre and Lapata, 2009), we do not follow a
generate-and-rank architecture. Instead, we search
the space of possible stories using Genetic Algo-
rithms (GAs) which we argue are advantageous
in the story generation setting, as they can search
large fitness landscapes while greatly reducing the
risk of getting stuck in local optima. By virtue of
exploring the search space more broadly, we are
able to generate creative stories without an explicit
interest scoring module.
In the remainder of this paper we give a brief
overview of the system described in McIntyre and
Lapata (2009) and discuss previous applications of
GAs in natural language generation (Section 2).
Next, we detail our approach, specifically how
plots are created and used in conjunction with ge-
netic search (Sections 3 and 4). Finally, we present
our experimental results (Sections 6 and 7) and
conclude the paper with discussion of future work.
2 Related Work
Our work builds on and extends the story genera-
tor developed in McIntyre and Lapata (2009). The
system creates simple children?s stories in an in-
teractive context: the user supplies the topic of the
story and its desired length (number of sentences).
The generator creates a story following a pipeline
architecture typical of natural language generation
systems (Reiter and Dale, 2000) consisting of con-
tent selection, sentence planning, and surface real-
ization.
The content of a story is determined by consult-
ing a data-driven knowledge base that records the
entities (i.e., nouns) appearing in a corpus and the
actions they perform. These are encoded as depen-
dency relations (e.g., subj-verb, verb-obj). In order
to promote between-sentence coherence the gen-
erator also make use of an action graph that con-
tains action-role pairs and the likelihood of tran-
sitioning from one to another. The sentence plan-
ner aggregates together entities and their actions
into a sentence using phrase structure rules. Fi-
nally, surface realization is performed by interfac-
ing RealPro (Lavoie and Rambow, 1997) with a
language model. The system searches for the best
story overall as well as the best sentences that can
be generated from the knowledge base. Unlikely
stories are pruned using beam search. In addition,
stories are reranked using two scoring functions
based on coherence and interest. These are learnt
from training data, i.e., stories labeled with nu-
meric values for interest and coherence.
Evolutionary search techniques have been pre-
viously employed in natural language generation,
especially in the context of document planning.
Structuring a set of facts into a coherent text is ef-
fectively a search problem that may lead to com-
binatorial explosion for large domains. Mellish
et al (1998) (and subsequently Karamanis and
Manurung 2002) advocate genetic algorithms as
an alternative to exhaustively searching for the op-
timal ordering of descriptions of museum arte-
facts. Rather than requiring a global optimum to
be found, the genetic algorithm selects an order
(based on coherence) that is good enough for peo-
ple to understand. Cheng and Mellish (2000) focus
on the interaction of aggregation and text planning
and use genetic algorithms to search for the best
aggregated document that satisfies coherence con-
straints.
The application of genetic algorithms to story
generation is novel to our knowledge. Our work
also departs from McIntyre and Lapata (2009) in
two important ways. Firstly, our generator does
not rely on a knowledge base of seemingly un-
related entities and relations. Rather, we employ
1563
a document planner to create and structure a plot
for a story. The planner is built automatically from
a training corpus and creates plots dynamically
depending on the protagonists of the story. Sec-
ondly, our search procedure is simpler and more
global; instead of searching for the best story twice
(i.e., by first finding the n-best stories and then
subsequently reranking them based on coherence
and interest), our genetic algorithm explores the
space of possible stories once.
3 Plot Generation
Following previous work (e.g., Shim and Kim
2002; McIntyre and Lapata 2009) we assume that
the user supplies a sentence (e.g., the princess
loves the prince) from which the system creates
a story. Each entity in this sentence (e.g., princess,
prince) is associated with its own narrative
schema, a set of key events and actors co-
occurring with it in the training corpus. Our nar-
rative schemas differ slightly from Chambers and
Jurafsky (2009). They acquire schematic represen-
tations of situations akin to FrameNet (Fillmore
et al, 2003): schemas consists of semantically
similar predicates and the entities evoked by them.
In our setting, every entity has its own schema, and
predicates associated with it are ordered. Plots are
generated by merging the entity-specific narrative
schemas which subsequently serve as the input to
the genetic algorithm. In the following we describe
how the narrative schemas are extracted and plots
merged, and then discuss our evolutionary search
procedure.
Entity-based Schema Extraction Before we
can generate a plot for a story we must have an
idea of the actions associated with the entities in
the story, the order in which these actions are per-
formed and also which other entities can partici-
pate. This information is stored in a directed graph
which we explain below. Our algorithm processes
each document at a time, it operates over depen-
dency structures and assumes that entity mentions
have been resolved. In our experiments we used
Rasp (Briscoe and Carroll, 2002), a broad cover-
age dependency parser, and the OpenNLP1 coref-
erence resolution engine.2 However, any depen-
dency parser or coreference tool could serve our
1See http://opennlp.sourceforge.net/.
2The coreference resolution tool we employ is not
error-free and on occasion will fail to resolve a pronoun. We
map unresolved pronouns to the generic labels person or ob-
ject.
purpose. We also assume that the actions associ-
ated with a given entity are ordered and that lin-
ear order corresponds to temporal order. This is a
gross simplification as it is well known that tem-
poral relationships between events are not limited
to precedence, they may overlap, occur simultane-
ously, or be temporally unrelated. We could have
obtained a more accurate ordering using a tempo-
ral classifier (see Chambers and Jurafsky 2008),
however we leave this to future work.
For each entity e in the corpus we build a di-
rected graph G = (V,E) whose nodes V denote
predicate argument relationships, and edges E rep-
resent transitions from node Vi to node Vj. As
an example of our schema construction process,
consider a very small corpus consisting of the
two documents shown in Figure 1. The schema
for princess after processing the first document is
given on the left hand side. Each node in this graph
corresponds to an action attested with princess (we
also record who performs it and where or how).
Nodes are themselves dependency trees (see Fig-
ure 4a), but are linearized in the figure for the
sake of brevity. Edges in the graph indicate order-
ing and are weighted using the mutual informa-
tion metric proposed in Lin (1998) (the weights
are omitted from the example).3 The first sentence
in the text gives rise to the first node in the graph,
the second sentence to the second node, and so on.
Note that the third sentence is not present in the
graph as it is not about the princess.
When processing the second document, we sim-
ply expand this graph. Before inserting a new
node, we check if it can be merged with an al-
ready existing one. Nodes are merged only if they
have the same verb and similar arguments, with
the focal entity (i.e., princess) appearing in the
same argument slot. In our example, the nodes
?prince marry princess in castle? and ?prince
marry princess in temple? can be merged as they
contain the same verb and number of similar ar-
guments. The nodes ?princess have influence?
and ?princess have baby? cannot be merged as
influence and baby are semantically unrelated.
We compute argument similarity using WordNet
(Fellbaum, 1998) and the measure proposed by
Wu and Palmer (1994) which is based on path
length. We merge nodes with related arguments
only if their similarity exceeds a threshold (deter-
mined empirically).
3We use mutual information to identify event sequences
strongly associated with the graph entity.
1564
The goblin holds the princess in a lair.
The prince rescues the princess and
marries her in a castle. The ceremony
is beautiful. The princess has influence
as the prince rules the country.
The dragon holds the princess in a
cave. The prince slays the dragon. The
princess loves the prince. The prince
asks the king?s permission. The prince
marries the princess in the temple. The
princess has a baby.
goblin hold princess in lair
prince rescue princess
prince marry princess in castle
princess have influence
[
goblin
dragon
]
hold princess in
[
lair
cave
]
prince rescue princess princess love prince
prince marry princess in
[
castle
temple
]
princess have influence princess have baby
Figure 1: Example of schema construction for the entity princess
The schema construction algorithm terminates
when graphs like the ones shown in Figure 1 (right
hand side) have been created for all entities in the
corpus.
Building a Story Plot Our generator takes an in-
put sentence and uses it to instantiate several plots.
We achieve this by merging the schemas associ-
ated with the entities in the sentence into a plot
graph. As an example, consider again the sentence
the princess loves the prince which requires comb-
ing the schemas representing prince and princess
shown in Figures 2 and 1 (right hand side), re-
spectively. Again, we look for nodes that can be
merged based on the identity of the actions in-
volved and the (WordNet) similarity of their ar-
guments. However, we disallow the merging of
nodes with focal entities appearing in the same ar-
gument slot (e.g., ?[prince, princess] cries?).
Once the plot graph is created, a depth first
search starting from the node corresponding to
the input sentence, finds all paths with length
matching the desired story length (cycles are dis-
allowed). Assuming we wish to generate a story
consisting of three sentences, the graph in Figure 3
would create four plots. These are (princess love
prince, prince marry princess in [castle, temple],
princess have influence), (princess love prince,
prince marry princess in [castle, temple], princess
have baby), (princess love prince, prince marry
princess in [castle, temple], prince rule country),
and (princess love prince, prince ask king?s per-
mission prince marry princess in [castle, temple]).
Each of these plots represents two different stories
one with castle and one with temple in it.
Sentence Planning The sentence planner is in-
terleaved with the story planner and influences
the final structure of each sentence in the story.
To avoid generating short sentences ? note that
nodes in the plot graph consist of a single ac-
tion and would otherwise correspond to a sentence
with a single clause ? we combine pairs of nodes
within the same graph by looking at intrasenten-
tial verb-verb co-occurrences in the training cor-
pus. For example, the nodes (prince have prob-
lem, prince keep secret) could become the sen-
tence the prince has a problem keeping a secret.
We leave it up to the sentence planner to decide
how the two actions should be combined.4 The
sentence planner will also insert adverbs and ad-
jectives, using co-occurrence likelihoods acquired
from the training corpus. It is essentially a phrase
structure grammar compiled from the lexical re-
sources made available by Korhonen and Briscoe
(2006) and Grishman et al (1994). The grammar
rules act as templates for combining clauses and
filling argument slots.
4We only turn an action into a subclause if its subject en-
tity is same as that of the previous action.
1565
prince slay dragon
prince rescue princess
princess love prince
prince marry princess in
[
castle
temple
]
prince ask king?s permission
prince rule country
Figure 2: Narrative schema for the entity prince.
4 Genetic Algorithms
The example shown in Figure 3 is a simplified ver-
sion of a plot graph. The latter would normally
contain hundreds of nodes and give rise to thou-
sands of stories once lexical variables have been
expanded. Searching the story space is a difficult
optimization problem, that must satisfy several
constraints: the story should be of a certain length,
overall coherent, creative, display some form of
event progression, and generally make sense. We
argue that evolutionary search is appealing here, as
it can find global optimal solutions in a more effi-
cient way than traditional optimization methods.
In this study we employ genetic algorithms
(GAs) a well-known search technique for finding
approximate (or exact) solutions to optimization
problems. The basic idea behind GAs is based
on ?natural selection? and the Darwinian princi-
ple of the survival of the fittest (Mitchell, 1998).
An initial population is randomly created contain-
ing a predefined number of individuals (or solu-
tions), each represented by a genetic string (e.g., a
population of chromosomes). Each individual is
evaluated according to an objective function (also
called a fitness function). A number of individu-
als are then chosen as parents from the population
according to their fitness, and undergo crossover
(also called recombination) and mutation in order
to develop the new population. Offspring with bet-
ter fitness are then inserted into the population,
replacing the inferior individuals in the previous
generation.
The algorithm thus identifies the individuals
with the optimizing fitness values, and those with
lower fitness will naturally get discarded from the
population. This cycle is repeated for a given num-
ber of generations, or stopped when the solution
[
goblin
dragon
]
hold princess in
[
lair
cave
]
prince rescue princess princess love prince
prince marry princess in
[
castle
temple
]
princess have influence
princess have baby
prince slay dragon
prince ask king?s
permission
prince rule country
Figure 3: Plot graph for the input sentence the
princess loves the prince.
obtained is considered optimal. This process leads
to the evolution of a population in which the in-
dividuals are more and more suited to their envi-
ronment, just as natural adaptation. We describe
below how we developed a genetic algorithm for
our story generation problem.
Initial Population Rather than start with a ran-
dom population, we seed the initial population
with story plots generated from our plot graph.
For an input sentence, we generate all possible
plots. The latter are then randomly sampled until a
population of the desired size is created. Contrary
to McIntyre and Lapata (2009), we initialize the
search with complete stories, rather than generate
one sentence at a time. The genetic algorithm will
thus avoid the pitfall of selecting early on a solu-
tion that will later prove detrimental.
Crossover Each plot is represented as an or-
dered graph of dependency trees (corresponding
to sentences). We have decided to use crossover of
a single point between two selected parents. The
children will therefore contain sentences up to the
crossover point of the first parent and sentences
after that point of the second. Figure 4a shows
two parents (prince rescue princess, prince marry
princess in castle, princess have baby) and (prince
rescue princess, prince love princess, princess kiss
prince) and how two new plots are created by
swapping their last nodes.
1566
a) rescue
prince princess
marry
prince princess castle
have
princess baby
rescue
prince princess
love
prince princess
kiss
princess prince
=?
rescue
prince princess
marry
prince princess castle
kiss
prince princess
rescue
prince princess
love
prince princess
have
princess baby
in in
b) marry
prince princess castle
hall
temple
forest
kingdom
c) rescue
prince princess
marry
prince princess castle
kiss
prince princess
in
rescue
prince princess
marry
prince princess castle
kiss
prince princess
in
d) rescue
prince princess
marry
prince princess castle
kiss
prince princess
in
=?
hold
prince princess
e) knows
prince
loves
princess child
=?
escape
princess dragon
Figure 4: Example of genetic algorithm operators as they are applied to plot structures: a) crossover of
two plots on a single point, indicated by the dashed line, resulting in two children which are a recombi-
nation of the parents; b) mutation of a lexical node, church can be replaced from a list of semantically
related candidates; c) sentences can be switched under mutation to create a potentially more coherent
structure; d) if the matrix verb undergoes mutation then, a random sentence is generated to replace it; e)
if the verb chosen for mutation is the head of a subclause, then a random subclause replaces it.
Mutation Mutation can occur on any verb,
noun, adverb, or adjective in the plot. If a noun,
adverb or adjective is chosen to undergo mutation,
then we simply substitute it with a new lexical item
that is sufficiently similar (see Figure 4b for an
example). Verbs, however, have structural impor-
tance in the stories and we cannot simply replace
them without taking account of their arguments.
If a matrix verb is chosen to undergo mutation,
then a new random sentence is generated to re-
place the entire sentence (see Figure 4d). If it is
a subclause, then it is replaced with a randomly
generated clause, headed by a verb that has been
seen in the corpus to co-occur with the matrix verb
(Figure 4e). The sentence planner selects and fills
template trees for generating random clauses. Mu-
tation may also change the order of any two nodes
in the graph in the hope that this will increase the
story?s coherence or create some element of sur-
prise (see Figure 4c).
Selection To choose the plots for the next gener-
ation, we used fitness proportional selection (also
know as roulette-wheel selection, Goldberg 1989)
which chooses candidates randomly but with a
bias towards those with a larger proportion of the
population?s combined fitness. We do not want to
always select the fittest candidates as there may
be valid partial solutions held within less fit mem-
bers of the population. However, we did employ
some elitism by allowing the top 1% of solutions
to be copied straight from one generation to the
next. Note that our candidates may also represent
invalid solutions. For instance, through crossover
it is possible to create a plot in which all or some
nodes are identical. If any such candidates are
identified, they are assigned a low fitness, without
however being eliminated from the population as
some could be used to create fitter solutions.
In a traditional GA, the fitness function deals
with one optimization objective. It is possible to
optimize several objectives either using a vot-
1567
ing model or more sophisticated methods such as
Pareto ranking (Goldberg, 1989). Following previ-
ous work (Mellish et al, 1998) we used a single fit-
ness function that scored candidates based on their
coherence. Our function was learned from training
data using the Entity Grid document representa-
tion proposed in Barzilay and Lapata (2007). An
entity grid is a two-dimensional array in which
columns correspond to entities and rows to sen-
tences. Each cell indicates whether an entity ap-
pears in a given sentence or not and whether it is a
subject, object or neither. For training, this repre-
sentation is converted into a feature vector of en-
tity transition sequences and a model is learnt from
examples of coherent and incoherent stories. The
latter can be easily created by permuting the sen-
tences of coherent stories (assuming that the orig-
inal story is more coherent than its permutations).
In addition to coherence, in McIntyre and La-
pata (2009) we used a scoring function based on
interest which we approximated with lexical and
syntactic features such as the number of noun/verb
tokens/types, the number of subjects/objects, the
number of letters, word familiarity, imagery, and
so on. An interest-based scoring function made
sense in our previous setup as a means of selecting
unusual stories. However, in the context of genetic
search such a function seems redundant as inter-
esting stories emerge naturally through the opera-
tions of crossover and mutation.
5 Surface Realization
Once the final generation of the population has
been reached, the fittest story is selected for sur-
face realization. The realizer takes each sentence
in the story and reformulates it into input com-
patible with the RealPro (Lavoie and Rambow,
1997) text generation engine. Realpro creates sev-
eral variants of the same story differing in the
choice of determiners, number (singular or plural),
and prepositions. A language model is then used
to select the most probable realization (Knight
and Hatzivassiloglou, 1995). Ideally, the realizer
should also select an appropriate tense for the sen-
tence. However, we make the simplifying assump-
tion that all sentences are in the present tense.
6 Experimental Setup
In this section we present our experimental set-up
for assessing the performance of our story genera-
tor. We give details on our training corpus, system,
parameters (such as the population size for the GA
search), the baselines used for comparison, and ex-
plain how our system output was evaluated.
Corpus The generator was trained on the same
corpus used in McIntyre and Lapata (2009), 437
stories from the Andrew Lang fairy tales collec-
tion.5 The average story length is 125.18 sen-
tences. The corpus contains 15,789 word tokens.
Following McIntyre and Lapata, we discarded to-
kens that did not appear in the Children?s Printed
Word Database6, a database of printed word fre-
quencies as read by children aged between five
and nine. From this corpus we extracted narrative
schemas for 667 entities in total. We disregarded
any graph that contained less than 10 nodes as too
small. The graphs had on average 61.04 nodes,
with an average clustering rate7 of 0.027 which in-
dicates that they are substantially connected.
Parameter Setting Considerable latitude is
available when selecting parameters for the GA.
These involve the population size, crossover, and
mutation rates. To evaluate which setting was best,
we asked two human evaluators to rate (on a 1?5
scale) stories produced with a population size
ranging from 1,000 to 10,000, crossover rate of 0.1
to 0.6 and mutation rate of 0.001 to 0.1. For each
run of the system a limit was set to 5,000 genera-
tions. The human ratings revealed that the best sto-
ries were produced for a population size of 10,000,
a crossover rate of 0.1% and a mutation rate
of 0.1%. Compared to previous work (e.g., Kara-
manis and Manurung 2002) our crossover rate
may seem low and the mutation rate high. How-
ever, it makes intuitively sense, as high crossover
may lead to incoherence by disrupting canonical
action sequences found in the plots. On the other
hand, a higher mutation will raise the likelihood of
a lexical item being swapped for another and may
improve overall coherence and interest. The fit-
ness function was trained on 200 documents from
the fairy tales collection using Joachims?s (2002)
SVMlight package and entity transition sequences
of length 2. The realizer was interfaced with a tri-
gram language model trained on the British Na-
tional Corpus with the SRI toolkit.
5Available from http://homepages.inf.ed.ac.uk/
s0233364/McIntyreLapata09/.
6http://www.essex.ac.uk/psychology/cpwd/
7Clustering rate (or transitivity) is the number of triangles
in the graph ? sets of three vertices each of which is con-
nected to each of the others.
1568
Evaluation We compared the stories gener-
ated by the GA against those produced by the
rank-based system described in McIntyre and La-
pata (2009) and a system that creates stories from
the plot graph, without any stochastic search.
Since plot graphs are weighted, we can simply se-
lect the graph with the highest weight. After ex-
panding all lexical variables, the chosen plot graph
will give rise to different stories (e.g., castle or
temple in the example above). We select the story
ranked highest according to our coherence func-
tion. In addition, we included a baseline which
randomly selects sentences from the training cor-
pus provided they contain either of the story pro-
tagonists (i.e., entities in the input sentence). Sen-
tence length was limited to 12 words or less as this
was on average the length of the sentences gener-
ated by our GA system.
Each system created stories for 12 input sen-
tences, resulting in 48 (4?12) stories for eval-
uation. The sentences used commonly occurring
entities in the fairy tales corpus (e.g., The child
watches the bird, The emperor rules the kingdom.,
The wizard casts the spell.). The stories were split
into three sets containing four stories from each
system but with only one story from each input
sentence. All stories had the same length, namely
five sentences. Human judges were presented with
one of the three sets and asked to rate the stories
on a scale of 1 to 5 for fluency (was the sentence
grammatical?), coherence (does the story make
sense overall?) and interest (how interesting is the
story?). The stories were presented in random or-
der and participants were told that all of them
were generated by a computer program. They were
instructed to rate more favorably interesting sto-
ries, stories that were comprehensible and overall
grammatical. The study was conducted over the
Internet using WebExp (Keller et al, 2009) and
was completed by 56 volunteers, all self reported
native English speakers.
7 Results
Our results are summarized in Table 1 which lists
the average human ratings for the four systems.
We performed an Analysis of Variance (ANOVA)
to examine the effect of system type on the story
generation task. Statistical tests were carried out
on the mean of the ratings shown in Table 1 for
fluency, coherence, and interest.
In terms of interest, the GA-based system is sig-
System Fluency Coherence Interest
GA-based 3.09 2.48 2.36
Plot-based 3.03 2.36 2.14?
Rank-based 1.96?? 1.65? 1.85?
Random 3.10 2.23? 2.20?
Table 1: Human evaluation results: mean story
ratings for four story generators; ? : p < 0.05,
?? : p < 0.01, significantly different from
GA-based system.
nificantly better than the Rank-based, Plot-based
and Random ones (using a Post-hoc Tukey test,
? < 0.05). With regard to fluency, the Rank-
based system is significantly worse than the rest
(? < 0.01). Interestingly, the sentences generated
by the GA and Plot-based systems are as fluent as
those created by humans. Recall that the Random
system, simply selects sentences from the train-
ing corpus. Finally, the GA system is significantly
more coherent than the Rank-based and Random
systems (? < 0.05), but not the Plot-based one.
This is not surprising, the GA and Plot-based sys-
tems rely on similar plots to create a coherent
story. The performance of the Random system is
also inferior as it does not have any explicit coher-
ence enforcing mechanism. The Rank-based sys-
tem is perceived overall worse. As this system is
also the least fluent, we conjecture that partici-
pants are influenced in their coherence judgments
by the grammaticality of the stories.
Overall our results indicate that an explicit story
planner improves the quality of the generated sto-
ries, especially when coupled with a search mech-
anism that advantageously explores the search
space. It is worth noting that the Plot-based sys-
tem is relatively simple, however the explicit use
of a story plot, seems to make up for the lack of
sophisticated search and more elaborate linguis-
tic information. Example stories generated by the
four systems are shown in Table 2 for the input
sentences The emperor rules the kingdom and The
child watches the bird.
Possible extensions and improvements to the
current work are many and varied. Firstly, we
could improve the quality of our plot graphs by
taking temporal knowledge into account and mak-
ing use of knowledge bases such as WordNet
and ConceptNet (Liu and Davenport, 2004), a
freely available commonsense knowledge base.
Secondly, our fitness function optimizes one ob-
1569
P
lo
tG
A
The emperor rules the kingdom. The kingdom
holds on to the emperor. The emperor rides
out of the kingdom. The kingdom speaks out
against the emperor. The emperor lies.
The child watches the bird. The bird weeps
for the child. The child begs the bird to lis-
ten.The bird dresses up the child. The child
grows up.
P
lo
t
The emperor rules the kingdom. The emperor
takes over. The emperor goes on to feel for the
kingdom. Possibly the emperor sleeps. The
emperor steals.
The child watches the bird. The bird comes
to eat away at the child. The child does thor-
oughly. The bird sees the child. The child sits
down.
R
an
k
The emperor rules the kingdom. The kingdom
lives from the reign to the emperor. The em-
peror feels that the brothers tempt a beauty
into the game. The kingdom saves the life
from crumbling the earth into the bird. The
kingdom forces the whip into wiping the tears
on the towel.
The child watches the bird. The bird lives
from the reign to the child. The child thanks
the victory for blessing the thought. The child
loves to hate the sun with the thought. The
child hopes to delay the duty from the happi-
ness.
R
an
do
m
Exclaimed the emperor when Petru had put
his question. In the meantime, mind you take
good care of our kingdom. At first the em-
peror felt rather distressed. The dinner of an
emperor! Thus they arrived at the court of the
emperor.
They cried, ?what a beautiful child!? ?No,
that I cannot do, my child? he said at last.
?What is the matter, dear child?? ?You wicked
child,? cried the Witch. Well, I will watch till
the bird comes.
Table 2: Stories generated by a system that uses plots and genetic search (PlotGA), a system that uses
only plots (Plot), McIntyre and Lapata (2009)?s rank-based system (Rank) and a system that randomly
pastes together sentences from the training corpus (Random).
jective, namely coherence. In the future we plan to
explore multiple objectives, such as whether the
story is verbose, readable (using existing readabil-
ity metrics), has two many or two few protago-
nists, and so on.
Thirdly, our stories would benefit from some ex-
plicit modeling of discourse structure. Although
the plot graph captures the progression of the ac-
tions in a story, we would also like to know where
in the story these actions are likely to occur?
some tend to appear in the beginning and others in
the end. Such information would allow us to struc-
ture the stories better and render them more natu-
ral sounding. For example, an improvement would
be the inclusion of proper endings, as the stories
are currently cut off at an arbitrary point when the
desired maximum length is reached.
Finally, the fluency of the stories would bene-
fit from generating referring expressions, multiple
tense forms, indirect speech, aggregation and gen-
erally more elaborate syntactic structure.
References
Agudo, Bele?n Dia?z, Pablo Gerva?s, and Fred-
erico Peinado. 2004. A case based reason-
ing approach to story plot generation. In
Proceedings of the 7th European Conference
on Case-Based Reasoning. Springer, Madrid,
Spain, pages 142?156.
Barros, Leandro Motta and Soraia Raupp Musse.
2007. Planning algorithms for interactive story-
telling. In Computers in Entertainment (CIE),
Association for Computing Machinery (ACM),
volume 5.
Barzilay, Regina and Mirella Lapata. 2007. Mod-
eling local coherence: An entity-based ap-
proach. Computational Linguistics 34(1):1?34.
Briscoe, E. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In Pro-
ceedings of the 3rd International Conference on
Language Resources and Evaluation. Las Pal-
mas, Gran Canaria, pages 1499?1504.
Chambers, Nathanael and Dan Jurafsky. 2008.
Unsupervised learning of narrative event chains.
In Proceedings of 46th Annual Meeting of the
Association for Computational Linguistics: Hu-
man Language Technologies. Columbus, Ohio,
pages 789?797.
Chambers, Nathanael and Dan Jurafsky. 2009.
1570
Unsupervised learning of narrative schemas and
their participants. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP.
Singapore, pages 602?610.
Cheng, Hua and Chris Mellish. 2000. Captur-
ing the interaction between aggregation and text
planning in two generation systems. In Pro-
ceedings of the 1st International Conference on
Natural Language Generation. Mitzpe Ramon,
Israel, pages 186?193.
Fellbaum. 1998. WordNet: An Electronic Lexi-
cal Database (Language, Speech, and Commu-
nication). The MIT Press, Cambridge, Mas-
sachusetts.
Fillmore, Charles J., Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicogra-
phy 16:235?250.
Goldberg, David E. 1989. Genetic Algorithms
in Search, Optimization and Machine Learning.
Addison-Wesley Longman Publishing Co., Inc.,
Boston, Massachusetts.
Grishman, Ralph, Catherine Macleod, and Adam
Meyers. 1994. COMLEX syntax: Building a
computational lexicon. In Proceedings of the
15th COLING. Kyoto, Japan, pages 268?272.
Joachims, Thorsten. 2002. Optimizing search en-
gines using clickthrough data. In Proceed-
ings of the 8th Proceedings of the eighth ACM
SIGKDD international conference on Knowl-
edge discovery and data mining. Edmonton, Al-
berta, pages 133?142.
Karamanis, Nikiforos and Hisar Maruli Manu-
rung. 2002. Stochastic text structuring using
the principle of continuity. In Proceedings of
the 2nd International Natural Language Gener-
ation Conference (INLG?02). pages 81?88.
Keller, Frank, Subahshini Gunasekharan, Neil
Mayo, and Martin Corley. 2009. Timing accu-
racy of web experiments: A case study using the
WebExp software package. Behavior Research
Methods 41(1):1?12.
Knight, Kevin and Vasileios Hatzivassiloglou.
1995. Two-level, many-paths generation. In
Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics
(ACL?95). Cambridge, Massachusetts, pages
252?260.
Korhonen, Y. Krymolowski, A. and E.J. Briscoe.
2006. A large subcategorization lexicon for nat-
ural language processing applications. In Pro-
ceedings of the 5th LREC. Genova, Italy.
Lavoie, Benoit and Owen Rambow. 1997. A fast
and portable realizer for text generation sys-
tems. In Proceedings of the 5th Conference on
Applied Natural Language Processing. Wash-
ington, D.C., pages 265?268.
Lin, Dekang. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of
the 17th International Conference on Compu-
tational Linguistic. Montreal, Quebec, pages
768?774.
Liu, Hugo and Glorianna Davenport. 2004. Con-
ceptNet: a practical commonsense reasoning
toolkit. BT Technology Journal 22(4):211?226.
Liu, Hugo and Push Singh. 2002. Using com-
monsense reasoning to generate stories. In Pro-
ceedings of the 18th National Conference on Ar-
tificial Intelligence. Edmonton, Alberta, pages
957?958.
McIntyre, Neil and Mirella Lapata. 2009. Learn-
ing to tell tales: A data-driven approach to story
generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP. Singa-
pore, pages 217?225.
Meehan, James. 1977. An interactive program that
writes stories. In Proceedings of the 5th In-
ternational Joint Conference on Artificial Intel-
ligence. Cambridge, Massachusetts, pages 91?
98.
Mellish, Chris, Alisdair Knott, Jon Oberlander,
and Mick O?Donnell. 1998. Experiments using
stochastic search for text planning. In Proceed-
ings of the 9th International Conference on Nat-
ural Language Generation. New Brunswick,
New Jersey, pages 98?107.
Mitchell, Melanie. 1998. An Introduction to Ge-
netic Algorithms. MIT Press, Cambridge, Mas-
sachusetts.
Pizzi, David, Fred Charles, Jean-Luc Lugrin, and
Marc Cavazza. 2007. Interactive storytelling
with literary feelings. In Proceedings of the 2nd
International Conference on Affective Comput-
ing and Intelligent Interaction. Lisbon, Portu-
gal, pages 630?641.
1571
Reiter, E and R Dale. 2000. Building Natural-
Language Generation Systems. Cambridge
University Press, Cambridge, UK.
Riedl, Mark O. and R. Michael Young. 2004. A
planning approach to story generation and his-
tory education. In Proceedings of the 3rd In-
ternational Conference on Narrative and Inter-
active Learning Environments. Edinburgh, UK,
pages 41?48.
Shim, Yunju and Minkoo Kim. 2002. Automatic
short story generator based on autonomous
agents. In Proceedings of the 5th Pacific Rim In-
ternational Workshop on Multi Agents. Tokyo,
pages 151?162.
Swartjes, I.M.T. and M. Theune. 2008. The vir-
tual storyteller: story generation by simulation.
In Proceedings of the 20th Belgian-Netherlands
Conference on Artificial Intelligence, BNAIC
2008. Enschede, the Netherlands, pages 257?
264.
Turner, Scott R. 1992. Ministrel: A Computer
Model of Creativity and Sotrytelling. University
of California, Los Angeles, California.
Wu, Zhibiao and Martha Palmer. 1994. Verb se-
mantics and lexical selection. In Proceedings
of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics. Las Cruces,
New Mexico, pages 133?138.
1572
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117?1126,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Semantic Role Induction via Split-Merge Clustering
Joel Lang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we describe an unsupervised
method for semantic role induction which
holds promise for relieving the data acqui-
sition bottleneck associated with supervised
role labelers. We present an algorithm that it-
eratively splits and merges clusters represent-
ing semantic roles, thereby leading from an
initial clustering to a final clustering of bet-
ter quality. The method is simple, surpris-
ingly effective, and allows to integrate lin-
guistic knowledge transparently. By com-
bining role induction with a rule-based com-
ponent for argument identification we obtain
an unsupervised end-to-end semantic role la-
beling system. Evaluation on the CoNLL
2008 benchmark dataset demonstrates that
our method outperforms competitive unsuper-
vised approaches by a wide margin.
1 Introduction
Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the re-
lations that hold between a predicate and its argu-
ments, abstracting over surface syntactic configura-
tions. In the example sentences below. window oc-
cupies different syntactic positions ? it is the object
of broke in sentences (1a,b), and the subject in (1c)
? while bearing the same semantic role, i.e., the
physical object affected by the breaking event. Anal-
ogously, rock is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).
(1) a. [Joe]A0 broke the [window]A1 with a
[rock]A2.
b. The [rock]A2 broke the [window]A1.
c. The [window]A1 broke.
The semantic roles in the examples are labeled
in the style of PropBank (Palmer et al, 2005), a
broad-coverage human-annotated corpus of seman-
tic roles and their syntactic realizations. Under the
PropBank annotation framework (which we will as-
sume throughout this paper) each predicate is as-
sociated with a set of core roles (named A0, A1,
A2, and so on) whose interpretations are specific to
that predicate1 and a set of adjunct roles (e.g., loca-
tion or time) whose interpretation is common across
predicates. This type of semantic analysis is admit-
tedly shallow but relatively straightforward to auto-
mate and useful for the development of broad cov-
erage, domain-independent language understanding
systems. Indeed, the analysis produced by existing
semantic role labelers has been shown to benefit a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al, 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al, 2005).
Since both argument identification and labeling
can be readily modeled as classification tasks, most
state-of-the-art systems to date conceptualize se-
1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).
1117
mantic role labeling as a supervised learning prob-
lem. Current approaches have high performance ?
a system will recall around 81% of the arguments
correctly and 95% of those will be assigned a cor-
rect semantic role (see Ma`rquez et al (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al, 2008).
Unfortunately, the reliance on role-annotated data
which is expensive and time-consuming to produce
for every language and domain, presents a major
bottleneck to the widespread application of semantic
role labeling. Given the data requirements for super-
vised systems and the current paucity of such data,
unsupervised methods offer a promising alternative.
They require no human effort for training thus lead-
ing to significant savings in time and resources re-
quired for annotating text. And their output can be
used in different ways, e.g., as a semantic prepro-
cessing step for applications that require broad cov-
erage understanding or as training material for su-
pervised algorithms.
In this paper we present a simple approach to un-
supervised semantic role labeling. Following com-
mon practice, our system proceeds in two stages.
It first identifies the semantic arguments of a pred-
icate and then assigns semantic roles to them. Both
stages operate over syntactically analyzed sentences
without access to any data annotated with semantic
roles. Argument identification is carried out through
a small set of linguistically-motivated rules, whereas
role induction is treated as a clustering problem. In
this setting, the goal is to assign argument instances
to clusters such that each cluster contains arguments
corresponding to a specific semantic role and each
role corresponds to exactly one cluster. We formu-
late a clustering algorithm that executes a series of
split and merge operations in order to transduce an
initial clustering into a final clustering of better qual-
ity. Split operations leverage syntactic cues so as to
create ?pure? clusters that contain arguments of the
same role whereas merge operations bring together
argument instances of a particular role located in
different clusters. We test the effectiveness of our
induction method on the CoNLL 2008 benchmark
dataset and demonstrate improvements over compet-
itive unsupervised methods by a wide margin.
2 Related Work
As mentioned earlier, much previous work has
focused on building supervised SRL systems
(Ma`rquez et al, 2008). A few semi-supervised ap-
proaches have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(Fu?rstenau and Lapata, 2009) or across different lan-
guages (Pado? and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) attempt
to increase the coverage of PropBank by leveraging
existing labeled data. Rather than annotating new
sentences that contain previously unseen verbs, they
find syntactically similar verbs and use their annota-
tions as surrogate training data.
Swier and Stevenson (2004) induce role labels
with a bootstrapping scheme where the set of la-
beled instances is iteratively expanded using a clas-
sifier trained on previously labeled instances. Their
method is unsupervised in that it starts with a dataset
containing no role annotations at all. However, it re-
quires significant human effort as it makes use of
VerbNet (Kipper et al, 2000) in order to identify the
arguments of predicates and make initial role assign-
ments. VerbNet is a broad coverage lexicon orga-
nized into verb classes each of which is explicitly
associated with argument realization and semantic
role specifications.
Abend et al (2009) propose an algorithm that
identifies the arguments of predicates by relying
only on part of speech annotations, without, how-
ever, assigning semantic roles. In contrast, Lang
and Lapata (2010) focus solely on the role induction
problem which they formulate as the process of de-
tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that makes
use of the close relationship between syntactic func-
tions and semantic roles. Grenager and Manning
1118
(2006) propose a directed graphical model which re-
lates a verb, its semantic roles, and their possible
syntactic realizations. Latent variables represent the
semantic roles of arguments and role induction cor-
responds to inferring the state of these latent vari-
ables.
Our own work also follows the unsupervised
learning paradigm. We formulate the induction of
semantic roles as a clustering problem and propose a
split-merge algorithm which iteratively manipulates
clusters representing semantic roles. The motiva-
tion behind our approach was to design a concep-
tually simple system, that allows for the incorpo-
ration of linguistic knowledge in a straightforward
and transparent manner. For example, arguments
occurring in similar syntactic positions are likely to
bear the same semantic role and should therefore
be grouped together. Analogously, arguments that
are lexically similar are likely to represent the same
semantic role. We operationalize these notions us-
ing a scoring function that quantifies the compatibil-
ity between arbitrary cluster pairs. Like Lang and
Lapata (2010) and Grenager and Manning (2006)
our method operates over syntactically parsed sen-
tences, without, however, making use of any infor-
mation pertaining to semantic roles (e.g., in form of
a lexical resource or manually annotated data). Per-
forming role-semantic analysis without a treebank-
trained parser is an interesting research direction,
however, we leave this to future work.
3 Learning Setting
We follow the general architecture of supervised se-
mantic role labeling systems. Given a sentence and
a designated verb, the SRL task consists of identify-
ing the arguments of the verbal predicate (argument
identification) and labeling them with semantic roles
(role induction).
In our case neither argument identification nor
role induction relies on role-annotated data or other
semantic resources although we assume that the in-
put sentences are syntactically analyzed. Our ap-
proach is not tied to a specific syntactic representa-
tion ? both constituent- and dependency-based rep-
resentations could be used. However, we opted for a
dependency-based representation, as it simplifies ar-
gument identification considerably and is consistent
with the CoNLL 2008 benchmark dataset used for
evaluation in our experiments.
Given a dependency parse of a sentence, our sys-
tem identifies argument instances and assigns them
to clusters. Thereafter, argument instances can be
labeled with an identifier corresponding to the clus-
ter they have been assigned to, similar to PropBank
core labels (e.g., A0, A1).
4 Argument Identification
In the supervised setting, a classifier is employed
in order to decide for each node in the parse tree
whether it represents a semantic argument or not.
Nodes classified as arguments are then assigned a se-
mantic role. In the unsupervised setting, we slightly
reformulate argument identification as the task of
discarding as many non-semantic arguments as pos-
sible. This means that the argument identification
component does not make a final positive decision
for any of the argument candidates; instead, this de-
cision is deferred to role induction. The rules given
in Table 1 are used to discard or select argument can-
didates. They primarily take into account the parts of
speech and the syntactic relations encountered when
traversing the dependency tree from predicate to ar-
gument. For each candidate, the first matching rule
is applied.
We will exemplify how the argument identifica-
tion component works for the predicate expect in the
sentence ?The company said it expects its sales to
remain steady? whose parse tree is shown in Fig-
ure 1. Initially, all words save the predicate itself
are treated as argument candidates. Then, the rules
from Table 1 are applied as follows. Firstly, words
the and to are discarded based on their part of speech
(rule (1)); then, remain is discarded because the path
ends with the relation IM and said is discarded as
the path ends with an upward-leading OBJ relation
(rule (2)). Rule (3) does not match and is therefore
not applied. Next, steady is discarded because there
is a downward-leading OPRD relation along the path
and the words company and its are discarded be-
cause of the OBJ relations along the path (rule (4)).
Rule (5) does not apply but words it and sales are
kept as likely arguments (rule (6)). Finally, rule (7)
does not apply, because there are no candidates left.
1119
1. Discard a candidate if it is a determiner, in-
finitival marker, coordinating conjunction, or
punctuation.
2. Discard a candidate if the path of relations
from predicate to candidate ends with coordi-
nation, subordination, etc. (see the Appendix
for the full list of relations).
3. Keep a candidate if it is the closest subject
(governed by the subject-relation) to the left
of a predicate and the relations from predi-
cate p to the governor g of the candidate are
all upward-leading (directed as g? p).
4. Discard a candidate if the path between the
predicate and the candidate, excluding the last
relation, contains a subject relation, adjectival
modifier relation, etc. (see the Appendix for
the full list of relations).
5. Discard a candidate if it is an auxiliary verb.
6. Keep a candidate if the predicate is its parent.
7. Keep a candidate if the path from predicate
to candidate leads along several verbal nodes
(verb chain) and ends with arbitrary relation.
8. Discard all remaining candidates.
Table 1: Argument identification rules.
5 Split-Merge Role Induction
We treat role induction as a clustering problem with
the goal of assigning argument instances (i.e., spe-
cific arguments occurring in an input sentence) to
clusters such that these represent semantic roles. In
accordance with PropBank, we induce a separate set
of clusters for each verb and each cluster thus repre-
sents a verb-specific role.
Our algorithm works by iteratively splitting and
merging clusters of argument instances in order to
arrive at increasingly accurate representations of se-
mantic roles. Although splits and merges could be
arbitrarily interleaved, our algorithm executes a sin-
gle split operation (split phase), followed by a se-
ries of merges (merge phase). The split phase par-
titions the seed cluster containing all argument in-
stances of a particular verb into more fine-grained
(sub-)clusters. This initial split results in a clustering
with high purity but low collocation, i.e., argument
instances in each cluster tend to belong to the same
role but argument instances of a particular role are
Figure 1: A sample dependency parse with depen-
dency labels SBJ (subject), OBJ (object), NMOD
(nominal modifier), OPRD (object predicative com-
plement), PRD (predicative complement), and IM
(infinitive marker). See Surdeanu et al (2008) for
more details on this variant of dependency syntax.
located in many clusters. The degree of dislocation
is reduced in the consecutive merge phase, in which
clusters that are likely to represent the same role are
merged.
5.1 Split Phase
Initially, all arguments of a particular verb are placed
in a single cluster. The goal then is to partition this
cluster in such a way that the split-off clusters have
high purity, i.e., contain argument instances of the
same role. Towards this end, we characterize each
argument instance by a key, formed by concatenat-
ing the following syntactic cues:
? verb voice (active/passive);
? argument linear position relative to predicate
(left/right);
? syntactic relation of argument to its governor;
? preposition used for argument realization.
A cluster is allocated for each key and all argument
instances with a matching key are assigned to that
cluster. Since each cluster encodes fine-grained syn-
tactic distinctions, we assume that arguments occur-
ring in the same position are likely to bear the same
semantic role. The assumption is largely supported
by our empirical results (see Section 7); the clusters
emerging from the initial split phase have a purity
of approximately 90%. While the incorporation of
additional cues (e.g., indicating the part of speech
of the subject or transitivity) would result in even
greater purity, it would also create problematically
small clusters, thereby negatively affecting the suc-
cessive merge phase.
1120
5.2 Merge Phase
The split phase creates clusters with high purity,
however, argument instances of a particular role are
often scattered amongst many clusters resulting in a
cluster assignment with low collocation. The goal
of the merge phase is to improve collocation by ex-
ecuting a series of merge steps. At each step, pairs
of clusters are considered for merging. Each pair is
scored by a function that reflects how likely the two
clusters are to contain arguments of the same role
and the best scoring pair is chosen for merging. In
the following, we will specify which pairs of clus-
ters are considered (candidate search), how they are
scored, and when the merge phase terminates.
5.2.1 Candidate Search
In principle, we could simply enumerate and score
all possible cluster pairs at each iteration. In practice
however, such a procedure has a number of draw-
backs. Besides being inefficient, it requires a scoring
function with comparable scores for arbitrary pairs
of clusters. For example, let a, b, c, and d denote
clusters. Then, score(a,b) and score(c,d) must be
comparable. This is a stronger requirement than de-
manding that only scores involving some common
cluster (e.g., score(a,b) and score(a,c)) be com-
parable. Moreover, it would be desirable to ex-
clude pairings involving small clusters (i.e., with
few instances) as scores for these tend to be unre-
liable. Rather than considering all cluster pairings,
we therefore select a specific cluster at each step and
score merges between this cluster and certain other
clusters. If a sufficiently good merge is found, it is
executed, otherwise the clustering does not change.
In addition, we prioritize merges between large clus-
ters and avoid merges between small clusters.
Algorithm 1 implements our merging procedure.
Each pass through the inner loop (lines 4?12) selects
a different cluster to consider at that step. Then,
merges between the selected cluster and all larger
clusters are considered. The highest-scoring merge
is executed, unless all merges are ruled out, i.e., have
a score below the threshold ?. After each comple-
tion of the inner loop, the thresholds contained in
the scoring function (discussed below) are adjusted
and this is repeated until some termination criterion
is met (discussed in Section 5.2.3).
Algorithm 1: Cluster merging procedure. Oper-
ation merge(Li,L j) merges cluster Li into cluster
L j and removes Li from the list L.
1 while not done do
2 L? a list of all clusters sorted by number
of instances in descending order
3 i? 1
4 while i < length(L) do
5 j? arg max
0? j?<i
score(Li,L j?)
6 if score(Li,L j)? ? then
7 merge(Li,L j)
8 end
9 else
10 i? i+1
11 end
12 end
13 adjust thresholds
14 end
5.2.2 Scoring Function
Our scoring function quantifies whether two clusters
are likely to contain arguments of the same role and
was designed to reflect the following criteria:
1. whether the arguments found in the two clus-
ters are lexically similar;
2. whether clause-level constraints are satisfied,
specifically the constraint that all arguments
of a particular clause have different semantic
roles, i.e., are assigned to different clusters;
3. whether the arguments present in the two clus-
ters have similar parts of speech.
Qualitatively speaking, criteria (2) and (3) provide
negative evidence in the sense that they can be used
to rule out incorrect merges but not to identify cor-
rect ones. For example, two clusters with drastically
different parts of speech are unlikely to represent
the same role. However, the converse is not neces-
sarily true as part of speech similarity does not im-
ply role-semantic similarity. Analogously, the fact
that clause-level constraints are not met provides ev-
idence against a merge, but the fact that these are
satisfied is not reliable evidence in favor of a merge.
In contrast, lexical similarity implies that the clus-
1121
ters are likely to represent the same semantic role.
It is reasonable to assume that due to selectional re-
strictions, verbs will be associated with lexical units
that are semantically related and assume similar syn-
tactic positions (e.g., eat prefers as an object edible
things such as apple, biscuit, meat), thus bearing the
same semantic role. Unavoidably, lexical similarity
will be more reliable for arguments with overt lex-
ical content as opposed to pronouns, however this
should not impact the scoring of sufficiently large
clusters.
Each of the criteria mentioned above is quantified
through a separate score and combined into an over-
all similarity function, which scores two clusters c
and c? as follows:
score(c,c?) =
?
??
??
0 if pos(c,c?) < ?,
0 if cons(c,c?) < ?,
lex(c,c?) otherwise.
(2)
The particular form of this function is motivated by
the distinction between positive and negative evi-
dence. When the part-of-speech similarity (pos) is
below a certain threshold ? or when clause-level
constraints (cons) are satisfied to a lesser extent than
threshold ?, the score takes value zero and the merge
is ruled out. If this is not the case, the lexical similar-
ity score (lex) determines the magnitude of the over-
all score. In the remainder of this section we will
explain how the individual scores (pos, cons, and
lex) are defined and then move on to discuss how
the thresholds ? and ? are adjusted.
Lexical Similarity We measure lexical similar-
ity between two clusters through cosine similarity.
Specifically, each cluster is represented as a vec-
tor whose components correspond to the occurrence
frequencies of the argument head words in the clus-
ter. The similarity on such vectors x and y is then
quantified as:
lex(x,y) = cossim(x,y) =
x?y
?x??y?
(3)
Clause-Level Constraints Arguments occurring
in the same clause cannot bear the same role. There-
fore, clusters should not merge if the resulting clus-
ter contains (many) arguments of the same clause.
For two clusters c and c? we assess how well they
satisfy this clause-level constraint by computing:
cons(c,c?) = 1?
2? viol(c,c?)
NC +NC?
(4)
where viol(c,c?) refers to the number of pairs of in-
stances (d,d?) ? c? c? for which d and d? occur in
the same clause (each instance can participate in at
most one pair) and NC and NC? are the number of
instances in clusters c and c?, respectively.
Part-of-speech Similarity Part-of-speech similar-
ity is also measured through cosine-similarity (equa-
tion (3)). Clusters are again represented as vectors x
and y whose components correspond to argument
part-of-speech tags and values to their occurrence
frequency.
5.2.3 Threshold Adaptation and Termination
As mentioned earlier the thresholds ? and ? which
parametrize the scoring function are adjusted at each
iteration. The idea is to start with a very restrictive
setting (high values) in which the negative evidence
rules out merges more strictly, and then to gradually
relax the requirement for a merge by lowering the
threshold values. This procedure prioritizes reliable
merges over less reliable ones.
More concretely, our threshold adaptation pro-
cedure starts with ? and ? both set to value 0.95.
Then ? is lowered by 0.05 at each step, leaving ?
unchanged. When ? becomes zero, ? is lowered
by 0.05 and ? is reset to 0.95. Then ? is iteratively
decreased again until it becomes zero, after which ?
is decreased by another 0.05. This is repeated until ?
becomes zero, at which point the algorithm termi-
nates. Note that the termination criterion is not tied
explicitly to the number of clusters, which is there-
fore determined automatically.
6 Experimental Setup
In this section we describe how we assessed the per-
formance of our system. We discuss the dataset
on which our experiments were carried out, explain
how our system?s output was evaluated and present
the methods used for comparison with our approach.
Data For evaluation purposes, the system?s out-
put was compared against the CoNLL 2008 shared
task dataset (Surdeanu et al, 2008) which provides
1122
Syntactic Function Lang and Lapata Split-Merge
PU CO F1 PU CO F1 PU CO F1
auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1
Table 2: Clustering results with our split-merge algorithm, the unsupervised model proposed in Lang and
Lapata (2010) and a baseline that assigns arguments to clusters based on their syntactic function.
PropBank-style gold standard annotations. The
dataset was taken from the Wall Street Journal por-
tion of the Penn Treebank corpus and converted into
a dependency format (Surdeanu et al, 2008). In
addition to gold standard dependency parses, the
dataset alo contains automatic parses obtained from
the MaltParser (Nivre et al, 2007). Although the
dataset provides annotations for verbal and nominal
predicate-argument constructions, we only consid-
ered the former, following previous work on seman-
tic role labeling (Ma`rquez et al, 2008).
Evaluation Metrics For each verb, we determine
the extent to which argument instances in a cluster
share the same gold standard role (purity) and the
extent to which a particular gold standard role is as-
signed to a single cluster (collocation).
More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the
percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, G j the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can then
be written as:
PU =
1
N ?i
max
j
|G j ?Ci| (5)
Collocation is defined as follows. For each gold role,
we determine the cluster with the largest number of
instances for that role (the role?s primary cluster)
and then compute the percentage of instances that
belong to the primary cluster for each gold role as:
CO =
1
N ?j
max
i
|G j ?Ci| (6)
The per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the micro-
average obtained by weighting the scores for indi-
vidual verbs proportionately to the number of in-
stances for that verb.
Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:
F1 =
2?CO?PU
CO+PU
(7)
Comparison Models We compared our split-
merge algorithm against two competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as point of com-
parison by other unsupervised semantic role label-
ing systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.
Our implementation allocates up to N = 21 clus-
ters2 for each verb, one for each of the 20 most fre-
quent functions in the CoNLL dataset and a default
cluster for all other functions. The second compar-
ison model is the one proposed in Lang and Lapata
(2010) (see Section 2). We used the same model set-
tings (with 10 latent variables) and feature set pro-
posed in that paper. Our method?s only parameter is
the threshold ? which we heuristically set to 0.1. On
average our method induces 10 clusters per verb.
7 Results
Our results are summarized in Table 2. We re-
port cluster purity (PU), collocation (CO) and their
harmonic mean (F1) for the baseline (Syntactic
Function), Lang and Lapata?s (2010) model and
our split-merge algorithm (Split-Merge) on four
2This is the number of gold standard roles.
1123
Syntactic Function Split-Merge
Verb Freq PU CO F1 PU CO F1
say 15238 91.4 91.3 91.4 93.6 81.7 87.2
make 4250 68.6 71.9 70.2 73.3 72.9 73.1
go 2109 45.1 56.0 49.9 52.7 51.9 52.3
increase 1392 59.7 68.4 63.7 68.8 71.4 70.1
know 983 62.4 72.7 67.1 63.7 65.9 64.8
tell 911 61.9 76.8 68.6 77.5 70.8 74.0
consider 753 63.5 65.6 64.5 79.2 61.6 69.3
acquire 704 75.9 79.7 77.7 80.1 76.6 78.3
meet 574 76.7 76.0 76.3 88.0 69.7 77.8
send 506 69.6 63.8 66.6 83.6 65.8 73.6
open 482 63.1 73.4 67.9 77.6 62.2 69.1
break 246 53.7 58.9 56.2 68.7 53.3 60.0
Table 3: Clustering results for individual verbs with
our split-merge algorithm and the syntactic function
baseline.
datasets. These result from the combination of au-
tomatic parses with automatically identified argu-
ments (auto/auto), gold parses with automatic argu-
ments (gold/auto), automatic parses with gold argu-
ments (auto/gold) and gold parses with gold argu-
ments (gold/gold). Bold-face is used to highlight the
best performing system under each measure on each
dataset (e.g., auto/auto, gold/auto and so on).
On all datasets, our method achieves the highest
purity and outperforms both comparison models by
a wide margin which in turn leads to a considerable
increase in F1. On the auto/auto dataset the split-
merge algorithm results in 9% higher purity than the
baseline and increases F1 by 2.8%. Lang and Lap-
ata?s (2010) logistic classifier achieves higher collo-
cation but lags behind our method on the other two
measures.
Not unexpectedly, we observe an increase in per-
formance for all models when using gold standard
parses. On the gold/auto dataset, F1 increases
by 2.7% for the split-merge algorithm, 2.7% for the
logistic classifier, and 5.5% for the syntactic func-
tion baseline. Split-Merge maintains the highest pu-
rity and levels the baseline in terms of F1. Perfor-
mance also increases if gold standard arguments are
used instead of automatically identified arguments.
Consequently, each model attains its best scores on
the gold/gold dataset.
We also assessed the argument identification com-
Syntactic Function Split-Merge
Role PU CO F1 PU CO F1
A0 74.5 87.0 80.3 79.0 88.7 83.6
A1 82.3 72.0 76.8 87.1 73.0 79.4
A2 65.0 67.3 66.1 82.8 66.2 73.6
A3 48.7 76.7 59.6 79.6 76.3 77.9
ADV 37.2 77.3 50.2 78.8 37.3 50.6
CAU 81.8 74.4 77.9 84.8 67.2 75.0
DIR 62.7 67.9 65.2 71.0 50.7 59.1
EXT 51.4 87.4 64.7 90.4 87.2 88.8
LOC 71.5 74.6 73.0 82.6 56.7 67.3
MNR 62.6 58.8 60.6 81.5 44.1 57.2
TMP 80.5 74.0 77.1 80.1 38.7 52.2
MOD 68.2 44.4 53.8 90.4 89.6 90.0
NEG 38.2 98.5 55.0 49.6 98.8 66.1
DIS 42.5 87.5 57.2 62.2 75.4 68.2
Table 4: Clustering results for individual semantic
roles with our split-merge algorithm and the syntac-
tic function baseline.
ponent on its own (settings auto/auto and gold/auto).
It obtained a precision of 88.1% (percentage of se-
mantic arguments out of those identified) and recall
of 87.9% (percentage of identified arguments out of
all gold arguments). However, note that these fig-
ures are not strictly comparable to those reported
for supervised systems, due to the fact that our ar-
gument identification component only discards non-
argument candidates.
Tables 3 and 4 shows how performance varies
across verbs and roles, respectively. We compare the
syntactic function baseline and the split-merge sys-
tem on the auto/auto dataset. Table 3 presents results
for 12 verbs which we selected so as to exhibit var-
ied occurrence frequencies and alternation patterns.
As can be seen, the macroscopic result ? increase
in F1 (shown in bold face) and purity ? also holds
across verbs. Some caution is needed in interpret-
ing the results in Table 43 since core roles A0?A3
are defined on a per-verb basis and do not necessar-
ily have a uniform corpus-wide interpretation. Thus,
conflating scores across verbs is only meaningful to
the extent that these labels actually signify the same
3Results are shown for four core roles (A0?A3) and all sub-
types of the ArgM role, i.e., adjuncts denoting general purpose
(ADV), cause (CAU), direction (DIR), extent (EXT), location
(LOC), manner (MNR), and time (TMP), modal verbs (MOD),
negative markers (NEG), and discourse connectives (DIS).
1124
role (which is mostly true for A0 and A1). Further-
more, the purity scores given here represent the av-
erage purity of those clusters for which the specified
role is the majority role. We observe that for most
roles shown in Table 4 the split-merge algorithm im-
proves upon the baseline with regard to F1, whereas
this is uniformly the case for purity.
What are the practical implications of these re-
sults, especially when considering the collocation-
purity tradeoff? If we were to annotate the clus-
ters induced by our system, low collocation would
result in higher annotation effort while low purity
would result in poorer data quality. Our system im-
proves purity substantially over the baselines, with-
out affecting collocation in a way that would mas-
sively increase the annotation effort. As an exam-
ple, consider how our system could support humans
in labeling an unannotated corpus. (The following
numbers are derived from the CoNLL dataset4 in the
auto/auto setting.) We might decide to annotate all
induced clusters with more than 10 instances. This
means we would assign labels to 74% of instances in
the dataset (excluding those discarded during argu-
ment identification) and attain a role classification
with 79.4% precision (purity).5 However, instead
of labeling all 165,662 instances contained in these
clusters individually we would only have to assign
labels to 2,869 clusters. Since annotating a cluster
takes roughly the same time as annotating a single
instance, the annotation effort is reduced by a factor
of about 50.
8 Conclusions
In this paper we presented a novel approach to un-
supervised role induction which we formulated as a
clustering problem. We proposed a split-merge al-
gorithm that iteratively manipulates clusters repre-
senting semantic roles whilst trading off cluster pu-
rity with collocation. The split phase creates ?pure?
clusters that contain arguments of the same role
whereas the merge phase attempts to increase col-
location by merging clusters which are likely to rep-
resent the same role. The approach is simple, intu-
4Of course, it makes no sense to label this dataset as it is
already labeled.
5Purity here is slightly lower than the score reported in Ta-
ble 2 (auto/auto setting), because it is computed over a different
number of clusters (only those with at least 10 instances).
itive and requires no manual effort for training. Cou-
pled with a rule-based component for automatically
identifying argument candidates our split-merge al-
gorithm forms an end-to-end system that is capable
of inducing role labels without any supervision.
Our approach holds promise for reducing the data
acquisition bottleneck for supervised systems. It
could be usefully employed in two ways: (a) to cre-
ate preliminary annotations, thus supporting the ?an-
notate automatically, correct manually? methodol-
ogy used for example to provide high volume anno-
tation in the Penn Treebank project; and (b) in com-
bination with supervised methods, e.g., by providing
useful out-of-domain data for training. An important
direction for future work lies in investigating how
the approach generalizes across languages as well as
reducing our system?s reliance on a treebank-trained
parser.
Acknowledgments We are grateful to Charles
Sutton for his valuable feedback on this work. The
authors acknowledge the support of EPSRC (grant
GR/T04540/01).
Appendix
The relations in Rule (2) from Table 1 are IM??,
PRT?, COORD??, P??, OBJ?, PMOD?, ADV?,
SUB??, ROOT?, TMP?, SBJ?, OPRD?. The sym-
bols ? and ? denote the direction of the dependency
arc (upward and downward, respectively).
The relations in Rule (3) are ADV??, AMOD??,
APPO??, BNF??-, CONJ??, COORD??, DIR??,
DTV??-, EXT??, EXTR??, HMOD??, IOBJ??,
LGS??, LOC??, MNR??, NMOD??, OBJ??,
OPRD??, POSTHON??, PRD??, PRN??, PRP??,
PRT??, PUT??, SBJ??, SUB??, SUFFIX??. De-
pendency labels are abbreviated here. A detailed
description is given in Surdeanu et al (2008), in
their Table 4.
References
O. Abend, R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing, pages 28?36, Singapore.
1125
D. Dowty. 1991. Thematic Proto Roles and Argument
Selection. Language, 67(3):547?619.
H. Fu?rstenau and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 11?20, Singa-
pore.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245?288.
A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192?199, Prague, Czech Republic.
T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1?8, Sydney, Australia.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691?696. AAAI Press / The MIT Press.
J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939?
947, Los Angeles, California.
L. Ma`rquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145?
159, June.
G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95?135.
S. Pado? and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307?340.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289?310.
D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12?21,
Prague, Czech Republic.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8?15, Sapporo, Japan.
M. Surdeanu, R. Johansson, A. Meyers, and L. Ma`rquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159?177, Manchester,
England.
R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95?102, Barcelona, Spain.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North
American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13?16, Boulder, Colorado.
1126
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 369?378,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Concept-to-text Generation via Discriminative Reranking
Ioannis Konstas and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
i.konstas@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
This paper proposes a data-driven method
for concept-to-text generation, the task of
automatically producing textual output from
non-linguistic input. A key insight in our ap-
proach is to reduce the tasks of content se-
lection (?what to say?) and surface realization
(?how to say?) into a common parsing prob-
lem. We define a probabilistic context-free
grammar that describes the structure of the in-
put (a corpus of database records and text de-
scribing some of them) and represent it com-
pactly as a weighted hypergraph. The hyper-
graph structure encodes exponentially many
derivations, which we rerank discriminatively
using local and global features. We propose a
novel decoding algorithm for finding the best
scoring derivation and generating in this set-
ting. Experimental evaluation on the ATIS do-
main shows that our model outperforms a
competitive discriminative system both using
BLEU and in a judgment elicitation study.
1 Introduction
Concept-to-text generation broadly refers to the
task of automatically producing textual output from
non-linguistic input such as databases of records,
logical form, and expert system knowledge bases
(Reiter and Dale, 2000). A variety of concept-to-
text generation systems have been engineered over
the years, with considerable success (e.g., Dale et
al. (2003), Reiter et al (2005), Green (2006), Turner
et al (2009)). Unfortunately, it is often difficult
to adapt them across different domains as they rely
mostly on handcrafted components.
In this paper we present a data-driven ap-
proach to concept-to-text generation that is domain-
independent, conceptually simple, and flexible. Our
generator learns from a set of database records and
textual descriptions (for some of them). An exam-
ple from the air travel domain is shown in Figure 1.
Here, the records provide a structured representation
of the flight details (e.g., departure and arrival time,
location), and the text renders some of this infor-
mation in natural language. Given such input, our
model determines which records to talk about (con-
tent selection) and which words to use for describing
them (surface realization). Rather than breaking up
the generation process into a sequence of local deci-
sions, we perform both tasks jointly. A key insight
in our approach is to reduce content selection and
surface realization into a common parsing problem.
Specifically, we define a probabilistic context-free
grammar (PCFG) that captures the structure of the
database and its correspondence to natural language.
This grammar represents multiple derivations which
we encode compactly using a weighted hypergraph
(or packed forest), a data structure that defines a
weight for each tree.
Following a generative approach, we could first
learn the weights of the PCFG by maximising the
joint likelihood of the model and then perform gen-
eration by finding the best derivation tree in the hy-
pergraph. The performance of this baseline system
could be potentially further improved using discrim-
inative reranking (Collins, 2000). Typically, this
method first creates a list of n-best candidates from
a generative model, and then reranks them with arbi-
trary features (both local and global) that are either
not computable or intractable to compute within the
369
Database:
Flight
from to
denver boston
Day Number
number dep/ar
9 departure
Month
month dep/ar
august departure
Condition
arg1 arg2 type
arrival time 1600 <
Search
type what
query flight
??expression:
Text:
?x. f light(x)? f rom(x,denver)? to(x,boston)?day number(x,9)?month(x,august)?
less than(arrival time(x),1600)
Give me the flights leaving Denver August ninth coming back to Boston before 4pm.
Figure 1: Example of non-linguistic input as a structured database and logical form and its corresponding text. We
omit record fields that have no value, for the sake of brevity.
baseline system.
An appealing alternative is to rerank the hyper-
graph directly (Huang, 2008). As it compactly en-
codes exponentially many derivations, we can ex-
plore a much larger hypothesis space than would
have been possible with an n-best list. Importantly,
in this framework non-local features are computed
at all internal hypergraph nodes, allowing the de-
coder to take advantage of them continuously at all
stages of the generation process. We incorporate
features that are local with respect to a span of a
sub-derivation in the packed forest; we also (approx-
imately) include features that arbitrarily exceed span
boundaries, thus capturing more global knowledge.
Experimental results on the ATIS domain (Dahl et
al., 1994) demonstrate that our model outperforms
a baseline based on the best derivation and a state-
of-the-art discriminative system (Angeli et al, 2010)
by a wide margin.
Our contributions in this paper are threefold: we
recast concept-to-text generation in a probabilistic
parsing framework that allows to jointly optimize
content selection and surface realization; we repre-
sent parse derivations compactly using hypergraphs
and illustrate the use of an algorithm for generating
(rather than parsing) in this framework; finally, the
application of discriminative reranking to concept-
to-text generation is novel to our knowledge and as
our experiments show beneficial.
2 Related Work
Early discriminative approaches to text generation
were introduced in spoken dialogue systems, and
usually tackled content selection and surface re-
alization separately. Ratnaparkhi (2002) concep-
tualized surface realization (from a fixed meaning
representation) as a classification task. Local and
non-local information (e.g., word n-grams, long-
range dependencies) was taken into account with the
use of features in a maximum entropy probability
model. More recently, Wong and Mooney (2007)
describe an approach to surface realization based on
synchronous context-free grammars. The latter are
learned using a log-linear model with minimum er-
ror rate training (Och, 2003).
Angeli et al (2010) were the first to propose a
unified approach to content selection and surface re-
alization. Their model operates over automatically
induced alignments of words to database records
(Liang et al, 2009) and decomposes into a sequence
of discriminative local decisions. They first deter-
mine which records in the database to talk about,
then which fields of those records to mention, and
finally which words to use to describe the chosen
fields. Each of these decisions is implemented as
a log-linear model with features learned from train-
ing data. Their surface realization component per-
forms decisions based on templates that are automat-
ically extracted and smoothed with domain-specific
knowledge in order to guarantee fluent output.
Discriminative reranking has been employed in
many NLP tasks such as syntactic parsing (Char-
niak and Johnson, 2005; Huang, 2008), machine
translation (Shen et al, 2004; Li and Khudanpur,
2009) and semantic parsing (Ge and Mooney, 2006).
Our model is closest to Huang (2008) who also
performs forest reranking on a hypergraph, using
both local and non-local features, whose weights
are tuned with the averaged perceptron algorithm
(Collins, 2002). We adapt forest reranking to gen-
eration and introduce several task-specific features
that boost performance. Although conceptually re-
lated to Angeli et al (2010), our model optimizes
content selection and surface realization simultane-
ously, rather than as a sequence. The discriminative
aspect of two models is also fundamentally different.
We have a single reranking component that applies
370
throughout, whereas they train different discrimina-
tive models for each local decision.
3 Problem Formulation
We assume our generator takes as input a set of
database records d and produces text w that verbal-
izes some of these records. Each record r ? d has a
type r.t and a set of fields f associated with it. Fields
have different values f .v and types f .t (i.e., integer
or categorical). For example, in Figure 1, flight is a
record type with fields from and to. The values of
these fields are denver and boston and their type is
categorical.
During training, our algorithm is given a corpus
consisting of several scenarios, i.e., database records
paired with texts like those shown in Figure 1. The
database (and accompanying texts) are next con-
verted into a PCFG whose weights are learned from
training data. PCFG derivations are represented as
a weighted directed hypergraph (Gallo et al, 1993).
The weights on the hyperarcs are defined by a vari-
ety of feature functions, which we learn via a dis-
criminative online update algorithm. During test-
ing, we are given a set of database records with-
out the corresponding text. Using the learned fea-
ture weights, we compile a hypergraph specific to
this test input and decode it approximately (Huang,
2008). The hypergraph representation allows us
to decompose the feature functions and compute
them piecemeal at each hyperarc (or sub-derivation),
rather than at the root node as in conventional n-best
list reranking. Note that the algorithm does not sep-
arate content selection from surface realization, both
subtasks are optimized jointly through the proba-
bilistic parsing formulation.
3.1 Grammar Definition
We capture the structure of the database with a num-
ber of CFG rewrite rules, in a similar way to how
Liang et al (2009) define Markov chains in their
hierarchical model. These rules are purely syn-
tactic (describing the intuitive relationship between
records, records and fields, fields and corresponding
words), and could apply to any database with sim-
ilar structure irrespectively of the semantics of the
domain.
Our grammar is defined in Table 1 (rules (1)?(9)).
Rule weights are governed by an underlying multi-
nomial distribution and are shown in square brack-
1. S? R(start) [Pr = 1]
2. R(ri.t)? FS(r j,start) R(r j.t) [P(r j.t |ri.t) ??]
3. R(ri.t)? FS(r j,start) [P(r j.t |ri.t) ??]
4. FS(r,r. fi)? F(r,r. f j) FS(r,r. f j) [P( f j | fi)]
5. FS(r,r. fi)? F(r,r. f j) [P( f j | fi)]
6. F(r,r. f )?W(r,r. f ) F(r,r. f ) [P(w |w?1,r,r. f )]
7. F(r,r. f )?W(r,r. f ) [P(w |w?1,r,r. f )]
8. W(r,r. f )? ? [P(? |r,r. f , f .t, f .v)]
9. W(r,r. f )? g( f .v)
[P(g( f .v).mode |r,r. f , f .t = int)]
Table 1: Grammar rules and their weights shown in
square brackets.
ets. Non-terminal symbols are in capitals and de-
note intermediate states; the terminal symbol ?
corresponds to all words seen in the training set,
and g( f .v) is a function for generating integer num-
bers given the value of a field f . All non-terminals,
save the start symbol S, have one or more constraints
(shown in parentheses), similar to number and gen-
der agreement constraints in augmented syntactic
rules.
Rule (1) denotes the expansion from the start
symbol S to record R, which has the special start
type (hence the notation R(start)). Rule (2) de-
fines a chain between two consecutive records ri
and r j. Here, FS(r j,start) represents the set
of fields of the target r j, following the source
record R(ri). For example, the rule R(search1.t)?
FS( f light1,start)R( f light1.t) can be interpreted as
follows. Given that we have talked about search1,
we will next talk about f light1 and thus emit its
corresponding fields. R( f light1.t) is a non-terminal
place-holder for the continuation of the chain of
records, and start in FS is a special boundary field
between consecutive records. The weight of this rule
is the bigram probability of two records conditioned
on their type, multiplied with a normalization fac-
tor ?. We have also defined a null record type i.e., a
record that has no fields and acts as a smoother for
words that may not correspond to a particular record.
Rule (3) is simply an escape rule, so that the parsing
process (on the record level) can finish.
Rule (4) is the equivalent of rule (2) at the field
371
level, i.e., it describes the chaining of two con-
secutive fields fi and f j. Non-terminal F(r,r. f )
refers to field f of record r. For example, the rule
FS( f light1, f rom) ? F( f light1, to)FS( f light1, to),
specifies that we should talk about the field to of
record f light1, after talking about the field f rom.
Analogously to the record level, we have also in-
cluded a special null field type for the emission of
words that do not correspond to a specific record
field. Rule (6) defines the expansion of field F to
a sequence of (binarized) words W, with a weight
equal to the bigram probability of the current word
given the previous word, the current record, and
field.
Rules (8) and (9) define the emission of words and
integer numbers from W, given a field type and its
value. Rule (8) emits a single word from the vocabu-
lary of the training set. Its weight defines a multino-
mial distribution over all seen words, for every value
of field f , given that the field type is categorical or
the special null field. Rule (9) is identical but for
fields whose type is integer. Function g( f .v) gener-
ates an integer number given the field value, using
either of the following six ways (Liang et al, 2009):
identical to the field value, rounding up or rounding
down to a multiple of 5, rounding off to the clos-
est multiple of 5 and finally adding or subtracting
some unexplained noise.1 The weight is a multino-
mial over the six generation function modes, given
the record field f .
The CFG in Table 1 will produce many deriva-
tions for a given input (i.e., a set of database records)
which we represent compactly using a hypergraph or
a packed forest (Klein and Manning, 2001; Huang,
2008). Simplified examples of this representation
are shown in Figure 2.
3.2 Hypergraph Reranking
For our generation task, we are given a set of
database records d, and our goal is to find the best
corresponding text w. This corresponds to the best
grammar derivation among a set of candidate deriva-
tions represented implicitly in the hypergraph struc-
ture. As shown in Table 1, the mapping from d to w
is unknown. Therefore, all the intermediate multino-
mial distributions, described in the previous section,
define a hidden correspondence structure h, between
records, fields, and their values. We find the best
1The noise is modeled as a geometric distribution.
Algorithm 1: Averaged Structured Perceptron
Input: Training scenarios: (di,w?,h+i )
N
i=1
1 ?? 0
2 for t? 1 . . .T do
3 for i? 1 . . .N do
4 (w?, h?) = argmaxw,h? ??(di,wi,hi)
5 if (w?i ,h
+
i ) 6= (w?i, h?i) then
6 ?? ?+?(di,w?i ,h
+
i )??(di, w?i, h?i)
7 return 1T ?
T
t=1
1
N ?
N
i=1?
i
t
scoring derivation (w?, h?) by maximizing over con-
figurations of h:
(w?, h?) = argmax
w,h
? ??(d,w,h)
We define the score of (w?, h?) as the dot product
between a high dimensional feature representation
?= (?1, . . . ,?m) and a weight vector ?.
We estimate the weights ? using the averaged
structured perceptron algorithm (Collins, 2002),
which is well known for its speed and good perfor-
mance in similar large-parameter NLP tasks (Liang
et al, 2006; Huang, 2008). As shown in Algo-
rithm 1, the perceptron makes several passes over
the training scenarios, and in each iteration it com-
putes the best scoring (w?, h?) among the candidate
derivations, given the current weights ?. In line 6,
the algorithm updates ? with the difference (if any)
between the feature representations of the best scor-
ing derivation (w?, h?) and the the oracle derivation
(w?,h+). Here, w? is the estimated text, w? the gold-
standard text, h? is the estimated latent configuration
of the model and h+ the oracle latent configuration.
The final weight vector ? is the average of weight
vectors over T iterations and N scenarios. This av-
eraging procedure avoids overfitting and produces
more stable results (Collins, 2002).
In the following, we first explain how we decode
in this framework, i.e., find the best scoring deriva-
tion (Section 3.3) and discuss our definition for the
oracle derivation (w?,h+) (Section 3.4). Our fea-
tures are described in Section 4.2.
3.3 Hypergraph Decoding
Following Huang (2008), we also distinguish fea-
tures into local, i.e., those that can be computed
within the confines of a single hyperedge, and non-
local, i.e., those that require the prior visit of nodes
other than their antecedents. For example, the
372
Alignment feature in Figure 2(a) is local, and thus
can be computed a priori, but the Word Trigrams
is not; in Figure 2(b) words in parentheses are sub-
generations created so far at each word node; their
combination gives rise to the trigrams serving as
input to the feature. However, this combination
may not take place at their immediate ancestors,
since these may not be adjacent nodes in the hy-
pergraph. According to the grammar in Table 1,
there is no direct hyperedge between nodes repre-
senting words (W) and nodes representing the set of
fields these correspond to (FS); rather, W and FS are
connected implicitly via individual fields (F). Note,
that in order to estimate the trigram feature at the
FS node, we need to carry word information in the
derivations of its antecedents, as we go bottom-up.2
Given these two types of features, we can then
adapt Huang?s (2008) approximate decoding algo-
rithm to find (w?, h?). Essentially, we perform bottom-
up Viterbi search, visiting the nodes in reverse topo-
logical order, and keeping the k-best derivations for
each. The score of each derivation is a linear com-
bination of local and non-local features weights. In
machine translation, a decoder that implements for-
est rescoring (Huang and Chiang, 2007) uses the lan-
guage model as an external criterion of the good-
ness of sub-translations on account of their gram-
maticality. Analogously here, non-local features in-
fluence the selection of the best combinations, by
introducing knowledge that exceeds the confines of
the node under consideration and thus depend on
the sub-derivations generated so far. (e.g., word tri-
grams spanning a field node rely on evidence from
antecedent nodes that may be arbitrarily deeper than
the field?s immediate children).
Our treatment of leaf nodes (see rules (8) and (9))
differs from the way these are usually handled in
parsing. Since in generation we must emit rather
than observe the words, for each leaf node we there-
fore output the k-best words according to the learned
weights ? of the Alignment feature (see Sec-
tion 4.2), and continue building our sub-generations
bottom-up. This generation task is far from triv-
ial: the search space on the word level is the size of
the vocabulary and each field of a record can poten-
tially generate all words. Also, note that in decoding
it is useful to have a way to score different output
2We also store field information to compute structural fea-
tures, described in Section 4.2.
lengths |w|. Rather than setting w to a fixed length,
we rely on a linear regression predictor that uses the
counts of each record type per scenario as features
and is able to produce variable length texts.
3.4 Oracle Derivation
So far we have remained agnostic with respect to
the oracle derivation (w?,h+). In other NLP tasks
such as syntactic parsing, there is a gold-standard
parse, that can be used as the oracle. In our gener-
ation setting, such information is not available. We
do not have the gold-standard alignment between the
database records and the text that verbalizes them.
Instead, we approximate it using the existing de-
coder to find the best latent configuration h+ given
the observed words in the training text w?.3 This is
similar in spirit to the generative alignment model of
Liang et al (2009).
4 Experimental Design
In this section we present our experimental setup for
assessing the performance of our model. We give
details on our dataset, model parameters and fea-
tures, the approaches used for comparison, and ex-
plain how system output was evaluated.
4.1 Dataset
We conducted our experiments on the Air Travel In-
formation System (ATIS) dataset (Dahl et al, 1994)
which consists of transcriptions of spontaneous ut-
terances of users interacting with a hypothetical on-
line flight booking system. The dataset was orig-
inally created for the development of spoken lan-
guage systems and is partitioned in individual user
turns (e.g., flights from orlando to milwaukee, show
flights from orlando to milwaukee leaving after six
o?clock) each accompanied with an SQL query to a
booking system and the results of this query. These
utterances are typically short expressing a specific
communicative goal (e.g., a question about the ori-
gin of a flight or its time of arrival). This inevitably
results in small scenarios with a few words that of-
ten unambiguously correspond to a single record. To
avoid training our model on a somewhat trivial cor-
pus, we used the dataset introduced in Zettlemoyer
3In machine translation, Huang (2008) provides a soft al-
gorithm that finds the forest oracle, i.e., the parse among the
reranked candidates with the highest Parseval F-score. How-
ever, it still relies on the gold-standard reference translation.
373
and Collins (2007) instead, which combines the ut-
terances of a single user in one scenario and con-
tains 5,426 scenarios in total; each scenario corre-
sponds to a (manually annotated) formal meaning
representation (?-expression) and its translation in
natural language.
Lambda expressions were automatically con-
verted into records, fields and values following the
conventions adopted in Liang et al (2009).4 Given
a lambda expression like the one shown in Figure 1,
we first create a record for each variable and constant
(e.g., x, 9, august). We then assign record types ac-
cording to the corresponding class types (e.g., vari-
able x has class type flight). Next, fields and val-
ues are added from predicates with two arguments
with the class type of the first argument matching
that of the record type. The name of the predicate
denotes the field, and the second argument denotes
the value. We also defined special record types, such
as condition and search. The latter is introduced for
every lambda operator and assigned the categorical
field what with the value flight which refers to the
record type of variable x.
Contrary to datasets used in previous generation
studies (e.g., ROBOCUP (Chen and Mooney, 2008)
and WEATHERGOV (Liang et al, 2009)), ATIS has a
much richer vocabulary (927 words); each scenario
corresponds to a single sentence (average length
is 11.2 words) with 2.65 out of 19 record types
mentioned on average. Following Zettlemoyer and
Collins (2007), we trained on 4,962 scenarios and
tested on ATIS NOV93 which contains 448 examples.
4.2 Features
Broadly speaking, we defined two types of features,
namely lexical and structural ones. In addition,
we used a generatively trained PCFG as a baseline
feature and an alignment feature based on the co-
occurrence of records (or fields) with words.
Baseline Feature This is the log score of a gen-
erative decoder trained on the PCFG from Table 1.
We converted the grammar into a hypergraph, and
learned its probability distributions using a dynamic
program similar to the inside-outside algorithm (Li
and Eisner, 2009). Decoding was performed approx-
4The resulting dataset and a technical report describ-
ing the mapping procedure in detail are available from
http://homepages.inf.ed.ac.uk/s0793019/index.php?
page=resources
imately via cube pruning (Chiang, 2007), by inte-
grating a trigram language model extracted from the
training set (see Konstas and Lapata (2012) for de-
tails). Intuitively, the feature refers to the overall
goodness of a specific derivation, applied locally in
every hyperedge.
Alignment Features Instances of this feature fam-
ily refer to the count of each PCFG rule from Ta-
ble 1. For example, the number of times rule
R(search1.t)? FS( f light1,start)R( f light1.t) is in-
cluded in a derivation (see Figure 2(a))
Lexical Features These features encourage gram-
matical coherence and inform lexical selection over
and above the limited horizon of the language model
captured by Rules (6)?(9). They also tackle anoma-
lies in the generated output, due to the ergodicity of
the CFG rules at the record and field level:
Word Bigrams/Trigrams This is a group of
non-local feature functions that count word n-grams
at every level in the hypergraph (see Figure 2(b)).
The integration of words in the sub-derivations is
adapted from Chiang (2007).
Number of Words per Field This feature function
counts the number of words for every field, aiming
to capture compound proper nouns and multi-word
expressions, e.g., fields from and to frequently corre-
spond to two or three words such as ?new york? and
?salt lake city? (see Figure 2(d)).
Consecutive Word/Bigram/Trigram This feature
family targets adjacent repetitions of the same word,
bigram or trigram, e.g., ?show me the show me the
flights?.
Structural Features Features in this category tar-
get primarily content selection and influence appro-
priate choice at the field level:
Field bigrams/trigrams Analogously to the lexical
features mentioned above, we introduce a series of
non-local features that capture field n-grams, given
a specific record. For example the record flight in the
air travel domain typically has the values <from to>
(see Figure 2(c)). The integration of fields in sub-
derivations is implemented in fashion similar to the
integration of words.
Number of Fields per Record This feature family
is a coarser version of the Field bigrams/trigrams
374
R(search1.t)
FS(flight1.t,start) R(flight1.t)
FS0,3(search1.t,start)
w0(search1.t,type) ? ? ? w1,2(search1.t,what)
?
?
?
show
me
what
? ? ?
?
?
?
?
?
?
me the
me f lights
the f lights
? ? ?
?
?
?
FS2,6(flight1.t,start)
F2,4(flight1.t,from) FS4,6(flight1.t,from)
F4,6(flight1.t,to) ?
| 2 words |
(b)Word Trigrams (non-local)
<show me the>, <show me flights>, etc.
(a)Alignment Features (local)
<R(srch1.t)? FS(fl1.t,st) R(fl1.t)>
(c)Field Bigrams (non-local)
<from to> | flight
(d)Number of Words per Field (local)
<2 | from>
Figure 2: Simplified hypergraph examples with corresponding local and non-local features.
feature, which is deemed to be sparse for rarely-seen
records.
Field with No Value Although records in the ATIS
database schema have many fields, only a few are
assigned a value in any given scenario. For exam-
ple, the flight record has 13 fields, of which only 1.7
(on average) have a value. Practically, in a genera-
tive model this kind of sparsity would result in very
low field recall. We thus include an identity feature
function that explicitly counts whether a particular
field has a value.
4.3 Evaluation
We evaluated three configurations of our
model. A system that only uses the top scor-
ing derivation in each sub-generation and in-
corporates only the baseline and alignment
features (1-BEST+BASE+ALIGN). Our sec-
ond system considers the k-best derivations
and additionally includes lexical features
(k-BEST+BASE+ALIGN+LEX). The number of
k-best derivations was set to 40 and estimated
experimentally on held-out data. And finally,
our third system includes the full feature set
(k-BEST+BASE+ALIGN+LEX+STR). Note, that
the second and third system incorporate non-local
features, hence the use of k-best derivation lists.5
We compared our model to Angeli et al (2010)
whose approach is closest to ours.6
We evaluated system output automatically, using
the BLEU-4 modified precision score (Papineni et
5Since the addition of these features, essentially incurs
reranking, it follows that the systems would exhibit the exact
same performance as the baseline system with 1-best lists.
6We are grateful to Gabor Angeli for providing us with the
code of his system.
al., 2002) with the human-written text as reference.
We also report results with the METEOR score
(Banerjee and Lavie, 2005), which takes into ac-
count word re-ordering and has been shown to cor-
relate better with human judgments at the sentence
level. In addition, we evaluated the generated text by
eliciting human judgments. Participants were pre-
sented with a scenario and its corresponding verbal-
ization (see Figure 3) and were asked to rate the lat-
ter along two dimensions: fluency (is the text gram-
matical and overall understandable?) and semantic
correctness (does the meaning conveyed by the text
correspond to the database input?). The subjects
used a five point rating scale where a high number
indicates better performance. We randomly selected
12 documents from the test set and generated out-
put with two of our models (1-BEST+BASE+ALIGN
and k-BEST+BASE+ALIGN+LEX+STR) and Angeli
et al?s (2010) model. We also included the original
text (HUMAN) as a gold standard. We thus obtained
ratings for 48 (12? 4) scenario-text pairs. The study
was conducted over the Internet, using Amazon Me-
chanical Turk, and was completed by 51 volunteers,
all self reported native English speakers.
5 Results
Table 2 summarizes our results. As can be seen, in-
clusion of lexical features gives our decoder an ab-
solute increase of 6.73% in BLEU over the 1-BEST
system. It also outperforms the discriminative sys-
tem of Angeli et al (2010). Our lexical features
seem more robust compared to their templates. This
is especially the case with infrequent records, where
their system struggles to learn any meaningful infor-
mation. Addition of the structural features further
boosts performance. Our model increases by 8.69%
375
System BLEU METEOR
1-BEST+BASE+ALIGN 21.93 34.01
k-BEST+BASE+ALIGN+LEX 28.66 45.18
k-BEST+BASE+ALIGN+LEX+STR 30.62 46.07
ANGELI 26.77 42.41
Table 2: BLEU-4 and METEOR results on ATIS.
over the 1-BEST system and 3.85% over ANGELI in
terms of BLEU. We observe a similar trend when
evaluating system output with METEOR. Differ-
ences in magnitude are larger with the latter metric.
The results of our human evaluation study are
shown in Table 5. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
type (1-BEST, k-BEST, ANGELI, and HUMAN) on
the fluency and semantic correctness ratings. Means
differences were compared using a post-hoc Tukey
test. The k-BEST system is significantly better than
the 1-BEST and ANGELI (a < 0.01) both in terms
of fluency and semantic correctness. ANGELI is
significantly better than 1-BEST with regard to flu-
ency (a < 0.05) but not semantic correctness. There
is no statistically significant difference between the
k-BEST output and the original sentences (HUMAN).
Examples of system output are shown in Table 3.
They broadly convey similar meaning with the gold-
standard; ANGELI exhibits some long-range repeti-
tion, probably due to re-iteration of the same record
patterns. We tackle this issue with the inclusion of
non-local structural features. The 1-BEST system
has some grammaticality issues, which we avoid by
defining features over lexical n-grams and repeated
words. It is worth noting that both our system and
ANGELI produce output that is semantically com-
patible with but lexically different from the gold-
standard (compare please list the flights and show
me the flights against give me the flights). This is
expected given the size of the vocabulary, but raises
concerns regarding the use of automatic metrics for
the evaluation of generation output.
6 Conclusions
We presented a discriminative reranking framework
for an end-to-end generation system that performs
both content selection and surface realization. Cen-
tral to our approach is the encoding of generation
as a parsing problem. We reformulate the input (a
set of database records and text describing some of
System FluencySemCor
1-BEST+BASE+ALIGN 2.70 3.05
k-BEST+BASE+ALIGN+LEX+STR 4.02 4.04
ANGELI 3.74 3.17
HUMAN 4.18 4.02
Table 3: Mean ratings for fluency and semantic correct-
ness (SemCor) on system output elicited by humans.
Flight
from to
phoenix milwaukee
Time
when dep/ar
evening departure
Day
day dep/ar
wednesday departure
Search
type what
query flight
H
U
M
A
N
A
N
G
E
L
I
k-
B
E
S
T
1-
B
E
S
T
give me the flights from phoenix to milwaukee on
wednesday evening
show me the flights from phoenix to milwaukee on
wednesday evening flights from phoenix to milwaukee
please list the flights from phoenix to milwaukee on
wednesday evening
on wednesday evening from from phoenix to
milwaukee on wednesday evening
Figure 3: Example of scenario input and system output.
them) as a PCFG and convert it to a hypergraph. We
find the best scoring derivation via forest reranking
using both local and non-local features, that we train
using the perceptron algorithm. Experimental eval-
uation on the ATIS dataset shows that our model at-
tains significantly higher fluency and semantic cor-
rectness than any of the comparison systems. The
current model can be easily extended to incorporate,
additional, more elaborate features. Likewise, it can
port to other domains with similar database struc-
ture without modification, such as WEATHERGOV
and ROBOCUP. Finally, distributed training strate-
gies have been developed for the perceptron algo-
rithm (McDonald et al, 2010), which would allow
our generator to scale to even larger datasets.
In the future, we would also like to tackle more
challenging domains (e.g., product descriptions) and
to enrich our generator with some notion of dis-
course planning. An interesting question is how to
extend the PCFG-based approach advocated here so
as to capture discourse-level document structure.
376
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502?512, Cambridge, MA.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor, Michigan, June.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128?135, Helsinki, Finland.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the 17th In-
ternational Conference on Machine Learning, pages
175?182, Stanford, California.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing, pages 1?8, Philadelphia, Penn-
sylvania.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the Work-
shop on Human Language Technology, pages 43?48,
Plainsboro, New Jersey.
Robert Dale, Sabine Geldof, and Jean-Philippe Prost.
2003. Coral: Using natural language generation for
navigational assistance. In Proceedings of the 26th
Australasian Computer Science Conference, pages
35?44, Adelaide, Australia.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and appli-
cations. Discrete Applied Mathematics, 42:177?201.
Ruifang Ge and Raymond J. Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
the COLING/ACL 2006 Main Conference Poster Ses-
sions, pages 263?270, Sydney, Australia.
Nancy Green. 2006. Generation of biomedical argu-
ments for lay readers. In Proceedings of the 5th In-
ternational Natural Language Generation Conference,
pages 114?121, Sydney, Australia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the 7th Interna-
tional Workshop on Parsing Technologies, pages 123?
134, Beijing, China.
Ioannis Konstas and Mirella Lapata. 2012. Unsuper-
vised concept-to-text generation with hypergraphs. To
appear in Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Montre?al, Canada.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 40?51, Suntec, Sin-
gapore.
Zhifei Li and Sanjeev Khudanpur. 2009. Forest rerank-
ing for machine translation with the perceptron algo-
rithm. In GALE Book. GALE.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 761?768,
Sydney, Australia.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 91?99, Suntec, Singapore.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 456?464, Los Angeles, CA, June. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
377
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia.
Adwait Ratnaparkhi. 2002. Trainable approaches to sur-
face natural language generation and their application
to conversational dialog systems. Computer Speech &
Language, 16(3-4):435?455.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167:137?169.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL 2004: Main Proceedings, pages 177?
184, Boston, Massachusetts.
Ross Turner, Yaji Sripada, and Ehud Reiter. 2009. Gen-
erating approximate geographic descriptions. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 42?49, Athens, Greece.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?179, Rochester, NY.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 678?687, Prague, Czech
Republic.
378
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 516?525,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Tweet Recommendation with Graph Co-Ranking
Rui Yan?
?Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
r.yan@pku.edu.cn
Mirella Lapata?
?Institute for Language,
Cognition and Computation,
University of Edinburgh,
Edinburgh EH8 9AB, UK
mlap@inf.ed.ac.uk
Xiaoming Li?, \
\State Key Laboratory of Software
Development Environment,
Beihang University,
Beijing 100083, China
lxm@pku.edu.cn
Abstract
As one of the most popular micro-blogging
services, Twitter attracts millions of users,
producing millions of tweets daily. Shared in-
formation through this service spreads faster
than would have been possible with tradi-
tional sources, however the proliferation of
user-generation content poses challenges to
browsing and finding valuable information. In
this paper we propose a graph-theoretic model
for tweet recommendation that presents users
with items they may have an interest in. Our
model ranks tweets and their authors simulta-
neously using several networks: the social net-
work connecting the users, the network con-
necting the tweets, and a third network that
ties the two together. Tweet and author entities
are ranked following a co-ranking algorithm
based on the intuition that that there is a mu-
tually reinforcing relationship between tweets
and their authors that could be reflected in the
rankings. We show that this framework can be
parametrized to take into account user prefer-
ences, the popularity of tweets and their au-
thors, and diversity. Experimental evaluation
on a large dataset shows that our model out-
performs competitive approaches by a large
margin.
1 Introduction
Online micro-blogging services have revolutionized
the way people discover, share, and distribute infor-
mation. Twitter is perhaps the most popular such
service with over 140 million active users as of
2012.1 Twitter enables users to send and read text-
based posts of up to 140 characters, known as tweets.
Twitter users follow others or are followed. Being a
follower on Twitter means that the user receives all
the tweets from those she follows. Common prac-
tice of responding to a tweet has evolved into a well-
defined markup culture (e.g., RT stands for retweet,
?@? followed by an identifier indicates the user).
The strict limit of 140 characters allows for quick
and immediate communication in real time, whilst
enforcing brevity. Moreover, the retweet mecha-
nism empowers users to spread information of their
choice beyond the reach of their original followers.
Twitter has become a prominent broadcast-
ing medium, taking priority over traditional news
sources (Teevan et al, 2011). Shared information
through this channel spreads faster than would have
been possible with conventional news sites or RSS
feeds and can reach a far wider population base.
However, the proliferation of user-generated con-
tent comes at a price. Over 340 millions of tweets
are being generated daily amounting to thousands
of tweets per second!2 Twitter?s own search en-
gine handles more than 1.6 billion search queries per
day.3 This enormous amount of data renders it in-
feasible to browse the entire Twitter network; even
if this was possible, it would be extremely difficult
for users to find information they are interested in.
A hypothetical tweet recommendation system could
1For details see http://blog.twitter.com/2012/03/
twitter-turns-six.html
2In fact, the peak record is 6,939 tweets per second, reported
by http://blog.twitter.com/2011/03/numbers.html.
3See http://engineering.twitter.com/2011/05/
engineering-behind-twitters-new-search.html
516
alleviate this acute information overload, e.g., by
limiting the stream of tweets to those of interest to
the user, or by discovering intriguing content outside
the user?s following network.
The tweet recommendation task is challenging for
several reasons. Firstly, Twitter does not merely
consist of a set of tweets. Rather, it contains many
latent networks including the following relationships
among users and the retweeting linkage (which in-
dicates information diffusion). Secondly, the rec-
ommendations ought to be of interest to the user
and likely to to attract user response (e.g., to be
retweeted). Thirdly, recommendations should be
personalized (Cho and Schonfeld, 2007; Yan et al,
2011), avoid redundancy, and demonstrate diversity.
In this paper we present a graph-theoretic approach
to tweet recommendation that attempts to address
these challenges.
Our recommender operates over a heterogeneous
network that connects the users (or authors) and the
tweets they produce. The user network represents
links among authors based on their following be-
havior, whereas the tweet network connects tweets
based on content similarity. A third bipartite graph
ties the two together. Tweet and author entities in
this network are ranked simultaneously following a
co-ranking algorithm (Zhou et al, 2007). The main
intuition behind co-ranking is that there is a mu-
tually reinforcing relationship between authors and
tweets that could be reflected in the rankings. Tweets
are important if they are related to other important
tweets and authored by important users who in turn
are related to other important users. The model ex-
ploits this mutually reinforcing relationship between
tweets and their authors and couples two random
walks, one on the tweet graph and one on the author
graph, into a combined one. Rather than creating a
global ranking over all tweets in a collection, we ex-
tend this framework to individual users and produce
personalized recommendations. Moreover, we in-
corporate diversity by allowing the random walk on
the tweet graph to be time-variant (Mei et al, 2010).
Experimental results on a real-world dataset con-
sisting of 364,287,744 tweets from 9,449,542 users
show that the co-ranking approach substantially im-
proves performance over the state of the art. We ob-
tain a relative improvement of 18.3% (in nDCG) and
7.8% (in MAP) over the best comparison system.
2 Related Work
Tweet Search Given the large amount of tweets
being posted daily, ranking strategies have be-
come extremely important for retrieving information
quickly. Many websites currently offer a real-time
search service which returns ranked lists of Twit-
ter posts or shared links according to user queries.
Ranking methods used by these sites employ three
criteria, namely recency, popularity and content rel-
evance (Dong et al, 2010). State-of-art tweet re-
trieval methods include a linear regression model bi-
ased towards text quality with a regularization factor
inspired by the hypothesis that documents similar
in content may have similar quality (Huang et al,
2011). Duan et al (2010) learn a ranking model us-
ing SVMs and features based on tweet content, the
relations among users, and tweet specific character-
istics (e.g., urls, number of retweets).
Tweet Recommendation Previous work has also
focused on tweet recommendation systems, assum-
ing no explicit query is provided by the users.
Collaborative filtering is perhaps the most obvious
method for recommending tweets (Hannon et al,
2010). Chen et al (2010) investigate how to se-
lect interesting URLs linked from Twitter and rec-
ommend the top ranked ones to users. Their rec-
ommender takes three dimensions into account: the
source, the content topic, and social voting. Sim-
ilarly, Abel et al (2011a; 2011b; 2011c) recom-
mend external websites linked to Twitter. Their
method incorporates user profile modeling and tem-
poral recency, but they do not utilize the social
networks among users. R. et al (2009) propose
a diffusion-based recommendation framework es-
pecially for tweets representing critical events by
constructing a diffusion graph. Hong et al (2011)
recommend tweets based on popularity related fea-
tures. Ramage et al (2010) investigate which topics
users are interested in following a Labeled-LDA ap-
proach, by deciding whether a user is in the followee
list of a given user or not. Uysal and Croft (2011) es-
timate the likelihood of a tweet being reposted from
a user-centric perspective.
Our work also develops a tweet recommendation
system. Our model exploits the information pro-
vided by the tweets and the underlying social net-
works in a unified co-ranking framework. Although
517
these sources have been previously used to search
or recommend tweets, our model considers them
simultaneously and produces a ranking that is in-
formed by both. Furthermore, we argue that the
graph-theoretic framework upon which co-ranking
operates is beneficial as it allows to incorporate per-
sonalization (we provide user-specific rankings) and
diversity (the ranking is optimized so as to avoid re-
dundancy). The co-ranking framework has been ini-
tially developed for measuring scientific impact and
modeling the relationship between authors and their
publications (Zhou et al, 2007). However, the adap-
tation of this framework to the tweet recommenda-
tion task is novel to our knowledge.
3 Tweet Recommendation Framework
Our method operates over a heterogeneous network
that connects three graphs representing the tweets,
their authors and the relationships between them.
Let G denote the heterogeneous graph with nodes V
and edges E, and G = (V,E) = (VM ?VU ,EM ?EU ?
EMU). G is divided into three subgraphs, GM, GU
and GMU . GM = (VM,EM) is a weighted undirected
graph representing the tweets and their relationships.
Let VM = {mi|mi ?VM} denote a collection of |VM|
tweets and EM the set of links representing relation-
ships between them. The latter are established by
measuring how semantically similar any two tweets
are (see Section 3.4 for details). GU = (VU ,EU) is
an unweighted directed graph representing the so-
cial ties among Twitter users. VU = {ui|ui ? VU} is
the set of users with size |VU |. Links EU among
users are established by observing their following
behavior. GMU = (VMU ,EMU) is an unweighted bi-
partite graph that ties GM and GU together and repre-
sents tweet-author relationships. The graph consists
of nodes VMU = VM ?VU and edges EMU connect-
ing each tweet with all of its authors. Typically, a
tweet m is written by only one author u. However,
because of retweeting we treat all users involved in
reposting a tweet as ?co-authors?. The three subnet-
works are illustrated in Figure 1.
The framework includes three random walks, one
on GM, one on GU and one on GMU . A random walk
on a graph is a Markov chain, its states being the
vertices of the graph. It can be described by a square
n? n matrix M, where n is the number of vertices
in the graph. M is a stochastic matrix prescribing
Figure 1: Tweet recommendation based on a co-ranking
framework including three sub-networks. The undirected
links between tweets indicate semantic correlation. The
directed links between users denotes following. A bipar-
tite graph (whose edges are shown with dashed lines) ties
the tweet and author networks together.
the transition probabilities from one vertex to the
next. The framework couples the two random walks
on GM, and GU that rank tweets and theirs authors in
isolation. and allows to obtain a more global rank-
ing by taking into account their mutual dependence.
In the following sections we first describe how we
obtain the rankings on GM and GU , and then move
on to discuss how the two are coupled.
3.1 Ranking the Tweet Graph
Popularity We rank the tweet network follow-
ing the PageRank paradigm (Brin and Page, 1998).
Consider a random walk on GM and let M be the
transition matrix (defined in Section 3.4). Fix some
damping factor ? and say that at each time step with
probability (1-?) we stick to random walking and
with probability ? we do not make a usual random
walk step, but instead jump to any vertex, chosen
uniformly at random:
m = (1??)MTm+
?
|VM|
11T (1)
Here, vector m contains the ranking scores for the
vertices in GM. The fact that there exists a unique so-
518
lution to (1) follows from the random walk M being
ergodic (? >0 guarantees irreducibility, because we
can jump to any vertex). MT is the transpose of M.
1 is the vector of |VM| entries, each being equal to
one. Let m? RVM , ||m||1 = 1 be the only solution.
Personalization The standard PageRank algo-
rithm performs a random walk, starting from any
node, then randomly selects a link from that node to
follow considering the weighted matrix M, or jumps
to a random node with equal probability. It pro-
duces a global ranking over all tweets in the col-
lection without taking specific users into account.
As there are billions of tweets available on Twit-
ter covering many diverse topics, it is reasonable
to assume that an average user will only be inter-
ested in a small subset (Qiu and Cho, 2006). We
operationalize a user?s topic preference as a vec-
tor t = [t1, t2, . . . , tn]1?n, where n denotes the num-
ber of topics, and ti represents the degree of prefer-
ence for topic i. The vector t is normalized such
that ?ni=1 ti = 1. Intuitively, such vectors will be
different for different users. Note that user prefer-
ences can be also defined at the tweet (rather than
topic) level. Although tweets can illustrate user in-
terests more directly, in most cases a user will only
respond to a small fraction of tweets. This means
that most tweets will not provide any information
relating to a user?s interests. The topic preference
vector allows to propagate such information (based
on whether a tweet has been reposted or not) to other
tweets within the same topic cluster.
Given n topics, we obtain a topic distribution ma-
trix D using Latent Dirichlet Allocation (Blei et al,
2003). Let Di j denote the probability of tweet mi to
belong to topic t j. Consider a user with a topic pref-
erence vector t and topic distribution matrix D. We
calculate the response probability r for all tweets for
this user as:
r = tDT (2)
where r=[r1, r2, . . . , rVM ]1?|VM | represents the re-
sponse probability vector and ri the probability for a
user to respond to tweet mi. We normalize r so that
?ri?r ri = 1. Now, given the observed response prob-
ability vector r = [r1,r2, . . . ,rw]1?w, where w<|VM|
for a given user and the topic distribution ma-
trix D, our task is estimate the topic preference
vector t. We do this using maximum-likelihood
estimation. Assuming a user has responded to w
tweets, we approximate t so as to maximize the ob-
served response probability. Let r(t) = tDT. As-
suming all responses are independent, the probabil-
ity for w tweets r1, r2, . . . , rw is then ?wi=1 ri(t) under
a given t. The value of t is chosen when the proba-
bility is maximized:
t = argmax
t
( w
?
i=1
ri(t)
)
(3)
In a simple random walk, it is assumed that all
nodes in the matrix M are equi-probable before the
walk. In contrast, we use the topic preference vector
as a prior on M. Let Diag(r) denote a diagonal ma-
trix whose eigenvalue is vector r. Then m becomes:
m = (1??)[Diag(r)M]Tm+?r
= (1??)[Diag(tDT)M]Tm+?tDT
(4)
Diversity We would also like our output to be
diverse without redundant information. Unfortu-
nately, equation (4) will have the opposite effect,
as it assigns high scores to closely connected node
communities. A greedy algorithm such as Maxi-
mum Marginal Relevance (Carbonell and Goldstein,
1998; Wan et al, 2007; Wan et al, 2010) may
achieve diversity by iteratively selecting the most
prestigious or popular vertex and then penalizing the
vertices ?covered? by those that have been already
selected. Rather than adopting a greedy vertex selec-
tion method, we follow DivRank (Mei et al, 2010)
a recently proposed algorithm that balances popular-
ity and diversity in ranking, based on a time-variant
random walk. In contrast to PageRank, DivRank as-
sumes that the transition probabilities change over
time. Moreover, it is assumed that the transition
probability from one state to another is reinforced by
the number of previous visits to that state. At each
step, the algorithm creates a dynamic transition ma-
trix M(.). After z iterations, the matrix becomes:
M(z) = (1??)M(z?1) ?m(z?1)+?tDT (5)
and hence, m can be calculated as:
m(z) = (1??)[Diag(tDT)M(z)]Tm+?tDT (6)
Equation (5) increases the probability for nodes
with higher popularity. Nodes with high weights are
519
likely to ?absorb? the weights of their neighbors di-
rectly, and the weights of their neighbors? neighbors
indirectly. The process iteratively adjusts the ma-
trix M according to m and then updates m according
to the changed M. Essentially, the algorithm favors
nodes with high popularity and as time goes by there
emerges a rich-gets-richer effect (Mei et al, 2010).
3.2 Ranking the Author Graph
As mentioned earlier, we build a graph of au-
thors (and obtain the affinity U) using the follow-
ing linkage. We rank the author network using
PageRank analogously to equation (1). Besides
popularity, we also take personalization into ac-
count. Intuitively, users are likely to be interested
in their friends even if these are relatively unpopu-
lar. Therefore, for each author, we include a vec-
tor p = [p1, p2, . . . , p|VU |]1?|VU | denoting their prefer-
ence for other authors. The preference factor for au-
thor u toward other authors ui is defined as:
pui =
#tweets from ui
#tweets of u
(7)
which represents the proportion of tweets inherited
from user ui. A large pui means that u is more likely
to respond to ui?s tweets.
In theory, we could also apply DivRank on the au-
thor graph. However, as the authors are unique, we
assume that they are sufficiently distinct and there is
no need to promote diversity.
3.3 The Co-Ranking Algorithm
So far we have described how we rank the network
of tweets GM and their authors GU independently
following the PageRank paradigm. The co-ranking
framework includes a random walk on GM, GU ,
and GMU . The latter is a bipartite graph representing
which tweets are authored by which users. The ran-
dom walks on GM and GU are intra-class random
walks, because take place either within the tweets?
or the users? networks. The third (combined) ran-
dom walk on GMU is an inter-class random walk. It
is sufficient to describe it by a matrix MU|VM|?|VU|
and a matrix UM|VU|?|VM|, since GMU is bipartite.
One intra-class step changes the probability distribu-
tion from (m, 0) to (Mm, 0) or from (0, u) to (0, Uu),
while one inter-class step changes the probability
distribution from (m, u) to (UMT u, MUT m). The
design of M, U, MU and UM is detailed in Sec-
tion 3.4.
The two intra-class random walks are coupled
using the inter-class random walk on the bipartite
graph. The coupling is regulated by ?, a parameter
quantifying the importance of GMU versus GM and
GU . In the extreme case, if ? is set to 0, there is no
coupling. This amounts to separately ranking tweets
and authors by PageRank. In general, ? represents
the extent to which the ranking of tweets and their
authors depend on each other.
There are two intuitions behind the co-ranking al-
gorithm: (1) a tweet is important if it associates to
other important tweets, and is authored by impor-
tant users and (2) a user is important if they asso-
ciate to other important users, and they write impor-
tant tweets. We formulate these intuitions using the
following iterative procedure:
Step 1 Compute tweet saliency scores:
m(z+1) = (1??)([Diag(r)M(z)]T)m(z)+?UMTu(z)
m(z+1) = m(z+1)/||m(z+1)|| (8)
Step 2 Compute author saliency scores:
u(z+1) = (1??)([Diag(p)U]T)u(z)+?MUTm(z)
u(z+1) = u(z+1)/||u(z+1)|| (9)
Here, m(z) and u(z) are the ranking vectors for tweets
and authors for the z-th iteration. To guarantee con-
vergence, m and u are normalized after each itera-
tion. Note that the tweet transition matrix M is dy-
namic due to the computation of diversity while the
author transition matrix U is static. The algorithm
typically converges when the difference between the
scores computed at two successive iterations for any
tweet/author falls below a threshold ? (set to 0.001
in this study).
3.4 Affinity Matrices
The co-ranking framework is controlled by four
affinity matrices: M, U, MU and UM. In this section
we explain how these matrices are defined in more
detail.
The tweet graph is an undirected weighted graph,
where an edge between two tweets mi and m j repre-
sents their cosine similarity. An adjacency matrix M
520
describes the tweet graph where each entry corre-
sponds to the weight of a link in the graph:
Mij =
F (mi,m j)
?kF (mi,mk)
, F (mi,m j) =
~mi ?~m j
||~mi||||~m j||
(10)
where F (.) is the cosine similarity and ~m is a term
vector corresponding to tweet m. We treat a tweet
as a short document and weight each term with tf.idf
(Salton and Buckley, 1988), where tf is the term fre-
quency and idf is the inverse document frequency.
The author graph is a directed graph based on the
following linkage. When ui follows u j, we add a link
from ui to u j. Let the indicator function I (ui,u j) de-
note whether ui follows u j. The adjacency matrix U
is then defined as:
Uij =
I (ui,u j)
?k I (ui,uk)
, I (ui,u j)=
{
1if ei j ? EU
0if ei j /? EU
(11)
In the bipartite tweet-author graph GMU , the
entry EMU(i, j) is an indicator function denoting
whether tweet mi is authored by user u j:
A(mi,u j) =
{
1 if ei j ? EMU
0 if ei j /? EMU
(12)
Through EMU we define MU and UM, using the
weight matrices MU= [W?ij] and UM=[W?ji], con-
taining the conditional probabilities of transitioning
from mi to u j and vice versa:
W?ij =
A(mi,u j)
?kA(mi,uk)
, W?ji =
A(mi,u j)
?kA(mk,u j)
(13)
4 Experimental Setup
Data We crawled Twitter data from 23 seed users
(who were later invited to manually evaluate the
output of our system). In addition, we collected
the data of their followees and followers by travers-
ing the following edges, and exploring all newly
included users in the same way until no new
users were added. This procedure resulted in
a relatively large dataset consisting of 9,449,542
users, 364,287,744 tweets, 596,777,491 links, and
55,526,494 retweets. The crawler monitored the
data from 3/25/2011 to 5/30/2011. We used approx-
imately one month of this data for training and the
rest for testing.
Before building the graphs (i.e., the tweet graph,
the author graph, and the tweet-author graph), the
dataset was preprocessed as follows. We removed
tweets of low linguistic quality and subsequently
discarded users without any linkage to the remain-
ing tweets. We measured linguistic quality follow-
ing the evaluation framework put forward in Pitler
et al (2010). For instance, we measured the out-of-
vocabulary word ratio (as a way of gauging spelling
errors), entity coherence, fluency, and so on. We fur-
ther removed stopwords and performed stemming.
Parameter Settings We ran LDA with 500 itera-
tions of Gibbs sampling. The number of topics n
was set to 100 which upon inspection seemed gen-
erally coherent and meaningful. We set the damp-
ing factor ? to 0.15 following the standard PageRank
paradigm. We opted for more or less generic param-
eter values as we did not want to tune our frame-
work to the specific dataset at hand. We examined
the parameter ? which controls the balance of the
tweet-author graph in more detail. We experimented
with values ranging from 0 to 0.9, with a step size
of 0.1. Small ? values place little emphasis on the
tweet graph, whereas larger values rely more heav-
ily on the author graph. Mid-range values take both
graphs into account. Overall, we observed better
performance with values larger than 0.4. This sug-
gests that both sources of information ? the content
of the tweets and their authors ? are important for
the recommendation task. All our experiments used
the same ? value which was set to 0.6.
System Comparison We compared our approach
against three naive baselines and three state-of-the-
art systems recently proposed in the literature. All
comparison systems were subject to the same fil-
tering and preprocessing procedures as our own al-
gorithm. Our first baseline ranks tweets randomly
(Random). Our second baseline ranks tweets ac-
cording to token length: longer tweets are ranked
higher (Length). The third baseline ranks tweets
by the number of times they are reposted assum-
ing that more reposting is better (RTnum). We also
compared our method against Duan et al (2010).
Their model (RSVM) ranks tweets based on tweet
content features and tweet authority features using
the RankSVM algorithm (Joachims, 1999). Our
fifth comparison system (DTC) was Uysal and Croft
521
(2011) who use a decision tree classifier to judge
how likely it is for a tweet to be reposted by a spe-
cific user. This scenario is similar to ours when rank-
ing tweets by retweet likelihood. Finally, we com-
pared against Huang et al (2011) who use weighted
linear combination (WLC) to grade the relevance of
a tweet given a query. We implemented their model
without any query-related features as in our setting
we do not discriminate tweets depending on their
relevance to specific queries.
Evaluation We evaluated system output in two
ways, i.e., automatically and in a user study. Specif-
ically, we assume that if a tweet is retweeted it is rel-
evant and is thus ranked higher over tweets that have
not been reposted. We used our algorithm to predict
a ranking for the tweets in the test data which we
then compared against a goldstandard ranking based
on whether a tweet has been retweeted or not. We
measured ranking performance using the normalized
Discounted Cumulative Gain (nDCG; Ja?rvelin and
Keka?la?inen (2002)):
nDCG(k,VU) =
1
|VU|
?
u?VU
1
Zu
k
?
i=1
2r
u
i ?1
log(1+ i)
(14)
where VU denotes users, k indicates the top-k posi-
tions in a ranked list, and Zu is a normalization factor
obtained from a perfect ranking for a particular user.
rui is the relevance score (i.e., 1: retweeted, 0: not
retweeted) for the i-th tweet in the ranking list for
user u.
We also evaluated system output in terms of Mean
Average Precision (MAP), under the assumption
that retweeted tweets are relevant and the rest irrele-
vant:
MAP =
1
|VU|
?
u?VU
1
Nu
k
?
i=1
Pui ? r
u
i (15)
where Nu is the number of reposted tweets for user u,
and Pui is the precision at i-th position for user u
(Manning et al, 2008).
The automatic evaluation sketched above does not
assess the full potential of our recommendation sys-
tem. For instance, it is possible for the algorithm to
recommend tweets to users with no linkage to their
publishers. Such tweets may be of potential interest,
however our goldstandard data can only provide in-
formation for tweets and users with following links.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
Random 0.068 0.111 0.153 0.180 0.167
Length 0.275 0.288 0.298 0.335 0.258
RTNum 0.233 0.219 0.225 0.249 0.239
RSVM 0.392 0.400 0.421 0.444 0.558
DTC 0.441 0.468 0.492 0.473 0.603
WLC 0.404 0.421 0.437 0.464 0.592
CoRank 0.519 0.546 0.550 0.585 0.617
Table 1: Evaluation of tweet ranking output produced by
our system and comparison baselines against goldstan-
dard data.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
Random 0.081 0.103 0.116 0.107 0.175
Length 0.291 0.307 0.246 0.291 0.264
RTNum 0.258 0.318 0.343 0.346 0.257
RSVM 0.346 0.443 0.384 0.414 0.447
DTC 0.545 0.565 0.579 0.526 0.554
WLC 0.399 0.447 0.460 0.481 0.506
CoRank 0.567 0.644 0.715 0.643 0.628
Table 2: Evaluation of tweet ranking output produced by
our system and comparison baselines against judgments
elicited by users.
We therefore asked the 23 users whose Twitter data
formed the basis of our corpus to judge the tweets
ranked by our algorithm and comparison systems.
The users were asked to read the systems? recom-
mendations and decide for every tweet presented to
them whether they would retweet it or not, under the
assumption that retweeting takes place when users
find the tweet interesting.
In both automatic and human-based evaluations
we ranked all tweets in the test data. Then for each
date and user we selected the top 50 ones. Our
nDCG and MAP results are averages over users and
dates.
5 Results
Our results are summarized in Tables 1 and 2. Ta-
ble 1 reports results when model performance is
evaluated against the gold standard ranking obtained
from the Twitter network. In Table 2 model per-
formance is compared against rankings elicited by
users.
As can be seen, the Random method performs
worst. This is hardly surprising as it recommends
tweets without any notion of their importance or user
interest. Length performs considerably better than
522
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
PageRank 0.493 0.481 0.509 0.536 0.604
PersRank 0.501 0.542 0.558 0.560 0.611
DivRank 0.487 0.505 0.518 0.523 0.585
CoRank 0.519 0.546 0.550 0.585 0.617
Table 3: Evaluation of individual system components
against goldstandard data.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
PageRank 0.557 0.549 0.623 0.559 0.588
PersRank 0.571 0.595 0.655 0.613 0.601
DivRank 0.538 0.591 0.594 0.547 0.589
CoRank 0.637 0.644 0.715 0.643 0.628
Table 4: Evaluation of individual system components
against human judgments.
Random. This might be due to the fact that infor-
mativeness is related to tweet length. Using merely
the number of retweets does not seem to capture the
tweet importance as well as Length. This suggests
that highly retweeted posts are not necessarily in-
formative. For example, in our data, the most fre-
quently reposted tweet is a commercial advertise-
ment calling for reposting!
The supervised systems (RSVM, DTC, and
WLC) greatly improve performance over the naive
baselines. These methods employ standard machine
learning algorithms (such as SVMs, decision trees
and linear regression) on a large feature space. Aside
from the learning algorithm, their main difference
lies in the selection of the feature space, e.g., the way
content is represented and whether authority is taken
into account. DTC performs best on most evalua-
tion criteria. However, neither DTC nor RSVM, or
WLC take personalization into account. They gen-
erate the same recommendation lists for all users.
Our co-ranking algorithm models user interest with
respect to the content of the tweets and their pub-
lishers. Moreover, it attempts to create diverse out-
put and has an explicit mechanism for minimizing
redundancy. In all instances, using both DCG and
MAP, it outperforms the comparison systems. Inter-
estingly, the performance of CoRank is better when
measured against human judgments. This indicates
that users are interested in tweets that fall outside
the scope of their followers and that recommenda-
tion can improve user experience.
We further examined the contribution of the in-
dividual components of our system to the tweet
recommendation task. Tables 3 and 4 show how
the performance of our co-ranking algorithm varies
when considering only tweet popularity using the
standard PageRank algorithm, personalization (Per-
sRank), and diversity (DivRank). Note that DivRank
is only applied to the tweet graph. The PageR-
ank algorithm on its own makes good recommenda-
tions, while incorporating personalization improves
the performance substantially, which indicates that
individual users show preferences to specific topics
or other users. Diversity on its own does not seem
to make a difference, however it improves perfor-
mance when combined with personalization. Intu-
itively, users are more likely to repost tweets from
their followees, or tweets closely related to those
retweeted previously.
6 Conclusions
We presented a co-ranking framework for a tweet
recommendation system that takes popularity, per-
sonalization and diversity into account. Central to
our approach is the representation of tweets and
their users in a heterogeneous network and the abil-
ity to produce a global ranking that takes both in-
formation sources into account. Our model obtains
substantial performance gains over competitive ap-
proaches on a large real-world dataset (it improves
by 18.3% in DCG and 7.8% in MAP over the best
baseline). Our experiments suggest that improve-
ments are due to the synergy of the two information
sources (i.e., tweets and their authors). The adopted
graph-theoretic framework is advantageous in that
it allows to produce user-specific recommendations
and incorporate diversity in a unified model. Evalua-
tion with actual Twitter users shows that our recom-
mender can indeed identify interesting information
that lies outside the the user?s immediate following
network. In the future, we plan to extend the co-
ranking framework so as to incorporate information
credibility and temporal recency.
Acknowledgments This work was partially
funded by the Natural Science Foundation of China
under grant 60933004, and the Open Fund of the
State Key Laboratory of Software Development
Environment under grant SKLSDE-2010KF-03.
Rui Yan was supported by a MediaTek Fellowship.
523
References
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011a. Analyzing temporal dynamics in Twitter pro-
files for personalized recommendations in the social
web. In Proceedings of the ACM Web Science Confer-
ence 2011, pages 1?8, Koblenz, Germany.
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011b. Analyzing user modeling on Twitter for per-
sonalized news recommendations. User Modeling,
Adaptation and Personalization, pages 1?12.
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011c. Semantic enrichment of twitter posts for user
profile construction on the social web. The Semanic
Web: Research and Applications, pages 375?389.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alddress. Journal of Machine
Learning Research, 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. Pro-
ceedings of the 7th International Conference on World
Wide Web, 30(1-7):107?117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 335?336, Melbourne, Australia.
Jilin Chen, Rowan Nairn, Les Nelson, Michael Bernstein,
and Ed Chi. 2010. Short and tweet: experiments on
recommending content from information streams. In
Proceedings of the 28th International Conference on
Human Factors in Computing Systems, pages 1185?
1194, Atlanta, Georgia.
Junghoo Cho and Uri Schonfeld. 2007. Rankmass
crawler: a crawler with high personalized pagerank
coverage guarantee. In Proceedings of the 33rd Inter-
national Conference on Very Large Data Bases, pages
375?386, Vienna, Austria.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: improv-
ing recency ranking using Twitter data. In Proceed-
ings of the 19th International Conference on World
Wide Web, pages 331?340, Raleigh, North Carolina.
Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 295?303, Beijing, China.
John Hannon, Mike Bennett, and Barry Smyth. 2010.
Recommending twitter users to follow using content
and collaborative filtering approaches. In Proceedings
of the 4th ACM Conference on Recommender Systems,
pages 199?206, Barcelona, Spain.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison. 2011.
Predicting popular messages in Twitter. In Proceed-
ings of the 20th International Conference Companion
on World Wide Web, pages 57?58, Hyderabad, India.
Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011.
Quality-biased ranking of short texts in microblogging
services. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
373?382, Chiang Mai, Thailand.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:422?446.
Thorsten Joachims. 1999. Making large-scale svm learn-
ing practical. In Advances in Kernel Methods: Support
Vector Learning, pages 169?184. MIT press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schutze. 2008. Introduction to Information Re-
trieval, volume 1. Cambridge University Press.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010.
Divrank: the interplay of prestige and diversity in
information networks. In Proceedings of the 16th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1009?1018,
Washington, DC.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544?554, Uppsala, Sweden.
Feng Qiu and Junghoo Cho. 2006. Automatic identi-
fication of user interest for personalized search. In
Proceedings of the 15th International Conference on
World Wide Web, pages 727?736, Edinburgh, Scot-
land.
Sun Aaron R., Cheng Jiesi, Zeng, and Daniel Dajun.
2009. A novel recommendation framework for micro-
blogging based on information diffusion. In Pro-
ceedings of the 19th Annual Workshop on Information
Technologies and Systems, pages 199?216, Phoenix,
Arizona.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In In-
ternational AAAI Conference on Weblogs and Social
Media, pages 130?137. The AAAI Press.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Jaime Teevan, Daniel Ramage, and Meredith Ringel Mor-
ris. 2011. #Twittersearch: a comparison of microblog
search and web search. In Proceedings of the 4th ACM
524
International Conference on Web Search and Data
Mining, pages 35?44, Hong Kong, China.
Ibrahim Uysal and W. Bruce Croft. 2011. User oriented
tweet ranking: a filtering approach to microblogs.
In Proceedings of the 20th ACM International Con-
ference on Information and Knowledge Management,
pages 2261?2264, Glasgow, Scotland.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Single document summarization with document ex-
pansion. In Proceedings of the 22nd Conference
on Artificial Intelligence, pages 931?936, Vancouver,
British Columbia.
Xiaojun Wan, Huiying Li, and Jianguo Xiao. 2010.
Cross-language document summarization based on
machine translation quality prediction. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 917?926, Uppsala,
Sweden.
Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011. Sum-
marize what you are interested in: An optimiza-
tion framework for interactive personalized summa-
rization. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1342?1351. Association for Computational Lin-
guistics.
Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha, and
C. Lee Giles. 2007. Co-ranking authors and docu-
ments in a heterogeneous network. In Proceedings of
the 7th IEEE International Conference on Data Min-
ing, pages 739?744. IEEE.
525
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572?582,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Models of Semantic Representation with Visual Attributes
Carina Silberer, Vittorio Ferrari, Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
c.silberer@ed.ac.uk, vferrari@inf.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
We consider the problem of grounding the
meaning of words in the physical world
and focus on the visual modality which we
represent by visual attributes. We create
a new large-scale taxonomy of visual at-
tributes covering more than 500 concepts
and their corresponding 688K images. We
use this dataset to train attribute classi-
fiers and integrate their predictions with
text-based distributional models of word
meaning. We show that these bimodal
models give a better fit to human word as-
sociation data compared to amodal models
and word representations based on hand-
crafted norming data.
1 Introduction
Recent years have seen increased interest in
grounded language acquisition, where the goal is
to extract representations of the meaning of nat-
ural language tied to the physical world. The
language grounding problem has assumed sev-
eral guises in the literature such as semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Kate and Mooney, 2007; Lu et
al., 2008; Bo?rschinger et al, 2011), mapping nat-
ural language instructions to executable actions
(Branavan et al, 2009; Tellex et al, 2011), associ-
ating simplified language to perceptual data such
as images or video (Siskind, 2001; Roy and Pent-
land, 2002; Gorniak and Roy, 2004; Yu and Bal-
lard, 2007), and learning the meaning of words
based on linguistic and perceptual input (Bruni
et al, 2012b; Feng and Lapata, 2010; Johns and
Jones, 2012; Andrews et al, 2009; Silberer and
Lapata, 2012).
In this paper we are concerned with the latter
task, namely constructing perceptually grounded
distributional models. The motivation for models
that do not learn exclusively from text is twofold.
From a cognitive perspective, there is mounting
experimental evidence suggesting that our inter-
action with the physical world plays an impor-
tant role in the way we process language (Barsa-
lou, 2008; Bornstein et al, 2004; Landau et al,
1998). From an engineering perspective, the abil-
ity to learn representations for multimodal data has
many practical applications including image re-
trieval (Datta et al, 2008) and annotation (Chai
and Hung, 2008), text illustration (Joshi et al,
2006), object and scene recognition (Lowe, 1999;
Oliva and Torralba, 2007; Fei-Fei and Perona,
2005), and robot navigation (Tellex et al, 2011).
One strand of research uses feature norms as a
stand-in for sensorimotor experience (Johns and
Jones, 2012; Andrews et al, 2009; Steyvers, 2010;
Silberer and Lapata, 2012). Feature norms are ob-
tained by asking native speakers to write down at-
tributes they consider important in describing the
meaning of a word. The attributes represent per-
ceived physical and functional properties associ-
ated with the referents of words. For example,
apples are typically green or red, round, shiny,
smooth, crunchy, tasty, and so on; dogs have four
legs and bark, whereas chairs are used for sit-
ting. Feature norms are instrumental in reveal-
ing which dimensions of meaning are psychologi-
cally salient, however, their use as a proxy for peo-
ple?s perceptual representations can itself be prob-
lematic (Sloman and Ripps, 1998; Zeigenfuse and
Lee, 2010). The number and types of attributes
generated can vary substantially as a function of
the amount of time devoted to each concept. It is
not entirely clear how people generate attributes
and whether all of these are important for repre-
senting concepts. Finally, multiple participants are
required to create a representation for each con-
572
cept, which limits elicitation studies to a small
number of concepts and the scope of any compu-
tational model based on feature norms.
Another strand of research focuses exclusively
on the visual modality, even though the grounding
problem could involve auditory, motor, and hap-
tic modalities as well. This is not entirely sur-
prising. Visual input represents a major source of
data from which humans can learn semantic rep-
resentations of linguistic and non-linguistic com-
municative actions (Regier, 1996). Furthermore,
since images are ubiquitous, visual data can be
gathered far easier than some of the other modali-
ties. Distributional models that integrate the visual
modality have been learned from texts and im-
ages (Feng and Lapata, 2010; Bruni et al, 2012b)
or from ImageNet (Deng et al, 2009), e.g., by
exploiting the fact that images in this database
are hierarchically organized according to WordNet
synsets (Leong and Mihalcea, 2011). Images are
typically represented on the basis of low-level fea-
tures such as SIFT (Lowe, 2004), whereas texts
are treated as bags of words.
Our work also focuses on images as a way
of physically grounding the meaning of words.
We, however, represent them by high-level vi-
sual attributes instead of low-level image fea-
tures. Attributes are not concept or category spe-
cific (e.g., animals have stripes and so do cloth-
ing items; balls are round, and so are oranges and
coins), and thus allow us to express similarities
and differences across concepts more easily. Fur-
thermore, attributes allow us to generalize to un-
seen objects; it is possible to say something about
them even though we cannot identify them (e.g., it
has a beak and a long tail). We show that this
attribute-centric approach to representing images
is beneficial for distributional models of lexical
meaning. Our attributes are similar to those pro-
vided by participants in norming studies, however,
importantly they are learned from training data (a
database of images and their visual attributes) and
thus generalize to new images without additional
human involvement.
In the following we describe our efforts to cre-
ate a new large-scale dataset that consists of 688K
images that match the same concrete concepts
used in the feature norming study of McRae et al
(2005). We derive a taxonomy of 412 visual at-
tributes and explain how we learn attribute clas-
sifiers following recent work in computer vision
(Lampert et al, 2009; Farhadi et al, 2009). Next,
we show that this attribute-based image represen-
tation can be usefully integrated with textual data
to create distributional models that give a better fit
to human word association data over models that
rely on human generated feature norms.
2 Related Work
Grounding semantic representations with visual
information is an instance of multimodal learn-
ing. In this setting the data consists of multiple
input modalities with different representations and
the learner?s objective is to extract a unified repre-
sentation that fuses the modalities together. The
literature describes several successful approaches
to multimodal learning using different variants of
deep networks (Ngiam et al, 2011; Srivastava and
Salakhutdinov, 2012) and data sources including
text, images, audio, and video.
Special-purpose models that address the fusion
of distributional meaning with visual information
have been also proposed. Feng and Lapata (2010)
represent documents and images by a common
multimodal vocabulary consisting of textual words
and visual terms which they obtain by quantizing
SIFT descriptors (Lowe, 2004). Their model is es-
sentially Latent Dirichlet Allocation (LDA, Blei et
al., 2003) trained on a corpus of multimodal docu-
ments (i.e., BBC news articles and their associated
images). Meaning in this model is represented as
a vector whose components correspond to word-
topic distributions. A related model has been pro-
posed by Bruni et al (2012b) who obtain distinct
representations for the textual and visual modali-
ties. Specifically, they extract a visual space from
images contained in the ESP-Game data set (von
Ahn and Dabbish, 2004) and a text-based seman-
tic space from a large corpus collection totaling
approximately two billion words. They concate-
nate the two modalities and subsequently project
them to a lower-dimensionality space using Sin-
gular Value Decomposition (Golub et al, 1981).
Traditionally, computer vision algorithms de-
scribe visual phenomena (e.g., objects, scenes,
faces, actions) by giving each instance a categor-
ical label (e.g., cat, beer garden, Brad Pitt, drink-
ing). The ability to describe images by their at-
tributes allows to generalize to new instances for
which there are no training examples available.
Moreover, attributes can transcend category and
task boundaries and thus provide a generic de-
scription of visual data.
Initial work (Ferrari and Zisserman, 2007)
573
focused on simple color and texture attributes
(e.g., blue, stripes) and showed that these can be
learned in a weakly supervised setting from im-
ages returned by a search engine when using the
attribute as a query. Farhadi et al (2009) were
among the first to use visual attributes in an ob-
ject recognition task. Using an inventory of 64 at-
tribute labels, they developed a dataset of approx-
imately 12,000 instances representing 20 objects
from the PASCAL Visual Object Classes Chal-
lenge 2008 (Everingham et al, 2008). Visual
semantic attributes (e.g., hairy, four-legged) were
used to identify familiar objects and to describe
unfamiliar objects when new images and bound-
ing box annotations were provided. Lampert et al
(2009) showed that attribute-based representations
can be used to classify objects when there are no
training examples of the target classes available.
Their dataset contained over 30,000 images repre-
senting 50 animal concepts and used 85 attributes
from the norming study of Osherson et al (1991).
Attribute-based representations have also been ap-
plied to the tasks of face detection (Kumar et al,
2009), action identification (Liu et al, 2011), and
scene recognition (Patterson and Hays, 2012).
The use of visual attributes in models of distri-
butional semantics is novel to our knowledge. We
argue that they are advantageous for two reasons.
Firstly, they are cognitively plausible; humans em-
ploy visual attributes when describing the proper-
ties of concept classes. Secondly, they occupy the
middle ground between non-linguistic low-level
image features and linguistic words. Attributes
crucially represent image properties, however by
being words themselves, they can be easily inte-
grated in any text-based distributional model thus
eschewing known difficulties with rendering im-
ages into word-like units.
A key prerequisite in describing images by
their attributes is the availability of training data
for learning attribute classifiers. Although our
database shares many features with previous work
(Lampert et al, 2009; Farhadi et al, 2009) it dif-
fers in focus and scope. Since our goal is to
develop distributional models that are applicable
to many words, it contains a considerably larger
number of concepts (i.e., more than 500) and at-
tributes (i.e., 412) based on a detailed taxonomy
which we argue is cognitively plausible and ben-
eficial for image and natural language processing
tasks. Our experiments evaluate a number of mod-
els previously proposed in the literature and in
Attribute Categories Example Attributes
color patterns (25) is red, has stripes
diet (35) eats nuts, eats grass
shape size (16) is small, is chubby
parts (125) has legs, has wheels
botany;anatomy (25;78) has seeds, has fur
behavior (in)animate (55) flies, waddles, pecks
texture material (36) made of metal, is shiny
structure (3) 2 pieces, has pleats
Table 1: Attribute categories and examples of at-
tribute instances. Parentheses denote the number
of attributes per category.
all cases show that the attribute-based represen-
tation brings performance improvements over just
using the textual modality. Moreover, we show
that automatically computed attributes are compa-
rable and in some cases superior to those provided
by humans (e.g., in norming studies).
3 The Attribute Dataset
Concepts and Images We created a dataset of
images and their visual attributes for the nouns
contained in McRae et al?s (2005) feature norms.
The norms cover a wide range of concrete con-
cepts including animate and inanimate things
(e.g., animals, clothing, vehicles, utensils, fruits,
and vegetables) and were collected by presenting
participants with words and asking them to list
properties of the objects to which the words re-
ferred. To avoid confusion, in the remainder of
this paper we will use the term attribute to refer to
properties of concepts and the term feature to refer
to image features, such as color or edges.
Images for the concepts in McRae et al?s (2005)
production norms were harvested from ImageNet
(Deng et al, 2009), an ontology of images based
on the nominal hierarchy of WordNet (Fellbaum,
1998). ImageNet has more than 14 million im-
ages spanning 21K WordNet synsets. We chose
this database due to its high coverage and the high
quality of its images (i.e., cleanly labeled and high
resolution). McRae et al?s norms contain 541 con-
cepts out of which 516 appear in ImageNet1 and
are represented by 688K images overall. The av-
erage number of images per concept is 1,310 with
the most popular being closet (2,149 images) and
the least popular prune (5 images).
1Some words had to be modified in order to match the cor-
rect synset, e.g., tank (container) was found as storage tank.
574
behavior eats, walks, climbs, swims, runs
diet drinks water, eats anything
shape size is tall, is large
anatomy has mouth, has head, has nose, has tail, has claws,
has jaws, has neck, has snout, has feet, has tongue
color patterns is black, is brown, is white
botany has skin, has seeds, has stem, has leaves, has pulp
color patterns purple, white, green, has green top
shape size is oval, is long
texture material is shiny
behavior rolls
parts has step through frame, has fork, has 2 wheels, has chain, has pedals
has gears, has handlebar, has bell, has breaks has seat, has spokes
texture material made of metal
color patterns different colors, is black, is red, is grey, is silver
Table 2: Human-authored attributes for bear, eggplant, and bike.
The images depicting each concept were ran-
domly partitioned into a training, development,
and test set. For most concepts the development
set contained a maximum of 100 images and the
test set a maximum of 200 images. Concepts with
less than 800 images in total were split into 1/8
test and development set each, and 3/4 training set.
The development set was used for devising and re-
fining our attribute annotation scheme. The train-
ing and test sets were used for learning and eval-
uating, respectively, attribute classifiers (see Sec-
tion 4).
Attribute Annotation Our aim was to develop a
set of visual attributes that are both discriminating
and cognitively plausible, i.e., humans would gen-
erally use them to describe a concrete concept. As
a starting point, we thus used the visual attributes
from McRae et al?s (2005) norming study. At-
tributes capturing other primary sensory informa-
tion (e.g., smell, sound), functional/motor proper-
ties, or encyclopaedic information were not taken
into account. For example, is purple is a valid vi-
sual attribute for an eggplant, whereas a vegetable
is not, since it cannot be visualized. Collating all
the visual attributes in the norms resulted in a to-
tal of 673 which we further modified and extended
during the annotation process explained below.
The annotation was conducted on a per-concept
rather than a per-image basis (as for example in
Farhadi et al (2009)). For each concept (e.g., bear
or eggplant), we inspected the images in the devel-
opment set and chose all McRae et al (2005) vi-
sual attributes that applied. If an attribute was gen-
erally true for the concept, but the images did not
provide enough evidence, the attribute was never-
theless chosen and labeled with <no evidence>.
For example, a plum has a pit, but most images in
ImageNet show plums where only the outer part
of the fruit is visible. Attributes supported by
the image data but missing from the norms were
added. For example, has lights and has bumper
are attributes of cars but are not included in the
norms. Attributes were grouped in eight general
classes shown in Table 1. Annotation proceeded
on a category-by-category basis, e.g., first all food-
related concepts were annotated, then animals, ve-
hicles, and so on. Two annotators (both co-authors
of this paper) developed the set of attributes for
each category. One annotator first labeled con-
cepts with their attributes, and the other annota-
tor reviewed the annotations, making changes if
needed. Annotations were revised and compared
per category in order to ensure consistency across
all concepts of that category.
Our methodology is slightly different from
Lampert et al (2009) in that we did not simply
transfer the attributes from the norms to the con-
cepts in question but refined and extended them
according to the visual data. There are several
reasons for this. Firstly, it makes sense to se-
lect attributes corroborated by the images. Sec-
ondly, by looking at the actual images, we could
eliminate errors in McRae et al?s (2005) norms.
For example, eight study participants erroneously
thought that a catfish has scales. Thirdly, dur-
ing the annotation process, we normalized syn-
onymous attributes (e.g., has pit and has stone)
and attributes that exhibited negligible variations
575
has 2 pieces, has pointed end, has strap, has thumb, has buckles, has heels
has shoe laces, has soles, is black, is brown, is white, made of leather, made of rubber
climbs, climbs trees, crawls, hops, jumps, eats, eats nuts, is small, has bushy tail
has 4 legs, has head, has neck, has nose, has snout, has tail, has claws
has eyes, has feet, has toes,
diff colours, has 2 legs, has 2 wheels, has windshield, has floorboard, has stand, has tank
has mudguard, has seat, has exhaust pipe, has frame, has handlebar, has lights, has mirror
has step-through frame, is black, is blue, is red, is white, made of aluminum, made of steel
Table 3: Attribute predictions for sandals, squirrel, and motorcycle.
in meaning (e.g., has stem and has stalk). Finally,
our aim was to collect an exhaustive list of vi-
sual attributes for each concept which is consis-
tent across all members of a category. This is un-
fortunately not the case in McRae et al?s norms.
Participants were asked to list up to 14 different
properties that describe a concept. As a result, the
attributes of a concept denote the set of properties
humans consider most salient. For example, both,
lemons and oranges have pulp. But the norms pro-
vide this attribute only for the second concept.
On average, each concept was annotated with
19 attributes; approximately 14.5 of these were
not part of the semantic representation created by
McRae et al?s (2005) participants for that con-
cept even though they figured in the representa-
tions of other concepts. Furthermore, on average
two McRae et al attributes per concept were dis-
carded. Examples of concepts and their attributes
from our database2 are shown in Table 2.
4 Attribute-based Classification
Following previous work (Farhadi et al, 2009;
Lampert et al, 2009) we learned one classifier per
attribute (i.e., 350 classifiers in total).3 The train-
ing set consisted of 91,980 images (with a maxi-
mum of 350 images per concept). We used an L2-
regularized L2-loss linear SVM (Fan et al, 2008)
to learn the attribute predictions. We adopted the
training procedure of Farhadi al. (2009).4 To learn
a classifier for a particular attribute, we used all
images in the training data. Images of concepts
annotated with the attribute were used as positive
examples, and the rest as negative examples. The
2Available from http://homepages.inf.ed.ac.uk/
mlap/index.php?page=resources.
3We only trained classifiers for attributes corroborated by
the images and excluded those labeled with <no evidence>.
4http://vision.cs.uiuc.edu/attributes/
data was randomly split into a training and valida-
tion set of equal size in order to find the optimal
cost parameter C. The final SVM for the attribute
was trained on the entire training data, i.e., on all
positive and negative examples.
The SVM learners used the four different fea-
ture types proposed in Farhadi et al (2009),
namely color, texture, visual words, and edges.
Texture descriptors were computed for each pixel
and quantized to the nearest 256 k-means centers.
Visual words were constructed with a HOG spa-
tial pyramid. HOG descriptors were quantized
into 1000 k-means centers. Edges were detected
using a standard Canny detector and their orien-
tations were quantized into eight bins. Color de-
scriptors were sampled for each pixel and quan-
tized to the nearest 128 k-means centers. Shapes
and locations were represented by generating his-
tograms for each feature type for each cell in a grid
of three vertical and horizontal blocks. Our clas-
sifiers used 9,688 features in total. Table 3 shows
their predictions for three test images.
Note that attributes are predicted on an image-
by-image basis; our task, however, is to describe a
concept w by its visual attributes. Since concepts
are represented by many images we must some-
how aggregate their attributes into a single repre-
sentation. For each image iw ? Iw of concept w,
we output an F-dimensional vector containing pre-
diction scores scorea(iw) for attributes a = 1, ...,F.
We transform these attribute vectors into a single
vector pw ? [0,1]1?F , by computing the centroid
of all vectors for concept w. The vector is nor-
malized to obtain a probability distribution over
attributes given w:
pw = (?iw?Iw scorea(iw))a=1,...,F?Fa=1?iw?Iw scorea(iw)
(1)
We additionally impose a threshold ? on pw by set-
576
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
P
r
e
c
i
s
i
o
n
Recall
Figure 1: Attribute classifier performance for dif-
ferent thresholds ? (test set).
ting each entry less than ? to zero.
Figure 1 shows the results of the attribute pre-
diction on the test set on the basis of the computed
centroids; specifically, we plot recall against pre-
cision based on threshold ?.5 Table 4 shows the
10 nearest neighbors for five example concepts
from our dataset. Again, we measure the cosine
similarity between a concept and all other con-
cepts in the dataset when these are represented by
their visual attribute vector pw.
5 Attribute-based Semantic Models
We evaluated the effectiveness of our attribute
classifiers by integrating their predictions with tra-
ditional text-only models of semantic representa-
tion. These models have been previously proposed
in the literature and were also described in a recent
comparative study (Silberer and Lapata, 2012).
We represent the visual modality by attribute
vectors computed as shown in Equation (1). The
linguistic environment is approximated by textual
attributes. We used Strudel (Baroni et al, 2010)
to obtain these attributes for the nouns in our
dataset. Given a list of target words, Strudel ex-
tracts weighted word-attribute pairs from a lem-
matized and pos-tagged text corpus (e.g., egg-
plant?cook-v, eggplant?vegetable-n). The weight
of each word-attribute pair is a log-likelihood ratio
score expressing the pair?s strength of association.
In our experiments we learned word-attribute pairs
from a lemmatized and pos-tagged (2009) dump
of the English Wikipedia.6 In the remainder of
this section we will briefly describe the models we
5Threshold values ranged from 0 to 0.9 with 0.1 stepsize.
6The corpus can be downloaded from http://wacky.
sslmit.unibo.it/doku.php?id=corpora.
Concept Nearest Neighbors
boat ship, sailboat, yacht, submarine, canoe,
whale, airplane, jet, helicopter, tank (army)
rooster chicken, turkey, owl, pheasant, peacock, stork,
pigeon, woodpecker, dove, raven
shirt blouse, robe, cape, vest, dress, coat, jacket,
skirt, camisole, nightgown
spinach lettuce, parsley, peas, celery, broccoli, cab-
bage, cucumber, rhubarb, zucchini, asparagus
squirrel chipmunk, raccoon, groundhog, gopher, por-
cupine, hare, rabbit, fox, mole, emu
Table 4: Ten most similar concepts computed on
the basis of averaged attribute vectors and ordered
according to cosine similarity.
used in our study and how the textual and visual
modalities were fused to create a joint representa-
tion.
Concatenation Model Variants of this model
were originally proposed in Bruni et al (2011)
and Johns and Jones (2012). Let T ? RN?D de-
note a term-attribute co-occurrence matrix, where
each cell records a weighted co-occurrence score
of a word and a textual attribute. Let P ? [0,1]N?F
denote a visual matrix, representing a probability
distribution over visual attributes for each word.
A word?s meaning can be then represented by the
concatenation of its normalized textual and visual
vectors.
Canonical Correlation Analysis The second
model uses Canonical Correlation Analysis (CCA,
Hardoon et al (2004)) to learn a joint semantic
representation from the textual and visual modali-
ties. Given two random variables x and y (or two
sets of vectors), CCA can be seen as determining
two sets of basis vectors in such a way, that the cor-
relation between the projections of the variables
onto these bases is mutually maximized (Borga,
2001). In effect, the representation-specific de-
tails pertaining to the two views of the same phe-
nomenon are discarded and the underlying hidden
factors responsible for the correlation are revealed.
The linguistic and visual views are the same as
in the simple concatenation model just explained.
We use a kernelized version of CCA (Hardoon et
al., 2004) that first projects the data into a higher-
dimensional feature space and then performs CCA
in this new feature space. The two kernel matrices
are KT = T T ? and KP = PP?. After applying CCA
we obtain two matrices projected onto l basis vec-
tors, T? ?RN?l , resulting from the projection of the
577
textual matrix T onto the new basis and P? ?RN?l ,
resulting from the projection of the corresponding
visual attribute matrix. The meaning of a word is
then represented by T? or P?.
Attribute-topic Model Andrews et al (2009)
present an extension of LDA (Blei et al, 2003)
where words in documents and their associated
attributes are treated as observed variables that
are explained by a generative process. The
idea is that each document in a document col-
lection D is generated by a mixture of com-
ponents {x1, ...,xc, ...,xC} ? C , where a compo-
nent xc comprises a latent discourse topic coupled
with an attribute cluster. Inducing these attribute-
topic components from D with the extended LDA
model gives two sets of parameters: word prob-
abilities given components PW (wi|X = xc) for wi,
i = 1, ...,n, and attribute probabilities given com-
ponents PA(ak|X = xc) for ak, k = 1, ...,F . For ex-
ample, most of the probability mass of a compo-
nent x would be reserved for the words shirt, coat,
dress and the attributes has 1 piece, has seams,
made of material and so on.
Word meaning in this model is represented by
the distribution PX |W over the learned compo-
nents. Assuming a uniform distribution over com-
ponents xc in D , PX |W can be approximated as:
PX=xc|W=wi =
P(wi|xc)P(xc)
P(wi) ?
P(wi|xc)
C?
l=1
P(wi|xl)
(2)
where C is the total number of components.
In our work, the training data is a corpus D of
textual attributes (rather than documents). Each
attribute is represented as a bag-of-concepts,
i.e., words demonstrating the property expressed
by the attribute (e.g., vegetable-n is a property of
eggplant, spinach, carrot). For some of these con-
cepts, our classifiers predict visual attributes. In
this case, the concepts are paired with one of their
visual attributes. We sample attributes for a con-
cept w from their distribution given w (Eq. (1)).
6 Experimental Setup
Evaluation Task We evaluated the distribu-
tional models presented in Section 5 on the
word association norms collected by Nelson et al
(1998).7 These were established by presenting
a large number of participants with a cue word
(e.g., rice) and asking them to name an associate
7From http://w3.usf.edu/FreeAssociation/.
word in response (e.g., Chinese, wedding, food,
white). For each cue, the norms provide a set
of associates and the frequencies with which they
were named. We can thus compute the prob-
ability distribution over associates for each cue.
Analogously, we can estimate the degree of sim-
ilarity between a cue and its associates using our
models. The norms contain 63,619 unique cue-
associate pairs. Of these, 435 pairs were covered
by McRae et al (2005) and our models. We also
experimented with 1,716 pairs that were not part
of McRae et al?s study but belonged to concepts
covered by our attribute taxonomy (e.g., animals,
vehicles), and were present in our corpus and Ima-
geNet. Using correlation analysis (Spearman?s ?),
we examined the degree of linear relationship be-
tween the human cue-associate probabilities and
the automatically derived similarity values.8
Parameter Settings In order to integrate the vi-
sual attributes with the models described in Sec-
tion 5 we must select the appropriate threshold
value ? (see Eq. (1)). We optimized this value
on the development set and obtained best results
with ? = 0. We also experimented with thresh-
olding the attribute prediction scores and with ex-
cluding attributes with low precision. In both
cases, we obtained best results when using all at-
tributes. We could apply CCA to the vectors rep-
resenting each image separately and then compute
a weighted centroid on the projected vectors. We
refrained from doing this as it involves additional
parameters and assumes input different from the
other models. We measured the similarity between
two words using the cosine of the angle. For the
attribute-topic model, the number of predefined
components C was set to 10. In this model, sim-
ilarity was measured as defined by Griffiths et al
(2007). The underlying idea is that word associa-
tion can be expressed as a conditional distribution.
With regard to the textual attributes, we
obtained a 9,394-dimensional semantic space
after discarding word-attribute pairs with a
log-likelihood ratio score less than 19.9 We also
discarded attributes co-occurring with less than
two different words.
8Previous work (Griffiths et al, 2007) which also predicts
word association reports how many times the word with the
highest score under the model was the first associate in the
human norms. This evaluation metric assumes that there are
many associates for a given cue which unfortunately is not
the case in our study which is restricted to the concepts rep-
resented in our attribute taxonomy.
9Baroni et al (2010) use a similar threshold of 19.51.
578
Nelson Concat CCA TopicAttr TextAttr
Concat 0.24
CCA 0.30 0.72
TopicAttr 0.26 0.55 0.28
TextAttr 0.21 0.80 0.83 0.34
VisAttr 0.23 0.65 0.52 0.40 0.39
Table 5: Correlation matrix for seen Nelson et al
(1998) cue-associate pairs and five distributional
models. All correlation coefficients are statisti-
cally significant (p < 0.01, N = 435).
7 Results
Our experiments were designed to answer four
questions: (1) Do visual attributes improve the
performance of distributional models? (2) Are
there performance differences among different
models, i.e., are some models better suited to the
integration of visual information? (3) How do
computational models fare against gold standard
norming data? (4) Does the attribute-based repre-
sentation bring advantages over more conventional
approaches based on raw image features?
Our results are broken down into seen (Table 5)
and unseen (Table 6) concepts. The former are
known to the attribute classifiers and form part
of our database, whereas the latter are unknown
and are not included in McRae et al?s (2005)
norms. We report the correlation coefficients we
obtain when human-derived cue-associate proba-
bilities (Nelson et al, 1998) are compared against
the simple concatenation model (Concat), CCA,
and Andrews et al?s (2009) attribute-topic model
(TopicAttr). We also report the performance of
a distributional model that is based solely on the
output of our attribute classifiers, i.e., without any
textual input (VisAttr) and conversely the perfor-
mance of a model that uses textual information
only (i.e., Strudel attributes) without any visual in-
put (TextAttr). The results are displayed as a cor-
relation matrix so that inter-model correlations can
also be observed.
As can be seen in Table 5 (second column), two
modalities are in most cases better than one when
evaluating model performance on seen data. Dif-
ferences in correlation coefficients between mod-
els with two versus one modality are all statis-
tically significant (p < 0.01 using a t-test), with
the exception of Concat when compared against
VisAttr. It is also interesting to note that Topi-
cAttr is the least correlated model when compared
against other bimodal models or single modali-
Nelson Concat CCA TopicAttr TextAttr
Concat 0.11
CCA 0.15 0.66
TopicAttr 0.17 0.69 0.48
TextAttr 0.11 0.65 0.25 0.39
VisAttr 0.13 0.57 0.87 0.57 0.34
Table 6: Correlation matrix for unseen Nelson
et al (1998) cue-associate pairs and five distribu-
tional models. All correlation coefficients are sta-
tistically significant (p < 0.01, N = 1,716).
ties. This indicates that the latent space obtained
by this model is most distinct from its constituent
parts (i.e., visual and textual attributes). Perhaps
unsuprisingly Concat, CCA, VisAttr, and TextAttr
are also highly intercorrelated.
On unseen pairs (see Table 6), Concat fares
worse than CCA and TopicAttr, achieving simi-
lar performance to TextAttr. CCA and TopicAttr
are significantly better than TextAttr and VisAttr
(p < 0.01). This indicates that our attribute classi-
fiers generalize well beyond the concepts found in
our database and can produce useful visual infor-
mation even on unseen images. Compared to Con-
cat and CCA, TopicAttr obtains a better fit with the
human association norms on the unseen data.
To answer our third question, we obtained dis-
tributional models from McRae et al?s (2005)
norms and assessed how well they predict Nelson
et al?s (1998) word-associate similarities. Each
concept was represented as a vector with dimen-
sions corresponding to attributes generated by par-
ticipants of the norming study. Vector components
were set to the (normalized) frequency with which
participants generated the corresponding attribute
when presented with the concept. We measured
the similarity between two words using the co-
sine coefficient. Table 7 presents results for dif-
ferent model variants which we created by ma-
nipulating the number and type of attributes in-
volved. The first model uses the full set of at-
tributes present in the norms (All Attributes). The
second model (Text Attributes) uses all attributes
but those classified as visual (e.g., functional, en-
cyclopaedic). The third model (Visual Attributes)
considers solely visual attributes.
We observe a similar trend as with our compu-
tational models. Taking visual attributes into ac-
count increases the fit with Nelson?s (1998) associ-
ation norms, whereas visual and textual attributes
on their own perform worse. Interestingly, CCA?s
579
Models Seen
All Attributes 0.28
Text Attributes 0.20
Visual Attributes 0.25
Table 7: Model performance on seen Nelson et
al. (1998) cue-associate pairs; models are based
on gold human generated attributes (McRae et al,
2005). All correlation coefficients are statistically
significant (p < 0.01, N = 435).
Models Seen Unseen
Concat 0.22 0.10
CCA 0.26 0.15
TopicAttr 0.23 0.19
TextAttr 0.20 0.08
VisAttr 0.21 0.13
MixLDA 0.16 0.11
Table 8: Model performance on a subset of Nelson
et al (1998) cue-associate pairs. Seen are concepts
known to the attribute classifiers and covered by
MixLDA (N = 85). Unseen are concepts covered
by LDA but unknown to the attribute classifiers
(N = 388). All correlation coefficients are statisti-
cally significant (p < 0.05).
performance is comparable to the All Attributes
model (see Table 5, second column), despite us-
ing automatic attributes (both textual and visual).
Furthermore, visual attributes obtained through
our classifiers (see Table 5) achieve a marginally
lower correlation coefficient against human gener-
ated ones (see Table 7).
Finally, to address our last question, we com-
pared our approach against Feng and Lapata
(2010) who represent visual information via quan-
tized SIFT features. We trained their MixLDA
model on their corpus consisting of 3,361 BBC
news documents and corresponding images (Feng
and Lapata, 2008). We optimized the model pa-
rameters on a development set consisting of cue-
associate pairs from Nelson et al (1998), exclud-
ing the concepts in McRae et al (2005). We
used a vocabulary of approximately 6,000 words.
The best performing model on the development set
used 500 visual terms and 750 topics and the asso-
ciation measure proposed in Griffiths et al (2007).
The test set consisted of 85 seen and 388 unseen
cue-associate pairs that were covered by our mod-
els and MixLDA.
Table 8 reports correlation coefficients for our
models and MixLDA against human probabili-
ties. All attribute-based models significantly out-
perform MixLDA on seen pairs (p < 0.05 using a
t-test). MixLDA performs on a par with the con-
catenation model on unseen pairs, however CCA,
TopicAttr, and VisAttr are all superior. Although
these comparisons should be taken with a grain
of salt, given that MixLDA and our models are
trained on different corpora (MixLDA assumes
that texts and images are collocated, whereas our
images do not have collateral text), they seem to
indicate that attribute-based information is indeed
beneficial.
8 Conclusions
In this paper we proposed the use of automatically
computed visual attributes as a way of physically
grounding word meaning. Our results demonstrate
that visual attributes improve the performance of
distributional models across the board. On a
word association task, CCA and the attribute-topic
model give a better fit to human data when com-
pared against simple concatenation and models
based on a single modality. CCA consistently out-
performs the attribute-topic model on seen data (it
is in fact slightly better over a model that uses gold
standard human generated attributes), whereas the
attribute-topic model generalizes better on unseen
data (see Tables 5, 6, and 8). Since the attribute-
based representation is general and text-based we
argue that it can be conveniently integrated with
any type of distributional model or indeed other
grounded models that rely on low-level image fea-
tures (Bruni et al, 2012a; Feng and Lapata, 2010)
In the future, we would like to extend our
database to actions and show that this attribute-
centric representation is useful for more applied
tasks such as image description generation and ob-
ject recognition. Finally, we have only scratched
the surface in terms of possible models for inte-
grating the textual and visual modality. Interest-
ing frameworks which we plan to explore are deep
belief networks and Bayesian non-parametrics.
References
M. Andrews, G. Vigliocco, and D. Vinson. 2009.
Integrating Experiential and Distributional Data to
Learn Semantic Representations. Psychological Re-
view, 116(3):463?498.
M. Baroni, B. Murphy, E. Barbu, and M. Poesio.
2010. Strudel: A Corpus-Based Semantic Model
580
Based on Properties and Types. Cognitive Science,
34(2):222?254.
L. W. Barsalou. 2008. Grounded Cognition. Annual
Review of Psychology, 59:617?845.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet Allocation. Journal of Machine Learning
Research, 3:993?1022, March.
M. Borga. 2001. Canonical Correlation ? a Tutorial,
January.
M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.
Park, L. Pascual, M. G. Pe?cheux, J. Ruel, P. Venuti,
and A. Vyt. 2004. Cross-linguistic Analysis of
Vocabulary in Young Children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development, 75(4):1115?1139.
B. Bo?rschinger, B. K. Jones, and M. Johnson. 2011.
Reducing Grounded Learning Tasks to Grammatical
Inference. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, UK.
S.R.K. Branavan, H. Chen, L. S. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 82?90, Suntec, Singapore.
E. Bruni, G. Tran, and M. Baroni. 2011. Distributional
Semantics from Text and Images. In Proceedings of
the GEMS 2011 Workshop on GEometrical Models
of Natural Language Semantics, pages 22?32, Edin-
burgh, UK.
E. Bruni, G. Boleda, M. Baroni, and N. Tran. 2012a.
Distributional Semantics in Technicolor. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 136?145, Jeju Island, Korea.
E. Bruni, J. Uijlings, M. Baroni, and N. Sebe. 2012b.
Distributional semantics with eyes: Using im-
age analysis to improve computational representa-
tions of word meaning. In Proceedings of the
20th ACM International Conference on Multimedia,
pages 1219?1228., New York, NY.
C. Chai and C. Hung. 2008. Automatically Annotating
Images with Keywords: A Review of Image Annota-
tion Systems. Recent Patents on Computer Science,
1:55?68.
R. Datta, D. Joshi, J. Li, and J. Z. Wang. 2008. Image
Retrieval: Ideas, Influences, and Trends of the New
Age. ACM Computing Surveys, 40(2):1?60.
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchi-
cal Image Database. In Proceedings of the IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, pages 248?255, Miami,
Florida.
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2008. The
PASCAL Visual Object Classes Challenge
2008 (VOC2008) Results. http://www.pascal-
network.org/challenges/VOC/voc2008/workshop.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008. LIBLINEAR: A Library for Large Linear
Classification. Journal of Machine Learning Re-
search, 9:1871?1874.
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009.
Describing Objects by their Attributes. In Proceed-
ings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pages
1778?1785, Miami Beach, Florida.
L. Fei-Fei and P. Perona. 2005. A Bayesian Hierarchi-
cal Model for Learning Natural Scene Categories. In
Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition,
pages 524?531, San Diego, California.
C. Fellbaum, editor. 1998. WordNet: an Electronic
Lexical Database. MIT Press.
Y. Feng and M. Lapata. 2008. Automatic image anno-
tation using auxiliary text information. In Proceed-
ings of ACL-08: HLT, pages 272?280, Columbus,
Ohio.
Y. Feng and M. Lapata. 2010. Visual Informa-
tion in Semantic Representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99, Los
Angeles, California. ACL.
V. Ferrari and A. Zisserman. 2007. Learning Visual
Attributes. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 433?440. MIT Press,
Cambridge, Massachusetts.
G. H. Golub, F. T. Luk, and M. L. Overton. 1981.
A block lanczoz method for computing the singular
values and corresponding singular vectors of a ma-
trix. ACM Transactions on Mathematical Software,
7:149?169.
P. Gorniak and D. Roy. 2004. Grounded Semantic
Composition for Visual Scenes. Journal of Artificial
Intelligence Research, 21:429?470.
T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum.
2007. Topics in Semantic Representation. Psycho-
logical Review, 114(2):211?244.
D. R. Hardoon, S. R. Szedmak, and J. R. Shawe-
Taylor. 2004. Canonical Correlation Analysis: An
Overview with Application to Learning Methods.
Neural Computation, 16(12):2639?2664.
B. T. Johns and M. N. Jones. 2012. Perceptual Infer-
ence through Global Lexical Similarity. Topics in
Cognitive Science, 4(1):103?120.
D. Joshi, J.Z. Wang, and J. Li. 2006. The Story Pictur-
ing Engine?A System for Automatic Text illustra-
tion. ACM Transactions on Multimedia Computing,
Communications, and Applications, 2(1):68?89.
581
R. J. Kate and R. J. Mooney. 2007. Learning Lan-
guage Semantics from Ambiguous Supervision. In
Proceedings of the 22nd Conference on Artificial In-
telligence, pages 895?900, Vancouver, Canada.
N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Na-
yar. 2009. Attribute and Simile Classifiers for Face
Verification. In Proceedings of the IEEE 12th In-
ternational Conference on Computer Vision, pages
365?372, Kyoto, Japan.
C. H. Lampert, H. Nickisch, and S. Harmeling. 2009.
Learning To Detect Unseen Object Classes by
Between-Class Attribute Transfer. In Computer Vi-
sion and Pattern Recognition, pages 951?958, Mi-
ami Beach, Florida.
B. Landau, L. Smith, and S. Jones. 1998. Object Per-
ception and Object Naming in Early Development.
Trends in Cognitive Science, 27:19?24.
C. Leong and R. Mihalcea. 2011. Going Beyond
Text: A Hybrid Image-Text Approach for Measuring
Word Relatedness. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 1403?1407, Chiang Mai, Thailand.
J. Liu, B. Kuipers, and S. Savarese. 2011. Recognizing
Human Actions by Attributes. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 3337?3344, Colorado Springs,
Colorado.
D. G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. In Proceedings of the In-
ternational Conference on Computer Vision, pages
1150?1157, Corfu, Greece.
D. Lowe. 2004. Distinctive Image Features from
Scale-invariant Keypoints. International Journal of
Computer Vision, 60(2):91?110.
W. Lu, H. T. Ng, W.S. Lee, and L. S. Zettlemoyer.
2008. A Generative Model for Parsing Natural Lan-
guage to Meaning Representations. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 783?792, Hon-
olulu, Hawaii.
K. McRae, G. S. Cree, M. S. Seidenberg, and C. Mc-
Norgan. 2005. Semantic Feature Production Norms
for a Large Set of Living and Nonliving Things. Be-
havior Research Methods, 37(4):547?559.
D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998.
The University of South Florida Word Association,
Rhyme, and Word Fragment Norms.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and
A. Y. Ng. 2011. Multimodal deep learning. In
Proceedings of the 28th International Conference on
Machine Leanring, pages 689?696, Bellevue, Wash-
ington.
A. Oliva and A. Torralba. 2007. The Role of Context in
Object Recognition. Trends in Cognitive Sciences,
11(12):520?527.
D. N. Osherson, J. Stern, O. Wilkie, M. Stob, and E. E.
Smith. 1991. Default Probability. Cognitive Sci-
ence, 2(15):251?269.
G. Patterson and J. Hays. 2012. SUN Attribute
Database: Discovering, Annotating and Recogniz-
ing Scene Attributes. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 2751?2758, Providence, Rhode Island.
T. Regier. 1996. The Human Semantic Potential. MIT
Press, Cambridge, Massachusetts.
D. Roy and A. Pentland. 2002. Learning Words from
Sights and Sounds: A Computational Model. Cog-
nitive Science, 26(1):113?146.
C. Silberer and M. Lapata. 2012. Grounded Mod-
els of Semantic Representation. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433, Jeju
Island, Korea.
J. M. Siskind. 2001. Grounding the Lexical Semantics
of Verbs in Visual Perception using Force Dynamics
and Event Logic. Journal of Artificial Intelligence
Research, 15:31?90.
S. A. Sloman and L. J. Ripps. 1998. Similarity as an
Explanatory Construct. Cognition, 65:87?101.
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In Pro-
ceedings of the 26th Annual Conference on Neural
Information Processing Systems, pages 2231?2239,
Lake Tahoe, Nevada.
M. Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234?342.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter,
A. Gopal Banerjee, S. Teller, and N. Roy. 2011.
Understanding Natural Language Commands for
Robotic Navigation and Manipulation. In Proceed-
ings of the 25th National Conference on Artificial
Intelligence, pages 1507?1514, San Francisco, Cali-
fornia.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceeings of the Human
Factors in Computing Systems Conference, pages
319?326, Vienna, Austria.
C. Yu and D. H. Ballard. 2007. A Unified Model of
Early Word Learning Integrating Statistical and So-
cial Cues. Neurocomputing, 70:2149?2165.
M. D. Zeigenfuse and M. D. Lee. 2010. Finding the
Features that Represent Stimuli. Acta Psychologi-
cal, 133(3):283?295.
J. M. Zelle and R. J. Mooney. 1996. Learning to Parse
Database Queries Using Inductive Logic Program-
ming. In Proceedings of the 13th National Con-
ference on Artificial Intelligence, pages 1050?1055,
Portland, Oregon.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
Map Sentences to Logical Form: Structured Classi-
fication with Probabilistic Categorial Grammars. In
Proceedings of the Conference on Uncertainty in Ar-
tificial Intelligence, pages 658?666, Edinburgh, UK.
582
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721?732,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Grounded Meaning Representations with Autoencoders
Carina Silberer and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
c.silberer@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we address the problem of
grounding distributional representations of
lexical meaning. We introduce a new
model which uses stacked autoencoders to
learn higher-level embeddings from tex-
tual and visual input. The two modali-
ties are encoded as vectors of attributes
and are obtained automatically from text
and images, respectively. We evaluate our
model on its ability to simulate similar-
ity judgments and concept categorization.
On both tasks, our approach outperforms
baselines and related models.
1 Introduction
Recent years have seen a surge of interest in sin-
gle word vector spaces (Turney and Pantel, 2010;
Collobert et al, 2011; Mikolov et al, 2013) and
their successful use in many natural language ap-
plications. Examples include information retrieval
(Manning et al, 2008), search query expansions
(Jones et al, 2006), document classification (Se-
bastiani, 2002), and question answering (Yih et al,
2013). Vector spaces have been also popular in
cognitive science figuring prominently in simula-
tions of human behavior involving semantic prim-
ing, deep dyslexia, text comprehension, synonym
selection, and similarity judgments (see Griffiths
et al, 2007). In general, these models specify
mechanisms for constructing semantic representa-
tions from text corpora based on the distributional
hypothesis (Harris, 1970): words that appear in
similar linguistic contexts are likely to have related
meanings.
Word meaning, however, is also tied to the
physical world. Words are grounded in the exter-
nal environment and relate to sensorimotor experi-
ence (Regier, 1996; Landau et al, 1998; Barsalou,
2008). To account for this, new types of perceptu-
ally grounded distributional models have emerged.
These models learn the meaning of words based
on textual and perceptual input. The latter is ap-
proximated by feature norms elicited from humans
(Andrews et al, 2009; Steyvers, 2010; Silberer
and Lapata, 2012), visual information extracted
automatically from images, (Feng and Lapata,
2010; Bruni et al, 2012a; Silberer et al, 2013)
or a combination of both (Roller and Schulte im
Walde, 2013). Despite differences in formulation,
most existing models conceptualize the problem
of meaning representation as one of learning from
multiple views corresponding to different modali-
ties. These models still represent words as vectors
resulting from the combination of representations
with different statistical properties that do not nec-
essarily have a natural correspondence (e.g., text
and images).
In this work, we introduce a model, illus-
trated in Figure 1, which learns grounded mean-
ing representations by mapping words and im-
ages into a common embedding space. Our model
uses stacked autoencoders (Bengio et al, 2007)
to induce semantic representations integrating vi-
sual and textual information. The literature de-
scribes several successful approaches to multi-
modal learning using different variants of deep
networks (Ngiam et al, 2011; Srivastava and
Salakhutdinov, 2012) and data sources including
text, images, audio, and video. Unlike most pre-
vious work, our model is defined at a finer level
of granularity ? it computes meaning representa-
tions for individual words and is unique in its use
of attributes as a means of representing the textual
and visual modalities. We follow Silberer et al
(2013) in arguing that an attribute-centric repre-
sentation is expedient for several reasons.
Firstly, attributes provide a natural way of ex-
pressing salient properties of word meaning as
demonstrated in norming studies (e.g., McRae
et al, 2005) where humans often employ attributes
when asked to describe a concept. Secondly, from
721
a modeling perspective, attributes allow for eas-
ier integration of different modalities, since these
are rendered in the same medium, namely, lan-
guage. Thirdly, attributes are well-suited to de-
scribing visual phenomena (e.g., objects, scenes,
actions). They allow to generalize to new in-
stances for which there are no training exam-
ples available and to transcend category and task
boundaries whilst offering a generic description of
visual data (Farhadi et al, 2009).
Our model learns multimodal representations
from attributes which are automatically inferred
from text and images. We evaluate the embed-
dings it produces on two tasks, namely word sim-
ilarity and categorization. In the first task, model
estimates of word similarity (e.g., gem?jewel are
similar but glass?magician are not) are compared
against elicited similarity ratings. We performed
a large-scale evaluation on a new dataset consist-
ing of human similarity judgments for 7,576 word
pairs. Unlike previous efforts such as the widely
used WordSim353 collection (Finkelstein et al,
2002), our dataset contains ratings for visual and
textual similarity, thus allowing to study the two
modalities (and their contribution to meaning rep-
resentation) together and in isolation. We also
assess whether the learnt representations are ap-
propriate for categorization, i.e., grouping a set
of objects into meaningful semantic categories
(e.g., peach and apple are members of FRUIT,
whereas chair and table are FURNITURE). On both
tasks, our model outperforms baselines and related
models.
2 Related Work
The presented model has connections to several
lines of work in NLP, computer vision research,
and more generally multimodal learning. We re-
view related work in these areas below.
Grounded Semantic Spaces Grounded seman-
tic spaces are essentially distributional models
augmented with perceptual information. A model
akin to Latent Semantic Analysis (Landauer and
Dumais, 1997) is proposed in Bruni et al (2012b)
who concatenate two independently constructed
textual and visual spaces and subsequently project
them onto a lower-dimensional space using Singu-
lar Value Decomposition.
Several other models have been extensions of
Latent Dirichlet Allocation (Blei et al, 2003)
where topic distributions are learned from words
and other perceptual units. Feng and Lapata
(2010) use visual words which they extract from a
corpus of multimodal documents (i.e., BBC news
articles and their associated images), whereas oth-
ers (Steyvers, 2010; Andrews et al, 2009; Silberer
and Lapata, 2012) use feature norms obtained in
longitudinal elicitation studies (see McRae et al
(2005) for an example) as an approximation of the
visual environment. More recently, topic mod-
els which combine both feature norms and vi-
sual words have also been introduced (Roller and
Schulte im Walde, 2013). Drawing inspiration
from the successful application of attribute clas-
sifiers in object recognition, Silberer et al (2013)
show that automatically predicted visual attributes
act as substitutes for feature norms without any
critical information loss.
The visual and textual modalities on which our
model is trained are decoupled in that they are not
derived from the same corpus (we would expect
co-occurring images and text to correlate to some
extent) but unified in their representation by natu-
ral language attributes. The use of stacked autoen-
coders to extract a shared lexical meaning repre-
sentation is new to our knowledge, although, as
we explain below related to a large body of work
on deep learning.
Multimodal Deep Learning Our work employs
deep learning (a.k.a deep networks) to project lin-
guistic and visual information onto a unified rep-
resentation that fuses the two modalities together.
The goal of deep learning is to learn multiple lev-
els of representations through a hierarchy of net-
work architectures, where higher-level representa-
tions are expected to help define higher-level con-
cepts.
A large body of work has focused on projecting
words and images into a common space using a va-
riety of deep learning methods ranging from deep
and restricted Boltzman machines (Srivastava and
Salakhutdinov, 2012; Feng et al, 2013), to au-
toencoders (Wu et al, 2013), and recursive neural
networks (Socher et al, 2013b). Similar methods
have been employed to combine other modalities
such as speech and video (Ngiam et al, 2011) or
images (Huang and Kingsbury, 2013). Although
our model is conceptually similar to these studies
(especially those applying stacked autoencoders),
it differs considerably from them in at least two
aspects. Firstly, most of these approaches aim to
learn a shared representation between modalities
722
so as to infer some missing modality from others
(e.g., to infer text from images and vice versa); in
contrast, we aim to learn an optimal representa-
tion for each modality and their optimal combi-
nation. Secondly, our problem setting is different
from the former studies, which usually deal with
classification tasks and fine-tune the deep neural
networks using training data with explicit class la-
bels; in contrast we fine-tune our autoencoders us-
ing a semi-supervised criterion. That is, we use
indirect supervision in the form of object classifi-
cation in addition to the objective of reconstruct-
ing the attribute-centric input representation.
3 Autoencoders for Grounded Semantics
3.1 Background
Our model learns higher-level meaning represen-
tations for single words from textual and visual
input in a joint fashion. We first briefly review
autoencoders in Section 3.1 with emphasis on as-
pects relevant to our model which we then de-
scribe in Section 3.2.
Autoencoders An autoencoder is an unsuper-
vised neural network which is trained to recon-
struct a given input from its latent representation
(Bengio, 2009). It consists of an encoder f
?
which
maps an input vector x
(i)
to a latent representa-
tion y
(i)
= f
?
(x
(i)
) = s(Wx
(i)
+ b), with s being
a non-linear activation function, such as a sig-
moid function. A decoder g
?
?
then aims to recon-
struct input x
(i)
from y
(i)
, i.e., ?x
(i)
= g
?
?
(y
(i)
) =
s(W
?
y
(i)
+ b
?
). The training objective is the de-
termination of parameters
?
? = {W,b} and
?
?
?
=
{W
?
,b
?
} that minimize the average reconstruction
error over a set of input vectors {x
(1)
, ...,x
(n)
}:
?
?,
?
?
?
= argmin
?,?
?
1
n
n
?
i=1
L(x
(i)
,g
?
?
( f
?
(x
(i)
))), (1)
where L is a loss function, such as cross-entropy.
Parameters ? and ?
?
can be optimized by gradient
descent methods.
Autoencoders are a means to learn representa-
tions of some input by retaining useful features in
the encoding phase which help to reconstruct the
input, whilst discarding useless or noisy ones. To
this end, different strategies have been employed
to guide parameter learning and constrain the hid-
den representation. Examples include imposing
a bottleneck to produce an under-complete rep-
resentation of the input, using sparse representa-
tions, or denoising.
Denoising Autoencoders The training criterion
with denoising autoencoders is the reconstruction
of clean input x
(i)
given a corrupted version ?x
(i)
(Vincent et al, 2010). The underlying idea is that
the learned latent representation is good if the au-
toencoder is capable of reconstructing the actual
input from its corruption. The reconstruction error
for an input x
(i)
with loss function L then is:
L(x
(i)
,g
?
?
( f
?
(?x
(i)
))) (2)
One possible corruption process is masking noise,
where the corrupted version ?x
(i)
results from ran-
domly setting a fraction v of x
(i)
to 0.
Stacked Autoencoders Several (denoising) au-
toencoders can be used as building blocks to form
a deep neural network (Bengio et al, 2007; Vin-
cent et al, 2010). For that purpose, the autoen-
coders are pre-trained layer by layer, with the cur-
rent layer being fed the latent representation of the
previous autoencoder as input. Using this unsuper-
vised pre-training procedure, initial parameters are
found which approximate a good solution. Subse-
quently, the original input layer and hidden repre-
sentations of all the autoencoders are stacked and
all network parameters are fine-tuned with back-
propagation.
To further optimize the parameters of the net-
work, a supervised criterion can be imposed on top
of the last hidden layer such as the minimization
of a prediction error on a supervised task (Bengio,
2009). Another approach is to unfold the stacked
autoencoders and fine-tune them with respect to
the minimization of the global reconstruction error
(Hinton and Salakhutdinov, 2006). Alternatively,
a semi-supervised criterion can be used (Ranzato
and Szummer, 2008; Socher et al, 2011) through
combination of the unsupervised training criterion
(global reconstruction) with a supervised criterion
(prediction of some target given the latent repre-
sentation).
3.2 Semantic Representations
To learn meaning representations of single words
from textual and visual input, we employ stacked
(denoising) autoencoders (SAEs). Both input
modalities are vector-based representations of
words, or, more precisely, the objects they refer to
(e.g., canary, trolley). The vector dimensions cor-
respond to textual and visual attributes, examples
of which are shown in Table 1. We explain how
these representations are obtained in more detail
723
...
...
...
input x
TEXT
W
(1)
W
(3)
...
...
...
IMAGES
W
(2)
W
(4)
...
bimodal coding ?y
W
(5
?
)
W
(5)
...
softmax
?
t
W
(6)
...
...
W
(3
?
)
...
...
W
(4
?
)
...
reconstruction
?
x
W
(1
?
)
...
W
(2
?
)
Figure 1: Stacked autoencoder trained with semi-supervised objective. Input to the model are single-
word vector representations obtained from text and images. Vector dimensions correspond to textual and
visual attributes, respectively (see Table 1).
in Section 4.1. We first train SAEs with two hid-
den layers (codings) for each modality separately.
Then, we join these two SAEs by feeding their re-
spective second coding simultaneously to another
autoencoder, whose hidden layer thus yields the
fused meaning representation. Finally, we stack
all layers and unfold them in order to fine-tune
the SAE. Figure 1 illustrates the model.
Unimodal Autoencoders For both modalities,
we use the hyperbolic tangent function as activa-
tion function for encoder f
?
and decoder g
?
?
and an
entropic loss function for L. The weights of each
autoencoder are tied, i.e., W
?
= W
T
. We employ
denoising autoencoders (DAEs) for pre-training
the textual modality. Regarding the visual autoen-
coder, we derive a new (?denoised?) target vector
to be reconstructed for each input vector x
(i)
, and
treat x
(i)
itself as corrupted input. The unimodal
autoencoder is thus trained to denoise a given in-
put. The target vector is derived as follows: each
object o in our data is represented by multiple im-
ages, and each image is in turn represented by a
visual attribute vector x
(i)
. The target vector is the
sum of x
(i)
and the centroid x
(j)
of the remaining
attribute vectors representing object o.
Bimodal Autoencoder The bimodal autoen-
coder is fed with the concatenated final hidden
codings of the visual and textual modalities as in-
put and maps these inputs to a joint hidden layer ?y
with B units. We normalize both unimodal input
codings to unit length. Again, we use tied weights
for the bimodal autoencoder. We also encourage
the autoencoder to detect dependencies between
the two modalities while learning the mapping
to the bimodal hidden layer. We therefore apply
masking noise to one modality with a masking fac-
tor v (see Section 3.1), so that the corrupted modal-
ity optimally has to rely on the other modality in
order to reconstruct its missing input features.
Stacked Bimodal Autoencoder We finally
build a stacked bimodal autoencoder (SAE) with
all pre-trained layers and fine-tune them with re-
spect to a semi-supervised criterion. That is, we
unfold the stacked autoencoder and furthermore
add a softmax output layer on top of the bimodal
layer ?y that outputs predictions
?
t with respect to
the inputs? object labels (e.g., boat):
?
t
(i)
=
exp(W
(6)
?y
(i)
+b
(6)
)
?
O
k=1
exp(W
(6)
k.
?y
(i)
+b
(6)
k
)
, (3)
with weights W
(6)
? R
O?B
, b
(6)
? R
O?1
, where O
is the number of unique object labels. The over-
all objective to be minimized is therefore the
weighted sum of the reconstruction error L
r
and
the classification error L
c
:
L =
1
n
n
?
i=1
(
?
r
L
r
(x
(i)
, ?x
(i)
)+?
c
L
c
(t
(i)
,
?
t
(i)
)
)
+?R (4)
where ?
r
and ?
c
are weighting parameters that
give different importance to the partial objectives,
724
eats seeds has beak has claws has handlebar has wheels has wings is yellow made of wood
canary 0.05 0.24 0.15 0.00 ?0.10 0.19 0.34 0.00
V
i
s
u
a
l
trolley 0.00 0.00 0.00 0.30 0.32 0.00 0.00 0.25
bird:n breed:v cage:n chirp:v fly:v track:n ride:v run:v rail:n wheel:n
canary 0.16 0.19 0.39 0.13 0.13 0.00 0.00 0.00 0.00 ?0.05
T
e
x
t
u
a
l
trolley ?0.40 0.00 0.00 0.00 0.00 0.14 0.16 0.33 0.17 0.20
Table 1: Examples of attribute-based representations provided as input to our autoencoders.
L
c
and L
r
are entropic loss functions, and R is
a regularization term with R =
?
5
j=1
2||W
(j)
||
2
+
||W
(6)
||
2
. Finally,
?
t
(i)
is the object label vector pre-
dicted by the softmax layer for input vector x
(i)
,
and t
(i)
is the correct object label, represented as a
O-dimensional one-hot vector
1
.
The additional supervised criterion drives the
learning towards a representation capable of dis-
criminating between different objects. Further-
more, the semi-supervised setting affords flexibil-
ity, allowing to adapt the architecture to specific
tasks. For example, by setting the corruption pa-
rameter v for the textual modality to one and ?
r
to zero, a standard object classification model for
images can be trained. Setting v close to one for ei-
ther modality enables the model to infer the other
(missing) modality. As our input consists of nat-
ural language attributes, the model would infer
textual attributes given visual attributes and vice
versa.
4 Experimental Setup
In this section we present our experimental setup
for assessing the performance of our model. We
give details on the tasks and datasets used for eval-
uation, we explain how the textual and visual in-
puts were constructed, how the SAE model was
trained, and describe the approaches used for com-
parison with our own work.
4.1 Data
We learn meaning representations for the nouns
contained in McRae et al?s (2005) feature norms.
These are 541 concrete animate and inanimate ob-
jects (e.g., animals, clothing, vehicles, utensils,
fruits, and vegetables). The norms were elicited
by asking participants to list properties (e.g., barks,
an animal, has legs) describing the nouns they were
presented with.
1
In a one-hot vector, the element corresponding to the ob-
ject label is one and the others are zero.
As shown in Figure 1, our model takes as in-
put two (real-valued) vectors representing the vi-
sual and textual modalities. Vector dimensions
correspond to textual and visual attributes, respec-
tively. Textual attributes were extracted by run-
ning Strudel (Baroni et al, 2010) on a 2009 dump
of the English Wikipedia.
2
Strudel is a fully
automatic method for extracting weighted word-
attribute pairs (e.g., bat?species:n, bat?bite:v) from
a lemmatized and POS-tagged corpus. Weights
are log-likelihood ratio scores expressing how
strongly an attribute and a word are associated. We
only retained the ten highest scored attributes for
each target word. This returned a total of 2,362
dimensions for the textual vectors. Association
scores were scaled to the [?1,1] range.
To obtain visual vectors, we followed the
methodology put forward in Silberer et al (2013).
Specifically, we used an updated version of their
dataset to train SVM-based attribute classifiers
that predict visual attributes for images (Farhadi
et al, 2009). The dataset is a taxonomy of 636 vi-
sual attributes (e.g., has wings, made of wood) and
nearly 700K images from ImageNet (Deng et al,
2009) describing more than 500 of McRae et al?s
(2005) nouns. The classifiers perform reason-
ably well with an interpolated average precision
of 0.52. We only considered attributes assigned
to at least two nouns in the dataset, obtaining a
414 dimensional vector for each noun. Analo-
gously to the textual representations, visual vec-
tors were scaled to the [?1,1] range.
We follow Silberer et al?s (2013) partition of the
dataset into training, validation, and test set and
acquire visual vectors for each of the sets. We use
the visual vectors of the training and development
set for training the autoencoders, and the vectors
for the test set for evaluation.
2
The corpus is downloadable from http://wacky.
sslmit.unibo.it/doku.php?id=corpora.
725
4.2 Model Architecture
Model parameters were optimized on a subset of
the word association norms collected by Nelson
et al (1998).
3
These were established by present-
ing participants with a cue word (e.g., canary) and
asking them to name an associate word in response
(e.g., bird, sing, yellow). For each cue, the norms
provide a set of associates and the frequencies
with which they were named. The dataset con-
tains a very large number of cue-associate pairs
(63,619 in total) some of which luckily are cov-
ered in McRae et al (2005).
4
During training
we used correlation analysis (Spearman?s ?) to
monitor the degree of linear relationship between
model cue-associate (cosine) similarities and hu-
man probabilities.
The best autoencoder on the word association
task obtained a correlation coefficient of 0.33.
This performance is superior to the results re-
ported in Silberer et al (2013) (their correlation
coefficients range from 0.16 to 0.28). This model
has the following architecture: the textual autoen-
coder (see Figure 1, left-hand side) consists of 700
hidden units which are then mapped to the sec-
ond hidden layer with 500 units (the corruption
parameter was set to v = 0.1); the visual autoen-
coder (see Figure 1, right-hand side) has 170 and
100 hidden units, in the first and second layer, re-
spectively. The 500 textual and 100 visual hidden
units were fed to a bimodal autoencoder contain-
ing 500 latent units, and masking noise was ap-
plied to the textual modality with v = 0.2. The
weighting parameters for the joint training objec-
tive of the stacked autoencoder were set to ?
r
= 0.8
and ?
c
= 1 (see Equation (4)).
We used the model described above and the
meaning representations obtained from the out-
put of the bimodal latent layer for all the eval-
uation tasks detailed below. Some performance
gains could be expected if parameter optimization
took place separately for each task. However, we
wanted to avoid overfitting, and show that our pa-
rameters are robust across tasks and datasets.
4.3 Evaluation Tasks
Word Similarity We first evaluated how well
our model predicts word similarity ratings. Al-
though several relevant datasets exist, such as
3
http://w3.usf.edu/Freeassociation.
4
435 word pairs constitute the overlap between Nelson et
al.?s norms (1998) and McRae et al?s (2005) nouns.
the widely used WordSim353 (Finkelstein et al,
2002) or the more recent Rel-122 norms (Szum-
lanski et al, 2013), they contain many abstract
words, (e.g., love?sex or arrest?detention) which
are not covered in McRae et al (2005). This is for
a good reason, as most abstract words do not have
discernible attributes, or at least attributes that par-
ticipants would agree upon. We thus created a
new dataset consisting exclusively of McRae et al
(2005) nouns which we hope will be useful for the
development and evaluation of grounded semantic
space models.
5
Initially, we created all possible pairings over
McRae et al?s (2005) nouns and computed their
semantic relatedness using Patwardhan and Peder-
sen (2006)?s WordNet-based measure. We opted
for this specific measure as it achieves high corre-
lation with human ratings and has a high coverage
on our nouns. Next, for each word we randomly
selected 30 pairs under the assumption that they
are representative of the full variation of semantic
similarity. This resulted in 7,576 word pairs for
which we obtained similarity ratings using Ama-
zon Mechanical Turk (AMT). Participants were
asked to rate a pair on two dimensions, visual
and semantic similarity using a Likert scale of 1
(highly dissimilar) to 5 (highly similar). Each task
consisted of 32 pairs covering examples of weak
to very strong semantic relatedness. Two con-
trol pairs from Miller and Charles (1991) were in-
cluded in each task to potentially help identify and
eliminate data from participants who assigned ran-
dom scores. Examples of the stimuli and mean
ratings are shown in Table 2.
The elicitation study comprised overall 255
tasks, each task was completed by five volun-
teers. The similarity data was post-processed so
as to identify and remove outliers. We consid-
ered an outlier to be any individual whose mean
pairwise correlation fell outside two standard de-
viations from the mean correlation. 11.5% of
the annotations were detected as outliers and re-
moved. After outlier removal, we further ex-
amined how well the participants agreed in their
similarity judgments. We measured inter-subject
agreement as the average pairwise correlation co-
efficient (Spearman?s ?) between the ratings of all
annotators for each task. For semantic similarity,
the mean correlation was 0.76 (Min =0.34, Max
5
Available from http://homepages.inf.ed.ac.uk/
mlap/index.php?page=resources.
726
Word Pairs Semantic Visual
football?pillow 1.0 1.2
dagger?pencil 1.0 2.2
motorcycle?wheel 2.4 1.8
orange?pumpkin 2.5 3.0
cherry?pineapple 3.6 1.2
pickle?zucchini 3.6 4.0
canary?owl 4.0 2.4
jeans?sweater 4.5 2.2
pan?pot 4.7 4.0
hornet?wasp 4.8 4.8
airplane?jet 5.0 5.0
Table 2: Mean semantic and visual similarity rat-
ings for the McRae et al (2005) nouns using a
scale of 1 (highly dissimilar) to 5 (highly similar).
=0.97, StD =0.11) and for visual similarity 0.63
(Min =0.19, Max =0.90, SD =0.14). These re-
sults indicate that the participants found the task
relatively straightforward and produced similarity
ratings with a reasonable level of consistency. For
comparison, Patwardhan and Pedersen?s (2006)
measure achieved a coefficient of 0.56 on the
dataset for semantic similarity and 0.48 for vi-
sual similarity. The correlation between the aver-
age ratings of the AMT annotators and the Miller
and Charles (1991) dataset was ? = 0.91. In our
experiments (see Section 5), we correlate model-
based cosine similarities with mean similarity rat-
ings (again using Spearman?s ?).
Categorization The task of categorization
(i.e., grouping objects into meaningful categories)
is a classic problem in the field of cognitive
science, central to perception, learning, and the
use of language. We evaluated model output
against a gold standard set of categories created
by Fountain and Lapata (2010). The dataset
contains a classification, produced by human
participants, of McRae et al?s (2005) nouns into
(possibly multiple) semantic categories (40 in
total).
6
To obtain a clustering of nouns, we used Chi-
nese Whispers (Biemann, 2006), a randomized
graph-clustering algorithm. In the categorization
setting, Chinese Whispers (CW) produces a hard
clustering over a weighted graph whose nodes cor-
6
The dataset can be downloaded from http:
//homepages.inf.ed.ac.uk/s0897549/data/.
respond to words and edges to cosine similarity
scores between vectors representing their mean-
ing. CW is a non-parametric model, it induces the
number of clusters (i.e., categories) from the data
as well as which nouns belong to these clusters.
In our experiments, we initialized Chinese Whis-
pers with different graphs resulting from different
vector-based representations of the McRae et al
(2005) nouns. We also transformed the dataset
into hard categorizations by assigning each noun
to its most typical category as extrapolated from
human typicality ratings (for details see Foun-
tain and Lapata, 2010). CW can optionally ap-
ply a minimum weight threshold which we opti-
mized using the categorization dataset from Ba-
roni et al (2010). The latter contains a classifica-
tion of 82 McRae et al (2005) nouns into 10 cate-
gories. These nouns were excluded from the gold
standard (Fountain and Lapata, 2010) in our final
evaluation.
We evaluated the clusters produced by CW us-
ing the F-score measure introduced in the Se-
mEval 2007 task (Agirre and Soroa, 2007); it is
the harmonic mean of precision and recall defined
as the number of correct members of a cluster di-
vided by the number of items in the cluster and
the number of items in the gold-standard class, re-
spectively.
4.4 Comparison with Other Models
Throughout our experiments we compare a bi-
modal stacked autoencoder against unimodal au-
toencoders based solely on textual and visual in-
put (left- and right-hand sides in Figure 1, respec-
tively). We also compare our model against two
approaches that differ in their fusion mechanisms.
The first one is based on kernelized canonical cor-
relation (kCCA, Hardoon et al, 2004) with a lin-
ear kernel which was the best performing model
in Silberer et al (2013). The second one emulates
Bruni et al?s (2014) fusion mechanism. Specifi-
cally, we concatenate the textual and visual vec-
tors and project them onto a lower dimensional la-
tent space using SVD (Golub and Reinsch, 1970).
All these models run on the same datasets/items
and are given input identical to our model, namely
attribute-based textual and visual representations.
We furthermore report results obtained with
Bruni et al?s (2014) bimodal distributional model,
which employs SVD to integrate co-occurrence-
based textual representations with visual repre-
727
Semantic Visual
Models T V T+V T V T+V
McRae 0.71 0.49 0.68 0.58 0.52 0.62
Attributes 0.58 0.61 0.68 0.46 0.56 0.58
SAE 0.65 0.60 0.70 0.52 0.60 0.64
SVD ? ? 0.67 ? ? 0.57
kCCA ? ? 0.57 ? ? 0.55
Bruni ? ? 0.52 ? ? 0.46
RNN-640 0.41 ? ? 0.34 ? ?
Table 3: Correlation of model predictions against
similarity ratings for McRae et al (2005) noun
pairs (using Spearman?s ?).
sentations constructed from low-level image fea-
tures. In their model, the textual modality is
represented by the 30K-dimensional vectors ex-
tracted from UKWaC and WaCkypedia.
7
The
visual modality is represented by bag-of-visual-
words histograms built on the basis of clustered
SIFT features (Lowe, 2004). We rebuilt their
model on the ESP image dataset (von Ahn and
Dabbish, 2004) using Bruni et al?s (2013) publicly
available system.
Finally, we also compare to the word embed-
dings obtained using Mikolov et al?s (2011) re-
current neural network based language model.
These were pre-trained on Broadcast news data
(400M words) using the word2vec tool.
8
We re-
port results with the 640-dimensional embeddings
as they performed best.
5 Results
Table 3 presents our results on the word simi-
larity task. We report correlation coefficients of
model predictions against similarity ratings. As an
indicator to how well automatically extracted at-
tributes can approach the performance of clean hu-
man generated attributes, we also report results of
a distributional model induced from McRae et al?s
(2005) norms (see the row labeled McRae in the
table). Each noun is represented as a vector with
dimensions corresponding to attributes elicited by
participants of the norming study. Vector compo-
nents are set to the (normalized) frequency with
which participants generated the corresponding at-
tribute. We show results for three models, using all
attributes except those classified as visual (T), only
7
We thank Elia Bruni for providing us with their data.
8
Available from http://www.rnnlm.org/.
# Pair # Pair
1 pliers?tongs 11 cello?violin
2 cathedral?church 12 cottage?house
3 cathedral?chapel 13 horse?pony
4 pistol?revolver 14 gun?rifle
5 chapel?church 15 cedar?oak
6 airplane?helicopter 16 bull?ox
7 dagger?sword 17 dress?gown
8 pistol?rifle 18 bolts?screws
9 cloak?robe 19 salmon?trout
10 nylons?trousers 20 oven?stove
Table 4: Word pairs with highest semantic and vi-
sual similarity according to SAE model. Pairs are
ranked from highest to lowest similarity.
visual attributes (V), and all available attributes
(V+T).
9
As baselines, we also report the perfor-
mance of a model based solely on textual attributes
(which we obtain from Strudel), visual attributes
(obtained from our classifiers), and their concate-
nation (see row Attributes in Table 3, and columns
T, V, and T+V, respectively). The automatically
obtained textual and visual attribute vectors serve
as input to SVD, kCCA, and our stacked autoen-
coder (SAE). The third row in the table presents
three variants of our model trained on textual and
visual attributes only (T and V, respectively) and
on both modalities jointly (T+V).
Recall that participants were asked to provide
ratings on two dimensions, namely semantic and
visual similarity. We would expect the textual
modality to be more dominant when modeling se-
mantic similarity and conversely the perceptual
modality to be stronger with respect to visual sim-
ilarity. This is borne out in our unimodal SAEs.
The textual SAE correlates better with seman-
tic similarity judgments (? = 0.65) than its vi-
sual equivalent (? = 0.60). And the visual SAE
correlates better with visual similarity judgments
(? = 0.60) compared to the textual SAE (? = 0.52).
Interestingly, the bimodal SAE is better than the
unimodal variants on both types of similarity judg-
ments, semantic and visual. This suggests that
both modalities contribute complementary infor-
mation and that the SAE model is able to extract
a shared representation which improves general-
ization performance across tasks by learning them
9
Classification of attributes into categories is provided by
McRae et al (2005) in their dataset.
728
Models T V T+V
McRae 0.52 0.31 0.42
Attributes 0.35 0.37 0.33
SAE 0.36 0.35 0.43
SVD ? ? 0.39
kCCA ? ? 0.37
Bruni ? ? 0.34
RNN-640 0.32 ? ?
Table 5: F-score results on concept categorization.
jointly. The bimodal autoencoder (SAE, T+V)
outperforms all other bimodal models on both sim-
ilarity tasks. It yields a correlation coefficient
of ? = 0.70 on semantic similarity and ? = 0.64 on
visual similarity. Human agreement on the former
task is 0.76 and 0.63 on the latter. Table 4 shows
examples of word pairs with highest semantic and
visual similarity according to the SAE model.
We also observe that simply concatenating
textual and visual attributes (Attributes, T+V)
performs competitively with SVD and better
than kCCA. This indicates that the attribute-based
representation is a powerful predictor on its own.
Interestingly, both Bruni et al (2013) and Mikolov
et al (2011) which do not make use of attributes
are out-performed by all other attribute-based sys-
tems (see columns T and T+V in Table 3).
Our results on the categorization task are given
in Table 5. In this task, simple concatenation of vi-
sual and textual attributes does not yield improved
performance over the individual modalities (see
row Attributes in Table 5). In contrast, all bimodal
models (SVD, kCCA, and SAE) are better than
their unimodal equivalents and RNN-640. The
SAE outperforms both kCCA and SVD by a large
margin delivering clustering performance similar
to the McRae et al?s (2005) norms. Table 6 shows
examples of clusters produced by Chinese Whis-
pers when using vector representations provided
by the SAE model.
In sum, our experiments show that the bi-
modal SAE model delivers superior performance
across the board when compared against competi-
tive baselines and related models. It is interesting
to note that the unimodal SAEs are in most cases
better than the raw textual or visual attributes.
This indicates that higher level embeddings may
be beneficial to NLP tasks in general, not only to
those requiring multimodal information.
STICK-LIKE UTENSILS baton, ladle, peg, spatula,
spoon
RELIGIOUS BUILDINGS cathedral, chapel, church
WIND INSTRUMENTS clarinet, flute, saxophone, trom-
bone, trumpet, tuba
AXES axe, hatchet, machete, toma-
hawk
FURNITURE W/ LEGS bed, bench, chair, couch, desk,
rocker, sofa, stool, table
FURNITURE W/O LEGS bookcase, bureau, cabinet,
closet, cupboard, dishwasher,
dresser
LIGHTINGS candle, chandelier, lamp,
lantern
ENTRY POINTS door, elevator, gate
UNGULATES bison, buffalo, bull, calf, camel,
cow, donkey, elephant, goat,
horse, lamb, ox, pig, pony,
sheep
BIRDS crow, dove, eagle, falcon, hawk,
ostrich, owl, penguin, pigeon,
raven, stork, vulture, wood-
pecker
Table 6: Examples of clusters produced by CW
using the representations obtained from the SAE
model.
6 Conclusions
In this paper, we presented a model that uses
stacked autoencoders to learn grounded meaning
representations by simultaneously combining tex-
tual and visual modalities. The two modalities are
encoded as vectors of natural language attributes
and are obtained automatically from decoupled
text and image data. To the best of our knowl-
edge, our model is novel in its use of attribute-
based input in a deep neural network. Experimen-
tal results in two tasks, namely simulation of word
similarity and word categorization, show that our
model outperforms competitive baselines and re-
lated models trained on the same attribute-based
input. Our evaluation also reveals that the bimodal
models are superior to their unimodal counterparts
and that higher-level unimodal representations are
better than the raw input. In the future, we would
like to apply our model to other tasks, such as im-
age and text retrieval (Hodosh et al, 2013; Socher
et al, 2013b), zero-shot learning (Socher et al,
2013a), and word learning (Yu and Ballard, 2007).
Acknowledgment We would like to thank Vit-
torio Ferrari, Iain Murray and members of the
ILCC at the School of Informatics for their valu-
able feedback. We acknowledge the support of
EPSRC through project grant EP/I037415/1.
729
References
Agirre, Eneko and Aitor Soroa. 2007. SemEval-
2007 Task 02: Evaluating Word Sense Induc-
tion and Discrimination Systems. In Proceed-
ings of the Fourth International Workshop on
Semantic Evaluations. Prague, Czech Republic,
pages 7?12.
Andrews, M., G. Vigliocco, and D. Vinson. 2009.
Integrating Experiential and Distributional Data
to Learn Semantic Representations. Psycholog-
ical Review 116(3):463?498.
Baroni, M., B. Murphy, E. Barbu, and M. Poe-
sio. 2010. Strudel: A Corpus-Based Semantic
Model Based on Properties and Types. Cogni-
tive Science 34(2):222?254.
Barsalou, Lawrence W. 2008. Grounded Cogni-
tion. Annual Review of Psychology 59:617?845.
Bengio, Y., P. Lamblin, D. Popovici, and
H. Larochelle. 2007. Greedy Layer-Wise Train-
ing of Deep Networks. In Bernhard Sch?olkopf,
John Platt, and Thomas Hoffman, editors, Ad-
vances in Neural Information Processing Sys-
tems 19. MIT Press, pages 153?160.
Bengio, Yoshua. 2009. Learning Deep Architec-
tures for AI. Foundations and Trends in Ma-
chine Learning 2(1):1?127.
Biemann, Chris. 2006. Chinese Whispers ? an Ef-
ficient Graph Clustering Algorithm and its Ap-
plication to Natural Language Processing Prob-
lems. In Proceedings of TextGraphs: the 1st
Workshop on Graph Based Methods for Natu-
ral Language Processing. New York, NY, pages
73?80.
Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine
Learning Research 3:993?1022.
Bruni, E., G. Boleda, M. Baroni, and N. Tran.
2012a. Distributional Semantics in Technicolor.
In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics.
Jeju Island, Korea, pages 136?145.
Bruni, E., U. Bordignon, A. Liska, J. Uijlings, and
I. Sergienya. 2013. Vsem: An open library for
visual semantics representation. In Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics: System Demon-
strations. Sofia, Bulgaria, pages 187?192.
Bruni, E., N. Tran, and M. Baroni. 2014. Multi-
modal distributional semantics. J. Artif. Intell.
Res. (JAIR) 49:1?47.
Bruni, E., J. Uijlings, M. Baroni, and N. Sebe.
2012b. Distributional Semantics with Eyes: Us-
ing Image Analysis to Improve Computational
Representations of Word Meaning. In Proceed-
ings of the 20th ACM International Conference
on Multimedia. Nara, Japan, pages 1219?1228.
Collobert, R., J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural
Language Processing (almost) from Scratch.
Journal of Machine Learning Research
12:2493?2537.
Deng, J., W. Dong, R. Socher, L. Li, K. Li, and
L. Fei-Fei. 2009. ImageNet: A Large-Scale
Hierarchical Image Database. In Proceedings
of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition. Mi-
ami, Florida, pages 248?255.
Farhadi, A., I. Endres, D. Hoiem, and D. Forsyth.
2009. Describing Objects by their Attributes.
In Proceedings of the IEEE Computer Soci-
ety Conference on Computer Vision and Pat-
tern Recognition. Miami Beach, Florida, pages
1778?1785.
Feng, Fangxiang, Ruifan Li, and Xiaojie Wang.
2013. Constructing Hierarchical Image-tags Bi-
modal Representations for Word Tags Alter-
native Choice. In Proceedings of the ICML
2013 Workshop on Challenges in Representa-
tion Learning. Atlanta, Georgia.
Feng, Yansong and Mirella Lapata. 2010. Visual
Information in Semantic Representation. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics.
Los Angeles, California, pages 91?99.
Finkelstein, L., E. Gabrilovich, Y. Matias,
E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.
2002. Placing Search in Context: The Concept
Revisited. ACM Transactions on Information
Systems 20(1):116?131.
Fountain, Trevor and Mirella Lapata. 2010. Mean-
ing Representation in Natural Language Cat-
egorization. In Proceedings of the 31st An-
nual Conference of the Cognitive Science Soci-
ety. Amsterdam, The Netherlands, pages 1916?
1921.
730
Golub, Gene and Christian Reinsch. 1970. Sin-
gular Value Decomposition and Least Squares
Solutions. Numerische Mathematik 14(5):403?
420.
Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum.
2007. Topics in Semantic Representation. Psy-
chological Review 114(2):211?244.
Hardoon, D. R., S. R. Szedmak, and J. R. Shawe-
Taylor. 2004. Canonical Correlation Analy-
sis: An Overview with Application to Learning
Methods. Neural Computation 16(12):2639?
2664.
Harris, Zellig. 1970. Distributional Structure. In
Papers in Structural and Transformational Lin-
guistics, pages 775?794.
Hinton, Geoffrey E. and Ruslan R. Salakhutdinov.
2006. Reducing the Dimensionality of Data
with Neural Networks. Science 313(5786):504?
507.
Hodosh, Micah, Peter Young, and Julia Hocken-
maier. 2013. Framing Image Description as a
Ranking Task: Data, Models and Evaluation
Metrics. Journal of Artificial Intelligence Re-
search 47:853?899.
Huang, Jing and Brian Kingsbury. 2013. Audio-
visual Deep Learning for Noise Robust Speech
Recognition. In Proceedings of the 38th Inter-
national Conference on Acoustics, Speech, and
Signal Processing. Vancouver, Canada, pages
7596?7599.
Jones, R., B. Rey, O. Madani, and W. Greiner.
2006. Generating Query Substititions. In Pro-
ceedings of the 15th International Conference
on the World-Wide Web. Edinburgh, Scotland,
pages 387?396.
Landau, B., L. Smith, and S. Jones. 1998. Object
Perception and Object Naming in Early Devel-
opment. Trends in Cognitive Science 27:19?24.
Landauer, Thomas and Susan T. Dumais. 1997. A
Solution to Plato?s Problem: the Latent Seman-
tic Analysis Theory of Acquisition, Induction,
and Representation of Knowledge. Psychologi-
cal Review 104(2):211?240.
Lowe, D. 2004. Distinctive Image Features from
Scale-invariant Keypoints. International Jour-
nal of Computer Vision 60(2):91?110.
Manning, C. D., P. Raghavan, and H. Sch?utze.
2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY.
McRae, K., G. S. Cree, M. S. Seidenberg, and
C. McNorgan. 2005. Semantic Feature Pro-
duction Norms for a Large Set of Living and
Nonliving Things. Behavior Research Methods
37(4):547?559.
Mikolov, T., S. Kombrink, L. Burget, J.
?
Cernock?y,
and S. Khudanpur. 2011. Extensions of Recur-
rent Neural Network Language Model. In Pro-
ceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Pro-
cessing. Prague, Czech Republic, pages 5528?
5531.
Mikolov, T., Wen-tau Yih, and G. Zweig. 2013.
Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the
2013 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies. At-
lanta, Georgia, pages 746?751.
Miller, George A. and Walter G. Charles. 1991.
Contextual Correlates of Semantic Similarity.
Language and Cognitive Processes 6(1).
Nelson, D. L., C. L. McEvoy, and T. A.
Schreiber. 1998. The University of South
Florida Word Association, Rhyme, and Word
Fragment Norms.
Ngiam, Jiquan, Aditya Khosla, Mingyu Kim,
Juhan Nam, Honglak Lee, and Andrew Ng.
2011. Multimodal Deep Learning. In Pro-
ceedings of the 28th International Conference
on Machine Learning. Bellevue, Washington,
pages 689?696.
Patwardhan, Siddharth and Ted Pedersen. 2006.
Using WordNet-based Context Vectors to Es-
timate the Semantic Relatedness of Concepts.
In Proceedings of the EACL 2006 Workshop
on Making Sense of Sense: Bringing Compu-
tational Linguistics and Psycholinguistics To-
gether. Trento, Italy, pages 1?8.
Ranzato, Marc?Aurelio and Martin Szummer.
2008. Semi-supervised Learning of Com-
pact Document Representations with Deep Net-
works. In Proceedings of the 25th International
Conference on Machine Learning. Helsinki,
Finland, pages 792?799.
Regier, Terry. 1996. The Human Semantic Poten-
tial. MIT Press, Cambridge, Massachusetts.
Roller, Stephen and Sabine Schulte im Walde.
2013. A Multimodal LDA Model integrating
731
Textual, Cognitive and Visual Modalities. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing.
Seattle, Washington, pages 1146?1157.
Sebastiani, Fabrizio. 2002. Machine Learning in
Automated Text Categorization. ACM Comput-
ing Surveys 34:1?47.
Silberer, C., V. Ferrari, and M. Lapata. 2013. Mod-
els of Semantic Representation with Visual At-
tributes. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics. Sofia, Bulgaria, pages 572?582.
Silberer, Carina and Mirella Lapata. 2012.
Grounded Models of Semantic Representation.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning. Jeju Island, Korea, pages 1423?1433.
Socher, R., M. Ganjoo, C. D. Manning, and A. Y.
Ng. 2013a. Zero-shot learning through cross-
modal transfer. In Advances in Neural Informa-
tion Processing Systems 26, pages 935?943.
Socher, R., Quoc V. Le, C. D. Manning, and A. Y.
Ng. 2013b. Grounded Compositional Seman-
tics for Finding and Describing Images with
Sentences. In Proceedings of the NIPS Deep
Learning Workshop.
Socher, R., J. Pennington, E. H. Huang, A. Y. Ng,
and C. D. Manning. 2011. Semi-Supervised Re-
cursive Autoencoders for Predicting Sentiment
Distributions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Edinburgh, Scotland, pages
151?161.
Srivastava, Nitish and Ruslan Salakhutdinov.
2012. Multimodal Learning with Deep Boltz-
mann Machines. In Advances in Neural In-
formation Processing Systems 25, pages 2231?
2239.
Steyvers, Mark. 2010. Combining Feature Norms
and Text Data with Topic Models. Acta Psycho-
logica 133(3):234?342.
Szumlanski, S. R., F. Gomez, and V. K. Sims.
2013. A New Set of Norms for Semantic Re-
latedness Measures. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics. Sofia, Bulgaria, pages 890?
895.
Turney, Peter D. and Patrick Pantel. 2010. From
Frequency to Meaning: Vector Space Models
of Semantics. Journal of Artificial Intelligence
Research 37(1):141?188.
Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio,
and P. Manzagol. 2010. Stacked Denoising Au-
toencoders: Learning Useful Representations in
a Deep Network with a Local Denoising Cri-
terion. Journal of Machine Learning Research
11:3371?3408.
von Ahn, Luis and Laura Dabbish. 2004. Labeling
Images with a Computer Game. In Proceedings
of the SIGCHI Conference on Human Factors
in Computing Systems. Vienna, Austria, pages
319?326.
Wu, Pengcheng, Steven C. H. Hoi, Hao Xia, Peilin
Zhao, Dayong Wang, and Chunyan Miao. 2013.
Online Multimodal Deep Similarity Learning
with Application to Image Retrieval. In Pro-
ceedings of the 21st ACM International Con-
ference on Multimedia. Barcelona, Spain, pages
153?162.
Yih, Wen-tau, Ming-Wei Chang, Christopher
Meek, and Andrzej Pastusiak. 2013. Question
Answering Using Enhanced Lexical Semantic
Models. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics. Sofia, Bulgaria, pages 1744?1753.
Yu, C. and D. H. Ballard. 2007. A Unified Model
of Early Word Learning Integrating Statistical
and Social Cues. Neurocomputing 70:2149?
2165.
732
