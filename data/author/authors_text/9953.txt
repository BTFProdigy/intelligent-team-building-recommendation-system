Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 353?362,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Transliteration as Constrained Optimization
Dan Goldwasser Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61801
{goldwas1,danr}@uiuc.edu
Abstract
This paper introduces a new method for iden-
tifying named-entity (NE) transliterations in
bilingual corpora. Recent works have shown
the advantage of discriminative approaches to
transliteration: given two strings (ws, wt) in
the source and target language, a classifier is
trained to determine if wt is the translitera-
tion of ws. This paper shows that the translit-
eration problem can be formulated as a con-
strained optimization problem and thus take
into account contextual dependencies and con-
straints among character bi-grams in the two
strings. We further explore several methods
for learning the objective function of the opti-
mization problem and show the advantage of
learning it discriminately. Our experiments
show that the new framework results in over
50% improvement in translating English NEs
to Hebrew.
1 Introduction
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to some
target language based on phonetic similarity be-
tween the entities. Identifying transliteration pairs
is an important component in many linguistic appli-
cations which require identifying out-of-vocabulary
words, such as machine translation and multilingual
information retrieval (Klementiev and Roth, 2006b;
Hermjakob et al, 2008).
It may appear at first glance that identifying the
phonetic correlation between names based on an
orthographic analysis is a simple, straight-forward
Figure 1: Named entities transliteration pairs in English
and Hebrew and the character level mapping between the
two names. The Hebrew names can be romanized as ee-
ta-l-ya and a-ya
task; however in many cases a consistent deter-
ministic mapping between characters does not ex-
ist; rather, the mapping depends on the context the
characters appear in and on transliteration conven-
tions which may change across domains. Figure 1
exhibits two examples of NE transliterations in En-
glish and Hebrew, with the correct mapping across
the two scripts. Although the two Hebrew names
share a common prefix1, this prefix can be mapped
into a single English character or into two differ-
ent characters depending on the context it appears
in. Similarly, depending on the context it appears in,
the English character a can be mapped into different
characters or to an ?empty? character.
1In all our example the Hebrew script is shown left-to-right
to simplify the visualization of the transliteration mapping.
353
In recent years, as it became clear that solutions
that are based on linguistics rules are not satisfac-
tory, machine learning approaches have been de-
veloped to address this problem. The common ap-
proach adopted is therefore to view this problem
as a classification problem (Klementiev and Roth,
2006a; Tao et al, 2006) and train a discriminative
classifier. That is, given two strings, one in the
source and the other in the target language, extract
pairwise features, and train a classifier that deter-
mines if one is a transliteration of the other. Sev-
eral papers have followed up on this basic approach
and focused on semi-supervised approaches to this
problem or on extracting better features for the dis-
criminative classifier (Klementiev and Roth, 2006b;
Bergsma and Kondrak, 2007; Goldwasser and Roth,
2008). While it has been clear that the relevancy of
pairwise features is context sensitive and that there
are contextual constraints among them, the hope was
that a discriminative approach will be sufficient to
account for those by weighing features appropri-
ately. This has been shown to be difficult for lan-
guage pairs which are very different, such as English
and Hebrew (Goldwasser and Roth, 2008).
In this paper, we address these difficulties by
proposing to view the transliteration decision as a
globally phrased constrained optimization problem.
We formalize it as an optimization problem over
a set of local pairwise features ? character n-gram
matches across the two string ? and subject to legit-
imacy constraints.
We use a discriminatively trained classifier as a
way to learn the objective function for the global
constrained optimization problem. Our technical
approach follows a large body of work developed
over the last few years, following (Roth and Yih,
2004) that has formalized global decisions problems
in NLP as constrained optimization problems and
solved these optimization problems using Integer
Linear Programming (ILP) or other methods (Pun-
yakanok et al, 2005; Barzilay and Lapata, 2006;
Clarke and Lapata, ; Marciniak and Strube, 2005).
We investigate several ways to train our objective
function, which is represented as a dot product be-
tween a set of features chosen to represent a pair
(ws, wt), and a vector of initial weights. Our first
baseline makes use of all features extracted from a
pair, along with a simple counting method to deter-
mine initial weights. We then use a method simi-
lar to (Klementiev and Roth, 2006a; Goldwasser and
Roth, 2008) in order to discriminatively train a better
weight vector for the objective function.
Our key contribution is that we use a constrained
optimization approach also to determine a better fea-
ture representation for a given pair. (Bergsma and
Kondrak, 2007) attempted a related approach to re-
stricting the set of features representing a transliter-
ation candidate. However, rather than directly align-
ing the two strings as done there, we exploit the ex-
pressiveness of the ILP formulation and constraints
to generate a better representation of a pair. This
is the representation we then use to discriminatively
learn a better weight vector for the objective func-
tion used in our final model.
Our experiments focus on Hebrew-English
transliteration, which were shown to be very dif-
ficult in a previous work (Goldwasser and Roth,
2008). We show very significant improvements over
existing work with the same data set, proving the
advantage of viewing the transliteration decision as
a global inference problem. Furthermore, we show
the importance of using a discriminatively trained
objective function.
The rest of the paper is organized as follows. The
main algorithmic contribution of this paper is de-
scribed in Sec. 2. Our experimental study is de-
scribes in Sec. 3 and Sec. 4 concludes.
2 Using inference for transliteration
In this section we present our transliteration decision
framework, which is based on solving a constrained
optimization problem with an objective function that
is discriminatively learned. Our framework consists
of three key elements:
1. Decision Model When presented with a NE
in the source language ws and a set of candi-
dates {wt}k1 in the target language, the decision
model ranks the candidate pairs (ws, wt) and
selects the ?best? candidate pair. This is framed
as an optimization problem
w?t = argmaxi{w ? F (ws, wit)}, (1)
where F is a feature vector representation of
the pair (ws, wit) and w is a vector of weights
assigned to each feature.
354
2. Representation A pair s = (ws, wt) of source
and target NEs is represented as a vector of fea-
tures, each of which is a pair of character n-
grams, from ws and wt, resp. Starting with a
baseline representation introduced in (Klemen-
tiev and Roth, 2006a), denoted here AF (s),
we refine this representation to take into ac-
count dependencies among the individual n-
gram pairs. This refinement process is framed
as a constrained optimization problem:
F (s)? = argmaxF?AF {w ?AF (s)}, (2)
subject to a set C of linear constraints. Here
AF is the initial representation (All?Features),
w is a vector of weights assigned to each fea-
ture and C is a set of constraints accounting for
interdependencies among features.
3. Weight Vector Each pairwise n-gram feature is
associated with a weight; this weigh vector is
used in both optimization formulations above.
The weight vector is determined by considering
the whole training corpus. The initial weight
vector is obtained generatively, by counting the
relative occurrence of substring pairs in posi-
tive examples. The representation is refined by
discriminatively training a classifier to maxi-
mize transliteration performance on the train-
ing data. In doing that, each example is rep-
resented using the feature vector representation
described above.
The three key operations described above are be-
ing used in several stages, with different parameters
(weight vectors and representations) as described
in Alg. 1. In each stage a different element is re-
fined. The input to this process is a training corpus
Tr=(DS ,DT ) consisting of NE transliteration pairs
s = (ws, wt), where ws, wt are NEs in the source
and target language, respectively. Each such sam-
ple point is initially represented as a feature vector
AF (s) (for All?Features), where features are pairs
of substrings from the two words (following (Kle-
mentiev and Roth, 2006a)).
Given the set of feature vectors generated by ap-
plying AF to Tr, we assign initial weights W to
the features ((1) in Alg. 1). These weights form
the initial objective function used to construct a new
feature based representation, Informative?Features,
IFW (s) ((2) in Alg. 1). Specifically, for an instance
s, IFW (s) is the solution of the optimization prob-
lem in Eq. 2, with W as the weight vector, AF (s)
as the representation, and a set of constraints ensur-
ing the ?legitimacy? of the selected set of features
(Sec. 2.2.1).
Input: Training Corpora Tr=(DS ,DT )
Output: Transliteration model M
1. Initial Representation and Weights
For each sample s ? Tr, use AF to generate a
feature vector
{(fs, ft)1, (fs, ft)2, . . . , (fs, ft)n} ? {0, 1}n.
Define W :f ?R s.t. foreach feature f =(fs, ft)
W (f) = #(fs,ft)#(fs) ?
#(fs,ft)
#(ft)
2. Inferring Informative Representation (W )
Modify the initial representation by solving the
following constrained optimization problem:
IFW (s)? = argmaxIF (s)?(AF (s))W ?AF (s),
subject to constraints C.
3. Discriminative Training
Train a discriminative model on Tr, using
{IF (s)}s?Tr.
Let WD be the new weight vector obtained by
discriminative training.
4. Inferring Informative Representation (WD)
Modify the initial representation by solving the
following constrained optimization problem. This
time, the objective function is determined by the
discriminatively trained weight vector WD.
IFWD (s)? = argmaxIF (s)?(AF (s))WD ?AF (s),
subject to constraints C.
5. Decision Model
Given a word ws and a list of candidates
w1t , w2t , . . . wkt , the chosen transliteration is wt? ,
determined by:
t? = argmaxi{WD ? IFWD ((ws, wit))}
Algorithm 1: Transliteration Framework.
The new feature extraction operator IFW (s) is
now used to construct a new representation of the
training corpus. With this representation, we train
discriminately a new weight vector WD. This
weight vector, now defines a new objective function
for the optimization problem in Eq. 2; WD is the
weight vector and AF (s) the representation. We de-
355
note by IFWD(s) the solution of this optimization
problem for an instance s.
Given a representation and a weight vector, the
optimization problem in Eq. 1 is used to find the
transliteration of ws. Our best decision model makes
use of Eq. 1 using WD as the feature vector and
IFWD(s) as the feature representation of s.
The rest of this section provides details on the op-
erations and how we use them in different stages.
2.1 Initial Representation and Weights
The feature space we consider consists of n po-
tential features, each feature f = (fs, ft) repre-
sents a pairing of character level n-grams, where
fs ? {Source-Language ? empty-string } and ft ?
{Target-Language ? empty-string}. A given sample
(ws, wt) consisting of a pair of NEs is represented
as a features vector s ? {0, 1}n. We say that a fea-
ture f i is active if f i = 1 and that s1 ? s2, ??
{f i}{f i= 1 in s1} ? {f i}{f i=1 in s2}. We represent
the active features corresponding to a pair as a bipar-
tite graph G = (V,E), in which each vertex v ? V
either represents the empty string, a single character
or a bi-gram. V S , V T denote the vertices represent-
ing source and target language n-grams respectively.
Each of these sets is composed of two disjoint sub-
sets: VS = V SU ? V SB , VT = V TU ? V TB consisting
of vertices representing the uni-gram and bi-gram
strings. Given a vertex v, degree(v, V ?)denotes the
degree of v in a subgraph of G, consisting only of
V ? ? V ; index(v) is the index of the substring rep-
resented by v in the original string.
Edges in the bipartite graph represent active fea-
tures. The only deviation is that the vertex represent-
ing the empty string can be connected to any other
(non-empty) vertex.
Our initial feature extraction method follows the
one presented in (Klementiev and Roth, 2006a),
in which the feature space consists of n-gram pairs
from the two languages. Given a pair, each word
is decomposed into a set of character substrings of
up to a given length (including the empty string).
Features are generated by pairing substrings from
the two sets whose relative positions in the original
words differ by k or less places, or formally:
E = {e = (vi, vj) | (vi ? VS ? vj ? VT ) ?
(index(vj) + k ? index(vi) ? index(vj)? k) ?
Figure 2: All possible unigram and bigram pairs gener-
ated by the AF operator. The Hebrew name can be ro-
manized as lo-n-do-n
(vi 6= vempty?string ? vj 6= vempty?string)}.
In our experiments we used k=1 which tested em-
pirically, achieved the best performance.
Figure 2 exhibits the active features in the exam-
ple using the graph representation. We refer to this
feature extraction method as All-Features (AF ),
and define it formally as an operator AF : s ?
{(fs, ft)i} that maps a sample point s = (ws, wt)
to a set of active features.
The initial sample representation generates fea-
tures by coupling substrings from the two terms
without considering the dependencies between the
possible consistent combinations. Ideally, given
a positive sample, it is desirable that paired sub-
strings would encode phonetic similarity or a dis-
tinctive context in which the two substrings corre-
late. However, AF simply pairs substrings from the
two words, resulting in a noisy representation of the
sample point. Given enough positive samples, we
assume that features appearing with distinctive fre-
quency will encode the desired relation. We use this
observation, and construct a weight vector, associ-
ating each feature with a positive number indicating
its relative occurrence frequency in the training data
representation formed by AF . This weight is com-
puted as follows:
Definition 1 (Initial Feature Weights Vector) Let
W :f ?R s.t. for each feature f={fs, ft},
W (f) = #(fs, ft)#(fs) ?
#(fs, ft)
#(ft) ,
where #(fs, ft) is the number of occurrences of that
feature in the positive sample set, and #(fL), L =
{s, t} is the number of occurrences of an individual
substring, in any of the features extracted from pos-
itive samples in the training set.
356
These weights transform every example into a
weighted graph, where each edge is associated by W
with the weight assigned to the feature it represents.
As we empirically tested, this initialization assigns
high weights to features that preserve the phonetic
correlation between the two languages. The top part
of figure 5 presents several examples of weights as-
signed by W to features composed of different En-
glish and Hebrew substrings combinations. It can be
observed that combination which are phonetically
similar are associated with a higher weight. How-
ever, as it turns out, transliteration mappings do not
consist of ?clean? and consistent mappings of pho-
netically similar substrings. In the following section
we explain how to use these weights to generate a
more compact representation of samples.
2.2 Inferring Informative Representations
In this section we suggest a new feature extraction
method for determining the representation of a given
word pair. We use the strength of the active features
computed above, along with legitimacy constraints
on mappings between source and target strings to
find an optimal set of consistent active features that
represents a pair. This problem can be naturally en-
coded as a linear optimization problem, which seeks
to maximize a linear objective function determined
by W , over a set of variables representing the ac-
tive features selection, subject to a set of linear con-
straints representing the dependencies between se-
lections. We follow the formulation given by (Roth
and Yih, 2004), and define it as an Integer Linear
Programming (ILP) optimization problem, in which
each integer variable a(j,k), defined over {0, 1}, rep-
resents whether a feature pairing an n-gram j ? S
with an n-gram k ? T , is active. Although using ILP
is in general NP-hard, it has been used efficiently in
many natural language (see section 1). Our experi-
ence as well has been that this process is very effi-
cient due to the sparsity of the constraints used.
2.2.1 Constraining Feature Dependencies
To limit the selection of active features in each
sample we require that each element in the decom-
position of ws into bi-grams should be paired with
an element in wt, and the vice-versa. We restrict
the possible pairs by allowing only a single n-gram
to be matched to any other n-gram, with one excep-
tion - we allow every bi-gram to be mapped into an
empty string. Viewed as a bipartite graph, we allow
each node (with the exception of the empty string)
to have only one connected edge. These constraints,
given the right objective function, should enforce an
alignment of bi-grams according to phonetic simi-
larity; for example, the word pairs described in Fig-
ure 1, depicts a character level alignment between
the words, where in some cases a bi-gram is mapped
into a single character and in other cases single char-
acters are mapped to each other, based on phonetic
similarity encoded by the two scripts. However, im-
posing these constraints over the entire set of candi-
date features would be too restrictive; it is unlikely
that one can consistently represent a single ?correct?
phonetic mapping. We wish to represent both the
character level and bi-gram mapping between names
as both represent informative features on the corre-
spondence between the names over the two scripts.
To allow this, we decompose the problem into two
disjoint sets of constraints imposing 1-1 mappings,
one over the set of single character substrings and
the other over the bi-gram substrings. Given the bi-
partite graph generated by AF, we impose the fol-
lowing constraints:
Definition 2 (Transliteration Constraints) Let C
be the set of constraints, consisting of the following
predicates:
?v ? V S , degree(v,V S?V TU )?1 ?
?v ? V S , degree(v,V S?V TB )?1 ?
?v ? V T , degree(v,V T?V SU )?1 ?
?v ? V T , degree(v,V T?V SB )?1
For example, Figure 2 shows the graph of all pos-
sible candidates produced by AF . In Figure 3, the
graph is decomposed into two graphs, each depict-
ing possible matches between the character level
uni-gram or bi-gram substrings. the ILP constraints
ensure that in each graph, every node (with the ex-
ception of the empty string) has a degree of one .
Figure 4 gives the results of the ILP process ? a
unified graph in which every node has only a single
edge associated with it.
Definition 3 (Informative Feature Extraction (IF))
We define the Informative-Features(IF ) feature
extraction operator, IF : s ? {(fs, ft)i} as the
solution to the ILP problem in Eq. 2. Namely,
357
Figure 3: Find informative features by solving an ILP
problem. Dependencies between matching decisions are
modeled by allowing every node to be connected to a sin-
gle edge (except the node representing the empty-string).
Figure 4: The result of applying the IF operator by solv-
ing an ILP problem, represented as a pruned graph.
IF (s)? = argmaxIF (s)?(AF (s))w ?AF (s),
subject to constraints C.
We will use this operator with w = W , defined
above, and denote it IFW , and also use it with a
different weight vector, trained discriminatively, as
described next.
2.3 Discriminative Training
Using the IFW operator, we generate a better rep-
resentation of the training data, which is now used
to train a discriminative model. We use a linear
classifier trained with a regularized average percep-
tron update rule (Grove and Roth, 2001) as imple-
mented in SNoW, (Roth, 1998). This learning al-
gorithm provides a simple and general linear clas-
sifier that has been demonstrated to work well in
other NLP classification tasks, e.g. (Punyakanok
et al, 2005), and allows us to incorporate extensions
such as strength of features naturally into the train-
ing algorithm. We augment each sample in the train-
Figure 5: Several examples of weights assigned to fea-
tures generated by coupling English and Hebrew sub-
strings. Top figure: initial weights. Bottom figure: Dis-
criminatively learned weights. The Hebrew characters,
ordered left to right, can be romanized as y,z,t,sh
ing data with feature weights; given a sample, the
learner is presented with a real-valued feature vec-
tor instead of a binary vector. This can be viewed
as providing a better starting point for the learner,
which improves the learning rate (Golding and Roth,
1999; Ng and Jordan, 2001).
The weight vector learned by the discriminative
training is denoted WD. Given the new weight vec-
tor, we can define a new feature extraction opera-
tor, that we get by applying the objective function in
Eq. 2 with WD instead of W . Given a sample s, the
feature representation generated by this new infor-
mation extraction operator is denoted IFWD(s). The
key difference between W and WD is that the latter
was trained over a corpora containing both negative
and positive examples, and as a result WD contains
negative weights. To increase the impact of training
we multiplied the negative weights by 2.
Figure 5 presents some examples of the benefit
of discriminately learning the objective function; the
weighted edges in the top figure show the values as-
signed to features by W , while the bottom figure
shows the weights assigned by WD. In all cases,
phonetically similar characters were assigned higher
scores by WD, and character pairs not phonetically
similar were typically assigned negative weights. It
is also interesting to note a special phenomena oc-
curring in English-Hebrew transliterations. The En-
glish vowels will be paired to almost any Hebrew
character when generating pairs using AF , since
vowels in most cases are omitted in Hebrew, there
is no distinctive context in which English vowels
appear. We can see for example, in the top graph
358
presented in Figure 5 an edge matching a vowel to
a Hebrew character with a high weight, the bottom
graph showing the results of the discriminative train-
ing process show that this edge is associated with a
zero weight score.
2.4 Decision Models
This section defines several transliteration decision
models given a word ws and a list of candidates
w1t , w2t , . . . wkt . The models are used to identify the
correct transliteration pair from the set of candidates
{si = (ws, wit)}i=1...k.
In all cases, the decision is formulated as in Eq. 1,
where different models differ by the representations
and weight vectors used.
Decision Model 1 Ranking the transliteration can-
didates is done by evaluating
s? = argmaxi W ?AF (si),
which selects the transliteration pair which maxi-
mizes the objective function based on the genera-
tively computed weight vector.
Decision Model 2 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi WD ?AF (si)).
This decision model is essentially equivalent to the
transliteration models used in (Klementiev and
Roth, 2006a; Goldwasser and Roth, 2008), in which
a linear transliteration model was trained using a fea-
ture extraction method equivalent to AF.
Decision Model 3 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi W ? IFW (si),
which maximizes the objective function with the
generatively computed weight vector and the infor-
mative feature representation derived based on it.
Decision Model 4 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi WD ? IFW (si)),
which conceptually resembles the transliteration
model presented in (Bergsma and Kondrak, 2007),
in that a discriminative classifier was trained and
used over a pruned feature set.
Decision Model 5 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi WD ? IFWD(si),
which maximize the objective function with the dis-
criminately derived weight vector and the informa-
tive features inferred based on it. This decision
model is the only model that incorporates discrim-
inative weights as part of the feature extraction pro-
cess; WD is used as the objective function used
when inferring IFWD .
3 Evaluation
We evaluated our approach over a corpus of 300
English-Hebrew transliteration pairs, and used an-
other 250 different samples for training the models.
We constructed the test set by pairing each English
name with all Hebrew names in the corpus. The sys-
tem was evaluated on its ability to correctly iden-
tify the 300 transliteration pairs out of all the pos-
sible transliteration candidates. We measured per-
formance using the Mean Reciprocal Rank (MRR)
measure. This measure, originally introduced in
the field of information retrieval, is used to evaluate
systems that rank several options according to their
probability of correctness. MRR is a natural mea-
sure in our settings and has been used previously
for evaluating transliteration systems, for example
by (Tao et al, 2006).
Given a set Q of queries and their respective
responses ranked according to the system?s confi-
dence, we denote the rank of the correct response
to a query qi ? Q as rank(qi). MRR is then de-
fined as the average of the multiplicative inverse of
the rank of the correct answer, that is:
MRR = 1|Q|
?
i=1...|Q|
1
rank(qi) .
In our experiments we solved an ILP problem for
every transliteration candidate pairs, and computed
MRR with respect to the confidence of our decision
model across the candidates. Although this required
solving thousands of ILP instances, it posed no com-
putational burden as these instances typically con-
tained a small number of variables and constraints.
The entire test set is solved in less than 20 minutes
359
using the publicly available GLPK package (http:
//www.gnu.org/software/glpk/ ).
The performance of the different models is sum-
marized in table 1, these results are based on a train-
ing set of 250 samples used to train the discrimi-
native transliteration models and also to construct
the initial weight vector W . Figure 6 shows perfor-
mance over different number of training examples.
Our evaluation is concerns with the core transliter-
ation and decision models presented here and does
not consider any data set optimizations that were in-
troduced in previous works, which we view as or-
thogonal additions, hence the difference with the re-
sults published in (Goldwasser and Roth, 2008).
The results clearly show that our final model,
model 5, outperform other models. Interestingly,
model 1, a simplistic model, significantly outper-
forms the discriminative model presented in (Kle-
mentiev and Roth, 2006b). We believe that this is
due to two reasons. It shows that discriminative
training over the representation obtained using AF
is not efficient; moreover, this phenomenon is ac-
centuated given that we train over a very small data
set, which favors generative estimation of weights.
This is also clear when comparing the performance
of model 1 to model 4, which shows that learning
over the representation obtained using constrained
optimization (IF) results in a very significant perfor-
mance improvement.
The improvement of using IFW is not automatic.
Model 3, which uses IFW , and model 1, which uses
AF, converge to nearly the same result. Both these
models use generative weights to make the translit-
eration decision, and this highlights the importance
of discriminative training. Both model 4 and model
5 use discriminatively trained weights and signifi-
cantly outperform model 3. These results indicate
that using constraint optimization to generate the ex-
amples? representation in itself may not help; the ob-
jective function used in this inference has a signifi-
cant role in improved performance.
The benefit of discriminatively training the objec-
tive function becomes even clearer when compar-
ing the performance of model 5 to that of model 4,
which uses the original weight vector when inferring
the sample representation.
It can be assumed that this algorithm can bene-
fit from further iterations ? generating a new feature
Decision Model MRR
Baseline model, used in (KR?06,GR?08)
Model 2 0.51
Models presented in this paper
Model 1 0.713
Model 3 0.715
Model 4 0.832
Model 5 0.848
Table 1: Results of the different transliteration models,
trained using 250 samples. To facilitate readability (Kle-
mentiev and Roth, 2006b; Goldwasser and Roth, 2008)
are referenced as KR?06 and GR?08 respectively.
Figure 6: Results of the different constraint optimization
transliteration models. Performance is compared relative
to the number of samples used for training.
representations, training a model on it, and using the
resulting model as a new objective function. How-
ever, it turns out that after a single round, improved
weights due to additional training do not change the
feature representation; the inference process does
not yield a different outcome.
3.1 Normalized Objective Function
Formulating the transliteration decision as an op-
timization problem also allows us to naturally en-
code other considerations into our objective func-
tion. in this case we give preference to matching
short words. We encode this preference as a normal-
ization factor for the objective function. When eval-
uating on pair (ws, wt), we divide the weight vector
length of the shorter word; our decision model now
becomes:
Decision Model 6 (Model 5 - LengthNormalization)
360
Decision Model MRR
Model 5 0.848
Model 5 - LN 0.894
Table 2: Results of using model 5 with and without a
normalized objective function. Both models were trained
using 250 samples. The LN suffix in the model?s name
indicate that the objective function used length normal-
ization.
Figure 7: Results of using model 5 with and without a
normalized objective function. Performance is compared
relative to the number of samples used for training.
Ranking the transliteration candidates is done by
evaluating:
s? = argmaxi WD ? IFWD(si)/min(|ws|, |wt|)
As described in table 2 and figure 7, using
length normalization significantly improves the re-
sults. This can be attributed to the fact that typically
Hebrew names are shorter and therefore every pair
(ws, wt) considered by our model will be effected
differently by this normalization factor.
4 Discussion
We introduced a new approach for identifying NE
transliteration, viewing the transliteration decision
as a global inference problem. We explored sev-
eral methods for combining discriminative learning
in a global constraint optimization framework and
showed that discriminatively learning the objective
function improves performance significantly.
From an algorithmic perspective, our key contri-
bution is the introduction of a new method, in which
learning and inference are used in an integrated way.
We use learning to generate an objective function for
the inference process; use the inference process to
generate a better representation for the learning pro-
cess, and iterate these stages.
From the transliteration perspective, our key con-
tribution is in deriving and showing the significance
of a good representation for a pair of NEs. Our
representation captures both phonetic similarity and
distinctive occurrence patterns across character level
matchings of the two input strings, while enforcing
the constraints induced by the interdependencies of
the individual matchings. As we show, this represen-
tation serves to improve the ability of a discrimina-
tive learning algorithm to weigh features appropri-
ately and results in significantly better transliteration
models. This representation can be viewed as a com-
promise between models that do not consider depen-
dencies between local decisions and those that try to
align the two strings. Achieving this compromise is
one of the advantages of the flexibility allowed by
the constrained optimization framework we use. We
plan to investigate using more constraints within this
framework, such as soft constraints which can pe-
nalize unlikely local decisions while not completely
eliminating the entire solution.
Acknowledgments
We wish to thank Alex Klementiev and the anony-
mous reviewers for their insightful comments. This
work is partly supported by NSF grant SoD-HCER-
0613885 and DARPA funding under the Bootstrap
Learning Program.
References
R. Barzilay and M. Lapata. 2006. Aggregation via Set
Partitioning for Natural Language Generation. In Pro-
ceedings of HLT/NAACL, pages 359?366, New York
City, USA, June. Association for Computational Lin-
guistics.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 656?663, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
J. Clarke and M. Lapata. Modeling compression with
discourse constraints. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
361
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1?11.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
D. Goldwasser and D. Roth. 2008. Active sample selec-
tion for named entity transliteration. In Proceedings
of ACL-08: HLT, Short Papers, Columbus, OH, USA,
Jun. Association for Computational Linguistics.
A. Grove and D. Roth. 2001. Linear concepts and hidden
variables. Machine Learning, 42(1/2):123?141.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
A. Klementiev and D. Roth. 2006a. Named entity
transliteration and discovery from multilingual com-
parable corpora. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 82?88, June.
A. Klementiev and D. Roth. 2006b. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proc. of the Annual
Meeting of the ACL, July.
T. Marciniak and M. Strube. 2005. Beyond the Pipeline:
Discrete Optimization in NLP. In Proceedings of the
Ninth CoNLL, pages 136?143, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
A. Y. Ng and M. I. Jordan. 2001. On discriminative vs.
generative classifiers: A comparison of logistic regres-
sion and na??ve bayes. In Neural Information Process-
ing Systems, pages 841?848.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI), pages 1117?1123.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 1?8. Association for
Computational Linguistics.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat,
and ChengXiang Zhai. 2006. Unsupervised named
entity transliteration using temporal and phonetic cor-
relation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 250?257, Sydney, Australia, July. Association
for Computational Linguistics.
362
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958?967,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Reading to Learn: Constructing Features from Semantic Abstracts
Jacob Eisenstein
?
James Clarke
?
Dan Goldwasser
?
Dan Roth
??
?
Beckman Institute for Advanced Science and Technology,
?
Department of Computer Science
University of Illinois
Urbana, IL 61801
{jacobe,clarkeje,goldwas1,danr}@illinois.edu
Abstract
Machine learning offers a range of tools
for training systems from data, but these
methods are only as good as the underly-
ing representation. This paper proposes to
acquire representations for machine learn-
ing by reading text written to accommo-
date human learning. We propose a novel
form of semantic analysis called read-
ing to learn, where the goal is to obtain
a high-level semantic abstract of multi-
ple documents in a representation that fa-
cilitates learning. We obtain this abstract
through a generative model that requires
no labeled data, instead leveraging repe-
tition across multiple documents. The se-
mantic abstract is converted into a trans-
formed feature space for learning, result-
ing in improved generalization on a rela-
tional learning task.
1 Introduction
Machine learning offers a range of powerful tools
for training systems to act in complex environ-
ments, but these methods depend on a well-chosen
representation for features. For learning to suc-
ceed the representation often must be crafted with
knowledge about the application domain. This
poses a bottleneck, requiring expertise in both ma-
chine learning and the application domain. How-
ever, domain experts often express their knowl-
edge through text; one direct expression is through
text designed to aid human learning. In this paper
we exploit text written by domain experts in or-
der to build a more expressive representation for
learning. We term this approach reading to learn.
The following scenario demonstrates the moti-
vation for reading to learn. Imagine an agent given
a task within its world/environment. The agent has
no prior knowledge of the task but can perceive the
world through low-level sensors. Learning directly
from the sensors may be difficult, as interesting
tasks typically require a complex combination of
sensors. Our goal is to acquire domain knowledge
through the semantic analysis of text, so as to pro-
duce higher-level relations through combinations
of sensors.
As a concrete example consider the problem of
learning how to make legal moves in Freecell soli-
taire. Relevant sensors may indicate if an object
is a card or a freecell, whether a card is a certain
value, and whether two values are in sequence.
Although it is possible to express the rules with
a combination of sensors, learning this combina-
tion is difficult. Text can facilitate learning by pro-
viding relations at the appropriate level of gen-
eralization. For example, the sentence: ?You can
place a card on an empty freecell,? suggests not
only which sensors are useful together but also
how these sensors should be linked. Assuming the
sensors are represented as predicates, one possi-
ble relation this sentence suggests is: r(x, y) =
card(x) ? freecell(y) ? empty(y). Armed
with this new relation the agent?s learning task
may be simpler. Throughout the paper we refer to
low-level sensory input as sensor or predicate, and
to a higher level concept as a logical formula or re-
lation.
Our approach to semantic analysis does not re-
quire a complete semantic representation of the
text. We merely wish to acquire a semantic ab-
stract of a document or document collection, and
use the discovered relations to facilitate data-
driven learning. This will allow us to directly eval-
uate the contribution of the extracted relations for
learning.
We develop an approach to recover semantic ab-
stracts that uses minimal supervision: we assume
only a very small set of lexical glosses, which map
from words to sensors. This marks a substantial
departure from previous work on semantic pars-
ing, which requires either annotations of the mean-
ings of each individual sentence (Zettlemoyer and
Collins, 2005; Liang et al, 2009), or alignments
of sentences to grounded representations of the
958
world (Chen and Mooney, 2008). For the purpose
of learning, this approach may be inapplicable, as
such text is often written at a high level of abstrac-
tion that permits no grounded representation.
There are two properties of our setting that
make unsupervised learning feasible. First, it is
not necessary to extract a semantic representation
of each individual sentence, but rather a summary
of the semantics of the document collection. Er-
rors in the semantic abstract are not fatal, as long
it guides the learning component towards a more
useful representation. Second, we can exploit rep-
etition across documents, which should generally
express the same underlying meaning. Logical for-
mulae that are well-supported by multiple docu-
ments are especially likely to be useful.
The rest of this paper describes our approach
for recovering semantic abstracts and outlines how
we apply and evaluate this approach on the Free-
cell domain. The paper contributes the following
key ideas: (1) Interpreting abstract ?instructional?
text, written at a level that does not correspond
to concrete sensory inputs in the world, so that
no grounded representation is possible, (2) read-
ing to learn, a new setting in which extracted se-
mantic representations are evaluated by whether
they facilitate learning; (3) abstractive semantic
summarization, aimed at capturing broad seman-
tic properties of a multi-document dataset, rather
than a semantic parse of individual sentences; (4) a
novel, minimally-supervised generative model for
semantic analysis which leverages both lexical and
syntactic properties of text.
2 Approach Overview
We describe our approach to text analysis as mul-
tidocument semantic abstraction, with the goal of
discovering a compact set of logical formulae to
explain the text in a document collection. To this
end, we develop a novel generative model in which
natural language sentences (e.g., ?You can always
place cards in empty freecells?) are stochastically
generated from logical formulae (e.g., card(x)?
freecell(y) ? empty(y)). We formally define
a generative process that reflects our intuitions
about the relationship between formulae and sen-
tences (Section 3), and perform sampling-based
inference to recover the formulae most likely to
have generated the observed data (Section 4). The
top N such formulae can then be added as addi-
tional predicates for relational learning.
Our semantic representation consists of con-
junctions of literals, each of which includes a sin-
gle predicate (e.g., empty) and one or more vari-
ables (e.g., x). Predicates describe atomic seman-
tic concepts, while variables construct networks
of relationships between them. While the impor-
tance of the predicates is obvious, the variable
assignments also exert a crucial influence on the
semantics of the conjunction: modifying a sin-
gle variable in the formula above from empty(y)
to empty(x) yields a formula that is trivially
false for all groundings (since cards can never be
empty).
Thus, our generative model must account for the
influence of both predicates and variables on the
sentences in the documents. A natural choice is to
use the predicates to influence the lexical items,
while letting the variables determine the syntac-
tic structure. For example, the formula card(x)?
freecell(y) ? empty(y) contains three pred-
icates and two variables. The predicates influence
the lexical items in a direct way: we expect that
sentences generated from this formula will include
a member of the gloss set for each predicate ?
the sentence ?Put the cards on the empty free-
cells? should be more likely than ?Columns are
constructed by playing cards in alternating colors.?
The impact of the variables on the generative
process is more subtle. The sharing of the variable
y suggests a relationship between the predicates
freecell and empty. This should be realized
in the syntactic structure of the sentence. Model-
ing syntax using a dependency tree, we expect that
the glosses for predicates that share terms will ap-
pear in compact sub-trees, while predicates that do
not share terms should be more distant. One pos-
sible surface realization of this logical formula is
the sentence, ?Put the card on the empty freecell,?
whose dependency parse is shown in the left tree
of Figure 1. The glosses empty and freecell are im-
mediately adjacent, while card is more remote.
We develop two metrics that quantify the com-
pactness of a set of variable assignments with
respect to a dependency tree: excess terms, and
shared terms. The number of excess terms in a
subtree is the number of unique terms assigned
to words in the subtree, minus the maximum arity
of any predicate in the subtree. Shared terms arise
whenever a node has multiple subtrees which each
contain the same variable. We will use the alterna-
tive alignments in Figure 1 to provide a more de-
tailed explanation. In each tree, the variables are
written in the nodes belonging to the associated
lexical items; variables are written over arrows to
indicate membership in some node in the subtree.
Excess Terms Alignment A of Fig-
ure 1, corresponding to the formula
959
Put
card on
freecell
the empty
the
X
X
X
XXX
X XX
XX
X
X
X
Y
XXY
X XY
XY
Y
X
Y
Y
XYY
X YY
YY
Y
X
Y
Z
XYZ
X YZ
YZ
Z
Dependency tree Alignment A Alignment B Alignment C Alignment D
Figure 1: A dependency parse and four different variable assignments. Each literal is aligned to a word (a
node in the graph), and the associated variables are written in the box. Variables belonging to descendant
nodes are written over the arrows.
card(x)?freecell(x)?empty(x), has zero
excess terms in every subtree; there is a total of one
variable, and all the predicates are unary. In Align-
ment B, card(x) ? freecell(x) ? empty(y),
there are excess terms at the root, and in the top
two subtrees on the right-hand side. Alignment C
contains an excess term at only the root node.
Even though it contains the same number of
unique variables as Alignment B, it is not penal-
ized as harshly because the alignment of variables
better corresponds to the syntactic structure.
Alignment D contains the greatest number of
excess terms: two at the root of the tree, and one
in each of the top two subtrees on the right side.
Shared Terms According to the excess term
metric, the best choice is simply to introduce as
few variables as possible. For this reason, we also
penalize shared terms which occur when a node
has subtree children that share a variable. In Fig-
ure 1, Alignments A and B each contain a shared
term at the top node; Alignments C and D contain
no shared terms.
Overall, we note that Alignment B is penalized
on both metrics, as it contains both excess terms
and shared terms; the syntactic structure of the
sentence makes such a variable assignment rela-
tively improbable.
card(x) & freecell(y) & empty(y)
f(y)e(y)c(x)
f(y)e(y)c(x)
Put the card on the empty freecell
(a)
(b)
(c)
(d)
(e)
Figure 2: A graphical depiction of the generative
process by which sentences are produced from for-
mulae
3 Generative Model
These intuitions are formalized in a generative
account of how sentences are stochastically pro-
duced from a set of logical formulae. This gener-
ative story guides an inference procedure for re-
covering logical formulae that are likely to have
generated any observed set of texts, which is de-
scribed in Section 4.
The outline of the generative process is depicted
in Figure 2. For each sentence, we begin in step (a)
by drawing a formula f from a Dirichlet pro-
cess (Ferguson, 1973). The Dirichlet process de-
960
fines a non-parametric mixture model, and has the
effect of adaptively selecting the appropriate num-
ber of formulae to explain the observed sentences
in the corpus.
1
We then draw the sentence length
from some distribution over positive integers; as
the sentence length is always observed, we need
not define the distribution (step (b)). In step (c), a
dependency tree is drawn from a uniform distribu-
tion over spanning trees with a number of nodes
equal to the length of the sentence. In step (d) we
draw an alignment of the literals in f to nodes in
the dependency tree, written a
t
(f). The distribu-
tion over alignments is described in Section 3.1.
Finally, the aligned literals are used to generate the
words at each slot in the dependency tree. A more
formal definition of this process is as follows:
? Draw ?, the expected number of literals per
formula, from a Gamma distribution G(u, v).
? Draw an infinite set of formulae f . For each
formula f
i
,
? Draw the formula length #|f
i
| from a
Poisson distribution, n
i
? Poisson(?).
? Draw n
i
literals from a uniform distri-
bution.
? Draw pi, an infinite multinomial distribution
over formulae: pi ? GEM(pi
0
), where GEM
refers to the stick-breaking prior (Sethura-
man, 1994) and pi
0
= 1 is the concentra-
tion parameter. By attaching the multinomial
pi to the infinite set of formulae f , we cre-
ate a Dirichlet process. This is conventionally
writtenDP (pi
0
, G
0
), where the base distribu-
tionG
0
encodes only the distribution over the
number of literals, Poisson(?).
? For each of D documents, draw the number
of sentences T ? Poisson. For each of the T
sentences in the document,
? Draw a formula f ? DP (pi
0
, G
0
) from
the Dirichlet Process described above.
? Draw a sentence length #|s| ? Poisson.
? Draw a dependency graph t (a spanning
tree of size #|s|) from a uniform distri-
bution.
? Draw an alignment a
t
(f), an injective
mapping from literals in f to nodes in
the dependency structure t. The distribu-
tion over alignments is described in Sec-
tion 3.1.
1
There are many recent applications of Dirichlet pro-
cesses in natural language processing, e.g. Goldwater et al
(2006).
? Draw the sentence s from the formula
f and the alignment a(f). For each
word token w
i
? s is drawn from
p(w
i
|a
t
(f, i)), where a
t
(f, i) indicates
the (possibly empty) literal assigned
to slot i in the alignment a
t
(f) (Sec-
tion 3.2).
3.1 Distribution over Alignments
The distribution over alignments reflects our intu-
ition that when literals share variables, they will
be aligned to word slots that are nearby in the de-
pendency structure; literals that do not share vari-
ables should be more distant. This is formalized by
applying the concepts of excess terms and shared
terms defined in Section 2. After computing the
number of excess and shared terms in each sub-
tree t
i
, we can compute a local score (LS ) for that
subtree:
LS (a
t
(f); t
i
) = ? ?NShared(a
t
(f), t
i
)
+ ? ?NExcess(a
t
(f), t
i
) ? height(t
i
).
This scoring function can be applied recursively to
each subtree in t; the overall score of the tree is the
recursive sum,
score(a
t
(f); t) = LS (a
t
(f); t)+
n
?
i
score(a
t
(f); t
i
),
(1)
where t
i
indicates the i
th
subtree of t. We hypoth-
esize a generative process that produces all possi-
ble alignments, scores them using score(a
t
(f); t),
and selects an alignment with probability,
p(a
t
(f)) ? exp{?score(a
t
(f); t)}. (2)
In our experiments, we define the parameters ? =
1, ? = 1.
3.2 Generation of Lexical Items
Once the logical formula is aligned to the parse
structure, the generation of the lexical items in
the sentence is straightforward. For word slots to
which no literals are aligned, the lexical item is
drawn from a language model ?, estimated from
the entire document collection. For slots to which
at least one literal is aligned, we construct a lan-
guage model ? in which the probability mass is
divided equally among all glosses of aligned pred-
icates. The language model ? is used as a backoff,
so that there is a strong bias in favor of generating
glosses, but some probability mass is reserved for
the other lexical items.
961
4 Inference
This section describes a sampling-based inference
procedure for obtaining a set of formulae f that
explain the observed text s and dependency struc-
tures t. We perform Gibbs sampling over the
formulae assigned to each sentence. Using the
Chinese Restaurant Process interpretation of the
Dirichlet Process (Aldous, 1985), we marginalize
pi, the infinite multinomial over all possible for-
mulae: at each sampling step we select either an
existing formula, or stochastically generate a new
formula. After each full round of Gibbs sampling,
a set of Metropolis-Hastings moves are applied to
explore modifications of the formulae. This proce-
dure converges on a stationary Markov chain cen-
tered on a set of formulae that cohere well with the
lexical and syntactic properties of the text.
4.1 Assigning Sentences to Formulae
For each sentence s
i
and dependency tree t
i
, a hid-
den variable y
i
indicates the index of the formula
that generates the text. We can resample y
i
using
Gibbs sampling. In the non-parametric setting, y
i
ranges over all non-negative integers; the Chinese
Restaurant Process formulation marginalizes the
infinite-dimensional parameter pi, yielding a prior
based on the counts for each ?active? formula (to
which at least one other sentence is assigned), and
a pseudo-count representing all non-active formu-
lae. Given K formulae, the prior on selecting for-
mula j is:
p(y
i
= j|y
?i
, pi
0
) ?
{
n
?i
(j) j < K
pi
0
j = K,
(3)
where y
?i
refers to the assignments of all y other
than y
i
and n
?i
refers to the counts over these as-
signments. Each j < K identifies an existing for-
mula in f , to which at least one other sentence is
assigned. When j = K, this means a new formula
f
?
must be generated.
To perform Gibbs sampling, we draw from the
posterior distribution over y
i
,
p(y
i
|s
i
, t
i
f , f
?
,y
?i
, pi
0
) ?
p(y
i
|y
?i
, pi
0
)p(s
i
, t
i
|y
i
, f , f
?
),
where the first term is the prior defined in Equa-
tion 3 and the latter term is the likelihood of gener-
ating the parsed sentence ?s
i
, t
i
? from the formula
indexed by y
i
.
To compute the probability of a parsed sentence
given a formula, we sum over alignments,
p(s, t|f) =
?
a
t
(f)
p(s, t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(4)
applying the chain rule and independence assump-
tions from the generative model. The result is a
product of three terms: the likelihood of the lexi-
cal items given the aligned predicates (defined in
Section 3.2; the likelihood of the alignment given
the dependency tree and formula (defined in equa-
tion 2), and the probability of the dependency tree
given the formula, which is uniform.
Equation 4 takes a sum across alignments, but
most of the probability mass of p(s|a
t
(f)) will
be concentrated on alignments in which predicates
cover words that gloss them. Thus, we can apply
an approximation,
p(s, t|f) ?
N
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(5)
in which we draw N samples in which predicates
are aligned to their glosses whenever possible.
Similarly, Equation 2 quantifies the likelihood
of an alignment only to a constant of proportional-
ity; again, a sum over possible alignments is nec-
essary. We do not expect the prior on alignments
to be strongly peaked like the sentence likelihood,
so we approximate the normalization term by sam-
pling M alignments at random and extrapolating:
p(a
t
(f)|t, f) ? q(a
t
(f); t)
=
q(a
t
(f); t)
?
a
?
t
(f)
q(a
?
t
(f); t)
?
#|a
?
t
(f)|
M
q(a
t
(f); t)
?
M
a
?
t
(f)
q(a
?
t
(f); t)
,
where q(a
t
(f); t) = exp{?score(a
t
(f); t)}, de-
fined in Equation 2. In our experiments, we set N
to at most 10, and M = 20. Drawing larger num-
bers of samples had no discernible effect on sys-
tem output.
962
4.1.1 Generating new formulae
Chinese Restaurant Process sampling requires the
generation of new candidate formulae at each re-
sampling stage. To generate a new formula, we
first sample the number of literals. As described
in the generative story (Section 3), the number
of literals is drawn from a Poisson distribution
with parameter ?. We treat ? as unknown and
marginalize, using the Gamma hyperprior G(u, v).
Due to Poisson-Gamma conjugacy, this marginal-
ization can be performed analytically, yielding
a Negative-Binomial distribution with parameters
?u+
?
i
#|f
i
|, (1+K+v)
?1
?, where
?
i
#|f
i
| is
the sum of the number of literals in each formula,
and K is the number of formulae which generate
at least one sentence. In this sense, the hyperpriors
u and v act as pseudo counts. We set u = 3, v = 1,
reflecting a weak prior expectation of three literals
per predicate.
After drawing the size of the formula, the predi-
cates are selected from a uniform random distribu-
tion. Finally, the terms are assigned: at each slot,
we reuse a previous term with probability 0.5, un-
less none is available; otherwise a new term is gen-
erated.
4.2 Proposing changes to formulae
The assignment resampling procedure has the
ability to generate new formulae, thus exploring
the space of relational features. However, to ex-
plore this space more rapidly, we introduce four
Metropolis-Hastings moves that modify existing
formulae (Gilks, 1995): adding a literal, deleting
a literal, substituting a literal, and rearranging the
terms of the formula. For each proposed move, we
recompute the joint likelihood of the formula and
all aligned sentences. The move is stochastically
accepted based on the ratio of the joint likelihoods
of the new and old configurations, multiplied by a
Hastings correction.
The joint likelihood with respect to formula f
is computed as p(s, t, f) = p(f)
?
i
p(s
i
, t
i
|f).
The prior on f considers only the number of liter-
als, using a Negative-Binomial distribution as de-
scribed in section 4.1.1. The likelihood p(s
i
, t
i
|f)
is given in equation 4. The Hastings correction is
p?(f
?
? f)/p?(f ? f
?
), with p?(f ? f
?
) indicat-
ing the probability of proposing a move from f
to f
?
,and p?(f
?
? f) indicating the probability of
proposing the reverse move. The Hastings correc-
tions depend on the arity of the predicates being
added and removed; the derivation is straightfor-
ward but tedious. We plan to release a technical
report with complete details.
4.3 Summary of inference
The final inference procedure iterates between
Gibbs sampling of assignments of formulae to
sentences, and manipulating the formulae through
Metropolis-Hastings moves. A full iteration com-
prises proposing a move to each formula, and then
using Gibbs sampling to reconsider all assign-
ments. If a formula no longer has any sentences
assigned to it, then it is dropped from the active
set, and can no longer be selected in Gibbs sam-
pling ? this is standard in the Chinese Restaurant
Process.
Five separate Markov chains are maintained in
parallel. To allow the sampling procedure to con-
verge to a stationary distribution, each chain be-
gins with 100 iterations of ?burn-in? sampling,
without storing the output. At this point, we per-
form another 100 iterations, storing the state at the
end of each iteration.
2
All formulae are ranked ac-
cording to the cumulative number of sentences to
which they are assigned (across all five Markov
chains), aggregating the counts for multiple in-
stances of identical formulae. This yields a ranked
list of formulae which will be used in our frame-
work as features for relational learning.
5 Evaluation
Our experimental setup is designed to evaluate the
quality of the semantic abstraction performed by
our model. The logical formulae obtained by our
system are applied as features for relational learn-
ing of the rules of the game of Freecell solitaire.
We investigate whether these features enable bet-
ter generalization given varying number of train-
ing examples of Freecell game states. We also
quantify the specific role of syntax, lexical choice,
and feature expressivity in learning performance.
This section describes the details of this evalua-
tion.
5.1 Relational Learning
We perform relational learning using Inductive
Logic Programming (ILP), which constructs gen-
eralized rules by assembling smaller logical for-
mulae to explain observed propositional exam-
ples (Muggleton, 1995). The lowest level formu-
lae consist of basic sensors that describe the en-
vironment. ILP?s expressivity enables it to build
complex conjunctions of these building blocks,
but at the cost of tractability. Our evaluation asks
whether the logical formulae abstracted from text
2
Sampling for more iterations was not found to affect per-
formance on development data, and the model likelihood ap-
peared stationary after 100 iterations.
963
Predicate Glosses
card(x) card
tableau(x) column, tableau
freecell(x) freecell, cell
homecell(x) foundation, cell, homecell
value(x,y) ace, king, rank, 8, 3, 7, lowest,
highest
successor(x,y) higher, sequence, sequential
color(x,y) black, red, color
suit(x,y) suit, club, diamond, spade,
heart
on(x,y) onto
top(x,y) bottom, available, top
empty(x) empty
Table 1: Predicates in the Freecell world model,
with natural language glosses obtained from the
development set text.
can transform the representation to facilitate learn-
ing. We compare against both the sensor-level rep-
resentation as well as richer representations that do
not benefit from the full power of our model?s se-
mantic analysis.
The ALEPH
3
ILP system, which is primarily
based on PROGOL (Muggleton, 1995), was used
to induce the rules of game. The search parame-
ters remained constant for all experiments.
5.2 Resources
There are four types of resources required to work
in the reading-to-learn setting: a world model, in-
structional text, a small set of glosses that map
from text to elements of the world model, and la-
beled examples of correct and incorrect actions
in the world. In our experiments, we consider
the domain of Freecell solitaire, a popular card
game (Morehead and Mott-Smith, 1983) in which
cards are moved between various types of loca-
tions, depending on their suit and rank. We now
describe the resources for the Freecell domain in
more detail.
WorldModel Freecell solitaire can be described
formally using first order logic; we consider a
slightly modified version of the representation
from the Planning Domain Definition Language
(PDDL), which is used in automatic game-playing
competitions. Specifically, there are 87 constants:
52 cards, 16 locations, 13 values, four suits, and
two colors. These constants are combined with a
fixed set of 11 predicates, listed in Table 1.
Instructional Text Our approach relies on text
that describes how to operate in the Freecell soli-
taire domain. A total of five instruction sets were
3
Freely available from http://www.comlab.ox.
ac.uk/activities/machinelearning/Aleph/
obtained from the Internet. Due to the popular-
ity of the Microsoft implementation of Freecell,
instructions often contain information specific to
playing Freecell on a computer. We manually re-
moved sentences which did not focus on the card
aspects of Freecell (e.g., how to set up the board
and information regarding where to click to move
cards). In order to use our semantic abstraction
model, the instructions were part-of-speech tagged
with the Stanford POS Tagger (Toutanova and
Manning, 2000) and dependency parses were ob-
tained using Malt (Nivre, 2006).
Glosses Our reading to learn setting requires a
small set of glosses, which are surface forms com-
monly used to represent predicates from the world
model. We envision an application scenario in
which a designer manually specifies a few glosses
for each predicate. However, for the purposes of
evaluation, it would be unprincipled for the exper-
imenters to handcraft the ideal set of glosses. In-
stead, we gathered a development set of text and
annotated the lexical mentions of the world model
predicates in text. This annotation is used to ob-
tain glosses to apply to the evaluation text. This
approximates a scenario in which the designer has
a reasonable idea of how the domain will be de-
scribed in text, but no prior knowledge of the spe-
cific details of the text instructions. Our exper-
iments used glosses that occurred two or more
times in the instructions: this yields a total of 32
glosses for 11 predicates, as shown in Table 1.
Evaluation game data Ultimately, the seman-
tic abstraction obtained from the text is applied
to learning on labeled examples of correct and
incorrect actions in the world model. For evalu-
ation, we automatically generated a set of move
scenarios: game states with one positive example
(a legal move) and one negative example (an ille-
gal move). To avoid bias in the data we generate
an equal number of move scenarios from each of
three types: moves to the freecells, homecells, and
tableaux. For our experiments we vary the number
of move scenarios in the training set; the develop-
ment and test sets consist of 900 and 1500 move
scenarios respectively.
5.3 Evaluation Settings
We compare four different feature sets, which
will be provided to the ALEPH ILP learner. All
feature sets include the sensor-level predicates
shown in Table 1. The FULL-MODEL feature
set alo includes the top logical formulae ob-
tained in our model?s semantic abstract (see Sec-
964
tion 4.3). The NO-SYNTAX feature set is obtained
from a variant of our model in which the in-
fluence of syntax is removed by setting parame-
ters ?, ? = 0. The SENSORS-ONLY feature set
uses only the sensor-level predicates. Finally, the
RELATIONAL-RANDOM feature set is constructed
by replacing each feature in the FULL-MODEL set
with a randomly generated relational feature of
identical expressivity (each predicate is replaced
by a randomly chosen alternative with identical
arity; terms are also assigned randomly). This en-
sures that any performance gains obtained by our
model were not due merely to the greater expres-
sivity of its relational features. The number of fea-
tures included in each scenario is tuned on a de-
velopment set of test examples.
The performance metric assesses the ability
of the ILP learner to classify proposed Freecell
moves as legal or illegal. As the evaluation set
contains an equal number of positive and negative
examples, accuracy is the appropriate metric. The
training scenarios are randomly generated; we re-
peat each run 50 times and average our results. For
the RELATIONAL-RANDOM feature set ? in which
predicates and terms are chosen randomly ? we
also regenerate the formulae per run.
6 Results
Table 2 shows a comparison of the results
using the setup described above. Our FULL-
MODEL achieves the best performance at ev-
ery training set size, consistently outperforming
the SENSORS-ONLY representation by an abso-
lute difference of three to four percent. This
demonstrates the semantic abstract obtained by
our model does indeed facilitate machine learning
in this domain.
RELATIONAL-RANDOM provides a baseline of
relational features with equal expressivity to those
chosen by our model, but with the predicates and
terms selected randomly. We consistently outper-
form this baseline, demonstrate that the improve-
ment obtained over the sensors only representation
is not due merely to the added expressivity of our
features.
The third row compares against NO-SYNTAX,
a crippled version of our model that incorpo-
rates lexical features but not the syntactic struc-
ture. The results are stronger than the SENSORS-
ONLY and RELATIONAL-RANDOM baselines, but
still weaker than our full system. This demon-
strates the syntactic features incorporated by our
model result in better semantic representations of
the underlying text.
Features Number of training scenarios
15 30 60 120
SENSORS-ONLY 79.12 88.07 92.77 93.73
RELATIONAL-RANDOM 82.72 89.14 93.08 94.17
NO-SYNTAX 80.98 89.79 94.11 97.04
FULL-MODEL 82.89 91.00 95.23 97.45
Table 2: Results as number of training examples
varied. Each value represents the accuracy of the
induced rules obtained with the given feature set.
card(x
1
) ? tableau(x
2
)
card(x
1
) ? freecell(x
2
)
homecell(x
1
) ? value(x
2
,x
3
)
empty(x
1
) ? freecell(x
1
)
card(x
1
) ? top(x
1
,x
2
)
card(x
1
) ? homecell(x
2
)
freecell(x
1
) ? homecell(x
2
)
card(x
1
) ? tableau(x
1
)
card(x
1
) ? top(x
2
,x
1
)
homecell(x
1
)
card(x
1
) ? homecell(x
1
)
color(x
1
,x
2
) ? value(x
3
,x
4
)
suit(x
1
,x
2
) ? value(x
3
,x
4
)
value(x
1
,x
2
) ? value(x
3
,x
4
)
homecell(x
1
) ? successor(x
2
,x
3
)
Figure 3: The top 15 features recovered by the se-
mantic abstraction of our full model.
Figure 3 shows the top 15 formulae recovered
by the full model running on the evaluation text.
Features such as empty(x
1
) ? freecell(x
1
)
are useful because they reuse variables to ensure
that objects have key properties ? in this case, en-
suring that a freecell is empty. Other features, such
as homecell(x
1
) ? value(x
2
, x
3
), help to fo-
cus the search on useful conjunctions of predicates
(in Freecell, the legality of playing a card on a
homecell depends on the value of the card). Note
that three of these 15 formulae are trivially use-
less, in that they are always false: e.g., card(x
1
)
? tableau(x
1
). This illustrates the importance
of term assignment in obtaining useful features
for learning. In the NO-SYNTAX system, which
ignores the relationship between term assignment
and syntactic structure, eight of the top 15 formu-
lae were trivially useless due to term incompatibil-
ity.
7 Related Work
This paper draws on recent literature on extract-
ing logical forms from surface text (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005; Downey
et al, 2005; Liang et al, 2009), interpreting lan-
guage in the context of a domain (Chen and
Mooney, 2008), and using an actionable domain
to guide text interpretation (Branavan et al, 2009).
We differentiate our research in several dimen-
sions:
965
Language Interpretation Instructional text de-
scribes generalized statements about entities in
the domain and the way they interact, thus the
text does not correspond directly to concrete sen-
sory inputs in the world (i.e., a specific world
state). Our interpretation captures these general-
izations as first-order logic statements that can be
evaluated given a specific state. This contrasts to
previous work which interprets ?directions? and
thus assumes a direct correspondence between text
and world state (Branavan et al, 2009; Chen and
Mooney, 2008).
Supervision Our work avoids supervision in the
form of labeled examples, using only a minimal
set of natural language glosses per predicate. Pre-
vious work also considered the supervision signal
obtained by interpreting natural language in the
context of a formal domain. Branavan et al (2009)
use feedback from a world model as a supervi-
sion signal. Chen and Mooney (2008) use tempo-
ral alignment of text and grounded descriptions of
the world state. In these approaches, concrete do-
main entities are grounded in language interpreta-
tion, and therefore require only a propositional se-
mantic representation. Previous approaches for in-
terpreting generalized natural language statements
are trained from labeled examples (Zettlemoyer
and Collins, 2005; Lu et al, 2008).
Level of analysis We aim for an abstractive
semantic summary across multiple documents,
whereas other approaches attempt to produce log-
ical forms for individual sentences (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005). We
avoid the requirement that each sentence have a
meaningful interpretation within the domain, al-
lowing us to handle relatively unstructured text.
Evaluation We do not evaluate the representa-
tions obtained by our model; rather we assess
whether these representations improve learning
performance. This is similar to work on Geo-
Query (Wong and Mooney, 2007; Ge and Mooney,
2005), and also to recent work on following step-
by-step directions (Branavan et al, 2009). While
these evaluations are performed on the basis of in-
dividual sentences, actions, or system responses,
we evaluate the holistic semantic analysis obtained
by our system.
Model We treat surface text as generated from a
latent semantic description. Lu et al (2008) ap-
ply a generative model, but require a complete
derivation from semantics to the lexical represen-
tation, while we favor a more flexible semantic
analysis that can be learned without annotation
and applied to noisy text. More similar is the work
of Liang et al (2009), which models the gener-
ation of semantically-relevant fields using lexical
and discourse features. Our approach differs by
accounting for syntax, which enables a more ex-
pressive semantic representation that includes un-
grounded variables.
Relational learning The output of our semantic
analysis is applied to learning in a structured rela-
tional space, using ILP. A key difficulty with ILP
is that the increased expressivity dramatically ex-
pands the hypothesis space, and it is widely agreed
that some learning bias is required for ILP to be
tractable (N?edellec et al, 1996; Cumby and Roth,
2003). Our work can be viewed as a new method
for acquiring such bias from text; moreover, our
approach is not specialized for ILP and may be
used to transform the feature space in other forms
of relational learning as well (Roth and Yih, 2001;
Cumby and Roth, 2003; Richardson and Domin-
gos, 2006).
8 Conclusion
This paper demonstrates a new setting for seman-
tic analysis, which we term reading to learn. We
handle text which describes the world in gen-
eral terms rather than refereing to concrete enti-
ties in the domain. We obtain a semantic abstract
of multiple documents, using a novel, minimally-
supervised generative model that accounts for both
syntax and lexical choice. The semantic abstract
is represented as a set of predicate logic formu-
lae, which are applied as higher-order features for
learning. We demonstrate that these features im-
prove learning performance, and that both the lex-
ical and syntactic aspects of our model yield sub-
stantial contributions.
In the current setup, we produce an ?overgener-
ated? semantic representation comprised of useful
features for learning but also some false positives.
Learning in our system can be seen as the process
of pruning this representation by selecting useful
formulae based on interaction with the training
data. In the future we hope to explore ways to in-
terleave semantic analysis with exploration of the
learning domain, by using the environment as a
supervision signal for linguistic analysis.
Acknowledgments We thank Gerald DeJong,
Julia Hockenmaier, Alex Klementiev and the
anonymous reviewers for their helpful feedback.
This work is supported by DARPA funding under
the Bootstrap Learning Program and the Beckman
Institute Postdoctoral Fellowship.
966
References
Aldous, David J. 1985. Exchangeability and re-
lated topics. Lecture Notes in Math 1117:1?198.
Branavan, S. R. K., Harr Chen, Luke Zettle-
moyer, and Regina Barzilay. 2009. Reinforce-
ment learning for mapping instructions to ac-
tions. In Proceedings of the Joint Conference
of the Association for Computational Linguis-
tics and International Joint Conference on Nat-
ural Language Processing Processing (ACL-
IJCNLP 2009). Singapore.
Chen, David L. and Raymond J. Mooney. 2008.
Learning to sportscast: A test of grounded lan-
guage acquisition. In Proceedings of 25th In-
ternational Conference on Machine Learning
(ICML 2008). Helsinki, Finland, pages 128?
135.
Cumby, Chad and Dan Roth. 2003. On kernel
methods for relational learning. In Proceed-
ings of the Twentieth International Conference
(ICML 2003). Washington, DC, pages 107?114.
Downey, Doug, Oren Etzioni, and Stephen Soder-
land. 2005. A probabilistic model of redun-
dancy in information extraction. In Proceedings
of the International Joint Conference on Arti-
ficial Intelligence (IJCAI 2005). pages 1034?
1041.
Ferguson, Thomas S. 1973. A bayesian analysis
of some nonparametric problems. The Annals
of Statistics 1(2):209?230.
Ge, Ruifang and Raymond J. Mooney. 2005. A
statistical semantic parser that integrates syn-
tax and semantics. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL-2005). Ann Arbor,
MI, pages 128?135.
Gilks, Walter R. 1995. Markov Chain Monte
Carlo in Practice. Chapman & Hall/CRC.
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics
(COLING-ACL 2006). Sydney, Australia, pages
673?680.
Liang, Percy, Michael Jordan, and Dan Klein.
2009. Learning semantic correspondences with
less supervision. In Proceedings of the Joint
Conference of the Association for Computa-
tional Linguistics and International Joint Con-
ference on Natural Language Processing Pro-
cessing (ACL-IJCNLP 2009). Singapore.
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning representa-
tions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2008).
Honolulu, Hawaii, pages 783?792.
Morehead, Albert H. and Geoffrey Mott-Smith.
1983. The Complete Book of Solitaire and Pa-
tience Games. Bantam.
Muggleton, Stephen. 1995. Inverse entailment and
progol. New Generation Computing Journal
13:245?286.
N?edellec, C., C. Rouveirol, H. Ad?e, F. Bergadano,
and B. Tausend. 1996. Declarative bias in ILP.
In L. De Raedt, editor, Advances in Inductive
Logic Programming, IOS Press, pages 82?103.
Nivre, Joakim. 2006. Inductive dependency pars-
ing. Springer.
Richardson, Matthew and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107?136.
Roth, Dan and Wen-tau Yih. 2001. Relational
learning via propositional algorithms: An infor-
mation extraction case study. In Proceedings of
the International Joint Conference on Artificial
Intelligence (IJCAI 2001). pages 1257?1263.
Sethuraman, Jayaram. 1994. A constructive def-
inition of dirichlet priors. Statistica Sinica
4(2):639?650.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used
in a maximum entropy part-of-speech tagger.
In Proceedings of the Joint SIGDAT Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora
(EMNLP/VLC-2000). pages 63?70.
Wong, Yuk Wah and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic
parsing with lambda calculus. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 2007). Prague,
Czech Republic, pages 128?135.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form:
Structured classification with probabilistic cat-
egorial grammars. In Proceedings of the 21st
Conference on Uncertainty in Artificial Intelli-
gence (UAI 2005). pages 658?666.
967
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 299?307,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Constraint Driven Learning For Transliteration Discovery
Ming-Wei Chang Dan Goldwasser Dan Roth Yuancheng Tu
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang21,goldwas1,danr,ytu}@uiuc.edu
Abstract
This paper introduces a novel unsupervised
constraint-driven learning algorithm for iden-
tifying named-entity (NE) transliterations in
bilingual corpora. The proposed method does
not require any annotated data or aligned cor-
pora. Instead, it is bootstrapped using a simple
resource ? a romanization table. We show that
this resource, when used in conjunction with
constraints, can efficiently identify translitera-
tion pairs. We evaluate the proposed method
on transliterating English NEs to three differ-
ent languages - Chinese, Russian and Hebrew.
Our experiments show that constraint driven
learning can significantly outperform existing
unsupervised models and achieve competitive
results to existing supervised models.
1 Introduction
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to some
target language while preserving its pronunciation in
the original language. Automatic NE transliteration
is an important component in many cross-language
applications, such as Cross-Lingual Information Re-
trieval (CLIR) and Machine Translation(MT) (Her-
mjakob et al, 2008; Klementiev and Roth, 2006a;
Meng et al, 2001; Knight and Graehl, 1998).
It might initially seem that transliteration is an
easy task, requiring only finding a phonetic mapping
between character sets. However simply matching
every source language character to its target lan-
guage counterpart is not likely to work well as in
practice this mapping depends on the context the
characters appear in and on transliteration conven-
tions which may change across domains. As a result,
current approaches employ machine learning meth-
ods which, given enough labeled training data learn
how to determine whether a pair of words consti-
tute a transliteration pair. These methods typically
require training data and language-specific expertise
which may not exist for many languages. In this pa-
per we try to overcome these difficulties and show
that when the problem is modeled correctly, a sim-
ple character level mapping is a sufficient resource.
In our experiments, English was used as the
source language, allowing us to use romanization ta-
bles, a resource commonly-available for many lan-
guages1. These tables contain an incomplete map-
ping between character sets, mapping every charac-
ter to its most common counterpart.
Our transliteration model takes a discriminative
approach. Given a word pair, the model determines
if one word is a transliteration of the other. The
features used by this model are character n-gram
matches across the two strings. For example, Fig-
ure 1 describes the decomposition of a word pair into
unigram features as a bipartite graph in which each
edge represents an active feature.
We enhance the initial model with constraints, by
framing the feature extraction process as a struc-
tured prediction problem - given a word pair, the set
of possible active features is defined as a set of latent
binary variables. The contextual dependency be-
1The romanization tables available at the Library of
Congress website (http://www.loc.gov/catdir/cpso/roman.html)
cover more than 150 languages written in various non-Roman
scripts
299
Figure 1: Top: The space of all possible features that can be
generated given the word pair. Bottom: A pruned features rep-
resentation generated by the inference process.
tween features is encoded as a set of constraints over
these variables. Features are extracted by finding
an assignment that maximizes the similarity score
between the two strings and conforms to the con-
straints. The model is bootstrapped using a roman-
ization table and uses a discriminatively self-trained
classifier as a way to improve over several training
iterations. Furthermore, when specific knowledge
about the source and target languages exists, it can
be directly injected into the model as constraints.
We tested our approach on three very differ-
ent languages - Russian, a Slavic language, He-
brew a Semitic language, and Chinese, a Sino-
Tibetan language. In all languages, using this sim-
ple resource in conjunction with constraints pro-
vided us with a robust transliteration system which
significantly outperforms existing unsupervised ap-
proaches and achieves comparable performance to
supervised methods.
The rest of the paper is organized as follows.
Sec. 2 briefly examines more related work. Sec. 3
explains our model and Sec. 4 provide a linguistic
intuition for it. Sec. 5 describes our experiments and
evaluates our results followed by sec. 6 which con-
cludes our paper.
2 Related Works
Transliteration methods typically fall into two cate-
gories: generative approaches (Li et al, 2004; Jung
et al, 2000; Knight and Graehl, 1998) that try to
produce the target transliteration given a source lan-
guage NE, and discriminative approaches (Gold-
wasser and Roth, 2008b; Bergsma and Kondrak,
2007; Sproat et al, 2006; Klementiev and Roth,
2006a), that try to identify the correct translitera-
tion for a word in the source language given several
candidates in the target language. Generative meth-
ods encounter the Out-Of-Vocabulary (OOV) prob-
lem and require substantial amounts of training data
and knowledge of the source and target languages.
Discriminative approaches, when used to for dis-
covering NE in a bilingual corpora avoid the OOV
problem by choosing the transliteration candidates
from the corpora. These methods typically make
very little assumptions about the source and target
languages and require considerably less data to con-
verge. Training the transliteration model is typi-
cally done under supervised settings (Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b), or
weakly supervised settings with additional tempo-
ral information (Sproat et al, 2006; Klementiev and
Roth, 2006a). Our work differs from these works
in that it is completely unsupervised and makes no
assumptions about the training data.
Incorporating knowledge encoded as constraints
into learning problems has attracted a lot of atten-
tion in the NLP community recently. This has been
shown both in supervised settings (Roth and Yih,
2004; Riedel and Clarke, 2006) and unsupervised
settings (Haghighi and Klein, 2006; Chang et al,
2007) in which constraints are used to bootstrap the
model. (Chang et al, 2007) describes an unsuper-
vised training of a Constrained Conditional Model
(CCM), a general framework for combining statisti-
cal models with declarative constraints. We extend
this work to include constraints over possible assign-
ments to latent variables which, in turn, define the
underlying representation for the learning problem.
In the transliteration community there are sev-
eral works (Ristad and Yianilos, 1998; Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b) that
show how the feature representation of a word pair
can be restricted to facilitate learning a string sim-
ilarity model. We follow the approach discussed
in (Goldwasser and Roth, 2008b), which considers
the feature representation as a structured prediction
problem and finds the set of optimal assignments (or
feature activations), under a set of legitimacy con-
straints. This approach stresses the importance of
interaction between learning and inference, as the
model iteratively uses inference to improve the sam-
ple representation for the learning problem and uses
the learned model to improve the accuracy of the in-
300
ference process. We adapt this approach to unsu-
pervised settings, where iterating over the data im-
proves the model in both of these dimensions.
3 Unsupervised Constraint Driven
Learning
In this section we present our Unsupervised Con-
straint Driven Learning (UCDL) model for discov-
ering transliteration pairs. Our task is in essence a
ranking task. Given a NE in the source language and
a list of candidate transliterations in the target lan-
guage, the model is supposed to rank the candidates
and output the one with the highest score. The model
is bootstrapped using two linguistic resources: a ro-
manization table and a set of general and linguistic
constraints. We use several iterations of self training
to learn the model. The details of the procedure are
explained in Algorithm 1.
In our model features are character pairs (cs, ct),
where cs ? Cs is a source word character and
ct ? Ct is a target word character. The feature
representation of a word pair vs, vt is denoted by
F (vs, vt). Each feature (cs, ct) is assigned a weight
W (cs, ct) ? R. In step 1 of the algorithm we initial-
ize the weights vector using the romanization table.
Given a pair (vs, vt), a feature extraction process
is used to determine the feature based representation
of the pair. Once features are extracted to represent
a pair, the sum of the weights of the extracted fea-
tures is the score assigned to the target translitera-
tion candidate. Unlike traditional feature extraction
approaches, our feature representation function does
not produce a fixed feature representation. In step
2.1, we formalize the feature extraction process as a
constrained optimization problem that captures the
interdependencies between the features used to rep-
resent the sample. That is, obtaining F (vs, vt) re-
quires solving an optimization problem. The techni-
cal details are described in Sec. 3.1. The constraints
we use are described in Sec. 3.2.
In step 2.2 the different candidates for every
source NE are ranked according to the similarity
score associated with their chosen representation.
This ranking is used to ?label? examples for a dis-
criminative learning process that learns increasingly
better weights, and thus improve the representation
of the pair: each source NE paired with its top
ranked transliteration is labeled as a positive exam-
ples (step 2.3) and the rest of the samples are consid-
ered as negative samples. In order to focus the learn-
ing process, we removed from the training set al
negative examples ruled-out by the constraints (step
2.4). As the learning process progresses, the initial
weights are replaced by weights which are discrimi-
natively learned (step 2.5). This process is repeated
several times until the model converges, and repeats
the same ranking over several iterations.
Input: Romanization table T : Cs ? Ct, Constraints
C, Source NEs: Vs, Target words: Vt
1. Initialize Model
Let W : Cs ? Ct ? R be a weight vector.
Initialize W using T by the following procedure
?(cs, ct), (cs, ct) ? T ? W(cs, ct) = 0,
?(cs, ct),?((cs, ct) ? T ) ?W(cs, ct) = ?1,
?cs,W(cs, ) = ?1, ?ct,W( , ct) = ?1.
2. Constraints driven unsupervised training
while not converged do
1. ?vs ? Vs, vt ? Vt, use C and W
to generate a representation F (vs, vt)
2. ?vs ? Vs, find the top ranking transliteration
pair (vs, v?t ) by solving
v?t = argmaxvt score(F (vs, vt)).
3. D = {(+, F (vs, v?t )) | ?vs ? Vs}.
4. ?vs ? Vs, vt ? Vt, if vt 6= v?t and
score(F (vs, vt)) 6= ??, then
D = D ? {(?, F (vs, vt))}.
5. W ? train(D)
end
Algorithm 1: UCDL Transliteration Framework.
In the rest of this section we explain this process
in detail. We define the feature extraction inference
process in Sec. 3.1, the constraints used in Sec. 3.2
and the inference algorithm in Sec. 3.3. The linguis-
tic intuition for our model is described in Sec. 4.
3.1 Finding Feature Representation as
Constrained Optimization
We use the formulation of Constrainted Conditional
Models (CCMs) (Roth and Yih, 2004; Roth and Yih,
2007; Chang et al, 2008). Previous work on CCM
models dependencies between different decisions in
structured prediction problems. Transliteration dis-
covery is a binary classification problem, however,
301
the underlying representation of each sample can be
modeled as a CCM, defined as a set of latent vari-
ables corresponding to the set of all possible features
for a given sample. The dependencies between the
features are captured using constraints.
Given a word pair, the set of all possible features
consists of all character mappings from the source
word to the target word. Since in many cases the
size of the words differ we augment each of the
words with a blank character (denoted as ? ?). We
model character omission by mapping the character
to the blank character. This process is formally de-
fined as an operator mapping a transliteration can-
didate pair to a set of binary variables, denoted as
All-Features (AF ).
AF = {(cs, ct)|cs ? vs ? { }, ct ? vt ? { }}
This representation is depicted at the top of Figure 1.
The initial sample representation (AF ) gener-
ates features by coupling substrings from the two
terms without considering the dependencies be-
tween the possible combinations. This representa-
tion is clearly noisy and in order to improve it we
select a subset F ? AF of the possible features.
The selection process is formulated as a linear op-
timization problem over binary variables encoding
feature activations in AF . Variables assigned 1 are
selected to be in F , and those assigned 0 are not.
The objective function maximized is a linear func-
tion over the variables in AF , each with its weight as
a coefficient, as in the left part of Equation 1 below.
We seek to maximize this linear sum subject to a set
of constraints. These represent the dependencies be-
tween selections and prior knowledge about possible
legitimate character mappings and correspond to the
right side of Equation 1. In our settings only hard
constraints are used and therefore the penalty (?) for
violating any of the constraints is set to ?. The spe-
cific constraints used are discussed in Sec. 3.2. The
score of the mapping F (vs, vt) can be written as fol-
lows:
1
|vt|
(W ? F (vs, vt)?
?
ci?C
?ci(F (vs, vt)) (1)
We normalize this score by dividing it by the size of
the target word, since the size of candidates varies,
normalization improved the ranking of candidates.
The result of the optimization process is a set F of
active features, defined in Equation 2. The result of
this process is described at the bottom of Figure 1.
F ?(vs, vt) = argmaxF?AF (vs,vt)score(F ). (2)
The ranking process done by our model can now be
naturally defined - given a source word vs, and a
set of candidates target words v0t , . . . , vnt , find the
candidate whose optimal representation maximizes
Equation 1. This process is defined in Equation 3.
v?t = argmaxvit
score(F (vs, vit)). (3)
3.2 Incorporating Mapping Constraints
We consider two types of constraints: language spe-
cific and general constraints that apply to all lan-
guages. Language specific constraints typically im-
pose a local restriction such as individually forcing
some of the possible character mapping decisions.
The linguistic intuition behind these constraints is
discussed in Section 4. General constraints encode
global restrictions, capturing the dependencies be-
tween different mapping decisions.
General constraints: To facilitate readability we
denote the feature mapping the i-th source word
character to the j-th target word character as a
Boolean variable aij that is 1 if that feature is active
and 0 otherwise.
? Coverage - Every character must be mapped
only to a single character or to the blank char-
acter. We can formulate this as: ?j aij = 1
and ?i aij = 1.
? No Crossing - Every character mapping, except
mapping to blank character, should preserve the
order of appearance in the source and target
words, or formally - ?i, j s.t. aij = 1 ? ?l <
i, ?k > j, alk = 0. Another constraint is ?i, j
s.t. aij = 1 ? ?l > i, ?k < j, alk = 0.
Language specific constraints
? Restricted Mapping: These constraints restrict
the possible local mappings between source
and target language characters. We maintain a
set of possible mappings {cs ? ?cs}, where
?cs ? Ct and {ct ? ?ct}, where ?ct ? Cs.
Any feature (cs, ct) such that cs /? ?ct or
ct /? ?cs is penalized in our model.
302
? Length restriction: An additional constraint
restricts the size difference between the two
words. We formulate this as follows: ?vs ?
Vs,?vt ? Vt, if ?|vt| > |vs| and ?|vs| > |vt|,
score(F (vs, vt)) = ??. Although ? can
take different values for different languages, we
simply set ? to 2 in this paper.
In addition to biasing the model to choose the
right candidate, the constraints also provide a com-
putational advantage: a given a word pair is elimi-
nated from consideration when the length restriction
is not satisfied or there is no way to satisfy the re-
stricted mapping constraints.
3.3 Inference
The optimization problem defined in Equation 2 is
an integer linear program (ILP). However, given
the structure of the problem it is possible to de-
velop an efficient dynamic programming algorithm
for it, based on the algorithm for finding the mini-
mal edit distance of two strings. The complexity of
finding the optimal set of features is only quadratic
in the size of the input pair, a clear improvement
over the ILP exponential time algorithm. The al-
gorithm minimizes the weighted edit distance be-
tween the strings, and produces a character align-
ment that satisfies the general constraints (Sec. 3.2).
Our modifications are only concerned with incorpo-
rating the language-specific constraints into the al-
gorithm. This can be done simply by assigning a
negative infinity score to any alignment decision not
satisfying these constraints.
4 Bootstrapping with Linguistic
Information
Our model is bootstrapped using two resources - a
romanization table and mapping constraints. Both
resources capture the same information - character
mapping between languages. The distinction be-
tween the two represents the difference in the con-
fidence we have in these resources - the romaniza-
tion table is a noisy mapping covering the character
set and is therefore better suited as a feature. Con-
straints, represented by pervasive, correct character
mapping, indicate the sound mapping tendency be-
tween source and target languages. For example,
certain n-gram phonemic mappings, such as r ? l
from English to Chinese, are language specific and
can be captured by language specific sound change
patterns.
Phonemes Constraints
Vowel i ? y; u ? w; a ? a
Nasal m ? m; m,n ? m
Approximant
r ? l; l, r ? l
l ? l; w ? h,w, f
h, o, u, v ? w; y ? y
Fricative v ? w, b, fs ? s, x, z; s, c ? s
Plosive
p ? b, p; p ? p
b ? b; t ? t
t, d ? d; q ? k
Table 1: All language specific constraints used in our English
to Chinese transliteration (see Sec. 3.2 for more details). Con-
straints in boldface apply to all positions, the rest apply only to
characters appearing in initial position.
These patterns have been used by other systems
as features or pseudofeatures (Yoon et al, 2007).
However, in our system these language specific rule-
of-thumbs are systematically used as constraints to
exclude impossible alignments and therefore gener-
ate better features for learning. We listed in Table 1
all 20 language specific constraints we used for Chi-
nese. There is a total of 24 constraints for Hebrew
and 17 for Russian.
The constraints in Table 1 indicate a systematic
sound mapping between English and Chinese un-
igram character mappings. Arranged by manners
of articulation each row of the table indicates the
sound change tendency among vowels, nasals, ap-
proximants (retroflex and glides), fricatives and plo-
sives. For example, voiceless plosive sounds such as
p, t in English, tend to map to both voiced (such as b,
d) and voiceless sounds in Chinese. However, if the
sound is voiceless in Chinese, its backtrack English
sound must be voiceless. This voice-voiceless sound
change tendency is captured by our constraints such
as p ? b, p and p ? p; t ? t.
5 Experiments and Analysis
In this section, we demonstrate the effectiveness
of constraint driven learning empirically. We start
by describing the datasets and experimental settings
and then proceed to describe the results. We eval-
uated our method on three very different target lan-
303
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  2  4  6  8  10  12  14  16  18  20
A
C
C
Number of Rounds
[KR 06] + temporal information[KR 06] All Cons. + unsupervsied learning
Figure 2: Comparison between our models and weakly su-
pervised learning methods (Klementiev and Roth, 2006b).
Note that one of the models proposed in (Klementiev and Roth,
2006b) takes advantage of the temporal information. Our best
model, the unsupervised learning with all constraints, outper-
forms both models in (Klementiev and Roth, 2006b), even
though we do not use any temporal information.
guages: Russian, Chinese, and Hebrew, and com-
pared our results to previously published results.
5.1 Experimental Settings
In our experiments the system is evaluated on its
ability to correctly identify the gold transliteration
for each source word. We evaluated the system?s
performance using two measures adopted in many
transliteration works. The first one is Mean Recip-
rocal Rank (MRR), used in (Tao et al, 2006; Sproat
et al, 2006), which is the average of the multiplica-
tive inverse of the rank of the correct answer. For-
mally, Let n be the number of source NEs. Let Gol-
dRank(i) be the rank the algorithm assigns to the
correct transliteration. Then, MRR is defined by:
MRR = 1n
n?
i=1
1
goldRank(i) .
Another measure is Accuracy (ACC) used in (Kle-
mentiev and Roth, 2006a; Goldwasser and Roth,
2008a), which is the percentage of the top rank can-
didates being the gold transliteration. In our im-
plementation we used the support vector machine
(SVM) learning algorithm with linear kernel as our
underlying learning algorithm (mentioned in part
2.5 of Algorithm 1) . We used the package LIB-
LINEAR (Hsieh et al, 2008) in our experiments.
Through all of our experiments, we used the 2-norm
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 0  2  4  6  8  10  12  14  16  18  20
M
R
R
Number of Rounds
[GR 08] 250 labeled ex. with cons[GR 08] 250 labeled ex. w/o consGeneral cons + unsupervised learningAll cons. + unsupervised learning
Figure 3: Comparison between our works and supervised
models in (Goldwasser and Roth, 2008b). We show the learn-
ing curves for Hebrew under two different settings: unsuper-
vised learning with general and all constraints. The results of
two supervised models (Goldwasser and Roth, 2008b) are also
included here. Note that our unsupervised model with all con-
straints is competitive to the supervised model with 250 labeled
examples. See the text for more comparisons and details.
hinge loss as our loss function and fixed the regular-
ization parameter C to be 0.5.
5.2 Datasets
We experimented using three different target lan-
guages Russian, Chinese, and Hebrew. We used En-
glish as the source language in all these experiments.
The Russian data set2, originally introduced in
(Klementiev and Roth, 2006b), is comprised of tem-
porally aligned news articles. The dataset contains
727 single word English NEs with a correspond-
ing set of 50,648 potential Russian candidate words
which include not only name entities, but also other
words appearing in the news articles.
The Chinese dataset is taken directly from an
English-Chinese transliteration dictionary, derived
from LDC Gigaword corpus3. The entire dictionary
consists of 74,396 pairs of English-Chinese NEs,
where Chinese NEs are written in Pinyin, a roman-
ized spelling system of Chinese. In (Tao et al, 2006)
a dataset which contains about 600 English NEs and
700 Chinese candidates is used. Since the dataset
is not publicly available, we created a dataset in a
similar way. We randomly selected approximately
600 NE pairs and then added about 100 candidates
which do not correspond to any of the English NE
2The corpus is available http://L2R.cs.uiuc.edu/?cogcomp.
3http://www.ldc.upenn.edu
304
Language UCDL Prev. works
Rus. (ACC) 73 63 (41) (KR?06)
Heb. (MRR) 0.899 0.894 (GR?08)
Table 2: Comparison to previously published results. UCDL
is our method, KR?06 is described in (Klementiev and Roth,
2006b) and GR?08 in (Goldwasser and Roth, 2008b). Note that
our results for Hebrew are comparable with a supervised sys-
tem.
previously selected.
The Hebrew dataset, originally introduced in
(Goldwasser and Roth, 2008a), consists of 300
English-Hebrew pairs extracted from Wikipedia.
5.3 Results
We begin by comparing our model to previously
published models tested over the same data, in two
different languages, Russian and Hebrew. For Rus-
sian, we compare to the model presented in (Kle-
mentiev and Roth, 2006b), a weakly supervised al-
gorithm that uses both phonetic information and
temporal information. The model is bootstrapped
using a set of 20 labeled examples. In their setting
the candidates are ranked by combining two scores,
one obtained using the transliteration model and a
second by comparing the relative occurrence fre-
quency of terms over time in both languages. Due
to computational tractability reasons we slightly
changed Algorithm 1 to use only a small subset of
the possible negative examples.
For Hebrew, we compare to the model presented
in (Goldwasser and Roth, 2008b), a supervised
model trained using 250 labeled examples. This
model uses a bigram model to represent the translit-
eration samples (i.e., features are generated by pair-
ing character unigrams and bigrams). The model
also uses constraints to restrict the feature extrac-
tion process, which are equivalent to the coverage
constraint we described in Sec. 3.2.
The results of these experiments are reported us-
ing the evaluation measures used in the original pa-
pers and are summarized in Table 2. The results
show a significant improvement over the Russian
data set and comparable performance to the super-
vised method used for Hebrew.
Figure 2 describes the learning curve of our
method over the Russian dataset. We compared our
algorithm to two models described in (Klementiev
and Roth, 2006b) - one uses only phonetic simi-
larity and the second also considers temporal co-
occurrence similarity when ranking the translitera-
tion candidates. Both models converge after 50 it-
erations. When comparing our model to their mod-
els, we found that even though our model ignores
the temporal information it achieves better results
and converges after fewer iterations. Their results
report a significant improvement when using tempo-
ral information - improving an ACC score of 41%
without temporal information to 63% when using
it. Since the temporal information is orthogonal to
the transliteration model, our model should similarly
benefit from incorporating the temporal information.
Figure 3 compares the learning curve of our
method to an existing supervised method over the
Hebrew data and shows we get comparable results.
Unfortunately, we could not find a published Chi-
nese dataset. However, our system achieved similar
results to other systems, over a different dataset with
similar number of training examples. For example,
(Sproat et al, 2006) presents a supervised system
that achieves a MRR score of 0.89, when evaluated
over a dataset consisting of 400 English NE and 627
Chinese words. Our results for a different dataset of
similar size are reported in Table 3.
5.4 Analysis
The resources used in our framework consist of
- a romanization table, general and language spe-
cific transliteration constraints. To reveal the impact
of each component we experimented with different
combination of the components, resulting in three
different testing configurations.
Romanization Table: We initialized the weight
vector using a romanization table and did not use any
constraints. To generate features we use a modified
version of our AF operator (see Sec. 3), which gen-
erates features by coupling characters in close posi-
tions in the source and target words. This configura-
tion is equivalent to the model used in (Klementiev
and Roth, 2006b).
+General Constraints: This configuration uses the
romanization table for initializing the weight vector
and general transliteration constraints (see Sec. 3.2)
for feature extraction.
+All Constraints: This configuration uses lan-
guage specific constraints in addition to the gen-
305
Settings Chinese Russian Hebrew
Romanization table 0.019 (0.5) 0.034 (1.0) 0.046 (1.7)
Romanization table +learning 0.020 (0.3) 0.048 (1.3) 0.028 (0.7)
+Gen Constraints 0.746 (67.1) 0.809 (74.3) 0.533 (45.0)
+Gen Constraints +learning 0.867 (82.2) 0.906 (86.7) 0.834 (76.0)
+All Constraints 0.801 (73.4) 0.849 (79.3) 0.743 (66.0)
+All Constraints +learning 0.889 (84.7) 0.931 (90.0) 0.899 (85.0)
Table 3: Results of an ablation study of unsupervised method for three target languages. Results for ACC are inside parentheses,
and for MRR outside. When the learning algorithm is used, the results after 20 rounds of constraint driven learning are reported.
Note that using linguistic constraints has a significant impact in the English-Hebrew experiments. Our results show that a small
amount of constraints can go a long way, and better constraints lead to better learning performance.
eral transliteration constraints to generate the feature
representation. (see Sec. 4).
+Learning: Indicates that after initializing the
weight vector, we update the weight using Algo-
rithm 1. In all of the experiments, we report the
results after 20 training iterations.
The results are summarized in Table 3. Due to the
size of the Russian dataset, we used a subset consist-
ing of 300 English NEs and their matching Russian
transliterations for the analysis presented here. Af-
ter observing the results, we discovered the follow-
ing regularities for all three languages. Using the
romanization table directly without constraints re-
sults in very poor performance, even after learning.
This can be used as an indication of the difficulty of
the transliteration problem and the difficulties ear-
lier works have had when using only romanization
tables, however, when used in conjunction with con-
straints results improve dramatically. For example,
in the English-Chinese data set, we improve MRR
from 0.02 to 0.746 and for the English-Russian data
set we improve 0.03 to 0.8. Interestingly, the results
for the English-Hebrew data set are lower than for
other languages - we achieve 0.53 MRR in this set-
ting. We attribute the difference to the quality of
the mapping in the romanization table for that lan-
guage. Indeed, the weights learned after 20 train-
ing iterations improve the results to 0.83. This im-
provement is consistent across all languages, after
learning we are able to achieve a MRR score of 0.87
for the English-Chinese data set and 0.91 for the
English-Russian data set. These results show that
romanization table contains enough information to
bootstrap the model when used in conjunction with
constraints. We are able to achieve results compa-
rable to supervised methods that use a similar set of
constraints and labeled examples.
Bootstrapping the weight vector using language
specific constraints can further improve the results.
They provide several advantages: a better starting
point, an improved learning rate and a better final
model. This is clear in all three languages, for exam-
ple results for the Russian and Chinese bootstrapped
models improve by 5%, and by over 20% for He-
brew. After training the difference is smaller- only
3% for the first two and 6% for Hebrew. Figure 3 de-
scribes the learning curve for models with and with-
out language specific constraints for the English-
Hebrew data set, it can be observed that using these
constraints the model converges faster and achieves
better results.
6 Conclusion
In this paper we develop a constraints driven ap-
proach to named entity transliteration. In doing it
we show that romanization tables are a very useful
resource for transliteration discovery if proper con-
straints are included. Our framework does not need
labeled data and does not assume that bilingual cor-
pus are temporally aligned. Even without using any
labeled data, our model is competitive to existing
supervised models and outperforms existing weakly
supervised models.
7 Acknowledgments
We wish to thank the reviewers for their insightful
comments. This work is partly supported by NSF
grant SoD-HCER-0613885 and DARPA funding un-
der the Bootstrap Learning Program.
306
References
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 656?663, Prague, Czech Republic,
June. Association for Computational Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 280?287, Prague,
Czech Republic, Jun. Association for Computational
Linguistics.
M. Chang, L. Ratinov, N. Rizzolo, and D. Roth. 2008.
Learning and inference with constraints. In Proc.
of the National Conference on Artificial Intelligence
(AAAI), July.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), June.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In Proc. of the Conference
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 353?362, Oct.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. of the Annual Meet-
ing of the North American Association of Computa-
tional Linguistics (NAACL).
U. Hermjakob, K. Knight, and H. Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 389?397, Columbus, Ohio, June.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In ICML
?08: Proceedings of the 25th international conference
on Machine learning, pages 408?415, New York, NY,
USA. ACM.
S. Jung, S. Hong, and E. Paek. 2000. An english to
korean transliteration model of extended markov win-
dow. In Proc. the International Conference on Com-
putational Linguistics (COLING), pages 383?389.
A. Klementiev and D. Roth. 2006a. Named entity
transliteration and discovery from multilingual com-
parable corpora. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 82?88, June.
A. Klementiev and D. Roth. 2006b. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages USS,TL,ADAPT, July.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, pages 599?612.
H. Li, M. Zhang, and J. Su. 2004. A joint source-channel
model for machine transliteration. In Proc. of the An-
nual Meeting of the Association of Computational Lin-
guistics (ACL), pages 159?166, Barcelona, Spain, July.
H. Meng, W. Lo, B. Chen, and K. Tang. 2001.
Generating phonetic cognates to handle named en-
tities in english-chinese cross-langauge spoken doc-
ument retreival. In Proceedings of the Automatic
Speech Recognition and Understanding Workshop,
pages 389?397.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 129?
137, Sydney, Australia.
E. S. Ristad and P. N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recogni-
tion and Machine Intelligence, 20(5):522?532, May.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
pages 1?8. Association for Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
R. Sproat, T. Tao, and C. Zhai. 2006. Named entity
transliteration with comparable corpora. In Proc. of
the Annual Meeting of the Association of Computa-
tional Linguistics (ACL), pages 73?80, Sydney, Aus-
tralia, July.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entitly transliteration using tem-
poral and phonetic correlation. In Proc. of the Con-
ference on Empirical Methods for Natural Language
Processing (EMNLP), pages 250?257.
S. Yoon, K. Kim, and R. Sproat. 2007. Multilingual
transliteration using feature based phonetic method.
In Proc. of the Annual Meeting of the Association
of Computational Linguistics (ACL), pages 112?119,
Prague, Czech Republic, June.
307
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 53?56,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Active Sample Selection for Named Entity Transliteration
Dan Goldwasser Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61801
{goldwas1,danr}@uiuc.edu
Abstract
This paper introduces a new method for
identifying named-entity (NE) transliterations
within bilingual corpora. Current state-of-the-
art approaches usually require annotated data
and relevant linguistic knowledge which may
not be available for all languages. We show
how to effectively train an accurate transliter-
ation classifier using very little data, obtained
automatically. To perform this task, we intro-
duce a new active sampling paradigm for guid-
ing and adapting the sample selection process.
We also investigate how to improve the clas-
sifier by identifying repeated patterns in the
training data. We evaluated our approach us-
ing English, Russian and Hebrew corpora.
1 Introduction
This paper presents a new approach for constructing
a discriminative transliteration model.
Our approach is fully automated and requires little
knowledge of the source and target languages.
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to a target
language based on phonetic similarity between the
entities. Figure 1 provides examples of NE translit-
erations in English Russian and Hebrew.
Identifying transliteration pairs is an important
component in many linguistic applications such as
machine translation and information retrieval, which
require identifying out-of-vocabulary words.
In our settings, we have access to source language
NE and the ability to label the data upon request.
We introduce a new active sampling paradigm that
Figure 1: NE in English, Russian and Hebrew.
aims to guide the learner toward informative sam-
ples, allowing learning from a small number of rep-
resentative examples. After the data is obtained it is
analyzed to identify repeating patterns which can be
used to focus the training process of the model.
Previous works usually take a generative approach,
(Knight and Graehl, 1997). Other approaches ex-
ploit similarities in aligned bilingual corpora; for ex-
ample, (Tao et al, 2006) combine two unsupervised
methods. (Klementiev and Roth, 2006) bootstrap
with a classifier used interchangeably with an un-
supervised temporal alignment method. Although
these approaches alleviate the problem of obtain-
ing annotated data, other resources are still required,
such as a large aligned bilingual corpus.
The idea of selectively sampling training samples
has been wildly discussed in machine learning the-
ory (Seung et al, 1992) and has been applied suc-
cessfully to several NLP applications (McCallum
and Nigam, 1998). Unlike other approaches,our ap-
proach is based on minimizing the distance between
the feature distribution of a comprehensive reference
set and the sampled set.
2 Training a Transliteration Model
Our framework works in several stages, as summa-
rized in Algorithm 1. First, a training set consisting
53
of NE transliteration pairs (ws, wt) is automatically
generated using an active sample selection scheme.
The sample selection process is guided by the Suf-
ficient Spanning Features criterion (SSF) introduced
in section 2.2, to identify informative samples in the
source language.An oracle capable of pairing a NE
in the source language with its counterpart in the tar-
get language is then used. Negative training samples
are generated by reshuffling the terms in these pairs.
Once the training data has been collected, the data
is analyzed to identify repeating patterns in the data
which are used to focus the training process by as-
signing weights to features corresponding to the ob-
served patterns. Finally, a linear model is trained us-
ing a variation of the averaged perceptron (Freund
and Schapire, 1998) algorithm. The remainder of
this section provides details about these stages; the
basic formulation of the transliteration model and
the feature extraction scheme is described in section
2.1, in section 2.2 the selective sampling process is
described and finally section 2.3 explains how learn-
ing is focused by using feature weights.
Input: Bilingual, comparable corpus (S , T ), set of
named entities NES from S, Reference
Corpus RS , Transliteration Oracle O,
Training Corpora D=DS ,DT
Output: Transliteration model M
Guiding the Sampling Process1
repeat2
select a set C ? NES randomly3
ws = argminw?Cdistance(R,DS ? {ws})4
D = D ? {Ws, O(Ws)}5
until distance(R,DS ? {Ws}) ? distance(R,DS) ;6
Determining Features Activation Strength7
Define W:f ? < s.t. foreach feature f ={fs, ft}8
W (f) = ](fs,ft)](fs) ?
](fs,ft)
](ft)9
Use D to train M;10
Algorithm 1: Constructing a transliteration
model.
2.1 Transliteration Model
Our transliteration model takes a discriminative ap-
proach; the classifier is presented with a word pair
(ws, wt) , where ws is a named entity and it is
asked to determine whether wt is a transliteration
Figure 2: Features extraction process
of the NE in the target language. We use a linear
classifier trained with a regularized perceptron up-
date rule (Grove and Roth, 2001) as implemented
in SNoW, (Roth, 1998). The classifier?s confi-
dence score is used for ranking of positively tagged
transliteration candidates. Our initial feature extrac-
tion scheme follows the one presented in (Klemen-
tiev and Roth, 2006), in which the feature space con-
sists of n-gram pairs from the two languages. Given
a sample, each word is decomposed into a set of sub-
strings of up to a given length (including the empty
string). Features are generated by pairing substrings
from the two sets whose relative positions in the
original words differ by one or less places; first each
word is decomposed into a set of substrings then
substrings from the two sets are coupled to complete
the pair representation. Figure 2 depicts this process.
2.2 Guiding the Sampling Process with SSF
The initial step in our framework is to generate a
training set of transliteration pairs; this is done by
pairing highly informative source language candi-
date NEs with target language counterparts. We de-
veloped a criterion for adding new samples, Suffi-
ciently Spanning Features (SSF), which quantifies
the sampled set ability to span the feature space.
This is done by evaluating the L-1 distance be-
tween the frequency distributions of source language
word fragments in the current sampled set and in
a comprehensive set of source language NEs, serv-
ing as reference. We argue that since the features
used for learning are n-gram features, once these
two distributions are close enough, our examples
space provides a good and concise characterization
of all named entities we will ever need to con-
sider. A special care should be given to choos-
ing an appropriate reference; as a general guide-
line the reference set should be representative of
the testing data. We collected a set R, consisting
54
of 50,000 NE by crawling through Wikipedia?s arti-
cles and using an English NER system available at
- http://L2R.cs.uiuc.edu/ cogcomp. The frequency
distribution was generated over all character level
bi-grams appearing in the text, as bi-grams best cor-
relate with the way features are extracted. Given a
reference text R, the n-grams distribution of R can be
defined as follows -DR(ngi) = ]ngi?
j ]ngj
,where ng
is an n-gram in R. Given a sample set S, we measure
the L1 distance between the distributions:
distance (R,S) =?ng?R | DR(ng)?DS(ng) | Sam-
ples decreasing the distance between the distribu-
tions were added to the training data. Given a set
C of candidates for annotation, a sample ws ? C
was added to the training set, if -
ws = argminw?Cdistance(R,DS ? {ws}).
A sample set is said to have SSF, if the distance re-
mains constant as more samples are added.
2.2.1 Transliteration Oracle Implementation
The transliteration oracle is essentially a mapping
between the named entities, i.e. given an NE in the
source language it provides the matching NE in the
target language. An automatic oracle was imple-
mented by crawling through Wikipedia topic aligned
document pairs. Given a pair of topic aligned doc-
uments in the two languages, the topic can be iden-
tified either by identifying the top ranking terms or
by simply identifying the title of the documents. By
choosing documents in Wikipedia?s biography cate-
gory we ensured that the topic of the documents is
person NE.
2.3 Training the transliteration model
The feature extraction scheme we use generates fea-
tures by coupling substrings from the two terms.
Ideally, given a positive sample, it is desirable that
paired substrings would encode phonetically simi-
lar or a distinctive context in which the two scripts
correlate. Given enough positive samples, such fea-
tures will appear with distinctive frequency. Tak-
ing this idea further, these features were recognized
by measuring the co-occurrence frequency of sub-
strings of up to two characters in both languages.
Each feature f=(fs, ft) composed of two substrings
taken from English and Hebrew words was associ-
ated with weight. W (f) = ](fs,ft)](fs) ?
](fs,ft)
](ft) where
Data Set Method Rus Heb
1 SSF 0.68 NA
1 KR?06 0.63 NA
2 SSF 0.71 0.52
Table 1: Results summary. The numbers are the pro-
portion of NE recognized in the target language. Lines 1
and 2 compare the results of SSF directed approach with
the baseline system on the first dataset. Line 3 summa-
rizes the results on the second dataset.
](fs, ft) is the number of occurrences of that feature
in the positive sample set, and ](fL) is the number of
occurrences of an individual substring, in any of the
features extracted from positive samples in the train-
ing set. The result of this process is a weight table,
in which, as we empirically tested, the highest rank-
ing weights were assigned to features that preserve
the phonetic correlation between the two languages.
To improve the classifier?s learning rate, the learn-
ing process is focused around these features. Given
a sample, the learner is presented with a real-valued
feature vector instead of a binary vector, in which
each value indicates both that the feature is active
and its activation strength - i.e. the weight assigned
to it.
3 Evaluation
We evaluated our approach in two settings; first, we
compared our system to a baseline system described
in (Klementiev and Roth, 2006). Given a bilingual
corpus with the English NE annotated, the system
had to discover the NE in target language text. We
used the English-Russian news corpus used in the
baseline system. NEs were grouped into equiva-
lence classes, each containing different variations of
the same NE. We randomly sampled 500 documents
from the corpus. Transliteration pairs were mapped
into 97 equivalence classes, identified by an expert.
In a second experiment, different learning parame-
ters such as selective sampling efficiency and feature
weights were checked. 300 English-Russian and
English-Hebrew NE pairs were used; negative sam-
ples were generated by coupling every English NE
with all other target language NEs. Table 1 presents
the key results of these experiments and compared
with the baseline system.
55
Extraction Number Recall Recall
method of Top one Top two
samples
Directed 200 0.68 0.74
Random 200 0.57 0.65
Random 400 0.63 0.71
Table 2: Comparison of correctly identified English-
Russian transliteration pairs in news corpus. The model
trained using selective sampling outperforms models
trained using random sampling, even when trained with
twice the data. The top one and top two results
columns describe the proportion of correctly identified
pairs ranked in the first and top two places, respectively.
3.1 Using SSF directed sampling
Table 2 describes the effect of directed sampling
in the English-Russian news corpora NE discovery
task. Results show that models trained using selec-
tive sampling can outperform models trained with
more than twice the amount of data.
3.2 Training using feature weights
Table 3 describes the effect training the model with
weights.The training set consisted of 150 samples
extracted using SSF directed sampling. Three varia-
tions were tested - training without feature weights,
using the feature weights as the initial network
weights without training and training with weights.
The results clearly show that using weights for train-
ing improve the classifier?s performance for both
Russian and Hebrew. It can also be observed that
in many cases the correct pair was ranked in any of
the top five places.
4 Conclusions and future work
In this paper we presented a new approach for con-
structing a transliteration model automatically and
efficiently by selectively extracting transliteration
samples covering relevant parts of the feature space
and focusing the learning process on these features.
We show that our approach can outperform sys-
tems requiring supervision, manual intervention and
a considerable amount of data. We propose a new
measure for selective sample selection which can be
used independently. We currently investigate apply-
ing it in other domains with potentially larger feature
Learning Russian Hebrew
Train- Feature Top Top Top Top
ing weights one five one five
+ + 0.71 0.89 0.52 0.88
- + 0.63 0.82 0.33 0.59
+ - 0.64 0.79 0.37 0.68
Table 3: The proportion of correctly identified transliter-
ation pairs with/out using weights and training. The top
one and top five results columns describe the proportion
of correctly identified pairs ranked in the first place and
in any of the top five places, respectively. The results
demonstrate that using feature weights improves perfor-
mance for both target languages.
space than used in this work. Another aspect inves-
tigated is using our selective sampling for adapting
the learning process for data originating from dif-
ferent sources; using the a reference set representa-
tive of the testing data, training samples, originating
from a different source , can be biased towards the
testing data.
5 Acknowledgments
Partly supported by NSF grant ITR IIS-0428472 and
DARPA funding under the Bootstrap Learning Pro-
gram.
References
Y. Freund and R. E. Schapire. 1998. Large margin clas-
sification using the perceptron algorithm. In COLT.
A. Grove and D. Roth. 2001. Linear concepts and hidden
variables. ML, 42.
A. Klementiev and D. Roth. 2006. Weakly supervised
named entity transliteration and discovery from multi-
lingual comparable corpora. In ACL.
K. Knight and J. Graehl. 1997. Machine transliteration.
In EACL.
D. K. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification. In
ICML.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In AAAI.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entity transliteration using tem-
poral and phonetic correlation. In EMNLP.
56
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 655?663,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
?I Object!? Modeling Latent Pragmatic Effects in Courtroom Dialogues
Dan Goldwasser
University of Maryland
Institute for Advanced Computer Studies
College Park, MD , USA
goldwas1@umiacs.edu
Hal Daum
?
e III
Department of Computer Science
University of Maryland
College Park, MD , USA
hal@cs.umd.edu
Abstract
Understanding the actionable outcomes of
a dialogue requires effectively modeling
situational roles of dialogue participants,
the structure of the dialogue and the rele-
vance of each utterance to an eventual ac-
tion. We develop a latent-variable model
that can capture these notions and apply
it in the context of courtroom dialogues,
in which the objection speech act is used
as binary supervision to drive the learning
process. We demonstrate quantitatively
and qualitatively that our model is able to
uncover natural discourse structure from
this distant supervision.
1 Introduction
Many dialogues lead to decisions and actions. The
participants in such dialogues each come with
their own goals and agendas, their own perspec-
tives on dialogue topics, and their own ways of
interacting with others. Understanding the action-
able results of a dialogue requires accurately mod-
eling both the content of dialogue utterances, as
well as the relevant features of its participants.
In this work, we devise a discriminative latent
variable model that is able to capture the overall
structure of a dialogue as relevant to specific acts
that occur as a result of that dialogue. We aim to
model both the relevance of preceding dialogue to
particular action, as well as a binary structured re-
lationship among utterances, while taking into ac-
count the pragmatic effect introduced by the dif-
ferent speakers? perspectives.
We focus on a particular domain of dialogue:
courtroom transcripts. This domain has the advan-
tage that while its range of topics can be broad, the
roles of participants are relatively well-defined.
Courtroom dialogues also contain a specialized
speech act: the objection.
In real court settings (as opposed to fictional-
ized courts), an objection is a decision made by the
party opposing the side holding the floor, to inter-
rupt the flow of the courtroom discussion. While
motivation behind taking this decision can stem
from different reasons, it is typically an indication
that a particular pragmatic rule has been broken.
The key insight is that objections are sustained
when a nuanced rule of court is being violated: for
instance, the argumentative objection is ?raised in
response to a question which prompts a witness to
draw inferences from facts of the case?
1
, as op-
posed to the witness stating concrete facts.
The objectionable aspects of the preceding di-
alogue can be identified by a well-trained person;
however these aspects are quite subtle to a com-
putational model. In this work we take a first step
toward addressing this problem computationally,
and focus on identifying the key properties of dia-
logue interactions relevant for learning to identify
and classify courtroom objections.
Our technical goal is to drive latent learning of
dialogue structure based on a combination of raw
input and pragmatic binary supervision. The bi-
nary supervision we use is derived from objection
speech acts appearing in the dialogue (described
in Section 2.1).
We are primarily interested in constructing a
representation suitable for learning the challeng-
ing task of identifying objections in courtroom
proceedings (Figure 1 provides an example).
In order to make classifications reliably, a
deeper representation of the dialogue is required.
1
Source: Wikipedia, July 2011, http://en.wikipedia.
org/wiki/Argumentative.
655
MR. COCHRAN Attorney  Defense 
Det. LANGE Witness  Prosecution  SPEA
KER  
SPEAKER  
DIALOGUE RELATION  
And then she filed the case, right?  
DETECTIVE LANGE  That?s correct 
MR. COCHRAN  And before you submitted this case you had heard or seen Miss Clarke on Television saying this was a sole murderer case; isn?t that correct? you had heard that, hadn?t you?  
MS. CLARKE  Objection your honor.  
THE COURT  Hearsay. Sustained  
2 
RELEVANCE 
1 
MR. COCHRAN 
Label 
Dialogue Input  
1 
1 
Figure 1: Moving from raw text to a meaningful rep-
resentation. The raw textual representation hides complex
interactions, relevant for understanding the dialogue flow and
making decisions over it. We break the text into dialogue
turns, each associated with a speaker, explicitly annotated
with their role and side in the court case. Judgements of the
relevance of each dialogue component for the classification
task, produce a more accurate representation of the dialogue
which is easier to learn. These judgments can be over indi-
vidual sentences ( 1 ) or over pairs of sentences across dif-
ferent turns ( 2 ), which represent relevant information flow.
The parameters required for making these judgements are ob-
tained via interaction with the learning process. We explain
these consideration and the construction stages in Section 2.
Our model makes use of three conceptually differ-
ent components capturing linguistic and pragmatic
considerations and their relevance in the context of
the dialogue structure.
Our linguistic model focuses on enriching a
lexical representation of the dialogue utterances
using linguistic resources capturing biased lan-
guage use, such as subjective speech, expressions
of sentiment, intensifiers and hedges. For exam-
ple, the phrase ?So he was driving negligently??
is an argumentative expression, as it requires the
witness to draw inferences, rather than describe
facts. Identifying the use of biased language in this
phrase can help capture this objectionable aspect.
In addition, we use a named entity recognizer, as
we observe that relevant entity mentions provide
a good indication of the dialogue focus. We refer
the reader to Section 2.2 for further explanations.
The surface representation of dialogue turns
hides the complex interactions between its partici-
pants. These interactions are driven by their agen-
das and roles in the trial. Understanding the lexical
cues in this context requires situating the dialogue
in the context of the court case. We condition the
lexical representation of a turn on its speaker, the
speaker?s role and side in the trial, thus allowing
the model to capture the relevant pragmatic influ-
ences introduced by the different speakers.
Next, a discriminative latent variable model
learns a structured representation of the dia-
logue that is useful in making high-level seman-
Notation Explanation
x Input dialogue
x
Sit
Situated dialogue
h Latent structure variables
t Dialogue turn
t.speaker.{name,role,side} Speaker information
t.text Text in a dialogue turn
t.s
i
.{text,type,subj,entities} Sentence level information
Table 1: Notation Summary
tic/pragmatic predictions (section 2.3). The latent
variable model consists of two types of variables.
The first type of latent variable aims to identify
content relevant for the objection identification de-
cision. To this end, it determines the relevance of
individual sentences to the classification decision,
based on properties such as the lexical items ap-
pearing in the sentence, the sentence type, and ex-
pressions of subjectivity. The second latent vari-
able type focuses on the information flow between
speakers. It identifies relevant dialogue relations
between turns. This decision is made by construct-
ing a joint representation of two sentences, across
different dialogue turns, capturing responses to
questions and joining lexical items appearing in
factual sentences across different turns.
Both dialogue aspects are formalized as latent
variables, trained jointly with the final classifica-
tion task using automatically extracted supervi-
sion. In Sec. 3 we describe the learning process.
We evaluate our approach over short dialogue
snippets extracted from the O.J. Simpson murder
trial. Our experiments evaluate the contribution of
the different aspects of our system, showing that
the dialogue representation determined by our la-
tent model results in considerable improvements.
Our evaluation process considers several differ-
ent views of the extracted data. Interestingly, de-
spite the formal definitions of objections, the ma-
jority of objections are raised without justification
(and are subsequently overruled), typically for the
purpose of interrupting the opposing side when
controversial topics are touched upon. Our exper-
iments analyze the differences between sustained
and overruled objections and show that sustained
objections are easier to detect. We describe our
experiments in section 4.
2 Dialogue Structure Modeling
Making predictions in such a complex domain re-
quires a rich representation, capturing the interac-
tions between different participants, the tone of
656
conversation, understanding of controversial is-
sues presented during the trial, and their different
interpretations by either side in the trial. Obtaining
this information manually is a labor intensive task,
furthermore, its subjective nature allows for many
different interpretations of the interactions leading
to the objection.
Our approach, therefore, tries to avoid this diffi-
culty by using a data-driven approach to learn the
correct representation for the input, jointly with
learning to classify correctly. Our representation
transforms the raw input, dialogue snippets ex-
tracted automatically from court proceedings, into
meaningful interactions between dialogue partic-
ipants using a set of variables to determine the
relevant parts of the dialogue and the relations
between them. We inform these decisions using
generic resources providing linguistic knowledge
and pragmatic information, situating the dialogue
in the context of the trial.
In this section we explain this process, starting
from the automatic process of extracting examples
(Section 2.1), the linguistic knowledge resources
and pragmatic information used (Section 2.2), we
summarize the notation used to describe the dia-
logue and its properties in Table 1. We formulate
the inference process, identifying the meaning-
ful interactions for prediction as an Integer Linear
Programming (ILP) optimization problem (Sec-
tion 2.3). The objective function used when solv-
ing this optimization problem is learned from data,
by treating these decisions as latent variables dur-
ing learning. We explain the learning process and
its interaction with inference in Section 3.
2.1 Mining Courtroom Proceedings
The first step in forming our dataset consists of
collecting a large set of relevant courtroom dia-
logue snippets. First, we look for textual occur-
rences of objections in the trial transcript by look-
ing for sustain or overrule word lemma patterns,
attributed to the judge. We treat the judge ruling
turn and the one preceding it as sources of super-
vision, from which an indication of an objection,
its type and sustained/overruled ruling, can be ex-
tracted.
2
We treat the preceding dialogue as the cause for
the objection, which could appear in any of the
previous turns (or sequence of several turns inter-
vening).We consider the previous n=6 turns as the
2
In 4 we provide details about the extracted dataset and its
distribution according to types.
context potentially relevant for the decision and let
the latent variable model learn which aspects of
the context are actually relevant.
2.2 Linguistic and Pragmatic Information
Objection decisions often rely on semantic and
pragmatic patterns which are not explicitly en-
coded. Rather than annotating these manually, we
use generic resources to enrich our representation.
We make a conceptual distinction between two
types of resources. The first, an array of linguis-
tic resources, which provides us an indication of
structure, topics of controversy, and the sentiment
and tone of language used in the dialogue.
The second captures pragmatic considerations
by situating the dialogue utterances in the context
of the courtroom. Each utterance is attributed to
a speaker, thus capturing meaningful patterns spe-
cific to individual speakers.
Linguistic Resources (1) Named Entities pro-
vide strong indications of the topics discussed in
the dialogue and help uncover relevant utterances,
such as ones making claims associating individu-
als with locations. We use the Named Entity Rec-
ognizer (NER) described in (Finkel et al., 2005) to
identify this information.
(2) Subjective and Biased Language Equally im-
portant to understanding the topics of conversation
is the way they are discussed. Expressions of sub-
jectivity and sentiment are useful linguistic tools
for changing the tone of the dialogue and are likely
to attract opposition. We use several resources
to capture this information. We use a lexicon of
subjective and positive/negative sentiment expres-
sions (Riloff and Wiebe, 2003). This resource can
help identify subjective statements attempting to
bias the discussion (e.g., ?So he was driving neg-
ligently??)
We use a list of hedges and boosters (Hyland,
2005). This resource can potentially allow the
model to identify evasive (?I might have seen
him?) and (overly) confident responses (?I am ab-
solutely sure that I have seen him?).
We use a lexicon of biased language provided
by (Recasens et al., 2013), this lexicon extracted
from Wikipedia edits consists of words indicative
of bias, for example in an attempt to frame the
facts raised in the discussion according to one of
the viewpoints (?The death of Nicolle Simposon?
vs. ?The murder of Nicolle Simposon?).
Finally we use a Patient Polarity Verbs lexi-
con (Goyal et al., 2010). This lexicon consists
657
of verbs in which the agent performs an action
with a positive (?He donated money to the foun-
dation?) or negative (?He stole money from the
foundation?) consequence to the patient.
(3) Sentence Segmentation Many turns discuss
multiple topics, some more relevant than others.
In order to accommodate a finer-grained analysis,
we segment each turn into its sentences. Each sen-
tence is associated with a label, taken from a small
set of generic labels. Labels include FORMALITY (e.g.,
a witness being sworn in), QUESTION, RESPONSE (which
could be either POSITIVE or NEGATIVE) and a general
STATEMENT
3
.
Capturing Pragmatic Effects We observe that
in the context of a courtroom discussion, utterance
interpretation (and subsequent dialogue actions) is
conditioned to a large extent on the speaker?s mo-
tivation and goals rather than in isolation. We cap-
ture this information by explicitly associating rele-
vant characteristics of the speakers involved in the
dialogue with their utterances. We use the list of
actors which appear in the trial transcripts, and as-
sociate each turn with a speaker, their role in the
trial and the side they represent. We augment the
lexical turn representation with this information
(see Sec. 2.3.4).
2.3 Identifying Relevant Interactions using
Constrained Optimization
In this section we take the next step towards a
meaningful representation by trying to identify di-
alogue content and information flow relevant for
objection identification. Since this information
is not pre-annotated, we allow it to be learned
as latent variables. These latent variables act as
boolean indicator variables, which determine how
each dialogue input example will be represented.
This process consists of two conceptual stages,
corresponding to two types of boolean variables:
(1) relevant utterances are identified; (2) mean-
ingful connections between them, across dialogue
turns, are identified. This information is exempli-
fied as 1 and 2 in Figure 1. These decisions
are taken jointly by formalizing this process as an
optimization problem over the space of possible
binary relations between dialogue turns and sen-
tences.
3
Determined by lexical information (question marks,
dis/agreement indications and sentence length)
2.3.1 Relevance Decisions
Our raw representation allows as many as six pre-
vious turns to be relevant to the classification de-
cision, however not all turns are indeed relevant,
and even relevant turns may consist only of a
handful of relevant sentences. Given a dialogue
consisting of (t
1
, .., t
n
) turns, each consisting of
(t
i
.s
1
, .., t
i
.s
k
) sentences, we associate with each
sentence.
? Relevance variables, denoted by h
r
i,j
, indi-
cating the relevance of the j-th sentence in the
i-th turn, for the classification decision.
? Irrelevance variables, denoted by h
i
i,j
, indi-
cating that the j-th sentence in the i-th turn is
not relevant for the classification decision.
? Variable pair activation constraints Given
a sentence the activation of these variables
should be mutually exclusive. We encode this
fact by constraining the decision with a linear
constraint.
?i, j, h
r
i,j
+ h
i
i,j
= 1 (1)
2.3.2 Dialogue Structure Decisions
In many cases the information required to make
the classification is not contained in a single dia-
logue turn, but rather is the product of the infor-
mation flow between dialogue participants. Given
a dialogue consisting of (t
1
, .., t
n
) turns, each con-
sisting of (t
i
.s
1
, .., t
i
.s
k
) sentences, we associate
with every two sentences, s
j
? t
i
, s
k
? t
l
, such
that (i 6= l):
? Sentences-Connected variables, denoted by
h
c
(i,j),(k,l)
, indicating that the combination of
the two sentences is relevant for the classifi-
cation decision.
? Sentences-not-Connected variables, de-
noted by h
n
(i,j),(k,l)
, indicating that the
combination of the two sentences is not
relevant for the classification decision.
? Variable pair activation constraints Given
a sentence pair the activation of these vari-
ables should be mutually exclusive. We en-
code this fact by constraining the decision
with a linear constraint.
?i, j, k, l h
c
(i,j),(k,l)
+ h
n
(i,j),(k,l)
= 1 (2)
658
? Decision Consistency constraints Given a
sentence pair, the activation of the variable
indicating the relevance of the sentence pair
entails the activation of the variables indicat-
ing the relevance of the individual sentences.
?i, j, k, l, (h
c
(i,j),(k,l)
) =? (h
r
i,j
? h
r
k,l
)
(3)
2.3.3 Overall Optimization Function
The boolean variables described in the previous
section define a space of competing dialogue rep-
resentations, each representation considers differ-
ent parts of the dialogue as relevant for the objec-
tion classification decision. When making this de-
cision a single representation is selected, by quan-
tifying the decisions and looking for the optimal
set of decisions maximizing the overall sum of de-
cision scores. We construct this objective function
by associating each decision with a feature vector,
obtained using a feature function ? (described in
Section 2.3.4), mapping the relevant part of the in-
put to a feature set.
More formally, given an input x, we denote the
space of all possible dialogue entities (i.e., sen-
tences and sentence pairs) as ?(x). Assuming that
?(x) is of size N , we denote latent representation
decisions as h ? {0, 1}
N
, a set of indicator vari-
ables, that selects a subset of the possible dialogue
entities that constitute the dialogue representation.
For a given dialogue input x and a dialog entity
s ? ?(x), we denote ?
s
(x) as the feature vector
of s. Given a fixed weight vector w that scores
intermediate representations for the final classifi-
cation task, our decision function (for predicting
?objectionable or not?) becomes:
f
w
(x) = max
h
?
s
h
s
w
T
?
s
(x)
subject to (1)-(3); ?s;h
s
? {0, 1}(4)
In our experiments, we formalize Eq. (4) as an
ILP instance, which we solve using the highly op-
timized Gurobi toolkit
4
.
2.3.4 Features
In this section we describe the features used in
each of the different decision types.
Relevance (h
r
) :
Bag-of-words: {(w, t.speaker. ?
5
)|?w ? t.s.text}
4
http://www.gurobi.com/
5
?
*
? denotes all properties
Biased-Language:{(w, resourceContains(w), t.speaker.?)
|?w ? t.s.text}
6
Irrelevance (h
i
) :
SentType: (t.s.type)
ContainsNamedEntity (t.s.entities 6= ?)
Sentences-(not)-Connected (h
c
,h
n
) :
SentTypes: (t
i
.s
j
.type, t
k
.s
l
.type)
QA pair: (t
i
.s
j
.type = Question) ? (t
k
.s
l
.type =
Response)
? {qa|?w ? t
i
.s
j
.text, qa = (w, t
k
.s
l
.type)}
FactPair: (t
i
.s
j
.type = Statement) ? (t
k
.s
l
.type =
Statement)
? {qa|?w ? t
i
.s
j
.text, qa = (w, t
k
.s
l
.type)}
SpeakerPair: (t
i
.speaker.?, t
k
.speaker.?)
3 Learning and Inference
Unlike the traditional classification settings, in
which learning is done over a fixed representation
of the input, we define the learning process over
a set of latent variables. The process of choos-
ing a good representation is formalized as an op-
timization problem that selects the elements and
associated features that best contribute to success-
ful classification. In the rest of this section we ex-
plain the learning process for the parameters of the
model needed both for the representation decision
and the final classification decision.
3.1 Learning
Similar to the traditional formalization of support
vector machines (Boser et al., 1992), learning is
formulated as the following margin-based opti-
mization problem, where ? is a regularization pa-
rameter, and ` is the squared-hinge loss function:
min
w
?
2
?w?
2
+
?
i
` (?y
i
f
w
(x
i
)) (5)
Unlike standard support vector machines, our de-
cision function f
w
(x
i
) is defined over a set of la-
tent variables. We substitute Eq. (4) into Eq.(5),
and obtain the following formulation for a latent
structure classifier:
min
w
?
2
?w?
2
+
?
i
`
?
?
?y
i
max
h?C
w
T
?
s??(x)
h
s
?
s
(x
i
)
?
?
(6)
6
refers to all linguistic resources used. We also included a
+/-1 word window around words appearing in these resources
659
This formulation is not a convex optimization
problem and care must be taken to find a good op-
timum. In our experiments, we use the algorithm
presented in (Chang et al., 2010) to solve this
problem. The algorithm solves this non-convex
optimization function iteratively, decreasing the
value of the objective in each iteration until con-
vergence. In each iteration, the algorithm deter-
mines the values of the latent variables of positive
examples, and optimizes the modified objective
function using a cutting plane algorithm. This al-
gorithmic approach is conceptually (and algorith-
mically) related to the algorithm suggested by (Yu
and Joachims, 2009).
As standard, we classify x as positive iff
f
w
(x) ? 0. In Eq. (4), w
T
?
s
(x) is the score
associated with the substructure s, and f
w
(x) is
the score for the entire intermediate representa-
tion. Therefore, our decision function f
w
(x) ? 0
makes use of the intermediate representation and
its score to classify the input.
4 Empirical Study
Our experiments were designed with two objec-
tives in mind. Since this work is the first to tackle
the challenging task of objection prediction, we
are interested in understanding the scope and fea-
sibility of finding learning-based solutions.
Our second goal is to examine the individual as-
pects of our model and how they impact the over-
all decision and the latent structure it imposes. In
particular, we are interested in understanding the
effect that modeling the situated context (pragmat-
ics) of the dialogue has on objection prediction.
4.1 Experimental Setup
Evaluated Systems In order to understand the
different components of our system, we construct
several variations, which differ according to the re-
sources used during learning (see Section 2.2 for
details), and the latent variable formulation used
(see Section 2.3). We compare our latent model
with and without using pragmatic information (de-
noted DIAL(x
Sit
) and DIAL(x), respectively). We also
compare two baseline systems, which do not use
the latent variable formulation, these systems are
trained, using linear SVM, directly over all the fea-
tures activated by the h
r
decisions for all the turns
in the dialogue. Again, we consider two varia-
tions, with and without pragmatic information (de-
noted ALL(x
Sit
) and ALL(x), respectively).
4.2 Datasets
Our dataset consists of dialogue snippets collected
from the transcripts of the famous O.J. Simpson
murder trial
7
, collected between January of 1995
to September of that year. We also extracted from
the same resource a list of all trial participants,
their roles in the murder case. Section 2.1 de-
scribes the technical details concerned with min-
ing these examples. The collected dataset consists
of 4981 dialogue snippets resulting in an objection
being raised, out of which 2153 were sustained. In
addition, we also mined the trial transcript for neg-
ative examples, collecting 6269 of those examples.
Negative examples are dialogue snippets which do
not result in an objection. To ensure fair evalua-
tion, we mined negative examples from each hear-
ing, proportionally to the number of positive ex-
amples identified in the same hearing. These ex-
amples were mined randomly, by selecting dia-
logue snippets that were not followed by an ob-
jection in any of the three subsequent turns.
We constructed several datasets, each capturing
different characteristics of courtroom interaction.
All Objections Our first dataset consists of all
the objections (both sustained and overruled). The
objection might not be justified, but the corre-
sponding dialogue either has the characteristics of
a justified objection, or it touches upon points of
controversy. In order to simulate this scenario,
we use all the examples, treating all examples re-
sulting in an objection as positive examples. We
randomly select 20% as test data. We refer to
this dataset as ALLOBJ. In addition, to examine
the different properties of sustained and overruled
objections we create two additional dataset, con-
sisting only of sustained/overruled objections and
negative examples. We denote the dataset con-
sisting only of sustained/overruled objections as
SUSTAINEDOBJ and OVERRULEDOBJ, respectively.
Objections by Type Our final dataset breaks the
objections down by type. Unfortunately, most ob-
jections are not raised with an explanation of their
type. We therefore can only use subsets of the
larger ALLOBJ dataset. We use the occurrences of
each objection type as the test dataset and match it
with negative examples, proportional to the size of
the typed dataset. For training, we use all the pos-
itive examples marked with an UNKNOWN type. The
size of each typed dataset appears in Table 3.
7
http://en.wikipedia.org/wiki/O._J._Simpson_
murder_case
660
Objection Type #Pos/#Neg DIAL(x
Sit
) DIAL(x) ALL(x
Sit
) ALL(x)
CALLS FOR SPECULATION 304 / 364 59.4 58.6 58 58
IRRELEVANT 275 / 330 58.5 58.6 55.2 56.6
LACK OF FOUNDATION 238 / 285 60.6 55 57 52.1
HEARSAY 164 / 196 60.3 57.2 60 55
ARGUMENTATIVE 153 / 183 68.8 65.8 64.8 64.8
FACTS NOT IN EVIDENCE 120 / 144 64.7 65.5 59.8 59.4
LEADING QUESTION 116 / 139 56.7 58.4 56.8 58
Table 3: Accuracy results by objection type. Note that the dataset size varies according to the objection type.
System ALLOBJ OVERRULEDOBJ SUSTAINEDOBJ
ALL(x) 64.9 63.7 66.9
ALL(x
Sit
) 65.1 63.7 67.9
DIAL(x) 65.4 65.1 66.7
DIAL(x
Sit
) 69.1 66.3 70.2
Table 2: Overall Accuracy results. Results show consider-
able improvement when using our latent learning framework
with pragmatic information.
4.3 Empirical Analysis
Overall results We begin our discussion with
the experiments conducted over the three larger
datasets (ALLOBJ, SUSTAINEDOBJ, OVERRULEDOBJ). Table 2
summarizes the results obtained by the different
variations of our systems over these datasets.
The most striking observation emerging from
these results is the combined contribution of cap-
turing relevant dialogue content and interaction
(using latent variables), combined with pragmatic
information. For example in the ALLOBJ, when used
in conjunction, their joint contribution pushed per-
formance to 69.1 accuracy, a considerable im-
provement over using each one in isolation - 65.1
for the deterministic system using pragmatic infor-
mation, and 65.4 of the latent-variable formulation
which does not use this information. These results
are consistent in all of our experiments.
We also observe that sustained objections are
easier to predict than overruled objections. This
is not surprising since objections raised for unjus-
tified reasons are harder to detect.
Pragmatic Considerations Pragmatic informa-
tion in our system is modeled by using the x
Sit
representation, which conditions all decisions on
the speaker identity and role. The results in Ta-
ble 2 show that this information typically results
in better quality predictions.
An interesting side effect of using pragmatic
information is its impact on the dialogue struc-
ture predictions learned as latent variables dur-
ing learning. We can quantify the effect by look-
ing at the number of latent variables activated
for each model. When pragmatic information is
used, 5.6 relevance variables are used on average
(per dialogue snipped). In contrast, when prag-
matic information is not used, this number rises to
6.3
8
. In addition, the average number of sentence-
connection variables active when pragmatic infor-
mation is used is 3.44. This number drops to 2.53
when it is not. These scores suggest that infor-
mation about the dialogue pragmatics allows the
model to take advantage of the dialogue structure
at the level of the latent information, focusing the
learner of higher level information, such as the re-
lation between turns, and less on low level, lexi-
cal information. The effect of using the pragmatic
information can be observed qualitatively as ex-
emplified in Figure 2, where the latent decisions,
when pragmatic information is available, construct
a more topically centered representation of the di-
alogue for the classification decision.
Typed Objections The results over the different
objection types are summarized in Table 3. These
results provide some intuition on which of the ob-
jection types are harder to predict, and the contri-
bution of each aspect of our system for that ob-
jection type.
9
We can see that across the objec-
tion types, using latent variables modeling typi-
cally results in a considerable improvement in per-
formance. The most striking example of the im-
portance of using pragmatic information is the LACK
OF FOUNDATION objection type. This objection defini-
tion as ?the evidence lacks testimony as to its au-
thenticity or source.?
10
can explain this fact, as
information about the side in the trial introducing
specific evidence in testimony is very likely to im-
pact the objection decision.
5 Related Work and Discussion
Our work applies latent variable learning to the
problem of uncovering pragmatic effects in court-
8
The average number of sentences per dialogue is 8.6
9
Since these datasets vary in size, their results are neither
directly comparable to each other nor to the results in Table 2.
10
http://en.wikipedia.org/wiki/Foundation_
(evidence)
661
MR. NEUFELD 
MR. NEUFELD  
MS. KESTLER  
 ?  MR. DARDEN  Your Honor, this is hearsay THE COURT   Overruled  MS. KESTLER    I don't recall if there was that day or not.  I know at some point, we had a meeting as to what evidence we had and what was going to be tested and who was going to test it.  
Do you recall being at a meeting with Erin Reilly, Collin Yamauchi and Dennis Fung and Greg Matheson about this case on June 16th?  
I don't recall.  
On the very next day, June 16th, did you participate in another meeting about this case?  
 ?  
Figure 2: Example of the pragmatic effect on latent
dialogue structure. Constructing the latent dialogue struc-
ture over situated text marks unrelated sentences as irrele-
vant, while marking topically related sentences and identi-
fying the connection between the question-answer pair (de-
cisions marked in solid blue lines). When trained without
situated information, the latent output structure marks topi-
cally unrelated sentences as relevant for objection classifica-
tion. Note that in this case all the edge variables are turned
off (marked with dashed red lines).
room dialogues. We adopted the structured latent
variable model defined in (Chang et al., 2010), and
use ILP to solve the structure prediction inference
problem (Roth and Yih, 2007).
Our prediction task, identifying the actionable
result of a dialogue, requires capturing the dia-
logue and discourse relations. While we view
these relations as latent variables in the context
of action prediction, studying these relations in-
dependently has been the focus of significant re-
search efforts, such as discourse relations (Prasad
et al., 2008), rhetorical structure (Marcu, 1997)
and dialogue act modeling (Stolcke et al., 2000).
Fully supervised approaches for learning to pre-
dict dialogue and discourse relations (such as
(Baldridge and Lascarides, 2005)) typically re-
quires heavy supervision and has been applied
only to limited domains.
Moving away from full supervision, the work of
(Golland et al., 2010) uses a game-theoretic model
to explicitly model the roles of dialogue partic-
ipants. In the context of dialogue and situated
language understanding, the work of (Artzi and
Zettlemoyer, 2011) shows how to derive supervi-
sion for dialogue processing from its structure.
Discriminative latent variables models have
seen a surge of interest in recent years, both in the
machine learning community (Yu and Joachims,
2009; Quattoni et al., 2007) as well as various ap-
plication domains such as NLP (T?ackstr?om and
McDonald, 2011) and computer vision (Felzen-
szwalb et al., 2010). In NLP, one of the most well-
known applications of discriminative latent struc-
tured classification is to the Textual Entailment
(TE) task (Chang et al., 2010; Wang and Manning,
2010). The TE task bears some resemblances ours,
as both tasks require making a binary decision
on the basis of a complex input object (i.e., the
history of dialogue, pairs of paragraphs), creating
the need for a learning framework that is flexible
enough to model the complex latent structure that
exists in the input. Another popular application
domain is sentiment analysis (Yessenalina et al.,
2010; T?ackstr?om and McDonald, 2011; Trivedi
and Eisenstein, 2013). The latent variable model
allows the learner to identify finer grained senti-
ment expression than annotated in the data.
A related area of work with different motiva-
tions and different technical approaches has fo-
cused on attempting to understand narrative struc-
ture. For instance, Chambers and Jurafsky (Cham-
bers and Jurafsky, 2008; Chambers and Juraf-
sky, 2009) model narrative flow in the style of
Schankian scripts (Schank and Abelson, 1977).
Their focus is on common sequences of actions,
not specifically related to dialogue. Somewhat
more related is recent work (Goyal et al., 2010)
that aimed to build a computational model of
Lehnert?s Plot Units (Lehnert, 1981) model. That
work focused primarily on actions and not on di-
alogue: in fact, their results showed that the lack
of dialogue understanding was a significant detri-
ment to their ability to model plot structure.
Instead of focusing on actions, like the above
work, we focus on dialogue content and relation-
ships between utterances. Furthermore, unlike
most of the relevant work in NLP, our approach
requires only very lightweight annotation coming
for ?free? in the form of courtroom objections,
and use a latent variable model to provide judge-
ments of relevant linguistic and dialogue relations,
rather than annotating it manually. We enhance
this model using pragmatic information, captur-
ing speakers? identity and role in the dialogue, and
show empirically the relevance of this information
when making predictions.
It is important to recognize that courtroom ob-
jections are not the only actionable result of di-
alogues. Many discussions that occur on online
forums, in social media, and by email result in
measurable real-world outcomes. We have shown
that one particular type of outcome, realized as a
speech-act, can drive dialogue interpretation; the
field is wide open to investigate others.
662
References
Adam Vogel and Christopher Potts and Dan Jurafsky.
2011. Implicatures and Nested Beliefs in Approxi-
mate Decentralized-POMDPs. In EMNLP.
Yoav Artzi and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. In
EMNLP.
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In CoNLL.
B. E. Boser, I. M. Guyon, and V. N. Vapnik. 1992.
A training algorithm for optimal margin classifiers.
In Proc. 5th Annu. Workshop on Comput. Learning
Theory, pages 144?152.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, June.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In ACL/IJCNLP, pages 602?610.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning over
constrained latent representations. In NAACL.
Pedro F. Felzenszwalb, Ross B. Girshick, David A.
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. IEEE Trans. Pattern Anal. Mach. Intell.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In EMNLP.
Amit Goyal, Ellen Riloff, and Hal Daum?e III. 2010.
Automatically producing plot unit representations
for narrative text. In Empirical Methods in Natural
Language Processing (EMNLP).
K. Hyland. 2005. Metadiscourse: Exploring inter-
action in writing. In Continuum, London and New
York.
W. G. Lehnert. 1981. Plot units and narrative summa-
rization. In Cognitive Science.
Daniel Marcu. 1997. The rhetorical parsing of natural
language texts. In ACL.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In LREC.
Ariadna Quattoni, Sybor Wang, L-P Morency, Michael
Collins, and Trevor Darrell. 2007. Hidden condi-
tional random fields. Pattern Analysis and Machine
Intelligence, IEEE Transactions on.
Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic models for an-
alyzing and detecting biased language. In Proceed-
ings of ACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In NAACL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. In ACL/IJCNLP.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialogue act modeling
for automatic tagging and recognition of conversa-
tional speech. COMPUTATIONAL LINGUISTICS,
26:339?373.
Oscar T?ackstr?om and Ryan T. McDonald. 2011. Dis-
covering fine-grained sentiment with latent variable
structured prediction models. In ECIR.
Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse
connectors for latent subjectivity in sentiment anal-
ysis. classification. In NAACL.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question an-
swering. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING
2010).
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document-
level sentiment classification. In EMNLP.
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In Proc. of the International
Conference on Machine Learning (ICML).
663
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429?437,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Discriminative Learning over Constrained Latent Representations
Ming-Wei Chang and Dan Goldwasser and Dan Roth and Vivek Srikumar
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang,goldwas1,danr,vsrikum2}@uiuc.edu
Abstract
This paper proposes a general learning frame-
work for a class of problems that require learn-
ing over latent intermediate representations.
Many natural language processing (NLP) de-
cision problems are defined over an expressive
intermediate representation that is not explicit
in the input, leaving the algorithm with both
the task of recovering a good intermediate rep-
resentation and learning to classify correctly.
Most current systems separate the learning
problem into two stages by solving the first
step of recovering the intermediate representa-
tion heuristically and using it to learn the final
classifier. This paper develops a novel joint
learning algorithm for both tasks, that uses the
final prediction to guide the selection of the
best intermediate representation. We evalu-
ate our algorithm on three different NLP tasks
? transliteration, paraphrase identification and
textual entailment ? and show that our joint
method significantly improves performance.
1 Introduction
Many NLP tasks can be phrased as decision prob-
lems over complex linguistic structures. Successful
learning depends on correctly encoding these (of-
ten latent) structures as features for the learning sys-
tem. Tasks such as transliteration discovery (Kle-
mentiev and Roth, 2008), recognizing textual en-
tailment (RTE) (Dagan et al, 2006) and paraphrase
identification (Dolan et al, 2004) are a few proto-
typical examples. However, the input to such prob-
lems does not specify the latent structures and the
problem is defined in terms of surface forms only.
Most current solutions transform the raw input into
a meaningful intermediate representation1, and then
encode its structural properties as features for the
learning algorithm.
Consider the RTE task of identifying whether the
meaning of a short text snippet (called the hypoth-
esis) can be inferred from that of another snippet
(called the text). A common solution (MacCartney
et al, 2008; Roth et al, 2009) is to begin by defining
an alignment over the corresponding entities, pred-
icates and their arguments as an intermediate rep-
resentation. A classifier is then trained using fea-
tures extracted from the intermediate representation.
The idea of using a intermediate representation also
occurs frequently in other NLP tasks (Bergsma and
Kondrak, 2007; Qiu et al, 2006).
While the importance of finding a good inter-
mediate representation is clear, emphasis is typi-
cally placed on the later stage of extracting features
over this intermediate representation, thus separat-
ing learning into two stages ? specifying the la-
tent representation, and then extracting features for
learning. The latent representation is obtained by an
inference process using predefined models or well-
designed heuristics. While these approaches often
perform well, they ignore a useful resource when
generating the latent structure ? the labeled data for
the final learning task. As we will show in this pa-
per, this results in degraded performance for the ac-
tual classification task at hand. Several works have
considered this issue (McCallum et al, 2005; Gold-
wasser and Roth, 2008b; Chang et al, 2009; Das
and Smith, 2009); however, they provide solutions
1In this paper, the phrases ?intermediate representation? and
?latent representation? are used interchangeably.
429
that do not easily generalize to new tasks.
In this paper, we propose a unified solution to the
problem of learning to make the classification deci-
sion jointly with determining the intermediate rep-
resentation. Our Learning Constrained Latent Rep-
resentations (LCLR) framework is guided by the in-
tuition that there is no intrinsically good intermedi-
ate representation, but rather that a representation is
good only to the extent to which it improves perfor-
mance on the final classification task. In the rest of
this section we discuss the properties of our frame-
work and highlight its contributions.
Learning over Latent Representations This pa-
per formulates the problem of learning over latent
representations and presents a novel and general so-
lution applicable to a wide range of NLP applica-
tions. We analyze the properties of our learning
solution, thus allowing new research to take advan-
tage of a well understood learning and optimization
framework rather than an ad-hoc solution. We show
the generality of our framework by successfully ap-
plying it to three domains: transliteration, RTE and
paraphrase identification.
Joint Learning Algorithm In contrast to most
existing approaches that employ domain specific
heuristics to construct intermediate representations
to learn the final classifier, our algorithm learns to
construct the optimal intermediate representation to
support the learning problem. Learning to represent
is a difficult structured learning problem however,
unlike other works that use labeled data at the in-
termediate level, our algorithm only uses the binary
supervision supplied for the final learning problem.
Flexible Inference Successful learning depends
on constraining the intermediate representation with
task-specific knowledge. Our framework uses the
declarative Integer Linear Programming (ILP) infer-
ence formulation, which makes it easy to define the
intermediate representation and to inject knowledge
in the form of constraints. While ILP has been ap-
plied to structured output learning, to the best of our
knowledge, this is the first work that makes use of
ILP in formalizing the general problem of learning
intermediate representations.
2 Preliminaries
We introduce notation using the Paraphrase Iden-
tification task as a running example. This is the bi-
nary classification task of identifying whether one
sentence is a paraphrase of another. A paraphrase
pair from the MSR Paraphrase corpus (Quirk et al,
2004) is shown in Figure 1. In order to identify
that the sentences paraphrase each other , we need
to align constituents of these sentences. One possi-
ble alignment is shown in the figure, in which the
dotted edges correspond to the aligned constituents.
An alignment can be specified using binary variables
corresponding to every edge between constituents,
indicating whether the edge is included in the align-
ment. Different activations of these variables induce
the space of intermediate representations.
The notification was first reported Friday by MSNBC.
MSNBC.com first reported the CIA request on Friday.
Figure 1: The dotted lines represent a possible intermediate
representation for the paraphrase identification task. Since dif-
ferent representation choices will impact the binary identifica-
tion decision directly, our approach chooses the representation
that facilitates the binary learning task.
To formalize this setting, let x denote the input
to a decision function, which maps x to {?1, 1}.
We consider problems where this decision depends
on an intermediate representation (for example, the
collection of all dotted edges in Figure 1), which can
be represented by a binary vector h.
In the literature, a common approach is to sepa-
rate the problem into two stages. First, a genera-
tion stage predicts h for each x using a pre-defined
model or a heuristic. This is followed by a learn-
ing stage, in which the classifier is trained using h.
In our example, if the generation stage predicts the
alignment shown, then the learning stage would use
the features computed based on the alignments. For-
mally, the two-stage approach uses a pre-defined in-
ference procedure that finds an intermediate repre-
sentation h?. Using features ?(x,h?) and a learned
weight vector ?, the example is classified as positive
if ?T?(x,h?) ? 0.
However, in the two stage approach, the latent
representation, which is provided to the learning al-
gorithm, is determined before learning starts, and
without any feedback from the final task. It is dic-
tated by the intuition of the developer. This approach
makes two implicit assumptions: first, it assumes
430
the existence of a ?correct? latent representation and,
second, that the model or heuristic used to generate
it is the correct one for the learning problem at hand.
3 Joint Learning with an Intermediate
Representation
In contrast to two-stage approaches, we use the
annotated data for the final classification task to
learn a suitable intermediate representation which,
in turn, helps the final classification.
Choosing a good representation is an optimization
problem that selects which of the elements (features)
of the representation best contribute to success-
ful classification given some legitimacy constraints;
therefore, we (1) set up the optimization framework
that finds legitimate representations (Section 3.1),
and (2) learn an objective function for this optimiza-
tion problem, such that it makes the best final deci-
sion (Section 3.2.)
3.1 Inference
Our goal is to correctly predict the final label
rather than matching a ?gold? intermediate repre-
sentation. In our framework, attempting to learn the
final decision drives both the selection of the inter-
mediate representation and the final predictions.
For each x, let ?(x) be the set of all substructures
of all possible intermediate representations. In Fig-
ure 1, this could be the set of all alignment edges
connecting the constituents of the sentences. Given
a vocabulary of such structures of sizeN , we denote
intermediate representations by h ? {0, 1}N , which
?select? the components of the vocabulary that con-
stitute the intermediate representation. We define
?s(x) to be a feature vector over the substructure
s, which is used to describe the characteristics of s,
and define a weight vector u over these features.
Let C denote the set of feasible intermediate repre-
sentations h, specified by means of linear constraints
over h. While ?(x) might be large, the set of those
elements in h that are active can be constrained by
controlling C. After we have learned a weight vec-
tor u that scores intermediate representations for the
final classification task, we define our decision func-
tion as
fu(x) = max
h?C
uT
?
s??(x)
hs?s(x), (1)
and classify the input as positive if fu(x) ? 0.
In Eq. (1), uT?s(x) is the score associated with
the substructure s, and fu(x) is the score for the en-
tire intermediate representation. Therefore, our de-
cision function fu(x) ? 0 makes use of the interme-
diate representation and its score to classify the in-
put. An input is labeled as positive if its underlying
intermediate structure allows it to cross the decision
threshold. The intermediate representation is cho-
sen to maximize the overall score of the input. This
design is especially beneficial for many phenomena
in NLP, where only positive examples have a mean-
ingful underlying structure. In our paraphrase iden-
tification example, good alignments generally exist
only for positive examples.
One unique feature of our framework is that we
treat Eq. (1) as an Integer Linear Programming
(ILP) instance. A concrete instantiation of this set-
ting to the paraphrase identification problem, along
with the actual ILP formulation is shown in Section
4.
3.2 Learning
We now present an algorithm that learns the
weight vector u. For a loss function ` : R ? R,
the goal of learning is to solve the following opti-
mization problem:
min
u
?
2
?u?2 +
?
i
` (?yifu(xi)) (2)
Here, ? is the regularization parameter. Substituting
Eq. (1) into Eq. (2), we get
min
u
?
2
?u?2+
?
i
`
?
??yi max
h?C
uT
?
s??(x)
hs?s(xi)
?
? (3)
Note that there is a maximization term inside the
global minimization problem, making Eq. (3) a non-
convex problem. The minimization drives u towards
smaller empirical loss while the maximization uses
u to find the best representation for each example.
The algorithm for Learning over Constrained La-
tent Representations (LCLR) is listed in Algorithm
1. In each iteration, first, we find the best feature
representations for all positive examples (lines 3-5).
This step can be solved with an off-the-shelf ILP
solver. Having fixed the representations for the pos-
itive examples, we update the u by solving Eq. (4)
at line 6 in the algorithm. It is important to observe
431
Algorithm 1 LCLR :The algorithm that optimizes (3)
1: initialize: u? u0
2: repeat
3: for all positive examples (xi, yi = 1) do
4: Find h?i ? arg maxh?C
?
s
hsuT?s(xi)
5: end for
6: Update u by solving
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?C
uT
?
s
hs?s(xi)) (4)
7: until convergence
8: return u
that for positive examples in Eq. (4), we use the in-
termediate representations h? from line 4.
Algorithm 1 satisfies the following property:
Theorem 1 If the loss function ` is a non-
decreasing function, then the objective function
value of Eq. (3) is guaranteed to decrease in every
iteration of Algorithm 1. Moreover, if the loss func-
tion is also convex, then Eq. (4) in Algorithm 1 is
convex.
Due to the space limitation, we omit the proof.
Theoretically, we can use any loss function that
satisfies the conditions of the theorem. In the exper-
iments in this paper, we use the squared-hinge loss
function: `(?yfu(x)) = max(0, 1? yfu(x))2.
Recall that Eq. (4) is not the traditional SVM or
logistic regression formulation. This is because in-
side the inner loop, the best representation for each
negative example must be found. Therefore, we
need to perform inference for every negative exam-
ple when updating the weight vector solution. In-
stead of solving a difficult non-convex optimization
problem (Eq. (3)), LCLR iteratively solves a series
of easier problems (Eq. (4)). This is especially true
for our loss function because Eq. (4) is convex and
can be solved efficiently.
We use a cutting plane algorithm to solve Eq. (4).
A similar idea has been proposed in (Joachims et al,
2009). The algorithm for solving Eq. (4) is presented
as Algorithm 2. This algorithm uses a ?cache? Hj
to store all intermediate representations for negative
examples that have been seen in previous iterations
Algorithm 2 Cutting plane algorithm to optimize Eq. (4)
1: for each negative example xj , Hj ? ?
2: repeat
3: for each negative example xj do
4: Find h?j ? arg maxh?C
?
s hsu
T?s(xj)
5: Hj ? Hj ? {h?j}
6: end for
7: Solve
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?Hj
uT
?
s
hs?s(xi)) (5)
8: until no new element is added to any Hj
9: return u
(lines 3-6) 2. The difference between Eq. (5) in line
7 of Algorithm 2 and Eq. (4) is that in Eq. (5), we do
not search over the entire space of intermediate rep-
resentations. The search space for the minimization
problem Eq. (5) is restricted to the cache Hj . There-
fore, instead of solving the minimization problem
Eq. (4), we can now solve several simpler problems
shown in Eq. (5). The algorithm is guaranteed to
stop (line 8) because the space of intermediate rep-
resentations is finite. Furthermore, in practice, the
algorithm needs to consider only a small subset of
?hard? examples before it converges.
Inspired by (Hsieh et al, 2008), we apply an effi-
cient coordinate descent algorithm for the dual for-
mulation of (5) which is guaranteed to find its global
minimum. Due to space considerations, we do not
present the derivation of dual formulation and the
details of the optimization algorithm.
4 Encoding with ILP: A Paraphrase
Identification Example
In this section, we define the latent representation
for the paraphrase identification task. Unlike the ear-
lier example, where we considered the alignment of
lexical items, we describe a more complex interme-
diate representation by aligning graphs created using
semantic resources.
An input example is represented as two acyclic
2In our implementation, we keep a global cache Hj for each
negative example xj . Therefore, in Algorithm 2, we start with
a non-empty cache improving the speed significantly.
432
graphs, G1 and G2, corresponding to the first
and second input sentences. Each vertex in the
graph contains word information (lemma and part-
of-speech) and the edges denote dependency rela-
tions, generated by the Stanford parser (Klein and
Manning, 2003). The intermediate representation
for this task can now be defined as an alignment be-
tween the graphs, which captures lexical and syntac-
tic correlations between the sentences.
We use V (G) and E(G) to denote the set of ver-
tices and edges in G respectively, and define four
hidden variable types to encode vertex and edge
mappings between G1 and G2.
? The word-mapping variables, denoted by
hv1,v2 , define possible pairings of vertices,
where v1 ? V (G1) and v2 ? V (G2).
? The edge-mapping variables, denoted by
he1,e2 , define possible pairings of the graphs
edges, where e1 ? E(G1) and e2 ? E(G2).
? The word-deletion variables hv1,? (or h?,v2) al-
low for vertices v1 ? V (G1) (or v2 ? V (G2))
to be deleted. This accounts for omission of
words (like function words).
? The edge-deletion variables, he1,? (or h?,e2) al-
low for deletion of edges from G1 (or G2).
Our inference problem is to find the optimal set of
hidden variable activations, restricted according to
the following set of linear constraints
? Each vertex inG1 (orG2) can either be mapped
to a single vertex in G2 (or G1) or marked as
deleted. In terms of the word-mapping and
word-deletion variables, we have
?v1 ? V (G1);hv1,? +
?
v2?V (G2)
hv1,v2 = 1 (6)
?v2 ? V (G2);h?,v2 +
?
v1?V (G1)
hv1,v2 = 1 (7)
? Each edge in G1 (or G2) can either be mapped
to a single edge in G2 (or G1) or marked as
deleted. In terms of the edge-mapping and
edge-deletion variables, we have
?e1 ? E(G1);he1,? +
?
e2?E(G2)
he1,e2 = 1 (8)
?e2 ? E(G2);h?,e2 +
?
e1?E(G1)
he1,e2 = 1 (9)
? The edge mappings can be active if, and only
if, the corresponding node mappings are ac-
tive. Suppose e1 = (v1, v?1) ? E(G1) and
e2 = (v2, v?2) ? E(G2), where v1, v
?
1 ? V (G1)
and v2, v?2 ? V (G2). Then, we have
hv1,v2 + hv?1,v?2 ? he1,e2 ? 1 (10)
hv1,v2 ? he1,e2 ;hv?1,v?2 ? he1,e2 (11)
These constraints define the feasible set for the in-
ference problem specified in Equation (1). This in-
ference problem can be formulated as an ILP prob-
lem with the objective function from Equation (1):
max
h
?
s
hsuT?s(x)
subject to (6)-(11); ?s;hs ? {0, 1} (12)
This example demonstrates the use of integer linear
programming to define intermediate representations
incorporating domain intuition.
5 Experiments
We applied our framework to three different NLP
tasks: transliteration discovery (Klementiev and
Roth, 2008), RTE (Dagan et al, 2006), and para-
phrase identification (Dolan et al, 2004).
Our experiments are designed to answer the fol-
lowing research question: ?Given a binary classifi-
cation problem defined over latent representations,
will the joint LCLR algorithm perform better than a
two-stage approach?? To ensure a fair comparison,
both systems use the same feature functions and def-
inition of intermediate representation. We use the
same ILP formulation in both configurations, with a
single exception ? the objective function parameters:
the two stage approach uses a task-specific heuristic,
while LCLR learns it iteratively.
The ILP formulation results in very strong two
stage systems. For example, in the paraphrase iden-
tification task, even our two stage system is the cur-
rent state-of-the-art performance. In these settings,
the improvement obtained by our joint approach is
non-trivial and can be clearly attributed to the su-
periority of the joint learning algorithm. Interest-
ingly, we find that our more general approach is bet-
ter than specially designed joint approaches (Gold-
wasser and Roth, 2008b; Das and Smith, 2009).
Since the objective function (3) of the joint ap-
proach is not convex, a good initialization is re-
quired. We use the weight vector learned by the two
433
stage approach as the starting point for the joint ap-
proach. The algorithm terminates when the relative
improvement of the objective is smaller than 10?5.
5.1 Transliteration Discovery
Transliteration discovery is the problem of iden-
tifying if a word pair, possibly written using two
different character sets, refers to the same underly-
ing entity. The intermediate representation consists
of all possible character mappings between the two
character sets. Identifying this mapping is not easy,
as most writing systems do not perfectly align pho-
netically and orthographically; rather, this mapping
can be context-dependent and ambiguous.
For an input pair of words (w1, w2), the interme-
diate structure h is a mapping between their charac-
ters, with the latent variable hij indicating if the ith
character in w1 is aligned to the jth character in w2.
The feature vector associated with the variable hij
contains unigram character mapping, bigram char-
acter mapping (by considering surrounding charac-
ters). We adopt the one-to-one mapping and non-
crossing constraint used in (Chang et al, 2009).
We evaluated our system using the English-
Hebrew corpus (Goldwasser and Roth, 2008a),
which consists of 250 positive transliteration pairs
for training, and 300 pairs for testing. As negative
examples for training, we sample 10% from random
pairings of words from the positive data. We report
two evaluation measurements ? (1) the Mean Recip-
rocal Rank (MRR), which is the average of the mul-
tiplicative inverse of the rank of the correct answer,
and (2) the accuracy (Acc), which is the percentage
of the top rank candidates being correct.
We initialized the two stage inference process as
detailed in (Chang et al, 2009) using a Romaniza-
tion table to assign uniform weights to prominent
character mappings. This initialization procedure
resembles the approach used in (Bergsma and Kon-
drak, 2007). An alignment is first built by solving
the constrained optimization problem. Then, a sup-
port vector machine with squared-hinge loss func-
tion is used to train a classifier using features ex-
tracted from the alignment. We refer to this two
stage approach as Alignment+Learning.
The results summarized in Table 1 show the sig-
nificant improvement obtained by the joint approach
(95.4% MRR) compared to the two stage approach
Transliteration System Acc MRR
(Goldwasser and Roth,
2008b)
N/A 89.4
Alignment + Learning 80.0 85.7
LCLR 92.3 95.4
Table 1: Experimental results for transliteration. We compare
a two-stage system: ?Alignment+Learning? with LCLR, our
joint algorithm. Both ?Alignment+Learning? and LCLR use
the same features and the same intermediate representation def-
inition.
(85.7%). Moreover, LCLR outperforms the joint
system introduced in (Goldwasser and Roth, 2008b).
5.2 Textual Entailment
Recognizing Textual Entailment (RTE) is an im-
portant textual inference task of predicting if a given
text snippet, entails the meaning of another (the hy-
pothesis). In many current RTE systems, the entail-
ment decision depends on successfully aligning the
constituents of the text and hypothesis, accounting
for the internal linguistic structure of the input.
The raw input ? the text and hypothesis ? are
represented as directed acyclic graphs, where ver-
tices correspond to words. Directed edges link verbs
to the head words of semantic role labeling argu-
ments produced by (Punyakanok et al, 2008). All
other words are connected by dependency edges.
The intermediate representation is an alignment be-
tween the nodes and edges of the graphs. We used
three hidden variable types from Section 4 ? word-
mapping, word-deletion and edge-mapping, along
with the associated constraints as defined earlier.
Since the text is typically much longer than the hy-
pothesis, we create word-deletion latent variables
(and features) only for the hypothesis.
The second column of Table 2 lists the resources
used to generate features corresponding to each hid-
den variable type. For word-mapping variables, the
features include a WordNet based metric (WNSim),
indicators for the POS tags and negation identifiers.
We used the state-of-the-art coreference resolution
system of (Bengtson and Roth, 2008) to identify the
canonical entities for pronouns and extract features
accordingly. For word deletion, we use only the POS
tags of the corresponding tokens (generated by the
LBJ POS tagger3) to generate features. For edge
3
http://L2R.cs.uiuc.edu/?cogcomp/software.php
434
Hidden RTE Paraphrase
Variable features features
word-mapping WordNet, POS,
Coref, Neg
WordNet, POS,
NE, ED
word-deletion POS POS, NE
edge-mapping NODE-INFO NODE-INFO,
DEP
edge-deletion N/A DEP
Table 2: Summary of latent variables and feature resources for
the entailment and paraphrase identification tasks. See Section
4 for an explanation of the hidden variable types. The linguistic
resources used to generate features are abbreviated as follows ?
POS: Part of speech, Coref: Canonical coreferent entities; NE:
Named Entity, ED: Edit distance, Neg: Negation markers, DEP:
Dependency labels, NODE-INFO: corresponding node align-
ment resources, N/A: Hidden variable not used.
Entailment System Acc
Median of TAC 2009 systems 61.5
Alignment + Learning 65.0
LCLR 66.8
Table 3: Experimental results for recognizing textual entail-
ment. The first row is the median of best performing systems of
all teams that participated in the RTE5 challenge (Bentivogli et
al., 2009). Alignment + Learning is our two-stage system im-
plementation, and LCLR is our joint learning algorithm. Details
about these systems are provided in the text.
mapping variables, we include the features of the
corresponding word mapping variables, scaled by
the word similarity of the words forming the edge.
We evaluated our system using the RTE-5
data (Bentivogli et al, 2009), consisting of 600 sen-
tence pairs for training and testing respectively, in
which positive and negative examples are equally
distributed. In these experiments the joint LCLR al-
gorithm converged after 5 iterations.
For the two stage system, we used WN-
Sim to score alignments during inference. The
word-based scores influence the edge variables
via the constraints. This two-stage system (the
Alignment+Learning system) is significantly better
than the median performance of the RTE-5 submis-
sions. Using LCLR further improves the result by al-
most 2%, a substantial improvement in this domain.
5.3 Paraphrase Identification
Our final task is Paraphrase Identification, dis-
cussed in detail at Section 4. We use all the four
hidden variable types described in that section. The
features used are similar to those described earlier
Paraphrase System Acc
Experiments using (Dolan et al, 2004)
(Qiu et al, 2006) 72.00
(Das and Smith, 2009) 73.86
(Wan et al, 2006) 75.60
Alignment + Learning 76.23
LCLR 76.41
Experiments using Extended data set
Alignment + Learning 72.00
LCLR 72.75
Table 4: Experimental Result For Paraphrasing Identification.
Our joint LCLR approach achieves the best results compared
to several previously published systems, and our own two stage
system implementation (Alignment + Learning). We evaluated
the systems performance across two datasets: (Dolan et al,
2004) dataset and the Extended dataset, see the text for details.
Note that LCLR outperforms (Das and Smith, 2009), which is a
specifically designed joint approach for this task.
for the RTE system and are summarized in Table 2.
We used the MSR paraphrase dataset of (Dolan
et al, 2004) for empirical evaluation. Additionally,
we generated a second corpus (called the Extended
dataset) by sampling 500 sentence pairs from the
MSR dataset for training and using the entire test
collection of the original dataset. In the Extended
dataset, for every sentence pair, we extended the
longer sentence by concatenating it with itself. This
results in a more difficult inference problem because
it allows more mappings between words. Note that
the performance on the original dataset sets the ceil-
ing on the second one.
The results are summarized in Table 4. The first
part of the table compares the LCLR system with
a two stage system (Alignment + Learning) and
three published results that use the MSR dataset.
(We only list single systems in the table4) Inter-
estingly, although still outperformed by our joint
LCLR algorithm, the two stage system is able per-
form significantly better than existing systems for
that dataset (Qiu et al, 2006; Das and Smith, 2009;
Wan et al, 2006). We attribute this improvement,
consistent across both the ILP based systems, to the
intermediate representation we defined.
We hypothesize that the similarity in performance
between the joint LCLR algorithm and the two stage
4Previous work (Das and Smith, 2009) has shown that com-
bining the results of several systems improves performance.
435
(Alignment + Learning) systems is due to the limited
intermediate representation space for input pairs in
this dataset. We evaluated these systems on the more
difficult Extended dataset. Results indeed show that
the margin between the two systems increases as the
inference problem becomes harder.
6 Related Work
Recent NLP research has largely focused on two-
stage approaches. Examples include RTE (Zanzotto
and Moschitti, 2006; MacCartney et al, 2008; Roth
et al, 2009); string matching (Bergsma and Kon-
drak, 2007); transliteration (Klementiev and Roth,
2008); and paraphrase identification (Qiu et al,
2006; Wan et al, 2006).
(MacCartney et al, 2008) considered construct-
ing a latent representation to be an independent task
and used manually labeled alignment data (Brockett,
2007) to tune the inference procedure parameters.
While this method identifies alignments well, it does
not improve entailment decisions. This strengthens
our intuition that the latent representation should be
guided by the final task.
There are several exceptions to the two-stage ap-
proach in the NLP community (Haghighi et al,
2005; McCallum et al, 2005; Goldwasser and Roth,
2008b; Das and Smith, 2009); however, the interme-
diate representation and the inference for construct-
ing it are closely coupled with the application task.
In contrast, LCLR provides a general formulation
that allows the use of expressive constraints, mak-
ing it applicable to many NLP tasks.
Unlike other latent variable SVM frameworks
(Felzenszwalb et al, 2009; Yu and Joachims, 2009)
which often use task-specific inference procedure,
LCLR utilizes the declarative inference framework
that allows using constraints over intermediate rep-
resentation and provides a general platform for a
wide range of NLP tasks.
The optimization procedure in this work and
(Felzenszwalb et al, 2009) are quite different.
We use the coordinate descent and cutting-plane
methods ensuring we have fewer parameters and
the inference procedure can be easily parallelized.
Our procedure also allows different loss functions.
(Cherry and Quirk, 2008) adopts the Latent SVM al-
gorithm to define a language model. Unfortunately,
their implementation is not guaranteed to converge.
In CRF-like models with latent variables (McCal-
lum et al, 2005), the decision function marginal-
izes over the all hidden states when presented with
an input example. Unfortunately, the computational
cost of applying their framework is prohibitive with
constrained latent representations. In contrast, our
framework requires only the best hidden representa-
tion instead of marginalizing over all possible repre-
sentations, thus reducing the computational effort.
7 Conclusion
We consider the problem of learning over an inter-
mediate representation. We assume the existence of
a latent structure in the input, relevant to the learn-
ing problem, but not accessible to the learning algo-
rithm. Many NLP tasks fall into these settings and
each can consider a different hidden input structure.
We propose a unifying thread for the different prob-
lems and present a novel framework for Learning
over Constrained Latent Representations (LCLR).
Our framework can be applied to many different la-
tent representations such as parse trees, orthographic
mapping and tree alignments. Our approach con-
trasts with existing work in which learning is done
over a fixed representation, as we advocate jointly
learning it with the final task.
We successfully apply the proposed framework to
three learning tasks ? Transliteration, Textual En-
tailment and Paraphrase Identification. Our joint
LCLR algorithm achieves superior performance in
all three tasks. We attribute the performance im-
provement to our novel training algorithm and flex-
ible inference procedure, allowing us to encode do-
main knowledge. This presents an interesting line of
future work in which more linguistic intuitions can
be encoded into the learning problem. For these rea-
sons, we believe that our framework provides an im-
portant step forward in understanding the problem
of learning over hidden structured inputs.
Acknowledgment We thank James Clarke and Mark Sam-
mons for their insightful comments. This research was partly sponsored
by the Army Research Laboratory (ARL) (accomplished under Cooper-
ative Agreement Number W911NF-09-2-0053) and by Air Force Re-
search Laboratory (AFRL) under prime contract no. FA8750-09-C-
0181. Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not necessarily
reflect the view of the ARL or of AFRL.
436
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In Proc. of TAC Work-
shop.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In ACL.
C. Brockett. 2007. Aligning the RTE 2006 corpus.
In Technical Report MSR-TR-2007-77, Microsoft Re-
search.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
C. Cherry and C. Quirk. 2008. Discriminative, syntactic
language modeling through latent svms. In Proc. of
the Eighth Conference of AMTA.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
ACL.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrimina-
tively trained part based models. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In ACL. Short
Paper.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In EMNLP.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In HLT-EMNLP.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural svms. Machine
Learning.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
NIPS.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Cyril Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Translation.
B. MacCartney, M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural language
inference. In EMNLP.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In UAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
EMNLP.
D. Roth, M. Sammons, and V.G. Vydiswaran. 2009. A
framework for entailed relation recognition. In ACL.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the p?ara-farceo?ut
of paraphrase. In Proc. of the Australasian Language
Technology Workshop (ALTW).
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In ICML.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In ACL.
437
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1486?1495,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Confidence Driven Unsupervised Semantic Parsing
Dan Goldwasser ? Roi Reichart ? James Clarke ? Dan Roth ?
?Department of Computer Science, University of Illinois at Urbana-Champaign
{goldwas1,clarkeje,danr}@illinois.edu
?Computer Science and Artificial Intelligence Laboratory, MIT
roiri@csail.mit.edu
Abstract
Current approaches for semantic parsing take
a supervised approach requiring a consider-
able amount of training data which is expen-
sive and difficult to obtain. This supervision
bottleneck is one of the major difficulties in
scaling up semantic parsing.
We argue that a semantic parser can be trained
effectively without annotated data, and in-
troduce an unsupervised learning algorithm.
The algorithm takes a self training approach
driven by confidence estimation. Evaluated
over Geoquery, a standard dataset for this
task, our system achieved 66% accuracy, com-
pared to 80% of its fully supervised counter-
part, demonstrating the promise of unsuper-
vised approaches for this task.
1 Introduction
Semantic parsing, the ability to transform Natural
Language (NL) input into a formal Meaning Repre-
sentation (MR), is one of the longest standing goals
of natural language processing. The importance of
the problem stems from both theoretical and practi-
cal reasons, as the ability to convert NL into a formal
MR has countless applications.
The term semantic parsing has been used ambigu-
ously to refer to several semantic tasks (e.g., se-
mantic role labeling). We follow the most common
definition of this task: finding a mapping between
NL input and its interpretation expressed in a well-
defined formal MR language. Unlike shallow se-
mantic analysis tasks, the output of a semantic parser
is complete and unambiguous to the extent it can be
understood or even executed by a computer system.
Current approaches for this task take a data driven
approach (Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007), in which the learning algorithm is
given a set of NL sentences as input and their cor-
responding MR, and learns a statistical semantic
parser ? a set of parameterized rules mapping lex-
ical items and syntactic patterns to their MR. Given
a sentence, these rules are applied recursively to de-
rive the most probable interpretation.
Since semantic interpretation is limited to the syn-
tactic patterns observed in the training data, in or-
der to work well these approaches require consider-
able amounts of annotated data. Unfortunately an-
notating sentences with their MR is a time consum-
ing task which requires specialized domain knowl-
edge and therefore minimizing the supervision ef-
fort is one of the key challenges in scaling semantic
parsers.
In this work we present the first unsupervised
approach for this task. Our model compensates
for the lack of training data by employing a self
training protocol based on identifying high confi-
dence self labeled examples and using them to re-
train the model. We base our approach on a sim-
ple observation: semantic parsing is a difficult struc-
tured prediction task, which requires learning a com-
plex model, however identifying good predictions
can be done with a far simpler model capturing re-
peating patterns in the predicted data. We present
several simple, yet highly effective confidence mea-
sures capturing such patterns, and show how to use
them to train a semantic parser without manually an-
notated sentences.
Our basic premise, that predictions with high con-
fidence score are of high quality, is further used to
improve the performance of the unsupervised train-
1486
ing procedure. Our learning algorithm takes an EM-
like iterative approach, in which the predictions of
the previous stage are used to bias the model. While
this basic scheme was successfully applied to many
unsupervised tasks, it is known to converge to a
sub optimal point. We show that by using confi-
dence estimation as a proxy for the model?s pre-
diction quality, the learning algorithm can identify
a better model compared to the default convergence
criterion.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), consist-
ing of natural language questions and their prolog
interpretations used to query a database consisting
of U.S. geographical information. Our experimental
results show that using our approach we are able to
train a good semantic parser without annotated data,
and that using a confidence score to identify good
models results in a significant performance improve-
ment.
2 Semantic Parsing
We formulate semantic parsing as a structured pre-
diction problem, mapping a NL input sentence (de-
noted x), to its highest ranking MR (denoted z). In
order to correctly parametrize and weight the pos-
sible outputs, the decision relies on an intermediate
representation: an alignment between textual frag-
ments and their meaning representation (denoted y).
Fig. 1 describes a concrete example of this termi-
nology. In our experiments the input sentences x
are natural language queries about U.S. geography
taken from the Geoquery dataset. The meaning rep-
resentation z is a formal language database query,
this output representation language is described in
Sec. 2.1.
The prediction function, mapping a sentence to its
corresponding MR, is formalized as follows:
z? = Fw(x) = arg max
y?Y,z?Z
wT?(x,y, z) (1)
Where ? is a feature function defined over an input
sentence x, alignment y and output z. The weight
vector w contains the model?s parameters, whose
values are determined by the learning process.
We refer to the arg max above as the inference
problem. Given an input sentence, solving this in-
How many states does the Colorado river run through? 
count( state( traverse( river( const(colorado))))
x 
z 
y 
Figure 1: Example of an input sentence (x), meaning rep-
resentation (z) and the alignment between the two (y) for
the Geoquery domain
ference problem based on ? and w is what com-
promises our semantic parser. In practice the pars-
ing decision is decomposed into smaller decisions
(Sec. 2.2). Sec. 4 provides more details about the
feature representation and inference procedure used.
Current approaches obtain w using annotated
data, typically consisting of (x, z) pairs. In Sec. 3 we
describe our unsupervised learning procedure, that is
how to obtain w without annotated data.
2.1 Target Meaning Representation
The output of the semantic parser is a logical for-
mula, grounding the semantics of the input sen-
tence in the domain language (i.e., the Geoquery
domain). We use a subset of first order logic con-
sisting of typed constants (corresponding to specific
states, etc.) and functions, which capture relations
between domains entities and properties of entities
(e.g., population : E ? N ). The seman-
tics of the input sentence is constructed via func-
tional composition, done by the substitution oper-
ator. For example, given the function next to(x)
and the expression const(texas), substitution
replaces the occurrence of the free variable x
with the expression, resulting in a new formula:
next to(const(texas)). For further details
we refer the reader to (Zelle and Mooney, 1996).
2.2 Semantic Parsing Decisions
The inference problem described in Eq. 1 selects the
top ranking output formula. In practice this decision
is decomposed into smaller decisions, capturing lo-
cal mapping of input tokens to logical fragments and
their composition into larger fragments. These deci-
sions are further decomposed into a feature repre-
sentation, described in Sec. 4.
The first type of decisions are encoded directly by
the alignment (y) between the input tokens and their
corresponding predicates. We refer to these as first
1487
order decisions. The pairs connected by the align-
ment (y) in Fig. 1 are examples of such decisions.
The final output structure z is constructed by
composing individual predicates into a complete
formula. For example, consider the formula pre-
sented in Fig. 1: river( const(colorado))
is a composition of two predicates river and
const(colorado). We refer to the composition
of two predicates, associated with their respective
input tokens, as second order decisions.
In order to formulate these decisions, we intro-
duce the following notation. c is a constituent in the
input sentence x and D is the set of all function and
constant symbols in the domain. The alignment y is
a set of mappings between constituents and symbols
in the domain y = {(c, s)} where s ? D.
We denote by si the i-th output predicate compo-
sition in z, by si?1(si) the composition of the (i?1)-
th predicate on the i-th predicate and by y(si) the in-
put word corresponding to that predicate according
to the alignment y.
3 Unsupervised Semantic Parsing
Our learning framework takes a self training ap-
proach in which the learner is iteratively trained over
its own predictions. Successful application of this
approach depends heavily on two important factors
- how to select high quality examples to train the
model on, and how to define the learning objective
so that learning can halt once a good model is found.
Both of these questions are trivially answered
when working in a supervised setting: by using the
labeled data for training the model, and defining the
learning objective with respect to the annotated data
(for example, loss-minimization in the supervised
version of our system).
In this work we suggest to address both of the
above concerns by approximating the quality of
the model?s predictions using a confidence measure
computed over the statistics of the self generated
predictions. Output structures which fall close to the
center of mass of these statistics will receive a high
confidence score.
The first issue is addressed by using examples as-
signed a high confidence score to train the model,
acting as labeled examples.
We also note that since the confidence score pro-
vides a good indication for the model?s prediction
performance, it can be used to approximate the over-
all model performance, by observing the model?s to-
tal confidence score over all its predictions. This
allows us to set a performance driven goal for our
learning process - return the model maximizing the
confidence score over all predictions. We describe
the details of integrating the confidence score into
the learning framework in Sec. 3.1.
Although using the model?s prediction score (i.e.,
wT?(x,y, z)) as an indication of correctness is a
natural choice, we argue and show empirically, that
unsupervised learning driven by confidence estima-
tion results in a better performing model. This
empirical behavior also has theoretical justification:
training the model using examples selected accord-
ing to the model?s parameters (i.e., the top rank-
ing structures) may not generalize much further be-
yond the existing model, as the training examples
will simply reinforce the existing model. The statis-
tics used for confidence estimation are different than
those used by the model to create the output struc-
tures, and can therefore capture additional informa-
tion unobserved by the prediction model. This as-
sumption is based on the well established idea of
multi-view learning, applied successfully to many
NL applications (Blum and Mitchell, 1998; Collins
and Singer, 1999). According to this idea if two
models use different views of the data, each of them
can enhance the learning process of the other.
The success of our learning procedure hinges
on finding good confidence measures, whose confi-
dence prediction correlates well with the true quality
of the prediction. The ability of unsupervised confi-
dence estimation to provide high quality confidence
predictions can be explained by the observation that
prominent prediction patterns are more likely to be
correct. If a non-random model produces a predic-
tion pattern multiple times it is likely to be an in-
dication of an underlying phenomenon in the data,
and therefore more likely to be correct. Our specific
choice of confidence measures is guided by the intu-
ition that unlike structure prediction (i.e., solving the
inference problem) which requires taking statistics
over complex and intricate patterns, identifying high
quality predictions can be done using much simpler
patterns that are significantly easier to capture.
In the reminder of this section we describe our
1488
Algorithm 1 Unsupervised Confidence driven
Learning
Input: Sentences {xl}Nl=1,
initial weight vector w
1: define Confidence : X ? Y ? Z ? R,
i = 0, Si = ?
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = arg maxy,zw
T?(xl,y, z)
5: Si = Si ? {xl, y?, z?}
6: end for
7: Confidence = compute confidence statistics
8: Sconfi = select from Si using Confidence
9: wi ? Learn(?iS
conf
i )
10: i = i+ 1
11: until Sconfi has no new unique examples
12: best = arg maxi(
?
s?Si
Confidence(s))/|S|
13: return wbest
learning approach. We begin by introducing the
overall learning framework (Sec. 3.1), we then ex-
plain the rational behind confidence estimation over
self-generated data and introduce the confidence
measures used in our experiments (Sec. 3.2). We
conclude with a description of the specific learning
algorithms used for updating the model (Sec. 3.3).
3.1 Unsupervised Confidence-Driven Learning
Our learning framework works in an EM-like
manner, iterating between two stages: making pre-
dictions based on its current set of parameters and
then retraining the model using a subset of the pre-
dictions, assigned high confidence. The learning
process ?discovers? new high confidence training
examples to add to its training set over multiple it-
erations, and converges when the model no longer
adds new training examples.
While this is a natural convergence criterion, it
provides no performance guarantees, and in practice
it is very likely that the quality of the model (i.e., its
performance) fluctuates during the learning process.
We follow the observation that confidence estima-
tion can be used to approximate the performance of
the entire model and return the model with the high-
est overall prediction confidence.
We describe this algorithmic framework in detail
in Alg. 1. Our algorithm takes as input a set of
natural language sentences and a set of parameters
used for making the initial predictions1. The algo-
rithm then iterates between the two stages - predict-
ing the output structure for each sentence (line 4),
and updating the set of parameters (line 9). The
specific learning algorithms used are discussed in
Sec. 3.3. The training examples required for learn-
ing are obtained by selecting high confidence exam-
ples - the algorithm first takes statistics over the cur-
rent predicted set of output structures (line 7), and
then based on these statistics computes a confidence
score for each structure, selecting the top ranked
ones as positive training examples, and if needed,
the bottom ones as negative examples (line 8). The
set of top confidence examples (for either correct or
incorrect prediction), at iteration i of the algorithm,
is denoted Sconfi . The exact nature of the confidence
computation is discussed in Sec. 3.2.
The algorithm iterates between these two stages,
at each iteration it adds more self-annotated exam-
ples to its training set, learning therefore converges
when no new examples are added (line 11). The al-
gorithm keeps track of the models it trained at each
stage throughout this process, and returns the one
with the highest averaged overall confidence score
(lines 12-13). At each stage, the overall confidence
score is computed by averaging over all the confi-
dence scores of the predictions made at that stage.
3.2 Unsupervised Confidence Estimation
Confidence estimation is calculated over a batch of
input (x) - output (z) pairs. Each pair decomposes
into smaller first order and second order decisions
(defined Sec. 2.2). Confidence estimation is done by
computing the statistics of these decisions, over the
entire set of predicted structures. In the rest of this
section we introduce the confidence measures used
by our system.
Translation Model The first approach essentially
constructs a simplified translation model, capturing
word-to-predicate mapping patterns. This can be
considered as an abstraction of the prediction model:
we collapse the intricate feature representation into
1Since we commit to the max-score output prediction, rather
than summing over all possibilities, we require a reasonable ini-
tialization point. We initialized the weight vector using simple,
straight-forward heuristics described in Sec. 5.
1489
high level decisions and take statistics over these de-
cisions. Since it takes statistics over considerably
less variables than the actual prediction model, we
expect this model to make reliable confidence pre-
dictions. We consider two variations of this ap-
proach, the first constructs a unigram model over the
first order decisions and the second a bigram model
over the second order decisions. Formally, given a
set of predicted structures we define the following
confidence scores:
Unigram Score:
p(z|x) =
|z|?
i=1
p(si|y(si))
Bigram Score:
p(z|x) =
|z|?
i=1
p(si?1(si)|y(si?1), y(si))
Structural Proportion Unlike the first approach
which decomposes the predicted structure into in-
dividual decisions, this approach approximates the
model?s performance by observing global properties
of the structure. We take statistics over the propor-
tion between the number of predicates in z and the
number of words in x.
Given a set of structure predictions S, we com-
pute this proportion for each structure (denoted as
Prop(x, z)) and calculate the average proportion
over the entire set (denoted as AvProp(S)). The
confidence score assigned to a given structure (x,y)
is simply the difference between its proportion and
the averaged proportion, or formally
PropScore(S, (x, z)) = AvProp(S)?Prop(x, z)
This measure captures the global complexity of the
predicted structure and penalizes structures which
are too complex (high negative values) or too sim-
plistic (high positive values).
Combined The two approaches defined above
capture different views of the data, a natural question
is then - can these two measures be combined to pro-
vide a more powerful estimation? We suggest a third
approach which combines the first two approaches.
It first uses the score produced by the latter approach
to filter out unlikely candidates, and then ranks the
remaining ones with the former approach and selects
those with the highest rank.
3.3 Learning Algorithms
Given a set of self generated structures, the param-
eter vector can be updated (line 9 in Alg. 1). We
consider two learning algorithm for this purpose.
The first is a binary learning algorithm, which
considers learning as a classification problem, that
is finding a set of weights w that can best sepa-
rate correct from incorrect structures. The algo-
rithm decomposes each predicted formula and its
corresponding input sentence into a feature vector
?(x,y, z) normalized by the size of the input sen-
tence |x|, and assigns a binary label to this vector2.
The learning process is defined over both positive
and negative training examples. To accommodate
that we modify line 8 in Alg. 1, and use the con-
fidence score to select the top ranking examples as
positive examples, and the bottom ranking examples
as negative examples. We use a linear kernel SVM
with squared-hinge loss as the underlying learning
algorithm.
The second is a structured learning algorithm
which considers learning as a ranking problem, i.e.,
finding a set of weights w such that the ?gold struc-
ture? will be ranked on top, preferably by a large
margin to allow generalization.The structured learn-
ing algorithm can directly use the top ranking pre-
dictions of the model (line 8 in Alg. 1) as training
data. In this case the underlying algorithm is a struc-
tural SVM with squared-hinge loss, using hamming
distance as the distance function. We use the cutting-
plane method to efficiently optimize the learning
process? objective function.
4 Model
Semantic parsing as formulated in Eq. 1 is an in-
ference procedure selecting the top ranked output
logical formula. We follow the inference approach
in (Roth and Yih, 2007; Clarke et al, 2010) and
formalize this process as an Integer Linear Program
(ILP). Due to space consideration we provide a brief
description, and refer the reader to that paper for
more details.
2Without normalization longer sentences would have more
influence on binary learning problem. Normalization is there-
fore required to ensure that each sentence contributes equally to
the binary learning problem regardless of its length.
1490
4.1 Inference
The inference decision (Eq. 1) is decomposed into
smaller decisions, capturing mapping of input to-
kens to logical fragments (first order) and their com-
position into larger fragments (second order). We
encode a first-order decision as ?cs, a binary vari-
able indicating that constituent c is aligned with the
logical symbol s. A second-order decision ?cs,dt, is
encoded as a binary variable indicating that the sym-
bol t (associated with constituent d) is an argument
of a function s (associated with constituent c). We
frame the inference problem over these decisions:
Fw(x) = arg max
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?w
T?2(x, c, s, d, t) (2)
We restrict the possible assignments to the deci-
sion variables, forcing the resulting output formula
to be syntactically legal, for example by restricting
active ?-variables to be type consistent, and force
the resulting functional composition to be acyclic.
We take advantage of the flexible ILP framework,
and encode these restrictions as global constraints
over Eq. 2. We refer the reader to (Clarke et al,
2010) for a full description of the constraints used.
4.2 Features
The inference problem defined in Eq. (2) uses two
feature functions: ?1 and ?2.
First-order decision features ?1 Determining if
a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.3 Existing ap-
proaches rely on annotated data to extend the lexi-
con. Instead we rely on external knowledge (Miller
et al, 1990) and add features which measure the lex-
ical similarity between a constituent and a logical
symbol?s surface forms (as defined by the lexicon).
3The lexicon contains on average 1.42 words per function
and 1.07 words per constant.
Model Description
INITIAL MODEL Manually set weights (Sec. 5.1)
PRED. SCORE normalized prediction (Sec. 5.1)
ALL EXAMPLES All top structures (Sec. 5.1)
UNIGRAM Unigram score (Sec. 3.2)
BIGRAM Bigram score (Sec. 3.2)
PROPORTION Words-predicate prop (Sec. 3.2)
COMBINED Combined estimators (Sec. 3.2)
RESPONSE BASED Supervised (binary) (Sec. 5.1)
SUPERVISED Fully Supervised (Sec. 5.1)
Table 1: Compared systems and naming conventions.
Second-order decision features ?2 Second order
decisions rely on syntactic information. We use
the dependency tree of the input sentence. Given
a second-order decision ?cs,dt, the dependency fea-
ture takes the normalized distance between the head
words in the constituents c and d. In addition, a set
of features indicate which logical symbols are usu-
ally composed together, without considering their
alignment to the text.
5 Experiments
In this section we describe our experimental evalua-
tion. We compare several confidence measures and
analyze their properties. Tab. 1 defines the naming
conventions used throughout this section to refer to
the different models we evaluated. We begin by de-
scribing our experimental setup and then proceed to
describe the experiments and their results. For the
sake of clarity we focus on the best performing mod-
els (COMBINED using BIGRAM and PROPORTION)
first and discuss other models later in the section.
5.1 Experimental Settings
In all our experiments we used the Geoquery
dataset (Zelle and Mooney, 1996), consisting of U.S.
geography NL questions and their corresponding
Prolog logical MR. We used the data split described
in (Clarke et al, 2010), consisting of 250 queries for
evaluation purposes. We compared our system to
several supervised models, which were trained us-
ing a disjoint set of queries. Our learning system
had access only to the NL questions, and the log-
ical forms were only used to evaluate the system?s
performance. We report the proportion of correct
structures (accuracy). Note that this evaluation cor-
1491
responds to the 0/1 loss over the predicted structures.
Initialization Our learning framework requires an
initial weight vector as input. We use a straight for-
ward heuristic and provide uniform positive weights
to three features. This approach is similar in spirit
to previous works (Clarke et al, 2010; Zettlemoyer
and Collins, 2007). We refer to this system as INI-
TIAL MODEL throughout this section.
Competing Systems We compared our system to
several other systems:
(1) PRED. SCORE: An unsupervised frame-
work using the model?s internal prediction score
(wT?(x,y, z)) for confidence estimation.
(2) ALL EXAMPLES: Treating all predicted struc-
tures as correct, i.e., at each iteration the model is
trained over all the predictions it made. The re-
ported score was obtained by selecting the model at
the training iteration with the highest overall confi-
dence score (see line 12 in Alg. 1).
(3) RESPONSE BASED: A natural upper bound to
our framework is the approach used in (Clarke et al,
2010). While our approach is based on assessing
the correctness os the model?s predictions according
to unsupervised confidence estimation, their frame-
work is provided with external supervision for these
decisions, indicating if the predicted structures are
correct.
(4) SUPERVISED: A fully supervised framework
trained over 250 (x, z) pairs using structured SVM.
5.2 Results
Our experiments aim to clarify three key points:
(1) Can a semantic parser indeed be trained with-
out any form of external supervision? this is our
key question, as this is the first attempt to approach
this task with an unsupervised learning protocol.4 In
order to answer it, we report the overall performance
of our system in Tab. 2.
The manually constructed model INITIALMODEL
achieves a performance of 0.22. We can expect
learning to improve on this baseline. We com-
pare three self-trained systems, ALL EXAMPLES,
PREDICTIONSCORE and COMBINED, which differ
4While unsupervised learning for various semantic tasks has
been widely discussed, this is the first attempt to tackle this task.
We refer the reader to Sec. 6 for further discussion of this point.
in their sample selection strategy, but all use con-
fidence estimation for selecting the final seman-
tic parsing model. The ALL EXAMPLES approach
achieves an accuracy score of 0.656. PREDICTION-
SCORE only achieves a performance of 0.164 us-
ing the binary learning algorithm and 0.348 us-
ing the structured learning algorithm. Finally, our
confidence-driven technique COMBINED achieved a
score of 0.536 for the binary case and 0.664 for the
structured case, the best performing models in both
cases. As expected, the supervised systems RE-
SPONSE BASED and SUPERVISED achieve the best
performance.
These results show that training the model with
training examples selected carefully will improve
learning - as the best performance is achieved with
perfect knowledge of the predictions correctness
(RESPONSE BASED). Interestingly the difference
between the structured version of our system and
that of RESPONSE BASED is only 0.07, suggesting
that we can recover the binary feedback signal with
high precision. The low performance of the PRE-
DICTIONSCORE model is also not surprising, and it
demonstrates one of the key principles in confidence
estimation - the score should be comparable across
predictions done over different inputs, and not the
same input, as done in PREDICTIONSCORE model.
(2) How does confidence driven sample selection
contribute to the learning process? Comparing
the systems driven by confidence sample-selection
to the ALL EXAMPLES approach uncovers an inter-
esting tradeoff between training with more (noisy)
data and selectively training the system with higher
quality examples. We argue that carefully select-
ing high quality training examples will result in bet-
ter performance. The empirical results indeed sup-
port our argument, as the best performing model
(RESPONSE BASED) is achieved by sample selec-
tion with perfect knowledge of prediction correct-
ness. The confidence-based sample selection system
(COMBINED) is the best performing system out of
all the self-trained systems. Nonetheless, the ALL
EXAMPLES strategy performs well when compared
to COMBINED, justifying a closer look at that aspect
of our system.
We argue that different confidence measures cap-
ture different properties of the data, and hypothe-
1492
size that combining their scores will improve the re-
sulting model. In Tab. 3 we compare the results of
the COMBINED measure to the results of its individ-
ual components - PROPORTION and BIGRAM. We
compare these results both when using the binary
and structured learning algorithms. Results show
that using the COMBINED measure leads to an im-
proved performance, better than any of the individ-
ual measures, suggesting that it can effectively ex-
ploit the properties of each confidence measure. Fur-
thermore, COMBINED is the only sample selection
strategy that outperforms ALL EXAMPLES.
(3) Can confidence measures serve as a good
proxy for the model?s performance? In the unsu-
pervised settings we study the learning process may
not converge to an optimal model. We argue that
by selecting the model that maximizes the averaged
confidence score, a better model can be found. We
validate this claim empirically in Tab. 4. We com-
pare the performance of the model selected using
the confidence score to the performance of the fi-
nal model considered by the learning algorithm (see
Sec. 3.1 for details). We also compare it to the best
model achieved in any of the learning iterations.
Since these experiments required running the
learning algorithm many times, we focused on the
binary learning algorithm as it converges consider-
ably faster. In order to focus the evaluation on the
effects of learning, we ignore the initial model gen-
erated manually (INITIAL MODEL) in these exper-
iments. In order to compare models performance
across the different iterations fairly, a uniform scale,
such as UNIGRAM and BIGRAM, is required. In the
case of the COMBINED measure we used the BI-
GRAM measure for performance estimation, since it
is one of its underlying components. In the PRED.
SCORE and PROPORTION models we used both their
confidence prediction, and the simple UNIGRAM
confidence score to evaluate model performance (the
latter appear in parentheses in Tab. 4).
Results show that the over overall confidence
score serves as a reliable proxy for the model perfor-
mance - using UNIGRAM and BIGRAM the frame-
work can select the best performing model, far better
than the performance of the default model to which
the system converged.
Algorithm Supervision Acc.
INITIAL MODEL ? 0.222
SELF-TRAIN: (Structured)
PRED. SCORE ? 0.348
ALL EXAMPLES ? 0.656
COMBINED ? 0.664
SELF-TRAIN: (Binary)
PRED. SCORE ? 0.164
COMBINED ? 0.536
RESPONSE BASED
BINARY 250 (binary) 0.692
STRUCTURED 250 (binary) 0.732
SUPERVISED
STRUCTURED 250 (struct.) 0.804
Table 2: Comparing our Self-trained systems with
Response-based and supervised models. Results show
that our COMBINED approach outperforms all other un-
supervised models.
Algorithm Accuracy
SELF-TRAIN: (Structured)
PROPORTION 0.6
BIGRAM 0.644
COMBINED 0.664
SELF-TRAIN: (Binary)
BIGRAM 0.532
PROPORTION 0.504
COMBINED 0.536
Table 3: Comparing COMBINED to its components BI-
GRAM and PROPORTION. COMBINED results in a better
score than any of its components, suggesting that it can
exploit the properties of each measure effectively.
Algorithm Best Conf. estim. Default
PRED. SCORE 0.164 0.128 (0.164) 0.134
UNIGRAM 0.52 0.52 0.4
BIGRAM 0.532 0.532 0.472
PROPORTION 0.504 0.27 (0.504) 0.44
COMBINED 0.536 0.536 0.328
Table 4: Using confidence to approximate model perfor-
mance. We compare the best result obtained in any of the
learning algorithm iterations (Best), the result obtained
by approximating the best result using the averaged pre-
diction confidence (Conf. estim.) and the result of us-
ing the default convergence criterion (Default). Results
in parentheses are the result of using the UNIGRAM con-
fidence to approximate the model?s performance.
1493
6 Related Work
Semantic parsing has attracted considerable interest
in recent years. Current approaches employ various
machine learning techniques for this task, such as In-
ductive Logic Programming in earlier systems (Zelle
and Mooney, 1996; Tang and Mooney, 2000) and
statistical learning methods in modern ones (Ge and
Mooney, 2005; Nguyen et al, 2006; Wong and
Mooney, 2006; Kate and Mooney, 2006; Zettle-
moyer and Collins, 2005; Zettlemoyer and Collins,
2007; Zettlemoyer and Collins, 2009).
The difficulty of providing the required supervi-
sion motivated learning approaches using weaker
forms of supervision. (Chen and Mooney, 2008;
Liang et al, 2009; Branavan et al, 2009; Titov and
Kozhevnikov, 2010) ground NL in an external world
state directly referenced by the text. The NL input in
our setting is not restricted to such grounded settings
and therefore we cannot exploit this form of supervi-
sion. Recent work (Clarke et al, 2010; Liang et al,
2011) suggest using response-based learning proto-
cols, which alleviate some of the supervision effort.
This work takes an additional step in this direction
and suggest an unsupervised protocol.
Other approaches to unsupervised semantic anal-
ysis (Poon and Domingos, 2009; Titov and Kle-
mentiev, 2011) take a different approach to seman-
tic representation, by clustering semantically equiv-
alent dependency tree fragments, and identifying
their predicate-argument structure. While these ap-
proaches have been applied successfully to semantic
tasks such as question answering, they do not ground
the input in a well defined output language, an essen-
tial component in our task.
Our unsupervised approach follows a self training
protocol (Yarowsky, 1995; McClosky et al, 2006;
Reichart and Rappoport, 2007b) enhanced with con-
straints restricting the output space (Chang et al,
2007; Chang et al, 2009). A Self training proto-
col uses its own predictions for training. We esti-
mate the quality of the predictions and use only high
confidence examples for training. This selection cri-
terion provides an additional view, different than the
one used by the prediction model. Multi-view learn-
ing is a well established idea, implemented in meth-
ods such as co-training (Blum and Mitchell, 1998).
Quality assessment of a learned model output was
explored by many previous works (see (Caruana and
Niculescu-Mizil, 2006) for a survey), and applied
to several NL processing tasks such as syntactic
parsing (Reichart and Rappoport, 2007a; Yates et
al., 2006), machine translation (Ueffing and Ney,
2007), speech (Koo et al, 2001), relation extrac-
tion (Rosenfeld and Feldman, 2007), IE (Culotta and
McCallum, 2004), QA (Chu-Carroll et al, 2003)
and dialog systems (Lin and Weng, 2008).
In addition to sample selection we use confidence
estimation as a way to approximate the overall qual-
ity of the model and use it for model selection. This
use of confidence estimation was explored in (Re-
ichart et al, 2010), to select between models trained
with different random starting points. In this work
we integrate this estimation deeper into the learning
process, thus allowing our training procedure to re-
turn the best performing model.
7 Conclusions
We introduced an unsupervised learning algorithm
for semantic parsing, the first for this task to the best
of our knowledge. To compensate for the lack of
training data we use a self-training protocol, driven
by unsupervised confidence estimation. We demon-
strate empirically that our approach results in a high
preforming semantic parser and show that confi-
dence estimation plays a vital role in this success,
both by identifying good training examples as well
as identifying good over all performance, used to
improve the final model selection.
In future work we hope to further improve un-
supervised semantic parsing performance. Particu-
larly, we intend to explore new approaches for confi-
dence estimation and their usage in the unsupervised
and semi-supervised versions of the task.
Acknowledgments We thank the anonymous re-
viewers for their helpful feedback. This material
is based upon work supported by DARPA under
the Bootstrap Learning Program and Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the view
of the DARPA, AFRL, or the US government.
1494
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In ACL.
R. Caruana and A. Niculescu-Mizil. 2006. An empiri-
cal comparison of supervised l earning algorithms. In
ICML.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the ACL.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
D. Chen and R. Mooney. 2008. Learning to sportscast: a
test of grounded language acquisition. In ICML.
J. Chu-Carroll, J. Prager K. Czuba, and A. Ittycheriah.
2003. In question answering, two heads are better than
on. In HLT-NAACL.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL, 7.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP?VLC.
A. Culotta and A. McCallum. 2004. Confidence estima-
tion for information extraction. In HLT-NAACL.
R. Ge and R. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In CoNLL.
R. Kate and R. Mooney. 2006. Using string-kernels for
learning semantic parsers. In ACL.
Y. Koo, C. Lee, and B. Juang. 2001. Speech recogni-
tion and utterance verification based on a generalized
confidence score. IEEE Transactions on Speech and
Audio Processing, 9(8):821?832.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
ACL.
P. Liang, M.I. Jordan, and D. Klein. 2011. Deep compo-
sitional semantics from shallow supervision. In ACL.
F. Lin and F. Weng. 2008. Computing confidence scores
for all sub parse trees. In ACL.
D. McClosky, E. Charniak, and Mark Johnson. 2006.
Effective self-training for parsing. In HLT-NAACL.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J.
Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Seman-
tic parsing with structured svm ensemble classification
models. In ACL.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In EMNLP.
R. Reichart and A. Rappoport. 2007a. An ensemble
method for selection of high quality parses. In ACL.
R. Reichart and A. Rappoport. 2007b. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
R. Reichart, R. Fattal, and A. Rappoport. 2010. Im-
proved unsupervised pos induction using intrinsic
clustering quality and a zipfian constraint. In CoNLL.
B. Rosenfeld and R. Feldman. 2007. Using corpus statis-
tics on entities to improve semi?supervised relation
extraction from the web. In ACL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construction
of database interfaces: integrating statistical and rela-
tional learning for semantic parsing. In EMNLP.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In ECML.
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In ACL.
I. Titov and M. Kozhevnikov. 2010. Bootstrapping
semantic analyzers from non-contradictory texts. In
ACL.
N. Ueffing and H. Ney. 2007. Word-level confidence es-
timation for machine translation. Computational Lin-
guistics, 33(1):9?40.
Y.W. Wong and R. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
NAACL.
Y.W. Wong and R. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised method. In ACL.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In EMNLP.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
AAAI.
L. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI.
L. Zettlemoyer and M. Collins. 2007. Online learning of
relaxed CCG grammars for parsing to logical form. In
CoNLL.
L. Zettlemoyer and M. Collins. 2009. Learning context-
dependent mappings from sentences to logical form.
In ACL.
1495
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 462?466,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Leveraging Domain-Independent Information in Semantic Parsing
Dan Goldwasser
University of Maryland
College Park, MD 20740
goldwas1@umiacs.umd.edu
Dan Roth
University of Illinois
Urbana, IL 61801
danr@illinois.edu
Abstract
Semantic parsing is a domain-dependent
process by nature, as its output is defined
over a set of domain symbols. Motivated
by the observation that interpretation can
be decomposed into domain-dependent
and independent components, we suggest
a novel interpretation model, which aug-
ments a domain dependent model with ab-
stract information that can be shared by
multiple domains. Our experiments show
that this type of information is useful and
can reduce the annotation effort signifi-
cantly when moving between domains.
1 Introduction
Natural Language (NL) understanding can be intu-
itively understood as a general capacity, mapping
words to entities and their relationships. However,
current work on automated NL understanding
(typically referenced as semantic parsing (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2007; Chen and Mooney, 2008; Kwiatkowski et
al., 2010; Bo?rschinger et al, 2011)) is restricted
to a given output domain1 (or task) consisting of a
closed set of meaning representation symbols, de-
scribing domains such as robotic soccer, database
queries and flight ordering systems.
In this work, we take a first step towards con-
structing a semantic interpreter that can leverage
information from multiple tasks. This is not a
straight forward objective ? the domain specific
nature of semantic interpretation, as described in
the current literature, does not allow for an easy
move between domains. For example, a sys-
tem trained for the task of understanding database
queries will not be of any use when it will be given
a sentence describing robotic soccer instructions.
In order to understand this difficulty, a closer
look at semantic parsing is required. Given a sen-
tence, the interpretation process breaks it into a
1The term domain is overloaded in NLP; in this work we
use it to refer to the set of output symbols.
set of interdependent decisions, which rely on an
underlying representation mapping words to sym-
bols and syntactic patterns into compositional de-
cisions. This representation takes into account do-
main specific information (e.g., a lexicon mapping
phrases to a domain predicate) and is therefore of
little use when moving to a different domain.
In this work, we attempt to develop a domain in-
dependent approach to semantic parsing. We do it
by developing a layer of representation that is ap-
plicable to multiple domains. Specifically, we add
an intermediate layer capturing shallow semantic
relations between the input sentence constituents.
Unlike semantic parsing which maps the input to
a closed set of symbols, this layer can be used to
identify general predicate-argument structures in
the input sentence.The following example demon-
strates the key idea behind our representation ?
two sentences from two different domains have a
similar intermediate structure.
Example 1. Domains with similar intermediate structures
? The [Pink goalie]ARG [kicks]PRED to [Pink11]ARG
pass(pink1, pink11)
? [She]ARG [walks]PRED to the [kitchen]ARG
go(sister, kitchen)
In this case, the constituents of the first
sentence (from the Robocup domain (Chen
and Mooney, 2008)), are assigned domain-
independent predicate-argument labels (e.g., the
word corresponding to a logical function is identi-
fied as a PRED). Note that it does not use any do-
main specific information, for example, the PRED
label assigned to the word ?kicks? indicates that
this word is the predicate of the sentence, not a
specific domain predicate (e.g., pass(?)). The in-
termediate layer can be reused across domains.
The logical output associated with the second sen-
tence is taken from a different domain, using a dif-
ferent set of output symbols, however it shares the
same predicate-argument structure.
Despite the idealized example, in practice,
462
leveraging this information is challenging, as the
logical structure is assumed to only weakly corre-
spond to the domain-independent structure, a cor-
respondence which may change in different do-
mains. The mismatch between the domain in-
dependent (linguistic) structure and logical struc-
tures typically stems from technical considera-
tions, as the domain logical language is designed
according to an application-specific logic and not
according to linguistic considerations. This situa-
tion is depicted in the following example, in which
one of the domain-independent labels is omitted.
? The [Pink goalie]ARG [kicks]PRED the [ball]ARG to [Pink11]ARG
pass(pink1, pink11)
In order to overcome this difficulty, we suggest
a flexible model that is able to leverage the super-
vision provided in one domain to learn an abstract
intermediate layer, and show empirically that it
learns a robust model, improving results signifi-
cantly in a second domain.
2 Semantic Interpretation Model
Our model consists of both domain-dependent
(mapping between text and a closed set of sym-
bols) and domain independent (abstract predicate-
argument structures) information. We formulate
the joint interpretation process as a structured pre-
diction problem, mapping a NL input sentence (x),
to its highest ranking interpretation and abstract
structure (y). The decision is quantified using a
linear objective, which uses a vector w, mapping
features to weights and a feature function ? which
maps the output decision to a feature vector. The
output interpretation y is described using a sub-
set of first order logic, consisting of typed con-
stants (e.g., robotic soccer player), functions cap-
turing relations between entities, and their prop-
erties (e.g., pass(x, y), where pass is a function
symbol and x, y are typed arguments). We use
data taken from two grounded domains, describing
robotic soccer events and household situations.
We begin by formulating the domain-specific
process. We follow (Goldwasser et al, 2011;
Clarke et al, 2010) and formalize semantic infer-
ence as an Integer Linear Program (ILP). Due to
space consideration, we provide a brief descrip-
tion (see (Clarke et al, 2010) for more details).
We then proceed to augment this model with
domain-independent information, and connect the
two models by constraining the ILP model.
2.1 Domain-Dependent Model
Interpretation is composed of several decisions,
capturing mapping of input tokens to logical frag-
ments (first order) and their composition into
larger fragments (second). We encode a first-order
decision as ?cs, a binary variable indicating that
constituent c is aligned with the logical symbol s.
A second-order decision ?cs,dt, is encoded as a bi-
nary variable indicating that the symbol t (associ-
ated with constituent d) is an argument of a func-
tion s (associated with constituent c). The overall
inference problem (Eq. 1) is as follows:
Fw(x) = arg max?,?
?
c?x
?
s?D ?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D ?cs,dt ?wT?2(x, c, s, d, t) (1)
We restrict the possible assignments to the deci-
sion variables, forcing the resulting output formula
to be syntactically legal, for example by restrict-
ing active ?-variables to be type consistent, and
forcing the resulting functional composition to be
acyclic and fully connected (we refer the reader to
(Clarke et al, 2010) for more details). We take ad-
vantage of the flexible ILP framework and encode
these restrictions as global constraints.
Features We use two types of feature, first-order
?1 and second-order ?2. ?1 depends on lexical
information: each mapping of a lexical item c to a
domain symbol s generates a feature. In addition
each combination of a lexical item c and an sym-
bol type generates a feature.
?2 captures a pair of symbols and their alignment
to lexical items. Given a second-order decision
?cs,dt, a feature is generated considering the nor-
malized distance between the head words in the
constituents c and d. Another feature is gener-
ated for every composition of symbols (ignoring
the alignment to the text).
2.2 Domain-Independent Information
We enhance the decision process with informa-
tion that abstracts over the attributes of specific
domains by adding an intermediate layer consist-
ing of the predicate-argument structure of the sen-
tence. Consider the mappings described in Exam-
ple 1. Instead of relying on the mapping between
Pink goalie and pink1, this model tries to iden-
tify an ARG using different means. For example, the
fact that it is preceded by a determiner, or capital-
ized provide useful cues. We do not assume any
language specific knowledge and use features that
help capture these cues.
463
This information is used to assist the overall
learning process. We assume that these labels cor-
respond to a binding to some logical symbol, and
encode it as a constraint forcing the relations be-
tween the two models. Moreover, since learning
this layer is a by-product of the learning process
(as it does not use any labeled data) forcing the
connection between the decisions is the mecha-
nism that drives learning this model.
Our domain-independent layer bears some
similarity to other semantic tasks, most no-
tably Semantic-Role Labeling (SRL) introduced
in (Gildea and Jurafsky, 2002), in which identi-
fying the predicate-argument structure is consid-
ered a preprocessing step, prior to assigning ar-
gument labels. Unlike SRL, which aims to iden-
tify linguistic structures alone, in our framework
these structures capture both natural-language and
domain-language considerations.
Domain-Independent Decision Variables We
add two new types of decisions abstracting over
the domain-specific decisions. We encode the new
decisions as ?c and ?cd. The first (?) captures local
information helping to determine if a given con-
stituent c is likely to have a label (i.e., ?Pc for pred-
icate or ?Ac for argument). The second (?) consid-
ers higher level structures, quantifying decisions
over both the labels of the constituents c,d as a
predicate-argument pair. Note, a given word c can
be labeled as PRED or ARG if ?c and ?cd are active.
Model?s Features We use the following fea-
tures: (1) Local Decisions ?3(?(c)) use a feature
indicating if c is capitalized, a set of features cap-
turing the context of c (window of size 2), such
as determiner and quantifier occurrences. Finally
we use a set of features capturing the suffix letters
of c, these features are useful in identifying verb
patterns. Features indicate if c is mapped to an ARG
or PRED. (2) Global Decision ?4(?(c, d)): a feature
indicating the relative location of c compared to d
in the input sentence. Additional features indicate
properties of the relative location, such as if the
word appears initially or finally in the sentence.
Combined Model In order to consider both
types of information we augment our decision
model with the new variables, resulting in the fol-
lowing objective function (Eq. 2).
Fw(x) = arg max?,?
?
c?x
?
s?D ?cs?w1T?1(x, c, s)+
?
c,d?x
?
s,t?D
?
i,j ?csi,dtj ? w2T?2(x, c, si, d, tj) +?
c?x ?c ?w3T?3(x, c)+
?
c,d?x ?cd ?w4T?4(x, c, d) (2)
For notational convenience we decompose the
weight vector w into four parts, w1,w2 for fea-
tures of (first, second) order domain-dependent de-
cisions, and similarly for the independent ones.
In addition, we also add new constraints tying
these new variables to semantic interpretation :
?c ? x (?c ? ?c,s1 ? ?c,s2 ? ... ? ?c,sn)
?c ? x, ?d ? x (?c,d ? ?c,s1,dt1??c,s2,dt1?...??c,sn,dtn)
(where n is the length of x).
2.3 Learning the Combined Model
The supervision to the learning process is given
via data consisting of pairs of sentences and (do-
main specific) semantic interpretation. Given that
we have introduced additional variables that cap-
ture the more abstract predicate-argument struc-
ture of the text, we need to induce these as la-
tent variables. Our decision model maps an input
sentence x, into a logical output y and predicate-
argument structure h. We are only supplied with
training data pertaining to the input (x) and out-
put (y). We use a variant of the latent structure
perceptron to learn in these settings2.
3 Experimental Settings
Situated Language This dataset, introduced in
(Bordes et al, 2010), describes situations in a sim-
ulated world. The dataset consists of triplets of the
form - (x,u, y), where x is a NL sentence describ-
ing a situation (e.g., ?He goes to the kitchen?), u
is a world state consisting of grounded relations
(e.g., loc(John, Kitchen)) description, and y is
a logical interpretation corresponding to x.
The original dataset was used for concept tag-
ging, which does not include a compositional as-
pect. We automatically generated the full logical
structure by mapping the constants to function ar-
guments. We generated additional function sym-
bols of the same relation, but of different arity
when needed 3. Our new dataset consists of 25 re-
lation symbols (originally 15). In our experiments
we used a set of 5000 of the training triplets.
Robocup The Robocup dataset, originally in-
troduced in (Chen and Mooney, 2008), describes
robotic soccer events. The dataset was collected
for the purpose of constructing semantic parsers
from ambiguous supervision and consists of both
?noisy? and gold labeled data. The noisy dataset
2Details omitted, see (Chang et al, 2010) for more details.
3For example, a unary relation symbol for ?He plays?,
and a binary for ?He plays with a ball?.
464
System Training Procedure
DOM-INIT w1: Noisy probabilistic model, described below.
PRED-ARGS Onlyw3,w4 Trained over the Situ. dataset.
COMBINEDRL w1,w2,w3,w4:learned from Robocup gold
COMBINEDRI+S w3,w4: learned from the Situ. dataset,
w1 uses the DOM-INIT Robocup model.
COMBINEDRL+S w3,w4: Initially learned over the Situ. dataset,
updated jointly with w1,w2 over Robocup gold
Table 1: Evaluated System descriptions.
was constructed by temporally aligning a stream
of soccer events occurring during a robotic soc-
cer match with human commentary describing the
game. This dataset consists of pairs (x, {y0, yk}),
x is a sentence and {y0, yk} is a set of events (log-
ical formulas). One of these events is assumed to
correspond to the comment, however this is not
guaranteed. The gold labeled labeled data con-
sists of pairs (x, y). The data was collected from
four Robocup games. In our experiments we fol-
low other works and use 4-fold cross validation,
training over 3 games and testing over the remain-
ing game. We evaluate the Accuracy of the parser
over the test game data.4 Due to space consider-
ations, we refer the reader to (Chen and Mooney,
2008) for further details about this dataset.
Semantic Interpretation Tasks We consider
two of the tasks described in (Chen and Mooney,
2008) (1) Semantic Parsing requires generating
the correct logical form given an input sentence.
(2) Matching, given a NL sentence and a set of
several possible interpretation candidates, the sys-
tem is required to identify the correct one. In all
systems, the source for domain-independent infor-
mation is the Situated domain, and the results are
evaluated over the Robocup domain.
Experimental Systems We tested several vari-
ations, all solving Eq. 2, however different re-
sources were used to obtain Eq. 2 parameters (see
sec. 2.2). Tab. 1 describes the different varia-
tions. We used the noisy Robocup dataset to ini-
tialize DOM-INIT, a noisy probabilistic model, con-
structed by taking statistics over the noisy robocup
data and computing p(y|x). Given the training set
{(x, {y1, .., yk})}, every word in x is aligned to
every symbol in every y that is aligned with it. The
probability of a matching (x, y)is computed as the
product: ?ni=1 p(yi|xi), where n is the number
of symbols appearing in y, and xi, yi is the word
4In our model accuracy is equivalent to F-measure.
System Matching Parsing
PRED-ARGS 0.692 ?
DOM-INIT 0.823 0.357
COMBINEDRI+S 0.905 0.627
(BO?RSCHINGER ET AL., 2011) ? 0.86
(KIM AND MOONEY, 2010) 0.885 0.742
Table 2: Results for the matching and parsing tasks. Our
system performs well on the matching task without any do-
main information. Results for both parsing and matching
tasks show that using domain-independent information im-
proves results dramatically.
level matching to a logical symbol. Note that this
model uses lexical information only.
4 Knowledge Transfer Experiments
We begin by studying the role of domain-
independent information when very little domain
information is available. Domain-independent in-
formation is learned from the situated domain
and domain-specific information (Robocup) avail-
able is the simple probabilistic model (DOM-INIT).
This model can be considered as a noisy proba-
bilistic lexicon, without any domain-specific com-
positional information, which is only available
through domain-independent information.
The results, summarized in Table 2, show that
in both tasks domain-independent information is
extremely useful and can make up for missing do-
main information. Most notably, performance for
the matching task using only domain independent
information (PRED-ARGS) was surprisingly good,
with an accuracy of 0.69. Adding domain-specific
lexical information (COMBINEDRI+S) pushes this
result to over 0.9, currently the highest for this task
? achieved without domain specific learning.
The second set of experiments study whether
using domain independent information, when rel-
evant (gold) domain-specific training data is avail-
able, improves learning. In this scenario, the
domain-independent model is updated according
to training data available for the Robocup domain.
We compare two system over varying amounts
of training data (25, 50, 200 training samples
and the full set of 3 Robocup games), one boot-
strapped using the Situ. domain (COMBINEDRL+S)
and one relying on the Robocup training data
alone (COMBINEDRL). The results, summarized in
table 3, consistently show that transferring domain
independent information is helpful, and helps push
the learned models beyond the supervision offered
by the relevant domain training data. Our final
system, trained over the entire dataset achieves a
465
System # training Parsing
COMBINEDRL+S (COMBINEDRL) 25 0.16 (0.03)
COMBINEDRL+S (COMBINEDRL) 50 0.323 (0.16)
COMBINEDRL+S (COMBINEDRL) 200 0.385 (0.36)
COMBINEDRL+S (COMBINEDRL) full game 0.86 (0.79)
(CHEN ET AL., 2010) full game 0.81
Table 3: Evaluating our model in a learning settings. The
domain-independent information is used to bootstrap learn-
ing from the Robocup domain. Results show that this infor-
mation improves performance significantly, especially when
little data is available
score of 0.86, significantly outperforming (Chen
et al, 2010), a competing supervised model. It
achieves similar results to (Bo?rschinger et al,
2011), the current state-of-the-art for the pars-
ing task over this dataset. The system used in
(Bo?rschinger et al, 2011) learns from ambigu-
ous training data and achieves this score by using
global information. We hypothesize that it can be
used by our model and leave it for future work.
5 Conclusions
In this paper, we took a first step towards a new
kind of generalization in semantic parsing: con-
structing a model that is able to generalize to a
new domain defined over a different set of sym-
bols. Our approach adds an additional hidden
layer to the semantic interpretation process, cap-
turing shallow but domain-independent semantic
information, which can be shared by different do-
mains. Our experiments consistently show that
domain-independent knowledge can be transferred
between domains. We describe two settings; in
the first, where only noisy lexical-level domain-
specific information is available, we observe that
the model learned in the other domain can be used
to make up for the missing compositional infor-
mation. For example, in the matching task, even
when no domain information is available, iden-
tifying the abstract predicate argument structure
provides sufficient discriminatory power to iden-
tify the correct event in over 69% of the times.
In the second setting domain-specific examples
are available. The learning process can still utilize
the transferred knowledge, as it provides scaffold-
ing for the latent learning process, resulting in a
significant improvement in performance.
6 Acknowledgement
The authors would like to thank Julia Hockenmaier, Gerald
DeJong, Raymond Mooney and the anonymous reviewers for
their efforts and insightful comments.
Most of this work was done while the first author was
at the University of Illinois. The authors gratefully ac-
knowledge the support of the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. In addition, this material is based
on research sponsored by DARPA under agreement number
FA8750-13-2-0008. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. The views
and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies or endorsements, either expressed or implied, of
DARPA,AFRL, or the U.S. Government.
References
A. Bordes, N. Usunier, R. Collobert, and J. Weston.
2010. Towards understanding situated natural lan-
guage. In AISTATS.
B. Bo?rschinger, B. K. Jones, and M. Johnson. 2011.
Reducing grounded learning tasks to grammatical
inference. In EMNLP.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In NAACL.
D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In ICML.
D. L. Chen, J. Kim, and R. J. Mooney. 2010. Training
a multilingual sportscaster: Using perceptual con-
text to learn language. Journal of Artificial Intelli-
gence Research, 37:397?435.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s
response. In CoNLL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics.
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic
parsing. In ACL.
J. Kim and R. J. Mooney. 2010. Generative alignment
and semantic parsing for learning from ambiguous
supervision. In COLING.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, , and
M. Steedman. 2010. Inducing probabilistic ccg
grammars from logical form with higher-order uni-
fication. In EMNLP.
Y.W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In ACL.
L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In UAI.
466
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1501?1511,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Predicting Instructor?s Intervention in MOOC forums
Snigdha Chaturvedi Dan Goldwasser Hal Daum
?
e III
Department of Computer Science,
University of Maryland, College Park, Maryland
{snigdhac, goldwas1, hal}@umiacs.umd.edu
Abstract
Instructor intervention in student discus-
sion forums is a vital component in
Massive Open Online Courses (MOOCs),
where personalized interaction is limited.
This paper introduces the problem of pre-
dicting instructor interventions in MOOC
forums. We propose several prediction
models designed to capture unique aspects
of MOOCs, combining course informa-
tion, forum structure and posts content.
Our models abstract contents of individ-
ual posts of threads using latent categories,
learned jointly with the binary interven-
tion prediction problem. Experiments over
data from two Coursera MOOCs demon-
strate that incorporating the structure of
threads into the learning problem leads to
better predictive performance.
1 Introduction
Ubiquitous computing and easy access to high
bandwidth internet have reshaped the modus
operandi in distance education towards Massive
Open Online Courses (MOOCs). Courses offered
by ventures such as Coursera and Udacity now im-
part inexpensive and high-quality education from
field-experts to thousands of learners across geo-
graphic and cultural barriers.
Even as the MOOC model shows exciting pos-
sibilities, it presents a multitude of challenges that
must first be negotiated to completely realize its
potential. MOOCs platforms have been especially
criticized on grounds of lacking a personalized
educational experience (Edmundson, 2012). Un-
like traditional classrooms, the predominant mode
of interaction between students and instructors in
MOOCs is via online discussion forums. Ideally,
forum discussions can help make up for the lack
of direct interaction, by enabling students to ask
questions and clarify doubts. However, due to
huge class sizes, even during the short duration
of a course, MOOCs witness a very large number
of threads on these forums. Owing to extremely
skewed ratios of students to instructional staff, it
can be prohibitively time-consuming for the in-
structional staff to manually follow all threads of a
forum. Hence there is a pressing need for automat-
ically curating the discussions for the instructors.
In this paper, we focus on identifying situa-
tions in which instructor (used interchangeably
with ?instructional staff? in this paper) interven-
tion is warranted. Using existing forum posts and
interactions, we frame this as a binary prediction
problem of identifying instructor?s intervention in
forum threads. Our initial analysis revealed that
instructors usually intervene on threads discussing
students? issues close to a quiz or exam. They
also take interest in grading issues and logistics
problems. There are multiple cues specific to the
MOOC setting, which when combined with the
rich lexical information present in the forums, can
yield useful predictive models.
Analyzing forum-postings contents and bring-
ing the most pertinent content to the instructor?s
attention would help instructors receive timely
feedback and design interventions as needed.
From the students? perspective, the problem is ev-
ident from an examination of existing forum con-
tent, indicating that if students want instructor?s
input on some issues, the only way for them to
get his/her attention is by ?up-voting? their votes.
Fig. 1 provides some examples of this behavior.
This is clearly an inefficient solution.
Our main technical contribution is introducing
three different models addressing the task of pre-
dicting instructor interventions. The first uses a lo-
gistic regression model that primarily incorporates
high level information about threads and posts.
However, forum threads have structure which is
not leveraged our initial model. We present two
1501
?The problem summary: Anyone else having problems viewing the video lecture...very choppy. If you are also experi-
encing this issue; please upvote this post.?
?I read that by up-voting threads and posts you can get the instructors? attention faster.?
?Its is very bad to me that I achieved 10 marks in my 1st assignment and now 9 marks in my 2nd assignment, now I won?t
get certificate, please Course staff it is my appeal to change the passing scheme or please be lenient. Please upvote my
post so that staff take this problem under consideration.?
Figure 1: Sample posts that showing students desiring instructor?s attention have to resolve to the ineffi-
cient method of getting their posts upvoted.
additional structured models. Both models assume
that posts of a thread structure it in form of a story
or a ?chain of events.? For example, an opening
post of a thread might pose a question and the fol-
lowing posts can then answer or comment on the
question. Our second and third models tap this
linear ?chain of events? behavior by assuming that
individual posts belong to latent categories which
represent their textual content at an abstract level
and that an instructor?s decision to reply to a post
is based on this chain of events (represented by the
latent categories). We present two different ways
of utilizing this ?chain of events? behavior for pre-
dicting instructor?s intervention which can be ei-
ther simply modeled as the ?next step? is this chain
of events (Linear Chain Markov Model) or as a
decision globally depending on the entire chain
(Global Chain Model). Our experiments on two
different datasets reveal that using the latent post
categories helps in better prediction.
Our contributions can be summarized as:
? We motivate and introduce the important
problem of predicting instructor intervention
in MOOC forums
? We present two chain based models that in-
corporate thread structure.
? We show the utility of modeling thread struc-
ture, and the value of lexical and domain spe-
cific knowledge for the prediction task
2 Related Work
To the best of our knowledge, the problem of pre-
dicting instructor?s intervention in MOOC forums
has not been addressed yet. Prior work deals with
analyzing general online discussion forums of so-
cial media sites (Kleinberg, 2013): such as pre-
dicting comment volume (Backstrom et al, 2013;
De Choudhury et al, 2009; Wang et al, 2012;
Tsagkias et al, 2009; Yano and Smith, 2010; Artzi
et al, 2012) and rate of content diffusion (Kwak et
al., 2010; Lerman and Ghosh, 2010; Bakshy et al,
2011; Romero et al, 2011; Artzi et al, 2012) and
also question answering (Chaturvedi et al, 2014).
Wang et al (2007) incorporate thread structure
of conversations using features in email threads
while Goldwasser and Daum?e III (2014) use la-
tent structure, aimed to identify relevant dialog
segments, for predicting objections during court-
room deliberations. Other related work include
speech act recognition in emails and forums but
at a sentence level (Jeong et al, 2009), and us-
ing social network analysis to improve message
classification into pre-determined types (Fortuna
et al, 2007). Discussion forums data has also been
used to address other interesting challenges such
as extracting chatbox knowledge for use in gen-
eral online forums (Huang et al, 2007) and auto-
matically extracting answers from discussion fo-
rums (Catherine et al, 2013), subjectivity analy-
sis of online forums (Biyani et al, 2013). Most
of these methods use ideas similar to ours: identi-
fying that threads (or discussions) have an under-
lying structure and that messages belong to cate-
gories. However, they operate in a different do-
main, which makes their goals and methods dif-
ferent from ours.
Our work is most closely related to that of Back-
strom et al (2013) which introduced the re-entry
prediction task ?predicting whether a user who
has participated in a thread will later contribute
another comment to it. While seemingly related,
their prediction task, focusing on users who have
already commented on a thread, and their algorith-
mic approach are different than ours. Our work
is also very closely related to that of Wang et al
(2013) who predict solvedness ?which predicts
if there is a solution to the original problem posted
in the thread. Like us, they believe that category
of posts can assist in the prediction task, however,
possibly owing to the complexity of general dis-
cussion forums, they had to manually create and
annotate data with a sophisticated taxonomy. We
do not make such assumptions.
The work presented in (G?omez et al, 2008;
1502
Liben-Nowell and Kleinberg, 2008; Kumar et al,
2010; Golub and Jackson, 2010; Wang et al,
2011; Aumayr et al, 2011) discuss characteriz-
ing threads using reply-graphs (often trees) and
learning this structure. However, this representa-
tion is not natural for the MOOC domain where
discussions are relatively more focused on the
thread topic and are better organized using sec-
tions within the forums.
Although most prior work focuses on discus-
sion forums of social media sites such as Twitter
or Facebook, where the dynamics of interaction is
very different from MOOCs, a small number of
recent work address the unique MOOC setting.
Stump et al (2013) propose a framework for
categorizing forum posts by designing a taxonomy
and annotating posts manually to assist general fo-
rum analysis. Our model learns categories in a
data-driven manner guided by the binary super-
vision (intervention decision) and serves a differ-
ent purpose. Nevertheless, in Sec. 4.3 we compare
the categories learnt by our models with those pro-
posed by Stump et al (2013).
Apart from this, recent works have looked into
interesting challenges in this domain such as bet-
ter peer grading models (Piech et al, 2013), code
review (Huang et al, 2013; Nguyen et al, 2014),
improving student engagement (Anderson et al,
2014) and understanding how students learn and
code (Piech et al, 2012; Kizilcec et al, 2013;
Ramesh et al, 2013).
3 Intervention Prediction Models
In this section, we explain our models in detail.
3.1 Problem Setting
In our description it is assumed that a discus-
sion board is organized into multiple forums (rep-
resenting topics such as ?Assignment?, ?Study
Group? etc.). A forum consists of multiple
threads. Each thread (t) has a title and consists of
multiple posts (p
i
). Individual posts do not have
a title and the number of posts varies dramatically
from one thread to another. We address the prob-
lem of predicting if the course instructor would in-
tervene on a thread, t. The instructor?s decision to
intervene, r, equals 0 when the instructor doesn?t
reply to the thread and 1 otherwise. The individual
posts are not assumed to be labeled with any cat-
egory and the only supervision given to the model
during training is in form of intervention decision.
3.2 Logistic Regression (LR)
Our first attempt at solving this problem involved
training a logistic regression for the binary predic-
tion task which models P (r|t).
3.2.1 Feature Engineering
Our logistic regression model uses the follow-
ing two types of features: Thread only features
and Aggregated post features. ?Thread only fea-
tures? capture information about the thread such
as when, where, by who was the thread posted and
lexical features based on the title of the thread.
While these features provide a high-level infor-
mation about the thread, it is also important to
analyze the contents of the posts of the thread.
In order to maintain a manageable feature space,
we compress the features from posts and represent
them using our ?Aggregated post features?.
Thread only features:
1. a binary feature indicating if the thread was
started by an anonymous user
2. three binary features indicating whether the
thread was marked as approved, unresolved
or deleted (respectively)
3. forum id in which the thread was posted
4. time when the thread was started
5. time of last posting on the thread
6. total number of posts in the thread
7. a binary feature indicating if the thread title
contains the words lecture or lectures
8. a binary feature indicating if the thread title
contains the words assignment, quiz, grade,
project, exam (and their plural forms)
Aggregated post features:
9. sum of number of votes received by the indi-
vidual posts
10. mean and variance of the posting times of in-
dividual posts in the thread
11. mean of time difference between the post-
ing times of individual posts and the closest
course landmark. A course landmark is the
deadline of an assignment, exam or project.
12. sum of count of occurrences of assessment
related words e.g. grade, exam, assignment,
quiz, reading, project etc. in the posts
13. sum of count of occurrences of words indicat-
ing technical problems e.g. problem, error
14. sum of count of occurrences of thread con-
clusive words like thank you and thank
15. sum of count of occurrences of request, sub-
mit, suggest
1503
h1
h
2
h
n
r ?(t)
p
1
p
2
p
n
T
(a) Linear Chain Markov Model (LCMM)
h
1
h
2
h
n
r ?(t)
p
1
p
2
p
n
T
(b) Global Chain Model (GCM)
Figure 2: Diagrams of the Linear Chain Markov
Model (LCMM) and the Global Chain Model
(GCM). p
i
, r and ?(t) are observed and h
i
are the
latent variables. p
i
and h
i
represent the posts of
the thread and their latent categories respectively;
r represents the instructor?s intervention and ?(t)
represent the non-structural features used by the
logistic regression model.
We had also considered and dropped (because
of no performance gain) other features about iden-
tity of the user who started the thread, number
of distinct participants in the thread (an impor-
tant feature used by Backstrom et al (2013)), bi-
nary feature indicating if the first and the last posts
were by the same user, average number of words
in the thread?s posts, lexical features capturing ref-
erences to the instructors in the posts etc.
3.3 Linear Chain Markov Model (LCMM)
The logistic regression model is good at exploit-
ing the thread level features but not the content of
individual posts. The ?Aggregated post features?
attempt to capture this information but since the
number of posts in a thread is variable, these fea-
tures relied on aggregated values. We believe that
considering aggregate values is not sufficient for
the task in hand. As noted before, posts of a thread
are not independent of each other. Instead, they
are arranged chronologically such that a post is
published in reply to the preceding posts and this
For every thread, t, in the dataset:
1. Choose a start state, h
1
, and emit the first
post, p
1
.
2. For every subsequent post, p
i
? i ?
{2 . . . n} :
(a) Transition from h
i?1
to h
i
.
(b) Emit post p
i
.
3. Generate the instructor?s intervention
decision, r, using the last state h
n
and
non-structural features, ?(t).
Figure 3: Instructor?s intervention decision pro-
cess for the Linear Chain Markov Model.
might effect an instructor?s decision to reply. For
example, consider a thread that starts with a ques-
tion. The following posts will be students? attempt
to answer the question or raise further concerns or
comment on previous posts. The instructor?s post,
though a future event, will be a part of this process.
We, therefore, propose to model this complete
process using a linear chain markov model shown
in Fig. 2a. The model abstractly represents the in-
formation from individual posts (p
i
) using latent
categories (h
i
). The intervention decision, r, is
the last step in the chain and thus incorporates in-
formation from the individual posts. It also de-
pends on the thread level features: ?Thread only
features? and the ?Aggregated post features? jointly
represented by ?(t) (also referred to as the non-
structural features). This process is explained in
Fig. 3.
We use hand-crafted features to model the dy-
namics of the generative process. Whenever a la-
tent state emits a post or transits to another latent
state (or to the final intervention decision state),
emission and transition features get fired which are
then multiplied by respective weights to compute
a thread?s ?score?:
f
w
(t, p) = max
h
[w ? ?(p, r, h, t)] (1)
Note that the non-structural features, ?(t), also
contribute to the final score.
3.3.1 Learning and Inference
During training we maximize the combined scores
of all threads in the dataset using a generic EM
style algorithm. The supervision in this model is
provided only in form of the observed interven-
tion decision, r and the post categories, h
i
are hid-
1504
den. The model uses the pseudocode shown in Al-
gorithm 1 to iteratively refine the weight vectors.
In each iteration, the model first uses viterbi algo-
rithm to decode thread sequences with the current
weights w
t
to find optimal highest scoring latent
state sequences that agree with the observed in-
tervention state (r = r
?
). In the next step, given
the latent state assignments from the previous step,
a structured perceptron algorithm (Collins, 2002)
is used to update the weights w
t+1
using weights
from the previous step, w
t
, initialization.
Algorithm 1 Training algorithm for LCMM
1: Input: Labeled data D = {(t, p, r)
i
}
2: Output: Weights w
3: Initialization: Set w
j
randomly, ?j
4: for t : 1 to N do
5:
?
h
i
= argmax
h
[w
t
? ?(p, r, h, t)] such
that r = r
i
?i
6: w
t+1
= StructuredPerceptron(t, p,
?
h, r)
7: end for
8: return w
While testing, we use the learned weights and
viterbi decoding to compute the intervention state
and the best scoring latent category sequence.
3.3.2 Feature Engineering
In addition to the ?Thread Only Features? and the
?Aggregated post features?, ?(t) (Sec. 3.2.1, this
model uses the following emission and transition
features:
Post Emission Features:
1. ?(p
i
, h
i
) = count of occurrences of question
words or question marks in p
i
if the state is
h
i
; 0 otherwise.
2. ?(p
i
, h
i
) = count of occurrences of thank
words (thank you or thanks) in p
i
if the state
is h
i
; 0 otherwise.
3. ?(p
i
, h
i
) = count of occurrences of greeting
words (e.g. hi, hello, good morning, welcome
etc ) in p
i
if the state is h
i
; 0 otherwise.
4. ?(p
i
, h
i
) = count of occurrences of assess-
ment related words (e.g. grade, exam, assign-
ment, quiz, reading, project etc.) in p
i
if the
state is h
i
; 0 otherwise.
5. ?(p
i
, h
i
) = count of occurrences of request,
submit or suggest in p
i
if the state is h
i
; 0
otherwise.
6. ?(p
i
, h
i
) = log(course duration/t(p
i
)) if the
state is h
i
; 0 otherwise. Here t(p
i
) is the dif-
ference between the posting time of p
i
and
the closest course landmark (assignment or
project deadline or exam).
7. ?(p
i
, p
i?1
, h
i
) = difference between posting
times of p
i
and p
i?1
normalized by course
duration if the state is h
i
; 0 otherwise.
Transition Features:
1. ?(h
i?1
, h
i
) = 1 if previous state is h
i?1
and
current state is h
i
; 0 otherwise.
2. ?(h
i?1
, h
i
, p
i
, p
i?1
) = cosine similarity be-
tween p
i?1
and p
i
if previous state is h
i?1
and current state is h
i
; 0 otherwise.
3. ?(h
i?1
, h
i
, p
i
, p
i?1
) = length of p
i
if previ-
ous state is h
i?1
, p
i?1
has non-zero question
words and current state is h
i
; 0 otherwise.
4. ?(h
n
, r) = 1 if last post?s state is h
n
and in-
tervention decision is r; 0 otherwise.
5. ?(h
n
, r, p
n
) = 1 if last post?s state is h
n
, p
n
has non-zero question words and intervention
decision is r; 0 otherwise.
6. ?(h
n
, r, p
n
) = log(course duration/t(p
n
)) if
last post?s state is h
n
and intervention deci-
sion is r; 0 otherwise. Here t(p
n
) is the dif-
ference between the posting time of p
n
and
the closest course landmark (assignment or
project deadline or exam).
3.4 Global Chain Model (GCM)
In this model we propose another way of incorpo-
rating the chain structure of a thread. Like the pre-
vious model, this model also assumes that posts
belong to latent categories. It, however, doesn?t
model the instructor?s intervention decision as a
step in the thread generation process. Instead, it
assumes that instructor?s decision to intervene is
dependent on all the posts in the threads, mod-
eled using the latent post categories. This model
is shown in Fig. 2b. Assuming that p represents
posts of thread t, h represents the latent category
assignments, r represents the intervention deci-
sion; feature vector, ?(p, r, h, t), is extracted for
each thread and using the weight vector, w, this
model defines a decision function, similar to what
is shown in Equation 1.
3.4.1 Learning and Inference
Similar to the traditional maximum margin based
Support Vector Machine (SVM) formulation, our
model?s objective function is defined as:
min
w
?
2
||w||
2
+
T
?
j
l(?r
j
f
w
(t
j
, p
j
)) (2)
1505
where ? is the regularization coefficient, t
j
is the
j
th
thread with intervention decision r
j
and p
j
are
the posts of this thread. w is the weight vector, l(?)
is the squared hinge loss function and f
w
(t
j
, p
j
) is
defined in Equation 1.
Replacing the term f
w
(t
j
, p
j
) with the con-
tents of Equation 1 in the minimization objective
above, reveals the key difference from the tradi-
tional SVM formulation - the objective function
has a maximum term inside the global minimiza-
tion problem making it non-convex.
We, therefore, employ the optimization algo-
rithm presented in (Chang et al, 2010) to solve
this problem. Exploiting the semi-convexity prop-
erty (Felzenszwalb et al, 2010), the algorithm
works in two steps, each executed iteratively. In
the first step, it determines the latent variable as-
signments for positive examples. The algorithm
then performs two step iteratively - first it deter-
mines the structural assignments for the negative
examples, and then optimizes the fixed objective
function using a cutting plane algorithm. Once
this process converges for negative examples, the
algorithm reassigns values to the latent variables
for positive examples, and proceeds to the second
step. The algorithm stops once a local minimum
is reached. A somewhat similar approach, which
uses the Convex-Concave Procedure (CCCP) is
presented by (Yu and Joachims, 2009).
At test time, given a thread, t, and it posts, p,
we use the learned weights to compute f
w
(t, p)
and classify it as belonging to the positive class
(instructor intervenes) if f
w
(t, p) ? 0.
3.4.2 Feature Engineering
The feature set used by this model is very sim-
ilar to the features used by the previous model.
In addition to the non-structural features used
by the logistic regression model (Sec. 3.2.1), it
uses all the Post Emission features and the three
transition features represented by ?(h
i?1
, h
i
) and
?(h
i?1
, h
i
, p
i
, p
i?1
) as described in Sec. 3.3.2.
4 Empirical Evaluation
This section describes our experiments.
4.1 Datasets and Evaluation Measure
For our experiments, we have used the forum
content of two MOOCs from different domains
(science and humanities), offered by Coursera
1
,
1
https://www.coursera.org/
a leading education technology company. Both
courses were taught by professors from the Uni-
versity of Maryland, College Park.
Genes and the Human Condition (From Behav-
ior to Biotechnology) (GHC) dataset.
2
This
course was attended by 30,000 students and the
instructional staff comprised of 2 instructors, 3
Teaching Assistants and 56 technical support staff.
The discussion forum of this course consisted of
980 threads composed of about 3,800 posts.
Women and the Civil Rights Movement (WCR)
dataset.
3
The course consisted of a classroom
of about 14,600 students, 1 instructor, 6 Teaching
Assistants and 49 support staff. Its discussion fo-
rum consisted of 800 threads and 3,900 posts.
We evaluate our models on held-out test sets.
For the GHC dataset, the test set consisted of 186
threads out of which the instructor intervened on
24 while, for the WCR dataset, the instructor in-
tervened on 21 out of 155 threads.
Also, it was commonly observed that after an
instructor intervenes on a thread, its posting and/or
viewing behavior increases. We, therefore, only
consider the student posts until the instructor?s first
intervention. Care was also taken to not use fea-
tures that increased/decreased disproportionately
because of the instructor?s intervention such as
number of views or votes of a thread.
In our evaluation we approximate instructor?s
?should reply? instances with those where the in-
structor indeed replied. Unlike general forum
users, we believe that the correlation between the
two scenarios is quite high for instructors. It is
their responsibility to reply, and by choosing to a
MOOC, they have ?bought in? to the idea of forum
participation. The relatively smaller class sizes of
these two MOOCs also ensured that most threads
were manually reviewed, thus reducing instances
of ?missed? threads while retaining the posting be-
havior and content of a typical MOOC.
4.2 Experimental Results
Since the purpose of solving this problem is to
identify the threads which should be brought to
the notice of the instructors, we measure the per-
formance of our models using F-measure of the
positive class. The values of various parameters
were selected using 10-fold Cross Validation on
2
https://www.coursera.org/course/genes
3
https://www.coursera.org/course/
womencivilrights
1506
Model
Genes and the Human Condition (GHC) Women and the Civil Rights (WCR)
P R F P R F
LR 44.44 16.67 24.24 66.67 15.38 25.00
J48 45.50 20.80 28.55 25.00 23.10 24.01
LCMM 33.33 29.17 31.11 42.86 23.08 30.00
GCM 60.00 25.00 35.29 50.00 18.52 27.03
Table 1: Held-out test set performances of chain models, LCMM and GCM, are better than that of the
unstructured models, LR and J48.
Figure 4: Visualization of lexical contents of the
categories learnt by our model from the GHC
dataset. Each row is a category and each column
represents a feature vector. Bright cream color
represents high values while lower values are rep-
resented by darker shades. Dark beige columns
are used to better separate the five feature clusters,
F1-F5, which represent words that are common in
thanking, logistics-related, introductory, syllabus
related and miscellaneous posts respectively. Cat-
egories 1,2,3 and 4 are dominated by F2, F4, F1
and F3 respectively indicating a semantic segrega-
tion of posts by our model?s categories.
the training set. Table 1 presents the performances
of the proposed models on the held-out test sets.
We also report performance of a decision tree
(J48) on the test sets for sake of comparison.
We can see that the chain based models, Linear
Chain Markov Model (LCMM) and Global Chain
Model (GCM), outperform the unstructured mod-
els, namely Logistic regression (LR) and Decision
Trees (J48). This validates our hypothesis that us-
ing the post structure results in better modeling of
instructor?s intervention.
The table also reveals that GCM yields high pre-
cision and low recall values, which is possibly due
to the model being more conservative owing to in-
formation from all posts of the thread.
4.3 Visual Exploration of Categories
Our chain based models assume that posts belong
to different (latent) categories and use these cate-
gories to make intervention predictions. Since this
process of discovering categories is data driven, it
would be interesting to examine the contents of
these categories. Fig. 4 presents a heat map of
lexical content of categories identified by LCMM
from the GHC dataset. The value of H (num-
ber of categories) was set to be 4 and was pre-
determined during the model selection procedure.
Each row of the heat map represents a category
and the columns represent values of individual fea-
tures, f(w, c), defined as: f(w, c) =
C(w,c)
<C(w,c)>
where, C(w, c) is total count of occurrences of a
word, w, in all posts assigned to category, c and
< C(w, c) > represents its expected count based
on its frequency in the dataset. While the actual
size of vocabulary is huge, we use only a small
subset of words in our feature vector for this visu-
alization. These feature values, after normaliza-
tion, are represented in the heat map using col-
ors ranging from bright cream (high value) to dark
black (low value). The darker the shade of a cell,
the lower is the value represented by it.
For visual convenience, the features are man-
ually clustered into five groups (F1 to F5) each
separated by a dark beige colored column in the
heat map. The first column of the heat map rep-
resents the F1 group which consists of words like
thank you, thanks etc. These words are character-
istic of posts that mark either the conclusion of a
resolved thread or are posted towards the end of
the course. Rows corresponding to the category 3
in Table 2 show two examples of such posts. Simi-
larly, F2 represents the features related to logistics
of the course and F3 captures introductory posts
by new students. Finally, F4 contains words that
are closely related to the subfield of gene and hu-
man conditions and would appear in posts that dis-
cuss specific aspects or chapters of the course con-
1507
tents, while F5 contains general buzz words that
would appear frequently in any biology course.
Analyzing individual rows of the heat map, we
can see that out of F1 to F4, Categories 1, 2, 3 and
4 are dominated by logistics (F2), course content
related (F4), thank you (F1) and introductory posts
(F3) respectively, represented by bright colors in
their respective rows. We also observe similar cor-
relations while examining the columns of the heat
map. Also, F5, which contains words common to
the gene and human health domain, is scattered
across multiple categories. For example, dna/rna
and breeding are sufficiently frequent in category
1 as well as 2.
Table 2 gives examples of representative posts
from the four clusters. Due to space constraints,
we show only part of the complete post. We can
see that these examples agree with our observa-
tions from the heat map.
Furthermore, as noted in Sec. 2, we compare
the semantics of clusters learnt by our models with
those proposed by Stump et al (2013) even though
the two categorizations are not directly compara-
ble. Nevertheless, generally speaking, our cate-
gory 1 corresponds to Stump et al (2013)?s Course
structure/policies and category 2 corresponds to
Content. Interestingly, categories 3 and 4, which
represent valedictory and introductory posts, cor-
respond to a single Social/affective from the previ-
ous work.
We can, therefore, conclude that the model, in-
deed splits the posts into categories that look se-
mantically coherent to the human eyes.
4.4 Choice of Number of Categories
Our chain based models, assigning forum posts to
latent categories, are parameterized with H , the
number of categories. We therefore, study the sen-
sitivity of our models to this parameter. Fig. 5,
plots the 10-fold cross validation performance of
the models with increasing values ofH for the two
datasets. Interestingly, the sensitivity of the two
models to the value of H is very different.
The LCMM model?s performance fluctuates as
the value of H increases. The initial performance
improvement might be due to an increase in the ex-
pressive power of the model. Performance peaks
at H = 4 and then decreases, perhaps owing to
over-fitting of the data.
In contrast, GCM performance remains steady
for various values of H which might be attributed
(a) Genes and the Human Condition dataset
(b) Women and the Civil Rights Movement dataset
Figure 5: Cross validation performances of the
two models with increasing number of categories.
to the explicit regularization coefficient which
helps combat over-fitting, by encouraging zero
weights for unnecessary categories.
4.5 How important are linguistic features?
We now focus on the structure independent fea-
tures and experiment with their predictive value,
according to types. We divide the features used by
the LR into the following categories:
4
? Full: set of all features (feature no. 1 to 15)
? lexical: based on content of thread titles and
posts (feature no. 7 to 8 and 12 to 13)
? landmark: based on course landmarks (e.g,
exams, quizzes) information (feature no. 11)
? MOOCs-specific: features specific to the
MOOCs domain (lexical + landmark fea-
tures)
? post: based only on aggregated posts infor-
mation (feature no. 9 to 15)
? temporal: based on posting time patterns
(feature no. 4, 5 and 10)
Fig. 6 shows 10-fold cross validation F-measure
of the positive class for LR when different types of
features are excluded from the full set.
The figure reveals that the MOOCs-specific
features (purple bar) are important for both the
datasets indicating a need for designing special-
ized models for forums analysis in this domain.
4
Please refer to Sec 3.2.1 for description of the feature id.
1508
Category Example posts
1 ?I?m having some issues with video playback. I have downloaded the videos to my laptop...?
1 ?There was no mention of the nuclear envelope in the Week One lecture, yet it was in the quiz. Is this a mistake??
2 ?DNA methylation is a crucial part of normal development of organisms and cell differentiation in higher organisms...?
2 ?In the lecture, she said there are...I don?t see how tumor-suppressor genes are a cancer group mutation.?
3 ?Thank you very much for a most enjoyable and informative course.?
3 ?Great glossary! Thank you!?
4 ?Hello everyone, I?m ... from the Netherlands. I?m a life science student.
4 ?Hi, my name is ... this is my third class with coursera?
Table 2: Representative posts from the four categories learnt by our model. Due to space and privacy
concerns we omit some parts of the text, indicated by ?. . . ?.
(a) Genes and the Human Condition dataset
(b) Women and the Civil Rights Movement dataset
Figure 6: Cross validation performances of the
various feature types for the two datasets.
Also, lexical features (red bar) and post features
(blue bar) have pretty dramatic effects in GHC and
WCR data respectively.
Interestingly, removing the landmark feature set
(green bar) causes a considerable drop in predic-
tive performance, even though it consists of only
one feature. Other temporal features (orange bar)
also turn out to be important for the prediction.
From a separate instructor activity vs time graph
(not shown due to space constraints), we observed
that instructors tend to get more active as the
course progresses and their activity level also in-
creases around quizzes/exams deadlines.
We can, therefore, conclude that all feature
types are important and that lexical as well as
MOOC specific analysis is necessary for model-
ing instructor?s intervention.
5 Conclusion
One of the main challenges in MOOCs is man-
aging student-instructor interaction. The massive
scale of these courses rules out any form of per-
sonalized interaction, leaving instructors with the
need to go over the forum discussions, gauge stu-
dent reactions and selectively respond when ap-
propriate. This time consuming and error prone
task stresses the need for methods and tools sup-
plying this actionable information automatically.
This paper takes a first step in that direction,
and formulates the novel problem of predicting in-
structor intervention in MOOC discussion forums.
Our main technical contribution is to construct
predictive models combining information about
forum post content and posting behavior with in-
formation about the course and its landmarks.
We propose three models for addressing the
task. The first, a logistic regression model is
trained on thread level and aggregated post fea-
tures. The other two models take thread structure
into account when making the prediction. These
models assume that posts can be represented by
categories which characterize post content at an
abstract level, and treat category assignments as
latent variables organized according to, and influ-
enced by, the forum thread structure.
Our experiments on forum data from two differ-
ent Coursera MOOCs show that utilizing thread
structure is important for predicting instructor?s
behavior. Furthermore, our qualitative analysis
shows that our latent categories are semantically
coherent to human eye.
1509
References
Ashton Anderson, Daniel P. Huttenlocher, Jon M.
Kleinberg, and Jure Leskovec. 2014. Engaging with
massive online courses. In WWW, pages 687?698.
Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012.
Predicting responses to microblog posts. In Pro-
ceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 602?606, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Erik Aumayr, Jeffrey Chan, and Conor Hayes. 2011.
Reconstruction of threaded conversations in online
discussion forums. In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, ICWSM.
The AAAI Press.
Lars Backstrom, Jon Kleinberg, Lillian Lee, and Cris-
tian Danescu-Niculescu-Mizil. 2013. Characteriz-
ing and curating conversation threads: Expansion,
focus, volume, re-entry. In Proceedings of the Sixth
ACM International Conference on Web Search and
Data Mining, WSDM ?13, pages 13?22, New York,
NY, USA. ACM.
Eytan Bakshy, Jake M. Hofman, Winter A. Mason, and
Duncan J. Watts. 2011. Everyone?s an influencer:
Quantifying influence on twitter. In Proceedings of
the Fourth ACM International Conference on Web
Search and Data Mining, WSDM ?11, pages 65?74,
New York, NY, USA. ACM.
Prakhar Biyani, Cornelia Caragea, and Prasenjit Mitra.
2013. Predicting subjectivity orientation of online
forum threads. In CICLing (2), pages 109?120.
Rose Catherine, Rashmi Gangadharaiah, Karthik
Visweswariah, and Dinesh Raghu. 2013. Semi-
supervised answer extraction from discussion fo-
rums. In Proceedings of the Sixth International Joint
Conference on Natural Language Processing, pages
1?9, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning over
constrained latent representations. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 429?
437, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Snigdha Chaturvedi, Vittorio Castelli, Radu Florian,
Ramesh M. Nallapati, and Hema Raghavan. 2014.
Joint question clustering and relevance prediction
for open domain non-factoid question answering. In
Proceedings of the 23rd International Conference on
World Wide Web, WWW ?14, pages 503?514, Re-
public and Canton of Geneva, Switzerland. Interna-
tional World Wide Web Conferences Steering Com-
mittee.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Munmun De Choudhury, Hari Sundaram, Ajita John,
and Dor?ee Duncan Seligmann. 2009. What makes
conversations interesting? themes, participants and
consequences of conversations in online social me-
dia. In 18th International World Wide Web Confer-
ence (WWW), pages 331?331, April.
Mark Edmundson. 2012. The trouble with online edu-
cation, July 19. http://www.nytimes.com/
2012/07/20/opinion/the-trouble-
with-online-education.html.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(9):1627?1645.
Blaz Fortuna, Eduarda Mendes Rodrigues, and Natasa
Milic-Frayling. 2007. Improving the classifica-
tion of newsgroup messages through social network
analysis. In Proceedings of the Sixteenth ACM Con-
ference on Conference on Information and Knowl-
edge Management, CIKM ?07, pages 877?880, New
York, NY, USA. ACM.
Dan Goldwasser and Hal Daum?e III. 2014. ?I object!?
modeling latent pragmatic effects in courtroom di-
alogues. European Chapter of the Association for
Computational Linguistics (EACL), April. To ap-
pear.
Benjamin Golub and Matthew O. Jackson. 2010. See-
ing only the successes: The power of selection bias
in explaining the structure of observed internet dif-
fusions.
Vicenc? G?omez, Andreas Kaltenbrunner, and Vicente
L?opez. 2008. Statistical analysis of the social net-
work and discussion threads in slashdot. In Proceed-
ings of the 17th International Conference on World
Wide Web, WWW ?08, pages 645?654, New York,
NY, USA. ACM.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbox knowledge from online discussion
forums. In Proceedings of the 20th International
Joint Conference on Artifical Intelligence, IJCAI?07,
pages 423?428, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Jonathan Huang, Chris Piech, Andy Nguyen, and
Leonidas J. Guibas. 2013. Syntactic and functional
variability of a million code submissions in a ma-
chine learning mooc. In AIED Workshops.
1510
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3 - Volume 3, EMNLP
?09, pages 1250?1259, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ren?e F. Kizilcec, Chris Piech, and Emily Schnei-
der. 2013. Deconstructing disengagement: analyz-
ing learner subpopulations in massive open online
courses. In LAK, pages 170?179.
Jon M. Kleinberg. 2013. Computational perspectives
on social phenomena at global scales. In Francesca
Rossi, editor, IJCAI. IJCAI/AAAI.
Ravi Kumar, Mohammad Mahdian, and Mary McGlo-
hon. 2010. Dynamics of conversations. In Pro-
ceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ?10, pages 553?562, New York, NY, USA.
ACM.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is twitter, a social network
or a news media? In Proceedings of the 19th In-
ternational Conference on World Wide Web, WWW
?10, pages 591?600, New York, NY, USA. ACM.
K. Lerman and R. Ghosh. 2010. Information conta-
gion: An empirical study of the spread of news on
digg and twitter social networks. In Proceedings of
4th International Conference on Weblogs and Social
Media (ICWSM).
David Liben-Nowell and Jon Kleinberg. 2008. Trac-
ing the flow of information on a global scale using
Internet chain-letter data. Proceedings of the Na-
tional Academy of Sciences, 105(12):4633?4638, 25
March.
Andy Nguyen, Christopher Piech, Jonathan Huang,
and Leonidas J. Guibas. 2014. Codewebs: scalable
homework search for massive open online program-
ming courses. In WWW, pages 491?502.
Chris Piech, Mehran Sahami, Daphne Koller, Steve
Cooper, and Paulo Blikstein. 2012. Modeling how
students learn to program. In SIGCSE, pages 153?
160.
Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong
Do, Andrew Ng, and Daphne Koller. 2013. Tuned
models of peer assessment in MOOCs. In Proceed-
ings of The 6th International Conference on Educa-
tional Data Mining (EDM 2013).
Arti Ramesh, Dan Goldwasser, Bert Huang, Hal
Daum?e III, and Lise Getoor. 2013. Modeling
learner engagement in moocs using probabilistic soft
logic. In NIPS Workshop on Data Driven Education.
Daniel M. Romero, Brendan Meeder, and Jon Klein-
berg. 2011. Differences in the mechanics of in-
formation diffusion across topics: Idioms, political
hashtags, and complex contagion on twitter. In Pro-
ceedings of the 20th International Conference on
World Wide Web, WWW ?11, pages 695?704, New
York, NY, USA. ACM.
Glenda S. Stump, Jennifer DeBoer, Jonathan Whit-
tinghill, and Lori Breslow. 2013. Development of a
framework to classify mooc discussion forum posts:
Methodology and challenges.
Manos Tsagkias, Wouter Weerkamp, and Maarten
de Rijke. 2009. Predicting the volume of com-
ments on online news stories. In Proceedings of the
18th ACM Conference on Information and Knowl-
edge Management, CIKM ?09, pages 1765?1768,
New York, NY, USA. ACM.
Yi-Chia Wang, Mahesh Joshi, and Carolyn Penstein
Ros. 2007. A feature based approach to leveraging
context for classifying newsgroup style discussion
segments. In John A. Carroll, Antal van den Bosch,
and Annie Zaenen, editors, ACL. The Association
for Computational Linguistics.
Hongning Wang, Chi Wang, ChengXiang Zhai, and Ji-
awei Han. 2011. Learning online discussion struc-
tures by conditional random fields. In Proceedings
of the 34th International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, SIGIR ?11, pages 435?444, New York, NY,
USA. ACM.
Chunyan Wang, Mao Ye, and Bernardo A. Huberman.
2012. From user comments to on-line conversa-
tions. In Proceedings of the 18th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, KDD ?12, pages 244?252, New
York, NY, USA. ACM.
Li Wang, Su Nam Kim, and Timothy Baldwin. 2013.
The utility of discourse structure in forum thread re-
trieval. In AIRS, pages 284?295.
Tae Yano and Noah A. Smith. 2010. What?s worthy of
comment? content and comment volume in political
blogs. In William W. Cohen and Samuel Gosling,
editors, ICWSM. The AAAI Press.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, ICML ?09, pages
1169?1176, New York, NY, USA. ACM.
1511
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 18?27,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Driving Semantic Parsing from the World?s Response
James Clarke Dan Goldwasser Ming-Wei Chang Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61820
{clarkeje,goldwas1,mchang21,danr}@illinois.edu
Abstract
Current approaches to semantic parsing,
the task of converting text to a formal
meaning representation, rely on annotated
training data mapping sentences to logi-
cal forms. Providing this supervision is
a major bottleneck in scaling semantic
parsers. This paper presents a new learn-
ing paradigm aimed at alleviating the su-
pervision burden. We develop two novel
learning algorithms capable of predicting
complex structures which only rely on a
binary feedback signal based on the con-
text of an external world. In addition we
reformulate the semantic parsing problem
to reduce the dependency of the model on
syntactic patterns, thus allowing our parser
to scale better using less supervision. Our
results surprisingly show that without us-
ing any annotated meaning representations
learning with a weak feedback signal is ca-
pable of producing a parser that is compet-
itive with fully supervised parsers.
1 Introduction
Semantic Parsing, the process of converting text
into a formal meaning representation (MR), is one
of the key challenges in natural language process-
ing. Unlike shallow approaches for semantic in-
terpretation (e.g., semantic role labeling and in-
formation extraction) which often result in an in-
complete or ambiguous interpretation of the natu-
ral language (NL) input, the output of a semantic
parser is a complete meaning representation that
can be executed directly by a computer program.
Semantic parsing has mainly been studied in the
context of providing natural language interfaces
to computer systems. In these settings the target
meaning representation is defined by the seman-
tics of the underlying task. For example, provid-
ing access to databases: a question posed in nat-
ural language is converted into a formal database
query that can be executed to retrieve information.
Example 1 shows a NL input query and its corre-
sponding meaning representation.
Example 1 Geoquery input text and output MR
?What is the largest state that borders Texas??
largest(state(next to(const(texas))))
Previous works (Zelle and Mooney, 1996; Tang
and Mooney, 2001; Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) employ
machine learning techniques to construct a seman-
tic parser. The learning algorithm is given a set of
input sentences and their corresponding meaning
representations, and learns a statistical semantic
parser ? a set of rules mapping lexical items and
syntactic patterns to their meaning representation
and a score associated with each rule. Given a sen-
tence, these rules are applied recursively to derive
the most probable meaning representation. Since
semantic interpretation is limited to syntactic pat-
terns identified in the training data, the learning
algorithm requires considerable amounts of anno-
tated data to account for the syntactic variations
associated with the meaning representation. An-
notating sentences with their MR is a difficult,
time consuming task; minimizing the supervision
effort required for learning is a major challenge in
scaling semantic parsers.
This paper proposes a new model and learning
paradigm for semantic parsing aimed to alleviate
the supervision bottleneck. Following the obser-
vation that the target meaning representation is to
be executed by a computer program which in turn
provides a response or outcome; we propose a re-
sponse driven learning framework capable of ex-
ploiting feedback based on the response. The feed-
back can be viewed as a teacher judging whether
the execution of the meaning representation pro-
duced the desired response for the input sentence.
18
This type of supervision is very natural in many
situations and requires no expertise, thus can be
supplied by any user.
Continuing with Example 1, the response gen-
erated by executing a database query would be
used to provide feedback. The feedback would be
whether the generated response is the correct an-
swer for the input question or not, in this case New
Mexico is the desired response.
In response driven semantic parsing, the learner
is provided with a set of natural language sen-
tences and a feedback function that encapsulates
the teacher. The feedback function informs the
learner whether its interpretation of the input sen-
tence produces the desired response. We consider
scenarios where the feedback is provided as a bi-
nary signal, correct +1 or incorrect ?1.
This weaker form of supervision poses a chal-
lenge to conventional learning methods: semantic
parsing is in essence a structured prediction prob-
lem requiring supervision for a set of interdepen-
dent decisions, while the provided supervision is
binary, indicating the correctness of a generated
meaning representation. To bridge this difference
we propose two novel learning algorithms suited
to the response driven setting.
Furthermore, to account for the many syntac-
tic variations associated with the MR, we propose
a new model for semantic parsing that allows us
to learn effectively and generalize better. Cur-
rent semantic parsing approaches extract parsing
rules mapping NL to their MR, restricting pos-
sible interpretations to previously seen syntactic
patterns. We replace the rigid inference process
induced by the learned parsing rules with a flex-
ible framework. We model semantic interpreta-
tion as a sequence of interdependent decisions,
mapping text spans to predicates and use syntac-
tic information to determine how the meaning of
these logical fragments should be composed. We
frame this process as an Integer Linear Program-
ming (ILP) problem, a powerful and flexible in-
ference framework that allows us to inject rele-
vant domain knowledge into the inference process,
such as specific domain semantics that restrict the
space of possible interpretations.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), a
database consisting of U.S. geographical informa-
tion, and natural language questions. Our experi-
mental results show that our model with response
driven learning can outperform existing models
trained with annotated logical forms.
The key contributions of this paper are:
Response driven learning for semantic parsing
We propose a new learning paradigm for learn-
ing semantic parsers without any annotated mean-
ing representations. The supervision for learning
comes from a binary feedback signal based a re-
sponse generated by executing a meaning repre-
sentation. This type of supervision signal is nat-
ural to produce and can be acquired from non-
expert users.
Novel training algorithms Two novel train-
ing algorithms are developed within the response
driven learning paradigm. The training algorithms
are applicable beyond semantic parsing and can be
used in situations where it is possible to obtain bi-
nary feedback for a structured learning problem.
Flexible semantic interpretation process We
propose a novel flexible semantic parsing model
that can handle previously unseen syntactic varia-
tions of the meaning representation.
2 Semantic Parsing
The goal of semantic parsing is to produce a func-
tion F : X ? Z that maps from the space natural
language input sentences, X , to the space of mean-
ing representations, Z . This type of task is usu-
ally cast as a structured output prediction problem,
where the goal is to obtain a model that assigns the
highest score to the correct meaning representa-
tion given an input sentence. However, in the task
of semantic parsing, this decision relies on identi-
fying a hidden intermediate representation (or an
alignment) that captures the way in which frag-
ments of the text correspond to the meaning repre-
sentation. Therefore, we formulate the prediction
function as follows:
z? = Fw(x) = argmax
y?Y ,z?Z
wT?(x,y, z) (1)
Where ? is a feature function that describes the
relationships between an input sentence x, align-
ment y and meaning representation z. w is the
weight vector which contains the parameters of the
model. We refer to the argmax above as the in-
ference problem. The feature function combined
with the nature of the inference problem defines
the semantic parsing model. The key to producing
19
What is the largest Texas?
largest( const(texas))))
New Mexico
x:
y:
z:
r:
that bordersstate
state( next_to(
Figure 1: Example input sentence, meaning repre-
sentation, alignment and answer for the Geoquery
domain
a semantic parser involves defining a model and a
learning algorithm to obtain w.
In order to exemplify these concepts we con-
sider the Geoquery domain. Geoquery contains a
query language for a database of U.S. geograph-
ical facts. Figure 1 illustrates concrete examples
of the terminology introduce. The input sentences
x are natural language queries about U.S. geog-
raphy. The meaning representations z are logical
forms which can be executed on the database to
obtain a response which we denote with r. The
alignment y captures the associations between x
and z.
Building a semantic parser involves defining the
model (feature function ? and inference problem)
and a learning strategy to obtain weights (w) as-
sociated with the model. We defer discussion of
our model until Section 4 and first focus on our
learning strategy.
3 Structured Learning with Binary
Feedback
Previous approaches to semantic parsing have
assumed a fully supervised setting where
a training set is available consisting of ei-
ther: input sentences and logical forms
{(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins,
2005)) or input sentences, logical forms
and a mapping between their constituents
{(xl,yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)).
Given such training examples a weight vector w
can be learned using structured learning methods.
Obtaining, through annotation or other means, this
form of training data is an expensive and difficult
process which presents a major bottleneck for
semantic parsing.
To reduce the burden of annotation we focus
on a new learning paradigm which uses feedback
from a teacher. The feedback signal is binary
(+1,?1) and informs the learner whether a pre-
dicted logical form z? when executed on the target
Algorithm 1 Direct Approach (Binary Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Bl ? {} for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: add (?(xl, y?, z?)/|xl|, f) to Bl
7: end for
8: w? BinaryLearn(B) where B = ?lBl
9: until no Bl has new unique examples
10: return w
domain produces the desired response or outcome.
This is a very natural method for providing super-
vision in many situations and requires no exper-
tise. For example, a user can observe the response
and provide a judgement. The general form of
the teacher?s feedback is provided by a function
Feedback : X ? Z ? {+1,?1}.
For the Geoquery domain this amounts to
whether the logical form produces the correct re-
sponse r for the input sentence. Geoquery has the
added benefit that the teacher can be automated
if we have a dataset consisting of input sentences
and response pairs {(xl, rl)}Nl=1. Feedback eval-
uates whether a logical form produces a response
matching r:
Feedback (xl, z) =
{
+1 if execute(z) = rl
?1 otherwise
We are now ready to present our learning
with feedback algorithms that operate in situations
where input sentences, {xl}Nl=1, and a teacher
feedback mechanism, Feedback , are available. We
do not assume the availability of logical forms.
3.1 Direct Approach (Binary Learning)
In general, a weight vector can be considered
good if when used in the inference problem (Equa-
tion (1)) it scores the correct logical form and
alignment (which may be hidden) higher than all
other logical forms and alignments for a given in-
put sentence. The intuition behind the direct ap-
proach is that the feedback function can be used to
subsample the space of possible structures (align-
ments and logical forms (Y ? Z)) for a given in-
put x. The feedback mechanism indicates whether
the structure is good (+1) or bad (?1). Using this
20
intuition we can cast the problem of learning a
weight vector for Equation (1) as a binary classifi-
cation problem where we directly consider struc-
tures the feedback assigns +1 as positive examples
and those assigned ?1 as negative.
We represent the input to the binary classifier
as the feature vector ?(x,y, z) normalized by the
size of the input sentence1 |x|, and the label as the
result from Feedback (x, z).
Algorithm 1 outlines the approach in detail. The
first stage of the algorithm iterates over all the
training input sentences and computes the best
logical form z? and alignment y? by solving the in-
ference problem (line 4). The feedback function
is queried (line 5) and a training example for the
binary predictor created using the normalized fea-
ture vector from the triple containing the sentence,
alignment and logical form as input and the feed-
back as the label. This training example is added
to the working set of training examples for this in-
put sentence (line 6). All the feedback training ex-
amples are used to train a binary classifier whose
weight vector is used in the next iteration (line 8).
The algorithm repeats until no new unique training
examples are added to any of the working sets for
any input sentence. Although the number of possi-
ble training examples is very large, in practice the
algorithm is efficient and converges quickly. Note
that this approach is capable of using a wide va-
riety of linear classifiers as the base learner (line
8).
A policy is required to specify the nature of
the working set of training examples (Bl) used for
training the base classifier. This is pertinent in line
6 of the algorithm. Possible policies include: al-
lowing duplicates in the working set (i.e., Bl is
a multiset), disallowing duplicates (Bl is a set),
or only allowing one example per input sentence
(?Bl? = 1). We adopt the first approach in this
paper.2
3.2 Aggressive Approach (Structured
Learning)
There is important implicit information which
the direct approach ignores. It is implicit that
when the teacher indicates an input paired with
an alignment and logical form is good (+1 feed-
1Normalization is required to ensure that each sentence
contributes equally to the binary learning problem regardless
of the sentence?s length.
2The working set Bl for each sentence may contain multi-
ple positive examples with the same and differing alignments.
Algorithm 2 Aggressive Approach (Structured
Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Sl ? ? for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: if f is +1 then
7: Sl ? {(xl, y?, z?)}
8: end if
9: end for
10: w? StructLearn(S,?) where S = ?lSl
11: until no Sl has changed
12: return w
back) that in order to repeat this behavior all other
competing structures should be made suboptimal
(or bad). To leverage this implicit information
we adopt a structured learning strategy in which
we consider the prediction as the optimal structure
and all others as suboptimal. This is in contrast to
the direct approach where only structures that have
explicitly received negative feedback are consid-
ered subopitmal.
When a structure is found with positive feed-
back it is added to the training pool for a struc-
tured learner. We consider this approach aggres-
sive as the structured learner implicitly considers
all other structures as being suboptimal. Negative
feedback indicates that the structure should not be
added to the training pool as it will introduce noise
into the learning process.
Algorithm 2 outlines the learning in more detail.
As before, y? and z? are predicted using the cur-
rent weight vector and feedback received (lines 4
and 5). When positive feedback is received a new
training instance for a structured learner is created
from the input sentence and prediction (line 7) this
training instance replaces any previous instance
for the input sentence. When negative feedback
is received the training pool Sl is not updated. A
weight vector is learned using a structured learner
where the training data S contains at most one ex-
ample per input sentence. In the first iteration of
the outer loop the training data S will contain very
few examples. In each subsequent iteration the
newly learned weight vector allows the algorithm
to acquire new examples. This is repeated until no
21
new examples are added or changed in S.
Like the direct approach, this learning frame-
work is makes very few assumptions about the
type of structured learner used as a base learner
(line 10).3
4 Model
Semantic parsing is the process of converting a
natural language input into a formal logic repre-
sentation. This process is performed by associat-
ing lexical items and syntactic patterns with logi-
cal fragments and composing them into a complete
formula. Existing approaches rely on extracting
a set of parsing rules, mapping text constituents
to a logical representation, from annotated train-
ing data and applying them recursively to obtain
the meaning representation. Adapting to new data
is a major limitation of these approaches as they
cannot handle inputs containing syntactic patterns
which were not observed in the training data. For
example, assume the training data produced the
following set of parsing rules:
Example 2 Typical parsing rules
(1) NP [?x.capital(x)]? capital
(2) PP [ const(texas)]? of Texas
(3) NNP [ const(texas)]? Texas
(4) NP [capital(const(texas))]?
NP[?x.capital(x)] PP [ const(texas)]
At test time the parser is given the sentences in
Example 3. Despite the lexical similarity in these
examples, the semantic parser will correctly parse
the first sentence but fail to parse the second be-
cause the lexical items belong to different a syn-
tactic category (i.e., the word Texas is not part of a
preposition phrase in the second sentence).
Example 3 Syntactic variations of the same MR
Target logical form: capital(const(texas))
Sentence 1: ?What is the capital of Texas??
Sentence 2: ?What is Texas? capital??
The ability to adapt to unseen inputs is one
of the key challenges in semantic parsing. Sev-
eral works (Zettlemoyer and Collins, 2007; Kate,
2008) have addressed this issue explicitly by man-
ually defining syntactic transformation rules that
can help the learned parser generalize better. Un-
fortunately these are only partial solutions as a
3Mistake driven algorithms that do not enforce margin
constraints may not be able to generalize using this proto-
col since they will repeat the same prediction at training time
and therefore will not update the model.
manually constructed rule set cannot cover the
many syntactic variations.
Given the previous example, we observe
that it is enough to identify that the function
capital(?) and the constant const(texas)
appear in the target MR, since there is only a single
way to compose these entities into a single formula
? capital(const(texas)).
Motivated by this observation we define our
meaning derivation process over the rules of the
MR language and use syntactic information as a
way to bias the MR construction process. That
is, our inference process considers the entire space
of meaning representations irrespective of the pat-
terns observed in the training data. This is possi-
ble as the MRs are defined by a formal language
and formal grammar.4 The syntactic information
present in the natural language is used as soft ev-
idence (features) which guides the inference pro-
cess to good meaning representations.
This formulation is a major shift from existing
approaches that rely on extracting parsing rules
from the training data. In existing approaches
the space of possible meaning representations is
constrained by the patterns in the training data
and syntactic structure of the natural language in-
put. Our formulation considers the entire space of
meaning representations and allows the model to
adapt to previously unseen data and always pro-
duce a semantic interpretation by using the pat-
terns observed in the input.
We frame our semantic interpretation process
as a constrained optimization process, maximiz-
ing the objective function defined by Equation 1
which relies on extracting lexical and syntactic
features instead of parsing rules. In the remain-
der of this section we explain the components of
our inference model.
4.1 Target Meaning Representation
Following previous work, we capture the se-
mantics of the Geoquery domain using a sub-
set of first-order logic consisting of typed con-
stants and functions. There are two types: en-
tities E in the domain and numeric values N .
Functions describe a functional relationship over
types (e.g., population : E ? N ). A com-
plete logical form is constructed through func-
tional composition; in our formalism this is per-
4This is true for all meaning representations designed to
be executed by a computer system.
22
formed by the substitution operator. For ex-
ample, given the function next to(x) and
the expression const(texas), substitution re-
places the occurrence of the free variable x, with
the expression, resulting in a new logical form:
next to(const(texas)). Due to space lim-
itations we refer the reader to (Zelle and Mooney,
1996) for a detailed description of the Geoquery
domain.
4.2 Semantic Parsing as Constrained
Optimization
Recall that the goal of semantic parsing is to pro-
duce the following function (Equation (1)):
Fw(x) = argmax
y,z
wT?(x,y, z)
However, given that y and z are complex struc-
tures it is necessary to decompose the structure
into a set of smaller decisions to facilitate efficient
inference.
In order to define our decomposition we intro-
duce additional notation: c is a constituent (or
word span) in the input sentence x and D is the
set of all function and constant symbols in the do-
main. The alignment y is defined as a set of map-
pings between constituents and symbols in the do-
main y = {(c, s)} where s ? D.
We decompose the construction of an alignment
and logical form into two types of decisions:
First-order decisions. A mapping between con-
stituents and logical symbols (functions and con-
stants).
Second-order decisions. Expressing how logi-
cal symbols are composed into a complete logical
interpretation. For example, whether next to
and state forms next to(state(?)) or
state(next to(?)).
Note that for all possible logical forms and
alignments there exists a one-to-one mapping to
these decisions.
We frame the inference problem as an Integer
Linear Programming (ILP) problem (Equation (2))
in which the first-order decisions are governed by
?cs, a binary decision variable indicating that con-
stituent c is aligned with logical symbol s. And
?cs,dt capture the second-order decisions indicat-
ing the symbol t (associated with constituent d)
is an argument to function s (associated with con-
stituent c).
Fw(x) = argmax
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?wT?2(x, c, s, d, t) (2)
It is clear that there are dependencies between
the ?-variables and ?-variables. For example,
given that ?cs,dt is active, the corresponding ?-
variables ?cs and ?dt must also be active. In order
to ensure a consistent solution we introduce a set
of constraints on Equation (2). In addition we add
constraints which leverage the typing information
inherent in the domain to eliminate logical forms
that are invalid in the Geoquery domain. For ex-
ample, the function length only accepts river
types as input. The set of constraints are:
? A given constituent can be associated with
exactly one logical symbol.
? ?cs,dt is active if and only if ?cs and ?dt are
active.
? If ?cs,dt is active, s must be a function and
the types of s and t should be consistent.
? Functional composition is directional and
acyclic.
The flexibility of ILP has previously been advan-
tageous in natural language processing tasks (Roth
and Yih, 2007) as it allows us to easily incorporate
such constraints.
4.3 Features
The inference problem defined in Equation (2)
uses two feature functions: ?1 and ?2.
First-order decision features ?1 Determining
if a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.5 This lexicon is
small and only used as a starting point. Existing
approaches rely on annotated logical forms to ex-
tend the lexicon. However, in our setting we do
not have access to annotated logical forms, instead
we rely on external knowledge to supply further
5The lexicon contains on average 1.42 words per func-
tion and 1.07 words per constant. For example the function
next to has the lexical entries: borders, next, adjacent and
the constant illinois the lexical item illinois.
23
information. We add features which measure the
lexical similarity between a constituent and a logi-
cal symbol?s surface forms (as defined by the lexi-
con). Two metrics are used: stemmed word match
and a similarity metric based on WordNet (Miller
et al, 1990) which allows our model to account
for words not in the lexicon. The WordNet met-
ric measures similarity based on synonymy, hy-
ponymy and meronymy (Do et al, 2010). In the
case where the constituent is a preposition, which
are notorious for being ambiguous, we add a fea-
ture that considers the current lexical context (one
word to the left and right) in addition to word sim-
ilarity.
Second-order decision features ?2 Determin-
ing how to compose two logical symbols relies on
syntactic information, in our model we use the de-
pendency tree (Klein and Manning, 2003) of the
input sentence. Given a second-order decision
?cs,dt, the dependency feature takes the normal-
ized distance between the head words in the con-
stituents c and d. A set of features also indicate
which logical symbols are usually composed to-
gether, without considering their alignment to text.
5 Experiments
In this section we describe our experimental setup,
which includes the details of the domain, re-
sources and parameters.
5.1 Domain and Corpus
We evaluate our system on the Geoquery domain
as described previously. The domain consists of
a database and Prolog query language for U.S.
geographical facts. The corpus contains of 880
natural language queries paired with Prolog log-
ical form queries ((x, z) pairs). We follow previ-
ous approaches and transform these queries into a
functional representation. We randomly select 250
sentences for training and 250 sentences for test-
ing.6 We refer to the training set as Response 250
(R250) indicating that each example x in this data
set has a corresponding desired database response
r. We refer the testing set as Query 250 (Q250)
where the examples only contain the natural lan-
guage queries.
6Our inference problem is less constrained than previous
approaches thus we limit the training data to 250 examples
due to scalability issues. We also prune the search space by
limiting the number of logical symbol candidates per word
(on average 13 logical symbols per word).
Precision and recall are typically used as eval-
uation metrics in semantic parsing. However, as
our model inherently has the ability to map any
input sentence into the space of meaning repre-
sentations the trade off between precision and re-
call does not exist. Thus, we report accuracy: the
percentage of meaning representations which pro-
duce the correct response. This is equivalent to
recall in previous work (Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007).
5.2 Resources and Parameters
Feedback Recall that our learning framework
does not require meaning representation annota-
tions. However, we do require a Feedback func-
tion that informs the learner whether a predicted
meaning representation when executed produces
the desired response for a given input sentence.
We automatically generate a set of natural lan-
guage queries and response pairs {(x, r)} by exe-
cuting the annotated logical forms on the database.
Using this data we construct an automatic feed-
back function as described in Section 3.
Domain knowledge Our learning approaches
require an initial weight vector as input. In or-
der to provide an initial starting point, we initialize
the weight vector using a similar procedure to the
one used in (Zettlemoyer and Collins, 2007) to set
weights for three features and a bias term. The
weights were developed on the training set using
the feedback function to guide our choices.
Underlying Learning Algorithms In the direct
approach the base linear classifier we use is a lin-
ear kernel Support Vector Machine with squared-
hinge loss. In the aggressive approach we de-
fine our base structured learner to be a structural
Support Vector Machine with squared-hinge loss
and use hamming distance as the distance func-
tion. We use a custom implementation to op-
timize the objective function using the Cutting-
Plane method, this allows us to parrallelize the
learning process by solving the inference problem
for multiple training examples simultaneously.
6 Results
Our experiments are designed to answer three
questions:
1. Is it possible to learn a semantic parser with-
out annotated logical forms?
24
Algorithm R250 Q250
NOLEARN 22.2 ?
DIRECT 75.2 69.2
AGGRESSIVE 82.4 73.2
SUPERVISED 87.6 80.4
Table 1: Accuracy of learned models on R250 data and
Q250 (testing) data. NOLEARN: using initialized weight
vector, DIRECT: using feedback with the direct approach,
AGGRESSIVE: using feedback with the aggressive approach,
SUPERVISED: using gold 250 logical forms for training.
Note that none of the approaches use any annotated logical
forms besides the SUPERVISED approach.
Algorithm # LF Accuracy
AGGRESSIVE ? 73.2
SUPERVISED 250 80.4
W&M 2006 ? 310 ? 60.0
W&M 2007 ? 310 ? 75.0
Z&C 2005 600 79.29
Z&C 2007 600 86.07
W&M 2007 800 86.59
Table 2: Comparison against previously published results.
Results show that with a similar number of logical forms
(# LF) for training our SUPERVISED approach outperforms
existing systems, while the AGGRESSIVE approach remains
competitive without using any logical forms.
2. How much performance do we sacrifice by
not restricting our model to parsing rules?
3. What, if any, are the differences in behaviour
between the two learning with feedback ap-
proaches?
We first compare how well our model performs
under four different learning regimes. NOLEARN
uses a manually initialized weight vector. DIRECT
and AGGRESSIVE use the two response driven
learning approaches, where a feedback function
but no logical forms are provided. As an up-
per bound we train the model using a fully SU-
PERVISED approach where the input sentences are
paired with hand annotated logical forms.
Table 1 shows the accuracy of each setup. The
model without learning (NOLEARN) gives a start-
ing point with an accuracy of 22.2%. The re-
sponse driven learning methods perform substan-
tially better than the starting point. The DIRECT
approach which uses a binary learner reaches an
accuracy of 75.2% on the R250 data and 69.2% on
the Q250 (testing) data. While the AGGRESSIVE
approach which uses a structured learner sees a
bigger improvement, reaching 82.4% and 73.2%
respectively. This is only 7% below the fully SU-
PERVISED upper bound of the model.
To answer the second question, we compare a
supervised version of our model to existing se-
mantic parsers. The results are in Table 2. Al-
though the numbers are not directly comparable
due to different splits in the data7, we can see that
with a similar number of logical forms for train-
ing our SUPERVISED approach outperforms ex-
isting systems (Wong and Mooney, 2006; Wong
and Mooney, 2007), while the AGGRESSIVE ap-
proach remains competitive without using any log-
ical forms. Our SUPERVISED model is still very
competitive with other approaches (Zettlemoyer
and Collins, 2007; Wong and Mooney, 2007),
which used considerably more annotated logical
forms in the training phase.
In order to answer the third question, we turn
our attention to the differences between the two
response driven learning approaches. The DIRECT
and AGGRESSIVE approaches use binary feedback
to learn, however they utilize the signal differently.
DIRECT uses the signal directly to learn a bi-
nary classifier capable of replicating the feedback,
whereas AGGRESSIVE learns a structured predic-
tor that can repeatedly obtain the logical forms
for which positive feedback was received. Thus,
although the AGGRESSIVE outperforms the DI-
RECT approach the concepts each approach learns
may be different. Analysis over the training data
shows that in 66.8% examples both approaches
predict a logical form that gives the correct an-
swer. While AGGRESSIVE correctly answers an
additional 16% which DIRECT gets incorrect. In
the opposite direction, DIRECT correctly answers
8.8% that AGGRESSIVE does not. Leaving only
8.4% of the examples that both approaches pre-
dict incorrect logical forms. This suggests that an
approach which combines DIRECT and AGGRES-
SIVE may be able to improve even further.
Figure 2 shows the accuracy on the entire train-
ing data (R250) at each iteration of learning. We
see that the AGGRESSIVE approach learns to cover
more of the training data and at a faster rate than
DIRECT. Note that the performance of the DI-
RECT approach drops at the first iteration. We hy-
pothesize this is due to imbalances in the binary
feedback dataset (too many negative examples) in
the first iteration.
7It is relatively difficult to compare different approaches
in the Geoquery domain given that many existing papers do
not use the same data split.
25
70 1 2 3 4 5 6
90
0
10
20
30
40
50
60
70
80
Learning Iterations
A
cc
ur
ac
y 
on
 R
es
po
ns
e 
25
0
Direct Approach
Aggressive Approach
Initialization
Figure 2: Accuracy on training set as number of learning
iterations increases.
7 Related Work
Learning to map sentences to a meaning repre-
sentation has been studied extensively in the NLP
community. Early works (Zelle and Mooney,
1996; Tang and Mooney, 2000) employed induc-
tive logic programming approaches to learn a se-
mantic parser. More recent works apply statisti-
cal learning methods to the problem. In (Ge and
Mooney, 2005; Nguyen et al, 2006), the input to
the learner consists of complete syntactic deriva-
tions for the input sentences annotated with logi-
cal expressions. Other works (Wong and Mooney,
2006; Kate and Mooney, 2006; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009) try to alleviate the
annotation effort by only taking sentence and log-
ical form pairs to train the models. Learning is
then defined over hidden patterns in the training
data that associate logical symbols with lexical
and syntactic elements.
In this work we take an additional step to-
wards alleviating the difficulty of training seman-
tic parsers and present a world response based
training protocol. Several recent works (Chen and
Mooney, 2008; Liang et al, 2009; Branavan et
al., 2009) explore using an external world context
as a supervision signal for semantic interpretation.
These works operate in settings different to ours as
they rely on an external world state that is directly
referenced by the input text. Although our frame-
work can also be applied in these settings we do
not assume that the text can be grounded in a world
state. In our experiments the input text consists of
generalized statements which describe some infor-
mation need that does not correspond directly to a
grounded world state.
Our learning framework closely follows recent
work on learning from indirect supervision. The
direct approach resembles learning a binary clas-
sifier over a latent structure (Chang et al, 2010a);
while the aggressive approach has similarities with
work that uses labeled structures and a binary
signal indicating the existence of good structures
to improve structured prediction (Chang et al,
2010b).
8 Conclusions
In this paper we tackle one of the key bottlenecks
in semantic parsing ? providing sufficient super-
vision to train a semantic parser. Our solution is
two fold, first we present a new training paradigm
for semantic parsing that relies on natural, hu-
man level supervision. Second, we suggest a new
model for semantic interpretation that does not
rely on NL syntactic parsing rules, but rather uses
the syntactic information to bias the interpretation
process. This approach allows the model to gener-
alize better and reduce the required amount of su-
pervision. We demonstrate the effectiveness of our
training paradigm and interpretation model over
the Geoquery domain, and show that our model
can outperform fully supervised systems.
Acknowledgements We are grateful to Rohit Kate and
Raymond Mooney for their help with the Geoquery dataset.
Thanks to Yee Seng Chan, Nick Rizzolo, Shankar Vembu
and the three anonymous reviewers for their insightful com-
ments. This material is based upon work supported by the
Air Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181 and by DARPA under the Bootstrap
Learning Program. Any opinions, findings, and conclusion or
recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the AFRL
or DARPA.
References
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning for map-
ping instructions to actions. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained la-
tent representations. In Proc. of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
26
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010b. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In Proc. of
the International Conference on Machine Learning
(ICML).
Q. Do, D. Roth, M. Sammons, Y. Tu, and V.G. Vydis-
waran. 2010. Robust, Light-weight Approaches to
compute Lexical Similarity. Computer Science Re-
search and Technical Reports, University of Illinois.
http://hdl.handle.net/2142/15462.
R. Ge and R. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
R. Kate and R. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).
R. Kate. 2008. Transforming meaning representation
grammars to improve semantic parsing. In Proc. of
the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Proc. of the Conference on Advances in
Neural Information Processing Systems (NIPS).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Semantic
parsing with structured svm ensemble classification
models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construc-
tion of database interfaces: integrating statistical and
relational learning for semantic parsing. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming
for semantic parsing. In Proc. of the European Con-
ference on Machine Learning (ECML).
Y.-W. Wong and R. Mooney. 2006. Learning for
semantic parsing with statistical machine transla-
tion. In Proc. of the Annual Meeting of the North
American Association of Computational Linguistics
(NAACL).
Y.-W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Proc. of the National Conference on Artificial In-
telligence (AAAI).
L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. of
the Annual Conference in Uncertainty in Artificial
Intelligence (UAI).
L. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Pro-
cessing and on Computational Natural Language
Learning (EMNLP-CoNLL).
L. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
27
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 28?33,
Baltimore, Maryland USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Understanding MOOC Discussion Forums using Seeded LDA
1
Arti Ramesh,
1
Dan Goldwasser,
1
Bert Huang,
1
Hal Daum
?
e III,
2
Lise Getoor
1
University of Maryland, College Park
2
University of California, Santa Cruz
{artir, bert, hal}@cs.umd.edu, goldwas1@umiacs.umd.edu, getoor@soe.ucsc.edu
Abstract
Discussion forums serve as a platform for
student discussions in massive open online
courses (MOOCs). Analyzing content in
these forums can uncover useful informa-
tion for improving student retention and
help in initiating instructor intervention.
In this work, we explore the use of topic
models, particularly seeded topic models
toward this goal. We demonstrate that fea-
tures derived from topic analysis help in
predicting student survival.
1 Introduction
This paper highlights the importance of under-
standing MOOC discussion forum content, and
shows that capturing discussion forum content
can help uncover students? intentions and motiva-
tion and provide useful information in predicting
course completion.
MOOC discussion forums provide a platform
for exchange of ideas, course administration and
logistics questions, reporting errors in lectures,
and discussion about course material. Unlike
classroom settings, where there is face-to-face in-
teraction between the instructor and the students
and among the students, MOOC forums are the
primary means of interaction in MOOCs. How-
ever, due to the large number of students and the
large volume of posts generated by them, MOOC
forums are not monitored completely. Forums
can include student posts expressing difficulties in
course-work, grading errors, dissatisfaction in the
course, which are possible precursors to students
dropping out.
Previous work analyzing discussion forum con-
tent tried manually labeling posts by categories of
interest (Stump et al., 2013). Unfortunately, the
effort involved in manually annotating the large
amounts of posts prevents using such solutions on
a large scale. Instead, we suggest using natural
language processing tools for identifying relevant
aspects of forum content automatically. Specifi-
cally, we explore SeededLDA (Jagarlamudi et al.,
2012), a recent extension of topic models which
can utilize a lexical seed set to bias the topics ac-
cording to relevant domain knowledge.
Exploring data from three MOOCs, we find
that forum posts usually belong to these three
categories?a) course content, which include dis-
cussions about course material (COURSE), b)
meta-level discussions about the course, including
feedback and course logistics (LOGISTICS), and c)
other general discussions, which include student
introductions, discussions about online courses
(GENERAL). In order to capture these categories
automatically we provide seed words for each cat-
egory. For example, we extract seed words for
the COURSE topic from each course?s syllabus.
In addition to the automatic topic assignment, we
capture the sentiment polarity using Opinionfinder
(Wilson et al., 2005). We use features derived
from topic assignments and sentiment to predict
student course completion (student survival). We
measure course completion by examining if the
student attempted the final exam/ last few assign-
ments in the course. We follow the observation
that LOGISTICS posts contain feedback about the
course. Finding high-confidence LOGISTICS posts
can give a better understanding of student opinion
about the course. Similarly, posting in COURSE
topic and receiving good feedback (i.e., votes) is
an indicator of student success and might con-
tribute to survival. We show that modeling these
intuitions using topic assignments together with
sentiment scores, helps in predicting student sur-
vival. In addition, we examine the topic assign-
ment and sentiment patterns of some users and
show that topic assignments help in understanding
student concerns better.
28
2 Modeling Student Survival
Our work builds on work by Ramesh et al. (2013)
and (2014) on modeling student survival using
Probabilistic Soft Logic (PSL). The authors in-
cluded behavioral features, such as lecture views,
posting/voting/viewing discussion forum content,
linguistic features, such as sentiment and subjec-
tivity of posts, and social interaction features de-
rived from forum interaction. The authors looked
at indication of sentiment without modeling the
context in which the sentiment was expressed:
positive sentiment implying survival and negative
sentiment implying drop-out. In this work, we
tackle this problem by adding topics, enabling rea-
soning about specific types of posts. While senti-
ment of posts can indicate general dissatisfaction,
we expect this to be more pronounced in LOGIS-
TICS posts as posts in this category correspond to
issues and feedback about the course. In contrast,
sentiment in posts about course material may sig-
nal a particular topic of discussion in a course and
may not indicate attitude of the student toward the
course. In Section 4.3, we show some examples of
course-related posts and their sentiment, and we
illustrate that they are not suggestive of student
survival. For example, in Women and the Civil
Rights Movement course, the post??I think our
values are shaped by past generations in our fam-
ily as well, sometimes negatively.??indicates an
attitude towards an issue discussed as part of the
course. Hence, identifying posts that fall under
LOGISTICS can improve the value of sentiment in
posts. In Section 3, we show how these are trans-
lated into rules in our model.
2.1 Probabilistic Soft Logic
We briefly overview the some technical details be-
hind Probabilistic Soft Logic (PSL). For brevity,
we omit many specifics, and we refer the reader
to (Broecheler et al., 2010; Bach et al., 2013)
for more details. PSL is a framework for collec-
tive, probabilistic reasoning in relational domains.
Like other statistical relational learning methods
(Getoor and Taskar, 2007), PSL uses weighted
rules to model dependencies in a domain. How-
ever, one distinguishing aspect is that PSL uses
continuous variables to represent truth values, re-
laxing Boolean truth values to the interval [0,1].
Table 1 lists some PSL rules from our model.
The predicate posts captures the relationship be-
tween a post and the user who posted it. Predicate
polarity(P) represents sentiment via its truth value
in [0, 1], where 1.0 signifies positive sentiment,
and 0.0 signifies negative sentiment. upvote(P) is
1.0 if the post has positive feedback and 0.0 if the
post had negative or no feedback. U and P refer to
user and post respectively. These features can be
combined to produce rules in Table 1. For exam-
ple, the first rule captures the idea that posts with
positive sentiment imply student survival.
? posts(U,P ) ? polarity(P ) ? survival(U)
? posts(U,P ) ? ?polarity(P ) ? ?survival(U)
? posts(U,P ) ? upvote(P ) ? survival(U)
Table 1: Example rules in PSL
3 Enhancing Student Survival Models
with Topic Modeling
Discussion forums in online courses are organized
into threads to facilitate grouping of posts into top-
ics. For example, a thread titled errata, grading
issues is likely a place for discussing course logis-
tics and a thread titled week 1, lecture 1 is likely a
place for discussing course content. But a more
precise examination of such threads reveals that
these heuristics do not always hold. We have ob-
served that course content threads often house lo-
gistic content and vice-versa. This demands the
necessity of using computational linguistics meth-
ods to classify the content in discussion forums.
In this work, we?1) use topic models to map
posts to topics in an unsupervised way, and 2)
employ background knowledge from the course
syllabus and manual inspection of discussion fo-
rum posts to seed topic models to get better sep-
arated topics. We use data from three Cours-
era MOOCs: Surviving Disruptive Technologies,
Women and the Civil Rights Movement, and Gene
and the Human Condition for our analysis. In dis-
cussion below, we refer to these courses as DISR-
TECH, WOMEN, and GENE, respectively.
3.1 Latent Dirichlet Allocation
Table 2 gives the topics given by latent Dirichlet
allocation (LDA) on discussion forum posts. The
words that are likely to fall under LOGISTICS are
underlined in the table. It can be observed that
these words are spread across more than one topic.
Since we are especially interested in posts that are
on LOGISTICS, we use SeededLDA (Jagarlamudi
et al., 2012), which allows one to specify seed
words that can influence the discovered topics to-
ward our desired three categories.
29
topic 1: kodak, management, great, innovation, post, agree, film, understand, something, problem, businesses, changes, needs
topic 2: good, change, publishing, brand, companies, publishers, history, marketing, traditional, believe, authors
topic 3: think, work, technologies, newspaper, content, paper, model, business, disruptive, information, survive, print, media, course, assignment
topic 4: digital, kodak, company, camera, market, quality, phone, development, future, failed, high, right, old,
topic 5: amazon, books, netflix, blockbuster, stores, online, experience, products, apple, nook, strategy, video, service
topic 6: time, grading, different, class, course, major, focus, product, like, years
topic 7: companies, interesting, class, thanks, going, printing, far, wonder, article, sure
Table 2: Topics identified by LDA
topic 1: thank, professor, lectures, assignments, concept, love, thanks, learned, enjoyed, forums, subject, question, hard, time, grading, peer, lower, low
topic 2: learning, education, moocs, courses, students, online, university, classroom, teaching, coursera
Table 3: Seed words in LOGISTICS and GENERAL for DISR-TECH, WOMEN and GENE courses
topic 3a: disruptive, technology, innovation, survival, digital, disruption, survivor
topic 3b: women, civil, rights, movement, american, black, struggle, political, protests, organizations, events, historians, african, status, citizenship
topic 3c: genomics, genome, egg, living, processes, ancestors, genes, nature, epigenitics, behavior, genetic, engineering, biotechnology
Table 4: Seed words for COURSE topic for DISR-TECH, WOMEN and GENE courses
topic 1: time, thanks, one, low, hard, question, course, love, professor, lectures, lower, another, concept, agree, peer, point, never
topic 2: online, education, coursera, students, university, courses, classroom, moocs, teaching, video
topic 3: digital, survival, management, disruption, technology, development, market, business, innovation
topic 4: publishing, publisher, traditional, companies, money, history, brand
topic 5: companies, social, internet, work, example
topic 6: business, company, products, services, post, consumer, market, phone, changes, apple
topic 7: amazon, book, nook, readers, strategy, print, noble, barnes
Table 5: Topics identified by SeededLDA for DISR-TECH
topic 1: time, thanks, one, hard, question, course, love, professor, lectures, forums, help, essays, problem, thread, concept, subject
topic 2: online, education, coursera, students, university, courses, classroom, moocs, teaching, video, work, english, interested, everyone
topic 3: women, rights, black, civil, movement, african, struggle, social, citizenship, community, lynching, class, freedom, racial, segregation
topic 4: violence, public, people, one, justice, school,s state, vote, make, system, laws
topic 5: idea, believe, women, world, today, family, group, rights
topic 6: one, years, family, school, history, person, men, children, king, church, mother, story, young
topic 7: lynching, books, mississippi, march, media, youtube, death, google, woman, watch, mrs, south, article, film
Table 6: Topics identified by SeededLDA for WOMEN
topic 1: time, thanks, one, answer, hard, question, course, love, professor, lectures, brian, lever, another, concept, agree, peer, material, interesting
topic 2: online, education, coursera, students, university, courses, classroom, moocs, teaching, video, knowledge, school
topic 3: genes, genome, nature, dna, gene, living, behavior, chromosomes, mutation, processes
topic 4: genetic, biotechnology, engineering, cancer, science, research, function, rna
topic 5: reproduce, animals, vitamin, correct, term, summary, read, steps
topic 6: food, body, cells, alleles blood, less, area, present, gmo, crops, population, stop
topic 7: something, group, dna, certain, type, early, large, cause, less, cells
Table 7: Topics identified by SeededLDA for GENE
3.2 Seeded LDA
We experiment by providing seed words for top-
ics that fall into the three categories. The seed
words for the three courses are listed in tables 3
and 4. The seed words for LOGISTICS and GEN-
ERAL are common across all the three courses.
The seed words for the COURSE topic are chosen
from the course-syllabus of the courses. This con-
struction of seed words enables the model to be
applied to new courses easily. Topics 3a, 3b, and
3c denote the course specific seed words for DISR-
TECH, WOMEN, and GENE courses respectively.
Since the syllabus is only an outline of the class,
it does not contain all the terms that will be used
in class discussions. To capture other finer course
content discussions as separate topics, we include
k more topics when we run the SeededLDA. We
notice that not including more topics here, only
including the seeded topics (i.e., run SeededLDA
with exactly three topics) results in some words
from course content discussions, which were not
specified in the course-seed words, appearing in
the LOGISTICS or GENERAL topics. Thus, the k
extra topics help represent COURSE topics that do
not directly correspond to the course seeds. Note
that these extra topics are not seeded. We exper-
imented with different values of k on our exper-
iments and found by manual inspection that the
topic-terms produced by our model were well sep-
arated for k = 3. Thus, we run SeededLDA with
7 total topics. Tables 5, 6, and 7 give the top-
ics identified for DISR-TECH, WOMEN and GENE
by SeededLDA. The topic assignments so obtained
are used as input features to the PSL model?the
predicate for the first topic is LOGISTICS, the sec-
ond one is GENERAL and the rest are summed up
to get the topic assignment for COURSE.
3.3 Using topic assignments in PSL
We construct two models?a) DIRECT model, in-
cluding all features except features from topic
30
survival = 0.0 polarity = 0.25 logistics = 0.657
general = 0.028
course = 0.314
JSTOR allowed 3 items (texts/writings) on my ?shelf? for 14 days. But, I read the items and wish to return them, but
cannot, until 14 days has expired. It is difficult then, to do the extra readings in the ?Exploring Further? section of Week
1 reading list in a timely manner. Does anyone have any ideas for surmounting this issue?
survival = 0.0 polarity = 0.0 logistics = 0.643
general = 0.071
course = 0.285
There are some mistakes on quiz 2. Questions 3, 5, and 15 mark you wrong for answers that are correct.
survival = 0.0 polarity = 0.25 logistics = 0.652
general = 0.043
course = 0.304
I see week 5 quiz is due April 1( by midnight 3/31/13).I am concerned about this due date being on Easter, some of us
will be traveling, such as myself. Can the due date be later in the week? Thank you
Table 8: Logistics posts containing negative sentiment for dropped-out students
survival = 1.0 polarity = 0.0 logistics = 0.67
general = 0.067
course = 0.267
I was just looking at the topics for the second essay assignments. The thing is I dont see what the question choices are.
I have the option of Weeks and I have no idea what that even means. Can someone help me out here and tell me what
the questions for the second essay assignment are I think my computer isnt allowing me to see the whole assignment!
Someone please help me out and let me know that the options are.
survival = 1.0 polarity = 0.25 logistics = 0.769
general = 0.051
course = 0.179
I?d appreciate someone looks into the following: Lecture slides for the videos (week 5) don?t open (at all) (irrespective
of the used browser). Some required reading material for week 5 won?t open either (error message). I also have a sense
that there should be more material posted for the week (optional readings, more videos, etc). Thanks. ? I am not
seeing a quiz posted for Week 5.
survival = 1.0 polarity = 0.78 logistics = 0.67
general = 0.067
course = 0.267
Hopefully the Terrell reading and the Lecture PowerPoints now open for you. Thanks for reporting this.
Table 9: Example of change in sentiment in a course logistic thread
survival = 1.0 polarity = 0.25 logistics = 0.372
general = 0.163
course = 0.465
I?ve got very interested in the dynamic of segregation in terms of space and body pointed by Professor Brown and
found a document written by GerShun Avilez called ?Housing the Black Body: Value, Domestic Space,and Segregation
Narratives?.
survival = 1.0 polarity = 0.9 logistics = 0.202
general = 0.025
course = 0.772
I think that you hit it on the head, the whole idea of Emancipation came as a result not so much of rights but of the need
to get the Transcontinental Railroad through the mid-west and the north did not want the wealth of the southern slave
owners to overshadow the available shares. There are many brilliant people ?good will hunting?, and their brilliance
either dies with them or dies while they are alive due to intolerance. Many things have happened in my life to cause me
to be tolerant to others and see what their debate is, Many very evil social ills and stereotypes are a result of ignorance.
It would be awesome if the brilliant minds could all come together for reform and change.
survival = 1.0 polarity = 0.167 logistics = 0.052
general = 0.104
course = 0.844
I think our values are shaped by past generations in our family as well ? sometimes negatively. In Bliss, Michigan
where I come from, 5 families settled when the government kicked out the residents ? Ottowa Tribe Native Americans.
I am descended from the 5 families. All of the cultural influences in Bliss were white Christian ? the Native American
population had never been welcomed back or invited to stay as they had in Cross Village just down the beach. My
family moved to the city for 4 years during my childhood, and I had African American, Asian, and Hispanic classmates
and friends. When we moved back to the country I was confronted with the racism and generational wrong-doings of
my ancestors. At the tender age of 10 my awareness had been raised! Was I ever pissed off when the full awareness of
the situation hit me! I still am.
Table 10: Posts talking about COURSE content
DIRECT DIRECT+TOPIC
posts(U,P ) ? polarity(P ) ? survival(U) posts(U,P ) ? topic(P, LOGISTICS) ? ?polarity(P ) ? survival(U)
posts(U,P ) ? ?polarity(P ) ? ?survival(U) posts(U,P ) ? topic(P, LOGISTICS) ? ?polarity(P ) ? survival(U)
posts(U,P ) ? survival(U) posts(U,P ) ? topic(P, GENERAL) ? ?survival(U)
posts(U,P ) ? upvote(P ) ? survival(U) posts(U,P ) ? topic(P, COURSE) ? upvote(P ) ? survival(U)
posts(U
1
, P ) ? posts(U
2
, P ) ? topic(P, COURSE) ? survival(U
1
) ?
survival(U
2
)
Table 11: Rules modified to include topic features
modeling, and b) DIRECT+TOPIC model, includ-
ing the topic assignments as features in the model.
Our DIRECT model is borrowed from Ramesh
(2014). We refer the reader to (Ramesh et al.,
2013) and (Ramesh et al., 2014) for a complete
list of features and rules in this model.
Table 11 contains examples of rules in the DI-
RECT model and the corresponding rules includ-
ing topic assignments in DIRECT+TOPIC model.
The first and second rules containing polarity are
changed to include LOGISTICS topic feature, fol-
lowing our observation that polarity matters in
meta-course posts. While the DIRECT model re-
gards posting in forums as an indication of sur-
vival, in the DIRECT+TOPIC model, this rule is
changed to capture that students that post a lot of
general stuff only on the forums do not necessar-
ily participate in course-related discussions. The
fourth rule containing upvote predicate, which sig-
nifies posts that received positive feedback in the
form of votes, is changed to include the topic-
feature COURSE. This captures the significance
of posting course-related content that gets posi-
tive feedback as opposed to logistics or general
content in the forums. This rule helps us discern
posts in general/logistic category that can get a lot
31
of positive votes (upvote), but do not necessarily
indicate student survival. For example, some in-
troduction threads have a lot of positive votes, but
do not necessarily signify student survival.
4 Empirical Evaluation
We conducted experiments to answer the follow-
ing question?how much do the topic assignments
from SeededLDA help in predicting student sur-
vival? We also perform a qualitative analysis
of topic assignments, the sentiment of posts, and
their correspondence with student survival.
COURSE MODEL AUC-PR
POS.
AUC-PR
NEG.
AUC-
ROC
DISR-TECH
DIRECT 0.764 0.628 0.688
DIRECT+TOPIC 0.794 0.638 0.708
WOMEN
DIRECT 0.654 0.899 0.820
DIRECT+TOPIC 0.674 0.900 0.834
GENE
DIRECT 0.874 0.780 0.860
DIRECT+TOPIC 0.894 0.791 0.873
Table 12: Performance of DIRECT and DI-
RECT+TOPIC models in predicting student sur-
vival. Statistically significant scores typed in bold.
4.1 Datasets and Experimental Setup
We evaluate our models on three Coursera
MOOCs: DISR-TECH, WOMEN-CIVIL, and GENE,
respectively. Our data consists of anonymized stu-
dent records, grades, and online behavior recorded
during the seven week duration of each course. We
label students as survival = 1.0 if they take the fi-
nal exam/quiz and survival = 0.0 otherwise. In
our experiments, we only consider students that
completed at least one quiz/assignment. We eval-
uate our models using area under precision-recall
curve for positive and negative survival labels and
area under ROC curve. We use ten-fold cross-
validation on each of the courses, leaving out 10%
of users for testing and revealing the rest of the
users for training the model weights. We evaluate
statistical significance using a paired t-test with a
rejection threshold of 0.05.
4.2 Survival Prediction using topic features
Table 12 shows the prediction performance of the
DIRECT and DIRECT+TOPIC model. The inclu-
sion of the topic-features improves student sur-
vival prediction in all the three courses.
4.3 Discussion topic analysis using topic
features
Table 8 shows some posts by users that did not
survive the class. All these posts have negative
sentiment scores by Opinionfinder and belong to
LOGISTICS. Also, in the forum, all these posts
were not answered. This suggests that students
might drop out if their course-logistics questions
are not answered. Table 9 gives examples of stu-
dent posts that also have a negative sentiment. But
the sentiment of the thread changes when the issue
is resolved (last row in the table). We observe that
these two students survive the course and a timely
answer to their posts might have been a reason in-
fluencing these students to complete the course.
Tables 8 and 9 show how student survival may
depend on forum interaction and responses they
receive. Our approach can help discover potential
points of contention in the forums, identifying po-
tential drop outs that can be avoided by interven-
tion.
Table 10 shows posts flagged as COURSE by the
SeededLDA. The polarity scores in the COURSE
posts indicate opinions and attitude toward course
specific material. For example, post #3 in Table 10
indicates opinion towards human rights. While the
post?s polarity is negative, it is clear that this po-
larity value is not directed at the course and should
not be used to predict student survival. In fact, all
these users survive the course. We find that par-
ticipation in course related discussion is a sign of
survival. These examples demonstrate that analy-
sis on COURSE posts can mislead survival and jus-
tify our using topic predictions to focus sentiment
analysis on LOGISTICS posts.
5 Discussion
In this paper, we have taken a step toward un-
derstanding discussion content in massive open
online courses. Our topic analysis is coarse-
grained, grouping posts into three categories. In
our analysis, all the meta-content?course logis-
tics and course feedback?were grouped under the
same topic category. Instead, a finer-grained topic
model could be seeded with different components
of meta-content as separate topics. The same ap-
plies for course-related posts too, where a finer-
grained analysis could help identify difficult topics
that may cause student frustration and dropout.
Acknowledgements We thank the instructors for letting us
use data from their course. This work is supported by Na-
tional Science Foundation under Grant No. CCF0937094.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the views of the National Science Founda-
tion.
32
References
Stephen H. Bach, Bert Huang, Ben London, and Lise Getoor.
2013. Hinge-loss Markov random fields: Convex infer-
ence for structured prediction. In Uncertainty in Artificial
Intelligence.
Matthias Broecheler, Lilyana Mihalkova, and Lise Getoor.
2010. Probabilistic similarity logic. In Uncertainty in Ar-
tificial Intelligence (UAI).
Lise Getoor and Ben Taskar. 2007. Introduction to Statistical
Relational Learning (Adaptive Computation and Machine
Learning). The MIT Press.
Jagadeesh Jagarlamudi, Hal Daum?e, III, and Raghavendra
Udupa. 2012. Incorporating lexical priors into topic
models. In Proceedings of the 13th Conference of the
European Chapter of the Association for Computational
Linguistics, EACL ?12, pages 204?213, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Angelika Kimmig, Stephen H. Bach, Matthias Broecheler,
Bert Huang, and Lise Getoor. 2012. A short introduction
to probabilistic soft logic. In NIPS Workshop on Proba-
bilistic Programming: Foundations and Applications.
Arti Ramesh, Dan Goldwasser, Bert Huang, Hal Daume III,
and Lise Getoor. 2013. Modeling learner engagement in
MOOCs using probabilistic soft logic. In NIPS Workshop
on Data Driven Education.
Arti Ramesh, Dan Goldwasser, Bert Huang, Hal Daume III,
and Lise Getoor. 2014. Learning latent engagement pat-
terns of students in online courses. In Proceedings of the
Twenty-Eighth AAAI Conference on Artificial Intelligence.
Glenda S. Stump, Jennifer DeBoer, Jonathan Whittinghill,
and Lori Breslow. 2013. Development of a framework to
classify mooc discussion forum posts: Methodology and
challenges. In NIPS Workshop on Data Driven Education.
Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Ja-
son Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,
Ellen Riloff, and Siddharth Patwardhan. 2005. Opinion-
Finder: A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations.
33
