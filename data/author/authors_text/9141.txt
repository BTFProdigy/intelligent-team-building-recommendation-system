Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 793?802,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Learning Term-weighting Functions for Similarity Measures
Wen-tau Yih
Microsoft Research
Redmond, WA, USA
scottyih@microsoft.com
Abstract
Measuring the similarity between two
texts is a fundamental problem in many
NLP and IR applications. Among the ex-
isting approaches, the cosine measure of
the term vectors representing the origi-
nal texts has been widely used, where the
score of each term is often determined
by a TFIDF formula. Despite its sim-
plicity, the quality of such cosine similar-
ity measure is usually domain dependent
and decided by the choice of the term-
weighting function. In this paper, we pro-
pose a novel framework that learns the
term-weighting function. Given the la-
beled pairs of texts as training data, the
learning procedure tunes the model pa-
rameters by minimizing the specified loss
function of the similarity score. Com-
pared to traditional TFIDF term-weighting
schemes, our approach shows a significant
improvement on tasks such as judging the
quality of query suggestions and filtering
irrelevant ads for online advertising.
1 Introduction
Measuring the semantic similarity between two
texts is an important problem that has many use-
ful applications in both NLP and IR communi-
ties. For example, Lin (1998) defined a similar-
ity measure for automatic thesaurus creation from
a corpus. Mihalcea et al (2006) developed sev-
eral corpus-based and knowledge-based word sim-
ilarity measures and applied them to a paraphrase
recognition task. In the domain of web search, dif-
ferent methods of measuring similarity between
short text segments have recently been proposed
for solving problems like query suggestion and al-
ternation (Jones et al, 2006; Sahami and Heilman,
2006; Metzler et al, 2007; Yih and Meek, 2007).
Among these similarity measures proposed in
various applications, the vector-based methods are
arguably the most widely used. In this approach,
the text being compared with is first represented
by a term vector, where each term is associated
with a weight that indicates its importance. The
similarity function could be cosine (i.e., the inner
product of two normalized unit term vectors, or
equivalently a linear kernel), or other kernel func-
tions such as the Gaussian kernel.
There are essentially two main factors that de-
cide the quality of a vector-based similarity mea-
sure. One is the vector operation that takes as in-
put the term vectors and computes the final simi-
larity score (e.g., cosine). The other is how these
term vectors are constructed, including the term
selection process and how the weights are deter-
mined. For instance, a TFIDF scheme for mea-
suring document similarity may follow the bag-of-
words strategy to include all the words in the doc-
ument when constructing the term vectors. The
weight of each term is simply the product of its
term frequency (i.e., the number of occurrences
in the document) and inverse document frequency
(i.e., the number of documents in a collection that
contain this term).
Despite its simplicity and reasonable perfor-
mance, such approach suffers from several weak-
nesses. For instance, the similarity measure is not
domain-dependent and cannot be easily adjusted
to better fit the final objective, such as being a
metric value used for clustering or providing better
ranking results. Researchers often need to experi-
ment with variants of TFIDF formulas and differ-
ent term selection strategies (e.g., removing stop-
words or stemming) to achieve acceptable perfor-
mance (Manning et al, 2008). In addition, when
more information is available, such as the position
of a term in the document or whether a term is part
of an anchor text, incorporating it in the similarity
measure in a principled manner may not be easy.
793
In this paper, we propose a general term-
weighting learning framework, TWEAK, that
learns the term-weighting function for the vector-
based similarity measures. Instead of using a
fixed formula to decide the weight of each term,
TWEAK uses a parametric function of features of
each term, where the model parameters are learned
from labeled data. Although the weight of each
term conceptually represents its importance with
respect to the document, tuning the model param-
eters to optimize for such objectives may not be
the best strategy due to two reasons. While the
label of whether a pair of texts is similar is not dif-
ficult to collect from human annotators1, the label
of whether a term in a document is important is
often very ambiguous and hard to decide. Even
if such annotation issue can be resolved, aligning
the term weights with the true importance of each
term may not necessarily lead to our real objec-
tive ? deriving a better similarity measure for the
target application. Therefore, our learning frame-
work, TWEAK, assumes that we are given only the
labels of the pairs of texts being compared, such
as whether the two texts are considered similar by
human subjects.
TWEAK is flexible in choosing various loss
functions that are close to the true objectives,
while still maintaining the simplicity of the vector-
based similarity measures. For example, a system
that implements the TFIDF cosine measure can
easily replace the original term-weighting scores
with the ones output by TWEAK without changing
other portions of the algorithm. TWEAK is also
novel compared to other existing learning meth-
ods for similarity measures. For instance, we do
not learn the scores of all the terms in the vocab-
ulary directly, which is one of the methods pro-
posed by Bilenko and Mooney (2003). Because
the vocabulary size is typically large in the text
domain (e.g., all possible words in English), learn-
ing directly the term-weighting scores may suffer
from the data sparsity issue and cannot general-
ize well in practice. Instead, we focus on learning
the model parameters for features that each term
may have, which results in a much smaller fea-
ture space. TWEAK also differs from the model
combination approach proposed by Yih and Meek
(2007), where the output scores of different simi-
larity measures are combined via a learned linear
1As argued in (Sheng et al, 2008), low-cost labels may
nowadays be provided by outsourcing systems such as Ama-
zon?s Mechanical Turk or online ESP games.
function. In contrast, TWEAK effectively learns
a new similarity measure by tuning the term-
weighting function and can potentially be comple-
mentary to the model combination approach.
As will be demonstrated in our experiments, in
applications such as judging the relevance of dif-
ferent query suggestions and determining whether
a paid-search ad is related to the user query,
TWEAK can incorporate various kinds of term?
document information and learn a term-weighting
function that significantly outperforms the tradi-
tional TFIDF scheme in several evaluation met-
rics, when using the same vector operation (i.e.,
cosine) and the same set of terms.
We organize the rest of the paper as follows.
Sec. 2 first gives a high-level view of our term-
weighting learning framework. We then formally
define our model and present the loss functions
that can be optimized for in Sec. 3. Experiments
on target applications are presented in Sec. 4. Fi-
nally, we compare our approach with some related
work in Sec. 5 and conclude the paper in Sec. 6.
2 Problem Statement
To simplify the description, assume that the texts
we are comparing are two documents. A general
architecture of vector-based similarity measures
can be formally described as follows. Given two
documents D
p
and D
q
, a similarity function maps
them to a real-valued number, where a higher
value indicates these two documents are seman-
tically more related, considered by the measure.
Suppose a pre-defined vocabulary set V =
{t
1
, t
2
, ? ? ? , t
n
} consists of all possible terms (e.g.,
tokens, words) that may occur in the documents.
Each document D
p
is represented by a term vector
of length n: v
p
= (s
1
p
, s
2
p
, ? ? ? , s
n
p
), where si
p
? R
is the weight of term t
i
, and is determined by the
term-weighting function tw that depends on the
term and the document (i.e., si
p
? tw(t
i
, D
p
)).
The similarity between documents D
p
and D
q
is then computed by a vector operation function
f
sim
: (v
p
,v
q
) ? R, illustrated in Fig. 1.
Determining the specific functions f
sim
and tw
effectively decides the final similarity measure.
For example, the functions that construct the tra-
ditional TFIDF cosine similarity can be:
f
sim
(v
p
,v
q
) ?
v
p
? v
q
||v
p
|| ? ||v
q
||
(1)
tw(t
i
, D
p
) ? tf(t
i
, D
p
) ? log
(
N
df(t
i
)
)
(2)
794
Figure 1: A general architecture of vector-based
similarity measures
where N is the size of the document collection for
deriving document frequencies, tf and df are the
functions computing the term frequency and doc-
ument frequency, respectively.
In contrast, TWEAK also takes a specified vec-
tor function f
sim
but assumes a parametric term-
weighting function tw
w
. Given the training data,
it learns the model parameters w that optimize for
the designated loss function.
3 Model
As a specific instantiation of our learning frame-
work, the term-weighting function used in this pa-
per is a linear combination of features extracted
from the input term and document. In particular,
the weight of term t
i
with respect to document D
p
is
s
i
p
= tw
w
(t
i
, D
p
) ?
?
j
w
j
?
j
(t
i
, D
p
), (3)
where ?
j
is the j-th feature function and w
j
is the
corresponding model parameter.
As for the vector operation function f
sim
, we
use the same cosine function (Eq. 1). Notice that
we choose these functional forms for their sim-
plicity and good empirical performance shown in
preliminary experiments. However, other smooth
functions can certainly be used.
The choice of loss function for training model
parameters depends on the true objective in the
target application. In this work, we consider two
different learning settings: learning directly the
similarity metric and learning the preference or-
dering, and compare several loss functions exper-
imentally.
3.1 Learning Similarity Metric
In this setting, we assume that the learning al-
gorithm is given a set of document pairs. Each
of them is associated with a label that indicates
whether these two documents are similar (e.g., a
binary label where 1 means similar and 0 oth-
erwise) or the degree of similarity (e.g., a real-
valued label ranges from 0 to 1), considered by the
human subjects. A training set of m examples can
be denoted as {(y
1
, (D
p
1
, D
q
1
)), (y
2
, (D
p
2
, D
q
2
)),
? ? ?, (y
m
, (D
p
m
, D
q
m
))}, where y
k
is the label
and (D
p
k
, D
q
k
) is the pair of documents to com-
pare. Following the vector construction described
in Eq. 3, let v
p
1
,v
q
1
, ? ? ? ,v
p
m
,v
q
m
be the corre-
sponding term vectors of these documents.
We consider two commonly used loss functions,
sum-of-squares error and log loss2:
L
sse
(w) =
1
2
m
?
k
(y
k
? f
sim
(v
p
k
,v
q
k
))
2 (4)
L
log
(w) =
m
?
k
?y
k
log(f
sim
(v
p
k
,v
q
k
))
?(1 ? y
k
) log(1 ? f
sim
(v
p
k
,v
q
k
)) (5)
Eq. 4 and Eq. 5 can further be regularized by
adding ?
2
||w||
2 in the loss function, which may
improve the performance empirically and also
constrain the range of the final term-weighting
scores. Learning the model parameters for min-
imizing these loss functions can be done us-
ing standard gradient-based optimization methods.
We choose the L-BFGS (Nocedal and Wright,
2006) method in our experiments for its guaran-
tee to find a local minimum and fast convergence.
The derivation of gradients is fairly straightfor-
ward, which we skip here.
Notice that other loss functions can also be used
in this framework. Interested readers can refer to,
say, (Bishop, 1995), for other loss functions and
their theoretical justifications.
3.2 Learning Preference Ordering
In many applications where the similarity measure
is applied, the goal is to obtain a ranked list of the
candidate elements. For example, in the task of
2Although in theory the cosine function may return a neg-
ative value and make the log-loss uncomputable, this can
be easily avoided in practice by selecting appropriate ini-
tial model parameters and by constraining the term-weighting
scores to be non-negative.
795
filtering irrelevant ads, a good similarity measure
is expected to rank appropriate ads higher than
the irrelevant ones. A desired trade-off of false-
positive (mistakenly filtered good ads) and false-
negative (unfiltered bad ads) can be achieved by
selecting a decision threshold. The exact value
of the similarity measure, in this case, is not cru-
cial. For these applications, it is more important if
the model parameters can better predict the pair-
wise preference. Learning preference ordering is
also motivated by the observation that preference
annotations are generally more reliable than cat-
egorical similarity labels (Carterette et al, 2008)
and has been advocated recently by researchers
(e.g., Burges et al (2005)).
In the setting of learning preference ordering,
we assume that each training example consists
of two pairs of documents, associated with a la-
bel indicating which pair of documents is consid-
ered more preferable. A training set of m exam-
ples can be formally denoted as {(y
1
, (x
a
1
, x
b
1
)),
(y
2
, (x
a
2
, x
b
2
)), ? ? ?, (y
m
, (x
a
m
, x
b
m
))}, where
x
a
k
= (D
p
a
k
, D
q
a
k
) and x
b
k
= (D
p
b
k
, D
q
b
k
) are
two pairs of documents and y
k
? {0, 1} indicates
the pairwise order preference, where 1 means x
a
k
should be ranked higher than x
b
k
and 0 otherwise.
We use a loss function that is very similar to
the one proposed by Dekel et al (2004) for label
ranking. Let ?
k
be the difference of the similarity
scores of these two document pairs. Namely,
?
k
= f
sim
(v
p
a
k
,v
q
a
k
) ? f
sim
(v
p
b
k
,v
q
b
k
)
The loss function L, which can be shown to upper
bound the pairwise accuracy (i.e., the 0-1 loss of
the pairwise predictions), is:
L(w) =
m
?
k=1
log(1+exp(?y
k
??
k
?(1?y
k
)?(??
k
)))
(6)
Similarly, Eq. 6 can be regularized by adding
?
2
||w||
2 in the loss function.
4 Experiments
We demonstrate how to apply our term-weighting
learning framework, TWEAK, to measuring sim-
ilarity for short text segments and to judging
the relevance of an ad landing page given an
query. In addition, we compare experimentally the
performance of using different training settings,
loss functions and features against the traditional
TFIDF term-weighting scheme.
4.1 Similarity for Short Text Segments
Judging the similarity between two short text seg-
ments is a crucial problem for many search and on-
line advertising applications. For instance, query
reformulation or query substitution needs to mea-
sure the similarity between two queries. A prod-
uct keyword recommendation system needs to de-
termine whether the given product name and the
suggested keyword is related.
Because the length of the text segment is typi-
cally short, ranging from a single word to a dozen
words, naively applying methods based on word
overlapping such as the Jaccard coefficient leads
to poor results (Sahami and Heilman, 2006; Yih
and Meek, 2007). To overcome this difficulty, Sa-
hami and Heilman (2006) proposes a Web-kernel
function, which first expands the short text seg-
ment by issuing it to a search engine as the query,
and then collectes the snippets of the top results to
construct a pseudo-document. TFIDF term vectors
of the pseudo-documents are used to represent the
original short text segments and the cosine score
of these two vectors is used as the similarity mea-
sure.
In this section, we apply TWEAK to this
problem by replacing the TFIDF term-weighting
scheme with the learned term-weighting function,
when constructing the vectors from the pseudo-
documents. Our target application is query sug-
gestion ? automatically presenting queries that are
related to the one issued by the user. In particu-
lar, we would like to use our similarity measure
as a filter to determine whether queries suggested
by various algorithms and heuristics are indeed
closely related to the target query.
4.1.1 Task & Data
Our query suggestion dataset has been previously
used in (Metzler et al, 2007; Yih and Meek, 2007)
and is collected in the following way. From the
search logs of a commercial search engine, a ran-
dom sample of 363 thousand queries from the top
1 million most frequent queries in late 2005 were
first taken as the query and suggestion candidates.
Among them, 122 queries were chosen randomly
as our target queries; each of them had up to 100
queries used as suggestions, generated by various
query suggestion mechanisms.
Given these pairs of query and suggestions, hu-
man annotators judged the level of similarity using
a 4-point scale ? Excellent, Good, Fair and Bad,
796
where Excellent and Good suggestions are consid-
ered clearly related to the query intent, while the
other two categories mean the suggestions are ei-
ther too general or totally unrelated. In the end,
4,852 query/suggestion pairs that had effective an-
notations were collected. The distribution of the
four labels is: Excellent - 5%, Good - 12%, Fair -
44% and Bad - 39%.
For the simplicity of both presentation and im-
plementation, query/suggestion pairs labeled as
Excellent or Good are treated as positive examples
and the rest as negative ones. Notice that TWEAK
is not restricted in using only binary labels. For
instance, the pairwise preference learning setting
only needs to know which pair of objects being
compared is more preferred. The model and algo-
rithm do not have to change regardless of whether
the label reflects the degree of similarity (e.g, the
original 4-scale labels) or binary categories. For
the metric learning setting, an ordinal regression
approach (e.g, (Herbrich et al, 2000)) can be ap-
plied for multi-category labels.
We used the same query expansion method as
described in (Sahami and Heilman, 2006). Each
query/suggestion was first issued to a commercial
search engine. The result page with up to 200
snippets (i.e., titles and summaries) was used as
the pseudo-document to create the term vector that
represents the original query/suggestion. As de-
scribed earlier in Eq. 3, the weight of each term
is a linear function of a set of predefined features,
which are described next.
4.1.2 Features
Because the pseudo-documents are constructed
using the search result snippets instead of regular
web documents, special formatting or link infor-
mation provided by HTML is not very meaning-
ful. Therefore, we focused on using features that
are available for plain-text documents, including:
? Bias: 1 for all examples.
? TF: We used log(tf + 1) as the term fre-
quency feature, where tf is the number of
times the term occurs in the original pseudo-
document.
? DF: We used log(df + 1) as the document
frequency feature, where df is the number of
documents in our collection that contain this
term.
? QF: The search engine query log reflects the
distribution of the words/phrases in which
people are interested (Goodman and Car-
valho, 2005; Yih et al, 2006). We took a log
file with the most frequent 7.5 million queries
and used log(qf + 1) as feature, where qf is
the query frequency.
? Cap: A capitalized word may indicate being
part of a proper noun or being more impor-
tant. When the term is capitalized in at least
one occurrence in the pseudo-document, the
value of this feature is 1; otherwise, it is 0.
? Loc & Len: The beginning of a regular doc-
ument often contains a summary with impor-
tant words. In the pseudo-documents cre-
ated using search snippets, words that occur
in the beginning come from the top results,
which are potentially more relevant to the
original query/suggestion. We created two
specific features using this location informa-
tion. Let loc be the word position of the target
term and len be the total number of words of
this pseudo-document. The logarithmic value
log(loc + 1) and the ratio loc/len were both
used as features. In order for the learning pro-
cedure to adjust the scaling, the logarithmic
value of the document length, log(len + 1),
was also used.
4.1.3 Results
We conducted the experiments using 10-fold
cross-validation. The whole query/suggestion
pairs were first split into 10 subsets of roughly
equal sizes. Pairs with the same target query were
put in the same subset. In each round, one subset
was used for testing. 95% of the remaining data
was used for training the model and 5% was used
as the development set. We trained six models
with different values of the regularization hyper-
parameter ? ? {0.003, 0.01, 0.03, 0.1, 0.3, 1} and
determined which model to use based on its per-
formance on the development set, although the re-
sult actually did not vary a lot as ? changed.
We compared three learning configurations
? metric learning with sum-of-squares error
(Metric
sse
) and log loss (Metric
log
) and the
pairwise preference learning (Preference). The
learned term-weighting functions were used to
compare with the Web-kernel similarity function,
which implemented the TFIDF term-weighting
scheme using Eq. 2.
797
Table 1: The AUC scores, mean averaged preci-
sion and precision at 3 of similarity measures us-
ing different term-weighting functions. The num-
bers with the ? sign are statistically significantly
better compared to the Web-kernel method.
Method AUC MAP Prec@3
Web-kernel 0.732 0.540 0.556
Metric
sse
0.775? 0.590 0.553
Metric
log
0.781? 0.585 0.545
Preference 0.782? 0.597? 0.570
We evaluated these models using three different
evaluation metrics: the AUC score, precision at
k and MAP (mean averaged precision). The area
under the ROC curve (AUC) is typically used to
judge the overall quality of a ranking function. It
has been shown equivalent to the averaged accu-
racy of the pairwise preference predictions of all
possible element pairs in the sequence, and can be
calculated by the the following Wilcoxon-Mann-
Whitney statistic (Cortes and Mohri, 2004):
A(f ;x,y) =
?
i,j:y
i
>y
j
I
f(x
i
)>f(x
j
)
+
1
2
I
f(x
i
)=f(x
j
)
,
where f is the similarity measure, x is the se-
quence of compared elements and y is the labels.
Another metric that is commonly used in a rank-
ing scenario is precision at k, which computes
the accuracy of the top-ranked k elements and ig-
nores the rest. We used k = 3 in our task, which
means that for each target query, we selected three
suggestions with the highest similarity scores and
computed the averaged accuracy.
One issue of precision at k is that it does not
provide an overall quality measure of the ranking
function. Therefore, we also present MAP (mean
averaged precision), which is a single number that
summarizes the performance of the ranking func-
tion by considering both precision and recall, and
has been shown reliable in evaluating various in-
formation retrieval tasks (Manning et al, 2008).
Suppose there are m relevant elements in a se-
quence, where r
1
, r
2
, ? ? ? , r
m
are their locations.
The averaged precision is then:
AP =
1
m
m
?
j=1
Prec(r
j
),
where Prec(r
j
) is the precision at r
j
. We com-
puted the averaged precision values of the 10 test
sets in our cross-validation setting and report their
mean value.
As shown in Table 1, all three learned term-
weighting functions lead to better similarity mea-
sures compared to the TFIDF scheme in terms of
the AUC and MAP scores, where the preference
order learning setting performs the best. However,
for the precision at 3 metric, only the preference
learning setting has a higher score than the TFIDF
scheme, but the difference is not statistically sig-
nificant3. This is somewhat understandable since
the design of our loss function focuses on the over-
all quality instead of only the performance of the
top ranked elements.
4.2 Query/Page Similarity
Measuring whether a page is relevant to a given
query is the main problem in information retrieval
and has been studied extensively. Instead of re-
trieving web pages that are relevant to the query
according to the similarity measure, our goal is
to implement a paid-search ad filter for commer-
cial search engines. In this scenario, textual ads
with bid keywords that match the query can en-
ter the auction and have a chance to be shown on
the search result page. However, as the advertisers
may bid on keywords that are not related to their
advertisements, it is important for the system to fil-
ter irrelevant ads to ensure that users only receive
useful information. For this purpose, we measure
the similarity between the query and the ad land-
ing page (i.e., the page pointed by the ad) and re-
move the ad when the score of its landing page is
below a pre-selected threshold4.
Given a pair of query and ad landing page,
while the query term vector is constructed using
the same query expansion technique described in
Sec. 4.1, the page term vector can be created di-
rectly from the web page since it is a regular doc-
ument that contains enough content. As usual,
our goal is to produce a better similarity measure
by learning the term-weighting functions for these
two types of vectors jointly.
3We conducted a paired-t test on the 10 individual
scores from the cross-validation results of each learned term-
weighting function versus the Web-kernel method. The re-
sults are considered statistically significant when the p-value
is lower than 0.05.
4One may argue that the filter should measure the simi-
larity between the query and ad-text. However, an ad will
not provide useful information to the user if the final destina-
tion page is not relevant to the query, even if its ad-text looks
appealing.
798
4.2.1 Data
We first collected a random sample of queries and
paid-search ads shown on a commercial search en-
gine during 2008, as well as the ad landing pages.
Judged by several human annotators, each page
was labeled as relevant or not compared to the is-
sued query. After removing some pairs where the
query intent was not clear or the landing page was
no longer available, we managed to collect 13,341
query/page pairs with reliable labels. Among
them, 8,309 were considered relevant and 5,032
were labeled irrelevant.
4.2.2 Features
In this experiment, we tested the effect of using
different features and experimented with three fea-
ture sets: TF&DF, Plain-text and HTML. TF&DF
contains only log(tf +1), log(df +1) and the bias
feature. The goal of using this feature set is to
test whether we can learn a better term-weighting
function given the same amount of information as
the TFIDF scheme has. The second feature set,
Plain-text, consists of all the features described in
Sec. 4.1.2. As mentioned earlier, this set of fea-
tures can be used for regular text documents that
do not have special formatting information. Fi-
nally, feature set HTML is composed of all the
features used in Plain-text plus features extracted
from some special properties of web documents,
including:
? Hypertext: The anchor text in an HTML
document usually provides important infor-
mation. If there is at least one occurrence of
the term that appears in some anchor text, the
value of this feature is 1; otherwise, it is 0.
? URL: A web document has a uniquely useful
property ? the name of the document, which
is its URL. If the term is a substring of the
URL, then the value of this feature is 1; oth-
erwise, it is 0.
? Title: The value of this feature is 1 when the
term is part of the title; otherwise, it is 0.
? Meta: Besides Title, several meta tags used
in the HTML header explicitly show the im-
portant words selected by the page author.
Specifically, whether the term is part of a
meta-keyword is used as a binary feature.
Whether the term is in the meta-description
segment is also used.
Table 2: The AUC scores, true-positive rates at
false-positive rates 0.1 and 0.2 of the ad filter
based on different term-weighting functions. The
difference between any pair of numbers of the
same evaluation metric is statistically significant.
Method AUC TPR
fnr=0.1
TPR
fnr=0.2
TFIDF 0.794 0.527 0.658
TF&DF 0.806 0.430 0.639
Plain-text 0.832 0.503 0.704
HTML 0.855 0.568 0.750
Because the term vector that represents the
query is created from the pseudo-document (i.e., a
collection of search snippets), the values of these
HTML-specific features are all 0 for the query
term vector. This set of features are only useful for
deciding the weights of the terms in a page term
vector.
4.2.3 Results
We split our data into 10 subsets and conducted
the experiments using the same 10-fold cross-
validation setting described in Sec. 4.1.3, includ-
ing how we used the development set to select the
regularization hyper-parameter ?. The pairs that
have the same target query were again put in the
same subsets. We used only the preference or-
dering learning setting for its good performance
shown in the previous set of experiments. Models
compared here were learned from the three dif-
ferent sets of features, as well as the same fixed
TFIDF term-weighting formula (i.e., Eq. 2) used
in Sec. 4.1. Table 2 reports the averaged results
of the 10 testing sets in AUC, as well as the true-
positive rates at two low false-positive rate points
(FPR=0.1 and FPR=0.2). The difference between
any pair of numbers of the same evaluation metric
is statistically significant5.
As we can see from the table, having more fea-
tures does lead to a better term-weighting func-
tion. With all features (i.e., HTML), the model
achieves the highest AUC score among all con-
figurations. Features available in plain-text doc-
uments (i.e., Plain-text) other than term frequency
and document frequency can still improve the per-
formance significantly. When only the TF and DF
features are available, the learned term-weighting
function still outperforms the TFIDF scheme, al-
5We conduct paired-t tests as described in Sec. 4.1.3. All
the p-values after Bonferroni correction are less than 0.01.
799
Figure 2: ROC Curves of the ad filters using dif-
ferent term-weighting functions
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6
Tr
u
e
Po
sit
iv
e
R
at
e
False Positive Rate
ROC Curves
TFIDF
TF&DF
Plain-text
HTML
though the improvement gain is much smaller
compared to the other two settings.
Notice that the behaviors of these models at dif-
ferent false-positive regions varies from the tra-
ditional TFIDF scheme. At a low false-positive
point (e.g., FPR=10%), only the model that uses
all features performs better than TFIDF. This phe-
nomenon can be clearly observed from the ROC
curves plotted in Fig. 2, where the models were
trained using half of the data and applied to the
other half to generate the similarity scores. If only
the performance at a very low false-positive rate
matters, TWEAK can still be easily adjusted by
modifying the loss function using techniques such
as training with utility (Domingos, 1999; Morik et
al., 1999).
5 Related Work
Our term-weighting learning framework can be
analogous to the ?Siamese? architecture for learn-
ing jointly two neural networks that share the same
set of model weights (Bromley et al, 1993). For
instance, a term vector can be viewed as a very
large single-layer neural network, where each term
in the vocabulary is a node that takes as input the
features and outputs the learned term-weighting
score. Previous applications of this learning ma-
chine are typically problems in image processing
or computer vision. For example, Chopra et al
(2005) designed an algorithm to learn a similar-
ity metric for face verification, which is based on
the difference between two vectors. In our earlier
experiments (not reported in this paper) of using
vector difference instead of cosine, we did not ob-
serve positive outcomes. We hypothesize that be-
cause the length of the term vector in our problem
can be extremely large (i.e., the size of the vocab-
ulary), a similarity measure based on vector differ-
ence can easily be affected by terms that do not oc-
cur in both documents, even when the co-occurred
terms have very large weights.
Learning similarity measures for text has also
been proposed by several researchers. For in-
stance, Bilenko and Mooney (2003) applied SVMs
to directly learn the weights of co-occurred words
in two text records, which are then used for
measuring similarity for duplicate detection. Al-
though this approach worked moderately well in
the database domain, it may not be suitable to han-
dle general text similarity problems for two rea-
sons. First, the vocabulary size is typically large,
which results in a very high dimensional feature
space for the learning problem. It is very likely
that some rarely used and yet important terms oc-
cur in the testing documents but not in the training
data. The weights of those terms may not be reli-
able or even be learned. Second, this learning ap-
proach can only learn the importance of the terms
from the labels of whether two texts are considered
similar, how to incorporate the basic information
of these terms such as the position or query log
frequency is not clear.
An alternative learning approach is to combine
multiple similarity measures with learned coeffi-
cients (Yih and Meek, 2007), or to apply the tech-
nique of kernel alignment (Cristianini et al, 2002)
to combining a set of kernel functions for tun-
ing a more appropriate kernel based on labeled
data. This type of approaches can be viewed
as constructing an ensemble of different existing
similarity measures without modifying the term
weighting function, and may not generate math-
ematically equivalent similarity functions as de-
rived by TWEAK. Although learning in this ap-
proach is usually very fast due to the model form
and the small number of parameters to learn, its
improvement is limited by the quality of the in-
dividual similarity measures. In spite of the fun-
damental difference between our approach and
this combination method, it is worth noticing that
these two approaches are in fact complementary
to each other. Having a newly learned term-
weighting function effectively provides a new sim-
ilarity measure and therefore can be combined
with other measures.
800
6 Conclusions
In this paper, we presented a novel term-weighting
learning framework, TWEAK, for improving sim-
ilarity measures based on term vectors. Given the
labels of text pairs for training, our method learns
the model parameters to calculate the score of each
term, optimizing the desired loss function that is
suitable for the target application. As we demon-
strated in the experiments, TWEAK with differ-
ent features and training settings significantly out-
performs the traditional TFIDF term-weighting
scheme.
TWEAK also enjoys several advantages com-
pared to existing methods. From an engineer-
ing perspective, adopting the new term-weighting
scores produced by our model is straightforward.
If a similarity measure has been implemented,
the algorithm need not be changed ? only the
term vectors need to be updated. From the learn-
ing perspective, additional information regard-
ing each term with respect to the document can
now be incorporated easily via feature functions.
Weights (i.e., model parameters) of these features
are learned in a principled way instead of being
adjusted manually. Finally, TWEAK is potentially
complementary to other methods for improving
the similarity measure, such as model combination
of various types of similarity measures (Yih and
Meek, 2007) or different term vector construction
methods such as Latent Semantic Analysis (Deer-
wester et al, 1990).
In the future, we plan to explore more vector op-
erations other than the inner-product (i.e., cosine)
as well as different functional forms of the term-
weighting function (e.g. log-linear instead of lin-
ear). Designing new loss functions to better fit the
true objectives in various target applications and
studying the quality of a similarity measure based
on both term-weighting learning and model com-
bination are also on our agenda. In terms of appli-
cations, we would like to apply TWEAK in other
problems such as paraphrase recognition and near-
duplicate detection.
Acknowledgments
The author thanks the anonymous reviewers for
their valuable comments and is grateful to Asela
Gunawardana, Chris Meek, John Platt and Misha
Bilenko for many useful discussions.
References
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proceedings of KDD-2003,
pages 39?48.
Christopher M. Bishop. 1995. Neural Networks for
Pattern Recognition. Oxford University Press.
Jane Bromley, James W. Bentz, Le?on Bottou, Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
Sa?ckinger, and Roopak Shah. 1993. Signature ver-
ification using a ?Siamese? time delay neural net-
work. International Journal Pattern Recognition
and Artificial Intelligence, 7(4):669?688.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd International Conference
on Machine learning (ICML-05), pages 89?96.
Ben Carterette, Paul N. Bennett, David Maxwell
Chickering, and Susan Dumais. 2008. Here or
there: Preference judgments for relevance. In Pro-
ceedings of the 30th European Conference on Infor-
mation Retrieval (ECIR 2008).
Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with
application to face verification. In Proceedings of
CVPR-2005, pages 539?546.
Corinna Cortes and Mehryar Mohri. 2004. AUC opti-
mization vs. error rate minimization. In Advances
in Neural Information Processing Systems (NIPS
2003).
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff,
and Jaz Kandola. 2002. On kernel-target algnment.
In Advances in Neural Information Processing Sys-
tems 14, pages 367?373. MIT Press.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal
of the American Society for Information Science,
41(6):391?407.
Ofer Dekel, Christopher D. Manning, and Yoram
Singer. 2004. Log-linear models for label ranking.
In Advances in Neural Information Processing Sys-
tems (NIPS 2003).
Pedro Domingos. 1999. MetaCost: A general method
for making classifiers cost-sensitive. In Proceedings
of KDD-1999, pages 155?164.
Joshua Goodman and Vitor R. Carvalho. 2005. Im-
plicit queries for email. In Proceedings of the 2nd
conference on Email and Anti-Spam (CEAS-2005).
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
2000. Large margin rank boundaries for ordinal
regression. Advances in Large Margin Classifiers,
pages 115?132.
801
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th World Wide Web Confer-
ence.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLING-ACL 98.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Pres.
Donald Metzler, Susan Dumais, and Christopher Meek.
2007. Similarity measures for short segments of
text. In Proceedings of the 29th European Confer-
ence on Information Retrieval (ECIR 2007).
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of AAAI-2006.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning
with a knowledge-based approach ? a case study in
intensive care monitoring. In Proceedings of the Six-
teenth International Conference on Machine Learn-
ing (ICML-1999), pages 268?277.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
Mehran Sahami and Timothy D. Heilman. 2006. A
web-based kernel function for measuring the simi-
larity of short text snippets. In Proceedings of the
15th World Wide Web Conference.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? Improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of KDD-2008, pages 614?622.
Wen-tau Yih and Christopher Meek. 2007. Improving
similarity measures for short segments of text. In
Proceedings of AAAI-2007, pages 1489?1494.
Wen-tau Yih, Joshua Goodman, and Vitor Carvalho.
2006. Finding advertising keywords on web pages.
In Proceedings of the 15th World Wide Web Confer-
ence.
802
The Importance of Syntactic Parsing and
Inference in Semantic Role Labeling
Vasin Punyakanok??
BBN Technologies
Dan Roth??
University of Illinois at
Urbana-Champaign
Wen-tau Yih??
Microsoft Research
We present a general framework for semantic role labeling. The framework combines a machine-
learning technique with an integer linear programming?based inference procedure, which in-
corporates linguistic and structural constraints into a global decision process. Within this
framework, we study the role of syntactic parsing information in semantic role labeling. We
show that full syntactic parsing information is, by far, most relevant in identifying the argument,
especially, in the very first stage?the pruning stage. Surprisingly, the quality of the pruning
stage cannot be solely determined based on its recall and precision. Instead, it depends on the
characteristics of the output candidates that determine the difficulty of the downstream prob-
lems. Motivated by this observation, we propose an effective and simple approach of combining
different semantic role labeling systems through joint inference, which significantly improves its
performance.
Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling,
and achieves the highest F1 score among 19 participants.
1. Introduction
Semantic parsing of sentences is believed to be an important task on the road to natural
language understanding, and has immediate applications in tasks such as informa-
tion extraction and question answering. Semantic Role Labeling (SRL) is a shallow
semantic parsing task, in which for each predicate in a sentence, the goal is to identify
all constituents that fill a semantic role, and to determine their roles (Agent, Patient,
Instrument, etc.) and their adjuncts (Locative, Temporal, Manner, etc.).
? 10 Moulton St., Cambridge, MA 02138, USA. E-mail: vpunyaka@bbn.com.
?? Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N. Goodwin Ave.,
Urbana, IL 61801, USA. E-mail: danr@uiuc.edu.
? One Microsoft Way, Redmond, WA 98052, USA. E-mail: scottyih@microsoft.com.
? Most of the work was done when these authors were at the University of Illinois at Urbana-Champaign.
Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
The PropBank project (Kingsbury and Palmer 2002; Palmer, Gildea, and Kingsbury
2005), which provides a large human-annotated corpus of verb predicates and their ar-
guments, has enabled researchers to apply machine learning techniques to develop SRL
systems (Gildea and Palmer 2002; Chen and Rambow 2003; Gildea and Hockenmaier
2003; Pradhan et al 2003; Surdeanu et al 2003; Pradhan et al 2004; Xue and Palmer 2004;
Koomen et al 2005). However, most systems rely heavily on full syntactic parse trees.
Therefore, the overall performance of the system is largely determined by the quality
of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak
2001) is still far from perfect.
Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they
do not provide as much information as a full syntactic parser, have been shown to
be more robust in their specific tasks (Li and Roth 2001). This raises the very natural
and interesting question of quantifying the importance of full parsing information to
semantic parsing and whether it is possible to use only shallow syntactic information to
build an outstanding SRL system.
Although PropBank is built by adding semantic annotations to the constituents in
the Penn Treebank syntactic parse trees, it is not clear how important syntactic parsing
is for an SRL system. To the best of our knowledge, this problem was first addressed
by Gildea and Palmer (2002). In their attempt to use limited syntactic information, the
parser they used was very shallow?clauses were not available and only chunks were
used. Moreover, the pruning stage there was very strict?only chunks were considered
as argument candidates. This results in over 60% of the actual arguments being ignored.
Consequently, the overall recall in their approach was very low.
The use of only shallow parsing information in an SRL system has largely been
ignored until the recent CoNLL-2004 shared task competition (Carreras and Ma`rquez
2004). In that competition, participants were restricted to using only shallow parsing
information, which included part-of-speech tags, chunks, and clauses (the definitions of
chunks and clauses can be found in Tjong Kim Sang and Buchholz [2000] and Carreras
et al [2002], respectively). As a result, the performance of the best shallow parsing?
based system (Hacioglu et al 2004) in the competition is about 10 points in F1 below the
best system that uses full parsing information (Koomen et al 2005). However, this is not
the outcome of a true and fair quantitative comparison. The CoNLL-2004 shared task
used only a subset of the data for training, which potentially makes the problem harder.
Furthermore, an SRL system is usually complicated and consists of several stages. It
was still unclear howmuch syntactic information helps and precisely where it helps the
most.
The goal of this paper is threefold. First, we describe an architecture for an SRL
system that incorporates a level of global inference on top of the relatively common
processing steps. This inference step allows us to incorporate structural and linguistic
constraints over the possible outcomes of the argument classifier in an easy way. The
inference procedure is formalized via an Integer Linear Programming framework and
is shown to yield state-of-the-art results on this task. Second, we provide a fair com-
parison between SRL systems that use full parse trees and systems that only use shal-
low syntactic information. As with our full syntactic parse?based SRL system (Koomen
et al 2005), our shallow parsing?based SRL system is based on the system that achieves
very competitive results and was one of the top systems in the CoNLL-2004 shared
task competition (Carreras and Ma`rquez 2004). This comparison brings forward a care-
ful analysis of the significance of full parsing information in the SRL task, and provides
an understanding of the stages in the process in which this information makes the most
difference. Finally, to relieve the dependency of the SRL system on the quality of
258
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
automatic parsers, we suggest a way to improve semantic role labeling significantly by
developing a global inference algorithm, which is used to combine several SRL systems
based on different state-of-the-art full parsers. The combination process is done through
a joint inference stage, which takes the output of each individual system as input and
generates the best predictions, subject to various structural and linguistic constraints.
The underlying system architecture can largely affect the outcome of our study.
Therefore, to make the conclusions of our experimental study as applicable as possible
to general SRL systems, the architecture of our SRL system follows the most widely
used two-step design. In the first step, the system is trained to identify argument candi-
dates for a given verb predicate. In the second step, the system classifies the argument
candidates into their types. In addition, it is also a simple procedure to prune obvious
non-candidates before the first step, and to use post-processing inference to fix incon-
sistent predictions after the second step. These two additional steps are also employed
by our system.
Our study of shallow and full syntactic information?based SRL systems was done
by comparing their impact at each stage of the process. Specifically, our goal is to investi-
gate at what stage full parsing information is most helpful relative to a shallow parsing?
based system. Therefore, our experiments were designed so that the compared systems
are as similar as possible, and the addition of the full parse tree?based features is the
only difference. The most interesting result of this comparison is that although each
step of the shallow parsing information?based system exhibits very good performance,
the overall performance is significantly inferior to the system that uses full parsing
information. Our explanation is that chaining multiple processing stages to produce
the final SRL analysis is crucial to understanding this analysis. Specifically, the quality
of the information passed from one stage to the other is a decisive issue, and it is
not necessarily judged simply by considering the F-measure. We conclude that, for
the system architecture used in our study, the significance of full parsing information
comes into play mostly at the pruning stage, where the candidates to be processed later
are determined. In addition, we produce a state-of-the-art SRL system by combining
different SRL systems based on two automatic full parsers (Collins 1999; Charniak 2001),
which achieves the best result in the CoNLL-2005 shared task (Carreras and Ma`rquez
2005).
The rest of this paper is organized as follows. Section 2 introduces the task of
semantic role labeling in more detail. Section 3 describes the four-stage architecture of
our SRL system, which includes pruning, argument identification, argument classifi-
cation, and inference. The features used for building the classifiers and the learning
algorithm applied are also explained there. Section 4 explains why and where full
parsing information contributes to SRL by conducting a series of carefully designed
experiments. Inspired by the result, we examine the effect of inference in a single system
and propose an approach that combines different SRL systems based on joint inference
in Section 5. Section 6 presents the empirical evaluation of our system in the CoNLL-
2005 shared task competition. After that, we discuss the related work in Section 7 and
conclude this paper in Section 8.
2. The Semantic Role Labeling (SRL) Task
The goal of the semantic role labeling task is to discover the predicate?argument struc-
ture of each predicate in a given input sentence. In this work, we focus only on the verb
predicate. For example, given a sentence I left my pearls to my daughter-in-law in my will,
259
Computational Linguistics Volume 34, Number 2
the goal is to identify the different arguments of the verb predicate left and produce the
output:
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-in-law] [AM-LOC in my will].
Here A0 represents the leaver, A1 represents the thing left, A2 represents the beneficiary,
AM-LOC is an adjunct indicating the location of the action, and V determines the
boundaries of the predicate, which is important when a predicate contains many words,
for example, a phrasal verb. In addition, each argument can be mapped to a constituent
in its corresponding full syntactic parse tree.
Following the definition of the PropBank and CoNLL-2004 and 2005 shared tasks,
there are six different types of arguments labeled as A0?A5 and AA. These labels have
different semantics for each verb and each of its senses as specified in the PropBank
Frame files. In addition, there are also 13 types of adjuncts labeled as AM-adj where adj
specifies the adjunct type. For simplicity in our presentation, we will also refer to these
adjuncts as arguments. In some cases, an argument may span over different parts of
a sentence; the label C-arg is then used to specify the continuity of the arguments, as
shown in this example:
[A1 The pearls] , [A0 I] [V said] , [C-A1 were left to my daughter-in-law].
In some other cases, an argumentmight be a relative pronoun that in fact refers to the ac-
tual agent outside the clause. In this case, the actual agent is labeled as the appropriate
argument type, arg, while the relative pronoun is instead labeled as R-arg. For example,
[A1 The pearls] [R-A1 which] [A0 I] [V left] [A2 to my daughter-in-law] are fake.
Because each verb may have different senses producing different semantic roles
for the same labels, the task of discovering the complete set of semantic roles should
involve not only identifying these labels, but also the underlying sense for a given
verb. However, as in all current SRL work, this article focuses only on identifying the
boundaries and the labels of the arguments, and ignores the verb sense disambiguation
problem.
The distribution of these argument labels is fairly unbalanced. In the official release
of PropBank I, core arguments (A0?A5 and AA) occupy 71.26% of the arguments, where
the largest parts are A0 (25.39%) and A1 (35.19%). The rest mostly consists of adjunct
arguments (24.90%). The continued (C-arg) and referential (R-arg) arguments are rela-
tively few, occupying 1.22% and 2.63%, respectively. For more information on PropBank
and the semantic role labeling task, readers can refer to Kingsbury and Palmer (2002)
and Carreras and Ma`rquez (2004, 2005).
Note that the semantic arguments of the same verb do not overlap. We define over-
lapping arguments to be those that share some of their parts. An argument is considered
embedded in another argument if the second argument completely covers the first one.
Arguments are exclusively overlapping if they are overlapping but are not embedded.
3. SRL System Architecture
Adhering to the most common architecture for SRL systems, our SRL system consists of
four stages: pruning, argument identification, argument classification, and inference.
In particular, the goal of pruning and argument identification is to identify argument
candidates for a given verb predicate. In the first three stages, however, decisions
are independently made for each argument, and information across arguments is not
260
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
incorporated. The final inference stage allows us to use this type of information along
with linguistic and structural constraints in order to make consistent global predictions.
This system architecture remains unchanged when used for studying the impor-
tance of syntactic parsing in SRL, although different information and features are used.
Throughout this article, when full parsing information is available, we assume that
the system is presented with the full phrase-structure parse tree as defined in the Penn
Treebank (Marcus, Marcinkiewicz, and Santorini 1993) but without trace and functional
tags. On the other hand, when only shallow parsing information is available, the full
parse tree is reduced to only the chunks and the clause constituents.
A chunk is a phrase containing syntactically related words. Roughly speaking,
chunks are obtained by projecting the full parse tree onto a flat tree; hence, they are
closely related to the base phrases. Chunks were not directly defined as part of the
standard annotation of the treebank, but, rather, their definition was introduced in the
CoNLL-2000 shared task on text chunking (Tjong Kim Sang and Buchholz 2000), which
aimed to discover such phrases in order to facilitate full parsing. A clause, on the other
hand, is the clausal constituent as defined by the treebank standard. An example of
chunks and clauses is shown in Figure 1.
3.1 Pruning
When the full parse tree of a sentence is available, only the constituents in the parse
tree are considered as argument candidates. Our system exploits the heuristic rules
introduced by Xue and Palmer (2004) to filter out simple constituents that are very
unlikely to be arguments. This pruning method is a recursive process starting from the
target verb. It first returns the siblings of the verb as candidates; then it moves to the
parent of the verb, and collects the siblings again. The process goes on until it reaches
the root. In addition, if a constituent is a PP (prepositional phrase), its children are also
collected. For example, in Figure 1, if the predicate (target verb) is assume, the pruning
heuristic will output: [PP by John Smith who has been elected deputy chairman], [NP John
Smith who has been elected deputy chairman], [VB be], [MD will], and [NP His duties].
3.2 Argument Identification
The argument identification stage utilizes binary classification to identify whether a
candidate is an argument or not. When full parsing is available, we train and apply
the binary classifiers on the constituents supplied by the pruning stage. When only
shallow parsing is available, the system does not have a pruning stage, and also does
not have constituents to begin with. Therefore, conceptually, the system has to consider
all possible subsequences (i.e., consecutive words) in a sentence as potential argument
candidates. We avoid this by using a learning scheme that utilizes two classifiers, one to
predict the beginnings of possible arguments, and the other the ends. The predictions
are combined to form argument candidates. However, we can employ a simple heuristic
to filter out some candidates that are obviously not arguments. The final predication
includes those that do not violate the following constraints.
1. Arguments cannot overlap with the predicate.
2. If a predicate is outside a clause, its arguments cannot be embedded in
that clause.
3. Arguments cannot exclusively overlap with the clauses.
261
Computational Linguistics Volume 34, Number 2
Figure 1
An example of a parse tree and its predicate?argument structure.
The first constraint comes from the definition of this task that the predicate simply
cannot take itself or any constituents that contain itself as arguments. The other two
constraints are due to the fact that a clause can be treated as a unit that has its own
verb?argument structure. If a verb predicate is outside a clause, then its argument can
only be the whole clause, but may not be embedded in or exclusively overlap with the
clause.
For the argument identification classifier, the features used in full parsing and
shallow parsing settings are all binary features, which are described subsequently.
3.2.1 Features Used When Full Parsing is Available. Most of the features used in our
system are common features for the SRL task. The creation of PropBank was inspired
by the works of Levin (1993) and Levin and Hovav (1996), which discuss the relation
between syntactic and semantic information. Following this philosophy, the features
aim to indicate the properties of the predicate, the constituent which is an argument
candidate, and the relationship between them through the available syntactic infor-
mation. We explain these features herein. For further discussion of these features, we
262
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
refer the readers to the article by Gildea and Jurafsky (2002), which introduced these
features.
 Predicate and POS tag of predicate: indicate the lemma of the predicate
verb and its POS tag.
 Voice: indicates passive/active voice of the predicate.
 Phrase type: provides the phrase type of the constituent, which is the tag
of the corresponding constituent in the parse tree.
 Head word and POS tag of the head word: provides the head word of the
constituent and its POS tag. We use the rules introduced by Collins (1999)
to extract this feature.
 Position: describes if the constituent is before or after the predicate,
relative to the position in the sentence.
 Path: records the tags of parse tree nodes in the traversal path from the
constituent to the predicate. For example, in Figure 1, if the predicate is
assume and the constituent is [S who has been elected deputy chairman], the
path is S?NP?PP?VP?VBN, where ? and ? indicate the traversal direction
in the path.
 Subcategorization: describes the phrase structure around the predicate?s
parent. It records the immediate structure in the parse tree that expands to
its parent. As an example, if the predicate is elect in Figure 1, its
subcategorization is VP?(VBN)-NP while the subcategorization of the
predicate assume is VP?(VBN)-PP. Parentheses indicate the position of the
predicate.
Generally speaking, we consider only the arguments that correspond to some con-
stituents in parse trees. However, in some cases, we need to consider an argument that
does not exactly correspond to a constituent, for example, in our experiment in Sec-
tion 4.2 where the gold-standard boundaries are used with the parse trees generated by
an automatic parse. In such cases, if the information on the constituent, such as phrase
type, needs to be extracted, the deepest constituent that covers the whole argument will
be used. For example, in Figure 1, the phrase type for by John Smith is PP, and its path
feature to the predicate assume is PP?VP?VBN.
We also use the following additional features. These features have been shown
to be useful for the systems by exploiting other information in the absence of the
full parse tree information (Punyakanok et al 2004), and, hence, can be helpful in
conjunction with the features extracted from a full parse tree. They also aim to encode
the properties of the predicate, the constituent to be classified, and their relationship in
the sentence.
 Context words and POS tags of the context words: the feature
includes the two words before and after the constituent, and their
POS tags.
 Verb class: the feature is the VerbNet (Kipper, Palmer, and Rambow 2002)
class of the predicate as described in PropBank Frames. Note that a
263
Computational Linguistics Volume 34, Number 2
verb may inhabit many classes and we collect all of these classes as
features, regardless of the context-specific sense which we do not attempt
to resolve.
 Lengths: of the constituent, in the numbers of words and chunks
separately.
 Chunk: tells if the constituent ?is,? ?embeds,? ?exclusively overlaps,? or
?is embedded in? a chunk with its type. For instance, in Figure 1, if the
constituents are [NP His duties], [PP by John Smith], and [VBN elected], then
their chunk features are ?is-NP,? ?embed-PP & embed-NP,? and
?embedded-in-VP,? respectively.
 Chunk pattern: encodes the sequence of chunks from the constituent to
the predicate. For example, in Figure 1 the chunk sequence from [NP His
duties] to the predicate elect is VP-PP-NP-NP-VP.
 Chunk pattern length: the feature counts the number of chunks in the
chunk pattern feature.
 Clause relative position: encodes the position of the constituent relative
to the predicate in the pseudo-parse tree constructed only from clause
constituents, chunks, and part-of-speech tags. In addition, we label the
clause with the type of chunk that immediately precedes the clause.
This is a simple rule to distinguish the type of clause based on
the intuition that a subordinate clause often modifies the part of the
sentence immediately before it. Figure 2 shows the pseudo-parse
tree of the parse tree in Figure 1. By disregarding the chunks, there
are four configurations??target constituent and predicate are
siblings,? ?target constituent?s parent is an ancestor of predicate,?
?predicate?s parent is an ancestor of target word,? or ?otherwise.?
This feature can be viewed as a generalization of the Path feature
described earlier.
 Clause coverage: describes how much of the local clause from the
predicate is covered by the target argument.
 NEG: the feature is active if the target verb chunk has not or n?t.
 MOD: the feature is active when there is a modal verb in the verb chunk.
The rules of the NEG and MOD features are used in a baseline SRL system
developed by Erik Tjong Kim Sang (Carreras and Ma`rquez 2004).
Figure 2
The pseudo-parse tree generated from the parse tree in Figure 1.
264
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
In addition, we also use the conjunctions of features which conjoin any two features
into a new feature. For example, the conjunction of the predicate and path features
for the predicate assume and the constituent [S who has been elected deputy chairman] in
Figure 1 is (S?NP?PP?VP?VBN, assume).
3.2.2 Features Used When Only Shallow Parsing is Available. Most features used here are
similar to those used by the systemwith full parsing information. However, for features
that need full parse trees in their extraction procedures, we either try to mimic them
with some heuristic rules or discard them. The details of these features are as follows.
 Phrase type: uses a simple heuristic to identify the type of the argument
candidate as VP, PP, or NP.
 Head word and POS tag of the head word: are the rightmost word for
NP, and the leftmost word for VP and PP.
 Shallow-Path: records the traversal path in the pseudo-parse tree.
This aims to approximate the Path features extracted from the full
parse tree.
 Shallow-Subcategorization: describes the chunk and clause structure
around the predicate?s parent in the pseudo-parse tree. This aims to
approximate the Subcategorization feature extracted from the full parse
tree.
3.3 Argument Classification
This stage assigns labels to the argument candidates identified in the previous stage.
A multi-class classifier is trained to predict the types of the argument candidates. In
addition, to reduce the excessive candidates mistakenly output by the previous stage,
the classifier can also label an argument as ?null? (meaning ?not an argument?) to dis-
card it.
The features used here are the same as those used in the argument identification
stage. However, when full parsing is available, an additional feature introduced by Xue
and Palmer (2004) is used.
 Syntactic frame: describes the sequential pattern of the noun phrases and
the predicate in the sentence which aims to complement the Path and
Subcategorization features.
The learning algorithm used for training the argument classifier and argument iden-
tifier is a variation of the Winnow update rule incorporated in SNoW (Roth 1998;
Carlson et al 1999), a multi-class classifier that is tailored for large scale learning tasks.
SNoW learns a sparse network of linear functions, in which the targets (argument
border predictions or argument type predictions, in this case) are represented as linear
functions over a common feature space; multi-class decisions are done via a winner-
take-all mechanism. It improves the basic Winnow multiplicative update rule with a
regularization term, which has the effect of separating the data with a large margin
separator (Dagan, Karov, and Roth 1997; Grove and Roth 2001; Zhang, Damerau, and
Johnson 2002) and voted (averaged) weight vector (Freund and Schapire 1999; Golding
and Roth 1999).
265
Computational Linguistics Volume 34, Number 2
The softmax function (Bishop 1995) is used to convert raw activation to conditional
probabilities. If there are n classes and the raw activation of class i is acti, the posterior
estimation for class i is
Prob(i) = e
acti
?
1?j?n e
actj
Note that in training this classifier, unless specified otherwise, the argument can-
didates used to generate the training examples are obtained from the output of the
argument identifier, not directly from the gold-standard corpus. In this case, we au-
tomatically obtain the necessary examples to learn for class ?null.?
3.4 Inference
In the previous stages, decisions were always made for each argument independently,
ignoring the global information across arguments in the final output. The purpose
of the inference stage is to incorporate such information, including both linguistic
and structural knowledge, such as ?arguments do not overlap? or ?each verb takes
at most one argument of each type.? This knowledge is useful to resolve any incon-
sistencies of argument classification in order to generate final legitimate predictions.
We design an inference procedure that is formalized as a constrained optimization
problem, represented as an integer linear program (Roth and Yih 2004). It takes as
input the argument classifiers? confidence scores for each type of argument, along
with a list of constraints. The output is the optimal solution that maximizes the lin-
ear sum of the confidence scores, subject to the constraints that encode the domain
knowledge.
The inference stage can be naturally extended to combine the output of several
different SRL systems, as we will show in Section 5. In this section we first introduce
the constraints and formalize the inference problem for the semantic role labeling task.
We then demonstrate how we apply integer linear programming (ILP) to generate the
global label assignment.
3.4.1 Constraints over Argument Labeling. Formally, the argument classifiers attempt to
assign labels to a set of arguments, S1:M, indexed from 1 toM. Each argument Si can take
any label from a set of argument labels, P , and the indexed set of arguments can take a
set of labels, c1:M ? PM. If we assume that the classifiers return a score score(Si = ci) that
corresponds to the likelihood of argument Si being labeled ci then, given a sentence, the
unaltered inference task is solved by maximizing the overall score of the arguments,
c?1:M = argmax
c1:M?PM
score(S1:M = c1:M) = argmax
c1:M?PM
M
?
i=1
score(Si = ci) (1)
In the presence of global constraints derived from linguistic information and struc-
tural considerations, our system seeks to output a legitimate labeling that maximizes this
score. Specifically, it can be thought of as if the solution space is limited through the use
of a filter function, F , which eliminates many argument labelings from consideration.
266
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Here, we are concerned with global constraints as well as constraints on the arguments.
Therefore, the final labeling becomes
c?1:M = argmax
c1:M?F (PM )
M
?
i=1
score(Si = ci) (2)
When the confidence scores correspond to the conditional probabilities estimated by
the argument classifiers, the value of the objective function represents the expected
number of correct argument predictions. Hence, the solution of Equation (2) is the one
that maximizes this expected value among all legitimate outputs.
The filter function used considers the following constraints:1
1. Arguments cannot overlap with the predicate.
2. Arguments cannot exclusively overlap with the clauses.
3. If a predicate is outside a clause, its arguments cannot be embedded in
that clause.
4. No overlapping or embedding arguments.
This constraint holds because semantic arguments are labeled on
non-embedding constituents in the syntactic parse tree. In addition, as
defined in the CoNLL-2004 and 2005 shared tasks, the legitimate output of
an SRL system must satisfy this constraint.
5. No duplicate argument classes for core arguments, such as A0?A5 and AA.
The only exception is when there is a conjunction in the sentence. For
example,
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter] and [A1 my gold] [A2 to
my son].
Despite this exception, we treat it as a hard constraint because it almost
always holds.
6. If there is an R-arg argument, then there has to be an arg argument. That is,
if an argument is a reference to some other argument arg, then this
referenced argument must exist in the sentence. This constraint is directly
derived from the definition of R-arg arguments.
7. If there is a C-arg argument, then there has to be an arg argument; in
addition, the C-arg argument must occur after arg. This is stricter than
the previous rule because the order of appearance also needs to be
considered. Similarly, this constraint is directly derived from the definition
of C-arg arguments.
8. Given the predicate, some argument classes are illegal (e.g., predicate
stalk can take only A0 or A1). This information can be found in
PropBank Frames.
1 There are other constraints such as ?exactly one V argument per class,? or ?V?A1?C-V pattern? as
introduced by Punyakanok et al (2004). However, we did not find them particularly helpful in our
experiments. Therefore, we exclude those constraints in the presentation here.
267
Computational Linguistics Volume 34, Number 2
This constraint comes from the fact that different predicates take
different types and numbers of arguments. By checking the
PropBank Frame file of the target verb, we can exclude some core
argument labels.
Note that constraints 1, 2, and 3 are actually implemented in the argument identifi-
cation stage (see Section 3.2). In addition, they need to be explicitly enforced only when
full parsing information is not available because the output of the pruning heuristics
never violates these constraints.
The optimization problem (Equation (2)) can be solved using an ILP solver by
reformulating the constraints as linear (in)equalities over the indicator variables that
represent the truth value of statements of the form [argument i takes label j], as described
in detail next.
3.4.2 Using Integer Linear Programming. As discussed previously, a collection of po-
tential arguments is not necessarily a valid semantic labeling because it may not
satisfy all of the constraints. We enforce a legitimate solution using the following
inference algorithm. In our context, inference is the process of finding the best (ac-
cording to Equation (1)) valid semantic labels that satisfy all of the specified con-
straints. We take a similar approach to the one previously used for entity/relation
recognition (Roth and Yih 2004), and model this inference procedure as solving an ILP
problem.
An integer linear program is a linear program with integral variables. That is,
the cost function and the (in)equality constraints are all linear in terms of the variables.
The only difference in an integer linear program is that the variables can only take
integers as their values. In our inference problem, the variables are in fact binary. A
general binary integer linear programming problem can be stated as follows.
Given a cost vector p ? 
d, a collection of variables u = (u1, . . . ,ud) and cost ma-
trices C1 ? 
c1 ?
d,C2 ? 
c2 ?
d , where c1 and c2 are the numbers of inequality and
equality constraints and d is the number of binary variables, the ILP solution u? is the
vector that maximizes the cost function,
u? = argmax
u?{0,1}d
p ? u
subject to
C1u ? b1, and C2u = b2
where b1 ? 
c1 ,b2 ? 
c2 , and for all u ? {0, 1}d.
To solve the problem of Equation (2) in this setting, we first reformulate the
original cost function
?M
i=1 score(S
i = ci) as a linear function over several binary vari-
ables, and then represent the filter function F using linear inequalities and equalities.
We set up a bijection from the semantic labeling to the variable set u. This is done
by setting u to be a set of indicator variables that correspond to the labels assigned to ar-
guments. Specifically, let uic = [S
i = c] be the indicator variable that represents whether
268
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
or not the argument type c is assigned to Si, and let pic = score(S
i = c). Equation (1) can
then be written as an ILP cost function as
argmax
uic?{0,1}:?i?[1,M],c?P
M
?
i=1
?
c?P
picuic
subject to
?
c?P
uic = 1 ?i ? [1,M]
which means that each argument can take only one type. Note that this new constraint
comes from the variable transformation, and is not one of the constraints used in the
filter function F .
Of the constraints listed earlier, constraints 1 through 3 can be evaluated on a per-
argument basis and, for the sake of efficiency, arguments that violate these constraints
are eliminated even before being given to the argument classifier. Next, we show how to
transform the constraints in the filter function into the form of linear (in)equalities over
u and use them in this ILP setting. For a more complete example of this ILP formulation,
please see Appendix A.
Constraint 4: No overlapping or embedding. If arguments Sj1 , . . . ,Sjk cover the same word
in a sentence, then this constraint ensures that at most one of the arguments is assigned
to an argument type. In other words, at least k? 1 arguments will be the special class
null. If the special class null is represented by the symbol ?, then for every set of such
arguments, the following linear equality represents this constraint.
k
?
i=1
uji? ? k? 1
Constraint 5: No duplicate argument classes. Within the same clause, several types of
arguments cannot appear more than once. For example, a predicate can only take one
A0. This constraint can be represented using the following inequality.
M
?
i=1
uiA0 ? 1
Constraint 6: R-arg arguments. Suppose the referenced argument type is A0 and the
referential type is R-A0. The linear inequalities that represent this constraint are:
?m ? {1, . . . ,M} :
M
?
i=1
uiA0 ? umR-A0
If there are ? referential types, then the total number of inequalities needed is ?M.
Constraint 7: C-arg arguments. This constraint is similar to the reference argument con-
straints. The difference is that the continued argument arg has to occur before C-arg.
269
Computational Linguistics Volume 34, Number 2
Assume that the argument pair is A0 and C-A0, and arguments are sorted by their
beginning positions, i.e., if i < k, the position of the beginning of Sjk is not before that of
the beginning of Sji . The linear inequalities that represent this constraint are:
?m ? {2, . . . ,M} :
m?1
?
i=1
ujiA0 ? ujmC-A0
Constraint 8: Illegal argument types. Given a specific verb, some argument types should
never occur. For example, most verbs do not have arguments A5. This constraint is
represented by summing all the corresponding indicator variables to be 0.
M
?
i=1
uiA5 = 0
Using ILP to solve this inference problem enjoys several advantages. Linear con-
straints are very general, and are able to represent any Boolean constraint (Gue?ret, Prins,
and Sevaux 2002). Table 1 summarizes the transformations of common constraints (most
are Boolean), which are revised from Gue?ret, Prins, and Sevaux (2002), and can be used
for constructing complicated rules.
Previous approaches usually rely on dynamic programming to resolve non-
overlapping/embedding constraints (i.e., Constraint 4) when the constraint structure
is sequential. However, they are not able to handle more expressive constraints
such as those that take long-distance dependencies and counting dependencies into
account (Roth and Yih 2005). The ILP approach, on the other hand, is flexible enough
to handle more expressive and general constraints. Although solving an ILP problem is
NP-hard in the worst case, with the help of today?s numerical packages, this problem
can usually be solved very quickly in practice. For instance, in our experiments it
only took about 10 minutes to solve the inference problem for 4,305 sentences, using
Table 1
Rules of mapping constraints to linear (in)equalities for Boolean variables.
Original constraint Linear form
exactly k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn = k
at most k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn ? k
at least k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn ? k
a ? b a ? b
a = b? a = 1? b
a ? b? a+ b ? 1
a? ? b a+ b ? 1
a ? b a = b
a ? b ? c a ? b and a ? c
a ? b ? c a ? b+ c
b ? c ? a a ? b+ c? 1
b ? c ? a a ? (b+ c)/2
a? at least k of x1, x2, ? ? ? , xn a ? (x1 + x2 + ? ? ?+ xn)/k
At least k of x1, x2, ? ? ? , xn ? a a ? (x1 + x2 + ? ? ?+ xn ? (k? 1))/(n? (k? 1))
270
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Xpress-MP (2004) running on a Pentium-III 800 MHz machine. Note that ordinary
search methods (e.g., beam search) are not necessarily faster than solving an ILP
problem and do not guarantee the optimal solution.
4. The Importance of Syntactic Parsing
We experimentally study the significance of syntactic parsing by observing the effects
of using full parsing and shallow parsing information at each stage of an SRL system.
We first describe, in Section 4.1, how we prepare the data. The comparison of full
parsing and shallow parsing on the first three stages of the process is presented in the
reverse order (Sections 4.2, 4.3, 4.4). Note that in the following sections, in addition
to the performance comparison at various stages, we present also the overall system
performance for the different scenarios. In all cases, the overall system performance is
derived after the inference stage.
4.1 Experimental Setting
We use PropBank Sections 02 through 21 as training data, Section 23 as testing, and
Section 24 as a validation set when necessary. In order to apply the standard CoNLL
shared task evaluation script, our system conforms to both the input and output format
defined in the shared task.
The goal of the experiments in this section is to understand the effective contribu-
tion of full parsing information versus shallow parsing information (i.e., using only the
part-of-speech tags, chunks, and clauses). In addition, we also compare performance
when using the correct (gold-standard) data versus using automatic parse data. The
performance is measured in terms of precision, recall, and the F1 measure. Note that
all the numbers reported here do not take into account the V arguments as it is quite
trivial to predict V and, hence, this gives overoptimistic overall performance if included.
When doing the comparison, we also compute the 95% confidence interval of F1 us-
ing the bootstrap resampling method (Noreen 1989), and the difference is considered
significant if the compared F1 lies outside this interval. The automatic full parse trees
are derived using Charniak?s parser (2001) (version 0.4). In automatic shallow parsing,
the information is generated by different state-of-the-art components, including a POS
tagger (Even-Zohar and Roth 2001), a chunker (Punyakanok and Roth 2001), and a
clauser (Carreras, Ma`rquez, and Castro 2005).
4.2 Argument Classification
To evaluate the performance gap between full parsing and shallow parsing in argument
classification, we assume the argument boundaries are known, and only train classifiers
to classify the labels of these arguments. In this stage, the only difference between the
uses of full parsing and shallow parsing information is the construction of phrase type,
head word, POS tag of the head word, path, subcategorization, and syntactic frame features.
As described in Section 3.2.2, most of these features can be approximated using chunks
and clauses, with the exception of the syntactic frame feature. It is unclear how this
feature can be mimicked because it relies on the internal structure of a full parse tree.
Therefore, it does not have a corresponding feature in the shallow parsing case.
Table 2 reports the experimental results of argument classification when argument
boundaries are known. In this case, because the argument classifier of our SRL system
does not overpredict or miss any arguments, we do not need to train with a null class,
271
Computational Linguistics Volume 34, Number 2
Table 2
The accuracy of argument classification when argument boundaries are known.
Full Parsing Shallow Parsing
Gold 91.50 ? 0.48 90.75 ? 0.45
Auto 90.32 ? 0.48 89.71 ? 0.50
and we can simply measure the performance using accuracy instead of F1. The training
examples include 90,352 propositions with a total of 332,381 arguments. The test data
contain 5,246 propositions and 19,511 arguments. As shown in the table, although the
full-parsing features are more helpful than the shallow-parsing features, the perfor-
mance gap is quite small (0.75% on gold-standard data and 0.61% with the automatic
parsers).
The rather small difference in the performance between argument classifiers using
full parsing and shallow parsing information almost disappears when their output is
processed by the inference stage. Table 3 shows the final results in recall, precision, and
F1, when the argument boundaries are known. In all cases, the differences in F1 between
the full parsing?based and the shallow parsing?based systems are not statistically
significant.
Conclusion. When the argument boundaries are known, the performance of the full
parsing?based SRL system is about the same as the shallow parsing?based SRL system.
4.3 Argument Identification
Argument identification is an important stage that effectively reduces the number of
argument candidates after the pruning stage. Given an argument candidate, an argu-
ment identifier is a binary classifier that decides whether or not the candidate should be
considered as an argument. To evaluate the influence of full parsing information in this
stage, the candidate list used here is the outputs of the pruning heuristic applied on the
gold-standard parse trees. The heuristic results in a total number of 323,155 positive and
686,887 negative examples in the training set, and 18,988 positive and 39,585 negative
examples in the test set.
Similar to the argument classification stage, the only difference between full
parsing? and shallow parsing?based systems is in the construction of some features.
Specifically, phrase type, head word, POS tag of the head word, path, and subcategorization
features are approximated using chunks and clauses when the binary classifier is trained
using shallow parsing information.
Table 4 reports the performance of the argument identifier on the test set using
the direct predictions of the trained binary classifier. The recall and precision of the
Table 3
The overall system performance when argument boundaries are known.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 91.58 91.90 91.74 ? 0.51 91.14 91.48 91.31 ? 0.51
Auto 90.71 91.14 90.93 ? 0.53 90.50 90.88 90.69 ? 0.53
272
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 4
The performance of argument identification after pruning (based on the gold standard full parse
trees).
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 96.53 93.57 95.03 ? 0.32 93.66 91.72 92.68 ? 0.38
Auto 94.68 90.60 92.59 ? 0.39 92.31 88.36 90.29 ? 0.43
full parsing?based system are around 2 to 3 percentage points higher than the shallow
parsing?based system on the gold-standard data. As a result, the F1 score is 2.5 percent-
age points higher. The performance on automatic parse data is unsurprisingly lower
but the difference between the full parsing? and the shallow parsing?based systems is
as observed previously. In terms of filtering efficiency, around 25% of the examples are
predicted as positive. In other words, both argument identifiers filter out around 75%
of the argument candidates after pruning.
Because the recall in the argument identification stage sets the upper-bound the
recall in argument classification, the threshold that determines when examples are
predicted to be positive is usually lowered to allow more positive predictions. That
is, a candidate is predicted as positive when its probability estimation is larger than
the threshold. Table 5 shows the performance of the argument identifiers when the
threshold is 0.1.2
Because argument identification is just an intermediate step in a complete system,
a more realistic evaluation method is to see how each final system performs. Using an
argument identifier with threshold = 0.1 (i.e., Table 5), Table 6 reports the final results
in recall, precision, and F1. The F1 difference is 1.5 points when using the gold-standard
data. However, when automatic parsers are used, the shallow parsing?based system is,
in fact, slightly better; although the difference is not statistically significant. This may be
due to the fact that chunk and clause predictions are very important here, and shallow
parsers are more accurate in chunk or clause predictions than a full parser (Li and Roth
2001).
Conclusion. Full parsing information helps in argument identification. However, when
the automatic parsers are used, using the full parsing information may not have better
overall results compared to using shallow parsing.
4.4 Pruning
As shown in the previous two sections, the overall performance gaps of full parsing and
shallow parsing are small. When automatic parsers are used, the difference is less than 1
point in F1 or accuracy. Therefore, we conclude that themain contribution of full parsing
is in the pruning stage. Because the shallow parsing system does not have enough in-
formation for the pruning heuristics, we train two word-based classifiers to replace the
pruning stage. One classifier is trained to predict whether a given word is the start (S) of
2 The value was determined by experimenting with the complete system using automatic full parse trees,
on the development set. In our tests, lowering the threshold in argument identification always leads to
higher overall recall and lower overall precision. As a result, the gain in F1 is limited.
273
Computational Linguistics Volume 34, Number 2
Table 5
The performance of argument identification after pruning (based on the gold-standard full parse
trees) and with threshold = 0.1.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 92.13 95.62 93.84 ? 0.37 88.54 94.81 91.57 ? 0.42
Auto 89.48 94.14 91.75 ? 0.41 86.14 93.21 89.54 ? 0.47
Table 6
The overall system performance using the output from the pruning heuristics, applied on the
gold-standard full parse trees.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 86.22 87.40 86.81 ? 0.59 84.14 85.31 84.72 ? 0.63
Auto 84.21 85.04 84.63 ? 0.63 86.17 84.02 85.08 ? 0.63
an argument; the other classifier is to predict the end (E) of an argument. If the product
of probabilities of a pair of S and E predictions is larger than a predefined threshold,
then this pair is considered as an argument candidate. The threshold used here was
obtained by using the validation set. Both classifiers use very similar features to those
used by the argument identifier as explained in Section 3.2, treating the target word as
a constituent. Particularly, the features are predicate, POS tag of the predicate, voice,
context words, POS tags of the context words, chunk pattern, clause relative position,
and shallow-path. The headword and its POS tag are replaced by the target word and its
POS tag. The comparison of using the classifiers and the heuristics is shown in Table 7.
Even without the knowledge of the constituent boundaries, the classifiers seem
surprisingly better than the pruning heuristics. Using either the gold-standard data set
or the output of automatic parsers, the classifiers achieve higher F1 scores. One possible
reason for this phenomenon is that the accuracy of the pruning strategy is limited by
the number of agreements between the correct arguments and the constituents of the
parse trees. Table 8 summarizes the statistics of the examples seen by both strategies.
The pruning strategy needs to decide which are the potential arguments among all con-
stituents. This strategy is upper-bounded by the number of correct arguments that agree
with some constituent. On the other hand, the classifiers do not have this limitation. The
number of examples they observe is the total number of words to be processed, and the
positive examples are those arguments that are annotated as such in the data set.
Table 7
The performance of pruning using heuristics and classifiers.
Full Parsing Classifier Threshold = 0.04
Prec Rec F1 Prec Rec F1
Gold 25.94 97.27 40.96 ? 0.51 29.58 97.18 45.35 ? 0.83
Auto 22.79 86.08 36.04 ? 0.52 24.68 94.80 39.17 ? 0.79
274
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 8
Statistics of the training and test examples for the pruning stage.
Words Arguments Constituents Agreements
Gold Auto Gold Auto
Train 2,575,665 332,381 4,664,954 4,263,831 327,603 319,768
Test 147,981 19,511 268,678 268,482 19,266 18,301
The Agreements column shows the number of arguments that match the boundaries of some
constituents.
Note that because each verb is processed independently, a sentence is processed
once for each verb in the sentence. Therefore, the words and constituents in each
sentence are counted as many times as the number of verbs to be processed.
As before, in order to compare the systems that use full parsing and shallow
parsing information, we need to see the impact on the overall performance. There-
fore, we built two semantic role systems based on full parsing and shallow parsing
information. The full parsing?based system follows the pruning, argument identifica-
tion, argument classification, and inference stages, as described earlier. For the shallow
parsing system, the pruning heuristic is replaced by the word-based pruning classi-
fiers, and the remaining stages are designed to use only shallow parsing as described in
previous sections. Table 9 shows the overall performance of the two evaluation systems.
As indicated in the tables, the gap in F1 between full parsing and shallow parsing?
based systems enlarges tomore than 11 points on the gold-standard data. At first glance,
this result seems to contradict our conclusion in Section 4.3. After all, if the pruning
stage of shallow parsing SRL system performs equally well or even better, the overall
performance gap in F1 should be small.
After we carefully examined the output of the word-based classifier, we realized
that it filters out easy candidates, and leaves examples that are difficult to the later
stages. Specifically, these argument candidates often overlap and differ only in one or
twowords. On the other hand, the pruning heuristic based on full parsing never outputs
overlapping candidates and consequently provides input that is easier for the next stage
to handle. Indeed, the following argument identification stage turns out to be good in
discriminating these non-overlapping candidates.
Conclusion. The most crucial contribution of full parsing is in the pruning stage. The
internal tree structure significantly helps in discriminating argument candidates, which
makes the work done by the following stages easier.
Table 9
The overall system performance.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 86.22 87.40 86.81 ? 0.59 75.34 75.28 75.31 ? 0.76
Auto 77.09 75.51 76.29 ? 0.76 75.48 67.13 71.06 ? 0.80
275
Computational Linguistics Volume 34, Number 2
5. The Effect of Inference
Our inference procedure plays an important role in improving accuracy when the local
predictions violate the constraints among argument labels. In this section, we first
present the overall system performance when most constraints are not used. We then
demonstrate how the inference procedure can be used to combine the output of several
systems to yield better performance.
5.1 Inference with Limited Constraints
The inference stage in our system architecture provides a principled way to resolve
conflicting local predictions. It is interesting to see whether this procedure improves the
performance differently for the full parsing? vs. the shallow parsing?based system, as
well as gold-standard vs. automatic parsing input.
Table 10 shows the results of using only constraints 1, 2, 3, and 4. As mentioned
previously, the first three constraints are handled before the argument classification
stage. Constraint 4, which forbids overlapping or embedding arguments, is required
in order to use the official CoNLL-2005 evaluation script and is therefore kept.
By comparing Table 9 with Table 10, we can see that the effect of adding more
constraints is quite consistent over the four settings. Precision is improved by 1 to 2 per-
centage points but recall is decreased a little. As a result, the gain in F1 is about 0.5 to 1
point. It is not surprising to see this lower recall and higher precision phenomenon after
the constraints described in Section 3.4.1 are examined. Most constraints punish false
non-null output, but do not regulate false null predictions. For example, an assignment
that has two A1 arguments clearly violates the non-duplication constraint. However, if
an assignment has no predicted arguments at all, it still satisfies all the constraints.
5.2 Joint Inference
The empirical study in Section 4 indicates that the performance of an SRL system
primarily depends on the very first stage?pruning, which is directly derived from
the full parse trees. This also means that in practice the quality of the syntactic parser
is decisive to the quality of the SRL system. To improve semantic role labeling, one
possible way is to combine different SRL systems through a joint inference stage, given
that the systems are derived using different full parse trees.
To test this idea, we first build two SRL systems that use Collins?s parser (Collins
1999)3 and Charniak?s parser (Charniak 2001), respectively. In fact, these two parsers
have noticeably different outputs. Applying the pruning heuristics on the output of
Collins?s parser produces a list of candidates with 81.05% recall. Although this number
is significantly lower than the 86.08% recall produced by Charniak?s parser, the union
of the two candidate lists still significantly improves recall to 91.37%. We construct the
two systems by implementing the first three stages, namely, pruning, argument identifi-
cation, and argument classification. When a test sentence is given, a joint inference stage
is used to resolve the inconsistency of the output of argument classification in these two
systems.
We first briefly review the objective function used in the inference procedure in-
troduced in Section 3.4. Formally speaking, the argument classifiers attempt to assign
3 We use the Collins parser implemented by Bikel (2004).
276
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 10
The impact of removing most constraints in overall system performance.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 85.07 87.50 86.27 ? 0.58 73.19 75.63 74.39 ? 0.75
Auto 75.88 75.81 75.84 ? 0.75 73.56 67.45 70.37 ? 0.80
labels to a set of arguments, S1:M, indexed from 1 toM. Each argument Si can take any
label from a set of argument labels, P , and the indexed set of arguments can take a
set of labels, c1:M ? PM. If we assume that the argument classifier returns an estimated
conditional probability distribution, Prob(Si = ci), then, given a sentence, the inference
procedure seeks a global assignment that maximizes the objective function denoted by
Equation (2), which can be rewritten as follows,
c?1:M = argmax
c1:M?F (PM )
M
?
i=1
Prob(Si = ci) (3)
where the linguistic and structural constraints are represented by the filter F . In other
words, this objective function reflects the expected number of correct argument predic-
tions, subject to the constraints.
When there are two or more argument classifiers from different SRL systems, a joint
inference procedure can take the output estimated probabilities for all these candidates
as input, although some candidates may refer to the same phrases in the sentence. For
example, Figure 3 shows the two candidate sets for a fragment of a sentence, ..., traders
say, unable to cool the selling panic in both stocks and futures. In this example, system A has
two argument candidates, a1 = traders and a4 = the selling panic in both stocks and futures;
system B has three argument candidates, b1 = traders, b2 = the selling panic, and b3 = in
both stocks and futures.
A straightforward solution to the combination is to treat each argument produced
by a system as a possible output. Each possible labeling of the argument is associated
with a variable which is then used to set up the inference procedure. However, the final
predictionwill be likely dominated by the system that producesmore candidates, which
is system B in this example. The reason is that our objective function is the sum of the
probabilities of all the candidate assignments.
This bias can be corrected by the following observation. Although system A only
has two candidates, a1 and a4, it can be treated as if it also has two additional phantom
candidates, a2 and a3, where a2 and b2 refer to the same phrase, and so do a3 and b3.
Similarly, system B has a phantom candidate b4 that corresponds to a4. Because systemA
does not really generate a2 and a3, we can assume that these two phantom candidates are
predicted by it as ?null? (i.e., not an argument). We assign the same prior distribution to
each phantom candidate. In particular, the probability of the ?null? class is set to be 0.55
based on empirical tests, and the probabilities of the remaining classes are set based on
their occurrence frequencies in the training data.
Then, we treat each possible final argument output as a single unit. Each probability
estimation by a system can be viewed as evidence in the final probability estimation and,
therefore, we can simply average their estimation. Formally, let Si be the argument set
277
Computational Linguistics Volume 34, Number 2
Figure 3
The output of two SRL systems: system A has two candidates, a1 = traders and a4 = the selling
panic in both stocks and futures; system B has three argument candidates, b1 = traders, b2 = the
selling panic, and b3 = in both stocks and futures. In addition, we create two phantom candidates a2
and a3 for system A that correspond to b2 and b3 respectively, and b4 for system B that
corresponds to a4.
output by system i, and S =
?k
i=1 Si be the set of all arguments where k is the number
of systems; let N be the cardinality of S. Our augmented objective function is then:
c?1:N = argmax
c1:N?F (PN )
N
?
i=1
Prob(Si = ci) (4)
where Si ? S, and
Prob(Si = ci) = 1
k
k
?
j=1
Probj(S
i = ci) (5)
where Probj is the probability output by system j.
Note that we may also treat the individual systems differently by applying different
priors (i.e., weights) on the estimated probabilities of the argument candidates. For
example, if the performance of system A is much better than system B, then we may
want to trust system A?s output more by multiplying the output probabilities by a
larger weight.
Table 11 reports the performance of two individual systems based on Collins?s
parser and Charniak?s parser, as well as the joint system, where the two individual
systems are equally weighted. The joint system based on this straightforward strategy
significantly improves the performance compared to the two original SRL systems in
both recall and precision, and thus achieves a much higher F1.
6. Empirical Evaluation?CoNLL Shared Task 2005
In this section, we present the detailed evaluation of our SRL system, in the competi-
tion on semantic role labeling?the CoNLL-2005 shared task (Carreras and Ma`rquez
278
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 11
The performance of individual and combined SRL systems.
Prec Rec F1
Collins? parser 75.92 71.45 73.62 ? 0.79
Charniak?s parser 77.09 75.51 76.29 ? 0.76
Combined result 80.53 76.94 78.69 ? 0.71
2005). The setting of this shared task is basically the same as it was in 2004, with
some extensions. First, it allows much richer syntactic information. In particular, full
parse trees generated using Collins?s parser (Collins 1999) and Charniak?s parser
(Charniak 2001) were provided. Second, the full parsing standard partition was used?
the training set was enlarged and covered Sections 02?21, the development set was
Section 24, and the test set was Section 23. Finally, in addition to the Wall Street Journal
(WSJ) data, three sections of the Brown corpus were used to provide cross-corpora
evaluation.
The system we used to participate in the CoNLL-2005 shared task is an enhanced
version of the system described in Sections 3 and 5. The main difference was that
the joint-inference stage was extended to combine six basic SRL systems instead of
two. Specifically for this implementation, we first trained two SRL systems that use
Collins?s parser and Charniak?s parser, respectively, because of their noticeably dif-
ferent outputs. In evaluation, we ran the system that was trained with Charniak?s
parser five times, with the top-5 parse trees output by Charniak?s parser. Together we
have six different outputs per predicate. For each parse tree output, we ran the first
three stages, namely, pruning, argument identification, and argument classification.
Then, a joint-inference stage, where each individual system is weighted equally, was
used to resolve the inconsistency of the output of argument classification in these
systems.
Table 12 shows the overall results on the development set and different test sets; the
detailed results on WSJ section 23 are shown in Table 13. Table 14 shows the results of
individual systems and the improvement gained by the joint inference procedure on the
development set.
Our system reached the highest F1 scores on all the test sets and was the best system
among the 19 participating teams. After the competition, we improved the system
slightly by tuning the weights of the individual systems in the joint inference procedure,
where the F1 scores onWSJ test section and the Brown test set are 79.59 points and 67.98
points, respectively.
Table 12
Overall CoNLL-2005 shared task results.
Prec. Rec. F1
Development 80.05 74.83 77.35
Test WSJ 82.28 76.78 79.44
Test Brown 73.38 62.93 67.75
Test WSJ+Brown 81.18 74.92 77.92
279
Computational Linguistics Volume 34, Number 2
Table 13
Detailed CoNLL-2005 shared task results on the WSJ test set.
Test WSJ Prec. Rec. F1
Overall 82.28 76.78 79.44
A0 88.22 87.88 88.05
A1 82.25 77.69 79.91
A2 78.27 60.36 68.16
A3 82.73 52.60 64.31
A4 83.91 71.57 77.25
AM-ADV 63.82 56.13 59.73
AM-CAU 64.15 46.58 53.97
AM-DIR 57.89 38.82 46.48
AM-DIS 75.44 80.62 77.95
AM-EXT 68.18 46.88 55.56
AM-LOC 66.67 55.10 60.33
AM-MNR 66.79 53.20 59.22
AM-MOD 96.11 98.73 97.40
AM-NEG 97.40 97.83 97.61
AM-PNC 60.00 36.52 45.41
AM-TMP 78.16 76.72 77.44
R-A0 89.72 85.71 87.67
R-A1 70.00 76.28 73.01
R-A2 85.71 37.50 52.17
R-AM-LOC 85.71 57.14 68.57
R-AM-TMP 72.34 65.38 68.69
In terms of the computation time, for both the argument identifier and the argument
classifier, the training of each model, excluding feature extraction, takes 50?70 minutes
using less than 1GB memory on a 2.6GHz AMD machine. On the same machine, the
average test time for each stage, excluding feature extraction, is around 2 minutes.
7. Related Work
The pioneering work on building an automatic semantic role labeler was proposed
by Gildea and Jurafsky (2002). In their setting, semantic role labeling was treated as a
tagging problem on each constituent in a parse tree, solved by a two-stage architecture
consisting of an argument identifier and an argument classifier. This is similar to our
Table 14
The results of individual systems and the result with joint inference on the development set.
Prec. Rec. F1
Charniak-1 75.40 74.13 74.76
Charniak-2 74.21 73.06 73.63
Charniak-3 73.52 72.31 72.91
Charniak-4 74.29 72.92 73.60
Charniak-5 72.57 71.40 71.98
Collins 73.89 70.11 71.95
Joint inference 80.05 74.83 77.35
280
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
main architecture with the exclusion of the pruning and inference stages. There are
two additional key differences between their system and ours. First, their system
used a back-off probabilistic model as its main engine. Second, it was trained on
FrameNet (Baker, Fillmore, and Lowe 1998)?another large corpus, besides PropBank,
that contains selected examples of semantically labeled sentences.
Later that year, the same approach was applied on PropBank by Gildea and Palmer
(2002). Their system achieved 57.7% precision and 50.0% recall with automatic parse
trees, and 71.1% precision and 64.4% recall with gold-standard parse trees. It is worth
noticing that at that time the PropBank project was not finished and the data set
available was only a fraction in size of what it is today. Since these pioneering works, the
task has gained increasing popularity and created a new line of research. The two-step
constituent-by-constituent architecture became a common blueprint for many systems
that followed.
Partly due to the expansion of the PropBank dataset, researchers have gradually
made improvement on the performance of automatic SRL systems by using new tech-
niques and new features. Some of the early systems are described in Chen and Rambow
(2003), Gildea and Hockenmaier (2003), and Surdeanu et al (2003). All are based on a
two-stage architecture similar to the one proposed by Gildea and Palmer (2002) with
the differences in the machine-learning techniques and the features used. The first
breakthrough in terms of performance was due to Pradhan et al (2003), who first
viewed the task as a massive classification problem and applied multiple SVMs to it.
Their final result (after a few more improvements) reported in Pradhan et al (2004)
achieved 84% and 75% in precision and recall, respectively.
A second significant contribution beyond the two-stage architecture is due to Xue
and Palmer (2004), who introduced the pruning heuristics to the two-stage architecture,
and remarkably reduced the number of candidate arguments a system needs to con-
sider; this approach was adopted by many systems. Another significant advancement
was in the realization that global information can be exploited and benefits the results
significantly. Inference based on an integer linear programming technique, which was
originally introduced by Roth and Yih (2004) on a relation extraction problem, was
first applied to the SRL problem by Punyakanok et al (2004). It showed that domain
knowledge can be easily encoded and contributes significantly through inference over
the output of classifiers. The idea of exploiting global information, which is detailed in
this paper, was pursued later by other researchers, in different forms.
Besides the constituent-by-constituent based architecture, others have also been
explored. The alternative frameworks include representing semantic role labeling as
a sequence-tagging problem (Ma`rquez, Pere Comas, and Catala` 2005) and tagging the
edges in the corresponding dependency trees (Hacioglu 2004). However, the most pop-
ular architecture by far is the constituent-by-constituent based multi-stage architecture,
perhaps due to its conceptual simplicity and its success. In the CoNLL-2005 shared
task competition (Carreras and Ma`rquez 2005), the majority of the systems followed
the constituent-by-constituent based two-stage architecture, and the use of the pruning
heuristics was also fairly common.
The CoNLL-2005 shared task also highlighted the importance of system combina-
tion, such as our ILP technique when used in joint inference, in order to achieve superior
performance. The top four systems, which produced significantly better results than the
rest, all used some schemes to combine the output of several SRL systems, ranging from
using a fixed combination function (Haghighi, Toutanova, and Manning 2005; Koomen
et al 2005) to using a machine-learned combination strategy (Ma`rquez, Pere Comas,
and Catala` 2005; Pradhan, Hacioglu, Ward et al 2005).
281
Computational Linguistics Volume 34, Number 2
The work of Gildea and Palmer (2002) pioneered not only the fundamental archi-
tecture of SRL, but was also the first to investigate the interesting question regarding
the significance of using full parsing for high quality SRL. They compared their full
system with another system that only used chunking, and found that the chunk-based
system performed much worse. The precision and recall dropped from 57.7% and
50.0% to 27.6% and 22.0%, respectively. That led to the conclusion that full parsing
information is necessary to solving the SRL problem, especially at the stage of argu-
ment identification?a finding that is quite similar to ours in this article. However,
their chunk-based approach was very weak?only chunks were considered as possible
candidates; hence, it is not very surprising that the boundaries of the arguments could
not be reliably found. In contrast, our shallow parse?based system does not have these
restrictions on the argument boundaries and therefore performs much better at this
stage, providing a more fair comparison.
A related comparison can be found also in the work by Pradhan, Hacioglu, Krugler
et al (2005) (their earlier version appeared in Pradhan et al [2003]), which reported
the performance on several systems using different information sources and system
architectures. Their shallow parse?based system is modeled as a sequence tagging prob-
lem while the full system is a constituent-by-constituent based two-stage system. Due
to technical difficulties, though, they reported the results of the chunk-based systems
only on a subset of the full data set. Their shallow parse?based system achieved 60.4%
precision and 51.4% recall and their full system achieved 80.6% precision and 67.1%
recall on the same data set (but 84% precision and 75% recall with the full data set).
Therefore, due to the use of different architectures and data set sizes, the questions
of ?how much one can gain from full parsing over shallow parsing when using the
full PropBank data set? and ?what are the sources of the performance gain? were left
open.
Similarly, in the CoNLL-2004 shared task (Carreras andMa`rquez 2004), participants
were asked to develop SRL systems with the restriction that only shallow parsing infor-
mation (i.e., chunks and clauses) were allowed. The performance of the best systemwas
at 72.43% precision and 66.77% recall, which was about 10 points in F1 lower than the
best system based on full parsing in the literature. However, the training examples were
derived from only 5 sections and not all the 19 sections usually used in the standard
setting. Hence, the question was not yet fully answered.
Our experimental study, on the other hand, is done with a consistent architecture,
by considering each stage in a controlled manner, and using the full data set, allowing
one to draw direct conclusions regarding the impact of this information source.
8. Conclusion
This paper studies the important task of semantic role labeling. We presented an ap-
proach to SRL and a principled and general approach to incorporating global informa-
tion in natural language decisions. Beyond presenting this approach which leads to a
state-of-the-art SRL system, we focused on investigating the significance of using full
parse tree information as input to an SRL system adhering to the most common system
architecture, and the stages in the process where this information has the most impact.
We performed a detailed and fair experimental comparison between shallow and full
parsing information and concluded that, indeed, full syntactic information can improve
the performance of an SRL system. In particular, we have shown that this information
is most crucial in the pruning stage of the system, and relatively less important in the
following stages.
282
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
In addition, we showed the importance of global inference to good performance in
this task, characterized by rich structural and linguistic constraints among the predicted
labels of the arguments. Our integer linear programming?based inference procedure
is a powerful and flexible optimization strategy that finds the best solution subject to
these constraints. As we have shown, it can be used to resolve conflicting argument
predictions in an individual system but can also serve as an effective and simple
approach to combining different SRL systems, resulting in a significant improvement
in performance.
In the future, we plan to extend our work in several directions. By adding more
constraints to the inference procedure, an SRL system may be further improved.
Currently, the constraints are provided by human experts in advance. Learning both
hard and statistical constraints from the data will be our top priority. Some work on
combining statistical and declarative constraints has already started and is reported
in Roth and Yih (2005). Another issue we want to address is domain adaptation.
It has been clearly shown in the CoNLL-2005 shared task that the performance of
current SRL systems degrades significantly when tested on a corpus different from
the one used in training. This may be due to the underlying components, especially
the syntactic parsers which are very sensitive to changes in data genre. Developing
a better model that more robustly combines these components could be a promising
direction. In addition, although the shallow parsing?based system was shown here to
be inferior, shallow parsers were shown to be more robust than full parsers (Li and
Roth 2001). Therefore, combining these two systems may bring forward both of their
advantages.
Appendix A: An ILP Formulation for SRL
In this section, we show a complete example of the ILP formulation formulated to solve
the inference problem as described in Section 3.4.
Example. Assume the sentence is four words long with the following argument
candidates, and the following illegal argument types for the predicate of interest.
Sentence: w1 w2 w3 w4
Candidates: [ S1 ] [ S2 ] [ S3 ] [ S5 ]
[ S4 ]
Illegal argument types: A3, A4, A5
Indicator Variables and Their Costs. The followings are the indicator variables and their
associated costs set up for the example.
Indicator Variables:
u1A0,u1A1, . . . ,u1AM-LOC, . . . ,u1C-A0, . . . ,u1R-A0, . . . ,u1?
u2A0,u2A1, . . . ,u2AM-LOC, . . . ,u2C-A0, . . . ,u2R-A0, . . . ,u2?
...
u5A0,u5A1, . . . ,u5AM-LOC, . . . ,u5C-A0, . . . ,u5R-A0, . . . ,u5?
Costs:
p1A0, p1A1, . . . , p1AM-LOC, . . . , p1C-A0, . . . , p1R-A0, . . . , p1?
p2A0, p2A1, . . . , p2AM-LOC, . . . , p2C-A0, . . . , p2R-A0, . . . , p2?
...
p5A0, p5A1, . . . , p5AM-LOC, . . . , p5C-A0, . . . , p5R-A0, . . . , p5?
283
Computational Linguistics Volume 34, Number 2
Objective Function. The objective function can be written as the following.
argmaxuic?{0,1}:?i?[1,5],c?P
?5
i=1
?
c?P picuic
where
P = {A0,A1, . . . , AM-LOC, . . . , C-A0, . . . , R-A0, . . . ,?}
subject to
u1A0 + u1A1 + . . .+ u1AM-LOC + . . .+ u1C-A0 + . . .+ u1R-A0 + . . .+ u1? = 1
u2A0 + u2A1 + . . .+ u2AM-LOC + . . .+ u2C-A0 + . . .+ u2R-A0 + . . .+ u2? = 1
...
u5A0 + u5A1 + . . .+ u5AM-LOC + . . .+ u2C-A0 + . . .+ u5R-A0 + . . .+ u5? = 1
Additional Constraints. The rest of the constraints can be formulated as the following.
Constraint 4: No overlapping or embedding
u3? + u4? ? 1
u4? + u5? ? 1
Constraint 5: No duplicate argument classes
u1A0 + u2A0 + . . .+ u5A0 ? 1
u1A1 + u2A1 + . . .+ u5A1 ? 1
u1A2 + u2A2 + . . .+ u5A2 ? 1
Constraint 6: R-arg arguments
u1A0 + u2A0 + . . .+ u5A0 ? u1R-A0
u1A0 + u2A0 + . . .+ u5A0 ? u2R-A0
...
u1A0 + u2A0 + . . .+ u5A0 ? u5R-A0
u1A1 + u2A1 + . . .+ u5A1 ? u1R-A1
...
u1AM-LOC + u2AM-LOC + . . .+ u5AM-LOC ? u1R-AM-LOC
...
Constraint 7: C-arg arguments
u1A0 ? u2C-A0
u1A0 + u2A0 ? u3C-A0
...
u1A0 + u2A0 + . . .+ u4A0 ? u5C-A0
u1A1 ? u2C-A1
...
u1AM-LOC ? u2C-AM-LOC
...
284
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Constraint 8: Illegal argument types
u1A3 + u2A3 + . . .+ u5A3 = 0
u1A4 + u2A4 + . . .+ u5A4 = 0
u1A5 + u2A5 + . . .+ u5A5 = 0
Acknowledgments
We thank Xavier Carreras and Llu??s Ma`rquez
for the data and scripts, Szu-ting Yi for her
help in improving our joint inference
procedure, and Nick Rizzolo as well as the
anonymous reviewers for their comments
and suggestions. We are also grateful to Dash
Optimization for the free academic use of
Xpress-MP and AMD for their equipment
donation. This research is supported by the
Advanced Research and Development
Activity (ARDA)?s Advanced Question
Answering for Intelligence (AQUAINT)
Program, a DOI grant under the Reflex
program, NSF grants ITR-IIS-0085836,
ITR-IIS-0085980, and IIS-9984168,
EIA-0224453, and an ONR MURI Award.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley Framenet
project. In Proceedings of COLING-ACL,
pages 86?90, Montreal, Canada.
Bikel, Daniel M. 2004. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Bishop, Christopher M., 1995. Neural
Networks for Pattern Recognition, chapter
6.4: Modelling conditional distributions,
page 215. Oxford University Press,
Oxford, UK.
Carlson, Andrew J., Chad M. Cumby, Jeff L.
Rosen, and Dan Roth. 1999. The SNoW
learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer
Science Department.
Carreras, Xavier and Llu?is Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
tasks: Semantic role labeling. In Proceedings
of CoNLL-2004, pages 89?97, Boston, MA.
Carreras, Xavier and Llu?is Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 152?164, Ann Arbor, MI.
Carreras, Xavier, Llu?is Ma`rquez, and Jorge
Castro. 2005. Filtering?ranking perceptron
learning for partial parsing.Machine
Learning, 60:41?71.
Carreras, Xavier, Llu?is Ma`rquez, Vasin
Punyakanok, and Dan Roth. 2002.
Learning and inference for clause
identification. In Proceedings of the 13th
European Conference on Machine Learning
(ECML-2002), pages 35?47, Helsinki,
Finland.
Charniak, Eugene. 2001. Immediate-head
parsing for language models. In
Proceedings of the 39th Annual Meeting of the
Association of Computational Linguistics
(ACL-2001), pages 116?123, Toulouse,
France.
Chen, John and Owen Rambow. 2003. Use of
deep linguistic features for the recognition
and labeling of semantic arguments. In
Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 41?48,
Sapporo, Japan.
Collins, Michael. 1999. Head-driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, Computer Science
Department, University of Pennsylvania,
Philadelphia, PA.
Dagan, Ido, Yael Karov, and Dan Roth.
1997. Mistake-driven learning in text
categorization. In Proceedings of the
Second Conference on Empirical Methods
in Natural Language Processing
(EMNLP-1997), pages 55?63,
Providence, RI.
Even-Zohar, Yair and Dan Roth. 2001. A
sequential model for multi-class
classification. In Proceedings of the 2001
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2001),
pages 10?19, Pittsburgh, PA.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
Perceptron algorithm.Machine Learning,
37(3):277?296.
Gildea, Daniel and Julia Hockenmaier. 2003.
Identifying semantic roles using
combinatory categorial grammar. In
Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 57?64,
Sapporo, Japan.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
285
Computational Linguistics Volume 34, Number 2
Gildea, Daniel and Martha Palmer. 2002.
The necessity of parsing for predicate
argument recognition. In Proceedings
of the 40th Annual Meeting of the
Association of Computational Linguistics
(ACL-2002), pages 239?246,
Philadelphia, PA.
Golding, Andrew R. and Dan Roth. 1999.
A Winnow based approach to
context-sensitive spelling correction.
Machine Learning, 34(1-3):107?130.
Grove, Adam J. and Dan Roth. 2001. Linear
concepts and hidden variables.Machine
Learning, 42(1?2):123?141.
Gue?ret, Christelle, Christian Prins, and Marc
Sevaux. 2002. Applications of Optimization
with Xpress-MP. Dash Optimization.
Translated and revised by Susanne
Heipcke. http://www.dashoptimization.
com/home/downloads/book/booka4.pdf.
Hacioglu, Kadri. 2004. Semantic role labeling
using dependency trees. In Proceedings of
the 20th International Conference on
Computational Linguistics (COLING),
Geneva, Switzerland.
Hacioglu, Kadri, Sameer Pradhan, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2004. Semantic role labeling by
tagging syntactic chunks. In Proceedings of
CoNLL-2004, pages 110?113, Boston, MA.
Haghighi, Aria, Kristina Toutanova, and
Christopher D. Manning. 2005. A joint
model for semantic role labeling. In
Proceedings of the Ninth Conference on
Computational Natural Language
Learning (CoNLL-2005), pages 173?176,
Ann Arbor, MI.
Kingsbury, Paul and Martha Palmer. 2002.
From Treebank to PropBank. In Proceedings
of LREC-2002, Las Palmas, Canary Islands,
Spain.
Kipper, Karin, Martha Palmer, and Owen
Rambow. 2002. Extending PropBank with
VerbNet semantic predicates. In
Proceedings of Workshop on Applied
Interlinguas, Tiburon, CA.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 181?184, Ann Arbor, MI.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levin, Beth and Malka R. Hovav. 1996. From
lexical semantics to argument realization.
Unpublished manuscript.
Li, Xin and Dan Roth. 2001. Exploring
evidence for shallow parsing. In
Proceedings of CoNLL-2001, pages 107?110,
Toulouse, France.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ma`rquez, Llu?is, Jesus Gime?nez Pere Comas,
and Neus Catala`. 2005. Semantic role
labeling as sequential tagging. In
Proceedings of the Ninth Conference on
Computational Natural Language
Learning (CoNLL-2005), pages 193?196,
Ann Arbor, MI.
Noreen, Eric W. 1989. Computer-Intensive
Methods for Testing Hypotheses. New York:
John Wiley & Sons.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005. Support vector
learning for semantic argument
classification.Machine Learning, 60:11?39.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2003. Semantic role parsing
adding semantic structure to
unstructured text. In Proceedings of the
3rd IEEE International Conference on Data
Mining (ICDM 2003), pages 629?632,
Melbourne, FL.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2005. Semantic role chunking
combining complementary syntactic
views. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 217?220,
Ann Arbor, MI.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James H. Martin, and Daniel
Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In
Proceedings of NAACL-HLT 2004,
pages 233?240, Boston, MA.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings the 20th
International Conference on Computational
Linguistics (COLING), pages 1346?1352,
Geneva, Switzerland.
Punyakanok, Vasin and Dan Roth. 2001. The
use of classifiers in sequential inference. In
286
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Todd K. Leen, Thomas G. Dietterich, and
Volker Tresp, editors, Advances in Neural
Information Processing Systems 13,
pages 995?1001. MIT Press.
Roth, Dan. 1998. Learning to resolve
natural language ambiguities: A unified
approach. In Proceedings of the Fifteenth
National Conference on Artificial
Intelligence (AAAI-98), pages 806?813,
Madison, WI.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks. In
Proceedings of CoNLL-2004, pages 1?8,
Boston, MA.
Roth, Dan and Wen-tau Yih. 2005. Integer
linear programming inference for
conditional random fields. In Proceedings of
the 22nd International Conference on Machine
Learning (ICML-2005), pages 737?744,
Bonn, Germany.
Surdeanu, Mihai, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using
predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting on Association for
Computational Linguistics, pages 8?15,
Sapporo, Japan.
Tjong Kim Sang, Erik F. and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In
Proceedings of CoNLL-2000 and LLL-2000,
pages 127?132, Lisbon, Portugal.
Xpress-MP. 2004. Dash Optimization.
Xpress-MP. http://www.
dashoptimization.com.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004),
pages 88?94, Barcelona, Spain.
Zhang, Tong, Fred Damerau, and David
Johnson. 2002. Text chunking based on a
generalization of Winnow. Journal of
Machine Learning Research, 2:615?637.
287

Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 309?310,
New York City, June 2006. c?2006 Association for Computational Linguistics
6. Automatic Semantic Role Labeling
Scott Wen-tau Yih and Kristina Toutanova, Microsoft Research
The goal of semantic role labeling is to map sentences to domain-independent semantic representations,
which abstract away from syntactic structure and are important for deep NLP tasks such as question answer-
ing, textual entailment, and complex information extraction. Semantic role labeling has recently received
significant interest in the natural language processing community. In this tutorial, we will first describe the
problem and history of semantic role labeling, and introduce existing corpora and other related tasks. Next,
we will provide a detailed survey of state-of-the-art machine learning approaches to building a semantic role
labeling system. Finally, we will conclude the tutorial by discussing directions for improving semantic role
labeling systems and their application to other natural language problems.
6.1 Tutorial Outline
1. Introduction
? What is semantic role labeling?
? Why is SRL important?
? Existing corpora: FrameNet & PropBank
? Corpora in development
? Relation to other tasks
2. Survey of Existing SRL Systems
? History of the development of automatic SRL systems
? Pioneering Work
? Basic architecture of a generic SRL system
? Major components
? Machine learning technologies
? CoNLL-04 and CoNLL-05 shared tasks on SRL
? Details of several CoNLL-05 systems
? Overall comparisons of CoNLL-05 systems
3. Analysis of Systems and Future Directions
? Error Analysis
? Influence of parser errors
? Per argument performance
? Directions for improving SRL
4. Applications
? Information Extraction
? Textual Entailment
? Machine Translation
6.2 Target Audience
The main target audience is NLP students and researchers who are interested in learning about semantic role
labeling, but have not followed all developments in the field. Additionally, researchers already working on
semantic role labeling should profit from a global view and summary of relevant work. The tutorial will
also be valuable for researchers working in the related areas of information extraction and spoken language
understanding.
309
Scott Wen-tau Yih received his PhD in Computer Science from the University of Illinois at Urbana-Champaign
in 2005 and is currently a Post-Doc Researcher in the Machine Learning and Applied Statistics group at Mi-
crosoft Research. His research focuses on different problems in natural language processing and machine
learning, such as information extraction and semantic parsing. Scott has published several papers on seman-
tic role labeling in CoNLL-04&05, COLING-04 and IJCAI-05. The SRL system he built at UIUC was the
best system in the CoNLL-05 shared task.
Kristina Toutanova obtained her PhD in Computer Science from Stanford University in 2005 and joined
Microsoft Research as a Researcher in the Natural Language Processing group. Her areas of expertise
include semantic role labeling, syntactic parsing, machine learning, and machine translation. Kristina has
published two papers on semantic role labeling in CoNLL-05 and ACL-05. The SRL system she built at
Stanford was the runner-up system in the CoNLL-05 shared task.
310
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 513?520,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improved Discriminative Bilingual Word Alignment
Robert C. Moore Wen-tau Yih Andreas Bode
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,scottyhi,abode}@microsoft.com
Abstract
For many years, statistical machine trans-
lation relied on generative models to pro-
vide bilingual word alignments. In 2005,
several independent efforts showed that
discriminative models could be used to
enhance or replace the standard genera-
tive approach. Building on this work,
we demonstrate substantial improvement
in word-alignment accuracy, partly though
improved training methods, but predomi-
nantly through selection of more and bet-
ter features. Our best model produces the
lowest alignment error rate yet reported on
Canadian Hansards bilingual data.
1 Introduction
Until recently, almost all work in statistical ma-
chine translation was based on word alignments
obtained from combinations of generative prob-
abalistic models developed at IBM by Brown et
al. (1993), sometimes augmented by an HMM-
based model or Och and Ney?s ?Model 6? (Och
and Ney, 2003). In 2005, however, several in-
dependent efforts (Liu et al, 2005; Fraser and
Marcu, 2005; Ayan et al, 2005; Taskar et al,
2005; Moore, 2005; Ittycheriah and Roukos,
2005) demonstrated that discriminatively trained
models can equal or surpass the alignment accu-
racy of the standard models, if the usual unla-
beled bilingual training corpus is supplemented
with human-annotated word alignments for only
a small subset of the training data.
The work cited above makes use of various
training procedures and a wide variety of features.
Indeed, whereas it can be difficult to design a fac-
torization of a generative model that incorporates
all the desired information, it is relatively easy to
add arbitrary features to a discriminative model.
We take advantage of this, building on our ex-
isting framework (Moore, 2005), to substantially
reduce the alignment error rate (AER) we previ-
ously reported, given the same training and test
data. Through a careful choice of features, and
modest improvements in training procedures, we
obtain the lowest error rate yet reported for word
alignment of Canadian Hansards data.
2 Overall Approach
As in our previous work (Moore, 2005), we train
two models we call stage 1 and stage 2, both in
the form of a weighted linear combination of fea-
ture values extracted from a pair of sentences and
a proposed word alignment of them. The possible
alignment having the highest overall score is se-
lected for each sentence pair. Thus, for a sentence
pair (e, f) we seek the alignment a? such that
a? = argmaxa
n
?
i=1
?ifi(a, e, f)
where the fi are features and the ?i are weights.
The models are trained on a large number of bilin-
gual sentence pairs, a small number of which
have hand-created word alignments provided to
the training procedure. A set of hand alignments
of a different subset of the overall training corpus
is used to evaluate the models.
In the stage 1 model, all the features are based
on surface statistics of the training data, plus the
hypothesized alignment. The entire training cor-
pus is then automatically aligned using this model.
The stage 2 model uses features based not only
on the parallel sentences themselves but also on
statistics of the alignments produced by the stage
513
1 model. The stage 1 model is discussed in Sec-
tion 3, and the stage 2 model, in Section 4. After
experimenting with many features and combina-
tions of features, we made the final selection based
on minimizing training set AER.
For alignment search, we use a method nearly
identical to our previous beam search procedure,
which we do not discuss in detail. We made two
minor modifications to handle the possiblity that
more than one alignment may have the same score,
which we previously did not take into account.
First, we modified the beam search so that the
beam size dynamically expands if needed to ac-
comodate all the possible alignments that have the
same score. Second we implemented a structural
tie breaker, so that the same alignment will always
be chosen as the one-best from a set of alignments
having the same score. Neither of these changes
significantly affected the alignment results.
The principal training method is an adaptation
of averaged perceptron learning as described by
Collins (2002). The differences between our cur-
rent and earlier training methods mainly address
the observation that perceptron training is very
sensitive to the order in which data is presented to
the learner. We also investigated the large-margin
training technique described by Tsochantaridis et
al. (2004). The training procedures are described
in Sections 5 and 6.
3 Stage 1 Model
In our previous stage 1 model, we used five fea-
tures. The most informative feature was the sum
of bilingual word-association scores for all linked
word pairs, computed as a log likelihood ratio. We
used two features to measure the degree of non-
monotonicity of alignments, based on traversing
the alignment in the order of the source sentence
tokens, and noting the instances where the corre-
sponding target sentence tokens were not in left-
to-right order. One feature counted the number of
times there was a backwards jump in the order of
the target sentence tokens, and the other summed
the magnitudes of these jumps. In order to model
the trade-off between one-to-one and many-to-one
alignments, we included a feature that counted the
number of alignment links such that one of the
linked words participated in another link. Our fifth
feature was the count of the number of words in
the sentence pair left unaligned.
In addition to these five features, we employed
two hard constraints. One constraint was that the
only alignment patterns allowed were 1?1, 1?2, 1?
3, 2?1, and 3?1. Thus, many-to-many link pat-
terns were disallowed, and a single word could be
linked to at most three other words. The second
constraint was that a possible link was considered
only if it involved the strongest degree of associ-
ation within the sentence pair for at least one of
the words to be linked. If both words had stronger
associations with other words in the sentence pair,
then the link was disallowed.
Our new stage 1 model includes all the features
we used previously, plus the constraint on align-
ment patterns. The constraint involving strongest
association is not used. In addition, our new stage
1 model employs the following features:
association score rank features We define the
rank of an association with respect to a word in a
sentence pair to be the number of association types
(word-type to word-type) for that word that have
higher association scores, such that words of both
types occur in the sentence pair. The contraint on
strength of association we previously used can be
stated as a requirement that no link be considered
unless the corresponding association is of rank 0
for at least one of the words. We replace this hard
constraint with two features based on association
rank. One feature totals the sum of the associa-
tion ranks with respect to both words involved in
each link. The second feature sums the minimum
of association ranks with respect to both words in-
volved in each link. For alignments that obey the
previous hard constraint, the value of this second
feature would always be 0.
jump distance difference feature In our origi-
nal models, the only features relating to word or-
der were those measuring nonmonotonicity. The
likelihoods of various forward jump distances
were not modeled. If alignments are dense
enough, measuring nonmonotonicity gets at this
indirectly; if every word is aligned, it is impossible
to have large forward jumps without correspond-
ingly large backwards jumps, because something
has to link to the words that are jumped over. If
word alignments are sparse, however, due to free
translation, it is possible to have alignments with
very different forward jumps, but the same back-
wards jumps. To differentiate such alignments,
we introduce a feature that sums the differences
between the distance between consecutive aligned
514
source words and the distance between the closest
target words they are aligned to.
many-to-one jump distance features It seems
intuitive that the likelihood of a large forward
jump on either the source or target side of an align-
ment is much less if the jump is between words
that are both linked to the same word of the other
language. This motivates the distinction between
the d1 and d>1 parameters in IBM Models 4 and 5.
We model this by including two features. One fea-
ture sums, for each word w, the number of words
not linked to w that fall between the first and last
words linked to w. The other features counts only
such words that are linked to some word other than
w. The intuition here is that it is not so bad to have
a function word not linked to anything, between
two words linked to the same word.
exact match feature We have a feature that
sums the number of words linked to identical
words. This is motivated by the fact that proper
names or specialized terms are often the same in
both languages, and we want to take advantage of
this to link such words even when they are too rare
to have a high association score.
lexical features Taskar et al (2005) gain con-
siderable benefit by including features counting
the links between particular high frequency words.
They use 25 such features, covering all pairs of
the five most frequent non-punctuation words in
each language. We adopt this type of feature but
do so more agressively. We include features for
all bilingual word pairs that have at least two co-
occurrences in the labeled training data. In addi-
tion, we include features counting the number of
unlinked occurrences of each word having at least
two occurrences in the labeled training data.
In training our new stage 1 model, we were con-
cerned that using so many lexical features might
result in overfitting to the training data. To try to
prevent this, we train the stage 1 model by first op-
timizing the weights for all other features, then op-
timizing the weights for the lexical features, with
the other weights held fixed to their optimium val-
ues without lexical features.
4 Stage 2 Model
In our original stage 2 model, we replaced the log-
likelihood-based word association statistic with
the logarithm of the estimated conditional prob-
ability of a cluster of words being linked by the
stage 1 model, given that they co-occur in a
pair of aligned sentences, computed over the full
(500,000 sentence pairs) training data. We esti-
mated these probabilities using a discounted max-
imum likelihood estimate, in which a small fixed
amount was subtracted from each link count:
LPd(w1, . . . , wk) =
links1(w1, . . . , wk)? d
cooc(w1, . . . , wk)
LPd(w1, . . . , wk) represents the estimated condi-
tional link probability for the cluster of words
w1, . . . , wk; links1(w1, . . . , wk) is the number of
times they are linked by the stage 1 model, d is
the discount; and cooc(w1, . . . , wk) is the number
of times they co-occur. We found that d = 0.4
seemed to minimize training set AER.
An important difference between our stage 1
and stage 2 models is that the stage 1 model con-
siders each word-to-word link separately, but al-
lows multiple links per word, as long as they lead
to an alignment consisting only of one-to-one and
one-to-many links (in either direction). The stage
2 model, however, uses conditional probabilities
for both one-to-one and one-to-many clusters, but
requires all clusters to be disjoint. Our original
stage 2 model incorporated the same addtional fea-
tures as our original stage 1 model, except that the
feature that counts the number of links involved in
non-one-to-one link clusters was omitted.
Our new stage 2 model differs in a number of
ways from the original version. First we replace
the estimated conditional probability of a cluster
of words being linked with the estimated condi-
tional odds of a cluster of words being linked:
LO(w1, . . . , wk) =
links1(w1, . . . , wk) + 1
(cooc(w1, . . . , wk)? links1(w1, . . . , wk)) + 1
LO(w1, . . . , wk) represents the estimated con-
ditional link odds for the cluster of words
w1, . . . , wk. Note that we use ?add-one? smooth-
ing in place of a discount.
Additional features in our new stage 2 model in-
clude the unaligned word feature used previously,
plus the following features:
symmetrized nonmonotonicity feature We
symmetrize the previous nonmonontonicity fea-
ture that sums the magnitude of backwards jumps,
by averaging the sum of of backwards jumps in
the target sentence order relative to the source
515
sentence order, with the sum of the backwards
jumps in the source sentence order relative to the
target sentence order. We omit the feature that
counts the number of backwards jumps.
multi-link feature This feature counts the num-
ber of link clusters that are not one-to-one. This
enables us to model whether the link scores for
these clusters are more or less reliable than the link
scores for one-to-one clusters.
empirically parameterized jump distance fea-
ture We take advantage of the stage 1 alignment
to incorporate a feature measuring the jump dis-
tances between alignment links that are more so-
phisticated than simply measuring the difference
in source and target distances, as in our stage 1
model. We measure the (signed) source and target
distances between all pairs of links in the stage 1
alignment of the full training data. From this, we
estimate the odds of each possible target distance
given the corresponding source distance:
JO(dt|ds) =
C(target dist = dt ? source dist = ds) + 1
C(target dist 6= dt ? source dist = ds) + 1
We similarly estimate the odds of each possi-
ble source distance given the corresponding target
distance. The feature values consist of the sum
of the scaled log odds of the jumps between con-
secutive links in a hypothesized alignment, com-
puted in both source sentence and target sentence
order. This feature is applied only when both the
source and target jump distances are non-zero, so
that it applies only to jumps between clusters, not
to jumps on the ?many? side of a many-to-one
cluster. We found it necessary to linearly scale
these feature values in order to get good results (in
terms of training set AER) when using perceptron
training.1 We found empirically that we could get
good results in terms of training set AER by divid-
ing each log odds estimate by the largest absolute
value of any such estimate computed.
5 Perceptron Training
We optimize feature weights using a modification
of averaged perceptron learning as described by
Collins (2002). Given an initial set of feature
weight values, the algorithm iterates through the
1Note that this is purely for effective training, since after
training, one could adjust the feature weights according to the
scale factor, and use the original feature values.
labeled training data multiple times, comparing,
for each sentence pair, the best alignment ahyp ac-
cording to the current model with the reference
alignment aref . At each sentence pair, the weight
for each feature is is incremented by a multiple of
the difference between the value of the feature for
the best alignment according to the model and the
value of the feature for the reference alignment:
?i ? ?i + ?(fi(aref , e, f)? fi(ahyp, e, f))
The updated feature weights are used to compute
ahyp for the next sentence pair. The multiplier ?
is called the learning rate. In the averaged percep-
tron, the feature weights for the final model are
the average of the weight values over all the data
rather than simply the values after the final sen-
tence pair of the final iteration.
Differences between our approach and Collins?s
include averaging feature weights over each pass
through the data, rather than over all passes; ran-
domizing the order of the data for each learn-
ing pass; and performing an evaluation pass af-
ter each learning pass, with feature weights fixed
to their average values for the preceding learning
pass, during which training set AER is measured.
This procedure is iterated until a local minimum
on training set AER is found.
We initialize the weight of the anticipated most-
informative feature (word-association scores in
stage 1; conditional link probabilities or odds in
stage 2) to 1.0, with other feature weights intial-
ized to 0. The weight for the most informative fea-
ture is not updated. Allowing all weights to vary
allows many equivalent sets of weights that differ
only by a constant scale factor. Fixing one weight
eliminates a spurious apparent degree of freedom.
Previously, we set the learning rate ? differently
in training his stage 1 and stage 2 models. For the
stage 2 model, we used a single learning rate of
0.01. For the stage 1 model, we used a sequence
of learning rates: 1000, 100, 10, and 1.0. At each
transition between learning rates, we re-initialized
the feature weights to the optimum values found
with the previous learning rate.
In our current work, we make a number of mod-
ifications to this procedure. We reset the feature
weights to the best averaged values we have yet
seen at the begining of each learning pass through
the data. Anecdotally, this seems to result in faster
convergence to a local AER minimum. We also
use multiple learning rates for both the stage 1 and
516
stage 2 models, setting the learning rates automat-
ically. The initial learning rate is the maximum ab-
solute value (for one word pair/cluster) of the word
association, link probability, or link odds feature,
divided by the number of labeled training sentence
pairs. Since many of the feature values are simple
counts, this allows a minimal difference of 1 in
the feature value, if repeated in every training ex-
ample, to permit a count feature to have as large
a weighted value as the most informative feature,
after a single pass through the data.
After the learning search terminates for a given
learning rate, we reduce the learning rate by a fac-
tor of 10, and iterate until we judge that we are at
a local minimum for this learning rate. We con-
tinue with progressively smaller learning rates un-
til an entire pass through the data produces fea-
ture weights that differ so little from their values
at the beginning of the pass that the training set
AER does not change.
Two final modifications are inspired by the real-
ization that the results of perceptron training are
very sensitive to the order in which the data is
presented. Since we randomize the order of the
data on every pass, if we make a pass through the
training data, and the training set AER increases, it
may be that we simply encountered an unfortunate
ordering of the data. Therefore, when training set
AER increases, we retry two additional times with
the same initial weights, but different random or-
derings of the data, before giving up and trying a
smaller learning rate. Finally, we repeat the entire
training process multiple times, and average the
feature weights resulting from each of these runs.
We currently use 10 runs of each model. This final
averaging is inspired by the idea of ?Bayes-point
machines? (Herbrich and Graepel, 2001).
6 SVM Training
After extensive experiments with perceptron train-
ing, we wanted to see if we could improve the re-
sults obtained with our best stage 2 model by using
a more sophisticated training method. Perceptron
training has been shown to obtain good results for
some problems, but occasionally very poor results
are reported, notably by Taskar et al (2005) for the
word-alignment problem. We adopted the support
vector machine (SVM) method for structured out-
put spaces of Tsochantaridis et al (2005), using
Joachims? SV M struct package.
Like standard SVM learning, this method tries
to find the hyperplane that separates the training
examples with the largest margin. Despite a very
large number of possible output labels (e.g., all
possible alignments of a given pair of sentences),
the optimal hyperplane can be efficiently approx-
imated given the desired error rate, using a cut-
ting plane algorithm. In each iteration of the al-
gorithm, it adds the ?best? incorrect predictions
given the current model as constraints, and opti-
mizes the weight vector subject only to them.
The main advantage of this algorithm is that
it does not pose special restrictions on the out-
put structure, as long as ?decoding? can be done
efficiently. This is crucial to us because sev-
eral features we found very effective in this task
are difficult to incorporate into structured learning
methods that require decomposable features. This
method also allows a variety of loss functions, but
we use only simple 0-1 loss, which in our case
means whether or not the alignment of a sentence
pair is completely correct, since this worked as
well as anything else we tried.
Our SVM method has a number of free param-
eters, which we tried tuning in two different ways.
One way is minimizing training set AER, which
is how we chose the stopping points in perceptron
training. The other is five-fold cross validation.
In this method, we train five times on 80% of the
training data and test on the other 20%, with five
disjoint subsets used for testing. The parameter
values yielding the best averaged AER on the five
test subsets of the training set are used to train the
final model on the entire training set.
7 Evaluation
We used the same training and test data as in our
previous work, a subset of the Canadian Hansards
bilingual corpus supplied for the bilingual word
alignment workshop held at HLT-NAACL 2003
(Mihalcea and Pedersen, 2003). This subset com-
prised 500,000 English-French sentences pairs, in-
cluding 224 manually word-aligned sentence pairs
for labeled training data, and 223 labeled sen-
tences pairs as test data. Automatic sentence
alignment of the training data was provided by Ul-
rich Germann, and the hand alignments of the la-
beled data were created by Franz Och and Her-
mann Ney (Och and Ney, 2003).
For baselines, Table 1 shows the test set re-
sults we previously reported, along with results for
IBM Model 4, trained with Och?s Giza++ software
517
Alignment Recall Precision AER
Prev LLR 0.829 0.848 0.160
CLP1 0.889 0.934 0.086
CLP2 0.898 0.947 0.075
Giza E? F 0.870 0.890 0.118
Giza F? E 0.876 0.907 0.106
Giza union 0.929 0.845 0.124
Giza intersection 0.817 0.981 0.097
Giza refined 0.908 0.929 0.079
Table 1: Baseline Results.
package, using the default configuration file (Och
and Ney, 2003).2 ?Prev LLR? is our earlier stage
1 model, and CLP1 and CLP2 are two versions
of our earlier stage 2 model. For CLP1, condi-
tional link probabilities were estimated from the
alignments produced by our ?Prev LLR? model,
and for CLP2, they were obtained from a yet
earlier, heuristic alignment model. Results for
IBM Model 4 are reported for models trained in
both directions, English-to-French and French-to-
English, and for the union, intersection, and what
Och and Ney (2003) call the ?refined? combina-
tion of the those two alignments.
Results for our new stage 1 model are presented
in Table 2. The first line is for the model described
in Section 3, optimizing non-lexical features be-
fore lexical features. The second line gives results
for optimizing all features simultaneously. The
next line omits lexical features entirely. The last
line is for our original stage 1 model, but trained
using our improved perceptron training method.
As we can see, our best stage 1 model reduces
the error rate of previous stage 1 model by almost
half. Comparing the first two lines shows that two-
phase training of non-lexical and lexical features
produces a 0.7% reduction in test set error. Al-
though the purpose of the two-phase training was
to mitigate overfitting to the training data, we also
found training set AER was reduced (7.3% vs.
8.8%). Taken all together, the results show a 7.9%
total reduction in error rate: 4.0% from new non-
lexical features, 3.3% from lexical features with
two-phase training, and 0.6% from other improve-
ments in perceptron training.
Table 3 presents results for perceptron training
of our new stage 2 model. The first line is for the
model as described in Section 4. Since the use of
log odds is somewhat unusual, in the second line
2Thanks to Chris Quirk for providing Giza++ alignments.
Alignment Recall Precision AER
Two-phase train 0.907 0.928 0.081
One-phase train 0.911 0.912 0.088
No lex feats 0.889 0.885 0.114
Prev LLR (new train) 0.834 0.855 0.154
Table 2: Stage 1 Model Results.
Alignment Recall Precision AER
Log odds 0.935 0.964 0.049
Log probs 0.934 0.962 0.051
CLP1 (new A & T) 0.925 0.952 0.060
CLP1 (new A) 0.917 0.955 0.063
Table 3: Stage 2 Model Results.
we show results for a similiar model, but using log
probabilities instead of log odds for both the link
model and the jump model. This result is 0.2%
worse than the log-odds-based model, but the dif-
ference is small enough to warrant testing its sig-
nificance. Comparing the errors on each test sen-
tence pair with a 2-tailed paired t test, the results
were suggestive, but not significant (p = 0.28)
The third line of Table 3 shows results for our
earlier CLP1 model with probabilities estimated
from our new stage 1 model alignments (?new
A?), using our recent modifications to perceptron
training (?new T?). These results are significantly
worse than either of the two preceding models
(p < 0.0008). The fourth line is for the same
model and stage 1 alignments, but with our earlier
perceptron training method. While the results are
0.3% worse than with our new training method,
the difference is not significant (p = 0.62).
Table 4 shows the results of SVM training of
the model that was best under perceptron training,
tuning free parameters either by minimizing error
on the entire training set or by 5-fold cross val-
idation on the training set. The cross-validation
method produced slightly lower test-set AER, but
both results rounded to 4.7%. While these results
are somewhat better than with perceptron training,
the differences are not significant (p ? 0.47).
8 Comparisons to Other Work
At the time we carried out the experiments de-
scribed above, our sub-5% AER results were the
best we were aware of for word alignment of
Canadian Hansards bilingual data, although direct
comparisons are problematic due to differences in
518
Alignment Recall Precision AER
Min train err 0.941 0.962 0.047
5 ? CV 0.942 0.962 0.047
Table 4: SVM Training Results.
total training data, labeled training data, and test
data. The best previously reported result was by
Och and Ney (2003), who obtained 5.2% AER
for a combination including all the IBM mod-
els except Model 2, plus the HMM model and
their Model 6, together with a bilingual dictionary,
for the refined alignment combination, trained on
three times as much data as we used.
Cherry and Lin?s (2003) method obtained an
AER of 5.7% as reported by Mihalcea and Peder-
sen (2003), the previous lowest reported error rate
for a method that makes no use of the IBM mod-
els. Cherry and Lin?s method is similar to ours
in using explicit estimates of the probability of a
link given the co-occurence of the linked words;
but it is generative rather than discriminative, it re-
quires a parser for the English side of the corpus,
and it does not model many-to-one links. Taskar
et al (2005) reported 5.4% AER for a discrimina-
tive model that includes predictions from the inter-
section of IBM Model 4 alignments as a feature.
Their best result without using information from
the IBM models was 10.7% AER.
After completing the experiments described in
Section 7, we became aware further developments
in the line of research reported by Taskar et al
(Lacoste-Julien et al, 2006). By modifying their
previous approach to allow many-to-one align-
ments and first-order interactions between align-
ments, Lacoste-Julien et al have improved their
best AER without using information from the
more complex IBM models to 6.2%. Their best
result, however, is obtained from a model that in-
cludes both a feature recording intersected IBM
Model 4 predictions, plus a feature whose val-
ues are the alignment probabilities obtained from a
pair of HMM alignment models trained in both di-
rections in such a way that they agree on the align-
ment probabilities (Liang et al, 2006). With this
model, they obtained a much lower 3.8% AER.
Lacoste-Julien very graciously provided both
the IBM Model 4 predictions and the probabili-
ties estimated by the bidirectional HMM models
that they had used to compute these additional fea-
ture values. We then added features based on this
information to see how much we could improve
our best model. We also eliminated one other dif-
ference between our results and those of Lacoste-
Julien et al, by training on all 1.1 million English-
French sentence pairs from the 2003 word align-
ment workshop, rather than the 500,000 sentence
pairs we had been using.
Since all our other feature values derived from
probabilities are expressed as log odds, we also
converted the HMM probabilities estimated by
Liang et al to log odds. To make this well de-
fined in all cases, we thresholded high probabili-
ties (including 1.0) at 0.999999, and low probabil-
ities (including 0.0) at 0.1 (which we found pro-
duced lower training set error than using a very
small non-zero probability, although we have not
searched systematically for the optimal value).
In our latest experiments, we first established
that simply increasing the unlabled training data
to 1.1 million sentence pairs made very little dif-
ference, reducing the test-set AER of our stage 2
model under perceptron training only from 4.9%
to 4.8%. Combining our stage 2 model features
with the HMM log odds feature using SVM train-
ing with 5-fold cross validation yielded a substan-
tial reduction in test-set AER to 3.9% (96.9% pre-
cision, 95.1% recall). We found it somewhat dif-
ficult to improve these results further by including
IBM Model 4 intersection feature. We finally ob-
tained our best results, however, for both training-
set and test-set AER, by holding the stage 2 model
feature weights at the values obtained by SVM
training with the HMM log odds feature, and op-
timizing the HMM log odds feature weight and
IBM Model 4 intersection feature weight with per-
ceptron training.3 This produced a test-set AER of
3.7% (96.9% precision, 95.5% recall).
9 Conclusions
For Canadian Hansards data, the test-set AER of
4.7% for our stage 2 model is one of the lowest
yet reported for an aligner that makes no use of
the expensive IBM models, and our test-set AER
of 3.7% for the stage 2 model in combination with
the HMM log odds and Model 4 intersection fea-
tures is the lowest yet reported for any aligner.4
Perhaps if any general conclusion is to be drawn
from our results, it is that in creating a discrim-
3At this writing we have not yet had time to try this with
SVM training.
4However, the difference between our result and the 3.8%
of Lacoste-Julien et al is almost certainly not significant.
519
inative word alignment model, the model struc-
ture and features matter the most, with the dis-
criminative training method of secondary impor-
tance. While we obtained a small improvements
by varying the training method, few of the differ-
ences were statistically significant. Having better
features was much more important.
References
Necip Fazil Ayan, Bonnie J. Dorr, and
Christof Monz. 2005. NeurAlign: Combining
Word Alignments Using Neural Networks. In
Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical
Methods in Natural Language Processing,
pp. 65?72, Vancouver, British Columbia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A Proba-
bility Model to Improve Word Alignment. In
Proceedings of the 41st Annual Meeting of the
ACL, pp. 88?95, Sapporo, Japan.
Michael Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory
and Experiments with Perceptron Algorithms.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pp. 1?8, Philadelphia, Pennsylvania.
Alexander Fraser and Daniel Marcu. 2005. ISI?s
Participation in the Romanian-English Align-
ment Task. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts,
pp. 91?94, Ann Arbor, Michigan.
Ralf Herbrich and Thore Graepel. 2001. Large
Scale Bayes Point Machines Advances. In
Neural Information Processing Systems 13,
pp. 528?534.
Abraham Ittycheriah and Salim Roukos. 2005. A
Maximum Entropy Word Aligner for Arabic-
English Machine Translation. In Proceedings
of the Human Language Technology Conference
and Conference on Empirical Methods in Nat-
ural Language Processing, pp. 89?96, Vancou-
ver, British Columbia.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2006. Word Alignment via
Quadratic Assignment. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, pp. 112?119, New
York City.
Percy Liang, Ben Taskar, and Dan Klein. 2006.
Alignment by Agreement. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, pp. 104?111, New
York City.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear Models for Word Alignment. In Proceed-
ings of the 43rd Annual Meeting of the ACL,
pp. 459?466, Ann Arbor, Michigan.
Rada Mihalcea and Ted Pedersen. 2003. An Eval-
uation Exercise for Word Alignment. In Pro-
ceedings of the HLT-NAACL 2003 Workshop,
Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond, pp. 1?6, Ed-
monton, Alberta.
Robert C. Moore. 2005. A Discriminative Frame-
work for Bilingual Word Alignment. In Pro-
ceedings of the Human Language Technology
Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pp. 81?
88, Vancouver, British Columbia.
Franz Joseph Och and Hermann Ney. 2003. A
Systematic Comparison of Various Statistical
Alignment Models. Computational Linguistics,
29(1):19?51.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A Discriminative Matching Approach
to Word Alignment. In Proceedings of the
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pp. 73?80, Vancouver,
British Columbia.
Ioannis Tsochantaridis, Thomas Hofmann,
Thorsten Joachims, and Yasemin Altun. 2005.
Large Margin Methods for Structured and
Interdependent Output Variables. Journal
of Machine Learning Research (JMLR),
pp. 1453?1484.
520
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 6?7,
Vancouver, October 2005.
Demonstrating an Interactive Semantic Role Labeling System
Vasin Punyakanok Dan Roth Mark Sammons
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{punyakan,danr,mssammon}@uiuc.edu
Wen-tau Yih
Microsoft Research
Redmond, WA 98052, USA
scottyih@microsoft.com
Abstract
Semantic Role Labeling (SRL) is the task
of performing a shallow semantic analy-
sis of text (i.e., Who did What to Whom,
When, Where, How). This is a cru-
cial step toward deeper understanding of
text and has many immediate applications.
Preprocessed information on text, mostly
syntactic, has been shown to be impor-
tant for SRL. Current research focuses on
improving the performance assuming that
this lower level information is given with-
out any attention to the overall efficiency
of the final system, although minimizing
execution time is a necessity in order to
support real world applications. The goal
of our demonstration is to present an inter-
active SRL system that can be used both
as a research and an educational tool. Its
architecture is based on the state-of-the-
art system (the top system in the 2005
CoNLL shared task), modified to process
raw text through the addition of lower
level processors, while achieving effective
real time performance.
1 Introduction
Semantic parsing of sentences is believed to be an
important subtask toward natural language under-
standing, and has immediate applications in tasks
such information extraction and question answering.
We study semantic role labeling (SRL), defined as
follows: for each verb in a sentence, the goal is to
identify all constituents that fill a semantic role, and
to determine their roles (such as Agent, Patient or In-
strument) and their adjuncts (such as Locative, Tem-
poral or Manner). The PropBank project (Kingsbury
and Palmer, 2002), which provides a large human-
annotated corpus of semantic verb-argument rela-
tions, has opened doors for researchers to apply ma-
chine learning techniques to this task.
The focus of the research has been on improving
the performance of the SRL system by using, in ad-
dition to raw text, various syntactic and semantic in-
formation, e.g. Part of Speech (POS) tags, chunks,
clauses, syntactic parse tree, and named entities,
which is found crucial to the SRL system (Pun-
yakanok et al, 2005).
In order to support a real world application such
as an interactive question-answering system, the
ability of an SRL system to analyze text in real time
is a necessity. However, in previous research, the
overall efficiency of the SRL system has not been
considered. At best, the efficiency of an SRL sys-
tem may be reported in an experiment assuming that
all the necessary information has already been pro-
vided, which is not realistic. A real world scenario
requires the SRL system to perform all necessary
preprocessing steps in real time. The overall effi-
ciency of SRL systems that include the preproces-
sors is not known.
Our demonstration aims to address this issue. We
present an interactive system that performs the SRL
task from raw text in real time. Its architecture is
based on the top system in the 2005 CoNLL shared
task (Koomen et al, 2005), modified to process raw
text using lower level processors but maintaining
6
good real time performance.
2 The SRL System Architecture
Our system begins preprocessing raw text by
using sentence segmentation tools (available at
http://l2r.cs.uiuc.edu/?cogcomp/tools.php). Next,
sentences are analyzed by a state-of-the-art syntac-
tic parser (Charniak, 2000) the output of which pro-
vides useful information for the main SRL module.
The main SRL module consists of four stages:
pruning, argument identification, argument classifi-
cation, and inference. The following is the overview
of these four stages. Details of them can be found
in (Koomen et al, 2005).
Pruning The goal of pruning is to filter out un-
likely argument candidates using simple heuristic
rules. Only the constituents in the parse tree are
considered as argument candidates. In addition, our
system exploits a heuristic modified from that intro-
duced by (Xue and Palmer, 2004) to filter out very
unlikely constituents.
Argument Identification The argument identifi-
cation stage uses binary classification to identify
whether a candidate is an argument or not. We train
and apply the binary classifiers on the constituents
supplied by the pruning stage.
Argument Classification This stage assigns the
final argument labels to the argument candidates
supplied from the previous stage. A multi-class clas-
sifier is trained to classify the types of the arguments
supplied by the argument identification stage.
Inference The purpose of this stage is to incor-
porate some prior linguistic and structural knowl-
edge, such as ?arguments do not overlap? and ?each
verb takes at most one argument of each type.? This
knowledge is used to resolve any inconsistencies in
argument classification in order to generate legiti-
mate final predictions. The process is formulated as
an integer linear programming problem that takes as
input confidence values for each argument type sup-
plied by the argument classifier for each constituent,
and outputs the optimal solution subject to the con-
straints that encode the domain knowledge.
The system in this demonstration, however, dif-
fers from its original version in several aspects.
First, all syntactic information is extracted from the
output of the full parser, where the original version
used different information obtained from different
processors. Second, the named-entity information is
discarded. Finally, no combination of different parse
tree outputs is performed. These alterations aim to
enhance the efficiency of the system while maintain-
ing strong performance.
Currently the system runs at the average speed of
1.25 seconds/predicate. Its performance is 77.88 and
65.87 F1-score on WSJ and Brown test sets (Car-
reras and Ma`rquez, 2005) while the original system
achieves 77.11 and 65.6 on the same test sets with-
out the combination of multiple parser outputs and
79.44 and 67.75 with the combination.
3 Goal of Demonstration
The goal of the demonstration is to present the sys-
tem?s ability to perform the SRL task on raw text in
real time. An interactive interface allows users to in-
put free form text and to receive the SRL analysis
from our system. This demonstration can be found
at http://l2r.cs.uiuc.edu/?cogcomp/srl-demo.php.
Acknowledgments
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA?s AQUAINT Program, DOI?s Re-
flex program, and an ONR MURI Award.
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
conll-2005 shared tasks: Semantic role labeling. In
Proc. of CoNLL-2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL 2000.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proc. of LREC-2002, Spain.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005.
Generalized Inference with Multiple Semantic Role
Labeling Systems. In Proceedings of CoNLL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of IJCAI-2005.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proc. of the EMNLP-2004.
7
Probabilistic Reasoning for Entity & Relation Recognition?
Dan Roth Wen-tau Yih
Department of Computer Science
University of Illinois at Urbana-Champaign
{danr, yih}@uiuc.edu
Abstract
This paper develops a method for recognizing rela-
tions and entities in sentences, while taking mutual
dependencies among them into account. E.g., the kill
(Johns, Oswald) relation in: ?J. V. Oswald was
murdered at JFK after his assassin,
K. F. Johns...? depends on identifying Oswald
and Johns as people, JFK being identified as a location,
and the kill relation between Oswald and Johns; this, in
turn, enforces that Oswald and Johns are people.
In our framework, classifiers that identify entities and
relations among them are first learned from local infor-
mation in the sentence; this information, along with con-
straints induced among entity types and relations, is used
to perform global inference that accounts for the mutual
dependencies among the entities.
Our preliminary experimental results are promising
and show that our global inference approach improves
over learning relations and entities separately.
1 Introduction
Recognizing and classifying entities and relations in text
data is a key task in many NLP problems such as in-
formation extraction (IE) (Califf and Mooney, 1999;
Freitag, 2000; Roth and Yih, 2001), question an-
swering (QA) (Voorhees, 2000) and story comprehen-
sion (Hirschman et al, 1999). In a typical IE application
of constructing a jobs database from unstructured text,
the system has to extract meaningful entities like title and
salary and, ideally, to determine whether the entities are
associated with the same position. In a QA system, many
questions ask for specific entities involved in some rela-
tions. For example, the question ?Where was Poe born??
in TREC-9 asks for the location entity in which Poe was
born. The question ?Who killed Lee Harvey Oswald??
seeks a person entity that has the relation kill with the
person Lee Harvey Oswald.
In all earlier works we know of, the tasks of identify-
ing entities and relations were treated as separate prob-
lems. The common procedure is to first identify and clas-
sify entities using a named entity recognizer and only
? Research supported by NSF grants CAREER IIS-9984168 and ITR
IIS-0085836 and an ONR MURI Award.
then determine the relations between the entities. How-
ever, this approach has several problems. First, errors
made by the named entity recognizer propagate to the
relation classifier and may degrade its performance sig-
nificantly. For example, if ?Boston? is mislabeled as a
person, it will never be classified as the location of Poe?s
birthplace. Second, relation information is sometimes
crucial to resolving ambiguous named entity recognition.
For instance, if the entity ?JFK? is identified as the vic-
tim of the assassination, the named entity recognizer is
unlikely to misclassify it as a location (e.g. JFK airport).
This paper develops a novel approach for this prob-
lem ? a probabilistic framework for recognizing entities
and relations together. In this framework, separate clas-
sifiers are first trained for entities and relations. Their
output is used to represent a conditional distribution for
each entity and relation, given the observed data. This
information, along with constraints induced among rela-
tions and entities (e.g. the first argument of kill is likely
to be a person; the second argument of born in is a lo-
cation) are used to make global inferences for the most
probable assignment for all entities and relations of in-
terest. Our global inference approach accepts as input
conditional probabilities which are the outcomes of ?lo-
cal? classifiers. Note that each of the local classifiers
could depend on a large number of features, but these
are not viewed as relevant to the inference process and
are abstracted away in this process of ?inference with
classifiers?. In this sense, this work extends previous
works in this paradigm, such as (Punyakanok and Roth,
2001), in which inference with classifiers was studied
when the outcomes of the classifiers were sequentially
constrained; here the constraints are more general, which
necessitates a different inference approach.
The rest of the paper is organized as follows. Sec-
tion 2 defines the problem in a formal way. Section 3
describes our approach to this problem. It first intro-
duces how we learn the classifiers, and then introduces
the belief network we use to reason for global predic-
tions. Section 4 records preliminary experiments we ran
and exhibits some promising results. Finally, section 5
discusses some of the open problems and future work in
this framework.
E1
R31
SpellingPOS
...Label
Label-1Label-2
...Label-n
E2 E3
R32R32
R23R12
R13
Figure 1: Conceptual view of entities and relations
2 Global Inference of Entities/Relations
The problem at hand is that of producing a coherent la-
beling of entities and relations in a given sentence. Con-
ceptually, the entities and relations can be viewed, tak-
ing into account the mutual dependencies, as the labeled
graph in Figure 1, where the nodes represent entities
(e.g. phrases) and the links denote the binary relations
between the entities. Each entity and relation has sev-
eral properties ? denoted as labels of nodes and edges in
the graph. Some of the properties, such as words inside
the entities, can be read directly from the input; others,
like pos tags of words in the context of the sentence, are
easy to acquire via learned classifiers. However, proper-
ties like semantic types of phrases (i.e., class labels, such
as ?people?, ?locations?) and relations among them are
more difficult to acquire. Identifying the labels of entities
and relations is treated here as the target of our learning
problem. In particular, we learn these target properties
as functions of all other ?simple to acquire? properties of
the sentence.
To describe the problem in a formal way, we first de-
fine sentences and entities as follows.
Definition 2.1 (Sentence & Entity) A sentence S is a
linked list which consists of words w and entities E. An
entity can be a single word or a set of consecutive words
with a predefined boundary. Entities in a sentence are
labeled as E1, E2, ? ? ? according to their order, and they
take values that range over a set of entity types CE .
Notice that determining the entity boundaries is also
a difficult problem ? the segmentation (or phrase de-
tection) problem (Abney, 1991; Punyakanok and Roth,
2001). Here we assume it is solved and given to us as
input; thus we only concentrate on classification.
Example 2.1 The sentence in Figure 2 has three enti-
ties: E1 = ?Dole?, E2 = ?Elizabeth?, and E3 = ?Sal-
isbury, N.C.?
Dole ?s wife , Elizabeth , is a native of Salisbury , N.C.
 E1         E2                E3
Figure 2: A sentence that has three entities
A relation is defined by the entities that are involved in
it (its arguments). In this paper, we only discuss binary
relations.
Definition 2.2 (Relation) A (binary) relation Rij =
(Ei, Ej) represents the relation between Ei and Ej ,
where Ei is the first argument and Ej is the second. In
addition, Rij can range over a set of entity types CR.
Example 2.2 In the sentence given in Figure 2, there are
six relations between the entities: R12 = (?Dole?, ?Eliz-
abeth?), R21 = (?Elizabeth?, ?Dole?), R13 = (?Dole?,
?Salisbury, N.C.?), R31 = (?Salisbury, N.C.?, ?Dole?),
R23 = (?Elizabeth?, ?Salisbury, N.C.?), and R32 =
(?Salisbury, N.C.?, ?Elizabeth?)
We define the types (i.e. classes) of relations and enti-
ties as follows.
Definition 2.3 (Classes) We denote the set of predefined
entity classes and relation classes as CE and CR respec-
tively. CE has one special element other ent, which rep-
resents any unlisted entity class. Similarly, CR also has
one special element other rel, which means the involved
entities are irrelevant or the relation class is undefined.
When clear from the context, we use Ei and Rij to refer
to the entity and relation, as well as their types (class
labels).
Example 2.3 Suppose CE = { other ent, person, lo-
cation } and CR = { other rel, born in, spouse of }.
For the entities in Figure 2, E1 and E2 belong to person
and E3 belongs to location. In addition, relation R23 is
born in, R12 and R21 are spouse of. Other relations are
other rel.
The class label of a single entity or relation depends
not only on its local properties, but also on properties
of other entities and relations. The classification task is
somewhat difficult since the predictions of entity labels
and relation labels are mutually dependent. For instance,
the class label of E1 depends on the class label of R12
and the class label of R12 also depends on the class la-
bel of E1 and E2. While we can assume that all the
data is annotated for training purposes, this cannot be
assumed at evaluation time. We may presume that some
local properties such as the word, pos, etc. are given, but
none of the class labels for entities or relations is.
To simplify the complexity of the interaction within
the graph but still preserve the characteristic of mutual
dependency, we abstract this classification problem in the
following probabilistic framework. First, the classifiers
are trained independently and used to estimate the proba-
bilities of assigning different labels given the observation
(that is, the easily classified properties in it). Then, the
output of the classifiers is used as a conditional distribu-
tion for each entity and relation, given the observation.
This information, along with the constraints among the
relations and entities, is used to make global inferences
for the most probable assignment of types to the entities
and relations involved.
The class labels of entities and relations in a sentence
must satisfy some constraints. For example, if E1, the
first argument of R12, is a location, then R12 cannot be
born in because the first argument of relation born in has
to be a person. We define constraints as follows.
Definition 2.4 (Constraint) A constraint C is a 3-tuple
(R, E1, E2), where R ? CR and E1, E2 ? CE . If the
class label of a relation is R, then the legitimate class
labels of its two entity arguments are E1 and E2 respec-
tively.
Example 2.4 Some examples of constraints are:
(born in, person, location), (spouse of, person, person),
and (murder, person, person)
The constraints described above could be modeled us-
ing a joint probability distribution over the space of val-
ues of the relevant entities and relations. In the context of
this work, for algorithmic reasons, we model only some
of the conditional probabilities. In particular, the proba-
bility P (Rij |Ei, Ej) has the following properties.
Property 1 The probability of the label of relation Rij
given the labels of its arguments Ei and Ej has the fol-
lowing properties.
? P (Rij = other rel|Ei = e1, Ej = e2) = 1, if there
exists no r, such that (r, e1, e2) is a constraint.
? P (Rij = r|Ei = e1, Ej = e2) = 0, if there exists
no constraint c, such that c = (r, e1, e2).
Note that the conditional probabilities do not need to
be specified manually. In fact, they can be easily learned
from an annotated training dataset.
Under this framework, finding the most suitable
coherent labels becomes the problem of searching
the most probable assignment to all the E and R
variables. In other words, the global prediction
e1, e2, ..., en, r12, r21, ..., rn(n?1) satisfies the following
equation.
(e1, ..., en, r12, r21, ..., rn(n?1)) =
argmaxei,rjkProb(E1, ..., En, R12, R21, ..., Rn(n?1)).
3 Computational Approach
Each nontrivial property of the entities and relations,
such as the class label, depends on a very large number
of variables. In order to predict the most suitable co-
herent labels, we would like to make inferences on sev-
eral variables. However, when modeling the interaction
between the target properties, it is crucial to avoid ac-
counting for dependencies among the huge set of vari-
ables on which these properties depend. Incorporating
these dependencies into our inference is unnecessary and
will make the inference intractable. Instead, we can ab-
stract these dependencies away by learning the proba-
bility of each property conditioned upon an observation.
The number of features on which this learning problem
depends could be huge, and they can be of different gran-
ularity and based on previous learned predicates (e.g.
pos), as caricatured using the ?network-like? structure in
Figure 1. Inference is then made based on the probabili-
ties. This approach is similar to (Punyakanok and Roth,
2001; Lafferty et al, 2001) only that there it is restricted
to sequential inference, and done for syntactic structures.
The following subsections describe the details of these
two stages. Section 3.1 explains the feature extraction
method and learning algorithm we used. Section 3.2 in-
troduces the idea of using a belief network in search of
the best global class labeling and the applied inference
algorithm.
3.1 Learning Basic Classifiers
Although the labels of entities and relations from a sen-
tence mutually depend on each other, two basic classi-
fiers for entities and relations are first learned, in which
a multi-class classifier for E(or R) is learned as a func-
tion of all other ?known? properties of the observation.
The classifier for entities is a named entity classifier, in
which the boundary of an entity is predefined (Collins
and Singer, 1999). On the other hand, the relation clas-
sifier is given a pair of entities, which denote the two
arguments of the target relation. Accurate predictions of
these two classifiers seem to rely on complicated syntax
analysis and semantics related information of the whole
sentence. However, we derive weak classifiers by treat-
ing these two learning tasks as shallow text processing
problems. This strategy has been successfully applied on
several NLP tasks, such as information extraction (Califf
and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001)
and chunking (i.e. shallow paring) (Munoz et al, 1999).
It assumes that the class labels can be decided by lo-
cal properties, such as the information provided by the
words around or inside the target. Examples include
the spelling of a word, part-of-speech, and semantic re-
lated attributes acquired from external resources such as
WordNet.
The propositional learner we use is SNoW (Roth,
1998; Carleson et al, 1999) 1 SNoW is a multi-class clas-
sifier that is specifically tailored for large scale learning
tasks. The learning architecture makes use of a network
of linear functions, in which the targets (entity classes
or relation classes, in this case) are represented as linear
1available at http://L2R.cs.uiuc.edu/?cogcomp/cc-software.html
functions over a common feature space. Within SNoW,
we use here a learning algorithm which is a variation of
Winnow (Littlestone, 1988), a feature efficient algorithm
that is suitable for learning in NLP-like domains, where
the number of potential features is very large, but only a
few of them are active in each example, and only a small
fraction of them are relevant to the target concept.
While typically SNoW is used as a classifier, and pre-
dicts using a winner-take-all mechanism over the activa-
tion value of the target classes, here we rely directly on
the raw activation value it outputs, which is the weighted
linear sum of the features, to estimate the posteriors.
It can be verified that the resulting values are mono-
tonic with the confidence in the prediction, therefore is
a good source of probability estimation. We use softmax
(Bishop, 1995) over the raw activation values as proba-
bilities. Specifically, suppose the number of classes is n,
and the raw activation values of class i is acti. The pos-
terior estimation for class i is derived by the following
equation.
pi = e
acti
?
1?j?n eactj
3.2 Bayesian Inference Model
Broadly used in the AI community, belief network
is a graphical representation of a probability distri-
bution (Pearl, 1988). It is a directed acyclic graph
(DAG), where the nodes are random variables and
each node is associated with a conditional probabil-
ity table which defines the probability given its par-
ents. We construct a belief network that represents
the constraints existing among R?s and E?s. Then,
for each sentence, we use the classifiers from sec-
tion 3.1 to compute the Prob(E|observations) and
Prob(R|observations), and use the belief network to
compute the most probable global predictions of the class
labels.
The structure of our belief network, which represents
the constraints is a bipartite graph. In particular, the vari-
able E?s and R?s are the nodes in the network, where the
E nodes are in one layer, and the R nodes are in the other.
Since the label of a relation is dependent on the entity
classes of its arguments, the links in the network connect
the entity nodes, and the relation nodes that have these
entities as arguments. For instance, node Rij has two
incoming links from nodes Ei and Ej . The conditional
probabilities P (Rij |Ei, Ej) encodes the constraints as in
Property 1. As an illustration, Figure 3 shows a be-
lief network that consists of 3 entity nodes and 6 relation
nodes.
Finding a most probable class assignment to the en-
tities and relations is equivalent to finding the assign-
ment of all the variables in the belief network that
maximizes the joint probability. However, this most-
probable-explanation (MPE) inference problem is in-
tractable (Roth, 1996) if the network contains loops
E2
E1
E3
R12
R21
R13
R31
R23
R32
P(R12|X)
P(R21|X)
P(R13|X)
P(R31|X)
P(R23|X)
P(R32|X)
P(E1|X)
P(E2|X)
P(E3|X)
Figure 3: Belief network of 3 entity nodes and 6 relation
nodes
(undirected cycles), which is exactly the case in our net-
work. Therefore, we resort to the following approxima-
tion method instead.
Recently, researchers have achieved great success in
solving the problem of decoding messages through a
noisy channel with the help of belief networks (Gal-
lager, 1962; MacKay, 1999). The network structure used
in their problem is similar to the network used here,
namely a loopy bipartite DAG. The inference algorithm
they used is Pearl?s belief propagation algorithm (Pearl,
1988), which outputs exact posteriors in linear time if the
network is singly connected (i.e. without loops) but does
not guarantee to converge for loopy networks. However,
researchers have empirically demonstrate that by iterat-
ing the belief propagation algorithm several times, the
outputted values often converge to the right posteriors
(Murphy et al, 1999). Due to the existence of loops, we
also apply belief propagation algorithm iteratively as our
inference procedure.
4 Experiments
The following subsections describe the data preparation
process, the approaches tested in the experiments, and
the experimental results.
4.1 Data Preparation
In order to build different datasets, we first collected sen-
tences from TREC documents, which are mostly daily
news such as Wall Street Journal, Associated Press, and
San Jose Mercury News. Among the collected sentences,
245 sentences contain relation kill (i.e. two entities that
have the murder-victim relation). 179 sentences contain
relation born in (i.e. a pair of entities where the second
is the birthplace of the first). In addition to the above
sentences, we also collected 502 sentences that contain
no relations.2
2available at http://l2r.cs.uiuc.edu/?cogcomp/Data/ER/
Entities in these sentences are segmented by the sim-
ple rule: consecutive proper nouns and commas are com-
bined and treated as an entity. Predefined entity class la-
bels include other ent, person, and location. Moreover,
relations are defined by every pair of entities in a sen-
tence, and the relation class labels defined are other rel,
kill, and birthplace.
Three datasets are constructed using the collected sen-
tences. Dataset ?kill? has all the 245 sentences of re-
lation kill. Dataset ?born in? has all the 179 sentences
of relation born in. The third dataset ?all? mixes all the
sentences.
4.2 Tested Approaches
We compare three approaches in the experiments: basic,
omniscient, and BN. The first approach, basic, tests our
baseline ? the performance of the basic classifiers. As
described in Section 3.1, these classifiers are learned in-
dependently using local features and make predictions on
entities and relations separately. Without taking global
interactions into account, the features extracted are de-
scribed as follows. For the entity classifier, features from
the words around each entity are: words, tags, conjunc-
tions of words and tags, bigram and trigram of words and
tags. Features from the entity itself include the number
of words it contains, bigrams of words in it, and some
attributes of the words inside such as the prefix and suf-
fix. In addition, whether the entity has some strings that
match the names of famous people and places is also
used as a feature. For the relation classifier, features are
extracted from words around and between the two en-
tity arguments. The types of features include bigrams,
trigrams, words, tags, and words related to ?kill? and
?birth? retrieved from WordNet.
The second approach, omniscient, is similar to basic.
The only difference here is the labels of entities are re-
vealed to the R classifier and vice versa. It is certainly
impossible to know the true entity and relation labels in
advance. However, this experiment may give us some
ideas about how much the performance of the entity clas-
sifier can be enhanced by knowing whether the target is
involved in some relations, and also how much the rela-
tion classifier can be benefited from knowing the entity
labels of its arguments. In addition, it also provides a
comparison to see how well the belief network inference
model can improve the results.
The third approach, BN, tests the ability of making
global inferences in our framework. We use the Bayes
Net Toolbox for Matlab by Murphy 3 to implement the
network and set the maximum number of the iteration of
belief propagation algorithm as 20. Given the probabili-
ties estimated by basic classifiers, the network infers the
labels of the entities and relations globally in a sentence.
Compared to the first two approaches, where some pre-
dictions may violate the constraints, the belief network
model incorporates the constraints between entities and
3available at http://www.cs.berkeley.edu/?murphyk/Bayes/bnt.html
relations, thus all the predictions it makes will be coher-
ent.
All the experiments of these approaches are done in
5-fold validation. In other words, these datasets are ran-
domly separated into 5 disjoint subsets, and experiments
are done 5 times by iteratively using 4 of them as training
data and the rest as testing.
4.3 Results
The experimental results in terms of recall, precision,and
F?=1 for datasets ?kill?, ?born in?, and ?all? are given
in Table 1, Table 2, and Table 3 respectively. We discuss
two interesting facts of the results as follows.
First, the belief network approach tends to decrease re-
call in a small degree but increase precision significantly.
This phenomenon is especially clear on the classification
results of some relations. As a result, the F1 value of
the relation classification results is still enhanced to the
extent that is near or even higher than the results of the
Omniscient approach. This may be explained by the fact
that if the label of a relation is predicted as positive (i.e.
not other rel), the types of its entity arguments must sat-
isfy the constraints. This inference process reduces the
number of false positive, thus enhance the precision.
Second, knowing the class labels of relations does not
seem to help the entity classifier much. In all three
datasets, the difference of Basic and Omniscient ap-
proaches is usually less than 3% in terms of F1, which
is not very significant given the size of our datasets. This
phenomenon may be due to the fact that only a few of en-
tities in a sentence are involved in some relations. There-
fore, it is unlikely that the entity classifier can use the
relation information to correct its prediction.
Approach person location
Rec Prec F1 Rec Prec F1
Basic 96.6 92.3 94.4 76.3 91.9 83.1
BN 89.0 96.1 92.4 78.8 86.3 82.1
Omniscient 96.4 92.6 94.5 75.4 90.2 81.9
Approach kill
Rec Prec F1
Basic 61.8 57.2 58.6
BN 49.8 85.4 62.2
Omniscient 67.7 63.6 64.8
Table 1: Results for dataset ?kill?
5 Discussion
The promising results of our preliminary experiments
demonstrate the feasibility of our probabilistic frame-
work. For the future work, we plan to extend this re-
search in the following directions.
The first direction we would like to explore is to apply
our framework in a boot-strapping manner. The main dif-
ficulty in applying learning on NLP problems is not lack
of text corpus, but lack of labeled data. Boot-strapping,
applying the classifiers to autonomously annotate the
Approach person location
Rec Prec F1 Rec Prec F1
Basic 85.5 90.7 87.8 89.5 93.2 91.1
BN 87.0 90.9 88.8 87.5 93.4 90.3
Omniscient 90.6 93.4 91.7 90.7 96.5 93.4
Approach born in
Rec Prec F1
Basic 81.4 63.4 70.9
BN 87.6 70.7 78.0
Omniscient 86.9 71.8 78.0
Table 2: Results for dataset ?born in?
Approach person location
Rec Prec F1 Rec Prec F1
Basic 92.1 87.0 89.4 83.2 81.1 82.0
BN 78.8 94.7 86.0 83.0 81.3 82.1
Omniscient 93.4 87.3 90.2 83.5 83.1 83.2
Approach kill born in
Rec Prec F1 Rec Prec F1
Basic 43.8 78.6 55.0 69.0 72.9 70.5
BN 47.2 86.8 60.7 68.4 87.5 76.6
Omniscient 52.8 79.5 62.1 76.1 71.3 73.2
Table 3: Results for dataset ?all?
data and using the new data to train and improve exist-
ing classifiers, is a promising approach. Since the pre-
cision of our framework is pretty high, it seems possible
to use the global inference to annotate new data. Based
on this property, we can derive an EM-like approach for
labelling and inferring the types of entities and relations
simultaneously. The basic idea is to use the global infer-
ence output as a means to annotate entities and relations.
The new annotated data can then be used to train classi-
fiers, and the whole process is repeated again.
The second direction is to improve our probabilistic
inference model in several ways. First, since the results
of the inference procedure we use, the loopy belief prop-
agation algorithm, produces approximate values, some
of the results can be wrong. Although the computational
time of the exact inference algorithm for loopy network
is exponential, we may still be able to run it given the
small number of variables that are of interest each time
in our case. Therefore, we can further check if the perfor-
mance suffers from the approximation. Second, the be-
lief network model may not be expressive enough since
it allows no cycles. To fully model the problem, cycles
may be needed. For example, the class labels of R12
and R21 actually depend on each other. (e.g. If R12 is
born in, then R21 will not be born in or kill.) Similarly,
the class labels of E1 and E2 can depend on the labels
of R12. To fully represent the mutual dependencies, we
would like to explore other probabilistic models that are
more expressive than the belief network.
References
S. P. Abney. 1991. Parsing by chunks. In S. P. Abney
R. C. Berwick and C. Tenny, editors, Principle-based
parsing: Computation and Psycholinguistics, pages
257?278. Kluwer, Dordrecht.
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
M. Califf and R. Mooney. 1999. Relational learning of
pattern-match rules for information extraction. In Na-
tional Conference on Artificial Intelligence.
A. Carleson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science De-
partment, May.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for name entity classification. In EMNLP-VLC?99,
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, June.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learning,
39(2/3):169?202.
R. Gallager. 1962. Low density parity check codes. IRE
Trans. Info. Theory, IT-8:21?28, Jan.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep read: A reading comprehension system. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the
International Conference on Machine Learning.
N. Littlestone. 1988. Learning quickly when irrelevant
attributes abound: A new linear-threshold algorithm.
Machine Learning, 2:285?318.
D. MacKay. 1999. Good error-correcting codes based
on very sparse matrices. IEEE Transactions on Infor-
mation Theory, 45.
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In
EMNLP-VLC?99, the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, June.
K. Murphy, Y. Weiss, and M. Jordan. 1999. Loopy belief
propagation for approximate inference: An empirical
study. In Proc. of Uncertainty in AI, pages 467?475.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems. Morgan Kaufmann.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems.
D. Roth and W. Yih. 2001. Relational learning via
propositional algorithms: An information extraction
case study. In Proc. of the International Joint Con-
ference on Artificial Intelligence, pages 1257?1263.
D. Roth. 1996. On the hardness of approximate reason-
ing. Artificial Inteligence, 82(1-2):273?302, April.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. National Con-
ference on Artificial Intelligence, pages 806?813.
E. Voorhees. 2000. Overview of the trec-9 question an-
swering track. In The Ninth Text Retrieval Conference
(TREC-9), pages 71?80. NIST SP 500-249.
Semantic Role Labeling via Integer Linear Programming Inference
Vasin Punyakanok Dan Roth Wen-tau Yih Dav Zimak
Department of Computer Science
University of Illinois at Urbana-Champaign
{punyakan,danr,yih,davzimak}@uiuc.edu
Abstract
We present a system for the semantic role la-
beling task. The system combines a machine
learning technique with an inference procedure
based on integer linear programming that sup-
ports the incorporation of linguistic and struc-
tural constraints into the decision process. The
system is tested on the data provided in CoNLL-
2004 shared task on semantic role labeling and
achieves very competitive results.
1 Introduction
Semantic parsing of sentences is believed to be an
important task toward natural language understand-
ing, and has immediate applications in tasks such
information extraction and question answering. We
study semantic role labeling(SRL). For each verb in
a sentence, the goal is to identify all constituents
that fill a semantic role, and to determine their roles,
such as Agent, Patient or Instrument, and their ad-
juncts, such as Locative, Temporal or Manner.
The PropBank project (Kingsbury and Palmer,
2002) provides a large human-annotated corpus
of semantic verb-argument relations. Specifically,
we use the data provided in the CoNLL-2004
shared task of semantic-role labeling (Carreras and
Ma`rquez, 2003) which consists of a portion of the
PropBank corpus, allowing us to compare the per-
formance of our approach with other systems.
Previous approaches to the SRL task have made
use of a full syntactic parse of the sentence in or-
der to define argument boundaries and to determine
the role labels (Gildea and Palmer, 2002; Chen and
Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2003; Pradhan et al, 2004; Sur-
deanu et al, 2003). In this work, following the
CoNLL-2004 shared task definition, we assume that
the SRL system takes as input only partial syn-
tactic information, and no external lexico-semantic
knowledge bases. Specifically, we assume as input
resources a part-of-speech tagger, a shallow parser
that can process the input to the level of based
chunks and clauses (Tjong Kim Sang and Buch-
holz, 2000; Tjong Kim Sang and De?jean, 2001),
and a named-entity recognizer (Tjong Kim Sang
and De Meulder, 2003). We do not assume a full
parse as input.
SRL is a difficult task, and one cannot expect
high levels of performance from either purely man-
ual classifiers or purely learned classifiers. Rather,
supplemental linguistic information must be used
to support and correct a learning system. So far,
machine learning approaches to SRL have incorpo-
rated linguistic information only implicitly, via the
classifiers? features. The key innovation in our ap-
proach is the development of a principled method to
combine machine learning techniques with linguis-
tic and structural constraints by explicitly incorpo-
rating inference into the decision process.
In the machine learning part, the system we
present here is composed of two phases. First, a
set of argument candidates is produced using two
learned classifiers?one to discover beginning po-
sitions and one to discover end positions of each
argument type. Hopefully, this phase discovers a
small superset of all arguments in the sentence (for
each verb). In a second learning phase, the candi-
date arguments from the first phase are re-scored
using a classifier designed to determine argument
type, given a candidate argument.
Unfortunately, it is difficult to utilize global prop-
erties of the sentence into the learning phases.
However, the inference level it is possible to in-
corporate the fact that the set of possible role-
labelings is restricted by both structural and lin-
guistic constraints?for example, arguments cannot
structurally overlap, or, given a predicate, some ar-
gument structures are illegal. The overall decision
problem must produce an outcome that consistent
with these constraints. We encode the constraints as
linear inequalities, and use integer linear program-
ming(ILP) as an inference procedure to make a fi-
nal decision that is both consistent with the con-
straints and most likely according to the learning
system. Although ILP is generally a computation-
ally hard problem, there are efficient implementa-
tions that can run on thousands of variables and con-
straints. In our experiments, we used the commer-
cial ILP package (Xpress-MP, 2003), and were able
to process roughly twenty sentences per second.
2 Task Description
The goal of the semantic-role labeling task is to dis-
cover the verb-argument structure for a given input
sentence. For example, given a sentence ? I left my
pearls to my daughter-in-law in my will?, the goal is
to identify different arguments of the verb left which
yields the output:
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-
in-law] [AM-LOC in my will].
Here A0 represents the leaver, A1 represents the
thing left, A2 represents the benefactor, AM-LOC
is an adjunct indicating the location of the action,
and V determines the verb.
Following the definition of the PropBank, and
CoNLL-2004 shared task, there are six different
types of arguments labelled as A0-A5 and AA.
These labels have different semantics for each verb
as specified in the PropBank Frame files. In addi-
tion, there are also 13 types of adjuncts labelled as
AM-XXX where XXX specifies the adjunct type.
In some cases, an argument may span over differ-
ent parts of a sentence, the label C-XXX is used to
specify the continuity of the arguments, as shown in
the example below.
[A1 The pearls] , [A0 I] [V said] , [C-A1 were left
to my daughter-in-law].
Moreover in some cases, an argument might be a
relative pronoun that in fact refers to the actual agent
outside the clause. In this case, the actual agent is la-
beled as the appropriate argument type, XXX, while
the relative pronoun is instead labeled as R-XXX.
For example,
[A1 The pearls] [R-A1 which] [A0 I] [V left] , [A2
to my daughter-in-law] are fake.
See the details of the definition in Kingsbury and
Palmer (2002) and Carreras and Ma`rquez (2003).
3 System Architecture
Our semantic role labeling system consists of two
phases. The first phase finds a subset of arguments
from all possible candidates. The goal here is to
filter out as many as possible false argument candi-
dates, while still maintaining high recall. The sec-
ond phase focuses on identifying the types of those
argument candidates. Since the number of candi-
dates is much fewer, the second phase is able to use
slightly complicated features to facilitate learning
a better classifier. This section first introduces the
learning system we use and then describes how we
learn the classifiers in these two phases.
3.1 SNoW Learning Architecture
The learning algorithm used is a variation of the
Winnow update rule incorporated in SNoW (Roth,
1998; Roth and Yih, 2002), a multi-class classifier
that is specifically tailored for large scale learning
tasks. SNoW learns a sparse network of linear func-
tions, in which the targets (argument border predic-
tions or argument type predictions, in this case) are
represented as linear functions over a common fea-
ture space. It incorporates several improvements
over the basic Winnow multiplicative update rule.
In particular, a regularization term is added, which
has the effect of trying to separate the data with a
thick separator (Grove and Roth, 2001; Hang et al,
2002). In the work presented here we use this regu-
larization with a fixed parameter.
Experimental evidence has shown that SNoW
activations are monotonic with the confidence in
the prediction. Therefore, it can provide a good
source of probability estimation. We use soft-
max (Bishop, 1995) over the raw activation values
as conditional probabilities, and also the score of the
target. Specifically, suppose the number of classes
is n, and the raw activation values of class i is acti.
The posterior estimation for class i is derived by the
following equation.
score(i) = pi = e
acti
?
1?j?n eactj
The score plays an important role in different
places. For example, the first phase uses the scores
to decide which argument candidates should be fil-
tered out. Also, the scores output by the second-
phase classifier are used in the inference procedure
to reason for the best global labeling.
3.2 First Phase: Find Argument Candidates
The first phase is to predict the argument candidates
of a given sentence that correspond to the active
verb. Unfortunately, it turns out that it is difficult to
predict the exact arguments accurately. Therefore,
the goal here is to output a superset of the correct
arguments by filtering out unlikely candidates.
Specifically, we learn two classifiers, one to de-
tect beginning argument locations and the other
to detect end argument locations. Each multi-
class classifier makes predictions over forty-three
classes?thirty-two argument types, ten continuous
argument types, and one class to detect not begin-
ning/not end. Features used for these classifiers are:
? Word feature includes the current word, two
words before and two words after.
? Part-of-speech tag (POS) feature includes the
POS tags of all words in a window of size two.
? Chunk feature includes the BIO tags for
chunks of all words in a window of size two.
? Predicate lemma & POS tag show the lemma
form and POS tag of the active predicate.
? Voice feature is the voice (active/passive) of
the current predicate. This is extracted with a
simple rule: a verb is identified as passive if it
follows a to-be verb in the same phrase chunk
and its POS tag is VBN(past participle) or it
immediately follows a noun phrase.
? Position feature describes if the current word
is before or after the predicate.
? Chunk pattern encodes the sequence of
chunks from the current words to the predicate.
? Clause tag indicates the boundary of clauses.
? Clause path feature is a path formed from a
semi-parsed tree containing only clauses and
chunks. Each clause is named with the chunk
preceding it. The clause path is the path from
predicate to target word in the semi-parse tree.
? Clause position feature is the position of the
target word relative to the predicate in the
semi-parse tree containing only clauses. There
are four configurations ? target word and pred-
icate share the same parent, target word parent
is an ancestor of predicate, predicate parent is
an ancestor of target word, or otherwise.
Because each argument consists of a single be-
ginning and a single ending, these classifiers can be
used to construct a set of potential arguments (by
combining each predicted begin with each predicted
end after it of the same type).
Although this phase identifies typed arguments
(i.e. labeled with argument types), the second phase
will re-score each phrase using phrase-based classi-
fiers ? therefore, the goal of the first phase is sim-
ply to identify non-typed phrase candidates. In this
task, we achieves 98.96% and 88.65% recall (over-
all, without verb) on the training and the develop-
ment set, respectively. Because these are the only
candidates passed to the second phase, the final sys-
tem performance is upper-bounded by 88.65%.
3.3 Second Phase: Argument Classification
The second phase of our system assigns the final ar-
gument classes to (a subset) of the argument can-
didates supplied from the first phase. Again, the
SNoW learning architecture is used to train a multi-
class classifier to label each argument to one of the
argument types, plus a special class?no argument
(null). Training examples are created from the argu-
ment candidates supplied from the first phase using
the following features:
? Predicate lemma & POS tag, voice, position,
clause Path, clause position, chunk pattern
Same features as those in the first phase.
? Word & POS tag from the argument, includ-
ing the first,last,and head1 word and tag.
? Named entity feature tells if the target argu-
ment is, embeds, overlaps, or is embedded in a
named entity with its type.
? Chunk tells if the target argument is, embeds,
overlaps, or is embedded in a chunk with its
type.
? Lengths of the target argument, in the numbers
of words and chunks separately.
? Verb class feature is the class of the active
predicate described in PropBank Frames.
? Phrase type uses simple heuristics to identify
the target argument as VP, PP, or NP.
? Sub-categorization describes the phrase
structure around the predicate. We separate
the clause where the predicate is in into three
parts?the predicate chunk, segments before
and after the predicate, and use the sequence
of phrase types of these three segments.
? Baseline features identified not in the main
verb chunk as AM-NEG and modal verb in the
main verb chunk as AM-MOD.
? Clause coverage describes how much of the
local clause (from the predicate) is covered by
the target argument.
? Chunk pattern length feature counts the num-
ber of patterns in the argument.
? Conjunctions join every pair of the above fea-
tures as new features.
? Boundary words & POS tag include two
words/tags before and after the target argu-
ment.
? Bigrams are pairs of words/tags in the window
from two words before the target to the first
word of the target, and also from the last word
to two words after the argument.
1We use simple rules to first decide if a candidate phrase
type is VP, NP, or PP. The headword of an NP phrase is the
right-most noun. Similarly, the left-most verb/proposition of a
VP/PP phrase is extracted as the headword
? Sparse collocation picks one word/tag from
the two words before the argument, the first
word/tag, the last word/tag of the argument,
and one word/tag from the two words after the
argument to join as features.
Although the predictions of the second-phase
classifier can be used directly, the labels of argu-
ments in a sentence often violate some constraints.
Therefore, we rely on the inference procedure to
make the final predictions.
4 Inference via ILP
Ideally, if the learned classifiers are perfect, argu-
ments can be labeled correctly according to the clas-
sifiers? predictions. In reality, labels assigned to ar-
guments in a sentence often contradict each other,
and violate the constraints arising from the struc-
tural and linguistic information. In order to resolve
the conflicts, we design an inference procedure that
takes the confidence scores of each individual ar-
gument given by the second-phase classifier as in-
put, and outputs the best global assignment that
also satisfies the constraints. In this section we first
introduce the constraints and the inference prob-
lem in the semantic role labeling task. Then, we
demonstrate how we apply integer linear program-
ming(ILP) to reason for the global label assignment.
4.1 Constraints over Argument Labeling
Formally, the argument classifier attempts to assign
labels to a set of arguments, S1:M , indexed from 1
to M . Each argument Si can take any label from a
set of argument labels, P , and the indexed set of
arguments can take a set of labels, c1:M ? PM .
If we assume that the classifier returns a score,
score(Si = ci), corresponding to the likelihood of
seeing label ci for argument Si, then, given a sen-
tence, the unaltered inference task is solved by max-
imizing the overall score of the arguments,
c?1:M = argmax
c1:M?PM
score(S1:M = c1:M )
= argmax
c1:M?PM
M?
i=1
score(Si = ci).
(1)
In the presence of global constraints derived from
linguistic information and structural considerations,
our system seeks for a legitimate labeling that max-
imizes the score. Specifically, it can be viewed as
the solution space is limited through the use of a fil-
ter function, F , that eliminates many argument la-
belings from consideration. It is interesting to con-
trast this with previous work that filters individual
phrases (see (Carreras and Ma`rquez, 2003)). Here,
we are concerned with global constraints as well as
constraints on the arguments. Therefore, the final
labeling becomes
c?1:M = argmax
c1:M?F(PM )
M?
i=1
score(Si = ci) (2)
The filter function used considers the following con-
straints:
1. Arguments cannot cover the predicate except
those that contain only the verb or the verb and
the following word.
2. Arguments cannot overlap with the clauses
(they can be embedded in one another).
3. If a predicate is outside a clause, its arguments
cannot be embedded in that clause.
4. No overlapping or embedding arguments.
5. No duplicate argument classes for A0?A5,V.
6. Exactly one V argument per verb.
7. If there is C-V, then there should be a sequence
of consecutive V, A1, and C-V pattern. For ex-
ample, when split is the verb in ?split it up?,
the A1 argument is ?it? and C-V argument is
?up?.
8. If there is an R-XXX argument, then there has
to be an XXX argument. That is, if an ar-
gument is a reference to some other argument
XXX, then this referenced argument must exist
in the sentence.
9. If there is a C-XXX argument, then there has
to be an XXX argument; in addition, the C-
XXX argument must occur after XXX. This is
stricter than the previous rule because the order
of appearance also needs to be considered.
10. Given the predicate, some argument classes
are illegal (e.g. predicate ?stalk? can take only
A0 or A1). This linguistic information can be
found in PropBank Frames.
We reformulate the constraints as linear
(in)equalities by introducing indicator variables.
The optimization problem (Eq. 2) is solved using
ILP.
4.2 Using Integer Linear Programming
As discussed previously, a collection of potential ar-
guments is not necessarily a valid semantic label-
ing since it must satisfy all of the constraints. In
this context, inference is the process of finding the
best (according to Equation 1) valid semantic labels
that satisfy all of the specified constraints. We take
a similar approach that has been previously used
for entity/relation recognition (Roth and Yih, 2004),
and model this inference procedure as solving an
ILP.
An integer linear program(ILP) is basically the
same as a linear program. The cost function and the
(in)equality constraints are all linear in terms of the
variables. The only difference in an ILP is the vari-
ables can only take integers as their values. In our
inference problem, the variables are in fact binary.
A general binary integer programming problem can
be stated as follows.
Given a cost vector p ? <d, a set of variables,
z = (z1, . . . , zd) and cost matrices C1 ? <t1 ?
<d,C2 ? <t2?<d , where t1 and t2 are the numbers
of inequality and equality constraints and d is the
number of binary variables. The ILP solution z? is
the vector that maximizes the cost function,
z? = argmax
z?{0,1}d
p ? z,
subject to C1z ? b1, and C2z = b2,
where b1,b2 ? <d, and for all z ? z, z ? {0, 1}.
To solve the problem of Equation 2 in this set-
ting, we first reformulate the original cost function?M
i=1 score(Si = ci) as a linear function over sev-
eral binary variables, and then represent the filter
function F using linear inequalities and equalities.
We set up a bijection from the semantic labeling
to the variable set z. This is done by setting z to a set
of indicator variables. Specifically, let zic = [Si =
c] be the indicator variable that represents whether
or not the argument type c is assigned to Si, and
let pic = score(Si = c). Equation 1 can then be
written as an ILP cost function as
argmax
z?{0,1}d
M?
i=1
|P|?
c=1
piczic,
subject to
|P|?
c=1
zic = 1 ?zic ? z,
which means that each argument can take only one
type. Note that this new constraint comes from the
variable transformation, and is not one of the con-
straints used in the filter function F .
Constraints 1 through 3 can be evaluated on a per-
argument basis ? the sake of efficiency, arguments
that violate these constraints are eliminated even
before given the second-phase classifier. Next, we
show how to transform the constraints in the filter
function into the form of linear (in)equalities over
z, and use them in this ILP setting.
Constraint 4: No overlapping or embedding If
arguments Sj1 , . . . , Sjk occupy the same word in a
sentence, then this constraint restricts only one ar-
guments to be assigned to an argument type. In
other words, k ? 1 arguments will be the special
class null, which means the argument candidate is
not a legitimate argument. If the special class null
is represented by the symbol ?, then for every set of
such arguments, the following linear equality repre-
sents this constraint.
k?
i=1
zji? = k ? 1
Constraint 5: No duplicate argument classes
Within the same sentence, several types of argu-
ments cannot appear more than once. For example,
a predicate can only take one A0. This constraint
can be represented using the following inequality.
M?
i=1
ziA0 ? 1
Constraint 6: Exactly one V argument For each
verb, there is one and has to be one V argument,
which represents the active verb. Similarly, this con-
straint can be represented by the following equality.
M?
i=1
ziV = 1
Constraint 7: V?A1?C-V pattern This con-
straint is only useful when there are three consec-
utive candidate arguments in a sentence. Suppose
arguments Sj1 , Sj2 , Sj3 are consecutive. If Sj3 is
C-V, then Sj1 and Sj2 have to be V and A1, respec-
tively. This if-then constraint can be represented by
the following two linear inequalities.
zj3C-V ? zj1V, and zj3C-V ? zj2A1
Constraint 8: R-XXX arguments Suppose the
referenced argument type is A0 and the reference
type is R-A0. The linear inequalities that represent
this constraint are:
?m ? {1, . . . ,M} :
M?
i=1
ziA0 ? zmR-A0
If there are ? reference argument pairs, then the
total number of inequalities needed is ?M .
Constraint 9: C-XXX arguments This con-
straint is similar to the reference argument con-
straints. The difference is that the continued argu-
ment XXX has to occur before C-XXX. Assume
that the argument pair is A0 and C-A0, and argu-
ment Sji appears before Sjk if i ? k. The linear
inequalities that represent this constraint are:
?m ? {2, . . . ,M} :
j?1?
i=1
zjiA0 ? zmR-A0
Constraint 10: Illegal argument types Given a
specific verb, some argument types should never oc-
cur. For example, most verbs don?t have arguments
A5. This constraint is represented by summing all
the corresponding indicator variables to be 0.
M?
i=1
ziA5 = 0
Using ILP to solve this inference problem en-
joys several advantages. Linear constraints are
very general, and are able to represent many types
of constraints. Previous approaches usually rely
on dynamic programming to resolve non over-
lapping/embedding constraints (i.e., Constraint 4)
when the data is sequential, but are unable to han-
dle other constraints. The ILP approach is flexible
enough to handle constraints regardless of the struc-
ture of the data. Although solving an ILP prob-
lem is NP-hard, with the help of todays commer-
cial numerical packages, this problem can usually
be solved very fast in practice. For instance, it only
takes about 10 minutes to solve the inference prob-
lem for 4305 sentences on a Pentium-III 800 MHz
machine in our experiments. Note that ordinary
search methods (e.g., beam search) are not neces-
sarily faster than solving an ILP problem and do not
guarantee the optimal solution.
5 Experimental Results
The system is evaluated on the data provided in
the CoNLL-2004 semantic-role labeling shared task
which consists of a portion of PropBank corpus.
The training set is extracted from TreeBank (Mar-
cus et al, 1993) section 15?18, the development set,
used in tuning parameters of the system, from sec-
tion 20, and the test set from section 21.
We first compare this system with the basic tagger
that we have, the CSCL shallow parser from (Pun-
yakanok and Roth, 2001), which is equivalent to us-
ing the scoring function from the first phase with
only the non-overlapping/embedding constraints. In
Prec. Rec. F?=1
1st-phase, non-overlap 70.54 61.50 65.71
1st-phase, All Const. 70.97 60.74 65.46
2nd-phase, non-overlap 69.69 64.75 67.13
2nd-phase, All Const. 71.96 64.93 68.26
Table 1: Summary of experiments on the development
set. All results are for overall performance.
Precision Recall F?=1
Without Inference 86.95 87.24 87.10
With Inference 88.03 88.23 88.13
Table 2: Results of second phase phrase prediction
and inference assuming perfect boundary detection in
the first phase. Inference improves performance by re-
stricting label sequences rather than restricting structural
properties since the correct boundaries are given. All re-
sults are for overall performance on the development set.
addition, we evaluate the effectiveness of using only
this constraint versus all constraints, as in Sec. 4.
Table 1 shows how additional constraints over the
standard non-overlapping constraints improve per-
formance on the development set. The argument
scoring is chosen from either the first phase or the
second phase and each is evaluated by considering
simply the non-overlapping/embedding constraint
or the full set of linguistic constraints. To make
a fair comparison, parameters were set separately
to optimize performance when using the first phase
results. In general, using all constraints increases
F?=1 by about 1% in this system, but slightly de-
creases the performance when only the first phase
classifier is used. Also, using the two-phase archi-
tecture improves both precision and recall, and the
enhancement reflected in F?=1 is about 2.5%.
It is interesting to find out how well the second
phase classifier can perform given perfectly seg-
mented arguments. This evaluates the quality of the
argument classifier, and also provides a conceptual
upper bound. Table 2 first shows the results without
using inference (i.e. F(PM ) = PM ). The second
row shows adding inference to the phrase classifica-
tion can further improve F?=1 by 1%.
Finally, the overall result on the official test set
is given in Table 3. Note that the result here is not
comparable with the best in this domain (Pradhan et
al., 2004) where the full parse tree is assumed given.
For a fair comparison, our system was among the
best at CoNLL-04, where the best system (Hacioglu
et al, 2004) achieve a 69.49 F1 score.
6 Conclusion
We show that linguistic information is useful for se-
mantic role labeling, both in extracting features and
Dist. Prec. Rec. F?=1
Overall 100.00 70.07 63.07 66.39
A0 26.87 81.13 77.70 79.38
A1 35.73 74.21 63.02 68.16
A2 7.44 54.16 41.04 46.69
A3 1.56 47.06 26.67 34.04
A4 0.52 71.43 60.00 65.22
AM-ADV 3.20 39.36 36.16 37.69
AM-CAU 0.51 45.95 34.69 39.53
AM-DIR 0.52 42.50 34.00 37.78
AM-DIS 2.22 52.00 67.14 58.61
AM-EXT 0.15 46.67 50.00 48.28
AM-LOC 2.38 33.47 34.65 34.05
AM-MNR 2.66 45.19 36.86 40.60
AM-MOD 3.51 92.49 94.96 93.70
AM-NEG 1.32 85.92 96.06 90.71
AM-PNC 0.89 32.79 23.53 27.40
AM-TMP 7.78 59.77 56.89 58.30
R-A0 1.66 81.33 76.73 78.96
R-A1 0.73 58.82 57.14 57.97
R-A2 0.09 100.00 22.22 36.36
R-AM-TMP 0.15 54.55 42.86 48.00
Table 3: Results on the test set.
deriving hard constraints on the output. We also
demonstrate that it is possible to use integer linear
programming to perform inference that incorporates
a wide variety of hard constraints, which would be
difficult to incorporate using existing methods. In
addition, we provide further evidence supporting
the use of scoring arguments over scoring argument
boundaries for complex tasks. In the future, we plan
to use the full PropBank corpus to see the improve-
ment when more training data is provided. In addi-
tion, we would like to explore the possibility of in-
teger linear programming approach using soft con-
straints. As more constraints are considered, we ex-
pect the overall performance to improve.
7 Acknowledgments
We thank Xavier Carreras and Llu??s Ma`rquez for the
data and scripts, Martha Palmer and the anonymous
referees for their useful comments, AMD for their
equipment donation, and Dash Optimization for the
free academic use of their Xpress-MP software.
This research is supported by NSF grants ITR-IIS-
0085836, ITR-IIS-0085980 and IIS-9984168, EIA-
0224453 and an ONR MURI Award.
References
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
X. Carreras and L. Ma`rquez. 2003. Phrase recognition
by filtering and ranking with perceptrons. In Proc. of
RANLP-2003.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In Proc. of EMNLP-2003, Sapporo, Japan.
D. Gildea and J. Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar.
In Proc. of the EMNLP-2003, Sapporo, Japan.
D. Gildea and M. Palmer. 2002. The necessity of parsing
for predicate argument recognition. In Proc. of ACL
2002, pages 239?246, Philadelphia, PA.
A. Grove and D. Roth. 2001. Linear concepts and hid-
den variables. Machine Learning, 42(1/2):123?141.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2004. Semantic role labeling by tagging
syntactic chunks. In Proc. of CoNLL-04.
T. Hang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. J. of
Machine Learning Research, 2:615?637.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proc. of LREC-2002, Spain.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330, June.
S. Pradhan, K. Hacioglu, W. ward, J. Martin, and D. Ju-
rafsky. 2003. Semantic role parsing adding semantic
structure to unstructured text. In Proc. of ICDM-2003,
Melbourne, FL.
S. Pradhan, W. Ward, K. Hacioglu, J. H. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proc. of NAACL-HLT
2004.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems, pages 995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for
entity & relation recognition. In Proc. of COLING-
2002, pages 835?841.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CoNLL-2004.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. of AAAI, pages
806?813.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL 2003.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In
Proc. of the CoNLL-2000 and LLL-2000.
E. F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL-2003.
E. F. Tjong Kim Sang and H. De?jean. 2001. Introduction
to the CoNLL-2001 shared task: Clause identification.
In Proc. of the CoNLL-2001.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
A Linear Programming Formulation for Global Inference in Natural
Language Tasks
Dan Roth Wen-tau Yih
Department of Computer Science
University of Illinois at Urbana-Champaign
{danr, yih}@uiuc.edu
Abstract
Given a collection of discrete random variables
representing outcomes of learned local predic-
tors in natural language, e.g., named entities
and relations, we seek an optimal global as-
signment to the variables in the presence of
general (non-sequential) constraints. Examples
of these constraints include the type of argu-
ments a relation can take, and the mutual activ-
ity of different relations, etc. We develop a lin-
ear programming formulation for this problem
and evaluate it in the context of simultaneously
learning named entities and relations. Our ap-
proach allows us to efficiently incorporate do-
main and task specific constraints at decision
time, resulting in significant improvements in
the accuracy and the ?human-like? quality of
the inferences.
1 Introduction
Natural language decisions often depend on the out-
comes of several different but mutually dependent predic-
tions. These predictions must respect some constraints
that could arise from the nature of the data or from do-
main or task specific conditions. For example, in part-of-
speech tagging, a sentence must have at least one verb,
and cannot have three consecutive verbs. These facts can
be used as constraints. In named entity recognition, ?no
entities can overlap? is a common constraint used in var-
ious works (Tjong Kim Sang and De Meulder, 2003).
Efficient solutions to problems of these sort have been
given when the constraints on the predictors are sequen-
tial (Dietterich, 2002). These solutions can be cate-
gorized into the following two frameworks. Learning
global models trains a probabilistic model under the con-
straints imposed by the domain. Examples include varia-
tions of HMMs, conditional models and sequential varia-
tions of Markov random fields (Lafferty et al, 2001). The
other framework, inference with classifiers (Roth, 2002),
views maintaining constraints and learning classifiers as
separate processes. Various local classifiers are trained
without the knowledge of constraints. The predictions
are taken as input on the inference procedure which then
finds the best global prediction. In addition to the concep-
tual simplicity of this approach, it also seems to perform
better experimentally (Tjong Kim Sang and De Meulder,
2003).
Typically, efficient inference procedures in both frame-
works rely on dynamic programming (e.g., Viterbi),
which works well in sequential data. However, in many
important problems, the structure is more general, result-
ing in computationally intractable inference. Problems of
these sorts have been studied in computer vision, where
inference is generally performed over low level measure-
ments rather than over higher level predictors (Levin et
al., 2002; Boykov et al, 2001).
This work develops a novel inference with classifiers
approach. Rather than being restricted on sequential data,
we study a fairly general setting. The problem is defined
in terms of a collection of discrete random variables rep-
resenting binary relations and their arguments; we seek
an optimal assignment to the variables in the presence of
the constraints on the binary relations between variables
and the relation types.
The key insight to this solution comes from re-
cent techniques developed for approximation algo-
rithms (Chekuri et al, 2001). Following this work, we
model inference as an optimization problem, and show
how to cast it as a linear program. Using existing numer-
ical packages, which are able to solve very large linear
programming problems in a very short time1, inference
can be done very quickly.
Our approach could be contrasted with other ap-
1For example, (CPLEX, 2003) is able to solve a linear pro-
gramming problem of 13 million variables within 5 minutes.
proaches to sequential inference or to general Markov
random field approaches (Lafferty et al, 2001; Taskar et
al., 2002). The key difference is that in these approaches,
the model is learned globally, under the constraints im-
posed by the domain. In our approach, predictors do not
need to be learned in the context of the decision tasks,
but rather can be learned in other contexts, or incorpo-
rated as background knowledge. This way, our approach
allows the incorporation of constraints into decisions in a
dynamic fashion and can therefore support task specific
inferences. The significance of this is clearly shown in
our experimental results.
We develop our models in the context of natural lan-
guage inferences and evaluate it here on the problem of
simultaneously recognizing named entities and relations
between them.
1.1 Entity and Relation Recognition
This is the problem of recognizing the kill (KFJ, Os-
wald) relation in the sentence ?J. V. Oswald was
murdered at JFK after his assassin,
R. U. KFJ...? This task requires making several
local decisions, such as identifying named entities in the
sentence, in order to support the relation identification.
For example, it may be useful to identify that Oswald
and KFJ are people, and JFK is a location. This, in turn,
may help to identify that the kill action is described in the
sentence. At the same time, the relation kill constrains its
arguments to be people (or at least, not to be locations)
and helps to enforce that Oswald and KFJ are likely to
be people, while JFK is not.
In our model, we first learn a collection of ?local? pre-
dictors, e.g., entity and relation identifiers. At decision
time, given a sentence, we produce a global decision that
optimizes over the suggestions of the classifiers that are
active in the sentence, known constraints among them
and, potentially, domain or tasks specific constraints rel-
evant to the current decision.
Although a brute-force algorithm may seem feasible
for short sentences, as the number of entity variable
grows, the computation becomes intractable very quickly.
Given n entities in a sentence, there are O(n2) possible
relations between them. Assume that each variable (en-
tity or relation) can take l labels (?none? is one of these
labels). Thus, there are ln2 possible assignments, which
is too large even for a small n.
When evaluated on simultaneous learning of named
entities and relations, our approach not only provides
a significant improvement in the predictors? accuracy;
more importantly, it provides coherent solutions. While
many statistical methods make ?stupid? mistakes (i.e.,
inconsistency among predictions), that no human ever
makes, as we show, our approach improves also the qual-
ity of the inference significantly.
The rest of the paper is organized as follows. Section 2
formally defines our problem and section 3 describes the
computational approach we propose. Experimental re-
sults are given in section 4, followed by some discussion
and conclusion in section 5.
2 The Relational Inference Problem
We consider the relational inference problem within the
reasoning with classifiers paradigm, and study a spe-
cific but fairly general instantiation of this problem, moti-
vated by the problem of recognizing named entities (e.g.,
persons, locations, organization names) and relations be-
tween them (e.g. work for, located in, live in). We con-
sider a set V which consists of two types of variables V =
E ? R. The first set of variables E = {E1, E2, ? ? ? , En}
ranges LE . The value (called ?label?) assigned to Ei ? E
is denoted fEi ? LE . The second set of variables
R = {Rij}{1?i,j?n;i6=j} is viewed as binary relations
over E . Specifically, for each pair of entities Ei and Ej ,
i 6= j, we use Rij and Rji to denote the (binary) relations
(Ei, Ej) and (Ej , Ei) respectively. The set of labels of
relations is LR and the label assigned to relation Rij ? R
is fRij ? LR.
Apparently, there exists some constraints on the labels
of corresponding relation and entity variables. For in-
stance, if the relation is live in, then the first entity should
be a person, and the second entity should be a location.
The correspondence between the relation and entity vari-
ables can be represented by a bipartite graph. Each rela-
tion variable Rij is connected to its first entity Ei , and
second entity Ej . We use N 1 and N 2 to denote the entity
variables of a relation Rij . Specifically, Ei = N 1(Rij)
and Ej = N 2(Rij).
In addition, we define a set of constraints on the out-
comes of the variables in V . C1 : LE ? LR ? {0, 1}
constraint values of the first argument of a relation. C2
is defined similarly and constrains the second argument
a relation can take. For example, (born in, person) is
in C1 but not in C2 because the first entity of relation
born in has to be a person and the second entity can only
be a location instead of a person. Note that while we
define the constraints here as Boolean, our formalisms
in fact allows for stochastic constraints. Also note that
we can define a large number of constraints, such as
CR : LR ? LR ? {0, 1} which constrain types of re-
lations, etc. In fact, as will be clear in Sec. 3 the language
for defining constraints is very rich ? linear (in)equalities
over V .
We exemplify the framework using the problem of si-
multaneous recognition of named entities and relations in
sentences. Briefly speaking, we assume a learning mech-
anism that can recognize entity phrases in sentences,
based on local contextual features. Similarly, we assume
a learning mechanism that can recognize the semantic re-
lation between two given phrases in a sentence.
We seek an inference algorithm that can produce a co-
herent labeling of entities and relations in a given sen-
tence. Furthermore, it follows, as best as possible the
recommendation of the entity and relation classifiers, but
also satisfies natural constraints that exist on whether spe-
cific entities can be the argument of specific relations,
whether two relations can occur together at the same
time, or any other information that might be available at
the inference time (e.g., suppose it is known that enti-
ties A and B represent the same location; one may like to
incorporate an additional constraint that prevents an in-
ference of the type: ?C lives in A; C does not live in B?).
We note that a large number of problems can be mod-
eled this way. Examples include problems such as chunk-
ing sentences (Punyakanok and Roth, 2001), coreference
resolution and sequencing problems in computational bi-
ology. In fact, each of the components of our problem
here, the separate task of recognizing named entities in
sentences and the task of recognizing semantic relations
between phrases, can be modeled this way. However,
our goal is specifically to consider interacting problems
at different levels, resulting in more complex constraints
among them, and exhibit the power of our method.
The most direct way to formalize our inference prob-
lem is via the formalism of Markov Random Field (MRF)
theory (Li, 2001). Rather than doing that, for compu-
tational reasons, we first use a fairly standard transfor-
mation of MRF to a discrete optimization problem (see
(Kleinberg and Tardos, 1999) for details). Specifically,
under weak assumptions we can view the inference prob-
lem as the following optimization problem, which aims
to minimize the objective function that is the sum of the
following two cost functions.
Assignment cost: the cost of deviating from the assign-
ment of the variables V given by the classifiers. The spe-
cific cost function we use is defined as follows: Let l be
the label assigned to variable u ? V . If the marginal prob-
ability estimation is p = P (fu = l), then the assignment
cost cu(l) is ? log p.
Constraint cost: the cost imposed by breaking con-
straints between neighboring nodes. The specific cost
function we use is defined as follows: Consider two en-
tity nodes Ei, Ej and its corresponding relation node Rij ;
that is, Ei = N 1(Rij) and Ej = N 2(Rij). The con-
straint cost indicates whether the labels are consistent
with the constraints. In particular, we use: d1(fEi , fRij )
is 0 if (fRij , fEi) ? C1; otherwise, d1(fEi , fRij ) is ? 2.
Similarly, we use d2 to force the consistency of the sec-
ond argument of a relation.
2In practice, we use a very large number (e.g., 915).
Since we are seeking the most probable global assign-
ment that satisfies the constraints, therefore, the overall
cost function we optimize, for a global labeling f of all
variables is:
C(f) =
?
u?V
cu(fu)
+
?
Rij?R
[
d1(fRij , fEi) + d2(fRij , fEj )
] (1)
3 A Computational Approach to
Relational Inference
Unfortunately, it is not hard to see that the combinatorial
problem (Eq. 1) is computationally intractable even when
placing assumptions on the cost function (Kleinberg and
Tardos, 1999). The computational approach we adopt is
to develop a linear programming (LP) formulation of the
problem, and then solve the corresponding integer lin-
ear programming (ILP) problem. Our LP formulation is
based on the method proposed by (Chekuri et al, 2001).
Since the objective function (Eq. 1) is not a linear func-
tion in terms of the labels, we introduce new binary vari-
ables to represent different possible assignments to each
original variable; we then represent the objective function
as a linear function of these binary variables.
Let x{u,i} be a {0, 1}-variable, defined to be 1 if and
only if variable u is labeled i, where u ? E , i ? LE or
u ? R, i ? LR. For example, x{E1,2} = 1 when the
label of entity E1 is 2; x{R23,3} = 0 when the label of re-
lation R23 is not 3. Let x{Rij ,r,Ei,e1} be a {0, 1}-variable
indicating whether relation Rij is assigned label r and
its first argument, Ei, is assigned label e1. For instance,
x{R12,1,E1,2} = 1 means the label of relation R12 is 1
and the label of its first argument, E1, is 2. Similarly,
x{Rij ,r,Ej ,e2} = 1 indicates that Rij is assigned label r
and its second argument, Ej , is assigned label e2. With
these definitions, the optimization problem can be repre-
sented as the following ILP problem (Figure 1).
Equations (2) and (3) require that each entity or rela-
tion variable can only be assigned one label. Equations
(4) and (5) assure that the assignment to each entity or
relation variable is consistent with the assignment to its
neighboring variables. (6), (7), and (8) are the integral
constraints on these binary variables.
There are several advantages of representing the prob-
lem in an LP formulation. First of all, linear (in)equalities
are fairly general and are able to represent many types
of constraints (e.g., the decision time constraint in the
experiment in Sec. 4). More importantly, an ILP prob-
lem at this scale can be solved very quickly using current
commercial LP/ILP packages, like (Xpress-MP, 2003) or
(CPLEX, 2003). We introduce the general strategies of
solving an ILP problem here.
min
?
E?E
?
e?LE
cE(e) ? x{E,e} +
?
R?R
?
r?LR
cR(r) ? x{R,r}
+
?
Ei,Ej?E
Ei 6=Ej
[
?
r?LR
?
e1?LE
d1(r, e1) ? x{Rij ,r,Ei,e1} +
?
r?LR
?
e2?LE
d2(r, e2) ? x{Rij ,r,Ej ,e2}
]
subject to:
?
e?LE
x{E,e} = 1 ?E ? E (2)
?
r?LR
x{R,r} = 1 ?R ? R (3)
x{E,e} =
?
r?LR
x{R,r,E,e} ?E ? E and ?R ? {R : E = N 1(R) or R : E = N 2(R)} (4)
x{R,r} =
?
e?LE
x{R,r,E,e} ?R ? R and ?E = N 1(R) or E = N 2(R) (5)
x{E,e} ? {0, 1} ?E ? E , e ? LE (6)
x{R,r} ? {0, 1} ?R ? R, r ? LR (7)
x{R,r,E,e} ? {0, 1} ?R ? R, r ? LR, E ? E , e ? LE (8)
Figure 1: Integer Linear Programming Formulation
3.1 Linear Programming Relaxation (LPR)
To solve an ILP problem, a natural idea is to relax the
integral constraints. That is, replacing (6), (7), and (8)
with:
x{E,e} ? 0 ?E ? E , e ? LE (9)
x{R,r} ? 0 ?R ? R, r ? LR (10)
x{R,r,E,e} ? 0 ?R ? R, r ? LR,
E ? E , e ? LE (11)
If LPR returns an integer solution, then it is also the
optimal solution to the ILP problem. If the solution is
non integer, then at least it gives a lower bound to the
value of the cost function, which can be used in modi-
fying the problem and getting closer to deriving an op-
timal integer solution. A direct way to handle the non
integer solution is called rounding, which finds an inte-
ger point that is close to the non integer solution. Un-
der some conditions of cost functions, which do not hold
here, a well designed rounding algorithm can be shown
that the rounded solution is a good approximation to the
optimal solution (Kleinberg and Tardos, 1999; Chekuri et
al., 2001). Nevertheless, in general, the outcomes of the
rounding procedure may not even be a legal solution to
the problem.
3.2 Branch & Bound and Cutting Plane
Branch and bound is the method that divides an ILP prob-
lem into several LP subproblems. It uses LPR as a sub-
routine to generate dual (upper and lower) bounds to re-
duce the search space, and finds the optimal solution as
well. When LPR finds a non integer solution, it splits the
problem on the non integer variable. For example, sup-
pose variable xi is fractional in an non integer solution to
the ILP problem min{cx : x ? S, x ? {0, 1}n}, where S
is the linear constraints. The ILP problem can be split into
two sub LPR problems, min{cx : x ? S?{xi = 0}} and
min{cx : x ? S?{xi = 1}}. Since any feasible solution
provides an upper bound and any LPR solution generates
a lower bound, the search tree can be effectively cut.
Another strategy of dealing with non integer points,
which is often combined with branch & bound, is called
cutting plane. When a non integer solution is given by
LPR, it adds a new linear constraint that makes the non in-
teger point infeasible, while still keeps the optimal integer
solution in the feasible region. As a result, the feasible
region is closer to the ideal polyhedron, which is the con-
vex hull of feasible integer solutions. The most famous
cutting plane algorithm is Gomory?s fractional cutting
plane method (Wolsey, 1998), which can be shown that
only finite number of additional constraints are needed.
Moreover, researchers develop different cutting plane al-
gorithms for different types of ILP problems. One exam-
ple is (Wang and Regan, 2000), which only focuses on
binary ILP problems.
Although in theory, a search based strategy may need
several steps to find the optimal solution, LPR always
generates integer solutions in our experiments. This phe-
nomenon may link to the theory of unimodularity.
3.3 Unimodularity
When the coefficient matrix of a given linear program
in its standard form is unimodular, it can be shown that
the optimal solution to the linear program is in fact inte-
gral (Schrijver, 1986). In other words, LPR is guaranteed
to produce an integer solution.
Definition 3.1 A matrix A of rank m is called unimodu-
lar if all the entries ofA are integers, and the determinant
of every square submatrix of A of order m is in 0,+1,-1.
Theorem 3.1 (Veinott & Dantzig) Let A be an (m,n)-
integral matrix with full row rank m. Then the polyhe-
dron {x|x ? 0;Ax = b} is integral for each integral
vector b, if and only if A is unimodular.
Theorem 3.1 indicates that if a linear programming
problem is in its standard form, then regardless of the
cost function and the integral vector b, the optimal so-
lution is an integer if and only if the coefficient matrix A
is unimodular.
Although the coefficient matrix in our problem is not
unimodular, LPR still produces integer solutions for all
the (thousands of cases) we have experimented with. This
may be due to the fact that the coefficient matrix shares
many properties of a unimodular matrix. As a result, most
of the vertices of the polyhedron are integer points. An-
other possible reason is that given the cost function we
have, the optimal solution is always integer. Because of
the availability of very efficient LP/ILP packages, we de-
fer the exploration of this direction for now.
4 Experiments
We describe below two experiments on the problem of
simultaneously recognizing entities and relations. In the
first, we view the task as a knowledge acquisition task
? we let the system read sentences and identify entities
and relations among them. Given that this is a difficult
task which may require quite often information beyond
the sentence, we consider also a ?forced decision? task,
in which we simulate a question answering situation ?
we ask the system, say, ?who killed whom? and evaluate
it on identifying correctly the relation and its arguments,
given that it is known that somewhere in this sentence
this relation is active. In addition, this evaluation exhibits
the ability of our approach to incorporate task specific
constraints at decision time.
Our experiments are based on the TREC data set
(which consists of articles from WSJ, AP, etc.) that we
annotated for named entities and relations. In order to
effectively observe the interaction between relations and
entities, we picked 1437 sentences that have at least one
active relation. Among those sentences, there are 5336
entities, and 19048 pairs of entities (binary relations). En-
tity labels include 1685 persons, 1968 locations, 978 or-
ganizations and 705 others. Relation labels include 406
located in, 394 work for, 451 orgBased in, 521 live in,
268 kill, and 17007 none. Note that most pairs of entities
have no active relations at all. Therefore, relation none
significantly outnumbers others. Examples of each rela-
tion label and the constraints between a relation variable
and its two entity arguments are shown as follows.
Relation Entity1 Entity2 Example
located in loc loc (New York, US)
work for per org (Bill Gates, Microsoft)
orgBased in org loc (HP, Palo Alto)
live in per loc (Bush, US)
kill per per (Oswald, JFK)
In order to focus on the evaluation of our inference
procedure, we assume the problem of segmentation (or
phrase detection) (Abney, 1991; Punyakanok and Roth,
2001) is solved, and the entity boundaries are given to us
as input; thus we only concentrate on their classifications.
We evaluate our LP based global inference procedure
against two simpler approaches and a third that is given
more information at learning time. Basic, only tests our
entity and relation classifiers, which are trained indepen-
dently using only local features. In particular, the relation
classifier does not know the labels of its entity arguments,
and the entity classifier does not know the labels of rela-
tions in the sentence either. Since basic classifiers are
used in all approaches, we describe how they are trained
here.
For the entity classifier, one set of features are ex-
tracted from words within a size 4 window around the
target phrase. They are: (1) words, part-of-speech tags,
and conjunctions of them; (2) bigrams and trigrams of
the mixture of words and tags. In addition, some other
features are extracted from the target phrase, including:
symbol explanation
icap the first character of a word is capitalized
acap all characters of a word are capitalized
incap some characters of a word are capitalized
suffix the suffix of a word is ?ing?, ?ment?, etc.
bigram bigram of words in the target phrase
len number of words in the target phrase
place3 the phrase is/has a known place?s name
prof3 the phrase is/has a professional title (e.g. Lt.)
name3 the phrase is/has a known person?s name
For the relation classifier, there are three sets of fea-
tures: (1) features similar to those used in the entity clas-
sification are extracted from the two argument entities of
3We collect names of famous places, people and popular ti-
tles from other data sources in advance.
Pattern Example
arg1 , arg2 San Jose, CA
arg1 , ? ? ? a ? ? ? arg2 prof John Smith, a Starbucks manager ? ? ?
in/at arg1 in/at/, arg2 Officials in Perugia in Umbria province said ? ? ?
arg2 prof arg1 CNN reporter David McKinley ? ? ?
arg1 ? ? ? native of ? ? ? arg2 Elizabeth Dole is a native of Salisbury, N.C.
arg1 ? ? ? based in/at arg2 Leslie Kota, a spokeswoman for K mart based in Troy, Mich. said ? ? ?
Table 1: Some patterns used in relation classification
the relation; (2) conjunctions of the features from the two
arguments; (3) some patterns extracted from the sentence
or between the two arguments. Some features in category
(3) are ?the number of words between arg1 and arg2 ?,
?whether arg1 and arg2 are the same word?, or ?arg1 is
the beginning of the sentence and has words that consist
of all capitalized characters?, where arg1 and arg2 rep-
resent the first and second argument entities respectively.
In addition, Table 1 presents some patterns we use.
The learning algorithm used is a variation of the Win-
now update rule incorporated in SNoW (Roth, 1998;
Roth and Yih, 2002), a multi-class classifier that is specif-
ically tailored for large scale learning tasks. SNoW learns
a sparse network of linear functions, in which the targets
(entity classes or relation classes, in this case) are repre-
sented as linear functions over a common feature space.
While SNoW can be used as a classifier and predicts us-
ing a winner-take-all mechanism over the activation value
of the target classes, we can also rely directly on the raw
activation value it outputs, which is the weighted linear
sum of the active features, to estimate the posteriors. It
can be verified that the resulting values are monotonic
with the confidence in the prediction, therefore provide a
good source of probability estimation. We use softmax
(Bishop, 1995) over the raw activation values as condi-
tional probabilities. Specifically, suppose the number of
classes is n, and the raw activation values of class i is
acti. The posterior estimation for class i is derived by the
following equation.
pi =
eacti
?
1?j?n eactj
Pipeline, mimics the typical strategy in solving com-
plex natural language problems ? separating a task into
several stages and solving them sequentially. For exam-
ple, a named entity recognizer may be trained using a dif-
ferent corpus in advance, and given to a relation classifier
as a tool to extract features. This approach first trains an
entity classifier as described in the basic approach, and
then uses the prediction of entities in addition to other
local features to learn the relation identifier. Note that
although the true labels of entities are known here when
training the relation identifier, this may not be the case
in general NLP problems. Since only the predicted en-
tity labels are available in testing, learning on the predic-
tions of the entity classifier presumably makes the rela-
tion classifier more tolerant to the mistakes of the entity
classifier. In fact, we also observe this phenomenon em-
pirically. When the relation classifier is trained using the
true entity labels, the performance is much worse than
using the predicted entity labels.
LP, is our global inference procedure. It takes as in-
put the constraints between a relation and its entity argu-
ments, and the output (the estimated probability distribu-
tion of labels) of the basic classifiers. Note that LP may
change the predictions for either entity labels or relation
labels, while pipeline fully trusts the labels of entity clas-
sifier, and only the relation predictions may be different
from the basic relation classifier. In other words, LP is
able to enhance the performance of entity classification,
which is impossible for pipeline.
The final approach, Omniscience, tests the conceptual
upper bound of this entity/relation classification problem.
It also trains the two classifiers separately as the basic
approach. However, it assumes that the entity classifier
knows the correct relation labels, and similarly the rela-
tion classifier knows the right entity labels as well. This
additional information is then used as features in training
and testing. Note that this assumption is totally unrealis-
tic. Nevertheless, it may give us a hint that how much a
global inference can achieve.
4.1 Results
Tables 2 & 3 show the performance of each approach in
F?=1 using 5-fold cross-validation. The results show that
LP performs consistently better than basic and pipeline,
both in entities and relations. Note that LP does not apply
learning at all, but still outperforms pipeline, which uses
entity predictions as new features in learning. The results
of the omniscient classifiers reveal that there is still room
for improvement. One option is to apply learning to tune
a better cost function in the LP approach.
One of the more significant results in our experiments,
we believe, is the improvement in the quality of the deci-
sions. As mentioned in Sec. 1, incorporating constraints
helps to avoid inconsistency in classification. It is in-
Approach person organization location
Rec. Prec. F1 Rec. Prec. F1 Rec. Prec. F1
Basic 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9
Pipeline 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9
LP 90.4 90.0 90.2 88.5 91.7 90.1 71.5 91.0 80.1
Omniscient 94.9 93.5 94.2 92.3 96.5 94.4 88.3 93.4 90.8
Table 2: Results of Entity Classification
Approach located in work for orgBased in
Rec. Prec. F1 Rec. Prec. F1 Rec. Prec. F1
Basic 54.7 43.0 48.2 42.1 51.6 46.4 36.1 84.9 50.6
Pipeline 51.2 51.6 51.4 41.4 55.6 47.5 36.9 76.6 49.9
LP 53.2 59.5 56.2 40.4 72.9 52.0 36.3 90.1 51.7
Omniscient 64.0 54.5 58.9 50.5 69.1 58.4 50.2 76.7 60.7
Approach live in kill
Rec. Prec. F1 Rec. Prec. F1
Basic 39.7 61.6 48.3 82.1 73.6 77.6
Pipeline 42.6 62.2 50.6 83.2 76.4 79.6
LP 41.5 68.1 51.6 81.3 82.2 81.7
Omniscient 57.0 60.7 58.8 82.1 74.6 78.2
Table 3: Results of Relation Classification
teresting to investigate how often such mistakes happen
without global inference, and see how effectively the
global inference enhances this.
For this purpose, we define the quality of the decision
as follows. For an active relation of which the label is
classified correctly, if both its argument entities are also
predicted correctly, we count it as a coherent prediction.
Quality is then the number of coherent predictions di-
vided by the sum of coherent and incoherent predictions.
Since the basic and pipeline approaches do not have a
global view of the labels of entities and relations, 5%
to 25% of the predictions are incoherent. Therefore, the
quality is not always good. On the other hand, our global
inference procedure, LP, takes the natural constraints into
account, so it never generates incoherent predictions. If
the relation classifier has the correct entity labels as fea-
tures, a good learner should learn the constraints as well.
As a result, the quality of omniscient is almost as good as
LP.
Another experiment we did is the forced decision test,
which boosts the F1 of ?kill? relation to 86.2%. Here
we consider only sentences in which the ?kill? relation
is active. We force the system to determine which of the
possible relations in a sentence (i.e., which pair of en-
tities) has this relation by adding a new linear equality.
This is a realistic situation (e.g., in the context of ques-
tion answering) in that it adds an external constraint, not
present at the time of learning the classifiers and it eval-
uates the ability of our inference algorithm to cope with
it. The results exhibit that our expectations are correct.
In fact, we believe that in natural situations the number
of constraints that can apply is even larger. Observing
the algorithm performs on other, specific, forced deci-
sion tasks verifies that LP is reliable in these situations.
As shown in the experiment, it even performs better than
omniscience, which is given more information at learning
time, but cannot adapt to the situation at decision time.
5 Discussion
We presented an linear programming based approach
for global inference where decisions depend on the out-
comes of several different but mutually dependent classi-
fiers. Even in the presence of a fairly general constraint
structure, deviating from the sequential nature typically
studied, this approach can find the optimal solution effi-
ciently.
Contrary to general search schemes (e.g., beam
search), which do not guarantee optimality, the linear pro-
gramming approach provides an efficient way to finding
the optimal solution. The key advantage of the linear
programming formulation is its generality and flexibility;
in particular, it supports the ability to incorporate classi-
fiers learned in other contexts, ?hints? supplied and de-
cision time constraints, and reason with all these for the
best global prediction. In sharp contrast with the typi-
cally used pipeline framework, our formulation does not
blindly trust the results of some classifiers, and therefore
is able to overcome mistakes made by classifiers with the
help of constraints.
Our experiments have demonstrated these advantages
by considering the interaction between entity and rela-
tion classifiers. In fact, more classifiers can be added and
used within the same framework. For example, if coref-
erence resolution is available, it is possible to incorporate
it in the form of constraints that force the labels of the co-
referred entities to be the same (but, of course, allowing
the global solution to reject the suggestion of these clas-
sifiers). Consequently, this may enhance the performance
of entity/relation recognition and, at the same time, cor-
rect possible coreference resolution errors. Another ex-
ample is to use chunking information for better relation
identification; suppose, for example, that we have avail-
able chunking information that identifies Subj+Verb and
Verb+Object phrases. Given a sentence that has the verb
?murder?, we may conclude that the subject and object of
this verb are in a ?kill? relation. Since the chunking in-
formation is used in the global inference procedure, this
information will contribute to enhancing its performance
and robustness, relying on having more constraints and
overcoming possible mistakes by some of the classifiers.
Moreover, in an interactive environment where a user can
supply new constraints (e.g., a question answering situa-
tion) this framework is able to make use of the new in-
formation and enhance the performance at decision time,
without retraining the classifiers.
As we show, our formulation supports not only im-
proved accuracy, but also improves the ?human-like?
quality of the decisions. We believe that it has the poten-
tial to be a powerful way for supporting natural language
inferences.
Acknowledgements This research has been supported
by NFS grants CAREER IIS-9984168, ITR IIS-0085836,
EIA-0224453, an ONR MURI Award, and an equipment
donation from AMD. We also thank the anonymous ref-
erees for their useful comments.
References
S. Abney. 1991. Parsing by chunks. In S. Abney
R. Berwick and C. Tenny, editors, Principle-based
parsing: Computation and Psycholinguistics, pages
257?278. Kluwer, Dordrecht.
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast ap-
proximate energy minimization via graph cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 23(11):1222?1239, November.
C. Chekuri, S. Khanna, J. Naor, and L. Zosin. 2001. Ap-
proximation algorithms for the metric labeling prob-
lem via a new linear programming formulation. In
Symposium on Discrete Algorithms, pages 109?118.
CPLEX. 2003. ILOG, Inc. CPLEX.
http://www.ilog.com/products/cplex/.
T. Dietterich. 2002. Machine learning for sequential
data: A review. In Structural, Syntactic, and Statistical
Pattern Recognition, pages 15?30. Springer-Verlag.
J. Kleinberg and E. Tardos. 1999. Approximation algo-
rithms for classification problems with pairwise rela-
tionships: Metric labeling and markov random fields.
In IEEE Symposium on Foundations of Computer Sci-
ence, pages 14?23.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th
International Conf. on Machine Learning, pages 282?
289. Morgan Kaufmann, San Francisco, CA.
A. Levin, A. Zomet, and Yair Weiss. 2002. Learning
to perceive transparency from the statistics of natu-
ral scenes. In NIPS-15; The 2002 Conference on Ad-
vances in Neural Information Processing Systems.
S. Li. 2001. Markov Random Field Modeling in Image
Analisys. Springer-Verlag.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS-13; The 2000 Confer-
ence on Advances in Neural Information Processing
Systems, pages 995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for
entity & relation recognition. In COLING 2002, The
19th International Conference on Computational Lin-
guistics, pages 835?841.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. of AAAI, pages
806?813.
D. Roth. 2002. Reasoning with classifiers. In Proc. of
the European Conference on Machine Learning, pages
506?510.
A. Schrijver. 1986. Theory of Linear and Integer Pro-
gramming. Wiley Interscience series in discrete math-
matics. John Wiley & Sons, December.
B. Taskar, A. Pieter, and D. Koller. 2002. Discrimina-
tive probabilistic models for relational data. In Proc. of
Uncertainty in Artificial Intelligence, pages 485?492.
E. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL-2003, pages 142?147. Edmonton, Canada.
X. Wang and A. Regan. 2000. A cutting plane method
for integer programming problems with binary vari-
ables. Technical Report UCI-ITS-WP-00-12, Univer-
sity of California, Irvine.
L. Wolsey. 1998. Integer Programming. John Wiley &
Sons, Inc.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Semantic Role Labeling Via Generalized Inference Over Classifiers
Vasin Punyakanok, Dan Roth, Wen-tau Yih, Dav Zimak Yuancheng Tu
Department of Computer Science Department of Linguistics
University of Illinois at Urbana-Champaign
{punyakan,danr,yih,davzimak,ytu}@uiuc.edu
Abstract
We present a system submitted to the CoNLL-
2004 shared task for semantic role labeling.
The system is composed of a set of classifiers
and an inference procedure used both to clean
the classification results and to ensure struc-
tural integrity of the final role labeling. Lin-
guistic information is used to generate features
during classification and constraints for the in-
ference process.
1 Introduction
Semantic role labeling is a complex task to discover pat-
terns within sentences corresponding to semantic mean-
ing. We believe it is hopeless to expect high levels of per-
formance from either purely manual classifiers or purely
learned classifiers. Rather, supplemental linguistic infor-
mation must be used to support and correct a learning
system. The system we present here is composed of two
phases.
First, a set of phrase candidates is produced using two
learned classifiers?one to discover beginning positions
and one to discover end positions for each argument type.
Hopefully, this phase discovers a small superset of all
phrases in the sentence (for each verb).
In the second phase, the final prediction is made. First,
candidate phrases from the first phase are re-scored using
a classifier designed to determine argument type, given
a candidate phrase. Because phrases are considered as a
whole, global properties of the candidates can be used to
discover how likely it is that a phrase is of a given ar-
gument type. However, the set of possible role-labelings
is restricted by structural and linguistic constraints. We
encode these constraints using linear functions and use
integer programming to ensure the final prediction is con-
sistent (see Section 4).
2 SNoW Learning Architecture
The learning algorithm used is a variation of the Winnow
update rule incorporated in SNoW (Roth, 1998; Roth and
Yih, 2002), a multi-class classifier that is specifically tai-
lored for large scale learning tasks. SNoW learns a sparse
network of linear functions, in which the targets (phrase
border predictions or argument type predictions, in this
case) are represented as linear functions over a common
feature space. It incorporates several improvements over
the basic Winnow update rule. In particular, a regular-
ization term is added, which has the affect of trying to
separate the data with a think separator (Grove and Roth,
2001; Hang et al, 2002). In the work presented here we
use this regularization with a fixed parameter.
Experimental evidence has shown that SNoW activa-
tions are monotonic with the confidence in the prediction
Therefore, it can provide a good source of probability es-
timation. We use softmax (Bishop, 1995) over the raw ac-
tivation values as conditional probabilities. Specifically,
suppose the number of classes is n, and the raw activa-
tion values of class i is acti. The posterior estimation for
class i is derived by the following equation.
score(i) = pi =
eacti
?
1?j?n eactj
3 First Phase: Find Argument Candidates
The first phase is to predict the phrases of a given sen-
tence that correspond to some argument (given the verb).
Unfortunately, it turns out that it is difficult to predict the
exact phrases accurately. Therefore, the goal of the first
phase is to output a superset of the correct phrases by fil-
tering out unlikely candidates.
Specifically, we learn two classifiers, one to detect
beginning phrase locations and a second to detect end
phrase locations. Each multi-class classifier makes pre-
dictions over forty-three classes ? thirty-two argument
types, ten continuous argument types, one class to detect
not begging and one class to detect not end. The follow-
ing features are used:
? Word feature includes the current word, two words
before and two words after.
? Part-of-speech tag (POS) feature includes the POS
tags of the current word, two words before and after.
? Chunk feature includes the BIO tags for chunks of
the current word, two words before and after.
? Predicate lemma & POS tag show the lemma form
and POS tag of the active predicate.
? Voice feature indicates the voice (active/passive) of
the current predicate. This is extracted with a simple
rule: a verb is identified as passive if it follows a to-
be verb in the same phrase chuck and its POS tag
is VBN(past participle) or it immediately follows a
noun phrase.
? Position feature describes if the current word is be-
fore of after the predicate.
? Chunk pattern feature encodes the sequence of
chunks from the current words to the predicate.
? Clause tag indicates the boundary of clauses.
? Clause path feature is a path formed from a semi-
parsed tree containing only clauses and chunks.
Each clause is named with the chunk immediately
preceding it. The clause path is the path from predi-
cate to target word in the semi-parsed tree.
? Clause position feature is the position of the tar-
get word relative to the predicate in the semi-parsed
tree containing only clauses. Specifically, there
are four configurations?target word and predicate
share same parent, parent of target word is ancestor
of predicate, parent of predicate is ancestor of target
word, or otherwise.
Because each phrase consists of a single beginning and
a single ending, these classifiers can be used to construct
a set of potential phrases (by combining each predicted
begin with each predicted end after it of the same type).
Although the outputs of this phase are potential ar-
gument candidates, along with their types, the second
phase re-scores the arguments using all possible types.
After eliminating the types from consideration, the first
phase achieves 98.96% and 88.65% recall (overall, with-
out verb) on the training and the development set, respec-
tively. Because these are the only candidates that are
passed to the second phase, 88.65% is an upper bound
of the recall for our overall system.
4 Second Phase: Phrase Classification
The second phase of our system assigns the final argu-
ment classes to (a subset) of the phrases supplied from the
first phase. This task is accomplished in two steps. First,
a multi-class classifier is used to supply confidence scores
corresponding to how likely individual phrases are to
have specific argument types. Then we look for the most
likely solution over the whole sentence, given the matrix
of confidences and linguistic information that serves as a
set of global constraints over the solution space.
Again, the SNoW learning architecture is used to train
a multi-class classifier to label each phrase to one of
the argument types, plus a special class ? no argument.
Training examples are created from the phrase candidates
supplied from the first phase using the following features:
? Predicate lemma & POS tag, voice, position,
clause Path, clause position, chunk pattern Same
features as the first phase.
? Word & POS tag from the phrase, including the
first/last word and tag, and the head word1.
? Named entity feature tells if the target phrase is,
embeds, overlaps, or is embedded in a named entity.
? Chunk features are the same as named entity (but
with chunks, e.g. noun phrases).
? Length of the target phrase, in the numbers of words
and chunks.
? Verb class feature is the class of the active predicate
described in the frame files.
? Phrase type uses simple heuristics to identify the
target phrase like VP, PP, or NP.
? Sub-categorization describes the phrase structure
around the predicate. We separate the clause where
the predicate is in into three part ? the predicate
chunk, segments before and after the predicate. The
sequence of the phrase types of these three segments
is our feature.
? Baseline follows the rule of identifying AM-NEG
and AM-MOD and uses them as features.
? Clause coverage describes how much of local
clause (from the predicate) is covered by the target
phrase.
? Chunk pattern length feature counts the number of
patterns in the phrase.
? Conjunctions join every pair of the above features
as new features.
? Boundary words & POS tags include one or two
words/tags before and after the target phrase.
1We use simple rules to first decide if a candidate phrase
type is VP, NP, or PP. The headword of an NP phrase is the
right-most noun. Similarly, the left-most verb/proposition of a
VP/PP phrase is extracted as the headword
? Bigrams are pairs of words/tags in the window from
two words before the target to the first word of the
target, and also from the last word to two words after
the phrase.
? Sparse colocation picks one word/tag from the two
words before the phrase, the first word/tag, the last
word/tag of the phrase, and one word/tag from the
two words after the phrase to join as features.
Alternately, we could have derived a scoring function
from the first phase confidences of the open and closed
predictors for each argument type. This method has
proved useful in the literature for shallow parsing (Pun-
yakanok and Roth, 2001). However, it is hoped that ad-
ditional global features of the phrase would be necessary
due to the variety and complexity of the argument types.
See Table 1 for a comparison.
Formally (but very briefly), the phrase classifier is at-
tempting to assign labels to a set of phrases, S1:M , in-
dexed from 1 to M . Each phrase Si can take any label
from a set of phrase labels, P , and the indexed set of
phrases can take a set of labels, s1:M ? PM . If we as-
sume that the classifier returns a score, score(Si = si),
corresponding to the likelihood of seeing label si for
phrase Si, then, given a sentence, the unaltered inference
task that is solved by our system maximizes the score of
the phrase, score(S1:M = s1:M ),
s?1:M = argmax
s1:M?PM
score(S1:M = s1:M )
= argmax
s1:M?PM
M
?
i=1
score(Si = si).
(1)
The second step for phrase identification is eliminating
labelings using global constraints derived from linguistic
information and structural considerations. Specifically,
we limit the solution space through the used of a filter
function, F , that eliminates many phrase labelings from
consideration. It is interesting to contrast this with previ-
ous work that filters individual phrases (see (Carreras and
Ma`rquez, 2003)). Here, we are concerned with global
constraints as well as constraints on the phrases. There-
fore, the final labeling becomes
s?1:M = argmax
s1:M?F(PM)
M
?
i=1
score(Si = si) (2)
The filter function used considers the following con-
straints:
1. Arguments cannot cover the predicate except those
that contain only the verb or the verb and the follow-
ing word.
2. Arguments cannot overlap with the clauses (they can
be embedded in one another).
3. If a predicate is outside a clause, its arguments can-
not be embedded in that clause.
4. No overlapping or embedding phrases.
5. No duplicate argument classes for A0-A5,V.
6. Exactly one V argument per sentence.
7. If there is C-V, then there has to be a V-A1-CV pat-
tern.
8. If there is a R-XXX argument, then there has to be a
XXX argument.
9. If there is a C-XXX argument, then there has to be
a XXX argument; in addition, the C-XXX argument
must occur after XXX.
10. Given the predicate, some argument classes are ille-
gal (e.g. predicate ?stalk? can take only A0 or A1).
Constraint 1 is valid because all the arguments of a pred-
icate must lie outside the predicate. The exception is for
the boundary of the predicate itself. Constraint 1 through
constraint 3 are actually constraints that can be evaluated
on a per-phrase basis and thus can be applied to the indi-
vidual phrases at any time. For efficiency sake, we elimi-
nate these even before the second phase scoring is begun.
Constraints 5, 8, and 9 are valid for only a subset of the
arguments.
These constraints are easy to transform into linear con-
straints (for example, for each class c, constraint 5 be-
comes
?M
i=1[Si = c] ? 1) 2. Then the optimum solution
of the cost function given in Equation 2 can be found by
integer linear programming3. A similar method was used
for entity/relation recognition (Roth and Yih, 2004).
Almost all previous work on shallow parsing and
phrase classification has used Constraint 4 to ensure that
there are no overlapping phrases. By considering addi-
tional constraints, we show improved performance (see
Table 1).
5 Results
In this section, we present results. For the second phase,
we evaluate the quality of the phrase predictor. The re-
sult first evaluates the phrase classifier, given the perfect
phrase locations without using inference (i.e. F(PM ) =
PM ). The second, adds inference to the phrase classifica-
tion over the perfect classifiers (see Table 2). We evaluate
the overall performance of our system (without assum-
ing perfect phrases) by training and evaluating the phrase
classifier on the output from the first phase (see Table 3).
Finally,since this is a tagging task, we compare this
system with the basic tagger that we have, the CLCL
2where [x] is 1 if x is true and 0 otherwise
3(Xpress-MP, 2003) was used in all experiments to solve in-
teger linear programming.
Precision Recall F1
1st Phase, non-Overlap 70.54% 61.50% 65.71
1st Phase, All Const. 70.97% 60.74% 65.46
2nd Phase, non-Overlap 69.69% 64.75% 67.13
2nd Phase, All Const. 71.96% 64.93% 68.26
Table 1: Summary of experiments on the development set.
The phrase scoring is choosen from either the first phase or the
second phase and each is evaluated by considering simply non-
overlapping constraints or the full set of linguistic constraints.
To make a fair comparison, parameters were set seperately to
optimize performance when using the first phase results. All
results are for overall performance.
Precision Recall F1
Without Inference 86.95% 87.24% 87.10
With Inference 88.03% 88.23% 88.13
Table 2: Results of second phase phrase prediction and in-
ference assuming perfect boundary detection in the first phase.
Inference improves performance by restricting label sequences
rather than restricting structural properties since the correct
boundaries are given. All results are for overall performance
on the development set.
shallow parser from (Punyakanok and Roth, 2001), which
is equivalent to using the scoring function from the first
phase with only the non-overlapping constraints. Table 1
shows how how additional constraints over the standard
non-overlapping constraints improve performance on the
development set4.
6 Conclusion
We show that linguistic information is useful for semantic
role labeling used both to derive features and to derive
hard constraints on the output. We show that it is possible
to use integer linear programming to perform inference
that incorporates a wide variety of hard constraints that
would be difficult to incorporate using existing methods.
In addition, we provide further evidence supporting the
use of scoring phrases over scoring phrase boundaries for
complex tasks.
Acknowledgments This research is supported by
NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-
9984168, EIA-0224453 and an ONR MURI Award. We
also thank AMD for their equipment donation and Dash
Optimization for free academic use of their Xpress-MP
software.
References
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
4The test set was not publicly available to evaluate these re-
sults.
Precision Recall F?=1
Overall 70.07% 63.07% 66.39
A0 81.13% 77.70% 79.38
A1 74.21% 63.02% 68.16
A2 54.16% 41.04% 46.69
A3 47.06% 26.67% 34.04
A4 71.43% 60.00% 65.22
A5 0.00% 0.00% 0.00
AM-ADV 39.36% 36.16% 37.69
AM-CAU 45.95% 34.69% 39.53
AM-DIR 42.50% 34.00% 37.78
AM-DIS 52.00% 67.14% 58.61
AM-EXT 46.67% 50.00% 48.28
AM-LOC 33.47% 34.65% 34.05
AM-MNR 45.19% 36.86% 40.60
AM-MOD 92.49% 94.96% 93.70
AM-NEG 85.92% 96.06% 90.71
AM-PNC 32.79% 23.53% 27.40
AM-PRD 0.00% 0.00% 0.00
AM-TMP 59.77% 56.89% 58.30
R-A0 81.33% 76.73% 78.96
R-A1 58.82% 57.14% 57.97
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 54.55% 42.86% 48.00
V 98.37% 98.37% 98.37
Table 3: Results on the test set.
X. Carreras and L. Ma`rquez. 2003. Phrase recognition by filter-
ing and ranking with perceptrons. In Proceedings of RANLP-
2003.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123?141.
T. Hang, F. Damerau, , and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637.
V. Punyakanok and D. Roth. 2001. The use of classifiers in
sequential inference. In NIPS-13; The 2000 Conference on
Advances in Neural Information Processing Systems, pages
995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity
& relation recognition. In COLING 2002, The 19th Interna-
tional Conference on Computational Linguistics, pages 835?
841.
D. Roth and W. Yih. 2004. A linear programming formulation
for global inference in natural language tasks. In Proc. of
CoNLL-2004.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. of AAAI, pages 806?813.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 181?184, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Generalized Inference with Multiple Semantic Role Labeling Systems
Peter Koomen Vasin Punyakanok Dan Roth Wen-tau Yih
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{pkoomen2,punyakan,danr,yih}@uiuc.edu
Abstract
We present an approach to semantic role
labeling (SRL) that takes the output of
multiple argument classifiers and com-
bines them into a coherent predicate-
argument output by solving an optimiza-
tion problem. The optimization stage,
which is solved via integer linear pro-
gramming, takes into account both the rec-
ommendation of the classifiers and a set
of problem specific constraints, and is thus
used both to clean the classification results
and to ensure structural integrity of the fi-
nal role labeling. We illustrate a signifi-
cant improvement in overall SRL perfor-
mance through this inference.
1 SRL System Architecture
Our SRL system consists of four stages: prun-
ing, argument identification, argument classifica-
tion, and inference. In particular, the goal of pruning
and argument identification is to identify argument
candidates for a given verb predicate. The system
only classifies the argument candidates into their
types during the argument classification stage. Lin-
guistic and structural constraints are incorporated
in the inference stage to resolve inconsistent global
predictions. The inference stage can take as its input
the output of the argument classification of a single
system or of multiple systems. We explain the infer-
ence for multiple systems in Sec. 2.
1.1 Pruning
Only the constituents in the parse tree are considered
as argument candidates. In addition, our system ex-
ploits the heuristic introduced by (Xue and Palmer,
2004) to filter out very unlikely constituents. The
heuristic is a recursive process starting from the verb
whose arguments are to be identified. It first returns
the siblings of the verb; then it moves to the parent of
the verb, and collects the siblings again. The process
goes on until it reaches the root. In addition, if a con-
stituent is a PP (propositional phrase), its children
are also collected. Candidates consisting of only a
single punctuation mark are not considered.
This heuristic works well with the correct parse
trees. However, one of the errors by automatic
parsers is due to incorrect PP attachment leading to
missing arguments. To attempt to fix this, we con-
sider as arguments the combination of any consec-
utive NP and PP, and the split of NP and PP inside
the NP that was chosen by the previous heuristics.
1.2 Argument Identification
The argument identification stage utilizes binary
classification to identify whether a candidate is an
argument or not. We train and apply the binary clas-
sifiers on the constituents supplied by the pruning
stage. Most of the features used in our system are
standard features, which include
? Predicate and POS tag of predicate indicate the lemma
of the predicate and its POS tag.
? Voice indicates tbe voice of the predicate.
? Phrase type of the constituent.
? Head word and POS tag of the head word include head
word and its POS tag of the constituent. We use rules
introduced by (Collins, 1999) to extract this feature.
? First and last words and POS tags of the constituent.
? Two POS tags before and after the constituent.
? Position feature describes if the constituent is before or
after the predicate relative to the position in the sentence.
181
? Path records the traversal path in the parse tree from the
predicate to the constituent.
? Subcategorization feature describes the phrase structure
around the predicate?s parent. It records the immediate
structure in the parse tree that expands to its parent.
? Verb class feature is the class of the active predicate de-
scribed in PropBank Frames.
? Lengths of the target constituent, in the numbers of words
and chunks separately.
? Chunk tells if the target argument is, embeds, overlaps,
or is embedded in a chunk with its type.
? Chunk pattern length feature counts the number of
chunks from the predicate to the argument.
? Clause relative position is the position of the target word
relative to the predicate in the pseudo-parse tree con-
structed only from clause constituent. There are four
configurations?target constituent and predicate share the
same parent, target constituent parent is an ancestor of
predicate, predicate parent is an ancestor of target word,
or otherwise.
? Clause coverage describes how much of the local clause
(from the predicate) is covered by the argument. It is
round to the multiples of 1/4.
1.3 Argument Classification
This stage assigns the final argument labels to the ar-
gument candidates supplied from the previous stage.
A multi-class classifier is trained to classify the
types of the arguments supplied by the argument
identification stage. To reduce the excessive candi-
dates mistakenly output by the previous stage, the
classifier can also classify the argument as NULL
(?not an argument?) to discard the argument.
The features used here are the same as those used
in the argument identification stage with the follow-
ing additional features.
? Syntactic frame describes the sequential pattern of the
noun phrases and the predicate in the sentence. This is
the feature introduced by (Xue and Palmer, 2004).
? Propositional phrase head is the head of the first phrase
after the preposition inside PP.
? NEG and MOD feature indicate if the argument is a
baseline for AM-NEG or AM-MOD. The rules of the
NEG and MOD features are used in a baseline SRL sys-
tem developed by Erik Tjong Kim Sang (Carreras and
Ma`rquez, 2004).
? NE indicates if the target argument is, embeds, overlaps,
or is embedded in a named-entity along with its type.
1.4 Inference
The purpose of this stage is to incorporate some
prior linguistic and structural knowledge, such as
?arguments do not overlap? or ?each verb takes at
most one argument of each type.? This knowledge is
used to resolve any inconsistencies of argument clas-
sification in order to generate final legitimate pre-
dictions. We use the inference process introduced
by (Punyakanok et al, 2004). The process is formu-
lated as an integer linear programming (ILP) prob-
lem that takes as inputs the confidences over each
type of the arguments supplied by the argument clas-
sifier. The output is the optimal solution that maxi-
mizes the linear sum of the confidence scores (e.g.,
the conditional probabilities estimated by the argu-
ment classifier), subject to the constraints that en-
code the domain knowledge.
Formally speaking, the argument classifier at-
tempts to assign labels to a set of arguments, S1:M ,
indexed from 1 to M . Each argument Si can take
any label from a set of argument labels, P , and the
indexed set of arguments can take a set of labels,
c1:M ? PM . If we assume that the argument classi-
fier returns an estimated conditional probability dis-
tribution, Prob(Si = ci), then, given a sentence, the
inference procedure seeks an global assignment that
maximizes the following objective function,
c?1:M = argmax
c1:M?PM
M
?
i=1
Prob(Si = ci),
subject to linguistic and structural constraints. In
other words, this objective function reflects the ex-
pected number of correct argument predictions, sub-
ject to the constraints. The constraints are encoded
as the followings.
? No overlapping or embedding arguments.
? No duplicate argument classes for A0-A5.
? Exactly one V argument per predicate considered.
? If there is C-V, then there has to be a V-A1-CV pattern.
? If there is an R-arg argument, then there has to be an arg
argument.
? If there is a C-arg argument, there must be an arg argu-
ment; moreover, the C-arg argument must occur after arg.
? Given the predicate, some argument types are illegal (e.g.
predicate ?stalk? can take only A0 or A1). The illegal
types may consist of A0-A5 and their corresponding C-
arg and R-arg arguments. For each predicate, we look
for the minimum value of i such that the class Ai is men-
tioned in its frame file as well as its maximum value j.
All argument types Ak such that k < i or k > j are
considered illegal.
182
2 Inference with Multiple SRL Systems
The inference process allows a natural way to com-
bine the outputs from multiple argument classi-
fiers. Specifically, given k argument classifiers
which perform classification on k argument sets,
{S1, . . . , Sk}. The inference process aims to opti-
mize the objective function:
c?1:N = argmax
c1:N?PN
N
?
i=1
Prob(Si = ci),
where S1:N = ?ki=1 Si, and
Prob(Si = ci) = 1k
k
?
j=1
Probj(Si = ci),
where Probj is the probability output by system j.
Note that all systems may not output with the
same set of argument candidates due to the pruning
and argument identification. For the systems that do
not output for any candidate, we assign the proba-
bility with a prior to this phantom candidate. In par-
ticular, the probability of the NULL class is set to be
0.6 based on empirical tests, and the probabilities of
the other classes are set proportionally to their oc-
currence frequencies in the training data.
For example, Figure 1 shows the two candidate
sets for a fragment of a sentence, ?..., traders say,
unable to cool the selling panic in both stocks and
futures.? In this example, system A has two argu-
ment candidates, a1 = ?traders? and a4 = ?the sell-
ing panic in both stocks and futures?; system B has
three argument candidates, b1 = ?traders?, b2 = ?the
selling panic?, and b3 = ?in both stocks and fu-
tures?. The phantom candidates are created for a2,
a3, and b4 of which probability is set to the prior.
Specifically for this implementation, we first train
two SRL systems that use Collins? parser and Char-
niak?s parser respectively. In fact, these two parsers
have noticeably different output. In evaluation, we
run the system that was trained with Charniak?s
parser 5 times with the top-5 parse trees output by
Charniak?s parser1. Together we have six different
outputs per predicate. Per each parse tree output, we
ran the first three stages, namely pruning, argument
1The top parse tree were from the official output by CoNLL.
The 2nd-5th parse trees were output by Charniak?s parser.
cool
1
b1
b4
a4
a2
2b 3b
a3
..., traders say, unable to the selling panic in both stocks and futures.
a
Figure 1: Two SRL systems? output (a1, a4, b1, b2,
and b3), and phantom candidates (a2, a3, and b4).
identification, and argument classification. Then a
joint inference stage is used to resolve the incon-
sistency of the output of argument classification in
these systems.
3 Learning and Evaluation
The learning algorithm used is a variation of the
Winnow update rule incorporated in SNoW (Roth,
1998; Roth and Yih, 2002), a multi-class classi-
fier that is tailored for large scale learning tasks.
SNoW learns a sparse network of linear functions,
in which the targets (argument border predictions
or argument type predictions, in this case) are rep-
resented as linear functions over a common feature
space. It improves the basic Winnow multiplicative
update rule with a regularization term, which has the
effect of trying to separate the data with a large mar-
gin separator (Grove and Roth, 2001; Hang et al,
2002) and voted (averaged) weight vector (Freund
and Schapire, 1999).
Softmax function (Bishop, 1995) is used to con-
vert raw activation to conditional probabilities. If
there are n classes and the raw activation of class i
is acti, the posterior estimation for class i is
Prob(i) = e
acti
?
1?j?n eactj
.
In summary, training used both full and partial
syntactic information as described in Section 1. In
training, SNoW?s default parameters were used with
the exception of the separator thickness 1.5, the use
of average weight vector, and 5 training cycles. The
parameters are optimized on the development set.
Training for each system took about 6 hours. The
evaluation on both test sets which included running
183
Precision Recall F?=1
Development 80.05% 74.83% 77.35
Test WSJ 82.28% 76.78% 79.44
Test Brown 73.38% 62.93% 67.75
Test WSJ+Brown 81.18% 74.92% 77.92
Test WSJ Precision Recall F?=1
Overall 82.28% 76.78% 79.44
A0 88.22% 87.88% 88.05
A1 82.25% 77.69% 79.91
A2 78.27% 60.36% 68.16
A3 82.73% 52.60% 64.31
A4 83.91% 71.57% 77.25
A5 0.00% 0.00% 0.00
AM-ADV 63.82% 56.13% 59.73
AM-CAU 64.15% 46.58% 53.97
AM-DIR 57.89% 38.82% 46.48
AM-DIS 75.44% 80.62% 77.95
AM-EXT 68.18% 46.88% 55.56
AM-LOC 66.67% 55.10% 60.33
AM-MNR 66.79% 53.20% 59.22
AM-MOD 96.11% 98.73% 97.40
AM-NEG 97.40% 97.83% 97.61
AM-PNC 60.00% 36.52% 45.41
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.16% 76.72% 77.44
R-A0 89.72% 85.71% 87.67
R-A1 70.00% 76.28% 73.01
R-A2 85.71% 37.50% 52.17
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 72.34% 65.38% 68.69
V 98.92% 97.10% 98.00
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
with all six different parse trees (assumed already
given) and the joint inference took about 4.5 hours.
Precision Recall F?=1
Charniak-1 75.40% 74.13% 74.76
Charniak-2 74.21% 73.06% 73.63
Charniak-3 73.52% 72.31% 72.91
Charniak-4 74.29% 72.92% 73.60
Charniak-5 72.57% 71.40% 71.98
Collins 73.89% 70.11% 71.95
Joint inference 80.05% 74.83% 77.35
Table 2: The results of individual systems and the
result with joint inference on the development set.
Overall results on the development and test sets
are shown in Table 1. Table 2 shows the results of
individual systems and the improvement gained by
the joint inference on the development set.
4 Conclusions
We present an implementation of SRL system which
composed of four stages?1) pruning, 2) argument
identification, 3) argument classification, and 4) in-
ference. The inference provides a natural way to
take the output of multiple argument classifiers and
combines them into a coherent predicate-argument
output. Significant improvement in overall SRL per-
formance through this inference is illustrated.
Acknowledgments
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA?s AQUAINT Program, DOI?s Re-
flex program, and an ONR MURI Award.
References
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
X. Carreras and L. Ma`rquez. 2004. Introduction to the conll-
2004 shared tasks: Semantic role labeling. In Proc. of
CoNLL-2004.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, Computer Science Depart-
ment, University of Pennsylvenia, Philadelphia.
Y. Freund and R. Schapire. 1999. Large margin classifica-
tion using the perceptron algorithm. Machine Learning,
37(3):277?296.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123?141.
T. Hang, F. Damerau, and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Seman-
tic role labeling via integer linear programming inference. In
Proc. of COLING-2004.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity &
relation recognition. In Proc. of COLING-2002, pages 835?
841.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. of AAAI, pages 806?813.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proc. of the EMNLP-2004, pages 88?94,
Barcelona, Spain.
184
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 251?261,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Translingual Document Representations from Discriminative Projections
John C. Platt Kristina Toutanova
Microsoft Research
1 Microsoft Way
Redmond, WA 98005, USA
{jplatt,kristout,scottyih}@microsoft.com
Wen-tau Yih
Abstract
Representing documents by vectors that are
independent of language enhances machine
translation and multilingual text categoriza-
tion. We use discriminative training to create
a projection of documents from multiple lan-
guages into a single translingual vector space.
We explore two variants to create these pro-
jections: Oriented Principal Component Anal-
ysis (OPCA) and Coupled Probabilistic Latent
Semantic Analysis (CPLSA). Both of these
variants start with a basic model of docu-
ments (PCA and PLSA). Each model is then
made discriminative by encouraging compa-
rable document pairs to have similar vector
representations. We evaluate these algorithms
on two tasks: parallel document retrieval
for Wikipedia and Europarl documents, and
cross-lingual text classification on Reuters.
The two discriminative variants, OPCA and
CPLSA, significantly outperform their corre-
sponding baselines. The largest differences in
performance are observed on the task of re-
trieval when the documents are only compa-
rable and not parallel. The OPCA method is
shown to perform best.
1 Introduction
Given the growth of multiple languages on the In-
ternet, Natural Language Processing must operate
on dozens of languages. It is becoming critical that
computers reach high performance on the following
two tasks:
? Comparable and parallel document re-
trieval ? Cross-language information retrieval
and text categorization have become impor-
tant with the growth of the Web (Oard and
Diekema, 1998). In addition, machine trans-
lation (MT) systems can be improved by
training on sentences extracted from paral-
lel or comparable documents mined from the
Web (Munteanu and Marcu, 2005). Compa-
rable documents can also be used for learning
word-level translation lexicons (Fung and Yee,
1998; Rapp, 1999).
? Cross-language text categorization ? Appli-
cations of text categorization, such as sentiment
classification (Pang et al, 2002), are now re-
quired to run on multiple languages. Catego-
rization is usually trained on the language of
the developer: it needs to be easily extended to
other languages.
There are two broad approaches to comparable
document retrieval and cross-language text catego-
rization. One approach is to translate queries or a
training set from different languages into a single
target language. Standard monolingual retrieval and
classification algorithms can then be applied in the
target language.
Alternatively, a cross-language system can project
a bag-of-words vector into a translingual lower-
dimensional vector space. Ideally, vectors in this
space represent the semantics of a document, inde-
pendent of the language.
The advantage of pre-translation is that MT sys-
tems tend to preserve the meaning of documents.
However, MT can be very slow (more than 1 second
per document), preventing its use on large training
sets. When full MT is not practical, a fast word-by-
word translation model can be used instead, (Balles-
teros and Croft, 1996) but may be less accurate.
Conversely, applying a projection into a low-
dimensional space is quick. Linear projection al-
gorithms use matrix-sparse vector multiplication,
which can be easily parallelized. However, as seen
in section 3, the accuracies of previous projection
251
techniques are not as high as machine translation.
This paper presents two techniques: Oriented
PCA and Coupled PLSA. These techniques retain
the high speed of projection, while approaching or
exceeding the quality level of word glossing. We im-
prove the quality of the projections by the use of dis-
criminative training: we minimize the difference be-
tween comparable documents in the projected vec-
tor space. Oriented PCA minimizes the difference
by modifying the eigensystem of PCA (Diamantaras
and Kung, 1996), while Coupled PLSA uses poste-
rior regularization (Graca et al, 2008; Ganchev et
al., 2009) on the topic assignments of the compara-
ble documents.
1.1 Previous work
There has been extensive work in projecting mono-
lingual documents into a vector space. The ini-
tial algorithm for projecting documents was Latent
Semantic Analysis (LSA), which modeled bag-of-
word vectors as low-rank Gaussians (Deerwester et
al., 1990). Subsequent projection algorithms were
based on generative models of individual terms in
the documents, including Probabilistic Latent Se-
mantic Analysis (PLSA) (Hofmann, 1999) and La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003).
Work on cross-lingual projections followed a sim-
ilar pattern of moving from Gaussian models to
term-wise generative models. Cross-language La-
tent Semantic Indexing (CL-LSI) (Dumais et al,
1997) applied LSA to concatenated comparable doc-
uments from multiple languages. Similarly, Polylin-
gual Topic Models (PLTM) (Mimno et al, 2009)
generalized LDA to tuples of documents from mul-
tiple languages. The experiments in section 3 use
CL-LSI and an algorithm similar to PLTM as bench-
marks.
The closest previous work to this paper is the
use of Canonical Correlation Analysis (CCA) to find
projections for multiple languages whose results are
maximally correlated with each other (Vinokourov
et al, 2003).
PLSA-, LDA-, and CCA-based cross-lingual
models have also been trained without the use of par-
allel or comparable documents, using only knowl-
edge from a translation dictionary to achieve sharing
of topics across languages (Haghighi et al, 2008; Ja-
garlamudi and Daume?, 2010; Zhang et al, 2010).
Such work is complementary to ours and can be
used to extend the models to domains lacking par-
allel documents.
Outside of NLP, researchers have designed al-
gorithms to find discriminative projections. We
build on the Oriented Principal Component Analysis
(OPCA) algorithm (Diamantaras and Kung, 1996),
which finds projections that maximize a signal-to-
noise ratio (as defined by the user). OPCA has been
used to create discriminative features for audio fin-
gerprinting (Burges et al, 2003).
1.2 Structure of paper
This paper now presents two algorithms for translin-
gual document projection (in section 2): OPCA and
Coupled PLSA (CPLSA). To explain OPCA, we
first review CL-LSI in section 2.1, then discuss the
details of OPCA (section 2.2), and compare it to
CCA (section 2.3). To explain CPLSA, we first
introduce Joint PLSA (JPLSA), analogous to CL-
LSI, in section 2.4, and then describe the details of
CPLSA (section 2.5).
We have evaluated these algorithms on two dif-
ferent tasks: comparable document retrieval (sec-
tion 3.2) and cross-language text categorization
(section 3.3). We discuss the findings of the evalua-
tions and extensions to the algorithms in section 4.
2 Algorithms for translingual document
projection
2.1 Cross-language Latent Semantic Indexing
Cross-language Latent Semantic Indexing (CL-LSI)
is Latent Semantic Analysis (LSA) applied to multi-
ple languages. First, we review the mathematics of
LSA.
LSA models an n ? k document-term matrix D,
where n is the number of documents and k is the
number of terms. The model of the document-term
matrix is a low-rank Gaussian. Originally, LSA was
presented as performing a Singular Value Decompo-
sition (Deerwester et al, 1990), but here we present
it as eigendecomposition, to clarify its relationship
with OPCA.
LSA first computes the correlation matrix be-
tween terms:
C = DTD. (1)
252
The Rayleigh quotient for a vector ~v with the matrix
C is
~vTC~v
~vT~v
, (2)
and is equal to the variance of the data projected us-
ing the vector ~v, normalized by the length of ~v, if D
has columns that are zero mean. Good projections
retain a large amount of variance. LSA maximizes
the Rayleigh ratio by taking its derivative against ~v
and setting it to zero. This yields a set of projections
that are eigenvectors of C,
C~vj = ?j~vj , (3)
where ?j is the jth-largest eigenvalue. Each eigen-
value is also the variance of the data when projected
by the corresponding eigenvector ~vj . LSA simply
uses top d eigenvectors as projections.
LSA is very similar to Principal Components
Analysis (PCA). The only difference is that the cor-
relation matrix C is used, instead of the covariance
matrix. In practice, the document-term matrix D is
sparse, so the column means are close to zero, and
the correlation matrix is close to the covariance ma-
trix.
There are a number of methods to form the
document-term matrix D. One method that works
well in practice is to compute the log(tf)-idf weight-
ing: (Dumais, 1990; Wild et al, 2005)
Dij = log2(fij + 1) log2(n/dj), (4)
where fij is the number of times term j occurs in
document i, n is the total number of documents,
and dj is the total number of documents that con-
tain term j. Applying a logarthm to the term counts
makes the distribution of matrix entries approach
Gaussian, which makes the LSA model more valid.
Cross-language LSI is an application of LSA
where each row of D is formed by concatenating
comparable or parallel documents in multiple lan-
guages. If a single term occurs in multiple lan-
guages, the term only has one slot in the concate-
nation, and the term count accumulates for all lan-
guages. Such terms could be proper nouns, such as
?Smith? or ?Merkel?.
In general, the elements of D are computed via
Dij = log2
(
?
m
fmij + 1
)
log2(n/dj), (5)
where fmij is the number of times term j occurs in
document i in language m. Here, dj is the number
of documents term j appears in, and n is the total
number of documents across all languages.
Because CL-LSI is simply LSA applied to con-
catenated documents, it models terms in document
vectors jointly across languages as a single low-rank
Gaussian.
2.2 Oriented Principal Component Analysis
The limitations of CL-LSI can be illustrated by con-
sidering Oriented Principal Components Analysis
(OPCA), a generalization of PCA. A user of OPCA
computes a signal covariance matrix S and a noise
covariance matrix N. OPCA projections ~vj max-
imize the ratio of the variance of the signal pro-
jected by ~vj to the variance of the noise projected
by ~vj . This signal-to-noise ratio is the generalized
Rayleigh quotient: (Diamantaras and Kung, 1996)
~vTS~v
~vTN~v
. (6)
Taking the derivative of the Rayleigh quotient with
respect to the projections ~v and setting it to zero
yields the generalized eigenproblem
S~vj = ?jN~vj . (7)
This eigenproblem has no local minima, and can be
solved with commonly available parallel code.
PCA is a specialization of OPCA, where the noise
covariance matrix is assumed to be the identity (i.e.,
uncorrelated noise). PCA projections maximize the
signal-to-noise ratio where the signal is the empiri-
cal covariance of the data, and the noise is spherical
white noise. PCA projections are not truly appropri-
ate for forming multilingual document projections.
Instead, we want multilingual document projec-
tions to maximize the projected covariance of doc-
ument vectors across all languages, while simulta-
neously minimizing the projected distance between
comparable documents (see Figure 1). OPCA gives
us a framework for finding such discriminative pro-
jections. The covariance matrix for all documents
is the signal covariance in OPCA, and captures the
meaning of documents across all languages. The
projection of this covariance matrix should be max-
imized. The covariance matrix formed from differ-
ences between comparable documents is the noise
253
covariance in OPCA: we wish to minimize the lat-
ter covariance, to make the projection language-
independent.
Specifically, we create the weighted document-
term matrix Dm for each language:
Dij,m = log2(f
m
ij + 1)log2(n/dj). (8)
We then derive a signal covariance matrix over all
languages:
S =
?
m
DTmDm/n? ~?
T
m~?m, (9)
where ~?m is the mean of each Dm over its columns,
and a noise covariance matrix,
N =
?
m
(Dm ?D)T (Dm ?D)/n+ ?I, (10)
where D is the mean across all languages of the
document-term matrix,
D =
1
M
?
m
Dm, (11)
and M is the number of languages. Applying equa-
tion (7) to these matrices and taking the top gener-
alized eigenvectors yields the projection matrix for
OPCA.
Note the regularization term of ?I in equation
(10). The empirical sample of comparable docu-
ments may not cover the entire space of translation
noise the system will encounter in the test set. For
safety, we add a regularizer that prevents the vari-
ance of a term from getting too small. We tuned ?
on the development sets in section 3.2: for log(tf)-
idf weighted vectors, C = 0.1 works well for the
data sets and dimensionalities that we tried. We use
C = 0.1 for all final tests.
2.3 Canonical Correlation Analysis
Canonical Correlation Analysis (CCA) is a tech-
nique that is related to OPCA. CCA was kernelized
and applied to creating cross-language document
models by (Vinokourov et al, 2003). In CCA, a lin-
ear projection is found for each language, such that
the projections of the corpus from each language are
maximally correlated with each other. Similar to
OPCA, this linear projection can be found by find-
ing the top generalized eigenvectors of the system
en
esen e
n
enes es
es
Maxim
izes ov
erall v
arianc
e
? whi
le min
imizin
g dista
nce 
betwe
en com
parab
le pair
s
Figure 1: OPCA finds a projection that maximizes the
variance of all documents, while minimizing distance be-
tween comparable documents
(7), where S is now a matrix of cross-correlations
that the projection maximizes,
S =
[
0 C12
C21 0
]
, (12)
and N is a matrix of autocorrelations that the projec-
tion minimizes,
N =
[
C11 + ?I 0
0 C22 + ?I
]
. (13)
Here, Cij is the (cross-)covariance matrix, with di-
mension equal to the vocabulary size, that is com-
puted between the document vectors for languages
i and j. Analogous to OPCA, ? is a regularization
term, set by optimizing performance on a validation
set. Like OPCA, these matrices can be generalized
to more than two languages. Unlike OPCA, CCA
finds projections that maximize the cross-covariance
between the projected vectors, instead of minimiz-
ing Euclidean distance.1
By definition, CCA cannot take advantage of the
information that same term occurs simultaneously in
comparable documents. As shown in section 3, this
1Note that the eigenvectors have length equal to the sum of
the length of the vocabularies of each language. The projections
for each language are created by splitting the eigenvectors into
sections, each with length equal to the vocabulary size for each
language.
254
information is useful and helps OPCA perform bet-
ter then CCA. In addition, CCA encourages compa-
rable documents to be projected to vectors that are
mutually linearly predictable. This is not the same
OPCA?s projected vectors that have low Euclidean
distance: the latter may be preferred by algorithms
that consume the projections.
2.4 Cross-language Topic Models
We now turn to a baseline generative model that
is analogous to CL-LSI. Our baseline joint PLSA
model (JPLSA) is closely related to the poly-lingual
LDA model of (Mimno et al, 2009). The graphical
model for JPLSA is shown at the top in Figure 2.
We describe the model for two languages, but it is
straightforward to generalize to more than two lan-
guages, as in (Mimno et al, 2009).
z
z
??
w
w
?
TD
N1
N2
z
z
?1
?
w
w
?
TD
N1
N2
?2
?
?
Figure 2: Graphical models for JPLSA (top) and CPLSA
(bottom)
The model sees documents di as sequences of
words w1, w2, . . . , wni from a vocabulary V . There
are T cross-language topics, each of which has a dis-
tribution ?t over words in V . In the case of mod-
els for two languages, we define the vocabulary V
to contain word types from both languages. In this
way, each topic is shared across languages.
Each topic-specific distribution ?t, for t =
1 . . . T , is drawn from a symmetric Dirichlet prior
with concentration parameter ?. Given the topic-
specific word distributions, the generative process
for a corpus of paired documents [d1i , d
2
i ] in two lan-
guages L1 and L2 is described in the next paragraph.
For each pair of documents, pick a distribution
over topics ?i, from a symmetric Dirichlet prior with
concentration parameter ?. Then generate the doc-
uments d1i and d
2
i in turn. Each word token in each
document is generated independently by first pick-
ing a topic z from a multinomial distribution with
parameter ?i (MULTI(?i)), and then generating the
word token from the topic-specific word distribution
for the chosen topic MULTI(?z).
The probability of a document pair [d1, d2] with
words [w11, w
1
2, . . . , w
1
n1 ], [w
2
1, w
2
2, . . . , w
2
n2 ], topic
assignments [z11 , . . . , z
1
n1 ], [z
2
1 , . . . , z
2
n2 ], and a com-
mon topic vector ? is given by:
P (?|?)
n1?
j=1
P (z1j |?)P (w
1
j |?z1j )
n2?
j=1
P (z2j |?)P (w
2
j |?z2j )
The difference between the JPLSA model and the
poly-lingual topic model of (Mimno et al, 2009)
is that we merge the vocabularies in the two lan-
guages and learn topic-specific word distributions
over these merged vocabularies, instead of having
pairs of topic-specific word distributions, one for
each language, like in (Mimno et al, 2009). Thus
our model is more similar to the CL-LSI model, be-
cause it can be seen as viewing a pair of documents
in two languages as one bigger document containing
the words in both documents.
Another difference between our model and the
poly-lingual LDA model of (Mimno et al, 2009)
is that we use maximum aposteriori (MAP) instead
of Bayesian inference. Recently, MAP inference
was shown to perform comparably to the best in-
ference method for LDA (Asuncion et al, 2009),
if the hyper-parameters are chosen optimally for
the inference method. Our initial experiments with
Bayesian versus MAP inference for parallel docu-
ment retrieval using JPLSA confirmed this result.
In practice our baseline model outperforms poly-
lingual LDA as mentioned in our experiments.
2.5 Coupled Probabilistic Latent Semantic
Analysis
The JPLSA model assumes that a pair of translated
or comparable documents have a common topic dis-
tribution ?. JPLSA fits its parameters to optimize the
probability of the data, given this assumption.
For the task of comparable document retrieval, we
want our topic model to assign similar topic distri-
butions ? to a pair of corresponding documents. But
255
this is not exactly what the JPLSA model is doing.
Instead, it derives a common topic vector ? which
explains the union of all tokens in the English and
foreign documents, instead of making sure that the
best topic assignment for the English document is
close to the best topic assignment of the foreign doc-
ument. This difference becomes especially appar-
ent when corresponding documents have different
lengths. In this case, the model will tend to derive
a topic vector ? which explains the longer document
best, making the sum of the two documents? log-
likelihoods higher. Modeling the shorter document?s
best topic carries little weight.
Modeling both documents equally is what Cou-
pled PLSA (CPLSA) is designed to do. The graphi-
cal model for CPLSA is shown at the bottom of Fig-
ure 2. In this figure, the topic vectors of a pair of
documents in two languages are shown completely
independent. We use the log-likelihood according to
this model, but also add a regularization term, which
tries to make the topic assignments of correspond-
ing documents close. In particular, we use poste-
rior regularization (Graca et al, 2008; Ganchev et
al., 2009) to place linear constraints on the expec-
tations of topic assignments to two corresponding
documents.
For two linked documents d1 and d2, we would
like our model to be such that the expected fraction
of tokens in d1 that get assigned topic t is approxi-
mately the same as the expected fraction of tokens in
d2 that get assigned the same topic t, for each topic
t = 1 . . . T . This is exactly what we need to make
each pair of corresponding documents close.
Let z1 and z2 denote vectors of topic assignments
to the tokens in document d1 and d2, respectively.
Their dimensionality is equal to the lengths of the
two documents, n1 and n2. We define a space of
posterior distributions Q over hidden topic assign-
ments to the tokens in d1 and d2, that has the desired
property: the expected fraction of each topic is ap-
proximately equal in d1 and d2. We can formulate
this constrained space Q as follows:
Q = {q1(z1), q2(z2)}
such that
Eq1 [
?n1
j=1 1(z
1
j = t)
n1
]?Eq2 [
?n2
j=1 1(z
2
j = t)
n2
] ? t
Eq2 [
?n2
j=1 1(z
2
j = t)
n2
]?Eq1 [
?n1
j=1 1(z
1
j = t)
n1
] ? t
We then formulate an objective function that max-
imizes the log-likelihood of the data while simulta-
neously minimizing the KL-divergence between the
desired distribution set Q and the posterior distri-
bution according to the model: P (z1|d1, ?1, ?) and
P (z2|d2, ?2, ?).
The objective function for a single document pair
is as follows:
logP (d1|?1, ?) + logP (d2|?2, ?)
?KL(Q||P (z1|d1, ?1, ?), P (z2|d2, ?2, ?))
?||||
The final corpus-wide objective is summed over
document-pairs, and also contains terms for the
probabilities of the parameters ? and ? given the
Dirichlet priors. The norm of  is minimized, which
makes the expected proportions of topics in two doc-
uments as close as possible.
Following (Ganchev et al, 2009), we fit the pa-
rameters by an EM-like algorithm, where for each
document pair, after finding the posterior distri-
bution of the hidden variables, we find the KL-
projection of this posterior onto the constraint set,
and take expected counts with respect to this projec-
tion; these expected counts are used in the M-step.
The projection is found using a simple projected gra-
dient algorithm.2
For both the baseline JPLSA and the CPLSA
models, we performed learning through MAP infer-
ence using EM (with a projection step for CPLSA).
We did up to 500 iterations for each model, and did
early stopping based on task performance on the de-
velopment set. The JPLSA model required more it-
erations before reaching its peak accuracy, tending
to require around 300 to 450 iterations for conver-
gence. CPLSA required fewer iterations, but each
iteration was slower due to the projection step.
2We initialized the models deterministically by assigning
each word to exactly one topic to begin with, such that all topics
have roughly the same number of words. Words were sorted by
frequency and thus words of similar frequency are more likely
to be assigned to the same topic.This initialization method out-
performed random initialization and we use it for all models.
256
All models use ? = 1.1 and ? = 1.01 for the
values of the concentration parameters. We found
that the performance of the models was not very sen-
sitive to these values, in the region that we tested
(?, ? ? [1.001, 1.1]). Higher hyper-parameter val-
ues resulted in faster convergence, but the final per-
formance was similar across these different values.
3 Experimental validation
We test the proposed discriminative projections ver-
sus more established cross-language models on the
two tasks described in the introduction: retrieving
comparable documents from a corpus, and training
a classifier in one language and using it in another.
We measure accuracy on a test set, and also examine
the sensitivity to dimensionality of the projection on
development sets.
3.1 Speed of training and evaluation
We first test the speed of the various algorithms dis-
cussed in this paper, compared to a full machine
translation system. When finding document projec-
tions, CL-LSI, OPCA, CCA, JPLSA, and CPLSA
are equally fast: they perform a matrix multiplica-
tion and require O(nk) operations, where n is the
number of distinct words in the documents and k is
the dimensionality of the projection.3 A single CPU
core can read the indexed documents into memory
and take logarithms at 216K words per second. Pro-
jecting into a 2000-dimensional space operates at
41K words per second. Translating word-by-word
operates at 274K words per second. In contrast, ma-
chine translation processes 50 words per second, ap-
proximately 3 orders of magnitude slower.
Total training time for OPCA on 43,380 pairs of
comparable documents was 90 minutes, running on
an 8-core CPU for 2000 dimensions. On the same
corpus, JPLSA requires 31 minutes per iteration and
CPLSA requires 377 minutes per iteration. CPLSA
requires a factor of five times fewer iterations: over-
all, it is twice as slow as JPLSA.
3.2 Retrieval of comparable documents
In comparable document retrieval, a query is a doc-
ument in one language, which is compared to a cor-
3For JPLSA and CPLSA this is the case only when perform-
ing a single EM iteration at test time, which we found to per-
form best.
pus of documents in another language. By mapping
all documents into the same vector space, the com-
parison is a vector comparison. For our experiments
with CL-LSI, OPCA, and CCA, we use cosine sim-
ilarity between vectors to rank the documents.
For the JPLSA and CPLSA models, we map the
documents to corresponding topic vectors ?, and
compute distance between these probability vectors.
The mapping to topic vectors requires EM iterations,
or folding-in (Hofmann, 1999). We found that per-
forming a single EM iteration resulted in best per-
formance so we used this for all models. For com-
puting distance we used the L1-norm of the differ-
ence, which worked a bit better than the Jensen-
Shannon divergence between the topic vectors used
in (Mimno et al, 2009).
We test all algorithms on the Europarl data set
of documents in English and Spanish, and a set of
Wikipedia articles in English and Spanish that con-
tain interlanguage links between them (i.e., articles
that the Wikipedia community have identified as
comparable across languages).
For the Europarl data set, we use 52,685 doc-
uments as training, 11,933 documents as a devel-
opment set, and 18,415 documents as a final test
set. Documents are defined as speeches by a sin-
gle speaker, as in (Mimno et al, 2009).4 For the
Wikipedia set, we use 43,380 training documents,
8,675 development documents, and 8,675 final test
set documents.
For both corpora, the terms are extracted by word-
breaking all documents, removing the top 50 most
frequent terms and keeping the next 20,000 most fre-
quent terms. No stemming or folding is applied.
We assess performance by testing each document
in English against all possible documents in Span-
ish, and vice versa. We measure the Top-1 accu-
racy (i.e., whether the true comparable is the clos-
est in the test set), and the Mean Reciprocal Rank
of the true comparable, and report the average per-
formance over the two retrieval directions. Ties are
counted as errors.
We tuned the dimensionality of the projections on
the development set, as shown in Figures 3 and 4.
4The training section contains documents from the years 96
through 99 and the year 02; the dev section contains documents
from 01, and the test section contains documents from 00 plus
the first 9 months of 03.
257
We chose the best dimension on the development set
for each algorithm, and used it on the final test set.
The regularization ? was tuned for CCA: ? = 10 for
Europarl, and ? = 3 for Wikipedia.
Figure 3: Mean reciprocal rank versus dimension for Eu-
roparl
Figure 4: Mean reciprocal rank versus dimension for
Wikipedia
In the two figures, we evaluate the five projec-
tion methods, as well as a word-by-word transla-
tion method (denoted by WbW in the graphs). Here
?word-by-word? refers to using cosine distance after
applying a word-by-word translation model to the
Spanish documents.
The word-by-word translation model was trained
on the Europarl training set, using the WDHMM
model (He, 2007), which performs similarly to IBM
Model 4. The probability matrix of generating
English words from Spanish words was multiplied
by each document?s log(tf)-idf vector to produce a
translated document vector. We found that multi-
plying the probability matrix to the log(tf)-idf vector
was more accurate on the development set than mul-
tiplying the tf vector directly. This vector was either
tested as-is, or mapped through LSA learned from
the English training set of the corpus. In the figures,
the dimensionality of WbW translation refers to the
dimensionality of monolingual LSA.
The overall ordering of the six models is dif-
ferent for the Europarl and Wikipedia development
datasets. The discriminative models outperform
the corresponding generative ones (OPCA vs CL-
LSI) and (CPLSA vs JPLSA) for both datasets, and
OPCA performs best overall, dominating the best
fast-translation based model, as well as the other
projection methods, including CCA.
On Europarl, JPLSA and CPLSA outperform CL-
LSI, with the best dimension or JPLSA also slightly
outperforming the best setting for the word-by-word
translation model, whereas on Wikipedia the PLSA-
based models are significantly worse than the other
models.
The results on the final test set, evaluating each
model using its best dimensionality setting, confirm
the trends observed on the development set. The fi-
nal results are shown in Tables 1 and 2. For these
experiments, we use the unpaired t-test with Bon-
ferroni correction to determine the smallest set of
algorithms that have statistically significantly better
accuracy than the rest. The p-value threshold for sig-
nificance is chosen to be 0.05. The accuracies for
these significantly superior algorithms are shown in
boldface.
For Wikipedia and Europarl, we include an ad-
ditional baseline model,?Untranslated?: this refers
to applying cosine distance to both the Spanish and
English documents directly (since they share some
vocabulary terms). For Wikipedia, comparable doc-
uments seem to share many common terms, so co-
sine distance between untranslated documents is a
reasonable benchmark.
From the final Europarl results we can see that the
best models can learn to retrieve parallel documents
from the narrow Europarl domain very well. All
dimensionality reduction methods can learn from
258
cleanly parallel data, but discriminative training can
bring additional error reduction.
In previously reported work, (Mimno et al, 2009)
evaluate parallel document retrieval using PLTM on
Europarl speeches in English and Spanish, using
training and test sets of size similar to ours. They
report an accuracy of 81.2% when restricting to test
documents of length at least 100 and using 50 topics.
JPLSA with 50 topics obtains accuracy of 98.9% for
documents of that length.
The final Wikipedia results are also similar to the
the development set results. The problem setting for
Wikipedia is different, because corresponding doc-
uments linked in Wikipedia may have widely vary-
ing degrees of parallelism. While most linked doc-
uments share some main topics, they could cover
different numbers of sub-topics at varying depths.
Thus the training data of linked documents is noisy,
which makes it hard for projection methods to learn.
The word-by-word translation model in this setting
is trained on clean, but out-of-domain parallel data
(Europarl), so it has the disadvantage that it may not
have a good coverage of the vocabulary; however,
it is not able to make use of the Wikipedia train-
ing data since it requires sentence-aligned transla-
tions. We find it encouraging that the best projection
method OPCA outperformed word-by-word trans-
lation. This means that OPCA is able to uncover
topic correspondence given only comparable docu-
ment pairs, and to learn well in this noisy setting.
The PLSA-based models fare worse on Wikipedia
document retrieval. CPLSA outperforms JPLSA
more strongly, but both are worse than CL-LSI and
even the Untranslated baseline. We think this is
partly explained by the diverse vocabulary in the het-
erogenous Wikipedia collection. All other models
use log(tf)-idf weighting, which automatically as-
signs importance weights to terms, whereas the topic
models use word counts. This weighting is very use-
ful for Wikipedia. For example, if we apply the
untranslated matching using raw word counts, the
MRR is 0.1024 on the test set, compared to 0.5383
for log(tf)-idf. We hypothesize that using a hierar-
chical topic model that automatically learns about
more general and more topic-specific words would
be helpful in this case. It is also possible that PLSA-
based models require cleaner data to learn well.
The overall conclusion is that OPCA outper-
Algorithm Dimension Accuracy MRR
OPCA 1000 0.9742 0.9806
CPLSA 1000 0.9716 0.9782
Word-by-word N/A 0.9707 0.9779
Word-by-word 5000 0.9706 0.9778
JPLSA 1000 0.9645 0.9726
CCA 1500 0.9613 0.9705
CL-LSI 3000 0.9457 0.9595
Untranslated N/A 0.1595 0.2564
Table 1: Test results for comparable document retrieval
in Europarl. Boldface indicates statistically significant
superior results.
Algorithm Dimension Accuracy MRR
OPCA 2000 0.7255 0.7734
Word-by-word N/A 0.7033 0.7467
CCA 1500 0.6894 0.7378
Word-by-word 5000 0.6786 0.7236
CL-LSI 5000 0.5302 0.6130
Untranslated N/A 0.4692 0.5383
CPLSA 200 0.4579 0.5130
JPLSA 1000 0.3322 0.3619
Table 2: Test results for comparable document retrieval
in Wikipedia. Boldface indicates statistically significant
best result.
formed all other document retrieval methods we
tested, including fast machine translation of docu-
ments. Additionally, both discriminative projection
methods outperformed their generative counterparts.
3.3 Cross-language text classification
The second task is to train a text categorization sys-
tem in one language, and test it with documents in
another. To evaluate on this task, we use the Mul-
tilingual Reuters Collection, defined and provided
by (Amini et al, 2009). We test the English/Spanish
language pair. The collection has news articles in
English and Spanish, each of which has been trans-
lated to the other by the Portage translation sys-
tem (Ueffing et al, 2007).
From the English news corpus, we take 13,131
documents as training, 1,875 documents as develop-
ment, and 1,875 documents as test. We take the En-
glish training documents translated into Spanish as
our comparable training data. For testing, we use the
entire Spanish news corpus of 12,342 documents, ei-
259
ther mapped with cross-lingual projection, or trans-
lated by Portage.
The data set was provided by (Amini et al,
2009) as already-processed document vectors, using
BM25 weighting. Thus, we only test OPCA, CL-
LSI, and related methods: JPLSA and CPLSA re-
quire modeling the term counts directly.
The performance on the task is measured by clas-
sification accuracy on the six disjoint category la-
bels defined by (Amini et al, 2009). To introduce
minimal bias due to the classifier model, we use 1-
nearest neighbor on top of the cosine distance be-
tween vectors as a classifier. For all of the tech-
niques, we treated the vocabulary in each language
as completely separate, using the top 10,000 terms
from each language.
Note that no Spanish labeled data is provided
for training any of these algorithms: only English
and translated English news is labeled. The op-
timal dimension (and ? for CCA) on the devel-
opment set was chosen to maximize the accuracy
of English classification and translated English-to-
Spanish classification.
Algorithm Dim. English Spanish
Accuracy Accuracy
Full MT 50 0.8483 0.6484
OPCA 100 0.8412 0.5954
Word-by-word 50 0.8483 0.5780
CCA 150 0.8388 0.5384
Full MT N/A 0.8046 0.5323
CL-LSI 150 0.8401 0.5105
Word-by-word N/A 0.8046 0.4481
Table 3: Test results for cross-language text categoriza-
tion
The test classification accuracy is shown in Ta-
ble 3. As above, the smallest set of superior al-
gorithms as determined by Bonferroni-corrected t-
tests are shown in boldface. The results for MT and
word-by-word translation use the log(tf)-idf vector
directly for documents that were written in English,
and use a Spanish-to-English translated vector if the
document was written in Spanish. As in section 3.2,
word-by-word translation multiplied each log(tf)-idf
vector by the translation probability matrix trained
on Europarl.
The tests show that OPCA is better than CCA,
CL-LSI, plain word-by-word translation, and even
full translation for Spanish documents. However,
if we post-process full translation by an LSI model
trained on the English training set, full translation
is the most accurate. If full translation is time-
prohibitive, then OPCA is the best method: it is sig-
nificantly better than word-by-word translation fol-
lowed by LSI.
4 Discussion and Extensions
OPCA extends naturally to multiple languages.
However, it requires memory and computation time
that scales quadratically with the size of the vocab-
ulary. As the number of languages goes up, it may
become impractical to perform OPCA directly on a
large vocabulary.
Researchers have solved the problem of scaling
OPCA by using Distortion Discriminant Analysis
(DDA) (Burges et al, 2003). DDA performs OPCA
in two stages which avoids the need for solving a
very large generalized eigensystem. As future work,
DDA could be applied to mapping documents in
many languages simultaneously.
Spherical Admixture Models (Reisinger et al,
2010) have recently been proposed that combine an
LDA-like hierarchical generative model with the use
of tf-idf representations. A similar model could be
used for CPLSA: future work will show whether
such a model can outperform OPCA.
5 Conclusions
This paper presents two different methods for creat-
ing discriminative projections: OPCA and CPLSA.
Both of these methods avoid the use of artificial
concatenated documents. Instead, they model docu-
ments in multiple languages, with the constraint that
comparable documents should map to similar loca-
tions in the projected space.
When compared to other techniques, OPCA had
the highest accuracy while still having a run-time
that allowed scaling to large data sets. We therefore
recommend the use of OPCA as a pre-processing
step for large-scale comparable document retrieval
or cross-language text categorization.
260
References
Massih-Reza Amini, Nicolas Usunier, and Cyril Goutte.
2009. Learning from multiple partially observed
views - an application to multilingual text categoriza-
tion. In Advances in Neural Information Processing
Systems 22 (NIPS 2009), pages 28?36.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of Uncertainty in Ar-
tificial Intelligence, pages 27?34.
Lisa Ballesteros and Bruce Croft. 1996. Dictionary
methods for cross-lingual information retrieval. In
Proceedings of the 7th International DEXA Confer-
ence on Database and Expert Systems Applications,
pages 791?801.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet alocation.
Journal of Machine Learning Research, 3:993?1022.
Christopher J.C. Burges, John C. Platt, and Soumya Jana.
2003. Distortion discriminant analysis for audio fin-
gerprinting. IEEE Transactions on Speech and Audio
Processing, 11(3):165?174.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Konstantinos I. Diamantaras and S.Y. Kung. 1996. Prin-
cipal Component Neural Networks: Theory and Appli-
cations. Wiley-Interscience.
Susan T. Dumais, Todd A. Letsche, Michael L. Littman,
and Thomas K. Landauer. 1997. Automatic cross-
language retrieval using latent semantic indexing. In
AAAI-97 Spring Symposium Series: Cross-Language
Text and Speech Retrieval.
Susan T. Dumais. 1990. Enhancing performance in la-
tent semantic indexing (LSI) retrieval. Technical Re-
port TM-ARH-017527, Bellcore.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of COLING-ACL, pages
414?420.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical Report MS-
CIS-09-16, University of Pennsylvania.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, edi-
tors, Advances in Neural Information Processing Sys-
tems 20, pages 569?576. MIT Press, Cambridge, MA.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. ACL, pages 771?
779.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL 2nd Statistical MT work-
shop, pages 80?87.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence, pages 289?296.
Jagadeesh Jagarlamudi and Hal Daume?, III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In ECIR.
David Mimno, Hanna W. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of Empir-
ical Methods in Natural Language Processing, pages
880?889.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Douglas W. Oard and Anne R. Diekema. 1998. Cross-
language information retrieval. In Martha Williams,
editor, Annual Review of Information Science (ARIST),
volume 33, pages 223?256.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proc. EMNLP, pages
79?86.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the ACL, pages 519?526.
Joseph Reisinger, Austin Waters, Bryan Silverthorn, and
Raymond J. Mooney. 2010. Spherical topic models.
In Proc. ICML.
Nicola Ueffing, Michel Simard, Samuel Larkin, and
J. Howard Johnson. 2007. NRC?s PORTAGE system
for WMT 2007. In ACL-2007 2nd Workshop on SMT,
pages 185?188.
Alexei Vinokourov, John Shawe-Taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
S. Thrun S. Becker and K. Obermayer, editors, Ad-
vances in Neural Information Processing Systems 15,
pages 1473?1480, Cambridge, MA. MIT Press.
Fridolin Wild, Christina Stahl, Gerald Stermsek, and
Gustaf Neumann. 2005. Parameters driving effective-
ness of automated essay scoring with LSA. In Pro-
ceedings 9th Internaional Computer-Assisted Assess-
ment Conference, pages 485?494.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proc. ACL,
pages 1128?1137, Uppsala, Sweden. Association for
Computational Linguistics.
261
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212?1222, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Polarity Inducing Latent Semantic Analysis
Wen-tau Yih Geoffrey Zweig John C. Platt
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{scottyih,gzweig,jplatt}@microsoft.com
Abstract
Existing vector space models typically map
synonyms and antonyms to similar word vec-
tors, and thus fail to represent antonymy. We
introduce a new vector space representation
where antonyms lie on opposite sides of a
sphere: in the word vector space, synonyms
have cosine similarities close to one, while
antonyms are close to minus one.
We derive this representation with the aid of a
thesaurus and latent semantic analysis (LSA).
Each entry in the thesaurus ? a word sense
along with its synonyms and antonyms ? is
treated as a ?document,? and the resulting doc-
ument collection is subjected to LSA. The key
contribution of this work is to show how to as-
sign signs to the entries in the co-occurrence
matrix on which LSA operates, so as to induce
a subspace with the desired property.
We evaluate this procedure with the Grad-
uate Record Examination questions of (Mo-
hammed et al 2008) and find that the method
improves on the results of that study. Further
improvements result from refining the sub-
space representation with discriminative train-
ing, and augmenting the training data with
general newspaper text. Altogether, we im-
prove on the best previous results by 11 points
absolute in F measure.
1 Introduction
Vector space representations have proven useful
across a wide variety of text processing applications
ranging from document clustering to search rele-
vance measurement. In these applications, text is
represented as a vector in a multi-dimensional con-
tinuous space, and a similarity metric such as co-
sine similarity can be used to measure the related-
ness of different items. Vector space representations
have been used both at the document and word lev-
els. At the document level, they are effective for
applications including information retrieval (Salton
and McGill, 1983; Deerwester et al 1990), docu-
ment clustering (Deerwester et al 1990; Xu et al
2003), search relevance measurement (Baeza-Yates
and Ribiero-Neto, 1999) and cross-lingual docu-
ment retrieval (Platt et al 2010). At the word level,
vector representations have been used to measure
word similarity (Deerwester et al 1990; Turney and
Littman, 2005; Turney, 2006; Turney, 2001; Lin,
1998; Agirre et al 2009; Reisinger and Mooney,
2010) and for language modeling (Bellegarda, 2000;
Coccaro and Jurafsky, 1998). While quite success-
ful, these applications have typically been consistent
with a very general notion of similarity in which
basic association is measured, and finer shades of
meaning need not be distinguished. For example,
latent semantic analysis might assign a high degree
of similarity to opposites as well as synonyms (Lan-
dauer and Laham, 1998; Landauer, 2002).
Independent of vector-space representations, a
number of authors have focused on identifying dif-
ferent kinds of relatedness. At the simplest level,
we may wish to distinguish between synonyms and
antonyms, which can be further differentiated. For
example, in synonymy, we may wish to distinguish
hyponyms and hypernyms. Moreover, Cruse (1986)
notes that numerous kinds of antonymy are possible,
for example antipodal pairs like ?top-bottom? or
1212
gradable opposites like ?light-heavy.? Work in this
area includes (Turney, 2001; Lin et al 2003; Tur-
ney and Littman, 2005; Turney, 2006; Curran and
Moens, 2002; van der Plas and Tiedemann, 2006;
Mohammed et al 2008; Mohammed et al 2011).
Despite the existence of a large amount of related
work in the literature, distinguishing synonyms and
antonyms is still considered as a difficult open prob-
lem in general (Poon and Domingos, 2009).
In this paper, we fuse these two strands of re-
search in an attempt to develop a vector space rep-
resentation in which the synonymy and antonymy
are naturally differentiated. We follow Schwab et
al. (2002) in requiring a representation in which
two lexical items in an antonymy relation should lie
at opposite ends of an axis. However, in contrast
to the logical axes used previously, we desire that
antonyms should lie at the opposite ends of a sphere
lying in a continuous and automatically induced vec-
tor space. To generate this vector space, we present
a novel method for assigning both negative and pos-
itive values to the TF-IDF weights used in latent se-
mantic analysis.
To determine these signed values, we exploit the
information present in a thesaurus. The result is a
vector space representation in which synonyms clus-
ter together, and the opposites of a word tend to clus-
ter together at the opposite end of a sphere.
This representation provides several advantages
over the raw thesaurus. First, by finding the items
most and least similar to a word, we are able to dis-
cover new synonyms and antonyms. Second, as dis-
cussed in Section 5, the representation provides a
natural starting point for gradient-descent based op-
timization. Thirdly, as we discuss in Section 6, it is
straightforward to embed new words into the derived
subspace by using information from a large unsuper-
vised text corpus such as Wikipedia.
The remainder of this paper is organized as fol-
lows. Section 2 describes previous work. Section 3
presents the classical LSA approach and analyzes
some of its limitations. In Section 4 we present our
polarity inducing extension to LSA. Section 5 fur-
ther extends the approach by optimizing the vector
space representation with supervised discriminative
training. Section 6 describes the proposed method of
embedding new words in the thesaurus-derived sub-
space. The experimental results of Section 7 indi-
cate that the proposed method outperforms previous
approaches on a GRE test of closest-opposites (Mo-
hammed et al 2008). Finally, Section 8 concludes
the paper.
2 Related Work
The detection of antonymy has been studied in a
number of previous papers. Mohammed et al(2008)
approach the problem by combining information
from a published thesaurus with corpus statistics de-
rived from the Google n-gram corpus (Brants and
Franz, 2006). Their method consists of two main
steps: first, detecting contrasting word categories
(e.g. ?WORK? vs. ?ACTIVITY FOR FUN?) and
then determining the degree of antonymy. Cate-
gories are defined by a thesaurus; contrasting cat-
egories are found by using affix rules (e.g., un- &
dis-) and WordNet antonymy links. Words belong-
ing to contrasting categories are treated as antonyms
and the degree of contrast is determined by distri-
butional similarity. Mohammed et al(2008) also
provides a publicly available dataset for detection of
antonymy, which we have adopted. This work has
been extended in (Mohammed et al 2011) to in-
clude a study of antonymy based on crowd-sourcing
experiments.
Turney (2008) proposes a unified approach to
handling analogies, synonyms, antonyms and asso-
ciations by transforming the last three cases into
cases of analogy. A supervised learning method
is then used to solve the resulting analogical prob-
lems. This is evaluated on a set of 136 ESL ques-
tions. Lin et al(2003) builds on (Lin, 1998) and
identifies antonyms as semantically related words
which also happen to be found together in a database
in pre-identified phrases indicating opposition. Lin
et al(2003) further note that whereas synonyms
will tend to translate to the same word in another
language, antonyms will not. This observation is
used to select antonyms from amongst distribution-
ally similar words. Antonymy is used in (de Si-
mone and Kazakov, 2005) for document clustering
and (Harabagiu et al 2006) to find contradiction.
The automatic detection of synonyms has been
more extensively studied. Lin (1998) presents
a thorough comparison of word-similarity metrics
based on distributional similarity, where this is de-
1213
termined from co-occurrence statistics in depen-
dency triples extracted by parsing a large dataset.
Related studies are described in (Curran and Moens,
2002; van der Plas and Bouma, 2005). Later, van
der Plas and Tiedemann (2006) extend the use of
multilingual data present in Lin et al(2003) by mea-
suring distributional similarity based on the contexts
that a word occurs in once translated into a new lan-
guage. This is used to improve the precision/recall
characteristics on synonym pairs. Structured infor-
mation can be important in determining relatedness,
and thesauri and Wikipedia links have been studied
in (Milne and Witten, 2008; Jarmasz and Szpakow-
icz, 2003). Combinations of approaches are studied
in (Turney et al 2003).
Vector-space models and latent semantic analysis
in particular have a long history of use in synonym
detection, which in fact was suggested in some of
the earliest LSA papers. Deerwester et al(1990)
defines a metric for measuring word similarity based
on LSA, and it has been used in (Landauer and Du-
mais, 1997; Landauer et al 1998) to answer word
similarity questions derived from the Test of English
as a Foreign Language (TOEFL). Turney (2001)
proposes the use of point-wise mutual information in
conjunction with LSA, and again presents results on
synonym questions derived from the TOEFL. Vari-
ants of vector space models are further analyzed
in (Turney and Littman, 2005; Turney, 2006; Tur-
ney and Pantel, 2010).
3 Latent Semantic Analysis
Latent Semantic Analysis (Deerwester et al 1990)
is a widely used method for representing words and
documents in a low dimensional vector space. The
method is based on applying singular value decom-
position (SVD) to a matrix W which indicates the
occurrence of words in documents. To perform
LSA, one proceeds as follows. The input is a col-
lection of d documents which are expressed in terms
of words from a vocabulary of size n. These docu-
ments may be actual documents such as newspaper
articles, or simply notional documents such as sen-
tences, or any other collection in which words are
grouped together. Next, a d?n document-term ma-
trix W is formed1. At its simplest form, the ijth
entry contains the number of times word j has oc-
curred in document i ? its term frequency or TF
value. More conventionally, the entry is weighted
by some notion of the importance of word j, for ex-
ample the negative logarithm of the fraction of doc-
uments that contain it, resulting in a TF-IDF weight-
ing (Salton et al 1975). The similarity between two
documents can be computed using the cosine simi-
larity of their corresponding row vectors:
sim(x,y) =
x ? y
? x ?? y ?
Similarly, the cosine similarity of two column vec-
tors can be used to judge the similarity of the corre-
sponding words. Finally, to obtain a subspace repre-
sentation of dimension k, W is decomposed as
W ? USV T
where U is d ? k, V T is k ? n, and S is a k ? k
diagonal matrix. In applications, k  n and k  d;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the columns
of SV T ? which now represent the words ? behave
similarly to the original columns of W , in the sense
that the cosine similarity between two columns in
SV T approximates the cosine similarity between the
corresponding columns in W . This follows from
the observation that W TW = V S2V T , and the fact
that the ijth entry of W TW is the dot product of
the ith and jth columns (words) in W . We will
use this observation subsequently in the derivation
of polarity-inducing LSA. For efficiency, we nor-
malize the columns of SV T to unit length, allow-
ing the cosine similarity between two words to be
computed with a single dot-product; this also has the
property of mapping each word to a point on a multi-
dimensional sphere.
A second important property of LSA is that in the
word representations which result can by viewed as
the result of applying a projection matrix U to the
original vectors as:
UTW = SV T
1(Bellegarda, 2000) constructs the transpose of this, but we
have found it convenient in data processing for documents to
represent rows.
1214
In Section 5, we will viewU simply as a d?k matrix
learned through gradient descent so as to optimize
an objective function.
3.1 Limitation of LSA
Word similarity as determined by LSA assigns high
values to words which tend to co-occur in doc-
uments. However, as noted by (Landauer and
Laham, 1998; Landauer, 2002), there is no no-
tion of antonymy; words with low or negative co-
sine scores are simply unrelated. In comparison,
words with high cosine similarity scores are typi-
cally semantically related, which includes both syn-
onyms and antonyms, as contrasting words often co-
occur (Murphy and Andrew, 1993; Mohammed et
al., 2008). To illustrate this, we have performed
SVD with the aid of the Encarta thesaurus developed
by Bloomsbury Publishing Plc. This thesaurus con-
tains approximately 47k word senses and a vocab-
ulary of 50k words and phrases. Each ?document?
is taken to be the thesaurus entry for a word-sense,
including synonyms and antonyms. For example,
the word ?admirable? induces a document consist-
ing of {admirable, estimable, commendable, vener-
able, good, splendid, worthy, marvelous, excellent,
unworthy}. Note that the last word in this set is its
antonym. Performing SVD on this set of thesaurus
derived ?meaning-documents? results in a subspace
representation for each word. This form of LSA is
similar to the use of Wikipedia in (Gabrilovich and
Markovitch, 2007).
Table 1 shows some words, their original the-
saurus documents, and the most and least similar
words in the LSA subspace. Several properties are
apparent:
? The vector-space representation of words is
able to identify related words that are not ex-
plicitly present in the original thesaurus. For
example, ?meritorious? for ?admirable? ? ar-
guably better than any of the words given in the
thesaurus itself.
? Similarity is based on co-occurrence, so the
co-occurrence of antonyms in the thesaurus-
derived documents induces their presence as
LSA-similar words. For example, ?con-
temptible? is identified as similar to ?ad-
mirable.? In the case of ?mourning,? opposites
acrimony rancor goodwill affection
acrimony 1 1 1 1
affection 1 1 1 1
Table 2: The W matrix for two thesaurus entries in
its original form. Rows represent documents; columns
words.
acrimony rancor goodwill affection
acrimony 1 1 -1 -1
affection -1 -1 1 1
Table 3: The W matrix for two thesaurus entries in its
polarity-inducing form.
such as ?joy? and ?elation? actually dominate
the list of LSA-similar words.
? The LSA-least-similar words have no relation-
ship at all to the word they are least-similar to.
For example, the least-similar word to ?consid-
ered? is ?ready-made-meal.?
In the next section, we will present a method for
inducing polarity in LSA subspaces, where opposite
words will tend to have negative cosine similarities,
analogous to the positive similarities of synonyms.
Thus, the least-similar words to a given word will be
its opposites.
4 Polarity Inducing LSA
We modify LSA so that we may exploit a thesaurus
to embed meaningful axes in the induced subspace
representation. Words with opposite meaning will
lie at opposite positions on a sphere. Recall that the
cosine similarity between word-vectors in the orig-
inal matrix W are preserved in the subspace repre-
sentation of words. Thus, if we construct the original
matrix so that the columns representing antonyms
will tend to have negative cosine similarities while
columns representing synonyms will tend to have
positive similarities, we will achieve the desired be-
havior.
This can be achieved by negating the TF-IDF en-
tries for the antonyms of a word when constructing
W from the thesaurus, which is illustrated in Ta-
bles 2 and 3. The two rows in these tables corre-
spond to thesaurus entries for the sense-categories
1215
Word Thesaurus Entry LSA Most-Similar Words LSA Least-Similar Words
admirable estimable, commendable,
venerable, good, splen-
did, worthy, marvelous,
excellent, unworthy
commendable, creditable,
laudable, praiseworthy,
worthy, meritorious,
scurvy, contemptible,
despicable, estimable
easy-on-the-eye, peace-
keeper, peace-lover,
conscientious-objector,
uninviting, dishy, dessert,
pudding, seductive
considered careful, measured, well-
thought-out, painstaking,
rash
calculated, premeditated,
planned, tactical, strate-
gic, thought-through, in-
tentional, fortuitous, pur-
poseful, unpremeditated
ready-made-meal, ready-
meal, disposed-to, apt-to,
wild-animals, big-game,
game-birds, game-fish,
rugger, rugby
mourning grief, bereavement, sor-
row, sadness, lamenta-
tion, woe, grieving, exul-
tation
sorrowfulness, anguish,
exultation, rejoicing, ju-
bilation, glee, heartache,
travail, joy, elation
muckiness, turn-the-
corner, impassibility,
filminess, pellucidity,
limpidity, sheerness
Table 1: LSA on a thesaurus. Thesaurus entries include antonyms in italics.
?acrimony,? and ?affection.? The thesaurus entries
induce two ?documents? containing the words and
their synonyms and antonyms. The complete set of
words is acrimony, rancor, goodwill, affection. For
simplicity, we assume that all TF-IDF weights are
1. In the original LSA formulation, we have the rep-
resentation of Table 2. ?Rancor? is listed as a syn-
onym of ?acrimony,? which has ?goodwill? and ?af-
fection? as its antonyms. This results in the first row.
Note that the cosine similarity between every pair of
words (columns) is 1.
Table 3 shows the polarity-inducing representa-
tion. Here, the cosine similarity between synony-
mous words (columns) is 1, and the cosine similarity
between antonymous words is -1. Since LSA tends
to preserve cosine similarities between words, in the
resulting subspace we may expect to find meaning-
ful axes, where opposite senses map to opposite ex-
tremes. We refer to this as polarity-inducing LSA or
PILSA.
In Table 4, we show the PILSA-similar and
PILSA-least-similar words for the same words as in
Table 1. We see now that words which are least
similar in the sense of having the lowest cosine-
similarity are indeed opposites. In this table gen-
erally the most similar words have similarities in the
range of 0.7 to 1.0 and the least similar in the range
of -0.7 to -1.0.
5 Discriminative Training
Although the cosine similarity of LSA-derived word
vectors are generally very effective in applications
such as judging the relevance of words or docu-
ments, or detecting antonyms as in our construction,
the process of singular value decomposition in LSA
does not explicitly try to achieve such goals. In this
section, we see that when supervised training data is
available, the projection matrix of LSA can be en-
hanced through a discriminative training technique
explicitly designed to create a representation suited
to a specific task.
Because LSA is closely related to principle com-
ponent analysis (PCA), extensions of PCA such as
canonical correlation analysis (CCA) and oriented
principle component analysis (OPCA) can leverage
the labeled data and produce the projection matrix
through general eigen-decomposition (Platt et al
2010). Along this line of work, Yih et al(2011)
proposed a Siamese neural network approach called
S2Net, which tunes the projection matrix directly
through gradient descent, and has shown to outper-
form other methods in several tasks. Below we de-
scribe briefly this technique and explain how we
adopt it for the task of antonym detection.
The goal of S2Net is to learn a concept vector
representation of the original sparse term vectors.
Although such transformation can be non-linear in
general, its current design chooses the model form
to be a linear projection matrix, which is identical to
1216
Word PILSA-Similar Words PILSA-Least-Similar Words
admirable commendable, creditable, laudable,
praiseworthy, worthy, meritorious, es-
timable, deserving, tiptop, valued
scurvy, contemptible, despicable,
lamentable, shameful, reprehensible,
unworthy, disgraceful, discreditable,
undeserving
considered calculated, premeditated, planned, tac-
tical, strategic, thought-through, inten-
tional, purposeful, intended, psycho-
logical
fortuitous, unpremeditated, unconsid-
ered, off-your-own-bat, unintended,
undirected, objectiveless, hit-and-miss,
unforced, involuntary
mourning sorrowful, doleful, sad, miserable,
wistful, pitiful, wailing, sobbing,
heavy-hearted, forlorn
smiley, happy, blissful, wooden, mirth-
ful, joyful, deadpan, fulfilled, straight-
faced, content
Table 4: PILSA on a thesaurus. Thesaurus entries are as in Table 1.
that of LSA, PCA, OPCA or CCA. Given a d-by-1
input vector f , the model of S2Net is a d-by-k ma-
trix A = [aij ]d?k, which maps f to a k-by-1 output
vector g = AT f . The fact that the transformation
can be viewed as a two-layer neural network leads
to the method?s name.
What differentiates S2Net from other approaches
is its loss function and optimization process. In
the ?parallel text? setting, the labeled data con-
sists of pairs of similar text objects such as doc-
uments. The objective of the training process is
to assign higher cosine similarities to these pairs
compared to others. More specifically, suppose the
training set consists of m pairs of raw input vectors
{(fp1 , fq1), (fp2 , fq2), ? ? ? , (fpm , fqm)}. Given a pro-
jection matrix A, the similarity score of any pair of
objects is simA(fpi , fqj ) = cosine(A
T fpi ,A
T fqj ).
Let ?ij = simA(fpi , fqi) ? simA(fpi , fqj ) be the
difference of the similarity scores of (fpi , fqi) and
(fpi , fqj ). The learning procedure tries to increase
?ij by using the following logistic loss:
L(?ij ;A) = log(1 + exp(???ij)),
where ? is a scaling factor that adjusts the loss func-
tion2. The loss of the whole training set is thus:
1
m(m? 1)
?
1?i,j?m,i 6=j
L(?ij ;A)
Parameter learning (i.e., tuning A) can be done
2As suggested in (Yih et al 2011), ? is set to 10 in our
experiments.
by standard gradient-based methods, such as L-
BFGS (Nocedal and Wright, 2006).
The original setting of S2Net can be directly ap-
plied to finding synonymous words, where the train-
ing data consists of pairs of vectors representing
two synonyms. It is also easy to modify the loss
function to apply it to the antonym detection prob-
lem. We first sample pairs of antonyms from the
thesaurus to create the training data. The raw input
vector f of a selected word is its corresponding col-
umn vector of the document-term matrix W (Sec-
tion 3) after inducing polarity (Section 4). When
each pair of vectors in the training data represents
two antonyms, we can redefine ?ij by flipping the
sign: ?ij = simA(fpi , fqj ) ? simA(fpi , fqi), and
leave others unchanged. As the loss function encour-
ages ?ij to be larger, an antonym pair will tend to
have a smaller cosine similarity than other pairs. Be-
cause S2Net uses a gradient descent technique and a
non-convex objective function, it is sensitive to ini-
tialization, and we have found that the PILSA pro-
jection matrix U (Section 3) provides an excellent
starting point. As illustrated in Section 7, learning
the word vectors with S2Net produces a significant
improvement over PILSA alone.
6 Extending PILSA to Out-of-thesaurus
Words
While PILSA is effective at representing synonym
and antonym information, in its pure form, it is lim-
ited to the vocabulary of the thesaurus. In order to
extend PILSA to operate on out-of-thesaurus words,
1217
we employ a two-stage strategy. We first conduct
some lexical analysis and try to match an unknown
word to one or more in-thesaurus words in their lem-
matized forms. If no such match can be found,
we then attempt to find semantically related in-
thesaurus words by leveraging co-occurrence statis-
tics from general text data. These two steps are de-
scribed in detail below.
6.1 Matching via Lexical Analysis
When a target word is not included in a thesaurus, it
is quite often that some of its morphological varia-
tions are covered. For example, although the Encarta
thesaurus does not have the word ?corruptibility,?
it does contain other forms like ?corruptible? and
?corruption.? Replacing the out-of-thesaurus target
word with these morphological variations may alter
the part-of-speech but typically does not change the
meaning3.
Given an out-of-thesaurus target word, we first
apply a morphological analyzer for English devel-
oped by Minnen et al(2001), which removes the
inflectional affixes and returns the lemma. If the
lemma still does not exist in the thesaurus, we then
apply Porter?s stemmer (Porter, 1980) and check
whether the target word can match any of the in-
thesaurus words in their stemmed forms. A sim-
ple rule that checks whether removing hyphens from
words can lead to a match and whether the target
word occurs as part of a compound word in the the-
saurus is applied when both morphological analysis
and stemming fail to find a match. When there are
more than one matched words, the centroid of their
PILSA vectors is used to represent the target word.
When there is only one matched word, the matched
word is treated as the target word.
6.2 Leveraging General Text Data
If no words in the thesaurus can be linked to the
target word through the simple lexical analysis pro-
cedure, we try to find matched words by creating
a context vector space model from a large docu-
ment collection, and then mapping from this space
to the PILSA space. We use contexts because of the
distributional hypothesis ? words that occur in the
same contexts tend to have similar meaning (Harris,
3The rules we use based on lexical analysis are moderately
conservative to avoid mistakes like mapping hopeless to hope.
1954). When a word is not in the thesaurus but ap-
pears in the corpus, we predict its PILSA vector rep-
resentation from the context vector space model by
using its k-nearest neighbors which are in the the-
saurus and consistent with each other.
6.2.1 Context Vector Space Model
Given a corpus of documents, we construct the
raw context vectors as follows. For each target word,
we first create a bag of words by collecting all the
terms within a window of [-10,+10] centered at each
occurrence of the target word in the corpus. The
non-identical terms form a term-vector, where each
term is weighted using its TF-IDF value. We then
perform LSA on the context-word matrix. The se-
mantic similarity/relatedness of two words can then
be determined using the cosine similarity of their
corresponding LSA word vectors. In the following
text, we refer to this LSA context vector space model
as the corpus space, in contrast to the PILSA the-
saurus space.
6.2.2 Embedding Out-of-Vocabulary Words
Given the context space model, we may use a
linear regression or a k-nearest neighbors approach
to embed out-of-thesaurus words into the thesaurus-
space representation. However, as near words in the
context space may be synonyms in addition to other
semantically related words (including antonyms),
such approaches can potentially be noisy. For ex-
ample, words like ?hot? and ?cold? may be close
to each other in the context space due to their sim-
ilar usage in text. An affine transform cannot ?tear
space? and map them to opposite poles in the the-
saurus space.
Therefore, we propose a revised k-nearest neigh-
bors approach. Suppose we are interested in an out-
of-thesaurus word w. We first find K-nearest in-
thesaurus neighbors to w in the context space. We
then select a subset of k members of these K words
such that the pairwise similarity of each of the k
members with every other member is positive. The
thesaurus-space centroid of these k items is com-
puted as w?s representation. This procedure has the
property that the k nearby words used to form the
embedding of a non-thesaurus word are selected to
be consistent with each other. In practice, we used
K = 10 and k = 3, which requires only around
1218
1000 pairwise computations even done in a brute-
force way. To provide a concrete example, if we
had the out-of-thesaurus word ?sweltering? with in-
thesaurus neighbors ?hot, cold, burning, scorching,
...? the procedure would return the centroid of ?hot,
burning, scorching? and exclude ?cold.?
7 Experimental Validation
In this section, we present our experimental results
on applying PILSA and its extensions to answering
the closest-opposite GRE questions.
7.1 Data Resources
The primary thesaurus we use is the Encarta The-
saurus developed by Bloomsbury Publishing Plc4.
Our version of this has approximately 47k word
senses and a vocabulary of 50k words, and con-
tains 125,724 pairs of antonyms. To experiment with
the effect of using a different thesaurus, we used
WordNet as an information source. Each synset in
WordNet maps to a row in the document-term ma-
trix; synonyms in a synset are weighted with posi-
tive TFIDF values, and antonyms are weighted neg-
ative TFIDF values. Entries corresponding to other
words in the vocabulary are 0. WordNet provides
significantly greater coverage with approximately
227k synsets involving multiple words, and a vo-
cabulary of about 190k words. However, it is also
much sparser, with 5.3 words per sense on average
as opposed to 10.3 in the thesaurus, and has only
62,821 pairs of antonyms. As general text data for
use in embedding out-of-vocabulary words, we used
a Nov-2010 dump of English Wikipedia, which con-
tains approximately 917M words.
7.2 Development and Test Data
For testing, we use the closest-opposite questions
from GRE tests provided by (Mohammed et al
2008). Each question contains a target word and
five choices, and asks which of the choice words has
the most opposite meaning to the target word. Two
datasets are made publicly available by Mohammad
et al(2008): the development set, which consists of
162 questions, and the test set, which has 950 ques-
tions5. We considered making our own, more exten-
4http://www.bloomsbury.com/
5http://www.umiacs.umd.edu/?saif/WebDocs/LC-
data/{devset,testset}.txt
Dimensions Bloomsbury Prec. WordNet Prec.
50 0.778 0.475
100 0.850 0.563
200 0.856 0.569
300 0.863 0.625
400 0.843 0.625
500 0.843 0.613
750 0.830 0.613
1000 0.837 0.544
2000 0.784 0.519
3000 0.778 0.494
Table 5: The performance of PILSA vs. the number of di-
mensions when applied to the closest-opposite questions
from the GRE development set. Out of the 162 ques-
tions, using the Bloomsbury thesaurus data we are able
to answer 153 of them. Using 300 dimensions gives the
best precision (132/153 = 0.863). This dimension set-
ting is also optimal when using the WordNet data, which
answers 100 questions correctly out of the 160 attempts
(100/160 = 0.625).
sive, test ? for example one which would require the
use of sentence context to choose between related
yet distributionally different antonyms (e.g. ?little,
small? as antonyms of ?big?) but chose to stick to a
previously used benchmark. This allows the direct
comparison with previously reported methods.
Some of these questions contain very rarely used
target or choice words, which are not included in
the thesaurus vocabulary. In order to provide a fair
comparison to existing methods, we do not try to
randomly answer these questions. Instead, when the
target word is out of vocabulary, we skip the whole
question. When the target word is in vocabulary but
one or more choices are unknown words, we ignore
those unknown words and pick the word with the
lowest cosine similarity from the rest as the answer.
The results of our methods are reported in precision
(the number of questions answered correctly divided
by the number of questions attempted), recall (the
number of questions answered correctly divided by
the number of all questions) and F1 (the harmonic
mean of precision and recall)6. We now turn to an
in-depth evaluation.
6Precision/recall/F1 were used in (Mohammed et al 2008)
as when their system ?did not find any evidence of antonymy
between the target and any of its alternatives, then it refrained
from attempting that question.? We adopt this convention to
provide a fair comparison to their system.
1219
Dev. Set Test Set
Prec Rec F1 Prec Rec F1
WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet signed-TFIDF w/o LSA 0.41 0.41 0.41 0.43 0.42 0.43
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
Bloomsbury lookup 0.65 0.61 0.63 0.61 0.56 0.59
Bloomsbury signed TFIDF w/o LSA 0.68 0.64 0.66 0.63 0.57 0.60
Bloomsbury PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Bloomsbury PILSA + S2Net 0.89 0.84 0.86 0.84 0.77 0.80
Bloomsbury PILSA + S2Net + Embedding 0.88 0.87 0.87 0.81 0.80 0.81
(Mohammed et al 2008) 0.76 0.66 0.70 0.76 0.64 0.70
Table 6: The overall results. PILSA performs LSA on the signed TF-IDF vectors.
7.3 Basic PILSA
When applying PILSA, we need to determine the
number of dimensions in the projected space. Eval-
uated on the GRE development set, Table 5 shows
the precision of PILSA, using two different training
datasets, Bloomsbury and WordNet, at different di-
mensions.
The Bloomsbury-based system is able to answer
153 questions, and the best dimension setting is
300, which answers 132 questions correctly and thus
archives 0.863 in precision. In contrast, the larger
vocabulary in WordNet helps the system answer 160
questions but the quality is not as good. We find
dimensions 300 and 400 are equally good, where
both answer 100 questions correctly (0.625 in pre-
cision)7. Because a lower number of dimensions
is preferred for saving storage space and computing
time, we choose 300 as the number of dimensions in
PILSA.
We now compare the proposed methods. All re-
sults are summarized in Table 6. When evaluated on
the GRE test set, the Bloomsbury thesaurus-based
methods (Lines 4?7) attempted 865 questions. The
precision, recall and F1 of the Bloomsbury-based
PILSA model (Line 6) are 0.81, 0.74 and 0.77,
which are all better than the best reported method
in (Mohammed et al 2008)8. In contrast, the
WordNet-based methods (Lines 1?3) attempted 936
7Note that the number of questions attempted is not a func-
tion of the number of dimensions.
8We take a conservative approach and assume that skipped
questions are answered incorrectly. The difference is statisti-
cally significant at 99% confidence level using a binomial test.
questions. However, consistent with what we ob-
served on the development set, the WordNet-based
model is inferior. Its precision, recall and F1 on
the test set are 0.60, 0.60 and 0.60 (Line 3). Al-
though the quality of the data source plays an im-
portant role, we need to emphasize that performing
LSA using our polarity inducing construction is in
fact a critical step in enhancing the model perfor-
mance. For example, directly using the antonym sets
in the Bloomsbury thesaurus gives 0.59 in F1 (Line
4), while using cosine similarity on the signed vec-
tors prior to LSA only reaches 0.60 in F1 (Line 5).
7.4 Improving Precision with Discriminative
Training
Building on the success of the unsupervised PILSA
model, we refine the projection matrix. As described
in Section 5, we take the PILSA projection matrix
as the initial model in S2Net and train the model
using 20,517 pairs of antonyms sampled from the
Bloomsbury thesaurus. A separate sample of 5,000
antonym pairs is used as the validation set for hyper-
parameter tuning in regularization. Encouragingly,
we found that the already strong results of PILSA
can indeed be improved, which gives 3 more points
in both precision (0.84), recall (0.77) and F1 (0.80).
7.5 Improving Recall with Unsupervised Data
We next evaluate our approach of extending the
word coverage with the help of an external text cor-
pus, as well as the lexical analysis procedure. Using
the Bloomsbury PILSA-S2Net thesaurus space and
the Wikipedia corpus space, our method increases
1220
recall by 3 points on the test set. Compared to the in-
vocabulary only setting, it attempted 75 more ques-
tions (865? 940) and had 33 of them correctly an-
swered.
While the accuracy on these questions is much
higher than random, the fact that it is substantially
below the precision of the original indicates some
room for improvement. We notice that the out-of-
thesaurus words are either offensive words excluded
in the thesaurus (e.g., moronic) or some very rarely
used words (e.g., froward). When the lexical analy-
sis procedure fails to match the target word to some
in-thesaurus words, the context vector embedding
approach solves the former case, but has difficulty
in handling the latter. The main reason is that such
words occur very infrequently in a general corpus,
which result in significant uncertainty in their se-
mantic vectors. Other than using a much larger
corpus, approaches that leverage character n-grams
may help. We leave this as future work.
8 Conclusion
In this paper we have tackled the problem of find-
ing a vector-space representation of words where,
by construction, synonyms and antonyms are easy
to distinguish. Specifically, we have defined a way
of assigning sign to the entries in the co-occurrence
matrix on which LSA operates, such that synonyms
will tend to have positive cosine similarity, and
antonyms will tend to have negative similarities. To
the best of our knowledge, our method of inducing
polarity to the document-term matrix before apply-
ing LSA is novel and has shown to effectively pre-
serve and generalize the synonymous/antonymous
information in the projected space. With this vector
space representation, we were able to bring to bear
the machinery of discriminative training in order to
further optimize the word representations. Finally,
by using the notion of closeness in this space, we
were able to embed new out-of-vocabulary words
into the space. On a standard test set, the proposed
methods improved the F measure by 11 points abso-
lute over previous results.
Acknowledgments
We thank Susan Dumais for her thoughtful com-
ments, Silviu-Petru Cucerzan for preparing the
Wikipedia data, and Saif Mohammed for sharing the
GRE datasets. We are also grateful to the anony-
mous reviewers for their useful suggestions.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceedings
of HLT-NAACL, pages 19?27.
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999.
Modern Information Retrieval. Addison-Wesley.
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, International Conference
on Spoken Language Processing (ICSLP-98).
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press.
James R. Curran and Marc Moens. 2002. Improvements
in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acqui-
sition - Volume 9, pages 59?66. Association for Com-
putational Linguistics.
Thomas de Simone and Dimitar Kazakov. 2005. Using
wordnet similarity and antonymy relations to aid doc-
ument retrieval. In Recent Advances in Natural Lan-
guage Processing (RANLP).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using wikipedia-based explicit se-
mantic analysis. In AAAI Conference on Artificial In-
telligence (AAAI).
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI Conference on Artificial Intelligence
(AAAI).
Zelig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Mario Jarmasz and Stan Szpakowicz. 2003. Rogets the-
saurus and semantic similarity. In Proceedings of the
International Conference on Recent Advances in Nat-
ural Language Processing (RANLP-2003).
1221
Thomas Landauer and Susan Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
T.K. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In Neural Information Processing
Systems (NIPS).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
T.K. Landauer. 2002. On the computational basis of
learning and cognition: Arguments from lsa. Psychol-
ogy of Learning and Motivation, 41:43?84.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In International Joint Conference on
Artificial Intelligence (IJCAI).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Milne and Ian H. Witten. 2008. An effective low-
cost measure of semantic relatedness obtained from
wikipedia links. In Proceedings of the AAAI 2008
Workshop on Wikipedia and Artificial Intelligence.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of english. Natural Lan-
guage Engineering, 7(3):207?223.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Gregory L. Murphy and Jane M. Andrew. 1993. The
conceptual basis of antonymy and synonymy in adjec-
tives. Journal of Memory and Language, 32(3):1?19.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251?261.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109?117.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw Hill.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
D. Schwab, M. Lafourcade, and V. Prince. 2002.
Antonymy and conceptual vectors. In International
Conference on Computational Linguistics (COLING).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, (37).
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In European Conference on
Machine Learning (ECML).
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Lonneke van der Plas and Gosse Bouma. 2005. Syntac-
tic contexts for finding semantically similar words. In
Proceedings of the Meeting of Computational Linguis-
tics in the Netherlands 2004 (CLIN).
Lonneke van der Plas and Jo?rg Tiedemann. 2006. Find-
ing synonyms using automatic word alignment and
measures of distributional similarity. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, COLING-ACL ?06, pages 866?873. Associa-
tion for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267?273, New York, NY,
USA. ACM.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteen Conference on Computational Nat-
ural Language Learning (CoNLL), pages 247?256,
Portland, Oregon, USA.
1222
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55?60,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Animacy Detection with Voting Models
Joshua L. Moore?
Dept. Computer Science
Cornell University
Ithaca, NY 14853
jlmo@cs.cornell.edu
Christopher J.C. Burges Erin Renshaw Wen-tau Yih
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{cburges, erinren, scottyih}
@microsoft.com
Abstract
Animacy detection is a problem whose solu-
tion has been shown to be beneficial for a
number of syntactic and semantic tasks. We
present a state-of-the-art system for this task
which uses a number of simple classifiers
with heterogeneous data sources in a voting
scheme. We show how this framework can
give us direct insight into the behavior of the
system, allowing us to more easily diagnose
sources of error.
1 Introduction
Animacy detection has proven useful for a va-
riety of syntactic and semantic tasks, such as
anaphora and coreference resolution (Ora?san and
Evans, 2007; Lee et al, 2013), verb argument dis-
ambiguation (Dell?Orletta et al, 2005) and depen-
dency parsing (?vrelid and Nivre, 2007). Existing
approaches for animacy detection typically rely on
two types of information: linguistic databases, and
syntactic cues observed from the corpus. They usu-
ally combine two types of approaches: rule based
systems, and machine learning techniques. In this
paper we explore a slightly different angle: we wish
to design an animacy detector whose decisions are
interpretable and correctable, so that downstream
semantic modeling systems can revisit those deci-
sions as needed. Thus here, we avoid defining
a large number of features and then using a ma-
chine learning method such as boosted trees, since
such methods, although powerful, result in hard-to-
interpret systems. Instead, we explore combining
interpretable voting models using machine learning
? Work performed while visiting Microsoft Research.
only to reweight their votes. We show that such
an approach can indeed result in a high perform-
ing system, with animacy detection accuracies in the
mid 90% range, which compares well with other re-
ported rates. Ensemble methods are well known (see
for example, Dietterich (2000)) but our focus here is
on using them for interpretability while still main-
taining accuracy.
2 Previous Work
2.1 Definitions of Animacy
Previous work uses several different definitions of
animacy. Ora?san and Evans (2007) define animacy
in the service of anaphora resolution: an NP is con-
sidered animate ?if its referent can also be referred
to using one of the pronouns he, she, him, her, his,
hers, himself, herself, or a combination of such pro-
nouns (e.g. his/her )?. Although useful for the task
at hand, this has counterintuitive consequences: for
example, baby may be considered animate or inan-
imate, and ant is considered inanimate (Ibid., Fig-
ure 1). Others have argued that animacy should be
captured by a hierarchy or by categories (Aissen,
2003; Silverstein, 1986). For instance, Zaenen et
al. (2004) propose three levels of animacy (human,
other animate and inanimate), which cover ten cat-
egories of noun phrases, with categories like ORG
(organization), ANIM (animal) and MAC (intelli-
gent machines such as robots) categorised as other
animate. Bowman and Chopra (2012) report results
for animacy defined both this way and with the cat-
egories collapsed to a binary (animate, inanimate)
definition.
55
2.2 Methods for Animacy Detection
Evans and Ora?san (2000) propose a rule-based sys-
tem based on the WordNet taxonomy (Fellbaum,
1998). Each synset is ascribed a binary animacy
label based on its unique beginner. A given noun
is then associated with the fraction of its animate
synsets (where all synsets are taken to be animate
or inanimate) and one minus that fraction, similarly
for a given verb. Animacy is then ascribed by ap-
plying a series of rules imposing thresholds on those
fractions, together with rules (and a gazetteer) to de-
tect names and acronyms, and a rule triggered by the
occurrence of who, or reflexives, in the NP. In later
work, Ora?san and Evans (2007) extend the algorithm
by propagating animacy labels in the WordNet graph
using a chi-squared test, and then apply a k-nearest
neighbor classifier based on four lexical features. In
their work, the only context used was the animacy of
the verb in the NP, for heads of subject NPs (e.g., the
subject of eat is typically animate). ?vrelid (2009)
and Bowman and Chopra (2012) extend this idea by
using dependency relations to generate features for
their classifier, enabled by corpora created by Zae-
nen et al (2004). In another approach, Ji and Lin
(2009) apply a simple ?relative-pronoun? pattern to
the Google n-gram corpus (Brants and Franz, 2006)
to assign animacy (see the List model in Section 5
for details). Although the animacy decision is again
context-independent, such a list provides a strong
baseline and thus benefit applications like anaphora
resolution (Lee et al, 2013).
3 The Task
We adopt a definition of animacy closest to the bi-
nary version in Bowman and Chopra (2012): we
define an entity to be animate if it is alive and has
the ability to move under its own will. We adopt
this simple definition because it fits well with the
common meaning and is therefore less error prone,
both in terms of incorporation into higher level mod-
els, and for labeling (Ora?san and Evans (2007) re-
port that the labeling of animacy tuned for anaphora
proved challenging for the judges). We also ap-
ply the label to single noun tokens where possible:
the only exceptions are compound names (?Sarah
Jones?) which are treated as single units. Thus,
for example, ?puppy food? is treated as two words,
with puppy animate and food inanimate. A more
complete definition would extend this to all noun
phrases, so that puppy food as a unit would be inan-
imate, a notion we plan to revisit in future work.
Note that even this simple definition presents chal-
lenges, so that a binary label must be applied de-
pending on the predominant meaning. In ?A plate
of chicken,? chicken is treated as inanimate since it
refers to food. In ?Caruso (1873-1921) is consid-
ered one of the world?s best opera singers. He...,?
although at the time of writing clearly Caruso was
not alive, the token is still treated as animate here
because the subsequent writing refers to a live per-
son.
4 The Data
We used the MC160 dataset, which is a subset of the
MCTest dataset and which is composed of 160 grade
level reading comprehension stories generated using
crowd sourcing (Richardson et al, 2013). Workers
were asked to write a short story (typically less than
300 words) with a target audience of 5 to 7 year
olds. The available vocabulary was limited to ap-
proximately 8000 words, to model the reading abil-
ity of a first or second grader. We labeled this data
for animacy using the definition given above. The
first 100 of the 160 stories were used as the training
set, and the remaining 60 were used for the test set.
These animacy labels will be made available on the
web site for MCTest (Richardson et al, 2013).
5 The Models
Since one of our key goals is interpretability we
chose to use an ensemble of simple voting models.
Each model is able to vote for the categories Ani-
mal, Person, Inanimate, or to abstain. The distinc-
tion between Animal and Person is only used when
we combine votes, where Animal and Person votes
appear as distinct inputs for the final voting combi-
nation model. Some voters do not distinguish be-
tween Person and Animal, and vote for Animate or
Inanimate. Our models are:
List: The n-gram list method from (Ji and Lin,
2009). Here, the frequencies with which the rela-
tive pronouns who, where, when, and which occur
are considered. Any noun followed most frequently
by who is classified as Animate, and any other noun
56
in the list is classified as Inanimate. This voter ab-
stains when the noun is not present in the list.
Anaphora Design: The WordNet-based approach
of Evans and Ora?san (2000).
WordNet: A simple approach using WordNet.
This voter chooses Animal or Person if the unique
beginner of the first synset of the noun is either of
these, and Inanimate otherwise.
WordSim: This voter uses the contextual vector
space model of Yih and Qazvinian (2012) computed
using Wikipedia and LA Times data. It uses short
lists of hand-chosen signal words for the categories
Animal, Person, and Inanimate to produce a ?re-
sponse? of the word to each category. This response
is equal to the maximum cosine similarity in the vec-
tor space of the query word to any signal word in the
category. The final vote goes to the category with
the highest response.
Name: We used an in-house named entity tagger.
This voter can recognize some inanimate entities
such as cities, but does not distinguish between peo-
ple and animals, and so can only vote Animate, Inan-
imate or Abstain.
Dictionaries: We use three different dictionary
sources (Simple English Wiktionary, Full English
Wiktionary, and the definitions found in Word-
Net) with a recursive dictionary crawling algorithm.
First, we fetch the first definition of the query and
use a dependency tree and simple heuristics to find
the head noun of the definition, ignoring qualifica-
tion NPs like ?piece? or ?member.? If this noun
belongs to a list of per-category signal words, the
voter stops and votes for that category. Otherwise,
the voter recursively runs on the found head noun.
To prevent cycling, if no prediction is made after 10
recursive lookups, the voter abstains.
Transfer: For each story, we first process each
sentence and detect instances of the patterns x
am/is/was/are/were y and y named x. In each of
these cases, we use majority vote of the remaining
voters to predict the animacy of y and transfer
its vote to x, applying this label (as a vote) to all
instances of x in the text.
The WordSim and Dictionaries voters share lists
of signal words, which were chosen early in the ex-
perimental process using the training set. The sig-
nal words for the Animal category were animal and
mammal1. Person contains person and people. Fi-
nally, Inanimate uses thing, object, space, place,
symbol, food, structure, sound, measure, and unit.
We considered two methods for combining vot-
ers: majority voting (where the reliable Name voter
overrides the others if it does not abstain) and a lin-
ear reweighting of votes. In the reweighting method,
a feature vector is formed from the votes. Except
for WordSim, this vector is an indicator vector of
the vote ? either Animal, Person, Animate (if the
voter doesn?t distinguish between animals and peo-
ple), Inanimate, or Abstain.
For Dictionaries, the vector?s non-zero compo-
nent is multiplied by the number of remaining al-
lowed recursive calls that can be performed, plus one
(so that a success on the final lookup gives a 1). For
example, if the third lookup finds a signal word and
chooses Animal, then the component corresponding
to Animal will have a value of 9.
For WordSim, instead of an indicator vector, the
responses to each category are used, or an indica-
tor for abstain if the model does not contain the
word. If the word is in the model, a second vec-
tor is appended containing the ratio of the maximum
response to the second-largest response in the com-
ponent for the maximum response category. These
per-voter feature vectors are concatenated to form a
35 dimensional vector, and a linear SVM is trained
to obtain the weights for combining the votes.
6 Results
We used the POS tagger in MSR SPLAT (Quirk et
al., 2012) to extract nouns from the stories in the
MC160 dataset and used these as labeled examples
for the SVM. This resulted in 5,120 extracted nouns
in the 100 training stories and 3,009 in the 60 test
stories. We use five-fold cross-validation on the
training set to select the SVM parameters. 57.2%
of the training examples were inanimate, as were
58.1% of the test examples.
Table 1 gives the test accuracy of each voter. List
1This was found to work well given typical dictionary defi-
nitions despite the fact that people are also mammals.
57
List Anaphora WNet WSim Dict Name
84.6 77.1 78.8 57.6 74.3 16.0
Table 1: Accuracy of various individual voters on the test
set. Abstentions are counted as errors. Note that Transfer
depends on a secondary source for classification, and is
therefore not listed here.
Majority SVM
N+WN+D+WS+AD+L 87.7 95.0
N+WN+WS 80.1 95.0
N+WN+D+WS+AD+L+T 87.4 95.0
N+WN+D+WS 86.4 94.8
N+WN+WS+AD+L 86.5 94.7
N+WN+D+WS+T 86.8 94.0
N+WN+D 86.1 93.7
N+WN 89.3 93.0
N+D 82.6 93.0
N+AD 87.6 89.4
N+L 85.4 88.9
Table 2: Accuracy of various combinations of voters
among Name (N), Anaphora Design (AD), List (L),
WordNet (WN), WordSim (WS), Dictionary (D), and
Transfer (T) under majority voting and SVM schemes.
Bold indicates a statistically significant difference over
the next lower bolded entry with p < 0.01, for the SVM.
comes out on top when taken alone, but we see in
later results that it is less critical when used with
other voters. Name performs poorly on its own, but
later we will see that it is a very accurate voter which
frequently abstains.
Table 2 gives the test performance of various com-
binations of voters, both under majority vote and
reweighting. Statistical significance was tested us-
ing a paired t-test, and bold indicates a method
was significant over the next lower bold line with
p value p < 0.01. We see a very large gain from
the SVM reweighting: 14.9 points in the case of
Name+WordNet+WordSim.
In Table 3, we show the results of ablation exper-
iments on the voters. We see that the most valuable
sources of information are WordSim and Dictionar-
ies.
Finally, in Table 4, we show a breakdown of
which voters cause the most errors, for the majority
vote system. In this table, we considered only ?fi-
nal errors,? i.e. errors that the entire system makes.
Over all such errors, we counted the number of times
Majority SVM
WordSim 87.6 93.7
SimpleWikt (dict) 87.3 94.1
FullWikt (dict) 86.4 94.3
Dict 87.4 94.5
Name 86.6 94.7
List 86.4 94.8
WordNet (dict) 88.7 94.8
WordNet 87.5 94.9
Anaphora Design 88.6 94.9
Transfer 87.7 95.0
Table 3: Test accuracy when leaving out various voters,
using both majority vote and and reweighting. Bold indi-
cates statistical significance over the next lower bold line
with p < 0.01.
each voter chose incorrectly, giving a count of how
many times each voter contributed to a final error.
We see that the Anaphora Design system has the
largest number of errors on both train and test sets.
After this, WordNet, List, and WordNet (dict) are also
large sources of error. On the other hand, Name and
WordSim have very few errors, indicating high re-
liability. The table also gives the number of criti-
cal errors, where the voter selected the wrong cate-
gory and was a deciding vote (that is, when chang-
ing its vote would have resulted in a correct overall
classification). We see a similar pattern here, with
Anaphora Design causing the most errors and Word-
Sim and Name among the most reliable. We included
Anaphora Design even though it uses a different def-
inition of animacy, to determine if its vote was nev-
ertheless valuable.
Error tables such as these show how voting mod-
els are more interpretable and therefore correctable
compared to more complex learned models. The ta-
bles indicate the largest sources of error and sug-
gest changes that could be made to increase accu-
racy. For example, we could make significant gains
by improving WordNet, WordNet (dictionary), or
List, whereas there is relatively little reason to ad-
just WordSim or Name.
7 Conclusions
We have shown that linear combinations of voting
models can give animacy detection rates in the mid
90% range. This is well above the accuracy found
58
Errors Critical
Train Test Train Test
Anaphora Design 555 266 117 76
WordNet 480 228 50 45
List 435 195 94 45
Transfer 410 237 54 58
WordNet (dict) 385 194 84 65
SimpleWikt (dict) 175 111 39 16
FullWikt (dict) 158 67 1 5
WordSim 107 89 11 19
Name 71 55 27 19
Table 4: Errors column: number of errors on train and
test where a source voted incorrectly, and was thus at
least in part responsible for an error of the overall sys-
tem. Critical column: number of errors on train and test
where a source voted incorrectly, and in addition cast a
deciding vote. Results are for majority vote.
by using the n-gram method of (Ji and Lin, 2009),
which is used as an animacy detection component
in other systems. In this sense the work presented
here improves upon the state of the art, but there are
caveats, since other workers define animacy differ-
ently and so a direct comparison with their work is
not possible. Our method has the added advantage
of interpretability, which we believe will be useful
when using it as a component in a larger system.
Acknowledgments
We wish to thank Andrzej Pastusiak for his help with
the labeling tool.
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language & Linguis-
tic Theory, 21(3):435?483.
Samuel Bowman and Harshit Chopra. 2012. Automatic
animacy classification. In Proceedings of the NAACL-
HLT 2012 Student Research Workshop.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Monte-
magni, and Vito Pirrelli. 2005. Climbing the path to
grammar: a maximum entropy model of subject/object
learning. In Proceedings of the Workshop on Psy-
chocomputational Models of Human Language Acqui-
sition, PMHLA ?05, pages 72?81, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. In Multiple Classifier Systems, pages
1?15.
Richard Evans and Constantin Ora?san. 2000. Improv-
ing anaphora resolution by identifying animate entities
in texts. In Proceedings of the Discourse Anaphora
and Reference Resolution Conference (DAARC2000),
pages 154?162.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from Web-scale n-grams for un-
supervised person mention detection. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 220?229, Hong
Kong, December. City University of Hong Kong.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Constantin Ora?san and Richard J. Evans. 2007. NP an-
imacy identification for anaphora resolution. Journal
of Artificial Intelligence Research (JAIR), 29:79?103.
Lilja ?vrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough ? Swedish
dependency parsing with rich linguistic features. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
pages 447?451.
Lilja ?vrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, Colin Cherry, and Lucy Vanderwende. 2012.
MSR SPLAT, a language analysis toolkit. In Proceed-
ings of the Demonstration Session at the Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 21?24, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Matthew Richardson, Chris Burges, and Erin Renshaw.
2013. MCTest: A challenge dataset for the open-
domain machine comprehension of text. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Michael Silverstein. 1986. Hierarchy of features and
ergativity. In P. Muysken and H. van Riemsdijk, ed-
itors, Features and Projections, pages 163?232. Foris
Publications Holland.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
59
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O?Connor, and Tom Wasow. 2004. An-
imacy encoding in English: Why and how. In Bon-
nie Webber and Donna K. Byron, editors, ACL 2004
Workshop on Discourse Annotation, pages 118?125,
Barcelona, Spain, July. Association for Computational
Linguistics.
60
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602?1612,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Multi-Relational Latent Semantic Analysis
Kai-Wei Chang?
University of Illinois
Urbana, IL 61801, USA
kchang10@illinois.edu
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Abstract
We present Multi-Relational Latent Seman-
tic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA). MRLSA pro-
vides an elegant approach to combining mul-
tiple relations between words by construct-
ing a 3-way tensor. Similar to LSA, a low-
rank approximation of the tensor is derived
using a tensor decomposition. Each word in
the vocabulary is thus represented by a vec-
tor in the latent semantic space and each re-
lation is captured by a latent square matrix.
The degree of two words having a specific
relation can then be measured through sim-
ple linear algebraic operations. We demon-
strate that by integrating multiple relations
from both homogeneous and heterogeneous
information sources, MRLSA achieves state-
of-the-art performance on existing benchmark
datasets for two relations, antonymy and is-a.
1 Introduction
Continuous semantic space representations have
proven successful in a wide variety of NLP and IR
applications, such as document clustering (Xu et al,
2003) and cross-lingual document retrieval (Dumais
et al, 1997; Platt et al, 2010) at the document level
and sentential semantics (Guo and Diab, 2012; Guo
and Diab, 2013) and syntactic parsing (Socher et
al., 2013) at the sentence level. Such representa-
tions also play an important role in applications for
lexical semantics, such as word sense disambigua-
tion (Boyd-Graber et al, 2007), measuring word
?Work conducted while interning at Microsoft Research.
similarity (Deerwester et al, 1990) and relational
similarity (Turney, 2006; Zhila et al, 2013; Mikolov
et al, 2013). In many of these applications, La-
tent Semantic Analysis (LSA) (Deerwester et al,
1990) has been widely used, serving as a fundamen-
tal component or as a strong baseline.
LSA operates by mapping text objects, typically
documents and words, to a latent semantic space.
The proximity of the vectors in this space implies
that the original text objects are semantically re-
lated. However, one well-known limitation of LSA
is that it is unable to differentiate fine-grained re-
lations. For instance, when applied to lexical se-
mantics, synonyms and antonyms may both be as-
signed high similarity scores (Landauer and Laham,
1998; Landauer, 2002). Asymmetric relations like
hyponyms and hypernyms also cannot be differenti-
ated. Although there exists some recent work, such
as PILSA which tries to overcome this weakness
of LSA by introducing the notion of polarity (Yih
et al, 2012). This extension, however, can only
handle two opposing relations (e.g., synonyms and
antonyms), leaving open the challenge of encoding
multiple relations.
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA), which strictly gener-
alizes LSA to incorporate information of multiple
relations concurrently. Similar to LSA or PILSA
when applied to lexical semantics, each word is still
mapped to a vector in the latent space. However,
when measuring whether two words have a specific
relation (e.g., antonymy or is-a), the word vectors
will be mapped to a new space according to the rela-
tion where the degree of having this relation will be
1602
judged by cosine similarity. The raw data construc-
tion in MRLSA is straightforward and similar to the
document-term matrix in LSA. However, instead of
using one matrix to capture all relations, we extend
the representation to a 3-way tensor. Each slice cor-
responds to the document-term matrix in the original
LSA design but for a specific relation. Analogous to
LSA, the whole linear transformation mapping is de-
rived through tensor decomposition, which provides
a low-rank approximation of the original tensor. As
a result, previously unseen relations between two
words can be discovered, and the information en-
coded in other relations can influence the construc-
tion of the latent representations, and thus poten-
tially improves the overall quality. In addition, the
information in different slices can come from het-
erogeneous sources (conceptually similar to (Riedel
et al, 2013)), which not only improves the model,
but also extends the word coverage in a reliable way.
We provide empirical evidence that MRLSA is ef-
fective using two different word relations: antonymy
and is-a. We use the benchmark GRE test of closest-
opposites (Mohammad et al, 2008) to show that
MRLSA performs comparably to PILSA, which was
the pervious state-of-the-art approach on this prob-
lem, when given the same amount of information. In
addition, when other words and relations are avail-
able, potentially from additional resources, MRLSA
is able to outperform previous methods significantly.
We use the is-a relation to demonstrate that MRLSA
is capable of handling asymmetric relations. We
take the list of word pairs from the Class-Inclusion
(i.e., is-a) relations in SemEval-2012 Task 2 (Jur-
gens et al, 2012), and use our model to measure the
degree of two words have this relation. The mea-
sures derived from our model correlate with human
judgement better than the best system that partici-
pated in the task.
The rest of this paper is organized as follows. We
first survey some related work in Section 2, followed
by a more detailed description of LSA and PILSA
in Section 3. Our proposed model, MRLSA, is pre-
sented in Section 4. Section 5 presents our experi-
mental results. Finally, Section 6 concludes the pa-
per.
2 Related Work
MRLSA can be viewed as a model that derives gen-
eral continuous space representations for capturing
lexical semantics, with the help of tensor decompo-
sition techniques. We highlight some recent work
related to our approach.
The most commonly used continuous space rep-
resentation of text is arguably the vector space
model (VSM) (Turney and Pantel, 2010). In this
representation, each text object can be represented
by a high-dimensional sparse vector, such as a
term-vector or a document-vector that denotes the
statistics of term occurrences (Salton et al, 1975)
in a large corpus. The text can also be repre-
sented by a low-dimensional dense vector derived
by linear projection models like latent semantic
analysis (LSA) (Deerwester et al, 1990), by dis-
criminative learning methods like Siamese neural
networks (Yih et al, 2011), recurrent neural net-
works (Mikolov et al, 2013) and recursive neu-
ral networks (Socher et al, 2011), or by graphical
models such as probabilistic latent semantic anal-
ysis (PLSA) (Hofmann, 1999) and latent Dirichlet
allocation (LDA) (Blei et al, 2003). As a general-
ization of LSA, MRLSA is also a linear projection
model. However, while the words are represented
by vectors as well, multiple relations between words
are captured separately by matrices.
In the context of lexical semantics, VSMs provide
a natural way of measuring semantic word related-
ness by computing the distance between the cor-
responding vectors, which has been a standard ap-
proach (Agirre et al, 2009; Reisinger and Mooney,
2010; Yih and Qazvinian, 2012). These approaches
do not apply directly to the problem of modeling
other types of relations. Existing methods that do
handle multiple relations often use a model com-
bination scheme to integrate signals from various
types of information sources. For instance, mor-
phological variations discovered from the Google
n-gram corpus have been combined with informa-
tion from thesauri and vector-based word related-
ness models for detecting antonyms (Mohammad et
al., 2008). An alternative approach proposed by Tur-
ney (2008) that handles synonyms, antonyms and
associations is to use a uniform approach by first
reducing the problem to determining whether two
1603
pairs of words can be analogous, and then predicting
it using a supervised model with features based on
the frequencies of patterns in the corpus. Similarly,
to measure whether two word pairs have the same
relation, Zhila et al (2013) proposed to combine het-
erogeneous models, which achieved state-of-the-art
performance. In comparison, MRLSA models mul-
tiple lexical relations holistically. The degree that
two words having a particular relation is estimated
using the same linear function of the corresponding
vectors and matrix.
Tensor decomposition generalizes matrix factor-
ization and has been applied to several NLP applica-
tions recently. For example, Cohen et al (2013) pro-
posed an approximation algorithm for PCFG pars-
ing that relies on Kruskal decomposition. Van de
Cruys et al (2013) modeled the composition of
subject-verb-object triples using Tucker decompo-
sition, which results in a better similarity measure
for transitive phrases. Similar to this construction
but used in the community-based question answer-
ing (CQA) scenario, Qiu et al (2013) represented
triples of question title, question content and answer
as a tensor and applied 3-mode SVD to derive latent
semantic representations for question matching. The
construction of MRLSA bears some resemblance to
the work that use tensors to capture triples. How-
ever, our goal of modeling different relations for lex-
ical semantics is very different from the intended us-
age of tensor decomposition in the existing work.
3 Latent Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used continuous vector space
model that maps words and documents into a low
dimensional space. LSA consists of two main steps.
First, taking a collection of d documents that con-
tains words from a vocabulary list of size n, it first
constructs a d ? n document-term matrix W to en-
code the occurrence information of a word in a docu-
ment. For instance, in its simplest form, the element
Wi,j can be the term frequency of the j-th word in
the i-th document. In practice, a weighting scheme
that better captures the importance of a word in the
document, such as TF?IDF (Salton et al, 1975),
is often used instead. Notice that ?document? here
simply means a group of words and has been applied
W V X = U T
Figure 1: SVD applied to a d?n document-term ma-
trix W. The rank-k approximation, X, is the mul-
tiplication of U, ? and VT , where U and V are
d ? k and n ? k orthonormal matrices and ? is a
k ? k diagonal matrix. The column vectors of VT
multiplied by the singular values ? represent words
in the latent semantic space.
to various texts including news articles, sentences
and bags of words. Once the matrix is constructed,
the second step is to apply singular value decom-
position (SVD) to W in order to derive a low-rank
approximation. To have a rank-k approximation, X
is the reconstruction matrix of W, defined as
W ? X = U?VT (1)
where the dimensions of U and V are d? k and
n? k, respectively, and ? is a k ? k diagonal ma-
trix. In addition, the columns in U and V are or-
thonormal and the elements in ? are the singular
values and are conventionally reverse-ordered. Fig-
ure 1 illustrates this decomposition.
LSA can be used to compute the similarity be-
tween two documents or two words in the latent
space. For instance, to compare the u-th and v-th
words in the vocabulary, one can compute the co-
sine similarity of the u-th and v-th column vectors
of X, the reconstruction matrix of W. In contrast to
a direct lexical matching via the columns of W, the
similarity measure computed as a result of the SVD
may have a nonzero similarity score even if these
two words do not co-occur in any documents. This
is due to the fact that those words can share some
latent components.
An alternative view of using LSA is to treat the
column vectors of ?VT as a representation of the
words in a new k-dimensional latent space. This
comes from the observation that the inner product
of every two column vectors in X is the inner prod-
uct of the corresponding column vectors of ?VT ,
1604
joyfulness
gladden
sad
1
anger
1
-1
0
1
1
0
0
-1
0
1
0
0
-1
1
0
0
0
0
1
0
0
0
0
0
0
0
0
Figure 2: The matrix construction of PILSA. The
vocabulary is {joy, gladden, sorrow, sadden, anger,
emotion, feeling} and target words are {joyfulness,
gladden, sad, anger}. For ease of presentation,
we show the numbers with 0-1 values instead of
TF?IDF scores. The polarity (i.e., sign) indicates
whether the term in the vocabulary is a synonym or
antonym of the target word.
which can be derived from the equations below.
XTX = (U?VT )T (U?VT )
= V?UTU?VT (? is diagonal)
= V?2VT (Columns of U are orthonormal)
= (?VT )T (?VT ) (2)
Thus, the semantic relatedness between the i-th and
j-th words can be computed by cosine similarity1:
cos(X:,i,X:,j) (3)
When used to compare words, one well-known
limitation of LSA is that the score captures the gen-
eral notion of semantic similarity, and is unable
to distinguish fine-grained word relations, such as
antonyms (Landauer and Laham, 1998; Landauer,
2002). This is due to the fact that the raw matrix rep-
resentation only records the occurrences of words in
documents without knowing the specific relation be-
tween the word and document. To address this issue,
Yih et al (2012) proposed a polarity inducing latent
semantic analysis model recently, which we intro-
duce next.
1Cosine similarity is equivalent to the inner product of the
normalized vectors.
3.1 Polarity Inducing Latent Semantic
Analysis
In order to distinguish antonyms from synonyms,
the polarity inducing LSA (PILSA) model (Yih et
al., 2012) takes a thesaurus as input. Synonyms and
antonyms of the same target word are grouped to-
gether as a ?document? and a document-term matrix
is constructed accordingly as done in LSA. Because
each word in a group belongs to either one of the two
opposite relations, synonymy and antonymy, the po-
larity information is induced by flipping the signs of
antonyms. While the absolute value of each element
in the matrix is still the same TF?IDF score, the
elements that correspond to the antonyms become
negative.
This design has an intriguing effect. When com-
paring two words using the cosine similarity (or sim-
ply inner product) of their corresponding column
vectors in the matrix, the score of a synonym pair
remains positive, but the score of an antonym pair
becomes negative. Figure 2 illustrates this design
using a simplified matrix as example.
Once the matrix is constructed, PILSA applies
SVD as done in LSA, which generalizes the model
to go beyond lexical matching. The sign of the co-
sine score of the column vectors of any two words
indicates whether they are close to synonyms or to
antonyms and the absolute value reflects the degree
of the relation. When all the column vectors are nor-
malized to unit vectors, it can also be viewed as syn-
onyms are clustered together and antonyms lie on
the opposite sides of a unit sphere. Although PILSA
successfully extends LSA to handle not just one sin-
gle occurrence relation, the extension is limited to
encoding two opposing relations
4 Multi-Relational Latent Semantic
Analysis
The fundamental reason why it is difficult to handle
multiple relations is due to the 2-dimensional ma-
trix representation. In order to overcome this, we
encode the raw data in a 3-way tensor. Each slice
captures a particular relation and is in the format of
the document-term matrix in LSA. Just as in LSA,
where the low-rank approximation by SVD helps
generalize the representation and discover unseen
relations, we apply a tensor decomposition method,
1605
joyfulness
gladden
sad
1
anger
1
0
0
1
1
0
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
(a) Synonym layer
joyfulness
gladden
sad
0
anger
0
1
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(b) Antonym layer
joyfulness
gladden
sad
0
anger
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
1
0
1
1
(c) Hypernym layer
Figure 3: The three slices of MRLSA raw tensorW for an example with vocabulary {joy, gladden, sorrow,
sadden, anger, emotion, feeling} and target words {joyfulness, gladden, sad, anger}. Figures 3(a), 3(b), 3(c)
show the matrices W:,:,syn, W:,:,ant, W:,:,hyper, respectively. Rows represent documents (see definition in
text), and columns represent words. For ease of presentation, we show numbers with 0-1 values instead of
TF?IDF scores.
the Tucker decomposition, to the tensor.
4.1 Representing Multi-Relational Data in
Tensors
A tensor is simply a multi-dimensional array. In this
work, we use a 3-way tensor W to encode multi-
ple word relations. An element of W is denoted
by Wi,j,k using its indices, and W:,:,k represents
the k-th slice of W (a slice of a 3-way tensor is
a matrix, obtained by fixing the third index). Fol-
lowing (Kolda and Bader, 2009), a fiber of a ten-
sor W:,j,k is a vector, which is a high order analog
of a matrix row or column.
When constructing the raw tensorW in MRLSA,
each slice is analogous to the document-term ma-
trix in LSA, but created based on the data of a par-
ticular relation, such as synonyms. With a slight
abuse of notation, we sometimes use the value rather
than index when there is no confusion. For in-
stance, W:,?word?,k represents the fiber correspond-
ing to the ?word? in slice k, and W:,:,syn refers to
the slice that encodes the synonymy relation. Below
we use an example to compare this construction to
the raw matrix in PILSA, and discuss how it extends
LSA.
Suppose we are interested in representing two re-
lations, synonymy and antonymy. The raw tensor in
MRLSA would then consist of two slices, W:,:,syn
and W:,:,ant, to encode synonyms and antonyms of
target words from a knowledge source (e.g., a the-
saurus). Each row in W:,:,syn represents the syn-
onyms of a target word, and the corresponding
row in W:,:,ant encodes its antonyms. Figures 3(a)
and 3(b) illustrate an example, where ?joy?, ?glad-
den? are synonyms of the target word ?joyfulness?
and ?sorrow? is its antonym. Therefore, the values
of the corresponding entries are 1. Notice that the
matrix W? = W:,:,syn ? W:,:,ant is identical to the
PILSA raw matrix. We can extend the construction
above to enable MRLSA to utilize other semantic
relations (e.g., hypernymy) by adding a slice cor-
responding to each relation of interest. Fig. 3(c)
demonstrates how to add another slice W:,:,hyper to
the tensor for encoding hypernyms.
4.2 Tensor Decomposition
The MRLSA raw tensor encodes relations in one or
more data resources, such as thesauri. However, the
knowledge from a thesaurus is usually noisy and in-
complete. In this section, we derive a low-rank ap-
proximation of the tensor to generalize the knowl-
edge. This step is analogous to the rank-k approxi-
mation in LSA.
Various tensor decomposition methods have been
proposed in literature. Among them, Tucker decom-
position (Tucker, 1966) is recognized as a multi-
dimensional extension of SVD and has been widely
used in many applications. An illustration of this
method is in Fig. 4(a). In Tucker decomposition,
a d? n?m tensor W is decomposed into four
components G,U,V,T. A low-rank approximation
1606
X U VG
T
T=W  
(a) Tucker Tensor Decomposition
X U VS: , : , 1 T=
S
(b) Our Reformulation
Figure 4: Fig. 4(a) illustrates the Tucker tensor decomposition method which factors a 3-way tensorW to
three orthogonal matrices, U,V,T, and a core tensor G. We further apply a n-mode matrix product on the
core tensor G with T. Consequently, each slice of the resulted core tensor S (a square matrix) captures a
semantic relation type, and each column of VT is a vector representing a word.
X ofW is defined by
Wi,j,k ? Xi,j,k
=
R1?
r1=1
R2?
r2=1
R3?
r3=1
Gr1,r2,r3Ui,r1Vj,r2Tk,r3 ,
where G is a core tensor with dimensionsR1?R2?
R3 and U,V,T are orthogonal matrices with di-
mensions d ? R1, n ? R2,m ? R3, respectively.
The rank parameters R1 ? d,R2 ? n,R3 ? m are
given as input to the algorithm. In MRLSA, m (the
number of relations) is usually small, while d and n
are typically large (often in the scale of hundreds
of thousands). Therefore, we choose R1 = R2 = ? ,
?  d, n andR3 = m, where ? is typically less than
1000.
To make the analogy to SVD clear, we rewrite the
results of Tucker decomposition by performing a n-
mode matrix product over the core tensor G with the
matrix T. This produces a tensor S where each slice
is a linear combination of the slices of G with coeffi-
cients given by T (see (Kolda and Bader, 2009) for
detail). That is, we have
S:,:,k =
m?
t=1
Tt,kG:,:,t, ?k.
An illustration is shown in Fig. 4(b), Then, a
straightforward calculation shows that k-th slice of
tensorW is approximated by
W:,:,k ? X:,:,k = US:,:,kV
T . (4)
Comparing Eq. (4) to Eq. (1), one can observe
that matrices U and V play similar roles here, and
each slice of the core tensor S is analogous to ?.
However, the square matrix G:,:,k is not necessary
to be diagonal. As in SVD, the column vectors
of G:,:,kVT (capture both word and relation infor-
mation) behave similarly to the column vectors of
the original tensor sliceW:,:,k.
4.3 Measuring the Degrees of Word Relations
In principle, the raw information in the input ten-
sor W can be used for computing lexical similarity
using the cosine score between the column vectors
for two words from the same slice of the tensor. To
measure the degree of other relations, however, our
approach requires one to specify a pivot slice. The
key role of the pivot slice is to expand the lexical
coverage of the relation of interest to additional lexi-
cal entries and, for this reason, the pivot slice should
be chosen to capture the equivalence of the lexical
entries. In this paper, we use the synonymy relation
as our pivot slice. First we consider measuring the
degree of a relation rel holding between the i-th and
j-th words using the raw tensor W , which can be
computed as
cos
(
W:,i,syn,W:,j,rel
)
. (5)
This measurement can be motivated from the logical
rule: syn(wordi, target) ? rel(target,wordj) ?
rel(wordi,wordj), where the pivot relation syn ex-
pands the coverage of the relation of interest rel.
Turning to the use of the tensor decomposition,
we use a similar derivation to Eq. (3), and measure
the degree of relation rel between two words by
cos
(
S:,:,synVTi,:,S:,:,relV
T
j,:
)
. (6)
1607
For instance, the degree of antonymy between
?joy? and ?sorrow? is measured by the co-
sine similarity between the respective fibers
cos(X:,?joy?,syn,X:,?sorrow?,ant). We can encode both
symmetric relations (e.g., antonymy and synonymy)
and asymmetric relations (e.g., hypernymy and
hyponymy) in the same tensor representation. For a
symmetric relation, we use both cos(X:,i,syn,X:,j,rel)
and cos(X:,j,syn,X:,i,rel) and measure the degree of
a symmetric relation by the average of these two
cosine similarity scores. However, for asymmetric
relations, we use only cos(X:,i,syn,X:,j,rel).
5 Experiments
We evaluate MRLSA on two tasks: answering the
closest-opposite GRE questions and measuring de-
grees of various class-inclusion (i.e., is-a) relations.
In both tasks, we design the experiments to empir-
ically validate the following claims. When encod-
ing two opposite relations from the same source,
MRLSA performs comparably to PILSA. However,
MRLSA generalizes LSA to model multiple rela-
tions, which could be obtained from both homoge-
neous and heterogeneous data sources. As a result,
the performance of a target task can be further im-
proved.
5.1 Experimental Setup
We construct the raw tensors to encode a particular
relation in each slice based on two data sources.
Encarta The Encarta thesaurus is developed by
Bloomsbury Publishing Plc2. For each target word,
it provides a list of synonyms and antonyms. We
use the same version of the thesaurus as in (Yih et
al., 2012), which contains about 47k words and a
vocabulary list of approximately 50k words.
WordNet We use four types of relations from
WordNet: synonymy, antonymy, hypernymy and
hyponymy. The number of target words and the
size of the vocabulary in our version are 117,791
and 149,400, respectively. WordNet has better vo-
cabulary coverage, but fewer antonym pairs. For
instance, the WordNet antonym slice contains only
46,945 nonzero entries, while the Encarta antonym
slice has 129,733.
2http://www.bloomsbury.com
We apply a memory-efficient Tucker decomposi-
tion algorithm (Kolda and Sun, 2008) implemented
in tensor toolbox v2.5 (Bader et al, 2012)3 to factor
the tensor. The largest tensor considered in this pa-
per can be decomposed in about 3 hours using less
than 4GB of memory with a commodity PC.
5.2 Answering GRE Antonym Questions
The first task is to answer the closest-opposite ques-
tions from the GRE test provided by Mohammad et
al. (2008)4. Each question in this test consists of
a target word and five candidate words, where the
goal is to pick the candidate word that has the most
opposite meaning to the target word. In order to
have a fair comparison, we use the same data split
as in (Mohammad et al, 2008), with 162 questions
used for the development set and 950 for test. Fol-
lowing (Mohammad et al, 2008; Yih et al, 2012),
we report the results in precision (accuracy of the
questions that the system attempts to answer), re-
call (percentage of the questions answered correctly
over all questions) and F1 (the harmonic mean of
precision and recall).
We tune two sets of parameters using the devel-
opment set: (1) the rank parameter ? in the tensor
decomposition and (2) the scaling factors of differ-
ent slices of the tensor. The rank parameter spec-
ifies the number of dimensions of the latent space.
In the experiments, We pick the best value of ? from
{100, 200, 300, 500, 750, 1000}. The scaling factors
adjust the values of each slice of the tensor. The el-
ements of each slice are multiplied by the scaling
factor before factorization. This is important be-
cause Tucker decomposition minimizes the recon-
struction error (the Frobenius norm of the residual
tensor). As a result, the slice with a larger range of
values becomes more influential to U and V. In this
work, we fixW:,:,ant, and search for the scaling fac-
tor of W:,:,syn in {0.25, 0.5, 1, 2, 4} and the factors
ofW:,:,hyper andW:,:,hypo in {0.0625, 0.125, 0.25}.
Table 1 summarizes the results of training
3http://www.sandia.gov/?tgkolda/
TensorToolbox. The Tucker decomposition involves
performing SVD on a large matrix. We modify the MATLAB
code of tensor toolbox to use the built-in svd function instead
of svds. This modification reduces both the running time and
memory usage.
4http://www.saifmohammad.com
1608
Dev. Set Test Set
Prec. Rec. F1 Prec. Rec. F1
WordNet Lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet RawTensor 0.42 0.41 0.42 0.42 0.41 0.42
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA:Syn+Ant 0.63 0.62 0.62 0.59 0.58 0.59
WordNet MRLSA:4-layers 0.66 0.65 0.65 0.61 0.59 0.60
Encarta Lookup 0.65 0.61 0.63 0.61 0.56 0.59
Encarta RawTensor 0.67 0.64 0.65 0.62 0.57 0.59
Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA:Syn+Ant 0.87 0.82 0.84 0.82 0.74 0.78
MRLSA:WordNet+Encarta 0.88 0.85 0.87 0.81 0.77 0.79
Table 1: GRE antonym test results of models based on Encarta and WordNet data in precision, recall and F1.
RawTensor evaluates the performance of the tensor with 2 slices encoding synonyms and antonyms be-
fore decomposition (see Eq. (5)), which is comparable to checking the original data directly (Lookup).
MRLSA:Syn+Ant applies Tucker decomposition to the raw tensor and measures the degree of antonymy
using Eq. (6). The result is similar to that of PILSA (see Sec. 3.1). MRLSA:4-layers adds hypernyms and
hyponyms from WordNet; MRLSA:WordNet+Encarta consists of synonyms/antonyms from Encarta and hy-
pernyms/hyponyms from WordNet, where the target words are aligned using the synonymy relations. Both
models demonstrate the advantage of encoding more relations, from either the same or different resources.
MRLSA using two different corpora, Encarta and
WordNet. The performance of the MRLSA raw
tensor is close to that of looking up the thesaurus.
This indicates the tensor representation is able to
capture the word relations explicitly described in
the thesaurus. After conducting tensor decomposi-
tion, MRLSA:Syn+Ant achieves similar results to
PILSA. This confirms our claim that when giv-
ing the same among of information, MRLSA per-
forms at least comparably to PILSA. However, the
true power of MRLSA is its ability to incorpo-
rate other semantic relations to boost the perfor-
mance of the target task. For example, when
we add the hypernymy and hyponymy relations to
the tensor, these class-inclusion relations provide a
weak signal to help resolve antonymy. We sus-
pect that this is due to the fact that antonyms typ-
ically share the same properties but only have the
opposite meaning on one particular semantic di-
mension. For instance, the antonyms ?sadness?
and ?happiness? are different forms of emotion.
When two words are hyponyms of a target word,
the likelihood that they are antonyms should thus
be increased. We show that the target relations
and these auxiliary semantic relations can be col-
lected from the same data source (e.g., WordNet
MRLSA:4-layers) or from multiple, heterogeneous
sources (e.g., MRLSA:WordNET+Encarta). In both
cases, the performance of the model improves as
more relations are incorporated. Moreover, our ex-
periments show that adding the hypernym and hy-
ponym layers from WordNet improves modeling
antonym relations based on the Encarta thesaurus.
This suggests that the weak signal from a resource
with a large vocabulary (e.g., WordNet) can help
predict relations between out-of-vocabulary words
and thus improve the recall.
To better understand the model, we examine the
top antonyms for three question words from the
GRE test. The lists below show antonyms and their
MRLSA scores for each of the GRE question words
as determined by the MRLSA:WordNET+Encarta
model. Antonyms that can be found directly in the
Encarta thesaurus are in italics.
inanimate alive (0.91), living (0.90), bodily (0.90), in-
the-flesh (0.89), incarnate (0.89)
alleviate exacerbate (0.68), make-worse (0.67), in-
flame (0.66), amplify (0.65), stir-up (0.64)
relish detest (0.33), abhor (0.33), abominate (0.33), de-
spise (0.33), loathe (0.31)
We can see that from these examples, MRLSA not
1609
Dev. Test
1a (Taxonomic) 1b (Functional) 1c (Singular) 1d (Plural) Avg.
WordNet Lookup 52.9 34.5 41.4 34.3 36.7
WordNet RawTensor 51.0 38.3 50.0 42.1 43.5
WordNet MRLSA:Syn+Hypony 55.8 41.7 (43.2) 51.0 (51.4) 37.5 (44.4) 43.4 (46.3)
WordNet MRLSA:4-layers 52.9 51.5 (53.9) 51.9 (60.0) 43.5 (50.5) 49.0 (54.8)
MRLSA:WordNet+Encarta 62.1 55.3 (58.7) 57.1 (65.7) 48.6 (53.7) 55.8 (60.1)
UTDNB (Rink and Harabagiu, 2012) - 38.3 36.7 28.2 34.4
Table 2: Results of measuring the class-inclusion (is-a) relations in MaxDiff accuracy (see text for de-
tail). RawTensor has synonym and hyponym slices and measures the degree of is-a relation using Eq. (5).
MRLSA:Syn+Hypo factors the raw tensor and judges the relation by Eq. (6). The constructions of
MRLSA:4-layers and MRLSA:WordNet+Encarta are the same as in Sec. 5.2 (see the caption of Table 1
for detail). For MRLSA models, numbers shown in the parentheses are the results when parameters are
tuned using the test sets. UTDNB is the results of the best performing system in SemEval-2012 Task 2.
only preserves the antonyms in the thesaurus, but
also discovers additional ones, such as exacerbate
and inflame for ?alleviate?. Another interesting find-
ing is that while the scores are useful in ranking
the candidate words, they might not be comparable
across different question words. This could be an
issue for some applications, which need to make a
binary decision on whether two words are antonyms.
5.3 Measuring degrees of Is-A relations
We evaluate MRLSA using the class-inclusion por-
tion of SemEval-2012 Task 2 data (Jurgens et al,
2012). Here the goal is to measure the degree
of two words having the is-a relation. Five an-
notated datasets are provided for different subcate-
gories of this relation: 1a-taxonomic, 1b-functional,
1c-singular, 1d-plural, 1e-class individual. We omit
1e because it focuses on real world entities (e.g.,
queen:Elizabeth, river:Nile), which are not included
in WordNet.
Each dataset contains about 100 questions based
on approximately 40 word pairs. The question con-
sists of 4 randomly chosen word pairs and asks the
best and worst pairs that exemplify the specific is-a
relation. The performance is measured by the av-
erage prediction accuracy, also called the MaxDiff
accuracy (Louviere and Woodworth, 1991).
Because the questions are generated from the
same set of word pairs, these questions are not mutu-
ally independent. Therefore, it is not proper to split
the data of each subcategory into the development
and test sets. Alternatively, we follow the setting
of SemEval-2012 Task 2 and use the first subcat-
egory (1a-taxonomy) to tune the model and eval-
uate its performance based on the results on other
datasets. Since the models are tuned and tested on
different types of subcategories, they might not be
the optimal ones when evaluated on the test sets.
Therefore, we show results using the best parame-
ters tuned on the development set and those tuned on
the test set, where the latter suggests a performance
upper-bound. Besides the rank parameter, we tune
the scaling factors of the synonym, hypernym and
hyponym slices from {4, 16, 64}. The scaling factor
of the antonym slice is fixed to 1.
Table 2 shows the performance in MaxDiff accu-
racy. Results show that even the raw tensor repre-
sentation (RawTensor) performs better than Word-
Net lookup. We suspect that this is because the
tensor representation can capture the fact that the
hyponyms of a word are usually synonymous to
each other. By performing Tucker decomposition
on the raw Tensor, MRLSA achieves better per-
formance. MRLSA:4-layers further leverages the
information from antonyms and hypernyms and
thus improves the model. As we notice in the
GRE antonym test, models based on the Encarta
thesaurus perform better in predicting antonyms.
Therefore, it is interesting to check if combining
synonyms and antonyms from Encarta helps. As
a result, MRLSA:WordNet+Encarta improves over
MRLSA:4-layers significantly. This demonstrates
again that MRLSA can leverage knowledge stored in
heterogeneous resources. Notably, MRLSA outper-
1610
forms the best system participated in the SemEval-
2012 task with a large margin, with a difference of
21.4 in MaxDiff accuracy.
Next we examine the top words that have the is-
a relation relative to three question words from the
task. The lists below show the hyponyms and their
respective MRLSA scores for each of the question
words as determined by MRLSA:4-layers.
bird ostrich (0.75), gamecock (0.75), nighthawk (0.75),
amazon (0.74), parrot (0.74)
automobile minivan (0.48), wagon (0.48), taxi (0.46),
minicab (0.45), gypsy cab (0.45)
vegetable buttercrunch (0.61), yellow turnip (0.61), ro-
maine (0.61), chipotle (0.61), chilli (0.61)
Although the model in general does a good job
finding hyponyms, we observe that some suggested
words, such as buttercrunch (a mild lettuce) vs.
?vegetable?, do not seem intuitive (e.g., compared to
carrot). Having one additional slice to capture the
general term co-occurrence relation may help im-
prove the model in this respect.
6 Conclusions
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA) for lexical seman-
tics. MRLSA models multiple word relations by
leveraging a 3-way tensor, where each slice cap-
tures one particular relation. A low-rank approx-
imation of the tensor is then derived using a ten-
sor decomposition. Consequently, words in the vo-
cabulary are represented by vectors in the latent se-
mantic space, and each relation is captured by a
latent square matrix. Given two words, MRLSA
not only can measure their degree of having a spe-
cific relation, but also can discover unknown rela-
tions between them. These advantages have been
demonstrated in our experiments. By encoding re-
lations from both homogeneous or heterogeneous
data sources, MRLSA achieves state-of-the-art per-
formance on existing benchmark datasets for two re-
lations, antonymy and is-a.
For future work, we plan to explore directions that
aim for improving both the quality and word cover-
age of the model. For instance, the knowledge en-
coded by MRLSA can be enriched by adding more
relations from a variety of linguistic resources, in-
cluding the co-occurrence relations from large cor-
pora. On model refinement, we notice that MRLSA
can be viewed as a 3-layer neural network without
applying the sigmoid function. Following the strat-
egy of using Siamese neural networks to enhance
PILSA (Yih et al, 2012), training MRLSA with a
multi-task discriminative learning setting can be a
promising approach as well.
Acknowledgments
We thank Geoff Zweig for valuable discussions and
the anonymous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Brett W. Bader, Tamara G. Kolda, et al 2012. Matlab
tensor toolbox version 2.5. Available online, January.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:993?1022.
Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP-CoNLL, pages 1024?1033.
Shay B. Cohen, Giorgio Satta, and Michael Collins.
2013. Approximate PCFG parsing using tensor de-
composition. In NAACL-HLT 2013, pages 487?496.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science, 41(96).
S. Dumais, T. Letsche, M. Littman, and T. Landauer.
1997. Automatic cross-language retrieval using latent
semantic indexing. In AAAI-97 Spring Symposium Se-
ries: Cross-Language Text and Speech Retrieval.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In ACL 2012, pages 864?872.
Weiwei Guo and Mona Diab. 2013. Improving lexical
semantics for sentential semantics: Modeling selec-
tional preference and similar words in a latent variable
model. In NAACL-HLT 2013, pages 739?745.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence, pages 289?296.
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 356?364.
1611
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable ten-
sor decompositions for multi-aspect data mining. In
ICDM 2008, pages 363?372.
T. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In NIPS 1998.
T. Landauer. 2002. On the computational basis of learn-
ing and cognition: Arguments from lsa. Psychology of
Learning and Motivation, 41:43?84.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In NAACL-HLT 2013.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251?261.
Xipeng Qiu, Le Tian, and Xuanjing Huang. 2013. Latent
semantic tensor indexing for community-based ques-
tion answering. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 434?439, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109?117.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT 2013, pages 74?84.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In ICML ?11.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ledyard R Tucker. 1966. Some mathematical
notes on three-mode factor analysis. Psychometrika,
31(3):279?311.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37(1):141?
188.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of se-
mantic compositionality. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1142?1151, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267?273, New York, NY,
USA. ACM.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning, pages 247?256, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey
Zweig, and Tomas Mikolov. 2013. Combining het-
erogeneous models for measuring relational similar-
ity. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1000?1009, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
1612
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1568?1579,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Typed Tensor Decomposition of Knowledge Bases for Relation Extraction
Kai-Wei Chang
??
Wen-tau Yih
\
Bishan Yang
??
Christopher Meek
\
?
University of Illinois, Urbana, IL 61801, USA
?
Cornell University, Ithaca, NY 14850, USA
\
Microsoft Research, Redmond, WA 98052, USA
Abstract
While relation extraction has traditionally
been viewed as a task relying solely on
textual data, recent work has shown that
by taking as input existing facts in the form
of entity-relation triples from both knowl-
edge bases and textual data, the perfor-
mance of relation extraction can be im-
proved significantly. Following this new
paradigm, we propose a tensor decompo-
sition approach for knowledge base em-
bedding that is highly scalable, and is es-
pecially suitable for relation extraction.
By leveraging relational domain knowl-
edge about entity type information, our
learning algorithm is significantly faster
than previous approaches and is better
able to discover new relations missing
from the database. In addition, when ap-
plied to a relation extraction task, our ap-
proach alone is comparable to several ex-
isting systems, and improves the weighted
mean average precision of a state-of-the-
art method by 10 points when used as a
subcomponent.
1 Introduction
Identifying the relationship between entities from
free text, relation extraction is a key task for ac-
quiring new facts to increase the coverage of a
structured knowledge base. Given a pre-defined
database schema, traditional relation extraction
approaches focus on learning a classifier using tex-
tual data alone, such as patterns between the oc-
currences of two entities in documents, to deter-
mine whether the entities have a particular rela-
tion. Other than using the existing known facts
to label the text corpora in a distant supervision
setting (Bunescu and Mooney, 2007; Mintz et al.,
?
Work conducted while interning at Microsoft Research.
2009; Riedel et al., 2010; Ritter et al., 2013), an
existing knowledge base is typically not involved
in the process of relation extraction.
However, this paradigm has started to shift re-
cently, as researchers showed that by taking exist-
ing facts of a knowledge base as an integral part of
relation extraction, the model can leverage richer
information and thus yields better performance.
For instance, Riedel et al. (2013) borrowed the
idea of collective filtering and constructed a ma-
trix where each row is a pair of entities and each
column is a particular relation. For a true entity-
relation triple (e
1
, r, e
2
), either from the text cor-
pus or from the knowledge base, the correspond-
ing entry in the matrix is 1. A previously unknown
fact (i.e., triple) can be discovered through ma-
trix decomposition. This approach can be viewed
as creating vector representations of each relation
and candidate pair of entities. Because each entity
does not have its own representation, relationships
of any unpaired entities cannot be discovered. Al-
ternatively, Weston et al. (2013) created two types
of embedding ? one based on textual similarity and
the other based on knowledge base, where the lat-
ter maps each entity and relation to the same d-
dimensional vector space using a model proposed
by Bordes et al. (2013a). They also showed that
combining these two models results in a signif-
icant improvement over the model trained using
only textual data.
To make such an integrated strategy work, it is
important to capture all existing entities and rela-
tions, as well as the known facts, from both tex-
tual data and large databases. In this paper, we
propose a new knowledge base embedding model,
TRESCAL, that is highly efficient and scalable,
with relation extraction as our target application.
Our work is built on top of RESCAL (Nickel
et al., 2011), which is a tensor decomposition
method that has proven its scalability by factoring
YAGO (Biega et al., 2013) with 3 million entities
1568
and 41 million triples (Nickel et al., 2012). We
improve the tensor decomposition model with two
technical innovations. First, we exclude the triples
that do not satisfy the relational constraints (e.g.,
both arguments of the relation spouse-of need to
be person entities) from the loss, which is done
by selecting sub-matrices of each slice of the ten-
sor during training. Second, we introduce a math-
ematical technique that significantly reduces the
computational complexity in both time and space
when the loss function contains a regularization
term. As a consequence, our method is more than
four times faster than RESCAL, and is also more
accurate in discovering unseen triples.
Our contributions are twofold. First, compared
to other knowledge base embedding methods de-
veloped more recently, it is much more efficient
to train our model. As will be seen in Sec. 5,
when applied to a large knowledge base created
using NELL (Carlson et al., 2010) that has 1.8M
entity-relation triples, our method finishes training
in 4 to 5 hours, while an alternative method (Bor-
des et al., 2013a) needs almost 3 days. Moreover,
the prediction accuracy of our model is competi-
tive to others, if not higher. Second, to validate its
value to relation extraction, we apply TRESCAL to
extracting relations from a free text corpus along
with a knowledge base, using the data provided
in (Riedel et al., 2013). We show that TRESCAL
is complementary to existing systems and signif-
icantly improves their performance when using it
as a subcomponent. For instance, this strategy im-
proves the weighted mean average precision of the
best approach in (Riedel et al., 2013) by 10 points
(47% to 57%).
The remainder of this paper is organized as fol-
lows. We survey most related work in Sec. 2 and
provide the technical background of our approach
in Sec. 3. Our approach is detailed in Sec. 4, fol-
lowed by the experimental validation in Sec. 5. Fi-
nally, Sec. 6 concludes the paper.
2 Related Work
Our approach of creating knowledge base em-
bedding is based on tensor decomposition, which
is a well-developed mathematical tool for data
analysis. Existing tensor decomposition models
can be categorized into two main families: the
CP and Tucker decompositions. The CP (CAN-
DECOMP/PARAFAC) decomposition (Kruskal,
1977; Kiers, 2000) approximates a tensor by a sum
of rank-one tensors, while the Tucker decompo-
sition (Tucker, 1966), also known as high-order
SVD (De Lathauwer et al., 2000), factorizes a ten-
sor into a core tensor multiplied by a matrix along
each dimension. A highly scalable distributional
algorithm using the Map-Reduce architecture has
been proposed recently for computing CP (Kang et
al., 2012), but not for the Tucker decomposition,
probably due to its inherently more complicated
model form.
Matrix and tensor decomposition methods have
been applied to modeling multi-relational data.
For instance, Speer et al. (2008) aimed to cre-
ate vectors of latent components for representing
concepts in a common sense knowledge base us-
ing SVD. Franz et al. (2009) proposed TripleRank
to model the subject-predicate-object
RDF triples in a tensor, and then applied the CP
decomposition to identify hidden triples. Fol-
lowing the same tensor encoding, Nickel et al.
(2011) proposed RESCAL, a restricted form of
Tucker decomposition for discovering previously
unknown triples in a knowledge base, and later
demonstrated its scalability by applying it to
YAGO, which was encoded in a 3M ? 3M ? 38
tensor with 41M triples (Nickel et al., 2012).
Methods that revise the objective function
based on additional domain information have been
proposed, such as MrWTD, a multi-relational
weighted tensor decomposition method (London
et al., 2013), coupled matrix and tensor fac-
torization (Papalexakis et al., 2014), and col-
lective matrix factorization (Singh and Gordon,
2008). Alternatively, instead of optimizing for the
least-squares reconduction loss, a non-parametric
Bayesian approach for 3-way tensor decomposi-
tion for modeling relational data has also been pro-
posed (Sutskever et al., 2009). Despite the exis-
tence of a wide variety of tensor decomposition
models, most methods do not scale well and have
only been tested on datasets that are much smaller
than the size of real-world knowledge bases.
Multi-relational data can be modeled by neural-
network methods as well. For instance, Bordes et
al. (2013b) proposed the Semantic Matching En-
ergy model (SME), which aims to have the same
d-dimensional vector representations for both en-
tities and relations. Given the vectors of entities
e
1
, e
2
and relation r. They first learn the latent
representations of (e
1
, r) and (e
2
, r). The score
of (e
1
, r, e
2
) is determined by the inner product
1569
of the vectors of (e
1
, r) and (e
2
, r). Later, they
proposed a more scalable method called translat-
ing embeddings (TransE) (Bordes et al., 2013a).
While both entities and relations are still repre-
sented by vectors, the score of (e
1
, r, e
2
) becomes
the negative dissimilarity measure of the corre-
sponding vectors ??e
i
+ r
k
? e
j
?, motivated by
the work in (Mikolov et al., 2013b; Mikolov et al.,
2013a). Alternatively, Socher et al. (2013) pro-
posed a Neural Tensor Network (NTN) that repre-
sents entities in d-dimensional vectors created sep-
arately by averaging pre-trained word vectors, and
then learns a d?d?m tensor describing the inter-
actions between these latent components in each
of the m relations. All these methods optimize
for loss functions that are more directly related to
the true objective ? the prediction accuracy of cor-
rect entity-relation triples, compared to the mean-
squared reconstruction error in our method. Nev-
ertheless, they typically require much longer train-
ing time.
3 Background
In this section, we first describe how entity-
relation triples are encoded in a tensor. We then
introduce the recently proposed tensor decompo-
sition method, RESCAL (Nickel et al., 2011) and
explain how it adopts an alternating least-squares
method, ASALSAN (Bader et al., 2007), to com-
pute the factorization.
3.1 Encoding Binary Relations in a Tensor
Suppose we are given a knowledge base with
n entities and m relation types, and the facts
in the knowledge base are denoted as a set of
entity-relation triples T = {(e
i
, r
k
, e
j
)}, where
i, j ? {1, 2, ? ? ?n} and k ? {1, 2, ? ? ?m}. A
triple (e
i
, r
k
, e
j
) simply means that the i-th en-
tity and the j-th entity have the k-th relation.
Following (Franz et al., 2009), these triples can
naturally be encoded in a 3-way tensor X ?
{0, 1}
n?n?m
, such that X
i,j,k
= 1 if and only if
the triple (e
i
, r
k
, e
j
) ? T
1
. The tensor can be
viewed as consisting of m slices, where each slice
is an n?n square matrix, denoting the interactions
of the entities of a particular relation type. In the
remainder of this paper, we will use X
k
to refer to
the k-th slice of the tensor X . Fig. 1 illustrates this
representation.
1
This representation can easily be extended for a proba-
bilistic knowledge base by allowing nonnegative real values.
e1  en
e 1  
 e n
?? k
Figure 1: A tensor encoding of m binary relation
types and n entities. A sliceX
k
denotes the entities
having the k-th relation.
3.2 RESCAL
In order to identify latent components in a ten-
sor for collective learning, Nickel et al. (2011)
proposed RESCAL, which is a tensor decomposi-
tion approach specifically designed for the multi-
relational data described in Sec. 3.1. Given a ten-
sor X
n?n?m
, RESCAL aims to have a rank-r ap-
proximation, where each slice X
k
is factorized as
X
k
? AR
k
A
T
. (1)
A is an n ? r matrix, where the i-th row denotes
the r latent components of the i-th entity. R
k
is an
asymmetric r ? r matrix that describes the inter-
actions of the latent components according to the
k-th relation. Notice that while R
k
differs in each
slice, A remains the same.
A and R
k
are derived by minimizing the loss
function below.
min
A,R
k
f(A,R
k
) + ? ? g(A,R
k
), (2)
where f(A,R
k
) =
1
2
(
?
k
?X
k
?AR
k
A
T
?
2
F
)
is the mean-squared reconstruction error and
g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
is the regu-
larization term.
RESCAL is a special form of Tucker decom-
position (Tucker, 1966) operating on a 3-way ten-
sor. Its model form (Eq. (1)) can also be regarded
as a relaxed form of DEDICOM (Bader et al.,
2007), which derives the low-rank approximation
as: X
k
? AD
k
RD
k
A
T
. To compare RESCAL
to other tensor decomposition methods, interested
readers can refer to (Kolda and Bader, 2009).
1570
The optimization problem in Eq. (2) can be
solved using the efficient alternating least-squares
(ALS) method. This approach alternatively fixes
R
k
to solve for A and then fixes A to solve
R
k
. The whole procedure stops until
f(A,R
k
)
?X?
2
F
con-
verges to some small threshold  or the maximum
number of iterations has been reached.
By finding the solutions where the gradients are
0, we can derive the update rules of A and R
k
as
below.
A?
[
?
k
X
k
AR
T
k
+X
T
k
AR
k
][
?
k
B
k
+C
k
+?I
]
?1
,
where B
k
= R
k
A
T
AR
T
k
and C
k
= R
T
k
A
T
AR
k
.
vec(R
k
)?
(
Z
T
Z + ?I
)
?1
Z
T
vec(X
k
), (3)
where vec(R
k
) is the vectorization of R
k
, Z =
A?A and the operator ? is the Kronecker prod-
uct.
Complexity Analysis Following the analysis in
(Nickel et al., 2012), we assume that each X
k
is a
sparse matrix, and let p be the number of non-zero
entries
2
. The complexity of computing X
k
AR
T
k
and X
T
k
AR
k
is O(pr + nr
2
). Evaluating B
k
and
C
k
requires O(nr
2
) and the matrix inversion re-
quires O(r
3
). Therefore, the complexity of updat-
ing A isO(pr+nr
2
) assuming n r. The updat-
ing rule of R
k
involves inverting an r
2
? r
2
ma-
trix. Therefore, directly computing the inversion
requires time complexity O(r
6
) and space com-
plexity O(r
4
). Although Nickel et al. (2012) con-
sidered using QR decomposition to simplify the
updates, it is still time consuming with the time
complexity O(r
6
+ pr
2
). Therefore, the total time
complexity isO(r
6
+pr
2
) and the step of updating
R
k
is the bottleneck in the optimization process.
We will describe how to reduce the time complex-
ity of this step to O(nr
2
+ pr) in Section 4.2.
4 Approach
We describe how we leverage the relational do-
main knowledge in this section. By removing the
incompatible entity-relation triples from the loss
2
Notice that we use a slightly different definition of p
from the one in (Nickel et al., 2012). The time complexity
of multiplying an n ? n sparse matrix X
k
with p non-zero
entries by an n? r dense matrix is O(pr) assuming n r.
function, training can be done much more effi-
ciently and results in a model with higher pre-
diction accuracy. In addition, we also introduce
a mathematical technique to reduce the compu-
tational complexity of the tensor decomposition
methods when taking into account the regulariza-
tion term.
4.1 Applying Relational Domain Knowledge
In the domain of knowledge bases, the notion of
entity types is the side information that commonly
exists and dictates whether some entities can be
legitimate arguments of a given predicate. For
instance, suppose the relation of interest is born-
in, which denotes the birth location of a person.
When asked whether an incompatible pair of en-
tities, such as two person entities like Abraham
Lincoln and John Henry, having this rela-
tion, we can immediately reject the possibility. Al-
though the type information and the constraints
are readily available, it is overlooked in the pre-
vious work on matrix and tensor decomposition
models for knowledge bases (Riedel et al., 2013;
Nickel et al., 2012). Ignoring the type information
has two implications. Incompatible entity-relation
triples still participate in the loss function of the
optimization problem, which incurs unnecessary
computation. Moreover, by choosing values for
these incompatible entries we introduce errors in
training the model that can reduce the quality of
the model.
Based on this observation, we propose Typed-
RESCAL, or TRESCAL, which leverages the en-
tity type information to improve both the effi-
ciency of model training and the quality of the
model in term of prediction accuracy. We em-
ploy a direct and simple approach by excluding
the triples of the incompatible entity types from
the loss in Eq. (2). For each relation, let L
k
and
R
k
be the set of entities with a compatible type to
the k-th relation. That is, (e
i
, r
k
, e
j
) is a feasible
triple if and only if e
i
? L
k
and e
j
? R
k
. For no-
tational convenience, we use A
k
l
,A
k
r
to denote
the sub-matrices of A that consists of rows asso-
ciated with L
k
and R
k
, respectively. Analogously,
let X
k
lr
be the sub-matrix of X
k
that consists of
only the entity pairs compatible to the k-th rela-
tion. The rows and columns of X
k
lr
map to the en-
tities in A
k
l
and A
k
r
, respectively. In other words,
entries of X
k
but not in X
k
lr
do not satisfy the type
constraint and are ignored from the computation.
1571
~ ~ ? ? 
?k A 
A TRk
A kl A krT?klr
e ??Lk
e ??Rk
Figure 2: The construction of TRESCAL. Suppose
the k-th relation is born-in. L
k
is then a set of
person entities and R
k
is a set of location entities.
Only the sub-matrix corresponds to the compati-
ble entity pairs (i.e., X
k
lr
) and the sub-matrices of
the associated entities (i.e., A
k
l
and A
T
k
r
) will be
included in the loss.
Fig. 2 illustrates this construction.
TRESCAL solves the following optimization
problem:
min
A,R
k
f
?
(A,R
k
) + ? ? g(A,R
k
), (4)
where f
?
(A,R
k
) =
1
2
?
k
?X
k
lr
?A
k
l
R
k
A
T
k
r
?
2
F
and g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
.
Similarly, A and R
k
can be solved using the
alternating least-squares method. The update rule
of A is
A?
[
?
k
(
X
k
lr
A
k
r
R
T
k
+ X
T
k
lr
A
k
l
R
k
)
]
?
[
?
k
B
k
r
+ C
k
l
+ ?I
]
?1
,
where B
k
r
= R
k
A
T
k
r
A
k
r
R
T
k
and C
k
l
=
R
T
k
A
T
k
l
A
k
l
R
k
.
The update ofR
k
becomes:
vec(R
k
)?
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
?
vec(A
k
l
T
X
k
lr
A
k
r
),
(5)
Complexity Analysis Let n? be the average
number of entities with a compatible type to a
relation. Follow a similar derivation in Sec. 3.2,
the time complexity of updating A isO(pr+ n?r
2
)
and the time complexity of updating R
k
remains
to be O(r
6
+ pr
2
).
4.2 Handling Regularization Efficiently
Examining the update rules of both RESCAL
and TRESCAL, we can see that the most time-
consuming part is the matrix inversions. For
RESCAL, this is the term (Z
T
Z+?I)
?1
in Eq. (3),
where Z = A?A. Nickel et al. (2011) made the
observation that if ? = 0, the matrix inversion can
be calculated by
(Z
T
Z)
?1
= (A
T
A)
?1
A? (A
T
A)
?1
A.
Then, it only involves an inversion of an r? r ma-
trix, namely A
T
A. However, if ? > 0, directly
calculating Eq. (3) requires to invert an r
2
? r
2
matrix and thus becomes a bottleneck in solving
Eq. (2).
To reduce the computational complexity of
the update rules of R
k
, we compute the inver-
sion
(
Z
T
Z + ?I
)
?1
by applying singular value
decomposition (SVD) to A, such that A =
U?V
T
, where U and V are orthogonal matrices
and ? is a diagonal matrix. Then by using proper-
ties of the Kronecker product we have:
(
Z
T
Z + ?I
)
?1
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
.
The last equality holds because V ? V is
also an orthogonal matrix. We leave the de-
tailed derivations in Appendix A. Notice that
(
?I + ?
2
??
2
)
?1
is a diagonal matrix. There-
fore, the inversion calculation is trivial.
This technique can be applied to TRESCAL
as well. By applying SVD to both A
k
l
and A
k
r
, we have A
k
l
= U
k
l
?
k
l
V
T
k
l
and
A
k
r
= U
k
r
?
k
r
V
T
k
r
, respectively. The computa-
tion of
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
of Eq. (5)
thus becomes:
(V
k
l
?V
k
r
)
(
?I + ?
2
k
l
??
2
k
r
)
?1
(V
k
l
?V
k
r
)
T
.
The procedure of updating R is depicted in Al-
gorithm 1.
Complexity Analysis For RESCAL, V and ?
can be computed by finding eigenvectors of A
T
A.
Therefore, computing SVD of A costs O(nr
2
+
r
3
) = O(nr
2
). Computing Step 4 in Algorithm 1
takes O(nr
2
+ pr). Step 5 and Step 6 require
1572
Algorithm 1 UpdatingR in TRESCAL
Require: X , A, and entity sets R
k
,L
k
,?k
Ensure: R
k
,?k.
1: for k = 1 . . .m do
2: [U
k
l
,?
2
k
l
,V
k
l
]? SVD(A
T
k
l
A
k
l
).
3: [U
k
r
,?
2
k
r
,V
k
r
]? SVD(A
T
k
r
A
k
r
).
4: M
1
? V
T
k
l
A
T
k
l
X
k
lr
A
k
r
V
k
r
.
5: M
2
? diag(?
2
k
l
) diag(?
2
k
r
)
T
+ ?1.
(1 is a matrix of all ones. Function diag
converts the diagonal entries of a matrix to
a vector. )
6: R
k
? V
k
l
(M
1
./M
2
)V
T
k
r
.
(The operator ?./? is element-wise divi-
sion.)
7: end for
O(r
2
) and O(r
3
), respectively. The overall time
complexity of updatingR
k
becomesO(nr
2
+pr).
Using a similar derivation, the time complex-
ity of updating R
k
in TRESCAL is O(n?r
2
+ pr).
Therefore, the total complexity of each iteration is
O(n?r
2
+ pr).
5 Experiments
We conduct two sets of experiments. The first
evaluates the proposed TRESCAL algorithm on
inferring unknown facts using existing relation?
entity triples, while the second demonstrates its
application to relation extraction when a text cor-
pus is available.
5.1 Knowledge Base Completion
We evaluate our approach on a knowledge base
generated by the CMU Never Ending Language
Learning (NELL) project (Carlson et al., 2010).
NELL collects human knowledge from the web
and has generated millions of entity-relation
triples. We use the data generated from version
165 for training
3
, and collect the new triples gen-
erated between NELL versions 166 and 533 as the
development set and those generated between ver-
sion 534 and 745 as the test set
4
. The data statistics
of the training set are summarized in Table 1. The
numbers of triples in the development and test sets
are 19,665 and 117,889, respectively. Notice that
this dataset is substantially larger than the datasets
used in recent work. For example, the Freebase
data used in (Socher et al., 2013) and (Bordes et
3
http://www.cs.cmu.edu/
?
nlao/
4
http://bit.ly/trescal
NELL
# entities 753k
# relation types 229
# entity types 300
# entity-relation triples 1.8M
Table 1: Data statistics of the training set from
NELL in our experiments.
al., 2013a) have 316k and 483k
5
triples, respec-
tively, compared to 1.8M in this dataset.
In the NELL dataset, the entity type informa-
tion is encoded in a specific relation, called Gen-
eralization. Each entity in the knowledge base is
assigned to at least one category presented by the
Generalization relationship. Based on this infor-
mation, the compatible entity type constraint of
each relation can be easily identified. Specifically,
we examined the entities and relations that occur
in the triples of the training data, and counted all
the types appearing in these instances of a given
relation legitimate.
We implement RESCAL and TRESCAL in
MATLAB with the Matlab tensor Toolbox (Bader
et al., 2012). With the efficient implementation
described in Section 4.2, all experiments can be
conducted on a commodity PC with 16 GB mem-
ory. We set the maximal number of iterations of
both RESCAL and TRESCAL to be 10, which we
found empirically to be enough to generate a sta-
ble model. Note that Eq. (4) is non-convex, and the
optimization process does not guarantee to con-
verge to a global minimum. Therefore, initial-
izing the model properly might be important for
the performance. Following the implementation of
RESCAL, we initialize A by performing singular
value decomposition over
?
X =
?
k
(X
k
+ X
T
k
),
such that
?
X = U?V
T
and set A = U. Then,
we apply the update rule ofR
k
to initialize {R
k
}.
RESCAL and TRESCAL have two types of param-
eters: (1) the rank r of the decomposed tensor and
(2) the regularization parameter ?. We tune the
rank parameter on development set in a range of
{100, 200, 300, 400} and the regularization pa-
rameter in a range of {0.01, 0.05, 0.1, 0.5, 1}.
For comparison, we also use the code released
by Bordes et al. (2013a), which is implemented
using Python and the Theano library (Bergstra
et al., 2010), to train a TransE model using the
5
In (Bordes et al., 2013a), there is a much larger dataset,
FB1M, that has 17.5M triples used for evaluation. However,
this dataset has not been released.
1573
Entity Retrieval Relation Retrieval
TransE RESCAL TRESCAL TransE RESCAL TRESCAL
w/o type checking 51.41%
?
51.59% 54.79% 75.88% 73.15%
?
76.12%
w/ type checking 67.56% 62.91%
?
69.26% 70.71%
?
73.08%
?
75.70%
Table 2: Model performance in mean average precision (MAP) on entity retrieval and relation retrieval.
? and ? indicate the comparison to TRESCAL in the same setting is statistically significant using a paired-
t test on average precision of each query, with p < 0.01 and p < 0.05, respectively. Enforcing type
constraints during test time improves entity retrieval substantially, but does not help in relation retrieval.
same NELL dataset. We reserved randomly 1%
of the training triples for the code to evaluate the
model performance in each iteration. As sug-
gested in their paper, we experiment with sev-
eral hyper-parameters, including learning rate of
{0.01, 0.001}, the latent dimension of {50, 100}
and the similarity measure of {L1, L2}. In addi-
tion, we also adjust the number of batches of {50,
100, 1000}. Of all the configurations, we keep the
models picked by the method, as well as the fi-
nal model after 500 training iterations. The final
model is chosen by the performance on our devel-
opment set.
5.1.1 Training Time Reduction
We first present experimental results demonstrat-
ing that TRESCAL indeed reduces the time re-
quired to factorize a knowledge database, com-
pared to RESCAL. The experiment is conducted
on NELL with r = 300 and ? = 0.1. When
? 6= 0, the original RESCAL algorithm described
in (Nickel et al., 2011; Nickel et al., 2012) cannot
handle a large r, because updating matrices {R
k
}
requires O(r
4
) memory. Later in this section, we
will show that in some situation a large rank r is
necessary for achieving good testing performance.
Comparing TRESCAL with RESCAL, each it-
eration of TRESCAL takes 1,608 seconds, while
that of RESCAL takes 7,415 seconds. In other
words, by inducing the entity type information
and constraints, TRESCAL enjoys around 4.6 times
speed-up, compared to an improved regularized
version of RESCAL. When updating A and {R
k
}
TRESCAL only requires operating on sub-matrices
of A, {R
k
} and {X
k
}, which reduces the compu-
tation substantially. In average, TRESCAL filters
96% of entity triples that have incompatible types.
In contrast, it takes TransE at least 2 days and 19
hours to finish training the model (the default 500
iterations)
6
, while TRESCAL finishes the training
6
It took almost 4 days to train the best TransE model that
in roughly 4 to 5 hours
7
.
5.1.2 Test Performance Improvement
We consider two different types of tasks to evalu-
ate the prediction accuracy of different models ?
entity retrieval and relation retrieval.
Entity Retrieval In the first task, we collect a
set of entity-relation pairs {(e
i
, r
k
)} and aim at
predicting e
j
such that the tuple (e
i
, r
k
, e
j
) is a
recorded triple in the NELL knowledge base. For
each pair (e
i
, r
k
), we collect triples {(e
i
, r
k
, e
?
j
)}
from the NELL test corpus as positive samples
and randomly pick 100 entries e
?
j
to form negative
samples {e
i
, r
k
, e
?
j
}. Given A and R
k
from the
factorization generated by RESCAL or TRESCAL,
the score assigned to a triple {e
i
, r
k
, e
?
j
} is com-
puted by a
T
i
R
k
a
j
where a
i
and a
j
are the i-th
and j-th rows of A. In TransE, the score is de-
termined by the negative dissimilarity measures of
the learned embeddings: ?d(e
i
, r
k
, e
?
j
) = ??e
i
+
r
k
? e
?
j
?
2
2
.
We evaluate the performance using mean aver-
age precision (MAP), which is a robust and sta-
ble metric (Manning et al., 2008). As can be
observed in Table 2 (left), TRESCAL achieves
54.79%, which outperforms 51.59% of RESCAL
and 51.41% of TransE. Adding constraints during
test time by assigning the lowest score to the en-
tity triples with incompatible types improves re-
sults of all models ? TRESCAL still performs the
best (69.26%), compared to TransE (67.56%) and
RESCAL (62.91%).
Relation Retrieval In the second task, given a
relation type r
k
, we are looking for the entity pairs
(e
i
, e
j
) that have this specific relationship. To gen-
erate test data, for each relation type, we collect
is included in Table 2.
7
We also tested the released code from (Socher et al.,
2013) for training a neural tensor network model. However,
we are not able to finish the experiments as each iteration of
this method takes almost 5 hours.
1574
gold entity pairs from the NELL knowledge base
as positive samples and randomly pick a set of en-
tity pairs as negative samples such that the number
of positive samples are the same as negative ones.
Results presented in Table 2 (right) show that
TRESCAL achieves 76.12%, while RESCAL and
TransE are 73.15% and 75.88%, respectively.
Therefore, incorporating the type information in
training seems to help in this task as well. Enforc-
ing the type constraints during test time does not
help as in entity retrieval. By removing incom-
patible entity pairs, the performance of TRESCAL,
RESCAL and TransE drop slightly to 75.70%,
73.08% and 70.71% respectively. One possible
explanation is that the task of relation retrieval is
easier than entity retrieval. The incorrect type in-
formation of some entities ends up filtering out a
small number of entity pairs that were retrieved
correctly by the model.
Notice that TRESCAL achieves different levels
of performance on various relations. For example,
it performs well on predicting AthletePlaysSport
(81%) and CoachesInLeague (88%), but achieves
suboptimal performance on predicting Works-
For (49%) and BuildingLocatedInCity (35%).
We hypothesize that it is easier to gener-
alize entity-relation triples when the relation
has several related relations. For examples,
AthletePlaysForTeam and TeamPlaysSport may
help discover entity-relation triples of Ath-
letePlaysSport.
5.1.3 Sensitivity to Parameters
We also study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?,
where the detailed results can be found in Ap-
pendix B. In short, we found that increasing the
rank r generally leads to better models. Also,
while the model is not very sensitive to the value
of the regularization parameter ?, tuning ? is still
necessary for achieving the best performance.
5.2 Relation Extraction
Next, we apply TRESCAL to the task of extract-
ing relations between entities, jointly from a text
corpus and a structured knowledge base. We use
a corpus from (Riedel et al., 2013) that is cre-
ated by aligning the entities in NYTimes and Free-
base. The corpus consists of a training set and a
test set. In the training set, a list of entity pairs
are provided, along with surface patterns extracted
from NYTimes and known relations obtained from
Freebase. In the test set, only the surface patterns
are given. By jointly factoring a matrix consist-
ing of the surface patterns and relations, Riedel et
al. (2013) show that their model is able to capture
the mapping between the surface patterns and the
structured relations and hence is able to extract the
entity relations from free text. In the following, we
show that TRESCAL can be applied to this task.
We focus on the 19 relations listed in Table 1
of (Riedel et al., 2013) and only consider the
surface patterns that co-occur with these 19 re-
lations. We prune the surface patterns that oc-
cur less than 5 times and remove the entities that
are not involved in any relation and surface pat-
tern. Based on the training and test sets, we
build a 80,698?80,698?1,652 tensor, where each
slice captures a particular structured relation or a
surface pattern between two entities. There are
72 fine types extracted from Freebase assigned
to 53,836 entities that are recorded in Freebase.
In addition, special types, PER, LOC, ORG and
MISC, are assigned to the remaining 26,862 enti-
ties based on the predicted NER tags provided by
the corpus. A type is considered incompatible to a
relation or a surface pattern if in the training data,
none of the argument entities of the relation be-
longs to the type. We use r = 400 and ? = 0.1 in
TRESCAL to factorize the tensor.
We compare the proposed TRESCAL model to
RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011),
MI09 (Mintz et al., 2009) and SU12 (Surdeanu et
al., 2012)
8
. We follow the protocol used in (Riedel
et al., 2013) to evaluate the results. Given a re-
lation as query, the top 1,000 entity pairs output
by each system are collected and the top 100 ones
are judged manually. Besides comparing individ-
ual models, we also report the results of combined
models. To combine the scores from two models,
we simply normalize the scores of entity-relation
tuples to zero mean and unit variance and take the
average. The results are summarized in Table 3.
As can been seen in the table, using TRESCAL
alone is not very effective and its performance is
only compatible to MI09 and YA11, and is sig-
nificantly inferior to RI13. This is understandable
because the problem setting favors RI13 as only
entity pairs that have occurred in the text or the
database will be considered in RI13, both during
model training and testing. In contrast, TRESCAL
8
The corpus and the system outputs are from http://
www.riedelcastro.org/uschema
1575
Relation # MI09 YA11 SU12 RI13 TR TR+SU12 TR+RI13
person/company 171 0.41 0.40 0.43 0.49 0.43 0.53 0.64
location/containedby 90 0.39 0.43 0.44 0.56 0.23 0.46 0.58
parent/child 47 0.05 0.10 0.25 0.31 0.19 0.24 0.35
person/place of birth 43 0.32 0.31 0.34 0.37 0.50 0.61 0.66
person/nationality 38 0.10 0.30 0.09 0.16 0.13 0.16 0.22
author/works written 28 0.52 0.53 0.54 0.71 0.00 0.39 0.62
person/place of death 26 0.58 0.58 0.63 0.63 0.54 0.72 0.89
neighborhood/neighborhood of 13 0.00 0.00 0.08 0.67 0.08 0.13 0.73
person/parents 8 0.21 0.24 0.51 0.34 0.01 0.16 0.38
company/founders 7 0.14 0.14 0.30 0.39 0.06 0.17 0.44
film/directed by 4 0.06 0.15 0.25 0.30 0.03 0.13 0.35
sports team/league 4 0.00 0.43 0.18 0.63 0.50 0.29 0.63
team/arena stadium 3 0.00 0.06 0.06 0.08 0.00 0.04 0.09
team owner/teams owned 2 0.00 0.50 0.70 0.75 0.00 0.00 0.75
roadcast/area served 2 1.00 0.50 1.00 1.00 0.50 0.83 1.00
structure/architect 2 0.00 0.00 1.00 1.00 0.00 0.02 1.00
composer/compositions 2 0.00 0.00 0.00 0.12 0.00 0.00 0.12
person/religion 1 0.00 1.00 1.00 1.00 0.00 1.00 1.00
film/produced by 1 1.00 1.00 1.00 0.33 0.00 1.00 0.25
Weighted MAP 0.33 0.36 0.39 0.47 0.30 0.44 0.57
Table 3: Weighted Mean Average Precisions. The # column shows the number of true facts in the pool.
Bold faced are winners per relation, italics indicate ties based on a sign test.
predicts all the possible combinations between en-
tities and relations, which makes the model less fit
to the task. However, when combining TRESCAL
with a pure text-based method, such as SU12,
we can clearly see TRESCAL is complementary
to SU12 (0.39 to 0.44 in weighted MAP score),
which makes the results competitive to RI13.
Interestingly, although both TRESCAL and RI13
leverage information from the knowledge base, we
find that by combining them, the performance is
improved quite substantially (0.47 to 0.57). We
suspect that the reason is that in our construc-
tion, each entity has its own vector representa-
tion, which is lacked in RI13. As a result, the
new triples that TRESCAL finds are very different
from those found by RI13. Nevertheless, com-
bining more methods do not always yield an im-
provement. For example, combining TR, RI13 and
SU12 together (not included in Table 3) achieves
almost the same performance as TR+RI13.
6 Conclusions
In this paper we developed TRESCAL, a tensor
decomposition method that leverages relational
domain knowledge. We use relational domain
knowledge to capture which triples are potentially
valid and found that, by excluding the triples that
are incompatible when performing tensor decom-
position, we can significantly reduce the train-
ing time and improve the prediction performance
as compared with RESCAL and TransE. More-
over, we demonstrated its effectiveness in the ap-
plication of relation extraction. Evaluated on the
dataset provided in (Riedel et al., 2013), the per-
formance of TRESCAL alone is comparable to sev-
eral existing systems that leverage the idea of dis-
tant supervision. When combined with the state-
of-the-art systems, we found that the results can
be further improved. For instance, the weighted
mean average precision of the previous best ap-
proach in (Riedel et al., 2013) has been increased
by 10 points (47% to 57%).
There are a number of interesting potential ex-
tensions of our work. First, while the experiments
in this paper are on traditional knowledge bases
and textual data, the idea of leveraging relational
domain knowledge is likely to be of value to other
linguistic databases as well. For instance, part-of-
speech tags can be viewed as the ?types? of words.
Incorporating such information in other tensor de-
composition methods (e.g., (Chang et al., 2013))
may help lexical semantic representations. Sec-
ond, relational domain knowledge goes beyond
entity types and their compatibility with specific
relations. For instance, the entity-relation triple
(e
1
, child-of, e
2
) can be valid only if e
1
.type =
person ? e
2
.type = person ? e
1
.age < e
2
.age.
It would be interesting to explore the possibility
of developing efficient methods to leverage other
types of relational domain knowledge. Finally, we
would like to create more sophisticated models of
knowledge base embedding, targeting complex in-
1576
ference tasks to better support semantic parsing
and question answering.
Acknowledgments
We thank Sebastian Riedel for providing the data
for experiments. We are also grateful to the anony-
mous reviewers for their valuable comments.
References
Brett W Bader, Richard A Harshman, and Tamara G
Kolda. 2007. Temporal analysis of semantic graphs
using ASALSAN. In ICDM, pages 33?42. IEEE.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan-
uary.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Joanna Biega, Erdal Kuzey, and Fabian M Suchanek.
2013. Inside YOGO2s: a transparent information
extraction architecture. In WWW, pages 325?328.
A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,
and O. Yakhnenko. 2013a. Translating Embeddings
for Modeling Multi-relational Data. In Advances in
Neural Information Processing Systems 26.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2013b. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, pages 1?27.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using mini-
mal supervision. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 576?583, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM journal on Matrix Analysis and Appli-
cations, 21(4):1253?1278.
Thomas Franz, Antje Schultz, Sergej Sizov, and Steffen
Staab. 2009. Triplerank: Ranking semantic web
data by tensor decomposition. In The Semantic Web-
ISWC 2009, pages 213?228. Springer.
U Kang, Evangelos Papalexakis, Abhay Harpale, and
Christos Faloutsos. 2012. Gigatensor: scaling ten-
sor analysis up by 100 times-algorithms and discov-
eries. In KDD, pages 316?324. ACM.
Henk AL Kiers. 2000. Towards a standardized nota-
tion and terminology in multiway analysis. Journal
of chemometrics, 14(3):105?122.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Joseph B Kruskal. 1977. Three-way arrays: rank and
uniqueness of trilinear decompositions, with appli-
cation to arithmetic complexity and statistics. Lin-
ear algebra and its applications, 18(2):95?138.
Alan J Laub, 2005. Matrix analysis for scientists and
engineers, chapter 13, pages 139?150. SIAM.
Ben London, Theodoros Rekatsinas, Bert Huang, and
Lise Getoor. 2013. Multi-relational learning using
weighted tensor decomposition with modular loss.
Technical report, University of Maryland College
Park. http://arxiv.org/abs/1303.1733.
C. Manning, P. Raghavan, and H. Schutze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013a. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing Systems
26.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML, pages
809?816.
1577
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: scalable ma-
chine learning for linked data. In WWW, pages 271?
280.
Evangelos E Papalexakis, Tom M Mitchell, Nicholas D
Sidiropoulos, Christos Faloutsos, Partha Pratim
Talukdar, and Brian Murphy. 2014. Turbo-smt:
Accelerating coupled sparse matrix-tensor factoriza-
tions by 200x. In SDM.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML/PKDD
2010. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL, pages 74?84.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics,
1:367?378, October.
Ajit P Singh and Geoffrey J Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 650?658. ACM.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In Advances in Neural Information Processing Sys-
tems 26.
Robert Speer, Catherine Havasi, and Henry Lieberman.
2008. Analogyspace: Reducing the dimensionality
of common sense knowledge. In AAAI, pages 548?
553.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Ilya Sutskever, Joshua B Tenenbaum, and Ruslan
Salakhutdinov. 2009. Modelling relational data us-
ing Bayesian clustered tensor factorization. In NIPS,
pages 1821?1828.
Ledyard R Tucker. 1966. Some mathematical notes
on three-mode factor analysis. Psychometrika,
31(3):279?311.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366?1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456?1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
Appendix A Detailed Derivation
We first introduce some lemmas that will be useful
for our derivation. Lemmas 2, 3 and 4 are the basic
properties of the Kronecker product. Their proofs
can be found at (Laub, 2005).
Lemma 1. Let V be an orthogonal matrix and
? a diagonal matrix. Then (I + V?V
T
)
?1
=
V(I + ?)
?1
V
T
.
Proof.
(I + V?V
T
)
?1
= (VIV
T
+ V?V
T
)
?1
= V(I + ?)
?1
V
T
Lemma 2. (A?B)(C?D) = AC?BD.
Lemma 3. (A?B)
T
= A
T
?B
T
.
Lemma 4. If A and B are orthogonal matrices,
then A?B will also be an orthogonal matrix.
Let Z = A ? A and apply singular value
decomposition to A = U?V
T
. The term
(
Z
T
Z + ?I
)
?1
can be rewritten as:
(
Z
T
Z + ?I
)
?1
=
(
?I + (A
T
?A
T
)(A?A)
)
?1
(6)
=
(
?I + A
T
A?A
T
A
)
?1
(7)
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
(8)
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
(9)
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
(10)
Eq. (6) is from replacing Z with A ? A and
Lemma 3. Eq. (7) is from Lemma 2. Eq. (8) is
from the properties of SVD, where U and V are
orthonormal matrices. Eq. (9) is from Lemma 2
and Lemma 3. Finally, Eq. (10) comes from
Lemma 1.
1578
Figure 3: Prediction performance of TRESCAL
and RESCAL with different rank (r).
Figure 4: Prediction performance of TRESCAL
with different regularization parameter (?).
Appendix B Hyper-parameter Sensitivity
We study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?.
We use the task of relation retrieval and present
the model performance on the development set.
Fig. 3 shows the performance of TRESCAL and
RESCAL with different rank (r) values while fix-
ing ? = 0.01. Results show that both TRESCAL
and RESCAL achieve better performance when r
is reasonably large. TRESCAL obtains a bet-
ter model with smaller r than RESCAL, because
TRESCAL only needs to fit the triples of the com-
patible entity types. Therefore, it allows to use
smaller number of latent variables to fit the train-
ing data.
Fixing r = 400, Fig. 4 shows the performance
of TRESCAL at different values of the regulariza-
tion parameter ?, including no regularization at
all (? = 0). While the results suggest that the
method is not very sensitive to ?, tuning ? is still
necessary for achieving the best performance.
1579
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 616?620,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Measuring Word Relatedness Using Heterogeneous Vector Space Models
Wen-tau Yih
Microsoft Research
One Microsoft Way
Redmond, WA
scottyih@microsoft.com
Vahed Qazvinian?
Department of EECS
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Abstract
Noticing that different information sources of-
ten provide complementary coverage of word
sense and meaning, we propose a simple and
yet effective strategy for measuring lexical se-
mantics. Our model consists of a committee
of vector space models built on a text cor-
pus, Web search results and thesauruses, and
measures the semantic word relatedness us-
ing the averaged cosine similarity scores. De-
spite its simplicity, our system correlates with
human judgements better or similarly com-
pared to existing methods on several bench-
mark datasets, including WordSim353.
1 Introduction
Measuring the semantic relatedness of words is a
fundamental problem in natural language process-
ing and has many useful applications, including
textual entailment, word sense disambiguation, in-
formation retrieval and automatic thesaurus discov-
ery. Existing approaches can be roughly catego-
rized into two kinds: knowledge-based and corpus-
based, where the former includes graph-based algo-
rithms and similarity measures operating on a lexical
database such as WordNet (Budanitsky and Hirst,
2006; Agirre et al, 2009) and the latter consists
of various kinds of vector space models (VSMs)
constructed with the help of a large collection of
text (Reisinger and Mooney, 2010; Radinsky et al,
2011). In this paper, we present a conceptually
simple model for solving this problem. Observing
that various kinds of information sources, such as
?Work conducted while interning at Microsoft Research.
general text corpora, Web search results and the-
sauruses, have different word and sense coverage,
we first build individual vector space models from
each of them separately. Given two words, each
VSM measures the semantic relatedness by the co-
sine similarity of the corresponding vectors in its
space. The final prediction is simply the averaged
cosine scores derived from these VSMs. Despite
its simplicity, our system surprisingly yields very
strong empirical performance. When comparing the
predictions with the human annotations on four dif-
ferent datasets, our system achieves higher correla-
tion than existing methods on two datasets and pro-
vides very competitive results on the others.
The rest of this paper is organized as follows. Sec-
tion 2 briefly reviews the related work. Section 3 de-
tails how we construct each individual vector space
model, followed by the experimental evaluation in
Section 4. Finally, Section 5 concludes the paper.
2 Background
Prior work on measuring lexical semantics can be
categorized as knowledge-based or corpus-based.
Knowledge-based methods leverage word relations
encoded in lexical databases such as WordNet and
provide graph-based similarity measures. Detailed
comparisons of these methods can be found in (Bu-
danitsky and Hirst, 2006). Corpus-based methods
assume related words tend to co-occur or to ap-
pear in similar context. For example, Gabrilovich
and Markovitch (2007) measure word relatedness by
whether they tend to occur in the same Wikipedia
topic. In contrast, Reisinger and Mooney (2010)
use the conventional ?context vector? ? neighboring
616
terms of the occurrences of a target word ? as the
word representation. In addition, they argue that it
is difficult to capture different senses of a word with
a single vector, and introduce a multi-prototype rep-
resentation. More recently, Radinsky et al (2011)
analyze the temporal aspects of words and argue that
non-identical terms in two term vectors should also
be compared based on their temporal usage when
computing the similarity score. They construct the
vectors using Wikipedia titles, Flickr image tags,
and Del.icio.us bookmarks, and extract the temporal
frequency of each concept from 130 years of New
York Times archive. Methods that combine models
from different sources do exist. For instance, Agirre
et al (2009) derive a WordNet-based measure us-
ing PageRank and combined it with several corpus-
based vector space models using SVMs.
3 Vector Space Models from
Heterogeneous Sources
In this section, we describe how we construct vari-
ous vector space models (VSMs) to represent words,
including corpus-based, Web-based and thesaurus-
based methods.
Corpus-based VSMs follow the standard ?distri-
butional hypothesis,? which states that words ap-
pearing in the same contexts tend to have simi-
lar meaning (Harris, 1954). Each target word is
thus represented by a high-dimensional sparse term-
vector that consists of words occurring in its con-
text. Given a corpus, we first collect terms within
a window of [?10,+10] centered at each occur-
rence of a target word. This bag-of-words repre-
sentation is then mapped to the TF-IDF term vector:
each term is weighted by log(freq) ? log(N/df),
where freq is the number of times the term appears
in the collection, df the document frequency of the
term in the whole corpus and N the number of total
documents. We further employed two simple tech-
niques to improve the quality of these term-vectors:
vocabulary and term trimming. Top 1,500 terms
with high document frequency values are treated
as stopwords and removed from the vocabulary.
Moreover, we adopted a document-specific feature
selection method (Kolcz and Yih, 2007) designed
originally for text classification and retain only the
top 200 high-weighted terms for each term-vector1.
The corpus-based VSMs are created using English
Wikipedia (Snapshot of Nov. 2010), consisting of
917M words after preprocessing (markup tags re-
moval and sentence splitting).
Web-based VSMs leverage Web search results to
form a vector of each query (Sahami and Heilman,
2006). For each word to compare, we issue it as a
query and retrieve the set of relevant snippets (top
30 in our experiments) using a popular commercial
search engine, Bing. All these snippets together are
viewed as a pseudo-document and mapped to a TF-
IDF vector as in the corpus-based method. We do
not allow for automatic query expansion in our ex-
periments to ensure that the retrieved snippets are di-
rectly relevant to the target word and not expansions
based on synonyms, hypernyms or hyponyms. We
apply vocabulary trimming (top 1,000 terms with
high DF values), but not term-trimming as the vec-
tors have much fewer terms due to the small number
of snippets collected.
Both the corpus-based and Web-based VSMs rely
on the distributional hypothesis, which is often criti-
cized for two weaknesses. The first is that word pairs
that appear in the same context or co-occur are not
necessarily highly semantically related. For exam-
ple, ?bread? and ?butter? often have cosine scores
higher than synonyms using corpus-based vectors
because of the phrase ?bread and butter?. The sec-
ond is that general corpora often have skewed cov-
erage of words due to the Zipf?s law. Regardless of
the size of the corpus, the number of occurrences
of a rarely used word is typically very low, which
makes the quality of the corresponding vector unre-
liable. To address these two issues, we include the
thesaurus-based VSMs in this work as well. For
each group of similar words (synset) defined in the
thesaurus, we treat it as a ?document? and create a
document?word matrix, where each word is again
weighted using its TF-IDF value. Each column vec-
tor in this matrix is thus the thesaurus-based vec-
tor of the corresponding word. Notice that given
two words and their corresponding vectors, the co-
sine score is more general than simply checking
1In preliminary experiments, we found that active terms
with low TF-IDF values tend to be noise. By aggressively
removing them, the quality of the term-vectors can be signifi-
cantly improved.
617
whether these two words belong to a group of sim-
ilar words, as it judges how often they overlap in
various documents (i.e., sets of similar words). We
explored using two different thesauri in our exper-
iments: WordNet and the Encarta thesaurus devel-
oped by Bloomsbury Publishing, where the former
consists of 227,446 synsets and 190,052 words and
the latter contains 46,945 synsets and 50,184 words.
Compared to existing knowledge-based approaches,
our VSM transformation is very simple and straight-
forward. It is also easy to extend our method to other
languages as only a thesaurus is required rather than
a complete lexical database such as WordNet.
4 Experimental Evaluation
In this section, we evaluate the quality of the VSMs
constructed using methods described in Section 3 on
different benchmark datasets, as well as the perfor-
mance when combining them.
4.1 Benchmark datasets
We follow the standard evaluation method, which di-
rectly tests the correlation of the word relatedness
measures with human judgements on a set of word
pairs, using the Spearman?s rank correlation coeffi-
cient. Our study was conducted using four differ-
ent datasets, including WS-353, RG-65, MC-30 and
MTurk-287.
The WordSim353 dataset (WS-353) is the largest
among them and has been used extensively in re-
cent work. Originally collected by Finkelstein et
al. (2001), the dataset consists of 353 word pairs.
The degree of relatedness of each pair is assessed
on a 0-10 scale by 13-16 human judges, where the
mean is used as the final score. Examining the
relations between the words in each pair, Agirre
et al (2009) further split this dataset into similar
pairs (WS-sim) and related pairs (WS-rel), where
the former contains synonyms, antonyms, identical
words and hyponyms/hypernyms and the latter cap-
ture other word relations. Collected by Rubenstein
and Goodenough (1965), RG-65 contains 65 pairs
of words that are either synonyms or unrelated, as-
sessed on a 0-4 scale by 51 human subjects. Taking
30 pairs from them, Miller and Charles (1991) cre-
ated the (MC-30) dataset by reassessing these word
pairs using 38 subjects. These 30 pairs of words
are also a subset of WS-353. Although these three
datasets contain overlapping word pairs, their scores
are different because of the degree of relatedness
were given by different human subjects. In addition
to these datasets, we also evaluate our VSMs on the
Mturk-287 dataset that consists of 287 word pairs
collected by (Radinsky et al, 2011) using Amazon
MTurk.
4.2 Results and Analysis
Table 1 summarizes the results of various methods,
where the top part lists the performance of state-of-
the-art systems and the bottom shows the results of
individual vector space models, as well as combin-
ing these models using the averaged cosine scores.
We make several observations here. First, while
none of the four VSMs we tested outperforms the
best existing systems on the benchmark datasets,
surprisingly, using the averaged cosine scores of
these models, the performance is improved substan-
tially. It achieves higher Spearman?s rank coeffi-
cient on WS-353 and MTurk-287 than any other sys-
tems2 and are close to the state-of-the-art on MC-
30 and RG-65. Unlike some approach like (Hughes
and Ramage, 2007), which performs well on some
datasets but poorly on others, combing the VSMs
from heterogeneous sources is more robust. Individ-
ually, we notice that Wikipedia context VSM pro-
vides consistently strong results, while thesaurus-
based models work only reasonable on MC-30 and
RG-65, potentially because other datasets contain
more out-of-vocabulary words or proper nouns. Due
to the inherent ambiguity of the task, there is a high
variance among judgements from different annota-
tors. Therefore, it is unrealistic to assume any of the
methods can correlate perfectly to the mean human
judgement scores. In fact, the inter-agreement study
done on the WS-353 dataset indicates that the result
of our approach of combining heterogeneous VSMs
is close to the averaged human performance.
It is intriguing to see that by using the averaged
cosine scores, the performance can be improved
over the best individual model (i.e., Wikipedia). Ex-
amining the scores of some word pairs carefully sug-
2This may not be statistically significant. Without having
the exact output of existing systems, it is difficult to conduct a
robust statistical significance test given the small sizes of these
datasets.
618
Spearman?s ?
Method WS-353 WS-sim WS-rel MC-30 RG-65 MTurk-287
(Radinsky et al, 2011) 0.80 - - - - 0.63
(Reisinger and Mooney, 2010) 0.77 - - - - -
(Agirre et al, 2009) 0.78 0.83 0.72 0.92 0.96 -
(Gabrilovich and Markovitch, 2007) 0.75 - - - - 0.59
(Hughes and Ramage, 2007) 0.55 - - 0.90 0.84 -
Web Search 0.56 0.56 0.54 0.48 0.44 0.44
Wikipedia 0.73 0.80 0.73 0.87 0.83 0.62
Bloomsbury 0.45 0.60 0.60 0.71 0.78 0.29
WordNet 0.37 0.49 0.49 0.79 0.78 0.25
Combining VSMs 0.81 0.87 0.77 0.89 0.89 0.68
Table 1: The performance of the state-of-the-art methods and different vector space models on measuring semantic
word relatedness using the cosine similarity.
gests the broader coverage of different words and
senses could be the reason. For example, some
of the words in the datasets have multiple senses,
such as ?jaguar vs. car? and ?jaguar vs. cat?. Al-
though in previous work, researchers try to capture
word senses using different vectors (Reisinger and
Mooney, 2010) from the same text corpus, this is in
fact difficult in practice. The usage of words in a big
text corpus, which contains diversified topics, may
still be biased to one word sense. For example, in
the Wikipeida term vector that represents ?jaguar?,
we found that most of the terms there are related to
?cat?. Although some terms are associated with the
?car? meaning, the signals are rather weak. Simi-
larly, WordNet does not indicate ?jaguar? could be
related to ?car? at all. In contrast, the ?car? sense
of ?jaguar? dominates the vector created using the
search engine. As a result, incorporating models
from different sources could be more effective than
relying on word sense discovering algorithms op-
erating solely on one corpus. Another similar but
different example is the pair of ?bread? and ?but-
ter?, which are treated as synonyms by corpus-based
VSMs, but is demoted after adding the thesaurus-
based models.
5 Conclusion
In this paper we investigated the usefulness of het-
erogeneous information sources in improving mea-
sures of semantic word relatedness. Particularly, we
created vector space models using 4 data sources
from 3 categories (corpus-based, Web-based and
thesaurus-based) and found that simply averaging
the cosine similarity derived from these models
yields a very robust measure. Other than directly ap-
plying it to measuring semantic relatedness, our ap-
proach is complementary to more sophisticated sim-
ilarity measures such as developing kernel functions
for different structured data (Croce et al, 2011),
where the similarity between words serves as a basic
component.
While this result is interesting and encouraging, it
also raises several research questions, such as how
to enhance the quality of each vector space model
and whether the models can be combined more ef-
fectively3. We also would like to study whether sim-
ilar techniques can be useful when comparing longer
text segments like phrases or sentences, with poten-
tial applications in paraphrase detection and recog-
nizing textual entailment.
Acknowledgments
We thank Joseph Reisinger for providing his pro-
totype vectors for our initial study, Silviu-Petru
Cucerzan for helping process the Wikipedia files and
Geoffrey Zweig for preparing the Bloomsbury the-
saurus data. We are also grateful to Chris Meek
for valuable discussions and to anonymous review-
ers for their comments.
3We conducted some preliminary experiments (not reported
here) on tuning the weights of combining different models
based on cross-validation, but did not find consistent improve-
ments, perhaps due to the limited size of the data.
619
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and wordnet-based ap-
proaches. In NAACL ?09, pages 19?27.
A. Budanitsky and G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13?47, March.
D. Croce, A. Moschitti, and R. Basili. 2011. Structured
lexical similarity via convolution kernels on depen-
dency trees. In Proceedings of EMNLP 2011, pages
1034?1046, July.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing
search in context: The concept revisited. In WWW,
pages 406?414. ACM.
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using wikipedia-based explicit se-
mantic analysis. In IJCAI ?07, pages 1606?1611.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
A. Kolcz and W. Yih. 2007. Raising the baseline for
high-precision text classifiers. In KDD ?07, pages
400?409.
G. Miller and W. Charles. 1991. Contextual correlates
of semantic similarity. Language and cognitive pro-
cesses, 6(1):1?28.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ?10.
H. Rubenstein and J. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8:627?633, October.
M. Sahami and T. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international con-
ference on World Wide Web, pages 377?386. ACM.
620
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of NAACL-HLT 2013, pages 746?751,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Linguistic Regularities in Continuous Space Word Representations
Tomas Mikolov? , Wen-tau Yih, Geoffrey Zweig
Microsoft Research
Redmond, WA 98052
Abstract
Continuous space language models have re-
cently demonstrated outstanding results across
a variety of tasks. In this paper, we ex-
amine the vector-space word representations
that are implicitly learned by the input-layer
weights. We find that these representations
are surprisingly good at capturing syntactic
and semantic regularities in language, and
that each relationship is characterized by a
relation-specific vector offset. This allows
vector-oriented reasoning based on the offsets
between words. For example, the male/female
relationship is automatically learned, and with
the induced vector representations, ?King -
Man + Woman? results in a vector very close
to ?Queen.? We demonstrate that the word
vectors capture syntactic regularities by means
of syntactic analogy questions (provided with
this paper), and are able to correctly answer
almost 40% of the questions. We demonstrate
that the word vectors capture semantic regu-
larities by using the vector offset method to
answer SemEval-2012 Task 2 questions. Re-
markably, this method outperforms the best
previous systems.
1 Introduction
A defining feature of neural network language mod-
els is their representation of words as high dimen-
sional real valued vectors. In these models (Ben-
gio et al, 2003; Schwenk, 2007; Mikolov et al,
2010), words are converted via a learned lookup-
table into real valued vectors which are used as the
?Currently at Google, Inc.
inputs to a neural network. As pointed out by the
original proposers, one of the main advantages of
these models is that the distributed representation
achieves a level of generalization that is not possi-
ble with classical n-gram language models; whereas
a n-gram model works in terms of discrete units that
have no inherent relationship to one another, a con-
tinuous space model works in terms of word vectors
where similar words are likely to have similar vec-
tors. Thus, when the model parameters are adjusted
in response to a particular word or word-sequence,
the improvements will carry over to occurrences of
similar words and sequences.
By training a neural network language model, one
obtains not just the model itself, but also the learned
word representations, which may be used for other,
potentially unrelated, tasks. This has been used to
good effect, for example in (Collobert and Weston,
2008; Turian et al, 2010) where induced word rep-
resentations are used with sophisticated classifiers to
improve performance in many NLP tasks.
In this work, we find that the learned word repre-
sentations in fact capture meaningful syntactic and
semantic regularities in a very simple way. Specif-
ically, the regularities are observed as constant vec-
tor offsets between pairs of words sharing a par-
ticular relationship. For example, if we denote the
vector for word i as xi, and focus on the singu-
lar/plural relation, we observe that xapple?xapples ?
xcar?xcars, xfamily?xfamilies ? xcar?xcars, and
so on. Perhaps more surprisingly, we find that this
is also the case for a variety of semantic relations, as
measured by the SemEval 2012 task of measuring
relation similarity.
746
The remainder of this paper is organized as fol-
lows. In Section 2, we discuss related work; Section
3 describes the recurrent neural network language
model we used to obtain word vectors; Section 4 dis-
cusses the test sets; Section 5 describes our proposed
vector offset method; Section 6 summarizes our ex-
periments, and we conclude in Section 7.
2 Related Work
Distributed word representations have a long his-
tory, with early proposals including (Hinton, 1986;
Pollack, 1990; Elman, 1991; Deerwester et al,
1990). More recently, neural network language
models have been proposed for the classical lan-
guage modeling task of predicting a probability dis-
tribution over the ?next? word, given some preced-
ing words. These models were first studied in the
context of feed-forward networks (Bengio et al,
2003; Bengio et al, 2006), and later in the con-
text of recurrent neural network models (Mikolov et
al., 2010; Mikolov et al, 2011b). This early work
demonstrated outstanding performance in terms of
word-prediction, but also the need for more compu-
tationally efficient models. This has been addressed
by subsequent work using hierarchical prediction
(Morin and Bengio, 2005; Mnih and Hinton, 2009;
Le et al, 2011; Mikolov et al, 2011b; Mikolov et
al., 2011a). Also of note, the use of distributed
topic representations has been studied in (Hinton
and Salakhutdinov, 2006; Hinton and Salakhutdi-
nov, 2010), and (Bordes et al, 2012) presents a se-
mantically driven method for obtaining word repre-
sentations.
3 Recurrent Neural Network Model
The word representations we study are learned by a
recurrent neural network language model (Mikolov
et al, 2010), as illustrated in Figure 1. This architec-
ture consists of an input layer, a hidden layer with re-
current connections, plus the corresponding weight
matrices. The input vector w(t) represents input
word at time t encoded using 1-of-N coding, and the
output layer y(t) produces a probability distribution
over words. The hidden layer s(t) maintains a rep-
resentation of the sentence history. The input vector
w(t) and the output vector y(t) have dimensional-
ity of the vocabulary. The values in the hidden and
Figure 1: Recurrent Neural Network Language Model.
output layers are computed as follows:
s(t) = f (Uw(t) +Ws(t?1)) (1)
y(t) = g (Vs(t)) , (2)
where
f(z) =
1
1 + e?z
, g(zm) =
ezm
?
k e
zk
. (3)
In this framework, the word representations are
found in the columns of U, with each column rep-
resenting a word. The RNN is trained with back-
propagation to maximize the data log-likelihood un-
der the model. The model itself has no knowledge
of syntax or morphology or semantics. Remark-
ably, training such a purely lexical model to max-
imize likelihood will induce word representations
with striking syntactic and semantic properties.
4 Measuring Linguistic Regularity
4.1 A Syntactic Test Set
To understand better the syntactic regularities which
are inherent in the learned representation, we created
a test set of analogy questions of the form ?a is to b
as c is to ? testing base/comparative/superlative
forms of adjectives; singular/plural forms of com-
mon nouns; possessive/non-possessive forms of
common nouns; and base, past and 3rd person
present tense forms of verbs. More precisely, we
tagged 267M words of newspaper text with Penn
747
Category Relation Patterns Tested # Questions Example
Adjectives Base/Comparative JJ/JJR, JJR/JJ 1000 good:better rough:
Adjectives Base/Superlative JJ/JJS, JJS/JJ 1000 good:best rough:
Adjectives Comparative/
Superlative
JJS/JJR, JJR/JJS 1000 better:best rougher:
Nouns Singular/Plural NN/NNS,
NNS/NN
1000 year:years law:
Nouns Non-possessive/
Possessive
NN/NN POS,
NN POS/NN
1000 city:city?s bank:
Verbs Base/Past VB/VBD,
VBD/VB
1000 see:saw return:
Verbs Base/3rd Person
Singular Present
VB/VBZ, VBZ/VB 1000 see:sees return:
Verbs Past/3rd Person
Singular Present
VBD/VBZ,
VBZ/VBD
1000 saw:sees returned:
Table 1: Test set patterns. For a given pattern and word-pair, both orderings occur in the test set. For example, if
?see:saw return: ? occurs, so will ?saw:see returned: ?.
Treebank POS tags (Marcus et al, 1993). We then
selected 100 of the most frequent comparative adjec-
tives (words labeled JJR); 100 of the most frequent
plural nouns (NNS); 100 of the most frequent pos-
sessive nouns (NN POS); and 100 of the most fre-
quent base form verbs (VB). We then systematically
generated analogy questions by randomly matching
each of the 100 words with 5 other words from the
same category, and creating variants as indicated in
Table 1. The total test set size is 8000. The test set
is available online. 1
4.2 A Semantic Test Set
In addition to syntactic analogy questions, we used
the SemEval-2012 Task 2, Measuring Relation Sim-
ilarity (Jurgens et al, 2012), to estimate the extent
to which RNNLM word vectors contain semantic
information. The dataset contains 79 fine-grained
word relations, where 10 are used for training and
69 testing. Each relation is exemplified by 3 or
4 gold word pairs. Given a group of word pairs
that supposedly have the same relation, the task is
to order the target pairs according to the degree to
which this relation holds. This can be viewed as an-
other analogy problem. For example, take the Class-
Inclusion:Singular Collective relation with the pro-
1http://research.microsoft.com/en-
us/projects/rnn/default.aspx
totypical word pair clothing:shirt. To measure the
degree that a target word pair dish:bowl has the same
relation, we form the analogy ?clothing is to shirt as
dish is to bowl,? and ask how valid it is.
5 The Vector Offset Method
As we have seen, both the syntactic and semantic
tasks have been formulated as analogy questions.
We have found that a simple vector offset method
based on cosine distance is remarkably effective in
solving these questions. In this method, we assume
relationships are present as vector offsets, so that in
the embedding space, all pairs of words sharing a
particular relation are related by the same constant
offset. This is illustrated in Figure 2.
In this model, to answer the analogy question a:b
c:d where d is unknown, we find the embedding
vectors xa, xb, xc (all normalized to unit norm), and
compute y = xb ? xa + xc. y is the continuous
space representation of the word we expect to be the
best answer. Of course, no word might exist at that
exact position, so we then search for the word whose
embedding vector has the greatest cosine similarity
to y and output it:
w? = argmaxw
xwy
?xw??y?
When d is given, as in our semantic test set, we
simply use cos(xb ? xa + xc, xd) for the words
748
Figure 2: Left panel shows vector offsets for three word
pairs illustrating the gender relation. Right panel shows
a different projection, and the singular/plural relation for
two words. In high-dimensional space, multiple relations
can be embedded for a single word.
provided. We have explored several related meth-
ods and found that the proposed method performs
well for both syntactic and semantic relations. We
note that this measure is qualitatively similar to rela-
tional similarity model of (Turney, 2012), which pre-
dicts similarity between members of the word pairs
(xb, xd), (xc, xd) and dis-similarity for (xa, xd).
6 Experimental Results
To evaluate the vector offset method, we used
vectors generated by the RNN toolkit of Mikolov
(2012). Vectors of dimensionality 80, 320, and 640
were generated, along with a composite of several
systems, with total dimensionality 1600. The sys-
tems were trained with 320M words of Broadcast
News data as described in (Mikolov et al, 2011a),
and had an 82k vocabulary. Table 2 shows results
for both RNNLM and LSA vectors on the syntactic
task. LSA was trained on the same data as the RNN.
We see that the RNN vectors capture significantly
more syntactic regularity than the LSA vectors, and
do remarkably well in an absolute sense, answering
more than one in three questions correctly. 2
In Table 3 we compare the RNN vectors with
those based on the methods of Collobert and We-
ston (2008) and Mnih and Hinton (2009), as imple-
mented by (Turian et al, 2010) and available online
3 Since different words are present in these datasets,
we computed the intersection of the vocabularies of
the RNN vectors and the new vectors, and restricted
the test set and word vectors to those. This resulted
in a 36k word vocabulary, and a test set with 6632
2Guessing gets a small fraction of a percent.
3http://metaoptimize.com/projects/wordreprs/
Method Adjectives Nouns Verbs All
LSA-80 9.2 11.1 17.4 12.8
LSA-320 11.3 18.1 20.7 16.5
LSA-640 9.6 10.1 13.8 11.3
RNN-80 9.3 5.2 30.4 16.2
RNN-320 18.2 19.0 45.0 28.5
RNN-640 21.0 25.2 54.8 34.7
RNN-1600 23.9 29.2 62.2 39.6
Table 2: Results for identifying syntactic regularities for
different word representations. Percent correct.
Method Adjectives Nouns Verbs All
RNN-80 10.1 8.1 30.4 19.0
CW-50 1.1 2.4 8.1 4.5
CW-100 1.3 4.1 8.6 5.0
HLBL-50 4.4 5.4 23.1 13.0
HLBL-100 7.6 13.2 30.2 18.7
Table 3: Comparison of RNN vectors with Turian?s Col-
lobert and Weston based vectors and the Hierarchical
Log-Bilinear model of Mnih and Hinton. Percent correct.
questions. Turian?s Collobert andWeston based vec-
tors do poorly on this task, whereas the Hierarchical
Log-Bilinear Model vectors of (Mnih and Hinton,
2009) do essentially as well as the RNN vectors.
These representations were trained on 37M words
of data and this may indicate a greater robustness of
the HLBL method.
We conducted similar experiments with the se-
mantic test set. For each target word pair in a rela-
tion category, the model measures its relational sim-
ilarity to each of the prototypical word pairs, and
then uses the average as the final score. The results
are evaluated using the two standard metrics defined
in the task, Spearman?s rank correlation coefficient
? and MaxDiff accuracy. In both cases, larger val-
ues are better. To compare to previous systems, we
report the average over all 69 relations in the test set.
From Table 4, we see that as with the syntac-
tic regularity study, the RNN-based representations
perform best. In this case, however, Turian?s CW
vectors are comparable in performance to the HLBL
vectors. With the RNN vectors, the performance im-
proves as the number of dimensions increases. Sur-
prisingly, we found that even though the RNN vec-
749
Method Spearman?s ? MaxDiff Acc.
LSA-640 0.149 0.364
RNN-80 0.211 0.389
RNN-320 0.259 0.408
RNN-640 0.270 0.416
RNN-1600 0.275 0.418
CW-50 0.159 0.363
CW-100 0.154 0.363
HLBL-50 0.149 0.363
HLBL-100 0.146 0.362
UTD-NB 0.230 0.395
Table 4: Results in measuring relation similarity
tors are not trained or tuned specifically for this task,
the model achieves better results (RNN-320, RNN-
640 & RNN-1600) than the previously best perform-
ing system, UTD-NB (Rink and Harabagiu, 2012).
7 Conclusion
We have presented a generally applicable vector off-
set method for identifying linguistic regularities in
continuous space word representations. We have
shown that the word representations learned by a
RNNLM do an especially good job in capturing
these regularities. We present a new dataset for mea-
suring syntactic performance, and achieve almost
40% correct. We also evaluate semantic general-
ization on the SemEval 2012 task, and outperform
the previous state-of-the-art. Surprisingly, both re-
sults are the byproducts of an unsupervised maxi-
mum likelihood training criterion that simply oper-
ates on a large amount of text data.
References
Y. Bengio, R. Ducharme, Vincent, P., and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Reseach, 3(6).
Y. Bengio, H. Schwenk, J.S. Sene?cal, F. Morin, and J.L.
Gauvain. 2006. Neural probabilistic language models.
Innovations in Machine Learning, pages 137?186.
A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012.
Joint learning of words and meaning representations
for open-text semantic parsing. In Proceedings of 15th
International Conference on Artificial Intelligence and
Statistics.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th
international conference on Machine learning, pages
160?167. ACM.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
J.L. Elman. 1991. Distributed representations, simple re-
current networks, and grammatical structure. Machine
learning, 7(2):195?225.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504?507.
G. Hinton and R. Salakhutdinov. 2010. Discovering bi-
nary codes for documents by learning deep generative
models. Topics in Cognitive Science, 3(1):74?91.
G.E. Hinton. 1986. Learning distributed representations
of concepts. In Proceedings of the eighth annual con-
ference of the cognitive science society, pages 1?12.
Amherst, MA.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. Semeval-2012 task 2: Measuring de-
grees of relational similarity. In *SEM 2012: The First
Joint Conference on Lexical and Computational Se-
mantics (SemEval 2012), pages 356?364. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and
F. Yvon. 2011. Structured output layer neural network
language model. In Proceedings of ICASSP 2011.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proceedings of ASRU 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Tomas Mikolov. 2012. RNN toolkit.
A. Mnih and G.E. Hinton. 2009. A scalable hierarchical
distributed language model. Advances in neural infor-
mation processing systems, 21:1081?1088.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
750
international workshop on artificial intelligence and
statistics, pages 246?252.
J.B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46(1):77?105.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
*SEM 2012: The First Joint Conference on Lexical
and Computational Semantics (SemEval 2012), pages
413?418. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492
? 518.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
P.D. Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533?585.
751
Proceedings of NAACL-HLT 2013, pages 1000?1009,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Combining Heterogeneous Models for Measuring Relational Similarity
Alisa Zhila?
Instituto Politecnico Nacional
Mexico City, Mexico
alisa.zhila@gmail.com
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Geoffrey Zweig
Microsoft Research
Redmond, WA 98052, USA
gzweig@microsoft.com
Tomas Mikolov?
BRNO University of Technology
BRNO, Czech Republic
tmikolov@gmail.com
Abstract
In this work, we study the problem of mea-
suring relational similarity between two word
pairs (e.g., silverware:fork and clothing:shirt).
Due to the large number of possible relations,
we argue that it is important to combine mul-
tiple models based on heterogeneous informa-
tion sources. Our overall system consists of
two novel general-purpose relational similar-
ity models and three specific word relation
models. When evaluated in the setting of a
recently proposed SemEval-2012 task, our ap-
proach outperforms the previous best system
substantially, achieving a 54.1% relative in-
crease in Spearman?s rank correlation.
1 Introduction
The problem of measuring relational similarity is
to determine the degree of correspondence between
two word pairs. For instance, the analogous word
pairs silverware:fork and clothing:shirt both exem-
plify well a Class-Inclusion:Singular Collective re-
lation and thus have high relational similarity. Un-
like the problem of attributional similarity, which
measures whether two words share similar attributes
and is addressed in extensive research work (Bu-
danitsky and Hirst, 2006; Reisinger and Mooney,
2010; Radinsky et al, 2011; Agirre et al, 2009; Yih
and Qazvinian, 2012), measuring relational similar-
ity is a relatively new research direction pioneered
by Turney (2006), but with many potential appli-
cations. For instance, problems of identifying spe-
cific relations between words, such as synonyms,
?Work conducted while interning at Microsoft Research.
antonyms or associations, can be reduced to mea-
suring relational similarity compared to prototypical
word pairs with the desired relation (Turney, 2008).
In scenarios like information extraction or question
answering, where identifying the existence of cer-
tain relations is often the core problem, measuring
relational similarity provides a more flexible solu-
tion rather than creating relational classifiers for pre-
defined or task-specific categories of relations (Tur-
ney, 2006; Jurgens et al, 2012).
In order to promote this research direction, Ju-
rgens et al (2012) proposed a new shared task of
measuring relational similarity in SemEval-2012 re-
cently. In this task, each submitted system is re-
quired to judge the degree of a target word pair
having a particular relation, measured by its re-
lational similarity compared to a few prototypical
example word pairs. The system performance is
evaluated by its correlation with the human judg-
ments using two evaluation metrics, Spearman?s
rank correlation and MaxDiff accuracy (more de-
tails of the task and evaluation metrics will be given
in Sec. 3). Although participating systems incorpo-
rated substantial amounts of information from lex-
ical resources (e.g., WordNet) and contextual pat-
terns from large corpora, only one system (Rink and
Harabagiu, 2012) is able to outperform a simple
baseline that uses PMI (pointwise mutual informa-
tion) scoring, which demonstrates the difficulty of
this task.
In this paper, we explore the problem of mea-
suring relational similarity in the same task setting.
We argue that due to the large number of possible
relations, building an ensemble of relational simi-
1000
larity models based on heterogeneous information
sources is the key to advance the state-of-the-art on
this problem. By combining two general-purpose re-
lational similarity models with three specific word-
relation models covering relations like IsA and syn-
onymy/antonymy, we improve the previous state-
of-the-art substantially ? having a relative gain of
54.1% in Spearman?s rank correlation and 14.7% in
the MaxDiff accuracy!
Our main contributions are threefold. First, we
propose a novel directional similarity method based
on the vector representation of words learned from
a recurrent neural network language model. The re-
lation of two words is captured by their vector off-
set in the latent semantic space. Similarity of rela-
tions can then be naturally measured by a distance
function in the vector space. This method alone
already performs better than all existing systems.
Second, unlike the previous finding, where SVMs
learn a much poorer model than naive Bayes (Rink
and Harabagiu, 2012), we show that using a highly-
regularized log-linear model on simple contextual
pattern features collected from a document collec-
tion of 20GB, a discriminative approach can learn a
strong model as well. Third, we demonstrate that by
augmenting existing word-relation models, which
cover only a small number of relations, the overall
system can be further improved.
The rest of this paper is organized as follows. We
first survey the related work in Sec. 2 and formally
define the problem in Sec. 3. We describe the indi-
vidual models in detail in Sec. 4. The combination
approach is depicted in Sec. 5, along with experi-
mental comparisons to individual models and exist-
ing systems. Finally, Sec. 6 concludes the paper.
2 Related Work
Building a classifier to determine whether a relation-
ship holds between a pair of words is a natural ap-
proach to the task of measuring relational similarity.
While early work was mostly based on hand-crafted
rules (Finin, 1980; Vanderwende, 1994), Rosario
and Hearst (2001) introduced a machine learning ap-
proach to classify word pairs. They targeted clas-
sifying noun modifier pairs from the medical do-
main into 13 classes of semantic relations. Fea-
tures for each noun modifier pair were constructed
using large medical lexical resources and a multi-
class classifier was trained using a feed-forward neu-
ral network with one hidden layer. This work was
later extended by Nastase and Szpakowicz (2003)
to classify general domain noun-modifier pairs into
30 semantic relations. In addition to extracting fea-
tures using WordNet and Roget?s Thesaurus, they
also experimented with several different learners in-
cluding decision trees, memory-based learning and
inductive logic programming methods like RIPPER
and FOIL. Using the same dataset as in (Nastase
and Szpakowicz, 2003), Turney and Littman (2005)
created a 128-dimentional feature vector for each
word pair based on statistics of their co-occurrence
patterns in Web documents and applied the k-NN
method (k = 1 in their work).
Measuring relational similarity, which determines
whether two word pairs share the same relation, can
be viewed as an extension of classifying relations
between two words. Treating a relational similar-
ity measure as a distance metric, a testing pair of
words can be judged by whether they have a rela-
tion that is similar to some prototypical word pairs
having a particular relation. A multi-relation clas-
sifier can thus be built easily in this framework as
demonstrated in (Turney, 2008), where the prob-
lems of identifying synonyms, antonyms and asso-
ciated words are all reduced to finding good anal-
ogous word pairs. Measuring relational similarity
has been advocated and pioneered by Turney (2006),
who proposed a latent vector space model for an-
swering SAT analogy questions (e.g., mason:stone
vs. carpenter:wood). In contrast, we take a slightly
different view when building a relational similarity
measure. Existing classifiers for specific word re-
lations (e.g., synonyms or Is-A) are combined with
general relational similarity measures. Empirically,
mixing heterogeneous models tends to make the fi-
nal relational similarity measure more robust.
Although datasets for semantic relation classifica-
tion or SAT analogous questions can be used to eval-
uate a relational similarity model, their labels are ei-
ther binary or categorical, which makes the datasets
suboptimal for determining the quality of a model
when evaluated on instances of the same relation
class. As a result, Jurgens et al (2012) proposed a
new task of ?Measuring Degrees of Relational Simi-
larity? at SemEval-2012, which includes 79 relation
1001
categories exemplified by three or four prototypical
word pairs and a schematic description. For exam-
ple, for the Class-Inclusion:Taxonomic relation, the
schematic description is ?Y is a kind/type/instance
of X?. Using Amazon Mechanical Turk1, they col-
lected word pairs for each relation, as well as their
degrees of being a good representative of a partic-
ular relation when compared with defining exam-
ples. Participants of this shared task proposed var-
ious kinds of approaches that leverage both lexical
resources and general corpora. For instance, the
Duluth systems (Pedersen, 2012) created word vec-
tors based on WordNet and estimated the degree of
a relation using cosine similarity. The BUAP sys-
tem (Tovar et al, 2012) represented each word pair
as a whole by a vector of 4 different types of fea-
tures: context, WordNet, POS tags and the aver-
age number of words separating the two words in
text. The degree of relation was then determined
by the cosine distance of the target pair from the
prototypical examples of each relation. Although
their models incorporated a significant amount of
information of words or word pairs, unfortunately,
the performance were not much better than a ran-
dom baseline, which indicates the difficulty of this
task. In comparison, a supervised learning approach
seems more promising. The UTD system (Rink and
Harabagiu, 2012), which mined lexical patterns be-
tween co-occurring words in the corpus and then
used them as features to train a Naive Bayes classi-
fier, achieved the best results. However, potentially
due to the large feature space, this strategy did not
work as well when switching the learning algorithm
to SVMs.
3 Problem Definition & Task Description
Following the setting of SemEval-2012 Task 2 (Ju-
rgens et al, 2012), the problem of measur-
ing the degree of relational similarity is to rate
word pairs by the degree to which they are
prototypical members of a given relation class.
For instance, comparing to the prototypical word
pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of
the Class-Inclusion:Singular Collective relation, we
would like to know among the input word pairs
{dish:bowl, book:novel, furniture:desk}, which one
1http://www.mturk.com
best demonstrates the relation.
Because our approaches are evaluated using the
data provided in this SemEval-2012 task, we de-
scribe briefly below how the data was collected, as
well as the metrics used to evaluate system perfor-
mance. The dataset consists of 79 relation classes
that are chosen according to (Bejar et al, 1991)
and broadly fall into 10 main categories, includ-
ing Class-Inclusion, Part-Whole, Similar and more.
With the help of Amazon Mechanical Turk, Jurgens
et al (2012) used a two-phase approach to collect
word pairs and their degrees. In the first phase,
a lexical schema, such as ?a Y is one item in a
collection/group of X? for the aforementioned rela-
tion Class-Inclusion:Singular Collective, and a few
prototypical pairs for each class were given to the
workers, who were asked to provide approximately
a list of 40 word pairs representing the same rela-
tion class. Naturally, some of these pairs were bet-
ter examples than the others. Therefore, in the sec-
ond phase, the goal was to measure the degree of
their similarity to the corresponding relation. This
was done using the MaxDiff technique (Louviere
and Woodworth, 1991). For each relation, about one
hundred questions were first created. Each question
consists of four different word pairs randomly sam-
pled from the list. The worker was then asked to
choose the most and least representative word pairs
for the specific relation in each question.
The set of 79 word relations were randomly split
into training and testing sets. The former contains
10 relations and the latter has 69. Word pairs in all
79 relations were given to the task participants in ad-
vance, but only the human judgments of the training
set were available for system development. In this
work, we treat the training set as the validation set
? all the model exploration and refinement is done
using this set of data, as well as the hyper-parameter
tuning when learning the final model combination.
The quality of a relational similarity measure is
estimated by its correlation to human judgments.
This is evaluated using two metrics in the task: the
MaxDiff accuracy and Spearman?s rank correlation
coefficient (?). A system is first asked to pick the
most and least representative word pairs of each
question in the MaxDiff setting. The average accu-
racy of the predictions compared to the human an-
swers is then reported. In contrast, Spearman?s ?
1002
measures the correlation between the total orderings
of all word pairs of a relation, where the total order-
ing is derived from the MaxDiff answers (see (Jur-
gens et al, 2012) for the exact procedure).
4 Models for Relational Similarity
We investigate three types of models for relational
similarity. Operating in a word vector space, the di-
rectional similarity model compares the vector dif-
ferences of target and prototypical word pairs to es-
timate their relational similarity. The lexical pat-
tern method collects contextual information of pairs
of words when they co-occur in large corpora, and
learns a highly regularized log-linear model. Finally,
the word relation models incorporate existing, spe-
cific word relation measures for general relational
similarity.
4.1 Directional Similarity Model
Our first model for relational similarity extends pre-
vious work on semantic word vector representa-
tions to a directional similarity model for pairs of
words. There are many different methods for cre-
ating real-valued semantic word vectors, such as
the distributed representation derived from a word
co-occurrence matrix and a low-rank approxima-
tion (Landauer et al, 1998), word clustering (Brown
et al, 1992) and neural-network language model-
ing (Bengio et al, 2003; Mikolov et al, 2010). Each
element in the vectors conceptually represents some
latent topicality information of the word. The goal
of these methods is that words with similar mean-
ings will tend to be close to each other in the vector
space.
Although the vector representation of single
words has been successfully applied to problems
like semantic word similarity and text classifica-
tion (Turian et al, 2010), the issue of how to repre-
sent and compare pairs of words in a vector space
remains unclear (Turney, 2012). In a companion
paper (Mikolov et al, 2013), we present a vector
offset method which performs consistently well in
identifying both syntactic and semantic regularities.
This method measures the degree of the analogy
?a is to b as c is to d? using the cosine score of
(~vb?~va +~vc, ~vd), where a, b, c, d are the four given
words and ~va, ~vb, ~vc, ~vd are the corresponding vec-
q 
shirt
clothing
furniture
desk
v1
v2'
v2'
Figure 1: Directional vectors ?1 and ?2 capture the rela-
tions of clothing:shirt and furniture:desk respectively in
this semantic vector space. The relational similarity of
these two word pairs is estimated by the cosine of ?.
tors. In this paper, we propose a variant called the
directional similarity model, which performs bet-
ter for semantic relations. Let ?i = (wi1 , wi2) and
?j = (wj1 , wj2) be the two word pairs being com-
pared. Suppose (~vi1 , ~vi2) and (~vj1 , ~vj2) are the cor-
responding vectors of these words. The directional
vectors of ?i and ?j are defined as ~?i ? ~vi2 ? ~vi1
and ~?j ? ~vj2 ? ~vj1 , respectively. Relational simi-
larity of these two word pairs can be measured by
some distance function of ?i and ?j , such as the co-
sine function:
~?i ? ~?j
?~?i??~?j?
The rationale behind this variant is as follows. Be-
cause the difference of two word vectors reveals the
change from one word to the other in terms of mul-
tiple topicality dimensions in the vector space, two
word pairs having similar offsets (i.e., being rela-
tively parallel) can be interpreted as they have simi-
lar relations. Fig. 1 further illustrates this method.
Compared to the original method, this variant
places less emphasis on the similarity between
words wj1 and wj2 . That similarity is necessary
for syntactic relations where the words are often re-
lated by morphology, but not for semantic relations.
On semantic relations studied in this paper, the di-
rectional similarity model performs about 18% rela-
tively better in Spearman?s ? than the original one.
The quality of the directional similarity method
depends heavily on the underlying word vector
space model. We compared two choices with dif-
1003
Word Embedding Spearman?s ? MaxDiff Acc. (%)
LSA-80 0.055 34.6
LSA-320 0.066 34.4
LSA-640 0.102 35.7
RNNLM-80 0.168 37.5
RNNLM-320 0.214 39.1
RNNLM-640 0.221 39.2
RNNLM-1600 0.234 41.2
Table 1: Results of measuring relational similarity using
the directional similarity method, evaluated on the train-
ing set. The 1600-dimensional RNNLM vector space
achieves the highest Spearman?s ? and MaxDiff accuracy.
ferent dimensionality settings: the word embedding
learned from the recurrent neural network language
model (RNNLM)2 and the LSA vectors, both were
trained using the same Broadcast News corpus of
320M words as described in (Mikolov et al, 2011).
All the word vectors were first normalized to unit
vectors before applying the directional similarity
method. Given a target word pair, we computed
its relational similarity compared with the prototyp-
ical word pairs of the same relation. The average
of these measurements was taken as the final model
score. Table 1 summarizes the results when evalu-
ated on the training set. As shown in the table, the
RNNLM vectors consistently outperform their LSA
counterparts with the same dimensionality. In addi-
tion, more dimensions seem to preserve more infor-
mation and lead to better performance. Therefore,
we take the 1600-dimensional RNNLM vectors to
construct our final directional similarity model.
4.2 Lexical Pattern Model
Our second model for measuring relational similar-
ity is built based on lexical patterns. It is well-known
that contexts in which two words co-occur often pro-
vide useful cues for identifying the word relation.
For example, having observed frequent text frag-
ments like ?X such as Y?, it is likely that there is a
Class-Inclusion:Taxonomic relation between X and
Y; namely, Y is a type of X. Indeed, by mining lexical
patterns from a large corpus, the UTD system (Rink
and Harabagiu, 2012) managed to outperform other
participants in the SemEval-2012 task of measuring
relational similarity.
2http://www.fit.vutbr.cz/?imikolov/rnnlm
In order to find more co-occurrences of each pair
of words, we used a large document set that con-
sists of the Gigaword corpus (Parker et al, 2009),
Wikipedia and LA Times articles3, summing up to
more than 20 Gigabytes of texts. For each word
pair (w1, w2) that co-occur in a sentence, we col-
lected the words in between as its context (or so-
called ?raw pattern?). For instance, ?such as? would
be the context extracted from ?X such as Y? for
the word pair (X, Y). To reduce noise, contexts with
more than 9 words were dropped and 914,295 pat-
terns were collected in total.
Treating each raw pattern as a feature where the
value is the logarithm of the occurrence count, we
then built a probabilistic classifier to determine the
association of the context and relation. For each re-
lation, we treated all its word pairs as positive ex-
amples and all the word pairs in other relations as
negative examples4. 79 classifiers were trained in
total, where each one was trained using 3,218 ex-
amples. The degree of relational similarity of each
word pair can then be judged by the output of the
corresponding classifier5. Although this seems like a
standard supervised learning setting, the large num-
ber of features poses a challenge here. Using almost
1M features and 3,218 examples, the model could
easily overfit if not regularized properly, which may
explain why learning SVMs on pattern features per-
formed poorly (Rink and Harabagiu, 2012). In-
stead of employing explicit feature selection meth-
ods, we used an efficient L1 regularized log-linear
model learner (Andrew and Gao, 2007) and chose
the hyper-parameters based on model performance
on the training data. The final models we chose
were trained with L1 = 3, where 28,065 features
in average were selected automatically by the algo-
3We used a Nov-2010 dump of English Wikipedia, which
contains approximately 917M words after pre-processing. The
LA Times corpus consists of articles from 1985 to 2002 and has
about 1.1B words.
4Given that not all word pairs belonging to the same relation
category are equally good, removing those with low judgment
scores may help improve the quality of the labeled data. We
leave this study to future work.
5Training a separate classifier for each MaxDiff question us-
ing all words pairs except the four target pairs appears to be a
better setting, as it would avoid including the target pairs in the
training process. We did not use this setting because it is more
complicated and performed roughly the same empirically.
1004
rithm. The performance on the training data is 0.322
in Spearman?s ? and 41.8% in MaxDiff accuracy.
4.3 Word Relation Models
The directional similarity and lexical pattern mod-
els can be viewed as general purpose methods for
relational similarity as they do not differentiate the
specific relation categories. In contrast, for specific
word relations, there exist several high-quality meth-
ods. Although they are designed for detecting spe-
cific relations between words, incorporating them
could still improve the overall results. Next, we ex-
plore the use of some of these word relation mod-
els, including information encoded in the knowledge
base and a lexical semantic model for synonymy and
antonymy.
4.3.1 Knowledge Bases
Predetermined types of relations can often be
found in existing lexical and knowledge databases,
such as WordNet?s Is-A taxonomy and the exten-
sive relations stored in the NELL (Carlson et al,
2010) knowledge base. Although in theory, these
resources can be directly used to solve the problem
of relational similarity, such direct approaches often
suffer from two practical issues. First, the word cov-
erage of these databases is usually very limited and
it is common that the relation of a given word pair
is absent. Second, the degree of relation is often not
included, which makes the task of measuring the de-
gree of relational similarity difficult.
One counter example, however, is Probase (Wu
et al, 2012), which is a knowledge base that es-
tablishes connections between more than 2.5 mil-
lion concepts discovered automatically from the
Web. For the Is-A and Attribute relations it en-
codes, Probase also returns the probability that two
input words share the relation, based on the co-
occurrence frequency. We used some relations in
the training set to evaluate the quality of Probase.
For instance, its Is-A model performs exception-
ally well on the relation Class-Inclusion:Taxonomic,
reaching a high Spearman?s ? = 0.642 and MaxD-
iff accuracy 55.8%. Similarly, its Attribute model
performs better than our lexical pattern model
on Attribute:Agent Attribute-State with Spearman?s
? = 0.290 and MaxDiff accuracy 32.7%.
4.3.2 Lexical Semantics Measures
Most lexical semantics measures focus on the se-
mantic similarity or relatedness of two words. Since
our task focuses on distinguishing the difference be-
tween word pairs in the same relation category. The
crude relatedness model does not seem to help in our
preliminary experimental study. Instead, we lever-
age the recently proposed polarity-inducing latent
semantic analysis (PILSA) model (Yih et al, 2012),
which specifically estimates the degree of synonymy
and antonymy. This method first forms a signed co-
occurrence matrix using synonyms and antonyms in
a thesaurus and then generalizes it using a low-rank
approximation derived by SVD. Given two words,
the cosine score of their PILSA vectors tend to be
negative if they are antonymous and positive if syn-
onymous. When tested on the Similar:Synonymity
relation, it has a Spearman?s ? = 0.242 and MaxD-
iff accuracy 42.1%, both are better than those of our
directional similarity and lexical pattern models.
5 Model Combination
In order to fully leverage the diverse models pro-
posed in Sec. 4, we experiment with a model combi-
nation approach and conduct a model ablation study.
Performance of the combined and individual models
is evaluated using the test set and compared with ex-
isting systems.
We seek an optimal linear combination of all the
individual models by treating their output as fea-
tures and use a logistic regression learner to learn
the weights6. The training setting is essentially the
same as the one used to learn the lexical pattern
model (Sec. 4.2). For each relation, we treat all the
word pairs in this relation group as positive exam-
ples and all other word pairs as negative ones. Con-
sequently, 79 sets of weights for model combination
are learned in total. The average Spearman?s ? of the
10 training relations is used for selecting the values
of the L1 and L2 regularizers7. Evaluated on the re-
maining 69 relations (i.e., the test set), the average
results of each main relation group and the overall
6Nonlinear methods, such as MART (Friedman, 2001), do
not perform better in our experiments (not reported here).
7We tested 15 combinations, where L1 ? {0, 0.01, 0.1} and
L2 ? {0, 0.001, 0.01, 1, 10}. The parameter setting that gave
the highest Spearman rank correlation coefficient score on the
training set was selected.
1005
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519
Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329
Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303
Contrast -0.049 0.000 0.142 0.206 0.063 0.298 -0.012 -0.032 -0.079 0.268
Attribute 0.037 -0.095 0.044 0.158 0.431 0.198 -0.008 0.016 -0.052 0.406
Non-Attribute -0.070 0.009 0.079 0.098 0.195 0.117 0.036 0.078 -0.093 0.296
Case Relations 0.090 -0.037 -0.011 0.241 0.503 0.288 0.076 -0.075 0.059 0.473
Cause-Purpose -0.011 0.114 0.021 0.183 0.362 0.234 0.044 -0.059 0.038 0.296
Space-Time 0.013 0.035 0.055 0.375 0.439 0.248 0.064 -0.002 -0.018 0.443
Reference 0.142 -0.001 0.028 0.346 0.301 0.119 0.033 -0.123 0.021 0.208
Average 0.018 0.014 0.050 0.229 0.324? 0.235 0.058? -0.010? -0.009? 0.353?
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 30.1 29.0 26.7 39.1 46.7 43.4 59.6 24.7 32.3 51.2
Part-Whole 31.9 35.1 29.4 40.9 43.9 38.1 31.3 29.5 31.0 42.9
Similar 31.5 29.1 37.1 39.8 38.5 38.4 30.8 36.3 34.2 43.3
Contrast 30.4 32.4 38.3 40.9 33.6 42.2 32.3 31.8 30.1 42.8
Attribute 30.2 29.2 31.9 36.5 47.9 38.3 30.7 31.0 28.8 48.3
Non-Attribute 28.9 30.4 36.0 36.8 38.7 36.7 32.3 32.8 27.7 42.6
Case Relations 32.8 29.5 28.2 40.6 54.3 42.2 32.8 25.7 31.0 50.6
Cause-Purpose 30.8 35.4 29.5 36.3 45.3 38.0 30.3 28.1 32.0 41.7
Space-Time 30.6 32.5 31.9 43.2 50.0 39.2 33.2 29.3 30.6 47.7
Reference 35.1 30.0 31.9 41.2 45.7 36.9 30.4 27.2 30.2 42.5
Average 31.2 31.7 32.4 39.4 44.5? 39.2 33.3? 29.8? 30.7? 45.2?
Table 2: Average Spearman?s ? (Top) and MaxDiff accuracy (%) (Bottom) of each major relation group and all 69
testing relations. The best result in each row is highlighted in boldface font. Statistical significance tests are conducted
by comparing each of our systems with the previous best performing system, UTDNB . ? and ? indicate the difference
in the average results is statistically significant with 95% or 99% confidence level, respectively.
results are presented in Table 2. For comparison, we
also show the performance of a random baseline and
the best performing system of each participant in the
SemEval-2012 task.
We draw two conclusions from this table. First,
both of our general relational similarity models, the
directional similarity (DS) and lexical pattern (Pat)
models are fairly strong. The former outperforms
the previous best system UTDNB in both Spear-
man?s ? and MaxDiff accuracy, where the differ-
ences are statistically significant8; the latter has
comparable performance, where the differences are
not statistically significant. In contrast, while the
IsA relation from Probase is exceptionally good
in identifying Class-Inclusion relations, with high
Spearman?s ? = 0.619 and MaxDiff accuracy
8We conducted a paired-t test on the results of each of the
69 relation. The difference is considered statistically significant
if the p-value is less than 0.05.
59.6%, it does not have high correlations with hu-
man judgments in other relations. Like in the case of
Probase Attribute and PILSA, specific word-relation
models individually are not good measures for gen-
eral relational similarity. Second, as expected, com-
bining multiple diverse models (Com) is a robust
strategy, which provides the best overall perfor-
mance. It achieves superior results in both evalua-
tion metrics compared to UTDNB and only a lower
Spearman?s ? value in one of the ten relation groups
(namely, Reference). The differences are statisti-
cally significant with p-value less than 10?3.
In order to understand the interaction among dif-
ferent component models, we conducted an ablation
study by iteratively removing one model from the fi-
nal combination. The weights are re-trained using
the same procedure that finds the best regularization
parameters with the help of training data. Table 3
summarizes the results and compares them with the
1006
Spearman?s ? MaxDiff Accuracy (%)
Relation Group Com. -Attr -IsA -PILSA -DS -Pat Com. -Attr -IsA -PILSA -DS -Pat
Class-Inclusion 0.519 0.557 0.467 0.593 0.490 0.570 51.2 53.7 49.2 54.6 49.3 56.2
Part-Whole 0.329 0.326 0.335 0.331 0.277 0.285 42.9 42.1 42.6 41.8 38.5 42.9
Similar 0.303 0.269 0.302 0.281 0.256 0.144 43.3 41.2 42.7 40.5 40.2 38.9
Contrast 0.268 0.234 0.267 0.289 0.260 0.156 42.8 42.0 42.4 41.5 42.7 38.1
Attribute 0.406 0.409 0.405 0.433 0.164 0.447 48.3 47.8 48.2 49.1 36.9 49.0
Non-Attribute 0.296 0.287 0.296 0.276 0.123 0.283 42.6 42.9 42.6 41.8 36.0 43.0
Case Relations 0.473 0.497 0.470 0.484 0.309 0.498 50.6 52.5 50.2 50.9 42.9 53.2
Cause-Purpose 0.296 0.282 0.299 0.301 0.205 0.296 41.7 41.6 41.6 41.2 36.6 44.1
Space-Time 0.443 0.425 0.443 0.420 0.269 0.431 47.7 47.2 47.7 46.9 40.5 49.5
Reference 0.208 0.238 0.205 0.168 0.102 0.210 42.5 42.3 42.6 41.8 36.1 41.4
Average 0.353 0.348 0.350 0.354 0.238? 0.329 45.2 45.0 44.9? 44.7 39.6? 45.4
Table 3: Average Spearman?s ? and MaxDiff accuracy results of different model combinations. Com indicates combin-
ing all models, where other columns show the results when the specified model is removed. The best result in each row
is highlighted in boldface font. Statistical significance tests are conducted by comparing each ablation configuration
with Com. ? indicates the difference in the average results is statistically significant with 99% confidence level.
original combination model.
Overall, it is clear that the directional similarity
method based on RNNLM vectors is the most crit-
ical component model. Removing it from the fi-
nal combination decreases both the Spearman?s ?
and MaxDiff accuracy by a large margin; both dif-
ferences (Com vs. -DS) are statistically significant.
The Probase IsA model also has an important im-
pact on the performance on the Class-Inclusion re-
lation group. Eliminating the IsA model makes
the overall MaxDiff accuracy statistically signifi-
cantly lower (Com vs. -IsA). Again, the benefits
of incorporating Probase Attribute and PILSA mod-
els are not clear. Removing them from the final
combination lowers the MaxDiff accuracy, but nei-
ther the difference in Spearman?s ? nor MaxDiff
accuracy is statistically significant. Compared to
the RNNLM directional similarity model, the lex-
ical pattern model seems less critical. Removing
it lowers the Similar and Contrast relation groups,
but improves some other relation groups like Class-
Inclusion and Case Relations. The final MaxDiff ac-
curacy becomes slightly higher but the Spearman?s
? drops a little (Com vs. -Pat); neither is statistically
significant.
Notice that the main purpose of the ablation study
is to verify the importance of an individual compo-
nent model when a significant performance drop is
observed after removing it. However, occasionally
the overall performance may go up slightly. Typi-
cally this is due to the fact that some models do not
provide useful signals to a particular relation, but in-
stead introduce more noise. Such effects can often
be alleviated when there are enough quality training
data, which is unfortunately not the case here.
6 Conclusions
In this paper, we presented a system that combines
heterogeneous models based on different informa-
tion sources for measuring relational similarity. Our
two individual general-purpose relational similarity
models, directional similarity and lexical pattern
methods, perform strongly when compared to ex-
isting systems. After incorporating specific word-
relation models, the final system sets a new state-of-
the-art on the SemEval-2012 task 2 test set, achiev-
ing Spearman?s ? = 0.353 and MaxDiff accuracy
45.4% ? resulting in 54.1% and 14.7% relative im-
provement in these two metrics, respectively.
Despite its simplicity, our directional similarity
approach provides a robust model for relational sim-
ilarity and is a critical component in the final sys-
tem. When the lexical pattern model is included, our
overall model combination method can be viewed
as a two-stage learning system. As demonstrated in
our work, with an appropriate regularization strat-
egy, high-quality models can be learned in both
stages. Finally, as we observe from the positive ef-
fect of adding the Probase IsA model, specific word-
relation models can further help improve the system
1007
although they tend to cover only a small number of
relations. Incorporating more such models could be
a steady path to enhance the final system.
In the future, we plan to pursue several research
directions. First, as shown in our experimental re-
sults, the model combination approach does not al-
ways outperform individual models. Investigating
how to select models to combine for each specific re-
lation or relation group individually will be our next
step for improving this work. Second, because the
labeling process of relational similarity comparisons
is inherently noisy, it is unrealistic to request a sys-
tem to correlate human judgments perfectly. Con-
ducting some user study to estimate the performance
ceiling in each relation category may help us focus
on the weaknesses of the final system to enhance
it. Third, it is intriguing to see that the directional
similarity model based on the RNNLM vectors per-
forms strongly, even though the RNNLM training
process is not related to the task of relational sim-
ilarity. Investigating the effects of different vector
space models and proposing some theoretical jus-
tifications are certainly interesting research topics.
Finally, we would like to evaluate the utility our ap-
proach in other applications, such as the SAT anal-
ogy problems proposed by Turney (2006) and ques-
tion answering.
Acknowledgments
We thank Richard Socher for valuable discussions,
Misha Bilenko for his technical advice and anony-
mous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML ?07.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cog-
nitive and psychometric analysis of analogical prob-
lem solving. Recent research in psychology. Springer-
Verlag.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13?47, March.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
J.H. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist, 29(5):1189?
1232.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
degrees of relational similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 356?364, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH, pages 1045?1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for train-
ing large scale neural network language models. In
ASRU.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013.
Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth edi-
tion. Technical report, Linguistic Data Consortium,
Philadelphia.
Ted Pedersen. 2012. Duluth: Measuring degrees of re-
lational similarity with the gloss vector measure of se-
mantic relatedness. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
1008
2012), pages 497?501, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ?10.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP-01, pages 82?90.
Mireya Tovar, J. Alejandro Reyes, Azucena Montes,
Darnes Vilarin?o, David Pinto, and Saul Leo?n. 2012.
BUAP: A first approximation to relational similarity
measuring. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 502?505, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research
(JAIR), 44:533?585.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782?788.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.
Zhu. 2012. Probase: a probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of
Data, pages 481?492, May.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
1009
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1744?1753,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Question Answering Using Enhanced Lexical Semantic Models
Wen-tau Yih Ming-Wei Chang Christopher Meek Andrzej Pastusiak
Microsoft Research
Redmond, WA 98052, USA
{scottyih,minchang,meek,andrzejp}@microsoft.com
Abstract
In this paper, we study the answer
sentence selection problem for ques-
tion answering. Unlike previous work,
which primarily leverages syntactic analy-
sis through dependency tree matching, we
focus on improving the performance us-
ing models of lexical semantic resources.
Experiments show that our systems can
be consistently and significantly improved
with rich lexical semantic information, re-
gardless of the choice of learning algo-
rithms. When evaluated on a bench-
mark dataset, the MAP and MRR scores
are increased by 8 to 10 points, com-
pared to one of our baseline systems using
only surface-form matching. Moreover,
our best system also outperforms pervious
work that makes use of the dependency
tree structure by a wide margin.
1 Introduction
Open-domain question answering (QA), which
fulfills a user?s information need by outputting di-
rect answers to natural language queries, is a chal-
lenging but important problem (Etzioni, 2011).
State-of-the-art QA systems often implement a
complicated pipeline architecture, consisting of
question analysis, document or passage retrieval,
answer selection and verification (Ferrucci, 2012;
Moldovan et al, 2003). In this paper, we focus
on one of the key subtasks ? answer sentence se-
lection. Given a question and a set of candidate
sentences, the task is to choose the correct sen-
tence that contains the exact answer and can suf-
ficiently support the answer choice. For instance,
although both of the following sentences contain
the answer ?Jack Lemmon? to the question ?Who
won the best actor Oscar in 1973?? only the first
sentence is correct.
A1: Jack Lemmon won the Academy Award for
Best Actor for Save the Tiger (1973).
A2: Oscar winner Kevin Spacey said that Jack
Lemmon is remembered as always making
time for other people.
One of the benefits of answer sentence selec-
tion is that the output can be provided directly to
the user. Instead of outputting only the answer, re-
turning the whole sentence often adds more value
as the user can easily verify the correctness with-
out reading a lengthy document.
Answer sentence selection can be naturally re-
duced to a semantic text matching problem. Con-
ceptually, we would like to measure how close
the question and sentence can be matched seman-
tically. Due to the variety of word choices and
inherent ambiguities in natural languages, bag-of-
words approaches with simple surface-form word
matching tend to produce brittle results with poor
prediction accuracy (Bilotti et al, 2007). As a
result, researchers put more emphasis on exploit-
ing both the syntactic and semantic structure in
questions/sentences. Representative examples in-
clude methods based on deeper semantic anal-
ysis (Shen and Lapata, 2007; Moldovan et al,
2007) and on tree edit-distance (Punyakanok et
al., 2004; Heilman and Smith, 2010) and quasi-
synchronous grammar (Wang et al, 2007) that
match the dependency parse trees of questions and
sentences. However, such approaches often re-
quire more computational resources. In addition
to applying a syntactic or semantic parser during
run-time, finding the best matching between struc-
tured representations of sentences is not trivial.
For example, the computational complexity of tree
matching is O(V 2L4), where V is the number of
nodes and L is the maximum depth (Tai, 1979).
Instead of focusing on the high-level seman-
tic representation, we turn our attention in this
work to improving the shallow semantic compo-
1744
nent, lexical semantics. We formulate answer se-
lection as a semantic matching problem with a la-
tent word-alignment structure as in (Chang et al,
2010) and conduct a series of experimental stud-
ies on leveraging recently proposed lexical seman-
tic models. Our main contributions in this work
are two key findings. First, by incorporating the
abundant information from a variety of lexical se-
mantic models, the answer selection system can
be enhanced substantially, regardless of the choice
of learning algorithms and settings. Compared to
the previous work, our latent alignment model im-
proves the result on a benchmark dataset by a wide
margin ? the mean average precision (MAP) and
mean reciprocal rank (MRR) scores are increased
by 25.6% and 18.8%, respectively. Second, while
the latent alignment model performs better than
unstructured models, the difference diminishes af-
ter adding the enhanced lexical semantics infor-
mation. This may suggest that compared to in-
troducing complex structured constraints, incorpo-
rating shallow semantic information is both more
effective and computationally inexpensive in im-
proving the performance, at least for the specific
word alignment model tested in this work.
The rest of the paper is structured as follows.
We first survey the related work in Sec. 2. Sec. 3
defines the problem of answer sentence selection,
along with the high-level description of our solu-
tion. The enhanced lexical semantic models and
the learning frameworks we explore are presented
in Sec. 4 and Sec. 5, respectively. Our experimen-
tal results on a benchmark QA dataset is shown in
Sec. 6. Finally, Sec. 7 concludes the paper.
2 Related Work
While the task of question answering has a long
history dated back to the dawn of artificial in-
telligence, early systems like STUDENT (Wino-
grad, 1977) and LUNAR (Woods, 1973) are typ-
ically designed to demonstrate natural language
understanding for a small and specific domain.
The Text REtrieval Conference (TREC) Question
Answering Track was arguably the first large-
scale evaluation of open-domain question answer-
ing (Voorhees and Tice, 2000). The task is de-
signed in an information retrieval oriented setting.
Given a factoid question along with a collection
of documents, a system is required to return the
exact answer, along with the document that sup-
ports the answer. In contrast, the Jeopardy! TV
quiz show provides another open-domain question
answering setting, in which IBM?s Watson system
famously beat the two highest ranked players (Fer-
rucci, 2012). Questions in this game are presented
in a statement form and the system needs to iden-
tify the true question and to give the exact answer.
A short sentence or paragraph to justify the answer
is not required in either TREC-QA or Jeopardy!
As any QA system can virtually be decomposed
into two major high-level components, retrieval
and selection (Echihabi and Marcu, 2003), the an-
swer selection problem is clearly critical. Limiting
the scope of an answer to a sentence is first high-
lighted by Wang et al (2007), who argued that it
was more informative to present the whole sen-
tence instead of a short answer to users.
Observing the limitations of the bag-of-words
models, Wang et al (2007) proposed a syntax-
driven approach, where each pair of question and
sentence are matched by their dependency trees.
The mapping is learned by a generative probabilis-
tic model based on a Quasi-synchronous Gram-
mar formulation (Smith and Eisner, 2006). This
approach was later improved by Wang and Man-
ning (2010) with a tree-edit CRF model that learns
the latent alignment structure. In contrast, gen-
eral tree matching methods based on tree-edit dis-
tance have been first proposed by Punyakanok et
al. (2004) for a similar answer selection task. Heil-
man and Smith (2010) proposed a discriminative
approach that first computes a tree kernel func-
tion between the dependency trees of the question
and candidate sentence, and then learns a classifier
based on the tree-edit features extracted.
Although lexical semantic information derived
from WordNet has been used in some of these
approaches, the research has mainly focused
on modeling the mapping between the syntac-
tic structures of questions and sentences, pro-
duced from syntactic analysis. The potential im-
provement from enhanced lexical semantic mod-
els seems to have been deliberately overlooked.1
3 Problem Definition
We consider the answer selection problem in a
supervised learning setting. For a set of ques-
tions {q1, ? ? ? , qm}, each question qi is associated
with a list of labeled candidate answer sentences
1For example, Heilman and Smith (2010) emphasized that
?The tree edit model, which does not use lexical semantics
knowledge, produced the best result reported to date.?
1745
What is the fastest car in the world?
The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet. 
Figure 1: An example pair of question and answer sentence, adapted from (Harabagiu and Moldovan,
2001). Words connected by solid lines are clear synonyms or hyponym/hypernym; words with weaker
semantic association are linked by dashed lines.
{(yi1 , si1), (yi1 , si2), ? ? ? , (yin , sin)}, where yij =
1 indicates that sentence sij is a correct answer to
question qi, and 0 otherwise. Using this labeled
data, our goal is to learn a probabilistic classifier
to predict the label of a new, unseen pair of ques-
tion and sentence.
Fundamentally, what the classifier predicts is
whether the sentence ?matches? the question se-
mantically. In other words, does s have the an-
swer that satisfies the semantic constraints pro-
vided in the question? Without representing the
question and sentence in logic or syntactic trees,
we take a word-alignment view for solving this
problem. We assume that there is an underly-
ing structure h that describes how q and s can
be associated through the relations of the words
in them. Figure 1 illustrates this setting using a
revised example from (Harabagiu and Moldovan,
2001). In this figure, words connected by solid
lines are clear synonyms or hyponym/hypernym;
words connected by dashed lines indicate that they
are weakly related. With this alignment structure,
features like the degree of mapping or whether all
the content words in the question can be mapped
to some words in the sentence can be extracted and
help improve the classifier. Notice that the struc-
ture representation in terms of word-alignment is
fairly general. For instance, if we assume a naive
complete bipartite matching, then effectively it re-
duces to the simple bag-of-words model.
Typically, the ?ideal? alignment structure is not
available in the data, and previous work exploited
mostly syntactic analysis (e.g., dependency trees)
to reveal the latent mapping structure. In this
work, we focus our study on leveraging the low-
level semantic cues from recently proposed lexical
semantic models. As will be shown in our experi-
ments, such information not only improves a latent
structure learning method, but also makes a simple
bipartite matching approach extremely strong.2
4 Lexical Semantic Models
In this section, we introduce the lexical seman-
tic models we adopt for solving the semantic
matching problem in answer selection. To go be-
yond the simple, limited surface-form matching,
we aim to pair words that are semantically re-
lated, specifically measured by models of word
relations including synonymy/antonymy, hyper-
nymy/hyponymy (the Is-A relation) and general se-
mantic word similarity.
4.1 Synonymy and Antonymy
Among all the word relations, synonymy is per-
haps the most basic one and needs to be handled
reliably. Although sets of synonyms can be eas-
ily found in thesauri or WordNet synsets, such
resources typically cover only strict synonyms.
When comparing two words, it is more useful to
estimate the degree of synonymy as well. For in-
stance, ship and boat are not strict synonyms be-
cause a ship is usually viewed as a large boat.
Knowing that two words are somewhat synony-
mous could be valuable in determining whether
they should be mapped.
In order to estimate the degree of synonymy, we
leverage a recently proposed polarity-inducing la-
tent semantic analysis (PILSA) model (Yih et al,
2012). Given a thesaurus, the model first con-
structs a signed d-by-n co-occurrence matrix W ,
where d is the number of word groups and n is
the size of the vocabulary. Each row consists of a
2Proposed by an anonymous reviewer, one justification of
this word-alignment approach, where syntactic analysis plays
a less important role, is that there are often few sensible com-
binations of words. For instance, knowing only the set of
words {?car?, ?fastest?, ?world?}, one may still guess cor-
rectly the question ?What is the fastest car in the world??
1746
group of synonyms and antonyms of a particular
sense and each column represents a unique word.
Values of the elements in each row vector are the
TFIDF values of the corresponding words in this
group. The notion of polarity is then induced by
making the values of words in the antonym groups
negative, and the matrix is generalized by a low-
rank approximation derived by singular-value de-
composition (SVD) in the end. This design has an
intriguing property ? if the cosine score of two col-
umn vectors are positive, then the two correspond-
ing words tend to be synonymous; if it?s negative,
then the two words are antonymous. The degree is
measured by the absolute value.
Following the setting described in (Yih et al,
2012), we construct a PILSA model based on the
Encarta thesaurus and enhance it with a discrimi-
native projection matrix training method. The es-
timated degrees of both synonymy and antonymy
are used our experiments.3
4.2 Hypernymy and Hyponymy
The Class-Inclusion or Is-A relation is commonly
observed between words in questions and answer
sentences. For example, to correctly answer the
question ?What color is Saturn??, it is crucial that
the selected sentence mentions a specific kind of
color, as in ?Saturn is a giant gas planet with
brown and beige clouds.? Another example is
?Who wrote Moonlight Sonata??, where compose
in ?Ludwig van Beethoven composed the Moon-
light Sonata in 1801.? is one kind of write.
Traditionally, WordNet taxonomy is the linguis-
tic resource for identifying hypernyms and hy-
ponyms, applied broadly to many NLP problems.
However, WordNet has a number of well-known
limitations including its rather limited or skewed
concept distribution and the lack of the coverage
of the Is-A relation (Song et al, 2011). For in-
stance, when a word refers to a named entity, the
particular sense and meaning is often not encoded.
As a result, relations such as ?Apple? is-a ?com-
pany? and ?Jaguar? is-a ?car? cannot be found in
WordNet. Similar to the case in synonymy, the
Is-A relation defined in WordNet does not provide
a native, real-valued degree of the relation, which
can only be roughly approximated using the num-
ber of links on the taxonomy path connecting two
3Mapping two antonyms may be desired if one of them is
in the scope of negation (Morante and Blanco, 2012; Blanco
and Moldovan, 2011). However, we do not attempt to resolve
the negation scope in this work.
concepts (Resnik, 1995).
In order to remedy these issues, we aug-
ment WordNet with the Is-A relations found in
Probase (Wu et al, 2012). Probase is a knowledge
base that establishes connections between 2.7 mil-
lion concepts, discovered automatically by apply-
ing Hearst patterns (Hearst, 1992) to 1.68 billion
Web pages. Its abundant concept coverage dis-
tinguishes it from other knowledge bases, such as
Freebase (Bollacker et al, 2008) and WikiTaxon-
omy (Ponzetto and Strube, 2007). Based on the
frequency of term co-occurrences, each Is-A rela-
tion from Probase is associated with a probability
value, indicating the degree of the relation.
We verified the quality of Probase Is-A relations
using a recently proposed SemEval task of rela-
tional similarity (Jurgens et al, 2012) in a com-
panion paper (Zhila et al, 2013), where a subset
of the data is to measure the degree of two words
having a class-inclusion relation. Probase?s pre-
diction correlates well with the human annotations
and achieves a high Spearman?s rank correlation
coefficient score, ? = 0.619. In comparison, the
previous best system (Rink and Harabagiu, 2012)
in the task only reaches ? = 0.233. These appeal-
ing qualities make Probase a robust lexical seman-
tic model for hypernymy/hyponymy.
4.3 Semantic Word Similarity
The third lexical semantic model we introduce tar-
gets a general notion of word similarity. Unlike
synonymy and hyponymy, word similarity is only
loosely defined when two words can be associated
by some implicit relation.4 The general word sim-
ilarity model can be viewed as a ?back-off? so-
lution when the exact lexical relation (e.g., part-
whole and attribute) is not available or cannot be
accurately detected.
Among various word similarity models (Agirre
et al, 2009; Reisinger and Mooney, 2010;
Gabrilovich and Markovitch, 2007; Radinsky et
al., 2011), the vector space models (VSMs) based
on the idea of distributional similarity (Turney
and Pantel, 2010) are often used as the core com-
ponent. Inspired by (Yih and Qazvinian, 2012),
which argues the importance of incorporating het-
erogeneous vector space models for measuring
word similarity, we leverage three different VSMs
in this work: Wiki term-vectors, recurrent neural
4Instead of making the distinction, word similarity here
refers to the larger set of relations commonly covered by word
relatedness (Budanitsky and Hirst, 2006).
1747
network language model (RNNLM) and a concept
vector space model learned from click-through
data. Semantic word similarity is estimated using
the cosine score of the corresponding word vectors
in these VSMs.
Contextual term-vectors created using the
Wikipedia corpus have shown to perform well
on measuring word similarity (Reisinger and
Mooney, 2010). Following the setting suggested
by Yih and Qazvinian (2012), we create term-
vectors representing about 1 million words by ag-
gregating terms within a window of [?10, 10] of
each occurrence of the target word. The vectors
are further refined by applying the same vocabu-
lary and feature pruning techniques.
A recurrent neural network language
model (Mikolov et al, 2010) aims to esti-
mate the probability of observing a word given its
preceding context. However, one by-product of
this model is the word embedding learned in its
hidden-layer, which can be viewed as capturing
the word meaning in some latent, conceptual
space. As a result, vectors of related words tend
to be close to each other. For this word similarity
model, we take a 640-dimensional version of
RNNLM vectors, which is trained using the
Broadcast News corpus of 320M words.5
The final word relatedness model is a projec-
tion model learned from the click-through data of
a commercial search engine (Gao et al, 2011).
Unlike the previous two models, which are cre-
ated or trained using a text corpus, the input for
this model is pairs of aggregated queries and ti-
tles of pages users click. This parallel data is
used to train a projection matrix for creating the
mapping between words in queries and documents
based on user feedback, using a Siamese neural
network (Yih et al, 2011). Each row vector of
this matrix is the dense vector representation of
the corresponding word in the vocabulary. Perhaps
due to its unique information source, we found this
particular word embedding seems to complement
the other two VSMs and tends to improve the word
similarity measure in general.
5 Learning QA Matching Models
In this section, we investigate the effectiveness of
various learning models for matching questions
and sentences, including the bag-of-words setting
5http://www.fit.vutbr.cz/?imikolov/
rnnlm/
and the framework of learning latent structures.
5.1 Bag-of-Words Model
The bag-of-words model treats each question and
sentence as an unstructured bag of words. When
comparing a question with a sentence, the model
first matches each word in the question to each
word in the sentence. It then aggregates features
extracted from each of these word pairs to rep-
resent the whole question/sentence pair. A bi-
nary classifier can be trained easily using any ma-
chine learning algorithm in this standard super-
vised learning setting.
Formally, let x = (q, s) be a pair of question q
and sentence s. Let Vq = {wq1 , wq2 , ? ? ? , wqm}
and Vs = {ws1 , ws2 , ? ? ? , wsn} be the sets of
words in q and s, respectively. Given a word pair
(wq, ws), where wq ? Vq and ws ? Vs, feature
functions ?1, ? ? ? , ?d map it to a d-dimensional
real-valued feature vector.
We consider two aggregate functions for defin-
ing the feature vectors of the whole ques-
tion/answer pair: average and max.
?avgj (q, s) =
1
mn
?
wq?Vq
ws?Vs
?j(wq, ws) (1)
?maxj (q, s) = maxwq?Vq
ws?Vs
?j(wq, ws) (2)
Together, each question/sentence pair is repre-
sented by a 2d-dimensional feature vector.
We tested two learning algorithms in this set-
ting: logistic regression and boosted decision
trees (Friedman, 2001). The former is the log-
linear model widely used in the NLP community
and the latter is a robust non-linear learning algo-
rithm that has shown great empirical performance.
The bag-of-words model does not require an ad-
ditional inference stage as in structured learning,
which may be computationally expensive. Nev-
ertheless, its lack of structure information could
limit the expressiveness of the model and make it
difficult to capture more sophisticated semantics
in the sentences. To address this concern, we in-
vestigate models of learning latent structures next.
5.2 Learning Latent Structures
One obvious issue of the bag-of-words model is
that words in the unrelated part of the sentence
may still be paired with words in the question,
which introduces noise to the final feature vector.
1748
This is observed in many question/sentence pairs,
such as the one below.
Q: Which was the first movie that James Dean
was in?
A: James Dean, who began as an actor on TV
dramas, didn?t make his screen debut until
1951?s ?Fixed Bayonet.?
While this sentence correctly answers the ques-
tion, the fact that James Dean began as a TV
actor is unrelated to the question. As a result,
an ?ideal? word alignment structure should not
link words in this clause to those in the ques-
tion. In order to leverage the latent structured in-
formation, we adapt a recently proposed frame-
work of learning constrained latent representa-
tions (LCLR) (Chang et al, 2010). LCLR can be
viewed as a variant of Latent-SVM (Felzenszwalb
et al, 2009) with different learning formulations
and a general inference framework. The idea of
LCLR is to replace the decision function of a stan-
dard linear model ?T?(x) with
arg max
h
?T?(x, h), (3)
where ? represents the weight vector and h repre-
sents the latent variables.
In this answer selection task, x = (q, s) rep-
resents a pair of question q and candidate sen-
tence s. As described in Sec. 3, h refers to the
latent alignment between q and s. The intuition
behinds Eq. (3) is: candidate sentence s correctly
answers question q if and only if the decision can
be supported by the best alignment h.
The objective function of LCLR is defined as:
min?
1
2 ||?||
2 + C
?
i
?2i
s.t. ?i ? 1? yi maxh ?
T?(x, h)
Note that the alignment is latent, so LCLR uses
the binary labels in the training data as feedback
to find the alignment for each example.
The computational difficulty of the inference
problem (Eq. (3)) largely depends on the con-
straints we enforce in the alignment. Complicated
constraints may result in a difficult inference prob-
lem, which can be solved by integer linear pro-
gramming (Roth and Yih, 2007). In this work,
we considered several sets of constraints for the
alignment task, including a two-layer phrase/word
alignment structure, but found that they generally
performed the same. Therefore, we chose the
many-to-one alignment6, where inference can be
solved exactly using a simple greedy algorithm.
6 Experiments
We present our experimental results in this sec-
tion by first introducing the data and evaluation
metrics, followed by the results of existing sys-
tems and some baseline methods. We then show
the positive impact of adding information of word
relations from various lexical semantics models,
with some discussion on the limitation of the
word-matching approach.
6.1 Data & Evaluation Metrics
The answer selection dataset we used was orig-
inally created by Wang et al (2007) based on
the QA track of past Text REtrieval Confer-
ences (TREC-QA). Questions in this dataset are
short factoid questions, such as ?What is Crips?
gang color?? In average, each question is associ-
ated with approximately 33 answer candidate sen-
tences. A pair of question and sentence is judged
positive if the sentence contains the exact answer
key and can provide sufficient context as support-
ing evidence.
The training set of the data contains manu-
ally labeled 5,919 question/sentence pairs from
TREC 8-12. The development and testing sets
are both from TREC 13, which contain 1,374
and 1,866 pairs, respectively. The task is treated as
a sentence ranking problem for each question and
thus evaluated in Mean Average Precision (MAP)
and Mean Reciprocal Rank (MRR), using the offi-
cial TREC evaluation program. Following (Wang
et al, 2007), candidate sentences with more than
40 words are removed from evaluation, as well as
questions with only positive or negative candidate
sentences.
6.2 Baseline Methods
Several systems have been proposed and tested
using this dataset. Wang et al (2007) pre-
sented a generative probabilistic model based on
a Quasi-synchronous Grammar formulation and
was later improved by Wang and Manning (2010)
with a tree-edit CRF model that learns the la-
tent alignment structure. In contrast, Heilman and
6Each word in the question needs to be linked to a word
in the sentence. Each word in the sentence can be linked to
zero or multiple words in the question.
1749
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Wang and Manning (2010) 0.5951 0.6951
Table 1: Test set results of existing methods, taken
from Table 3 of (Wang and Manning, 2010).
Dev Test
Baseline MAP MRR MAP MRR
Random 0.5243 0.5816 0.4708 0.5286
Word Cnt 0.6516 0.7216 0.6263 0.6822
Wgt Word Cnt 0.7112 0.7880 0.6531 0.7071
Table 2: Results of three baseline methods.
Smith (2010) proposed a discriminative approach
that first computes a tree kernel function between
the dependency trees of the question and candidate
sentence, and then learns a classifier based on the
tree-edit features extracted. Table 1 summarizes
their results on the test set. All these systems in-
corporated lexical semantics features derived from
WordNet and named entity features.
In order to further estimate the difficulty of
this task and dataset, we tested three simple base-
lines. The first is random scoring, which sim-
ply assigns a random score to each candidate sen-
tence. The second one, word count, is to count
how many words in the question that also occur in
the answer sentence, after removing stopwords7,
and lowering the case. Finally, the last base-
line method, weighted word count, is basically the
same as identical word matching, but the count is
re-weighted using the IDF value of the question
word. This is similar to the BM25 ranking func-
tion (Robertson et al, 1995). The results of these
three methods are shown in Table 1.
Somewhat surprisingly, we find that word count
is fairly strong and performs comparably to previ-
ous systems.8 In addition, weighting the question
words with their IDF values further improves the
results.
6.3 Incorporating Rich Lexical Semantics
We test the effectiveness of adding rich lexical
semantics information by creating examples of
different feature sets. As described in Sec. 5,
7We used a list of 101 stopwords, including articles, pro-
nouns and punctuation.
8The finding has been confirmed by the lead author
of (Wang et al, 2007).
all the features are based on the properties of
the pair of a word from the question and a
word from the candidate sentence. Stopwords
are first removed from both questions and sen-
tences and all words are lower-cased. Features
used in the experiments can be categorized into
six types: identical word matching (I), lemma
matching (L), WordNet (WN), enhanced Lexi-
cal Semantics (LS), Named Entity matching (NE)
and Answer type checking (Ans). Inspired by
the weighted word count baseline, all features ex-
cept (Ans) are weighted by the IDF value of the
question word. In other words, the IDF values help
decide the importance of word pairs to the model.
Staring from the our baseline model, weighted
word count, the identical word matching (I) fea-
ture checks whether the pair of words are the
same. Instead of checking the surface form of
the word, lemma matching (L) verifies whether
the two words have the same lemma form. Ar-
guably the most common source of word rela-
tions, WordNet (WN) provides the primitive fea-
tures of whether two words could belong to the
same synset in WordNet, could be antonyms and
whether one is a hypernym of the other. Alter-
natively, the enhanced lexical semantics (LS) fea-
tures apply the models described in Sec. 4 to the
word pair and use their estimated degree of syn-
onymy, antonymy, hyponymy and semantic relat-
edness as features. Named entity matching (NE)
checks whether two words are individually part
of some named entities with the same type. Fi-
nally, when the question word is the WH-word, we
check if the paired word belongs to some phrase
that has the correct answer type using simple rules,
such as ?Who should link to a word that is part of
a named entity of type Person.? We created exam-
ples in each round of experiments by augmenting
these features in the same order, and observed how
adding different information helped improve the
model performance.
Three models are included in our study. For
the unstructured, bag-of-words setting, we tested
logistic regression (LR) and boosted decision
trees (BDT). As mentioned in Sec. 5, the features
for the whole question/sentence pair are the aver-
age and max of features of all the word pairs. For
the structured-output setting, we used the frame-
work of learning constrained latent representa-
tion (LCLR) and required that each question word
needed to be mapped to a word in the sentence.
1750
LR BDT LCLR
Feature set MAP MRR MAP MRR MAP MRR
1: I 0.6531 0.7071 0.6323 0.6898 0.6629 0.7279
2: I+L 0.6744 0.7223 0.6496 0.6923 0.6815 0.7270
3: I+L+WN 0.7039 0.7705 0.6798 0.7450 0.7316 0.7921
4: I+L+WN+LS 0.7339 0.8107 0.7523 0.8455 0.7626 0.8231
5: All 0.7374 0.8171 0.7495 0.8450 0.7648 0.8255
Table 3: Test results of various models and feature groups. Logistic regression (LR) and boosted decision
trees (BDT) are the two unstructured models. LCLR is the algorithm for learning latent structures.
Feature groups are identical word matching (I), lemma matching (L), WordNet (WN) and enhanced
Lexical Semantics (LS). All includes these four plus Named Entity matching (NE) and Answer type
checking (Ans).
Hyper-parameters are selected using the ones that
achieve the best MAP score on the development
set. Results of these models and feature sets are
presented in Table 3.
We make two observations from the results.
First, while incorporating more information of the
word pairs in general helps, it is clear that map-
ping words beyond surface-form matching with
the help of WordNet (Line #3 vs. #2) is impor-
tant. Moreover, when richer information from
other lexical semantic models is available, the per-
formance can be further improved (Line #4 vs.
#3). Overall, by simply incorporating more in-
formation on word relations, we gain approxi-
mately 10 points in both MAP and MRR com-
pared to surface-form matching (Line #4 vs. #2),
consistently across all three models. However,
adding more information like named entity match-
ing and answer type verification does not seem to
help much (Line #5 vs. #4). Second, while the
structured-output model usually performs better
than both unstructured models (LCLR vs. LR &
BDT), the performance gain diminishes after more
information of word pairs is available (e.g., Lines
#4 and #5).
6.4 Limitation of Word Matching Models
Although we have demonstrated the benefits of
leveraging various lexical semantic models to help
find the association between words, the problem of
question answering is nevertheless far from solved
using the word-based approach. Examining the
output of the LCLR model with all features on the
development set, we found that there were three
main sources of errors, including uncovered or in-
accurate entity relations, the lack of robust ques-
tion analysis and the need of high-level semantic
representation and inference. While the first two
can be improved by, say, using a better named en-
tity tagger, incorporating other knowledge bases
and building a question classifier, how to solve the
third problem is tricky. Below is an example:
Q: In what film is Gordon Gekko the main char-
acter?
A: He received a best actor Oscar in 1987 for his
role as Gordon Gekko in ?Wall Street?.
This is a correct answer sentence because ?win-
ning a best actor Oscar? implies that the role Gor-
don Gekko is the main character. It is hard to be-
lieve that a pure word-matching model would be
able to solve this type of ?inferential question an-
swering? problem.
7 Conclusions
In this paper, we present an experimental study
on solving the answer selection problem using en-
hanced lexical semantic models. Following the
word-alignment paradigm, we find that the rich
lexical semantic information improves the models
consistently in the unstructured bag-of-words set-
ting and also in the framework of learning latent
structures. Another interesting finding we have
is that while the latent structured model, LCLR,
performs better than the other two unstructured
models, the difference diminishes after more in-
formation, including the enhanced lexical seman-
tic knowledge and answer type verification, has
been incorporated. This may suggest that adding
shallow semantic information is more effective
than introducing complex structured constraints,
at least for the specific word alignment model we
experimented with in this work.
1751
In the future, we plan to explore several di-
rections. First, although we focus on improv-
ing TREC-style open-domain question answering
in this work, we would like to apply the pro-
posed technology to other QA scenarios, such
as community-based QA (CQA). For instance,
the sentence matching technique can help map a
given question to some questions in an existing
CQA database (e.g., Yahoo! Answers). More-
over, the answer sentence selection scheme could
also be useful in extracting the most related sen-
tences from the answer text to form a summary
answer. Second, because the task of answer sen-
tence selection is very similar to paraphrase de-
tection (Dolan et al, 2004) and recognizing tex-
tual entailment (Dagan et al, 2006), we would like
to investigate whether systems for these tasks can
be improved by incorporating enhanced lexical se-
mantic knowledge as well. Finally, we would like
to improve our system for the answer sentence se-
lection task and for question answering in general.
In addition to following the directions suggested
by the error analysis presented in Sec. 6.4, we plan
to use logic-like semantic representations of ques-
tions and sentences, and explore the role of lexical
semantics for handling questions that require in-
ference.
Acknowledgments
We are grateful to Mengqiu Wang for providing
the dataset and helping clarify some issues in the
experiments. We also thank Chris Burges and Hoi-
fung Poon for valuable discussion and the anony-
mous reviewers for their useful comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas?ca and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of NAACL, pages
19?27.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of SIGIR, pages 351?358.
E. Blanco and D. Moldovan. 2011. Semantic repre-
sentation of negation using focus detection. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011).
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In ACM Conference on Management of Data
(SIGMOD), pages 1247?1250.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32:13?47,
March.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In Proceedings of NAACL.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge, volume 3944. Springer-Verlag, Berlin.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 16?23.
Oren Etzioni. 2011. Search needs a shake-up. Nature,
476(7358):25?26.
P. Felzenszwalb, R. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
99(1).
D. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3.4):1?1.
J. Friedman. 2001. Greedy function approximation:
a gradient boosting machine. Annals of Statistics,
29(5):1189?1232.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In AAAI Conference on Artificial
Intelligence (AAAI).
J. Gao, K. Toutanova, and W. Yih. 2011.
Clickthrough-based latent semantic models for web
search. In Proceedings of SIGIR, pages 675?684.
S. Harabagiu and D. Moldovan. 2001. Open-domain
textual question answering. Tutorial of NAACL-
2001.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539?545.
M. Heilman and N. Smith. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and
answers to questions. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 1011?1019.
1752
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012), pages 356?364.
T. Mikolov, M. Karafia?t, L. Burget, J. Cernocky?, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 1045?1048.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Sur-
deanu. 2003. Performance issues and error analy-
sis in an open-domain question answering system.
ACM Transactions on Information Systems (TOIS),
21(2):133?154.
D. Moldovan, C. Clark, S. Harabagiu, and D. Hodges.
2007. COGEX: A semantically and contextually en-
riched logic prover for question answering. Journal
of Applied Logic, 5(1):49?69.
R. Morante and E. Blanco. 2012. *SEM 2012 shared
task: Resolving the scope and focus of negation. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 265?274.
S. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. In AAAI Confer-
ence on Artificial Intelligence (AAAI).
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping
dependencies trees: An application to question an-
swering. In International Symposium on Artificial
Intelligence and Mathematics (AI & Math).
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis.
In WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In Proceed-
ings of NAACL.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International
Joint Conference on Artificial Intelligence (IJCAI).
B. Rink and S. Harabagiu. 2012. UTD: Determining
relational similarity using lexical patterns. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 413?418.
S. Robertson, S. Walker, S. Jones, M. Hancock-
Beaulieu, and M. Gatford. 1995. Okapi at TREC-3.
In Text REtrieval Conference (TREC), pages 109?
109.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL, pages 12?21.
D. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of the HLT-NAACL
Workshop on Statistical Machine Translation, pages
23?30.
Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. 2011.
Short text conceptualization using a probabilistic
knowledgebase. In International Joint Conference
on Artificial Intelligence (IJCAI), pages 2330?2336.
K. Tai. 1979. The tree-to-tree correction problem. J.
ACM, 26(3):422?433, July.
P. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37(1):141?
188.
E. Voorhees and D. Tice. 2000. Building a question
answering test collection. In Proceedings of SIGIR,
pages 200?207.
M. Wang and C. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for tex-
tual entailment and question answering. In Proceed-
ings of COLING.
M. Wang, N. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
T. Winograd. 1977. Five lectures on artificial intelli-
gence. In A. Zampolli, editor, Linguistic Structures
Processing, pages 399?520. North Holland.
W. Woods. 1973. Progress in natural language under-
standing: An application to lunar geology. In Pro-
ceedings of the National Computer Conference and
Exposition (AFIPS), pages 441?450.
W. Wu, H. Li, H. Wang, and K. Zhu. 2012. Probase:
a probabilistic taxonomy for text understanding. In
ACM Conference on Management of Data (SIG-
MOD), pages 481?492.
W. Yih and V. Qazvinian. 2012. Measuring word relat-
edness using heterogeneous vector space models. In
Proceedings of NAACL-HLT 2012, pages 616?620.
W. Yih, K. Toutanova, J. Platt, and C. Meek. 2011.
Learning discriminative projections for text similar-
ity measures. In ACL Conference on Natural Lan-
guage Learning (CoNLL), pages 247?256.
W. Yih, G. Zweig, and J. Platt. 2012. Polarity inducing
latent semantic analysis. In Proceedings of EMNLP-
CoNLL, pages 1212?1222.
A. Zhila, W. Yih, C. Meek, G. Zweig, and T. Mikolov.
2013. Combining heterogeneous models for mea-
suring relational similarity. In Proceedings of HLT-
NAACL.
1753
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699?709,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Learning Continuous Phrase Representations for                                     
Translation Modeling 
 
Jianfeng Gao    Xiaodong He    Wen-tau Yih    Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,xiaohe,scottyih,deng}@microsoft.com 
 
 
 
Abstract 
This paper tackles the sparsity problem in 
estimating phrase translation probabilities 
by learning continuous phrase representa-
tions, whose distributed nature enables the 
sharing of related phrases in their represen-
tations. A pair of source and target phrases 
are projected into continuous-valued vec-
tor representations in a low-dimensional 
latent space, where their translation score 
is computed by the distance between the 
pair in this new space. The projection is 
performed by a neural network whose 
weights are learned on parallel training 
data. Experimental evaluation has been 
performed on two WMT translation tasks. 
Our best result improves the performance 
of a state-of-the-art phrase-based statistical 
machine translation system trained on 
WMT 2012 French-English data by up to 
1.3 BLEU points. 
1 Introduction 
The phrase translation model, also known as the 
phrase table, is one of the core components of 
phrase-based statistical machine translation (SMT) 
systems. The most common method of construct-
ing the phrase table takes a two-phase approach 
(Koehn et al 2003). First, the bilingual phrase 
pairs are extracted heuristically from an automat-
ically word-aligned training data. The second 
phase, which is the focus of this paper, is parame-
ter estimation where each phrase pair is assigned 
with some scores that are estimated based on 
counting these phrases or their words using the 
same word-aligned training data. 
Phrase-based SMT systems have achieved 
state-of-the-art performance largely due to the fact 
that long phrases, rather than single words, are 
used as translation units so that useful context in-
formation can be captured in selecting translations. 
However, longer phrases occur less often in train-
ing data, leading to a severe data sparseness prob-
lem in parameter estimation. There has been a 
plethora of research reported in the literature on 
improving parameter estimation for the phrase 
translation model (e.g., DeNero et al 2006; 
Wuebker et al 2010; He and Deng 2012; Gao and 
He 2013).  
This paper revisits the problem of scoring a 
phrase translation pair by developing a Continu-
ous-space Phrase Translation Model (CPTM). 
The translation score of a phrase pair in this model 
is computed as follows. First, we represent each 
phrase as a bag-of-words vector, called word vec-
tor henceforth. We then project the word vector, 
in either the source language or the target lan-
guage, into a respective continuous feature vector 
in a common low-dimensional space that is lan-
guage independent. The projection is performed 
by a multi-layer neural network. The projected 
feature vector forms the continuous representa-
tion of a phrase. Finally, the translation score of a 
source-target phrase pair is computed by the dis-
tance between their feature vectors.  
The main motivation behind the CPTM is to 
alleviate the data sparseness problem associated 
with the traditional counting-based methods by 
grouping phrases with a similar meaning across 
different languages. This style of grouping is 
made possible because of the distributed nature of 
the continuous-space representations for phrases. 
No such sharing was possible in the original sym-
bolic space for representing words or phrases.  In 
this model, semantically or grammatically related 
phrases, in both the source and the target lan-
guages, would tend to have similar (close) feature 
vectors in the continuous space, guided by the 
training objective. Since the translation score is a 
smooth function of these feature vectors, a small 
699
change in the features should only lead to a small 
change in the translation score. 
The primary research task in developing the 
CPTM is learning the continuous representation 
of a phrase that is effective for SMT. Motivated 
by recent studies on continuous-space language 
models (e.g., Bengio et al 2003; Mikolov et al 
2011; Schwenk et al, 2012), we use a neural net-
work to project a word vector to a feature vector. 
Ideally, the projection would discover those latent 
features that are useful to differentiate good trans-
lations from bad ones, for a given source phrase. 
However, there is no training data with explicit 
annotation on the quality of phrase translations. 
The phrase translation pairs are hidden in the par-
allel source-target sentence pairs, which are used 
to train the traditional translation models. The 
quality of a phrase translation can only be judged 
implicitly through the translation quality of the 
sentences, as measured by BLEU, which contain 
the phrase pair. In order to overcome this chal-
lenge and let the BLEU metric guide the projec-
tion learning, we propose a new method to learn 
the parameters of a neural network. This new 
method, via the choice of an appropriate objective 
function in training, automatically forces the fea-
ture vector of a source phrase to be closer to the 
feature vectors of its candidate translations. As a 
result, the BLEU score is improved when these 
translations are selected by an SMT decoder to 
produce final, sentence-level translations. The 
new learning method makes use of the L-BFGS 
algorithm and the expected BLEU as the objective 
function defined on N-best lists. 
To the best of our knowledge, the CPTM pro-
posed in this paper is the first continuous-space 
phrase translation model that makes use of joint 
representations of a phrase in the source language 
and its translation in the target language (to be de-
tailed in Section 4) and that is shown to lead to 
significant improvement over a standard phrase-
based SMT system (to be detailed in Section 6).  
Like the traditional phrase translation model, 
the translation score of each bilingual phrase pair 
is modeled explicitly in our model. However, in-
stead of estimating the phrase translation score on 
aligned parallel data, our model intends to capture 
the grammatical and semantic similarity between 
a source phrase and its paired target phrase by pro-
jecting them into a common, continuous space 
that is language independent. 
                                                          
1 Niehues et al (2011) use different translation units in order 
to integrate the n-gram translation model into the phrase-
based approach. However, it is not clear how a continuous 
The rest of the paper is organized as follows. 
Section 2 reviews previous work. Section 3 re-
views the log-linear model for phrase-based SMT 
and Sections 4 presents the CPTM. Section 5 de-
scribes the way the model parameters are esti-
mated, followed by the experimental results in 
Section 6. Finally, Section 7 concludes the paper. 
2 Related Work 
Representations of words or documents as contin-
uous vectors have a long history. Most of the ear-
lier latent semantic models for learning such vec-
tors are designed for information retrieval 
(Deerwester et al 1990; Hofmann 1999; Blei et al 
2003). In contrast, recent work on continuous 
space language models, which estimate the prob-
ability of a word sequence in a continuous space 
(Bengio et al 2003; Mikolov et al 2010), have ad-
vanced the state of the art in language modeling, 
outperforming the traditional n-gram model on 
speech recognition (Mikolov et al 2012; Sunder-
meyer et al 2013) and machine translation 
(Mikolov 2012; Auli et al 2013). 
Because these models are developed for mono-
lingual settings, word embedding from these mod-
els is not directly applicable to translation. As a 
result, variants of such models for cross-lingual 
scenarios have been proposed so that words in dif-
ferent languages are projected into the shared la-
tent vector space (Dumais et al 1997; Platt et al 
2010; Vinokourov et al 2002; Yih et al 2011; 
Gao et al 2011; Huang et al 2013; Zou et al 
2013). In principle, a phrase table can be derived 
using any of these cross-lingual models, although 
decoupling the derivation from the SMT training 
often results in suboptimal performance (e.g., 
measured in BLEU), as we will show in Section 6. 
Recently, there is growing interest in applying 
continuous-space models for translation. The 
most related to this study is the work of continu-
ous space n-gram translation models (Schwenk et 
al. 2007; Schwenk 2012; Son et al 2012), where 
the feed-forward neural network language model 
is extended to represent translation probabilities. 
However, these earlier studies focused on the n-
gram translation models, where the translation 
probability of a phrase or a sentence is decom-
posed as a product of n-gram probabilities as in a 
standard n-gram language model. Therefore, it is 
not clear how their approaches can be applied to 
the phrase translation model1, which is much more 
version of such a model can be trained efficiently because the 
factor models used by Son et al cannot be applied directly. 
700
widely used in modern SMT systems. In contrast, 
our model learns jointly the representations of a 
phrase in the source language as well as its trans-
lation in the target language. The recurrent contin-
uous translation models proposed by Kalchbren-
ner and Blunsom (2013) also adopt the recurrent 
language model (Mikolov et al 2010). But unlike 
the n-gram translation models above, they make 
no Markov assumptions about the dependency of 
the words in the target sentence. Continuous space 
models have also been used for generating trans-
lations for new words (Mikolov et al 2013a) and 
ITG reordering (Li et al 2013). 
There has been a lot of research on improving 
the phrase table in phrase-based SMT (Marcu and 
Wong 2002; Lamber and Banchs 2005; Denero et 
al. 2006; Wuebker et al 2010; Zhang et al, 2011; 
He and Deng 2012; Gao and He 2013). Among 
them, (Gao and He 2013) is most relevant to the 
work described in this paper. They estimate 
phrase translation probabilities using a discrimi-
native training method under the N-best reranking 
framework of SMT. In this study we use the same 
objective function to learn the continuous repre-
sentations of phrases, integrating the strengths as-
sociated with these earlier studies. 
3 The Log-Linear Model for SMT 
Phrase-based SMT is based on a log-linear model 
which requires learning a mapping between input 
? ? ? to output ? ? ?. We are given 
? Training samples (?? , ??)  for ? = 1??,  
where each source sentence ?? is paired with 
a reference translation in target language ??; 
? A procedure GEN to generate a list of N-best 
candidates GEN(??) for an input ?? , where 
GEN  in this study is the baseline phrase-
based SMT system, i.e., an in-house 
implementation of the Moses system (Koehn 
et al 2007) that does not use the CPTM, and 
each ? ? GEN(??)  is labeled by the 
sentence-level BLEU score (He and Deng 
2012), denoted by sBleu(??, ?) , which 
measures the quality of ? with respect to its 
reference translation ??; 
? A vector of features ? ? ?? that maps each 
(??, ?) to a vector of feature values
2; and 
? A parameter vector ? ? ??, which assigns a 
real-valued weight to each feature. 
                                                          
2 Our baseline system uses a set of standard features sug-
gested in Koehn et al (2007), which is also detailed in Sec-
tion 6. 
The components GEN(. ), ? and ?  define a log-
linear model that maps ?? to an output sentence as 
follows: 
?? = argmax
(?,?)?GEN(??)
?T?(??, ?, ?) (1) 
which states that given ? and ?, argmax returns 
the highest scoring translation ??,  maximizing 
over  correspondences ?. In phrase-based SMT, ? 
consists of a segmentation of the source and target 
sentences into phrases and an alignment between 
source and target phrases. Since computing the 
argmax  exactly is intractable, it is commonly 
performed approximatedly by beam search (Och 
and Ney 2004). Following Liang et al (2006), we 
assume that every translation candidate is always 
coupled with a corresponding ?, called the Viterbi 
derivation, generated by (1). 
4 A Continuous-Space Phrase Transla-
tion Model (CPTM) 
The architecture of the CPTM is shown in Figures 
1 and 2, where for each pair of source and target 
phrases (??, ??)  in a source-target sentence pair, 
we first project them into feature vectors ??? and 
??? in a latent, continuous space via a neural net-
work with one hidden layer (as shown in Figure 
2), and then compute the translation score, 
score(??, ??), by the distance of their feature vec-
tors in that space. 
We start with a bag-of-words representation of 
a phrase ? ? ??, where ? is a word vector and ? 
is the size of the vocabulary consisting of words 
in both source and target languages, which is set 
to 200K in our experiments. We then learn to pro-
ject ? to a low-dimensional continuous space ??: 
?(?): ?? ? ??  
The projection is performed using a fully con-
nected neural network with one hidden layer and 
tanh activation functions. Let ?1 be the projec-
tion matrix from the input layer to the hidden layer 
and ?2  the projection matrix from the hidden 
layer to the output layer, we have 
? ? ?(?) = tanh (?2
T(tanh(?1
T?))) (2) 
 
 
701
 
Figure 2. A neural network model for phrases 
giving rise to their continuous representations. 
The model with the same form is used for both 
source and target languages. 
 
  
The translation score of a source phrase f and a 
target phrase e can be measured as the similarity   
(or distance) between their feature vectors. We 
choose the dot product as the similarity function3: 
score(?, ?) ? sim?(?? , ??) = ??
T?? (3) 
According to (2), we see that the value of the scor-
ing function is determined by the projection ma-
trices ? = {?1,?2}. 
The CPTM of (2) and (3) can be incorporated 
into the log-linear model for SMT (1) by 
                                                          
3 In our experiments, we compare dot product and the cosine 
similarity functions and find that the former works better for 
nonlinear multi-layer neural networks, and the latter works 
better for linear neural networks. For the sake of clarity, we 
choose dot product when we describe the CPTM and its train-
ing in Sections 4 and 5, respectively. 
4 The baseline SMT needs to be reasonably good in the 
sense that the oracle BLEU score on the generated n-best 
introducing a new feature ??+1  and a new feature 
weight ??+1. The new feature is defined as 
??+1(??, ?, ?) = ? sim?(?? , ??)(?,? )??   (4) 
Thus, the phrase-based SMT system, into which 
the CPTM is incorporated, is parameterized by 
(?, ?), where ? is a vector of a handful of param-
eters used in the log-linear model of (1), with one 
weight for each feature; and ? is the projection 
matrices used in the CPTM defined by (2) and (3). 
In our experiments we take three steps to learn 
(?, ?): 
1. We use a baseline phrase-based SMT sys-
tem to generate for each source sentence in 
training data an N-best list of translation hy-
potheses4. 
2. We set ? to that of the baseline system and 
let ??+1 = 1, and optimize ? w.r.t. a loss 
function on training data5. 
3. We fix ? , and optimize ?  using MERT 
(Och 2003) to maximize BLEU on dev data. 
In the next section, we will describe Step 2 in de-
tail as it is directly related to the CPTM training. 
 
lists needs to be significantly higher than that of the top-1 
translations so that the CPTM can be effectively trained. 
5 The initial value of ??+1 can also be tuned using the dev 
set. However, we find in a pilot study that it is good enough 
to set it to 1 when the values of all the baseline feature 
weights, used in the log-linear model of (1), are properly nor-
malized, such as by setting ?? = ??/?  for ? = 1?? , 
where ? is the unnormalized weight value of the target lan-
guage model. 
 
Figure 1. The architecture of the CPTM, where the mapping from a phrase to its continuous repre-
sentation is shown in Figure 2. 
 
 
200K (d) 
100 
100 (?) 
(?1???) 
Word vector 
Neural network 
Feature vector 
?1 
?2 
? 
? 
Raw phrase  ? or ? 
  ?(the process of)         (machine translation)     
(consists of). . . 
 ?(le processus de)    
(traduction automatique)  (consiste en). . .  
???1 ?? ??+1 
???1 ?? ??+1 
??? 
 
 
??? 
 
??(?)
score(??, ??) = ???
T ???  
 
Target phrases  
Continuous representations of  
target phrases  
Source phrases  
 
Continuous representations of  
source phrases  
Translation score as dot product of 
feature vectors in the continuous space 
702
5 Training CPTM 
This section describes the loss function we em-
ploy with the CPTM and the algorithm to train the 
neural network weights. 
We define the loss function ?(?) as the nega-
tive of the N-best list based expected BLEU, de-
noted by xBleu(?). In the reranking framework of 
SMT outlined in Section 3, xBleu(?) over one 
training sample (??, ??) is defined as 
xBleu(?) = ? ?(?|??)sBleu(??, ?)??GEN(??)  (5) 
where sBleu(??, ?)  is the sentence-level BLEU 
score, and  ?(?|??) is the translation probability 
from ?? to ? computed using softmax as  
?(?|??) =
exp(??T?(??,?,?))
? exp(??T?(??,??,?))???GEN(??)
  (6) 
where ?T? is the log-linear model of (1), which 
also includes the feature derived from the CPTM 
as defined by (4), and ? is a tuned smoothing fac-
tor. 
Let ?(?) be a loss function which is differen-
tiable w.r.t. the parameters of the CPTM, ?. We 
can compute the gradient of the loss and learn ? 
using gradient-based numerical optimization al-
gorithms, such as L-BFGS or stochastic gradient 
descent (SGD).  
5.1 Computing the Gradient 
Since the loss does not explicitly depend on ?, we 
use the chain rule for differentiation: 
??(?)
??
= ?
??(?)
?sim?(?? , ??)
?sim?(?? , ??)
??
(?,? )
 
= ? ??(?,?)
?sim?(?? , ??)
??
(?,? )
 (7) 
which takes the form of summation over all phrase 
pairs occurring either in a training sample (sto-
chastic mode) or in the entire training data (batch 
mode). ?(?,?) in (7) is known as the error term of 
the phrase pair (?, ?), and is defined as   
?(?,?) = ?
??(?)
?sim?(??,??)
  (8) 
It describes how the overall loss changes with the 
translation score of the phrase pair (?, ?). We will 
leave the derivation of ?(?,?) to Section 5.1.2, and 
will first describe how the gradient of 
sim?(?? , ??) w.r.t. ? is computed. 
5.1.1 Computing ?????(??, ??)/?? 
Without loss of generality, we use the following 
notations to describe a neural network: 
? ?? is the projection matrix for the l-th layer 
of the neural network; 
? ? is the input word vector of a phrase; 
? ?? is the sum vector of the l-th layer; and  
? ?? = ?(??) is the output vector of the l-th 
layer, where ? is an activation function; 
Thus, the CPTM defined by (2) and (3) can be rep-
resented as  
?1 = ?1
T?  
?1 = ?(?1)  
?2 = ?2
T?1  
?2 = ?(?2)  
sim?(?? , ??) = (??
2)
T
??
2  
The gradient of the matrix ?2 which projects the 
hidden vector to the output vector is computed as: 
?sim?(?? , ??)
??2
=
?(??
2)
T
??2
??
2 + (??
2)
T ???
2
??2
 
= ??
1 (??
2 ? ??(??
2))
T
+ ??
1 (??
2 ? ??(??
2))
T
 (9) 
where ? is the element-wise multiplication (Hada-
mard product). Applying the back propagation 
principle, the gradient of the projection matrix 
mapping the input vector to the hidden vector ?1 
is computed as 
?sim?(?? , ??)
??1
 
= ?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  
+?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  (10) 
The derivation can be easily extended to a neural 
network with multiple hidden layers.  
5.1.2 Computing ?(?,?) 
To simplify the notation, we rewrite our loss func-
tion of (5) and (6) over one training sample as  
703
?(?) = ?xBleu(?) = ?
G(?)
Z(?)
 (11) 
where 
G(?) = ? sBleu(?, ??) exp(?
T?(??, ?, ?))?   
Z(?) = ? exp(?T?(??, ?, ?))?   
Combining (8) and (11), we have 
?(?,?) =
?xBleu(?)
?sim?(?? , ??)
 (12) 
=
1
Z(?)
(
?G(?)
?sim?(?? , ??)
?
?Z(?)
?sim?(?? , ??)
xBleu(?)) 
Because ? is only relevant to ??+1 which is de-
fined in (4), we have 
??T?(??, ?, ?)
?sim?(?? , ??)
= ??+1
???+1(??, ?, ?)
?sim?(?? , ??)
  
= ??+1?(?, ?; ?) (13) 
where ?(?, ?; ?)  is the number of times the 
phrase pair (?, ?)  occurs in ? . Combining (12) 
and (13), we end up with the following equation 
?(?,?)
= ? U(?,?)?(?|??)??+1?(?, ?; ?)
(?,?)????(??)
 
where  (14) 
U(?, ?) = sBleu(??, ?) ? xBleu(?).  
5.2 The Training Algorithm 
In our experiments we train the parameters of the 
CPTM, ?, using the L-BFGS optimizer described 
in Andrew and Gao (2007), together with the loss 
function described in (5). The gradient is com-
puted as described in Sections 5.1. Although SGD 
has been advocated for neural network training 
due to its simplicity and its robustness to local 
minima (Bengio 2009), we find that in our task 
that the L-BFGS minimizes the loss in a desirable 
fashion empirically when iterating over the com-
plete training data (batch mode). For example, the 
convergence of the algorithm was found to be 
smooth, despite the non-convexity in our loss. An-
other merit of batch training is that the gradient 
over all training data can be computed efficiently. 
As shown in Section 5.1, computing 
?sim?(x? , x?)/??  requires large-scale matrix 
multiplications, and is expensive for multi-layer 
neural networks. Eq. (7) suggests that 
?sim?(x? , x?)/??  and ?(?,?)  can be computed 
separately, thus making the computation cost of 
the former term only depends on the number of 
phrase pairs in the phrase table, but not the size of 
training data. Therefore, the training method de-
scribed here can be used on larger amounts of 
training data with little difficulty.  
As described in Section 4, we take three steps 
to learn the parameters for both the log-linear 
model of SMT and the CPTM. While steps 1 and 
3 can be easily parallelized on a computer cluster, 
the CPTM training is performed on a single ma-
chine. For example, given a phrase table contain-
ing 16M pairs and a 1M-sentence training set, it 
takes a couple of hours to generate the N-best lists 
on a cluster, and about 10 hours to train the CPTM 
on a Xeon E5-2670 2.60GHz machine.   
For a non-convex problem, model initialization 
is important. In our experiments we always initial-
ize ?1 using a bilingual topic model trained on 
parallel data (see detail in Section 6.2), and ?2 as 
an identity matrix. In principle, the loss function 
of (5) can be further regularized (e.g. by adding a 
term of ?2 norm) to deal with overfitting. How-
ever, we did not find clear empirical advantage 
over the simpler early stop approach in a pilot 
study, which is adopted in the experiments in this 
paper.   
6 Experiments 
This section evaluates the CPTM presented on 
two translation tasks using WMT data sets. We 
first describe the data sets and baseline setup. 
Then we present experiments where we compare 
different versions of the CPTM and previous 
models. 
6.1 Experimental Setup 
Baseline. We experiment with an in-house 
phrase-based system similar to Moses (Koehn et 
al. 2007), where the translation candidates are 
scored by a set of common features including 
maximum likelihood estimates of source given 
target phrase mappings ????(?|?) and vice versa 
????(?|?), as well as lexical weighting estimates 
???(?|?) and ???(?|?), word and phrase penal-
ties, a linear distortion feature, and a lexicalized 
reordering feature. The baseline includes a stand-
ard 5-gram modified Kneser-Ney language model 
trained on the target side of the parallel corpora 
described below. Log-linear weights are estimated 
with the MERT algorithm (Och 2003). 
704
Evaluation. We test our models on two different 
data sets. First, we train an English to French sys-
tem based on the data of WMT 2006 shared task 
(Koehn and Monz 2006). The parallel corpus in-
cludes 688K sentence pairs of parliamentary pro-
ceedings for training. The development set con-
tains 2000 sentences, and the test set contains 
other 2000 sentences, all from the official WMT 
2006 shared task. 
Second, we experiment with a French to Eng-
lish system developed using 2.1M sentence pairs 
of training data, which amounts to 102M words, 
from the WMT 2012 campaign. The majority of 
the training data set is parliamentary proceedings 
except for 5M words which are newswire. We use 
the 2009 newswire data set, comprising 2525 sen-
tences, as the development set. We evaluate on 
four newswire domain test sets from 2008, 2010 
and 2011 as well as the 2010 system combination 
test set, containing 2034 to 3003 sentences. 
In this study we perform a detailed empirical 
comparison using the WMT 2006 data set, and 
verify our best models and results using the larger 
WMT 2012 data set. 
The metric used for evaluation is case insensi-
tive BLEU score (Papineni et al 2002). We also 
perform a significance test using the Wilcoxon 
signed rank test. Differences are considered statis-
tically significant when the p-value is less than 
0.05. 
6.2 Results of the CPTM 
Table 1 shows the results measured in BLEU eval-
uated on the WMT 2006 data set, where Row 1 is 
the baseline system. Rows 2 to 4 are the systems 
enhanced by integrating different versions of the 
CPTM. Rows 5 to 7 present the results of previous 
models. Row 8 is our best system. Table 2 shows 
the main results on the WMT 2012 data set. 
CPTM is the model described in Sections 4. 
As illustrated in Figure 2, the number of the nodes 
in the input layer is the vocabulary size ?. Both 
the hidden layer and the output layer have 100 
nodes6. That is, ?1 is a ? ? 100 matrix and ?2 
a 100 ? 100  matrix. The result shows that 
CPTM leads to a substantial improvement over 
the baseline system with a statistically significant 
margin of 1.0 BLEU points as in Table 1.  
We have developed a set of variants of CPTM 
to investigate two design choices we made in de-
veloping the CPTM: (1) whether to use a linear 
                                                          
6 We can achieve slightly better results using more nodes in 
the hidden and output layers, say 500 nodes. But the model 
projection or a multi-layer nonlinear projection; 
and (2) whether to compute the phrase similarity 
using word-word similarities as suggested by e.g., 
the lexical weighting model (Koehn et al 2003). 
We compare these variants on the WMT 2006 
data set, as shown in Table 1. 
CPTML (Row 3 in Table 1) uses a linear neural 
network to project a word vector of a phrase ? to 
a feature vector ?: ? ? ?(?) = ?T?, where ? is 
a ? ? 100  projection matrix. The translation 
score of a source phrase f and a target phrase e is 
measured as the similarity of their feature vectors. 
We choose cosine similarity because it works bet-
ter than dot product for linear projection. 
CPTMW (Row 4 in Table 1) computes the phrase 
similarity using word-word similarity scores. This 
follows the common smoothing strategy of ad-
dressing the data sparseness problem in modeling 
phrase translations, such as the lexical weighting 
model (Koehn et al 2003) and the word factored 
n-gram translation model (Son et al 2012). Let ? 
denote a word, and ? and ? the source and target 
phrases, respectively. We define 
sim(?, ?) =
1
|?|
? sim?(?, ?) +???
1
|?|
? sim?(?, ?)???   
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
sim?(?, ?)
=
? sim(?,??) exp(?sim(?,??))????
? exp(?sim(?,??))????
 
 
training is too slow to perform a detailed study within a rea-
sonable time. Therefore, all the models reported in this paper 
use 100 nodes.   
# Systems WMT test2006 
1 Baseline 33.06 
2 CPTM 34.10? 
3 CPTML 33.60
?? 
4 CPTMW 33.25
? 
5 BLTMPR 33.15
? 
6 DPM 33.29? 
7 MRFP 33.91
? 
8 Comb (2 + 7) 34.39?? 
Table 1: BLEU results for the English to French 
task using translation models and systems built 
on the WMT 2006 data set. The superscripts ? 
and ? indicate statistically significant difference 
(p < 0.05) from Baseline and CPTM, respec-
tively. 
 
705
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
where ? is the tuned smoothing parameter.  
Similar to CPTM, CPTMW also uses a nonlin-
ear projection to map each word (not a phrase vec-
tor as in CPTM) to a feature vector. 
Two observations can be made by comparing 
CPTM in Row 2 to its variants in Table 1. First of 
all, it is more effective to model the phrase trans-
lation directly than decomposing it into word-
word translations in the CPTMs. Second, we see 
that the nonlinear projection is able to generate 
more effective features, leading to better results 
than the linear projection. 
We  also compare the best version of the CPTM 
i.e., CPTM, with three related models proposed 
previously. We start the discussion with the re-
sults on the WMT 2006 data set in Table 1. 
Rows 5 and 6 in Table 1 are two state-of-the-
art latent semantic models that are originally 
trained on clicked query-document pairs (i.e., 
clickthrough data extracted from search logs) for 
query-document matching (Gao et al 2011). To 
adopt these models for SMT, we view source-tar-
get sentence pairs as clicked query-document 
pairs, and trained both models using the same 
methods as in Gao et al (2011) on the parallel bi-
lingual training data described earlier. Specifi-
cally, BTLMPR is an extension to PLSA, and is 
the best performer among different versions of the 
Bi-Lingual Topic Model (BLTM) described in 
Gao et al (2011). BLTM with Posterior Regular-
ization (BLTMPR) is trained on parallel training 
data using the EM algorithm with a constraint en-
forcing a source sentence and its paralleled target 
sentence to not only share the same prior topic dis-
tribution, but to also have similar fractions of 
words assigned to each topic. We incorporated the 
model into the log-linear model for SMT (1) as 
                                                          
7 Gao and He (2013) reported results of MRF models with 
different feature sets. We picked the MRF using phrase fea-
tures only (MRFP) for comparison since we are mainly inter-
ested in phrase representation. 
follows. First of all, the topic distribution of a 
source sentence ?? , denoted by ?(?|??) , is in-
duced from the learned topic-word distributions 
using EM. Then, each translation candidate ? in 
the N-best list GEN(??) is scored as 
?(?|??) = ? ? ?(?|?)?(?|??)????    
?(??|?) can be similarly computed. Finally, the 
logarithms of the two probabilities are incorpo-
rated into the log-linear model of (1) as two addi-
tional features. DPM is the Discriminative Projec-
tion Model described in Gao et al (2011), which 
is an extension of LSA. DPM uses a matrix to pro-
ject a word vector of a sentence to a feature vector. 
The projection matrix is learned on parallel train-
ing data using the S2Net alorithm (Yih et al 
2011). DPM can be incorporated into the log-lin-
ear model for SMT (1) by introducing a new fea-
ture ??+1 for each phrase pair, which is defined 
as the cosine similarity of the phrases in the pro-
ject space.  
As we see from Table 1, both latent semantic 
models, although leading to some slight improve-
ment over Baseline, are much less effective than 
CPTM. 
Finally, we compare the CPTM with the Mar-
kov Random Field model using phrase features 
(MRFP in Tables 1 and 2), proposed by Gao and 
He (2013)7, on both the WMT 2006 and WMT 
2012 datasets. MRFp is a state-of-the-art large 
scale discriminative training model that uses the 
same expected BLEU training criterion, which 
has proven to give superior performance across a 
range of MT tasks recently (He and Deng 2012, 
Setiawan and Zhou 2013, Gao and He 2013).  
Unlike CPTM, MRFp is a linear model that 
simply treats each phrase pair as a single feature. 
Therefore, although both are trained using the 
# Systems dev news2011 news2010 news2008 newssyscomb2010 
1 Baseline 23.58 25.24 24.35 20.36 24.14 
2 MRFP 24.07
? 26.00? 24.90 20.84? 25.05? 
3 CPTM 24.12? 26.25? 25.05? 21.15?? 24.91? 
4 Comb (2 + 3) 24.46?? 26.56?? 25.52?? 21.64?? 25.22? 
Table 2:   BLEU results for the French to English task using translation models and systems built on 
the WMT 2012 data set. The superscripts ? and ? indicate statistically significant difference (p < 
0.05) from Baseline and MRFp, respectively. 
 
 
706
same expected BLEU based objective function, 
CPTM and MRFp model the translation relation-
ship between two phrases from different angles. 
MRFp estimates one translation score for each 
phrase pair explicitly without parameter sharing, 
while in CPTM, all phrases share the same neural 
network that projects raw phrases to the continu-
ous space, providing a more smoothed estimation 
of the translation score for each phrase pair.  
The results in Tables 1 and 2 show that CPTM 
outperforms MRFP on most of the test sets across 
the two WMT data sets, but the difference be-
tween them is often not significant. Our interpre-
tation is that although CPTM provides a better 
smoothed estimation for low-frequent phrase 
pairs, which otherwise suffer the data sparsity is-
sue, MRFp provides a more precise estimation for 
those high-frequent phrase pairs. That is, CPTM 
and MRFp capture complementary information 
for translation. We thus combine CPTM and 
MRFP (Comb in Tables 1 and 2) by incorporating 
two features, each for one model, into the log-lin-
ear model of SMT (1). We observe that for both 
translation tasks, accuracy improves by up to 0.8 
BLEU over MRFP alone (e.g., on the news2008 
test set in Table 2). The results confirm that 
CPTM captures complementary translation infor-
mation to MRFp. Overall, we improve accuracy 
by up to 1.3 BLEU over the baseline on both 
WMT data sets. 
7 Conclusions 
The work presented in this paper makes two major 
contributions. First, we develop a novel phrase 
translation model for SMT, where joint represen-
tations are exploited of a phrase in the source lan-
guage and of its translation in the target language, 
and where the translation score of the pair of 
source-target phrases are represented as the dis-
tance between their feature vectors in a low-di-
mensional, continuous space. The space is derived 
from the representations generated using a multi-
layer neural network. Second, we present a new 
learning method to train the weights in the multi-
layer neural network for the end-to-end BLEU 
metric directly. The training method is based on 
L-BFGS. We describe in detail how the gradient 
in closed form, as required for efficient optimiza-
tion, is derived. The objective function, which 
takes the form of the expected BLEU computed 
from N-best lists, is very different from the usual 
objective functions used in most existing architec-
tures of neural networks, e.g., cross entropy (Hin-
ton et al 2012) or mean square error (Deng et al 
2012). We hence have provided details in the der-
ivation of the gradient, which can serve as an ex-
ample to guide the derivation of neural network 
learning with other non-standard objective func-
tions in the future. 
Our evaluation on two WMT data sets show 
that incorporating the continuous-space phrase 
translation model into the log-linear framework 
significantly improves the accuracy of a state-of-
the-art phrase-based SMT system, leading to a 
gain up to 1.3 BLEU. Careful implementation of 
the L-BFGS optimization based on the BLEU-
centric objective function, together with the asso-
ciated closed-form gradient, is a key to the suc-
cess.  
A natural extension of this work is to expand 
the model and learning algorithm from shallow to 
deep neural networks. The deep models are ex-
pected to produce more powerful and flexible se-
mantic representations (e.g., Tur et al, 2012), and 
thus greater performance gain than what is pre-
sented in this paper. 
8 Acknowledgements 
We thank Michael Auli for providing a dataset 
and for helpful discussions. We also thank the four 
anonymous reviewers for their comments.  
References 
Andrew, G. and Gao, J. 2007. Scalable training 
of L1-regularized log-linear models. In 
ICML.  
Auli, M., Galley, M., Quirk, C. and Zweig, G. 
2013 Joint language and translation modeling 
with recurrent neural networks. In EMNLP. 
Bengio, Y. 2009. Learning deep architectures for 
AI. Fundamental Trends Machine Learning, 
vol. 2, no. 1, pp. 1?127. 
Bengio, Y., Duharme, R., Vincent, P., and Janvin, 
C. 2003. A neural probabilistic language 
model. JMLR, 3:1137-1155. 
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3: 993-1022. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P. 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 
12. 
707
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Index-
ing by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
DeNero, J., Gillick, D., Zhang, J., and Klein, D. 
2006. Why generative phrase models underper-
form surface heuristics. In Workshop on Statis-
tical Machine Translation, pp. 31-38. 
Deng, L., Yu, D., and Platt, J. 2012. Scalable 
stacking and learning for building deep archi-
tectures. In ICASSP. 
Diamantaras, K. I., and Kung, S. Y. 1996. Princi-
ple Component Neural Networks: Theory and 
Applications. Wiley-Interscience. 
Dumais S., Letsche T., Littman M. and Landauer 
T. 1997. Automatic cross-language retrieval us-
ing latent semantic indexing. In AAAI-97 
Spring Symposium Series: Cross-Language 
Text and Speech Retrieval. 
Ganchev, K., Graca, J., Gillenwater, J., and 
Taskar, B. 2010. Posterior regularization for 
structured latent variable models. Journal of 
Machine Learning Research, 11 (2010): 2001-
2049. 
Gao, J., and He, X. 2013. Training MRF-based 
translation models using gradient ascent. In 
NAACL-HLT, pp. 450-459. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR, pp. 675-684.  
He, X., and Deng, L. 2012. Maximum expected 
bleu training of phrase and lexicon translation 
models. In ACL, pp. 292-301. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering Binary Codes for Documents by Learn-
ing Deep Generative Models. Topics in Cogni-
tive Science, pp. 1-18. 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, vol. 29, no. 6, pp. 82-97. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR, pp. 50-57. 
Huang, P-S., He, X., Gao, J., Deng, L., Acero, A. 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM.  
Kalchbrenner, N. and Blunsom, P. 2013. Recur-
rent continuous translation models. In EMNLP. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, 
C., Federico, M., Bertoldi, N., Cowan, B., Shen, 
W., Moran, C., Zens, R., Dyer, C., Bojar, O., 
Constantin, A., and Herbst, E. 2007. Moses: 
open source toolkit for statistical machine trans-
lation. In ACL 2007, demonstration session. 
Koehn, P. and Monz, C. 2006. Manual and auto-
matic evaluation of machine translation be-
tween European languages. In Workshop on 
Statistical Machine Translation, pp. 102-121. 
Koehn, P., Och, F., and Marcu, D. 2003. Statisti-
cal phrase-based translation. In HLT-NAACL, 
pp. 127-133. 
Lambert, P. and Banchs, R. E. 2005. Data inferred 
multi-word expressions for statistical machine 
translation. In MT Summit X, Phuket, Thailand. 
Li, P., Liu, Y., and Sun, M. 2013. Recursive auto-
encoders for ITG-based translation. In EMNLP. 
Liang,P., Bouchard-Cote,A., Klein, D. and 
Taskar, B. 2006. An end-to-end discriminative 
approach to machine translation. In COLING-
ACL. 
Marcu, D., and Wong, W. 2002. A phrase-based, 
joint probability model for statistical machine 
translation. In EMNLP. 
Mikolov, T., Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In INTER-
SPEECH, pp. 1045-1048. 
Mikolov, T., Kombrink, S., Burget, L., Cernocky, 
J., and Khudanpur, S. 2011. Extensions of re-
current neural network language model. In 
ICASSP, pp. 5528-5531. 
Mikolov, T. 2012. Statistical Language Model 
based on Neural Networks. Ph.D. thesis, Brno 
University of Technology. 
Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. 
Exploiting similarities among languages for 
machine translation. CoRR. 2013; 
abs/1309.4148. 
Mikolov, T., Yih, W. and Zweig, G. 2013b. Lin-
guistic Regularities in Continuous Space Word 
Representations. In NAACL-HLT. 
Mimno, D., Wallach, H., Naradowsky, J., Smith, 
D. and McCallum, A. 2009. Polylingual topic 
models. In EMNLP. 
708
Niehues J., Herrmann, T., Vogel, S., and Waibel, 
A. 2011. Wider context by using bilingual lan-
guage models in machine translation. 
Och, F. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL, pp. 160-
167.  
Och, F., and Ney, H. 2004. The alignment tem-
plate approach to statistical machine translation. 
Computational Linguistics, 29(1): 19-51. 
Papineni, K., Roukos, S., Ward, T., and Zhu W-J. 
2002. BLEU: a method for automatic evaluation 
of machine translation. In ACL. 
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual Document Representations from 
Discriminative Projections. In EMNLP. 
Rosti, A-V., Hang, B., Matsoukas, S., and 
Schwartz, R. S. 2011. Expected BLEU training 
for graphs: bbn system description for WMT 
system combination task. In Workshop on Sta-
tistical Machine Translation. 
Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J. 
A. R. 2007. Smooth bilingual n-gram transla-
tion. In EMNLP-CoNLL, pp. 430-438. 
Schwenk, H. 2012. Continuous space translation 
models for phrase-based statistical machine 
translation. In COLING. 
Schwenk, H., Rousseau, A., and Mohammed A. 
2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine 
translation. In NAACL-HLT Workshop on the 
future of language modeling for HLT, pp. 11-
19. 
Setiawan, H. and Zhou, B., 2013. Discriminative 
training of 150 million translation parameters 
and its application to pruning. In NAACL. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic Compositionality through Recursive 
Matrix-Vector Spaces. In EMNLP. 
Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 
2011. Parsing natural scenes and natural lan-
guage with recursive neural networks. In ICML. 
Son, L. H., Allauzen, A., and Yvon, F. 2012. Con-
tinuous space translation models with neural 
networks. In NAACL-HLT, pp. 29-48. 
Sundermeyer, M., Oparin, I., Gauvain, J-L. 
Freiberg, B., Schluter, R. and Ney, H. 2013. 
Comparison of feed forward and recurrent neu-
ral network language models. In ICASSP, pp. 
8430?8434. 
Tur, G, Deng, L., Hakkani-Tur, D., and He, X., 
2012. Towards deeper understanding: deep con-
vex networks for semantic utterance classifica-
tion. In ICASSP. 
Vinokourov,A., Shawe-Taylor,J. and Cristia-
nini,N. 2002. Inferring a semantic representa-
tion of text via cross-language correlation anal-
ysis. In NIPS. 
Weston, J., Bengio, S., and Usunier, N. 2011. 
Large scale image annotation: learning to rank 
with joint word-image embeddings. In IJCAI. 
Wuebker, J., Mauser, A., and Ney, H. 2010. Train-
ing phrase translation models with leaving-one-
out. In ACL, pp. 475-484. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
Zhang, Y., Deng, L., He, X., and Acero, A. 2011. 
A novel decision function and the associated de-
cision-feedback learning for speech translation. 
In ICASSP. 
Zhila, A., Yih, W., Meek, C., Zweig, G. and 
Mikolov, T. 2013. Combining heterogeneous 
models for measuring relational similarity. In 
NAACL-HLT. 
Zou, W. Y., Socher, R., Cer, D., and Manning, C. 
D. 2013. Bilingual word embeddings for 
phrase-based machine translation. In EMNLP. 
709
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643?648,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Semantic Parsing for Single-Relation Question Answering
Wen-tau Yih Xiaodong He Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,xiaohe,meek}@microsoft.com
Abstract
We develop a semantic parsing framework
based on semantic similarity for open do-
main question answering (QA). We focus
on single-relation questions and decom-
pose each question into an entity men-
tion and a relation pattern. Using convo-
lutional neural network models, we mea-
sure the similarity of entity mentions with
entities in the knowledge base (KB) and
the similarity of relation patterns and re-
lations in the KB. We score relational
triples in the KB using these measures
and select the top scoring relational triple
to answer the question. When evaluated
on an open-domain QA task, our method
achieves higher precision across different
recall points compared to the previous ap-
proach, and can improve F
1
by 7 points.
1 Introduction
Open-domain question answering (QA) is an im-
portant and yet challenging problem that remains
largely unsolved. In this paper, we focus on an-
swering single-relation factual questions, which
are the most common type of question observed in
various community QA sites (Fader et al, 2013),
as well as in search query logs. We assumed
such questions are answerable by issuing a single-
relation query that consists of the relation and an
argument entity, against a knowledge base (KB).
Example questions of this type include: ?Who is
the CEO of Tesla?? and ?Who founded Paypal??
While single-relation questions are easier to
handle than questions with more complex and
multiple relations, such as ?When was the child of
the former Secretary of State in Obama?s admin-
istration born??, single-relation questions are still
far from completely solved. Even in this restricted
domain there are a large number of paraphrases of
the same question. That is to say that the problem
of mapping from a question to a particular relation
and entity in the KB is non-trivial.
In this paper, we propose a semantic parsing
framework tailored to single-relation questions.
At the core of our approach is a novel semantic
similarity model using convolutional neural net-
works. Leveraging the question paraphrase data
mined from the WikiAnswers corpus by Fader et
al. (2013), we train two semantic similarity mod-
els: one links a mention from the question to an
entity in the KB and the other maps a relation pat-
tern to a relation. The answer to the question can
thus be derived by finding the relation?entity triple
r(e
1
, e
2
) in the KB and returning the entity not
mentioned in the question. By using a general se-
mantic similarity model to match patterns and re-
lations, as well as mentions and entities, our sys-
tem outperforms the existing rule learning system,
PARALEX (Fader et al, 2013), with higher pre-
cision at all the recall points when answering the
questions in the same test set. The highest achiev-
able F
1
score of our system is 0.61, versus 0.54 of
PARALEX.
The rest of the paper is structured as follows.
We first survey related work in Sec. 2, followed by
the problem definition and the high-level descrip-
tion of our approach in Sec. 3. Sec. 4 details our
semantic models and Sec. 5 shows the experimen-
tal results. Finally, Sec. 6 concludes the paper.
2 Related Work
Semantic parsing of questions, which maps nat-
ural language questions to database queries, is
a critical component for KB-supported QA. An
early example of this research is the semantic
parser for answering geography-related questions,
learned using inductive logic programming (Zelle
and Mooney, 1996). Research in this line origi-
nally used small, domain-specific databases, such
as GeoQuery (Tang and Mooney, 2001; Liang et
643
al., 2013). Very recently, researchers have started
developing semantic parsers for large, general-
domain knowledge bases like Freebase and DB-
pedia (Cai and Yates, 2013; Berant et al, 2013;
Kwiatkowski et al, 2013). Despite significant
progress, the problem remains challenging. Most
methods have not yet been scaled to large KBs
that can support general open-domain QA. In con-
trast, Fader et al (2013) proposed the PARALEX
system, which targets answering single-relation
questions using an automatically created knowl-
edge base, ReVerb (Fader et al, 2011). By
applying simple seed templates to the KB and
by leveraging community-authored paraphrases of
questions from WikiAnswers, they successfully
demonstrated a high-quality lexicon of pattern-
matching rules can be learned for this restricted
form of semantic parsing.
The other line of work related to our approach
is continuous representations for semantic simi-
larity, which has a long history and is still an
active research topic. In information retrieval,
TF-IDF vectors (Salton and McGill, 1983), latent
semantic analysis (Deerwester et al, 1990) and
topic models (Blei et al, 2003) take the bag-of-
words approach, which captures well the contex-
tual information for documents, but is often too
coarse-grained to be effective for sentences. In
a separate line of research, deep learning based
techniques have been proposed for semantic un-
derstanding (Mesnil et al, 2013; Huang et al,
2013; Shen et al, 2014b; Salakhutdinov and Hin-
ton, 2009; Tur et al, 2012). We adapt the work
of (Huang et al, 2013; Shen et al, 2014b) for mea-
suring the semantic distance between a question
and relational triples in the KB as the core compo-
nent of our semantic parsing approach.
3 Problem Definition & Approach
In this paper, we focus on using a knowledge
base to answer single-relation questions. A single-
relation question is defined as a question com-
posed of an entity mention and a binary rela-
tion description, where the answer to this ques-
tion would be an entity that has the relation with
the given entity. An example of a single-relation
question is ?When were DVD players invented??
The entity is dvd-player and the relation is
be-invent-in. The answer can thus be de-
scribed as the following lambda expression:
?x. be-invent-in(dvd-player, x)
Q? RP ?M (1)
RP ? when were X invented (2)
M ? dvd players (3)
when were X invented
? be-invent-in (4)
dvd players
? dvd-player (5)
Figure 1: A potential semantic parse of the ques-
tion ?When were DVD players invented??
A knowledge base in this work can be simply
viewed as a collection of binary relation instances
in the form of r(e
1
, e
2
), where r is the relation and
e
1
and e
2
are the first and second entity arguments.
Single-relation questions are perhaps the easiest
form of questions that can directly be answered
by a knowledge base. If the mapping of the re-
lation and entity in the question can be correctly
resolved, then the answer can be derived by a sim-
ple table lookup, assuming that the fact exists in
the KB. However, due to the large number of para-
phrases of the same question, identifying the map-
ping accurately remains a difficult problem.
Our approach in this work can be viewed as a
simple semantic parser tailored to single-relation
questions, powered by advanced semantic similar-
ity models to handle the paraphrase issue. Given a
question, we first separate it into two disjoint parts:
the entity mention and the relation pattern. The
entity mention is a subsequence of consecutive
words in the question, where the relation pattern
is the question where the mention is substituted
by a special symbol. The mapping between the
pattern and the relation in the KB, as well as the
mapping between the mention and the entity are
determined by corresponding semantic similarity
models. The high-level approach can be viewed
as a very simple context-free grammar, which is
shown in Figure 1.
The probability of the rule in (1) is 1 since
we assume the input is a single-relation ques-
tion. For the exact decomposition of the ques-
tion (e.g., (2), (3)), we simply enumerate all com-
binations and assign equal probabilities to them.
The performance of this approach depends mainly
on whether the relation pattern and entity mention
can be resolved correctly (e.g., (4), (5)). To deter-
644
15K 15K 15K 15K 15K
500 500 500
max max
...
...
... max
500
...
...
Word hashing layer: ft
Convolutional layer: ht
Max pooling layer: v
Semantic layer: y
     <s>             w1              w2           wT             <s>Word sequence: xt
Word hashing matrix: Wf
Convolution matrix: Wc
Max pooling operation
Semantic projection matrix: Ws
... ...
500
Figure 2: The CNNSM maps a variable-length
word sequence to a low-dimensional vector in a
latent semantic space. A word contextual window
size (i.e., the receptive field) of three is used in the
illustration. Convolution over word sequence via
learned matrix W
c
is performed implicitly via the
earlier word hashing layer?s mapping with a local
receptive field. The max operation across the se-
quence is applied for each of 500 feature dimen-
sions separately.
mine the probabilities of such mappings, we pro-
pose using a semantic similarity model based on
convolutional neural networks, which is the tech-
nical focus in this paper.
4 Convolutional Neural Network based
Semantic Model
Following (Collobert et al, 2011; Shen et al,
2014b), we develop a new convolutional neural
network (CNN) based semantic model (CNNSM)
for semantic parsing. The CNNSM first uses a
convolutional layer to project each word within a
context window to a local contextual feature vec-
tor, so that semantically similar word-n-grams are
projected to vectors that are close to each other
in the contextual feature space. Further, since the
overall meaning of a sentence is often determined
by a few key words in the sentence, CNNSM uses
a max pooling layer to extract the most salient lo-
cal features to form a fixed-length global feature
vector. The global feature vector can be then fed
to feed-forward neural network layers to extract
non-linear semantic features. The architecture of
the CNNSM is illustrated in Figure 2. In what fol-
lows, we describe each layer of the CNNSM in
detail, using the annotation illustrated in Figure 2.
In our model, we leverage the word hash-
ing technique proposed in (Huang et al, 2013)
where we first represent a word by a letter-
trigram count vector. For example, given a
word (e.g., cat), after adding word boundary sym-
bols (e.g., #cat#), the word is segmented into a se-
quence of letter-n-grams (e.g., letter-trigrams: #-
c-a, c-a-t, a-t-#). Then, the word is represented
as a count vector of letter-trigrams. For exam-
ple, the letter-trigram representation of ?cat? is:
In Figure 2, the word hashing matrix W
f
de-
notes the transformation from a word to its letter-
trigram count vector, which requires no learning.
Word hashing not only makes the learning more
scalable by controlling the size of the vocabulary,
but also can effectively handle the OOV issues,
sometimes due to spelling mistakes. Given the
letter-trigram based word representation, we rep-
resent a word-n-gram by concatenating the letter-
trigram vectors of each word, e.g., for the t-th
word-n-gram at the word-n-gram layer, we have:
l
t
=
[
f
T
t?d
, ? ? ? , f
T
t
, ? ? ? , f
T
t+d
]
T
, t = 1, ? ? ? , T
where f
t
is the letter-trigram representation of the
t-th word, and n = 2d + 1 is the size of the con-
textual window. The convolution operation can
be viewed as sliding window based feature extrac-
tion. It captures the word-n-gram contextual fea-
tures. Consider the t-th word-n-gram, the convo-
lution matrix projects its letter-trigram representa-
tion vector l
t
to a contextual feature vector h
t
. As
shown in Figure 2, h
t
is computed by
h
t
= tanh(W
c
? l
t
), t = 1, ? ? ? , T
where W
c
is the feature transformation matrix, as
known as the convolution matrix, which are shared
among all word n-grams. The output of the con-
volutional layer is a sequence of local contextual
feature vectors, one for each word (within a con-
textual window). Since many words do not have
significant influence on the semantics of the sen-
tence, we want to retain in the global feature vector
only the salient features from a few key words. For
this purpose, we use a max operation, also known
as max pooling, to force the network to retain only
645
the most useful local features produced by the con-
volutional layers. Referring to the max-pooling
layer of Figure 2, we have
v(i) = max
t=1,??? ,T
{f
t
(i)}, i = 1, ? ? ? ,K
where v(i) is the i-th element of the max pool-
ing layer v, h
t
(i) is the i-th element of the t-th
local feature vector h
t
. K is the dimensionality
of the max pooling layer, which is the same as
the dimensionality of the local contextual feature
vectors {h
t
}. One more non-linear transformation
layer is further applied on top of the global feature
vector v to extract the high-level semantic repre-
sentation, denoted by y. As shown in Figure 2, we
have y = tanh(W
s
? v), where v is the global fea-
ture vector after max pooling, W
s
is the semantic
projection matrix, and y is the vector representa-
tion of the input query (or document) in latent se-
mantic space. Given a pattern and a relation, we
compute their relevance score by measuring the
cosine similarity between their semantic vectors.
The semantic relevance score between a pattern Q
and a relation R is defined as the cosine score of
their semantic vectors y
Q
and y
R
.
We train two CNN semantic models from sets of
pattern?relation and mention?entity pairs, respec-
tively. Following (Huang et al, 2013), for every
pattern, the corresponding relation is treated as a
positive example and 100 randomly selected other
relations are used as negative examples. The set-
ting for the mention?entity model is similar.
The posterior probability of the positive relation
given the pattern is computed based on the cosine
scores using softmax:
P (R
+
|Q) =
exp(? ? cos(y
R
+ , y
Q
))
?
R
?
exp(? ? cos(y
R
?
, y
Q
))
where ? is a scaling factor set to 5. Model train-
ing is done by maximizing the log-posteriori us-
ing stochastic gradient descent. More detail can
be found in (Shen et al, 2014a).
5 Experiments
In order to provide a fair comparison to previ-
ous work, we experimented with our approach
using the PARALAX dataset (Fader et al, 2013),
which consists of paraphrases of questions mined
from WikiAnswers and answer triples from Re-
Verb. In this section, we briefly introduce the
dataset, describe the system training and evalua-
tion processes and, finally, present our experimen-
tal results.
5.1 Data & Model Training
The PARALEX training data consists of ap-
proximately 1.8 million pairs of questions and
single-relation database queries, such as ?When
were DVD players invented??, paired with
be-invent-in(dvd-player,?). For eval-
uation, the authors further sampled 698 questions
that belong to 37 clusters and hand labeled the an-
swer triples returned by their systems.
To train our two CNN semantic models, we
derived two parallel corpora based on the PAR-
ALEX training data. For relation patterns, we first
scanned the original training corpus to see if there
was an exact surface form match of the entity (e.g.,
dvd-player would map to ?DVD player? in the
question). If an exact match was found, then the
pattern would be derived by replacing the mention
in the question with the special symbol. The corre-
sponding relation of this pattern was thus the rela-
tion used in the original database query, along with
the variable argument position (i.e., 1 or 2, indicat-
ing whether the answer entity was the first or sec-
ond argument of the relation). In the end, we de-
rived about 1.2 million pairs of patterns and rela-
tions. We then applied these patterns to all the 1.8
million training questions, which helped discover
160 thousand new mentions that did not have the
exact surface form matches to the entities.
When training the CNNSM for the pattern?
relation similarity measure, we randomly split the
1.2 million pairs of patterns and relations into two
sets: the training set of 1.19 million pairs, and
the validation set of 12 thousand pairs for hyper-
parameter tuning. Data were tokenized by re-
placing hyphens with blank spaces. In the ex-
periment, we used a context window (i.e., the re-
ceptive field) of three words in the convolutional
neural networks. There were 15 thousand unique
letter-trigrams observed in the training set (used
for word hashing). Five hundred neurons were
used in the convolutional layer, the max-pooling
layer and the final semantic layer, respectively.
We used a learning rate of 0.002 and the train-
ing converged after 150 iterations. A similar set-
ting was used for the CNNSM for the mention?
entity model, which was trained on 160 thousand
mention-entity pairs.
5.2 Results
We used the same test questions in the PARALEX
dataset to evaluate whether our system could find
646
F1
Precision Recall MAP
CNNSM
pm
0.57 0.58 0.57 0.28
CNNSM
p
0.54 0.61 0.49 0.20
PARALEX 0.54 0.77 0.42 0.22
Table 1: Performance of two variations of our sys-
tems, compared with the PARALEX system.
the answers from the ReVerb database. Because
our systems might find triples that were not re-
turned by the PARALEX systems, we labeled these
new question?triple pairs ourselves.
Given a question, the system first enumerated
all possible decompositions of the mentions and
patterns, as described earlier. We then computed
the similarity scores between the pattern and all
relations in the KB and retained 150 top-scoring
relation candidates. For each selected relation, the
system then checked all triples in the KB that had
this relation and computed the similarity score be-
tween the mention and corresponding argument
entity. The product of the probabilities of these
two models, which are derived from the cosine
similarity scores using softmax as described in
Sec. 4, was used as the final score of the triple for
ranking the answers. The top answer triple was
used to compute the precision and recall of the sys-
tem when reporting the system performance. By
limiting the systems to output only answer triples
with scores higher than a predefined threshold, we
could control the trade-off between recall and pre-
cision and thus plot the precision?recall curve.
Table 1 shows the performance in F
1
, preci-
sion, recall and mean average precision of our sys-
tems and PARALEX. We provide two variations
here. CNNSM
pm
is the full system and consists
of two semantic similarity models for pattern?
relation and mention?entity. The other model,
CNNSM
p
, only measures the similarity between
the patterns and relations, and maps a mention to
an entity when they have the same surface form.
Since the trade-off between precision and re-
call can be adjusted by varying the threshold, it
is more informative to compare systems on the
precision?recall curves, which are shown in Fig-
ure 3. As we can observe from the figure, the
precision of our CNNSM
pm
system is consistently
higher than PARALEX across all recall regions.
The CNNSM
m
system also performs similarly to
CNNSM
pm
in the high precision regime, but is in-
ferior when recall is higher. This is understandable
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6
Pre
cis
ion
Recall
  CNNSMpm  CNNSMp  Paralex
Figure 3: The precision?recall curves of the two
variations of our systems and PARALEX.
since the system does not match mentions with
entities of different surface forms (e.g., ?Robert
Hooke? to ?Hooke?). Notice that the highest F
1
values of them are 0.61 and 0.56, compared to
0.54 of PARALEX. Tuning the thresholds using a
validation set would be needed if there is a metric
(e.g., F
1
) that specifically needs to be optimized.
6 Conclusions
In this work, we propose a semantic parsing
framework for single-relation questions. Com-
pared to the existing work, our key insight is to
match relation patterns and entity mentions using
a semantic similarity function rather than lexical
rules. Our similarity model is trained using convo-
lutional neural networks with letter-trigrams vec-
tors. This design helps the model go beyond bag-
of-words representations and handles the OOV is-
sue. Our method achieves higher precision on the
QA task than the previous work, PARALEX, con-
sistently at different recall points.
Despite the strong empirical performance, our
system has room for improvement. For in-
stance, due to the variety of entity mentions in
the real world, the parallel corpus derived from
the WikiAnswers data and ReVerb KB may not
contain enough data to train a robust entity link-
ing model. Replacing this component with a
dedicated entity linking system could improve
the performance and also reduce the number of
pattern/mention candidates when processing each
question. In the future, we would like to extend
our method to other more structured KBs, such as
Freebase, and to explore approaches to extend our
system to handle multi-relation questions.
647
References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533?1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Qingqing Cai and Alexander Yates. 2013. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 423?433,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research.
Scott Deerwester, Susan Dumais, Thomas Landauer,
George Furnas, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference of Em-
pirical Methods in Natural Language Processing
(EMNLP ?11), Edinburgh, Scotland, UK, July 27-
31.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1608?1618,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion & knowledge management, pages 2333?2338.
ACM.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969?978.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw
Hill.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014a. A convolutional latent
semantic model for web search. Technical Report
MSR-TR-2014-55, Microsoft Research.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014b. Learning semantic
representations using convolutional neural networks
for web search. In Proceedings of the Companion
Publication of the 23rd International Conference on
World Wide Web Companion, pages 373?374.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Machine Learn-
ing: ECML 2001, pages 466?477. Springer.
Gokhan Tur, Li Deng, Dilek Hakkani-Tur, and Xi-
aodong He. 2012. Towards deeper understanding:
deep convex networks for semantic utterance classi-
fication. In Acoustics, Speech and Signal Processing
(ICASSP), 2012 IEEE International Conference on,
pages 5045?5048. IEEE.
John Zelle and Raymond Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the National Confer-
ence on Artificial Intelligence, pages 1050?1055.
648
Transactions of the Association for Computational Linguistics, 1 (2013) 207?218. Action Editor: Ben Taskar.
Submitted 10/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Dual Coordinate Descent Algorithms for Efficient
Large Margin Structured Prediction
Ming-Wei Chang Wen-tau Yih
Microsoft Research
Redmond, WA 98052, USA
{minchang,scottyih}@microsoft.com
Abstract
Due to the nature of complex NLP problems,
structured prediction algorithms have been
important modeling tools for a wide range of
tasks. While there exists evidence showing
that linear Structural Support Vector Machine
(SSVM) algorithm performs better than struc-
tured Perceptron, the SSVM algorithm is still
less frequently chosen in the NLP community
because of its relatively slow training speed.
In this paper, we propose a fast and easy-to-
implement dual coordinate descent algorithm
for SSVMs. Unlike algorithms such as Per-
ceptron and stochastic gradient descent, our
method keeps track of dual variables and up-
dates the weight vector more aggressively. As
a result, this training process is as efficient as
existing online learning methods, and yet de-
rives consistently better models, as evaluated
on four benchmark NLP datasets for part-of-
speech tagging, named-entity recognition and
dependency parsing.
1 Introduction
Complex natural language processing tasks are in-
herently structured. From sequence labeling prob-
lems like part-of-speech tagging and named entity
recognition to tree construction tasks like syntactic
parsing, strong dependencies exist among the la-
bels of individual components. By modeling such
relations in the output space, structured output pre-
diction algorithms have been shown to outperform
significantly simple binary or multi-class classi-
fiers (Lafferty et al, 2001; Collins, 2002; McDonald
et al, 2005).
Among the existing structured output prediction
algorithms, the linear Structural Support Vector
Machine (SSVM) algorithm (Tsochantaridis et al,
2004; Joachims et al, 2009) has shown outstanding
performance in several NLP tasks, such as bilingual
word alignment (Moore et al, 2007), constituency
and dependency parsing (Taskar et al, 2004b; Koo
et al, 2007), sentence compression (Cohn and La-
pata, 2009) and document summarization (Li et al,
2009). Nevertheless, as a learning method for NLP,
the SSVM algorithm has been less than popular al-
gorithms such as the structured Perceptron (Collins,
2002). This may be due to the fact that cur-
rent SSVM implementations often suffer from sev-
eral practical issues. First, state-of-the-art imple-
mentations of SSVM such as cutting plane meth-
ods (Joachims et al, 2009) are typically compli-
cated.1 Second, while methods like stochastic gradi-
ent descent are simple to implement, tuning learning
rates can be difficult. Finally, while SSVM mod-
els can achieve superior accuracy, this often requires
long training time.
In this paper, we propose a novel optimiza-
tion method for efficiently training linear SSVMs.
Our method not only is easy to implement, but
also has excellent training speed, competitive with
both structured Perceptron (Collins, 2002) and
MIRA (Crammer et al, 2005). When evaluated on
several NLP tasks, including POS tagging, NER and
dependency parsing, this optimization method also
outperforms other approaches in terms of prediction
accuracy. Our final algorithm is a dual coordinate
1Our algorithm is easy to implement mainly because we use
the square hinge loss function.
207
descent (DCD) algorithm for solving a structured
output SVM problem with a 2-norm hinge loss func-
tion. The algorithm consists of two main compo-
nents. One component behaves analogously to on-
line learning methods and updates the weight vector
immediately after inference is performed. The other
component is similar to the cutting plane method
and updates the dual variables (and the weight vec-
tor) without running inference. Conceptually, this
hybrid approach operates at a balanced trade-off
point between inference and weight update, per-
forming better than with either component alone.
Our contributions in this work can be summarized
as follows. Firstly, our proposed algorithm shows
that even for structured output prediction, an SSVM
model can be trained as efficiently as a structured
Perceptron one. Secondly, we conducted a careful
experimental study on three NLP tasks using four
different benchmark datasets. When compared with
previous methods for training SSVMs (Joachims
et al, 2009), our method achieves similar perfor-
mance using less training time. When compared to
commonly used learning algorithms such as Percep-
tron and MIRA, the model trained by our algorithm
performs consistently better when given the same
amount of training time. We believe our method can
be a powerful tool for many different NLP tasks.
The rest of our paper is organized as follows.
We first describe our approach by formally defining
the problem and notation in Sec. 2, where we also
review some existing, closely-related structured-
output learning algorithms and optimization tech-
niques. We introduce the detailed algorithmic de-
sign in Sec. 3. The experimental comparisons of
variations of our approach and the existing methods
on several NLP benchmark tasks and datasets are re-
ported in Sec. 4. Finally, Sec. 5 concludes the paper.
2 Background and Related Work
We first introduce notations used throughout this pa-
per. An input example is denoted by x and an out-
put structure is denoted by y. The feature vector
?(x,y) is a function defined over an input-output
pair (x,y). We focus on linear models with predic-
tions made by solving the decoding problem:
arg max
y?Y(xi)
wT?(xi,y). (1)
The set Y(xi) represents all possible (exponentially
many) structures that can be generated from the ex-
ample xi. Let yi be the true structured label of xi.
The difference between the feature vectors of the
correct label yi and y is denoted as ?yi,y(xi) ?
?(xi,yi) ? ?(xi,y). We define ?(yi,y) as a dis-
tance function between two structures.
2.1 Perceptron and MIRA
Structured Perceptron First introduced by
Collins (2002), the structured Perceptron algorithm
runs two steps iteratively: first, it finds the best
structured prediction y for an example with the
current weight vector using Eq. (1); then the
weight vector is updated according to the difference
between the feature vectors of the true label and
the prediction: w ? w + ?yi,y(xi). Inspired by
Freund and Schapire (1999), Collins (2002) also
proposed the averaged structured Perceptron, which
maintains an averaged weight vector throughout the
training procedure. This technique has been shown
to improve the generalization ability of the model.
MIRA The Margin Infused Relaxed Algo-
rithm (MIRA), which was introduced by Crammer
et al (2005), explicitly uses the notion of margin to
update the weight vector. The MIRA updates the
weight vector by calculating the step size using
min
w
1
2?w ?w0?
2
S.T. wT?yi,y(xi) ? ?(y,yi),?y ? Hk,
where Hk is a set containing the best-k structures
according to the weight vector w0. MIRA is a
very popular method in the NLP community and has
been applied to NLP tasks like word segmentation
and part-of-speech tagging (Kruengkrai et al, 2009),
NER and chunking (Mejer and Crammer, 2010) and
dependency parsing (McDonald et al, 2005).
2.2 Structural SVM
Structural SVM (SSVM) is a maximum margin
model for the structured output prediction setting.
Training SSVM is equivalent to solving the follow-
ing global optimization problem:
min
w
?w?2
2 + C
l?
i=1
L(xi,yi,w), (2)
208
where l is the number of labeled examples and
L(xi,yi,w) = `
(
max
y
[
?(yi,y)?wT?yxi,y(xi)
])
The typical choice of ` is `(a) = at. If t = 2 is used,
we refer to the SSVM defined in Eq. (2) as the L2-
Loss SSVM. If hinge loss (t = 1) is used in Eq. (2),
we refer to it as the L1-Loss SSVM. Note that the
function ? is not only necessary,2 but also enables
us to use more information on the differences be-
tween the structures in the training phase. For ex-
ample, using Hamming distance for sequence label-
ing is a reasonable choice, as the model can express
finer distinctions between structures yi and y.
When training an SSVM model, we often need to
solve the loss-augmented inference problem,
arg max
y?Y(xi)
[
wT?(xi,y) + ?(yi,y)
]
. (3)
Note that it is a different inference problem than the
decoding problem in Eq. (1).
Algorithms for training SSVM Cutting
plane (CP) methods (Tsochantaridis et al, 2004;
Joachims et al, 2009) have been the dominant
method for learning the L1-Loss SSVM. Eq. (2)
contains an exponential number of constraints.
The cutting plane (CP) methods iteratively select
a subset of active constraints for each example
then solve a sub-problem which contains active
constraints to improve the model. CP has proven
useful for solving SSVMs. For instance, Yu and
Joachims (2009) proposed using CP methods to
solve a 1-slack variable formulation, and showed
that solving for a 1-slack variable formulation
is much faster than solving the l-slack variable
one (Eq. (2)). Chang et al (2010) also proposed
a variant of cutting plane method for solving the
L2-Loss SSVM. This method uses a dual coordinate
descent algorithm to solve the sub-problems. We
call their approach the CPD method.
Several other algorithms also aim at solv-
ing the L1-Loss SSVM. Stochastic gradient de-
scent (SGD) (Bottou, 2004; Shalev-Shwartz et al,
2007) is a technique for optimizing general con-
vex functions and has been applied to solving the
2Without ?(y,yi) in Eq. 2, the optimal w would be zero.
L1-Loss SSVM (Ratliff et al, 2007). Taskar et
al. (2004a) proposed a structured SMO algorithm.
Because the algorithm solves the dual formulation
of the L1-Loss SSVM, it requires picking a vio-
lation pair for each update. In contrast, because
each dual variable can be updated independently in
our DCD algorithm, the implementation is relatively
simple. The extragradient algorithm has also been
applied to solving the L1-Loss SSVM (Taskar et al,
2005). Unlike our DCD algorithm, the extragradient
method requires the learning rate to be specified.
The connections between dual methods and the
online algorithms have been previously discussed.
Specifically, Shalev-Shwartz and Singer (2006) con-
nects the dual methods to a wide range of online
learning algorithms. In (Martins et al, 2010), the au-
thors apply similar techniques on L1-Loss SSVMs
and show that the proposed algorithm can be faster
than the SGD algorithm.
Exponentiated Gradient (EG) descent (Kivinen
and Warmuth, 1995; Collins et al, 2008) has re-
cently been applied to solving the L1-Loss SSVM.
Compared to other SSVM learners, EG requires
manual tuning of the step size. In addition, EG re-
quires solution of the sum-product inference prob-
lem, which can be more expensive than solving
Eq. (3) (Taskar et al, 2006). Very recently, Lacoste-
Julien et al (2013) proposed a block-coordinate de-
scent algorithm for the L1-Loss SSVM based on the
Frank-Wolfe algorithm (FW-Struct), which has been
shown to outperform the EG algorithm significantly.
Similar to our DCD algorithm, FW calculates the
optimal learning rate when updating the dual vari-
ables.
The Sequential Dual Method (SDM) (Shevade et
al., 2011) is probably the most related to this paper.
SDM solves the L1-Loss SSVM problem using mul-
tiple updating policies, which is similar to our ap-
proach. However, there are several important differ-
ences in the detailed algorithmic design. As will be
clear in Sec. 3, our dual coordinate descent (DCD)
algorithm is very simple, while SDM (which is not
a DCD algorithm) uses a complicated procedure to
balance different update policies. By targeting the
L2-Loss SSVM formulation, our methods can up-
date the weight vector more efficiently, since there
are no equality constraints in the dual.
209
3 Dual Coordinate Descent Algorithms for
Structural SVM
In this work, we focus on solving the dual of linear
L2-Loss SSVM, which can be written as follows:
min
?i,y?0
1
2?
?
i,y
?i,y?yi,y(xi)?2 (4)
+ 14C
?
i
(
?
y?Y(xi)
?i,y)2 ?
?
i,y
?(y,yi)?i,y.
In the above equation, a dual variable ?i,y is asso-
ciated with a structure y ? Y(xi). Therefore, the
total number of dual variables can be quite large: its
upper bound is lB, where B = maxi |Y(xi)|.
The connection between the dual variables and
the weight vector w at optimal solutions is through
the following equation:
w =
l?
i=1
?
y?Y(xi)
?i,y?yi,y(xi). (5)
Advantages of L2-Loss SSVM The use of the
2-norm hinge loss function eliminates the need of
equality constraints3; only non-negative constraints
(?i,y ? 0) remain. This is important because now
each dual variable can be updated without changing
values of the other dual variables. We can then up-
date one single dual variable at a time. As a result,
this dual formulation allows us to design a simple
and principled dual coordinate descent (DCD) opti-
mization method.
DCD algorithms consist of two iterative steps:
1. Pick a dual variable ?i,y.
2. Update the dual variable and the weight vector.
Go to 1.
In the normal binary classification case, how to
select dual variables to solve is not an issue as
choosing them randomly works effectively in prac-
tice (Hsieh et al, 2008). However, this is not a prac-
tical scheme for training SSVM models given that
the number of dual variables in Eq. (4) can be very
large because of the exponentially many legitimate
output structures. To address this issue, we intro-
duce the concept of working set below.
3For L1-Loss SSVM, there are the equality constraints:?
y?Y(xi) ?i,y = C, ?i.
Working Set The number of non-zero variables in
the optimal ? can be small when solving Eq. (4).
Hence, it is often feasible to use a small working set
Wi for each example to keep track of the structures
for non-zero ??s. More formally,
Wi = {y | ?y ? Y(xi), ?i,y > 0}.
Intuitively, the working set Wi records the output
structures that are similar to the true structure yi.We
set al dual variables to be zero initially (therefore,
w = 0 as well), soWi = ? for all i. Then the algo-
rithm starts to build the working set in the training
procedure. After training, the weight vector is com-
puted using dual variables in the working set and
thus equivalent to
w =
l?
i=1
?
y?Wi
?i,y?yi,y(xi). (6)
Connections to Structured Perceptron The pro-
cess of updating a dual variable is in fact very simi-
lar to the update rule used in Perceptron and MIRA.
Take structured Perceptron for example, its weight
vector can be determined using the following equa-
tion:
wperc =
l?
i=1
?
y??(xi)
?i,y?yi,y(xi), (7)
where ?(xi) is the set containing all structures Per-
ceptron predicts for xi during training, and ?i,y is
the number of times Perceptron predicts y for xi
during training. By comparing Eq. (6) and Eq. (7), it
is clear that SSVM is just a more principled way to
update the weight vector, as ??s are computed based
on the notion of margin.4
Updating Dual Variables and Weights After
picking a dual variable ?i,y?, we first show how to
update it optimally. Recall that a dual variable ?i,y?
is associated with the i-th example and a structure y?.
The optimal update size d for ?i,y? can be calculated
analytically from the following optimization prob-
4Of course, Wi could be very different from ?(xi), the con-
struction of the working sets will be discussed in Sec. 3.1.
210
Algorithm 1 UPDATEWEIGHT(i,w):
Update the weight vector w and the dual variables
in the working set of the i-th example. C is the reg-
ularization parameter defined in Eq. (2).
1: Shuffle the elements inWi (but retain the newest
member of the working set to be updated first.
See Theorem 1 for the reasons.)
2: for y? ? Wi do
3: d? ?(y?,yi)?w
T?yi,y?(xi)?
?
y?Wi ?i,y
2C
??yi,y?(xi)?2+ 12C
4: ?? ? max(?i,y? + d, 0)
5: w? w + (?? ? ?i,y?)?yi,y?(xi)
6: ?i,y? ? ??
7: end for
lem (derived from Eq. (4)):
min
d???i,y?
1
2?w + d?yi,y?(x)?
2+
1
4C (d+
?
y?Wi
?i,y)2 ? d?(yi, y?), (8)
where the w is defined in Eq. (6). Compared to
stochastic gradient descent, DCD algorithms keep
track of dual variables and do not need to tune the
learning rate.
Instead of updating one dual variable at a time,
our algorithm updates all dual variables once in the
working set. This step is important for the conver-
gence of the DCD algorithms.5 The exact update
algorithm is presented in Algorithm 1. Line 3 cal-
culates the optimal step size (the analytical solution
to the above optimization problem). Line 4 makes
sure that dual variables are non-negative. Lines 5
and 6 update the weight vectors and the dual vari-
ables. Note that every update ensures Eq. (4) to be
no greater than the original value.
3.1 Two DCD Optimization Algorithms
Now we are ready to present two novel DCD algo-
rithms for L2-Loss SSVM: DCD-Light and DCD-
SSVM.
3.1.1 DCD-Light
The basic idea of DCD-Light is just like online
learning algorithms. Instead of doing inference for
5Specifically, updating all of the structures in the working
set is a necessary condition for our algorithms to converge.
the whole batch of examples before updating the
weight vector in each iteration, as done in CPD and
1-slack variable formulation of SVM-Struct, DCD-
Light updates the model weights after solving the in-
ference problem for each individual example. Algo-
rithm 2 depicts the detailed steps. In Line 5, the loss-
augmented inference (Eq. (3)) is performed; then
the weight vector is updated in Line 9 ? all of the
structures and dual variables in the working set are
used to update the weight vector. Note that there is
a ? parameter in Line 6 to control how precise we
would like to solve this SSVM problem. As sug-
gested in (Hsieh et al, 2008), we shuffle the exam-
ples in each iteration (Line 3) as it helps the algo-
rithm converge faster.
DCD-Light has several noticeable differences
when compared to the most popular online learn-
ing method, averaged Perceptron. First, DCD-Light
performs the loss-augmented inference (Eq. (3)) at
Line 5 instead of the argmax inference (Eq. (1)).
Second, the algorithm updates the weight vector
with all structures in the working set. Finally, DCD-
light does not average the weight vectors.
3.1.2 DCD-SSVM
Observing that DCD-Light does not fully utilize
the saved dual variables in the working set, we pro-
pose a hybrid approach called DCD-SSVM, which
combines ideas from DCD-Light and cutting plane
methods. In short, after running the updates on a
batch of examples, we refine the model by solving
the dual variables further in the current working sets.
The key advantage of keeping track of these dual
variables is that it allows us to update the saved dual
variables without performing any inference, which
is often an expensive step in structured prediction
algorithms.
DCD-SSVM is summarized in Algorithm 3.
Lines 10 to 16 are from DCD-Light. In Lines 3 to
8, we grab the idea from cutting plane methods by
updating the weight vector using the saved dual vari-
ables in the working sets without any inference (note
that Lines 3 to 8 do not have any effect at the first
iteration). By revisiting the dual variables, we can
derive a better intermediate model, resulting in run-
ning the inference procedure less frequently. Similar
to DCD-Light, we also shuffle the examples in each
iteration.
211
Algorithm 2 DCD-Light: The lightweight dual co-
ordinate descent algorithm for optimizing Eq. (4).
1: w? 0,Wi ? ?,?i
2: for t = 1 . . . T do
3: Shuffle the order of the training examples
4: for i = 1 . . . l do
5: y?? arg maxy wT?(xi,y) + ?(y,yi)
6: if ?(y?,yi)?wT?yi,y?(xi)?
?
y?Wi
?i,y
2C ? ?then
7: Wi ?Wi ? {y?}
8: end if
9: UPDATEWEIGHT(i,w) {Algo. 1}
10: end for
11: end for
DCD algorithms are similar to column generation
algorithms for linear programming (Desrosiers and
Lu?bbecke, 2005), where the master problem is to
solve the dual problem that focuses on the variables
in the working sets, and the subproblem is to find
new variables for the working sets. In Sec. 4, we
will demonstrate the importance of balancing these
two problems by comparing DCD-SSVM and DCD-
Light.
3.2 Convergence Analysis
We now present the theoretic analysis of both DCD-
Light and DCD-SSVM, and address two main top-
ics: (1) whether the working sets will grow expo-
nentially and (2) the convergence rate. Due to the
lack of space, we show only the main theorems.
Leveraging Theorem 5 in (Joachims et al, 2009),
we can prove that the DCD algorithms only add a
limited number of variables in the working sets, and
have the following theorem.
Theorem 1. The number of times that DCD-Light
or DCD-SSVM adds structures into working sets is
bounded by O
(2(R2+ 12C )lC?2
?2
)
, where R2 is de-
fined as maxi,y? ??yi,y?(xi)?2, and ? is the upper
bound of ?(yi, y?), ?yi, y? ? Y(xi).
We discuss next the convergence rates of our
DCD algorithms under two different conditions ?
when the working sets are fixed and the general case.
If the working sets are fixed in DCD algorithms,
they become cyclic dual coordinate descent meth-
Algorithm 3 DCD-SSVM: a hybrid dual coor-
dinate descent algorithm that combines ideas from
DCD-Light and cutting plane algorithms.
1: w? 0,Wi ? ?,?i
2: for t = 1 . . . T do
3: for j = 1 . . . r do
4: Shuffle the order of the training examples
5: for i = 1 . . . l do
6: UPDATEWEIGHT(i,w) {Algo. 1}
7: end for
8: end for
9: Shuffle the order of the training examples
10: for i = 1 . . . l do
11: y?? arg maxy wT?(xi,y) + ?(y,yi)
12: if ?(y?,yi)?wT?yi,y?(xi)?
?
y?Wi
?i,y
2C ? ?then
13: Wi ?Wi ? {y?}
14: end if
15: UPDATEWEIGHT(i,w) {Algo. 1}
16: end for
17: end for
ods. In this case, we denote the minimization prob-
lem Eq. (4) as F (?). For fixed working sets {Wi},
we denote FS(?) as the minimization problem that
focuses on the dual variables in the working set only.
By applying the results from (Luo and Tseng, 1993;
Wang and Lin, 2013) to L2-Loss SSVM, we have
the following theorem.
Theorem 2. For any given non-empty working sets
{Wi}, if the DCD algorithms do not extend the
working sets (i.e., line 6-8 in Algorithm 2 are not ex-
ecuted), then the DCD algorithms will obtain the -
optimal solution for FS(?) in O(log(1 )) iterations.
Based on Theorem 1 and Theorem 2, we have the
following theorem.
Theorem 3. DCD-SSVM obtains an -optimal solu-
tion in O( 12 log(1 )) iterations.
To the best of our knowledge, this is the first con-
vergence analysis result for L2-Loss SSVM. Com-
pared to other theoretic analysis results for L1-Loss
SSVM, a tighter bound might exist given a better
theoretic analysis. We leave this for future work.6
6Noticeably, the use of working sets complicates the theo-
212
0 20 40 60 80 10078
80
82
84
86
Training Time (seconds)
Tes
tF1
(a) Test F1 vs. Time in NER-CoNLL
0 100 200 300 400
96.6
96.8
97
97.2
Training Time (seconds)
Tes
tA
cc
(b) Test Acc vs. Time in POS
0 2,000 4,000 6,00086
88
90
Training Time (seconds)
Tes
tA
cc
(c) Test Acc vs. Time in DP-WSJ
0 20 40 60 80 100
2,000
4,000
6,000
Training Time (seconds)
Prim
alO
bjec
tive
Val
ue
(d) Primal Objective Value in NER-CoNLL
0 100 200 300 400 500
1
1.5
2 ?104
Training Time (seconds)
Prim
alO
bjec
tive
Val
ue
(e) Primal Objective Value in POS
0 2,000 4,000 6,000
2
4
6 ?104
Training Time (seconds)
Prim
alO
bjec
tive
Val
ue
DCD-SSVM
DCD-Light
CPD
(f) Primal Objective Value in DP-WSJ
Figure 1: We plot the testing performance (top row) and the primal objective function (bottom row) versus training
time for three optimization methods for learning the L2-Loss SSVM. In general, DCD-SSVM is the best algorithm for
both the objective function and the testing performance.
4 Experiments
In order to verify the effectiveness of the proposed
algorithm, we conduct a set of experiments on dif-
ferent optimization and learning algorithms. Before
going to the experimental results, we first introduce
the tasks and settings used in the experiments.
4.1 Tasks and Data
We evaluated our method and existing structured
output learning approaches on named entity recog-
nition (NER), part-of-speech tagging (POS) and de-
pendency parsing (DP) on four benchmark datasets.
NER-MUC7 MUC-7 data contains a subset of
North American News Text Corpora annotated with
many types of entities. We followed the settings
in (Ratinov and Roth, 2009) and consider three main
entities categories: PER, LOC and ORG. We evalu-
ated the results using phrase-level F1.
retic analysis significantly. Also note that Theorem 2 shows
that if we put all possible structures in the working sets (i.e.,
F (?) = FS(?)), then the DCD algorithms can obtain -optimal
solution in O(log( 1 )) iterations.
NER-CoNLL This is the English dataset from the
CoNLL 2003 shared task (T. K. Sang and De Meul-
der, 2003). The data set labels sentences from the
Reuters Corpus, Volume 1 (Lewis et al, 2004) with
four different entity types: PER, LOC, ORG and
MISC. We evaluated the results using phrase-level
F1.
POS-WSJ The standard set for evaluating the per-
formance of a part-of-speech tagger. The training,
development and test sets consist of sections 0-18,
19-21 and 22-24 of the Penn Treebank data (Marcus
et al, 1993), respectively. We evaluated the results
by token-level accuracy.
DP-WSJ We took sections 02-21 of Penn Tree-
bank as the training set, section 00 as the develop-
ment set and section 23 as the test set. We imple-
ment a simple version of hash kernel to speed up of
training procedure for this task (Bohnet, 2010). We
reported the unlabeled attachment accuracy for this
task (McDonald et al, 2005).
213
0 10 20 3076
77
78
79
80
Training Time (seconds)
Tes
tF1
(a) Test F1 vs. Time in NER-MUC7
0 20 40 60 80 100 120
80
82
84
86
Training Time (seconds)
Tes
tF1
(b) Test F1 vs. Time in NER-CoNLL
0 200 400 60095
96
97
Training Time (seconds)
Tes
tA
cc
DCD-SSVM
FW-Struct
SVM-Struct
(c) Test Acc vs. Time in POS-WSJ
Figure 2: Comparisons between the testing performance of DCD-SSVM, FW-Struct and SVM-Struct. Note that
DCD-SSVM often obtain a better model with much less training time when comparing to SVM-Struct.
4.2 Features and Inference Algorithms
For the sequence labeling tasks, NER and POS,
we followed the discriminative HMM settings used
in (Joachims et al, 2009) and defined the features as
?(x,y) =
N?
i=l
?
?????
?emi(xi, yi)
[yi = 1][yi?1 = 1]
[yi = 1][yi?1 = 2]
. . .
[yi = k][yi?1 = k]
?
?????
,
where ?emi is the feature vector dedicated to the i-th
token (or, the emission features), N represents the
number of tokens in this sequence, yi represents the
i-th token in the sequence y, [yi = 1] is the indictor
variable and k is the number of tags.
The inference problems are solved by the Viterbi
algorithm. The emission features used in both POS
and NER are the standard ones, including word fea-
tures, word-shape features, etc. For NER, we used
additional simple gazetteer features7 and word clus-
ter features (Turian et al, 2010)
For dependency parsing, we followed the setting
described in (McDonald et al, 2005) and used sim-
ilar features. The decoding algorithm is the first-
order Eisner?s algorithm (Eisner, 1997).
4.3 Algorithms and Implementation Detail
For all SSVM algorithms (including SGD), C was
chosen among the set {0.01, 0.05, 0.1, 0.5, 1, 5} ac-
cording to the accuracy/F1 on the development set.
For each task, the same features were used by all
7Adding Wikipedia gazetteers would likely increase the per-
formance significantly (Ratinov and Roth, 2009).
algorithms. For NER-MUC7, NER-CoNLL and
POS-WSJ, we ran the online algorithms and DCD-
SSVM for 25 iterations. For DP-WSJ, we only let
the algorithms run for 10 iterations as the inference
procedure is very expensive computationally. The
algorithms in the experiments are:
DCD Our dual coordinate descent method on the
L2-Loss SSVM. For DCD-SSVM, r is set to be 5.
For both DCD-Light and DCD-SSVM , we follow
the suggestion in (Joachims et al, 2009): if the value
of a dual variable becomes zero, its corresponding
structure will be removed from the working set to
improve the speed.
SVM-Struct We used the latest (v3.10) of SVM-
HMM.8 This version uses the cutting plane method
on a 1-slack variable formulation (Joachims et al,
2009) for the L1-Loss SSVM. SVM-Struct was im-
plemented in C and all the other algorithms are im-
plemented in C#. We did not apply SVM-Struct to
DP-WSJ because there is no native implementation.
Perceptron This refers to the averaged structured
Perceptron method introduced by Collins (2002). To
speed up the convergence rate, we shuffle the train-
ing examples at each iteration.
MIRA Margin Infused Relaxed Algorithm
(MIRA) (Crammer et al, 2005) is the online
learning algorithm that explicitly uses the notion
of margin to update the weight vector. We use
1-best MIRA in our experiments. To increase
8http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
214
the convergence speed, we shuffle the training
examples at each iteration. Following (McDonald et
al., 2005), we did not tune the C parameter for the
MIRA algorithm.
SGD Stochastic gradient descent (SGD) (Bottou,
2004) is a technique for optimizing general convex
functions. In this paper, we use SGD as an alterna-
tive baseline for optimizing the L1-Loss SSVM ob-
jective function (Eq. (2) with higne loss).9 When us-
ing SGD, the learning rate must be carefully tuned.
Following (Bottou, 2004), the learning rate is ob-
tained by
?0
(1.0 + (?0T/C))0.75
,
where C is the regularization parameter, T is the
number of updates so far and ?0 is the initial step
size. The parameter ?0 was selected among the set
{2?1, 2?2, 2?3, 2?4, 2?5} by running the SGD al-
gorithm on a set of 1000 randomly sampled exam-
ples, and then choosing the ?0 with lowest primal
objective function on these examples.
FW-Struct FW-Struct represents the Frank-Wolfe
algorithm for the L1-Loss SSVM (Lacoste-Julien et
al., 2013).
In order to improve the training speed, we cached
all the feature vectors generated by the gold la-
beled data once computed. This applied to all al-
gorithms except SVM-Struct, which has its own
caching mechanism. We report the performance
of the averaged weight vectors for Perceptron and
MIRA.
4.4 Results
We present the experimental results below on com-
paring different dual coordinate descent methods,
as well as comparing our main algorithm, DCD-
SSVM, with other structured learning approaches.
4.4.1 Comparisons of DCD Methods
We compared three DCD methods: DCD-Light,
DCD-SSVM and CPD. CPD is a cutting plane
method proposed by Chang et al (2010), which uses
9To compare with SGD using its best setting, we report only
the results of SGD on the L1-Loss SSVM as we found tuning
the step size for the L2-Loss SSVM is more difficult.
a dual coordinate descent algorithm to solve the in-
ternal sub-problems. We specifically included CPD
as it also targets at the L2-Loss SSVM.
Because different optimization strategies will
reach the same objective values eventually, compar-
ing them on prediction accuracy of the final models
is not meaningful. Instead, here we compare how
fast each algorithm converges as shown in Figure 1.
Each marker on the line in this figure represents one
iteration of the corresponding algorithm. Generally
speaking, CPD improves the model very slowly in
the early stages, but much faster after several iter-
ations. In comparison, DCD-Light often behaves
much better initially, and DCD-SSVM is generally
the most efficient algorithm here.
The reason behind the slow performance of CPD
is clear. During early rounds of the algorithm,
the weight vector is far from optimal, so it spends
too much time using ?bad? weight vectors to find
the most violated structures. On the other hand,
DCD-Light updates the weight vector more fre-
quently, so it behaves much better in general. DCD-
SSVM spends more time on updating models during
each batch, but keeps the same amount of time doing
inference as DCD-Light. As a result, it finds a better
trade-off between inference and learning.
4.4.2 DCD-SSVM, SVM-Struct and FW-Struct
Joachims et al (2009) proposed a 1-slack vari-
able method for the L1-Loss SSVM. They showed
that solving a 1-slack variable formulation is an
order-of-magnitude faster than solving the original
formulation (l-slack variables formulation). Nev-
ertheless, from Figure 2, we can see the clear ad-
vantage of DCD-SSVM over SVM-Struct. Al-
though using 1-slack variable has improved the
learning speed, SVM-Struct still converges slower
than DCD-SSVM. In addition, the performance of
models trained by SVM-Struct in the early stage is
quite unstable, which makes early stopping an in-
effective strategy in practice when training time is
limited.
We also compared our algorithms to FW-Struct.
Our results agree with (Lacoste-Julien et al, 2013),
which shows that the FW-Struct outperforms the
SVM-Struct. In our experiments, we found that our
DCD algorithms were competitive, sometimes con-
verged faster than the FW-Struct.
215
0 10 20 3076
77
78
79
80
Training Time (seconds)
Tes
tF1
(a) Test F1 vs. Time in NER-MUC7
0 100 200 300 400
96.6
96.8
97
97.2
Training Time (seconds)
Tes
tA
cc
(b) Test Acc vs. Time in POS-WSJ
0 2,000 4,000 6,00088
89
90
91
Training Time (seconds)
Tes
tA
cc
DCD-SSVM
PERP
MIRA
SGD
(c) Test Acc vs. Time in DP-WSJ
Figure 3: Comparisons between DCD-SSVM and popular online learning algorithms. Note that the results diverge
when comparing Perceptron and MIRA. In general, DCD-SSVM is the most stable algorithm.
Task/Data DCD Percep MIRA SGD
NER-MUC7 79.4 78.5 78.8 77.8
NER-CoNLL 85.6 85.3 85.1 84.2
POS-WSJ 97.1 96.9 96.9 96.9
DP-WSJ 90.8 90.3 90.2 90.9
Table 1: Performance of online learning algorithms and
the DCD-SSVM algorithm on the testing sets. NER is
measured by F1 while others by accuracy.
4.4.3 DCD-SSVM, MIRA, Perceptron and
SGD
As in binary classification, large-margin methods
like SVMs often perform better than algorithms like
Perceptron and SGD (Hsieh et al, 2008; Shalev-
Shwartz and Zhang, 2013), here we observe similar
behaviors in the structured output domain. Table 1
shows the final test accuracy numbers or F1 scores of
models trained by algorithms including Perceptron,
MIRA and SGD, compared to those of the SSVM
models trained by DCD-SSVM. Among the bench-
mark datasets and tasks we have experimented with,
DCD-SSVM derived the most accurate models, ex-
cept for DP-WSJ when compared to SGD.
Perhaps a more interesting comparison is on
the training speed, which can be observed in Fig-
ure 3. Compared to other online algorithms, DCD-
SSVM can take advantage of cached dual variables
and structures. We show that the training speed of
DCD-SSVM can be competitive to that of the on-
line learning algorithms, unlike SVM-Struct. Note
that SGD is not very stable for NER-MUC7, even
though we tuned the step size very carefully.
5 Conclusion
In this paper, we present a novel approach for learn-
ing the L2-Loss SSVM model. By combining the
ideas of dual coordinate descent and cutting plane
methods, the hybrid approach, DCD-SSVM outper-
forms other SSVM training methods both in terms
of objective value reduction and testing error rate
reduction. As demonstrated in our experiments
on several NLP tasks, our approach also tends to
learn more accurate models than commonly used
structured learning algorithms, including structured
Perceptron, MIRA and SGD. Perhaps more inter-
estingly, our SSVM learning method is very effi-
cient: the model training time is competitive to on-
line learning algorithms such as structured Percep-
tron and MIRA. These unique qualities make DCD-
SSVM an excellent choice for solving a variety of
complex NLP problems.
In the future, we would like to compare our algo-
rithm to other structured prediction approaches, such
as conditional random fields (Lafferty et al, 2001)
and exponential gradient descent methods (Collins
et al, 2008). Expediting the learning process fur-
ther by leveraging approximate inference is also an
interesting direction to investigate.
Acknowledgments
We sincerely thank John Platt, Lin Xiao and Kaiwei Chang for
the discussions and feedback. We are grateful to Po-Wei Wang
and Chih-Jen Lin for providing their work on convergence rate
analysis on feasible descent methods. We also thank the review-
ers for their detailed comments on this paper.
216
References
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, Proceedings the International Conference
on Computational Linguistics (COLING).
L. Bottou. 2004. Stochastic learning. In Olivier Bous-
quet and Ulrike von Luxburg, editors, Advanced Lec-
tures on Machine Learning, Lecture Notes in Artifi-
cial Intelligence, LNAI 3176, pages 146?168. Springer
Verlag, Berlin.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect super-
vision. In Proceedings of the International Conference
on Machine Learning (ICML).
T. Cohn and M. Lapata. 2009. Sentence compression
as tree transduction. Journal of AI Research, 34:637?
674, April.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L.
Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin Markov
networks. Journal of Machine Learning Research, 9.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods for Natural Language Pro-
cessing (EMNLP).
K. Crammer, R. Mcdonald, and F. Pereira. 2005. Scal-
able large-margin online learning for structured clas-
sification. Technical report, Department of Computer
and Information Science, University of Pennsylvania.
J. Desrosiers and M. E. Lu?bbecke. 2005. A primer in
column generation. In Column Generation, pages 1?
32. Springer.
J. M. Eisner. 1997. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
the International Conference on Computational Lin-
guistics (COLING), pages 340?345.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the Perceptron algorithm. Machine
Learning, 37(3):277?296.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear SVM. In Proceedings
of the International Conference on Machine Learning
(ICML), New York, NY, USA. ACM.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural SVMs. Machine
Learning, 77(1):27?59.
J. Kivinen and M. K. Warmuth. 1995. Exponentiated
gradient versus gradient descent for linear predictors.
In ACM Symp. of the Theory of Computing.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proceedings of the 2007 Joint Conference of
EMNLP-CoNLL, pages 141?150.
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang,
K. Torisawa, and H. Isahara. 2009. An error-driven
word-character hybrid model for joint chinese word
segmentation and pos tagging. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 513?521.
S. Lacoste-Julien, M. Jaggi, M. W. Schmidt, and
P. Pletscher. 2013. Stochastic block-coordinate Frank-
Wolfe optimization for structural SVMs. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML).
D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1:
A new benchmark collection for text categorization
research. Journal of Machine Learning Research,
5:361?397.
L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. 2009.
Enhancing diversity, coverage and balance for summa-
rization through structure learning. In Proceedings of
the 18th international conference on World wide web,
The International World Wide Web Conference, pages
71?80, New York, NY, USA. ACM.
Z.-Q. Luo and P. Tseng. 1993. Error bounds and conver-
gence analysis of feasible descent methods: A general
approach. Annals of Operations Research, 46(1):157?
178.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330, June.
A. F. Martins, K. Gimpel, N. A. Smith, E. P. Xing, M. A.
Figueiredo, and P. M. Aguiar. 2010. Learning struc-
tured classifiers with dual coordinate ascent. Technical
report, Technical report CMU-ML-10-109.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 91?98, Ann
Arbor, Michigan.
A. Mejer and K. Crammer. 2010. Confidence in
structured-prediction using confidence-weighted mod-
els. In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing, Pro-
ceedings of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 971?
981.
217
R. C. Moore, W. Yih, and A. Bode. 2007. Improved dis-
criminative bilingual word alignment. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL), Jun.
N. Ratliff, J. Andrew (Drew) Bagnell, and M. Zinkevich.
2007. (Online) subgradient methods for structured
prediction. In Proceedings of the International Work-
shop on Artificial Intelligence and Statistics, March.
S. Shalev-Shwartz and Y. Singer. 2006. Online learn-
ing meets optimization in the dual. In Proceedings of
the Annual ACM Workshop on Computational Learn-
ing Theory (COLT).
S. Shalev-Shwartz and T. Zhang. 2013. Stochastic dual
coordinate ascent methods for regularized loss min-
imization. Journal of Machine Learning Research,
14:567?599.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: primal estimated sub-gradient solver for SVM.
In Zoubin Ghahramani, editor, Proceedings of the In-
ternational Conference on Machine Learning (ICML),
pages 807?814. Omnipress.
S. Shevade, P. Balamurugan, S. Sundararajan, and
S. Keerthi. 2011. A sequential dual method for struc-
tural SVMs. In IEEE International Conference on
Data Mining(ICDM).
E. F. T. K. Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Walter Daelemans and
Miles Osborne, editors, Proceedings of CoNLL-2003,
pages 142?147. Edmonton, Canada.
B. Taskar, C. Guestrin, and D. Koller. 2004a. Max-
margin markov networks. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004b. Max-margin parsing. In Proceedings
of the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
B. Taskar, S. Lacoste-julien, and M. I. Jordan. 2005.
Structured prediction via the extragradient method. In
The Conference on Advances in Neural Information
Processing Systems (NIPS).
B. Taskar, S. Lacoste-Julien, and M. I Jordan. 2006.
Structured prediction, dual extragradient and bregman
projections. Journal of Machine Learning Research,
7:1627?1653.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interde-
pendent and structured output spaces. In Proceedings
of the International Conference on Machine Learning
(ICML).
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 384?394, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Po-Wei Wang and Chih-Jen Lin. 2013. Iteration com-
plexity of feasible descent methods for convex opti-
mization. Technical report, National Taiwan Univer-
sity.
C. Yu and T. Joachims. 2009. Learning structural SVMs
with latent variables. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML).
218
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247?256,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Learning Discriminative Projections for Text Similarity Measures
Wen-tau Yih Kristina Toutanova John C. Platt Christopher Meek
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{scottyih,kristout,jplatt,meek}@microsoft.com
Abstract
Traditional text similarity measures consider
each term similar only to itself and do not
model semantic relatedness of terms. We pro-
pose a novel discriminative training method
that projects the raw term vectors into a com-
mon, low-dimensional vector space. Our ap-
proach operates by finding the optimal matrix
to minimize the loss of the pre-selected sim-
ilarity function (e.g., cosine) of the projected
vectors, and is able to efficiently handle a
large number of training examples in the high-
dimensional space. Evaluated on two very dif-
ferent tasks, cross-lingual document retrieval
and ad relevance measure, our method not
only outperforms existing state-of-the-art ap-
proaches, but also achieves high accuracy at
low dimensions and is thus more efficient.
1 Introduction
Measures of text similarity have many applications
and have been studied extensively in both the NLP
and IR communities. For example, a combination
of corpus and knowledge based methods have been
invented for judging word similarity (Lin, 1998;
Agirre et al, 2009). Similarity derived from a large-
scale Web corpus has been used for automatically
extending lists of typed entities (Vyas and Pantel,
2009). Judging the degree of similarity between
documents is also fundamental to classical IR prob-
lems such as document retrieval (Manning et al,
2008). In all these applications, the vector-based
similarity method is the most widely used. Term
vectors are first constructed to represent the origi-
nal text objects, where each term is associated with
a weight indicating its importance. A pre-selected
function operating on these vectors, such as cosine,
is used to output the final similarity score. This ap-
proach has not only proved to be effective, but is also
efficient. For instance, only the term vectors rather
than the raw data need to be stored. A pruned inverse
index can be built to support fast similarity search.
However, the main weakness of this term-vector
representation is that different but semantically re-
lated terms are not matched and cannot influence
the final similarity score. As an illustrative ex-
ample, suppose the two compared term-vectors
are: {purchase:0.4, used:0.3, automobile:0.2} and
{buy:0.3, pre-owned: 0.5, car: 0.4}. Even though
the two vectors represent very similar concepts, their
similarity score will be 0, for functions like cosine,
overlap or Jaccard. Such an issue is more severe
in cross-lingual settings. Because language vocab-
ularies typically have little overlap, term-vector rep-
resentations are completely inapplicable to measur-
ing similarity between documents in different lan-
guages. The general strategy to handle this prob-
lem is to map the raw representation to a common
concept space, where extensive approaches have
been proposed. Existing methods roughly fall into
three categories. Generative topic models like La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
assume that the terms are sampled by probabil-
ity distributions governed by hidden topics. Lin-
ear projection methods like Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) learn a projec-
tion matrix and map the original term-vectors to the
dense low-dimensional space. Finally, metric learn-
ing approaches for high-dimensional spaces have
247
also been proposed (Davis and Dhillon, 2008).
In this paper, we propose a new projection learn-
ing framework, Similarity Learning via Siamese
Neural Network (S2Net), to discriminatively learn
the concept vector representations of input text ob-
jects. Following the general Siamese neural network
architecture (Bromley et al, 1993), our approach
trains two identical networks concurrently. The in-
put layer corresponds to the original term vector
and the output layer is the projected concept vector.
Model parameters (i.e., the weights on the edges)
are equivalently the projection matrix. Given pairs
of raw term vectors and their labels (e.g., similar or
not), the model is trained by minimizing the loss of
the similarity scores of the output vectors. S2Net
is closely related to the linear projection and met-
ric learning approaches, but enjoys additional ad-
vantages over existing methods. While its model
form is identical to that of LSA, CCA and OPCA, its
objective function can be easily designed to match
the true evaluation metric of interest for the target
task, which leads to better performance. Compared
to existing high-dimensional metric learning meth-
ods, S2Net can learn from a much larger number
of labeled examples. These two properties are cru-
cial in helping S2Net outperform existing methods.
For retrieving comparable cross-lingual documents,
S2Net achieves higher accuracy than the best ap-
proach (OPCA) at a much lower dimension of the
concept space (500 vs. 2,000). In a monolingual
setting, where the task is to judge the relevance of
an ad landing page to a query, S2Net alo has the
best performance when compared to a number of ap-
proaches, including the raw TFIDF cosine baseline.
In the rest of the paper, we first survey some
existing work in Sec. 2, with an emphasis on ap-
proaches included in our experimental comparison.
We present our method in Sec. 3 and report on an
extensive experimental study in Sec. 4. Other re-
lated work is discussed in Sec. 5 and finally Sec. 6
concludes the paper.
2 Previous Work
In this section, we briefly review existing ap-
proaches for mapping high-dimensional term-
vectors to a low-dimensional concept space.
2.1 Generative Topic Models
Probabilistic Latent Semantic Analysis
(PLSA) (Hofmann, 1999) assumes that each
document has a document-specific distribution ?
over some finite number K of topics, where each
token in a document is independently generated
by first selecting a topic z from a multinomial
distribution MULTI(?), and then sampling a word
token from the topic-specific word distribution
for the chosen topic MULTI(?z). Latent Dirichlet
Allocation (LDA) (Blei et al, 2003) generalizes
PLSA to a proper generative model for documents
and places Dirichlet priors over the parameters
? and ?. In the experiments in this paper, our
implementation of PLSA is LDA with maximum a
posteriori (MAP) inference, which was shown to be
comparable to the current best Bayesian inference
methods for LDA (Asuncion et al, 2009).
Recently, these topic models have been general-
ized to handle pairs or tuples of corresponding doc-
uments, which could be translations in multiple lan-
guages, or documents in the same language that are
considered similar. For instance, the Poly-lingual
Topic Model (PLTM) (Mimno et al, 2009) is an
extension to LDA that views documents in a tu-
ple as having a shared topic vector ?. Each of the
documents in the tuple uses ? to select the topics
z of tokens, but could use a different (language-
specific) word-topic-distribution MULTI(?Lz ). Two
additional models, Joint PLSA (JPLSA) and Cou-
pled PLSA (CPLSA) were introduced in (Platt et al,
2010). JPLSA is a close variant of PLTM when doc-
uments of all languages share the same word-topic
distribution parameters, and MAP inference is per-
formed instead of Bayesian. CPLSA extends JPLSA
by constraining paired documents to not only share
the same prior topic distribution ?, but to also have
similar fractions of tokens assigned to each topic.
This constraint is enforced on expectation using pos-
terior regularization (Ganchev et al, 2009).
2.2 Linear Projection Methods
The earliest method for projecting term vectors into
a low-dimensional concept space is Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990). LSA
models all documents in a corpus using a n ?
d document-term matrix D and performs singular
248
value decomposition (SVD) on D. The k biggest
singular values are then used to find the d ? k pro-
jection matrix. Instead of SVD, LSA can be done
by applying eigen-decomposition on the correlation
matrix between terms C = DTD. This is very sim-
ilar to principal component analysis (PCA), where a
covariance matrix between terms is used. In prac-
tice, term vectors are very sparse and their means
are close to 0. Therefore, the correlation matrix is in
fact close to the covariance matrix.
To model pairs of comparable documents,
LSA/PCA has been extended in different ways. For
instance, Cross-language Latent Semantic Indexing
(CL-LSI) (Dumais et al, 1997) applies LSA to con-
catenated comparable documents from different lan-
guages. Oriented Principal Component Analysis
(OPCA) (Diamantaras and Kung, 1996; Platt et al,
2010) solves a generalized eigen problem by intro-
ducing a noise covariance matrix to ensure that com-
parable documents can be projected closely. Canon-
ical Correlation Analysis (CCA) (Vinokourov et al,
2003) finds projections that maximize the cross-
covariance between the projected vectors.
2.3 Distance Metric Learning
Measuring the similarity between two vectors can be
viewed as equivalent to measuring their distance, as
the cosine score has a bijection mapping to the Eu-
clidean distance of unit vectors. Most work on met-
ric learning learns a Mahalanobis distance, which
generalizes the standard squared Euclidean distance
by modeling the similarity of elements in different
dimensions using a positive semi-definite matrix A.
Given two vectors x and y, their squared Maha-
lanobis distance is: dA = (x ? y)TA(x ? y).
However, the computational complexity of learn-
ing a general Mahalanobis matrix is at least O(n2),
where n is the dimensionality of the input vectors.
Therefore, such methods are not practical for high
dimensional problems in the text domain.
In order to tackle this issue, special metric
learning approaches for high-dimensional spaces
have been proposed. For example, high dimen-
sion low-rank (HDLR) metric learning (Davis and
Dhillon, 2008) constrains the form of A = UUT ,
where U is similar to the regular projection ma-
trix, and adapts information-theoretic metric learn-
ing (ITML) (Davis et al, 2007) to learn U.
sim(vp,vq) 
1t
dtvp vq 
it
1c
kcjc
'tw
tw
Figure 1: Learning concept vectors. The output layer
consists of a small number of concept nodes, where the
weight of each node is a linear combination of all the
original term weights.
3 Similarity Learning via Siamese Neural
Network (S2Net)
Given pairs of documents with their labels, such as
binary or real-valued similarity scores, our goal is
to construct a projection matrix that maps the corre-
sponding term-vectors into a low-dimensional con-
cept space such that similar documents are close
when projected into this space. We propose a sim-
ilarity learning framework via Siamese neural net-
work (S2Net) to learn the projection matrix directly
from labeled data. In this section, we introduce its
model design and describe the training process.
3.1 Model Design
The network structure of S2Net consists of two lay-
ers. The input layer corresponds to the raw term vec-
tor, where each node represents a term in the original
vocabulary and its associated value is determined by
a term-weighting function such as TFIDF. The out-
put layer is the learned low-dimensional vector rep-
resentation that captures relationships among terms.
Similarly, each node of the output layer is an ele-
ment in the new concept vector. In this work, the
final similarity score is calculated using the cosine
function, which is the standard choice for document
similarity (Manning et al, 2008). Our framework
can be easily extended to other similarity functions
as long as they are differentiable.
The output of each concept node is a linear com-
249
bination of the weights of all the terms in the orig-
inal term vector. In other words, these two layers
of nodes form a complete bipartite graph as shown
in Fig. 1. The output of a concept node cj is thus
defined as:
tw?(cj) =
?
ti?V
?ij ? tw(ti) (1)
Notice that it is straightforward to add a non-linear
activation function (e.g., sigmoid) in Eq. (1), which
can potentially lead to better results. However, in
the current design, the model form is exactly the
same as the low-rank projection matrix derived by
PCA, OPCA or CCA, which facilitates comparison
to alternative projection methods. Using concise
matrix notation, let f be a raw d-by-1 term vector,
A = [?ij ]d?k the projection matrix. g = AT f is
thus the k-by-1 projected concept vector.
3.2 Loss Function and Training Procedure
For a pair of term vectors fp and fq, their similar-
ity score is defined by the cosine value of the corre-
sponding concept vectors gp and gq according to the
projection matrix A.
simA(fp, fq) =
gTp gq
||gp||||gq||
,
where gp = AT fp and gq = AT fq. Let ypq be
the true label of this pair. The loss function can
be as simple as the mean-squared error 12(ypq ?
simA(fp, fq))2. However, in many applications, the
similarity scores are used to select the closest text
objects given the query. For example, given a query
document, we only need to have the comparable
document in the target language ranked higher than
any other documents. In this scenario, it is more
important for the similarity measure to yield a good
ordering than to match the target similarity scores.
Therefore, we use a pairwise learning setting by con-
sidering a pair of similarity scores (i.e., from two
vector pairs) in our learning objective.
Consider two pairs of term vectors (fp1 , fq1) and
(fp2 , fq2), where the first pair has higher similarity.
Let ? be the difference of their similarity scores.
Namely, ? = simA(fp1 , fq1)? simA(fp2 , fq2). We
use the following logistic loss over ?, which upper-
bounds the pairwise accuracy (i.e., 0-1 loss):
L(?;A) = log(1 + exp(???)) (2)
Because of the cosine function, we add a scaling
factor ? that magnifies ? from [?2, 2] to a larger
range, which helps penalize more on the prediction
errors. Empirically, the value of ? makes no dif-
ference as long as it is large enough1. In the ex-
periments, we set the value of ? to 10. Optimizing
the model parameters A can be done using gradi-
ent based methods. We derive the gradient of the
whole batch and apply the quasi-Newton optimiza-
tion method L-BFGS (Nocedal and Wright, 2006)
directly. For a cleaner presentation, we detail the
gradient derivation in Appendix A. Given that the
optimization problem is not convex, initializing the
model from a good projection matrix often helps re-
duce training time and may lead to convergence to
a better local minimum. Regularization can be done
by adding a term ?2 ||A ? A0||
2 in Eq. (2), which
forces the learned model not to deviate too much
from the starting point (A0), or simply by early stop-
ping. Empirically we found that the latter is more
effective and it is used in the experiments.
4 Experiments
We compare S2Net experimentally with existing ap-
proaches on two very different tasks: cross-lingual
document retrieval and ad relevance measures.
4.1 Comparable Document Retrieval
With the growth of multiple languages on the Web,
there is an increasing demand of processing cross-
lingual documents. For instance, machine trans-
lation (MT) systems can benefit from training on
sentences extracted from parallel or comparable
documents retrieved from the Web (Munteanu and
Marcu, 2005). Word-level translation lexicons can
also be learned from comparable documents (Fung
and Yee, 1998; Rapp, 1999). In this cross-lingual
document retrieval task, given a query document in
one language, the goal is to find the most similar
document from the corpus in another language.
4.1.1 Data & Setting
We followed the comparable document retrieval
setting described in (Platt et al, 2010) and evalu-
ated S2Net on the Wikipedia dataset used in that pa-
per. This data set consists of Wikipedia documents
1Without the ? parameter, the model still outperforms other
baselines in our experiments, but with a much smaller gain.
250
in two languages, English and Spanish. An article
in English is paired with a Spanish article if they
are identified as comparable across languages by the
Wikipedia community. To conduct a fair compari-
son, we use the same term vectors and data split as in
the previous study. The numbers of document pairs
in the training/development/testing sets are 43,380,
8,675 and 8,675, respectively. The dimensionality
of the raw term vectors is 20,000.
The models are evaluated by using each English
document as query against all documents in Span-
ish and vice versa; the results from the two direc-
tions are averaged. Performance is evaluated by two
metrics: the Top-1 accuracy, which tests whether
the document with the highest similarity score is the
true comparable document, and the Mean Recipro-
cal Rank (MRR) of the true comparable.
When training the S2Net model, all the compara-
ble document pairs are treated as positive examples
and all other pairs are used as negative examples.
Naively treating these 1.8 billion pairs (i.e., 433802)
as independent examples would make the training
very inefficient. Fortunately, most computation in
deriving the batch gradient can be reused via com-
pact matrix operations and training can still be done
efficiently. We initialized the S2Net model using the
matrix learned by OPCA, which gave us the best per-
formance on the development set2.
Our approach is compared with most methods
studied in (Platt et al, 2010), including the best per-
forming one. For CL-LSI, OPCA, and CCA, we in-
clude results from that work directly. In addition, we
re-implemented and improved JPLSA and CPLSA
by changing three settings: we used separate vocab-
ularies for the two languages as in the Poly-lingual
topic model (Mimno et al, 2009), we performed 10
EM iterations for folding-in instead of only one, and
we used the Jensen-Shannon distance instead of the
L1 distance. We also attempted to apply the HDLR
algorithm. Because this algorithm does not scale
well as the number of training examples increases,
we used 2,500 positive and 2,500 negative docu-
ment pairs for training. Unfortunately, among all the
2S2Net outperforms OPCA when initialized from a random
or CL-LSI matrix, but with a smaller gain. For example, when
the number of dimensions is 1000, the MRR score of OPCA
is 0.7660. Starting from the CL-LSI and OPCA matrices, the
MRR scores of S2Net are 0.7745 and 0.7855, respectively.
Figure 2: Mean reciprocal rank versus dimension for
Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
hyper-parameter settings we tested, HDLR could not
outperform its initial model, which was the OPCA
matrix. Therefore we omit these results.
4.1.2 Results
Fig. 2 shows the MRR performance of all meth-
ods on the development set, across different dimen-
sionality settings of the concept space. As can be
observed from the figure, higher dimensions usually
lead to better results. In addition, S2Net consistently
performs better than all other methods across differ-
ent dimensions. The gap is especially large when
projecting input vectors to a low-dimensional space,
which is preferable for efficiency. For instance, us-
ing 500 dimensions, S2Net aleady performs as well
as OPCA with 2000 dimensions.
Table 1 shows the averaged Top-1 accuracy and
MRR scores of all methods on the test set, where
the dimensionality for each method is optimized on
the development set (Fig. 2). S2Net clearly outper-
forms all other methods and the difference in terms
of accuracy is statistically significant3.
4.2 Ad Relevance
Paid search advertising is the main revenue source
that supports modern commercial search engines.
To ensure satisfactory user experience, it is impor-
tant to provide both relevant ads and regular search
3We use the unpaired t-test with Bonferroni correction and
the difference is considered statistically significant when the p-
value is less than 0.01.
251
Algorithm Dimension Accuracy MRR
S2Net 2000 0.7447 0.7973
OPCA 2000 0.7255 0.7734
CCA 1500 0.6894 0.7378
CPLSA 1000 0.6329 0.6842
JPLSA 1000 0.6079 0.6604
CL-LSI 5000 0.5302 0.6130
Table 1: Test results for comparable document retrieval
in Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
results. Previous work on ad relevance focuses on
constructing appropriate term-vectors to represent
queries and ad-text (Broder et al, 2008; Choi et al,
2010). In this section, we extend the work in (Yih
and Jiang, 2010) and show how S2Net can exploit
annotated query?ad pairs to improve the vector rep-
resentation in this monolingual setting.
4.2.1 Data & Tasks
The ad relevance dataset we used consists of
12,481 unique queries randomly sampled from the
logs of the Bing search engine. For each query, a
number of top ranked ads are selected, which results
in a total number of 567,744 query-ad pairs in the
dataset. Each query-ad pair is manually labeled as
same, subset, superset or disjoint. In our experi-
ment, when the task is a binary classification prob-
lem, pairs labeled as same, subset, or superset are
considered relevant, and pairs labeled as disjoint are
considered irrelevant. When pairwise comparisons
are needed in either training or evaluation, the rele-
vance order is same > subset = superset > disjoint.
The dataset is split into training (40%), validation
(30%) and test (30%) sets by queries.
Because a query string usually contains only a few
words and thus provides very little content, we ap-
plied the same web relevance feedback technique
used in (Broder et al, 2008) to create ?pseudo-
documents? to represent queries. Each query in our
data set was first issued to the search engine. The
result page with up to 100 snippets was used as the
pseudo-document to create the raw term vectors. On
the ad side, we used the ad landing pages instead
of the short ad-text. Our vocabulary set contains
29,854 words and is determined using a document
frequency table derived from a large collection of
Web documents. Only words with counts larger than
a pre-selected threshold are retained.
How the data is used in training depends on the
model. For S2Net, we constructed preference pairs
in the following way. For the same query, each rel-
evant ad is paired with a less relevant ad. The loss
function from Eq. (2) encourages achieving a higher
similarity score for the more relevant ad. For HDLR,
we used a sample of 5,000 training pairs of queries
and ads, as it was not able to scale to more train-
ing examples. For OPCA, CCA, PLSA and JPLSA,
we constructed a parallel corpus using only rele-
vant pairs of queries and ads, as the negative exam-
ples (irrelevant pairs of queries and ads) cannot be
used by these models. Finally, PCA and PLSA learn
the models from all training queries and documents
without using any relevance information.
We tested S2Net and other methods in two differ-
ent application scenarios. The first is to use the ad
relevance measure as an ad filter. When the similar-
ity score between a query and an ad is below a pre-
selected decision threshold, this ad is considered ir-
relevant to the query and will be filtered. Evaluation
metrics used for this scenario are the ROC analysis
and the area under the curve (AUC). The second one
is the ranking scenario, where the ads are selected
and ranked by their relevance scores. In this sce-
nario, the performance is evaluated by the standard
ranking metric, Normalized Discounted Cumulative
Gain (NDCG) (Jarvelin and Kekalainen, 2000).
4.2.2 Results
We first compare different methods in their AUC
and NDCG scores. TFIDF is the basic term vec-
tor representation with the TFIDF weighting (tf ?
log(N/df)). It is used as our baseline and also as
the raw input for S2Net, HDLR and other linear pro-
jection methods. Based on the results on the devel-
opment set, we found that PCA performs better than
OPCA and CCA. Therefore, we initialized the mod-
els of S2Net and HDLR using the PCA matrix. Ta-
ble 2 summarizes results on the test set. All models,
except TFIDF, use 1000 dimensions and their best
configuration settings selected on the validation set.
TFIDF is a very strong baseline on this monolin-
gual ad relevance dataset. Among all the methods
we tested, at dimension 1000, only S2Net outper-
forms the raw TFIDF cosine measure in every eval-
uation metric, and the difference is statistically sig-
252
AUC NDCG@1 NDCG@3 NDCG@5
S2Net 0.892 0.855 0.883 0.901
TFIDF 0.861 0.825 0.854 0.876
HDLR 0.855 0.826 0.856 0.877
CPLSA 0.853 0.845 0.872 0.890
PCA 0.848 0.815 0.847 0.870
OPCA 0.844 0.817 0.850 0.872
JPLSA 0.840 0.838 0.864 0.883
CCA 0.836 0.820 0.852 0.874
PLSA 0.835 0.831 0.860 0.879
Table 2: The AUC and NDCG scores of the cosine sim-
ilarity scores on different vector representations. The di-
mension for all models except TFIDF is 1000.
 0.3 0.4 0.5 0.6 0.7 0.8 0.9
 0.05  0.1  0.15  0.2  0.25True-Positive Rate False-Positive RateThe ROC Curves S2NetTFIDFHDLRCPLSA
Figure 3: The ROC curves of S2Net, TFIDF, HDLR and
CPLSA when the similarity scores are used as ad filters.
nificant4. In contrast, both CPLSA and HDLR have
higher NDCG scores but lower AUC values, and
OPCA/CCA perform roughly the same as PCA.
When the cosine scores of these vector represen-
tations are used as ad filters, their ROC curves (fo-
cusing on the low false-positive region) are shown
in Fig. 3. It can be clearly observed that the similar-
ity score computed based on vectors derived from
S2Net indeed has better quality, compared to the
raw TFIDF representation. Unfortunately, other ap-
proaches perform worse than TFIDF and their per-
formance in the low false-positive region is consis-
tent with the AUC scores.
Although ideally we would like the dimensional-
ity of the projected concept vectors to be as small
4For AUC, we randomly split the data into 50 subsets and
ran a paired-t test between the corresponding AUC scores. For
NDCG, we compared the DCG scores per query of the com-
pared models using the paired-t test. The difference is consid-
ered statistically significant when the p-value is less than 0.01.
as possible for efficient processing, the quality of
the concept vector representation usually degrades
as well. It is thus interesting to know the best trade-
off point between these two variables. Table 3 shows
the AUC and NDCG scores of S2Net at different di-
mensions, as well as the results achieved by TFIDF
and PCA, HDLR and CPLSA at 1000 dimensions.
As can be seen, S2Net surpasses TFIDF in AUC at
dimension 300 and keeps improving as the dimen-
sionality increases. Its NDCG scores are also con-
sistently higher across all dimensions.
4.3 Discussion
It is encouraging to find that S2Net achieves strong
performance in two very different tasks, given that
it is a conceptually simple model. Its empirical suc-
cess can be attributed to two factors. First, it is flex-
ible in choosing the loss function and constructing
training examples and is thus able to optimize the
model directly for the target task. Second, it can
be trained on a large number of examples. For ex-
ample, HDLR can only use a few thousand exam-
ples and is not able to learn a matrix better than its
initial model for the task of cross-lingual document
retrieval. The fact that linear projection methods
like OPCA/CCA and generative topic models like
JPLSA/CPLSA cannot use negative examples more
effectively also limits their potential.
In terms of scalability, we found that methods
based on eigen decomposition, such as PCA, OPCA
and CCA, take the least training time. The complex-
ity is decided by the size of the covariance matrix,
which is quadratic in the number of dimensions. On
a regular eight-core server, it takes roughly 2 to 3
hours to train the projection matrix in both experi-
ments. The training time of S2Net scales roughly
linearly to the number of dimensions and training
examples. In each iteration, performing the projec-
tion takes the most time in gradient derivation, and
the complexity is O(mnk), where m is the num-
ber of distinct term-vectors, n is the largest number
of non-zero elements in the sparse term-vectors and
k is the dimensionality of the concept space. For
cross-lingual document retrieval, when k = 1000,
each iteration takes roughly 48 minutes and about 80
iterations are required to convergence. Fortunately,
the gradient computation is easily parallelizable and
further speed-up can be achieved using a cluster.
253
TFIDF HDLR CPLSA PCA S2Net100 S2Net300 S2Net500 S2Net750 S2Net1000
AUC 0.861 0.855 0.853 0.848 0.855 0.879 0.880 0.888 0.892
NDCG@1 0.825 0.826 0.845 0.815 0.843 0.852 0.856 0.860 0.855
NDCG@3 0.854 0.856 0.872 0.847 0.871 0.879 0.881 0.884 0.883
NDCG@5 0.876 0.877 0.890 0.870 0.890 0.897 0.899 0.902 0.901
Table 3: The AUC and NDCG scores of S2Net at different dimensions. PCA, HDLR & CPLSA (at dimension 1000)
along with the raw TFIDF representation are used for reference.
5 Related Work
Although the high-level design of S2Net follows the
Siamese architecture (Bromley et al, 1993; Chopra
et al, 2005), the network construction, loss func-
tion and training process of S2Net are all differ-
ent compared to previous work. For example, tar-
geting the application of face verification, Chopra
et al (2005) used a convolutional network and de-
signed a contrastive loss function for optimizing a
Eucliden distance metric. In contrast, the network
of S2Net is equivalent to a linear projection ma-
trix and has a pairwise loss function. In terms of
the learning framework, S2Net is closely related to
several neural network based approaches, including
autoencoders (Hinton and Salakhutdinov, 2006) and
finding low-dimensional word representations (Col-
lobert and Weston, 2008; Turian et al, 2010). Archi-
tecturally, S2Net is also similar to RankNet (Burges
et al, 2005), which can be viewed as a Siamese neu-
ral network that learns a ranking function.
The strategy that S2Net takes to learn from la-
beled pairs of documents can be analogous to the
work of distance metric learning. Although high
dimensionality is not a problem to algorithms like
HDLR, it suffers from a different scalability issue.
As we have observed in our experiments, the al-
gorithm can only handle a small number of simi-
larity/dissimilarity constraints (i.e., the labeled ex-
amples), and is not able to use a large number of
examples to learn a better model. Empirically, we
also found that HDLR is very sensitive to the hyper-
parameter settings and its performance can vary sub-
stantially from iteration to iteration.
Other than the applications presented in this pa-
per, concept vectors have shown useful in traditional
IR tasks. For instance, Egozi et al (2008) use ex-
plicit semantic analysis to improve the retrieval re-
call by leveraging Wikipedia. In a companion pa-
per, we also demonstrated that various topic mod-
els including S2Net can enhance the ranking func-
tion (Gao et al, 2011). For text categorization, simi-
larity between terms is often encoded as kernel func-
tions embedded in the learning algorithms, and thus
increase the classification accuracy. Representative
approaches include latent semantic kernels (Cris-
tianini et al, 2002), which learns an LSA-based ker-
nel function from a document collection, and work
that computes term-similarity based on the linguis-
tic knowledge provided by WordNet (Basili et al,
2005; Bloehdorn and Moschitti, 2007).
6 Conclusions
In this paper, we presented S2Net, a discrimina-
tive approach for learning a projection matrix that
maps raw term-vectors to a low-dimensional space.
Our learning method directly optimizes the model
so that the cosine score of the projected vectors can
become a reliable similarity measure. The strength
of this model design has been shown empirically in
two very different tasks. For cross-lingual document
retrieval, S2Net significantly outperforms OPCA,
which is the best prior approach. For ad selection
and filtering, S2Net alo outperforms all methods we
compared it with and is the only technique that beats
the raw TFIDF vectors in both AUC and NDCG.
The success of S2Net is truly encouraging, and
we would like to explore different directions to fur-
ther enhance the model in the future. For instance, it
will be interesting to extend the model to learn non-
linear transformations. In addition, since the pairs of
text objects being compared often come from differ-
ent distributions (e.g., English documents vs. Span-
ish documents or queries vs. pages), learning two
different matrices instead of one could increase the
model expressivity. Finally, we would like to apply
S2Net to more text similarity tasks, such as word
similarity and entity recognition and discovery.
254
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, June.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In UAI.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics via
kernel-based learning. In CoNLL.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:993?1022.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Combined syntactic and semantic kernels for text clas-
sification. In ECIR, pages 307?318.
Andrei Z. Broder, Peter Ciccolo, Marcus Fontoura,
Evgeniy Gabrilovich, Vanja Josifovski, and Lance
Riedel. 2008. Search advertising using web relevance
feedback. In CIKM, pages 1013?1022.
Jane Bromley, James W. Bentz, Le?on Bottou, Isabelle
Guyon, Yann LeCun, Cliff Moore, Eduard Sa?ckinger,
and Roopak Shah. 1993. Signature verification us-
ing a ?Siamese? time delay neural network. Interna-
tional Journal Pattern Recognition and Artificial Intel-
ligence, 7(4):669?688.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
ICML.
Y. Choi, M. Fontoura, E. Gabrilovich, V. Josifovski,
M. Mediano, and B. Pang. 2010. Using landing pages
for sponsored search ad selection. In WWW.
Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with ap-
plication to face verification. In Proceedings of CVPR-
2005, pages 539?546.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In ICML.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2002. Latent semantic kernels. Journal of Intelligent
Information Systems, 18(2?3):127?152.
Jason V. Davis and Inderjit S. Dhillon. 2008. Struc-
tured metric learning for high dimensional problems.
In KDD, pages 195?203.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and
Inderjit S. Dhillon. 2007. Information-theoretic met-
ric learning. In ICML.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Konstantinos I. Diamantaras and S.Y. Kung. 1996. Prin-
cipal Component Neural Networks: Theory and Appli-
cations. Wiley-Interscience.
Susan T. Dumais, Todd A. Letsche, Michael L. Littman,
and Thomas K. Landauer. 1997. Automatic cross-
linguistic information retrieval using latent seman-
tic indexing. In AAAI-97 Spring Symposium Series:
Cross-Language Text and Speech Retrieval.
Ofer Egozi, Evgeniy Gabrilovich, and Shaul Markovitch.
2008. Concept-based feature generation and selection
for information retrieval. In AAAI.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING-ACL.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical Report MS-
CIS-09-16, University of Pennsylvania.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In SIGIR.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504?507, July.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In SIGIR ?99, pages 50?57.
K. Jarvelin and J. Kekalainen. 2000. Ir evaluation meth-
ods for retrieving highly relevant documents. In SI-
GIR, pages 41?48.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL 98.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Pres.
David Mimno, Hanna W. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In EMNLP.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the ACL, pages 519?526.
255
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation of
text via cross-language correlation analysis. In NIPS-
15.
Vishnu Vyas and Patrick Pantel. 2009. Semi-automatic
entity set refinement. In NAACL ?09, pages 290?298.
Wen-tau Yih and Ning Jiang. 2010. Similarity models
for ad relevance measures. In MLOAD - NIPS 2010
Workshop on online advertising.
Appendix A. Gradient Derivation
The gradient of the loss function in Eq. (2) can be
derived as follows.
?L(?,A)
?A
=
??
1 + exp(???)
??
?A
??
?A
=
?
?A
simA(fp1 , fq1)?
?
?A
simA(fp2 , fq2)
?
?A
simA(fp, fq) =
?
?A
cos(gp,gq),
where gp = AT fp and gq = AT fq are the projected
concept vectors of fq and fq. The gradient of the
cosine score can be further derived in the following
steps.
cos(gp,gq) =
gTp gq
?gp??gq?
?Ag
T
p gq = (?AA
T fp)gq + (?AA
T fq)gp
= fpgTq + fqg
T
p
?A
1
?gp?
= ?A(g
T
p gp)
? 12
= ?
1
2
(gTp gp)
? 32?A(g
T
p gp)
= ?(gTp gp)
? 32 fpgTp
?A
1
?gq?
= ?(gTq gq)
? 32 fqgTq
Let a, b, c be gTp gq, 1/?gp? and 1/?gq?, respec-
tively.
?A
gTp gq
?gp??gq?
= ? abc3fqgTq ? acb
3fpgTp
+ bc(fpgTq + fqg
T
p )
256
