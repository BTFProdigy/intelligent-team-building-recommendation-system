Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 118?126,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Ranking Approach to Stress Prediction
for Letter-to-Phoneme Conversion
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{qdou,bergsma,sj,kondrak}@cs.ualberta.ca
Abstract
Correct stress placement is important in
text-to-speech systems, in terms of both
the overall accuracy and the naturalness of
pronunciation. In this paper, we formu-
late stress assignment as a sequence pre-
diction problem. We represent words as
sequences of substrings, and use the sub-
strings as features in a Support Vector Ma-
chine (SVM) ranker, which is trained to
rank possible stress patterns. The rank-
ing approach facilitates inclusion of arbi-
trary features over both the input sequence
and output stress pattern. Our system ad-
vances the current state-of-the-art, predict-
ing primary stress in English, German, and
Dutch with up to 98% word accuracy on
phonemes, and 96% on letters. The sys-
tem is also highly accurate in predicting
secondary stress. Finally, when applied in
tandem with an L2P system, it substan-
tially reduces the word error rate when
predicting both phonemes and stress.
1 Introduction
In many languages, certain syllables in words are
phonetically more prominent in terms of duration,
pitch, and loudness. This phenomenon is referred
to as lexical stress. In some languages, the loca-
tion of stress is entirely predictable. For example,
lexical stress regularly falls on the initial syllable
in Hungarian, and on the penultimate syllable in
Polish. In other languages, such as English and
Russian, any syllable in the word can be stressed.
Correct stress placement is important in text-
to-speech systems because it affects the accuracy
of human word recognition (Tagliapietra and Ta-
bossi, 2005; Arciuli and Cupples, 2006). How-
ever, the issue has often been ignored in previ-
ous letter-to-phoneme (L2P) systems. The sys-
tems that do generate stress markers often do not
report separate figures on stress prediction accu-
racy, or they only provide results on a single lan-
guage. Some only predict primary stress mark-
ers (Black et al, 1998; Webster, 2004; Demberg
et al, 2007), while those that predict both primary
and secondary stress generally achieve lower ac-
curacy (Bagshaw, 1998; Coleman, 2000; Pearson
et al, 2000).
In this paper, we formulate stress assignment as
a sequence prediction problem. We divide each
word into a sequence of substrings, and use these
substrings as features for a Support Vector Ma-
chine (SVM) ranker. For a given sequence length,
there is typically only a small number of stress
patterns in use. The task of the SVM is to rank
the true stress pattern above the small number of
acceptable alternatives. This is the first system
to predict stress within a powerful discriminative
learning framework. By using a ranking approach,
we enable the use of arbitrary features over the en-
tire (input) sequence and (output) stress pattern.
We show that the addition of a feature for the en-
tire output sequence improves prediction accuracy.
Our experiments on English, German, and
Dutch demonstrate that our ranking approach sub-
stantially outperforms previous systems. The
SVM ranker achieves exceptional 96.2% word ac-
curacy on the challenging task of predicting the
full stress pattern in English. Moreover, when
combining our stress predictions with a state-of-
the-art L2P system (Jiampojamarn et al, 2008),
we set a new standard for the combined prediction
of phonemes and stress.
The paper is organized as follows. Section 2
provides background on lexical stress and a task
definition. Section 3 presents our automatic stress
prediction algorithm. In Section 4, we confirm the
power of the discriminative approach with experi-
ments on three languages. Section 5 describes how
stress is integrated into L2P conversion.
118
2 Background and Task Definition
There is a long history of research into the prin-
ciples governing lexical stress placement. Zipf
(1929) showed that stressed syllables are of-
ten those with low frequency in speech, while
unstressed syllables are usually very common.
Chomsky and Halle (1968) proposed a set of
context-sensitive rules for producing English
stress from underlying word forms. Due to its
importance in text-to-speech, there is also a long
history of computational stress prediction sys-
tems (Fudge, 1984; Church, 1985; Williams,
1987). While these early approaches depend
on human definitions of vowel tensity, syllable
weight, word etymology, etc., our work follows
a recent trend of purely data-driven approaches to
stress prediction (Black et al, 1998; Pearson et al,
2000; Webster, 2004; Demberg et al, 2007).
In many languages, only two levels of stress
are distinguished: stressed and unstressed. How-
ever, some languages exhibit more than two levels
of stress. For example, in the English word eco-
nomic, the first and the third syllable are stressed,
with the former receiving weaker emphasis than
the latter. In this case, the initial syllable is said
to carry a secondary stress. Although each word
has only one primary stress, it may have any num-
ber of secondary stresses. Predicting the full stress
pattern is therefore inherently more difficult than
predicting the location of primary stress only.
Our objective is to automatically assign primary
and, where possible, secondary stress to out-of-
vocabulary words. Stress is an attribute of sylla-
bles, but syllabification is a non-trivial task in it-
self (Bartlett et al, 2008). Rather than assuming
correct syllabification of the input word, we in-
stead follow Webster (2004) in placing the stress
on the vowel which constitutes the nucleus of the
stressed syllable. If the syllable boundaries are
known, the mapping from the vowel to the cor-
responding syllable is straightforward.
We investigate the assignment of stress to two
related but different entities: the spoken word
(represented by its phonetic transcription), and
the written word (represented by its orthographic
form). Although stress is a prosodic feature, as-
signing stress to written words (?stressed orthog-
raphy?) has been utilized as a preprocessing stage
for the L2P task (Webster, 2004). This prepro-
cessing is motivated by two factors. First, stress
greatly influences the pronunciation of vowels in
English (c.f., allow vs. alloy). Second, since
phoneme predictors typically utilize only local
context around a letter, they do not incorporate the
global, long-range information that is especially
predictive of stress, such as penultimate syllable
emphasis associated with the suffix -ation. By tak-
ing stressed orthography as input, the L2P system
is able to implicitly leverage morphological infor-
mation beyond the local context.
Indicating stress on letters can also be help-
ful to humans, especially second-language learn-
ers. In some languages, such as Spanish, ortho-
graphic markers are obligatory in words with ir-
regular stress. The location of stress is often ex-
plicitly marked in textbooks for students of Rus-
sian. In both languages, the standard method of
indicating stress is to place an acute accent above
the vowel bearing primary stress, e.g., adio?s. The
secondary stress in English can be indicated with
a grave accent (Coleman, 2000), e.g., pre`ce?de.
In summary, our task is to assign primary and
secondary stress markers to stress-bearing vowels
in an input word. The input word may be either
phonemes or letters. If a stressed vowel is repre-
sented by more than one letter, we adopt the con-
vention of marking the first vowel of the vowel se-
quence, e.g., me?eting. In this way, we are able to
focus on the task of stress prediction, without hav-
ing to determine at the same time the exact sylla-
ble boundaries, or whether a vowel letter sequence
represents one or more spoken vowels (e.g., beat-
ing vs. be-at-i-fy).
3 Automatic Stress Prediction
Our stress assignment system maps a word, w, to a
stressed-form of the word, w?. We formulate stress
assignment as a sequence prediction problem. The
assignment is made in three stages:
(1) First, we map words to substrings (s), the ba-
sic units in our sequence (Section 3.1).
(2) Then, a particular stress pattern (t) is chosen
for each substring sequence. We use a sup-
port vector machine (SVM) to rank the possi-
ble patterns for each sequence (Section 3.2).
(3) Finally, the stress pattern is used to produce
the stressed-form of the word (Section 3.3).
Table 1 gives examples of words at each stage of
the algorithm. We discuss each step in more detail.
119
Word Substrings Pattern Word?
w ? s ? t ? w?
worker ? wor-ker ? 1-0 ? wo?rker
overdo ? ov-ver-do ? 2-0-1 ? o`verdo?
react ? re-ac ? 0-1 ? rea?ct
?bstr?kt ? ?b-r?k ? 0-1 ? ?bstr??kt
prisid ? ri-sid ? 2-1 ? pr?`s??d
Table 1: The steps in our stress prediction sys-
tem (with orthographic and phonetic prediction
examples): (1) word splitting, (2) support vector
ranking of stress patterns, and (3) pattern-to-vowel
mapping.
3.1 Word Splitting
The first step in our approach is to represent the
word as a sequence of N individual units: w ?
s = {s1-s2-...-sN}. These units are used to define
the features and outputs used by the SVM ranker.
Although we are ultimately interested in assigning
stress to individual vowels in the phoneme and let-
ter sequence, it is beneficial to represent the task in
units larger than individual letters.
Our substrings are similar to syllables; they
have a vowel as their nucleus and include con-
sonant context. By approximating syllables, our
substring patterns will allow us to learn recur-
rent stress regularities, as well as dependencies
between neighboring substrings. Since determin-
ing syllable breaks is a non-trivial task, we in-
stead adopt the following simple splitting tech-
nique. Each vowel in the word forms the nucleus
of a substring. Any single preceding or follow-
ing consonant is added to the substring unit. Thus,
each substring consists of at most three symbols
(Table 1).
Using shorter substrings reduces the sparsity of
our training data; words like cryer, dryer and fryer
are all mapped to the same form: ry-er. The
SVM can thus generalize from observed words to
similarly-spelled, unseen examples.
Since the number of vowels equals the num-
ber of syllables in the phonetic form of the word,
applying this approach to phonemes will always
generate the correct number of syllables. For let-
ters, splitting may result in a different number of
units than the true syllabification, e.g., pronounce
? ron-no-un-ce. This does not prevent the system
from producing the correct stress assignment after
the pattern-to-vowel mapping stage (Section 3.3)
is complete.
3.2 Stress Prediction with SVM Ranking
After creating a sequence of substring units, s =
{s1-s2-...-sN}, the next step is to choose an out-
put sequence, t = {t1-t2-...-tN}, that encodes
whether each unit is stressed or unstressed. We
use the number ?1? to indicate that a substring re-
ceives primary stress, ?2? for secondary stress, and
?0? to indicate no stress. We call this output se-
quence the stress pattern for a word. Table 1 gives
examples of words, substrings, and stress patterns.
We use supervised learning to train a system to
predict the stress pattern. We generate training
(s, t) pairs in the obvious way from our stress-
marked training words, w?. That is, we first ex-
tract the letter/phoneme portion, w, and use it
to create the substrings, s. We then create the
stress pattern, t, using w??s stress markers. Given
the training pairs, any sequence predictor can be
used, for example a Conditional Random Field
(CRF) (Lafferty et al, 2001) or a structured per-
ceptron (Collins, 2002). However, we can take
advantage of a unique property of our problem to
use a more expressive framework than is typically
used in sequence prediction.
The key observation is that the output space of
possible stress patterns is actually fairly limited.
Clopper (2002) shows that people have strong
preferences for particular sequences of stress, and
this is confirmed by our training data (Section 4.1).
In English, for example, we find that for each set
of spoken words with the same number of sylla-
bles, there are no more than fifteen different stress
patterns. In total, among 55K English training ex-
amples, there are only 70 different stress patterns.
In both German and Dutch there are only about
50 patterns in 250K examples.1 Therefore, for a
particular input sequence, we can safely limit our
consideration to only the small set of output pat-
terns of the same length.
Thus, unlike typical sequence predictors, we do
not have to search for the highest-scoring output
according to our model. We can enumerate the
full set of outputs and simply choose the highest-
scoring one. This enables a more expressive rep-
resentation. We can define arbitrary features over
the entire output sequence. In a typical CRF or
structured perceptron approach, only output fea-
tures that can be computed incrementally during
search are used (e.g. Markov transition features
that permit Viterbi search). Since search is not
1See (Dou, 2009) for more details.
120
needed here, we can exploit longer-range features.
Choosing the highest-scoring output from a
fixed set is a ranking problem, and we provide the
full ranking formulation below. Unlike previous
ranking approaches (e.g. Collins and Koo (2005)),
we do not rely on a generative model to produce
a list of candidates. Candidates are chosen in ad-
vance from observed training patterns.
3.2.1 Ranking Formulation
For a substring sequence, s, of length N , our task
is to select the correct output pattern from the set
of all length-N patterns observed in our training
data, a set we denote as TN . We score each possi-
ble input-output combination using a linear model.
Each substring sequence and possible output pat-
tern, (s, t), is represented with a set of features,
?(s, t). The score for a particular (s, t) combina-
tion is a weighted sum of these features, ???(s, t).
The specific features we use are described in Sec-
tion 3.2.2.
Let tj be the stress pattern for the jth training
sequence sj , both of length N . At training time,
the weights, ?, are chosen such that for each sj ,
the correct output pattern receives a higher score
than other patterns of the same length: ?u ?
TN ,u 6= tj,
? ??(sj, tj) > ? ??(sj ,u) (1)
The set of constraints generated by Equation 1
are called rank constraints. They are created sep-
arately for every (sj , tj) training pair. Essen-
tially, each training pair is matched with a set
of automatically-created negative examples. Each
negative has an incorrect, but plausible, stress pat-
tern, u.
We adopt a Support Vector Machine (SVM) so-
lution to these ranking constraints as described by
Joachims (2002). The learner finds the weights
that ensure a maximum (soft) margin separation
between the correct scores and the competitors.
We use an SVM because it has been successful in
similar settings (learning with thousands of sparse
features) for both ranking and classification tasks,
and because an efficient implementation is avail-
able (Joachims, 1999).
At test time we simply score each possible out-
put pattern using the learned weights. That is,
for an input sequence s of length N , we compute
? ??(s, t) for all t ? TN , and we take the highest
scoring t as our output. Note that because we only
Substring si, ti
si, i, ti
Context si?1, ti
si?1si, ti
si+1, ti
sisi+1, ti
si?1sisi+1, ti
Stress Pattern t1t2 . . . tN
Table 2: Feature Template
consider previously-observed output patterns, it is
impossible for our system to produce a nonsensi-
cal result, such as having two primary stresses in
one word. Standard search-based sequence pre-
dictors need to be specially augmented with hard
constraints in order to prevent such output (Roth
and Yih, 2005).
3.2.2 Features
The power of our ranker to identify the correct
stress pattern depends on how expressive our fea-
tures are. Table 2 shows the feature templates used
to create the features ?(s, t) for our ranker. We
use binary features to indicate whether each com-
bination occurs in the current (s,t) pair.
For example, if a substring tion is unstressed in
a (s, t) pair, the Substring feature {si, ti = tion,0}
will be true.2 In English, often the penultimate
syllable is stressed if the final syllable is tion.
We can capture such a regularity with the Con-
text feature si+1, ti. If the following syllable is
tion and the current syllable is stressed, the fea-
ture {si+1, ti = tion,1} will be true. This feature
will likely receive a positive weight, so that out-
put sequences with a stress before tion receive a
higher rank.
Finally, the full Stress Pattern serves as an im-
portant feature. Note that such a feature would
not be possible in standard sequence predictors,
where such information must be decomposed into
Markov transition features like ti?1ti. In a ranking
framework, we can score output sequences using
their full output pattern. Thus we can easily learn
the rules in languages with regular stress rules. For
languages that do not have a fixed stress rule, pref-
erences for particular patterns can be learned using
this feature.
2tion is a substring composed of three phonemes but we
use its orthographic representation here for clarity.
121
3.3 Pattern-to-Vowel Mapping
The final stage of our system uses the predicted
pattern t to create the stress-marked form of the
word, w?. Note the number of substrings created
by our splitting method always equals the number
of vowels in the word. We can thus simply map
the indicator numbers in t to markers on their cor-
responding vowels to produce the stressed word.
For our example, pronounce ? ron-no-un-ce,
if the SVM chooses the stress pattern, 0-1-0-
0, we produce the correct stress-marked word,
prono?unce. If we instead stress the third vowel, 0-
0-1-0, we produce an incorrect output, pronou?nce.
4 Stress Prediction Experiments
In this section, we evaluate our ranking approach
to stress prediction by assigning stress to spoken
and written words in three languages: English,
German, and Dutch. We first describe the data and
the various systems we evaluate, and then provide
the results.
4.1 Data
The data is extracted from CELEX (Baayen et al,
1996). Following previous work on stress predic-
tion, we randomly partition the data into 85% for
training, 5% for development, and 10% for test-
ing. To make results on German and Dutch com-
parable with English, we reduce the training, de-
velopment, and testing set by 80% for each. Af-
ter removing all duplicated items as well as abbre-
viations, phrases, and diacritics, each training set
contains around 55K words.
In CELEX, stress is labeled on syllables in the
phonetic form of the words. Since our objec-
tive is to assign stress markers to vowels (as de-
scribed in Section 2) we automatically map the
stress markers from the stressed syllables in the
phonetic forms onto phonemes and letters rep-
resenting vowels. For phonemes, the process is
straightforward: we move the stress marker from
the beginning of a syllable to the phoneme which
constitutes the nucleus of the syllable. For let-
ters, we map the stress from the vowel phoneme
onto the orthographic forms using the ALINE al-
gorithm (Dwyer and Kondrak, 2009). The stress
marker is placed on the first letter within the sylla-
ble that represents a vowel sound.3
3Our stand-off stress annotations for English, German,
and Dutch CELEX orthographic data can be downloaded at:
http://www.cs.ualberta.ca/?kondrak/celex.html.
System Eng Ger Dut
P+S P P P
SUBSTRING 96.2 98.0 97.1 93.1
ORACLESYL 95.4 96.4 97.1 93.2
TOPPATTERN 66.8 68.9 64.1 60.8
Table 3: Stress prediction word accuracy (%) on
phonemes for English, German, and Dutch. P:
predicting primary stress only. P+S: primary and
secondary.
CELEX also provides secondary stress annota-
tion for English. We therefore evaluate on both
primary and secondary stress (P+S) in English and
on primary stress assignment alone (P) for En-
glish, German, and Dutch.
4.2 Comparison Approaches
We evaluate three different systems on the letter
and phoneme sequences in the experimental data:
1) SUBSTRING is the system presented in Sec-
tion 3. It uses the vowel-based splitting
method, followed by SVM ranking.
2) ORACLESYL splits the input word into sylla-
bles according to the CELEX gold-standard,
before applying SVM ranking. The output
pattern is evaluated directly against the gold-
standard, without pattern-to-vowel mapping.
3) TOPPATTERN is our baseline system. It uses
the vowel-based splitting method to produce a
substring sequence of length N . Then it simply
chooses the most common stress pattern among
all the stress patterns of length N .
SUBSTRING and ORACLESYL use scores pro-
duced by an SVM ranker trained on the training
data. We employ the ranking mode of the popular
learning package SVMlight (Joachims, 1999). In
each case, we learn a linear kernel ranker on the
training set stress patterns and tune the parameter
that trades-off training error and margin on the de-
velopment set.
We evaluate the systems using word accuracy:
the percent of words for which the output form of
the word, w?, matches the gold standard.
4.3 Results
Table 3 provides results on English, German, and
Dutch phonemes. Overall, the performance of our
automatic stress predictor, SUBSTRING, is excel-
lent. It achieves 98.0% accuracy for predicting
122
System Eng Ger Dut
P+S P P P
SUBSTRING 93.5 95.1 95.9 91.0
ORACLESYL 94.6 96.0 96.6 92.8
TOPPATTERN 65.5 67.6 64.1 60.8
Table 4: Stress prediction word accuracy (%) on
letters for English, German, and Dutch. P: pre-
dicting primary stress only. P+S: primary and sec-
ondary.
primary stress in English, 97.1% in German, and
93.1% in Dutch. It also predicts both primary and
secondary stress in English with high accuracy,
96.2%. Performance is much higher than our base-
line accuracy, which is between 60% and 70%.
ORACLESYL, with longer substrings and hence
sparser data, does not generally improve perfor-
mance. This indicates that perfect syllabification
is unnecessary for phonetic stress assignment.
Our system is a major advance over the pre-
vious state-of-the-art in phonetic stress assign-
ment. For predicting stressed/unstressed syllables
in English, Black et al (1998) obtained a per-
syllable accuracy of 94.6%. We achieve 96.2%
per-word accuracy for predicting both primary and
secondary stress. Others report lower numbers
on English phonemes. Bagshaw (1998) obtained
65%-83.3% per-syllable accuracy using Church
(1985)?s rule-based system. For predicting both
primary and secondary stress, Coleman (2000)
and Pearson et al (2000) report 69.8% and 81.0%
word accuracy, respectively.
The performance on letters (Table 4) is also
quite encouraging. SUBSTRING predicts primary
stress with accuracy above 95% for English and
German, and equal to 91% in Dutch. Performance
is 1-3% lower on letters than on phonemes. On
the other hand, the performance of ORACLESYL
drops much less on letters. This indicates that
most of SUBSTRING?s errors are caused by the
splitting method. Letter vowels may or may not
represent spoken vowels. By creating a substring
for every vowel letter we may produce an incorrect
number of syllables. Our pattern feature is there-
fore less effective.
Nevertheless, SUBSTRING?s accuracy on letters
also represents a clear improvement over previ-
ous work. Webster (2004) reports 80.3% word
accuracy on letters in English and 81.2% in Ger-
man. The most comparable work is Demberg et al
 84
 86
 88
 90
 92
 94
 96
 98
 100
 10000  100000
W
or
d 
Ac
cu
ra
cy
 (%
)
Number of training examples
German
Dutch
English
Figure 1: Stress prediction accuracy on letters.
(2007), which achieves 90.1% word accuracy on
letters in German CELEX, assuming perfect letter
syllabification. In order to reproduce their strict
experimental setup, we re-partition the full set of
German CELEX data to ensure that no overlap of
word stems exists between the training and test
sets. Using the new data sets, our system achieves
a word accuracy of 92.3%, a 2.2% improvement
over Demberg et al (2007)?s result. Moreover, if
we also assume perfect syllabification, the accu-
racy is 94.3%, a 40% reduction in error rate.
We performed a detailed analysis to understand
the strong performance of our system. First of all,
note that an error could happen if a test-set stress
pattern was not observed in the training data; its
correct stress pattern would not be considered as
an output. In fact, no more than two test errors in
any test set were so caused. This strongly justi-
fies the reduced set of outputs used in our ranking
formulation.
We also tested all systems with the Stress Pat-
tern feature removed. Results were worse in all
cases. As expected, it is most valuable for pre-
dicting primary and secondary stress. On English
phonemes, accuracy drops from 96.2% to 95.3%
without it. On letters, it drops from 93.5% to
90.0%. The gain from this feature also validates
our ranking framework, as such arbitrary features
over the entire output sequence can not be used in
standard search-based sequence prediction.
Finally, we examined the relationship between
training data size and performance by plotting
learning curves for letter stress accuracy (Fig-
ure 1). Unlike the tables above, here we use the
123
full set of data in Dutch and German CELEX to
create the largest-possible training sets (255K ex-
amples). None of the curves are levelling off; per-
formance grows log-linearly across the full range.
5 Lexical stress and L2P conversion
In this section, we evaluate various methods of
combining stress prediction with phoneme gener-
ation. We first describe the specific system that we
use for letter-to-phoneme (L2P) conversion. We
then discuss the different ways stress prediction
can be integrated with L2P, and define the systems
used in our experiments. Finally, we provide the
results.
5.1 The L2P system
We combine stress prediction with a state-of-the-
art L2P system (Jiampojamarn et al, 2008). Like
our stress ranker, their system is a data-driven se-
quence predictor that is trained with supervised
learning. The score for each output sequence is
a weighted combination of features. The feature
weights are trained using the Margin Infused Re-
laxed Algorithm (MIRA) (Crammer and Singer,
2003), a powerful online discriminative training
framework. Like other recent L2P systems (Bisani
and Ney, 2002; Marchand and Damper, 2007; Ji-
ampojamarn et al, 2007), this approach does not
generate stress, nor does it consider stress when it
generates phonemes.
For L2P experiments, we use the same training,
testing, and development data as was used in Sec-
tion 4. For all experiments, we use the develop-
ment set to determine at which iteration to stop
training in the online algorithm.
5.2 Combining stress and phoneme
generation
Various methods have been used for combining
stress and phoneme generation. Phonemes can be
generated without regard to stress, with stress as-
signed as a post-process (Bagshaw, 1998; Cole-
man, 2000). Both van den Bosch (1997) and
Black et al (1998) argue that stress should be pre-
dicted at the same time as phonemes. They ex-
pand the output set to distinguish between stressed
and unstressed phonemes. Similarly, Demberg et
al. (2007) produce phonemes, stress, and syllable-
boundaries within a single joint n-gram model.
Pearson et al (2000) generate phonemes and stress
together by jointly optimizing a decision-tree
phoneme-generator and a stress predictor based on
stress pattern counts. In contrast, Webster (2004)
first assigns stress to letters, creating an expanded
input set, and then predicts both phonemes and
stress jointly. The system marks stress on let-
ter vowels by determining the correspondence be-
tween affixes and stress in written words.
Following the above approaches, we can expand
the input or output symbols of our L2P system to
include stress. However, since both decision tree
systems and our L2P predictor utilize only local
context, they may produce invalid global output.
One option, used by Demberg et al (2007), is to
add a constraint to the output generation, requiring
each output sequence to have exactly one primary
stress.
We enhance this constraint, based on the obser-
vation that the number of valid output sequences
is fairly limited (Section 3.2). The modified sys-
tem produces the highest-scoring sequence such
that the output?s corresponding stress pattern has
been observed in our training data. We call this
the stress pattern constraint. This is a tighter
constraint than having only one primary stress.4
Another advantage is that it provides some guid-
ance for the assignment of secondary stress.
Inspired by the aforementioned strategies, we
evaluate the following approaches:
1) JOINT: The L2P system?s input sequence is let-
ters, the output sequence is phonemes+stress.
2) JOINT+CONSTR: Same as JOINT, except it se-
lects the highest scoring output that obeys the
stress pattern constraint.
3) POSTPROCESS: The L2P system?s input is let-
ters, the output is phonemes. It then applies the
SVM stress ranker (Section 3) to the phonemes
to produce the full phoneme+stress output.
4) LETTERSTRESS: The L2P system?s input is
letters+stress, the output is phonemes+stress.
It creates the stress-marked letters by applying
the SVM ranker to the input letters as a pre-
process.
5) ORACLESTRESS: The same input/output as
LETTERSTRESS , except it uses the gold-
standard stress on letters (Section 4.1).
4In practice, the L2P system generates a top-N list, and
we take the highest-scoring output on the list that satisfies
the constraint. If none satisfy the constraint, we take the top
output that has only one primary stress.
124
System Eng Ger Dut
P+S P P P
JOINT 78.9 80.0 86.0 81.1
JOINT+CONSTR 84.6 86.0 90.8 88.7
POSTPROCESS 86.2 87.6 90.9 88.8
LETTERSTRESS 86.5 87.2 90.1 86.6
ORACLESTRESS 91.4 91.4 92.6 94.5
Festival 61.2 62.5 71.8 65.1
Table 5: Combined phoneme and stress predic-
tion word accuracy (%) for English, German, and
Dutch. P: predicting primary stress only. P+S:
primary and secondary.
Note that while the first approach uses only
local information to make predictions (features
within a context window around the current let-
ter), systems 2 to 5 leverage global information in
some manner: systems 3 and 4 use the predictions
of our stress ranker, while 2 uses a global stress
pattern constraint.5
We also generated stress and phonemes using
the popular Festival Speech Synthesis System6
(version 1.96, 2004) and report its accuracy.
5.3 Results
Word accuracy results for predicting both
phonemes and stress are provided in Table 5.
First of all, note that the JOINT approach,
which simply expands the output set, is 4%-
8% worse than all other comparison systems
across the three languages. These results clearly
indicate the drawbacks of predicting stress us-
ing only local information. In English, both
LETTERSTRESS and POSTPROCESS perform
best, while POSTPROCESS and the constrained
system are highest on German and Dutch. Results
using the oracle letter stress show that given
perfect stress assignment on letters, phonemes
and stress can be predicted very accurately, in all
cases above 91%.
We also found that the phoneme prediction ac-
curacy alone (i.e., without stress) is quite simi-
lar for all the systems. The gains over JOINT
on combined stress and phoneme accuracy are
almost entirely due to more accurate stress as-
signment. Utilizing the oracle stress on letters
markedly improves phoneme prediction in English
5This constraint could also help the other systems. How-
ever, since they already use global information, it yields only
marginal improvements.
6http://www.cstr.ed.ac.uk/projects/festival/
(from 88.8% to 91.4%). This can be explained by
the fact that English vowels are often reduced to
schwa when unstressed (Section 2).
Predicting both phonemes and stress is a chal-
lenging task, and each of our globally-informed
systems represents a major improvement over pre-
vious work. The accuracy of Festival is much
lower even than our JOINT approach, but the rel-
ative performance on the different languages is
quite similar.
A few papers report accuracy on the combined
stress and phoneme prediction task. The most di-
rectly comparable work is van den Bosch (1997),
which also predicts primary and secondary stress
using English CELEX data. However, the re-
ported word accuracy is only 62.1%. Three other
papers report word accuracy on phonemes and
stress, using different data sets. Pearson et al
(2000) report 58.5% word accuracy for predicting
phonemes and primary/secondary stress. Black et
al. (1998) report 74.6% word accuracy in English,
while Webster (2004) reports 68.2% on English
and 82.9% in German (all primary stress only).
Finally, Demberg et al (2007) report word accu-
racy on predicting phonemes, stress, and syllab-
ification on German CELEX data. They achieve
86.3% word accuracy.
6 Conclusion
We have presented a discriminative ranking ap-
proach to lexical stress prediction, which clearly
outperforms previously developed systems. The
approach is largely language-independent, appli-
cable to both orthographic and phonetic repre-
sentations, and flexible enough to handle multi-
ple stress levels. When combined with an exist-
ing L2P system, it achieves impressive accuracy
in generating pronunciations together with their
stress patterns. In the future, we will investigate
additional features to leverage syllabic and mor-
phological information, when available. Kernel
functions could also be used to automatically cre-
ate a richer feature space; preliminary experiments
have shown gains in performance using polyno-
mial and RBF kernels with our stress ranker.
Acknowledgements
This research was supported by the Natural
Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Al-
berta Informatics Circle of Research Excellence.
125
References
Joanne Arciuli and Linda Cupples. 2006. The pro-
cessing of lexical stress during visual word recog-
nition: Typicality effects and orthographic corre-
lates. Quarterly Journal of Experimental Psychol-
ogy, 59(5):920?948.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX2 lexical database.
LDC96L14.
Paul C. Bagshaw. 1998. Phonemic transcription by
analogy in text-to-speech synthesis: Novel word
pronunciation and lexicon compression. Computer
Speech and Language, 12(2):119?142.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic syllabification with structured
SVMs for letter-to-phoneme conversion. In ACL-
08: HLT, pages 568?576.
Maximilian Bisani and Hermann Ney. 2002. Investi-
gations on joint-multigram models for grapheme-to-
phoneme conversion. In ICSLP, pages 105?108.
Alan W Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In
The 3rd ESCA Workshop on Speech Synthesis, pages
77?80.
Noam Chomsky and Morris Halle. 1968. The sound
pattern of English. New York: Harper and Row.
Kenneth Church. 1985. Stress assignment in letter
to sound rules for speech synthesis. In ACL, pages
246?253.
Cynthia G. Clopper. 2002. Frequency of stress pat-
terns in English: A computational analysis. IULC
Working Papers Online.
John Coleman. 2000. Improved prediction of stress in
out-of-vocabulary words. In IEEE Seminar on the
State of the Art in Speech Synthesis.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?70.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphologi-
cal preprocessing for grapheme-to-phoneme conver-
sion. In ACL, pages 96?103.
Qing Dou. 2009. An SVM ranking approach to stress
assignment. Master?s thesis, University of Alberta.
Kenneth Dwyer and Grzegorz Kondrak. 2009. Reduc-
ing the annotation effort for letter-to-phoneme con-
version. In ACL-IJCNLP.
Erik C. Fudge. 1984. English word-stress. London:
Allen and Unwin.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In NAACL-HLT 2007, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL-
08: HLT, pages 905?913.
Thorsten Joachims. 1999. Making large-scale Support
Vector Machine learning practical. In B. Scho?lkopf
and C. Burges, editors, Advances in Kernel Meth-
ods: Support Vector Machines, pages 169?184.
MIT-Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD, pages 133?142.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282?289.
Yannick Marchand and Robert I. Damper. 2007. Can
syllabification improve pronunciation by analogy of
English? Natural Language Engineering, 13(1):1?
24.
Steve Pearson, Roland Kuhn, Steven Fincke, and Nick
Kibre. 2000. Automatic methods for lexical stress
assignment and syllabification. In ICSLP, pages
423?426.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields.
In ICML, pages 736?743.
Lara Tagliapietra and Patrizia Tabossi. 2005. Lexical
stress effects in Italian spoken word recognition. In
The XXVII Annual Conference of the Cognitive Sci-
ence Society, pages 2140?2144.
Antal van den Bosch. 1997. Learning to pronounce
written words: A study in inductive language learn-
ing. Ph.D. thesis, Universiteit Maastricht.
Gabriel Webster. 2004. Improving letter-
to-pronunciation accuracy with automatic
morphologically-based stress prediction. In
ICSLP, pages 2573?2576.
Briony Williams. 1987. Word stress assignment in a
text-to-speech synthesis system for British English.
Computer Speech and Language, 2:235?272.
George Kingsley Zipf. 1929. Relative frequency as a
determinant of phonetic change. Harvard Studies in
Classical Philology, 15:1?95.
126
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 28?31,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
DIRECTL: a Language-Independent Approach to Transliteration
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,abhargava,qdou,dwyer,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL: an online discrimi-
native sequence prediction model that em-
ploys a many-to-many alignment between
target and source. Our system incorpo-
rates input segmentation, target charac-
ter prediction, and sequence modeling in
a unified dynamic programming frame-
work. Experimental results suggest that
DIRECTL is able to independently dis-
cover many of the language-specific reg-
ularities in the training data.
1 Introduction
In the transliteration task, it seems intuitively im-
portant to take into consideration the specifics of
the languages in question. Of particular impor-
tance is the relative character length of the source
and target names, which vary widely depending on
whether languages employ alphabetic, syllabic, or
ideographic scripts. On the other hand, faced with
the reality of thousands of potential language pairs
that involve transliteration, the idea of a language-
independent approach is highly attractive.
In this paper, we present DIRECTL: a translit-
eration system that, in principle, can be applied to
any language pair. DIRECTL treats the transliter-
ation task as a sequence prediction problem: given
an input sequence of characters in the source lan-
guage, it produces the most likely sequence of
characters in the target language. In Section 2,
we discuss the alignment of character substrings
in the source and target languages. Our transcrip-
tion model, described in Section 3, is based on
an online discriminative training algorithm that
makes it possible to efficiently learn the weights
of a large number of features. In Section 4, we
provide details of alternative approaches that in-
corporate language-specific information. Finally,
in Section 5 and 6, we compare the experimental
results of DIRECTL with its variants that incor-
porate language-specific pre-processing, phonetic
alignment, and manual data correction.
2 Transliteration alignment
In the transliteration task, training data consist of
word pairs that map source language words to
words in the target language. The matching be-
tween character substrings in the source word and
target word is not explicitly provided. These hid-
den relationships are generally known as align-
ments. In this section, we describe an EM-based
many-to-many alignment algorithm employed by
DIRECTL. In Section 4, we discuss an alternative
phonetic alignment method.
We apply an unsupervised many-to-many align-
ment algorithm (Jiampojamarn et al, 2007) to the
transliteration task. The algorithm follows the ex-
pectation maximization (EM) paradigm. In the
expectation step shown in Algorithm 1, partial
counts ? of the possible substring alignments are
collected from each word pair (xT , yV ) in the
training data; T and V represent the lengths of
words x and y, respectively. The forward prob-
ability ? is estimated by summing the probabili-
ties of all possible sequences of substring pairings
from left to right. The FORWARD-M2M procedure
is similar to lines 5 through 12 of Algorithm 1, ex-
cept that it uses Equation 1 on line 8, Equation 2
on line 12, and initializes ?0,0 := 1. Likewise, the
backward probability ? is estimated by summing
the probabilities from right to left.
?t,v += ?(xtt?i+1, ?)?t?i,v (1)
?t,v += ?(xtt?i+1, yvv?j+1)?t?i,v?j (2)
The maxX and maxY variables specify the
maximum length of substrings that are permitted
when creating alignments. Also, for flexibility, we
allow a substring in the source word to be aligned
with a ?null? letter (?) in the target word.
28
Algorithm 1: Expectation-M2M alignment
Input: xT , yV ,maxX,maxY, ?
Output: ?
? := FORWARD-M2M (xT , yV ,maxX,maxY )1
? := BACKWARD-M2M (xT , yV ,maxX,maxY )2
if (?T,V = 0) then3
return4
for t = 0 . . . T , v = 0 . . . V do5
if (t > 0) then6
for i = 1 . . .maxX st t? i ? 0 do7
?(xtt?i+1, ?) +=
?t?i,v?(xtt?i+1,?)?t,v
?T,V8
if (v > 0 ? t > 0) then9
for i = 1 . . .maxX st t? i ? 0 do10
for j = 1 . . . maxY st v ? j ? 0 do11
?(xtt?i+1, yvv?j+1) +=
?t?i,v?j?(xtt?i+1,y
v
v?j+1)?t,v
?T,V12
In the maximization step, we normalize the par-
tial counts ? to the alignment probability ? using
the conditional probability distribution. The EM
steps are repeated until the alignment probability
? converges. Finally, the most likely alignment for
each word pair in the training data is computed
with the standard Viterbi algorithm.
3 Discriminative training
We adapt the online discriminative training frame-
work described in (Jiampojamarn et al, 2008) to
the transliteration task. Once the training data has
been aligned, we can hypothesize that the ith let-
ter substring xi ? x in a source language word
is transliterated into the ith substring yi ? y in
the target language word. Each word pair is rep-
resented as a feature vector ?(x,y). Our feature
vector consists of (1) n-gram context features, (2)
HMM-like transition features, and (3) linear-chain
features. The n-gram context features relate the
letter evidence that surrounds each letter xi to its
output yi. We include all n-grams that fit within
a context window of size c. The c value is deter-
mined using a development set. The HMM-like
transition features express the cohesion of the out-
put y in the target language. We make a first order
Markov assumption, so that these features are bi-
grams of the form (yi?1, yi). The linear-chain fea-
tures are identical to the context features, except
that yi is replaced with a bi-gram (yi?1, yi).
Algorithm 2 trains a linear model in this fea-
ture space. The procedure makes k passes over
the aligned training data. During each iteration,
the model produces the nmost likely output words
Y?j in the target language for each input word xj
in the source language, based on the current pa-
Algorithm 2: Online discriminative training
Input: Data {(x1,y1), (x2,y2), . . . , (xm,ym)},
number of iterations k, size of n-best list n
Output: Learned weights ?
? := ~01
for k iterations do2
for j = 1 . . .m do3
Y?j = {y?j1, . . . , y?jn} = argmaxy[? ? ?(xj ,y)]4
update ? according to Y?j and yj5
return ?6
rameters ?. The values of k and n are deter-
mined using a development set. The model param-
eters are updated according to the correct output
yj and the predicted n-best outputs Y?j , to make
the model prefer the correct output over the in-
correct ones. Specifically, the feature weight vec-
tor ? is updated by using MIRA, the Margin In-
fused Relaxed Algorithm (Crammer and Singer,
2003). MIRA modifies the current weight vector
?o by finding the smallest changes such that the
new weight vector ?n separates the correct and in-
correct outputs by a margin of at least ?(y, y?), the
loss for a wrong prediction. We define this loss to
be 0 if y? = y; otherwise it is 1 + d, where d is
the Levenshtein distance between y and y?. The
update operation is stated as a quadratic program-
ming problem in Equation 3. We utilize a function
from the SVMlight package (Joachims, 1999) to
solve this optimization problem.
min?n ? ?n ? ?o ?
subject to ?y? ? Y? :
?n ? (?(x,y) ? ?(x, y?)) ? ?(y, y?)
(3)
The argmax operation is performed by an exact
search algorithm based on a phrasal decoder (Zens
and Ney, 2004). This decoder simultaneously
finds the l most likely substrings of letters x that
generate the most probable output y, given the
feature weight vector ? and the input word xT .
The search algorithm is based on the following dy-
namic programming recurrence:
Q(0, $) = 0
Q(t, p) = max
p?,p,
t?maxX?t?<t
{? ? ?(xtt?+1, p?, p) +Q(t?, p?)}
Q(T+1, $) = max
p?
{? ? ?($, p?, $) +Q(T, p?)}
To find the n-best predicted outputs, the table
Q records the top n scores for each output sub-
string that has the suffix p substring and is gen-
erated by the input letter substring xt1; here, p? is
29
a sub-output generated during the previous step.
The notation ?(xtt?+1, p?, p) is a convenient way
to describe the components of our feature vector
?(x,y). The n-best predicted outputs Y? can be
discovered by backtracking from the end of the ta-
ble, which is denoted by Q(T + 1, $).
4 Beyond DIRECTL
4.1 Intermediate phonetic representation
We experimented with converting the original Chi-
nese characters to Pinyin as an intermediate repre-
sentation. Pinyin is the most commonly known
Romanization system for Standard Mandarin. Its
alphabet contains the same 26 letters as English.
Each Chinese character can be transcribed pho-
netically into Pinyin. Many resources for Pinyin
conversion are available online.1 A small percent-
age of Chinese characters have multiple pronunci-
ations represented by different Pinyin representa-
tions. For those characters (about 30 characters in
the transliteration data), we manually selected the
pronunciations that are normally used for names.
This preprocessing step significantly reduces the
size of target symbols from 370 distinct Chinese
characters to 26 Pinyin symbols which enables our
system to produce better alignments.
In order to verify whether the addition of
language-specific knowledge can improve the
overall accuracy, we also designed intermediate
representations for Russian and Japanese. We
focused on symbols that modify the neighbor-
ing characters without producing phonetic output
themselves: the two yer characters in Russian,
and the long vowel and sokuon signs in Japanese.
Those were combined with the neighboring char-
acters, creating new ?super-characters.?
4.2 Phonetic alignment with ALINE
ALINE (Kondrak, 2000) is an algorithm that
performs phonetically-informed alignment of two
strings of phonemes. Since our task requires
the alignment of characters representing different
writing scripts, we need to first replace every char-
acter with a phoneme that is the most likely to be
produced by that character.
We applied slightly different methods to the
test languages. In converting the Cyrillic script
into phonemes, we take advantage of the fact
that the Russian orthography is largely phonemic,
which makes it a relatively straightforward task.
1For example, http://www.chinesetopinyin.com/
In Japanese, we replace each Katakana character
with one or two phonemes using standard tran-
scription tables. For the Latin script, we simply
treat every letter as an IPA symbol (International
Phonetic Association, 1999). The IPA contains a
subset of 26 letter symbols that tend to correspond
to the usual phonetic value that the letter repre-
sents in the Latin script. The Chinese characters
are first converted to Pinyin, which is then handled
in the same way as the Latin script.
Similar solutions could be engineered for other
scripts. We observed that the transcriptions do not
need to be very precise in order for ALINE to pro-
duce high quality alignments.
4.3 System combination
The combination of predictions produced by sys-
tems based on different principles may lead to im-
proved prediction accuracy. We adopt the follow-
ing combination algorithm. First, we rank the in-
dividual systems according to their top-1 accuracy
on the development set. To obtain the top-1 pre-
diction for each input word, we use simple voting,
with ties broken according to the ranking of the
systems. We generalize this approach to handle n-
best lists by first ordering the candidate translitera-
tions according to the highest rank assigned by any
of the systems, and then similarly breaking ties by
voting and system ranking.
5 Evaluation
In the context of the NEWS 2009 Machine
Transliteration Shared Task (Li et al, 2009), we
tested our system on six data sets: from English to
Chinese (EnCh) (Li et al, 2004), Hindi (EnHi),
Russian (EnRu) (Kumaran and Kellner, 2007),
Japanese Katakana (EnJa), and Korean Hangul
(EnKo); and from Japanese Name to Japanese
Kanji (JnJk)2. We optimized the models? param-
eters by training on the training portion of the
provided data and measuring performance on the
development portion. For the final testing, we
trained the models on all the available labeled data
(training plus development data). For each data
set, we converted any uppercase letters to lower-
case. Our system outputs the top 10 candidate an-
swers for each input word.
Table 1 reports the performance of our system
on the development and final test sets, measured
in terms of top-1 word accuracy (ACC). For cer-
tain language pairs, we tested variants of the base
2http://www.cjk.org/
30
Task Model Dev Test
EnCh DIRECTL 72.4 71.7
INT(M2M) 73.9 73.4
INT(ALINE) 73.8 73.2
COMBINED 74.8 74.6
EnHi DIRECTL 41.4 49.8
DIRECTL+MC 42.3 50.9
EnJa DIRECTL 49.9 50.0
INT(M2M)? 49.6 49.2
INT(ALINE) 48.3 51.0
COMBINED? 50.6 50.5
EnKo DIRECTL 36.7 38.7
EnRu DIRECTL 80.2 61.3
INT(M2M) 80.3 60.8
INT(ALINE) 80.0 60.7
COMBINED? 80.3 60.8
JnJk DIRECTL 53.5 56.0
Table 1: Top-1 word accuracy on the development
and test sets. The asterisk denotes the results ob-
tained after the test reference sets were released.
system described in Section 4. DIRECTL refers
to our language-independent model, which uses
many-to-many alignments. The INT abbreviation
denotes the models operating on the language-
specific intermediate representations described in
Section 4.1. The alignment algorithm (ALINE or
M2M) is given in brackets.
In the EnHi set, many names consisted of mul-
tiple words: we assumed a one-to-one correspon-
dence between consecutive English words and
consecutive Hindi words. In Table 1, the results in
the first row (DIRECTL) were obtained with an au-
tomatic cleanup script that replaced hyphens with
spaces, deleted the remaining punctuation and nu-
merical symbols, and removed 43 transliteration
pairs with a disagreement between the number of
source and target words. The results in the sec-
ond row (DIRECTL+MC) were obtained when the
cases with a disagreement were individually ex-
amined and corrected by a Hindi speaker.
We did not incorporate any external resources
into the models presented in Table 1. In order
to emphasize the performance of our language-
independent approach, we consistently used the
DIRECTL model for generating our ?standard?
runs on all six language pairs, regardless of its rel-
ative performance on the development sets.
6 Discussion
DIRECTL, our language-independent approach to
transliteration achieves excellent results, espe-
cially on the EnCh, EnRu, and EnHi data sets,
which represent a wide range of language pairs
and writing scripts. Both the many-to-many
and phonetic alignment algorithms produce high-
quality alignments. The former can be applied di-
rectly to the training data without the need for an
intermediate representation, while the latter does
not require any training. Surprisingly, incorpo-
ration of language-specific intermediate represen-
tations does not consistently improve the perfor-
mance of our system, which indicates that DI-
RECTL may be able to discover the structures im-
plicit in the training data without additional guid-
ance. The EnHi results suggest that manual clean-
ing of noisy data can yield noticeable gains in ac-
curacy. On the other hand, a simple method of
combining predictions from different systems pro-
duced clear improvement on the EnCh set, but
mixed results on two other sets. More research on
this issue is warranted.
Acknowledgments
This research was supported by the Alberta Inge-
nuity, Informatics Circle of Research Excellence
(iCORE), and Natural Sciences of Engineering
Research Council of Canada (NSERC).
References
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
International Phonetic Association. 1999. Handbook
of the International Phonetic Association. Cam-
bridge University Press.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL, pages 905?913.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in kernel methods:
support vector learning, pages 169?184. MIT Press.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. SI-
GIR, pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source channel model for machine transliteration. In
Proc. ACL, pages 159?166.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. ACL-
IJCNLP Named Entities Workshop.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proc. HLT-NAACL, pages 257?264.
31
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 266?275, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Large Scale Decipherment for Out-of-Domain Machine Translation
Qing Dou and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,knight}@isi.edu
Abstract
We apply slice sampling to Bayesian de-
cipherment and use our new decipherment
framework to improve out-of-domain machine
translation. Compared with the state of the
art algorithm, our approach is highly scalable
and produces better results, which allows us
to decipher ciphertext with billions of tokens
and hundreds of thousands of word types with
high accuracy. We decipher a large amount
of monolingual data to improve out-of-domain
translation and achieve significant gains of up
to 3.8 BLEU points.
1 Introduction
Nowadays, state of the art statistical machine trans-
lation (SMT) systems are built using large amounts
of bilingual parallel corpora. Those corpora are
used to estimate probabilities of word-to-word trans-
lation, word sequences rearrangement, and even
syntactic transformation. Unfortunately, as paral-
lel corpora are expensive and not available for ev-
ery domain, performance of SMT systems drops
significantly when translating out-of-domain texts
(Callison-Burch et al2008).
In general, it is easier to obtain in-domain mono-
lingual corpora. Is it possible to use domain specific
monolingual data to improve an MT system trained
on parallel texts from a different domain? Some re-
searchers have attempted to do this by adding a do-
main specific dictionary (Wu et al2008), or mining
unseen words (Daume? and Jagarlamudi, 2011) us-
ing one of several translation lexicon induction tech-
niques (Haghighi et al2008; Koehn and Knight,
2002; Rapp, 1995). However, a dictionary is not al-
ways available, and it is difficult to assign probabil-
ities to a translation lexicon.
(Ravi and Knight, 2011b) have shown that one
can use decipherment to learn a full translation
model from non-parallel data. Their approach is able
to find translations, and assign probabilities to them.
But their work also has certain limitations. First of
all, the corpus they use to build the translation sys-
tem has a very small vocabulary. Secondly, although
their algorithm is able to handle word substitution
ciphers with limited vocabulary, its deciphering ac-
curacy is low.
The contributions of this work are:
? We improve previous decipherment work by in-
troducing a more efficient sampling algorithm.
In experiments, our new method improves de-
ciphering accuracy from 82.5% to 88.1% on
(Ravi and Knight, 2011b)?s domain specific
data set. Furthermore, we also solve a very
large word substitution cipher built from the
English Gigaword corpus and achieve 92.2%
deciphering accuracy on news text.
? With the ability to handle a much larger vocab-
ulary, we learn a domain specific translation ta-
ble from a large amount of monolingual data
and use the translation table to improve out-of-
domain machine translation. In experiments,
we observe significant gains of up to 3.8 BLEU
points. Unlike previous works, the translation
table we build from monolingual data do not
only contain unseen words but also words seen
in parallel data.
266
2 Word Substitution Ciphers
Before we present our new decipherment frame-
work, we quickly review word substitution decipher-
ment.
Recently, there has been an increasing interest in
decipherment work (Ravi and Knight, 2011a; Ravi
and Knight, 2008). While letter substitution ciphers
can be solved easily, nobody has been able to solve
a word substitution cipher with high accuracy.
As shown in Figure 1, a word substitution cipher
is generated by replacing each word in a natural lan-
guage (plaintext) sequence with a cipher token ac-
cording to a substitution table. The mapping in the
table is deterministic ? each plaintext word type is
only encoded with one unique cipher token. Solv-
ing a word substitution cipher means recovering the
original plaintext from the ciphertext without know-
ing the substitution table. The only thing we rely on
is knowledge about the underlying language.
Figure 1: Encoding and Decipherment of a Word Substi-
tution Cipher
How can we solve a word substitution cipher?
The approach is similar to those taken by cryptana-
lysts who try to recover keys that convert encrypted
texts to readable texts. Suppose we observe a large
cipher string f and want to decipher it into English e.
We can follow the work in (Ravi and Knight, 2011b)
and assume that the cipher string f is generated in
the following way:
? Generate English plaintext sequence e =
e1, e2...en with probability P(e).
? Replace each English plaintext token ei with a
cipher token fi with probability P (fi|ei).
Based on the above generative story, we write the
probability of the cipher string f as:
P (f)? =
?
e
P (e) ?
n
?
i
P?(fi|ei) (1)
We use this equation as an objective function for
maximum likelihood training. In the equation, P (e)
is given by an ngram language model, which is
trained using a large amount of monolingual texts.
The rest of the task is to manipulate channel prob-
abilities P?(fi|ei) so that the probability of the ob-
served texts P (f)? is maximized.
Theoretically, we can directly apply EM, as pro-
posed in (Knight et al2006), or Bayesian decipher-
ment (Ravi and Knight, 2011a) to solve the prob-
lem. However, unlike letter substitution ciphers,
word substitution ciphers pose much greater chal-
lenges to algorithm scalability. To solve a word sub-
stitution cipher, the EM algorithm has a computa-
tional complexity of O(N ? V 2 ? R) and the com-
plexity of Bayesian method is O(N ? V ? R), where
V is the size of plaintext vocabulary, N is the length
of ciphertext, and R is the number of iterations. In
the world of word substitution ciphers, both V and
N are very large, making these approaches impracti-
cal. (Ravi and Knight, 2011b) propose several mod-
ifications to the existing algorithms. However, the
modified algorithms are only an approximation of
the original algorithms and produce poor decipher-
ing accuracy, and they are still unable to handle very
large scale ciphers.
To address the above problems, we propose the
following two new improvements to previous deci-
pherment methods.
? We apply slice sampling (Neal, 2000) to scale
up to ciphers with a very large vocabulary.
? Instead of deciphering using the original ci-
phertext, we break the ciphertext into bigrams,
collect their counts, and use the bigrams with
their counts for decipherment.
267
The new improvements allow us to solve a word
substitution cipher with billions of tokens and hun-
dreds of thousands of word types. Through better
approximation, we achieve a significant increase in
deciphering accuracy. In the following section, we
present details of our new approach.
3 Slice Sampling for Bayesian
Decipherment
In this section, we first give an introduction to
Bayesian decipherment and then describe how to use
slice sampling for it.
3.1 Bayesian Decipherment
Bayesian inference has been widely used in natural
language processing (Goldwater and Griffiths, 2007;
Blunsom et al2009; Ravi and Knight, 2011b). It is
very attractive for problems like word substitution
ciphers for the following reasons. First, there are
no memory bottlenecks as compared to EM, which
has an O(N ? V 2) space complexity. Second, priors
encourage the model to learn a sparse distribution.
The inference is usually performed using Gibbs
sampling. For decipherment, a Gibbs sampler keeps
drawing samples from plaintext sequences accord-
ing to derivation probability P (d):
P (d) = P (e) ?
n
?
i
P (ci|ei) (2)
In Bayesian inference, P (e) is still given by an
ngram language model, while the channel probabil-
ity is modeled by the Chinese Restaurant Process
(CRP):
P (ci|ei) =
? ? prior + count(ci, ei)
?+ count(ei)
(3)
Where prior is the base distribution (we set prior
to 1C in all our experiments, where C is the number
of word types in ciphertext), and count, also called
?cache?, records events that occurred in the history.
Each sampling operation involves changing a plain-
text token ei, which has V possible choices, where
V is the plaintext vocabulary size, and the final sam-
ple is chosen with probability P (d)?V
n=1 P (d)
.
3.2 Slice Sampling
With Gibbs sampling, one has to evaluate all possi-
ble plaintext word types (10k?1M) for each sam-
ple decision. This become intractable when the vo-
cabulary is large and the ciphertext is long. Slice
sampling (Neal, 2000) can solve this problem by au-
tomatically adjusting the number of samples to be
considered for each sampling operation.
Suppose the derivation probability for current
sample is P (current s). Then slice sampling draws
a sample in two steps:
? Select a threshold T uniformly from the range
{0, P (current s)}.
? Draw a new sample new s uniformly from a
pool of candidates: {new s|P (new s) > T}.
From the above two steps, we can see that given a
threshold T , we only need to consider those samples
whose probability is higher than the threshold. This
will lead to a significant reduction on the number
of samples to be considered, if probabilities of the
most samples are below T . In practice, the first step
is easy to implement, while it is difficult to make the
second step efficient. An obvious way to collect can-
didate samples is to go over all possible samples and
record those with probabilities higher than T . How-
ever, doing this will not save any time. Fortunately,
for Bayesian decipherment, we are able to complete
the second step efficiently.
According to Equation 1, the probability of the
current sample is given by a language model P (e)
and a channel model P (c|e). The language model
is usually an ngram language model. Suppose our
current sample current s contains English tokens
X , Y , and Z at position i ? 1, i, and i + 1 respec-
tively. Let ci be the cipher token at position i. To
obtain a new sample, we just need to change token
Y to Y ?. Since the rest of the sample stays the same,
we only need to calculate the probability of any tri-
gram 1: P (XY ?Z) and the channel model probabil-
ity: P (ci|Y ?), and multiply them together as shown
in Equation 4.
P (XY ?Z) ? P (ci|Y ?) (4)
1The probability is given by a bigram language model.
268
In slice sampling, each sampling operation has
two steps. For the first step, we choose a thresh-
old T uniformly between 0 and P (XY Z) ?P (ci|Y ).
For the second step, there are two cases.
First, we notice that two types of Y ? are more
likely to pass the threshold T : (1) Those that have
a very high trigram probability , and (2) those that
have high channel model probability. To find can-
didates that have high trigram probability, we build
sorted lists ranked by P (XY ?Z), which can be pre-
computed off-line. We only keep the top K En-
glish words for each of the sorted list. When the
last item YK in the list satisfies P (XYkZ) ? prior <
T , We draw a sample in the following way: set
A = {Y ?|P (XY ?Z) ? prior > T} and set B =
{Y ?|count(ci, Y ?) > 0}, then we only need to sam-
ple Y ? uniformly from A ? B until Equation 4 is
greater than T . 2
Second, what happens when the last item YK in
the list does not satisfy P (XYkZ) ? prior < T ?
Then we always choose a word Y ? randomly and ac-
cept it as a new sample if Equation 4 is greater than
T .
Our algorithm alternates between the two cases.
The actual number of choices the algorithm looks at
depends on the K and the total number of possible
choices. In different experiments, we find that when
K = 500, the algorithm only looks at 0.06% of all
possible choices. When K = 2000, this further re-
duces to 0.007%.
3.3 Deciphering with Bigrams
Since we can decipher with a bigram language
model, we posit that a frequency list of ciphertext
bigrams may contain enough information for deci-
pherment. In our letter substitution experiments, we
find that breaking ciphertext into bigrams doesn?t
hurt decipherment accuracy. Table 1 shows how full
English sentences in the original data are broken into
bigrams and their counts.
Instead of doing sampling on full sentences, we
treat each bigram as a full ?sentence?. There are
2It is easy to prove that all other candidates that are not in
the sorted list and with count(ci, Y ?) = 0 have a upper bound
probability: P (XYkZ) ? prior. Therefore, they are ignored
when P (XYkZ) ? prior < T .
man they took our land .
they took our arable land .
took our 2
they took 2
land . 2
man they 1
arable land 1
Table 1: Converting full sentences to bigrams
two advantages to use bigrams and their counts for
decipherment.
First of all, the bigrams and counts are a much
more compact representation of the original cipher-
text with full sentences. For instance, after breaking
a billion tokens from the English Gigaword corpus,
we find only 29m bigrams and 58m tokens, which
is only 1/17 of the original text. In practice, we fur-
ther discard all bigrams with low frequency, which
makes the ciphertext even shorter.
Secondly, using bigrams significantly reduces the
number of sorted lists (from |V |2 to 2|V |) mentioned
in the previous section. The number of lists reduces
from |V |2 to 2|V | because words in a bigram only
have one neighbor. Therefore, for any word W in a
bigram, we need only 2|V | lists (?words to the right
of W? and ?words to the left of W?) instead of |V |2
lists (?pairs of words that surround W?).
3.4 Iterative Sampling
Although we can directly apply slice sampling on
a large number of bigrams, we find that gradually
including less frequent bigrams into a sampling pro-
cess saves deciphering time ? we call this iterative
sampling:
? Break the ciphertext into bigrams and collect
their counts
? Keep bigrams whose counts are greater than a
threshold ?. Then initialize the first sample
randomly and use slice sampling to perform
maximum likelihood training. In the end, ex-
tract a translation table T according to the final
sample.
? Lower the threshold ? to include more bi-
grams into the sampling process. Initialize the
first sample using the translation table obtained
from the previous sampling run (for each ci-
269
pher token f, choose a plaintext token e whose
P (e|f) is the largest). Perform sampling again.
? Repeat until ? = 1.
3.5 Parallel Sampling
Inspired by (Newman et al2009), our parallel sam-
pling procedure is described below:
? Collect bigrams and their counts from cipher-
text and split the bigrams into N parts.
? Run slice sampling on each part for 5 iterations
independently.
? Combine counts from each part to form a new
count table and run sampling again on each part
using the new table.3
4 Decipherment Experiments
In this section, we evaluate our new sampling algo-
rithm in two different experiments. In the first ex-
periment, we compare our method with (Ravi and
Knight, 2011b) on their data set to prove correct-
ness of our approach. In the second experiment, we
scale up to the whole English Gigaword corpus and
achieve a much higher deciphering accuracy.
4.1 Deciphering Transtac Corpus
4.1.1 Data
We split the Transtac corpus the same way it was
split in (Ravi and Knight, 2011b). The data used to
create ciphertext consists of 1 million tokens, and
3397 word types. The data for language model
training contains 2.7 million tokens and 25761 word
types.4 The ciphertext is created by replacing each
English word with a cipher word.
We use a bigram language model for decipher-
ment training. When the training terminates, a trans-
lation table with probability P (c|e) is built based on
the counts collected from the final sample. For de-
coding, we employ a trigram language model using
full sentences. We use Moses (Koehn et al2007)
3Except for combining the counts to form a new count table,
other parameters remain the same. For instance, each part i has
its own prior set to 1Ci , where Ci is the number of word types
in that part of ciphertext.
4In practice, we replaced singletons with a ?UNK? symbol,
leaving around 16904 word types.
Method Deciphering Accuracy
Ravi and Knight 80.0 (with bigram LM)
82.5 (with trigram LM)
Slice Sampling 88.1 (with bigram LM)
Table 2: Decipherment Accuracy on Transtac Corpus
from (Ravi and Knight, 2011b)
Gold Decoded
man i?ve come to file
a complaint against
some people .
man i?ve come to hand
a telephone lines some
people .
man they took our land
.
man they took our
farm .
they took our arable
land .
they took our slide
door .
okay man . okay man .
eighty donums . miflih donums .
Table 3: Sample Decoding Results on Transtac Corpus
from (Ravi and Knight, 2011b)
to perform the decoding. We set the distortion limit
to 0 and cube the translation probabilities. Essen-
tially, Moses tries to find an English sequence e that
maximizes P (e) ? P (c|e)3
4.1.2 Results
We evaluate the performance of our algorithm
by decipherment accuracy, which measures the per-
centage of correctly deciphered cipher tokens. Table
2 compares the deciphering accuracy with the state
of the art algorithm.
Results show that our algorithm improves the de-
ciphering accuracy to 88.1%, which amounts to 33%
reduction in error rate. This justifies our claim: do-
ing better approximation using slice sampling im-
proves decipherment accuracy.
Table 3 shows the first 5 decoding results and
compares them with the gold plaintext. From the ta-
ble we can see that the algorithm recovered the ma-
jority of the plaintext correctly.
4.2 Deciphering Gigaword Corpus
To prove the scalability of our new approach, we ap-
ply it to solve a much larger word substitution cipher
built from English Gigaword corpus. The corpus
contains news articles from different news agencies
270
and has a much larger vocabulary compared with the
Transtac corpus.
4.2.1 Data
We split the corpus into two parts chronologically.
Each part contains approximately 1.2 billion tokens.
We uses the first part to build a word substitution
cipher, which is 10k times longer than the one in the
previous experiment, and the second part to build a
bigram language model. 5
4.2.2 Results
We first use a single machine and apply iterative
sampling to solve a 68 million token cipher. Then
we use the result from the first step to initialize our
parallel sampling process, which uses as many as
100 machines. For evaluation, we calculate deci-
phering accuracy over the first 1000 sentences (33k
tokens).
After 2000 iterations of the parallel sampling pro-
cess, the deciphering accuracy reaches 92.2%. Fig-
ure 2 shows the learning curve of the algorithm. It
can be seen from the graph that both token and type
accuracy increase as more and more data becomes
available.
Figure 2: Learning curve for a very large word substitu-
tion cipher: Both token and type accuracy rise as more
and more ciphertext becomes available.
5Before building the language model, we replace low fre-
quency word types with an ?UNK? symbol, leaving 129k
unique word types.
5 Improving Out-of-Domain Machine
Translation
Domain specific machine translation (MT) is a chal-
lenge for statistical machine translation (SMT) sys-
tems trained on parallel corpora. It is common to see
a significant drop in translation quality when trans-
lating out-of-domain texts. Although it is hard to
find parallel corpora for any specific domain, it is
relatively easy to find domain specific monolingual
corpora. In this section, we show how to use our new
decipherment framework to learn a domain specific
translation table and use it to improve out-of-domain
translations.
5.1 Baseline SMT System
We build a state of the art phrase-based SMT system
using Moses (Koehn et al2007). The baseline sys-
tem has 3 models: a translation model, a reordering
model, and a language model. The language model
can be trained on monolingual data, and the rest are
trained on parallel data. By default, Moses uses the
following 8 features to score a candidate translation:
? direct and inverse translation probabilities
? direct and inverse lexical weighting
? phrase penalty
? a language model
? a re-ordering model
? word penalty
Each of the 8 features has its own weight, which
can be tuned on a held-out set using minimum error
rate training. (Och, 2003). In the following sections,
we describe how to use decipherment to learn do-
main specific translation probabilities, and use the
new features to improve the baseline.
5.2 Learning a New Translation Table with
Decipherment
From a decipherment perspective, machine transla-
tion is a much more complex task than solving a
word substitution cipher and poses three major chal-
lenges:
? Mappings between languages are nondetermin-
istic, as words can have multiple translations
271
? Re-ordering of words
? Insertion and deletion of words
Fortunately, our decipherment model does not as-
sume deterministic mapping and is able to discover
multiple translations. For the reordering problem,
we treat Spanish as a simple word substitution for
French. Despite the simplification in the assump-
tion, we still expect to learn a useful word-to-word
lexicon via decipherment and use the lexicon to im-
prove our baseline.
Problem formulation: By ignoring word re-
orderings, we can formulate MT decipherment prob-
lem as word substitution decipherment. We view
source language f as ciphertext and target language
e as plaintext. Our goal is to decipher f into e and
estimate translation probabilities based on the deci-
pherment.
Probabilistic decipherment: Similar to solving
a word substitution cipher, all we have to do here is
to estimate the translation model parameters P?(f |e)
using a large amount of monolingual data in f and
e respectively. According to Equation 5, our objec-
tive is to estimate the model parameters so that the
probability of source text P(f) is maximized.
argmax
?
?
e
P (e) ?
n
?
i
P?(fi|ei) (5)
Building a translation table: Once the sampling
process completes, we estimate translation probabil-
ity P (f |e) from the final sample using maximum
likelihood estimation. We also decipher from the re-
verse direction to estimate P (e|f). Finally, we build
a phrase table by taking translation pairs seen in both
decipherments.
5.3 Combining Phrase Tables
We now have two phrase tables: one learnt from par-
allel corpus and one learnt from non-parallel mono-
lingual corpus through decipherment. The phrase ta-
ble learnt through decipherment only contains word
to word translations, and each translation option
only has two scores. Moses has a function to decode
with multiple phrase tables, so we just need to add
the newly learnt phrase table and specify two more
weights for the scores in it. During decoding, if a
source word only appears in the decipherment table,
Train French: 28.5 million tokensSpanish: 26.6 million tokens
Tune French: 28k tokensSpanish: 26k tokens
Test French: 30k tokensSpanish: 28k tokens
Table 4: Europarl Training, Tuning, and Testing Data
that table?s translation will be used. If a source word
exists in both tables, Moses will create two separate
decoding paths and choose the best one after taking
other features into account. If a word is not seen in
either of the tables, it is copied literally to the output.
6 MT Experiments and Results
6.1 Data
In our MT experiments, we translate French into
Spanish and use the following corpora to learn our
translation systems:
? Europarl Corpus (Koehn, 2005): The Europarl
parallel corpus is extracted from the proceed-
ings of the European Parliament and includes
versions in 11 European languages. The cor-
pus contains articles from the political domain
and is used to train our baseline system. We
use the 6th version of the corpus. After clean-
ing, there are 1.3 million lines left for training.
We use the last 2k lines for tuning and testing
(1k for each), and the rest for training. Details
of training, tuning, and testing data are listed in
Table 4.
? EMEA Corpus (Tiedemann, 2009): EMEA is
a parallel corpus made out of PDF documents
from the European Medicines Agency. It con-
tains articles from the medical domain, which
is a good test bed for out-of-domain tasks. We
use the first 2k pairs of sentences for tuning
and testing (1k for each), and use the rest (1.1
million lines) for decipherment training. We
split the training corpus in ways that no parallel
sentences are included in the training set. The
splitting methods are listed in Table 5.
For decipherment training, we use lexical transla-
tion tables learned from the Europarl corpus to ini-
272
Comparable EMEA :
French: Every odd line, 8.7 million tokens
Spanish: Every even line, 8.1 million tokens
Non-parallel EMEA:
French: First 550k sentences, 9.1 million tokens
Spanish: Last 550k sentences, 7.7 million to-
kens
Table 5: EMEA Decipherment Training Data
tialize our sampling process.
6.2 Results
BLEU (Papineni et al2002) is used as a standard
evaluation metric. We compare the following 3 sys-
tems in our experiments, and present the results in
Table 6.
? Baseline: Trained on Europarl
? Decipher-CP: Trained on Europarl + Compa-
rable EMEA
? Decipher-NP: Trained on Europarl + Non-
Parallel EMEA
Our baseline system achieves 38.2 BLEU score
on Europarl test set. In the second row of Table
6, the test set changes to EMEA, and the baseline
BLEU score drops to 24.9. In the third row, the base-
line score rises to 30.5 with a language model built
from EMEA corpus. Although it is much higher
than the previous baseline, we further improve it
by including a new phrase table learnt from domain
specific monolingual data. In a real out-of-domain
task, we are unlikely to have any parallel data to
tune weights for the new phrase table. Therefore,
we can only set it manually. In experiments, each
score in the new phrase table has a weight of 5, and
the BLEU score rises up to 33.2. In the fourth row
of the table, we assume that there is a small amount
of domain specific parallel data for tuning. With
better weights, our baseline BLEU score increases
to 37.3, and our combined systems increase to 41.1
and 39.7 respectively. In the last row of the table, we
compare the combined systems with an even better
baseline. This time, the baseline is given half of the
EMEA tuning set for training and uses the other half
French Spanish P (fr|es) P (es|fr)
< < 0.32 1.00
he?patique hepa?tico 0.88 0.08
hepa?tica 0.76 0.85
injectable inyectable 0.91 0.92
dl dl 1.00 0.70
> > 0.32 1.00
ribavirine ribavirina 0.40 1.00
olanzapine olanzapina 0.57 1.00
clairance aclaramiento 0.99 0.64
pellicule?ss recubiertos 1.00 1.00
pharmaco-
cine?tique
farmaco-
cine?tico 1.00 1.00
Table 7: 10 most frequent OOV words in the table learnt
from non-parallel EMEA corpus
for weight tuning. Results show that our combined
systems still outperform the baseline.
The phrase table learnt from monolingual data
consists of both observed and unknown words. Ta-
ble 7 shows the top 10 most frequent OOV words
in the table learnt from non-parallel EMEA corpus.
Among the 10 words, 9 have correct translations. It
is interesting to see that our algorithm finds mul-
tiple correct translations for the word ?he?patique?.
The only mistake in the table is sensible as French
word ?pellicule?s? is translated into ?recubiertos con
pel??cula? in Spanish.
7 Conclusion and Future Work
We apply slice sampling to Bayesian Decipherment
and show significant improvement in deciphering
accuracy compared with the state of the art algo-
rithm. Our method is not only accurate but also
highly scalable. In experiments, we decipher at the
scale of the English Gigaword corpus, which con-
tains over billions of tokens and hundreds of thou-
sands word types. We further show the value of
our new decipherment algorithm by using it to im-
prove out-of-domain translation. In the future, we
will work with more language pairs, especially those
with significant word re-orderings. Moreover, the
monolingual corpora used in the experiments are far
smaller than what our algorithm can handle. We will
continue to work in scenarios where large amount of
monolingual data is readily available.
273
Train Data Tune Data Tune LM Test Data Test LM Baseline Decipher-CP
Decipher-
NP
Europarl Europarl Europarl Europarl Europarl 38.2
Europarl Europarl Europarl EMEA Europarl 24.9
Europarl Europarl Europarl EMEA EMEA 30.5 33.2(+2.7)
32.4
(+1.9)
Europarl EMEA EMEA EMEA EMEA 37.3 41.1(+3.8)
39.7
(+2.4)
Europarl +
EMEA EMEA EMEA EMEA EMEA 67.4
68.7
(+1.3)
68.7
(+1.3)
Table 6: MT experiment results: The table shows how much the combined systems outperform the baseline system in
different experiments. Each row has a different set of training, tuning, and testing data. Baseline is trained on parallel
data only. Tune LM and Test LM specify language models used for tuning and testing respectively. Decipher-CP and
Decipher-NP use a phrase table learnt from comparable and non-parallel EMEA corpus respectively.
8 Acknowledgments
This work was supported by NSF Grant 0904684.
The authors would like to thank Philip Koehen,
David Chiang, Jason Riesa, Ashish Vaswani, and
Hui Zhang for their comments and suggestions.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. Associa-
tion for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies. Association for Com-
putational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics. Association
for Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In In Proceedings of the
Tenth Machine Translation Summit, Phuket, Thailand.
Asia-Pacific Association for Machine Translation.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
David Newman, Arthur Asuncion, Padhrai Smyth, and
Max Welling. 2009. Distributed algorithms for topic
models. Journal of Machine Learning Research, 10.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
274
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics.
Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing. Associ-
ation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011a. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2011b. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Association for
Computational Linguistics.
Jo?rg Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing V, volume 309 of Current Issues in Linguistic
Theory. John Benjamins.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics. Association for Computa-
tional Linguistics.
275
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1668?1676,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Dependency-Based Decipherment for Resource-Limited Machine
Translation
Qing Dou and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,knight}@isi.edu
Abstract
We introduce dependency relations into deci-
phering foreign languages and show that de-
pendency relations help improve the state-of-
the-art deciphering accuracy by over 500%.
We learn a translation lexicon from large
amounts of genuinely non parallel data with
decipherment to improve a phrase-based ma-
chine translation system trained with limited
parallel data. In experiments, we observe
BLEU gains of 1.2 to 1.8 across three different
test sets.
1 Introduction
State-of-the-art machine translation (MT) systems
apply statistical techniques to learn translation rules
from large amounts of parallel data. However, par-
allel data is limited for many language pairs and do-
mains.
In general, it is easier to obtain non parallel data.
The ability to build a machine translation system
using monolingual data could alleviate problems
caused by insufficient parallel data. Towards build-
ing a machine translation system without a paral-
lel corpus, Klementiev et al (2012) use non paral-
lel data to estimate parameters for a large scale MT
system. Other work tries to learn full MT systems
using only non parallel data through decipherment
(Ravi and Knight, 2011; Ravi, 2013). However, the
performance of such systems is poor compared with
those trained with parallel data.
Given that we often have some parallel data,
it is more practical to improve a translation sys-
tem trained on parallel corpora with non parallel
Figure 1: Improving machine translation with deci-
pherment (Grey boxes represent new data and process).
Mono: monolingual; LM: language model; LEX: trans-
lation lexicon; TM: translation model.
data. Dou and Knight (2012) successfully apply
decipherment to learn a domain specific translation
lexicon from monolingual data to improve out-of-
domain machine translation. Although their ap-
proach works well for Spanish/French, they do not
show whether their approach works for other lan-
guage pairs. Moreover, the non parallel data used in
their experiments is created from a parallel corpus.
Such highly comparable data is difficult to obtain in
reality.
In this work, we improve previous work by Dou
and Knight (2012) using genuinely non parallel data,
1668
and propose a framework to improve a machine
translation system trained with a small amount of
parallel data. As shown in Figure 1, we use a lexi-
con learned from decipherment to improve transla-
tions of both observed and out-of-vocabulary (OOV)
words. The main contributions of this work are:
? We extract bigrams based on dependency re-
lations for decipherment, which improves the
state-of-the-art deciphering accuracy by over
500%.
? We demonstrate how to improve translations
of words observed in parallel data by us-
ing a translation lexicon obtained from large
amounts of non parallel data.
? We show that decipherment is able to find cor-
rect translations for OOV words.
? We use a translation lexicon learned by de-
ciphering large amounts of non parallel data
to improve a phrase-based MT system trained
with limited amounts of parallel data. In ex-
periments, we observe 1.2 to 1.8 BLEU gains
across three different test sets.
2 Previous Work
Motivated by the idea that a translation lexicon in-
duced from non parallel data can be applied to
MT, a variety of prior research has tried to build a
translation lexicon from non parallel or compara-
ble data (Rapp, 1995; Fung and Yee, 1998; Koehn
and Knight, 2002; Haghighi et al, 2008; Garera et
al., 2009; Bergsma and Van Durme, 2011; Daume?
and Jagarlamudi, 2011; Irvine and Callison-Burch,
2013b; Irvine and Callison-Burch, 2013a). Al-
though previous work is able to build a translation
lexicon without parallel data, little has used the lex-
icon to improve machine translation.
There has been increasing interest in learning
translation lexicons from non parallel data with de-
cipherment techniques (Ravi and Knight, 2011; Dou
and Knight, 2012; Nuhn et al, 2012). Decipher-
ment views one language as a cipher for another and
learns a translation lexicon that produces a good de-
cipherment.
In an effort to build a MT system without a paral-
lel corpus, Ravi and Knight (2011) view Spanish as a
cipher for English and apply Bayesian learning to di-
rectly decipher Spanish into English. Unfortunately,
their approach can only work on small data with lim-
ited vocabulary. Dou and Knight (2012) propose two
techniques to make Bayesian decipherment scalable.
First, unlike Ravi and Knight (2011), who deci-
pher whole sentences, Dou and Knight (2012) deci-
pher bigrams. Reducing a ciphertext to a set of bi-
grams with counts significantly reduces the amount
of cipher data. According to Dou and Knight (2012),
a ciphertext bigram F is generated through the fol-
lowing generative story:
? Generate a sequence of two plaintext tokens
e1e2 with probability P (e1e2) given by a lan-
guage model built from large numbers of plain-
text bigrams.
? Substitute e1 with f1 and e2 with f2 with prob-
ability P (f1|e1) ? P (f2|e2).
The probability of any cipher bigram F is:
P (F ) =
?
e1e2
P (e1e2)
2?
i=1
P (fi|ei)
Given a corpus of N cipher bigrams F1...FN , the
probability of the corpus is:
P (corpus) =
N?
j=1
P (Fj)
Given a plaintext bigram language model,
the goal is to manipulate P (f |e) to maximize
P (corpus). Theoretically, one can directly apply
EM to solve the problem (Knight et al, 2006). How-
ever, EM has time complexity O(N ? V 2e ) and space
complexity O(Vf ? Ve), where Vf , Ve are the sizes
of ciphertext and plaintext vocabularies respectively,
and N is the number of cipher bigrams.
Ravi and Knight (2011) apply Bayesian learning
to reduce the space complexity. Instead of esti-
mating probabilities P (f |e), Bayesian learning tries
to draw samples from plaintext sequences given ci-
phertext bigrams. During sampling, the probability
of any possible plaintext sample e1e2 is given as:
Psample(e1e2) = P (e1e2)
2?
i=1
Pbayes(fi|ei)
1669
misio?n de naciones unidas en oriente medio
misio?n de misio?n naciones
de naciones naciones unidas
naciones unidas misio?n en
unidas en en oriente
en oriente oriente medio
oriente medio
Table 1: Comparison of adjacent bigrams (left) and de-
pendency bigrams (right) extracted from the same Span-
ish text
with Pbayes(fi|ei) defined as:
Pbayes(fi|ei) =
?P0(fi|ei) + count(fi, ei)
?+ count(ei)
where P0 is a base distribution, and ? is a parameter
that controls how much we trust P0. count(fi, ei)
and count(ei) record the number of times fi, ei and
ei appear in previously generated samples respec-
tively.
At the end of sampling, P (fi|ei) is estimated by:
P (fi|ei) =
count(fi, ei)
count(ei)
However, Bayesian decipherment is still very
slow with Gibbs sampling (Geman and Geman,
1987), as each sampling step requires considering
Ve possibilities. Dou and Knight (2012) solve the
problem by introducing slice sampling (Neal, 2000)
to Bayesian decipherment.
3 From Adjacent Bigrams to Dependency
Bigrams
A major limitation of work by Dou and Knight
(2012) is their monotonic generative story for deci-
phering adjacent bigrams. While the generation pro-
cess works well for deciphering similar languages
(e.g. Spanish and French) without considering re-
ordering, it does not work well for languages that
are more different in grammar and word order (e.g.
Spanish and English). In this section, we first look
at why adjacent bigrams are bad for decipherment.
Then we describe how to use syntax to solve the
problem.
The left column in Table 1 contains adjacent bi-
grams extracted from the Spanish phrase ?misio?n
de naciones unidas en oriente medio?. The cor-
rect decipherment for the bigram ?naciones unidas?
should be ?united nations?. Since the deciphering
model described by Dou and Knight (2012) does
not consider word reordering, it needs to decipher
the bigram into ?nations united? in order to get
the right word translations ?naciones???nations?
and ?unidas???united?. However, the English lan-
guage model used for decipherment is built from En-
glish adjacent bigrams, so it strongly disprefers ?na-
tions united? and is not likely to produce a sensi-
ble decipherment for ?naciones unidas?. The Span-
ish bigram ?oriente medio? poses the same prob-
lem. Thus, without considering word reordering, the
model described by Dou and Knight (2012) is not a
good fit for deciphering Spanish into English.
However, if we extract bigrams based on depen-
dency relations for both languages, the model fits
better. To extract such bigrams, we first use de-
pendency parsers to parse both languages, and ex-
tract bigrams by putting head word first, followed
by the modifier.1 We call these dependency bi-
grams. The right column in Table 1 lists exam-
ples of Spanish dependency bigrams extracted from
the same Spanish phrase. With a language model
built with English dependency bigrams, the same
model used for deciphering adjacent bigrams is
able to decipher Spanish dependency bigram ?na-
ciones(head) unidas(modifier)? into ?nations(head)
united(modifier)?.
We might instead propose to consider word re-
ordering when deciphering adjacent bigrams (e.g.
add an operation to swap tokens in a bigram). How-
ever, using dependency bigrams has the following
advantages:
? First, using dependency bigrams avoids com-
plicating the model, keeping deciphering effi-
cient and scalable.
? Second, it addresses the problem of long dis-
tance reordering, which can not be modeled by
swapping tokens in bigrams.
Furthermore, using dependency bigrams al-
lows us to use dependency types to further
1As use of ?del? and ?de? in Spanish is much more frequent
than the use of ?of? in English, we skip those words by using
their head words as new heads if any of them serves as a head.
1670
improve decipherment. Suppose we have a
Spanish dependency bigram ?accepto?(verb) solici-
tud(object)?. Then all of the following English de-
pendency bigrams are possible decipherments: ?ac-
cepted(verb) UN(subject)?, ?accepted(verb) govern-
ment(subject)?, ?accepted(verb) request(object)?.
However, if we know the type of the Spanish depen-
dency bigram and use a language model built with
the same type in English, the only possible decipher-
ment is ?accepted(verb) request(object)?. If we limit
the search space, a system is more likely to find a
better decipherment.
4 Deciphering Spanish Gigaword
In this section, we compare dependency bigrams
with adjacent bigrams for deciphering Spanish into
English.
4.1 Data
We use the Gigaword corpus for our decipherment
experiments. The corpus contains news articles from
different news agencies and is available in Spanish
and English. We use only the AFP (Agence France-
Presse) section of the corpus in decipherment ex-
periments. We tokenize the corpus using tools that
come with the Europarl corpus (Koehn, 2005). To
shorten the time required for running different sys-
tems on large amounts of data, we keep only the top
5000 most frequent word types in both languages
and replace all other word types with UNK. We also
throw away lines with more than 40 tokens, as the
Spanish parser (Bohnet, 2010) we use is slow when
processing long sentences. After preprocessing, the
corpus contains approximately 440 million tokens in
Spanish and 350 million tokens in English. To ob-
tain dependency bigrams, we use the Bohnet parsers
(Bohnet, 2010) to parse both the Spanish and En-
glish version of the corpus.
4.2 Systems
Three systems are evaluated in the experiments. We
implement a baseline system, Adjacent, based on
Dou and Knight (2012). The baseline system col-
lects adjacent bigrams and their counts from Spanish
and English texts. It then builds an English bigram
language model using the English adjacent bigrams
and uses it to decipher the Spanish adjacent bigrams.
Dependency Types
Group 1 Verb/Subject
Group 2 Preposition/Preposition-Object,
Noun/Noun-Modifier
Group 3 Verb/Noun-Object
Table 2: Dependency relations divided into three groups
We build the second system, Dependency, using
dependency bigrams for decipherment. As the two
parsers do not output the same set of dependency re-
lations, we cannot extract all types of dependency
bigrams. Instead, we select a subset of dependency
bigrams whose dependency relations are shared by
the two parser outputs. The selected dependency re-
lations are: Verb/Subject, Verb/Noun-Object, Prepo-
sition/Object, Noun/Modifier. Decipherment runs
the same way as in the baseline system.
The third system, DepType, is built using both
dependent bigrams and their dependency types. We
first extract dependency bigrams for both languages,
then group them based on their dependency types.
As both parsers treat noun phrases dependent on
?del?, ?de?, and ?of? as prepositional phrases, we
choose to divide the dependency bigrams into 3
groups and list them in Table 2. A separate language
model is built for each group of English dependency
bigrams and used to decipher the group of Spanish
dependency bigrams with same dependency type.
For all the systems, language models are built us-
ing the SRILM toolkit (Stolcke, 2002). For the Ad-
jacent system, we use Good-Turing smoothing. For
the other systems, we use a mix of Witten-Bell and
Good-Turing smoothing.
4.3 Sampling Procedure
In experiments, we find that the iterative sam-
pling method described by Dou and Knight (2012)
helps improve deciphering accuracy. We also find
that combining results from different decipherments
helps find more correct translations at each iteration.
Thus, instead of using a single sampling process, we
use 10 different sampling processes at each iteration.
The details of the new sampling procedure are pro-
vided here:
? Extract dependency bigrams from parsing out-
puts and collect their counts.
1671
? Keep bigrams whose counts are greater than a
threshold ?. Then start 10 different randomly
seeded and initialized sampling processes. Per-
form sampling.
? At the end of sampling, extract word transla-
tion pairs (f, e) from the final sample. Esti-
mate translation probabilities P (e|f) for each
pair. Then construct a translation table by keep-
ing translation pairs (f, e) seen in more than
one decipherment and use the average P (e|f)
as the new translation probability.
? Lower the threshold ? to include more bigrams
into the sampling process. Start 10 differ-
ent sampling processes again and initialize the
first sample using the translation pairs obtained
from the previous step (for each Spanish token
f, choose an English token e whose P (e|f) is
the highest). Perform sampling again.
? Repeat until ? = 1.
4.4 Deciphering Accuracy
We choose the first 1000 lines of the monolingual
Spanish texts as our test data. The data contains
37,505 tokens and 6556 word types. We use type ac-
curacy as our evaluation metric: Given a word type
f in Spanish, we find a translation pair (f, e) with
the highest average P (e|f) from the translation ta-
ble learned through decipherment. If the translation
pair (f, e) can also be found in a gold translation
lexicon Tgold, we treat the word type f as correctly
deciphered. Let |C| be the number of word types
correctly deciphered, and |V | be the total number of
word types evaluated. We define type accuracy as
|C|
|V | .
To create Tgold, we use GIZA (Och and Ney,
2003) to align a small amount of Spanish-English
parallel text (1 million tokens for each language),
and use the lexicon derived from the alignment as
our gold translation lexicon. Tgold contains a subset
of 4408 types seen in the test data, among which,
2878 are also top 5000 frequent word types.
4.5 Results
During decipherment, we gradually increase the size
of Spanish texts and compare the learning curves of
three deciphering systems in Figure 2.
Figure 2: Learning curves for three decipherment sys-
tems. Compared with Adjacent (previous state of the art),
systems that use dependency bigrams improve decipher-
ing accuracy by over 500%.
With 100k tokens of Spanish text, the perfor-
mance of the three systems are similar. However, the
learning curve of Adjacent plateaus quickly, while
those of the dependency based systems soar up as
more data becomes available and still rise sharply
when the size of Spanish texts increases to 10 mil-
lion tokens, where the DepType system improves
deciphering accuracy of the Adjacent system from
4.2% to 24.6%. In the end, with 100 million tokens,
the accuracy of the DepType system rises to 27.0%.
The accuracy is even higher (41%), when evaluated
against the top 5000 frequent word types only.
5 Improving Machine Translation with
Decipherment
In this section, we demonstrate how to use a trans-
lation lexicon learned by deciphering large amounts
of in-domain (news) monolingual data to improve
a phrase-based machine translation system trained
with limited out-of-domain (politics) parallel data.
5.1 Data
We use approximately one million tokens of the Eu-
roparl corpus (Koehn, 2005) as our small out-of-
domain parallel training data and Gigaword as our
large in-domain monolingual training data to build
language models and a new translation lexicon to
improve a phrase-based MT baseline system. For
tuning and testing, we use the development data
1672
Parallel
Spanish English
Europarl 1.1 million 1.0 million
Tune-2008 52.6k 49.8k
Test-2009 68.1k 65.6k
Test-2010 65.5k 61.9k
Test-2011 79.4k 74.7k
Non Parallel
Spanish English
Gigaword 894 million 940 million
Table 3: Size of training, tuning, and testing data in num-
ber of tokens
from the NAACL 2012 workshop on statistical ma-
chine translation. The data contains test data in the
news domain from the 2008, 2009, 2010, and 2011
workshops. We use the 2008 test data for tuning and
the rest for testing. The sizes of the training, tuning,
and testing sets are listed in Table 3.
5.2 Systems
5.2.1 Baseline Machine Translation System
We build a state-of-the-art phrase-based MT sys-
tem, PBMT, using Moses (Koehn et al, 2007).
PBMT has 3 models: a translation model, a distor-
tion model, and a language model. We build a 5-
gram language model using the AFP section of the
English Gigaword. We train the other models using
the Europarl corpus. By default, Moses uses the fol-
lowing 8 features to score a candidate translation:
? direct and inverse translation probabilities
? direct and inverse lexical weighting
? a language model score
? a distortion score
? phrase penalty
? word penalty
The 8 features have weights adjusted on the tun-
ing data using minimum error rate training (MERT)
(Och, 2003). PBMT has a phrase table Tphrase.
During decoding, Moses copies out-of-vocabulary
(OOV) words, which can not be found in Tphrase,
directly to output. In the following sections, we de-
scribe how to use a translation lexicon learned from
large amounts of non parallel data to improve trans-
lation of OOV words, as well as words observed in
Tphrase.
5.2.2 Decipherment for Machine Translation
To achieve better decipherment, we:
? Increase the size of Spanish ciphertext from
100 million tokens to 894 million tokens.
? Keep top 50k instead of top 5k most frequent
word types of the ciphertext.
? Instead of seeding the sampling process ran-
domly, we use a translation lexicon learned
from a limited amount of parallel data as seed:
For each Spanish dependency bigram f1, f2,
where both f1 and f2 are found in the seed lex-
icon, we find the English sequence e1, e2 that
maximizes P (e1, e2)P (e1|f1)P (e2|f2). Other-
wise, for any Spanish token f that can be found
in the seed lexicon, we choose English word e,
where P (e|f) is the highest as the initial sam-
ple; for any f that are not seen in the seed lexi-
con, we do random initialization.
We perform 20 random restarts with 10k iter-
ations on each and build a word-to-word transla-
tion lexicon Tdecipher by collecting translation pairs
seen in at least 3 final decipherments with either
P (f |e) ? 0.2 or P (e|f) ? 0.2.
5.2.3 Improving Translation of Observed
Words with Decipherment
To improve translation of words observed in our
parallel corpus, we simply use Tdecipher as an addi-
tional parallel corpus. First, we filter Tdecipher by
keeping only translation pairs (f, e), where f is ob-
served in the Spanish part and e is observed in the
English part of the parallel corpus. Then we ap-
pend all the Spanish and English words in the fil-
tered Tdecipher to the end of Spanish part and En-
glish part of the parallel corpus respectively. The
training and tuning process is the same as the base-
line machine translation system PBMT. We denote
this system as Decipher-OBSV.
1673
5.2.4 Improving OOV translation with
Decipherment
As Tdecipher is learned from large amounts of in-
domain monolingual data, we expect that Tdecipher
contains a number of useful translations for words
not seen in the limited amount of parallel data (OOV
words). Instead of copying OOV words directly to
output, which is what Moses does by default, we try
to find translations from Tdecipher to improve trans-
lation.
During decoding, if a source word f is in Tphrase,
its translation options are collected from Tphrase ex-
clusively. If f is not in Tphrase but in Tdecipher,
the decoder will find translations from Tdecipher. If
f is not in either translation table, the decoder just
copies it directly to the output. We call this system
Decipher-OOV.
However, when an OOV?s correct translation is
same as its surface form and all its possible transla-
tions in Tdecipher are wrong, it is better to just copy
OOV words directly to output. This scenario hap-
pens frequently, as Spanish and English share many
common words. To avoid over trusting Tdecipher,
we add a new translation pair (f, f) for each source
word f in Tdecipher if the translation pair (f, f) is
not originally in Tdecipher. For each newly added
translation pair, both of its log translation probabil-
ities are set to 0. To distinguish the added transla-
tion pairs from the others learned through decipher-
ment, we add a binary feature ? to each translation
pair in Tdecipher. The final version of Tdecipher has
three feature scores: P (e|f), P (f |e), and ?. Finally,
we tune weights of the features in Tdecipher using
MERT (Och, 2003) on the tuning set.
5.2.5 A Combined Approach
In the end, we build a system Decipher-COMB,
which uses Tdecipher to improve translation of both
observed and OOV words with methods described in
sections 5.2.3 and 5.2.4.
5.3 Results
We tune each system three times with MERT and
choose the best weights based on BLEU scores on
tuning set.
Table 4 shows that the translation lexicon learned
from decipherment helps achieve higher BLEU
scores across tuning and testing sets. Decipher-
OBSV improves BLEU scores by as much as 1.2
points. We analyze the results and find the gain
mainly comes from two parts. First, adding Tdecipher
to small amounts of parallel corpus improves word
level translation probabilities, which lead to better
lexical weighting; second, Tdecipher contains new al-
ternative translations for words observed in the par-
allel corpus.
Moreover, Decipher-OOV also achieves better
BLEU scores compared with PBMT across all tun-
ing and test sets. We also observe that systems us-
ing Tdecipher learned by deciphering dependency bi-
grams leads to larger gains in BLEU scores. When
decipherment is used to improve translation of both
observed and OOV words, we see improvement in
BLEU score as high as 1.8 points on the 2010 news
test set.
The consistent improvement on the tuning and
different testing data suggests that decipherment is
capable of learning good translations for a number
of OOV words. To further demonstrate that our
decipherment approach finds useful translations for
OOV words, we list the top 10 most frequent OOV
words from both the tuning set and testing set as well
as their translations (up to three most likely transla-
tions) in Table 5. P (e|f) and P (f |e) are average
scores over different decipherment runs.
From the table, we can see that decipherment
finds correct translations (bolded) for 7 out of the
10 most frequent OOV words. Moreover, many
OOVs and their correct translations are homographs
, which makes copying OOVs directly to the output
a strong baseline to beat. Nonetheless, decipherment
still finds enough correct translations to improve the
baseline.
6 Conclusion
We introduce syntax for deciphering Spanish into
English. Experiment results show that using de-
pendency bigrams improves decipherment accuracy
by over 500% compared with the state-of-the-art
approach. Moreover, we learn a domain specific
translation lexicon by deciphering large amounts of
monolingual data and show that the lexicon can im-
prove a baseline machine translation system trained
with limited parallel data.
1674
Decipherment System Tune2008 Test2009 Test2010 Test2011
None PBMT (Baseline) 19.1 19.6 21.3 22.1
Adjacent
Decipher-OBSV 19.5 20.1 22.2 22.6
Decipher-OOV 19.4 19.9 21.7 22.5
Decipher-COMB 19.5 20.2 22.3 22.5
Dependency
Decipher-OBSV 19.7 20.5 22.5 23.0
Decipher-OOV 19.9 20.4 22.4 22.9
Decipher-COMB 20.0 20.8 23.1 23.4
Table 4: Systems that use translation lexicons learned from decipherment show consistent improvement over the
baseline system across tuning and testing sets. The best system, Decipher-COMB, achieves as much as 1.8 BLEU
point gain on the 2010 news test set.
Spanish English P (e|f) P (f |e)
obama his 0.33 0.01
bush 0.27 0.07
clinton 0.23 0.11
bush bush 0.47 0.45
yeltsin 0.28 0.81
he 0.24 0.05
festival event 0.68 0.35
festival 0.61 0.72
wikileaks zeta 0.03 0.33
venus venus 0.61 0.74
serena 0.47 0.62
colchones mattresses 0.55 0.73
cars 0.31 0.01
helado frigid 0.52 0.44
chill 0.37 0.14
sandwich 0.42 0.27
google microsoft 0.67 0.18
google 0.59 0.69
cantante singer 0.44 0.92
jackson 0.14 0.33
artists 0.14 0.77
mccain mccain 0.66 0.92
it 0.22 0.00
he 0.21 0.00
Table 5: Decipherment finds correct translations for 7 out
of 10 most frequent OOV word types.
7 Acknowledgments
This work was supported by NSF Grant 0904684
and ARO grant W911NF-10-1-0533. The authors
would like to thank David Chiang, Malte Nuhn,
Victoria Fossum, Ashish Vaswani, Ulf Hermjakob,
Yang Gao, and Hui Zhang (in no particular order)
for their comments and suggestions.
References
Shane Bergsma and Benjamin Van Durme. 2011. Learn-
ing bilingual lexicons using the visual similarity of
labeled web images. In Proceedings of the Twenty-
Second international joint conference on Artificial In-
telligence - Volume Volume Three. AAAI Press.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics. Coling.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies. Association for Com-
putational Linguistics.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Associa-
tion for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
1675
Linguistics - Volume 1. Association for Computational
Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In Proceed-
ings of the Thirteenth Conference on Computational
Natural Language Learning. Association for Compu-
tational Linguistics.
Stuart Geman and Donald Geman. 1987. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. In Readings in computer vision: is-
sues, problems, principles, and paradigms. Morgan
Kaufmann Publishers Inc.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013a. Combin-
ing bilingual and comparable corpora for low resource
machine translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation. Associ-
ation for Computational Linguistics, August.
Ann Irvine and Chris Callison-Burch. 2013b. Supervised
bilingual lexicon induction with multiple monolingual
signals. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: a parallel corpus for sta-
tistical machine translation. In In Proceedings of the
Tenth Machine Translation Summit, Phuket, Thailand.
Asia-Pacific Association for Machine Translation.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics: Long Papers - Volume 1. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics.
Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Association for
Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of the
51th Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing.
1676
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 557?565,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Beyond Parallel Data: Joint Word Alignment and Decipherment
Improves Machine Translation
Qing Dou , Ashish Vaswani, and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,avaswani,knight}@isi.edu
Abstract
Inspired by previous work, where decipher-
ment is used to improve machine translation,
we propose a new idea to combine word align-
ment and decipherment into a single learning
process. We use EM to estimate the model pa-
rameters, not only to maximize the probabil-
ity of parallel corpus, but also the monolingual
corpus. We apply our approach to improve
Malagasy-English machine translation, where
only a small amount of parallel data is avail-
able. In our experiments, we observe gains of
0.9 to 2.1 Bleu over a strong baseline.
1 Introduction
State-of-the-art machine translation (MT) systems ap-
ply statistical techniques to learn translation rules au-
tomatically from parallel data. However, this reliance
on parallel data seriously limits the scope of MT ap-
plication in the real world, as for many languages and
domains, there is not enough parallel data to train a de-
cent quality MT system.
However, compared with parallel data, there are
much larger amounts of non parallel data. The abil-
ity to learn a translation lexicon or even build a ma-
chine translation system using monolingual data helps
address the problems of insufficient parallel data. Ravi
and Knight (2011) are among the first to learn a full
MT system using only non parallel data through deci-
pherment. However, the performance of such systems
is much lower compared with those trained with par-
allel data. In another work, Klementiev et al. (2012)
show that, given a phrase table, it is possible to esti-
mate parameters for a phrase-based MT system from
non parallel data.
Given that we often have some parallel data, it is
more practical to improve a translation system trained
on parallel data by using additional non parallel data.
Rapp (1995) shows that with a seed lexicon, it is possi-
ble to induce new word level translations from non par-
allel data. Motivated by the idea that a translation lexi-
con induced from non parallel data can be used to trans-
late out of vocabulary words (OOV), a variety of prior
research has tried to build a translation lexicon from
non parallel or comparable data (Fung and Yee, 1998;
Koehn and Knight, 2002; Haghighi et al., 2008; Garera
Figure 1: Combine word alignment and decipherment
into a single learning process.
et al., 2009; Bergsma and Van Durme, 2011; Daum?e
and Jagarlamudi, 2011; Irvine and Callison-Burch,
2013b; Irvine and Callison-Burch, 2013a; Irvine et al.,
2013).
Lately, there has been increasing interest in learn-
ing translation lexicons from non parallel data with de-
cipherment techniques (Ravi and Knight, 2011; Dou
and Knight, 2012; Nuhn et al., 2012; Dou and Knight,
2013). Decipherment views one language as a cipher
for another and learns a translation lexicon that pro-
duces fluent text in the target (plaintext) language. Pre-
vious work has shown that decipherment not only helps
find translations for OOVs (Dou and Knight, 2012), but
also improves translations of observed words (Dou and
Knight, 2013).
We find that previous work using monolingual or
comparable data to improve quality of machine transla-
tion separates two learning tasks: first, translation rules
are learned from parallel data, and then the information
learned from parallel data is used to bootstrap learning
with non parallel data. Inspired by approaches where
joint inference reduces the problems of error propaga-
tion and improves system performance, we combine
the two separate learning processes into a single one,
as shown in Figure 1. The contributions of this work
are:
557
? We propose a new objective function for word
alignment that combines the process of word
alignment and decipherment into a single learning
task.
? In experiments, we find that the joint process out-
performs the previous pipeline approach, and ob-
serve Bleu gains of 0.9 and 2.1 on two different
test sets.
? We release 15.3 million tokens of monolingual
Malagasy data from the web, as well as a small
Malagasy dependency tree bank containing 20k
tokens.
2 Joint Word Alignment and
Decipherment
2.1 A New Objective Function
In previous work that uses monolingual data to im-
prove machine translation, a seed translation lexicon
learned from parallel data is used to find new transla-
tions through either word vector based approaches or
decipherment. In return, selection of a seed lexicon
needs to be careful as using a poor quality seed lexi-
con could hurt the downstream process. Evidence from
a number of previous work shows that a joint inference
process leads to better performance in both tasks (Jiang
et al., 2008; Zhang and Clark, 2008).
In the presence of parallel and monolingual data, we
would like the alignment and decipherment models to
benefit from each other. Since the decipherment and
word alignment models contain word-to-word transla-
tion probabilities t( f | e), having them share these pa-
rameters during learning will allow us to pool infor-
mation from both data types. This leads us to de-
velop a new objective function that takes both learn-
ing processes into account. Given our parallel data,
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), and monolingual
data F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, we seek to maximize
the likelihood of both. Our new objective function is
defined as:
F
joint
=
M
?
m=1
log P(F
m
| E
m
) + ?
N
?
n=1
log P(F
n
mono
) (1)
The goal of training is to learn the parameters that
maximize this objective, that is
?
?
= arg max
?
F
joint
(2)
In the next two sections, we describe the word align-
ment and decipherment models, and present how they
are combined to perform joint optimization.
2.2 Word Alignment
Given a source sentence F = f
1
, . . . , f
j
, . . . , f
J
and a
target sentence E = e
1
, . . . , e
i
, . . . , e
I
, word alignment
models describe the generative process employed to
produce the French sentence from the English sentence
through alignments a = a
1
, . . . , a
j
, . . . , a
J
.
The IBM models 1-2 (Brown et al., 1993) and the
HMM word alignment model (Vogel et al., 1996) use
two sets of parameters, distortion probabilities and
translation probabilities, to define the joint probabil-
ity of a target sentence and alignment given a source
sentence.
P(F, a | E) =
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
). (3)
These alignment models share the same translation
probabilities t( f
j
| e
a
j
), but differ in their treatment of
the distortion probabilities d(a
j
| a
j?1
, j). Brown et
al. (1993) introduce more advanced models for word
alignment, such as Model 3 and Model 4, which use
more parameters to describe the generative process. We
do not go into details of those models here and the
reader is referred to the paper describing them.
Under the Model 1-2 and HMM alignment models,
the probability of target sentence given source sentence
is:
P(F | E) =
?
a
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
).
Let ? denote all the parameters of the word align-
ment model. Given a corpus of sentence pairs
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), the standard ap-
proach for training is to learn the maximum likelihood
estimate of the parameters, that is,
?
?
= arg max
?
M
?
m=1
log P(F
m
| E
m
)
= arg max
?
log
?
?
?
?
?
?
?
?
a
P(F
m
, a | E
m
)
?
?
?
?
?
?
?
.
We typically use the EM algorithm (Dempster et al.,
1977), to carry out this optimization.
2.3 Decipherment
Given a corpus of N foreign text sequences (cipher-
text), F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, decipherment finds
word-to-word translations that best describe the cipher-
text.
Knight et al. (2006) are the first to study several natu-
ral language decipherment problems with unsupervised
learning. Since then, there has been increasing interest
in improving decipherment techniques and its applica-
tion to machine translation (Ravi and Knight, 2011;
558
Dou and Knight, 2012; Nuhn et al., 2012; Dou and
Knight, 2013; Nuhn et al., 2013).
In order to speed up decipherment, Dou and Knight
(2012) suggest that a frequency list of bigrams might
contain enough information for decipherment. Accord-
ing to them, a monolingual ciphertext bigram F
mono
is
generated through the following generative story:
? Generate a sequence of two plaintext tokens e
1
e
2
with probability P(e
1
e
2
) given by a language
model built from large numbers of plaintext bi-
grams.
? Substitute e
1
with f
1
and e
2
with f
2
with probabil-
ity t( f
1
|e
1
) ? t( f
2
|e
2
).
The probability of any cipher bigram F is:
P(F
mono
) =
?
e
1
e
2
P(e
1
e
2
) ? t( f
1
|e
1
) ? t( f
2
|e
2
) (4)
And the probability of the corpus is:
P(corpus) =
N
?
n=1
P(F
n
mono
) (5)
Given a plaintext bigram language model, the goal is
to manipulate t( f |e) to maximize P(corpus). Theoret-
ically, one can directly apply EM to solve the problem
(Knight et al., 2006). However, EM has time complex-
ity O(N ?V
2
e
) and space complexity O(V
f
?V
e
), where V
f
,
V
e
are the sizes of ciphertext and plaintext vocabularies
respectively, and N is the number of cipher bigrams.
There have been previous attempts to make decipher-
ment faster. Ravi and Knight (2011) apply Bayesian
learning to reduce the space complexity. However,
Bayesian decipherment is still very slow with Gibbs
sampling (Geman and Geman, 1987). Dou and Knight
(2012) make sampling faster by introducing slice sam-
pling (Neal, 2000) to Bayesian decipherment. Besides
Bayesian decipherment, Nuhn et al. (2013) show that
beam search can be used to solve a very large 1:1 word
substitution cipher. In subsection 2.4.1, we describe
our approach that uses slice sampling to compute ex-
pected counts for decipherment in the EM algorithm.
2.4 Joint Optimization
We now describe our EM approach to learn the param-
eters that maximize F
joint
(equation 2), where the dis-
tortion probabilities, d(a
j
| a
j?1
, j) in the word align-
ment model are only learned from parallel data, and
the translation probabilities, t( f | e) are learned using
both parallel and non parallel data. The E step and M
step are illustrated in Figure 2.
Our algorithm starts with EM learning only on par-
allel data for a few iterations. When the joint inference
starts, we first compute expected counts from parallel
data and non parallel data using parameter values from
the last M step separately. Then, we add the expected
counts from both parallel data and non parallel data to-
gether with different weights for the two. Finally we
Figure 2: Joint Word Alignment and Decipherment
with EM
renormalize the translation table and distortion table to
update parameters in the new M step.
The E step for parallel part can be computed effi-
ciently using the forward-backward algorithm (Vogel et
al., 1996). However, as we pointed out in Section 2.3,
the E step for the non parallel part has a time com-
plexity of O(V
2
) with the forward-backward algorithm,
where V is the size of English vocabulary, and is usu-
ally very large. Previous work has tried to make de-
cipherment scalable (Ravi and Knight, 2011; Dou and
Knight, 2012; Nuhn et al., 2013; Ravi, 2013). How-
ever, all of them are designed for decipherment with ei-
ther Bayesian inference or beam search. In contrast, we
need an algorithm to make EM decipherment scalable.
To overcome this problem, we modify the slice sam-
pling (Neal, 2000) approach used by Dou and Knight
(2012) to compute expected counts from non parallel
data needed for the EM algorithm.
2.4.1 Draw Samples with Slice Sampling
To start the sampling process, we initialize the first
sample by performing approximate Viterbi decoding
using results from the last EM iteration. For each for-
eign dependency bigram f
1
, f
2
, we find the top 50 can-
didates for f
1
and f
2
ranked by t(e| f ), and find the En-
glish sequence e
1
, e
2
that maximizes t(e
1
| f
1
) ? t(e
2
| f
2
) ?
P(e
1
, e
2
).
Suppose the derivation probability for current sam-
ple e current is P(e current), we use slice sampling to
draw a new sample in two steps:
? Select a threshold T uniformly between 0 and
P(e current).
? Draw a new sample e new uniformly from a pool
559
of candidates: {e new|P(e new) > T }.
The first step is straightforward to implement. How-
ever, it is not trivial to implement the second step. We
adapt the idea from Dou and Knight (2012) for EM
learning.
Suppose our current sample e current contains En-
glish tokens e
i?1
, e
i
, and e
i+1
at position i ? 1, i, and
i+1 respectively, and f
i
be the foreign token at position
i. Using point-wise sampling, we draw a new sample
by changing token e
i
to a new token e
?
. Since the rest
of the sample remains the same, only the probability of
the trigram P(e
i?1
e
?
e
i+1
) (The probability is given by a
bigram language model.), and the channel model prob-
ability t( f
i
|e
?
) change. Therefore, the probability of a
sample is simplified as shown Equation 6.
P(e
i?1
e
?
e
i+1
) ? t( f
i
|e
?
) (6)
Remember that in slice sampling, a new sample is
drawn in two steps. For the first step, we choose a
threshold T uniformly between 0 and P(e
i?1
e
i
e
i+1
) ?
t( f
i
|e
i
). We divide the second step into two cases based
on the observation that two types of samples are more
likely to have a probability higher than T (Dou and
Knight, 2012): (1) those whose trigram probability is
high, and (2) those whose channel model probability is
high. To find candidates that have high trigram proba-
bility, Dou and Knight (2012) build a top k sorted lists
ranked by P(e
i?1
e
?
e
i+1
), which can be pre-computed
off-line. Then, they test if the last item e
k
in the list
satisfies the following inequality:
P(e
i?1
e
k
e
i+1
) ? c < T (7)
where c is a small constant and is set to prior in their
work. In contrast, we choose c empirically as we do
not have a prior in our model. When the inequality in
Equation 7 is satisfied, a sample is drawn in the fol-
lowing way: Let set A = {e
?
|e
i?1
e
?
e
i+1
? c > T } and
set B = {e
?
|t( f
i
|e
?
) > c}. Then we only need to sample
e
?
uniformly from A ? B until P(e
i?1
e
?
e
i+1
) ? t( f
i
|e
?
) is
greater than T . It is easy to prove that all other candi-
dates that are not in the sorted list and with t( f
i
|e
?
) ? c
have a upper bound probability: P(e
i?1
e
k
e
i+1
)?c. There-
fore, they do not need to be considered.
Second, when the last item e
k
in the list does not
meet the condition in Equation 7, we keep drawing
samples e
?
randomly until its probability is greater than
the threshold T .
As we mentioned before, the choice of the small con-
stant c is empirical. A large c reduces the number of
items in set B, but makes the condition P(e
i?1
e
k
e
i+1
) ?
c < T less likely to satisfy, which slows down the sam-
pling. On the contrary, a small c increases the number
of items in set B significantly as EM does not encour-
age a sparse distribution, which also slows down the
sampling. In our experiments, we set c to 0.001 based
on the speed of decipherment. Furthermore, to reduce
the size of set B, we rank all the candidate translations
Spanish English
Parallel 10.3k 9.9k
Non Parallel 80 million 400 million
Table 1: Size of parallel and non parallel data for word
alignment experiments (Measured in number of tokens)
of f
i
by t(e
?
| f
i
), then we add maximum the first 1000
candidates whose t( f
i
|e
?
) >= c into set B. For the rest
of the candidates, we set t( f
i
|e
?
) to a value smaller than
c (0.00001 in experiments).
2.4.2 Compute Expected Counts from Samples
With the ability to draw samples efficiently for deci-
pherment using EM, we now describe how to compute
expected counts from those samples. Let f
1
, f
2
be a
specific ciphertext bigram, N be the number of sam-
ples we want to use to compute expected counts, and
e
1
, e
2
be one of the N samples. The expected counts
for pairs ( f
1
, e
1
) and ( f
2
, e
2
) are computed as:
? ?
count( f
1
, f
2
)
N
where count( f
1
, f
2
) is count of the bigram, and ? is the
weight for non parallel data as shown in Equation 1.
Expected counts collected for f
1
, f
2
are accumulated
from each of its N samples. Finally, we collect ex-
pected counts using the same approach from each for-
eign bigram.
3 Word Alignment Experiments
In this section, we show that joint word alignment and
decipherment improves the quality of word alignment.
We choose to evaluate word alignment performance
for Spanish and English as manual gold alignments
are available. In experiments, our approach improves
alignment F score by as much as 8 points.
3.1 Experiment Setup
As shown in Table 1, we work with a small amount of
parallel, manually aligned Spanish-English data (Lam-
bert et al., 2005), and a much larger amount of mono-
lingual data.
The parallel data is extracted from Europarl, which
consists of articles from European parliament plenary
sessions. The monolingual data comes from English
and Spanish versions of Gigaword corpra containing
news articles from different news agencies.
We view Spanish as a cipher of English, and follow
the approach proposed by Dou and Knight (2013) to
extract dependency bigrams from parsed Spanish and
English monolingual data for decipherment. We only
keep bigrams where both tokens appear in the paral-
lel data. Then, we perform Spanish to English (En-
glish generating Spanish) word alignment and Span-
ish to English decipherment simultaneously with the
method discussed in section 2.
560
3.1.1 Results
We align all 500 sentences in the parallel corpus, and
tune the decipherment weight (?) for Model 1 and
HMM using the last 100 sentences. The best weights
are 0.1 for Model 1, and 0.005 for HMM. We start with
Model 1 with only parallel data for 5 iterations, and
switch to the joint process for another 5 iterations with
Model 1 and 5 more iterations of HMM. In the end, we
use the first 100 sentence pairs of the corpus for evalu-
ation.
Figure 3 compares the learning curve of alignment
F-score between EM without decipherment (baseline)
and our joint word alignment and decipherment. From
the learning curve, we find that at the 6th iteration, 2
iterations after we start the joint process, alignment F-
score is improved from 34 to 43, and this improvement
is held through the rest of the Model 1 iterations. The
alignment model switches to HMM from the 11th iter-
ation, and at the 12th iteration, we see a sudden jump
in F-score for both the baseline and the joint approach.
We see consistent improvement of F-score till the end
of HMM iterations.
4 Improving Low Density Languages
Machine Translation with Joint Word
Alignment and Decipherment
In the previous section, we show that the joint word
alignment and decipherment process improves quality
of word alignment significantly for Spanish and En-
glish. In this section, we test our approach in a more
challenging setting: improving the quality of machine
translation in a real low density language setting.
In this task, our goal is to build a system to trans-
late Malagasy news into English. We have a small
amount of parallel data, and larger amounts of mono-
lingual data collected from online websites. We build a
dependency parser for Malagasy to parse the monolin-
gual data to perform dependency based decipherment
(Dou and Knight, 2013). In the end, we perform joint
word alignment and decipherment, and show that the
joint learning process improves Bleu scores by up to
2.1 points over a phrase-based MT baseline.
4.1 The Malagasy Language
Malagasy is the official language of Madagascar. It has
around 18 million native speakers. Although Mada-
gascar is an African country, Malagasy belongs to the
Malayo-Polynesian branch of the Austronesian lan-
guage family. Malagasy and English have very dif-
ferent word orders. First of all, in contrast to En-
glish, which has a subject-verb-object (SVO) word or-
der, Malagasy has a verb-object-subject (VOS) word
order. Besides that, Malagasy is a typical head ini-
tial language: Determiners precede nouns, while other
modifiers and relative clauses follow nouns (e.g. ny
?the? boky ?book? mena ?red?). The significant dif-
ferences in word order pose great challenges for both
Source Malagasy English
Parallel
Global Voices 2.0 million 1.8 million
Web News 2.2k 2.1k
Non Parallel
Gigaword N/A 2.4 billion
allAfrica N/A 396 million
Local News 15.3 million N/A
Table 2: Size of Malagasy and English data used in our
experiments (Measured in number of tokens)
machine translation and decipherment.
4.2 Data
Table 2 shows the data available to us in our experi-
ments. The majority of parallel text comes from Global
Voices
1
(GV). The website contains international news
translated into different foreign languages. Besides
that, we also have a very small amount of parallel text
containing local web news, with English translations
provided by native speakers at the University of Texas,
Austin. The Malagasy side of this small parallel corpus
also has syntactical annotation, which is used to train a
very basic Malagasy part of speech tagger and depen-
dency parser.
We also have much larger amounts of non paral-
lel data for both languages. For Malagasy, we spent
two months manually collecting 15.3 million tokens of
news text from local news websites in Madagascar.
2
We have released this data for future research use. For
English, we have 2.4 billion tokens from the Gigaword
corpus. Since the Malagasy monolingual data is col-
lected from local websites, it is reasonable to argue that
those data contain significant amount of information re-
lated to Africa. Therefore, we also collect 396 million
tokens of African news in English from allAfrica.com.
4.3 Building A Dependency Parser for Malagasy
Since Malagasy and English have very different word
orders, we decide to apply dependency based decipher-
ment for the two languages as suggested by Dou and
Knight (2013). To extract dependency relations, we
need to parse monolingual data in Malagasy and En-
glish. For English, there are already many good parsers
available. In our experiments, we use Turbo parser
(Martins et al., 2013) trained on the English Penn Tree-
bank (Marcus et al., 1993) to parse all our English
monolingual data. However, there is no existing good
parser for Malagasy.
The quality of a dependency parser depends on the
amount of training data available. State-of-the-art En-
glish parsers are built from Penn Treebank, which con-
tains over 1 million tokens of annotated syntactical
1
globalvoicesonline.org
2
aoraha.com, gazetiko.com, inovaovao.com,
expressmada.com, lakroa.com
561
Figure 3: Learning curve showing our joint word alignment and decipherment approach improves word alignment
quality over the traditional EM without decipherment (Model 1: Iteration 1 to 10, HMM: Iteration 11 to 15)
trees. In contrast, the available data for training a Mala-
gasy parser is rather limited, with only 168 sentences,
and 2.8k tokens, as shown in Table 2. At the very be-
ginning, we use the last 120 sentences as training data
to train a part of speech (POS) tagger using a toolkit
provided by Garrette et al. (2013) and a dependency
parser with the Turbo parser. We test the performance
of the parser on the first 48 sentences and obtain 72.4%
accuracy.
One obvious way to improve tagging and parsing ac-
curacy is to get more annotated data. We find more data
with only part of speech tags containing 465 sentences
and 10k tokens released by (Garrette et al., 2013), and
add this data as extra training data for POS tagger.
Also, we download an online dictionary that contains
POS tags for over 60k Malagasy word types from mala-
gasyword.org. The dictionary is very helpful for tag-
ging words never seen in the training data.
It is natural to think that creation of annotated data
for training a POS tagger and a parser requires large
amounts of efforts from annotators who understand the
language well. However, we find that through the help
of parallel data and dictionaries, we are able to create
more annotated data by ourselves to improve tagging
and parsing accuracy. This idea is inspired by previ-
ous work that tries to learn a semi-supervised parser
by projecting dependency relations from one language
(with good dependency parsers) to another (Yarowsky
and Ngai, 2001; Ganchev et al., 2009). However, we
find those automatic approaches do not work well for
Malagasy.
To further expand our Malagasy training data, we
first use a POS tagger and parser with poor perfor-
mance to parse 788 sentences (20k tokens) on the
Malagasy side of the parallel corpus from Global
Voices. Then, we correct both the dependency links
and POS tags based on information from dictionaries
3
and the English translation of the parsed sentence. We
spent 3 months to manually project English dependen-
cies to Malagasy and eventually improve test set pars-
ing accuracy from 72.4% to 80.0%. We also make this
data available for future research use.
4.4 Machine Translation Experiments
In this section, we present the data used for our MT
experiments, and compare three different systems to
justify our joint word alignment and decipherment ap-
proach.
4.4.1 Baseline Machine Translation System
We build a state-of-the-art phrase-based MT system,
PBMT, using Moses (Koehn et al., 2007). PBMT has 3
models: a translation model, a distortion model, and
a language model. We train the other models using
half of the Global Voices parallel data (the rest is re-
served for development and testing), and build a 5-
gram language model using 834 million tokens from
AFP section of English Gigaword, 396 million tokens
from allAfrica, and the English part of the parallel cor-
pus for training. For alignment, we run 10 iterations
of Model 1, and 5 iterations of HMM. We did not run
Model 3 and Model 4 as we see no improvements in
Bleu scores from running those models. We do word
3
an online dictionary from malagasyword.org, as well as
a lexicon learned from the parallel data
562
alignment in two directions and use grow-diag-final-
and heuristic to obtain final alignment. During decod-
ing, we use 8 standard features in Moses to score a can-
didate translation: direct and inverse translation proba-
bilities, direct and inverse lexical weighting, a language
model score, a distortion score, phrase penalty, and
word penalty. The weights for the features are learned
on the tuning data using minimum error rate training
(MERT) (Och, 2003).
To compare with previous decipherment approach to
improve machine translation, we build a second base-
line system. We follow the work by Dou and Knight
(2013) to decipher Malagasy into English, and build a
translation lexicon T
decipher
from decipherment. To im-
prove machine translation, we simply use T
decipher
as
an additional parallel corpus. First, we filter T
decipher
by keeping only translation pairs ( f , e), where f is ob-
served in the Spanish part and e is observed in the En-
glish part of the parallel corpus. Then we append all
the Spanish and English words in the filtered T
decipher
to the end of Spanish part and English part of the paral-
lel corpus respectively. The training and tuning process
is the same as the baseline machine translation system
PBMT. We call this system Decipher-Pipeline.
4.4.2 Joint Word Alignment and Decipherment
for Machine Translation
When deciphering Malagasy to English, we extract
Malagasy dependency bigrams using all available
Malagasy monolingual data plus the Malagasy part of
the Global Voices parallel data, and extract English
dependency bigrams using 834 million tokens from
English Gigaword, and 396 million tokens from al-
lAfrica news to build an English dependency language
model. In the other direction, we extract English de-
pendency bigrams from English part of the entire paral-
lel corpus plus 9.7 million tokens from allAfrica news
4
, and use 17.3 million tokens Malagasy monolingual
data (15.3 million from the web and 2.0 million from
Global Voices) to build a Malagasy dependency lan-
guage model. We require that all dependency bigrams
only contain words observed in the parallel data used
to train the baseline MT system.
During learning, we run Model 1 without decipher-
ment for 5 iterations. Then we perform joint word
alignment and decipherment for another 5 iterations
with Model 1 and 5 iterations with HMM. We tune
decipherment weights (?) for Model 1 and HMM us-
ing grid search against Bleu score on a development
set. In the end, we only extract rules from one di-
rection P(English|Malagasy), where the decipherment
weights for Model 1 and HMM are 0.5 and 0.005 re-
spectively. We chose this because we did not find any
benefits to tune the weights on each direction, and then
use grow-diag-final-end heuristic to form final align-
ments. We call this system Decipher-Joint.
4
We do not find further Bleu gains by using more English
monolingual data.
Parallel
Malagasy English
Train (GV) 0.9 million 0.8 million
Tune (GV) 22.2k 20.2k
Test (GV) 23k 21k
Test (Web) 2.2k 2.1k
Non Parallel
Malagasy English
Gigaword N/A 834 million
Web 15.3 million 396 million
Table 3: Size of training, tuning, and testing data in
number of tokens (GV: Global Voices)
4.5 Results
We tune each system three times with MERT and
choose the best weights based on Bleu scores on tuning
set.
Table 4 shows that while using a translation lexicon
learnt from decipherment does not improve the quality
of machine translation significantly, the joint approach
improves Bleu score by 0.9 and 2.1 on Global Voices
test set and web news test set respectively. The results
show that the parsing quality correlates with gains in
Bleu scores. Scores in the brackets in the last row of
the table are achieved using a dependency parser with
72.4% attachment accuracy, while scores outside the
brackets are obtained using a dependency parser with
80.0% attachment accuracy.
We analyze the results and find the gain mainly
comes from two parts. First, adding expected counts
from non parallel data makes the distribution of trans-
lation probabilities sparser in word alignment models.
The probabilities of translation pairs favored by both
parallel data and decipherment becomes higher. This
gain is consistent with previous observation where a
sparse prior is applied to EM to help improve word
alignment and machine translation (Vaswani et al.,
2012). Second, expected counts from decipherment
also help discover new translation pairs in the paral-
lel data for low frequency words, where those words
are either aligned to NULL or wrong translations in the
baseline.
5 Conclusion and Future Work
We propose a new objective function for word align-
ment to combine the process of word alignment and
decipherment into a single task. In, experiments, we
find that the joint process performs better than previous
pipeline approach, and observe Bleu gains of 0.9 and
2.1 point on Global Voices and local web news test sets,
respectively. Finally, our research leads to the release
of 15.3 million tokens of monolingual Malagasy data
from the web as well as a small Malagasy dependency
tree bank containing 20k tokens.
Given the positive results we obtain by using the
joint approach to improve word alignment, we are in-
563
Decipherment System Tune (GV) Test (GV) Test (Web)
None PBMT (Baseline) 18.5 17.1 7.7
Separate Decipher-Pipeline 18.5 17.4 7.7
Joint Decipher-Joint 18.9 (18.7) 18.0 (17.7) 9.8 (8.5)
Table 4: Decipher-Pipeline does not show significant improvement over the baseline system. In contrast, Decipher-
Joint using joint word alignment and decipherment approach achieves a Bleu gain of 0.9 and 2.1 on the Global
Voices test set and the web news test set, respectively. The results in brackets are obtained using a parser trained
with only 120 sentences. (GV: Global Voices)
spired to apply this approach to help find translations
for out of vocabulary words, and to explore other pos-
sible ways to improve machine translation with deci-
pherment.
Acknowledgments
This work was supported by NSF Grant 0904684 and
ARO grant W911NF-10-1-0533. The authors would
like to thank David Chiang, Malte Nuhn, Victoria Fos-
sum, Ashish Vaswani, Ulf Hermjakob, Yang Gao, and
Hui Zhang (in no particular order) for their comments
and suggestions.
References
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual similar-
ity of labeled web images. In Proceedings of the
Twenty-Second International Joint Conference on
Artificial Intelligence - Volume Three. AAAI Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19:263?311.
Hal Daum?e, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1?
38.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics.
Qing Dou and Kevin Knight. 2013. Dependency-
based decipherment for resource-limited machine
translation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1. Association for Com-
putational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1. Association for Computational
Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1987. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. In Readings in computer vi-
sion: issues, problems, principles, and paradigms.
Morgan Kaufmann Publishers Inc.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguis-
tics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion. Association for Computational Linguistics, Au-
gust.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
564
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Ann Irvine, Chris Quirk, and Hal Daume III. 2013.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?u.
2008. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT. Association for Com-
putational Linguistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions.
Association for Computational Linguistics.
Patrik Lambert, Adri?a De Gispert, Rafael Banchs, and
Jos?e B. Mari?no. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39(4):267?285.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the Turbo: Fast third-order non-
projective Turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers - Volume
1. Association for Computational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the 33rd an-
nual meeting of Association for Computational Lin-
guistics. Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller alignment models for better trans-
lations: Unsupervised word alignment with the l0-
norm. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1. Association for Computational
Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2. Associa-
tion for Computational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language Technologies. Association for Compu-
tational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, Columbus,
Ohio. Association for Computational Linguistics.
565
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 39?47,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Transliteration Generation and Mining with Limited Training Resources
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma, Aditya Bhargava,
Qing Dou, Mi-Young Kim, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,dwyer,bergsma,abhargava,qdou,miyoung2,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL+: an online dis-
criminative sequence prediction model
based on many-to-many alignments,
which is further augmented by the in-
corporation of joint n-gram features.
Experimental results show improvement
over the results achieved by DIRECTL in
2009. We also explore a number of diverse
resource-free and language-independent
approaches to transliteration mining,
which range from simple to sophisticated.
1 Introduction
Many out-of-vocabulary words in statistical ma-
chine translation and cross-language information
retrieval are named entities. If the languages in
question use different writing scripts, such names
must be transliterated. Transliteration can be de-
fined as the conversion of a word from one writ-
ing script to another, which is usually based on the
phonetics of the original word.
DIRECTL+ is our current approach to name
transliteration which is an extension of the DI-
RECTL system (Jiampojamarn et al, 2009). We
augmented the feature set with joint n-gram fea-
tures which allow the discriminative model to uti-
lize long dependencies of joint information of
source and target substrings (Jiampojamarn et al,
2010). Experimental results suggest an improve-
ment over the results achieved by DIRECTL in
2009.
Transliteration mining aims at automatically
obtaining bilingual lists of names written in differ-
ent scripts. We explore a number of different ap-
proaches to transliteration mining in the context of
the NEWS 2010 Shared Task.1 The sole resource
that is provided for each language pair is a ?seed?
1http://translit.i2r.a-star.edu.sg/
news2010
dataset that contains 1K transliteration word pairs.
The objective is then to mine transliteration pairs
from a collection of Wikipedia titles/topics that are
given in both languages.
We explore a number of diverse resource-free
and language-independent approaches to translit-
eration mining. One approach is to bootstrap the
seed data by generating pseudo-negative exam-
ples, which are combined with the positives to
form a dataset that can be used to train a clas-
sifier. We are particularly interested in achiev-
ing good performance without utilizing language-
specific resources, so that the same approach can
be applied with minimal or no modifications to an
array of diverse language pairs.
This paper is divided in two main parts that cor-
respond to the two tasks of transliteration genera-
tion and transliteration mining.
2 Transliteration generation
The structure of this section is as follows. In Sec-
tion 2.1, we describe the pre-processing steps that
were applied to all datasets. Section 2.2 reviews
two methods for aligning the source and target
symbols in the training data. We provide details
on the DIRECTL+ systems in Section 2.3. In Sec-
tion 2.4, we discuss extensions of DIRECTL+ that
incorporate language-specific information. Sec-
tion 2.5 summarizes our results.
2.1 Pre-processing
For all generation tasks, we pre-process the pro-
vided data as follows. First, we convert all char-
acters in the source word to lower case. Then,
we remove non-alphabetic characters unless they
appear in both the source and target words. We
normalize whitespace that surrounds a comma, so
that there are no spaces before the comma and ex-
actly one space following the comma. Finally, we
separate multi-word titles into single words, using
whitespace as the separator. We assume a mono-
39
tonic matching and ignore the titles that have a dif-
ferent number of words on both sides.
We observed that in the ArAe task there are
cases where an extra space is added to the target
when transliterating from Arabic names to their
English equivalents; e.g., ?Al Riyad?, ?El Sayed?,
etc. In order to prevent the pre-processing from
removing too many title pairs, we allow non-equal
matching if the source title is a single word.
For the English-Chinese (EnCh) task, we con-
vert the English letter ?x? to ?ks? to facilitate bet-
ter matching with its Chinese targets.
During testing, we pre-process test data in the
same manner, except that we do not remove non-
alphabetic characters. After the pre-processing
steps, our system proposes 10-best lists for single
word titles in the test data. For multi-word titles,
we construct 10-best lists by ranking the combina-
tion scores of single words that make up the test
titles.
2.2 Alignment
In the transliteration tasks, training data consist
of pairs of names written in source and target
scripts without explicit character-level alignment.
In our experiments, we applied two different algo-
rithms to automatically generate alignments in the
training data. The generated alignments provide
hypotheses of substring mappings in the training
data. Given aligned training data, a transliteration
model is trained to generate names in the target
language given names in the source language.
The M2M-aligner (Jiampojamarn et al, 2007)
is based on the expectation maximization (EM)
algorithm. It allows us to create alignments be-
tween substrings of various lengths. We opti-
mized the maximum substring sizes for the source
and target based on the performance of the end
task on the development sets. We allowed empty
strings (nulls) only on the target side. We used the
M2M-aligner for all alignment tasks, except for
English-Pinyin alignment. The source code of the
M2M-aligner is publicly available.2
An alternative alignment algorithm is based on
the phonetic similarity of graphemes. The key idea
of this approach is to represent each grapheme by a
phoneme or a sequence of phonemes that is likely
to be represented by the grapheme. The sequences
of phonemes on the source side and the target
side can then be aligned on the basis of phonetic
2http://code.google.com/p/m2m-aligner/
b a r c - l a y
| | | | | | | |
b a - k u r - i
Figure 1: An alignment example.
similarity between phonemes. The main advan-
tage of the phonetic alignment is that it requires
no training data. We use the ALINE phonetic
aligner (Kondrak, 2000), which aligns two strings
of phonemes. The example in Figure 1 shows
the alignment of the word Barclay to its Katakana
transliteration ba-ku-ri. The one-to-one alignment
can then be converted to a many-to-many align-
ment by grouping the Japanese phonemes that cor-
respond to individual Katakana symbols.
2.3 DIRECTL+
We refer to our present approach to transliteration
as DIRECTL+. It is an extension of our DIRECTL
system (Jiampojamarn et al, 2009). It includes ad-
ditional ?joint n-gram? features that allow the dis-
criminative model to correlate longer source and
target substrings. The additional features allow
our discriminative model to train on information
that is present in generative joint n-gram models,
and additionally train on rich source-side context,
transition, and linear-chain features that have been
demonstrated to be important in the transliteration
task (Jiampojamarn et al, 2010).
Our model is based on an online discriminative
framework. At each training iteration, the model
generates an m-best list for each given source
name based on the current feature weights. The
feature weights are updated according to the gold-
standard answers and the generated m-best an-
swer lists using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003). This
training process iterates over the training examples
until the model converges. For m-best and n-gram
parameters, we set m = 10 and n = 6 for all lan-
guage pairs. These parameters as well as others
were optimized on the development sets.
We trained our models directly on the data
that were provided by the organizers, with three
exceptions. In order to improve performance,
we gave special treatment to English-Korean
(EnKo), English-Chinese (EnCh), and English-
Hindi (EnHi). These special cases are described
in the next section.
40
2.4 Beyond DIRECTL+
2.4.1 Korean Jaso
A Korean syllable can be decomposed into two
or three components called Jaso: an initial con-
sonant, a middle vowel, and optionally a final con-
sonant. The Korean generation for EnKo involves
the following three steps: (1) English-to-Jaso gen-
eration, (2) correction of illegal Jaso sequences,
and (3) Jaso-to-Korean conversion.
In order to correct illegal Jaso sequences that
cannot be combined into Korean syllables in step
2, we consider both vowel and consonant rules.
A Korean vowel can be either a simple vowel or
a complex vowel that combines two simple vow-
els. We can use this information in order to replace
double vowels with one complex vowel. We also
use the silent consonant o (i-eung) when we need
to insert a consonant between double vowels. A
Korean vowel - (eu) is most commonly inserted
between two English consonants in transliteration.
In order to resolve three consecutive consonants, it
can be placed into the most probable position ac-
cording to the probability distribution of the train-
ing data.
2.4.2 Japanese Katakana
In the Japanese Katakana generation task, we re-
place each Katakana symbol with one or two let-
ters using standard romanization tables. This has
the effect of expressing the target side in Latin let-
ters, which facilitates the alignment. DIRECTL+
is trained on the converted data to generate the tar-
get from the source. A post-processing program
then attempts to convert the generated letters back
into Katakana symbols. Sequences of letters that
cannot be converted into Katakana are removed
from the output m-best lists and replaced by lower
scoring sequences that pass the back-conversion
filter. Otherwise, there is usually a single valid
mapping because most Katakana symbols are rep-
resented by single vowels or a consonant-vowel
pair. The only apparent ambiguity involves the
letter n, which can either stand by itself or clus-
ter with the following vowel letter. We resolve the
ambiguity by always assuming the latter case un-
less the letter n occurs at the end of the word.
2.4.3 Chinese Pinyin
Following (Jiampojamarn et al, 2009), we experi-
mented with converting the original Chinese char-
acters to Pinyin as an intermediate representation.
Pinyin is the most commonly known romanization
system for Standard Mandarin and many free tools
are available for converting Chinese characters to
Pinyin. Its alphabet contains the same 26 letters
as English. Each Chinese character can be tran-
scribed phonetically into Pinyin. A small percent-
age of Chinese characters have multiple pronunci-
ations, and are thus represented by different Pinyin
sequences. For those characters, we manually se-
lected the pronunciations that are normally used
for names. This pre-processing step significantly
reduces the size of the target symbols: from 370
distinct Chinese characters to 26 Pinyin symbols.
This allows our system to produce better align-
ments.
We developed three models: (1) trained on the
original Chinese characters, (2) trained on Pinyin,
and (3) the model that incorporates the phonetic
alignment described in Section 2.2. The combi-
nation of the predictions of the different systems
was performed using the following simple algo-
rithm (Jiampojamarn et al, 2009). First, we rank
the individual systems according to their top-1 ac-
curacy on the development set. To obtain the top-
1 prediction for each input word, we use simple
voting, with ties broken according to the ranking
of the systems. We generalize this approach to
handle n-best lists by first ordering the candidate
transliterations according to the rank assigned by
each individual system, and then similarly break-
ing ties by voting and using the ranking of the sys-
tems.
2.4.4 Language identification for Hindi
Bhargava and Kondrak (2010) apply support vec-
tor machines (SVMs) to the task of identifying
the language of names. The intuition here is that
language information can inform transliteration.
Bhargava and Kondrak (2010) test this hypothe-
sis on the NEWS 2009 English-Hindi transliter-
ation data by training language identification on
data manually tagged as being of either Indian or
non-Indian origin. It was found that splitting the
data disjointly into two sets and training separate
transliteration models yields no performance in-
crease due to the decreased size of the data for the
models.
We adopt this approach for the NEWS 2010
task, but here we do not use disjoint splits. In-
stead, we use the SVMs to generate probabilities,
and then we apply a threshold to these probabili-
ties to generate two datasets. For example, if we
set the threshold to be 0.05, then we determine the
41
probabilities of a given name being of Indian ori-
gin (phi) and of being of non-Indian origin (pen).
If phi < 0.05 then the name is excluded from the
Indian set, and if pen < 0.05 then the name is
excluded from the non-Indian set. Using the two
obtained non-disjoint sets, we then train a translit-
eration model for each set using DIRECTL+.
Since the two sets are not disjoint, we must de-
cide how to combine the two results. Given that a
name occurs in both sets, and both models provide
a ranked list of possible targets for that name, we
obtain a combined ranking using a linear combi-
nation over the mean reciprocal ranks (MRRs) of
the two lists. The weights used are phi and pen so
that the more likely a name is considered to be of
Indian origin, the more strongly the result from the
Indian set is considered relative to the result from
the non-Indian set.
2.5 Evaluation
In the context of the NEWS 2010 Machine
Transliteration Shared Task we tested our sys-
tem on all twelve datasets: from English to Chi-
nese (EnCh), Thai (EnTh), Hindi (EnHi), Tamil
(EnTa), Bangla (EnBa), Kannada (EnKa), Ko-
rean Hangul (EnKo), Japanese Katakana (EnJa),
Japanese Kanji (JnJk); and, in the opposite di-
rection, to English from Arabic (ArAe), Chi-
nese (ChEn), and Thai (ThEn). For all datasets,
we trained transliteration models on the provided
training and development sets without additional
resources.
Table 1 shows our best results obtained on the
datasets in terms of top-1 accuracy and mean F-
score. We also include the rank in standard runs
ordered by top-1 word accuracy. The EnCh re-
sult presented in the table refers to the output of
the three-system combination, using the combi-
nation algorithm described in Section 2.4.3. The
respective results for the three component EnCh
systems were: 0.357, 0.360, and 0.363. The
EnJa result in the table refers the system described
in Section 2.4.2 that applied specific treatment
to Japanese Katakana. Based on our develop-
ment results, this specific treatment improves as
much as 2% top-1 accuracy over the language-
independent model. The EnHi system that in-
corporates language identification obtained ex-
actly the same top-1 accuracy as the language-
independent model. However, the EnKo system
with Jaso correction produced the top-1 accu-
Task top-1 F-score Rank
EnCh 0.363 0.707 2
ChEn 0.137 0.740 1
EnTh 0.378 0.866 2
ThEn 0.352 0.861 2
EnHi 0.456 0.884 1
EnTa 0.390 0.891 2
EnKa 0.341 0.867 2
EnJa 0.398 0.791 1
EnKo 0.554 0.770 1
JnJk 0.126 0.426 1
ArAe 0.464 0.924 1
EnBa 0.395 0.877 2
Table 1: Transliteration generation results
racy of 0.554, which is a significant improvement
over 0.387 achieved by the language-independent
model.
3 Transliteration mining
This section is structured as follows. In Sec-
tion 3.1, we describe the method of extracting
transliteration candidates that serves as the input
to the subsequently presented mining approaches.
Two techniques for generating negative exam-
ples are discussed in Section 3.2. Our language-
independent approaches to transliteration mining
are described in Section 3.3, and a technique for
mining English-Chinese pairs is proposed in Sec-
tion 3.4. In Section 3.5, we address the issue of
overlapping predictions. Finally, Section 3.6 and
Section 3.7 summarize our results.
3.1 Extracting transliteration candidates
We cast the transliteration mining task as a bi-
nary classification problem. That is, given a word
in the source language and a word in the target
language, a classifier predicts whether or not the
pair constitutes a valid transliteration. As a pre-
processing step, we extract candidate translitera-
tions from the pairs of Wikipedia titles. Word seg-
mentation is performed based on sequences of one
or more spaces and/or punctuation symbols, which
include hyphens, underscores, brackets, and sev-
eral other non-alphanumeric characters. Apostro-
phes and single quotes are not used for segmenta-
tion (and therefore remain in a given word); how-
ever, all single quote-like characters are converted
into a generic apostrophe. Once an English ti-
tle and its target language counterpart have been
42
segmented into words, we form the candidate set
for this title as the cross product of the two sets
of words after discarding any words that contain
fewer than two characters.
After the candidates have been extracted, indi-
vidual words are flagged for certain attributes that
may be used by our supervised learner as addi-
tional features. Alternatively, the flags may serve
as criteria for filtering the list of candidate pairs
prior to classification. We identify words that are
capitalized, consist of all lowercase (or all capital)
letters, and/or contain one or more digits. We also
attempt to encode each word in the target language
as an ASCII string, and flag that word if the opera-
tion succeeds. This can be used to filter out words
that are written in English on both the source and
target side, which are not transliterations by defi-
nition.
3.2 Generating negative training examples
The main issue with applying a supervised learn-
ing approach to the NEWS 2010 Shared Task is
that annotated task-specific data is not available
to train the system. However, the seed pairs do
provide example transliterations, and these can be
used as positive training examples. The remaining
issue is how to select the negative examples.
We adopt two approaches for selecting nega-
tives. First, we generate all possible source-target
pairs in the seed data, and take as negatives those
pairs which are not transliterations but have a
longest common subsequence ratio (LCSR) above
0.58; this mirrors the approach used by Bergsma
and Kondrak (2007). The method assumes that
the source and target words are written in the same
script (e.g., the foreign word has been romanized).
A second possibility is to generate all seed pair-
ings as above, but then randomly select negative
examples, thus mirroring the approach in Klemen-
tiev and Roth (2006). In this case, the source and
target scripts do not need to be the same. Com-
pared with the LCSR technique, random sampling
in this manner has the potential to produce nega-
tive examples that are very ?easy? (i.e., clearly not
transliterations), and which may be of limited util-
ity when training a classifier. On the other hand, at
test time, the set of candidates extracted from the
Wikipedia data will include pairs that have very
low LCSR scores; hence, it can be argued that dis-
similar pairs should also appear as negative exam-
ples in the training set.
3.3 Language-independent approaches
In this section, we describe methods for transliter-
ation mining that can, in principle, be applied to a
wide variety of language pairs without additional
modification. For the purposes of the Shared Task,
however, we convert all source (English) words to
ASCII by removing diacritics and making appro-
priate substitutions for foreign letters. This is done
to mitigate sparsity in the relatively small seed sets
when training our classifiers.
3.3.1 Alignment-derived romanization
We developed a simple method of performing ro-
manization of foreign scripts. Initially, the seed set
of transliterations is aligned using the one-to-one
option of the M2M-aligner approach (Jiampoja-
marn et al, 2007). We allow nulls on both the
source and target sides. The resulting alignment
model contains pairs of Latin letters and foreign
script symbols (graphemes) sorted by their con-
ditional probability. Then, for each grapheme,
we select a letter (or a null symbol) that has the
highest conditional probability. The process pro-
duces an approximate romanization table that can
be obtained without any knowledge of the target
script. This method of romanization was used by
all methods described in the remainder of Sec-
tion 3.3.
3.3.2 Normalized edit distance
Normalized edit distance (NED) is a measure of
the similarity of two strings. We define a uniform
edit cost for each of the three operations: substitu-
tion, insertion, and deletion. NED is computed by
dividing the minimum edit distance by the length
of the longer string, and subtracting the resulting
fraction from 1. Thus, the extreme values of NED
are 1 for identical strings, and 0 for strings that
have no characters in common.
Our baseline method, NED+ is simply the NED
measure augmented with filtering of the candidate
pairs described in Section 3.1. In order to address
the issue of morphological variants, we also fil-
ter out the pairs in which the English word ends
in a consonant and the foreign word ends with a
vowel. With no development set provided, we set
the similarity thresholds for individual languages
on the basis of the average word length in the seed
sets. The values were 0.38, 0.48, 0.52, and 0.58
for Hindi, Arabic, Tamil, and Russian, respec-
tively, with the last number taken from Bergsma
and Kondrak (2007).
43
3.3.3 Alignment-based string similarity
NED selects transliteration candidates when the
romanized foreign strings have high character
overlap with their English counterparts. The mea-
sure is independent of the language pair. This
is suboptimal for several reasons. First of all,
phonetically unrelated words can share many in-
cidental character matches. For example, the
French word ?recettes? and the English word
?proceeds? share the letters r,c,e,e,s as a com-
mon subsequence, but the words are phonetically
unrelated. Secondly, many reliable, recurrent,
language-specific substring matches are prevalent
in true transliterations. These pairings may or may
not involve matching characters. NED can not
learn or adapt to these language-specific patterns.
In light of these drawbacks, researchers have
proposed string similarity measures that can learn
from provided example pairs and adapt the simi-
larity function to a specific task (Ristad and Yiani-
los, 1998; Bilenko and Mooney, 2003; McCallum
et al, 2005; Klementiev and Roth, 2006).
One particularly successful approach is by
Bergsma and Kondrak (2007), who use discrim-
inative learning with an improved feature repre-
sentation. The features are substring pairs that are
consistent with a character-level alignment of the
two strings. This approach strongly improved per-
formance on cognate identification, while varia-
tions of it have also proven successful in transliter-
ation discovery (Goldwasser and Roth, 2008). We
therefore adopted this approach for the translitera-
tion mining task.
We produce negative training examples using
the LCSR threshold approach described in Sec-
tion 3.2. For features, we extract from the aligned
word pairs all substring pairs up to a maximum
length of three. We also append characters mark-
ing the beginning and end of words, as described
in Bergsma and Kondrak (2007). For our clas-
sifier, we use a Support Vector Machine (SVM)
training with the very efficient LIBLINEAR pack-
age (Fan et al, 2008). We optimize the SVM?s
regularization parameter using 10-fold cross vali-
dation on the generated training data. At test time,
we apply our classifier to all the transliteration
candidates extracted from the Wikipedia titles,
generating transliteration pairs whenever there is
a positive classification.
3.3.4 String kernel classifier
The alignment-based classifier described in the
preceding section is limited to using substring fea-
tures that are up to (roughly) three or four letters
in length, due to the combinatorial explosion in the
number of unique features as the substring length
increases. It is natural to ask whether longer sub-
strings can be utilized to learn a more accurate pre-
dictor.
This question inspired the development of a sec-
ond SVM-based learner that uses a string kernel,
and therefore does not have to explicitly repre-
sent feature vectors. Our kernel is a standard n-
gram (or spectrum) kernel that implicitly embeds
a string in a feature space that has one co-ordinate
for each unique n-gram (see, e.g., (Shawe-Taylor
and Cristianini, 2004)). Let us denote the alphabet
over input strings as A. Given two input strings x
and x?, this kernel function computes:
k(x, x?) =
?
s?An
#(s, x)#(s, x?)
where s is an n-gram and #(a, b) counts the num-
ber of times a appears as a substring of b.
An extension of the n-gram kernel that we em-
ploy here is to consider all n-grams of length
1 ? n ? k, and weight each n-gram as a func-
tion of its length. In particular, we specify a value
? and weight each n-gram by a factor of ?n. We
implemented this kernel in the LIBSVM software
package (Chang and Lin, 2001). Optimal values
for k, ?, and the SVM?s regularization parame-
ter were estimated for each dataset using 5-fold
cross-validation. The values of (k, ?) that we ul-
timately used were: EnAr (3, 0.8), EnHi (8, 0.8),
EnRu (5, 1.2), and EnTa (5, 1.0).
Our input string representation for a candidate
pair is formed by first aligning the source and tar-
get words using M2M-aligner (Jiampojamarn et
al., 2007). Specifically, an alignment model is
trained on the seed examples, which are subse-
quently aligned and used as positive training ex-
amples. We then generate 20K negative examples
by random sampling (cf. Section 3.2) and apply
the alignment model to this set. Not all of these
20K word pairs will necessarily be aligned; we
randomly select 10K of the successfully aligned
pairs to use as negative examples in the training
set.
Each aligned pair is converted into an ?align-
ment string? by placing the letters that appear in
44
Word pair zubtsov z u b ov
Aligned pair z|u|b|t|s|o|v| z|u|b|| |o|v|
Align?t string zz|uu|bb|t|s |oo|vv
Table 2: An example showing how an alignment
string (the input representation for the string ker-
nel) is created from a word pair.
the same position in the source and target next to
one another, while retaining the separator charac-
ters (see Table 2). We also appended beginning
and end of word markers. Note that no romaniza-
tion of the target words is necessary for this pro-
cedure.
At test time, we apply the alignment model to
the candidate word pairs that have been extracted
from the train data, and retain all the successfully
aligned pairs. Here, M2M-aligner also acts as a
filter, since we cannot form alignment strings from
unaligned pairs ? these yield negative predictions
by default. We also filter out pairs that met any of
the following conditions: 1) the English word con-
sists of all all capital or lowercase letters, 2) the
target word can be converted to ASCII (cf. Sec-
tion 3.1), or 3) either word contains a digit.
3.3.5 Generation-based approach
In the mining tasks, we are interested in whether a
candidate pair (x, y) is a transliteration pair. One
approach is to determine if the generated translit-
erations of a source word y? = ?(x) and a target
word x? = ?(y) are similar to the given candi-
date pair. We applied DIRECTL+ to the mining
tasks by training transliteration generation models
on the provided seed data in forward and back-
ward transliteration directions, creating ?(x) and
?(y) models. We now define a transliteration
score function in Eq. 1. N(x?, x) is the normal-
ized edit distance between string x? and x, and w1
and w2 are combination weights to favor forward
and backward transliteration models.
S(x, y) = w1 ? N(y?, y) + w2 ? N(x?, x)w1 + w2
(1)
A candidate pair is considered a transliteration
pair if its S(x, y) > ? . Ideally, we would like
to optimize these parameters, ?, w1, w2 based on
a development set for each language pair. Unfor-
tunately, no development sets were provided for
the Shared Task. Therefore, following Bergsma
and Kondrak (2007), we adopt the threshold of
? = 0.58. We experimented with three sets of val-
ues for w1 and w2: (1, 0), (0.5, 0.5), and (0, 1).
Our final predictions were made using w0 = 0
and w1 = 1, which appeared to produce the best
results. Thus, only the backward transliteration
model was ultimately employed.
3.4 English-Chinese string matching
Due to the fact that names transliterated into Chi-
nese consist of multiple Chinese characters and
that the Chinese text provided in this shared task
is not segmented, we have to adopt a different ap-
proach to the English-Chinese mining task (Unlike
many other languages, there are no clear bound-
aries between Chinese words). We first train a
generation model using the seed data and then ap-
ply a greedy string matching algorithm to extract
transliteration pairs.
The generation model is built using the discrim-
inative training framework described in (Jiampoja-
marn et al, 2008). Two models are learned: one
is trained using English and Chinese characters,
while the other is trained on English and Pinyin (a
standard phonetic representation of Chinese char-
acters). In order to mine transliteration pairs from
Wikipedia titles, we first use the generation model
to produce transliterations for each English token
on the source side as both Chinese characters and
Pinyin. The generated Chinese characters are ul-
timately converted to Pinyin during string match-
ing. We also convert all the Chinese characters on
the target side to their Pinyin representations when
performing string matching.
The transliteration pairs are then mined by com-
bining two different strategies. First of all, we ob-
serve that most of the titles that contain a separa-
tion symbol ? ? ? on the target side are translit-
erations. In this case, the number of tokens on
both sides is often equal. Therefore, the mining
task can be formulated as a matching problem.
We use a competitive linking approach (Melamed,
2000) to find the best match. First, we select
links between all possible pairs if similarity of
strings on both sides is above a threshold (0.6 ?
length(Pinyin)). We then greedily extract the
pairs with highest similarity until the number of
unextracted segments on either side becomes zero.
The problem becomes harder when there is no
indication of word segmentation for Chinese. In-
stead of trying to segment the Chinese characters
first, we use an incremental string matching strat-
45
egy. For each token on the source side, the algo-
rithm calculates its similarity with all possible n-
grams (2 ? n ? L) on the target side, where L
is the length of the Chinese title (i.e., the number
of characters). If the similarity score of n-gram
with the highest similarity surpasses a threshold
(0.5 ? length(Pinyin)), the n-gram sequence is
proposed as a possible transliteration for the cur-
rent source token.
3.5 Resolving overlapping predictions
Given a set of candidate word pairs that have been
extracted from a given Wikipedia title according to
the procedure described in Section 3.1, our clas-
sifiers predict a class label for each pair inde-
pendently of the others. Pairs that receive neg-
ative predictions are discarded immediately and
are never reported as mined pairs. However, it
is sometimes necessary to arbitrate between pos-
itive predictions, since it is possible for a classifier
to mark as transliterations two or more pairs that
involve the same English word or the same target
word in the title. Clearly, mining multiple overlap-
ping pairs will lower the system?s precision, since
there is (presumably) at most one correct translit-
eration in the target language version of the title
for each English word.3
Our solution is to apply a greedy algorithm that
sorts the word pair predictions for a given title
in descending order according to the scores that
were assigned by the classifier. We make one pass
through the sorted list and report a pair of words as
a mined pair unless the English word or the target
language word has already been reported (for this
particular title).4
3.6 Results
In the context of the NEWS 2010 Shared Task
on Transliteration Generation we tested our sys-
tem on all five data sets: from English to Rus-
sian (EnRu), Hindi (EnHi), Tamil (EnTa), Arabic
(EnAr), and Chinese (EnCh). The EnCh set dif-
fers from the remaining sets in the lack of transpar-
ent word segmentation on the Chinese side. There
were no development sets provided for any of the
language pairs.
3On the other hand, mining all such pairs might improve
recall.
4A bug was later discovered in our implementation of this
algorithm, which had failed to add the words in a title?s first
mined pair to the ?already reported? list. This sometimes
caused up to two additional mined pairs per title to be re-
ported in the prediction files that were submitted.
Task System F P R
EnRu NED+ .875 .880 .869
BK-2007 .778 .684 .902
StringKernel* .811 .746 .889
DIRECTL+ .786 .778 .795
EnHi NED+ .907 .875 .941
BK-2007 .882 .883 .880
StringKernel .924 .954 .895
DIRECTL+ .904 .945 .866
EnTa NED+ .791 .916 .696
BK-2007 .829 .808 .852
StringKernel .914 .923 .906
DIRECTL+ .801 .919 .710
EnAr NED+ .800 .818 .783
BK-2007 .816 .834 .798
StringKernel* .827 .917 .753
DIRECTL+ .742 .861 .652
EnCh GreedyMatch .530 .698 .427
DIRECTL+ .009 .045 .005
Table 3: Transliteration mining results. An aster-
isk (*) indicates an unofficial result.
Table 3 shows the results obtained by our var-
ious systems on the final test sets, measured in
terms of F-score (F), precision (P), and recall
(R). The systems referred to as NED+, BK-2007,
StringKernel, DIRECTL+, and GreedyMatch are
described in Section 3.3.2, Section 3.3.3, Sec-
tion 3.3.4, Section 3.3.5, and Section 3.4 respec-
tively. The runs marked with an asterisk (*)
were produced after the Shared Task deadline, and
therefore are not included in the official results.
3.7 Discussion
No fixed ranking of the four approaches emerges
across the four alphabetic language pairs (all ex-
cept EnCh). However, StringKernel appears to be
the most robust, achieving the highest F-score on
three language pairs. This suggests that longer
substring features are indeed useful for classifying
candidate transliteration pairs. The simple NED+
method is a clear winner on EnRu, and obtains de-
cent scores on the remaining alphabetic language
pairs. The generation-based DIRECTL+ approach
ranks no higher than third on any language pair,
and it fails spectacularly on EnCh because of the
word segmentation ambiguity.
Finally, we observe that there are a number of
cases where the results for our discriminatively
trained classifiers, BK-2007 and StringKernel, are
46
not significantly better than those of the simple
NED+ approach. We conjecture that automatically
generating training examples is suboptimal for this
task. A more effective strategy may be to filter all
possible word pairs in the seed data to only those
with NED above a fixed threshold. We would then
apply the same threshold to the Wikipedia candi-
dates, only passing to the classifier those pairs that
surpass the threshold. This would enable a better
match between the training and test operation of
the system.
4 Conclusion
The results obtained in the context of the NEWS
2010 Machine Transliteration Shared Task con-
firm the effectiveness of our discriminative ap-
proaches to transliteration generation and mining.
Acknowledgments
This research was supported by the Alberta Inge-
nuity Fund, Informatics Circle of Research Excel-
lence (iCORE), and the Natural Sciences and En-
gineering Research Council of Canada (NSERC).
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proc. ACL.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
NAACL-HLT.
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proc. KDD.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JMLR,
9:1871?1874.
Dan Goldwasser and Dan Roth. 2008. Transliteration
as constrained optimization. In Proc. EMNLP.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In NEWS ?09: Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration, pages 28?31.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010. Integrating joint n-gram features
into a discriminative training framework. In Proc.
NAACL-HLT.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In Proc. HLT-NAACL.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proc. UAI.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, 20(5).
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
47
