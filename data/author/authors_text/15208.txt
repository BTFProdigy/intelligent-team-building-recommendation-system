Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9?12,
New York, June 2006. c?2006 Association for Computational Linguistics
Museli: A Multi-Source Evidence Integration Approach to Topic Seg-
mentation of Spontaneous Dialogue 
 
  
Jaime Arguello Carolyn Ros? 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213 Pittsburgh, PA 15213 
jarguell@andrew.cmu.edu cprose@cs.cmu.edu 
Abstract 
We introduce a novel topic segmentation 
approach that combines evidence of topic 
shifts from lexical cohesion with linguistic 
evidence such as syntactically distinct fea-
tures of segment initial contributions.  Our 
evaluation demonstrates that this hybrid 
approach outperforms state-of-the-art algo-
rithms even when applied to loosely struc-
tured, spontaneous dialogue. 
1 Introduction    
Use of topic-based models of dialogue has 
played a role in information retrieval (Oard et al, 
2004), information extraction (Baufaden, 2001), 
and summarization (Zechner, 2001). However, 
previous work on automatic topic segmentation has 
focused primarily on segmentation of expository 
text.  We present Museli, a novel topic segmenta-
tion approach for dialogue that integrates evidence 
of topic shifts from lexical cohesion with linguistic 
indicators such as syntactically distinct features of 
segment initial contributions. 
Our evaluation demonstrates that approaches de-
signed for text do not generalize well to dialogue.  
We demonstrate a significant advantage of Museli 
over competing approaches.  We then discuss why 
models based entirely on lexical cohesion fail on 
dialogue and how our algorithm compensates with 
other topic shift indicators.  
2 Previous Work 
Existing topic segmentation approaches can be 
loosely classified into two types: (1) lexical cohe-
sion models, and (2) content-oriented models.  The 
underlying assumption in lexical cohesion models 
is that a shift in term distribution signals a shift in 
topic (Halliday and Hassan, 1976). The best known 
algorithm based on this idea is TextTiling (Hearst, 
1997). In TextTiling, a sliding window is passed 
over the vector-space representation of the text. At 
each position, the cosine correlation between the 
upper and lower region of the sliding window is 
compared with that of the peak cosine correlation 
values to the left and right of the window.  A seg-
ment boundary is predicted when the magnitude of 
the difference exceeds a threshold.    
One drawback to relying on term co-occurrence 
to signal topic continuity is that synonyms or re-
lated terms are treated as thematically-unrelated. 
One solution to this problem is using a dimension-
ality reduction technique such as Latent Semantic 
Analysis (LSA) (Landauer and Dumais, 1997). 
Two such algorithms for segmentation are de-
scribed in (Foltz, 1998) and (Olney and Cai, 2005).  
Both TextTiling and Foltz?s approach measure 
coherence as a function of the repetition of the-
matically-related terms. TextTiling looks for co-
occurrences of terms or term-stems and Foltz uses 
LSA to measure semantic relatedness between 
terms.  Olney and Cai?s orthonormal basis ap-
proach also uses LSA, but allows a richer represen-
tation of discourse coherence, which is that coher-
ence is a function of how much new information a 
discourse unit (e.g. a dialogue contribution) adds  
(informativity) and how relevant it is to the local 
context (relevance) (Olney and Cai, 2005). 
Content-oriented models, such as (Barzilay and 
Lee, 2004), rely on the re-occurrence of patterns of 
topics over multiple realizations of thematically 
similar discourses, such as a series of newspaper 
articles about similar events. Their approach util-
izes a hidden Markov model where states corre-
spond to topics, and state transition probabilities 
correspond to topic shifts. To obtain the desired 
9
number of topics (states), text spans of uniform 
length (individual contributions, in our case) are 
clustered. Then, state emission probabilities are 
induced using smoothed cluster-specific language 
models. Transition probabilities are induced by 
considering the proportion of documents in which 
a contribution assigned to the source cluster (state) 
immediately precedes a contribution assigned to 
the target cluster (state). Using an EM-like Viterbi 
approach, each contribution is reassigned to the 
state most likely to have generated it.  
3 Overview of Museli Approach 
We will demonstrate that lexical cohesion alone 
does not adequately mark topic boundaries in dia-
logue.  Nevertheless, it can provide one meaning-
ful source of evidence towards segmenting dia-
logue. In our hybrid Museli approach, we com-
bined lexical cohesion with features that have the 
potential to capture something about the linguistic 
style that marks shifts in topic: word-unigrams, 
word-bigrams, and POS-bigrams for the current 
and previous contributions; the inclusion of at least 
one non-stopword term (contribution of content); 
time difference between contributions; contribution 
length; and the agent role of the previous and cur-
rent contribution.  
We cast the segmentation problem as a binary 
classification problem where each contribution is 
classified as NEW_TOPIC if the contribution in-
troduces a new topic and SAME_TOPIC other-
wise.  We found that using a Na?ve Bayes classifier 
(John & Langley, 1995) with an attribute selection 
wrapper using the chi-square test for ranking at-
tributes performed better than other state-of-the-art 
machine learning algorithms, perhaps because of 
the evidence integration oriented nature of the 
problem.  We conducted our evaluation using 10-
fold cross-validation, being careful not to include 
instances from the same dialogue in both the train-
ing and test sets on any fold so that the results we 
report would not be biased by idiosyncratic com-
municative patterns associated with individual 
conversational participants picked up by the 
trained model.  
Using the complete set of features enumerated 
above, we perform feature selection on the training 
data for each fold of the cross-validation sepa-
rately, training a model with the top 1000 features, 
and applying that trained model to the test data.  
Examples of high ranking features confirm our 
intuition that contributions that begin new topic 
segments are syntactically marked.  For example, 
many typical selected word bigrams were indica-
tive of imperatives, such as lets-do, do-the, ok-lets, 
ok-try, lets-see, etc.  Others included time oriented 
discourse markers such as now, then, next, etc. 
To capitalize on differences in conversational 
behavior between participants assigned to different 
roles in the conversation (i.e., student and tutor in 
our evaluation corpora), we learn separate models 
for each role in the conversation1. This decision is 
based on the observation that participants with dif-
ferent agent-roles introduce topics with a different 
frequency, introduce different types of topics, and 
may introduce topics in a different style that dis-
plays their status in the conversation. For instance, 
a tutor may introduce new topics with a contribu-
tion that ends with an imperative. A student may 
introduce new topics with a contribution that ends 
with a wh-question.   
4 Evaluation 
In this section we evaluate Museli in comparison 
to the best performing state-of-the-art approaches, 
demonstrating that our hybrid Museli approach 
out-performs all of these approaches on two differ-
ent dialogue corpora by a statistically significant 
margin (p < .01), in one case reducing the prob-
ability of error as measured by Beeferman's Pk to 
only 10% (Beeferman et al, 1999). 
4.1 Experimental Corpora 
We used two different dialogue corpora for our 
evaluation.  The first corpus, which we refer to as the 
Olney & Cai corpus, is a set of dialogues selected ran-
domly from the same corpus Olney and Cai selected 
their corpus from (Olney and Cai, 2005). The second 
corpus is a locally collected corpus of thermodynamics 
tutoring dialogues, which we refer to as the Thermo 
corpus. This corpus is particularly appropriate for ad-
dressing the research question of how to automatically 
segment dialogue for two reasons: First, the explora-
tory task that students and tutors engaged in together is 
more loosely structured than many task oriented do-
mains typically investigated in the dialogue commu-
nity, such as flight reservation or meeting scheduling.  
Second, because the tutor and student play asymmetric 
roles in the interaction, this corpus allows us to explore 
                                                 
1 Dissimilar agent-roles occur in other domains as well (e.g. 
Travel Agent and Customer)
10
how conversational role affects how speakers mark 
topic shifts.   
Table 1 presents statistics describing characteris-
tics of these two corpora.  Similar to (Passonneau 
and Litman, 1993), we adopt a flat model of topic-
segmentation for our gold standard based on dis-
course segment purpose, where a shift in topic cor-
responds to a shift in purpose that is acknowledged 
and acted upon by both conversational agents. We 
evaluated inter-coder reliability over 10% of the 
Thermo corpus mentioned above.  3 annotators 
were given a 10 page coding manual with explana-
tion of our informal definition of shared discourse 
segment purpose as well as examples of segmented 
dialogues.  Pairwise inter-coder agreement was 
above 0.7 kappa for all pairs of annotators. 
 
 Olney & Cai  
Corpus 
Thermo 
Corpus 
# Dialogues 42 22 
Contributions/ 
Dialogue 
195.40 217.90 
Contributions/ 
Topic 
24.00 13.31 
Topics/Dialogue 8.14 16.36 
Words/ 
Contribution 
28.63 5.12 
Table 1: Evaluation Corpora Statistics 
4.2 Baseline Approaches 
We evaluate Museli against the following algo-
rithms: (1) Olney and Cai (Ortho), (2) Barzilay and 
Lee (B&L), (3) TextTiling (TT), and (4) Foltz.  
As opposed to the other baseline algorithms, 
(Olney and Cai, 2005) applied their orthonormal 
basis approach specifically to dialogue, and prior 
to this work, report the highest numbers for topic 
segmentation of dialogue. Barzilay and Lee?s ap-
proach is the state of the art in modeling topic 
shifts in monologue text. Our application of B&L 
to dialogue attempts to harness any existing and 
recognizable redundancy in topic-flow across our 
dialogues for the purpose of topic segmentation.  
We chose TextTiling for its seminal contribution 
to monologue segmentation. TextTiling and Foltz 
consider lexical cohesion as their only evidence of 
topic shifts. Applying these approaches to dialogue 
segmentation sheds light on how term distribution 
in dialogue differs from that of expository mono-
logue text (e.g. news articles).  
The Foltz and Ortho approaches require a 
trained LSA space, which we prepared as de-
scribed in (Olney and Cai, 2005). Any parameter 
tuning for approaches other than our hybrid ap-
proach was computed over the entire test set, giv-
ing competing algorithms the maximum advantage. 
In addition to these approaches, we include 
segmentation results from three degenerate ap-
proaches: (1) classifying all contributions as 
NEW_TOPIC (ALL), (2) classifying no contribu-
tions as NEW_TOPIC (NONE), and (3) classifying 
contributions as NEW_TOPIC at uniform intervals 
(EVEN), corresponding to the average reference 
topic length (see Table 1). 
As a means for comparison, we adopt two evalua-
tion metrics: Pk and f-measure. An extensive argu-
ment of Pk?s robustness (if k is set to ? the average 
reference topic length) is present in (Beeferman, et al 
1999).  Pk measures the probability of misclassifying 
two contributions a distance of k contributions apart, 
where the classification question is are the two con-
tributions part of the same topic segment or not?  
Lower Pk values are preferred over higher ones. It 
equally captures the effect of false-negatives and 
false-positives and it favors near misses. F-measure 
punishes false positives equally, regardless of the 
distance to the reference boundary.  
4.3 Results 
Results for all approaches are displayed in Table 
2.  Note that lower values of Pk are preferred over 
higher ones. The opposite is true of F-measure.  In 
both corpora, Museli performed significantly better 
than all other approaches (p <  .01).   
 
 Olney & Cai Corpus Thermo Corpus 
 Pk F Pk F 
NONE 0.4897 -- 0.4900 -- 
ALL 0.5180 -- 0.5100 -- 
EVEN 0.5117 -- 0.5132 -- 
TT 0.6240 0.1475 0.5353 0.1614 
B&L 0.6351 0.1747 0.5086 0.1512 
Foltz 0.3270 0.3492 0.5058 0.1180 
Ortho 0.2754 0.6012 0.4898 0.2111 
Museli 0.1051 0.8013 0.4043 0.3693 
Table 2: Results on both corpora 
4.4 Error Analysis 
Results for all approaches are better on the Ol-
ney and Cai corpus than the Thermo corpus. The 
Thermo corpus differs profoundly from the Olney 
and Cai corpus in ways that very likely influenced 
the performance. For instance, in the Thermo cor-
pus each dialogue contribution is an average of 5 
words long, whereas in the Olney and Cai corpus 
11
each dialogue contribution contains an average of 
28 words. Thus, the vector space representation of 
the dialogue contributions is much more sparse in 
the Thermo corpus, which makes shifts in lexical 
coherence less reliable as topic shift indicators.   
In terms of Pk, TextTiling (TT) performed worse 
than the degenerate algorithms. TextTiling meas-
ures the term-overlap between adjacent regions in 
the discourse. However, dialogue contributions are 
often terse or even contentless. This produces 
many islands of contribution-sequences for which 
the local lexical cohesion is zero. TextTiling 
wrongfully classifies all of these as starts of new 
topics.  A heuristic improvement to prevent 
TextTiling from placing topic boundaries at every 
point along a sequence of contributions failed to 
produce a statistically significant improvement. 
The Foltz and the orthonormal basis approaches 
rely on LSA to provide strategic semantic gener-
alizations. Following (Olney and Cai, 2005), we 
built our LSA space using dialogue contributions 
as the atomic text unit.  However, in corpora such 
as the Thermo corpus, this may not be effective 
because of the brevity of contributions. 
Barzilay and Lee?s algorithm (B&L) did not 
generalize well to either dialogue corpus. One rea-
son could be that such probabilistic methods re-
quire that reference topics have significantly dif-
ferent language models, which was not true in ei-
ther of our evaluation corpora. We also noticed a 
number of instances in the dialogue corpora where 
participants referred to information from previous 
topic segments, which consequently may have 
blurred the distinction between the language mod-
els assigned to different topics. 
5 Current Directions 
In this paper we address the problem of auto-
matic topic segmentation of spontaneous dialogue.  
We demonstrated with an empirical evaluation that 
state-of-the-art approaches fail on spontaneous dia-
logue because word-distribution patterns alone are 
insufficient evidence of topic shifts in dialogue.  
We have presented a supervised learning algorithm 
for topic segmentation of dialogue that combines 
linguistic features signaling a contribution?s func-
tion with lexical cohesion. Our evaluation on two 
distinct dialogue corpora shows a significant im-
provement over the state of the art approaches.  
The disadvantage of our approach is that it re-
quires hand-labeled training data. We are currently 
exploring ways of bootstrapping a model from a 
small amount of hand labeled data in combination 
with lexical cohesion (tuned for high precision and 
consequently low recall) and some reliable dis-
course markers.  
Acknowledgments 
This work was funded by Office of Naval Re-
search, Cognitive and Neural Science Division, 
grant number N00014-05-1-0043. 
References  
Regina Barzilay and Lillian Lee (2004). Catching the 
drift: Probabilistic Content Models, with Applications 
to Generation and Summarization. In Proceedings of 
HLT-NAACL 2004.  
Doug Beeferman, Adam Berger, John D. Lafferty 
(1999).  Statistical Models for Text Segmentation. 
Machine Learning 34 (1-3): 177-210. 
Narj?s Boufaden, Guy Lapalme, Yoshua Bengio (2001). 
Topic Segmentation: A first stage to Dialog-based In-
formation Extraction. In Proceedings of NLPRS 2001. 
P.W. Foltz, W. Kintsch, and Thomas Landauer (1998). 
The measurement of textual cohesion with latent se-
mantic analysis. Discourse Processes, 25, 285-307. 
M. A. K. Halliday and Ruqaiya Hasan (1976). Cohesion 
in English. London: Longman. 
Marti Hearst. 1997. TextTiling: Segmenting Text into 
Multi-Paragragh Subtopic Passages. Computational 
Linguistics, 23(1), 33 ? 64.   
George John & Pat Langley (1995).  Estimating Con-
tinuous Distributions in Bayesian Classifiers.  In Pro-
ceedings of UAI 2005. 
Thomas Landauer, & Susan Dumais (1997). A Solution 
to Plato?s Problem: The Latent Semantic Analysis of 
Acquisition, Induction, and Representation of Knowl-
edge. Psychological Review, 104, 221-240.  
Douglas Oard, Bhuvana Ramabhadran, and Samuel 
Gustman (2004). Building an Information Retrieval 
Test Collection for Spontaneous Conversational 
Speech. In Proceedings of SIGIR 2004.  
Andrew Olney and Zhiqiang Cai (2005). An Orthonor-
mal Basis for Topic Segmentation of Tutorial Dia-
logue. In Proceedings of HLT-EMNLP 2005. 
Rebecca Passonneau and Diane Litman (1993). Inten-
tion-Based Segmentation: Human Reliability and 
Correlation with Linguistic Cues. In Proceedings 
ACL 2003.  
Klaus Zechner (2001). Automatic Generation of Con-
cise Summaries of Spoken Dialogues in Unrestricted 
Domains. In Proceedings of SIGIR 2001.  
12
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 253?256,
New York City, June 2006. c?2006 Association for Computational Linguistics
InfoMagnets: Making Sense of Corpus Data 
 
 Jaime Arguello Carolyn Ros? 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15216 Pittsburgh, PA 15216 
jarguell@andrew.cmu.edu cprose@cs.cmu.edu 
 
  
Abstract 
We introduce a new interactive corpus 
exploration tool called InfoMagnets. In-
foMagnets aims at making exploratory 
corpus analysis accessible to researchers 
who are not experts in text mining. As 
evidence of its usefulness and usability, it 
has been used successfully in a research 
context to uncover relationships between 
language and behavioral patterns in two 
distinct domains: tutorial dialogue 
(Kumar et al, submitted) and on-line 
communities (Arguello et al, 2006). As 
an educational tool, it has been used as 
part of a unit on protocol analysis in an 
Educational Research Methods course.  
1 Introduction 
Exploring large text corpora can be a daunting 
prospect. This is especially the case for behavioral 
researchers who have a vested interest in the latent 
patterns present in text, but are less interested in 
computational models of text-representation (e.g. 
the vector-space model) or unsupervised pattern-
learning (e.g. clustering). Our goal is to provide 
this technology to the broader community of learn-
ing scientists and other behavioral researchers who 
collect and code corpus data as an important part 
of their research.  To date none of the tools that are 
commonly used in the behavioral research com-
munity, such as HyperResearch, MacShapa, or 
Nvivo, which are used to support their corpus 
analysis efforts, make use of technology more ad-
vanced than simplistic word counting approaches.  
With InfoMagnets, we are working towards bridg-
ing the gap between the text-mining community 
and the corpus-based behavioral research commu-
nity. The purpose of our demonstration is to make 
the language technologies community more aware 
of opportunities for applications of language tech-
nologies to support corpus oriented behavioral re-
search. 
 
 
Figure 1: InfoMagnets Screenshot 
 
InfoMagnet?s novelty is two-fold: First, it pro-
vides an intuitive visual metaphor that allows the 
user to get a sense of their data and organize it for 
easy retrieval later. This is important during the 
sense making stage of corpus analysis work just 
before formal coding scheme development begins.  
Secondly, it allows the user to interact with cluster-
ing technology, and thus influence its behavior, in 
effect introducing human knowledge into the clus-
tering process.  Because of this give and take be-
tween the clustering technology and the human 
253
influence, the tool is able to achieve an organiza-
tion of textual units that is not just optimal from an 
algorithmic stand-point, but also optimal for the 
user?s unique purpose, which non-interactive clus-
tering algorithms are not in general capable of 
achieving.  
Using visual metaphors to convey to the user 
proximity and relations between documents and 
automatically generated clusters is not a new tech-
nique (Chalmers and Chitson, 1992; Dubin, 1995; 
Wise et al, 1995; Leuski and Allan, 2000; Ras-
mussen and Karypis, 2004). InfoMagnet?s novelty 
comes from giving the user more control over the 
ultimate clustering organization. The user is able to 
incrementally influence the formation and reor-
ganization of cluster centroids and immediately see 
the effect on the text-to-cluster assignment. Thus, 
the user can explore the corpus in more effective 
and meaningful ways.  
In what follows, we more concretely elaborate 
on InfoMagnet?s functionality and technical de-
tails. We then motivate its usability and usefulness 
with a real case study.   
2 Functionality  
Exploring a textual corpus in search of interest-
ing topical patterns that correlate with externally 
observable variables is a non-trivial task.  Take as 
an example the task of characterizing the process 
by which students and tutors negotiate with one 
another over a chat interface as they navigate in-
structional materials together in an on-line explora-
tory learning environment. A sensible approach is 
to segment all dialogue transcripts into topic-
oriented segments and then group the segments by 
topic similarity. If done manually, this is a chal-
lenging task in two respects. First, to segment each 
dialogue the analyst must rely on their knowledge 
of the domain to locate where the focus of the dia-
logue shifts from one topic to the next. This, of 
course, requires the analyst to know what to look 
for and to remain consistent throughout the whole 
set of dialogues. More importantly, it introduces 
into the topic analysis a primacy bias. The analyst 
may miss important dialogue digressions simply 
because they are not expected based on observa-
tions from the first few dialogues viewed in detail.  
InfoMagnets addresses these issues by offering 
users a constant bird?s eye view of their data.  See 
Figure 1. 
As input, InfoMagnets accepts a corpus of tex-
tual documents. As an option to the user, the docu-
ments can be automatically fragmented into 
topically-coherent segments (referred to also as 
documents from here on), which then become the 
atomic textual unit1. The documents (or topic seg-
ments) are automatically clustered into an initial 
organization that the user then incrementally ad-
justs through the interface. Figure 1 shows the ini-
tial document-to-topic assignment that 
InfoMagnets produces as a starting point for the 
user. The large circles represent InfoMagnets, or 
topic oriented cluster centroids, and the smaller 
circles represent documents. An InfoMagnet can 
be thought of as a set of words representative of a 
topic concept. The similarity between the vector 
representation of the words in a document and that 
of the words in an InfoMagnet translate into attrac-
tion in the two-dimensional InfoMagnet space.  
This semantic similarity is computed using Latent 
Semantic Analysis (LSA) (Landauer et al,  1998). 
Thus, a document appears closest to the InfoMag-
net that best represents its topic.  
A document that appears equidistant to two In-
foMagnets shares its content equally between the 
two represented topics. Topics with lots of docu-
ments nearby are popular topics. InfoMagnets with 
only a few documents nearby represent infrequent 
topics. Should the user decide to remove an In-
foMagnet, any document with some level of attrac-
tion to that InfoMagnet will animate and reposition 
itself based on the topics still represented by the 
remaining InfoMagnets.  At all times, the In-
foMagnets interface offers the analyst a bird?s eye 
view of the entire corpus as it is being analyzed 
and organized. 
Given the automatically-generated initial topic 
representation, the user typically starts by brows-
ing the different InfoMagnets and documents. Us-
ing a magnifying cross-hair lens, the user can view 
the contents of a document on the top pane. As 
noted above, each InfoMagnet represents a topic 
concept through a collection of words (from the 
corpus) that convey that concept. Selecting the In-
foMagnet displays this list of words on the left 
pane. The list is shown in descending order of im-
portance with respect to that topic. By browsing 
each InfoMagnet?s list of words and browsing 
                                                          
1 Due to lack of space, we do not focus on our topic-
segmentation algorithm. We intend to discuss this in the demo.  
254
nearby documents, the user can start recognizing 
topics represented in the InfoMagnet space and can 
start labeling those InfoMagnets.  
InfoMagnets with only a few neighboring 
documents can be removed. Likewise, InfoMag-
nets attracting too many topically-unrelated docu-
ments can be split into multiple topics. The user 
can do this semi-automatically (by requesting a 
split, and allowing the algorithm to determine 
where the best split is) or by manually selecting a 
set of terms from the InfoMagnet?s word list and 
creating a new InfoMagnet using those words to 
represent the new InfoMagnet?s topic. If the user 
finds words in an InfoMagnet?s word list that lack 
topical relevance, the user can remove them from 
InfoMagnet?s word list or from all the InfoMag-
nets? word lists at once. 
Users may also choose to manually assign a seg-
ment to a topic by ?snapping? that document to an 
InfoMagnet. ?Snapping? is a way of overriding the 
attraction between the document and other In-
foMagnets.  By ?snapping? a document to an In-
foMagnet, the relationship between the ?snapped? 
document and the associated InfoMagnet remains 
constant, regardless of any changes made to the 
InfoMagnet space subsequently.  
If a user would like to remove the influence of a 
subset of the corpus from the behavior of the tool, 
the user may select an InfoMagnet and all the 
documents close to it and place them in the ?quar-
antine? area of the interface. When placed in the 
quarantine, as when ?snapped?, a document?s as-
signment remains unchanged. This feature is used 
to free screen space for the user.  
If the user opts for segmenting each input dis-
course and working with topic segments rather 
than whole documents, an alternative interface al-
lows the user to quickly browse through the corpus 
sequentially (Figure 2).  By switching between this 
view and the bird?s eye view, the user is able to see 
where each segment fits sequentially into the larger 
context of the discourse it was extracted from.  The 
user can also use the sequential interface for mak-
ing minor adjustments to topic segment boundaries 
and topic assignments where necessary.  Once the 
user is satisfied with the topic representation in the 
space and the assignments of all documents to 
those topics, the tool can automatically generate an 
XML file, where all documents are tagged with 
their corresponding topic labels. 
 
 
Figure 2. InfoMagnet?s alternative sequential view 
3 Implementation 
As mentioned previously, InfoMagnets uses La-
tent Semantic Analysis (LSA) to relate documents 
to InfoMagnets. LSA is a dimensionality reduction 
technique that can be used to compute the semantic 
similarity between text spans of arbitrary size. For 
a more technical overview of LSA, we direct the 
reader to (Landauer et al, 1998).  
The LSA space is constructed using the corpus 
that the user desires to organize, possibly aug-
mented with some general purpose text (such as 
newsgroup data) to introduce more domain-general 
term associations. The parameters used in building 
the space are set by the user during pre-processing, 
so that the space is consistent with the semantic 
granularity the user is interested in capturing.  
Because documents (or topic-segments) tend to 
cover more than one relevant topic, our clustering 
approach is based on what are determined heuristi-
cally to be the most important terms in the corpus, 
and not on whole documents. This higher granular-
ity allows us to more precisely capture the topics 
discussed in the corpus by not imposing the as-
sumption that documents are about a single topic. 
First, all terms that occur less than n times and in 
less than m documents are removed from consid-
eration2. Then, the remaining terms are clustered 
via average-link clustering, using their LSA-based 
vector representations and using cosine-correlation 
as a vector similarity measure. Our clustering algo-
rithm combines top-down clustering (Bisecting K-
Means) and bottom-up clustering (Agglomerative 
Clustering) (Steinbach et al, 2000). This hybrid 
                                                          
2 n and m are parameters set by the user. 
255
clustering approach leverages the speed of bisect-
ing K-means and the greedy search of agglomera-
tive clustering, thus achieving a nice effectiveness 
versus efficiency balance.   
Cluster centroids (InfoMagnets) and documents 
(or topic segments) are all treated as bag-of-words. 
Their vector-space representation is the sum of the 
LSA vectors of their constituent terms. When the 
user changes the topic-representation by removing 
or adding a term to an InfoMagnet, a new LSA 
vector is obtained by projecting the new bag-of-
words onto the LSA space and re-computing the 
cosine correlation between all documents and the 
new topic.     
4 An Example of Use 
InfoMagnets was designed for easy usability by 
both computational linguistics and non-technical 
users.  It has been successfully used by social psy-
chologists working on on-line communities re-
search as well as learning science researchers 
studying tutorial dialogue interactions (which we 
discuss in some detail here).   
Using InfoMagnets, a thermodynamics domain 
expert constructed a topic analysis of a corpus of 
human tutoring dialogues collected during class-
room study focusing on thermodynamics instruc-
tion (Ros? et al, 2005). Altogether each student?s 
protocol was divided into between 10 and 25 seg-
ments such that the entire corpus was divided into 
approximately 379 topic segments altogether.  Us-
ing InfoMagnets, the domain expert identified 15 
distinct topics such that each student covered be-
tween 4 and 11 of these topics either once or mul-
tiple times throughout their interaction. 
The topic analysis of the corpus gives us a way 
of quickly getting a sense of how tutors divided 
their instructional time between different topics of 
conversation.  Based on this topic analysis of the 
human-tutoring corpus, the domain expert de-
signed 12 dialogues, which were then implemented 
using a dialogue authoring environment called 
TuTalk (Gweon et al, 2005).  In a recent very suc-
cessful classroom evaluation, we observed the in-
structional effectiveness of these implemented 
tutorial dialogue agents, as measured by pre and 
post tests. 
 
Acknowledgments 
This work was funded by Office of Naval Re-
search, Cognitive and Neural Science Division, 
grant number N00014-05-1-0043. 
References  
Jaime Arguello, Brian S. Butler, Lisa Joyce, Robert 
Kraut, Kimberly S. Ling, Carolyn Rose and Xiaoqing 
Wang (2006). Talk to Me: Foundations of Successful 
Individual-Group Interactions in Online Communi-
ties. To appear in Proceedings of CHI: Human Fac-
tors in Computing. 
Matthew Chalmers and Paul Chitson (1992). Bead: Ex-
plorations in Information Visualization. In Proceed-
ings of ACM SIGIR,  330-337 
David Dubin (1995). Document Analysis for Visualiza-
tion. In Proceedings of ACM SIGIR, 199-204. 
Gahgene Gweon, Jaime Arguello, Carol Pai, Regan 
Carey, Zachary Zaiss, and Carolyn Ros? (2005).  
Towards a Prototyping Tool for Behavior Oriented 
Authoring of Conversational Interfaces, Proceedings 
of the ACL Workshop on Educational Applications of 
NLP. 
Rohit Kumar, Carolyn Ros?, Vincent Aleven, Ana Igle-
sias, Allen Robinson (submitted). Evaluating the Ef-
fectiveness of Tutorial Dialogue Instruction in an 
Exploratory Learning Context, Submitted to ITS ?06 
Thomas Landauer, Peter W. Foltz, and Darrell Laham 
(1998).  Introduction to Latent Semantic Analysis. 
Discourse Processes, 25, 259-284.  
Anton Leuski and James Allan (2002). Lighthouse: 
Showing the Way to Relevant Information. In Pro-
ceedings of the IEEE InfoVis  2000 
Matt Rasmussen and George Karypis (2004). gCLUTO: 
An Interactive Clustering, Visualization, and Analy-
sis System. Technical Report # 04-021 
Carolyn Ros?, Vincent Aleven, Regan Carey, Allen 
Robinson, and Chih Wu (2005). A First Evaluation 
of the Instructional Value of Negotiable Problem 
Solving Goals on the Exploratory Learning Contin-
uum, Proceedings of AI in Education ?05 
Michael Steinbach, George Karypis, and Vipin Kuma 
(2000). A comparison of document clustering tech-
niques. In KDD Workshop on Text Mining. 
James A. Wise, James J. Thomas, Kelly Pennock, David 
Lantrip, Marc Pottier, and Anne Schur (1995).  Visu-
alizing the Non-Visual: Spatial Analysis and Interac-
tion with Information from Text Documents. In 
Proceedings of IEEE InfoVis ?95, 51-58. 
256
Proceedings of NAACL HLT 2009: Short Papers, pages 37?40,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Identifying Types of Claims in Online Customer Reviews
Shilpa Arora, Mahesh Joshi and Carolyn P. Rose?
Language Technologies Institute
School of Computer Science
Carnegie Mellon University, Pittsburgh PA 15213
{shilpaa,maheshj,cprose}@cs.cmu.edu
Abstract
In this paper we present a novel approach to
categorizing comments in online reviews as
either a qualified claim or a bald claim. We ar-
gue that this distinction is important based on
a study of customer behavior in making pur-
chasing decisions using online reviews. We
present results of a supervised algorithm for
learning this distinction. The two types of
claims are expressed differently in language
and we show that syntactic features capture
this difference, yielding improvement over a
bag-of-words baseline.
1 Introduction
There has been tremendous recent interest in opin-
ion mining from online product reviews and it?s ef-
fect on customer purchasing behavior. In this work,
we present a novel alternative categorization of com-
ments in online reviews as either being qualified
claims or bald claims.
Comments in a review are claims that reviewers
make about the products they purchase. A customer
reads the reviews to help him/her make a purchas-
ing decision. However, comments are often open
to interpretation. For example, a simple comment
like this camera is small is open to interpretation
until qualified by more information about whether
it is small in general (for example, based on a poll
from a collection of people), or whether it is small
compared to some other object. We call such claims
bald claims. Customers hesitate to rely on such bald
claims unless they identify (from the context or oth-
erwise) themselves to be in a situation similar to the
customer who posted the comment. The other cate-
gory of claims that are not bald are qualified claims.
Qualified claims such as it is small enough to fit
easily in a coat pocket or purse are more precise
claims as they give the reader more details, and are
less open to interpretation. Our notion of qualified
claims is similar to that proposed in the argumenta-
tion literature by Toulmin (1958). This distinction
of qualified vs. bald claims can be used to filter
out bald claims that can?t be verified. For the quali-
fied claims, the qualifier can be used in personalizing
what is presented to the reader.
The main contributions of this work are: (i) an an-
notation scheme that distinguishes qualified claims
from bald claims in online reviews, and (ii) a super-
vised machine learning approach that uses syntactic
features to learn this distinction. In the remainder
of the paper, we first motivate our work based on
a customer behavior study. We then describe the
proposed annotation scheme, followed by our su-
pervised learning approach. We conclude the paper
with a discussion of our results.
2 Customer Behavior Study
In order to study how online product reviews are
used to make purchasing decisions, we conducted
a user study. The study involved 16 pair of gradu-
ate students. In each pair there was a customer and
an observer. The goal of the customer was to de-
cide which camera he/she would purchase using a
camera review blog1 to inform his/her decision. As
the customer read through the reviews, he/she was
1http://www.retrevo.com/s/camera
37
asked to think aloud and the observer recorded their
observations.
The website used for this study had two types of
reviews: expert and user reviews. There were mixed
opinions about which type of reviews people wanted
to read. About six customers could relate more with
user reviews as they felt expert reviews were more
like a ?sales pitch?. On the other hand, about five
people were interested in only expert reviews as they
believed them to be more practical and well rea-
soned.
From this study, it was clear that the customers
were sensitive to whether a claim was qualified or
not. About 50% of the customers were concerned
about the reliability of the comments and whether
it applied to them. Half of them felt it was hard
to comprehend whether the user criticizing a feature
was doing so out of personal bias or if it represented
a real concern applicable to everyone. The other half
liked to see comments backed up with facts or ex-
planations, to judge if the claim could be qualified.
Two customers expressed interest in comments from
users similar to themselves as they felt they could
base their decision on such comments more reli-
ably. Also, exaggerations in reviews were deemed
untrustworthy by at least three customers.
3 Annotation Scheme
We now present the guidelines we used to distin-
guish bald claims from qualified claims. A claim
is called qualified if its validity or scope is limited
by making the conditions of its applicability more
explicit. It could be either a fact or a statement that
is well-defined and attributed to some source. For
example, the following comments from our data are
qualified claims according to our definition,
1. The camera comes with a lexar 16mb starter
card, which stores about 10 images in fine mode
at the highest resolution.
2. I sent my camera to nikon for servicing, took
them a whole 6 weeks to diagnose the problem.
3. I find this to be a great feature.
The first example is a fact about the camera. The
second example is a report of an event. The third
example is a self-attributed opinion of the reviewer.
Bald claims on the other hand are non-factual
claims that are open to interpretation and thus cannot
be verified. A straightforward example of the dis-
tinction between a bald claim and a qualified claim
is a comment like the new flavor of peanut butter is
being well appreciated vs. from a survey conducted
among 20 people, 80% of the people liked the new
flavor of peanut butter. We now present some exam-
ples of bald claims. A more detailed explanation is
provided in the annotation manual2:
? Not quantifiable gradable3 words such as
good, better, best etc. usually make a claim
bald, as there is no qualified definition of being
good or better.
? Quantifiable gradable words such as small,
hot etc. make a claim bald when used without
any frame of reference. For example, a com-
ment this desk is small is a bald claim whereas
this desk is smaller than what I had earlier is a
qualified claim, since the comparative smaller
can be verified by observation or actual mea-
surement, but whether something is small in
general is open to interpretation.
? Unattributed opinion or belief: A comment
that implicitly expresses an opinion or belief
without qualifying it with an explicit attribu-
tion is a bald claim. For example, Expectation
is that camera automatically figures out when
to use the flash.
? Exaggerations: Exaggerations such as on ev-
ery visit, the food has blown us away do not
have a well defined scope and hence are not
well qualified.
The two categories for gradable words defined above
are similar to what Chen (2008) describes as vague-
ness, non-objective measurability and imprecision.
4 Related work
Initial work by Hu and Liu (2004) on the product
review data that we have used in this paper focuses
on the task of opinion mining. They propose an ap-
proach to summarize product reviews by identifying
opinionated statements about the features of a prod-
uct. In our annotation scheme however, we classify
2www.cs.cmu.edu/?shilpaa/datasets/
opinion-claims/qbclaims-manual-v1.0.pdf
3http://en.wikipedia.org/wiki/English_
grammar#Semantic_gradability
38
all claims in a review, not restricting to comments
with feature mentions alone.
Our task is related to opinion mining, but with a
specific focus on categorizing statements as either
bald claims that are open to interpretation and may
not apply to a wide customer base, versus qualified
claims that limit their scope by making some as-
sumptions explicit. Research in analyzing subjec-
tivity of text by Wiebe et al (2005) involves identi-
fying expression of private states that cannot be ob-
jectively verified (and are therefore open to interpre-
tation). However, our task differs from subjectivity
analysis, since both bald as well as qualified claims
can involve subjective language. Specifically, objec-
tive statements are always categorized as qualified
claims, but subjective statements can be either bald
or qualified claims. Work by Kim and Hovy (2006)
involves extracting pros and cons from customer re-
views and as in the case of our task, these pros and
cons can be either subjective or objective.
In supervised machine learning approaches to
opinion mining, the results using longer n-grams and
syntactic knowledge as features have been both pos-
itive as well as negative (Gamon, 2004; Dave et al,
2003). In our work, we show that the qualified vs.
bald claims distinction can benefit from using syn-
tactic features.
5 Data and Annotation Procedure
We applied our annotation scheme to the product re-
view dataset4 released by Hu and Liu (2004). We
annotated the data for 3 out of 5 products. Each
comment in the review is evaluated as being quali-
fied or bald claim. The data has been made available
for research purposes5.
The data was completely double coded such that
each review comment received a code from the two
annotators. For a total of 1, 252 review comments,
the Cohen?s kappa (Cohen, 1960) agreement was
0.465. On a separate dataset (365 review com-
ments)6, we evaluated our agreement after remov-
ing the borderline cases (only about 14%) and there
4http://www.cs.uic.edu/?liub/FBS/
CustomerReviewData.zip
5www.cs.cmu.edu/?shilpaa/datasets/
opinion-claims/qbclaims-v1.0.tar.gz
6These are also from the Hu and Liu (2004) dataset, but not
included in our dataset yet.
was a statistically significant improvement in kappa
to 0.532. Since the agreement was low, we resolved
our conflict by consensus coding on the data that was
used for supervised learning experiments.
6 Experiments and Results
For our supervised machine learning experiments on
automatic classification of comments as qualified or
bald, we used the Support Vector Machine classifier
in the MinorThird toolkit (Cohen, 2004) with the de-
fault linear kernel. We report average classification
accuracy and average Cohen?s Kappa using 10-fold
cross-validation.
6.1 Features
We experimented with several different features in-
cluding standard lexical features such as word uni-
grams and bigrams; pseudo-syntactic features such
as Part-of-Speech bigrams and syntactic features
such as dependency triples7. Finally, we also used
syntactic scope relationships computed using the de-
pendency triples. Use of features based on syntactic
scope is motivated by the difference in how quali-
fied and bald claims are expressed in language. We
expect these features to capture the presence or ab-
sence of qualifiers for a stated claim. For example,
?I didn?t like this camera, but I suspect it will be a
great camera for first timers.? is a qualified claim,
whereas a comment like ?It will be a great camera
for first timers.? is not a qualified claim. Analysis of
the syntactic parse of the two comments shows that
in the first comment the word ?great? is in the scope
of ?suspect?, whereas this is not the case for the sec-
ond comment. We believe such distinctions can be
helpful for our task.
We compute an approximation to the syntactic
scope using dependency parse relations. Given
the set of dependency relations of the form
relation, headWord, dependentWord, such as
AMOD,camera,great, an in-scope feature is de-
fined as INSCOPE headWord dependentWord (IN-
SCOPE camera great). We then compute a tran-
sitive closure of such in-scope features, similar to
Bikel and Castelli (2008). For each in-scope feature
in the entire training fold, we also create a corre-
7We use the Stanford Part-of-Speech tagger and parser re-
spectively.
39
Features QBCLAIM HL-OP
Majority .694(.000) .531(.000)
Unigrams .706(.310) .683(.359)
+Bigrams .709(.321) .693(.378)
+POS-Bigrams .726*(.353*) .683(.361)
+Dep-Triples .711(.337) .692(.376)
+In-scope .706(.340) .688(.367)
+Not-in-scope .726(.360*) .687(.370)
+All-scope .721(.348) .699(.396)
Table 1: The table shows accuracy (& Cohen?s kappa in paren-
theses) averaged across ten folds. Each feature set is individ-
ually added to the baseline set of unigram features. * - Re-
sult is marginally significantly better than unigrams-only (p <
0.10, using a two-sided pairwise T-test). HL-OP - Opinion an-
notations from Hu and Liu (2004). QBCLAIM - Qualified/Bald
Claim.
sponding not-in-scope feature which triggers when
either (i) the dependent word appears in a comment,
but not in the transitive-closured scope of the head
word, or (ii) the head word is not contained in the
comment but the dependent word is present.
We evaluate the benefit of each type of feature
by adding them individually to the baseline set of
unigram features. Table 1 presents the results. We
use the majority classifier and unigrams-only perfor-
mance as our baselines. We also experimented with
using the same feature combinations to learn the
opinion category as defined by Hu and Liu (2004)
[HL-OP] on the same subset of data.
It can be seen from Table 1 that using purely
unigram features, the accuracy obtained is not
any better than the majority classifier for quali-
fied vs. bald distinction. However, the Part-of-
Speech bigram features and the not-in-scope fea-
tures achieve a marginally significant improvement
over the unigrams-only baseline.
For the opinion dimension from Hu and Liu
(2004), there was no significant improvement from
the type of syntactic features we experimented with.
Hu and Liu (2004)?s opinion category covers several
different types of opinions and hence finer linguis-
tic distinctions that help in distinguishing qualified
claims from bald claims may not apply in that case.
7 Conclusions
In this work, we presented a novel approach to re-
view mining by treating comments in reviews as
claims that are either qualified or bald. We argued
with examples and results from a user study as to
why this distinction is important. We also proposed
and motivated the use of syntactic scope as an ad-
ditional type of syntactic feature, apart from those
already used in opinion mining literature. Our eval-
uation demonstrates a marginally significant posi-
tive effect of a feature space that includes these and
other syntactic features over the purely unigram-
based feature space.
Acknowledgments
We would like to thank Dr. Eric Nyberg for the
helpful discussions and the user interface for doing
the annotations. We would also like to thank all the
anonymous reviewers for their helpful comments.
References
Daniel Bikel and Vittorio Castelli. Event Matching Using
the Transitive Closure of Dependency Relations. Pro-
ceedings of ACL-08: HLT, Short Papers, pp. 145?148.
Wei Chen. 2008. Dimensions of Subjectivity in Natural
Language. In Proceedings of ACL-HLT?08. Colum-
bus Ohio.
Jacob Cohen. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Measure-
ment, Vol. 20, No. 1., pp. 37-46.
William Cohen. 2004. Minorthird: Methods for Iden-
tifying Names and Ontological Relations in Text us-
ing Heuristics for Inducing Regularities from Data.
http://minorthird.sourceforge.net/
Kushal Dave, Steve Lawrence and David M. Pennock
2006. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
In Proc of WWW?03.
Michael Gamon. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and
the role of linguistic analysis. Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
Minqing Hu and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proc. of the ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining.
Soo-Min Kim and Eduard Hovy. 2006. Automatic Iden-
tification of Pro and Con Reasons in Online Reviews.
In Proc. of the COLING/ACL Main Conference Poster
Sessions.
Stephen Toulmin 1958 The Uses of Argument. Cam-
bridge University Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, vol-
ume 39, issue 2-3, pp. 165-210.
40
Proceedings of NAACL HLT 2009: Short Papers, pages 145?148,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluating the Syntactic Transformations in Gold Standard Corpora  for Statistical Sentence Compression   Naman K. Gupta, Sourish Chaudhuri, Carolyn P. Ros? Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {nkgupta,sourishc,cprose}@cs.cmu.edu   Abstract 
We present a policy-based error analysis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression.  We demon-strate that these limitations arise from the strong assumption of locality of the deci-sion making process in the search for an acceptable derivation in this paradigm. 1 Introduction In this paper we present a policy-based error analy-sis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression (Knight and Marcu, 2000; Turner and Charniak, 2005; McDonald, 2006; Clark and La-pata 2006).       Specifically, in typical statistical compression approaches, a simplifying assumption is made that compression is accomplished strictly by means of word deletion. Furthermore, each sequence of con-tiguous words that are dropped from a source sen-tence is considered independently of other sequences of words dropped from other portions of the sentence, so that the features that predict whether deleting a sequence of words is preferred or not is based solely on local considerations.  This simplistic approach allows all possible derivations to be modeled and decoded efficiently within the search space, using a dynamic programming algo-rithm.       In theory, it should be possible to learn how to generate effective compressions using a corpus of source-target sentence pairs, given enough exam-ples and sufficiently expressive features.  How-ever, our analysis casts doubt that this framework 
with its strong assumptions of locality is suffi-ciently powerful to learn the types of example compressions frequently found in corpora of hu-man generated gold standard compressions regard-less of how expressive the features are.     Work in sentence compression has been some-what hampered by the tremendous cost involved in producing a gold standard corpus.  Because of this tremendous cost, the same gold standard corpora are used in many different published studies almost as a black box.  This is done with little scrutiny of the limitations on the learnability of the desired target systems. These limitations result from in-consistencies due to the subtleties in the process by which humans generate the gold standard compres-sions from the source sentences, and from the strong locality assumptions inherent in the frame-works.    Typically, the humans who have participated in the construction of these corpora have been in-structed to preserve grammaticality and to produce compressions by deletion.  Human ratings of the gold standard compressions by separate judges confirm that the human developers have literally followed the instructions, and have produced com-pressions that are themselves largely grammatical.  Nevertheless, what we demonstrate with our error analysis is that they have used meaning preserving transformation that didn't consistently preserve the grammatical relations from the source sentence while transforming source sentences into target sentences.  This places limitations on how well the preferred patterns of compression can be learned using the current paradigm and existing corpora.     In the remainder of the paper, we discuss rele-vant work in sentence compression.  We then in-troduce our policy-based error analysis technique.  Next we discuss the error analysis itself and the conclusions we draw from it.  Finally, we conclude 
145
with future directions for broader application of this error analysis technique. 2 Related Work  Knight and Marcu (2000) present two approaches to the sentence compression problem- one using a noisy channel model and the other using a deci-sion-based model. Subsequent work (McDonald, 2006) has demonstrated an advantage for a soft constraint approach, where a discriminative model learns to make local decisions about dropping a sequence of words from the source sentence in or-der to produce the target compression.  Features in this system are defined over pairs of words in the source sentence, with the idea that the pair of words would appear adjacent in the resulting com-pression, with all intervening words dropped.  Thus, the features represent this transformation, and the feature weights are meant to indicate whether the transformation is associated with good compressions or not.      We use McDonald?s (2006) proposed model as a foundation for our work because its soft constraint approach allows for natural integration of a variety of classes of features, even overlapping features.  In our prior work we have explored the potential for improving the performance of a compression system by including additional, more sophisticated syntactically motivated features than those in-cluded in previously published models.  In this pa-per, we evaluate the gold standard corpus itself using similar syntactic grammar policies. 3 Grammar Policy Extraction In the domain of Sentence Compression, the cor-pus consists of source sentences each paired with a gold standard compressed sentence. Most of the above related work has been evaluated using the following 2 corpora, namely the Ziff-Davis (ZD) set (Knight and Marcu, 2002) consisting of 1055 sentences, and a partial Broadcast News Corpus (CL Corpus) (Clarke and Lapata, 2006) originally consisting of 1619 sentences, of which we used 1070 as the training set in our development work as well as in the error analysis below. Hence, we use these two popular corpora to present our work. We hypothesize certain grammar policies that in-tuitively should be followed while deriving the target-compressed sentence from the source sen-
tence if the mapping between source and target sentences is produced via grammatical transforma-tions. The basic idea behind these policies grows out of the same ideas motivating the syntactic fea-tures used in McDonald (2006). These policies, extracted using the MST (McDonald, 2005) de-pendency parse structure of the source sentence, are as follows:  1. The syntactic root word of a sentence should be retained in the compressed sen-tence. 2.  If a verb is retained in the compressed sentence, then the dependent subject of that verb should also be retained. 3. If a verb is retained in the compressed sen-tence, then the dependent object of that verb should also be retained. 4. If the verb is dropped in the compressed sentence then its arguments, namely sub-ject, object, prepositional phrases etc., should also be dropped. 5. If the Preposition in a Prepositional phrase (PP) is retained in the compressed sen-tence, then the dependent Noun Phrase (NP) of that Preposition should also be re-tained. 6. If the head noun of a Noun phrase (NP) within a Prepositional phrase is retained in the compressed sentence, then the syntac-tic parent Preposition of the NP should also be retained. 7. If a Preposition, the syntactic head of a Prepositional phrase (PP), is dropped in the compressed sentence, then the whole PP, including dependent Noun phrase in that PP, should also be dropped. 8. If the head noun of a Noun phrase within a Prepositional phrase (PP) is dropped in the compressed sentence, then the syntactic parent Preposition of the PP should also be dropped.  These grammar policies make predictions about where, in the phrase structure, constituents are likely to be dropped or retained in the compres-sion.  Thus, these policies have similar motivation to the syntactic features in the McDonald (2006) model. However, there is a fundamental difference in the way these policies are computed. In the McDonald (2006) model, the features are com-
146
puted locally over adjacent words yi-1 & yi in the compression and the words dropped from the original sentence between that word range yi-1 & yi. In cases where the syntactic structure of the in-volved words extends beyond this range, the ex-tracted features are not able to capture all of the relevant syntactic dependencies. On the other hand, in our analysis the policies are computed globally over the complete sentence without specifying any range of words. As an illustrative example, let us consider the following sentence from the CL Cor-pus (bold represents dropped words):  1. The1 leaflet2 given3 to4 Labour5 activists6 mentions7 none8 of9 these10 things11.  According to Policy 2, since the verb 'mentions' is retained, the subject of the verb ?the leaflet? should also be retained. In the McDonald (2006) model, by looking at the local range yi-1 = 5 and yi = 7 for the verb 'mentions', we will not be able to compute whether the subject(1,2) was retained in the compression or not. So this policy can be cap-tured only if the global context is taken into ac-count while evaluating the verb 'mentions'. Now we evaluate each sentence in the corpus to determine whether a particular policy was applica-ble and if applicable then whether it was violated. Table 1 shows the summary of the evaluation of all the sentences in the two corpora. Column 2 in the table shows the percentage of sentences in the ZD Corpus where the respective policies were applica-ble. And column 3 shows the percentage of sen-tences where the respective policies were violated, whenever applicable. Columns 4 and 5 show re-spective percentages for the CL corpus. 4 Evaluation In this section we discuss the results from evaluat-ing the 8 grammar policies discussed in Section 3 over the ZD and CL corpora, as discussed above.   The policies were evaluated with respect to whether they applied in a sentence, i.e., whether the premise of the ?if ? then? rule is true in the sentence, and whether the policy was broken when applied, i.e., if the premise is true but the conse-quent is false.  The striking finding is that for every one of the policies discussed in the previous sec-tion, they are violated for at least 10% of the sen-tences where they applied, and sometimes as much as 72%.  For most policies, the proportion of sen-tences where the policy is violated when applied is 
a minority of cases.  Thus, based on this, we can expect that grammar oriented features motivated by these policies and derived from a syntactic analysis of the source and/or target sentences in the gold standard could be used to improve the per-formance of compression systems that don?t make use of syntactic information to that extent.  How-ever, the noticeable proportion of violations with respect to some of the policies indicate that there is a limited extent to which these types of features can contribute towards improved performance. One observation we make from Table 1 is that while the proportion of sentences where the poli-cies (Columns 2 and 4) apply as well as the propor-tion of sentences where the policies are broken when applied (Columns 3 and 5) are highly corre-lated between the two corpora.  Nevertheless, the distributions are not identical. Thus, again, while we predict that using this style of dependency syn-tax features might improve performance of com-pression systems within a single corpus, we would not expect trained models that rely on these syntac-tic dependency features to generalize in an ideal way between corpora.   ZD (%  Appli-cable) ZD (% Viola-tions when Appli-cable) 
CL (%  Appli-cable) CL (%  Viola-tions when Appli-cable) Policy1 100% 34% 100% 14% Policy2 66% 18% 84% 18% Policy3 50% 10% 61% 24% Policy4 59% 59% 46% 72% Policy5 62% 17% 77% 27% Policy6 65% 22% 79% 29% Policy7 57% 25% 58% 40% Policy8 55% 16% 58% 36% Table 1: Summary of evaluation of grammar policies over the Ziff-Davis (ZD) training set and Clark-Lapata (CL) training set.  Beyond the above evaluation illustrating the extent to which grammar inspired policies are violated in human generated gold standard corpora, interesting insights into challenges that must be addressed in order to improve performance can be obtained by taking a close look at typical examples from the CL corpus where the policies are broken in the 
147
gold standard corpora (bold represents dropped words).  1. The attempt to put flesh and blood on the skeleton structure of a possible united Europe emerged. 2. Annely has used the gallery ?s three floors to divide the exhibits into three dis-tinct groups. 3. Labor has said it will scrap the system. 4. Montenegro ?s sudden rehabilitation of Nicholas ?s memory is a popular move.  In Sentence 1, retaining the dependent Noun struc-ture of the dropped Preposition on in the PP vio-lates Policy 7. Such a NP to Infinitive Phrase transformation changes the syntactic structure of the sentence. Sentence 2 also breaks several poli-cies, namely Policies 1, 4 and 7. The syntactic root has is dropped. Also the main verb has used is dropped while retaining the Subject Annely. In Sentence 3, breaking Policies 1, 2 and 4, the hu-man annotators replaced the pronoun it with the noun Labor, the subject of a dropped verb ?has said?. Such anaphora resolution cannot be done without relevant context, which is not available in strictly local paradigms of sentence compression. In Sentence 4, policies 3. 5 and 8 are violated. Transformations like substituting Nicholas?s mem-ory by the metonym Nicholas and popular move by popular need to be identified and analyzed. Such varied transformations, made in the syntactic struc-ture of the sentences by human annotators, are counter-intuitive, making them hard to be captured in the linear models learned in association with the syntactic features in current compression systems. 5 Conclusions and Current Directions In this paper we have introduced a policy-based error analysis technique that was used to investi-gate the potential impact and limitations of adding a particular style of dependency parse features to typical statistical compression systems.  We have argued that the reason for the limitation arises from the strong assumption of the local nature of the decisions that are made in obtaining the system-generated compression from a source sentence.       Other related technologies such as statistical machine translation and statistical paraphrase are based on similar paradigms with similar assump-
tions of the local nature of decisions that are made in the search for an acceptable derivation.  We con-jecture both that it is likely that the same issues related to the construction of the gold standard corpora likely apply and that a similar policy-based error analysis approach could be used in order to assess the extent to which this is true and identify possible directions for improving performance.  In our ongoing work, we plan to conduct a similar error analysis for these problems in order to evalu-ate the generality of the findings reported here.   Acknowledgments This work was funded in part by the Office of Na-val Research grant number N00014510043. References  James Clarke and Mirella Lapata. 2006. Constraint-Based Sentence Compression: An Integer Program-ming Approach. Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions (ACL-2006), pages 144-151, 2006. James Clarke and Mirella Lapata. 2006. Models for Sen-tence Compression: A Comparison across Domains, Training Requirements and Evaluation Measures.  Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 377-384. Sydney, Australia. Kevin Knight and Daniel Marcu. 2000. Statistics-Based Summarization ? Step One: Sentence Compression. Proceedings of AAAI-2000, Austin, TX, USA. Knight, Kevin and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence 139(1):91?107. Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of de-pendency parsers. Proc. ACL. Ryan Mcdonald, 2006. Discriminative sentence com-pression with soft syntactic constraints. Proceedings of the 11th EACL. Trento, Italy, pages 297--304. Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. Proc. ACL. 
148
Proceedings of NAACL HLT 2009: Demonstrations, pages 5?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building Conversational Agents with Basilica 
Rohit Kumar Carolyn P. Ros? 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
rohitk@cs.cmu.edu cprose@cs.cmu.edu 
 
 
Abstract 
Basilica is an event-driven software architec-
ture for creating conversational agents as a 
collection of reusable components. Software 
engineers and computer scientists can use this 
general architecture to create increasingly so-
phisticated conversational agents. We have 
developed agents based on Basilica that have 
been used in various application scenarios and 
foresee that agents build on Basilica can cater 
to a wider variety of interactive situations as 
we continue to add functionality to our archi-
tecture. 
1 Introduction 
Conversational Interfaces apply the metaphor of 
agent to an interface which allows the user to con-
versationally interact with the machine using natu-
ral language through speech or text. The current 
state of the art in the area of conversational inter-
faces is largely dominated by spoken dialog sys-
tems (SDS).  These SDS are most often used for 
the purpose of accessing information from a data-
base over the telephone. Other common applica-
tions of conversational agents include computer 
aided instruction (CAI) and human-robot interac-
tion (HRI). 
Conversational Agents in most of today?s SDS, 
CAI and HRI are designed to work within the 
scope of specific task domains which allows the 
scientists and engineers working on such systems 
to ensure satisfactory and relevant interaction with 
the user most of the time. Within the task domain, 
such agents can display intelligent interactive be-
havior like helping the user use the interface, ask-
ing remedial questions (Bohus and Rudnicky, 
2005), shaping the user behavior (Tomko and Ro-
senfeld, 2004) by using alternative phrasing of ut-
terances, responding to user affect (D?Mello et al, 
2008) through text, voice and gesture, engaging the 
user through the display of presence via backchan-
nels (Ward, 1996) and embodiment (Cassell et al, 
1999). 
As more and more of these intelligent interac-
tive agents get built for many task domains (Raux 
et al, 2005; Bohus et al, 2007; Gockley et al, 
2005; Amtrak Julie; ?) that surround our every-
day life, we observe a gradual transition in the use 
of the conversational agent technology to be a form 
of situated interaction. One of the characteristic 
requirements of this transition towards ubiquity of 
such interactive agents is the capability to sense 
and trigger behavior in a context sensitive way. 
In most conversational interfaces today, the on-
ly trigger used by the agents is that of initiation of 
conversation usually by sensing user presence 
through a telephone call, proximity detection or 
user login into a virtual environment. The initiation 
event is followed by a scripted task-oriented con-
versation with the agent. These scripts could be 
fairly complex depending on the representational 
formalism underlying the script. Most of the com-
mon software architectures/platforms used to 
create conversational agents like TellMe Studio, 
Voxeo Prophecy, Olympus (Bohus et al, 2007), 
DIPPER (Bos and Oka, 2003), etc. use one or more 
of these presence sensing techniques and one of the 
many existing scripting languages including 
VoiceXML, SALT, TuTalk (Jordan et al, 2007) 
and Ravenclaw (Bohus and Rudnicky, 2003) task 
specification language among others.  
However, in our recent work on building con-
versational agents situated in collaborative learning 
5
environments, we have discovered the need for a 
software architecture for creating agents that pers-
ist in an interactive environment in which human 
users interact with these agents as well as with 
each other. In this situation, the agents need to be 
able to sense many kinds of triggers at many points 
of time and choose to respond to some of those 
triggers through a variety of modalities including 
conversation. This observation was the motivation 
for creating Basilica which is our architecture for 
building conversational agents. In section 2, we 
talk more about the intricacies of Basilica and 
agents built on this architecture. Section 3 de-
scribes some of application scenarios in which we 
are using Conversational Agents based on Basilica. 
2 Basilica Architecture 
In order to meet the need for an architecture that 
enables development of Conversational Agents as 
a collection of behavioral components that can 
sense triggers and respond to those appropriately, 
we created the Basilica architecture. 
In this architecture, we model sensing and res-
ponding as two types of components that make up 
conversational agents. The sensing components 
referred to as Filters observe stimuli from various 
kinds of input sources and other components. They 
can also generate stimuli for other components. On 
the other hand, Actor components generate respon-
sive behavior that may be observed the user(s) and 
other components. Basilica provides the software 
elements required to tie Filters and Actors together 
through Connections that carry Events over them. 
We think that many of the state of the art intelli-
gent behaviors listed in section 1 can be imple-
mented as dyads of filter and actor components. 
The minimal set of behavioral component 
classes listed above can easily be extended. For 
example, certain agent designs may need memory 
components and coordination components which 
bridge across multiple actors or filters that do not 
necessarily share events with each others. Timer 
components may be used to generate regulated 
stimuli. Besides belonging to one of these classes 
of components, certain components may act as 
wrappers to external systems. For example, we use 
wrapper components to integrate TuTalk dialog 
management system (Jordan et al, 2007) for some 
of the instructive behavior exhibited by our agents. 
Also, certain components act as wrappers to the 
environment in which the agent is present. These 
wrappers help in easily integrating the same agent 
with multiple environments without having to 
change any underlying components except the 
wrappers to the environment.  
We believe that fairly intelligent conversational 
agents can be built for situated interaction applica-
tions by incrementally building a large number of 
behavioral components. Each of these components 
represent a decomposition of the agent?s perceptive 
and cognitive capabilities. Among the agents we 
have built using Basilica, we observe that some of 
these capabilities are common across agents. 
Hence the corresponding behavioral components 
get re-used in many cases. Some instances of com-
ponent re-use are mentioned in Section 3. 
Note that recently there has been other work on 
modeling conversational agents as a decomposition 
of components. Jaspis (Turunen and Hakulinen, 
2003) models the agent as a collection of manag-
ers, agents and evaluators which synchronize with 
each other through transactions. RIME (Nakano et 
al., 2008) distributes cognitive capabilities across a 
collection of experts of two types. However, eva-
luators and agents are configured as a pile of com-
ponents whereas our filters and actors are 
configured as a network. Hence, designing conver-
sational agents with Basilica gives the flexibility to 
change the network topology. Also, while Jaspis 
agents are stateless, actors in our architecture need 
not be stateless. In other work on event-based mul-
ti-layered architectures (Raux and Eskenazi, 2007), 
events are used for communication between layers 
as a mean to provide higher reactive compared to 
pipeline architectures. While we share this motiva-
tion, definition of events is extended here as events 
are used for all kinds of communication, coordina-
tion and control in Basilica. 
3 Current Application Scenarios 
In 2008, we built three conversational agents to 
support learners in collaborative learning environ-
ments. Also, we are currently using Basilica to de-
velop a cross-lingual assistive agent to support 
non-Spanish speaking 911 dispatchers in the 
southern states of the US. In this section, we will 
discuss these four conversational agents briefly. 
CycleTalk is an intelligent tutoring system that 
helps college sophomores studying Thermodynam-
ics learn about principles of designing Steam 
6
cycles. In our recent experiments, we have studied 
the effectiveness of conversational agents in this 
intelligent tutoring system (Kumar et al, 2007; 
Chaudhuri et al, 2008). Student use the system 
both individually and in pairs. The conversational 
agent monitors student interaction in a chat room 
as the students work on solving a design problem. 
The tutor provides the students with hints to help 
touch upon all the underlying concepts while the 
students work on the design exercise. Also the 
agent brings up reflective dialogs when it detects a 
relevant topic in the students conversation. One of 
the problems we observed over the years with the 
use of instructional dialogs in collaborative envi-
ronments is that the students tend to ignore the tu-
toring agent if it interrupts the students when they 
are talking to each other. Basilica helped us in re-
solving this problem by implementing a compo-
nent that tells that student that help is available on 
the topic they are talking about and they can ask 
for the dialog support when they are ready. Basili-
ca gives the flexibility to change the intervention 
strategy used by the agent when it is speaking with 
more than one student. 
In another version of this system, the tutoring 
agent prompted the students with some motiva-
tional prompts occasionally as we observed that 
many of the students found the design exercise 
very demanding to complete in the time permitted 
for this lab exercise. We found that the use of mo-
tivational prompts improved the student?s attitude 
towards the automated agent. 
We developed another agent to help college 
level mathematics students working on problem 
solving. This agent operates in a collaborative en-
vironment which includes a whiteboard. As in the 
case with the CycleTalk agent, the agent used here 
also helps the students with hints and dialogs. The 
component required for those behaviors were re-
used as-is with modifications only their configura-
tion files. Besides these behaviors, the agent coor-
dinates the problem solving sessions for the team 
by presenting the team with problems as images 
placed on the whiteboard and helping the students 
stay on track by answering questions about the 
amount of time left in the problem solving session. 
Recently, we modified the environment wrap-
per components of our CycleTalk agent and inte-
grated them with a SecondLife application 
(Weusijana et al, 2008). This integration helps 
developers of conversational agents create interac-
tive agents in the SecondLife virtual environment. 
Finally, in a currently ongoing project, we are 
building an agent that would interpret Spanish ut-
terances from a distressed 9-1-1 caller and work 
with a human dispatcher who does not know Span-
ish to attend to the call. We model the agent in this 
scenario after a human translator who does not just 
translate the caller?s input to English and vice ver-
sa. Instead the translator partners with the dis-
patcher to provide service to the caller. Partnering 
conversational agents with a human user to help 
another human user in a different role is a novel 
application of interactive agents. 
4 Building Agents using Basilica 
 
Figure 1. Components of the CycleTalk Agent 
 
Building conversational agents using Basilica in-
volves the process of representing the desired 
agent as a decomposition of components. Figure 1 
above shows the components that make up the 
CycleTalk conversational agent we mentioned in 
Section 3. The rectangles represent Filters and the 
parallelograms represent Actors. Connections are 
shown as solid lines. In a detailed design, these 
lines are annotated with the events they carry. 
Once an agent is designed, the agents and filters 
required for the implementation of the agent can be 
either re-used from the pre-existing components of 
Basilica or implemented as Java objects that ex-
tend the corresponding component class. Often the 
programming task is limited to implementing han-
dlers and generators for the events received and 
sent out by the component. Theoretically, the va-
lidity of a component can be verified if it can han-
dle and generate all the events as specified in the 
design diagram. 
As we continue to develop more conversational 
agents on this architecture, we intend to create de-
velopment tools which would easily translate a 
7
design like Figure 1 to the implementation and fa-
cilitate validation and debugging of the agent. 
5 Demonstration Outline 
The demonstration of our architecture will give the 
audience an opportunity to interact with the agents 
we have described in section 3 and discuss how we 
can design such agents using Basilica. We will 
have a poster to aid the discussion along with abili-
ty to probe into the code underlying the design of 
these agents. Attendees will be able to understand 
the process involved in building agents with Basi-
lica and assess the effort required. Additionally, if 
we have any specialized development tools to au-
tomatically map agent design as described in Sec-
tion 4 to Java code, we will demonstrate those 
tools. Up to date information about Basilica can be 
found at http://basilica.rohitkumar.net/wiki/ 
Acknowledgements 
 
This work is supported by NSF REESE/REC grant 
number 0723580. 
References 
 
Dan Bohus and Alex Rudnicky, 2005. Error Handling 
in the RavenClaw dialog management architecture, 
HLT-EMNLP-2005, Vancouver 
Stefanie Tomko and Roni Rosenfeld, 2004. Shaping 
Spoken Input in User-Initiative Systems. Interspeech 
2004, Jeju, Korea 
Antoine Raux, Brian Langner, Dan Bohus, Alan Black, 
and Maxine Eskenazi, 2005. Let's Go Public! Taking 
a Spoken Dialog System to the Real World, Inters-
peech 2005, Lisbon, Portugal 
Dan Bohus, Sergio Grau, David Huggins-Daines, Ven-
katesh Keri, Gopala Krishna A., Rohit Kumar, An-
toine Raux, and Stefanie Tomko, 2007.  Conquest - 
an Open-Source Dialog System for Conferences, 
HLT-NAACL 2007, Rochester, NY 
Amtrack Julie, http://www.networkworld.com/news/ 
2003/0619julie.html 
Justin Cassell, Timothy Bickmore, Billinghurst, M., 
Campbell, L., Chang, K., Vilhj?lmsson, H. and Yan, 
H., 1999. Embodiment in Conversational Interfaces: 
Rea, CHI'99, Pittsburgh, PA 
Nigel Ward, 1996. Using Prosodic Clues to decide 
when to produce Back-channel Utterances, ICSLP 96 
Sidney D' Mello, Tanner Jackson, Scotty Craig, Brent 
Morgan, Patrick Chipman, Holly White, Natalie Per-
son, Barry Kort, Rana el Kaliouby, Rosalid W. Pi-
card and Arthur Graesser, 2008, AutoTutor Detects 
and Responds to Learners Affective and Cognitive 
States, Workshop on Emotional and Cognitive Is-
sues, ITS 2008, Montreal 
Rachel Gockley, Allison Bruce, Jodi Forlizzi, Marek 
Michalowski, Anne Mundell, Stephanie Rosenthal, 
Brennan Sellner, Reid Simmons, Kevin Snipes, Alan 
C. Schultz and Jue Wang, 2005. Designing Robots 
for Long-Term Social Interaction, IROS 2005 
Dan Bohus, Antoine Raux, Thomas Harris, Maxine 
Eskenazi and Alex Rudnicky, 2007. Olympus: an 
open-source framework for conversational spoken 
language interface research HLT-NAACL 2007 
Workshop on Bridging the Gap: Academic and In-
dustrial Research in Dialog Technology, Rochester, 
NY  
Johan Bos and Tetsushi Oka, 2003. Building Spoken 
Dialogue Systems for Believable Characters, 7th 
workshop on the semantics & pragmatics of dialogue 
TellMe, https://studio.tellme.com/ 
Voxeo Prophecy, http://www.voxeo.com/products/ 
Pamela Jordan, Brian Hall, Michael Ringenberg, Yue 
Cui, Carolyn P. Ros?, 2007.  Tools for Authoring a 
Dialogue Agent that Participates in Learning Stu-
dies, AIED 2007 
Dan Bohus and Alex Rudnicky, 2003. RavenClaw: Di-
alog Management Using Hierarchical Task Decom-
position and an Expectation Agenda, Eurospeech 
2003, Geneva, Switzerland 
Markku Turunen, Jaakko Hakulinen, 2003. Jaspis - An 
Architecture for Supporting Distributed Spoken Di-
alogues, Eurospeech? 2003, Geneva, Switzerland 
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, Hi-
roshi Tsujino, 2008. A Framework for Building Con-
versational Agents Based on a Multi-Expert Model, 
9th SigDial Workshop on Discourse and Dialog, Co-
lumbus, Ohio 
Antoine Raux and Maxine Eskenazi, 2007. A Multi-
Layer Architecture for Semi-Synchronous Event-
Driven Dialogue Management, ASRU 2007, Kyoto 
Rohit Kumar, Carolyn Rose, Mahesh Joshi, Yi-Chia 
Wang, Yue Cui, Allen Robinson, Tutorial Dialogue 
as Adaptive Collaborative Learning Support, 13th 
AIED 2007, Los Angeles, California 
Sourish Chaudhuri, Rohit Kumar, Carolyn P. Rose, 
2008. It?s not easy being green - Supporting Colla-
borative Green Design Learning, ITS 2008, Montreal 
Baba Kofi A. Weusijana, Rohit Kumar, Carolyn P. 
Rose, 2008. MultiTalker: Building Conversational 
Agents in Second Life using Basilica, Second Life 
Education Community Convention, Purple Strand: 
Educational Tools and Products, 2008, Tampa, FL  
8
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 73?76,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Feature Based Approach to Leveraging Context for Classifying 
Newsgroup Style Discussion Segments 
Yi-Chia Wang, Mahesh Joshi 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{yichiaw,maheshj}@cs.cmu.edu 
Carolyn Penstein Ros? 
Language Technologies Institute/  
Human-Computer Interaction Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
cprose@cs.cmu.edu 
 
 
Abstract 
On a multi-dimensional text categorization 
task, we compare the effectiveness of a fea-
ture based approach with the use of a state-
of-the-art sequential learning technique that 
has proven successful for tasks such as 
?email act classification?.  Our evaluation 
demonstrates for the three separate dimen-
sions of a well established annotation 
scheme that novel thread based features 
have a greater and more consistent impact 
on classification performance.  
1 Introduction 
The problem of information overload in personal 
communication media such as email, instant mes-
saging, and on-line discussion boards is a well 
documented phenomenon (Bellotti, 2005).  Be-
cause of this, conversation summarization is an 
area with a great potential impact (Zechner, 2001). 
What is strikingly different about this form of 
summarization from summarization of expository 
text is that the summary may include more than 
just the content, such as the style and structure of 
the conversation (Roman et al, 2006).  In this pa-
per we focus on a classification task that will even-
tually be used to enable this form of conversation 
summarization by providing indicators of the qual-
ity of group functioning and argumentation. 
Lacson and colleagues (2006) describe a form of 
conversation summarization where a classification 
approach is first applied to segments of a conversa-
tion in order to identify regions of the conversation 
related to different types of information.  This aids 
in structuring a useful summary.  In this paper, we 
describe work in progress towards a different form 
of conversation summarization that similarly lev-
erages a text classification approach.  We focus on 
newsgroup style interactions.  The goal of assess-
ing the quality of interactions in that context is to 
enable the quality and nature of discussions that 
occur within an on-line discussion board to be 
communicated in a summary to a potential new-
comer or group moderators.   
We propose to adopt an approach developed in 
the computer supported collaborative learning 
(CSCL) community for measuring the quality of 
interactions in a threaded, online discussion forum 
using a multi-dimensional annotation scheme 
(Weinberger & Fischer, 2006).  Using this annota-
tion scheme, messages are segmented into idea 
units and then coded with several independent di-
mensions, three of which are relevant for our work, 
namely micro-argumentation, macro-
argumentation, and social modes of co-
construction, which categorizes spans of text as 
belonging to one of five consensus building cate-
gories.  By coding segments with this annotation 
scheme, it is possible to measure the extent to 
which group members? arguments are well formed 
or the extent to which they are engaging in func-
tional or dysfunctional consensus building behav-
ior. 
This work can be seen as analogous to work on 
?email act classification? (Carvalho & Cohen, 
2005).  However, while in some ways the structure 
of newsgroup style interaction is more straightfor-
ward than email based interaction because of the 
unambiguous thread structure (Carvalho & Cohen, 
2005), what makes this particularly challenging 
73
from a technical standpoint is that the structure of 
this type of conversation is multi-leveled, as we 
describe in greater depth below.   
We investigate the use of state-of-the-art se-
quential learning techniques that have proven suc-
cessful for email act classification in comparison 
with a feature based approach.  Our evaluation 
demonstrates for the three separate dimensions of a 
context oriented annotation scheme that novel 
thread based features have a greater and more con-
sistent impact on classification performance.  
2 Data and Coding 
We make use of an available annotated corpus of 
discussion data where groups of three students dis-
cuss case studies in an on-line, newsgroup style 
discussion environment (Weinberger & Fischer, 
2006).  This corpus is structurally more complex 
than the data sets used previously to demonstrate 
the advantages of using sequential learning tech-
niques for identifying email acts (Carvalho & 
Cohen, 2005).  In the email act corpus, each mes-
sage as a whole is assigned one or more codes.  
Thus, the history of a span of text is defined in 
terms of the thread structure of an email conversa-
tion. However, in the Weinberger and Fischer cor-
pus, each message is segmented into idea units.  
Thus, a span of text has a context within a message, 
defined by the sequence of text spans within that 
message, as well as a context from the larger 
thread structure.  
The Weinberger and Fischer annotation scheme 
has seven dimensions, three of which are relevant 
for our work.  
1. Micro-level of argumentation [4 categories] 
How an individual argument consists of a 
claim which can be supported by a ground 
with warrant and/or specified by a qualifier  
2. Macro-level of argumentation [6 categories] 
Argumentation sequences are examined in 
terms of how learners connect individual ar-
guments to create a more complex argument 
(for example, consisting of an argument, a 
counter-argument, and integration)  
3. Social Modes of Co-Construction [6 catego-
ries] To what degree or in what ways learn-
ers refer to the contributions of their learn-
ing partners, including externalizations, 
elicitations, quick consensus building, inte-
gration oriented consensus building, or con-
flict oriented consensus building, or other. 
For the two argumentation dimensions, the most 
natural application of sequential learning tech-
niques is by defining the history of a span of text in 
terms of the sequence of spans of text within a 
message, since although arguments may build on 
previous messages, there is also a structure to the 
argument within a single message.  For the Social 
Modes of Co-construction dimension, it is less 
clear.  However, we have experimented with both 
ways of defining the history and have not observed 
any benefit of sequential learning techniques by 
defining the history for sequential learning in terms 
of previous messages.  Thus, for all three dimen-
sions, we report results for histories defined within 
a single message in our evaluation below. 
3 Feature Based Approach 
In previous text classification research, more atten-
tion to the selection of predictive features has been 
done for text classification problems where very 
subtle distinctions must be made or where the size 
of spans of text being classified is relatively small.  
Both of these are true of our work. For the base 
features, we began with typical text features ex-
tracted from the raw text, including unstemmed uni-
grams and punctuation.  We did not remove stop 
words, although we did remove features that occured 
less than 5 times in the corpus.  We also included a 
feature that indicated the number of words in the 
segment. 
 
Thread Structure Features. The simplest context-
oriented feature we can add based on the threaded 
structure is a number indicating the depth in the 
thread where a message appears.  We refer to this 
feature as deep.  This is expected to improve per-
formance to the extent that thread initial messages 
may be rhetorically distinct from messages that 
occur further down in the thread.  The other con-
text oriented feature related to the thread structure 
is derived from relationships between spans of text 
appearing in the parent and child messages.  This 
feature is meant to indicate how semantically re-
lated a span of text is to the spans of text in the 
parent message.  This is computed using the mini-
mum of all cosine distance measures between the 
vector representation of the span of text and that of 
each of the spans of text in all parent messages, 
74
which is a typical shallow measure of semantic 
similarity.  The smallest such distance measure is 
included as a feature indicating how related the 
current span of text is to a parent message.  
 
Sequence-Oriented Features. We hypothesized that 
the sequence of codes within a message follows a 
semi-regular structure.  In particular, the discussion 
environment used to collect the Weinberger and 
Fischer corpus inserts prompts into the message 
buffers before messages are composed in order to 
structure the interaction.  Users fill in text under-
neath these prompts.  Sometimes they quote mate-
rial from a previous message before inserting their 
own comments.  We hypothesized that whether or 
not a piece of quoted material appears before a 
span of text might influence which code is appro-
priate.  Thus, we constructed the fsm feature, 
which indicates the state of a simple finite-state 
automaton that only has two states. The automaton 
is set to initial state (q0) at the top of a message. It 
makes a transition to state (q1) when it encounters a 
quoted span of text.  Once in state (q1), the automa-
ton remains in this state until it encounters a 
prompt. On encountering a prompt it makes a tran-
sition back to the initial state (q0).  The purpose is 
to indicate places where users are likely to make a 
comment in reference to something another par-
ticipant in the conversation has already contributed. 
4 Evaluation 
The purpose of our evaluation is to contrast our 
proposed feature based approach with a state-of-
the-art sequential learning technique (Collins, 
2002).  Both approaches are designed to leverage 
context for the purpose of increasing classification 
accuracy on a classification task where the codes 
refer to the role a span of text plays in context.   
We evaluate these two approaches alone and in 
combination over the same data but with three dif-
ferent sets of codes, namely the three relevant di-
mensions of the Weinberger and Fischer annota-
tion scheme.  In all cases, we employ a 10-fold 
cross-validation methodology, where we apply a 
feature selection wrapper in such as way as to se-
lect the 100 best features over the training set on 
each fold, and then to apply this feature space and 
the trained model to the test set.  The complete 
corpus comprises about 250 discussions of the par-
ticipants.  From this we have run our experiments 
with a subset of this data, using altogether 1250 
annotated text segments. Trained coders catego-
rized each segment using this multi-dimensional 
annotation scheme, in each case achieving a level 
of agreement exceeding .7 Kappa both for segmen-
tation and coding of all dimensions as previously 
published (Weinberger & Fischer, 2006). 
For each dimension, we first evaluate alternative 
combinations of features using SMO, Weka?s im-
plementation of Support Vector Machines (Witten 
& Frank, 2005).  For a sequential learning algo-
rithm, we make use of the Collins Perceptron 
Learner (Collins, 2002).  When using the Collins 
Perceptron Learner, in all cases we evaluate com-
binations of alternative history sizes (0 and 1) and 
alternative feature sets (base and base+AllContext).  
In our experimentation we have evaluated larger 
history sizes as well, but the performance was con-
sistently worse as the history size grew larger than 
1. Thus, we only report results for history sizes of 
0 and 1. 
Our evaluation demonstrates that we achieve a 
much greater impact on performance with carefully 
designed, automatically extractable context ori-
ented features.  In all cases we are able to achieve a 
statistically significant improvement by adding 
context oriented features, and only achieve a statis-
tically significant improvement using sequential 
learning for one dimension, and only in the ab-
sence of context oriented features. 
4.1 Feature Based Approach 
0.61
0.71
0.52
0.62
0.73
0.67
0.61
0.70
0.66
0.61
0.73
0.69
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Social Macro Micro
Dimension
Ka
pp
a 
fro
m
 
10
-
fo
ld
 
CV
Base Base+Thread Base+Seq Base+AllContext
 
Figure 1. Results with alternative features 
sets 
 
75
We first evaluated the feature based approach 
across all three dimensions and demonstrate that 
statistically significant improvements are achieved 
on all dimensions by adding context oriented fea-
tures.  The most dramatic results are achieved on 
the Social Modes of Co-Construction dimension 
(See Figure 1). All pairwise contrasts between al-
ternative feature sets within this dimension are sta-
tistically significant.  In the other dimensions, 
while Base+Thread is a significant improvement 
over Base, there is no significant difference be-
tween Base+Thread and Base+AllContext.   
4.2 Sequential Learning 
0.54
0.63
0.43
0.56
0.64
0.52
0.56
0.63
0.59
0.56
0.65
0.61
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Social Macro Micro
Dimension
Ka
pp
a 
fro
m
 
10
-
fo
ld
 
CV
Base / 0 Base /  1 Base+AllContext / 0 Base+AllContext / 1
 
Figure 2. Results with Sequential Learning 
 
The results for sequential learning are weaker than 
for the feature based (See Figure 2). While the 
Collins Perceptron learner possesses the capability 
of modeling sequential dependencies between 
codes, which SMO does not possess, it is not nec-
essarily a more powerful learner.  On this data set, 
the Collins Perceptron learner consistently per-
forms worse that SMO.  Even restricting our 
evaluation of sequential learning to a comparison 
between the Collins Perceptron learner with a his-
tory of 0 (i.e., no history) with the same learner 
using a history of 1, we only see a statistically sig-
nificant improvement on the Social Modes of Co-
Construction dimension.  This is when only using 
base features, although the trend was consistently 
in favor of a history of 1 over 0. Note that the stan-
dard deviation in the performance across folds was 
much higher with the Collins Perceptron learner, 
so that a much greater difference in average would 
be required in order to achieve statistical signifi-
cance.  Performance over a validation set was al-
ways worse with larger history sizes than 1.   
5 Conclusions  
We have described work towards an approach to 
conversation summarization where an assessment 
of conversational quality along multiple process 
dimensions is reported.  We make use of a well-
established annotation scheme developed in the 
CSCL community.  Our evaluation demonstrates 
that thread based features have a greater and more 
consistent impact on performance with this data. 
 
This work was supported by the National Sci-
ence Foundation grant number SBE0354420, and 
Office of Naval Research, Cognitive and Neural Sci-
ences Division Grant N00014-05-1-0043. 
References 
Bellotti, V., Ducheneaut, N., Howard, M. Smith, I., 
Grinter, R. (2005). Quality versus Quantity: Email-
centric task management and its relation with over-
load. Human-Computer Interaction, 2005, vol. 20 
Carvalho, V. & Cohen, W. (2005). On the Collective 
Classification of Email ?Speech Acts?, Proceedings 
of SIGIR ?2005. 
Collins, M (2002). Discriminative Training Methods for 
Hidden Markov Models: Theory and Experiments 
with Perceptron Algorithms. In Proceedings of 
EMNLP 2002.  
Lacson, R., Barzilay, R., & Long, W. (2006). Automatic 
analysis of medical dialogue in the homehemodialy-
sis domain: structure induction and summarization, 
Journal of Biomedical Informatics 39(5), pp541-555. 
Roman, N., Piwek, P., & Carvalho, A. (2006).  Polite-
ness and Bias in Dialogue Summarization : Two Ex-
ploratory Studies, in J. Shanahan, Y. Qu, & J. Wiebe 
(Eds.) Computing Attitude and Affect in Text: Theory 
and Applications, the Information Retrieval Series.   
Weinberger, A., & Fischer, F. (2006). A framework to 
analyze argumentative knowledge construction in 
computer-supported collaborative learning. Com-
puters & Education, 46, 71-95. 
Witten, I. H. & Frank, E. (2005).  Data Mining: Practi-
cal Machine Learning Tools and Techniques, sec-
ond edition, Elsevier: San Francisco. 
Zechner, K. (2001). Automatic Generation of Concise 
Summaries of Spoken Dialogues in Unrestricted 
Domains. Proceedings of ACM SIG-IR 2001. 
76
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 24?27,
Columbus, June 2008. c?2008 Association for Computational Linguistics
SIDE: The Summarization Integrated Development Environment 
 
Moonyoung Kang, Sourish Chaudhuri, Mahesh Joshi, Carolyn P. Ros? 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213 USA 
moonyoun,schaudhu,maheshj,cprose@cs.cmu.edu 
 
Abstract 
In this type-II demo, we introduce SIDE1 (the 
Summarization Integrated Development Envi-
ronment), an infrastructure that facilitates 
construction of summaries tailored to the 
needs of the user. It aims to address the issue 
that there is no such thing as the perfect sum-
mary for all purposes. Rather, the quality of a 
summary is subjective, task dependent, and 
possibly specific to a user. The SIDE frame-
work allows users flexibility in determining 
what they find more useful in a summary, 
both in terms of structure and content. As an 
educational tool, it has been successfully user 
tested by a class of 21 students in a graduate 
course on Summarization and Personal Infor-
mation Management. 
1 Introduction 
A wide range of summarization systems have 
been developed in the past 40 years, beginning 
with early work in the Library sciences field. To 
this day, a great deal of research in summarization 
focuses on alternative methods for selecting sub-
sets of text segments based on a variety of forms of 
rhetorical analysis and relevance rankings.  Never-
theless, while there is much in common between 
approaches used for summarization in a variety of 
contexts, each new summarization project tends to 
include a new system development effort, because 
a general purpose, extensible framework for sum-
                                                            
1
 The working system can be downloaded from 
http://www.cs.cmu.edu/~cprose/SIDE.html, and a video 
of an example of SIDE use can be found at 
http://ankara.lti.cs.cmu.edu/side/video.swf. 
This project is supported by ONR Cognitive and Neural 
Sciences Division, Grant number N000140510043 
 
 
marization has not been made available. As an ex-
ample, Teufel and Moens? (2002) argue that the 
summarization strategy for scientific articles must 
be different from news articles because the former 
focus on novelty of information, are much longer 
and very different in structure. 
A large proportion of summarization systems do 
not allow users to intervene in the summarization 
process so that the form of the summary could be 
tailored to the individual user?s needs (Mieskes, M., 
M?ller, C., & Strube, M., 2007). From the same 
document, many summaries can potentially be 
generated, and the most preferable one for one user 
will not, in general, be the same as what is pre-
ferred by a different user. The fact that users with 
similar backgrounds can have vastly differing in-
formation needs is highlighted by Paice and Jones? 
(1993) study where an informal sentence selection 
experiment had to be abandoned because the par-
ticipants, who were agriculture experts, were too 
influenced by their research interests to agree with 
each other. However, summarization systems tend 
to appear as black boxes from the user?s perspec-
tive and the users cannot specify what they would 
want in the summary.  
SIDE is motivated by the two scenarios men-
tioned above - the absence of a common tool for 
generating summaries from different contexts, as 
well as the fact that different users might have dif-
ferent information needs from the same document. 
Bellotti (2005) discusses the problem of informa-
tion overload in communication media such as e-
mail and online discussion boards. The rapid 
growth of weblogs, wikis and dedicated informa-
tion sources makes the problem of information 
overload more acute. It also means that summari-
zation systems have the responsibility of taking 
into account the kind of information that its user 
would be interested in. 
With SIDE, we attempt to give the user a greater 
say in deciding what kind of information and how 
much of it the user wants as part of his summary.  
24
In the following sections, we elaborate on the 
features of SIDE and its technical details.   
2 Functionality 
The design of SIDE is aimed at allowing the user 
as much involvement at every stage of the sum-
mary generation process as the user wishes. SIDE 
allows the user to select a set of documents to train 
the system upon, and to decide what aspects of 
input documents should be detected and used for 
making choices, particularly at the stage of select-
ing a subset of segments to preserve from the 
source documents. The other key feature of the 
development environment is that it allows devel-
opers to plug in custom modules using the Plugin 
Manager in the GUI. In this way, advanced users 
can extend the capabilities of SIDE for meeting 
their specific needs while still taking advantage of 
the existing, general purpose aspects of SIDE. 
     The subsequent sub-sections discuss individual 
parts of system behavior in greater detail at a con-
ceptual level. Screen shots and more step by step 
discussion of how to use the GUI are given with 
the case study that outlines the demo script. 
2.1 Filters 
To train the system and create a model, the user 
has to define a filter. Defining a filter has 4 steps ? 
creating annotated files with user-defined annota-
tions, choosing feature sets to train (unigrams, bi-
grams etc), choosing evaluation metrics (Word 
Token Counter, TF-IDF) and choosing a classifier 
to train the system. 
   Annotating Files: The GUI allows the user to 
create a set of unstructured documents. The user 
can create folders and import sets of documents or 
individual documents. The GUI allows the user to 
view the documents in their original form; alterna-
tively, the user can add it to the filter and segment 
it by sentence, paragraph, or by own definition. 
The user can define a set of annotations for each 
filter, and use those to annotate segments of the file. 
The system has sentence and paragraph segmenters 
built into it. The user can also define a segmenter 
and plug it in. 
   Feature Sets: The feature set panel allows the 
user to decide which features the user wants to use 
in training the model. It is built on top of TagHel-
per Tools (Donmez et al, 2005) and uses it to ex-
tract the features chosen by the user. The system 
has options for using unigrams, bigrams, Part-Of-
Speech bigrams and punctuation built into it, and 
the user can specify whether they wish to apply 
stemming and/or stop word removal. Like the 
segmenters, if the user wants to use a specific fea-
ture to train, the user can plug in the feature extrac-
tor for the same through the GUI. 
   Evaluation Metrics: The evaluation metric de-
cides how to order the sentences that are chosen to 
be part of the summary. In keeping with the plug-
in architecture of the system, the user can define 
own metric and plug it into the system using the 
Plugin Manager. 
   Classifier: The user can decide which classifier 
to train the model with. This functionality is built 
on top of TagHelper Tools, which uses the Weka 
toolkit (Witten & Frank, 2005) to give users a set 
of classifiers to choose from. Once the system has 
been trained, the user can see the training results in 
a panel which provides a performance summary - 
including the kappa scores computed through 10-
fold cross validation and the confusion matrix, the 
sets of features extracted from the text, and the 
settings that were used for training the model. 
    The user can choose the model for classifying 
segments in the target document. The user also can 
plug-in a machine learning algorithm to the system 
if necessary. 
2.2 Summaries 
Summaries are defined by Recipes that specify 
what types of segments should be included in the 
resulting summary, and how a subset of the ones 
that meet those requirements should be selected 
and then arranged. Earlier we discussed how filters 
are defined.  One or more filters can be applied to a 
text so that each segment has one or more labels.  
These labels can then be used to index into a text. 
For example, a Recipe might specify using a logi-
cal expression such that only a subset of segments 
whose labels meet some specified set of constraints 
should be selected. The selected subset is then op-
tionally ranked using a specified Evaluation metric. 
Finally, from this ranked list, some number or 
some percentage of segments will then finally be 
selected to be included in the resulting summary.  
The segments are then optionally re-ordered to the 
original document order before including them in 
the summary, which is then displayed to the user. 
25
3 Case Study  
The following subsections describe an example 
where the user starts with some unstructured doc-
uments and uses the system to generate a specifica-
tion for a summary, which can then be applied to 
other similar documents. 
    We illustrate a script outline of our demo pres-
entation. The demo shows how simple it is to move 
through the steps of configuring SIDE for a type of 
summary that a user would like to be able to gen-
erate.  In order to demonstrate this, we will lead the 
user through an annotation task where we assign 
dialogue acts to turns in some tutoring dialogues.  
From this annotated data, we can generate summa-
ries that pull out key actions of particular types.  
For example, perhaps we would like to look at all 
the instructions that the tutor has given to a student 
or all the questions the student has asked the tutor.  
The summarizing process consists of annotating 
training documents to define filters, deciding 
which features to use along with what machine 
learning algorithm to train the filters, training the 
actual filters, defining a summary in terms of the 
structured annotation that is accomplished by the 
defined filters, and finally, summarizing target files 
using the resulting configuration. The purpose of 
SIDE is to provide both an easy GUI interface for 
people who are not familiar with programming, 
and extensible, plug-and-play code for those who 
want to program and change SIDE into a more so-
phisticated and specialized type of summarizer. 
The demo will provide options for both novice us-
ers primarily interested in working with SIDE 
through its GUI interface and for more experienced 
users who would like to work with the code.   
3.1 Using the GUI 
The summarization process begins with loading 
unstructured training and testing documents. Next, 
filters are defined by adding training documents, 
segmenting each by choosing an automatic seg-
menter, and assigning annotations to the segments. 
   After a document is segmented, the segments are 
annotated with labels that classify segments using 
a user-defined coding scheme (Figure 1). Unanno-
tated segments are later ignored during the training 
phase. Next, a set of feature types, such as uni-
grams, bigrams, part of speech bigrams, etc., are 
selected, which together will be used to build the 
feature space that will be input to a selected ma-
chine learning algorithms, or ensemble of algo-
rithms. In this example, ?Punctuation? Feature 
Class Extractor, which can distinguish interroga-
tive sentence, is selected and for ?Evaluation Met-
rics?, ?Word Token Counter? is selected. Now, we 
train this model with an appropriate machine learn-
ing algorithm. In this example, J48 which is
 
 
Figure 1: The interface where segments are annotated. 
26
Boolean
Expression
Tree
Ranker
Limiter
 
Figure 2: The interface for defining how to build a summary from the annotated data. 
 
one of Weka?s (Witten & Frank, 2005) decision 
tree learners is chosen as the learning algorithm. 
Users can explore different ensembles of machine 
learning algorithms, compare performance over the 
training data using cross-validation, and select the 
best performing one to use for summarization. 
    Once one or more filters have been defined, we 
must define how summaries are built from the 
structured representation that is built by the filters.  
Figure 2 shows the main interface for doing this.  
Recipes consist of four parts, namely ?Selecting?, 
?Ranking?, ?Limiting?, ?Sequencing?. Selection is 
done using a boolean expression tree consisting of 
?and?, ?or?, and ?is? nodes. By doing selection, only 
those segments with proper annotations will be 
selected for inclusion in the resulting summary. 
Ranking is done by the Evaluation Metric selected 
when defining the Recipe. The size of a summary 
can be limited by limiting the number of segments 
you want in your summary. Finally, the summary 
can be reordered as you wish and displayed. 
4 Current Directions 
Currently, most of the functionality in SIDE fo-
cuses on the content selection problem.  We ac-
knowledge that to move beyond extractive forms 
of summarization, additional functionality at the 
summary generation stage is necessary.  Our cur-
rent work focuses on addressing these issues. 
References 
Bellotti, V., Ducheneaut, N., Howard, M., Smith, I., & 
Grinter, R. (2005). Quality versus Quantity: E-Mail 
Centric Task Management and Its Relation with 
Overload, Human-Computer Interaction, Volume 20,  
Donmez, P., Ros?, C. P., Stegmann, K., Weinberger, A., 
and Fischer, F. (2005). Supporting CSCL with Auto-
matic Corpus Analysis Technology , Proceedings of 
Computer Supported Collaborative Learning. 
Mieskes, M., M?ller, C., & Strube, M. (2007) Improv-
ing extractive dialogue summarization by utilizing 
human feedback, Proceedings of the 25th IASTED 
International Multi-Conference: artificial intelligence 
and applications, p.627-632 
Paice, Chris D. & Jones, Paul A. (1993) The identifica-
tion of important concepts in highly structured tech-
nical papers. In Proceedings of the 16th ACM-SIGIR 
Conference, pages 69?78 
Teufel, S. & Moens, M. (2002). Summarizing Scientific 
Articles: Experiments with Relevance and Rhetorical 
Status, Computational Linguistics, Vol 28, No. 1. 
Witten, Ian H.; Frank, Eibe (2005). Data Mining: Prac-
tical machine learning tools and techniques, 2nd Edi-
tion. Morgan Kaufmann, San Francisco.  
27
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 101?104,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Leveraging Structural Relations for Fluent Compressions                    
at Multiple Compression Rates 
 
Sourish Chaudhuri, Naman K. Gupta, Noah A. Smith, Carolyn P. Ros? 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA-15213, USA. 
{sourishc, nkgupta, nasmith, cprose}@cs.cmu.edu 
 
Abstract 
Prior approaches to sentence compression 
have taken low level syntactic constraints into 
account in order to maintain grammaticality. 
We propose and successfully evaluate a more 
comprehensive, generalizable feature set that 
takes syntactic and structural relationships into 
account in order to sustain variable compres-
sion rates while making compressed sentences 
more coherent, grammatical and readable.  
1 Introduction 
We present an evaluation of the effect of syntac-
tic and structural constraints at multiple levels of 
granularity on the robustness of sentence com-
pression at varying compression rates.  Our eval-
uation demonstrates that the new feature set pro-
duces significantly improved compressions 
across a range of compression rates compared to 
existing state-of-the-art approaches. Thus, we 
name our system for generating compressions the 
Adjustable Rate Compressor (ARC).   
Knight and Marcu (2000) (K&M, henceforth) 
presented two approaches to the sentence com-
pression problem: one using a noisy channel 
model, the other using a decision-based model. 
The performances of the two models were com-
parable though their experiments suggested that 
the noisy channel model degraded more smooth-
ly than the decision-based model when tested on 
out-of-domain data. Riezler et al (2003) applied 
linguistically rich LFG grammars to a sentence 
compression system. Turner and Charniak (2005) 
achieved similar performance to K&M using an 
unsupervised approach that induced rules from 
the Penn Treebank.  
A variety of feature encodings have previous-
ly been explored for the problem of sentence 
compression.  Clarke and Lapata (2007) included 
discourse level features in their framework to 
leverage context for enhancing coherence. 
McDonald?s (2006) model (M06, henceforth) is 
similar to K&M except that it uses discriminative 
online learning to train feature weights. A key 
aspect of the M06 approach is a decoding algo-
rithm that searches the entire space of compres-
sions using dynamic programming to choose the 
best compression (details in Section 2). We use 
M06 as a foundation for this work because its 
soft constraint approach allows for natural inte-
gration of additional classes of features. Similar 
to most previous approaches, our approach com-
presses sentences by deleting words only. 
The remainder of the paper is organized as 
follows. Section 2 discusses the architectural 
framework.  Section 3 describes the innovations 
in the proposed model. We conclude after pre-
senting the results of our evaluation in Section 4. 
2 Experimental Paradigm 
Supervised approaches to sentence compression 
typically use parallel corpora consisting of origi-
nal and compressed sentences (paired corpus, 
henceforth). In this paper, we will refer to these 
pairs as a 2-tuple <x, y>, where x is the original 
sentence and y is the compressed sentence. 
We implemented the M06 system as an expe-
rimental framework in which to conduct our in-
vestigation. The system uses as input the paired 
corpus, the corresponding POS tagged corpus, 
the paired corpus parsed using the Charniak 
parser (Charniak, 2000), and dependency parses 
from the MST parser (McDonald et al, 2005). 
Features are extracted over adjacent pairs of 
words in the compressed sentence and weights 
are learnt at training time using the MIRA algo-
rithm (Crammer and Singer, 2003). We decode 
as follows to find the best compression:  
Let the score of a compression y for a sen-
tence x be s(x, y). This score is factored using a 
first-order Markov assumption over the words in 
the compressed sentence, and is defined by the 
dot product between a high dimensional feature 
representation and a corresponding weight vector 
(for details, refer to McDonald, 2006). The equa-
tions for decoding are as follows: 
 
1),,,(][max][
0.0]1[
iijxsjCiC
C
ij
 
101
where C is the dynamic programming table and 
C[i] represents the highest score for compres-
sions ending at word i for the sentence x. 
The M06 system takes the best scoring com-
pression from the set of all possible compres-
sions.  In the ARC system, the model determines 
the compression rate and enforces a target com-
pression length by altering the dynamic pro-
gramming algorithm as suggested by M06: 
 
1,]][1[
0.0]1][1[
rrC
C  
,1i  
),,(]1][[max]][[ ijxsrjCriC ij
 
 
where C is the dynamic programming table as 
before and C[i][r] is the score for the best com-
pression of length r that ends at position i in the 
sentence x. This algorithm runs in O (n2r) time.  
We define the rate of human generated com-
pressions in the training corpus as the gold stan-
dard compression rate (GSCR). We train a linear 
regression model over the training data to predict 
the GSCR for a sentence based on the ratio be-
tween the lengths of each compressed-original 
sentence pair in the training set. The predicted 
compression rate is used to force the system to 
compress sentences in the test set to a specific 
target length. Based on the computed regression, 
the formula for computing the Predicted Com-
pression Rate (PCR) from the Original Sentence 
Length (OSL) is as follows: 
 
OSLPCR 004.086.0  
 
In our work, enforcing specific compression 
rates serves two purposes. First, it allows us to 
make a more controlled comparison across ap-
proaches, since variation in compression rate 
across approaches confounds comparison of oth-
er aspects of performance.  Second, it allows us 
to investigate how alternative models work at 
higher compression rates. Here our primary con-
tribution is of robustness of the approach with 
respect to alternative feature spaces and com-
pression rates. 
3 Extended Feature Set 
A major focus of our work is the inclusion of 
new types of features derived from syntactic ana-
lyses in order to make the resulting compressions 
more grammatical and thus increase the versatili-
ty of the resulting compression models.   
The M06 system uses features extracted from 
the POS tagged paired corpus: POS bigrams, 
POS context of the words added to or dropped 
from the compression, and other information 
about the dropped words. For a more detailed 
description, please refer to McDonald, 2006.   
From the phrase structure trees, M06 extracts 
context information about nodes that subsume 
dropped words. These features attempt to ap-
proximately encode changes in the grammar 
rules between source and target sentences. De-
pendency features include information about the 
dropped words? parents as well as conjunction 
features of the word and the parent. 
Our extensions to the M06 feature set are in-
spired by an analysis of the compressions gener-
ated by it, and allow for a richer encoding of 
dropped words and phrases using properties of 
the words and their syntactic relations to the rest 
of the sentence. Consider this example (dropped 
words are marked as such):  
 
* 68000 Sweden AB of Uppsala , Sweden , intro-
duced the TeleServe , an integrated answering 
machine and voice-message handler that links a 
Macintosh to Touch-Tone phones . 
  
Note in the above example that the syntactic 
head of the sentence introduced has been 
dropped. Using the dependency parse, we add a 
class of features to be learned during training that 
lets the system decide when to drop the syntactic 
head of the sentence. Also note that answering 
machine in the original sentence was preceded 
by an while the word the was used with Tele-
serve (dropped in the compression). While POS 
information helps the system to learn that the 
answering machine is a good POS sequence, we 
do not have information that links the correct 
article to the noun. Information from the depen-
dency parse allows us to learn when we can drop 
words whose heads are retained and when we 
can drop a head and still retain the dependent.  
Now, consider the following example: 
 
Examples for editors are applicable to awk pat-
terns , grep and egrep .  
 
    Here, Examples has been dropped, while for 
editors which has Examples as a head is retained. 
Besides, in the sequence, editors are applica-
ble?, the word editors behaves as the subject of 
are although the correct compression would have 
examples as its subject. A change in the argu-
ments of the verbs will distort the meaning of the 
sentence. We augmented the feature set to in-
clude a class of features about structural informa-
tion that tells us when the subject (or object) of a 
verb can be dropped while the verb itself is re-
tained. Thus, now if the system does retain the 
102
are, it is more likely to retain the correct argu-
ments of the word from the original sentence. 
    The new classes of features use only the de-
pendency labels generated by the parser and are 
not lexicalized. Intuitively, these features help 
create units within the sentences that are tightly 
bound together, e.g., a subject and an object with 
its parent verb. We notice, as one would expect, 
that some dependency bindings are less strong 
than others. For instance, when faced with a 
choice, our system drops a relative pronoun thus 
breaking the dependency between the retained 
noun and the relative pronoun, rather than drop 
the noun, which was the retained subject. 
Below is a summary of the information that 
the new features in our system encode: 
[Parent-Child]- When a word is dropped, is its 
parent retained in the compression?  
[Dependent]- When a word is dropped, are 
other words dependent on it (its children) 
also dropped or are they retained?  
[Verb-Arg]- Information from the dependency 
parse about the subjects and objects of 
verbs can be used to encode more specific 
features (similar to the above) that say 
whether or not the subject (or object) was 
retained when the verb was dropped.  
[Sent-Head-Dep]- Is the syntactic head of a 
sentence dropped? 
4 Evaluation 
We evaluate our model in comparison with M06. 
At training time, compression rates were not en-
forced on the ARC or M06 model. Our evalua-
tion demonstrates that the proposed feature set 
produces more grammatical sentences across 
varying compression rates.  In this section, 
GSCR denotes gold standard compression rate 
(i.e., the compression rate found in training data), 
CR denotes compression rate.   
4.1 Corpora 
Sentence compression systems have been tested 
on product review data from the Ziff-Davis (ZD, 
henceforth) Corpus by Knight and Marcu (2000), 
general news articles by Clarke and Lapata (CL, 
henceforth) corpus (2007) and biomedical ar-
ticles (Lin and Wilbur, 2007). To evaluate our 
system, we used 2 test sets: Set 1 contained 50 
sentences; all 32 sentences from the ZD test set 
and 18 additional sentences chosen randomly 
from the CL test set; Set 2 contained 40 sen-
tences selected from the CL corpus, 20 of which 
were compressed at 75% of GSCR and 20 at 
50% of GSCR (the percentages denote the en-
forced compression rates). 
Three examples comparing compressed sen-
tences are given below:  
 
 
Original: Like FaceLift, much of ATM 's screen 
performance depends on the underlying applica-
tion. 
Human: Much of ATM 's performance depends 
on the underlying application . 
M06: 's screen performance depends on applica-
tion  
ARC: ATM 's screen performance depends on 
the underlying application . 
 
Original: The discounted package for the Sparc-
server 470 is priced at $89,900 , down from the 
regular $107,795 . 
Human: The Sparcserver 470 is priced at 
$89,900 , down from the regular $107,795 . 
M06: Sparcserver 470 is $89,900 regular 
$107,795 
ARC: The discounted package is priced at 
$89,900 , regular $107,795 .  
 
 
The example below has compressions at 50% 
compression rate for M06 and ARC systems: 
 
 
Original: Cutbacks in local defence establish-
ments is also a factor in some constituencies . 
M06: establishments is a factor in some consti-
tuencies . 
ARC: Cutbacks is a factor in some constituen-
cies .  
 
 
Note that the subject of is is correctly retained 
in the ARC system. 
4.2 User Study 
In order to evaluate the effect of the features that 
we added to create the ARC model, we con-
ducted a user study, adopting an experimental 
methodology similar to that used by K&M and 
M06.  Each of four human judges, who were na-
tive speakers of English and not involved in the 
research we report in this paper, were instructed 
to rate two different sets of compressions along 
two dimensions, namely Grammaticality and 
Completeness, on a scale of 1 to 5. We chose to 
replace Importance (used by K&M), which is a 
task specific and possibly user specific notion, 
with the more general notion of Completeness, 
defined as the extent to which the compressed 
sentence is a complete sentence and communi-
cates the main idea of the original sentence.  
For Set 1, raters were given the original sen-
tence and 4 compressed versions (presented in 
103
random order as in the M06 evaluation): the hu-
man compression, the compression produced by 
the original M06 system, the compression from 
the M06 system with GSCR, and the ARC sys-
tem with GSCR. For Set 2, raters were given the 
original sentence, this time with two compressed 
versions, one from the M06 system and one from 
the ARC system, which were presented in a ran-
dom order.  Table 1 presents all the results in 
terms of human ratings of Grammaticality and 
Completeness as well as automatically computed 
ROUGE F1 scores (Lin and Hovy, 2003). The 
scores in parentheses denote standard deviations. 
 
 Grammati-
cality 
(Human 
Scores) 
Com-
pleteness 
(Human 
Scores) 
 
ROUGE 
F1 
Gold 
Standard 
4.60 (0.69) 3.80(.99) 1.00 (0) 
ARC 
(GSCR) 
3.70 (1.10) 3.50(1.10) .72 (.18) 
M06 3.50 (1.30) 3.10(1.30) .70 (.20) 
M06 
(GSCR) 
3.10 (1.10) 3.10(1.10) .71 (.18) 
ARC 
(75%CR) 
2.60 (1.10) 2.60(1.10) .72 (.14) 
M06 
(75%CR) 
2.20 (1.20) 2.00(1.00) .67 (.20) 
ARC 
(50%CR) 
2.30 (1.30) 1.90(1.00) .54 (.22) 
M06 
(50%CR) 
1.90 (1.10) 1.80(1.00) .58 (.22) 
Table 1: Results of human judgments and ROUGE F1 
 
 ROUGE scores were determined to have a 
significant positive correlation both with Gram-
maticality (R = .46, p < .0001) and Completeness 
(R = .39, p < .0001) when averaging across the 4 
judges? ratings.  On Set 1, a 2-tailed paired t-test 
reveals similar patterns for Grammaticality and 
Completeness: the human compressions are sig-
nificantly better than any of the systems.  ARC is 
significantly better than M06, both with enforced 
GSCR and without. M06 without GSCR is sig-
nificantly better than M06 with GSCR.  In Set 2 
(with 75% and 50% GSCR enforced), the quality 
of compressions degrade as compression rate is 
made more severe; however, the ARC model 
consistently outperforms the M06 model with a 
statistically significant margin across compres-
sion rates on both evaluation criteria. 
5 Conclusions and Future Work 
In this paper, we designed a set of new classes of 
features to generate better compressions, and 
they were found to produce statistically signifi-
cant improvements over the state-of-the-art. 
However, although the user study demonstrates 
the expected positive impact of grammatical fea-
tures, an error analysis (Gupta et al, 2009) re-
veals some limitations to improvements that can 
be obtained using grammatical features that refer 
only to the source sentence structure, since the 
syntax of the source sentence is frequently not 
preserved in the gold standard compression. In 
our future work, we hope to explore alternative 
approaches that allow reordering or paraphrasing 
along with deleting words to make compressed 
sentences more grammatical and coherent. 
 
Acknowledgments 
The authors thank Kevin Knight and Daniel 
Marcu for sharing the Ziff-Davis corpus as well 
as the output of their systems, and the anonym-
ous reviewers for their comments. This work was 
supported by the Cognitive and Neural Sciences 
Division, grant number N00014-00-1-0600. 
References  
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. of  NAACL. 
James Clarke and Mirella Lapata, 2007. Modelling 
Compression With Discourse Constraints. In Proc. 
of EMNLP-CoNLL. 
Koby Crammer and Y. Singer. 2003. Ultraconserva-
tive online algorithms for multi-class problems. 
JMLR. 
Naman K. Gupta, Sourish Chaudhuri and Carolyn P. 
Ros?, 2009. Evaluating the Syntactic Transforma-
tions in Gold Standard Corpora for Statistical Sen-
tence Compression . In Proc. of HLT-NAACL. 
Kevin Knight and Daniel Marcu. 2000. Statistics-
Based Summarization ? Step One: Sentence Com-
pression. In Proc. of AAAI. 
Jimmy Lin and W. John Wilbur. 2007. Syntactic sen-
tence compression in the biomedical domain: faci-
litating access to related articles. Information Re-
trieval, 10(4):393-414. 
Chin-Yew Lin and Eduard H. Hovy 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proc. of HLT-NAACL. 
Ryan McDonald, 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proc. of 
EACL.  
Ryan McDonald, Koby Crammer, and Fernando Pe-
reira. 2005. Online large-margin training of depen-
dency parsers. In Proc.of ACL. 
S. Riezler, T. H. King, R. Crouch, and A. Zaenen.  
2003. Statistical sentence condensation using am-
biguity packing and stochastic disambiguation me-
thods for lexical-functional grammar. In Proc. of 
HLT-NAACL. 
104
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 313?316,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Generalizing Dependency Features for Opinion Mining
Mahesh Joshi1 and Carolyn Penstein-Rose?1,2
1Language Technologies Institute
2Human-Computer Interaction Institute
Carnegie Mellon University, Pittsburgh, PA, USA
{maheshj,cprose}@cs.cmu.edu
Abstract
We explore how features based on syntac-
tic dependency relations can be utilized to
improve performance on opinion mining.
Using a transformation of dependency re-
lation triples, we convert them into ?com-
posite back-off features? that generalize
better than the regular lexicalized depen-
dency relation features. Experiments com-
paring our approach with several other ap-
proaches that generalize dependency fea-
tures or ngrams demonstrate the utility of
composite back-off features.
1 Introduction
Online product reviews are a crucial source of
opinions about a product, coming from the peo-
ple who have experienced it first-hand. However,
the task of a potential buyer is complicated by the
sheer number of reviews posted online for a prod-
uct of his/her interest. Opinion mining, or sen-
timent analysis (Pang and Lee, 2008) in product
reviews, in part, aims at automatically processing
a large number of such product reviews to identify
opinionated statements, and to classify them into
having either a positive or negative polarity.
One of the most popular techniques used for
opinion mining is that of supervised machine
learning, for which, many different lexical, syntac-
tic and knowledge-based feature representations
have been explored in the literature (Dave et al,
2003; Gamon, 2004; Matsumoto et al, 2005; Ng
et al, 2006). However, the use of syntactic fea-
tures for opinion mining has achieved varied re-
sults. In our work, we show that by altering
syntactic dependency relation triples in a partic-
ular way (namely, ?backing off? only the head
word in a dependency relation to its part-of-speech
tag), they generalize better and yield a significant
improvement on the task of identifying opinions
from product reviews. In effect, this work demon-
strates a better way to utilize syntactic dependency
relations for opinion mining.
In the remainder of the paper, we first discuss
related work. We then motivate our approach and
describe the composite back-off features, followed
by experimental results, discussion and future di-
rections for our work.
2 Related Work
The use of syntactic or deep linguistic features for
opinion mining has yielded mixed results in the lit-
erature so far. On the positive side, Gamon (2004)
found that the use of deep linguistic features ex-
tracted from phrase structure trees (which include
syntactic dependency relations) yield significant
improvements on the task of predicting satisfac-
tion ratings in customer feedback data. Mat-
sumoto et al (2005) show that when using fre-
quently occurring sub-trees obtained from depen-
dency relation parse trees as features for machine
learning, significant improvement in performance
is obtained on the task of classifying movie re-
views as having positive or negative polarity. Fi-
nally, Wilson et al (2004) use several different
features extracted from dependency parse trees to
improve performance on the task of predicting the
strength of opinion phrases.
On the flip side, Dave et al (2003) found
that for the task of polarity prediction, adding
adjective-noun dependency relationships as fea-
tures does not provide any benefit over a sim-
ple bag-of-words based feature space. Ng et al
(2006) proposed that rather than focusing on just
adjective-noun relationships, the subject-verb and
verb-object relationships should also be consid-
ered for polarity classification. However, they ob-
served that the addition of these dependency re-
lationships does not improve performance over a
feature space that includes unigrams, bigrams and
trigrams.
313
One difference that seems to separate the suc-
cesses from the failures is that of using the en-
tire set of dependency relations obtained from a
dependency parser and allowing the learning al-
gorithm to generalize, rather than picking a small
subset of dependency relations manually. How-
ever, in such a situation, one critical issue might be
the sparseness of the very specific linguistic fea-
tures, which may cause the classifier learned from
such features to not generalize. Features based on
dependency relations provide a nice way to enable
generalization to the right extent through utiliza-
tion of their structural aspect. In the next section,
we motivate this idea in the context of our task,
from a linguistic as well as machine learning per-
spective.
3 Identifying Opinionated Sentences
We focus on the problem of automatically identi-
fying whether a sentence in a product review con-
tains an opinion about the product or one of its
features. We use the definition of this task as for-
mulated by Hu and Liu (2004) on Amazon.com
and CNet.com product reviews for five different
products. Their definition of an opinion sentence
is reproduced here verbatim: ?If a sentence con-
tains one or more product features and one or
more opinion words, then the sentence is called an
opinion sentence.? Any other sentence in a review
that does not fit the above definition of an opinion
sentence is considered as a non-opinion sentence.
In general, these can be expected to be verifiable
statements or facts such as product specifications
and so on.
Before motivating the use of dependency rela-
tions as features for our task, a brief overview
about dependency relations follows.
3.1 Dependency Relations
The dependency parse for a given sentence is es-
sentially a set of triplets or triples, each of which is
composed of a grammatical relation and the pair of
words from the sentence among which the gram-
matical relation holds ({rel
i
, w
j
, w
k
}, where rel
i
is the dependency relation among words w
j
and
w
k
). The set of dependency relations is specific
to a given parser ? we use the Stanford parser1 for
computing dependency relations. The word w
j
is
usually referred to as the head word in the depen-
1http://nlp.stanford.edu/software/
lex-parser.shtml
dency triple, and the word w
k
is usually referred
to as the modifier word.
One straightforward way to use depen-
dency relations as features for machine
learning is to generate features of the form
RELATION HEAD MODIFIER and use them in a
standard bag-of-words type binary or frequency-
based representation. The indices of the head and
modifier words are dropped for the obvious reason
that one does not expect them to generalize across
sentences. We refer to such features as lexicalized
dependency relation features.
3.2 Motivation for our Approach
Consider the following examples (these are made-
up examples for the purpose of keeping the dis-
cussion succinct, but still capture the essence of
our approach):
(i) This is a great camera!
(ii) Despite its few negligible flaws, this really
great mp3 player won my vote.
Both of these sentences have an adjectival mod-
ifier (amod) relationship, the first one having
amod camera great) and the second one hav-
ing amod player great). Although both of
these features are good indicators of opinion sen-
tences and are closely related, any machine learn-
ing algorithm that treats these features indepen-
dently will not be able to generalize their rela-
tionship to the opinion class. Also, any new test
sentence that contains a noun different from either
?camera? or ?player? (for instance in the review
of a different electronic product), but is participat-
ing in a similar relationship, will not receive any
importance in favor of the opinion class ? the ma-
chine learning algorithm may not have even seen
it in the training data.
Now consider the case where we ?back off?
the head word in each of the above features to its
part-of-speech tag. This leads to a single feature:
amod NN great. This has two advantages: first,
the learning algorithm can now learn a weight for a
more general feature that has stronger evidence of
association with the opinion class, and second, any
new test sentence that contains an unseen noun in a
similar relationship with the adjective ?great? will
receive some weight in favor of the opinion class.
This ?back off? operation is a generalization of
the regular lexicalized dependency relations men-
tioned above. In the next section we describe all
such generalizations that we experimented with.
314
4 Methodology
Composite Back-off Features: The idea behind
our composite back-off features is to create more
generalizable, but not overly general back-off fea-
tures by backing off to the part-of-speech (POS)
tag of either the head word or the modifier word
(but not both at once, as in Gamon (2004) andWil-
son et al (2004)) ? hence the description ?compos-
ite,? as there is a lexical part to the feature, coming
from one word, and a POS tag coming from the
other word, along with the dependency relation it-
self.
The two types of composite back-off features
that we create from lexicalized dependency triples
are as follows:
(i) h-bo: Here we use features of the form
{rel
i
, POS
j
, w
k
}where the head word is replaced
by its POS tag, but the modifier word is retained.
(ii) m-bo: Here we use features of the form
{rel
i
, w
j
, POS
k
}, where the modifier word is re-
placed by its POS tag, but the head word is re-
tained.
Our hypothesis is that the h-bo features will
perform better than purely lexicalized dependency
relations for reasons mentioned in Section 3.2
above. Although m-bo features also generalize
the lexicalized dependency features, in a relation
such as an adjectival modifier (discussed in Sec-
tion 3.2 above), the head noun is a better candi-
date to back-off for enabling generalization across
different products, rather than the modifier adjec-
tive. For this reason, we do not expect their per-
formance to be comparable to h-bo features.
We compare our composite back-off features
with other similar ways of generalizing depen-
dency relations and lexical ngrams that have been
tried in previous work. We describe these below.
Full Back-off Features: Both Gamon (2004)
and Wilson et al (2004) utilize features based on
the following version of dependency relationships:
{rel
i
, POS
j
, POS
k
}, where they ?back off? both
the head word and the modifier word to their re-
spective POS tags (POS
j
and POS
k
). We refer
to this as hm-bo.
NGram Back-off Features: Similar to Mc-
Donald et al (2007), we utilize backed-off ver-
sions of lexical bigrams and trigrams, where all
possible combinations of the words in the ngram
are replaced by their POS tags, creating features
such as w
j
POS
k
, POS
j
w
k
, POS
j
POS
k
for
each lexical bigram and similarly for trigrams. We
refer to these as bi-bo and tri-bo features respec-
tively.
In addition to these back-off approaches, we
also use regular lexical bigrams (bi), lexical tri-
grams (tri), POS bigrams (POS-bi), POS trigrams
(POS-tri) and lexicalized dependency relations
(lexdep) as features. While testing all of our fea-
ture sets, we evaluate each of them individually by
adding them to the basic set of unigram (uni) fea-
tures.
5 Experiments and Results
Details of our experiments and results follow.
5.1 Dataset
We use the extended version of the Amazon.com /
CNet.com product reviews dataset released by Hu
and Liu (2004), available from their web page2.
We use a randomly chosen subset consisting of
2,200 review sentences (200 sentences each for
11 different products)3. The distribution is 1,053
(47.86%) opinion sentences and 1,147 (52.14%)
non-opinion sentences.
5.2 Machine Learning Parameters
We have used the Support Vector Machine (SVM)
learner (Shawe-Taylor and Cristianini, 2000) from
the MinorThird Toolkit (Cohen, 2004), along with
the ?-squared feature selection procedure, where
we reject features if their ?-squared score is not
significant at the 0.05 level. For SVM, we use
the default linear kernel with all other parameters
also set to defaults. We perform 11-fold cross-
validation, where each test fold contains all the
sentences for one of the 11 products, and the sen-
tences for the remaining ten products are in the
corresponding training fold. Our results are re-
ported in terms of average accuracy and Cohen?s
kappa values across the 11 folds.
5.3 Results
Table 1 shows the full set of results from our ex-
periments. Our results are comparable to those re-
ported by Hu and Liu (2004) on the same task;
as well as those by Arora et al (2009) on a sim-
ilar task of identifying qualified vs. bald claims
in product reviews. On the accuracy metric, the
composite features with the head word backed off
2http://www.cs.uic.edu/?liub/FBS/
sentiment-analysis.html
3http://www.cs.cmu.edu/?maheshj/
datasets/acl09short.html
315
Features Accuracy Kappa
uni .652 (?.048) .295 (?.049)
uni+bi .657 (?.066) .304 (?.089)
uni+bi-bo .650 (?.056) .299 (?.079)
uni+tri .655 (?.062) .306 (?.077)
uni+tri-bo .647 (?.051) .287 (?.075)
uni+POS-bi .676 (?.057) .349 (?.083)
uni+POS-tri .661 (?.050) .317 (?.064)
uni+lexdep .639 (?.055) .268 (?.079)
uni+hm-bo .670 (?.046) .336 (?.065)
uni+h-bo .679 (?.063) .351 (?.097)
uni+m-bo .657 (?.056) .308 (?.063)
Table 1: Shown are the average accuracy and Co-
hen?s kappa across 11 folds. Bold indicates statis-
tically significant improvements (p < 0.05, two-
tailed pairwise T-test) over the (uni) baseline.
are the only ones that achieve a statistically signif-
icant improvement over the uni baseline. On the
kappa metric, using POS bigrams also achieves
a statistically significant improvement, as do the
composite h-bo features. None of the other back-
off strategies achieve a statistically significant im-
provement over uni, although numerically hm-bo
comes quite close to h-bo. Evaluation of these
two types of features by themselves (without un-
igrams) shows that h-bo are significantly better
than hm-bo at p < 0.10 level. Regular lexical-
ized dependency relation features perform worse
than unigrams alone. These results thus demon-
strate that composite back-off features based on
dependency relations, where only the head word is
backed off to its POS tag present a useful alterna-
tive to encoding dependency relations as features
for opinion mining.
6 Conclusions and Future Directions
We have shown that for opinion mining in prod-
uct review data, a feature representation based on
a simple transformation (?backing off? the head
word in a dependency relation to its POS tag) of
syntactic dependency relations captures more gen-
eralizable and useful patterns in data than purely
lexicalized dependency relations, yielding a statis-
tically significant improvement.
The next steps that we are currently working
on include applying this approach to polarity clas-
sification. Also, the aspect of generalizing fea-
tures across different products is closely related
to fully supervised domain adaptation (Daume? III,
2007), and we plan to combine our approach with
the idea from Daume? III (2007) to gain insights
into whether the composite back-off features ex-
hibit different behavior in domain-general versus
domain-specific feature sub-spaces.
Acknowledgments
This research is supported by National Science
Foundation grant IIS-0803482.
References
Shilpa Arora, Mahesh Joshi, and Carolyn Rose?. 2009.
Identifying Types of Claims in Online Customer Re-
views. In Proceedings of NAACL 2009.
William Cohen. 2004. Minorthird: Methods for Iden-
tifying Names and Ontological Relations in Text us-
ing Heuristics for Inducing Regularities from Data.
Hal Daume? III. 2007. Frustratingly Easy Domain
Adaptation. In Proceedings of ACL 2007.
Kushal Dave, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Ex-
traction and Semantic Classification of Product Re-
views. In Proceedings of WWW 2003.
Michael Gamon. 2004. Sentiment Classification on
Customer Feedback Data: Noisy Data, Large Fea-
ture Vectors, and the Role of Linguistic Analysis. In
Proceedings of COLING 2004.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of ACM
SIGKDD 2004.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment Classification Using
Word Sub-sequences and Dependency Sub-trees. In
Proceedings of the 9th PAKDD.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. StructuredModels for
Fine-to-Coarse Sentiment Analysis. In Proceedings
of ACL 2007.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the Role of Linguistic Knowledge
Sources in the Automatic Identification and Classi-
fication of Reviews. In Proceedings of the COL-
ING/ACL 2006.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2).
John Shawe-Taylor and Nello Cristianini. 2000.
Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just How Mad Are You? Finding Strong
and Weak Opinion Clauses. In Proceedings of AAAI
2004.
316
A Little Goes a Long Way: Quick Authoring of Semantic Knowledge
Sources for Interpretation
Carolyn Penstein Rose?
Carnegie Mellon University
cprose@cs.cmu.edu
Brian S. Hall
University of Pittsburgh
mosesh@pitt.edu
Abstract
In this paper we present an evaluation of
Carmel-Tools, a novel behavior oriented ap-
proach to authoring and maintaining do-
main specific knowledge sources for ro-
bust sentence-level language understanding.
Carmel-Tools provides a layer of abstraction
between the author and the knowledge sources,
freeing up the author to focus on the desired
language processing behavior that is desired in
the target system rather than the linguistic de-
tails of the knowledge sources that would make
this behavior possible. Furthermore, Carmel-
Tools offers greater flexibility in output rep-
resentation than the context-free rewrite rules
produced by previous semantic authoring tools,
allowing authors to design their own predicate
language representations.
1 Introduction
One of the major obstacles that currently makes it imprac-
tical for language technology applications to make use of
sophisticated approaches to natural language understand-
ing, such as deep semantic analysis and domain level rea-
soning, is the tremendous expense involved in authoring
and maintaining domain specific knowledge sources. In
this paper we describe an evaluation of Carmel-Tools as a
proof of concept for a novel behavior oriented approach
to authoring and maintaining domain specific knowledge
sources for robust sentence-level language understand-
ing. What we mean by behavior oriented is that Carmel-
Tools provides a layer of abstraction between the author
and the knowledge sources, freeing up the author to fo-
cus on the desired language processing behavior that is
desired in the target system rather than the linguistic de-
tails of the knowledge sources that would make this be-
havior possible. Thus, Carmel-Tools is meant to make
the knowledge source engineering process accessible to
a broader audience. Carmel-Tools is used to author do-
main specific semantic knowledge sources for the Carmel
Workbench (Rose?, 2000; Rose? et al, 2002) that con-
tains broad coverage domain general syntactic and lexical
knowledge sources for robust language understanding in
English. Our evaluation demonstrates how Carmel-Tools
can be used to interpret sentences in the physics domain
as part of a content-based approach to automatic essay
grading.
Sentence: The man is moving horizontally at a constant
velocity with the pumpkin.
Predicate Language Representation:
((velocity id1 man horizontal constant non-zero)
(velocity id2 pumpkin ?dir ?mag-change ?mag-zero)
(rel-value id3 id1 id2 equal))
Gloss: The constant, nonzero, horizontal velocity of the
man is equal to the velocity of the pumpkin.
Figure 1: Simple example of how Carmel-Tools builds
knowledge sources capable of assigning representations
to sentences that are not constrained to mirror the exact
wording, structure, or literal surface meaning of the text.
While much work has been done in the area of ro-
bust semantic interpretation, current authoring tools for
building semantic knowledge sources (Cunningham et
al., 2003; Jay et al, 1997) are tailored for informa-
tion extraction tasks that emphasize the identification of
named entities such as people, locations, and organiza-
tions. While regular expression based recognizers, such
as JAPE (Cunningham et al, 2000), used for information
extraction systems, are not strictly limited to these stan-
dard entity types, it is not clear how they would handle
concepts expressing complex relationships between enti-
ties, where the complexity in the meaning can be real-
ized with a much greater degree of surface syntactic vari-
ation. Outside of the information extraction domain, a
concept acquisition authoring environment called SGStu-
dio (Wang and Acero, 2003) offers similar functionality
to JAPE for building language understanding modules for
dialogue systems, with similar limitations. Carmel-Tools
is more flexible in that it allows a wider range of linguis-
tics expression that communicate the same idea to match
against the same pattern. It accomplishes this by inducing
patterns that match against a deep syntactic parse rather
than a stream of words, in order to normalize as much
surface syntactic variation as possible, and thus reduc-
ing the number of patterns that the learned rules must
account for. Furthermore, Carmel-Tools offers greater
flexibility in output representation than the context-free
rewrite rules produced by previous semantic authoring
tools, allowing authors to design their own predicate lan-
guage representations that are not constrained to follow
the structure of the input text (See Figure 1 for a simple
example and Figure 2 for a more complex example.). See
Section 3 and (Rose?, 2000; Rose? et al, 2002) for more
details about CARMEL?s knowledge source representa-
tion.
Note that the predicate language representation uti-
lized by Carmel-Tools is in the style of Davidsonian event
based semantics (Hobbs, 1985). For example, in Figure
1 notice that the first argument of each predicate is an
identification token that represents the whole predicate.
These identification tokens can then be bound to argu-
ments of other predicates, and in that way be used to rep-
resent relationships between predicates. For example, the
rel-value predicate expresses the idea that the predi-
cates indicated by id1 and id2 are equal in value.
While language understanding systems with this style
of analysis are not a new idea, the contribution of this
work is a set of authoring tools that simplify the semantic
knowledge sources authoring process.
2 Motivation
While the technology presented in this paper is not spe-
cific to any particular application area, this work is mo-
tivated by a need within a growing community of re-
searchers working on educational applications of Natu-
ral Language Processing to extract detailed information
from student language input to be used for formulating
specific feedback directed at the details of what the stu-
dent has uttered. Such applications include tutorial dia-
logue systems (Zinn et al, 2002; Popescue et al, 2003)
and writing coaches that perform detailed assessments of
writing content (Rose? et al, 2003; Wiemer-Hastings et
al., 1998; Malatesta et al, 2002) as opposed to just gram-
mar (Lonsdale and Strong-Krause, 2003), and provide
detailed feedback rather than just letter grades (Burstein
et al, 1998; Foltz et al, 1998). Because of the important
role of language in the learning process (Chi et al, 2001),
and because of the unique demands educational applica-
tions place on the technology, especially where detailed
feedback based on student language input is offered to
students, educational applications present interesting op-
portunities for this community.
The area of automated essay grading has enjoyed a
great deal of success at applying shallow language pro-
cessing techniques to the problem of assigning general
quality measures to student essays (Burstein et al, 1998;
Foltz et al, 1998). The problem of providing reliable, de-
tailed, content-based feedback to students is a more diffi-
cult problem, however, that involves identifying individ-
ual pieces of content (Christie, 2003), sometimes called
?answer aspects? (Wiemer-Hastings et al, 1998). Previ-
ously, tutorial dialogue systems such as AUTO-TUTOR
(Wiemer-Hastings et al, 1998) and Research Methods
Tutor (Malatesta et al, 2002) have used LSA to per-
form an analysis of the correct answer aspects present
in extended student explanations. While straightfor-
ward applications of bag of words approaches such as
LSA have performed successfully on the content analy-
sis task in domains such as Computer Literacy (Wiemer-
Hastings et al, 1998), they have been demonstrated to
perform poorly in causal domains such as research meth-
ods (Malatesta et al, 2002) and physics (Rose? et al,
2003) because they base their predictions only on the
words included in a text and not on the functional rela-
tionships between them. Key phrase spotting approaches
such as (Christie, 2003) fall prey to the same problem. A
hybrid rule learning approach to classification involving
both statistical and symbolic features has been shown to
perform better than LSA and Naive Bayes classification
(McCallum and Nigam, 1998) for content analysis in the
physics domain (Rose? et al, 2003). Nevertheless, trained
approaches such as this perform poorly on low-frequency
classes and can be too coarse grained to provide enough
information to the system for it to provide the kind of
detailed feedback human tutors offer students (Lepper et
al., 1993) unless an extensive hierarchy of classes that
represent subtle differences in content is used (Popescue
et al, 2003). Popescue et al (2003) present impressive
results at using a symbolic classification approach involv-
ing hand-written rules for performing a detailed assess-
ment of student explanations in the Geometry domain.
Rule based approaches have also shown promise in non-
educational domains. For example, an approach to adapt-
ing the generic rule based MACE system for informa-
tion extraction has achieved an F-measure of 82.2% at
the ACE task (Maynard et al, 2002). Authoring tools for
speeding up and simplifying the task of writing symbolic
rules for assessing the content in student essays would
make it more practical to take advantage of the benefits
of rule based assessment approaches.
3 Carmel-Tools Interpretation Framework
Sentence: During the fall of the elevator the man and
the keys have the same constant downward acceleration
that the elevator has.
Predicate Language Representation:
((rel-time id0 id1 id2 equal)
(body-state id1 elevator freefall)
(and id2 id3 id4)
(rel-value id3 id5 id7 equal)
(rel-value id4 id6 id6 equal)
(acceleration id5 man down constant non-zero)
(acceleration id6 keys down constant non-zero)
(acceleration id7 elevator down constant non-zero))
Gloss: The elevator is in a state of freefall at the same
time when there is an equivalence between the elevator?s
acceleration and the constant downward nonzero
acceleration of both the man and the keys
Figure 2: Example of how deep syntactic analysis facili-
tates uncovering complex relationships encoded syntacti-
cally within a sentence
One of the goals behind the design of Carmel-Tools is
to leverage off of the normalization over surface syntac-
tic variation that deep syntactic analysis provides. While
our approach is not specific to a particular framework for
deep syntactic analysis, we have chosen to build upon
the publicly available LCFLEX robust parser (Rose? et al,
2002), the CARMEL grammar and semantic interpreta-
tion framework (Rose?, 2000), and the COMLEX lexicon
(Grishman et al, 1994). This same broad coverage, do-
main general interpretation framework has already been
used in a number of educational applications including
(Zinn et al, 2002; VanLehn et al, 2002).
Syntactic feature structures produced by the CARMEL
grammar normalize those aspects of syntax that modify
the surface realization of a sentence but do not change
its deep functional analysis. These aspects include tense,
negation, mood, modality, and syntactic transformations
such as passivization and extraction. Thus, a sentence
and it?s otherwise equivalent passive counterpart would
be encoded with the same set of functional relationships,
but the passive feature would be negative for the active
version of the sentence and positive for the passive ver-
sion. A verb?s direct object is assigned the obj role re-
gardless of where it appears in relation to the verb. Fur-
thermore, constituents that are shared between more than
one verb, for example a noun phrase that is the object of
a verb as well as the subject of a relative clause modi-
fier, will be assigned both roles, in that way ?undoing?
the relative clause extraction. In order to do this analy-
sis reliably, the component of the grammar that performs
the deep syntactic analysis of verb argument functional
relationships was generated automatically from a feature
representation for each of 91 of COMLEX?s verb subcat-
egorization tags (Rose? et al, 2002). Altogether there are
519 syntactic configurations of a verb in relation to its ar-
guments covered by the 91 subcategorization tags, all of
which are covered by the CARMEL grammar.
CARMEL provides an interface to allow semantic in-
terpretation to operate in parallel with syntactic interpre-
tation at parse time in a lexicon driven fashion (Rose?,
2000). Domain specific semantic knowledge is encoded
declaratively within a meaning representation specifica-
tion. Semantic constructor functions are compiled au-
tomatically from this specification and then linked into
lexical entries. Based on syntactic head/argument rela-
tionships assigned at parse time, the constructor func-
tions enforce semantic selectional restrictions and assem-
ble meaning representation structures by composing the
meaning representation associated with the constructor
function with the meaning representation of each of its
arguments. After the parser produces a semantic feature
structure representation of the sentence, predicate map-
ping rules then match against that representation in or-
der to produce a predicate language representation in the
style of Davidsonian event based semantics (Davidson,
1967; Hobbs, 1985), as mentioned above. The predicate
mapping stage is the key to the great flexibility in repre-
sentation that Carmel-Tools is able to offer. The mapping
rules perform two functions. First, they match a feature
structure pattern to a predicate language representation.
Next, they express where in the feature structure to look
for the bindings of the uninstantiated variables that are
part of the associated predicate language representation.
Because the rules match against feature structure patterns
and are thus above the word level, and because the pred-
icate language representations associated with them can
be arbitrarily complex, the mapping process is decompo-
sitional in manner but is not constrained to rigidly follow
the structure of the text.
Figure 2 illustrates the power in the pairing between
deep functional analyses and the predicate language rep-
resentation. The deep syntactic analysis of the sentence
makes it possible to uncover the fact that the expression
?constant downward acceleration? applies to the acceler-
ation of all three entities mentioned in the sentence. The
coordination in the subject of the sentence makes it pos-
sible to infer that both the acceleration of the man and of
the keys are individually in an equative relationship with
the acceleration of the elevator. The identification to-
ken of the and predicate allows the whole representation
of the matrix clause to be referred to in the rel-time
predicate that represents the fact that the equative rela-
tionships hold at the same time as the elevator is in a state
Figure 3: Predicate Language Definition Page
Figure 4: Example Map Page
of freefall. But individual predicates, each representing
a part of the meaning of the whole sentence, can also be
referred to individually if desired using their own identi-
fication tokens.
4 Carmel-Tools Authoring Process
The purpose of Carmel-Tools is to insulate the author
from the details of the underlying domain specific knowl-
edge sources. If an author were building knowledge
sources by hand for this framework, the author would be
responsible for building an ontology for the semantic fea-
ture structure representation produced by the parser, link-
ing pointers into this hierarchy into entries in the lexicon,
and writing predicate mapping rules. With Carmel-Tools,
the author never has to deal directly with these knowledge
sources. The Carmel-Tools authoring process involves
designing a Predicate Language Definition, augmenting
the base lexical resources by either loading raw human
tutoring corpora or entering example texts by hand, and
annotating example texts with their corresponding rep-
resentation in the defined Predicate Language Defini-
tion. From this authored knowledge, CARMEL?s se-
mantic knowledge sources can be generated and com-
piled. The knowledge source inference algorithm ensures
that knowledge coded redundantly across multiple exam-
ples is represented only once in the compiled knowledge
sources. The authoring interface allows the author or au-
thors to test the compiled knowledge sources and then
continue the authoring process by updating the Predicate
Language Definition, loading additional corpora, anno-
tating additional examples, or modifying already anno-
tated examples.
The Carmel-Tools authoring process was designed to
eliminate the most time-consuming parts of the authoring
Figure 5: Text Annotation Page Page
process. In particular, its GUI interface guides authors in
such a way as to prevent them from introducing inconsis-
tencies between knowledge sources, which is particularly
crucial when multiple authors work together. For exam-
ple, a GUI interface for entering propositional representa-
tions for example texts insures that the entered represen-
tation is consistent with the author?s Predicate Language
Definition. Compiled knowledge sources contain point-
ers back to the annotated examples that are responsible
for their creation. Thus, it is also able to provide trou-
bleshooting facilities to help authors track down potential
sources for incorrect analyses generated from compiled
knowledge sources. When changes are made to the Pred-
icate Language Definition, Carmel-Tools tests whether
each proposed change would cause conflicts with any an-
notated example texts. An example of such a change
would be deleting an argument from a predicate type
where some example has as part of its analysis an instan-
tiation of a predicate with that type where that argument
is bound. If so, it lists these example texts for the author
and requires the author to modify the annotated examples
first in such a way that the proposed change will not cause
a conflict, in this case that would mean uninstantiating the
variable that the author desires to remove. In cases where
changes would not cause any conflict, such as adding an
argument to a predicate type, renaming a predicate, to-
ken, or type, or removing an argument that is not bound
in any instantiated proposition, these changes are made
throughout the database automatically.
4.1 Defining the Predicate Language Definition
The author begins the authoring process by designing
the propositional language that will be the output repre-
sentation from CARMEL using the authored knowledge
sources. This is done on the Predicate Language Defi-
nition page of the Carmel-Tools interface, displayed in
Figure 3. The author is free to develop a representation
language that is as simple or complex as is required by the
type of reasoning, if any, that will be applied to the output
representations by the tutoring system as it formulates its
response to the student?s natural language input.
The interface includes facilities for defining a list of
predicates and Tokens to be used in constructing proposi-
tional analyses. Each predicate is associated with a basic
predicate type, which is a associated with a list of argu-
ments. Each basic predicate type argument is itself asso-
ciated with a type that defines the range of atomic values,
which may be tokens or identifier tokens referring to in-
stantiated predicates, that can be bound to it. Thus, to-
kens also have types. Each token has one or more basic
token types. Besides basic predicate types and basic to-
ken types, we also allow the definition of abstract types
that can subsume other types.
4.2 Generating Lexical Resources and Annotating
Example Sentences
When the predicate language definition is defined, the
next step is to generate the domain specific lexical re-
sources and annotate example sentences with their corre-
sponding representation within this defined predicate lan-
guage. The author begins this process on the Example
Map Page, displayed in Figure 4.
Carmel-Tools provides facilities for loading a raw hu-
man tutoring corpus file. Carmel-Tools then makes a list
of each unique morpheme it finds in the file and then aug-
ments both its base lexicon (using entries from COM-
LEX), in order to include all morphemes found in the
transcript file that were not already included in the base
lexicon, and the spelling corrector?s word list, so that it
includes all morphological forms of the new lexical en-
tries. It also segments the file into a list of student sen-
tence strings, which are then loaded into a Corpus Ex-
amples list, which appears on the right hand side of the
interface. Searching and sorting facilities are provided to
make it easy for authors to find sentences that have cer-
tain things in common in order to organize the list of sen-
tences extracted from the raw corpus file in a convenient
way. For example, a Sort By Similarity button
causes Carmel-Tools to sort the list of sentences accord-
ing to their respective similarity to a given text string ac-
cording to an LSA match between the example string and
each corpus sentence. The interface also includes the To-
ken List and the Predicate List, with all defined tokens
and predicates that are part of the defined predicate lan-
guage. When the author clicks on a predicate or token,
the Examples list beside it will display the list of anno-
tated examples that have been annotated with an analysis
containing that token or predicate.
Figure 5 displays how individual texts are annotated.
The Analysis box displays the propositional representa-
tion of the example text. This analysis is constructed
using the Add Token, Delete, Add Predicate,
and Modify Predicate buttons, as well as their sub-
windows, which are not shown. Once the analysis is en-
tered, the author may indicate the compositional break-
down of the example text by associating spans of text
with parts of the analysis by means of the Optional
Match and Mandatory Match buttons. For exam-
ple, the noun phrase ?the man? corresponds to the man
token, which is bound in two places. Each time a match
takes place, the Carmel-Tools internal data structures cre-
ate one or more templates that show how pieces of syn-
tactic analyses corresponding to spans of text are matched
up with their corresponding propositional representation.
From this match Carmel-Tools infers both that ?the man?
is a way of expressing the meaning of the man token in
text and that the subject of the verb hold can be bound to
the ?body1 argument of the become predicate. By de-
composing example texts in this way, Carmel-Tools con-
structs templates that are general and can be reused in
multiple annotated examples. It is these created templates
that form the basis for all compiled semantic knowledge
sources. Thus, even if mappings are represented redun-
dantly in annotated examples, they will not be repre-
sented redundantly in the compiled knowledge sournces.
The list of templates that indicates the hierarchical break-
down of this example text are displayed in the Templates
list on the right hand side of Figure 5. Note that while the
author matches spans to text to portions of the meaning
representation, the tool stores mappings between feature
structures and portions of meaning representation, which
is a more general mapping.
Templates can be generalized by entering paraphrases
for portions of template patterns. Internally what this ac-
complishes is that all paraphrases listed can be interpreted
by CARMEL as having the same meaning so that they
can be treated as interchangeable in the context of this
template. A paraphrase can be entered either as a specific
string or as a Defined Type, including any type defined
in the Predicate Language Definition. What this means is
that the selected span of text can be replaced by any span
of text that can be interpreted in such a way that its pred-
icate representation?s type is subsumed by the indicated
type.
4.3 Compiling Knowledge Sources
Each template that is created during the authoring pro-
cess corresponds to one or more elements of each of
the required domain specific knowledge sources, namely
the ontology, the lexicon with semantic pointers, and the
predicate mapping rules. Using the automatically gener-
ated knowledge sources, most of the ?work? for mapping
a novel text onto its predicate language representation is
done either by the deep syntactic analysis, where a lot of
surface syntactic variation is factored out, and during the
predicate mapping phase, where feature structure patterns
are mapped onto their corresponding predicate language
representations. The primary purpose of the sentence
level ontology that is used to generate a semantic fea-
ture structure at parse time is primarily for the purpose of
limiting the ambiguity produced by the parser. Very little
generalization is obtained by the semantic feature struc-
tures created by the automatically generated knowledge
sources over that provided by the deep syntactic analysis
alone. By default, the automatically generated ontology
contains a semantic concept corresponding to each word
appearing in at least one annotated example. A semantic
pointer to that concept is then inserted into all lexical en-
tries for the associated word that were used in one of the
annotated examples. An exception occurs where para-
phrases are entered into feature structure representations.
In this case, a semantic pointer is entered not only into the
entry for the word from the sentence, but also the words
from the paraphrase list, allowing all of the words in the
paraphrase list to be treated as equivalent at parse time.
The process is a bit more involved in the case of verbs.
In this case it is necessary to infer based on the parses of
the examples where the verb appears which set of sub-
categorization tags are consistent, thus limiting the set
of verb entries for each verb that will be associated with
a semantic pointer, and thus which entries can be used
at parse time in semantic interpretation mode. Carmel-
Tools makes this choice by considering both which argu-
ments are present with that verb in the complete database
of annotated examples as well as how the examples were
broken down at the matching stage. All non-extracted
arguments are considered mandatory. All extracted argu-
ments are considered optional. Each COMLEX subcat
tag is associated with a set of licensed arguments. Thus,
subcat tags are considered consistent if the set of licensed
arguments contains at least all mandatory arguments and
doesn?t license any arguments that are not either manda-
tory or optional. Predicate mapping rules are generated
for each template by first converting the corresponding
syntactic feature structure into the semantic representa-
tion defined by the automatically generated ontology and
lexicon with semantic pointers. Predicate mapping rules
are then created that map the resulting semantic feature
structure into the associated predicate language represen-
tation.
5 Evaluation
A preliminary evaluation was run for the physics domain.
We used for our evaluation a corpus of essays written by
students in response to 5 simple qualitative physics ques-
tions such as ?If a man is standing in an elevator hold-
ing his keys in front of his face, and if the cable hold-
ing the elevator snaps and the man then lets go of the
keys, what will be the relationship between the position
of the keys and that of the man as the elevator falls to the
ground? Explain why.? A predicate language definition
was designed consisting of 40 predicates, 31 predicate
types, 160 tokens, 37 token types, and 15 abstract types.
The language was meant to be able to represent phys-
ical objects mentioned in our set of physics problems,
body states (e.g., freefall, contact, non-contact), quanti-
ties that can be measured (e.g., force, velocity, acceler-
ation, speed, etc.), features of these quantities (e.g., di-
rection, magnitude, etc.), comparisons between quanti-
ties (equivalence, non-equivalence, relative size, relative
time, relative location), physics laws, and dependency re-
lations. An initial set of 250 example sentences was then
annotated, including sentences from each of a set of 5
physics problems.
Next a set of 202 novel test sentences, each between 4
and 64 words long, was extracted from the corpus. Since
comparisons, such as between the accelerations of objects
in freefall together, are important for the reasoning in all
of the questions used for corpus collection, we focused
the coverage evaluation specifically on sentences pertain-
ing to comparisons, such as in Figures 1 and 2. The goal
of the evaluation was to test the extent to which knowl-
edge generated from annotated examples generalizes to
novel examples.
Since obtaining the correct predicate language repre-
sentation requires obtaining a correct syntactic parse, we
first evaluated CARMEL?s syntactic coverage over the
corpus of test sentences to obtain an upper bound for ex-
pected performance. We assigned the syntactic interpre-
tation of each sentence a score of None, Bad, Partial, or
Acceptable. A grade of None indicates that no interpreta-
tion was built by the grammar. Bad indicates that parses
were generated, but they contained errorfull functional
relationships between constituents. Partial indicates that
no parse was generated that covered the entire sentence,
ut the portions that were completely correct for at least
one interpretation of the sentence. Acceptable indicates
that a complete parse was built that contained no incor-
rect functional relationships. If any word of the sentence
was not covered, it was one that would not change the
meaning of the sentence. For example, ?he had the same
velocity as you had? is the same as ?he had the same ve-
locity as you?, so if ?did? was not part of the final parse
but other than that, the parse was fine, it was counted as
Acceptable. Overall the coverage of the grammar was
very good. 166 sentences were graded Acceptable, which
is about 83% of the corpus. 8 received a grade of Partial,
26 Bad, and 1 None.
We then applied the same set of grades to the quality of
the predicate language output. Note that that the grade as-
signed to an analysis represents the correctness and com-
pleteness of the predicate representation the system ob-
tained for that sentence. In this case, a grade of Accept-
able meant that all aspects of intended meaning were ac-
counted for, and no misleading information was encoded.
Partial indicated that some non-trivial part of the intended
meaning was communicated. Any interpretation contain-
ing any misleading information was counted as Bad. If
no predicate language representation was returned, the
sentence was graded as None. As expected, grades for
semantic interpretation were not as high as for syntactic
analysis. In particular, 107 were assigned a grade of Ac-
ceptable, 45 were assigned a grade of Partial, 36 were
assigned a grade of Bad, and 14 received a nil interpre-
tation. Our evaluation demonstrates that knowledge gen-
erated from annotated examples can be used to interpret
novel sentences, however, there are still gaps in the cov-
erage of the automatically generated knowledge sources
that need to be filled in with new annotated examples.
Furthermore, the small but noticeable percentage of bad
interpretations indicates that some previously annotated
examples need to be modified in order to prevent these
bad interpretations from being generated.
6 Current Directions
In this paper we have introduced Carmel-Tools, a tool
set for quick authoring of semantic knowledge sources.
Our evaluation demonstrates that the semantic knowledge
sources inferred from examples annotated using Carmel-
Tools generalize to novel sentences. We are continuing
to work to enhance the ability of Carmel-Tools to learn
generalizable knowledge from examples as well as to im-
prove the user friendliness of the interface.
References
J. Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,
L. Braden-Harder, and M. D. Harris. 1998. Au-
tomated scoring using a hybrid feature identification
technique. In Proceedings of COLING-ACL?98, pages
206?210.
M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, and
R. G. Hausmann. 2001. Learning from human tutor-
ing. Cognitive Science, (25):471?533.
J. Christie. 2003. Automated essay marking for content:
Does it work? In CAA Conference Proceedings.
H. Cunningham, D. Maynard, and V. Tablan. 2000. Jape:
a java annotations patterns engine. Institute for Lan-
guage, Speach, and Hearing, University of Sheffield,
Tech Report CS-00-10.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2003. Gate: an architecture for develop-
ment of robust hlt applications. In Recent Advanced in
Language Processing.
D. Davidson. 1967. The logical form of action sentences.
In N. Rescher, editor, The Logic of Decision and Ac-
tion.
P. W. Foltz, W. Kintsch, and T. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse Processes, 25(2-3):285?307.
R. Grishman, C. Macleod, and A. Meyers. 1994. COM-
LEX syntax: Building a computational lexicon. In
Proceedings of the 15th International Conference on
Computational Linguistics (COLING-94).
J. R. Hobbs. 1985. Ontological promiscuity. In Proceed-
ings of the 23rd Annual Meeting of the Association for
Computational Linguistics.
D. Jay, J. Aberdeen, L. Hirschman, R. Kozierok,
P. Robinson, and M. Vilain. 1997. Mixed-initiative
development of language processing systems. In Fifth
Conference on Applied Natural Language Processing.
M. Lepper, M. Woolverton, D. Mumme, and J. Gurtner.
1993. Motivational techniques of expert human tutors:
Lessons for the design of computer based tutors. In
S. P. Lajoie and S. J. Derry, editors, Computers as Cog-
nitive Tools, pages 75?105.
D. Lonsdale and D. Strong-Krause. 2003. Automated
rating of esl essays. In Proceedings of the HLT-NAACL
2003 Workshop: Building Educational Applications
Using Natural Language Processing.
K. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the short answer question with research
methods tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference.
D. Maynard, K. Bontcheva, and H. Cunningham. 2002.
Towards a semantic extraction of named entities. In
40th Anniversary meeting of the Association for Com-
putational Linguistics.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Classification.
O. Popescue, V. Aleven, and K. Koedinger. 2003. A
kowledge based approach to understanding students?
explanations. In Proceedings of the AI in Education
Workshop on Tutorial Dialogue Systems: With a View
Towards the Classroom.
C. P. Rose?, D. Bhembe, A. Roque, and K. VanLehn.
2002. An efficient incremental architecture for ro-
bust interpretation. In Proceedings of the Human Lan-
guages Technology Conference, pages 307?312.
C. P. Rose?, A. Roque, D. Bhembe, and K VanLehn. 2003.
A hybrid text classification approach for analysis of
student essays. In Proceedings of the HLT-NAACL
2003 Workshop: Building Educational Applications
Using Natural Language Processing.
C. P. Rose?. 2000. A framework for robust semantic in-
terpretation. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 311?318.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural Lan-
guag e Tutoring Group. 2002. The architecture of
why2-atlas: a coach for qualitative physics essay writ-
ing. In Proceedings of the Intelligent Tutoring Systems
Conference, pages 159?167.
Y. Wang and A. Acero. 2003. Concept acquisition in
example-based grammar authoring. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing.
P. Wiemer-Hastings, A. Graesser, D. Harter, and the Tu-
toring Res earch Group. 1998. The foundations
and architecture of autotutor. In B. Goettl, H. Halff,
C. Redfield, and V. Shute, editors, Intelligent Tutor-
ing Systems: 4th International Conference (ITS ?98 ),
pages 334?343. Springer Verlag.
C. Zinn, J. D. Moore, and M. G. Core. 2002. A 3-
tier planning architecture for managing tutorial dia-
logue. In Proceedings of the Intelligent Tutoring Sys-
tems Conference, pages 574?584.
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 45?52, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Towards a Prototyping Tool for Behavior Oriented Authoring of  
Conversational Agents for Educational Applications 
 
 
Gahgene Gweon, Jaime Arguello, Carol Pai, Regan Carey, Zachary 
Zaiss, Carolyn Ros? 
Human-Computer Interaction Institute/ Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue, Pittsburgh, PA 15213 USA 
Ggweon,jarguell,cpai,rcarey,zzaiss,cp3a@andrew.cmu.edu 
 
   
Abstract 
Our goal is to develop tools for facili-
tating the authoring of conversational 
agents for educational applications, and 
in particular to enable non-
computational linguists to accomplish 
this task efficiently.  Such a tool would 
benefit both learning researchers, al-
lowing them to study dialogue in new 
ways, and educational technology re-
searchers, allowing them to quickly 
build dialogue based help systems for 
tutoring systems. We argue in favor of 
a user-centered design methodology.  
We present our work-in-progress de-
sign for authoring, which is motivated 
by our previous tool development ex-
periences and preliminary contextual 
interviews and then refined through 
user testing and iterative design.   
1 Introduction 
This paper reports work in progress towards 
developing TuTalk, an authoring environment 
developed with the long term goal of enabling 
the authoring of effective tutorial dialogue 
agents.  It was designed for developers without 
expertise in knowledge representation, artificial 
intelligence, or computational linguistics.  In our 
previous work we have reported progress to-
wards the development of authoring tools spe-
cifically focusing on robust language 
understanding capabilities (Ros? et al, 2003; 
Ros? & Hall, 2004; Ros?, et al, 2005).  In this 
paper, we explore issues related to authoring 
both at the dialogue and sentence level, as well 
as the interaction between these two levels of 
authoring.  Some preliminary work on the un-
derlying architecture is reported in (Jordan, Ro-
s?, & VanLehn, 2001; Aleven & Ros?, 2004; 
Ros? & Torrey, 2004).  In this paper we focus 
on the problem of making this computational 
linguistics technology accessible to our target 
user population.   
We are developing the TuTalk authoring en-
vironment in connection with a number of exist-
ing local research projects related to educational 
technology in general and tutorial dialogue in 
particular.  It is being developed primarily for 
use within the Pittsburgh Sciences of Learning 
Center (PSLC) data shop, which includes devel-
opment efforts for a suite of authoring tools to 
be used for building the infrastructure for 7 dif-
ferent computer enhanced courses designated as 
LearnLab courses.  These LearnLab courses, 
which are conducted within local secondary 
schools as well as universities, and which in-
clude Chinese, French, English as a Second 
Language, Physics, Algebra, Geometry, and 
Chemistry, involve heavy use of technology 
both for the purpose of supporting learning as 
well as for the purpose of conducting learning 
research in a classroom setting.  Other local pro-
jects related to calculus and thermodynamics 
45
tutoring also have plans to use TuTalk.  In this 
paper we will discuss specifically how we have 
used corpora related to ESL, physics, thermody-
namics, and calculus in our development effort. 
To support this multi-domain effort, it is es-
sential that the technology we develop be do-
main independent and usable by a non-technical 
user population, or at least a user population not 
possessing expertise in knowledge representa-
tion, artificial intelligence, or computational lin-
guistics.  Thus, we are employing a corpus based 
methodology that bootstraps domain specific 
authoring using examples of desired conversa-
tional behavior for the domain. 
2 A Historical Perspective 
While a focus on design based on standards 
and practices from human-computer interaction 
community have not received a great deal of 
attention in previously published tool develop-
ment efforts known to the computational linguis-
tics community, our experience tells us that 
insufficient attention to these details leads to the 
development of tools that are unusable, particu-
larly to the user population that we target with 
our work. 
Some desiderata related to the design of our 
system are obvious based on our target user 
population.  Currently, many educational tech-
nology oriented research groups do not have 
computational linguists on their staff with the 
expertise required to author domain specific 
knowledge sources for use with sophisticated 
state-of-the-art understanding systems, such as 
CARMEL (Ros?, 2000) or TRIPS (Allen et al, 
2001). However, previous studies have shown 
that, while scaffolding and guidance is required 
to support the authoring process, non-
computational linguists possess many of the ba-
sic skills required to author conversational inter-
faces (Ros?, Pai, & Arguello, 2005). Because the 
main barrier of entry to such sophisticated tools 
are expertise in understanding the underlying 
data structures and linguistically motivated rep-
resentation, our tools should have an interface 
that masks the unnecessary details and provides 
intuitive widgets that manipulate the data in 
ways that are consistent with the mental models 
the users bring with them to the authoring proc-
ess.  In order to be maximally accessible to de-
velopers of educational technology, the system 
should involve minimal programming.   
The design of Carmel-Tools (Ros? et al, 
2003; Ros? & Hall, 2004), the first generation of 
our authoring tools, was based on these obvious 
desiderata and not on any in-depth analysis of 
data collected from our target user population.  
While an evaluation of the underlying computa-
tional linguistics technology showed promise 
(Ros? & Hall, 2004), the results from actual au-
thoring use were tremendously disappointing.  
A formal study reported in (Ros?, et al, 2005) 
demonstrates that even individuals with exper-
tise in computational linguistics have difficulty 
predicting the coverage of knowledge sources 
that would be generated automatically from ex-
ample texts annotated with desired representa-
tions. Informal user studies involving actual use 
of Carmel-Tools then showed that a conse-
quence of this lack of ability is that authors were 
left without a clear strategy for moving through 
their corpus.  As a result, time was lost from an-
notating examples that did not yield the maxi-
mum amount of new knowledge in the generated 
knowledge sources.  Furthermore, since authors 
tended not to test the generated knowledge 
sources as they were annotating examples, errors 
were difficult for them to track later, despite fa-
cilities designed to help them with that task.   
Another finding from our user studies was 
that although the interface prevented authors 
from violating the constraints they designed into 
their predicate language, it did not keep authors 
from annotating similar texts with very different 
representations, thus introducing a great deal of 
spurious ambiguity.  Thus, they did not naturally 
maintain consistency in their application of their 
own designed meaning representation languages 
across example texts.  An additional problem 
was that authors sometimes decomposed exam-
ples in ways that lead to overly general rules, 
which then lead to incorrect analyses when these 
rules matched inappropriate examples.   
These disappointing results convinced us of 
the importance of taking a user-centered design 
approach to our authoring interface redesign 
process. 
 
 
46
3 Preliminary Design Intents from 
Contextual Interviews 
The core essence of the user-centered design 
approach is designing from data rather than from 
preconceived notions of what will be useful and 
what will work well.  Expert blind spots often 
lead to designs based on intuitions that overlook 
needs or overly emphasize issues that are not 
centrally important (Koedinger & Nathan, 2004; 
Nathan & Koedinger, 2000).  Contextual inquiry 
is used at an early stage in the user-centered de-
sign process to collect the foundational data on 
which to build a design (Beyer and Holtzbatt, 
2000). Contextual Inquiry is a popular method 
developed within the Human Computer Interac-
tion community where the design team gathers 
data from end users while watching what the 
users do in context of their work. Contextual 
interviews are used to illuminate these observa-
tions by engaging end-users in interviews in 
which they show specific instances within their 
work life that are relevant for the design process.  
These methods help define requirements as well 
as plan and prioritize important aspects of func-
tionality.  At the same time, the system design-
ers get a chance to gain insights about the users? 
environment, tasks, cultural influences and diffi-
culties in the current processes.  
Many aspects of the Tutalk tool were de-
signed based on contextual inquiry (CI) data. 
The design team conducted five CIs with users 
who have experience in using existing authoring 
tools such as Carmel-Tools (Ros? & Hall, 2004). 
The design team leader also spent one week ob-
serving novice tool users working with the cur-
rent set of tools at an Intelligent Tutoring 
Summer School.  Here we will discuss some 
findings from those CIs and observations and 
how they motivated some general design intents, 
which we flesh out later in the paper.  
A common pattern we observed in our CIs 
was that having different floating windows for 
different tasks fills up the computer screen rela-
tively quickly and confuses authors as to where 
they are in the process of authoring.  The TuTalk 
design addresses this observed problem by an-
choring the main window and switching only the 
components of the window as needed.  A stan-
dard logic for layout and view switching helps 
authors know what to expect in different con-
texts.  Placement of buttons in TuTalk is consis-
tently near the textboxes that they control, and a 
bounding box is drawn around related sets of 
controls so that the user does not get lost trying 
to figure out where the buttons are or what they 
are for.   
We observed that authors needed to refer to 
cheat sheets and user documentation to use their 
current tools effectively and that different users 
did not employ the same terminology to refer to 
similar functionality, which made communica-
tion difficult.  Furthermore, their current suites 
of tools were not designed as one integrated en-
vironment.  Thus, a lot of shuffling of files from 
one directory to another was required in order to 
complete the authoring process.  Users without 
Unix operating system experience found this 
especially confusing.  Our goal is to require only 
very minimal documentation that can be ob-
tained on-line in the context of use.   
TuTalk is a single, integrated environment 
that makes use of GUI widgets for actions rather 
then requiring any text-based commands or file 
system activity.  In this way we hope to avoid 
requiring the users to use a manual or a ?cheat-
sheet? reference for the commands they forget. 
As is common practice, TuTalk also uses consis-
tent labels throughout the interface to promote 
understandability and communication with tool 
developers as well as other dialogue system de-
velopers. 
4 Exploring the User?s Mental Model 
through User Studies 
As an additional way of gaining insights into 
what sort of interface would make the process of 
authoring conversational interfaces accessible, 
we conducted a small, exploratory user study in 
which we examined how members of our target 
user population think about the structure of lan-
guage.   
Two groups of college-level participants with 
no deep linguistics training were asked to read 
three transcribed conversations about ordering 
from a menu at a restaurant from our English as 
a Second Language corpus.  The three specific 
restaurant dialogues were chosen because of 
their breadth of topic coverage and richness in 
linguistic expression.  Participants were asked to 
perform tasks with these dialogues to mimic 
47
three levels of conversational interface author-
ing: 
 
Macro Organization Tasks (dialogue level) 
Level 1. How authors understand, seg-
ment, and organize dialogue topics 
Level 2.  How authors generalize across 
dialogues as part of constructing a 
?model? script 
Micro Organization Task (sentence level) 
Level 3.  How authors categorize and 
decompose sentences within these dia-
logues 
 
The first group (Group A, five participants) 
was asked to perform Macro Organization Tasks 
before processing sentences for the Micro Or-
ganization Tasks.  The second group (Group B, 
four participants) was asked to perform these 
sets of tasks in the opposite order. 
Our findings for the Macro Organization 
Tasks showed that participants effectively broke 
down dialogues into segments that reflected in-
tuitive breaks in the conversation.  These topics 
were then organized into semantically related 
categories.  Although participants were not ex-
plicitly instructed on how to organize the topics, 
every participant used spatial proximity as a rep-
resentation for semantic relatedness. Another 
finding was the presence of primacy effects in 
the ?model? restaurant scripts they were asked to 
construct. These scripts were heavily influenced 
by the first dialogue read. As a result, important 
topics that surfaced in the other two dialogues 
were omitted from the model scripts. 
Furthermore, we found that participants in 
Group B took much longer in completing the 
Micro Organization Task (35-40 minutes as op-
posed to 25-30 minutes) without performing the 
Macro Organization Tasks first. In general, we 
found that participants clustered sentences based 
on surface characteristics rather than creating 
ontologically similar classes that would be more 
useful from a system development perspective. 
In a follow-up study we are exploring ways of 
guiding users to cluster sentences in ways that 
are more useful from a system building perspec-
tive. 
Our preliminary findings show that getting an 
overall sense of the corpus facilitates micro-
level organization. This is hindered by two fac-
tors:  First, primacy effects interfere with macro-
level comprehension. Second, system developers 
struggle to strategically select portions of their 
corpus on which to focus their initial efforts.  
5 Stage One: Corpus Organization 
While existing tools from our previous work 
required authors to organize their corpus data 
prior to their interaction with the tools, both our 
contextual research and user studies indicated 
that support for organizing corpus data prior to 
authoring is important.   
In light of this concern, the TuTalk authoring 
process consists of three main stages.  Corpus 
collection, corpus data organization through 
what we call the InfoMagnet interface, and au-
thoring propper. First, a corpus is collected by 
asking users to engage in conversation using 
either a typed or spoken chat interface. In the 
case of spoken input, the speech is then tran-
scribed into textual form. Second, the raw cor-
pus data is automatically preprocessed for 
display and interactive organization using the 
InfoMagnet interface.  As part of the preprocess-
ing, dialogue protocols are segmented automati-
cally at topic boundaries, which can be adjusted 
by hand later during authoring propper.  The 
topic oriented segments are then clustered semi-
automatically into topic based classes. The out-
put from this stage is an XML file where dia-
logue segments are reassembled into their 
original dialogue contexts, with each utterance 
labeled by topic. This XML file is finally passed 
onto the authoring environment propper, which 
is then used for finer grained processing, such as 
shifting topic segment boundaries and labeling 
more detailed utterance functionality.   
Our design is for knowledge sources that are 
runable from our dialogue system engine to be 
generated directly from the knowledge base cre-
ated during the fine-grained authoring process as 
in Carmel-Tools (Ros? & Hall, 2004), however 
currently our focus is on iterative development 
of a prototype of the authoring interaction de-
sign.  Thus, more work is required to create the 
final end-to-end implementation.  In this section 
we focus on the design of the corpus collection 
and organization part of the authoring process. 
 
48
5.1 Corpus Collection  
An important part of our mission is developing 
technology that can use collected and automati-
cally pre-processed corpus data to guide and 
streamline the authoring process. Prior to the 
arduous process of organizing and extracting 
meaningful data, a corpus must be collected.  
As part of the PSLC and other local tutorial 
dialogue efforts we have collected corpus data 
from multiple domains that we have made use of 
in our development process. In particular, we 
have been working with data collected in con-
nection with the PSLC Physics and English as a 
Second Language LearnLab courses as well as 
local Calculus and Thermodynamics tutoring 
projects.  Currently we have physics tutoring 
data primarily from one physics tutor (interac-
tions with 40 students), thermodynamics data 
from four different tutors (interactions with 27 
students), Calculus data from four different tu-
tors (84 dialogues), and ESL dialogues collected 
from 15 pairs of students (30 dialogues alto-
gether).  
While we have drawn upon data from all of 
these domains for testing the underlying lan-
guage processing technology for our develop-
ment effort, for our user studies we have so far 
mainly drawn upon our ESL corpus, which in-
cludes conversations between students about 
every-day tasks such as ordering from a restau-
rant or about their pets.  We chose the language 
ESL data for our initial user tests because we 
expected it to be easy for a general population to 
relate to, but we plan to begin using calculus 
data as well.   
5.2 InfoMagnets Interface 
As mentioned previously, once the raw dia-
logue corpus is collected, the next step is to sift 
through this data and assign utterances (or 
groups of utterances) to classes conceptualized 
by the author. Clustering is a natural step in this 
kind of exploratory data analysis, as it promotes 
learning by grouping and generalizing from 
what we know about some of the objects in a 
cluster. For this purpose we have designed the 
InfoMagnets interface, which introduces a non-
technical metaphor to the task of iterative docu-
ment clustering. The InfoMagnets interface was 
designed to address the problems identified in 
the user study discussed above in Section 4.  
Specifically, we expected that those problems 
could be addressed with an interface that:  
1. Divides dialogues into topic based 
segments and automatically clusters 
them into conceptually similar classes 
2. Eliminates primacy effects of sequen-
tial dialogue consumption by creating an 
inclusive compilation of all dialogue 
topics 
3. Makes the topic similarity of docu-
ments easily accessible to the user  
 
The InfoMagnets interface is displayed in 
Figure 1.  The larger circles (InfoMagnets) cor-
respond to cluster centroids and the smaller ones 
(particles) correspond to actual spans of text. 
Lexical cohesion in the vector space translates 
into attraction in the InfoMagnet space. The at-
traction from each particle to each InfoMagnet is 
evident from the particle?s position with respect 
to all InfoMagnets and its reaction-time when an 
InfoMagnet is moved by the user, which causes 
the documents that have some attraction with it 
to redistribute themselves in the InfoMagnet 
space.  
 
 
Figure 1 InfoMagnets Interface 
 
Being an unsupervised learning method, clus-
tering often requires human-intervention for 
fine-tuning (e.g. removing semantically-weak 
discriminators, culling meaningless clusters, or 
deleting/splitting clusters too fine/coarse for the 
author?s purpose). The InfoMagnets interface 
provides all this functionality, while shielding 
the author from the computational details inher-
ent in these tasks 
49
Initially, the corpus is clustered using the Bi-
secting K-means Algorithm described in (Kumar 
et al, 1998).  Although this is a hard clustering 
algorithm, the InfoMagnet interface shows the 
particles association with all clusters, given by 
the position of the particle. Using a cross-hair 
lens, the author is able to view the contents of 
each cluster centroid and each particle. The au-
thor is able to select a group of particles and 
view the common features between these parti-
cles and any InfoMagnet in the space. The inter-
face allows the editing of InfoMagnets by 
adding and removing features, splitting In-
foMagnets, and removing InfoMagnets. When 
the user edits an InfoMagnets, the effect in the 
particle distribution is shown immediately and in 
an animated way.  
5.3 XML format 
The data collected from the conversations 
in .txt format are reformatted into XML format 
before being displayed with InfoMagnet tool.  
The basic XML file contains a transcription of 
the conversational data and has the following 
structure: Under the top root tag, there is <dia-
logue> tag which designates the conversion 
about a topic. It has an ?id? attribute so that we 
can keep track of each separate conversation. 
Then each sentence has a <sentence> tag with 
two attributes ?uid? and ?agent?. ?uid? is a uni-
versal id and ?agent? tells who was speaking.  
Additionally, sentences are grouped into seg-
ments, marked off with a <subtopic> tag. 
The user?s interaction with the InfoMagnet in-
terface adds a ?subtopic-name? attribute to the 
subtopic tag. Then, the authoring interface 
proper, described below, allows for further ad-
justments and additions to the xml tags.  The 
final knowledge sources will be generated from 
this XML based representation. 
6 Authoring 
The authoring environment proper consists of 
two main views, namely the authoring view and 
tutoring view. The authoring view is where the 
author designs the behavior of the conversa-
tional agent. The authoring view has two levels; 
the topic level and the subtopic level. The tutor-
ing view is what a student will be looking at 
when interacting with the conversational agent. 
Our focus here is on the Authoring view. 
Authoring View: Topic Level 
The Topic level of the authoring view allows for 
manipulating the relationship between subtopics 
as well as the definition of the subtopic. Figure 2 
shows the topic level authoring view, which 
consists of two panels. In the left, the author in-
puts the description of the task that the student 
will engage in with the agent. The author can 
specify whether the student will be typing or 
talking, the title of the topic, the task description, 
an optional picture that aids with the task (such 
as a menu or a map of a city), and a time limit.  
In the right panel of the topic level authoring 
view, the structure imposed on the data by inter-
action with the InfoMagnets interface is dis-
played in sequential form. The top section of the 
interface (figure 2, section A) has a textbox for 
specifying an xml file to read. The next section 
(figure 2, section B), ?Move / Rename Subtopic? 
displays the subtopics. The order of the subtop-
ics displayed in this section acts as a guideline 
for the agent to follow during the conversation. 
Double-clicking on a subtopic will display a 
subtopic view on the right panel. This view acts 
as a reference for the agent?s conversation 
within the subtopic and is explained in the next 
section. The author can also rearrange the order 
of subtopics by selecting a subtopic and using 
the ?>? and ?<? buttons to move the subtopic 
right or left respectively. ?x? is used to delete 
the subtopic. The author can also specify 
whether the discussion of a subtopic is required 
(displayed in red) or optional (in green) using 
the checkbox that is labeled ?required?. Clicking 
on the ?Hide Opt? button will only display the 
required subtopics. 
The last section of the right panel in topic 
level authoring view (figure 2, section C) is ti-
tled ?move subtopic divider?. A blue line de-
notes the border of the subtopic. The author can 
move the line up or down to move the boundary 
of the subtopics automatically inserted by the 
InfoMagnets interface. The author can also click 
on any part of conversation and press the ?split? 
button to split the subtopic in two sections. In 
addition, she can change the label of the sub-
topic segment using the drop down list. 
50
  
 
 
Figure 2: Topic Level Authoring View 
 
Authoring View: Subtopic Level 
While the Topic View portion of the authoring 
interface proper allows specification of which 
subtopics can occur as part of a dialogue, which 
are required and which are optional, and what 
the default ordering is, the Subtopic Level is for 
specification of the low level turn-by-turn details 
of what happens within a subtopic segment.  
This section reports early work on the design of 
this portion of the interface. 
The subtopic view displays a structure that the 
conversational agent refers to in deciding what 
its next contribution should be.  The building 
blocks from which knowledge sources for the 
dialogue engine will be generated are templates 
abstracted from example dialogue segments, 
similar to KCD specifications (Jordan, Ros?, & 
VanLehn, 2001; Ros? & Torry, 2004).  As part 
of the process of abstracting templates, each ut-
terance is tagged with its utterance type using a 
menu-based interface as in (Gweon et al, sub-
mitted).  The utterance type determines what 
would be an appropriate form for a response.  
Identifying this is meant to allow the dialogue 
manager to maintain coherence in the emerging 
dialogue.  Users may also trim out undesired 
portions of text from the actual example frag-
ments in abstracting out templates to be used for 
generating knowledge sources. 
Each utterance type has sets of template re-
sponse types associated with them. The full set 
of utterance types includes Open questions, 
Closed questions, Understanding check ques-
tions, Assertions, Commands/Requests, Ac-
knowledgements, Acceptances, and Rejections. 
The templates will not be used in their authored 
form.  Instead, they will be used to generate 
knowledge sources in the form required by the 
backend dialogue system as in (Ros? & Hall, 
2004), although this is still work in progress.  
Each template is composed of one or more ex-
changes during which the speaker who initiated 
the segment maintains conversational control. If 
control shifts to the other speakers, a new tem-
plate is used to guide the conversation.  After 
each of the controlling speaker?s turns within the 
segment are listed a number of prototypical re-
sponses.  One of these responses is a default re-
sponse that signals that the dialogue should 
proceed to the next turn in the template.  The 
other prototypical responses are associated with 
subgoals that are in turn associated with other 
templates.  Thus, the dialogue takes on a hierar-
chical structure.   
Mixed initiative interaction is meant to 
emerge from the underlying template-based 
structure by means of the multi-threaded dis-
course management approach discussed in (Ros? 
& Torrey, 2004).  To this end, templates are 
meant to be used in two ways.  The first way is 
51
when the dialogue system has conversational 
control.  In this case, conversations can be man-
aged as in (Ros? et al, 2001). The second way in 
which templates are used is for determining how 
to respond when user?s have conversational con-
trol.  Provided that the user?s utterances match 
what is expected of the conversational partici-
pant who is in control based on the current tem-
plate, then the system can simply pick one of the 
expected responses.  Otherwise if at some point 
the user?s response does not match, the system 
should check whether the user is initiating yet a 
different segment.  If not, then the system should 
take conversational control. 
7 Future Plans 
In this paper we have discussed our user re-
search and design process to date for the devel-
opment of TuTalk, an authoring environment for 
conversational agents for educational purposes.  
We are continuing our user research and design 
iteration with the plan of end-to-end system test-
ing in actual use starting this summer. 
 
Acknowledgements 
This work was supported in part by Office of Naval 
Research, Cognitive and Neural Sciences Division 
Grant N00014-05-1-0043 and NSF Grant 
SBE0354420.  
References  
Aleven , V. and Ros?, C. P. 2004.  Towards Easier 
Creation of Tutorial Dialogue Systems: Integration 
of Authoring Environments for Tutoring and Dia-
logue Systems, Proceedings of the ITS Workshop 
on Tutorial Dialogue Systems  
Allen, J., Byron, D., Dzikovska, M., Ferguson, G., 
Galescu, L., & Stent, A. 2000. An Architecture for 
a Generic Dialogue Shell. NLENG: Natural Lan-
guage Engineering, Cambridge University Press, 6 
(3), 1-16. 
Beyer, H. & Holtzblatt, K. (1998). Contextual De-
sign, Morgan Kaufmann Publishers. 
Gweon, G., Ros?, C., Wittwer, J., Nueckles, M. 
(submitted).  Supporting Efficient and Reliable 
Content Analysis with Automatic Text Processing 
Technology, Submitted to INTERACT ?05. 
Jordan, P., Ros?, C. P., & VanLehn, K. (2001). Tools 
for Authoring Tutorial Dialogue Knowledge. In J. 
D. Moore, C. L. Redfield, & W. L. Johnson (Eds.), 
Proceedings of AI-ED 2001 (pp. 222-233). Am-
sterdam, IOS Press. 
Koedinger, K. R. & Nathan, M. J. (2004).  The real 
story behind story problems: Effects of representa-
tions on quantitative reasoning.  The Journal of the 
Learning Sciences, 13(2). 
Nathan, M. J. & Koedinger, K. R. (2000).  Moving 
beyond teachers? intuitive beliefs about algebra 
learning.  Mathematics Teacher, 93, 218-223. 
Porter, M. 1980.  An Algorithm for Suffix Stripping, 
Program 14 {3}:130 ? 137. 
Robertson, S. and Walker, S., 1994.  Some simple 
effective approximations to the 2-poisson model 
for probabilistic weighted retrieval Proceedings of 
SIGIR-94. 
Ros?, C. P., and Torrey, C. (2004). ,DRESDEN: To-
wards a Trainable Tutorial Dialogue Manager to 
Support Negotiation Dialogues for Learning and 
Reflection, Proceedings of the Intelligent Tutoring 
Systems Conference. 
Ros?, C. P. and Hall, B. (2004). A Little Goes a Long 
Way: Quick Authoring of Semantic Knowledge 
Sources for Interpretation, Proceedings of SCa-
NaLu ?04. 
Ros?, C. P. 2000. A framework for robust semantic 
interpretation. In Proceedings of the First Meeting 
of the North American Chapter of the Association 
for Computational Linguistics, pages 311?318. 
Ros?, C. P., Pai, C., Arguello, J. 2005. Enabling Non-
Linguists to Author Advanced Conversational In-
terfaces Easily. Proceedings of FLAIRS 2005.  
Steinbach, Kepis, and Kumar, A Comparison of 
Document Clustering Techniques, pg. 8. 
http://lucene.apache.org 
52
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 194?195,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
TFLEX: Speeding up Deep Parsing with Strategic Pruning
Myroslava O. Dzikovska
Human Communication Research Centre
University of Edinburgh
Edinburgh, EH8 9LW, UK
mdzikovs@inf.ed.ac.uk
Carolyn P. Rose
Carnegie Mellon University
Language Technologies Institute
Pittsburgh PA 15213, USA
cprose@cs.cmu.edu
1 Introduction
This paper presents a method for speeding up a
deep parser through backbone extraction and prun-
ing based on CFG ambiguity packing.1 The TRIPS
grammar is a wide-coverage grammar for deep nat-
ural language understanding in dialogue, utilized in
6 different application domains, and with high cov-
erage and sentence-level accuracy on human-human
task-oriented dialogue corpora (Dzikovska, 2004).
The TRIPS parser uses a best-first beam search al-
gorithm and a chart size limit, both of which are a
form of pruning focused on finding an n-best list of
interpretations. However, for longer sentences lim-
iting the chart size results in failed parses, while in-
creasing the chart size limits significantly impacts
the parsing speed.
It is possible to speed up parsing by implement-
ing faster unification algorithms, but this requires
considerable implementation effort. Instead, we de-
veloped a new parser, TFLEX, which uses a sim-
pler technique to address efficiency issues. TFLEX
combines the TRIPS grammar with the fast parsing
technologies implemented in the LCFLEX parser
(Rose? and Lavie, 2001). LCFLEX is an all-paths
parser which uses left-corner prediction and ambi-
guity packing, and which was shown to be efficient
on other unification augmented context-free gram-
mars. We describe a way to transfer the TRIPS
grammar to LCFLEX, and a pruning method which
achieves significant improvements in both speed and
coverage compared to the original TRIPS parser.
1This material is based on work supported by grants from
the Office of Naval Research under numbers N000140510048
and N000140510043.
2 TFLEX
To use the TRIPS grammar in LCFLEX we first ex-
tracted a CFG backbone from the TRIPS grammar,
with CFG non-terminals corresponding directly to
TRIPS constituent categories. To each CFG rule
we attach a corresponding TRIPS rule. Whenever
a CFG rule completes, a TRIPS unification function
is called to do all the unification operations associ-
ated with the TRIPS rule. If the unification fails, the
constituent built by the CFG is cancelled.
The TFLEX pruning algorithm uses ambiguity
packing to provide good pruning points. For exam-
ple, in the sentence ?we have a heart attack victim
at marketplace mall? the phrase ?a heart attack vic-
tim? has two interpretations depending on whether
?heart? modifies ?attack? or ?attack victim?. These
interpretations will be ambiguity packed in the CFG
structure, which offers an opportunity to make prun-
ing more strategic by focusing specifically on com-
peting interpretations for the same utterance span.
For any constituent where ambiguity-packed non-
head daughters differ only in local features, we
prune the interpretations coming from them to a
specified prune beam width based on their TRIPS
scores. In the example above, pruning will happen
at the point of making a VP ?have a heart attack vic-
tim?. The NP will be ambiguity packed, and we will
prune alternative VP interpretations resulting from
combining the same sense of the verb ?have? and
different interpretations of the NP.
This approach works better than the original
TRIPS best-first algorithm, because for long sen-
tence the TRIPS chart contains a large number
194
of similar constituents, and the parser frequently
reaches the chart size limit before finding the correct
constituent to use. Ambiguity packing in TFLEX
helps chose the best constituents to prune by prun-
ing competing interpretations which cover the same
span and have the same non-local features, thus
making it less likely that a constituent essential for
building a parse will be pruned.
3 Evaluation
Our evaluation data is an excerpt from the Monroe
corpus that has been used in previous TRIPS re-
search on parsing speed and accuracy (Swift et al,
2004). The test contained 1042 utterances, from 1
to 45 words in length (mean 5.38 words/utt, st. dev.
5.7 words/utt). Using a hold-out set, we determined
that a beam width of 3 was an optimal setting for
TFLEX. We then compared TFLEX at beam width
3 to the TRIPS parser with chart size limits of 1500,
5000, and 10000. As our evaluation metrics we re-
port are average parse time per sentence and proba-
bility of finding at least one parse, the latter being a
measure approximating parsing accuracy.
The results are presented in Figure 1. We grouped
sentences into equivalence classes based on length
with a 5-word increment. On sentences greater
than 10 words long, TFLEX is significantly more
likely to produce a parse than any of the TRIPS
parsers (evaluated using a binary logistic regression,
p < .001). Moreover, for sentences greater than
20 words long, no form of TRIPS parser returned
a complete parse. TFLEX is significantly faster
than TRIPS-10000, statistically indistinguishable in
terms of parse time from TRIPS-5000, and signifi-
cantly slower than TRIPS-1500 (p < .001).
Thus, TFLEX presents a superior balance of cov-
erage and efficiency especially for long sentences
(10 words or more) since for these sentences it is
significantly more likely to find a parse than any ver-
sion of TRIPS, even a version where the chart size is
expanded to an extent that it becomes significantly
slower (i.e., TRIPS-10000).
4 Conclusions
In this paper, we described a combination of effi-
cient parsing techniques to improve parsing speed
and coverage with the TRIPS deep parsing grammar.
Figure 1: Parse times and probability of getting a
parse depending on (aggregated) sentence lengths.
5 denotes sentences with 5 or fewer words, 25 sen-
tences with more than 20 words.
The TFLEX system uses an all-paths left-corner
parsing from the LCFLEX parser, made tractable
by a pruning algorithm based on ambiguity packing
and local features, generalizable to other unification
grammars. Our pruning algorithm provides a bet-
ter efficiency-coverage balance than best-first pars-
ing with chart limits as utilised by the TRIPS parser.
References
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
C. P. Rose? and A. Lavie. 2001. Balancing robustness
and efficiency in unification-augmented context-free
parsers for large practical applications. In J.C. Junqua
and G Van Noord, editors, Robustness in Language
and Speech Technology. Kluwer Academic Press.
M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in
the parser: Using a shallow parser to improve deep
parsing. In Proceedings of COLING-04.
J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and J.
Allen. 2004. Discourse annotation in the monroe cor-
pus. In ACL-04 workshop on Discourse Annotation.
195
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 42?49,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Topic Segmentation of Dialogue 
 
Jaime Arguello Carolyn Ros? 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15217 Pittsburgh, PA 15217 
jarguell@andrew.cmu.edu cprose@cs.cmu.edu 
  
 
Abstract 
We introduce a novel topic segmentation 
approach that combines evidence of topic 
shifts from lexical cohesion with linguistic 
evidence such as syntactically distinct fea-
tures of segment initial and final contribu-
tions.  Our evaluation shows that this hy-
brid approach outperforms state-of-the-art 
algorithms even when applied to loosely 
structured, spontaneous dialogue.  Further 
analysis reveals that using dialogue ex-
changes versus dialogue contributions im-
proves topic segmentation quality. 
1 Introduction 
In this paper we explore the problem of topic 
segmentation of dialogue. Use of topic-based mod-
els of dialogue has played a role in information 
retrieval (Oard et al, 2004), information extraction 
(Baufaden, 2001), and summarization (Zechner, 
2001), just to name a few applications. However, 
most previous work on automatic topic segmenta-
tion has focused primarily on segmentation of ex-
pository text. This paper presents a survey of the 
state-of-the-art in topic segmentation technology. 
Using the definition of topic segment from (Pas-
sonneau and Litman, 1993) applied to two different 
dialogue corpora, we present an evaluation includ-
ing a detailed error analysis, illustrating why ap-
proaches designed for expository text do not gen-
eralize well to dialogue.  
We first demonstrate a significant advantage of 
our hybrid, supervised learning approach called 
Museli, a multi-source evidence integration ap-
proach, over competing algorithms. We then ex-
tend the basic Museli algorithm by introducing an 
intermediate level of analysis based on Sinclair and 
Coulthard?s notion of a dialogue exchange (Sin-
clair and Coulthard, 1975). We show that both our 
baseline and Museli approaches obtain a signifi-
cant improvement when using perfect, hand-
labeled dialogue exchanges, typically in the order 
of 2-3 contributions, as the atomic discourse unit in 
comparison to using the contribution as the unit of 
analysis. We further evaluate our success towards 
automatic classification of exchange boundaries 
using the same Museli framework.  
2 Defining Topic 
In the most general sense, the challenge of topic 
segmentation can be construed as the task of find-
ing locations in the discourse where the focus 
shifts from one topic to another. Thus, it is not pos-
sible to address topic segmentation of dialogue 
without first addressing the question of what a 
?topic? is. We began with the goal of adopting a 
definition of topic that meets three criteria. First, it 
should be reproducible by human annotators. Sec-
ond, it should not rely heavily on domain-specific 
knowledge or knowledge of the task structure. Fi-
nally, it should be grounded in generally accepted 
principles of discourse structure.  
The last point addresses a subtle, but important, 
criterion necessary to adequately serve down-
stream applications using our dialogue segmenta-
tion. Topic analysis of dialogue concerns itself 
mainly with thematic content. However, bounda-
ries should be placed in locations that are natural 
turning points in the discourse. Shifts in topic 
should be readily recognizable from surface char-
acteristics of the language. 
With these goals in mind, we adopted a defini-
tion of ?topic? that builds upon Passonneau and 
Litman?s seminal work on segmentation of mono-
logue (Passonneau and Litman, 1993).  They found 
that human annotators can successfully accomplish 
a flat monologue segmentation using an informal 
notion of speaker intention. 
42
Dialogue is inherently hierarchical in structure. 
However, a flat segmentation model is an adequate 
approximation. Passonneau and Litman?s pilot 
studies confirmed previously published results 
(Rotondo, 1984) that human annotators cannot re-
liably agree on a hierarchical segmentation of 
monologue. Using a stack-based hierarchical 
model of discourse, Flammia (1998) found that 
90% of all information-bearing dialogue turns re-
ferred to the discourse purpose at the top of the 
stack.  
We adopt a flat model of topic segmentation 
based on discourse segment purpose, where a shift 
in topic corresponds to a shift in purpose that is 
acknowledged and acted upon by both conversa-
tional participants. We place topic boundaries on 
contributions that introduce a speaker?s intention to 
shift the purpose of the discourse, while ignoring 
expressed intentions to shift discourse purposes 
that are not taken up by the other participant. We 
adopt the dialogue contribution as the basic unit of 
analysis, refraining from placing topic boundaries 
within a contribution. This decision is analogous to 
Hearst?s (Hearst, 1994, 1997) decision to shift the 
TextTiling induced boundaries to their nearest ref-
erence paragraph boundary.  
We evaluated the reproducibility of our notion 
of topic segment boundaries by assessing inter-
coder reliability over 10% of the corpus (see Sec-
tion 5.1).  Three annotators were given a 10 page 
coding manual with explanation of our informal 
definition of shared discourse segment purpose as 
well as examples of segmented dialogues.  Pair-
wise inter-coder agreement was above 0.7 for all 
pairs of annotators. 
3 Previous Work 
Existing topic segmentation approaches can be 
loosely classified into two types: (1) lexical cohe-
sion models, and (2) content-oriented models.  The 
underlying assumption in lexical cohesion models 
is that a shift in term distribution signals a shift in 
topic (Halliday and Hassan, 1976). The best known 
algorithm based on this idea is TextTiling (Hearst, 
1997). In TextTiling, a sliding window is passed 
over the vector-space representation of the text. At 
each position, the cosine correlation between the 
upper and lower regions of the sliding window is 
compared with that of the peak cosine correlation 
values to the left and right of the window.  A seg-
ment boundary is predicted when the magnitude of 
the difference exceeds a threshold.    
One drawback to relying on term co-occurrence 
to signal topic continuity is that synonyms or re-
lated terms are treated as thematically-unrelated. 
One proposed solution to this problem is Latent 
Semantic Analysis (LSA) (Landauer and Dumais, 
1997). Two LSA-based algorithms for segmenta-
tion are described in (Foltz, 1998) and (Olney and 
Cai, 2005). Foltz?s approach differs from 
TextTiling mainly in its use of an LSA-based vec-
tor space model. Olney and Cai address a problem 
not addressed by TextTiling or Foltz?s approach, 
which is that cohesion is not just a function of the 
repetition of thematically-related terms, but also a 
function of the presentation of new information in 
reference to information already presented. Their 
orthonormal basis approach allows for segmenta-
tion based on relevance and informativity.  
Content-oriented models, such as (Barzilay and 
Lee, 2004), rely on the re-occurrence of patterns of 
topics over multiple realizations of thematically 
similar discourses, such as a series of newspaper 
articles about similar events. Their approach util-
izes a hidden Markov model where states corre-
spond to topics and state transition probabilities 
correspond to topic shifts. To obtain the desired 
number of topics (states), text spans of uniform 
length (individual contributions, in our case) are 
clustered. Then, state emission probabilities are 
induced using smoothed cluster-specific language 
models. Transition probabilities are induced by 
considering the proportion of documents in which 
a contribution assigned to the source cluster (state) 
immediately precedes a contribution assigned to 
the target cluster (state). Following an EM-like 
approach, contributions are reassigned to states 
until the algorithm converges. 
4 Overview of Museli Approach 
We cast the segmentation problem as a binary 
classification problem where each contribution is 
classified as NEW_TOPIC if it introduces a new 
topic and SAME_TOPIC otherwise. In our hybrid 
Museli approach, we combined lexical cohesion 
with features that have the potential to capture 
something about the linguistic style that marks 
shifts in topic. Table 1 lists our features.  
 
 
 
43
Feature Description 
Lexical  
Cohesion 
Cosine correlation of adjacent 
regions in the discourse. Term 
vectors of adjacent regions are 
stemmed and stopwords are re-
moved. 
Word-
unigram 
Unigrams in previous and cur-
rent contributions 
Word-bigram Bigrams in previous and current 
contributions 
Punctuation Punctuation of previous and cur-
rent contributions. 
Part-of-
Speech (POS)  
Bigram 
POS-Bigrams in previous and 
current contributions.  
Time  
Difference 
Time difference between previ-
ous and current contribution, 
normalized by: 
(X ? MIN)/ (MAX ? MIN), 
where X corresponds to this time 
difference and MIN & MAX are 
with respect to the whole corpus. 
Content  
Contribution 
Binary-valued, is there a non-
stopword term in the current 
contribution? 
Contribution 
Length 
Number of words in the current 
contribution, normalized by:  
(X ? MIN) / (MAX ? MIN). 
Previous 
Agent1
Binary-valued, was the speaker 
of the previous contribution the 
student or the tutor? 
Table 1. Museli Features. 
 
  We found that using a Na?ve Bayes classifier 
with an attribute selection wrapper using the chi-
square test for ranking attributes performed better 
than other state-of-the-art machine learning algo-
rithms on our task, perhaps because of the evi-
dence integration oriented nature of the problem.  
We conducted our evaluation using 10-fold cross-
validation, being careful not to include instances 
from the same dialogue in both the training and 
test sets on any fold to avoid biasing the trained 
model with idiosyncratic communicative patterns 
associated with individual dialogue participants.  
To capitalize on differences in conversational 
behavior between participants assigned to different 
                                                 
1  The current contribution?s agent is implicit in the fact that 
we learn separate models for each agent-role (student & tutor). 
roles in the conversation (i.e., student and tutor), 
we learn separate models for each role. This deci-
sion is motivated by observations that participants 
with different speaker-roles, each with different 
goals in the conversation, introduce topics with a 
different frequency, introduce different types of 
topics, and may introduce topics in a different style 
that displays their status in the conversation. For 
instance, a tutor may be more likely to introduce 
new topics with a contribution that ends with an 
imperative. A student may be more likely to intro-
duce new topics with a contribution that ends with 
a wh-question. Dissimilar agent-roles also occur in 
other domains such as Travel Agent and Customer 
in flight booking scenarios. 
Using the complete set of features enumerated 
above, we perform feature selection on the training 
data for each fold of the cross-validation sepa-
rately, training a model with the top 1000 features, 
and applying that trained model to the test data.  
Examples of high ranking features output by our 
chi-squared feature selection wrapper confirm our 
intuition that initial and final contributions of a 
segment are marked differently. Moreover, the 
highest ranked features are different for our two 
speaker-roles. Some features highly-correlated 
with student-initiated segments are am_trying, 
should, what_is, and PUNCT_question, which re-
late to student questions and requests for informa-
tion. Some features highly-correlated with tutor-
initiated segments include ok_lets, do, see_what, 
and BEGIN_VERB (the POS of the first word in 
the contribution is VERB), which characterize im-
peratives, and features such as now, next, and first, 
which characterize instructional task ordering. 
5 Evaluation   
We evaluate Museli in comparison to the best 
performing state-of-the-art approaches, demon-
strating that our hybrid Museli approach out-
performs all of these approaches on two different 
dialogue corpora by a statistically significant mar-
gin (p < .01), in one case reducing the probability 
of error, as measured by Pk (Beeferman et al, 
1999), to about 10%. 
5.1 Experimental Corpora 
We used two different dialogue corpora from the 
educational domain for our evaluation. Both cor-
pora constitute of dialogues between a student and 
44
a tutor (speakers with asymmetric roles) and both 
were collected via chat software.  The first corpus, 
which we call the Olney & Cai corpus, is a set of 
dialogues selected randomly from the same corpus 
Olney and Cai obtained their corpus from (Olney 
and Cai, 2005). The dialogues discuss problems 
related to Newton?s Three Laws of Motion. The 
second corpus, the Thermo corpus, is a locally col-
lected corpus of thermodynamics tutoring dia-
logues, in which tutor-student pairs work together 
to solve an optimization task. Table 2 shows cor-
pus statistics from both corpora. 
 
 Olney & Cai 
 Corpus 
Thermo 
Corpus 
#Dialogues 42 22 
Conts./Dialogue 195.40 217.90 
Conts./Topic 24.00 13.31 
Topics/Dialogue 8.14 16.36 
Words/Cont. 28.63 5.12 
Student Conts. 4113 1431 
Tutor Conts. 4094 3363 
Table 2. Evaluation Corpora Statistics  
 
Both corpora seem adequate for attempting to 
harness systematic differences in how speakers 
with asymmetric roles may initiate or close topic 
segments. The Thermo corpus is particularly ap-
propriate for addressing the research question of 
how to automatically segment natural, spontaneous 
dialogue. The exploratory task is more loosely 
structured than many task-oriented domains inves-
tigated in the dialogue community, such as flight 
reservation or meeting scheduling. Students can 
interrupt with questions and tutors can digress in 
any way they feel may benefit the completion of 
the task. In the Olney and Cai corpus, the same 10 
physics problems are addressed in each session and 
the interaction is almost exclusively a tutor initia-
tion followed by student response, evident from the 
nearly equal number of student and tutor contribu-
tions. 
5.2 Baseline Approaches 
We evaluate Museli against the following four 
algorithms: (1) Olney and Cai (Ortho), (2) Barzilay 
and Lee (B&L), (3) TextTiling (TT), and (4) Foltz.  
As opposed to the other baseline algorithms, 
(Olney and Cai, 2005) applied their orthonormal 
basis approach specifically to dialogue, and prior 
to this work, report the highest numbers for topic 
segmentation of dialogue. Barzilay and Lee?s ap-
proach is the state of the art in modeling topic 
shifts in monologue text. Our application of B&L 
to dialogue attempts to harness any existing and 
recognizable redundancy in topic-flow across our 
dialogues for the purpose of topic segmentation.  
We chose TextTiling for its seminal contribution 
to monologue segmentation. TextTiling and Foltz 
consider lexical cohesion as their only evidence of 
topic shifts. Applying these approaches to dialogue 
segmentation sheds light on how term distribution 
in dialogue differs from that of expository mono-
logue text (e.g. news articles). The Foltz and Ortho 
approaches require a trained LSA space, which we 
prepared the same way as described in (Olney and 
Cai, 2005). Any parameter tuning for approaches 
other than our Museli was computed over the en-
tire test set, giving baseline algorithms the maxi-
mum advantage.  
In addition to these approaches, we include 
segmentation results from three degenerate ap-
proaches: (1) classifying all contributions as 
NEW_TOPIC (ALL), (2) classifying no contribu-
tions as NEW_TOPIC (NONE), and (3) classifying 
contributions as NEW_TOPIC at uniform intervals 
(EVEN), separated by the average reference topic 
length (see Table 2). 
As a means for comparison, we adopt two 
evaluation metrics: Pk and f-measure. An extensive 
argument in support of Pk?s robustness (if k is set 
to ? the average reference topic length) is pre-
sented in (Beeferman, et al 1999).  Pk measures the 
probability of misclassifying two contributions a 
distance of k contributions apart, where the classi-
fication question is are the two contributions part 
of the same topic segment or not?  Pk is the likeli-
hood of misclassifying two contributions, thus 
lower Pk values are preferred over higher ones. It 
equally captures the effect of false-negatives and 
false-positives and favors predictions that that are 
closer to the reference boundaries. F-measure pun-
ishes false positives equally, regardless of their 
distance to reference boundaries.  
5.3 Results 
Table 3 shows our evaluation results.  Note that 
lower values of Pk are preferred over higher ones. 
The opposite is true of F-measure.  In both cor-
pora, the Museli approach performed significantly 
better than all other approaches (p < .01).  
 
45
 Olney and Cai 
Corpus 
Thermo Corpus 
 Pk F Pk F 
NONE 0.4897 -- 0.4900 -- 
ALL 0.5180 -- 0.5100 -- 
EVEN 0.5117 -- 0.5131 -- 
TT 0.6240 0.1475 0.5353 0.1614 
B&L 0.6351 0.1747 0.5086 0.1512 
Foltz 0.3270 0.3492 0.5058 0.1180 
Ortho 0.2754 0.6012 0.4898 0.2111 
Museli 0.1051 0.8013 0.4043 0.3693 
Table 3. Results on both corpora 
5.4 Error Analysis 
Results for all approaches are better on the Ol-
ney and Cai corpus than the Thermo corpus. The 
Thermo corpus differs profoundly from the Olney 
and Cai corpus in ways that very likely influenced 
the performance. For instance, in the Thermo cor-
pus each dialogue contribution is on average 5 
words long, whereas in the Olney and Cai corpus 
each dialogue contribution contains an average of 
28 words. Thus, the vector space representation of 
the dialogue contributions is more sparse in the 
Thermo corpus, which makes shifts in lexical co-
herence less reliable as topic shift indicators.   
In terms of Pk, TextTiling (TT) performed worse 
than the degenerate algorithms. TextTiling meas-
ures the term overlap between adjacent regions in 
the discourse. However, dialogue contributions are 
often terse or even contentless. This produces 
many islands of contribution-sequences for which 
the local lexical coherence is zero. TextTiling 
wrongly classifies all of these as starts of new top-
ics. A heuristic improvement to prevent TextTiling 
from placing topic boundaries at every point along 
a sequence of contributions failed to produce a sta-
tistically significant improvement. 
The Foltz and the Ortho approaches rely on LSA 
to provide strategic semantic generalizations capa-
ble of detecting shifts in topic. Following (Olney 
and Cai, 2005), we built our LSA space using dia-
logue contributions as the atomic text unit.  In cor-
pora such as the Thermo corpus, however, this may 
not be effective due to the brevity of contributions. 
Barzilay and Lee?s algorithm (B&L) did not 
generalize well to either dialogue corpus. One rea-
son could be that probabilistic methods, such as 
their approach, require that reference topics have 
significantly different language models, which was 
not true in either of our evaluation corpora. We 
also noticed a number of instances in the dialogue 
corpora where participants referred to information 
from previous topic segments, which consequently 
may have blurred the distinction between the lan-
guage models assigned to different topics. 
6 Dialogue Exchanges  
Although results are reliably better than our 
baseline algorithms in both corpora, there is much 
room for improvement, especially in the more 
spontaneous Thermo corpus. We believe that an 
improvement can come from a multi-layer segmen-
tation approach, where a first pass segments a dia-
logue into dialogue exchanges and a second classi-
fier assigns topic shifts based on exchange initial 
contributions. Dialogue is hierarchical in nature. 
Topic and topic shift comprise only one of the 
many lenses through which dialogue behaves in 
seemingly structured ways. Thus, it seems logical 
that exploiting more fine-grained sub-parts of dia-
logue than our definition of topic might help us do 
better at predicting shifts in topic.  One such sub-
part of dialogue is the notion of dialogue exchange, 
typically between 2-3 contributions. 
Stubbs (1983) motivates the definition of an ex-
change with the following observation. In theory, 
there is no limit to the number of possible re-
sponses to the clause ?Is Harry at home??. How-
ever, constraints are imposed on the interpretation 
of the contribution that follows it: yes or no. Such a 
constraint is central to the concept of a dialogue 
exchange. Informally, an exchange is made from 
an initiation, for which the possibilities are open-
ended, followed by dialogue contributions that are 
pre-classified and thus increasingly restricted. A 
contribution is part of the next exchange when the 
constraint on its communicative act is lifted.  
Sinclair and Coulthard (1975) introduce a more 
formal definition of exchange with their Initiative-
Response-Feedback or IRF structure. An initiation 
produces a response and a response happens as 
direct consequence to an initiation. Feedback 
serves to close an exchange. Sinclair and Coulthard 
posit that if exchanges constitute the minimal unit 
of interaction, IRF is a primary structure of interac-
tive discourse in general.  
To measure the benefits of exchange boundaries 
in detecting topic shift in dialogue, we coded the 
Thermo corpus with exchanges following Sinclair 
46
and Coulthard?s IRF structure. The coder who la-
beled dialogue exchanges had no knowledge of our 
definition of topic or our intention to do topic-
analyses of the corpus. Any correlation between 
exchange boundaries and topic boundaries is not a 
bias introduced during the hand-labeling process.     
7 Topic Segmentation with Exchanges 
In our corpus, as we believe is true in domain-
general dialogue, knowledge of an exchange-
boundary increases the probability of a topic-
boundary significantly. One way to quantify this 
relation is with the following observation. In our 
experimental Thermo corpus, there are 4794 dia-
logue contributions, 360 topic shifts, and 1074 ex-
change shifts. Using maximum likelihood estima-
tion, the likelihood of being correct if we say that a 
randomly chosen contribution is a topic shift is 
0.075 (# topic shifts / # contributions). However, 
the likelihood of being correct if we have prior 
knowledge that an exchange-shift also occurs in 
that contribution is 0.25. Thus, knowledge that the 
contribution introduces a new exchange increases 
our confidence that it also introduces a new topic. 
More importantly, the probability that a contribu-
tion does not mark a topic shift, given that it does 
not mark an exchange-shift, is 0.98. Thus, ex-
changes show great promise in narrowing the 
search-space of tentative topic shifts. 
In addition to possibly narrowing the space of 
tentative topic-boundaries, exchanges are helpful 
in that they provide more coarse-grain building 
blocks for segmentation algorithms that rely on 
term-distribution as a proxy for dialogue coher-
ence, such as TextTiling (Hearst, 1994, 1997), the 
Foltz algorithm (Foltz, 1998), Orthonormal Basis 
(Olney and Cai, 2005), and Barzilay and Lee?s 
content modeling approach (Barzilay and Lee, 
2004).  At the heart of all these approaches is the 
assumption that a change in term distribution sig-
nals a shift in topic. When applied to dialogue, the 
major weakness of these approaches is that contri-
butions are often times contentless: terse and ab-
sent of thematically meaningful terms. Thus, a 
more coarse-grained discourse unit is needed.  
8 Barzilay and Lee with Exchanges 
Barzilay and Lee (2004) offer an attractive 
frame work for constructing a context-specific 
Hidden Markov Model (HMM) of topic drift. In 
our initial evaluation, we used dialogue contribu-
tions as the atomic discourse unit. Using contribu-
tions, our application of Barzilay and Lee?s algo-
rithm for segmenting dialogue fails at least in part 
because the model learns states that are not the-
matically meaningful, but instead relate to other 
systematic phenomena in dialogue, such as fixed 
expressions and discourse cues. Figure 1 shows the 
cluster (state) size distribution in terms of the per-
centage of the total discourse units (exchanges vs. 
contributions) in the Thermo corpus assigned to 
each cluster. In the horizontal axis, clusters (states) 
are sorted by size from largest to smallest.  
 
% of Total Discourse Units per Cluster
(clusters sorted by size, largest-to-smallest)
0%
10%
20%
30%
40%
50%
60%
70%
80%
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Cluster Rank 
%
 o
f 
D
is
co
u
rs
e 
U
n
it
s 
in
 C
lu
st
er
CONTRIBUTIONS EXCHANGES
 
Figure 1. Exchanges produce a more evenly dis-
tributed cluster size distribution. 
   
The largest cluster contains 70% of all contribu-
tions in the corpus. The second largest cluster only 
generates 10% of the contributions. In contrast, 
when using exchanges as the atomic unit, the clus-
ter size distribution is less skewed and corresponds 
more closely to a topic analysis performed by a 
domain expert.  In this analysis, the number of de-
sired cluster (states), which is an input to the algo-
rithm, was set to 16, the same number identified in 
a domain expert?s analysis of the Thermo corpus. 
Examples of such topics include high-level ones 
such as greeting, setup initialization, and general 
thermo concepts, as well as task-specific ones like 
sensitivity analysis and regeneration. 
A closer examination of the clusters (states) con-
firms our intuition that systematic topic-
independent phenomena in dialogue, coupled with 
the terse nature of contributions in spontaneous 
dialogue, leads to an overly skewed cluster size 
distribution. Examining the terms with the highest 
emission probabilities, the largest states contain 
47
topical terms like cycle, efficiency, increase, qual-
ity, plot, and turbine intermixed with terms like 
think, you, right, make, yeah, fine, and ok. Also the 
sets of topical terms in these larger states do not 
seem coherent with respect to the expert induced 
topics. This suggests that thematically ambiguous 
fixed expressions blur the distinction between the 
different topic-centered language models, produc-
ing an overly heavy-tailed cluster size distribution. 
One might argue that a possible solution to this 
problem would be to remove these fixed expres-
sions as part of pre-processing. However, that re-
quires knowledge of the particular domain and 
knowledge of the interaction style characteristic to 
the context. We believe that a more robust solution 
is to use exchanges as the atomic unit of discourse. 
9 Evaluation with Exchanges 
To show the value of dialogue exchanges in 
topic segmentation, in this section we re-formulate 
our problem from classifying contributions into 
NEW_TOPIC and SAME_TOPIC to classifying 
exchange initial contributions into NEW_TOPIC 
and SAME_TOPIC. For all algorithms, we con-
sider only predictions that coincide with hand-
coded exchange initial contributions. We show 
that, except for our own Museli approach, using 
exchange boundaries improves segmentation qual-
ity across all algorithms (p < .05) when compared 
to their respective counterparts that ignore ex-
changes. Using exchanges gives the Museli ap-
proach a significant advantage based on F-measure 
(p < .05), but only a marginally significant advan-
tage based on Pk. These results confirm our intui-
tion that what gives our Museli approach an advan-
tage over baseline algorithms is its ability to har-
ness the lexical, syntactic, and phrasal cues that 
mark shifts in topic. Given that shift-in-topic corre-
lates highly with shift-in-exchange, these features 
are discriminatory in both respects.  
 Of the degenerate strategies in section 5.2, only 
ALL lends itself to our reformulation of the topic 
segmentation problem. For the ALL heuristic, we 
classify all exchange initial contributions into 
NEW_TOPIC.  This degenerate heuristic alone 
produces better results than all algorithms classify-
ing utterances (Table 4). In our implementation of 
TextTiling (TT) with exchanges, we only consider 
predictions on contributions that coincide with ex-
change initial contributions, while ignoring predic-
tions made on contributions that do not introduce a 
new exchange. Consistent with our evaluation 
methodology from Section 5, we optimized the 
window size using the entire corpus and found an 
optimal window size of 13 contributions. Without 
exchanges, the optimal window size was 6 contri-
butions. The higher optimal window-size hints to 
the possibility that by using exchange initial con-
tributions an approach based on lexical cohesion 
may broaden its horizon without losing precision. 
 
 Thermo Corpus 
(Contributions) 
Thermo Corpus 
(Exchanges) 
 Pk F Pk F 
NONE 0.4900 -- N/A -- 
ALL 0.5100 -- 0.4398 0.3809 
EVEN 0.5132 -- N/A -- 
TT 0.5353 0.1614 0.4328 0.3031 
B&L 0.5086 0.1512 0.3817 0.3840 
Foltz 0.5058 0.1180 0.4242 0.3296 
Ortho 0.4898 0.2111 0.4398 0.3813 
Museli 0.4043 0.3693 0.3737 0.3897 
Table 4. Results using perfect exchange boundaries 
 
In this version of B&L, we use exchanges to 
build the initial clusters (states) and the final 
HMM. B&L with exchanges significantly im-
proves over B&L with contributions, in terms of 
both Pk and F-measure (p < .005) and significantly 
improves over our ALL heuristic (where all ex-
change initial contributions introduce a new topic) 
in terms of Pk (p < .0005). Thus, its use of ex-
changes goes beyond merely narrowing the space 
of possible NEW_TOPIC contributions: it also 
uses these more coarse-grained discourse units to 
build a more thematically-motivated topic model.  
Foltz?s and Olney and Cai?s (Ortho) approach 
both use an LSA space trained on the dialogue 
corpus. Instead of training the LSA space with in-
dividual contributions, we train the LSA space us-
ing exchanges. We hope that by training the space 
with more contentful text units LSA might capture 
more topically-meaningful semantic relations.  In 
addition, only exchange initial contributions where 
used for the logistic regression training phase. 
Thus, we aim to learn the regression equation that 
best discriminates between exchange initial contri-
butions that introduce a topic and those that do not. 
Both Foltz and Ortho improve over their non ex-
change counterparts, but neither improves over the 
ALL heuristic by a significant margin.  
48
For Museli with exchanges, we tried both train-
ing the model using only exchange initial contribu-
tions, and applying our previous model to only ex-
change initial contributions. Training our models 
using only exchange initial contributions produced 
slightly worse results. We believe that the reduc-
tion of the amount of training data prevents our 
models from learning good generalizations. Thus, 
we trained our models using contributions (as in 
Section 5) and consider predictions only on ex-
change initial contributions. The Museli approach 
offers a significant advantage over TT in terms of 
Pk and F-measure. Using perfect-exchanges, it is 
not significantly better than Barzilay and Lee. It is 
significantly better than Foltz?s approach based on 
F-measure and significantly better than Olney and 
Cai based on Pk (p < .05). 
These experiments used hand coded exchange 
boundaries.  We also evaluated our ability to 
automatically predict exchange boundaries.  On the 
Thermo corpus, Museli was able to predict ex-
change boundaries with precision = 0.48, recall = 
0.62, f-measure = 0.53, and Pk = 0.14. 
10 Conclusions and Current Directions 
In this paper we addressed the problem of auto-
matic topic segmentation of spontaneous dialogue.  
We demonstrated with an empirical evaluation that 
state-of-the-art approaches fail on spontaneous dia-
logue because term distribution alone fails to pro-
vide adequate evidence of topic shifts in dialogue.   
We have presented a supervised learning algo-
rithm for topic segmentation of dialogue called 
Museli that combines linguistic features signaling a 
contribution?s function with local context indica-
tors. Our evaluation on two distinct corpora shows 
a significant improvement over the state-of-the-art 
algorithms. We have also demonstrated that a sig-
nificant improvement in performance of state-of-
the-art approaches to topic segmentation can be 
achieved when dialogue exchanges, rather than 
contributions, are used as the basic unit of dis-
course.  We demonstrated promising results in 
automatically identifying exchange boundaries. 
Acknowledgments 
This work was funded by Office of Naval Re-
search, Cognitive and Neural Science Division; 
grant number N00014-05-1-0043. 
 
References  
Regina Barzilay and Lillian Lee. 2004. Catching the 
drift: Probabilistic Content Models, with Applica-
tions to Generation and Summarization. In Proceed-
ings of HLT-NAACL, 113 - 120.  
Doug Beeferman, Adam Berger, John D. Lafferty. 1999.  
Statistical Models for Text Segmentation. Machine 
Learning, 34 (1-3): 177-210. 
Narj?s Boufaden, Guy Lapalme, Yoshua Bengio. 2001. 
Topic Segmentation: A first stage to Dialog-based In-
formation Extraction. In Proceedings of NLPRS.  
Giovanni Flammia. 1998. Discourse Segmentation of 
Spoken Dialogue, PhD Thesis. Massachusetts Insti-
tute of Technology.  
Peter Foltz, Walter Kintsch, and Thomas Landauer. 
1998. The measurement of textual cohesion with 
LSA. Discourse Processes, 25, 285-307. 
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion 
in English. London: Longman. 
Marti Hearst. 1997. TextTiling: Segmenting Text into 
Multi-Paragragh Subtopic Passages. Computational 
Linguistics, 23(1), 33 ? 64.   
Thomas Landauer and Susan Dumais. A Solution to 
Plato?s Problem: The Latent Semantic Analysis of 
Acquisition, Induction, and Representation of 
Knowledge. Psychological Review, 104, 221-240.  
Douglas Oard, Bhuvana Ramabhadran, and Samuel 
Gustman. 2004. Building an Information Retrieval 
Test Collection for Spontaneous Conversational 
Speech. In Proceedings of SIGIR. 
Andrew Olney and Zhiqiang Cai. 2005. An Orthonor-
mal Basis for Topic Segmentation of Tutorial Dia-
logue. In Proceedings of HLT/EMNLP. 971-978. 
Rebecca Passonneau and Diane Litman. 1993. Inten-
tion-Based Segmentation: Human Reliability and 
Correlation with Linguistic Cues. In Proceedings of 
ACL, 148 ? 155.  
John Rotondo, 1984, Clustering Analysis of Subject 
Partitions of Text. Discourse Processes, 7:69-88 
John Sinclair and Malcolm Coulthard. 1975. Towards 
an Analysis of Discourse: the English Used by 
Teachers and Pupils. Oxford University Press.  
Michael Stubbs. 1983. Discourse Analysis. A Sociolin-
guistic Analysis of Natural Language. Basil Black-
well.  
Klaus Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. The-
sis. Carnegie Mellon University.
49
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 9?16,
New York City, June 2006. c?2006 Association for Computational Linguistics
Backbone Extraction and Pruning for Speeding Up a Deep Parser for
Dialogue Systems
Myroslava O. Dzikovska
Human Communication Research Centre
University of Edinburgh
Edinburgh, EH8 9LW, United Kingdom,
mdzikovs@inf.ed.ac.uk
Carolyn P. Rose?
Carnegie Mellon University
Language Technologies Institute,
Pittsburgh, PA 15213,
cprose@cs.cmu.edu
Abstract
In this paper we discuss issues related to
speeding up parsing with wide-coverage
unification grammars. We demonstrate
that state-of-the-art optimisation tech-
niques based on backbone parsing before
unification do not provide a general so-
lution, because they depend on specific
properties of the grammar formalism that
do not hold for all unification based gram-
mars. As an alternative, we describe an
optimisation technique that combines am-
biguity packing at the constituent structure
level with pruning based on local features.
1 Introduction
In this paper we investigate the problem of scaling
up a deep unification-based parser developed specif-
ically for the purpose of robust interpretation in dia-
logue systems by improving its speed and coverage
for longer utterances. While typical sentences in di-
alogue contexts are shorter than in expository text
domains, longer utterances are important in discus-
sion oriented domains. For example, in educational
applications of dialogue it is important to elicit deep
explanation from students and then offer focused
feedback based on the details of what students say.
The choice of instructional dialogue as a target ap-
plication influenced the choice of parser we needed
to use for interpretation in a dialogue system. Sev-
eral deep, wide-coverage parsers are currently avail-
able (Copestake and Flickinger, 2000; Rose?, 2000;
Baldridge, 2002; Maxwell and Kaplan, 1994), but
many of these have not been designed with issues re-
lated to interpretation in a dialogue context in mind.
The TRIPS grammar (Dzikovska et al, 2005) is a
wide-coverage unification grammar that has been
used very successfully in several task-oriented di-
alogue systems. It supports interpretation of frag-
ments and lexical semantic features (see Section 2
for a more detailed discussion), and provides addi-
tional robustness through ?robust? rules that cover
common grammar mistakes found in dialogue such
as missing articles or incorrect agreement. These
enhancements help parsing dialogue (both spoken
and typed), but they significantly increase grammar
ambiguity, a common concern in building grammars
for robust parsing (Schneider and McCoy, 1998). It
is specifically these robustness-efficiency trade-offs
that we address in this paper.
Much work has been done related to enhanc-
ing the efficiency of deep interpretation systems
(Copestake and Flickinger, 2000; Swift et al, 2004;
Maxwell and Kaplan, 1994), which forms the foun-
dation that we build on in this work. For example,
techniques for speeding up unification in HPSG lead
to dramatic improvements in efficiency (Kiefer et
al., 1999). Likewise ambiguity packing and CFG
backbone parsing (Maxwell and Kaplan, 1994; van
Noord, 1997) are known to increase parsing effi-
ciency. However, as we show in this paper, these
techniques depend on specific grammar properties
that do not hold for all grammars. This claim is con-
sistent with observations of Carroll (1994) that pars-
ing software optimisation techniques tend to be lim-
ited in their applicability to the individual grammars
they were developed for. While we used TRIPS as
our example unification-based grammar, this inves-
tigation is important not only for this project, but in
the general context of speeding up a wide-coverage
unification grammar which incorporates fragment
9
rules and lexical semantics, which may not be im-
mediately provided by other available systems.
In the remainder of the paper, we begin with a
brief description of the TRIPS parser and grammar,
and motivate the choice of LCFLEX parsing algo-
rithm to provide a fast parsing foundation. We then
discuss the backbone extraction and pruning tech-
niques that we used, and evaluate them in compar-
ison with the original parsing algorithm. We con-
clude with discussion of some implications for im-
plementing grammars that build deep syntactic and
semantic representations.
2 Motivation
The work reported in this paper was done as part
of the process of developing a dialogue system that
incorporates deep natural language understanding.
We needed a grammar that provides lexical seman-
tic interpretation, supports parsing fragmentary ut-
terance in dialogue, and could be used to start de-
velopment without large quantities of corpus data.
TRIPS fulfilled our requirements better than sim-
ilar alternatives, such as LINGO ERG (Copestake
and Flickinger, 2000) or XLE (Maxwell and Kaplan,
1994).
TRIPS produces logical forms which include se-
mantic classes and roles in a domain-independent
frame-based formalism derived from FrameNet and
VerbNet (Dzikovska et al, 2004; Kipper et al,
2000). Lexical semantic features are known to be
helpful in both deep (Tetreault, 2005) and shal-
low interpretation tasks (Narayanan and Harabagiu,
2004). Apart from TRIPS, these have not been in-
tegrated into existing deep grammars. While both
LINGO-ERG and XLE include semantic features
related to scoping, in our applications the avail-
ability of semantic classes and semantic role as-
signments was more important to interpretation,
and these features are not currently available from
those parsers. Finally, TRIPS provides a domain-
independent parse selection model, as well as rules
for interpreting discourse fragments (as was also
done in HPSG (Schlangen and Lascarides, 2003), a
feature actively used in interpretation.
While TRIPS provides the capabilities we need,
its parse times for long sentences (above 15 words
long) are intolerably long. We considered two pos-
sible techniques for speeding up parsing: speeding
up unification using the techniques similar to the
LINGO system (Copestake and Flickinger, 2000),
or using backbone extraction (Maxwell and Ka-
plan, 1994; Rose? and Lavie, 2001; Briscoe and Car-
roll, 1994). TRIPS already uses a fast unification
algorithm similar to quasi-destructive unification,
avoiding copying during unification.1 However,
the TRIPS grammar retains the notion of phrase
structure, and thus it was more natural to chose to
use backbone extraction with ambiguity packing to
speed up the parsing.
As a foundation for our optimisation work, we
started with the freely available LCFLEX parser
(Rose? and Lavie, 2001). LCFLEX is an all-paths
parser that uses left-corner prediction and ambigu-
ity packing to make all-paths parsing tractable, and
which was shown to be efficient for long sentences
with somewhat less complex unification augmented
context-free grammars. We show that all-paths pars-
ing with LCFLEX is not tractable for the ambiguity
level in the TRIPS grammar, but that by introduc-
ing a pruning method that uses ambiguity packing to
guide pruning decisions, we can achieve significant
improvements in both speed and coverage compared
to the original TRIPS parser.
3 The TRIPS and LCFLEX algorithms
3.1 The TRIPS parser
The TRIPS parser we use as a baseline is a bottom-
up chart parser with lexical entries and rules repre-
sented as attribute-value structures. To achieve pars-
ing efficiency, TRIPS uses a best-first beam search
algorithm based on the scores from a parse selection
model (Dzikovska et al, 2005; Elsner et al, 2005).
The constituents on the parser?s agenda are grouped
into buckets based on their scores. At each step, the
bucket with the highest scoring constituents is se-
lected to build/extend chart edges. The parsing stops
once N requested analyses are found. This guaran-
tees that the parser returns the N -best list of analyses
according to the parse selection model used, unless
the parser reaches the chart size limit.
1Other enhancements used by LINGO depend on disallow-
ing disjunctive features, and relying instead on the type system.
The TRIPS grammar is untyped and uses disjunctive features,
and converting it to a typed system would require as yet unde-
termined amount of additional work.
10
In addition to best-first parsing, the TRIPS parser
uses a chart size limit, to prevent the parser from
running too long on unparseable utterances, similar
to (Frank et al, 2003). TRIPS is much slower pro-
cessing utterances not covered in the grammar, be-
cause it continues its search until it reaches the chart
limit. Thus, a lower chart limit improves parsing
efficiency. However, we show in our evaluation that
the chart limit necessary to obtain good performance
in most cases is too low to find parses for utterances
with 15 or more words, even if they are covered by
the grammar.
The integration of lexical semantics in the TRIPS
lexicon has a major impact on parsing in TRIPS.
Each word in the TRIPS lexicon is associated with a
semantic type from a domain-independent ontology.
This enables word sense disambiguation and seman-
tic role labelling for the logical form produced by
the grammar. Multiple word senses result in addi-
tional ambiguity on top of syntactic ambiguity, but it
is controlled in part with the use of weak selectional
restrictions, similar to the restrictions employed by
the VerbNet lexicon (Kipper et al, 2000). Check-
ing semantic restrictions is an integral part of TRIPS
parsing, and removing them significantly decreases
speed and increases ambiguity of the TRIPS parser
(Dzikovska, 2004). We show that it also has an im-
pact on parsing with a CFG backbone in Section 4.1.
3.2 LCFLEX
The LCFLEX parser (Rose? and Lavie, 2001) is an
all-paths robust left corner chart parser designed to
incorporate various robustness techniques such as
word skipping, flexible unification, and constituent
insertion. Its left corner chart parsing algorithm
is similar to that described by Briscoe and Carroll
(1994). The system supports grammatical specifi-
cation in a unification framework that consists of
context-free grammar rules augmented with feature
bundles associated with the non-terminals of the
rules. LCFLEX can be used in two parsing modes:
either context-free parsing can be done first, fol-
lowed by applying the unification rules, or unifica-
tion can be done interleaved with context-free pars-
ing. The context free backbone allows for efficient
left corner predictions using a pre-compiled left cor-
ner prediction table, such as that described in (van
Noord, 1997). To enhance its efficiency, it incor-
porates a provably optimal ambiguity packing algo-
rithm (Lavie and Rose?, 2004).
These efficiency techniques make feasible all-
path parsing with the LCFLEX CARMEL grammar
(Rose?, 2000). However, CARMEL was engineered
with fast all-paths parsing in mind, resulting in cer-
tain compromises in terms of coverage. For exam-
ple, it has only very limited coverage for noun-noun
compounding, or headless noun phrases, which are a
major source of ambiguity with the TRIPS grammar.
4 Combining LCFLEX and TRIPS
4.1 Adding CFG Backbone
A simplified TRIPS grammar rule for verb phrases
and a sample verb entry are shown in Figure 1. The
features for building semantic representations are
omitted for brevity. Each constituent has an assigned
category that corresponds to its phrasal type, and a
set of (complex-valued) features.
The backbone extraction algorithm is reason-
ably straightforward, with CFG non-terminals cor-
responding directly to TRIPS constituent categories.
To each CFG rule we attach a corresponding TRIPS
unification rule. After parsing is complete, the
parses found are scored and ordered with the parse
selection model, and therefore parsing accuracy in
all-paths mode is the same or better than TRIPS ac-
curacy for the same model.
For constituents with subcategorized arguments
(verbs, nouns, adverbial prepositions), our back-
bone generation algorithm takes the subcategoriza-
tion frame into account. For example, the TRIPS
VP rule will split into 27 CFG rules corresponding
to different subcategorization frames: VP? V intr,
VP? V NP NP, VP? V NP CP NP CP, etc. For
each lexical entry, its appropriate CFG category is
determined based on the subcategorization frame
from TRIPS lexical representation. This improves
parsing efficiency using the prediction algorithms in
TFLEX operating on the CFG backbone. The ver-
sion of the TRIPS grammar used in testing con-
tained 379 grammar rules with 21 parts of speech
(terminal symbols) and 31 constituent types (non-
terminal symbols), which were expanded into 1121
CFG rules with 85 terminals and 36 non-terminals
during backbone extraction.
We found, however, that the previously used tech-
11
(a) ((VP (SUBJ ?!subj) (CLASS ?lf))
-vp1-role .99
(V (LF ?lf) (SUBJ ?!subj) (DOBJ ?dobj)
(IOBJ ?iobj) (COMP3 ?comp3))
?iobj ?dobj ?comp3)
(b) ((V (agr 3s) (LF LF::Filling)
(SUBJ (NP (agr 3s)))
(DOBJ (NP (case obj))) (IOBJ -) (COMP3 -)))
Figure 1: (a) A simplified VP rule from the TRIPS
grammar; (b) a simplified verb entry for a transitive
verb. Question marks denote variables.
nique of context-free parsing first followed by full
re-unification was not suitable for parsing with the
TRIPS grammar. The CFG structure extracted from
the TRIPS grammar contains 43 loops resulting
from lexical coercion rules or elliptical construc-
tions. A small number of loops from lexical coer-
cion were both obvious and easy to avoid, because
they are in the form N? N. However, there were
longer loops, for example, NP ? SPEC for sen-
tences like ?John?s car? and SPEC ? NP for head-
less noun phrases in sentences like ?I want three?.
LCFLEX uses a re-unification algorithm that asso-
ciates a set of unification rules with each CFG pro-
duction, which are reapplied at a later stage. To
be able to apply a unification rule corresponding to
N? N production, it has to be explicitly present in
the chart, leading to an infinite number of N con-
stituents produced. Applying the extra CFG rules
expanding the loops during re-unification would
complicate the algorithm significantly. Instead, we
implemented loop detection during CFG parsing.
The feature structures prevent loops in unifica-
tion, and we considered including certain grammat-
ical features into backbone extraction as done in
(Briscoe and Carroll, 1994). However, in the TRIPS
grammar the feature values responsible for break-
ing loops belonged to multi-valued features (6 val-
ued in the worst case), with values which may de-
pend on other multiple-valued features in daughter
constituents. Thus adding the extra features resulted
in major backbone size increases because of cate-
gory splitting. This can be remedied with additional
pre-compilation (Kiefer and Krieger, 2004), how-
ever, this requires that all lexical entries be known
in advance. One nice feature of the TRIPS lex-
icon is that it includes a mechanism for dynami-
cally adding lexical entries for unknown words from
wide-coverage lexicons such as VerbNet (Kipper et
al., 2000), which would be impractical to use in pre-
compilation.
Therefore, to use CFG parsing before unification
in our system, we implemented a loop detector that
checked the CFG structure to disallow loops. How-
ever, the next problem that we encountered is mas-
sive ambiguity in the CFG structure. Even a very
short phrase such as ?a train? had over 700 possi-
ble CFG analyses, and took 910 msec to parse com-
pared to 10 msec with interleaved unification. CFG
ambiguity is so high because noun phrase fragments
are allowed as top-level categories, and lexical am-
biguity is compounded with semantic ambiguity and
robust rules normally disallowed by features during
unification. Thus, in our combined algorithm we had
to use unification interleaved with parsing to filter
out the CFG constituents.
4.2 Ambiguity Packing
For building semantic representations in parallel
with parsing, ambiguity packing presents a set of
known problems (Oepen and Carroll, 2000). One
possible solution is to exclude semantic features dur-
ing an initial unification stage, use ambiguity pack-
ing, and re-unify with semantic features in a post-
processing stage. In our case, we found this strategy
difficult to implement, since selectional restrictions
are used to limit the ambiguity created by multiple
word senses during syntactic parsing. Therefore, we
chose to do ambiguity packing on the CFG structure
only, keeping the multiple feature structures associ-
ated with each packed CFG constituent.
To begin to evaluate the contribution of ambiguity
packing on efficiency, we ran a test on the first 39 ut-
terances in a hold out set not used in the formal eval-
uation below. Sentences ranged from 1 to 17 words
in length, 16 of which had 6 or more words. On this
set, the average parse time without ambiguity pack-
ing was 10 seconds per utterance, and 30 seconds per
utterance on utterances with 6 or more words. With
ambiguity packing turned on, the average parse time
decreased to 5 seconds per utterance, and 13.5 sec-
onds per utterance on the utterances with more than
6 words. While this evaluation showed that ambi-
12
guity packing improves parsing efficiency, we deter-
mined that further enhancements were necessary.
4.3 Pruning
We added a pruning technique based on the scor-
ing model discussed above and ambiguity packing
to enhance system performance. As an illustration,
consider an example from a corpus used in our eval-
uation where the TRIPS grammar generates a large
number of analyses, ?we have a heart attack vic-
tim at marketplace mall?. The phrase ?a heart at-
tack victim? has at least two interpretations,?a [N1
heart [N1 attack [N1 victim]]]? and ?a [N1 [N1 heart
[N1 attack]] [N1 victim]]?. The prepositional phrase
?at marketplace mall? can attach either to the noun
phrase or to the verb. Overall, this results in 4 basic
interpretations, with additional ambiguity resulting
from different possible senses of ?have?.
The best-first parsing algorithm in TRIPS uses
parse selection scores to suppress less likely inter-
pretations. In our example, the TRIPS parser will
chose the higher-scoring one of the two interpreta-
tions for ?a heart attack victim?, and use it first. For
this NP the features associated with both interpreta-
tions are identical with respect to further processing,
thus TRIPS will never come back to the other in-
terpretation, effectively pruning it. ?At? also has 2
possible interpretations due to word sense ambigu-
ity: LF::TIME-LOC and LF::SPATIAL-LOC. The
former has a slightly higher preference, and TRIPS
will try it first. But then it will be unable to find an
interpretation for ?at Marketplace Mall?, and back-
track to LF::SPATIAL-LOC to find a correct parse.
Without chart size limits the parser is guaran-
teed to find a parse eventually through backtracking.
However, this algorithm does not work quite as well
with chart size limits. If there are many similarly-
scored constituents in the chart for different parts of
the utterance, the best-first algorithm expands them
first, and the the chart size limit tends to interfere
before TRIPS can backtrack to an appropriate lower-
scoring analysis.
Ambiguity packing offers an opportunity to make
pruning more strategic by focusing specifically on
competing interpretations for the same utterance
span. The simplest pruning idea would be for ev-
ery ambiguity packed constituent to eliminate the in-
terpretations with low TRIPS scores. However, we
need to make sure that we don?t prune constituents
that are required higher up in the tree to make a
parse. Consider our example again.
The constituent for ?at? will be ambiguity
packed with its two meanings. But if we prune
LF::SPATIAL-LOC at that point, the parse for ?at
Marketplace Mall? will fail later. Formally, the com-
peting interpretations for ?at? have non-local fea-
tures, namely, the subcategorized complement (time
versus location) is different for those interpretations,
and is checked higher up in the parse. But for ?a
heart attack victim? the ambiguity-packed interpre-
tations differ only in local features. All features as-
sociated with this NP checked higher up come from
the head noun ?victim? and are identical in all inter-
pretations. Therefore we can eliminate the low scor-
ing interpretations with little risk of discarding those
essential for finding a complete parse. Thus, for
any constituent where ambiguity-packed non-head
daughters differ only in local features, we prune
the interpretations coming from them to a specified
prune beam width based on their TRIPS scores.
This pruning heuristic based on local features
can be generalised to different unification grammars.
For example, in HPSG pruning would be safe at all
points where a head is combined with ambiguity-
packed non-head constituents, due to the locality
principle. In the TRIPS grammar, if a trips rule
uses subcategorization features, the same locality
principle holds. This heuristic has perfect precision
though not complete recall, but, as our evaluation
shows, it is sufficient to significantly improve per-
formance in comparison with the TRIPS parser.
5 Evaluation
The purpose of our evaluation is to explore the ex-
tent to which we can achieve a better balance be-
tween parse time and coverage using backbone pars-
ing with pruning compared to the original best-first
algorithm. For our comparison we used an excerpt
from the Monroe corpus that has been used in previ-
ous TRIPS research on parsing speed and accuracy
(Swift et al, 2004) consisting of dialogues s2, s4,
s16 and s17. Dialogue s2 was a hold out set used for
pilot testing and setting parameters. The other three
dialogues were set aside for testing. Altogether, the
test set contained 1042 utterances, ranging from 1 to
13
45 words in length (mean 5.38 words/utt, st. dev. 5.7
words/utt). Using our hold-out set, we determined
that a beam width of three was an optimal setting.
Thus, we compared TFLEX using a beam width of 3
to three different versions of TRIPS that varied only
in terms of the maximum chart size, giving us a ver-
sion that is significantly faster than TFLEX overall,
one that has parse times that are statistically indis-
tinguishable from TFLEX, and one that is signifi-
cantly slower. We show that while lower chart sizes
in TRIPS yield speed ups in parse time, they come
with a cost in terms of coverage.
5.1 Evaluation Methodology
Because our goal is to explore the parse time versus
coverage trade-offs of two different parsing architec-
tures, the two evaluation measures that we report are
average parse time per sentence and probability of
finding at least one parse, the latter being a measure
estimating the effect of parse algorithm on parsing
coverage.
Since the scoring model is the same in TRIPS and
TFLEX, then as long as TFLEX can find at least one
parse (which happened in all but 1 instances on our
held-out set), the set returned will include the one
produced by TRIPS. We spot-checked the TFLEX
utterances in the test set for which TRIPS could
not find a parse to verify that the parses produced
were reasonable. The parses produced by TFLEX on
these sentences were typically acceptable, with er-
rors mainly stemming from attachment disambigua-
tion problems.
5.2 Results
We first compared parsers in terms of probability of
producing at least one parse (see Figure 2). Since
the distribution of sentence lengths in the test corpus
was heavily skewed toward shorter sentences, we
grouped sentences into equivalence classes based on
a range of sentence lengths with a 5-word increment,
with all of the sentences over 20 words aggregated
in the same class. Given a large number of short sen-
tences, there was no significant difference overall in
likelihood to find a parse. However, on sentences
greater than 10 words long, TFLEX is significantly
more likely to produce a parse than any of the TRIPS
parsers (evaluated using a binary logistic regression,
N = 334, G = 16.8, DF = 1, p < .001). Fur-
Parser <= 20 words >= 6 words
TFLEX 6.2 (20.2) 29.1 (96.3)
TRIPS-1500 2.3 (5.4) 6.9 (8.2)
TRIPS-5000 7.7 (30.2) 28.1 (56.4)
TRIPS-10000 22.7 (134.4) 107.6 (407.4)
Table 1: The average parse times for TRIPS and
TFLEX on utterances 6 words or more.
thermore, for sentences greater than 20 words long,
no form of TRIPS parser ever returned a complete
parse.
Next we compared the parsers in terms of aver-
age parse time on the whole data set across equiva-
lence classes of sentences, assigned based on Aggre-
gated Sentence Length (see Figure 2 and Table 1).
An ANOVA with Parser and Aggregated Sentence
Length as independent variables and Parse Time as
the dependent variable showed a significant effect
of Parser on Parse Time (F (3, 4164) = 270.03,
p < .001). Using a Bonferroni post-hoc analysis, we
determined that TFLEX is significantly faster than
TRIPS-10000 (p < .001), statistically indistinguish-
able in terms of parse time from TRIPS-5000, and
significantly slower than TRIPS-1500 (p < .001).
Since none of the TRIPS parsers ever returned a
parse for sentences greater than 20 words long, we
recomputed this analysis excluding the latter. We
still find a significant effect of Parser on Parse Time
(F (3, 4068) = 18.6, p < .001). However, a post-
hoc analysis reveals that parse times for TFLEX,
TRIPS-1500, and TRIPS-5000 are statistically in-
distinguishable for this subset, whereas TFLEX is
significantly faster than TRIPS-10000 (p < .001).
See Table 1 for for parse times of all four parsers.
Since TFLEX and TRIPS both spent 95% of their
computational effort on sentences with 6 or more
words, we also include results for this subset of the
corpus.
Thus, TFLEX presents a superior balance of cov-
erage and efficiency especially for long sentences
(10 words or more) since for these sentences it is
significantly more likely to find a parse than any ver-
sion of TRIPS, even a version where the chart size is
expanded to an extent that it becomes significantly
slower (i.e., TRIPS-10000). And while TRIPS-1500
is consistently faster than the other parsers, it is
not significantly faster than TFLEX on sentences 20
14
Figure 2: Parse times and probability of getting a parse depending on (aggregated) sentence lengths. 5
denotes sentences with 5 or fewer words, 25 sentences with more than 20 words.
words long or less, which is the subset of sentences
for which it is able to find a parse.
5.3 Discussion and Future Work
The most obvious lesson learned in this experience
is that the speed up techniques developed for specific
grammars and unification formalisms do not transfer
easily to other unification grammars. The features
that make TRIPS interesting ? the inclusion of lex-
ical semantics, and the rules for parsing fragments
? also make it less amenable to using existing effi-
ciency techniques.
Grammars with an explicit CFG backbone nor-
mally restrict the grammar writer from writing
grammar loops, a restriction not imposed by gen-
eral unification grammars. As we showed, there
can be a substantial number of loops in a CFG due
to the need to cover various elliptical constructions,
which makes CFG parsing not interleaved with uni-
fication less attractive in cases where we want to
avoid expensive CFG precompilation. Moreover, as
we found with the TRIPS grammar, in the context
of robust parsing with lexical semantics the ambigu-
ity in a CFG backbone grows large enough to make
CFG parsing followed by unification inefficient. We
described an alternative technique that uses pruning
based on a parse selection model.
Another option for speeding up parsing that we
have not discussed in detail is using a typed gram-
mar without disjunction and speeding up unification
as done in HPSG grammars (Kiefer et al, 1999). In
order to do this, we must first address the issue of
integrating the type of lexical semantics that we re-
quire with HPSG?s type system. Adding lexical se-
mantics while retaining the speed benefits obtained
through this type system would require that the se-
mantic type ontology be expressed in the same for-
malism as the syntactic types. We plan to further
explore this option in our future work.
Though longer sentences were relatively rare
in our test set, using the system in an educa-
tional domain (our ultimate goal) means that the
longer sentences are particularly important, because
they often correspond to significant instructional
events, specifically answers to deep questions such
as ?why? and ?how? questions. Our evaluation has
been designed to show system performance with ut-
terances of different length, which would roughly
correspond to the performance in interpreting short
and long student answers. Since delays in respond-
ing can de-motivate the student and decrease the
quality of the dialogue, improving handling of long
utterances can have an important effect on the sys-
tem performance. Evaluating this in practice is a
possible direction for future work.
6 Conclusions
We described a combination of efficient parsing
techniques to improve parsing speed and coverage
with the TRIPS deep parsing grammar. We showed
that context-free parsing was inefficient on a back-
bone extracted from an existing unification gram-
mar, and demonstrated how to make all-path pars-
ing more tractable by a new pruning algorithm based
15
on ambiguity packing and local features, general-
isable to other unification grammars. We demon-
strated that our pruning algorithm provides better
efficiency-coverage balance than the best-first pars-
ing with chart limits utilised by the TRIPS parser,
and discussed the design implications for other ro-
bust parsing grammars.
Acknowledgements
We thank Mary Swift and James Allen for their
help with the TRIPS code and useful comments.
This material is based on work supported by grants
from the Office of Naval Research under numbers
N000140510048 and N000140510043.
References
J. Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
T. Briscoe and J. Carroll. 1994. Generalized proba-
bilistic LR parsing of natural language (corpora) with
unification-based grammars. Computational Linguis-
tics, 19(1):25?59.
J. Carroll. 1994. Relating complexity to practical per-
formance in parsing with wide-coverage unification
grammars. In Proceedings of ACL-2004.
A. Copestake and D. Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC-2000, Athens, Greece.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004.
Building a computational lexicon and ontology with
framenet. In LREC workshop on Building Lexical Re-
sources from Semantically Annotated Corpora.
M. Dzikovska, M. Swift, J. Allen, and W. de Beaumont.
2005. Generic parsing for multi-domain semantic in-
terpretation. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT-05).
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
M. Elsner, M. Swift, J. Allen, and D. Gildea. 2005. On-
line statistics for a unification-based dialogue parser.
In Proceedings of the 9th International Workshop on
Parsing Technologies (IWPT-05).
A. Frank, B. Kiefer, B. Crysmann, M. Becker, and
U. Schafer. 2003. Integrated shallow and deep pars-
ing: TopP meets HPSG. In Proceedings of ACL 2003.
B. Kiefer and H.-U. Krieger. 2004. A context-free ap-
proximation of head-driven phrase structure grammar.
In H. Bunt, J. Carroll, and G. Satta, editors, New De-
velopments in Parsing Technology. Kluwer.
B. Kiefer, H. Krieger, J. Carroll, and R. Malouf. 1999.
Bag of useful techniques for efficient and robust pars-
ing. In Proceedings of ACL 1999.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of the 7th Conference on Artificial Intelligence and of
the 12th Conference on Innovative Applications of Ar-
tificial Intelligence.
A. Lavie and C. P. Rose?. 2004. Optimal ambiguity pack-
ing in context-free parsers with interleaved unification.
In H. Bunt, J. Carroll, and G. Satta, editors, Current Is-
sues in Parsing Technologies. Kluwer Academic Press.
J. T. Maxwell and R. M. Kaplan. 1994. The interface
between phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?590.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proceedings of
International Conference on Computational Linguis-
tics (COLING 2004), Geneva, Switzerland.
S. Oepen and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing - practical results. In Pro-
ceedings of NAACL?00.
C. P. Rose? and A. Lavie. 2001. Balancing robustness
and efficiency in unification-augmented context-free
parsers for large practical applications. In J. Junqua
and G. Van Noord, editors, Robustness in Language
and Speech Technology. Kluwer Academic Press.
C. Rose?. 2000. A framework for robust semantic in-
terpretation. In Proceedings 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
D. Schlangen and A. Lascarides. 2003. The interpreta-
tion of non-sentential utterances in dialogue. In Pro-
ceedings of the 4th SIGdial Workshop on Discourse
and Dialogue, Japan, May.
D. Schneider and K. F. McCoy. 1998. Recognizing syn-
tactic errors in the writing of second language learners.
In Proceedings of COLING-ACL?98.
M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in
the parser: Using a shallow parser to improve deep
parsing. In Proceedings of COLING-04.
J. Tetreault. 2005. Empirical Evaluations of Pronoun
Resolution. Ph.D. thesis, University of Rochester.
G. van Noord. 1997. An efficient implementation of
the head-corner parser. Computational Linguistics,
23(3):425?456.
16
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 18?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Estimating Annotation Cost for Active Learning
in a Multi-Annotator Environment
Shilpa Arora, Eric Nyberg and Carolyn P. Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{shilpaa,ehn,cprose}@cs.cmu.edu
Abstract
We present an empirical investigation of the
annotation cost estimation task for active
learning in a multi-annotator environment. We
present our analysis from two perspectives:
selecting examples to be presented to the user
for annotation; and evaluating selective sam-
pling strategies when actual annotation cost
is not available. We present our results on a
movie review classification task with rationale
annotations. We demonstrate that a combina-
tion of instance, annotator and annotation task
characteristics are important for developing an
accurate estimator, and argue that both corre-
lation coefficient and root mean square error
should be used for evaluating annotation cost
estimators.
1 Introduction
Active Learning is the process of selectively query-
ing the user to annotate examples with the goal
of minimizing the total annotation cost. Annota-
tion cost has been traditionally measured in terms
of the number of examples annotated, but it has
been widely acknowledged that different examples
may require different annotation effort (Settles et al,
2008; Ringger et al, 2008).
Ideally, we would use actual human annotation
cost for evaluating selective sampling strategies, but
this will require conducting several user studies, one
per strategy on the same dataset. Alternatively, we
may be able to simulate the real user by an annota-
tion cost estimator that can then be used to evaluate
several selective sampling strategies without having
to run a new user study each time. An annotation
cost estimator models the characteristics that can
differentiate the examples in terms of their annota-
tion time. The characteristics that strongly correlate
with the annotation time can be used as a criterion
in selective sampling strategies to minimize the total
annotation cost.
In some domains, the annotation cost of an ex-
ample is known or can be calculated exactly before
querying the user. For example, in biological ex-
periments it might be calculable from the cost of
the equipment and the material used (King et al,
2004). In NLP, sometimes a simplifying assumption
is made that the annotation cost for an example can
be measured in terms of its length (e.g. seconds of
voicemail annotated (Kapoor et al, 2007); number
of tokens annotated (Tomanek et al, 2007)). An-
other assumption is that the number of user anno-
tation actions can be used as a proxy for annota-
tion cost of an example (e.g. number of brackets
added for parsing a sentence (Hwa, 2000); number
of clicks for correcting named entities (Kristjannson
et al, 2004)). While these are important factors in
determining the annotation cost, none of them alone
can fully substitute for the actual annotation cost.
For example, a short sentence with a lot of embed-
ded clauses may be more costly to annotate than a
longer sentence with simpler grammatical structure.
Similarly, a short sentence with multiple verbs and
discontinuous arguments may take more time to an-
notate with semantic roles than a longer sentence
with a single verb and simple subject-verb-object
structure (Carreras and Ma?rquez, 2004).
What further complicates the estimation of anno-
tation cost is that even for the same example, anno-
tation cost may vary across annotators (Settles et al,
2008). For example, non-native speakers of English
were found to take longer time to annotate part of
18
speech tags (Ringger et al, 2008). Often multiple
annotators are used for creating an annotated cor-
pus to avoid annotator bias, and we may not know
all our annotators beforehand. Annotation cost also
depends on the user interface used for annotation
(Gweon et al, 2005), and the user interface may
change during an annotation task. Thus, we need
a general annotation cost estimator that can predict
annotation cost for a given annotator and user inter-
face. A general estimator can be built by using an-
notator and user interface characteristics in addition
to the instance characteristics for learning an anno-
tation cost model, and training on data from mul-
tiple annotators and multiple user interfaces. Such
a general estimator is important for active learning
research where the goal is to compare selective sam-
pling strategies independent of the annotator and the
user interface.
In this work, we investigate the annotation cost es-
timation problem for a movie review classification
task in a multi-annotator environment with a fixed
user interface. We demonstrate that a combination
of instance, annotation task and annotator charac-
teristics is important for accurately estimating the
annotation cost. In the remainder of the paper, we
first present a survey of related work and an analysis
of the data collected. We then describe the features
used for our supervised learning approach to anno-
tation cost estimation, followed by the experimental
setup and results. Finally, we conclude with some
future directions we would like to explore.
2 Related work
There has been some recent research effort in using
supervised learning for estimating annotation cost.
The most closely related work is that by Settles et al
(2008) and Ringger et al (2008). Settles et al (2008)
present a detailed analysis of annotation cost for four
NLP applications: named entity recognition, image
retrieval, speculative vs. definite distinction, and in-
formation extraction. They study the effect of do-
main, annotator, jitter, order of examples, etc., on
the annotation cost.
Results from Settles et al (2008) are promising
but leave much room for improvement. They used
only instance level features such as number of en-
tities, length, number of characters, percentage of
non-alpha numeric characters, etc. for annotation
cost estimation. For three of their tasks, the corre-
lation between the estimated and actual annotation
times was in the range (R = 0.587 to 0.852). Note
that the percentage of variance accounted for by a
model is obtained by squaring the R value from the
correlation coefficient. Thus, an R value of 0.587
indicates that only about 34% (R2) of the variance
is accounted for, so the model will make incorrect
predictions about ranking in the majority of cases.
Nevertheless, we acknowledge that our results are
not substantially better, although we argue that this
work contributes to the pool of knowledge that will
hopefully lead to better performance in the future.
Settles et al (2008) train and test their estimator
on data from the same annotator. Thus, in order
to use their model for a new annotator, we would
need to first collect data for that annotator and train
a model. In our work, a group of annotators anno-
tate the same text, and we train and test on different
annotators. We also show that using characteristics
of the annotators and annotation task in addition to
the instance characteristics improves performance.
Ringger et al (2008) use linear regression for an-
notation cost estimation for Part-Of-Speech (POS)
tagging. About 30 annotators annotated 36 different
instances each. The authors present about 13 de-
scriptive statistics of the data, annotator and annota-
tion task, but in their model they only used number
of tokens in the sentence and the number of correc-
tions needed as features. They report that the other
variables didn?t have a significant effect when eval-
uated using a Bayesian Information Criterion (from
the R package).
Ringger et al (2008) noticed that nativeness of
the annotator did have an effect on the annotation
time, but they chose not to include that feature
in their model as they expected to have a similar
mix of skills and background in their target anno-
tators. However, if annotation times differ substan-
tially across annotators, then not accounting for this
difference will reduce the performance of the model.
Also, the low adjusted correlation value for their
model (R = 0.181) indicates that there is only a
weak correlation between the annotation time and a
linear combination of the length of the example and
the number of corrections.
19
3 Analysis and Experiments
In this section, we present our annotation methodol-
ogy and analysis of the data we collected, followed
by a description of the features we used.We then
present our experimental setup followed by a dis-
cussion of our results.
3.1 Annotation Methodology and Data
Analysis
In this work, we estimate the annotation cost for a
movie review classification task. The data we used
were collected as part of a graduate course. Twenty
annotators (students and instructors) were grouped
into five groups of four each. The groups were cre-
ated such that each group had similar variance in
annotator characteristics such as department, educa-
tional experience, programming experience, etc. We
used the first 200 movie reviews from the dataset
provided by Zaidan et al (2007), with an equal dis-
tribution of positive and negative examples. Each
group annotated 25 movie reviews randomly se-
lected from the 200 reviews and all annotators in
each group annotated all 25 reviews. In addition
to voting positive or negative for a review, annota-
tors also annotated rationales (Zaidan et al, 2007),
spans of text in the review that support their vote.
Rationales can be used to guide the model by identi-
fying the most discriminant features. In related work
(Arora and Nyberg, 2009), we ascertain that with ra-
tionales the same performance can be achieved with
less annotated data. The annotation task with ra-
tionales involved a variety of user actions: voting
positive or negative, highlighting spans of text and
adding rationale annotations. We used the same an-
notation guidelines as Zaidan et al (2007). The data
has been made available for research purposes1. Fig-
ure 1 shows a screenshot of the GUI used. We per-
formed an analysis of our data similar to that con-
ducted by Settles et al (2008). We address the fol-
lowing main questions.
Are the annotation times variable enough? If all
examples take a similar time to annotate, then the
number of examples can be used as an approxima-
tion for the annotation cost. Figure 2 shows the his-
togram of averaged annotation times (averaged over
1www.cs.cmu.edu/?shilpaa/datasets/ial/
ial-uee-mr-v0.1.zip
Figure 1: The GUI used for the annotation task. The user
selects the review (segment) to annotate from the list in the
right panel. The review text is displayed in the left panel. The
user votes positive or negative using the radio buttons. Ratio-
nales are added by selecting a span of text and right clicking
to select the rationale tag. The start/stop button can be used to
pause the current task.
Figure 2: Distribution of averaged annotation times
4 annotators in a group). As can be seen from the
mean (? = 165 sec.) and the standard deviation
(? = 68.85), there is a meaningful variance in the
annotation times.
How do the annotation times vary across annota-
tors? A strong correlation between annotation times
from different annotators on a set of instances sug-
gests that there are certain characteristics of these in-
stances, independent of the annotator characteristics,
that can determine their ranking based on the time it
takes to annotate them. We evaluated the pairwise
correlation for all pairs of annotators in each group
(Table 1). As can be seen, there is significant pair-
wise correlation in more than half of the pairs of an-
notators that differ in nativeness (10/16). However,
not all such pairs of annotators are associated with
significant correlation. This suggests that it is im-
portant to consider both instance and annotator char-
acteristics for estimating annotation time.
20
group Avg-Na(Std) Avg-CR(Std) #sign-pairs
0 2.25(0.96) 0.54(0.27) 4/6 (4/5)
1 1.75(0.5) 0.45(0.08) 5/6 (2/3)
2 1(0) 0.13(0.17) 0/6 (0/0)
3 1.75(0.96) 0.36(0.12) 2/6 (1/5)
4 2.75(0.5) 0.47(0.04) 6/6 (3/3)
Avg. 1.9(0.58) 0.39(0.21) 17/30 (10/16)
Table 1: The Table shows the average nativeness and average
pairwise correlation between annotation times for the mem-
bers of each group (and their standard deviation). #sign-pairs
shows the fraction of pairwise correlations within the groups
that were significant (p < 0.05). In brackets, is the fraction
of correlations between annotators with different nativeness
within the groups that were significant.
The box plot in Figure 3 shows the distribution
of annotation times across annotators. As can be
seen, some annotators take in general much longer
than others, and the distribution of times is very dif-
ferent across annotators. For some, the annotation
times vary a lot, but not so much for others. This
suggests that using annotator characteristics as fea-
tures in addition to the instance characteristics may
be important for learning a better estimator.
Figure 3: Box plot shows the annotation time (in sec) dis-
tribution (y-axis) for an annotator (x-axis) for a set of 25 doc-
uments. g0-a1 represents annotator 1 of group 0 and g0-avg
represents the average annotation time. A box represents the
middle 50% of annotation times, with the line representing the
median. Whiskers on either side span the 1st and 4th quartiles
and asterisks indicate the outliers.
3.2 Feature Design
We group the features in the following three cat-
egories: Instance, Annotation Task and Annotator
characteristics.
3.2.1 Instance characteristics
Instance characteristics capture the properties of
the example the user annotates. Table 2 describes
the instance based features we used and the intu-
ition supporting their use for annotation cost esti-
mation. Table 3 shows the mean and standard de-
viation of each of these characteristics, and as can
be seen, these characteristics do vary across exam-
ples and hence these features can be beneficial for
distinguishing examples.
3.2.2 Annotation Task characteristics
Annotation task characteristics are those that can
be captured only during or after the annotation task.
We used the number of rationales as a feature from
this category. In addition to voting for movie re-
views as positive or negative, the user also adds ra-
tionales that support their vote. More rationales im-
ply more work since the user must look for the rele-
vant span of text and perform the physical action of
selecting the span and adding an annotation for each
rationale. Table 3 shows the distribution of the aver-
age Number of Rationales (NR) per example (aver-
aged over the four annotators for a given set).
3.2.3 Annotator characteristics
The annotation cost of an example may vary
across annotators. As reported in Table 1, the aver-
age correlation for annotators on the same document
is low (R = 0.39) with 17 out of 30 pairwise correla-
tions being significant. Thus, it is important to con-
sider annotator characteristics, such as whether the
annotator is a native speaker of English, their educa-
tion level, reading ability, etc. In this work, we only
use nativeness of the annotator as a feature and plan
to explore other characteristics in the future. We as-
signed each annotator a nativeness value. A value
of 3 was given to an annotator whose first language
is English. A value of 2 was given to an annotator
who has a different first language but has either been
educated in English or has been in the United States
for a long time. A value of 1 was assigned to the re-
maining annotators. Among the 20 annotators in the
study, there were 8 annotators with nativeness value
of 1, and 6 each for nativeness values of 2 and 3.
Table 1 shows the average and standard deviation of
the nativeness score in each group.
21
Feature Definition Intuition
Character
Length (CL)
Length of review in
terms of number of
characters
Longer documents
take longer to anno-
tate
Polar word
Count (PC)
Number of words
that are polar (strong
subjective words
from the lexicon
(Wilson et al, 2005))
More subjectivity
implies user would
need more time to
judge positive vs.
negative
Stop word
Percent (SC)
Percentage of words
that are stop words
A high percentage
of stop words im-
plies that the text is
not very complex and
hence easier to read.
Avg. Sen-
tence Length
(SL)
Average of the char-
acter length of sen-
tences in the review
Long sentences in a
review may make it
harder to read.
Table 2: Instance characteristics
Feature Mean Standard Deviation
CL 2.25 0.92
PC 41.50 20.39
SP 0.45 0.03
SL 121.90 28.72
NR 4.80 2.30
Table 3: Mean and the standard deviation for the feature oc-
currences in the data.
3.3 Evaluation Metric
We use both Root Mean Square (RMS) error and
Correlation Coefficient (CRCoef) to evaluate our
model, since the two metrics evaluate different as-
pects of an estimate. RMS is a way to quantify the
amount by which an estimator differs from the true
value of the quantity being estimated. It tells us how
?off? our estimate is from the truth. CRCoef on the
other hand measures the strength and direction of a
linear relationship between two random variables. It
tells us how well correlated our estimate is with the
actual annotation time. Thus, for evaluating how ac-
curate our model is in predicting annotation times,
RMS is a more appropriate metric. For evaluating
the utility of the estimated annotation cost as a cri-
terion for ranking and selecting examples for user?s
annotation, CRCoef is a better metric.
3.4 Experiments & Results
We learn an annotation cost estimator using the Lin-
ear Regression and SMO Regression (Smola and
Scholkopf, 1998) learners from the Weka machine
learning toolkit (Witten and Frank, 2005). As men-
tioned earlier, we have 5 sets of 25 documents each,
and each set was annotated by four annotators. The
results reported are averaged over five folds, where
each set is one fold, and two algorithms (Linear Re-
gression and SMO Regression). Varying the algo-
rithm helps us find the most predictive feature com-
binations across different algorithms. Since each set
was annotated by different annotators, we never train
and test on the data from same annotators. We used
the JMP2 and Minitab3 statistical tools for our analy-
sis. We used an ANOVA model with Standard Least
Squares fitting to compare the different experimen-
tal conditions. We make all comparisons in terms
of both the CRCoef and the RMS metrics. For sig-
nificance results reported, we used 2-tailed paired
T-test, considering (p < 0.05) as significant.
We present our results and analysis in three parts.
We first compare the four instance characteristics,
annotator and annotation task characteristics; and
their combination. We then present an analysis
of the interaction between features and annotation
time. Finally, we compare the ranking of features
based on the two evaluation metrics we used.
3.4.1 Comparing characteristics for annotation
cost estimation
Instance Characteristics: We compare the four
instance characteristics described in Section 3.2.1
and select the most predictive characteristic for fur-
ther analysis with annotator and annotation task
characteristics. As can be seen in Table 4, character
length performs the best, and it is significantly better
than stop word percent and average sentence length.
Character length also outperforms polar word count,
but this difference is not significant. Because of the
large significant difference between the performance
of stop word percent and average sentence length,
compared to character length, we do not consider
them for further analysis.
Feature Combinations: In Table 5, we compare
the feature combinations of instance, annotator and
annotation task characteristics. The table also shows
the weights for the features used and the constant for
the linear regression model trained on all the data. A
missing weight for a feature indicates that it wasn?t
used in that feature combination.
2http://www.jmp.com/software/
3http://www.minitab.com/
22
Feature CR-Coef RMS
CL 0.358 104.51
PC 0.337 105.92
SP -0.041* 114.34*
SL 0.042* 114.50*
Table 4: CR-Coef and RMS results for Character Length
(CL), Polar word Count (PC), Stop word Percent (SP) and av-
erage Sentence Length (SL). Best performance is highlighted
in bold. ? marks the results significantly worse than the best.
We use only the best performing instance charac-
teristic, the character length. The length of an ex-
ample has often been substituted for the annotation
cost (Kapoor et al, 2007; Tomanek et al, 2007).
We show in Table 5 that certain feature combina-
tions significantly outperform character length. The
combination of all three features (last row) performs
the best for both CRCoef and RMS, and this result
is significantly better than the character length (third
row). The combination of number of rationales and
nativeness (fourth row) also outperforms character
length significantly in CRCoef. This suggests that
the number of rationales we expect or require in a re-
view and the annotator characteristics are important
factors for annotation cost estimation and should be
considered in addition to the character length.
CL NR AN Const. CR-Coef RMS
-29.33 220.77 0.135? 123.93?
17.59 82.81 0.486 95.29
0.027 61.53 0.357? 104.51?
19.11 -40.78 153.21 0.55+ 96.04
0.028 32.79 120.18 0.397? 109.85?
0.02 15.15 17.57 0.553+ 90.27+
0.021 16.64 -41.84 88.09 0.626+ 88.44+
Table 5: CR-Coef and RMS results for seven feature com-
binations of Character Length (CL), Number of Rationales
(NR) and Annotator Nativeness (AN). The values in feature
and ?Const.? columns are weights and constant for the linear
regression model trained on all the data. The numbers in bold
are the results for the best feature combination. ? marks the
results significantly worse than the best. + marks the results
significantly better than CL.
The impact of the nativeness feature is somewhat
mixed. Adding the nativeness feature always im-
proves the correlation and for RMS, it helps when
added to the combined feature (CL+NR) but not oth-
erwise. Although this improvement with addition
of the nativeness feature is not significant, it does
suggest that annotator characteristics might be im-
portant to consider. To investigate this further, we
evaluated our assumption that native speakers take
less time to annotate. For each set, we compared the
average annotation times (averaged over examples)
against the nativeness values. For all sets, annotators
with nativeness value of 3 always took less time on
average than those with nativeness value of 2 or 1.
Between 2 and 1, there were no reliable differences.
Sometimes annotators with value of 1 took less time
than annotators with value of 2. Also, for group 2
which had all annotators with nativeness value of 1,
we observed a poor correlation between annotators
(Table 1). This suggest two things: 1) our assign-
ment of nativeness value may not be accurate and
we need other ways of quantifying nativeness, 2)
there are other annotator characteristics we should
take into consideration.
PC CL NR AN Const. CR RMS
0.027 61.53 0.358ab 104.5x
2.2 74.20 0.337a 105.9x
0.7 0.019 60.89 0.355b 104.9x
0.028 -32.8 120.2 0.397ab 109.8x
2.3 -35.5 135.1 0.382a 111.1x
1.1 0.016 -34.3 121.8 0.395b 109.9x
0.02 15.1 17.57 0.553a 90.27x
1.5 15.1 32.02 0.542a 91.65x
0.0 0.02 15.1 17.57 0.554a 90.40x
0.021 16.6 -41.8 88.09 0.626a 88.44x
1.6 16.5 -43.5 102.8 0.614a 90.42y
0.0 0.021 16.6 -41.8 88.09 0.626a 88.78x
Table 6: Each block of 3 rows in this table compares the
performance of Character Length (CL) and Polar word Count
(PC) in combination with Number of Rationales (NR) and An-
notator Nativeness (AN) features. The values in feature and
?Const.? columns are weights and constant for the linear re-
gression model trained on all the data. Best performance is
highlighted in bold. Results in a block not connected by same
letter are significantly different.
Polar word Count and Character Length: As we
saw in Table 4, the difference between character
length and polar word count is not significant. We
further compare these two instance characteristics
in the presence of the annotator and annotation task
characteristics. Our goal is to ascertain whether
character length performs better than polar word
count, or vice versa, and whether this difference is
significant. We also evaluate whether using both
performs better than using any one of them alone.
The results presented in Table 6 help us answer these
questions. For all feature combinations character
length, with and without polar word count, performs
23
better than polar word count, but this difference is
not significant except in three cases. These results
suggests that polar word count can be used as an al-
ternative to character length in annotation cost esti-
mation.
3.4.2 Interaction between Features and
Annotation Time
As a post-experiment analysis, we studied the
interaction between the features we used and an-
notation time, and the interaction among features
themselves. Table 7 reports the pairwise correlation
(Pearson, 1895) for these variables, calculated over
all 125 reviews. As can be seen, all features have
significant correlation with annotation time except
stop words percentage and average sentence length.
Note that number of rationales has higher correla-
tion with annotation time (R = 0.529) than charac-
ter length (R = 0.417). This suggests that number
of rationales may have more influence than charac-
ter length on annotation time, and a low correlation
between number of rationales and character length
(R = 0.238) indicates that it might not be the case
that longer documents necessarily contain more ra-
tionales. Annotating rationales requires cognitive
effort of identifying the right span and manual ef-
fort to highlight and add an annotation, and hence
more rationales implies more annotation time. We
also found some examples in our data where docu-
ments with substantially different lengths but same
number of rationales took a similar time to anno-
tate. One possible explanation for this observation is
user?s annotation strategy. If the annotator chooses
to skim through the remaining text when enough ra-
tionales are found, two examples with same number
of rationales but different lengths might take similar
time. We plan to investigate the effect of annotator?s
strategy on annotation time in the future.
A negative correlation of nativeness with annota-
tion time (R = ?0.219) is expected, since native
speakers (AN = 3) are expected to take less anno-
tation time than non-native speakers (AN = {2, 1}),
although this correlation is low. A low correla-
tion between number of rationales and nativeness
(R = 0.149) suggests that number of rationales
a user adds may not be influenced much by their
nativeness value. A not significant low correlation
(R = ?0.06) between character length and native-
AT CL NR AN PC SP SL
AT 1
CL 0.42 1
NR 0.53 0.24 1
AN -0.22 0.06 0.15 1
PC 0.4 0.89 0.28 0.11 1
SP 0.03 0.06 0.14 0.03 0.04 1
SL 0.08 0.15 0.01 -0.01 0.14 -0.13 1
Table 7: Correlation between Character Length (CL), Num-
ber of Rationales (NR), Annotator Nativeness (AN), Polar
word Count (PC), Stop word Percent (SP), average Sentence
Length (SL) and Annotation Time (AT), calculated over all
documents (125) and all annotators (20). Significant corre-
lations are highlighted in bold.
ness provides no evidence that reviews with different
lengths were distributed non-uniformly across anno-
tators with different nativeness.
The number of polar words in a document has a
similar correlation with annotation time as character
length (R = 0.4). There is also a strong correla-
tion between character length and polar word count
(R = 0.89). Since reviews are essentially people?s
opinions, we can expect longer documents to have
more polar words. This also explains why there is no
significant difference in performance for polar word
count and character length (Table 4). A more useful
feature may be the information about the number of
positive and negative polar words in a review, since a
review with both positive and negative opinions can
be difficult to classify as positive or negative. We
plan to explore these variations of the polar word
feature in the future. We also plan to investigate how
we can exploit this dependence between characteris-
tics for annotation cost estimation.
3.4.3 CRCoef Vs. RMS
We presented our results using correlation coef-
ficient and root mean squared error metrics. Ta-
ble 8 shows the ranking of the feature combinations
from better to worse for both these metrics and as
we can see, there is a difference in the order of fea-
ture combinations for the two metrics. Also, signif-
icance results differ in some cases for the two met-
rics. These differences suggest that features which
correlate well with the annotation times (higher CR-
Coef rank) can give an accurate ranking of examples
based on their annotation cost, but they may not be
as accurate in their absolute estimate for simulating
annotators and thus might have a lower RMS rank.
Thus, it is important to evaluate the user effort esti-
24
mator in terms of both these metrics so that the right
estimator can be chosen for a given objective.
Rank CR-Coef RMS
1 (CL+NR+AN) (CL+NR+AN)
2 (CL+NR) (CL+NR)
3 (NR+AN) (NR)
4 (NR) (NR+AN)
5 (CL+AN) (CL)
6 (CL) (CL+AN)
7 (AN) (AN)
Table 8: Ranking of feature combinations.
4 Towards a General Annotation Cost
Estimator
Our multi-annotator environment allows us to train
and test on data from different annotators by using
annotator characteristics as features in the annota-
tion cost estimation. A model trained on data from a
variety of annotators can be used for recommend-
ing examples to annotators not represented in our
training data but with similar characteristics. This
is important since we may not always know all our
annotators before building the model, and training
an estimator for each new annotator is costly. Also,
in active learning research, the goal is to evaluate
selective sampling approaches independently of the
annotator. Choosing annotators for supervised an-
notation cost estimation such that the within group
variance in annotator characteristics is high will give
us a more generic estimator and a stricter evaluation
criterion. Thus, we have a framework that has the
potential to be used to build a user-independent an-
notation cost estimator for a given task.
However, this framework is specific to the User
Interface (UI) used. A change in the user interface
might require recollecting the data from all the an-
notators and training a model on the new data. For
example, if annotating rationales was made signif-
icantly faster in a new UI design, it would have
a major impact on annotation cost. An alternative
would be to incorporate UI features in our model and
train it on several different UIs or modifications of
the same UI, which will allow us to use our trained
model with a new user interface or modifications of
the existing UIs, without having to recollect the data
and retrain the model. A few UI features that can be
used in our context are: adding a rationale annota-
tion, voting positive or negative, etc. The units for
expressing these features will be the low-level user
interface actions such as number of clicks, mouse
drags, etc. For example, in our task, adding a ra-
tionale annotation requires one mouse drag and two
clicks, and adding a vote requires one click. In a dif-
ferent user interface, adding a rationale annotation
might require just one mouse drag.
Using UI features raises a question of whether
they can replace the annotation task features; e.g.,
whether the UI feature for adding rationale anno-
tation can replace the number of rationales feature.
Our hypothesis is that number of rationales has more
influence on annotation time than just the manual ef-
fort of annotating them. It also requires the cognitive
effort of finding the rationale, deciding its span, etc.
We aim to explore incorporating UI features in our
annotation cost estimation model in the future.
5 Conclusion and Future Work
In this work we presented a detailed investigation of
annotation cost estimation for active learning with
multiple annotators. We motivated the task from two
perspectives: selecting examples to minimize anno-
tation cost and simulating annotators for evaluating
active learning approaches. We defined three cate-
gories of features based on instance, annotation task
and annotator characteristics. Our results show that
using a combination of features from all three cate-
gories performs better than any one of them alone.
Our analysis was limited to a small dataset. In the
future, we plan to collect a larger dataset for this task
and explore more features from each feature group.
With the multi-annotator annotation cost estima-
tor proposed, we also motivated the need for a gen-
eral estimator that can be used with new annotators
or user interfaces without having to retrain. We aim
to explore this direction in the future by extending
our model to incorporate user interface features. We
also plan to use the annotation cost model we devel-
oped in an active learning experiment.
Acknowledgments
We would like to thank Hideki Shima for his help
with the task setup and Jing Yang for helpful discus-
sions. We would also like to thank all the anony-
mous reviewers for their helpful comments.
25
References
Shilpa Arora and Eric Nyberg. 2009. Interactive An-
notation Learning with Indirect Feature Voting. In
Proceedings of NAACL-HLT 2009 (Student Research
Workshop).
Xavier Carreras and Llu?`s Ma?rquez. 2004. Intro-
duction to the CoNLL-2004 Shared Task: Seman-
tic Role Labeling. http://www.lsi.upc.edu/
?srlconll/st04/st04.html.
Gahgene Gweon, Carolyn Penstein Ros?e, Joerg Wittwer
and Matthias Nueckles. 2005. Supporting Efficient
and Reliable Content Analysis Using Automatic Text
Processing Technology. In proceedings of INTER-
ACT 2005: 1112-1115.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger and
Janes L. Cattoll. 2008. Return on Investment for Ac-
tive Learning. In proceedings of NIPS Workshop on
Cost Sensitive Learning.
Rebecca Hwa. 2000. Sample Selection for Statistical
Grammar Induction. In proceedings of joint SIGDAT
conference on Empirical Methods in NLP and Very
Large Corpora.
Ashish Kapoor, Eric Horvitz and Sumit Basu. 2007. Se-
lective supervision:Guiding supervised learning with
decision-theoretic active learning. In proceedings of
IJCAI, pages 877-882.
Ross D. King, Kenneth E. Whelan, Ffion M. Jones, Philip
G. K. Reiser, Christopher H. Bryant, Stephen H. Mug-
gleton, Douglas B. Kell and Stephen G. Oliver. 2004.
Functional Genomics hypothesis generation and ex-
perimentation by a robot scientist. In proceedings of
Nature, 427(6971):247-52.
Trausti Kristjansson, Aron Culotta, Paul Viola and An-
drew Mccallum. 2004. Interactive Information Ex-
traction with Constrained Conditional Random Fields.
In proceedings of AAAI.
Karl Pearson. 1895. Correlation Coefficient. Royal So-
ciety Proceedings, 58, 214.
Eric Ringger, Marc Carmen, Robbie Haertel, Kevin
Seppi, Deryle Lonsdale, Peter McClanahan, Janes L.
Cattoll and Noel Ellison. 2008. Assessing the Costs of
Machine-Assisted Corpus Annotation through a User
Study. In proceedings of LREC.
Burr Settles, Mark Craven and Lewis Friedland. 2008.
Active Learning with Real Annotation Costs. In pro-
ceedings of NIPS Workshop on Cost Sensitive Learn-
ing.
Alex J. Smola and Bernhard Scholkopf 1998. A Tutorial
on Support Vector Regression. NeuroCOLT2 Techni-
cal Report Series - NC2-TR-1998-030.
Katrin Tomanek, Joachim Wermter and Udo Hahn. 2007.
An approach to text corpus construction which cuts an-
notation costs and maintains reusability of annotated
data. In proceedings of EMNLP-CoNLL, pp. 486-
495.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In proceedings of
HLT/EMNLP, Vancouver, Canada.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. 2nd Edi-
tion, Morgan Kaufmann, San Francisco.
Omar Zaidan, Jason Eisner and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In Proceedings of
NAACL-HLT, pp. 260-267, Rochester, NY.
26
A Framework for Robust Semantic Interpretation 
Caro lyn  P.  Ros@ 
Learn ing  Research  and Deve lopment  Center  
Un ivers i ty  of  P i t t sburgh  
P i t t sburgh ,  PA 15260 
rosecp @pitt. edu 
Abst ract  
This paper describes AUTOSEM, a robust semantic 
interpretation framework that can operate both at 
parse time and repair time. The evaluation demon- 
strates that AUTOSEM achieves a high level of ro- 
bustness efficiently and without requiring any hand 
coded knowledge dedicated to repair. 
1 In t roduct ion  
In order for an approach to robust interpretation 
to be practical it must be efficient, address the ma- 
jor types of disfluencies that plague spontaneously 
produced language input, and be domain indepen- 
dent so thatachieving robustness in a particular do- 
main does not require an additional knowledge n- 
gineering effort. This paper describes AUTOSEM, 
a semantic interpretation framework that possesses 
these three qualities. While previous approaches 
to robust interpretation have offered robust parsers 
paired with separate repair modules~ with separate 
knowledge sources for each, AUTOSEM is a single 
unified framework that can operate both at parse 
time and repair time. AUTOSEM is integrated with 
the LCFLEx robust parser (Ros@ and Lavie, to ap- 
pear; Lavie and Ros@, 2000). Together AUTOSEM 
and LCFLEx constitute the robust understanding 
engine within the CARMEL natural language un- 
derstanding component developed in the context of 
the Atlas intelligent utoring project (Freedman at 
al., to appear). The evaluation reported here demon- 
strates that AUTOSEM's repair approach operates 
200 times faster than the most similar competing ap- 
proach while producing hypotheses of better quality. 
AUTOSEM provides an interface to allow seman- 
tic interpretation to operate in parallel with syntac- 
tic interpretation at parse time in a lexicon driven 
fashion. Domain specific semantic knowledge is en- 
coded declaratively within a meaning representation 
specification. Semantic constructor functions are 
compiled automatically from this specification and 
then linked into lexical entries as in the Glue Lan- 
guage Semantics approach to interpretation (Dal- 
rymple, 1999). Based on syntactic head/argument 
relationships assigned at parse time, the construc- 
tot functions enforce semantic selectiona\] restric- 
tions and assemble meaning representation struc- 
tures by composing the meaning representation asso- 
ciated with the constructor function with the mean- 
ing representation f each of its arguments. 
AUTOSEM first attempts to construct analy- 
ses that satisfy both syntactic and semantic well- 
formedness conditions. The LCFLEx parser has the 
ability to efficiently relax syntactic constraints as 
needed and as allowed by its parameterized flexi- 
bility settings. For sentences remaining beyond the 
parser's coverage, AUTOSEM's repair algorithm re- 
lies entirely on semantic knowledge to compose the 
partial analyses produced by the parser. Each se- 
mantic representation built by AUTOSEM's inter- 
pretation framework contains a pointer to the con- 
structor function that built it. Thus, each partial 
analysis can be treated as a constructor function 
with built in knowledge about how the associated 
partial analysis can be combined with other par- 
tial analyses in a semantically meaningful way. Ge- 
netic programming search (Koza, 1992; Koza, 1994) 
is used to efficiently compose the fragments pro- 
duced by the parser. The function definitions com- 
piled from the meaning representation specification 
allow the genetic search to use semantic onstraints 
to make effective use of its search space. Thus, AU- 
TOSEM operates efficiently, free of any hand coded 
repair rules or any knowledge specifically dedicated 
to repair unlike other approaches to recovery from 
parser failure (Danieli and Gerbino, 1995; Van No- 
ord, 1997; Kasper et al, 1999). 
2 The  Mean ing  Representat ion  
Spec i f i ca t ion  
At the heart of AUTOSEM is its interpretation 
framework composed of semantic onstructor func- 
tions compiled from a meaning representation spec- 
ification. These semantic onstructor functions can 
be used at parse time to build up semantic represen- 
tations. These same constructor functions can then 
be used in a repair stage to compose the fragments 
returned by the parser in the cases where the parser 
is not able to obtain a complete analysis for an ex- 
311 
(:type <*state> 
:isa (<>) 
:instances nil 
:vars (entity time duration polarity) 
:spec ((who <*entity> entity) 
(when <*when> time) 
(how-long <*time-length> duration) 
(negation \[+/-\] polarity))) 
(:type <*personal-state> 
:isa (<*state>) 
:instances nil 
:vars () 
:spec ((who <*who> entity))) 
(:type <busy> 
:isa (<*personal-state>) 
:instances nil 
:vars (activity) 
:spec ((frame *busy) 
(event <*event> activity))) 
( : type  \[+/-\] 
: i sa  (<>) 
:instances (+ -) 
:vars nil 
:spec nil) 
Figure 1: Sample  mean ing  representat ion  
spec i f icat ion ent r ies  
tragrammatical input sentence. 
The meaning representation specification pro- 
vides a venue for expressing domain specific se- 
mantic information declaratively. AUTOSEM pro- 
duces frame-based meaning representation struc- 
tures. Thus, each domain specific meaning repre- 
sentation specification must define a set of semantic 
types that together specify the set of frames and 
atomic feature values that make up the domain spe- 
cific frame-based language, which slots are associ- 
ated with each frame, and what range of frames and 
atomic feature values may fill each of those slots. 
AUTOSEM provides a simple formalism for defin- 
ing meaning representations. Each entry corre- 
sponds to a semantic type and contains five fields: 
: type,  : i sa ,  : ins tances ,  :vars ,  and :spec. Some 
sample entries for the appointment scheduling do- 
main are displayed in Figure 1. Some details are 
omitted for simplicity. The : type field simply con- 
talus the name of the type. The : vars  field contains 
a list of variables, each corresponding to a semantic 
role. The :spec field associates a frame and set of 
slots with a type. For each slot, the : spec field con- 
rains the name of the slot, the most general type re- 
striction on the slot, and a specification of where the 
slot filler comes from. This third piece of information 
can be either a variable name, indicating that what- 
ever is bound to that variable is what should fill that 
slot, or a function call to another semantic onstruc- 
tor function, allowing types to specify constraints at 
more than one level of embedding. Similar to the 
: spec field, the : ins tances  field associates a list of 
atomic values with a type. Inheritance relations are 
defined via the : i sa  field. Types inherit the values 
of each subsuming type's : ins tances ,  :vars ,  and 
: spec fields. 
3 Semant ic  In terpretat ion  a t  Parse  
T ime 
(:type <cancel> 
:isa (<*event>) 
: instances nil 
:vats (agent activity time polarity) 
:spec ((frame *cancel) 
(engagement <*event> activity))) 
Figure 2: Mean ing  representat ion  def in i t ion  of  
<cance l> 
(:morph cancel 
:syntax ((cat vlex) (root cancel) 
(vform bare) (irreg-past +) 
(irreg-pastpart +) 
(irreg-prespart +) 
(subcat (*or* intrans np)) 
(semtag cancel1)) 
:semantics (cancel1 <cancel> 
((subject agent) 
(object activity) 
(tempadjunct time) 
(negation polarity)))) 
Figure 3: Lex ica l  ent ry  for  the  verb  "cance l "  
As an extension to LCFLEx's LFG-like pseudo- 
unification grammar formalism, AUTOSEM pro- 
vides the inser t - ro le  function as an interface to 
allow semantic interpretation to operate in parallel 
with syntactic interpretation at parse time. When 
the insert-role function is used to insert a child 
constituent into the slot corresponding to its syntac- 
tic functional role in a parent constituent, the child 
constituent's semantic representation is passed in to 
the parent constituent's semantic onstructor func- 
tion as in the Glue Language Semantics approach 
to interpretation (Dalrymple, 1999). AUTOSEM's 
lexicon formalism allows semantic onstructor func- 
tions to be linked into lexical entries by means of 
the semtag feature. Each semtag feature value cor- 
responds to a semantic constructor function and 
312 
mappings between syntactic functional roles such 
as sub jec t ,  d i rec t  ob ject ,  and ind i rec t  object 
and semantic roles such as agent,  ac t iv i ty ,  or 
time. See Figures 2 and 3 discussed further be- 
low. Note that the syntactic features that appear 
in this example are taken from the COMLEX lex- 
icon (Grishman et al, 1994). In order to provide 
consistent input to the semantic onstructor func- 
tious, AUTOSEM assumes a syntactic approach in 
which deep syntactic functional roles are assigned as 
in CARMEL's syntactic parsing grammar evaluated 
in (Freedman et al, to appear). In this way, for ex- 
ample, the roles assigned within an active sentence 
and its corresponding passive sentence remain the 
same. 
Since the same constructor function is called with 
different arguments a number of times in order to 
construct an analysis incrementally, an argument is 
included in every constructor function that allows 
a "result so far" to be passed in and augmented. 
Its default value, which is used the first time the 
constructor function is executed, is the representa- 
tion associated with the corresponding type in the 
absence of any arguments being instantiated. Each 
time the constructor function is executed, each of its 
arguments that are instantiated are first checked to 
be certain that the structures they are instantiated 
with match all of the type restrictions on all of the 
slots that are bound to that argument. If they are, 
the instantiated arguments' tructures are inserted 
into the corresponding slots in the "result so far". 
Otherwise the constructor function fails. 
Take as an example the sentence "The meeting I 
had scheduled was canceled by you." as it is pro- 
cessed by AUTOSEM using the CARMEL grammar 
and lexicon, which is built on top of the COMLEX 
lexicon (Grishman et al, 1994). The grammar as- 
signs deep syntactic functional roles to constituents. 
Thus, "you" is the deep subject of "cancel", and 
"the meeting" is the direct object both of "cancel" 
and of "schedule". The detailed subcategorization 
classes associated with verbs, nouns, and adjectives 
in COMLEX make it possible to determine what 
these relationships hould be. The meaning repre- 
sentation entry for <cancel> as well as the lexical 
entry for the verb "cancel" are found in Figures 2 
and 3 respectively. Some details are left out for sim- 
plicity. When "the meeting I had scheduled" is ana- 
lyzed as the surface subject of "was canceled", it is 
assigned the deep syntactic role of object since "was 
canceled" is passive. The verb "cancel" has cance l l  
as its semtag value in the lexicon, cancel  1 is defined 
there as being associated with the type <cancel>, 
and the ob jec t  syntactic role is associated with the 
ac t iv i ty  argument. Thus, the <cancel> function 
is called with its ac t iv i ty  argument instantiated 
with the meaning of "the meeting I had scheduled". 
Next, when "by you" is attached, "you" is assigned 
the deep syntactic role of subject of "cancel". The 
sub jec t  role is associated with the agent argument 
in the definition of cance l l .  Thus, <cancel> is 
called a again, this time with "you" instantiating 
the agent argument and the result from the last 
call to <cancel> passed in through the "result so 
far" argument. 
4 Semantic Interpretation for Repair 
While the LCFLEX parser has been demonstrated 
to robustly parse a variety of disfluencies found in 
spontaneously generated language (Ros~ and Lavie, 
to appear), sentences till remain that are beyond 
its coverage. Previous research involving the ear- 
lier GLR* parser (Lavie, 1995) and an earlier repair 
module (Rosd, 1997) has demonstrated that divid- 
ing the task of robust interpretation i to two stages, 
namely parsing and repair, provides a better trade 
off between run time and coverage than attempt- 
ing to place the entire burden of robustness on the 
parser alone (Ros$, 1997). Thus, when the flexibility 
allowed at parse time is not sufficient o construct an 
analysis of an entire sentence for any reason, a frag- 
mentary analysis is passed into the repair module. 
For each pair of vertices in the chart the best single 
clause level and noun phrase level analysis accord- 
ing to LCFLEx's statistical disambiguation scores is 
included in the set of fragmentary analyses passed 
on to the repair stage. An example 1 is displayed in 
Figure 4. Here the sentence "Why don't we make it 
from like eleven to one?" failed to parse. In this case, 
the problem is that the insertion of "like" causes the 
sentence to be ungrammatical. When the parser's 
flexibility settings are such that it is constrained to 
build analyses only for contiguous portions of text, 
such an insertion would prevent he parser from con- 
structing an analysis covering the entire sentence. 
Nevertheless, it is able to construct analyses for a 
number of grammatical subsets of it. 
Genetic programming search (Koza, 1992; Koza, 
1994) is used to search for different ways to combine 
the fragments. Genetic programming is an oppor- 
tunistic search algorithm used for constructing com- 
puter programs to solve particular problems. Among 
its desirable properties is its ability to search a large 
space efficiently by first sampling widely and shal- 
lowly, and then narrowing in on the regions sur- 
rounding tile most promising looking points. 
ZThls example was generated with the grammar used in 
the evaluation. See Section 6. The AUTOSEM repair algo- 
rithm can he used with grammars that do not make use of 
AUTOSEM's parse-time interface by using a simple conver- 
sion program that automatically builds a function for each 
partial analysis corresponding to its semantic type. 
313 
Sentence: Why don't we make it from like eleven to one? 
Functions: 
(<SCHEDULE> NIL T NIL NIL NIL NIL NIL NIL T 
NIL NIL 
:STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S) 
(WHAT ((FRAME *IT)))) 
:COV (6 5 4 3 2 1) 
:SCORE 1.4012985e-45) 
(<SCHEDULE> NIL T NIL NIL NIL T T NIL NIL 
NIL NIL 
:STR ((FRAME *SCHEDULE) 
(WHAT ((FRAME *IT))) (NEGATIVE +) 
(WHO ((FRAME *WE)))) 
:COV (6 5 4 3 2) :SCORE 3.05483e-43) 
(<SCHEDULE> NIL NIL NIL NIL NIL NIL NIL 
NIL T NIL NIL 
:STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S)) 
:COV (5 4 3 2 1) 
:SCORE 1.4012985e-45) 
(<INTERVAL> NIL T T T 
:STR ((END ((FRAME *SIMPLE-TIME) 
(HOUR 1))) 
(START ((FRAME *SIMPLE-TIME) 
(HOUR 11)))  
(INCL-EXCL INCLUSIVE) 
(FRAME *INTERVAL)) 
:COV (11 10 9) 
:SCORE 1.2102125e-38) 
(<SCHEDULE> NIL NIL NIL NIL NIL T T NIL NIL 
NIL NIL 
:STR ((FRAME *SCHEDULE) (NEGATIVE +) 
(WHO ((FRAME *WE)))) 
:COV (5 4 3 2) 
:SCORE 2.2584231e-37) 
(<SCHEDULE> NIL T NIL NIL NIL T NIL NIL NIL 
NIL NIL 
:STR ((FRAME *SCHEDULE) (WHAT ((FRAME *IT))) 
(WHO ((FRAME *WE)))) 
:COV (6 5 4) 
:SCORE 1.861576e-27) 
(<PRO> NIL NIL NIL 
:STR ((FRAME *PRO)) 
:COV (II) 
:SCORE 2.2223547e-18) 
(<SIMPLE-TIME> NIL NIL NIL NIL NIL NIL T NIL 
:STR ((FRAME *SIMPLE-TIME) (HOUR 1)) 
:C0V (11) 
:SCORE 2.2223547e-18) 
(<SIMPLE-TIME> NIL NIL NIL NIL NIL NIL 
T NIL 
:STR ((FRAME *SIMPLE-TIME) (HOUR II)) 
:COV (9) 
:SCORE 1.0090891e-22) 
(<IT> 
:STR ((FRAME *IT)) 
:COV (6) 
:SCORE 2.593466e-13) 
Ideal Program: 
(<SCHEDULE> NIL NIL NIL NIL NIL NIL NIL 
(<INTERVAL> NIL NIL NIL NIL 
:STR ((END ((FRAME *SIMPLE-TIME) (HOUR I))) 
(START 
((FRAME *SIMPLE-TIME) (HOUR Ii))) 
(INCL-EXCL INCLUSIVE) 
(FRAME *INTERVAL)) 
:COV (II I0 9) 
:SCORE 1.2102125e-38) 
NIL NIL NIL 
:STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S) 
(WHAT ((FRAME *IT)))) 
:COV (6 5 4 3 2 I) 
:SCORE 1.4012985e-45) 
Interpretation: Let's schedule it for from eleven o'clock till one o'clock 
Figure 4: Repair example 
314. 
It first takes a list of functions and terminal sym- 
bols and randomly generates a population of pro- 
grams. It then evaluates each program for its "fit- 
hess" according to some predetermined set of crite- 
ria. The most fit programs are then paired up and 
used to produce the next generation by means of a 
crossover operation whereby a pair of subprograms, 
one from each parent program, are swapped. The 
new generation is evaluated for its fitness, and the 
process continues for a preset number of generations. 
As mentioned earlier, because ach semantic rep- 
resentation built by AUTOSEM contains a pointer 
to the constructor function that built it, each partial 
analysis can itself be treated as a constructor func- 
tion. Thus, the function set made available to the 
genetic programming search for each sentence need- 
ing repair is derived from the set of partial anal- 
yses extracted from the parser's chart. A number 
of the functions produced for the example are dis- 
played in Figure 4. Some functions have been omit- 
ted for brevity. The functions are displayed as func- 
tion calls, with the name of the function followed 
by its arguments. The name of each function corre- 
sponds to the semantic type from the meaning rep- 
resentation that corresponds to the associated par- 
tial analysis. Following this is a list of place holders 
corresponding to each argument position associated 
with the semantic type, as described in Section 2. 
Each place holder is either n i l  if it is an open place 
holder, or 1: if the position has already been filled 
in the corresponding partial analysis. The STR field 
contains the corresponding partial analysis. This is 
the "result so far" parameter discussed in Section 
3. The C0V field lists the positions in the sentence 
covered by the partial analysis. Note that in the 
example sentence, the word "don't" covers both po- 
sitions 2 and 3 since the parser expands the con- 
traction before parsing. The SCORE field contains 
the statistical score assigned by the parser's tatis- 
tical disambiguation procedure described in (Ros~ 
and Lavie, to appear). 
The repair process begins as the genetic program- 
ming algorithm composes the function definitions 
into programs that assemble the fragments produced 
by the parser. The genetic programming algorithm 
has access to a list of type restrictions that are placed 
on each argument position by the meaning repre- 
sentation specification. Thus, the algorithm ensures 
that the programs that are generated o not vio- 
late any of the meaning representation's type restric- 
tions. 
Once a population of programs is generated ran- 
domly, each program is evaluated for its fitness. A 
simple function implements a preference for pro- 
grams that cover more of the sentence with fewer 
steps while using the analyses the parser assigned 
the best statistical scores to. A score between 0 and 
1 is first assigned to each program corresponding to 
the percentage of the input sentence it covers. A 
second score between 0 and 1 estimates how com- 
plicated the program is by dividing the number of 
function calls by the length of the sentence and sub- 
tracting this number from 1. A third score is as- 
signed the average of the statistical scores assigned 
by the parser to the fragments used in the program. 
Using coefficients based on an intuitive assignment of 
relative importance to the three scores, the final fit- 
ness value of each program is 1 - \[(.55 *coverageS) +
(.25 ? complexityS) + (.2 ? statisticalS)\]. 
A typed version of the original crossover algorithm 
described in (Koza, 1992; Koza, 1994) was used to 
ensure that new programs would not violate any 
type restrictions or include more than one partial 
analysis covering the same span of text. This was 
accomplished by first making for each subprogram a 
list of the subprograms from the alternate program it 
could be inserted into without violating any seman- 
tic constraints. From these two lists it is possible to 
generate a list of all quadruples that specify a sub- 
program from each parent program to be removed 
and which subprogram from the alternate parent 
program they could be inserted into. From this list, 
all quadruples were removed that would either cause 
a span of text to be covered more than once in a 
resulting program or would require a subprogram 
to be inserted into a subprogram that would have 
been removed. From the remaining list, a quadruple 
was selected randomly. The corresponding crossover 
operation was then executed and the resulting two 
new programs were returned. While this typed vet- 
sion of crossover is more complex than the original 
crossover operation, it can be executed very rapidly 
in practice because the programs are relatively small 
and the semantic type restrictions ensure than the 
initial lists generated are correspondingly small. 
5 Re lated  Work 
Recent approaches to robust parsing focus on shal- 
low or partial parsing techniques (Van Noord, 1997; 
Worm, 1998; Ait-Mokhtar and Chanod, 1997; Ab- 
ney, 1996). Rather than attempting to construct a 
parse covering an entire ungrammatical sentence as 
in (Lehman, 1989; Hipp, 1992), these approaches at- 
tempt to construct analyses for maximal contiguous 
portions of the input. The weakness of these partial 
parsing approaches i that part of the original mean- 
ing of the utterance may be discarded with the por- 
tion(s) of the utterance that are skipped in order to 
find a parsable subset. Information communicated 
by the relationships between these fragments within 
the original text is lost if these fragments are not 
combined. Thus, these less powerful algorithms es- 
sentially trade effectiveness for efficiency. Their goal 
is to introduce enough flexibility to gain an accept- 
315 
able level of coverage at an acceptable computational 
expense. 
Some partial parsing approaches have been cou- 
pled with a post-parsing repair stage (Danieli and 
Gerbino, 1995; Ros6 and Waibel, 1994; Ros6, 1997; 
Van Noord, 1997; Kasper et al, 1999) The goal 
behind these two stage approaches is to increase 
the coverage over partial parsing alone at a rea- 
sonable computational cost. Until the introduction 
of AUTOSEM, the ROSE approach, introduced in 
(Ros6, 1997), was unique in that it achieved this 
goal without either requiring hand coded knowledge 
specifically dedicated to repair or excessive amounts 
of interaction with the user. However, although 
(Ros6, 1997) demonstrates that the two stage ROSE 
approach is significantly faster than attempting to 
achieve the same quality of results in a single stage 
parsing approach, our evaluation demonstrates that 
it remains computationally intractable, requiring on 
average 67 seconds to repair a single parse on a 330 
MHz Gateway 2000. In contrast, we demonstrate 
that AUTOSEM is on average 200 times faster, tak- 
ing only .33 seconds on average to repair a single 
parse while achieving results of superior quality. 
6 Eva luat ion  
An experiment was conducted to evaluate AU- 
TOSEM's robustness by comparing the effectiveness 
and efficiency of AUTOSEM's repair approach with 
that of the alternative ROSE approach. The test 
set used for this evaluation contains 750 sentences 
extracted from a corpus of spontaneous scheduling 
dialogues collected in English. For both repair ap- 
proaches we used the meaning representation devel- 
oped for the appointment scheduling domain that 
was used in previous evaluations of the ROSE ap- 
proach (Ros6, 1997). It consists of 260 semantic 
types, each expressing domain specific concepts for 
the appointment scheduling domain such as busy, 
cancel ,  and out-of-~;own. The ROSE meaning rep- 
resentation specification was easily converted to the 
format used in AUTOSEM. Because a pre-existing 
semantic grammar was available that parsed directly 
onto this meaning representation, that grammar was 
used in the parsing stage to construct analyses. The 
final meaning representation structures for the first 
300 sentences were then passed to a generation com- 
ponent, and the resulting texts were graded by a 
human judge not involved in developing the research 
reported here. Each result was graded as either Bad, 
Okay, or Perfect, where Perfect indicates that the re- 
sult was fluent and communicated the idea from the 
original sentence. A grade of Okay indicates that 
the result communicated the correct information, 
but not fluently. Those graded Bad either communi- 
cated incorrect information or were missing part or 
all of the information communicated in the original 
sentence. 
Each sentence was parsed in two different modes. 
In LC w/ res tar ts  mode, the parser was allowed to 
construct analyses for contiguous portions of input 
starting at any point in the sentence. In LCFLEx 
mode, the parser was allowed to start an analysis at 
any point and skip up to three words within the anal- 
ysis. Because the AUTOSEM repair algorithm runs 
significantly faster than the ROSE repair algorithm, 
repair was attempted after every parse rather than 
only when a parse quality heuristic indicated a need 
as in the ROSE approach (Ros6, 1997). We com- 
pared the results of both AUTOSEM and ROSE in 
conjunction with the LC w/ res tar ts  parsing mode. 
The results are displayed in Figures 5 and 7. Be- 
cause the ROSE approach only runs the full repair 
algorithm when its parse quality heuristic indicates 
a need and the parser returns more than one par- 
tial analysis, it only attempted repair for 14% of the 
sentences in the corpus. Nevertheless, although the 
AUTOSEM repair algorithm ran for each sentence, 
Figure 5 demonstrates that processing time for pars- 
ing plus repair in the AUTOSEM condition was dra- 
matically faster on average than with ROSE. Aver- 
age processing time for the ROSE algorithm was 200 
times slower than that for AUTOSEM on sentences 
where both repair algorithms were used. In addition 
to the advantage in terms of speed, the AUTOSEM 
repair approach achieved an acceptable grade (Okay 
or Perfect) on approximately 4% more sentences. 
Parsing in LC w/ res tar ts  mode plus repair was 
also compared with parsing in LCFLEx mode with 
skipping up to three words. Again, LC w/ res tar ts  
+ AUTOSEM repair achieved a slightly higher 
number of acceptable grades, although LCFLEx 
achieved a slightly higher number of Perfect grades. 
On long sentences (between 15 and 20 words), 
LCFLEx mode required almost three times as much 
time as LC w/ res tar ts  mode plus AUTOSEM re- 
pair. This evaluation confirms our previous results 
that two stage approaches offer a better processing 
time versus robustness trade-off. 
The primary difference between ROSE and AU- 
TOSEM is that ROSE uses a single repair function, 
MY-C0bIB, to combine any two fragments by referring 
to the meaning representation specification. While 
it is possible to obtain the same set of repair hy- 
potheses with ROSE as with AUTOSEM, the ROSE 
approach insulates the genetic search from the se- 
mantic restrictions imposed by the meaning repre- 
sentation. These restrictions are visible only locally 
within individual applications of the I~Y-COMB func- 
tion. Thus, FIY-COMB must be able to cope with the 
case where the arguments passed in cannot be com- 
bined. Large portions of the programs generated by 
ROSE as repair hypotheses do not end up contribut- 
ing to the resulting structure. The programs gener- 
316 
45 
40 
35 
3O 
.~ 25 
p- 
20 
15 
10 
5 
i i , 
ROSE ~ " 
CARMEL - - -x - - -  
)e-. . . . . . . .  "~ . . . . . . . . .  "~ . . . . . . . . .  k,- . . . . . . . . .  x . . . . . . . . . . . . . . . . . . . . . . . . .  
5 10 15 20 
Sentence Length 
Figure 5: Processing Times for Alternative Strategies 
14 
12 
10 
8 
E 
o. 6 
0;  
0 
' ' LCFLEX + repair 
LCFLEX 
LC w restarts + repair --.-m--- 
LC w restarts .---.~ ...... 
/ / / "  ..- : .~2 2. L-.:.:2 2 2.:.:.:2.:2  2 2 2 2:.2-" 2;:2 
5 '10 15 20 
Sentence Length 
Figure 6: Processing Times for Alternative Strategies 
ated by ROSE must therefore be much larger than in 
AUTOSEM in order to obtain the same results. Fur- 
thermore, the fitness of each repair hypothesis can 
only be computed by executing the program to ob- 
tain a result. The combination of all of these things 
makes the process of fitness evaluation in ROSE far 
more costly than in AUTOSEM. In contrast, AU- 
TOSEM's constructor function definitions make it 
possible for the genetic search to make use of seman- 
tic restrictions to speed up the process of converging 
on a high quality repair hypothesis. The tremendous 
speed-up offered by the AUTOSEM approach makes 
it practical to apply repair more often and to use a 
larger generation size (50 individuals as opposed to 
32) and a larger number of generations (5 as opposed 
to 4) for the genetic search. 
7 Cur rent  D i rec t ions  
In this paper we described AUTOSEM, a new robust 
semantic interpretation framework. Our evaluation 
demonstrates that AUTOSEM achieves a greater 
level of robustness 200 times more efficiently than 
the most similar competing approach. 
In AUTOSEM, the mapping between syntactic 
317 
Bad Okay Perfect 
LC w/restarts 92 (30.7%) 61 (20.3%) i47 (49.0%) 
LCFL~x 72 (24.0%) 67 (22.3%) 161 (53.7%) 
LC w/restarts + ROSE 75 (25.0%) 68 (22.7%) 157 (52.3%) 
LC w/restarts + AUTOSEM 64 (21.3%) 84 (28.0%) 152 (50.7%) 
LCFLEx + AUTOSEM 64 (21.3%) 78 (26.0%) i58 (52.7%) 
Total Acceptable 
209 (69.3%) 
228 (76.0%) 
225 (75.0%) 
236 (78.7%) 
236 (78.7%) 
Figure 7: Interpretat ion quality with and without  repair 
functional roles and semantic arguments i com- 
pletely determined in the current version. In some 
cases, such as with copular constructions and with 
adjunct prepositional phrases, it would be useful to 
introduce some non-determinism o that, for exam- 
ple, semantic selectional restrictions between the ob- 
ject. of the preposition and the semantic structure 
that the prepositional phrase is attaching to can 
more easily play a role in selecting the appropri- 
ate semantic relationship. Exploring approaches for 
achieving this non-determinism efficiently is one of 
our current objectives. 
8 Acknowledgements 
Special thanks are due to the JANUS multi- 
lingual speech-to-speech translation project for mak- 
ing their interlingua specification and semantic pars- 
ing and generation grammars available for the eval- 
uation reported here. This research was supported 
by NSF Grant IRI-94-57637 and Grant 9720359 to 
CIRCLE, a center for research on intelligent tutor- 
ing. 
References 
S. Abney. 1996. Partial parsing via finite-state 
cascades. In Proceedings of the Eighth European 
Summer School In Logic, Language and Informa- 
tion, Prague, Czech Republic. 
S. Ait-Mokhtar and J. Chanod. 1997. Incremental 
finite-state parsing. In Proceedings of the Fifth 
Conference on Applied Natural Language Process- 
ing. 
M. Dalrymple. 1999. Semantics and Syntax in Lex- 
ical Functional Grammar. The MIT Press. 
M. Danieli and E. Gerbino. 1995. Metrics for evalu- 
ating dialogue strategies in a spoken language sys- 
tem. In Working Notes of the AAAI Spring Sym- 
posium on Empirical Methods in Discourse Inter- 
pretation and Generation. 
R. Freedman, C. P. Ros6, M. A. Ringenberg, and 
K. VanLehn. to appear. Its tools for natural 
language dialogue: A domain-independent parser 
and planner. In Proceedings of the Intelligent Tu- 
toring Systems Conference. 
R. Grishman, C. Macleod, and A. Meyers. 1994. 
COMLEX syntax: Building a computational lexi- 
con. In Proceedings of the 15th International Con- 
ference on Computational Linguistics (COLING- 
94). 
D. R. Hipp. 1992. Design and Development of 
Spoken Natural-Language Dialog Parsing Systems. 
Ph.D. thesis, Dept. of Computer Science, Duke 
University. 
W. Kasper, B. Kiefer, H. Krieger, C. Rupp, and 
K. Worm. 1999. Charting the depths of robust 
speech parsing. In Proceedings of the 37th An- 
nual Meeting of the Association for Computa- 
tional Linguistics. 
J. Koza. 1992. Genetic Programming: On the Pro- 
gramming of Computers by Means of Natural Se- 
lection. MIT Press. 
J. Koza. 1994. Genetic Programming H. MIT Press. 
A. Lavie and C. P. Ros~. 2000. Optimal ambi- 
guity packing in unification-augmented context- 
free grammars. In Proceedings of the International 
Workshop on Parsing Technologies. 
A. Lavie. 1995. A Grammar Based Robust Parser 
For Spontaneous Speech. Ph.D. thesis, School of 
Computer Science, Carnegie Mellon University. 
J. F. Lehman. 1989. Adaptive Parsing: Self- 
Extending Natural Language Interfaces. Ph.D. 
thesis, School of Computer Science, Carnegie Mel- 
lon University. 
C. P. Ros~ and A. Lavie. to appear. BMancing ro- 
bustness and efficiency in unification augmented 
context-free parsers for large practical applica- 
tions. In J. C. Junqua and G. Van Noord, editors, 
Robustness in Language and Speech Technologies. 
Kluwer Academic Press. 
C. P. Ros~ and A. Waibel. 1994. Recovering from 
parser failures: A hybrid statistical/symbolic ap-
proach. In Proceedings of The Balancing Act: 
Combining Symbolic and Statistical Approaches to 
Language workshop at the 32nd Annual Meeting of 
the A CL. 
C. P. Ros& 1997. Robust Interactive Dialogue Inter- 
pretation. Ph.D. thesis, School of Computer Sci- 
ence, Carnegie Mellon University. 
G. Van Noord. 1997. An efficient implementation f 
the head-corner parser. Computational Linguis- 
tics, 23(3). 
K. Worm. 1998. A model of robust processing of 
spontaneous speech by integrating viable frag- 
ments. In Proceedings of COLING-A CL 98. 
318 
A Hybrid Approach to Content Analysis for Automatic Essay Grading
Carolyn P. Rose?, Antonio Roque, Dumisizwe Bhembe, and Kurt VanLehn
LRDC, University of Pittsburgh, 3939 O?hara St., Pittsburgh, PA 15260
rosecp@pitt.edu
Abstract
We present CarmelTC, a novel hybrid text clas-
sification approach for automatic essay grad-
ing. Our evaluation demonstrates that the hy-
brid CarmelTC approach outperforms two ?bag
of words? approaches, namely LSA and a Naive
Bayes, as well as a purely symbolic approach.
1 Introduction
In this paper we describe CarmelTC
 
, a novel automatic
essay grading approach using a hybrid text classifica-
tion technique for analyzing essay answers to qualitative
physics questions inside the Why2 tutorial dialogue sys-
tem (VanLehn et al, 2002). In contrast to many previ-
ous approaches to automated essay grading (Burstein et
al., 1998; Foltz et al, 1998; Larkey, 1998), our goal is
not to assign a letter grade to student essays. Instead, our
purpose is to tally which set of ?correct answer aspects?
are present in student essays. Previously, tutorial dia-
logue systems such as AUTO-TUTOR (Wiemer-Hastings
et al, 1998) and Research Methods Tutor (Malatesta et al,
2002) have used LSA (Landauer et al, 1998) to perform
the same type of content analysis for student essays that
we do in Why2. While Bag of Words approaches such as
LSA have performed successfully on the content analy-
sis task in domains such as Computer Literacy (Wiemer-
Hastings et al, 1998), they have been demonstrated to
perform poorly in causal domains such as research meth-
ods (Malatesta et al, 2002) because they base their pre-
dictions only on the words included in a text and not on
the functional relationships between them. Thus, we pro-
pose CarmelTC as an alternative. CarmelTC is a rule
learning text classification approach that bases its predic-
tions both on features extracted from CARMEL?s deep

This research was supported by the ONR, Cognitive Sci-
ence Division under grant number N00014-0-1-0600 and by
NSF grant number 9720359 to CIRCLE.
syntactic functional analyses of texts (Rose?, 2000) and a
?bag of words? classification of that text obtained from
Rainbow Naive Bayes (McCallum and Nigam, 1998).
We evaluate CarmelTC in the physics domain, which is
a highly causal domain like research methods. In our
evaluation we demonstrate that CarmelTC outperforms
both Latent Semantic Analysis (LSA) (Landauer et al,
1998) and Rainbow Naive Bayes (McCallum and Nigam,
1998), as well as a purely symbolic approach similar to
(Furnkranz et al, 1998). Thus, our evaluation demon-
strates the advantage of combining predictions from sym-
bolic and ?bag of words? approaches for content analysis
aspects of automatic essay grading.
2 Student Essay Analysis
We cast the Student Essay Analysis problem as a text clas-
sification problem where we classify each sentence in the
student?s essay as an expression one of a set of ?correct
answer aspects?, or ?nothing? in the case where no ?cor-
rect answer aspect? was expressed. Essays are first seg-
mented into individual sentence units. Next, each seg-
ment is classified as corresponding to one of the set of key
points or ?nothing? if it does not include any key point.
We then take an inventory of the classifications other than
?nothing? that were assigned to at least one segment. We
performed our evaluation over essays collected from stu-
dents interacting with our tutoring system in response to
the question ?Suppose you are running in a straight line at
constant speed. You throw a pumpkin straight up. Where
will it land? Explain.?, which we refer to as the Pumpkin
Problem. Thus, there are a total of six alternative classifi-
cations for each segment:
Class 1 After the release the only force acting on the
pumpkin is the downward force of gravity.
Class 2 The pumpkin continues to have a constant hori-
zontal velocity after it is released.
Class 3 The horizontal velocity of the pumpkin contin-
ues to be equal to the horizontal velocity of the man.
Class 4 The pumpkin and runner cover the same distance
over the same time.
Class 5 The pumpkin will land on the runner.
Class 6 Sentence does not adequately express any of the
above specified key points.
Often what distinguishes sentences from one class and
another is subtle. For example, ?The pumpkin?s horizon-
tal velocity, which is equal to that of the man when he re-
leased it, will remain constant.? belongs to Class 2. How-
ever, it could easily be mistaken for Class 3 based on the
set of words included, although it does not express that
idea since it does not address the relationship between the
pumpkin?s and man?s velocity after the release. Similarly,
?So long as no other horizontal force acts upon the pump-
kin while it is in the air, this velocity will stay the same.?,
belongs to Class 2 although looks similar on the surface to
either Class 1 or 3. Nevertheless, it does not express the
required propositional content for either of those classes.
The most frequent problem is that sentences that express
most but not all of the content associated with a required
point should be classified as ?nothing? although they have
a lot of words in common with sentences from the class
that they are most similar to. Similarly, sentences like ?It
will land on the ground where the runner threw it up.?
contain all of the words required to correctly express the
idea corresponding to Class 5, although it does not ex-
press that idea, and in fact expresses a wrong idea. These
very subtle distinctions pose problems for ?bag of words?
approaches since they base their decisions only on which
words are present regardless of their order or the func-
tional relationships between them.
The hybrid CarmelTC approach induces decision trees
using features from a deep syntactic functional analysis
of an input text as well as a prediction from the Rainbow
Naive Bayes text classifier (McCallum and Nigam, 1998).
Additionally, it uses features that indicate the presence or
absence of words found in the training examples. From
these features CarmelTC builds a vector representation
for each sentence. It then uses the ID3 decision tree learn-
ing algorithm (Quinlin, 1993) to induce rules for identify-
ing sentence classes based on these feature vectors.
From CARMEL?s deep syntactic analysis of a sen-
tence, we extract individual features that encode func-
tional relationships between syntactic heads (e.g., (subj-
throw man)), tense information (e.g., (tense-throw past)),
and information about passivization and negation (e.g.,
(negation-throw +) or (passive-throw -)). Syntactic fea-
ture structures produced by the grammar factor out those
aspects of syntax that modify the surface realization of
a sentence but do not change its deep functional analy-
sis, including syntactic transformations such as passiviza-
tion and extraction. These deep functional relationships
give CarmelTC the information lacking on Bag of Words
approaches that is needed for effective content analysis
in highly causal domains, such as research methods or
physics.
3 Evaluation
We conducted an evaluation to compare the effective-
ness of CarmelTC at analyzing student essays in compar-
ison to LSA, Rainbow, and a purely symbolic approach
similar to (Furnkranz et al, 1998), which we refer to
here as CarmelTCsymb. CarmelTCsymb is identical to
CarmelTC except that it does not include in its feature set
the prediction from Rainbow. We conducted our evalua-
tion over a corpus of 126 previouslyunseen student essays
in response to the Pumpkin Problem described above,
with a total of 500 text segments, and just under 6000
words altogether. Each text segment was hand tagged
by at least two coders, and conflicts were resolved at a
consensus meeting. Pairwise Kappas between our three
coders computed over initial codings of our data was al-
ways above .75.
The LSA space used for this evaluation was trained
over three first year physics text books. The Rainbow
models used to generate the Rainbow predictions that are
part of the feature set provided to CarmelTC were trained
over a development corpus of 248 hand tagged example
sentences extracted from a corpus of human-human tu-
toring dialogues, just like those included in the 126 es-
says mentioned above. However, when we evaluated the
performance of Rainbow for comparison with CarmelTC,
LSA, and the symbolic approach, we ran a 50 fold cross
validation evaluation using the complete set of examples
in both sets (i.e., the 248 sentences used to train the Rain-
bow models used to by CarmelTC as well as the 126 es-
says) so that Rainbow would have access to the exact
same training data as CarmelTC, to make it a fair com-
parison between alternative machine learning approaches.
On each iteration, we randomly selected a subset of essays
such that the number of text segments included in the test
set were greater than 10 but less than 15 and then train-
ing Rainbow using the remaining text segments. Thus,
CarmelTC uses the same set of training data, but unlike
the other approaches, it uses its training data in two sepa-
rate parts, namely one to train the Rainbow models it uses
to produce the Rainbow prediction that is part of the vec-
tor representation it builds for each text and one to train
the decision trees. This is because for CarmelTC, the data
for training Rainbow must be separate from that used to
train the decision trees so the decision trees are trained
from a realistic distribution of assigned Rainbow classes
based on its performance on unseen data rather than on
Figure 1: This Table compares the performance of the 3 alternative approaches
Approach Precision Recall False Alarm Rate F-Score
LSA 93% 54% 3% .70
Rainbow 81% 73% 9% .77
CarmelTCsymb 88% 72% 7% .79
CarmelTC 90% 80% 8% .85
Rainbow?s training data. Thus, for CarmelTC, we also
performed a 50 fold cross validation, but this time only
over the set of 126 example essays not used to train the
Rainbow models used by CarmelTC.
Note that LSA works by using its trained LSA space
to construct a vector representation for any text based on
the set of words included therein. It can thus be used
for text classification by comparing the vector obtained
for a set of exemplar texts for each class with that ob-
tained from the text to be classified. We tested LSA using
as exemplars the same set of examples used as Rainbow
training data, but it always performed better when using a
small set of hand picked exemplars. Thus, we present re-
sults here using only those hand picked exemplars. For
every approach except LSA, we first segmented the es-
says at sentence boundaries and classified each sentence
separately. However, for LSA, rather than classify each
segment separately, we compared the LSA vector for the
entire essay to the exemplars for each class (other than
?nothing?), since LSA?s performance is better with longer
texts. We verified that LSA also performed better specif-
ically on our task under these circumstances. Thus, we
compared each essay to each exemplar, and we counted
LSA as identifying the corresponding ?correct answer as-
pect? if the cosine value obtained by comparing the two
vectors was above a threshold. We used a threshold value
of .53, which we determined experimentally to achieve
the optimal f-score result, using a beta value of 1 in order
to treat precision and recall as equally important.
Figure 1 demonstrates that CarmelTC out performs the
other approaches, achieving the highest f-score, which
combines the precision and recall scores into a single
measure. Thus, it performs better at this task than two
commonly used purely ?bag of words? approaches as well
as to an otherwise equivalent purely symbolic approach.
References
J. Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,
L. Braden-Harder, and M. D. Harris. 1998. Automated
scoring using a hybrid feature identification technique.
In Proceedings of COLING-ACL?98, pages 206?210.
P. W. Foltz, W. Kintsch, and T. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse Processes, 25(2-3):285?307.
J. Furnkranz, T. Mitchell Mitchell, and E. Riloff. 1998.
A case study in using linguistic phrases for text cat-
egorization on the www. In Proceedings from the
AAAI/ICML Workshop on Learning for Text Catego-
rization.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25(2-3):259?284.
L. Larkey. 1998. Automatic essay grading using text cat-
egorization techniques. In Proceedings of SIGIR.
K. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the short answer question with research
methods tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Classification.
J. R. Quinlin. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers: San Mateo, CA.
C. P. Rose?. 2000. A framework for robust semantic in-
terpretation. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 311?318.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural Lan-
guage Tutoring Group. 2002. The architecture of
why2-atlas: a coach for qualitative physics essay writ-
ing. Proceedings of the Intelligent Tutoring Systems
Conference.
P. Wiemer-Hastings, A. Graesser, D. Harter, and the Tu-
toring Res earch Group. 1998. The foundations and
architecture of autotutor. In B. Goettl, H. Halff, C. Red-
field, and V. Shute, editors, Intelligent Tutoring Sys-
tems: 4th International Conference (ITS ?98 ), pages
334?343. Springer Verlag.
A Comparison of Tutor and Student Behavior in Speech Versus Text Based
Tutoring
Carolyn P. Rose?, Diane Litman, Dumisizwe Bhembe, Kate Forbes, Scott Silliman,
Ramesh Srivastava, Kurt VanLehn
Learning Research and Development Center, University of Pittsburgh,
3939 O?Hara St., Pittsburgh, PA 15260
rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edu
litman,vanlehn@cs.pitt.edu
Abstract
This paper describes preliminary work in ex-
ploring the relative effectiveness of speech ver-
sus text based tutoring. Most current tuto-
rial dialogue systems are text based (Evens et
al., 2001; Rose and Aleven, 2002; Zinn et
al., 2002; Aleven et al, 2001; VanLehn et al,
2002). However, prior studies have shown con-
siderable benefits of tutoring through spoken
interactions (Lemke, 1990; Chi et al, 1994;
Hausmann and Chi, 2002). Thus, we are cur-
rently developing a speech based dialogue sys-
tem that uses a text based system for tutoring
conceptual physics (VanLehn et al, 2002) as
its ?back-end?. In order to explore the relative
effectiveness between these two input modal-
ities in our task domain, we have started by
collecting parallel human-human tutoring cor-
pora both for text based and speech based tu-
toring. In both cases, students interact with the
tutor through a web interface. We present here
a comparison between the two on a number
of features of dialogue that have been demon-
strated to correlate reliably with learning gains
with students interacting with the tutor using
the text based interface (Rose? et al, submitted).
1 Introduction
This paper describes preliminary work in exploring the
relative effectiveness of speech versus text based tuto-
rial dialogue systems. Tutorial dialogue is a natural way
to provide students with a learning environment that ex-
hibits characteristics that have been shown to correlate
with student learning gains, such as student activity. For
example, it has been demonstrated that generating words
rather than simply reading them promotes subsequent re-
call of those words (Slamecka and Graf, 1978). (Chi
et al, 1994) notes that there is a general momentum in
the science education literature toward the importance
of talking, reflecting and explaining as ways to learn
(Lemke, 1990). Moreover, encouraging student self-
explanation, which includes both generating inferences
from material they have read and relating new material to
old material, has been shown to correlate with learning
(Chi et al, 1981; Chi et al, 1994; Renkl, 1997; Press-
ley et al, 1992). In a further study, prompting students
with zero content prompts to encourage them to self-
explain was also associated with student learning (Chi et
al., 2001). A second important advantage to dialogue is
that it affords the tutor the opportunity to tailor instruc-
tion to the needs of the student. While human tutors may
not always choose to tailor their instruction to the indi-
vidual characteristics of the knowledge state of their stu-
dents, tutors who ignore signs of student confusion may
run the risk of preventing learning (Chi, 1996). (Rose? et
al., submitted) explore the benefits of tutor adaptation by
comparing learning gains for naive learners and review
learners in a human tutoring condition and a non-adaptive
reading condition.
In recent years tutorial dialogue systems have be-
come more and more prevalent, most of which are text
based (Evens et al, 2001; Rose and Aleven, 2002; Zinn
et al, 2002; Aleven et al, 2001; VanLehn et al, 2002).
Many of these systems have yielded successful evalu-
ations with students (Rose? et al, 2001; Heffernan and
Koedinger, 2002; Ashley et al, 2002; Graesser et al,
2001a). However, while the majority of current tutorial
dialogue systems are text based, there is reason to believe
that speech based tutorial dialogue systems could be more
effective.
Prior studies have shown considerable benefits of
human-human tutoring through spoken interactions
(Lemke, 1990; Chi et al, 1994). (Hausmann and Chi,
2002) has shown that spontaneous self-explanation oc-
curs much more frequently in spoken tutoring then in
text based tutoring, suggesting that typing requires ad-
ditional cognitive capacity and thus reduces the cogni-
tive resources available for spontaneous self-explanation.
Other research projects (Mostow and Aist, 2001; Fry
et al, 2001) have shown that basic spoken natural lan-
guage capabilities can be implemented quite effectively
in computer tutoring systems. Moreover, speech con-
tains prosodic and acoustic information which has been
shown to improve the accuracy of predicting emotional
states (Ang et al, 2002; Batliner et al, 2000) and user
responses to system errors (Litman et al, 2001) that are
useful for triggering system adaptation. We are thus cur-
rently developing a speech based dialogue system that
uses a text based system (VanLehn et al, 2002) as its
?back-end?. These systems and their goals will be dis-
cussed in Section 2.
We expect that the different modalities used by these
systems (e.g. text based vs speech based) will display
interesting differences with respect to the characteristics
of dialogue interaction that may determine their relative
merits with respect to increasing student performance.
Although human-computer data from the speech based
system is not yet available for comparison, we have col-
lected parallel human-human corpora both for text based
and speech based tutoring, as discussed in Sections 3-4,
and these corpora already display similarities and differ-
ences with respect to features of their dialogue interac-
tions, as discussed in Section 5, that are wholly modality
based and that will likely be displayed to an even greater
extent in the comparable human-computer data.
2 Why2-Atlas and ITSPOKE Dialogue
Systems
Why2-Atlas is a text based intelligent tutoring dialogue
system (Rose? et al, 2002a; VanLehn et al, 2002). The
goal of Why2-Atlas is to provide a platform for testing
whether deep approaches to natural language process-
ing elicit more learning than shallower approaches, for
the task domain of qualitative physics explanation gen-
eration. Using Why2-Atlas, the activity in which stu-
dents engage is answering deep reasoning questions in-
volving topics in conceptual physics. One such question
that we used is, ?A lightweight car and a massive truck
have a head-on collision. On which vehicle is the im-
pact force greater? Which vehicle undergoes the greater
change in its motion? Explain.? This is an appropriate
task domain for pursuing questions about the benefits of
tutorial dialogue for learning because questions like this
one are known to elicit robust, persistent misconceptions
from students, such as ?heavier objects exert more force.?
(Hake, 1998; Halloun and Hestenes, 1985). We designed
a set of 10 essay questions to use as training problems.
Two physics professors and a computer science profes-
sor worked together to select a set of expectations (i.e.,
correct propositions that the tutors expected students to
include in their essays) and potential misconceptions as-
sociated with each question. Additionally, they agreed
on an ideal essay answer for each problem. In Why2-
Atlas, a student first types an essay answering a qualita-
tive physics problem. A computer tutor then engages the
student in a natural language dialogue to provide feed-
back, correct misconceptions, and to elicit more complete
explanations. The first version of Why2-Atlas was de-
ployed and evaluated with undergraduate students in the
spring of 2002; the system is continuing to be actively
developed (Graesser et al, 2002).
We are currently developing a speech-enabled version
of Why2-ATLAS, called ITSPOKE (Intelligent Tutor-
ing SPOKEn dialogue system), that uses the Why2-Atlas
system as its ?back-end?. To date we have interfaced
the Sphinx2 speech recognizer (Huang et al, 1993) with
stochastic language models trained from example user ut-
terances, and the Festival speech synthesizer (Black and
Taylor, 1997) for text-to-speech, to the Why2-Atlas back-
end. The rest of the needed natural language process-
ing components, e.g. the sentence-level syntactic and
semantic analysis modules (Rose?, 2000), discourse and
domain level processors (Makatchev et al, 2002), and a
finite-state dialogue manager (Rose? et al, 2001), are pro-
vided by a toolkit that is part of the Why2-Atlas back-
end. The student speech is digitized from microphone
input, while the tutor?s synthesized speech is played to
the student using a speaker and/or headphone. We are
now in the process of adapting the knowledge sources
needed by the spoken language components to our ap-
plication domain. For example, we have developed a set
of dialogue dependent language models using the exper-
imental human-computer typed corpus (4551 student ut-
terances) obtained during the Why2-Atlas 2002 evalua-
tion. Our language models will soon be enhanced using
student utterances from our parallel human-human spo-
ken language corpus.
One goal of the ITSPOKE system is simply replacing
text based dialogue interaction with spoken dialogue in-
teraction and leaving the rest of the Why2-Atlas back-end
unchanged, in order to test the hypothesis that student
self-explanation (which leads to greater learning (Haus-
mann and Chi, 2002)) might be easier to achieve in spo-
ken dialogues. This hypothesis is discussed further in
Section 5. Although not the focus of this paper, an-
other goal of the ITSPOKE system is to take full ad-
vantage of the speech modality. For example, speech
contains rich acoustic and prosodic information about
the speaker?s current emotional state that isn?t present
in typed dialogue. Connections between learning and
emotion have been well documented (Coles, 1999), so it
seems likely that the success of computer-based tutoring
systems could be greatly increased if they were capable
of predicting and adapting to student emotional states,
e.g. reinforcing positive states, while rectifying nega-
tive states (Evens, 2002). Preliminary machine learn-
ing experiments involving emotion annotation and au-
tomatic feature extraction from our corpus suggest that
ITSPOKE can indeed be enhanced to automatically pre-
dict and adapt to student emotional states (Litman et al,
2003).
3 Typed Human-Human Tutoring Corpus
The Why2-Atlas Human-Human Typed Tutoring Cor-
pus is a collection of typed tutoring dialogues between
(human) tutor and student collected via typed interface,
which the tutor plays the same role that Why2-Atlas
is designed to perform. The experimental procedure is
as follows: 1) students are given a pretest measuring
their knowledge of physics, 2) students are asked to read
through a small document of background material, 3) stu-
dents work through a set of up to 10 Why2-Atlas training
problems with the human tutor, and 4) students are given
a post-test that is similar to the pretest. The entire ex-
periment takes no more than 15 hours per student, and is
usually performed in 1-3 sessions of no more than 4 hours
each. Data collection began in the Fall 2002 semester and
is continuing in the Spring 2003 semester. The subjects
are all University of Pittsburgh students who have never
taken any college physics courses. One tutor currently
participates in the study.
As in the Why2-Atlas system, when the tutoring ses-
sion then begins, the student first types an essay answer-
ing a qualitative physics problem. Once the student sub-
mits his/her essay, the tutor then engages the student in a
typed natural language dialogue to provide feedback and
correct misconceptions, and to elicit more complete ex-
planations. This instruction is in the form of a dialogue
between the student and the tutor through a text based
chat interface with student and tutor in separate rooms.
At key points in the dialogue, the tutor asks the student
to revise the essay. This cycle of instruction and revision
continues until the tutor is satisfied with the student?s es-
say. A sample tutoring dialogue from the Why2-Atlas
typed human-human tutoring corpus is displayed in Fig-
ure 1.
The tutor was instructed to cover the expectations for
each problem, to watch for the specific set of expectations
and misconceptions associated with the problem, and to
end the discussion of each problem by showing the ideal
essay to the student. He was encouraged to avoid lectur-
ing the student and to attempt to draw out the student?s
own reasoning. He knew that transcripts of his tutoring
would be analyzed. Nevertheless, he was not required to
follow any prescribed tutoring strategies. So his tutoring
style was much more naturalistic than in previous stud-
ies such as the BEE study (Rose? et al, 2001) in which
two specific tutoring styles, namely Socratic and Didac-
tic, were contrasted. The results of that study revealed a
trend for students in the Socratic condition to learn more
than those in the Didactic condition. A further analysis
of the corpus collected during the BEE study (Core et
al., 2002) verified that the Socratic dialogues from the
BEE study were more interactive than the Didactic ones.
The biggest reliable difference between the two sets of
tutoring dialogues was the percentage of words spoken
by the student, i.e, number of student words divided by
total number of words. The Didactic dialogues contained
on average 26% student words, whereas the Socratic di-
alogues contained 33% student words. On average with
respect to percentage of student words, the dialogues in
our text based human tutoring corpus were more like the
Didactic dialogues from the BEE study, with average per-
centage of student text being 27%. Nevertheless, because
the tutor was not constrained to follow a prescribed tutor-
ing style, the level of interactivity varied widely through-
out the transcripts, at times being highly Socratic, and at
other times being highly Didactic.
Pre and post tests were used to measure learning gains
to be used for evaluating the effectiveness of various fea-
tures of tutorial dialogue found in our corpora. Thus, we
developed two tests: versions A and B, which were iso-
morphic to one another. That is, the problems on test A
and B differed only in the identities of the objects (e.g.,
cars vs. trucks) and other surface features that should not
affect the reasoning required to solve them. Each ver-
sion of the test (A and B) consisted of 40 multiple choice
questions. Each multiple choice question was written to
address a single expectation covered in the training prob-
lems. Some students were not able to complete all 10
problems before they reached the end of their participa-
tion time. Thus, they took the post-test after only working
through a subset of the training problems.
4 Spoken Human-Human Tutoring
Corpus
The ITSPOKE Human-Human Spoken Tutoring Corpus
is a parallel collection of spoken tutoring dialogues col-
lected via a web interface supplemented with a high qual-
ity audio link, where a human tutor performs the same
task that our ITSPOKE system is being designed to per-
form. The experimental procedure used to collect the cor-
pus is exactly the same as the procedure used to gather
the Why2-Atlas Human-Human Corpus: the same tutor
is used, the same subject pool1 is used, the same pre-test
and post-test are used, and the same set of physics prob-
1We assigned a greater percentage of students to the text
based condition as part of a separate experiment. Thus, the text
based corpus is larger than the speech based corpus.
PROBLEM: Suppose that you released 3 identical balls of clay in a vacuum at exactly the same instant. They would all hit the
ground at the same instant. Now you stick two of the balls together, forming one ball that is twice as heavy as the remaining,
untouched clay ball. Both balls are released in a vacuum at exactly the same instant. Which ball hits the ground first?
ESSAY: Both balls will hit the ground at the same time. The balls are in free fall (only gravitational forces). The ratio of the
masses and weight are equal.
. . . excerpt from 2 minutes into a typed dialogue . . .
Tutor1: You have correctly said the balls are in free fall. What do you conclude from this fact?
Student1: There is no air resistance and the balls? accelerations are constant in regards to one another
Tutor2: Right, so the conclusion is that they both have the same acceleration, the free fall acceleration. Now you have to show
that time taken by them to reach the ground will be the same. How do you do that?
Student2: F (net forces) / mass = f/m because F = m*a therefore ratio should equal one another
Tutor3: But that does not tell you anything about the time taken. What physical quantities will determine the time taken. Will the
initial velocity of the balls matter?
Student3: yes, assuming starting at rest? sorry, kind of at a loss at the moment
Tutor4: The initial velocity will matter, and here they both have the same initial velocity, zero. What else, will the height through
wich they fall matter?
Student4: not so much as long as they are both dropped from the same height
Tutor5: Height of fall will determine the time of fall, and here the height of fall is the same for both the balls. Now on the basis of
these facts can you conclude about the time taken by them to reach the ground?
Student5: since initial velocities are equal and the height of falls are equal, then the time taken is equal
Tutor6: How about acceleration, if they had different accelerations, even if they fall through the same height and have same initial
velocity , will they reach the ground at the same time?
Student6: no...
Tutor7: Right...
Figure 1: Excerpt from Human-Human Typed Dialogue Corpus
lems are used. Only the modality differs. In particular,
once the tutoring session begins and the student submits
his/her typed essay, the tutor and student then discuss the
student?s essay using spoken English. In contrast to the
text condition, where strict turn-taking is enforced, in the
spoken condition, interruptions and overlapping speech
are common. An example excerpt from the corpus is
shown in Figure 2. Note that turns ending in ?-? indi-
cate speech overlapping with the following turn. Even-
tually, the student will edit his/her typed explanation. As
in the text condition, the tutor then either expresses satis-
faction and ends the tutoring for the current problem, or
continues with another round of spoken dialogue interac-
tion and typed essay revision. As in the text condition,
students are presented with the ideal essay answer for a
problem upon completing that problem.
5 Differences between Typed and Spoken
Human-Tutoring
(Rose? et al, submitted) presents an analysis to uncover
which aspects of the tutorial dialogue were responsible
for its effectiveness in the text based condition. Longer
student answers to tutor questions reveal more of a stu-
dent?s reasoning. Very short answers, i.e., 10 words or
less, are normally composed of a single clause at most.
Longer, multi-clausal answers have the potential to com-
municate many more inter-connections between ideas.
Thus, if a tutor is attending to and responding directly
to the student?s revealed knowledge state, it would be
expected that the effectiveness of the tutor?s instruction
would increase as average student turn length increases.
To test this prediction, we computed a linear regression
of the sequence of student turn lengths over time for each
student in the text based condition in order to obtain an in-
tercept and a slope, since student turn lengths have been
observed to decline on average over the course of their
PROBLEM: If a car is able to accelerate at 2 m/s2, what acceleration can it attain if it is towing another car of equal mass?
ESSAY: If the car is towing another car of equal mass, the maximum acceleration would be the same because the car would be
towed behind and the friction caused would only be by the front of the first car.
. . . excerpt from 6.5 minutes into spoken dialogue . . .
Tutor1: So twice the mass multiplied by the acceleration should be equal to the force which you have already determined as the
mass of the first car times the acceleration. So essentially you are dividing it by two and that gives you the uh acceleration by
twice the because mass has become twice. Now this law that force is equal to mass times acceleration, what?s this law called?
This is uh since this it is a very important basic uh fact uh it is it is a law of physics. Um you have you have read it in the
background material. Can you recall it?
Student1: Um no it was one of Newton?s laws-
Tutor2: Right, right-
Student2: but I don?t remember which one. (laugh)
Tutor3: That-
Student3: he I-
Tutor4: is Newton?s second law of motion.
Student4: Ok, because I remember one, two, and three, but I didn?t know if there was a different name
Tutor5: Yeah that?s right you know Newton was a genius and-
Student5: (laugh)-
Tutor6: uh he looked at a large number of experiments and experimental data that was available and from that he could come to
this general law and it is known as Newton?s second law of motion. Um many many other scientists before him had seen all
this data which was collected by scientists but had not concluded this. Now it looks very simple but to come to a conclusion
from a mass of data was something which required the genius of Newton.
Student6: mm hm
Tutor7: So now you will give Newton full credit isn?t it? (laugh)
Student7: (laugh)
Figure 2: Excerpt from Human-Human Spoken Dialogue Corpus.
interaction with the turn. We then computed a multiple
regression with pre-test score, intercept, and gradient as
independent variables and post test score as the depen-
dent variable. We found a reliable correlation between
intercept and learning, with pre-test scores and gradients
regressed out (R=.836; p   .05). This result is consistent
with (Core et al, 2002) where percentage of student talk
is strongly correlated with learning. Consistent with this,
we found a strong and reliable correlation between ra-
tio of student words to tutor words and learning2. We
computed a correlation between ratio of student words to
tutor words and post-test score after pre-test scores were
regressed out (R=.866, p   .05).
One of our current research objectives is to compare
2Note that ratio of student words to tutor words is number
of student words divided by number of tutor words, whereas
percentage of student words is number of student words divided
by total number of words
the relative effectiveness of speech based and text based
tutoring. Thus, when we have enough speech data, we
would like to compare learning gains between the speech
and text based conditions to test whether or not speech
based tutoring is more effective than text based tutoring.
We also plan to test whether the same features that cor-
relate with learning in the text based condition also cor-
relate with learning in the speech based condition. Since
both average student turn length and overall ratio of stu-
dent words to tutor words correlated strongly with learn-
ing gains in the text based condition, in this paper we
compare these two measures between the text based tutor-
ing condition and the speech based tutoring condition, but
not yet in connection with learning gains in the speech-
based corpus.
Since strict turn taking was not enforced in the speech
condition, turn boundaries were manually annotated
(based on consensus labellings from two coders) when ei-
ther (1) the speaker stopped speaking and the other party
in the dialogue began to speak, (2) when the speaker
asked a question and stopped speaking to wait for an an-
swer, or (3) when the other party in the dialogue inter-
rupted the speaker and the speaker paused to allow the
other party to speak.
Currently, 13 students have started the typed human-
human tutoring experiment, 7 of whom have finished.
We have so far collected 78 typed dialogues from the
text based condition, 69 of which were used in our anal-
ysis. 9 students have started the spoken human-human
tutoring experiment, 6 of whom have finished. Thus, we
have collected 63 speech based dialogues (1290 minutes
of speech from 4 female and 4 male subjects), and have
transcribed 25 of them. We hope to have an analysis cov-
ering all of our data in both conditions by the time of the
workshop.
As shown in Table 1, analysis of the data that has
been collected and transcribed to date is already show-
ing interesting differences between the ITSPOKE (spo-
ken) and WHY2-ATLAS (text) corpora of human-human
dialogues. The #trns columns show mean and standard
deviation for the total number of turns taken by the stu-
dents or tutor in each problem dialogue, while the next
pair of columns show the mean and standard deviation
for the total number of words spoken or typed by the stu-
dents or tutor (#wds) in each problem dialogue. The last
pair of columns show mean and standard deviation for the
average number of student or tutor words per turn in each
problem dialogue.
Due to the fact that data is still being collected for both
corpora (and the fact that the speech corpus also requires
manual transcription), the sizes of the two data sets rep-
resented in the table differ somewhat. However, even at
this early stage in the development of both corpora, these
figures already show that the style of the interactions are
very different in each modality. In particular, in spoken
tutoring, both student and tutor take more turns on av-
erage than in text based tutoring, but these spoken turns
are on average shorter. Moreover, in spoken tutoring both
student and tutor on average use more words to commu-
nicate than in text based tutoring. Another interesting dif-
ference is that although in the speech condition both stu-
dent and tutor take more turns, students finish the speech
condition in less time. In particular, on average, students
in the text based tutoring condition require 370.58 min-
utes to finish the training problems, with a standard devi-
ation of 134.29 minutes, students in the speech condition
require only 159.9 minutes on average, with a standard
deviation of 58.6 minutes. We measured the statistical re-
liability of the difference between the two measures that
correlated reliably with learning in the text-based condi-
tion. A 2-tailed unpaired t-test indicates that this differ-
ence is significant (t(30)=8.99, p   .01). There are also
similarities across the two conditions. In particular, a 2-
tailed unpaired t-test shows that the relative proportion
of student and tutor word or turns do not differ signifi-
cantly on average across the two modalities (t(13)=1.225,
p=.242). As an illustration, Table 2 shows mean and stan-
dard deviation for the ratios of the total number of student
and tutor words (#Swds/#Twds) and turns (#Strns/#Ttrns)
in each problem dialogue3.
Average student turn length is significantly lower in
the speech based condition (t(13) = 4.5, p   .001). This
might predict that speech based tutoring may be less ef-
fective than text based tutoring. However, since ratio of
student words to tutor words does not differ significantly,
this would predict that learning will also not differ sig-
nificantly between conditions. Total number of words
uttered in the speech condition is larger than in the text
based condition as are total number of turns. This dif-
ference between the two conditions will likely be even
more pronounced in the human-computer comparison,
due to noisy student input that results from use of au-
tomatic speech recognition. For example, clarifications
and corrections made necessary by this will likely lead
to an increase in dialogue length. More careful analy-
sis is required to determine whether this means that more
self-explanation took place overall in the speech based
condition. If so, this would predict that the speech based
condition would be more effective for learning than the
text based condition. Thus, much interesting exploration
is left to be done after we have collected enough speech
data to compute a reliable comparison between the two
conditions.
6 Current Directions
Currently we are continuing to collect data both in the
speech and text based human tutoring conditions. Since
human tutors differ with each other with respect to both
their tutoring styles and their conversational styles, we
plan to collect data using several different human tutors
in order to test the robustness of our comparisons between
speech and text based human tutoring. Another possible
direction for further inquiry would be to contrast natural-
istic speech (where strict turn taking is not enforced, as
in this data collection effort), with a speech condition in
which strict turn taking is enforced, in order to separate
the effects of speech on learning from the effects of alter-
native turn taking policies.
As discussed in Section 2, we are currently develop-
ing both text based and speech based human-computer
tutorial systems. Our ultimate goal is to test the relative
effectiveness of speech versus text based computer tutors.
We expect differences both between text and speech con-
3Note that strict-turn taking is enforced in the text condition,
but not in the speech condition.
Table 1: Student and Tutor Characteristics in Human-Human Speech and Text Conditions
#trns #wds Avg#wds/trn
Condition Participant Mean STD Mean STD Mean STD
Speech Student 47.49 25.95 264.18 125.47 5.72 1.35
Text Student 9.71 6.79 146.72 57.96 13.39 5.55
Speech Tutor 46.94 20.90 1199.14 605.87 26.78 14.20
Text Tutor 11.03 7.04 391.85 136.89 39.04 6.23
Table 2: Student-Tutor Word and Turn Ratios in Speech and Text Conditions
Speech Condition Text Condition
#Swds/#Twds #Strns/#Ttrns #Swds/#Twds #Strns/#Ttrns
Mean STD Mean STD Mean STD Mean STD
0.29 0.15 0.99 0.15 0.37 0.08 0.81 0.15
ditions in the human-computer data and between human-
human and human-computer data. One of our first tasks
will thus be to use the baseline version of ITSPOKE
described in Section 2 to generate a corpus of human-
computer spoken dialogues, using a process comparable
to the human-human corpus collection described here.
This will allow us to 1) compare the ITSPOKE human-
human and human-computer corpora 2) compare the IT-
SPOKE human-computer spoken corpus with a compa-
rable Why2-Atlas text corpus, e.g. by expanding on the
just described pilot study of the two human-human cor-
pora, and 3) use the ITSPOKE human-computer corpus
to guide the development of a new version of ITSPOKE
that will attempt to increase its performance, by taking
advantage of information that is only available in speech,
and modifying its behavior in other ways to respect the
interaction differences in item 2.
7 Acknowledgments
This research was supported by the Office of Naval Re-
search, Cognitive Science Division under grant number
N00014-0-1-0600 and by NSF grant number 9720359 to
CIRCLE.
References
V. Aleven, O. Popescu, and K. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In J. D. Moore, C. L. Redfield, and W. L. John-
son, editors, Proceedings of Articial Intelligence in
Education, pages 246?255.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP.
K. D. Ashley, R. Desai, and J. M. Levine. 2002. Teach-
ing case-based argumentation concepts using didactic
arguments vs. didactic explanations. In Proceedings
of the Intelligent Tutoring Systems Conference, pages
585?595.
A. Batliner, R. Huber, H. R. Niemann, E. No?th, J. Spilker,
and K. Fischer. 2000. The recognition of emotion. In
Proc. of the ISCA Workshop on Speech and Emotion.
A. Black and P. Taylor. 1997. Festival speech synthesis
system: system documentation (1.1.1). Human Com-
munication Research Centre Technical Report 83, Uni-
versity of Edinburgh.
M. Chi, N. de Leeuw, M. Chiu, and C. LaVancher.
1981. Eliciting self-explanations improves under-
standing. Cognitive Science, 18(3).
Michelene Chi, Nicholas De Leeuw, Mei-Hung Chiu, and
Christian Lavancher. 1994. Eliciting self-explanations
improves understanding. Cognitive Science, 18:439?
477.
M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, and
R. G. Hausmann. 2001. Learning from human tutor-
ing. Cognitive Science, (25):471?533.
M. T. H. Chi. 1996. Learning processes in tutoring. Ap-
plied Cognitive Psychology, 10:S33?S49.
G. Coles. 1999. Literacy, emotions, and the brain. Read-
ing Online, March 1999.
M. Core, J. D. Moore, and C. Zinn. 2002. Initiative in
tutorial dialogue. In Proceedings of the ITS Workshop
on Empirical Methods for Tutorial Dialogue Systems,
pages 46?55.
M. Evens, S. Brandle, R. Chang, R. Freedman, M. Glass,
Y. H. Lee, L. S. Shim, C. W. Woo, Y. Zhang, Y. Zhou,
J. A. Michaeland, and A. A. Rovick. 2001. Circsim-
tutor: An intelligent tutoring system using natural lan-
guage dialogue. In Proceedings of the Twelfth Midwest
AI and Cognitive Science Conference, MAICS 2001,
pages 16?23, Oxford, OH.
M. Evens. 2002. New questions for Circsim-Tutor. Pre-
sentation at the 2002 Symposium on Natural Language
Tutoring, University of Pittsburgh.
J. Fry, M. Ginzton, S. Peters, B. Clark, and H. Pon-Barry.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proc. 2nd SigDial Work-
shop on Discourse and Dialogue.
A. Graesser, N. Person, and D. Harter et al 2001a.
Teaching tactics and dialog in Autotutor. International
Journal of Articial Intelligence in Education.
A. Graesser, K. Vanlehn, TRG, and NLT Group. 2002.
Why2 report: Evaluation of why/atlas, why/autotutor,
and accomplished human tutors on learning gains for
qualitative physics problems and explanations. Tech-
nical report, LRDC Tech Report, University of Pitts-
burgh.
R. R. Hake. 1998. Interactive-engagement versus tra-
ditional methods: A six-thousand student survey of
mechanics test data for introductory physics students.
American Journal of Physics, 66(64).
I. A. Halloun and D. Hestenes. 1985. The initial knowl-
edge state of college physics students. American Jour-
nal of Physics, 53(11):1043?1055.
Robert Hausmann and Michelene Chi. 2002. Can a com-
puter interface support self-explaining? The Interna-
tional Journal of Cognitive Technology, 7(1).
N. T. Heffernan and K. R. Koedinger. 2002. An intelli-
gent tutoring system incorporating a model of an expe-
rienced tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference, pages 596?608.
X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F.
Lee, and R. Rosenfeld. 1993. The SphinxII speech
recognition system: An overview. Computer, Speech
and Language.
J. L. Lemke. 1990. Talking Science: Language, Learn-
ing and Values. Ablex, Norwood, NJ.
D. Litman, J. Hirschberg, and M. Swerts. 2001. Predict-
ing user reactions to system error. In Proc.of ACL.
Diane Litman, Kate Forbes, and Scott Silliman. 2003.
Towards emotion prediction in spoken tutoring dia-
logues. Submitted.
M. Makatchev, P. Jordan, and K. VanLehn. 2002. Dis-
course processing for explanatory essays in tutorial ap-
plications. In Proceedings of the 3rd SIGdial Work-
shop on Discourse and Dialogue.
J. Mostow and G. Aist. 2001. Evaluating tutors that lis-
ten: An overview of Project LISTEN. In K. Forbus and
P. Feltovich, editors, Smart Machines in Education.
M. Pressley, E. Wood, V. E. Woloshyn, V. Martin,
A. King, and D. Menke. 1992. Encouraging mind-
ful use of prior knowledge: Attempting to construct
explanatory answers facilitates learning. Educational
Psychologist, 27:91?109.
A. Renkl. 1997. Learning from worked-out examples:
A study on individual differences. Cognitive Science,
21(1):1?29.
C. P. Rose and V. Aleven. 2002. Proc. of the ITS 2002
workshop on empirical methods for tutorial dialogue
systems. Technical report, San Sebastian, Spain, June.
C. P. Rose?, J. D. Moore, K. VanLehn, and D. Allbritton.
2001. A comparative evaluation of socratic versus di-
dactic tutoring. In Proceedings of the 23rd Annual
Conference of the Cognitive Science Society, pages
869?874.
C. P. Rose?, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive conceptual
tutoring in atlas-andes. In Proceedings of Articial In-
telligence in Education, pages 256?266.
C. P. Rose?, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. Vanlehn. 2002a. A hybrid language un-
derstanding approach for robust selection of tutoring
goals. In Proceedings of the Intelligent Tutoring Sys-
tems Conference, pages 552?561.
C. P. Rose?, K. VanLehn, and The Natural Language Tu-
toring Group. Submitted. Is human tutoring always
more effective than reading. In Annual Meeting of the
Cognitive Science Society.
C. P. Rose?. 2000. A framework for robust sentence
level interpretation. In Proceedings of the First Meet-
ing of the North American Chapter of the Association
for Computational Linguistics, pages 1129?1135.
N. J. Slamecka and P. Graf. 1978. The generation ef-
fect: Delineation of a phenomenon. Journal of Exper-
imental Psychology: Human Learning and Memory,
(4):592?604.
K. VanLehn, P. Jordan, C. Rose?, D. Bhembe, M. Bo?ttner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, R. Srivastava, and R. Wil-
son. 2002. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proc. of ITS.
Claus Zinn, Johanna D. Moore, and Mark G. Core. 2002.
A 3-tier planning architecture for managing tutorial di-
alogue. In Proceedings Intelligent Tutoring Systems,
Sixth International Conference (ITS 2002), Biarritz,
France, June.
A Hybrid Text Classication Approach for Analysis of Student Essays
Carolyn P. Rose?, Antonio Roque, Dumisizwe Bhembe, Kurt Vanlehn
Learning Research and Development Center, University of Pittsburgh,
3939 O?Hara St., Pittsburgh, PA 15260
rosecp,roque,bhembe,vanlehn@pitt.edu
Abstract
We present CarmelTC, a novel hybrid text clas-
sification approach for analyzing essay answers
to qualitative physics questions, which builds
upon work presented in (Rose? et al, 2002a).
CarmelTC learns to classify units of text based
on features extracted from a syntactic analysis
of that text as well as on a Naive Bayes clas-
sification of that text. We explore the trade-
offs between symbolic and ?bag of words? ap-
proaches. Our goal has been to combine the
strengths of both of these approaches while
avoiding some of the weaknesses. Our evalu-
ation demonstrates that the hybrid CarmelTC
approach outperforms two ?bag of words? ap-
proaches, namely LSA and a Naive Bayes, as
well as a purely symbolic approach.
1 Introduction
In this paper we describe CarmelTC, a novel hybrid
text classification approach for analyzing essay answers
to qualitative physics questions. In our evaluation we
demonstrate that the novel hybrid CarmelTC approach
outperforms both Latent Semantic Analysis (LSA) (Lan-
dauer et al, 1998; Laham, 1997) and Rainbow (Mc-
Callum, 1996; McCallum and Nigam, 1998), which is
a Naive Bayes approach, as well as a purely symbolic
approach similar to (Furnkranz et al, 1998). Whereas
LSA and Rainbow are pure ?bag of words? approaches,
CarmelTC is a rule learning approach where rules for
classifying units of text rely on features extracted from
a syntactic analysis of that text as well as on a ?bag
of words? classification of that text. Thus, our evalu-
ation demonstrates the advantage of combining predic-
tions from symbolic and ?bag of words? approaches for
text classification. Similar to (Furnkranz et al, 1998),
neither CarmelTC nor the purely symbolic approach re-
quire any domain specific knowledge engineering or text
annotation beyond providing a training corpus of texts
matched with appropriate classifications, which is also
necessary for Rainbow, and to a much lesser extent for
LSA.
CarmelTC was developed for use inside of the Why2-
Atlas conceptual physics tutoring system (VanLehn et al,
2002; Graesser et al, 2002) for the purpose of grad-
ing short essays written in response to questions such as
?Suppose you are running in a straight line at constant
speed. You throw a pumpkin straight up. Where will it
land? Explain.? This is an appropriate task domain for
pursuing questions about the benefits of tutorial dialogue
for learning because questions like this one are known
to elicit robust, persistent misconceptions from students,
such as ?heavier objects exert more force.? (Hake, 1998;
Halloun and Hestenes, 1985). In Why2-Atlas, a stu-
dent first types an essay answering a qualitative physics
problem. A computer tutor then engages the student in
a natural language dialogue to provide feedback, cor-
rect misconceptions, and to elicit more complete expla-
nations. The first version of Why2-Atlas was deployed
and evaluated with undergraduate students in the spring
of 2002; the system is continuing to be actively devel-
oped (Graesser et al, 2002).
In contrast to many previous approaches to automated
essay grading (Burstein et al, 1998; Foltz et al, 1998;
Larkey, 1998), our goal is not to assign a letter grade
to student essays. Instead, our purpose is to tally which
set of ?correct answer aspects? are present in student es-
says. For example, we expect satisfactory answers to the
example question above to include a detailed explana-
tion of how Newton?s first law applies to this scenario.
From Newton?s first law, the student should infer that the
pumpkin and the man will continue at the same constant
horizontal velocity that they both had before the release.
Thus, they will always have the same displacement from
the point of release. Therefore, after the pumpkin rises
and falls, it will land back in the man?s hands. Our goal
is to coach students through the process of constructing
good physics explanations. Thus, our focus is on the
physics content and not the quality of the student?s writ-
ing, in contrast to (Burstein et al, 2001).
2 Student Essay Analysis
We cast the Student Essay Analysis problem as a text
classification problem where we classify each sentence in
the student?s essay as an expression one of a set of ?cor-
rect answer aspects?, or ?nothing? in the case where no
?correct answer aspect? was expressed.
After a student attempts an initial answer to the ques-
tion, the system analyzes the student?s essay to assess
which key points are missing from the student?s argu-
ment. The system then uses its analysis of the student?s
essay to determine which help to offer that student. In
order to do an effective job at selecting appropriate inter-
ventions for helping students improve their explanations,
the system must perform a highly accurate analysis of the
student?s essay. Identifying key points as present in es-
says when they are not (i.e., false alarms), cause the sys-
tem to miss opportunities to help students improve their
essays. On the other hand, failing to identify key points
that are indeed present in student essays causes the sys-
tem to offer help where it is not needed, which can frus-
trate and even confuse students. A highly accurate inven-
tory of the content of student essays is required in order
to avoid missing opportunities to offer needed instruction
and to avoid offering inappropriate feedback, especially
as the completeness of student essays increases (Rose? et
al., 2002a; Rose? et al, 2002c).
In order to compute which set of key points, i.e., ?cor-
rect answer aspects?, are included in a student essay, we
first segment the essay at sentence boundaries. Note that
run-on sentences are broken up. Once an essay is seg-
mented, each segment is classified as corresponding to
one of the set of key points or ?nothing? if it does not
include any key point. We then take an inventory of the
classifications other than ?nothing? that were assigned to
at least one segment. Thus, our approach is similar in
spirit to that taken in the AUTO-TUTOR system (Wiemer-
Hastings et al, 1998), where Latent Semantic Analysis
(LSA) (Landauer et al, 1998; Laham, 1997) was used to
tally which subset of ?correct answer aspects? students
included in their natural language responses to short es-
say questions about computer literacy.
We performed our evaluation over essays collected
from students interacting with our tutoring system in re-
sponse to the question ?Suppose you are running in a
straight line at constant speed. You throw a pumpkin
straight up. Where will it land? Explain.?, which we refer
to as the Pumpkin Problem. Thus, there are a total of six
alternative classifications for each segment:
Class 1 Sentence expresses the idea that after the release
the only force acting on the pumpkin is the down-
ward force of gravity.
Class 2 Sentence expresses the idea that the pumpkin
continues to have a constant horizontal velocity after
it is released.
Class 3 Sentence expresses the idea that the horizontal
velocity of the pumpkin continues to be equal to the
horizontal velocity of the man.
Class 4 Sentence expresses the idea that the pumpkin
and runner cover the same distance over the same
time.
Class 5 Sentence expresses the idea that the pumpkin
will land on the runner.
Class 6 Sentence does not adequately express any of the
above specified key points.
Note that this classification task is strikingly different
from those typically used for evaluating text classifica-
tion systems. First, these classifications represent spe-
cific whole propositions rather than general topics, such
as those used for classifying web pages (Craven et al,
1998), namely ?student?, ?faculty?, ?staff?, etc. Sec-
ondly, the texts are much shorter, i.e., one sentence in
comparison with a whole web page, which is a disadvan-
tage for ?bag of words? approaches.
In some cases what distinguishes sentences from one
class and sentences from another class is very subtle.
For example, ?Thus, the pumpkin?s horizontal velocity,
which is equal to that of the man when he released it, will
remain constant.? belongs to Class 2 although it could
easily be mistaken for Class 3. Similarly, ?So long as
no other horizontal force acts upon the pumpkin while it
is in the air, this velocity will stay the same.?, belongs
to Class 2 although looks similar on the surface to ei-
ther Class 1 or 3. A related problem is that sentences
that should be classified as ?nothing? may look very sim-
ilar on the surface to sentences belonging to one or more
of the other classes. For example, ?It will land on the
ground where the runner threw it up.? contains all of the
words required to correctly express the idea correspond-
ing to Class 5, although it does not express this idea, and
in fact expresses a wrong idea. These very subtle distinc-
tions also pose problems for ?bag of words? approaches
since they base their decisions only on which words are
present regardless of their order or the functional relation-
ships between them. That might suggest that a symbolic
approach involving syntactic and semantic interpretation
might be more successful. However, while symbolic ap-
proaches can be more precise than ?bag of words? ap-
proaches, they are also more brittle. And approaches that
rely both on syntactic and semantic interpretation require
a larger knowledge engineering effort as well.
3 CarmelTC
Figure 1: This example shows the deep syntactic parse
of a sentence.
Sentence: The pumpkin moves slower because the
man is not exerting a force on it.
Deep Syntactic Analysis
((clause2
((mood *declarative)
(root move)
(tense present)
(subj
((cat dp)(root pumpkin)
(specifier ((cat detp)(def +)(root the)))
(modifier ((car adv) (root slow)))))))
(clause2
(mood *declarative)
(root exert)
(tense present)
(negation +)
(causesubj
((cat dp)(root man)(agr 3s)
(specifier
((cat detp)(def +)(root the)))))
(subj
((cat dp)(root force)
(specifier ((cat detp)(root a)))))
(obj ((cat dp)(root it))))
(connective because))
The hybrid CarmelTC approach induces decision trees
using features from both a deep syntactic functional anal-
ysis of an input text as well as a prediction from the Rain-
bow Naive Bayes text classifier (McCallum, 1996; Mc-
Callum and Nigam, 1998) to make a prediction about the
correct classification of a sentence. In addition, it uses
features that indicate the presence or absence of words
found in the training examples. Since the Naive Bayes
classification of a sentence is more informative than any
single one of the other features provided, CarmelTC can
be conceptualized as using the other features to decide
whether or not to believe the Naive Bayes classification,
and if not, what to believe instead.
From the deep syntactic analysis of a sentence, we ex-
tract individual features that encode functional relation-
Figure 2: This example shows the features extracted
from the deep syntactic parse of a sentence.
Sentence: The pumpkin moves slower because the
man is not exerting a force on it.
Extracted Features
(tense-move present)
(subj-move pumpkin)
(specifier-pumpkin the)
(modifier-move slow)
(tense-exert present)
(negation-exert +)
(causesubj-exert man)
(subj-exert force)
(obj-exert it)
(specifier-force a)
(specifier-man the)
ships between syntactic heads (e.g., (subj-throw man)),
tense information (e.g., (tense-throw past)), and infor-
mation about passivization and negation (e.g., (negation-
throw +) or (passive-throw -)). See Figures 1 and 2. Rain-
bow has been used for a wide range of text classification
tasks. With Rainbow, P(doc,Class), i.e., the probability of
a document belonging to class Class, is estimated by mul-
tiplying P(Class), i.e., the prior probability of the class,
by the product over all of the words   found in the text of

 	 
	 , i.e., the probability of the word given that
class. This product is normalized over the prior probabil-
ity of all words. Using the individual features extracted
from the deep syntactic analysis of the input as well as
the ?bag of words? Naive Bayes classification of the in-
put sentence, CarmelTC builds a vector representation
of each input sentence, with each vector position corre-
sponding to one of these features. We then use the ID3
decision tree learning algorithm (Mitchell, 1997; Quin-
lin, 1993) to induce rules for identifying sentence classes
based on these feature vectors.
The symbolic features used for the CarmelTC ap-
proach are extracted from a deep syntactic functional
analysis constructed using the CARMEL broad coverage
English syntactic parsing grammar (Rose?, 2000) and the
large scale COMLEX lexicon (Grishman et al, 1994),
containing 40,000 lexical items. For parsing we use an
incremental version of the LCFLEX robust parser (Rose?
et al, 2002b; Rose? and Lavie, 2001), which was designed
for efficient, robust interpretation. While computing a
deep syntactic analysis is more computationally expen-
sive than computing a shallow syntactic analysis, we can
do so very efficiently using the incrementalized version
of LCFLEX because it takes advantage of student typ-
ing time to reduce the time delay between when students
submit their essays and when the system is prepared to
respond.
Syntactic feature structures produced by the CARMEL
grammar factor out those aspects of syntax that modify
the surface realization of a sentence but do not change
its deep functional analysis. These aspects include tense,
negation, mood, modality, and syntactic transformations
such as passivization and extraction. In order to do this
reliably, the component of the grammar that performs the
deep syntactic analysis of verb argument functional re-
lationships was generated automatically from a feature
representation for each of COMLEX?s verb subcatego-
rization tags. It was verified that the 91 verb subcatego-
rization tags documented in the COMLEX manual were
covered by the encodings, and thus by the resulting gram-
mar rules. These tags cover a wide range of patterns of
syntactic control and predication relationships. Each tag
corresponds to one or more case frames. Each case frame
corresponds to a number of different surface realizations
due to passivization, relative clause extraction, and wh-
movement. Altogether there are 519 syntactic patterns
covered by the 91 subcategorization tags, all of which are
covered by the grammar.
There are nine syntactic functional roles assigned by
the grammar. These roles include subj (subject), caus-
esubj (causative subject), obj (object), iobj (indirect ob-
ject), pred (descriptive predicate, like an adjectival phrase
or an adverb phrase), comp (a clausal complement), mod-
ifier, and possessor. The roles pertaining to the rela-
tionship between a verb and its arguments are assigned
based on the subcat tags associated with verbs in COM-
LEX. However, in some cases, arguments that COM-
LEX assigns the role of subject get redefined as caus-
esubj (causative subject). For example, the subject in ?the
pumpkin moved? is just a subject but in ?the man moved
the pumpkin?, the subject would get the role causesubj
instead since ?move? is a causative-inchoative verb and
the obj role is filled in in the second case 1. The modifier
role is used to specify the relationship between any syn-
tactic head and its adjunct modifiers. Possessor is used
to describe the relationship between a head noun and its
genitive specifier, as in man in either ?the man?s pump-
kin? or ?the pumpkin of the man?.
With the hybrid CarmelTC approach, our goal has been
to keep as many of the advantages of both symbolic anal-
ysis as well as ?bag of words? classification approaches
as possible while avoiding some of the pitfalls of each.
Since the CarmelTC approach does not use the syntactic
analysis as a whole, it does not require that the system be
able to construct a totally complete and correct syntactic
analysis of the student?s text input. It can very effectively
1The causative-inchoative verb feature is one that we added
to verb entries in COMLEX, not one of the features provided
by the lexicon originally.
make use of partial parses. Thus, it is more robust than
purely symbolic approaches where decisions are based on
complete analyses of texts. And since it makes use only
of the syntactic analysis of a sentence, rather than also
making use of a semantic interpretation, it does not re-
quire any sort of domain specific knowledge engineering.
And yet the syntactic features provide information nor-
mally not available to ?bag of words? approaches, such
as functional relationships between syntactic heads and
scope of negation and other types of modifiers.
4 Related Work: Combining Symbolic and
Bag of Words Approaches
CarmelTC is most similar to the text classification ap-
proach described in (Furnkranz et al, 1998). In the ap-
proach described in (Furnkranz et al, 1998), features that
note the presence or absence of a word from a text as
well as extraction patterns from AUTOSLOG-TS (Riloff,
1996) form the feature set that are input to the RIPPER
(Cohen, 1995), which learns rules for classifying texts
based on these features. CarmelTC is similar in spirit
in terms of both the sorts of features used as well as the
general sort of learning approach. However, CarmelTC is
different from (Furnkranz et al, 1998) in several respects.
Where (Furnkranz et al, 1998) make use of
AUTOSLOG-TS extraction patterns, CarmelTC makes
use of features extracted from a deep syntactic analysis
of the text. Since AUTOSLOG-TS performs a surface
syntactic analysis, it would assign a different representa-
tion to all aspects of these texts where there is variation in
the surface syntax. Thus, the syntactic features extracted
from our syntactic analyses are more general. For exam-
ple, for the sentence ?The force was applied by the man
to the object?, our grammar assigns the same functional
roles as for ?The man applied the force to the object? and
also for the noun phrase ?the man that applied the force to
the object?. This would not be the case for AUTOSLOG-
TS.
Like (Furnkranz et al, 1998), we also extract word
features that indicate the presence or absence of a root
form of a word from the text. However, in contrast for
CarmelTC one of the features for each training text that is
made available to the rule learning algorithm is the clas-
sification obtained using the Rainbow Naive Bayes clas-
sifier (McCallum, 1996; McCallum and Nigam, 1998).
Because the texts classified with CarmelTC are so
much shorter than those of (Furnkranz et al, 1998), the
feature set provided to the learning algorithm was small
enough that it was not necessary to use a learning algo-
rithm as sophisticated as RIPPER (Cohen, 1995). Thus,
we used ID3 (Mitchell, 1997; Quinlin, 1993) instead with
excellent results. Note that in contrast to CarmelTC, the
(Furnkranz et al, 1998) approach is purely symbolic.
Thus, all of its features are either word level features or
surface syntactic features.
Recent work has demonstrated that combining multi-
ple predictors yields combined predictors that are supe-
rior to the individual predictors in cases where the in-
dividual predictors have complementary strengths and
weaknesses (Larkey and Croft, 1996; Larkey and Croft,
1995). We have argued that this is the case with symbolic
and ?bag of words? approaches. Thus, we have reason to
expect a hybrid approach that makes a prediction based
on a combination of these single approaches would yield
better results than either of these approaches alone. Our
results presented in Section 5 demonstrate that this is true.
Other recent work has demonstrated that symbolic and
?Bag of Words? approaches can be productively com-
bined. For example, syntactic information can be used
to modify the LSA space of a verb in order to make LSA
sensitive to different word senses (Kintsch, 2002). How-
ever, this approach has only been applied to the analysis
of mono-transitive verbs. Furthermore, it has never been
demonstrated to improve LSA?s effectiveness at classify-
ing texts.
In the alternative Structured Latent Semantic Analy-
sis (SLSA) approach, hand-coded subject-predicate in-
formation was used to improve the results obtained by
LSA for text classification (Wiemer-Hastings and Zipi-
tria, 2001), but no fully automated evaluation of this ap-
proach has been published.
In contrast to these two approaches, CarmelTC is both
fully automatic, in that the symbolic features it uses are
obtained without any hand coding whatsoever, and fully
general, in that it applies to the full range of verb subcat-
egorization frames covered by the COMLEX lexicon, not
only mono-transitive verbs. In Section 5 we demonstrate
that CarmelTC outperforms both LSA and Rainbow, two
alternative bag of words approaches, on the task of stu-
dent essay analysis.
5 Evaluation
We conducted an evaluation to compare the effective-
ness of CarmelTC at analyzing student essays in compar-
ison to LSA, Rainbow, and a purely symbolic approach
similar to (Furnkranz et al, 1998), which we refer to
here as CarmelTCsymb. CarmelTCsymb is identical to
CarmelTC except that it does not include in its feature
set the prediction from Rainbow. Thus, by comparing
CarmelTC with Rainbow and LSA, we can demonstrate
the superiority of our hybrid approach to purely ?bag of
words? approaches. And by comparing with CarmelTC-
symb, we can demonstrate the superiority of our hybrid
approach to an otherwise equivalent purely symbolic ap-
proach.
We conducted our evaluation over a corpus of 126 pre-
viously unseen student essays in response to the Pumpkin
Problem described above, with a total of 500 text seg-
ments, and just under 6000 words altogether. We first
tested to see if the text segments could be reliably tagged
by humans with the six possible Classes associated with
the problem. Note that this includes ?nothing? as a class,
i.e., Class 6. Three human coders hand classified text
segments for 20 essays. We computed a pairwise Kappa
coefficient (Cohen, 1960) to measure the agreement be-
tween coders, which was always greater than .75, thus
demonstrating good agreement according to the Krippen-
dorf scale (Krippendorf, 1980). We then selected two
coders to individually classify the remaining sentences in
the corpus. They then met to come to a consensus on
the tagging. The resulting consensus tagged corpus was
used as a gold standard for this evaluation. Using this
gold standard, we conducted a comparison of the four
approaches on the problem of tallying the set of ?correct
answer aspects? present in each student essay.
The LSA space used for this evaluation was trained
over three first year physics text books. The other three
approaches are trained over a corpus of tagged examples
using a 50 fold random sampling evaluation, similar to a
cross-validation methodology. On each iteration, we ran-
domly selected a subset of essays such that the number
of text segments included in the test set were greater than
10 but less than 15. The randomly selected essays were
then used as a test set for that iteration, and the remain-
der of the essays were used for training in addition to a
corpus of 248 hand tagged example sentences extracted
from a corpus of human-human tutoring transcripts in
our domain. The training of the three approaches dif-
fered only in terms of how the training data was parti-
tioned. Rainbow and CarmelTCsymb were trained us-
ing all of the example sentences in the corpus as a single
training set. CarmelTC, on the other hand, required parti-
tioning the training data into two subsets, one for training
the Rainbow model used for generating the value of its
Rainbow feature, and one subset for training the decision
trees. This is because for CarmelTC, the data for train-
ing Rainbow must be separate from that used to train the
decision trees so the decision trees are trained from a re-
alistic distribution of assigned Rainbow classes based on
its performance on unseen data rather than on Rainbow?s
training data.
In setting up our evaluation, we made it our goal to
present our competing approaches in the best possible
light in order to provide CarmelTC with the strongest
competitors as possible. Note that LSA works by using
its trained LSA space to construct a vector representation
for any text based on the set of words included therein. It
can thus be used for text classification by comparing the
vector obtained for a set of exemplar texts for each class
with that obtained from the text to be classified. We tested
LSA using as exemplars the same set of examples used
Figure 3: This Table compares the performance of the 4 alternative approaches in the per essay evaluation in
terms of precision, recall, false alarm rate, and f-score.
Approach Precision Recall False Alarm Rate F-Score
LSA 93% 54% 3% .70
Rainbow 81% 73% 9% .77
CarmelTCsymb 88% 72% 7% .79
CarmelTC 90% 80% 8% .85
as Rainbow training data, but it always performed better
when using a small set of hand picked exemplars. Thus,
we present results here using only those hand picked ex-
emplars. For every approach except LSA, we first seg-
mented the essays at sentence boundaries and classified
each sentence separately. However, for LSA, rather than
classify each segment separately, we compared the LSA
vector for the entire essay to the exemplars for each class
(other than ?nothing?), since LSA?s performance is better
with longer texts. We verified that LSA also performed
better specifically on our task under these circumstances.
Thus, we compared each essay to each exemplar, and we
counted LSA as identifying the corresponding ?correct
answer aspect? if the cosine value obtained by compar-
ing the two vectors was above a threshold. We tested
LSA with threshold values between .1 and .9 at incre-
ments of .1 as well as testing a threshold of .53 as is
used in the AUTO-TUTOR system (Wiemer-Hastings et
al., 1998). As expected, as the threshold increases from
.1 to .9, recall and false alarm rate both decrease together
as precision increases. We determined based on comput-
ing f-scores2 for each threshold level that .53 achieves the
best trade off between precision and recall. Thus, we used
a threshold of .53, to determine whether LSA identified
the corresponding key point in the student essay or not
for the evaluation presented here.
We evaluated the four approaches in terms of precision,
recall, false alarm rate, and f-score, which were computed
for each approach for each test essay, and then averaged
over the whole set of test essays. We computed preci-
sion by dividing the number of ?correct answer aspects?
(CAAs) correctly identified by the total number of CAAs
identified3 We computed recall by dividing the number of
CAAs correctly identified over the number of CAAs actu-
ally present in the essay4 False alarm rate was computed
by dividing the number of CAAs incorrectly identified by
the total number of CAAs that could potentially be incor-
2We computed our f-scores with a beta value of 1 in order to
treat precision and recall as equally important.
3For essays containing no CAAs, we counted precision as 1
if none were identified and 0 otherwise.
4For essays with no CAAs present, we counted recall as 1
for all approaches.
rectly identified5. F-scores were computed using 1 as the
beta value in order to treat precision and recall as equally
important.
The results presented in Figure 3 clearly demon-
strate that CarmelTC outperforms the other approaches.
In particular, CarmelTC achieves the highest f-score,
which combines the precision and recall scores into a
single measure. In comparison with CarmelTCsymb,
CarmelTC achieves a higher recall as well as a slightly
higher precision. While LSA achieves a slightly higher
precision, its recall is much lower. Thus, the difference
between the two approaches is clearly shown in the f-
score value, which strongly favors CarmelTC. Rainbow
achieves a lower score than CarmelTC in terms of preci-
sion, recall, false alarm rate, and f-score.
6 Conclusion and Current Directions
In this paper we have introduced the CarmelTC text clas-
sification approach as it is applied to the problem of stu-
dent essay analysis in the context of a conceptual physics
tutoring system. We have evaluated CarmelTC over data
collected from students interacting with our system in re-
sponse to one of its 10 implemented conceptual physics
problems. Our evaluation demonstrates that the novel
hybrid CarmelTC approach outperforms both Latent Se-
mantic Analysis (LSA) (Landauer et al, 1998; Laham,
1997) and a Naive Bayes approach (McCallum, 1996;
McCallum and Nigam, 1998) as well as a purely sym-
bolic approach similar to (Furnkranz et al, 1998). We
plan to run a larger evaluation with essays from multiple
problems to test the generality of our result. We also plan
to experiment with other rule learning approaches, such
as RIPPER (Cohen, 1995).
7 Acknowledgments
This research was supported by the Office of Naval Re-
search, Cognitive Science Division under grant number
N00014-0-1-0600 and by NSF grant number 9720359
to CIRCLE, Center for Interdisciplinary Research on
Constructive Learning Environments at the University of
Pittsburgh and Carnegie Mellon University.
5For essays containing all possible CAAs, false alarm rate
was counted as 0 for all approaches.
References
J. Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,
L. Braden-Harder, and M. D. Harris. 1998. Au-
tomated scoring using a hybrid feature identification
technique. In Proceedings of COLING-ACL?98, pages
206?210.
J. Burstein, D. Marcu, S. Andreyev, and M. Chodorow.
2001. Towards automatic classification of discourse
elements in essays. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics, Toulouse, France.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20(Winter):37?46.
W. W. Cohen. 1995. Fast effective rule induction. In
Proceedings of the 12th International Conference on
Machine Learning.
M. Craven, D. DiPasquio, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to extract symbolic knowledge from the world wide
web. In Proceedings of the 15th National Conference
on Articial Intelligence.
P. W. Foltz, W. Kintsch, and T. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse Processes, 25(2-3):285?307.
J. Furnkranz, T. Mitchell Mitchell, and E. Riloff. 1998.
A case study in using linguistic phrases for text cat-
egorization on the www. In Proceedings from the
AAAI/ICML Workshop on Learning for Text Catego-
rization.
A. Graesser, K. Vanlehn, TRG, and NLT Group. 2002.
Why2 report: Evaluation of why/atlas, why/autotutor,
and accomplished human tutors on learning gains for
qualitative physics problems and explanations. Tech-
nical report, LRDC Tech Report, University of Pitts-
burgh.
R. Grishman, C. Macleod, and A. Meyers. 1994. COM-
LEX syntax: Building a computational lexicon. In
Proceedings of the 15th International Conference on
Computational Linguistics (COLING-94).
R. R. Hake. 1998. Interactive-engagement versus tra-
ditional methods: A six-thousand student survey of
mechanics test data for introductory physics students.
American Journal of Physics, 66(64).
I. A. Halloun and D. Hestenes. 1985. The initial knowl-
edge state of college physics students. American Jour-
nal of Physics, 53(11):1043?1055.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
K. Krippendorf. 1980. Content Analysis: An Introduc-
tion to Its Methodology. Sage Publications.
D. Laham. 1997. Latent semantic analysis approaches
to categorization. In Proceedings of the Cognitive Sci-
ence Society.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25(2-3):259?284.
L. S. Larkey and W. B. Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical
Report IR-64, University of Massachusetts Center for
Intelligent Information Retrieval.
L. S. Larkey and W. B. Croft. 1996. Combining classi-
fiers in text categorization. In Proceedings of SIGIR.
L. Larkey. 1998. Automatic essay grading using text
categorization techniques. In Proceedings of SIGIR.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Classication.
Andrew Kachites McCallum. 1996. Bow: A toolkit
for statistical language modeling, text retrieval, clas-
sification and clustering. http://www.cs.cmu.edu/ mc-
callum/bow.
T. M. Mitchell. 1997. Machine Learning. McGraw Hill.
J. R. Quinlin. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers: San Mateo, CA.
E. Riloff. 1996. Using learned extraction patterns for text
classification. In S. Wermter, R. Riloff, and G. Scheler,
editors, Connectionist, Statistical, and Symbolic Ap-
proaches for Natural Language Processing. Springer-
Verlag.
C. P. Rose? and A. Lavie. 2001. Balancing robustness
and efficiency in unification augmented context-free
parsers for large practical applications. In J. C. Junqua
and G. Van Noord, editors, Robustness in Language
and Speech Technologies. Kluwer Academic Press.
C. P. Rose?, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. Vanlehn. 2002a. A hybrid language un-
derstanding approach for robust selection of tutoring
goals. In Proceedings of the Intelligent Tutoring Sys-
tems Conference.
C. P. Rose?, D. Bhembe, A. Roque, and K. VanLehn.
2002b. An efficient incremental architecture for ro-
bust interpretation. In Proceedings of the Human Lan-
guages Technology Conference, pages 307?312.
C. P. Rose?, P. Jordan, and K. VanLehn. 2002c. Can we
help students with high initial competency? In Pro-
ceedings of the ITS Workshop on Empirical Methods
for Tutorial Dialogue Systems.
C. P. Rose?. 2000. A framework for robust semantic in-
terpretation. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 311?318.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural Lan-
guag e Tutoring Group. 2002. The architecture of
why2-atlas: a coach for qualitative physics essay writ-
ing. In Proceedings of the Intelligent Tutoring Systems
Conference, pages 159?167.
P. Wiemer-Hastings and I. Zipitria. 2001. Rules for
syntax, vectors for semantics. In Proceedings of the
Twenty-third Annual Conference of the Cognitive Sci-
ence Society.
P. Wiemer-Hastings, A. Graesser, D. Harter, and the Tu-
toring Res earch Group. 1998. The foundations
and architecture of autotutor. In B. Goettl, H. Halff,
C. Redfield, and V. Shute, editors, Intelligent Tutor-
ing Systems: 4th International Conference (ITS ?98 ),
pages 334?343. Springer Verlag.
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 673?676,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Making Conversational Structure Explicit:  
Identification of Initiation-response Pairs within Online Discussions 
 
Yi-Chia Wang Carolyn P. Ros? 
Language Technologies Institute  Language Technologies Institute  
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213, USA Pittsburgh, PA 15213, USA 
yichiaw@cs.cmu.edu cprose@cs.cmu.edu 
 
 
Abstract 
In this paper we investigate how to identify 
initiation-response pairs in asynchronous, 
multi-threaded, multi-party conversations.  
We formulate the task of identifying initia-
tion-response pairs as a pairwise ranking 
problem. A novel variant of Latent Semantic 
Analysis (LSA) is proposed to overcome a li-
mitation of standard LSA models, namely that 
uncommon words, which are critical for sig-
naling initiation-response links, tend to be 
deemphasized as it is the more frequent terms 
that end up closer to the latent factors selected 
through singular value decomposition. We 
present experimental results demonstrating 
significantly better performance of the novel 
variant of LSA over standard LSA.  
1 Introduction 
In recent years, research in the analysis of social 
media (e.g., weblogs, discussion boards, and mes-
sengers) has grown in popularity. Unlike exposito-
ry text, the data produced through use of social 
media is often conversational, multi-threaded, and 
more complex because of the involvement of nu-
merous participants who are distributed both across 
time and across space. Recovering the multi-
threaded structure is an active area of research. 
In this paper, we form the foundation for a 
broader study of this type of data by investigating 
the basic unit of interaction, referred to as an initi-
ation-response pair (Schegloff, 2007). Initiation-
response pairs are pairs of utterances that are typi-
cally contributed by different participants, and 
where the first pair part sets up an expectation for 
the second pair part. Types of common initiation-
response pairs include question-answer, assess-
ment-agreement, blame-denial, etc. Note that al-
though sometimes discussion forum interfaces 
make the thread structure of the interaction expli-
cit, these affordances are not always present. And 
even in forums that have these affordances, the 
apparent structure of the discourse as represented 
through the interface may not capture all of the 
contingencies between contributions in the unfold-
ing conversation. Thus, the goal of this investiga-
tion is to investigate approaches for automatically 
identifying initiation-response pairs in conversa-
tions.   
One of the challenges in identifying initiation-
response pairs is that the related messages are not 
necessarily adjacent to each other in the stream of 
contributed messages, especially within the asyn-
chronous environment of social media. Further-
more, individual differences related to writing style 
or creative expression of self may also complicate 
the identification of the intended connections be-
tween contributions. Identification of initiation-
response pairs is an important step towards auto-
matic processing of conversational data. One po-
tential application of this work is conversation 
summarization. A summary should include both 
the initiation and response as a coherent unit or it 
may fail to capture the intended meaning. 
We formulate the task of identifying initiation-
response pairs as a pairwise ranking problem. The 
goal is to distinguish message pairs that constitute 
an initiation-response pair from those that do not. 
We believe a ranking approach, where the degree 
of relatedness between a message pair can be con-
sidered in light of the relatedness between each of 
them and the surrounding messages within the 
same thread, is a more suitable paradigm for this 
task than a discrete classification-based paradigm.    
 Previous work on recovering conversational 
structure has relied on simple lexical cohesion 
673
measures (i.e., cosine similarity), temporal infor-
mation (Lewis and Knowles, 1997; Wang et al, 
2008), and meta-data (Minkov et al, 2006). How-
ever, relatively little work has investigated the im-
portance of specifically in-focus connections 
between initiation-response pairs and utilized them 
as clues for the task. Consider, for example, the 
following excerpt discussing whether congress 
should pass a bill requiring the use of smaller cars 
to save the environment:   
a) Regressing to smaller vehicles would discourage 
business from producing more pollution. 
b) If CO2 emissions are lowered, wouldn't tax revenues 
be lowered as well? Are the democrats going to wil-
lingly give up Medicaid and social security? 
Although segment (b) is a reply to segment (a), the 
amount of word overlap is minimal. Nonetheless, 
we can determine that (b) is a response to (a) by 
recognizing the in-focus connections, such as "ve-
hicles-CO2" and "pollution-CO2." To properly 
account for connections between initiations and 
responses, we introduce a novel variant of Latent 
Semantic Analysis (LSA) into our ranking model.  
In section 2, we describe the Usenet data and 
how we extract a large corpus of initiation-
response pairs from it. Section 3 explains our rank-
ing model as well as the proposed novel LSA vari-
ation. The experimental results and discussion are 
detailed in Section 4 and Section 5, respectively. 
2 Usenet and Generation of Data 
The experiment for this paper was conducted using 
data crawled from the alt.politics.usa Usenet (User 
Network) discussion forum, including all posts 
from the period between June 2003 and June 2008.  
The resulting set contains 784,708 posts. The posts 
in this dataset alo contain meta-data that makes 
parent-child relationships explicit (i.e., through the 
References field). Thus, we know 625,116 of the 
posts are explicit responses to others posts. The 
messages are organized into a total of 77,985 dis-
cussion threads, each of which has 2 or more posts.   
In order to evaluate the quality of using the ex-
plicit reply structure as our gold standard for initia-
tion-response links, we asked human judges to 
annotate the response structure of a random-
selected medium-length discussion (19 posts) 
where we had removed the meta-data that indi-
cated the initiation-reply structure. The result 
shows the accuracy of our gold standard is 0.89. 
To set up the data as a pairwise ranking prob-
lem, we arranged the posts in the corpus into in-
stances containing three messages each, one of 
which is a response message, one of which is the 
actual initiating message, and the other of which is 
a foil selected from the same thread. The idea is 
that the ranking model will be trained to prefer the 
actual initiating message in contrast to the foil.   
The grain size of our examples is finer than 
whole messages. More specifically, positive exam-
ples are pairs of spans of text that have an initia-
tion-reply relationship. We began the process with 
pairs of messages where the meta-data indicates 
that an initiation-reply relationship exits, but we 
didn?t stop there. For our task it is important to 
narrow down to the specific spans of text that have 
the initiation-response relation. For this, we used 
the indication of quoted material within a message. 
We observed that when users explicitly quote a 
portion of a previously posted message, the portion 
of text immediately following the quoted material 
tends to have an explicit discourse connection with 
it. Consider the following example:  
>> Why is the quality of life of the child, mother,  
>> and society at large, more important than the 
>> sanctity of life? 
> Because in the case of anencephaly at least, 
> the life is ended before it begins. 
We disagree on this point. Why do you refuse to 
provide your very own positive definition of life?  
Do you believe life begins before birth?  At birth?  
After birth?  Never? 
In this thread, the reply expresses an opinion 
against the first level quote, but not the second lev-
el quote. Thus, we used segments of text with sin-
gle quotes as an initiation and the immediately 
following non-quoted text as the response. We ex-
tracted positive examples by scanning each post to 
locate the first level quote that is immediately fol-
lowed by unquoted content. If such quoted material 
was found, the quoted material and the unquoted 
response were both extracted to form a positive 
example. Otherwise, the message was discarded. 
For each post P where we extracted a positive 
example, we also extracted a negative example by 
picking a random post R from the same thread as 
P. We selected the negative example in such a way 
to make the task difficult in a realistic way. Choos-
ing R from other threads would make the task too 
easy because the topics of P and R would most 
likely be different. We also stipulated that R cannot 
be the parent, grandparent, sibling, or child of P. 
674
Together the non-quoted text of P and R forms a 
negative instance. Thus, the final dataset consists 
of pairs of message pairs ((pi, pj), (pi, pk)), where 
they have the same reply message pi, and pj is the 
correct quote message of pi, but pk is not. In other 
words, (pi, pj) is considered as a positive example; 
(pi, pk) is a negative example. We constructed a 
total of 100,028 instances for our dataset, 10,000 
(~10%) of which were used for testing, and 90,028 
(~90%) of which were the learning set used to con-
struct the LSA space described in the next section. 
3 Ranking Models for Identification of  
Initiation-Response Pairs 
Our pairwise ranking model1 takes as input an or-
dered pair of message pairs ((pi, pj), (pi, pk)) and 
computes their relatedness using a similarity func-
tion sim. Specifically, 
( xij, xik ) = ( sim (pi, pj), sim (pi, pk) ) 
where xij is the similarity value between post pi and 
pj; xik is the similarity value between post pi and pk.   
To determine which of the two message pairs ranks 
higher regarding initiation-response relatedness, 
we use the following scoring function to compare 
their corresponding similarity values: 
score (xij, xik) = xij ? xik  
If the score is positive, the model ranks (pi, pj) 
higher than (pi, pk) and vice versa. A message pair 
ranked higher means it has more evidence of being 
an initiation-reply link, compared to the other pair. 
3.1 Alternative Similarity Functions 
We introduce and motivate 3 alternative similarity 
functions, where the first two are considered as 
baseline approaches and the third one is a novel 
variation of LSA. We argue that the proposed LSA 
variation is an appropriate semantic similarity 
measurement for identifying topic continuation and 
initiation-reply pairs in online discussions.  
Cosine Similarity (cossim).  We choose an ap-
proach that uses only lexical cohesion as our base-
line. Previous work (Lewis and Knowles, 1997; 
Wang et al, 2008) has verified its usefulness for 
the thread identification task. In this case, 
                                                          
1 We cast the problem as a pairwise ranking problem in order 
to focus specifically on the issue of characterizing how initia-
tion-response links are encoded in language through lexical 
choice.  Note that once trained, pairwise ranking models can 
be used to rank multiple instances. 
sim(pi,pj) = cossim(pi,pj)  
where cossim(pi,pj) computes the cosine of the an-
gle between two posts pi and pj while they are 
represented as term vectors. 
LSA Average Similarity (lsaavg).  LSA is a well-
known method for grouping semantically related 
words (Landauer et al, 1998). It represents word 
meanings in a concept space with dimensionality k. 
Before we describe how to compute average simi-
larity given an LSA space, we explain how the 
LSA space was constructed in our work. First, we 
construct a term-by-document matrix, where we 
use the 90,028 message learning set mentioned at 
the end of Section 2. Next, LSA applies singular 
value decomposition to the matrix, and reduces the 
dimensionality of the feature space to a k dimen-
sional concept space. This generated LSA space is 
used by both lsaavg and lsacart later. 
For lsaavg, we follow Foltz et al (1998):  
 
 
 
 
The meaning of each post is represented as a vec-
tor in the LSA space by averaging across the LSA 
representations for each of its words. The similari-
ty between the two posts is then determined by 
computing the cosine value of their LSA vectors.  
This is the typical method for using LSA in text 
similarity comparisons. However, note that not all 
words carry equal weight within the vector that 
results from this averaging process. Words that are 
closer to the "semantic prototypes" represented by 
each of the k dimensions of the reduced vector 
space will have vectors with longer lengths than 
words that are less prototypical. Thus, those words 
that are closer to those prototypes will have a larg-
er effect on the direction of the resulting vector and 
therefore on the comparison with other texts. An 
important consideration is whether this is a desira-
ble effect. It would lead to deemphasizing those 
unusual types of information that might be being 
discussed as part of a post. However, one might 
expect that those things that are unusual types of 
information might actually be more likely to be the 
in-focus information within an initiation that res-
ponses may be likely to refer to. In that case, for 
our purposes, we would not expect this typical me-
thod for applying LSA to work well. 
LSA Cartesian Similarity (lsacart).  To properly 
account for connections between initiations and 
( ) ( )
??
?
?
?
?
??
?
?
?
?
==
??
??
j
pt
b
i
pt
a
jiji p
t
p
t
pplsaavgppsim jbia ,cos,,
675
responses that include unusual words, we introduce 
the following similarity function:  
 
 
 
 
where we take the mean of the cosine values for all 
the word pairs in the Cartesian product of posts pi 
and pj. Note that in this formulation, all words have 
an equal chance to affect the overall similarity be-
tween vectors since it is the angle represented by 
each word in a pair that comes to play when cosine 
distance is applied to a word pair. Length is no 
longer a factor. Moreover, the averaging is across 
cosine similarity scores rather than LSA vectors. 
4 Experimental Results 
The results are found in Table 1. For comparison, 
we also report the random baseline (0.50).  
 
  Random 
Baseline 
Cos- 
Similarity 
LSA- 
Average 
LSA- 
Cart 
Accuracy 0.50 0.66 0.60 0.71 
Table 1. Overview of results 
Besides the random baseline, LSA-Average per-
forms the worst (0.60), with simple Cosine similar-
ity (0.66) in the middle, and LSA-Cart (0.71) the 
best, with each of the pairwise contrasts being sta-
tistically significant. We believe the reason why 
LSA-Average performs so poorly on this task is 
precisely because, as discussed in last section, it 
deemphasizes those words that contribute the most 
unusual content. LSA-Cart addresses this issue. 
To further understand this effect, we conducted 
an error analysis. We divided the instances into 4 
sets based on the lexical cohesion between the re-
sponse and the true initiation and between the re-
sponse and the foil, by taking the median split on 
the distributions of these two cohesion scores.  Our 
finding is that model performances vary by subset. 
In particular, we find that it is only in cases where 
the positive example has low lexical cohesion (e.g. 
our "vehicles-CO2" and "pollution-CO2" example 
from the earlier section), that we see the benefit of 
the LSA-Cart approach.  In other cases, where the 
cohesion between the reply and the true initiation 
is high, Cos-Similarity performs best. 
5 Discussion and Conclusion 
We have argued why the task of detecting initia-
tion-response pairs in multi-party discussions is 
important and challenging. We proposed a method 
for acquiring a large corpus for use to identify init-
iation-response pairs. In our experiments, we have 
shown that the ranking model using a variant of 
LSA performs best, which affirms our hypothesis 
that unusual information and uncommon words 
tends to be the focus of ongoing discussions and 
therefore to be the key in identifying initiation-
response links. 
In future work, we plan to further investigate the 
connection between an initiation-response pairs 
from multiple dimensions, such as topical cohe-
rence, semantic relatedness, conversation acts, etc. 
One important current direction is to develop a 
richer operationalization of the interaction that ac-
counts for the way posts sometimes respond to a 
user, a collection of users, or a user?s posting histo-
ry, rather than specific posts per se. 
Acknowledgments 
We thank Mary McGlohon for sharing her data 
with us.  This research was funded through NSF 
grant DRL-0835426. 
References  
David D. Lewis and Kimberly A. Knowles. 1997. 
Threading electronic mail: A preliminary study. In-
formation Processing and Management, 33(2), 209?
217. 
Einat Minkov, William W. Cohen, Andrew Y. Ng. 
2006. Contextual Search and Name Disambiguation 
in Email using Graphs. In Proceedings of the Inter-
national ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR), pages 35?
42. ACM Press, 2006. 
Peter W. Foltz, Walter Kintsch, Thomas K. Landauer. 
1998. Textual coherence using latent semantic analy-
sis. Discourse Processes, 25, 285?307. 
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to latent semantic analysis. 
Discourse Processes, 25, 259-284.  
Schegloff, E. 2007.  Sequence Organization in Interac-
tion: A Primer in Conversation Analysis, Cambridge 
University Press. 
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, Ca-
rolyn P. Ros?. 2008. Recovering Implicit Thread 
Structure in Newsgroup Style Conversations. In Pro-
ceedings of the 2nd International Conference on 
Weblogs and Social Media (ICWSM II), Seattle, 
USA. 
( ) ( )
( )
ji
pptt
ba
jiji pp
tt
pplsacartppsim jiba
 
,cos
,,
),(
?
??
==
676
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 677?680,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Engaging learning groups using Social Interaction Strategies 
Rohit Kumar Carolyn P. Ros? 
Language Technologies Institute 
Carnegie Mellon University, Pittsburgh, PA, 15213 
rohitk@cs.cmu.edu cprose@cs.cmu.edu 
 
 
Abstract 
Conversational Agents have been shown to be 
effective tutors in a wide range of educational 
domains. However, these agents are often ig-
nored and abused in collaborative learning 
scenarios involving multiple students. In our 
work presented here, we design and evaluate 
interaction strategies motivated from prior re-
search in small group communication. We 
will discuss how such strategies can be im-
plemented in agents. As a first step towards 
evaluating agents that can interact socially, we 
report results showing that human tutors em-
ploying these strategies are able to cover more 
concepts with the students besides being rated 
as better integrated, likeable and friendlier. 
1 Introduction 
Conversational Agents (CAs) are autonomous in-
terfaces that interact with users via spoken or writ-
ten conversation. One of the applications of CAs is 
tutoring. Various research groups have developed 
tutoring agents in domains like reading, algebra, 
geometry, calculus, physics, computer literacy, 
programming, foreign languages, research methods 
and thermodynamics. Many of the evaluations 
show that CAs can be effective tutors (Arnott et. 
al., 2008; Kumar et. al., 2007; Graesser et. al., 
2005). 
Most systems that use CAs as tutors have been 
built for learning scenarios involving one student. 
Evaluation of learning technologies involving stu-
dents working in groups with interactive agents has 
shown that learners are helped both by learning as 
a group and receiving tutorials from agents (Kumar 
et. al., 2007). However, some previous studies 
have reported that students learning in groups ig-
nore the tutor?s messages, unlike the case where 
students are individually tutored. Groups are more 
likely to abuse tutors than individual students.  
We reason that the presence of other students in 
collaborative learning scenarios causes the agents 
to compete for the attention of the students. Since 
the agents are not adept at performing social inter-
active behavior, which makes up the bulk of for-
mative communication in a group, they are quickly 
pushed to the periphery of the group. 
Research on small group communication has 
identified twelve interaction categories that are 
commonly observed in small groups (Bales, 1950). 
These categories are broadly classified into task 
and social-emotional categories. Content presented 
by most current CAs mostly classifies under the 
task categories. In section 2, we will list the con-
versational strategies motivated from the three pos-
itive social-emotional interaction categories. 
Thereafter, the implementation and evaluation of a 
CA that interleaves these social interaction strate-
gies while executing a task plan will be described. 
2 Social Interaction Strategies 
Balesian methodology (Bales, 1950) identifies 
three positive social-emotional interaction catego-
ries: showing solidarity, showing tension release 
and agreeing. Participants contribute turns of these 
categories to address the problems of re-
integration, tension release and decision respec-
tively. We have mapped these categories to practi-
cally implementable conversational strategies. This 
mapping is shown in table 1 ahead. 
Each strategy is implemented as an instantiation 
of a conversational behavior. Most of the strategies 
listed in Table 1 are realized as prompts, triggered 
by rules based on agent plan, discourse and context 
features. For example, strategy 1e is triggered 
677
when one or more students in the group are found 
to be inactive for over 5 minutes. In this event, the 
tutor chooses to raise the status of the inactive stu-
dents by eliciting contributions from them through 
a prompt like: Do you have any suggestions Mike? 
More implementation details of these strategies 
and triggers are discussed in the following section. 
 
1. Showing Solidarity 
Raises other's status, gives help, reward 
1a. Do Introductions 
Introduce and ask names of all participants 
1b. Be Protective & Nurturing 
Discourage teasing 
1c. Give Re-assurance 
When student is discontent, asking for help 
1d. Complement / Praise 
To acknowledge student contributions 
1e. Encourage 
When group or members are inactive 
1f. Conclude Socially 
2. Showing Tension Release 
Jokes, laughs, shows satisfaction 
2a. Expression of feeling better 
After periods of tension, work pressure 
2b. Be cheerful 
2c. Express enthusiasm, elation, satisfaction 
On completing significant steps of the task 
3. Agreeing 
Shows passive acceptance, understands, 
concurs, complies 
3a. Show attention 
To student ideas as encouragement 
3b. Show comprehension / approval 
To student opinions and orientations 
Table 1. Social Interaction Strategies for three  
social-emotional interaction categories 
3 WrenchTalker: Implementation 
WrenchTalker is a CA we have built to employ the 
social interaction strategies listed in section 2. It 
helps teams of engineering students learn and ap-
ply basic concepts of mechanical stress while they 
participate in a freshmen lab project to design an 
aluminum wrench. Students can interact with this 
agent using a text-based chat environment. 
The agent is built using the Basilica architecture 
(Kumar and Ros?, 2009). Under this architecture, 
CAs are modeled as a network of behavioral 
components. There are three types of components: 
actors (actuators / performers), filters (perceptors / 
annotators / cordinators) and memories. Figure 1 
below shows a simplified depiction of the 
WrenchTalker component network. 
 
 
Figure 1. Component Network of WrenchTalker 
Three of the actor and filter components 
correspond to three observable behaviors of the 
tutor, i.e., Introducing (ai, fi), Prompting (ap, fp) and 
Tutoring (at, ft). Most of the other filter 
components form a sub-network that annotates 
turns with applicable semantic categories, 
accumulates them to identify inactive students and 
generates events that regulate the controllers. 
The plan controller (fplan) is responsible for 
executing the agent?s interaction plan, which is 
comprised of 37 steps. The plan is executed largely 
sequentially; however the plan controller can 
choose to skip some steps in the interest of time. In 
the experiment described in section 5, the same 
plan controller is used in all three conditions. The 
social controller (fsocial) implements the 12 
strategies listed earlier. The strategies are triggered 
by rules based on combinations of three 
conditions: the last executed plan step, semantic 
categories associated with the most recent student 
turns and the ratio of tutor turns generated by fsocial 
to fplan. The first two conditions attempt to ensure 
that social behavior is suitable in the current 
conversational context and the third condition 
regulates the amount of social behavior by the CA. 
The plan and social controllers are connected so 
that they regulate each other. For instance, when 
the plan controller is working, it blocks fsocial. Upon 
completion of the blocking step, fsocial is given 
control, which can then choose to perform a 
strategy by blocking fplan before it progresses to the 
next step. Reflex strategies like 1b are not blocked. 
Once the controllers determine a step or a strat-
egy that is to be generated, the actors generate their 
turns. For example, strategy 1a is generated by ac-
tor ai after it is triggered by the social controller. 
We note that Basilica provides the flexibility to 
build complicated pipelines, as demonstrated in 
this case by the use of two controllers. 
678
4 Related Work 
To contextualize our research with other work on 
CAs, we classify agents with the social interaction 
strategies listed in Table 1 as social interfaces fol-
lowing the taxonomy proposed by Isbister (2002). 
Within this class of CAs, researchers have investi-
gated the technical challenges and effects of con-
versational behavior that are similar in motivation 
to the ones we are exploring. Bickmore et. al. 
(2009) report that users found agents with autobio-
graphies, i.e., back stories in first person more en-
joyable and they completed more conversations 
with such agents. Dybala et. al. (2009) found that 
agents equipped with humor were evaluated as 
more human-like, funny and likeable. In a multi-
party conversational scenario, Dohsaka et. al. 
(2009) found that an agent?s use of emphatic ex-
pressions improved user satisfaction and user rat-
ing of the agent. We note that use of CAs as social 
interfaces has been found to have effects on both 
performance and perception metrics. 
5 Experimental Design 
In order to evaluate the effect of social interaction 
strategies listed in Table 1, we designed an expe-
riment with three conditions. In the experimental 
condition (Social), students interacted with an 
agent that was equipped with our social interaction 
strategies, unlike the control condition (Task). In 
the third condition, a human tutor was allowed to 
intervene while the students interacted with a Task 
agent. In all three conditions, students go through 
the same task plan. However, the degree of social 
performance is varied from minimal (Task) to ideal 
(Human). We hypothesize that the human and so-
cial agents will be rated better than the Task agent. 
We conducted a between subjects experiment 
during a freshmen computer aided engineering lab. 
98 students participated in the experiment, which 
was held over six sessions spread evenly between 
two days. The two days of the experiment were 
separated by two weeks. Students were grouped 
into teams of three to four individuals. Students 
were grouped so that no two members of the same 
team sat next to each other during the lab, to en-
sure all communication was recorded. The teams 
were distributed between the three conditions. 
Each session started with a follow-along tutori-
al of computer-aided analysis where the students 
analyzed a wrench they had designed earlier. The 
experimental manipulation happened during a col-
laborative design competition after the tutorial. 
Students were asked to work as a team to design a 
better wrench considering three aspects: ease of 
use, cost and safety. Students were instructed to 
make three new designs and calculate success 
measures of each of the three considerations. They 
were also told that a tutor will help them with two 
designs so that they are well-prepared to do the 
final design. No additional details about the tutor 
were given. The students communicated with each 
other and with the tutors using ConcertChat, an on-
line environment that provides text-based instant 
messaging and workspace sharing facilities. 
After spending 30-35 minutes on the design 
competition, each student filled out a question-
naire. It was comprised of eighteen questions on a 
seven point Likert-scale ranging from Strongly 
Disagree (1) to Strongly Agree (7). The questions 
were designed to elicit four types of ratings. 
? Ratings about the tutor 
? Ratings about the other team members 
? Ratings about the design task 
? Ratings about the team functioning 
The questions in the first two classes elicited 
perceived liking and integration and checked 
whether the students noticed the tutor?s display of 
the social interaction strategies. Task related ques-
tions asked about satisfaction, perceived legitimacy 
and discussion quality. 
6 Results 
Table 2 below shows the mean values for ques-
tionnaire categories apart from ratings about team 
members, since there were no significant effects 
related to those questions. 
 
  D1 D2  T S H 
Integration 3.85 3.94 3.03 3.94 4.77 
Liking 3.68 3.63 2.78 3.53 4.73 
Friendly 5.13 5.43 4.47 5.56 5.83 
T.Releasing 4.49 4.63 3.84 4.61 5.27 
Agreeing 4.30 4.45 3.97 4.44 4.73 
Satisfaction 4.66 5.77 5.09 4.75 5.97 
Table 2. Mean outcomes per condition ((T)ask,(S)ocial, 
(H)uman) and per day (Day1, and Day2) 
The means are highlighted appropriately 
(p<0.001, p<0.05, p<0.08) to indicate significant 
679
differences from Day1 to Day2 and between the 
Task condition and each of the other two using a 
pairwise Tukey comparison. 
First of all, we note that there is a significant 
difference in task satisfaction between the two 
days. We fine-tuned the timing parameters of the 
plan controller after day 1 so that the students had 
sufficient time to follow along with each of the 
steps. This was particularly useful for the task con-
dition where the steps would be executed rapidly 
due to lack of regulation by the social controller. 
On the right side of Table 2, we notice that the 
human tutors (H) were rated higher on being part 
of the team (Integration), being more liked, being 
friendlier and keeping the group more socially 
comfortable (T.Releasing). On the other hand, the 
social tutors (S) were rated to be friendlier and 
were only marginally better at being seen as part of 
the team. 
 
 Strategy Social Human 
Introducing 1a 2.67 3.80 
Friendly 1b-1e 5.61 8.10 
Concluding 1f 0.97 1.80 
T.Releasing 2a-2c 5.81 1.77 
Agreeing 3a-3b 1.78 4.90 
Sum  16.83 22.17 
Table 3. Mean counts of social turns by tutor 
Note that human tutors were restricted to exhi-
bit only social behaviors, which were displayed in 
addition to the same task related content given to 
students in the other two conditions. Clearly, the 
human tutors were better at employing the social 
interaction strategies. To further investigate this, 
we compare the number of turns corresponding to 
the broad categories of strategies in Table 3. Hu-
man tutors performed significantly more (p<0.001) 
social turns than the automated tutors in all strate-
gies except showing tension release. 
7 Conclusions 
In order to make CAs that can participate in multi-
party conversational scenarios, the agents must be 
able to employ Social Interaction Strategies. Here 
we have shown that the human tutors that use these 
strategies are better integrated into the group, and 
are considered more likeable and friendlier. These 
tutors also cover more steps and concepts and take 
less time to tutor the concepts, suggesting that the 
students are more engaged and responsive to them. 
On the other hand, automated tutors that employ 
these strategies in our current implementation do 
not show significant differences compared to task 
tutor. 
We note a contrast between the performance of 
the human and the automated tutors with respect to 
the frequency with which they employ these strat-
egies. Besides the frequent use of these strategies, 
we believe human tutors were better at identifying 
opportunities for employing these strategies, and 
they are able to customize the prompt to better suit 
the discourse context. 
Acknowledgments 
The research was supported by NSF grant number 
DUE 837661 
References 
Elizabeth Arnott, Peter Hastings and David Allbritton, 
2008, Research Methods Tutor: Evaluation of a di-
alogue-based tutoring system in the classroom, Beha-
vior Research Methods, 40 (3), 694-698 
Robert F. Bales, 1950, Interaction process analysis: A 
method for the study of small groups, Addison-
Wesley, Cambridge, MA 
Timothy Bickmore, Daniel Schulman and Langxuan 
Yin, Engagement vs. Deceit: Virtual Humans with 
Human Autobiographies, 2009, IVA, Amsterdam 
Kohji Dohsaka, Ryoto Asai, Ryichiro Higashinaka, Ya-
suhiro Minami and Eisaku Maeda, Effects of Con-
versational Agents on Human Communication in 
Though Evoking Multi-Party dialogues, 2009, 10th 
Annual SigDial, London, UK 
Pawel Dybala, Michal Ptaszynski, Rafal Rzepka and 
Kenji Araki, Humoroids: Conversational Agents that 
induce positive emotions with humor, 2009, AAMAS, 
Budapest, Hungary 
Arthur C. Graesser, Patrick Chipman, Brian C. Haynes, 
and Andrew Olney, 2005, AutoTutor: An Intelligent 
Tutoring System with Mixed-initiative Dialogue, 
IEEE Transactions in Education, 48, 612-618 
Katherine Isbister and Patrick Doyle, Design and Evalu-
ation of Embodied Conversational Agents: A Pro-
posed Taxonomy, 2002, AAMAS Workshop: 
Embodied Conversational Agents, Bologna, Italy 
Rohit Kumar, Carolyn Ros?, Mahesh Joshi, Yi-Chia 
Wang, Yue Cui and Allen Robinson, Tutorial Dialo-
gue as Adaptive Collaborative Learning Support, 
13th AIED 2007, Los Angeles, California 
Rohit Kumar, Carolyn Ros?, Building Conversational 
Agents with Basilica, 2009, NAACL, Boulder, CO 
680
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 25?28,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Interactive Tool for Supporting Error Analysis for Text Mining
Elijah Mayfield
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15216, USA
elijah@cmu.edu
Carolyn Penstein-Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15216, USA
cprose@cs.cmu.edu
Abstract
This demo abstract presents an interactive tool
for supporting error analysis for text mining,
which is situated within the Summarization
Integrated Development Environment (SIDE).
This freely downloadable tool was designed
based on repeated experience teaching text
mining over a number of years, and has been
successfully tested in that context as a tool for
students to use in conjunction with machine
learning projects.
1 Introduction
In the past decade, more and more work in the
language technologies community has shifted from
work on formal, rule-based methods to work involv-
ing some form of text categorization or text mining
technology. At the same time, use of this technology
has expanded; where it was once accessible only to
those within studying core language technologies,
it is now almost ubiquitous. Papers involving text
mining can currently be found even in core social
science and humanities conferences.
The authors of this demonstration are involved
in regular teaching of an applied machine learning
course, which attracts students from virtually every
field, including a variety of computer science related
fields, the humanities and social sciences, and the
arts. In five years of teaching this course, what has
emerged is the finding that the hardest skill to impart
to students is the ability to do a good error analysis.
In response to this issue, the interactive error analy-
sis tool presented here was designed, developed, and
successfully tested with students.
In the remainder of this demo abstract, we offer an
overview of the development environment that pro-
vides the context for this work. We then describe
on a conceptual level the error analysis process that
the tool seeks to support. Next, we step through the
process of conducting an error analysis with the in-
terface. We conclude with some directions for our
continued work, based on observation of students?
use of this interface.
2 Overview of SIDE
The interactive error analysis interface is situated
within an integrated development environment for
building summarization systems. Note that the
SIDE (Kang et al, 2008) software and comprehen-
sive user?s manual are freely available for down-
load1. We will first discuss the design of SIDE from
a theoretical standpoint, and then explore the details
of practical implementation.
2.1 Design Goals
SIDE was designed with the idea that documents,
whether they are logs of chat discussions, sets of
posts to a discussion board, or notes taken in a
course, can be considered relatively unstructured.
Nevertheless, when one thinks about their interpre-
tation of a document, or how they would use the in-
formation found within a document, then a structure
emerges. For example, an argument written in a pa-
per often begins with a thesis statement, followed by
supporting points, and finally a conclusion. A reader
1SIDE and its documentation are downloadable from
http://www.cs.cmu.edu/?cprose/SIDE.html
25
can identify with this structure even if there is noth-
ing in the layout of the text that indicates that certain
sentences within the argument have a different sta-
tus from the others. Subtle cues in the language can
be used to identify those distinct roles that sentences
might play.
Conceptually, then, the use of SIDE proceeds in
two main parts. The first part is to construct filters
that can impose that structure on the texts to be sum-
marized, to identify the role a sentence is playing
in a document; and the second part is constructing
specifications of summaries that refer to that struc-
ture, such as subsets of extracted text or data visu-
alizations. This demo is primarily concerned with
supporting error analysis for text mining. Thus, the
first of these two stages will be the primary focus.
This approach to summarization was inspired by
the process described in (Teufel and Moens, 2002).
That work focused on the summarization of scien-
tific articles to describe a new work in a way which
rhetorically situates that work?s contribution within
the context of related prior work. This is done by
first overlaying structure onto the documents to be
summarized, categorizing the sentences they contain
into one of a number of rhetorical functions. Once
this structure is imposed, using the information it
provides was shown to increase the quality of gener-
ated summaries.
2.2 Building Text Mining Models with SIDE
This demo assumes the user has already interacted
with the SIDE text mining interface for model build-
ing, including feature extraction and machine learn-
ing, to set up a model. Defining this in SIDE terms,
to train the system and create a model, the user first
has to define a filter. Filters are trained using ma-
chine learning technology. Two customization op-
tions are available to analysts in this process.
The first and possibly most important is the set of
customization options that affect the design of the
attribute space. The standard attribute space is set
up with one attribute per unique feature - the value
corresponds to the number of times that feature oc-
curs in a text. Options include unigrams, bigrams,
part-of-speech bigrams, stemming, and stopword re-
moval.
The next step is the selection of the machine
learning algorithm that will be used. Dozens of op-
tions are made available through the Weka toolkit
(Witten and Frank, 2005), although some are more
commonly used than others. The three options that
are most recommended to analysts beginning work
with machine learning are Na??ve Bayes (a prob-
abilistic model), SMO (Weka?s implementation of
Support Vector Machines), and J48, which is one
of many Weka implementations of a Decision Tree
learner. SMO is considered state-of-the-art for text
classification, so we expect that analysts will fre-
quently find that to be the best choice.
As this error analysis tool is built within SIDE, we
focus on applications to text mining. However, this
tool can also be used on non-text data sets, so long as
they are first preprocessed through SIDE. The details
of our error analysis approach are not specific to any
individual task or machine learning algorithm.
3 High Level View of Error Analysis
In an insightful usage of applied machine learning, a
practitioner will design an approach that takes into
account what is known about the structure of the
data that is being modeled. However, typically, that
knowledge is incomplete, and there is thus a good
chance that the decisions that are made along the
way are suboptimal. When the approach is evalu-
ated, it is possible to determine based on the pro-
portion and types of errors whether the performance
is acceptable for the application or not. If it is not,
then the practitioner should engage in an error analy-
sis process to determine what is malfunctioning and
what could be done to better model the structure in
the data.
In well-known machine learning toolkits such as
Weka, some information is available about what er-
rors are being made. Predictions can be printed out,
to allow a researcher to identify how a document is
being classified. One common format for summariz-
ing these predictions is a confusion matrix, usually
printed in a format like:
a b <-- classified as
67 19 | a = PT
42 70 | b = DR
This lists, for example, that 19 text segments were
classified as type DR but were actually type PT.
While this gives a rough view of what errors are
26
Figure 1: The error analysis interface with key function-
ality locations highlighted.
appearing, it gives no indication of why the errors
are being made. This is where a more extensive er-
ror analysis is necessary. Two common ways to ap-
proach this question are top down, which starts with
a learned model, and bottom up, which starts with
the confusion matrix from that model?s performance
estimate. In the first case, the model is examined
to find the attributes that are treated as most impor-
tant. These are the attributes that have the great-
est influence on the predictions made by the learned
model, and thus these attributes provide a good start-
ing point. In the second case, the bottom-up case,
one first examines the confusion matrix to identify
large off-diagonal cells, which represent common
confusions. The error analysis for any error cell is
then the process of determining relations between
three sets of text segments2 related to that cell.
Within the ?classified as DR but actually PT? cell,
for instance, error analysis would require finding
what makes these examples most different from ex-
amples correctly classified as PT, and what makes
these examples most similar to those correctly clas-
sified as DR. This can be done by identifying at-
tributes that mostly strongly differentiate the first
two sets, and attributes most similar between the first
and third sets. An ideal approach would combine
these two directions.
4 Error Analysis Process
Visitors to this demo will have the opportunity to ex-
periment with the error analysis interface. It will be
set up with multiple data sets and previously trained
text mining models. These models can first be exam-
ined from the model building window, which con-
tains information such as:
? Global feature collection, listing all features
that were included in the trained model.
? Cross-validation statistics, including variance
and kappa statistics, the confusion matrix and
other general information.
? Weights or other appropriate information for
the text mining model that was trained.
By moving to the error analysis interface, the user
can explore a model more deeply. The first step is
to select a model to examine. By default, all text
segments that were evaluated in cross-validation dis-
play in a scrolling list in the bottom right corner of
the window. Each row contains the text within a seg-
ment, and the associated feature vector. Users will
first be asked to examine this data to understand the
magnitude of the error analysis challenge.
Clicking on a cell in the confusion matrix (at the
top of the screen) will fill the scrolling list at the bot-
tom left corner of the screen with the classified seg-
ments that fall in that cell. A comparison chooser
dropdown menu gives three analysis options - full,
horizontal, and vertical. By default, full comparison
2Our interface assumes that the input text has been seg-
mented already; depending on the task involved, these segments
may correspond to a sentence, a paragraph, or even an entire
document.
27
is selected, and shows all text segments used in train-
ing. The two additional modes of comparison allow
some insight into what features are most representa-
tive of the subset of segments in that cell, compared
to the correct predictions aligned with that cell (ei-
ther vertically or horizontally within the confusion
matrix). By switching to horizontal comparison, the
scrolling list on the right changes to display only text
segments that fall in the cell which is along the con-
fusion matrix diagonal and horizontal to the selected
cell. Switching to vertical comparison changes this
list to display segments categorized in the cell which
is along the diagonal and vertically aligned with the
selected error cell.
Once a comparison method is selected, there is
a feature highlighting dropdown menu which is of
use. The contents in this menu are sorted by degree
of difference between the segments in the two lists
at the bottom of the screen. This means, for a hor-
izontal comparison, that features at the top of this
list are the most different between the two cells (this
difference is displayed in the menu). We compute
this difference by the difference in expected (aver-
age) value for that feature between the two sets. In a
vertical comparison, features are ranked by similar-
ity, instead of difference. Once a feature is selected
from this menu, two significant changes are made.
The first is that a second confusion matrix appears,
giving the confusion matrix values (mean and stan-
dard deviation) for the highlighted feature. The sec-
ond is that the two segment lists are sorted according
to the feature being highlighted.
User interface design elements were important in
this design process. One option available to users is
the ability to ?hide empty features,? which removes
features which did not occur at all in one or both of
the sets being studied. This allows the user to fo-
cus on features which are most likely to be causing
a significant change in a classifier?s performance. It
is also clear that the number of different subsets of
classified segments can become very confusing, es-
pecially when comparing various types of error in
one session. To combat this, the labels on the lists
and menus will change to reflect some of this infor-
mation. For instance, the left-hand panel gives the
predicted and actual labels of the segments you have
highlighted, while the right-hand panel is labelled
with the name of the category of correct prediction
you are comparing against. The feature highlighting
dropdown menu also changes to reflect similar in-
formation about the type of comparison being made.
5 Future Directions
This error analysis tool has been used in the text
mining unit for an Applied Machine Learning course
with approximately 30 students. In contrast to pre-
vious semesters where the tool was not available
to support error analysis, the instructor noticed that
many more students were able to begin surpassing
shallow observations, instead forming hypotheses
about where the weaknesses in a model are, and
what might be done to improve performance.
Based on our observations, however, the error
analysis support could still be improved by directing
users towards features that not only point to differ-
ences and similarities between different subsets of
instances, but also to more information about how
features are being used in the trained model. This
can be implemented either in algorithm-specific
ways (such as displaying the weight of features in
an SVM model) or in more generalizable formats,
for instance, through information gain. Investigating
how to score these general aspects, and presenting
this information in an intuitive way, are directions
for our continued development of this tool.
Acknowledgements
This research was supported by NSF Grant DRL-
0835426.
References
Moonyoung Kang, Sourish Chaudhuri, Mahesh Joshi,
and Carolyn Penstein-Rose? 2008. SIDE: The Summa-
rization Integrated Development Environment. Pro-
ceedings of the Association for Computational Lin-
guistics, Demo Abstracts.
Simone Teufel and Marc Moens 2002. Summarizing
Scientific Articles: Experiments with Relevance and
Rhetorical Status. Computational Linguistics, Vol. 28,
No. 1.
Ian Witten and Eibe Frank 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, second
edition. Elsevier: San Fransisco.
28
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1018?1026,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Recognizing Authority in Dialogue with an Integer Linear Programming
Constrained Model
Elijah Mayfield
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
elijah@cmu.edu
Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
cprose@cs.cmu.edu
Abstract
We present a novel computational formula-
tion of speaker authority in discourse. This
notion, which focuses on how speakers posi-
tion themselves relative to each other in dis-
course, is first developed into a reliable cod-
ing scheme (0.71 agreement between human
annotators). We also provide a computational
model for automatically annotating text using
this coding scheme, using supervised learning
enhanced by constraints implemented with In-
teger Linear Programming. We show that this
constrained model?s analyses of speaker au-
thority correlates very strongly with expert hu-
man judgments (r2 coefficient of 0.947).
1 Introduction
In this work, we seek to formalize the ways speak-
ers position themselves in discourse. We do this in
a way that maintains a notion of discourse structure,
and which can be aggregated to evaluate a speaker?s
overall stance in a dialogue. We define the body of
work in positioning to include any attempt to formal-
ize the processes by which speakers attempt to influ-
ence or give evidence of their relations to each other.
Constructs such as Initiative and Control (Whittaker
and Stenton, 1988), which attempt to operationalize
the authority over a discourse?s structure, fall under
the umbrella of positioning. As we construe posi-
tioning, it also includes work on detecting certainty
and confusion in speech (Liscombe et al, 2005),
which models a speaker?s understanding of the in-
formation in their statements. Work in dialogue act
tagging is also relevant, as it seeks to describe the ac-
tions and moves with which speakers display these
types of positioning (Stolcke et al, 2000).
To complement these bodies of work, we choose
to focus on the question of how speakers position
themselves as authoritative in a discourse. This
means that we must describe the way speakers intro-
duce new topics or discussions into the discourse;
the way they position themselves relative to that
topic; and how these functions interact with each
other. While all of the tasks mentioned above focus
on specific problems in the larger rhetorical question
of speaker positioning, none explicitly address this
framing of authority. Each does have valuable ties
to the work that we would like to do, and in section
2, we describe prior work in each of those areas, and
elaborate on how each relates to our questions.
We measure this as an authoritativeness ratio. Of
the contentful dialogue moves made by a speaker,
in what fraction of those moves is the speaker po-
sitioned as the primary authority on that topic? To
measure this quantitatively, we introduce the Nego-
tiation framework, a construct from the field of sys-
temic functional linguistics (SFL), which addresses
specifically the concepts that we are interested in.
We present a reproducible formulation of this so-
ciolinguistics research in section 3, along with our
preliminary findings on reliability between human
coders, where we observe inter-rater agreement of
0.71. Applying this coding scheme to data, we see
strong correlations with important motivational con-
structs such as Self-Efficacy (Bandura, 1997) as well
as learning gains.
Next, we address automatic coding of the Ne-
gotiation framework, which we treat as a two-
1018
dimensional classification task. One dimension is
a set of codes describing the authoritative status of
a contribution1. The other dimension is a segmen-
tation task. We impose constraints on both of these
models based on the structure observed in the work
of SFL. These constraints are formulated as boolean
statements describing what a correct label sequence
looks like, and are imposed on our model using an
Integer Linear Programming formulation (Roth and
Yih, 2004). In section 5, this model is evaluated
on a subset of the MapTask corpus (Anderson et
al., 1991) and shows a high correlation with human
judgements of authoritativeness (r2 = 0.947). After
a detailed error analysis, we will conclude the paper
in section 6 with a discussion of our future work.
2 Background
The Negotiation framework, as formulated by the
SFL community, places a special emphasis on how
speakers function in a discourse as sources or recip-
ients of information or action. We break down this
concept into a set of codes, one code per contribu-
tion. Before we break down the coding scheme more
concretely in section 3, it is important to understand
why we have chosen to introduce a new framework,
rather than reusing existing computational work.
Much work has examined the emergence of dis-
course structure from the choices speakers make at
the linguistic and intentional level (Grosz and Sid-
ner, 1986). For instance, when a speaker asks a
question, it is expected to be followed with an an-
swer. In discourse analysis, this notion is described
through dialogue games (Carlson, 1983), while con-
versation analysis frames the structure in terms of
adjacency pairs (Schegloff, 2007). These expec-
tations can be viewed under the umbrella of con-
ditional relevance (Levinson, 2000), and the ex-
changes can be labelled discourse segments.
In prior work, the way that people influence dis-
course structure is described through the two tightly-
related concepts of initiative and control. A speaker
who begins a discourse segment is said to have ini-
tiative, while control accounts for which speaker is
being addressed in a dialogue (Whittaker and Sten-
ton, 1988). As initiative passes back and forth be-
tween discourse participants, control over the con-
1We treat each line in our corpus as a single contribution.
versation similarly transfers from one speaker to an-
other (Walker and Whittaker, 1990). This relation is
often considered synchronous, though evidence sug-
gests that the reality is not straightforward (Jordan
and Di Eugenio, 1997).
Research in initiative and control has been ap-
plied in the form of mixed-initiative dialogue sys-
tems (Smith, 1992). This is a large and ac-
tive field, with applications in tutorial dialogues
(Core, 2003), human-robot interactions (Peltason
and Wrede, 2010), and more general approaches to
effective turn-taking (Selfridge and Heeman, 2010).
However, that body of work focuses on influenc-
ing discourse structure through positioning. The
question that we are asking instead focuses on how
speakers view their authority as a source of informa-
tion about the topic of the discourse.
In particular, consider questioning in discourse.
In mixed-initiative analysis of discourse, asking a
question always gives you control of a discourse.
There is an expectation that your question will be
followed by an answer. A speaker might already
know the answer to a question they asked - for
instance, when a teacher is verifying a student?s
knowledge. However, in most cases asking a ques-
tion represents a lack of authority, treating the other
speakers as a source for that knowledge. While there
have been preliminary attempts to separate out these
specific types of positioning in initiative, such as
Chu-Carroll and Brown (1998), it has not been stud-
ied extensively in a computational setting.
Another similar thread of research is to identify
a speaker?s certainty, that is, the confidence of a
speaker and how that self-evaluation affects their
language (Pon-Barry and Shieber, 2010). Substan-
tial work has gone into automatically identifying
levels of speaker certainty, for example in Liscombe
et al (2005) and Litman et al (2009). The major
difference between our work and this body of liter-
ature is that work on certainty has rarely focused on
how state translates into interaction between speak-
ers (with some exceptions, such as the application
of certainty to tutoring dialogues (Forbes-Riley and
Litman, 2009)). Instead, the focus is on the person?s
self-evaluation, independent of the influence on the
speaker?s positioning within a discourse.
Dialogue act tagging seeks to describe the moves
people make to express themselves in a discourse.
1019
This task involves defining the role of each contri-
bution based on its function (Stolcke et al, 2000).
We know that there are interesting correlations be-
tween these acts and other factors, such as learning
gains (Litman and Forbes-Riley, 2006) and the rel-
evance of a contribution for summarization (Wrede
and Shriberg, 2003). However, adapting dialogue
act tags to the question of how speakers position
themselves is not straightforward. In particular,
the granularity of these tagsets, which is already a
highly debated topic (Popescu-Belis, 2008), is not
ideal for the task we have set for ourselves. Many
dialogue acts can be used in authoritative or non-
authoritative ways, based on context, and can posi-
tion a speaker as either giver or receiver of informa-
tion. Thus these more general tagsets are not specific
enough to the role of authority in discourse.
Each of these fields of prior work is highly valu-
able. However, none were designed to specifically
describe how people present themselves as a source
or recipient of knowledge in a discourse. Thus, we
have chosen to draw on a different field of soci-
olinguistics. Our formalization of that theory is de-
scribed in the next section.
3 The Negotiation Framework
We now present the Negotiation framework2, which
we use to answer the questions left unanswered in
the previous section. Within the field of SFL, this
framework has been continually refined over the last
three decades (Berry, 1981; Martin, 1992; Martin,
2003). It attempts to describe how speakers use their
role as a source of knowledge or action to position
themselves relative to others in a discourse.
Applications of the framework include distin-
guishing between focus on teacher knowledge and
student reasoning (Veel, 1999) and distribution of
authority in juvenile trials (Martin et al, 2008). The
framework can also be applied to problems similar
to those studied through the lens of initiative, such
as the distinction between authority over discourse
structure and authority over content (Martin, 2000).
A challenge of applying this work to language
technologies is that it has historically been highly
2All examples are drawn from the MapTask corpus and in-
volve an instruction giver (g) and follower (f). Within examples,
discourse segment boundaries are shown by horizontal lines.
qualitative, with little emphasis placed on repro-
ducibility. We have formulated a pared-down, repro-
ducible version of the framework, presented in Sec-
tion 3.1. Evidence of the usefulness of that formu-
lation for identifying authority, and of correlations
that we can study based on these codes, is presented
briefly in Section 3.2.
3.1 Our Formulation of Negotiation
The codes that we can apply to a contribution us-
ing the Negotiation framework are comprised of four
main codes, K1, K2, A1, and A2, and two additional
codes, ch and o. This is a reduction over the many
task-specific or highly contextual codes used in the
original work. This was done to ensure that a ma-
chine learning classification task would not be over-
whelmed with many infrequent classes.
The main codes are divided by two questions.
First, is the contribution related to exchanging infor-
mation, or to exchanging services and actions? If the
former, then it is a K move (knowledge); if the latter,
then an A move (action). Second, is the contribution
acting as a primary actor, or secondary? In the case
of knowledge, this often correlates to the difference
between assertions (K1) and queries (K2). For in-
stance, a statement of fact or opinion is a K1:
g K1 well i?ve got a great viewpoint
here just below the east lake
By contrast, asking for someone else?s knowledge
or opinion is a K2:
g K2 what have you got underneath the
east lake
f K1 rocket launch
In the case of action, the codes usually corre-
spond to narrating action (A1) and giving instruc-
tions (A2), as below:
g A2 go almost to the edge of the lake
f A1 yeah
A challenge move (ch) is one which directly con-
tradicts the content or assertion of the previous line,
or makes that previous contribution irrelevant. For
instance, consider the exchange below, where an in-
struction is rejected because its presuppositions are
broken by the challenging statement.
g A2 then head diagonally down to-
wards the bottom of the dead tree
f ch i have don?t have a dead tree i
have a dutch elm
1020
All moves that do not fit into one of these cate-
gories are classified as other (o). This includes back-
channel moves, floor-grabbing moves, false starts,
and any other non-contentful contributions.
This theory makes use of discourse segmenta-
tion. Research in the SFL community has focused
on intra-segment structure, and empirical evidence
from this research has shown that exchanges be-
tween speakers follow a very specific pattern:
o* X2? o* X1+ o*
That is to say, each segment contains a primary
move (a K1 or an A1) and an optional preceding
secondary move, with other non-contentful moves
interspersed throughout. A single statement of fact
would be a K1 move comprising an entire segment,
while a single question/answer pair would be a K2
move followed by a K1. Longer exchanges of many
lines obviously also occur.
We iteratively developed a coding manual which
describes, in a reproducible way, how to apply the
codes listed above. The six codes we use, along with
their frequency in our corpus, are given in Table 1.
In the next section, we evaluate the reliability and
utility of hand-coded data, before moving on to au-
tomation in section 4.
3.2 Preliminary Evaluation
This coding scheme was evaluated for reliability on
two corpora using Cohen?s kappa (Cohen, 1960).
Within the social sciences community, a kappa
above 0.7 is considered acceptable. Two conversa-
tions were each coded by hand by two trained anno-
tators. The first conversation was between three stu-
dents in a collaborative learning task; inter-rater re-
liability kappa for Negotiation labels was 0.78. The
second conversation was from the MapTask corpus,
and kappa was 0.71. Further data was labelled by
hand by one trained annotator.
In our work, we label conversations using the cod-
ing scheme above. To determine how well these
codes correlate with other interesting factors, we
choose to assign a quantitative measure of authori-
tativeness to each speaker. This measure can then
be compared to other features of a speaker. To do
this, we use the coded labels to assign an Authori-
tativeness Ratio to each speaker. First, we define a
Code Meaning Count Percent
K1 Primary Knower 984 22.5
K2 Secondary Knower 613 14.0
A1 Primary Actor 471 10.8
A2 Secondary Actor 708 16.2
ch Challenge 129 2.9
o Other 1469 33.6
Total 4374 100.0
Table 1: The six codes in our coding scheme, along with
their frequency in our corpus of twenty conversations.
functionA(S, c, L) for a speaker, a contribution, and
a set of labels L ? {K1,K2, A1, A2, o, ch} as:
A(S, c, L) =
{
1 c spoken by S with label l ? L
0 otherwise.
We then define the Authoritativeness ratio
Auth(S) for a speaker S in a dialogue consisting
of contributions c1...cn as:
Auth(S) =
n?
i=1
A(S, ci, {K1, A2})
n?
i=1
A(S, ci, {K1,K2, A1, A2})
The intuition behind this ratio is that we are only
interested in the four main label types in our analy-
sis - at least for an initial description of authority, we
do not consider the non-contentful o moves. Within
these four main labels, there are clearly two that ap-
pear ?dominant? - statements of fact or opinion, and
commands or instructions - and two that appear less
dominant - questions or requests for information,
and narration of an action. We sum these together
to reach a single numeric value for each speaker?s
projection of authority in the dialogue.
The full details of our external validations of this
approach are available in Howley et al (2011). To
summarize, we considered two data sets involving
student collaborative learning. The first data set con-
sisted of pairs of students interacting over two days,
and was annotated for aggressive behavior, to assess
warning factors in social interactions. Our analysis
1021
showed that aggressive behavior correlated with au-
thoritativeness ratio (p < .05), and that less aggres-
sive students became less authoritative in the second
day (p < .05, effect size .15?). The second data
set was analyzed for Self-Efficacy - the confidence
of each student in their own ability (Bandura, 1997)
- as well as actual learning gains based on pre- and
post-test scores. We found that the Authoritativeness
ratio was a significant predictor of learning gains
(r2 = .41, p < .04). Furthermore, in a multiple re-
gression, we determined that the Authoritativeness
ratio of both students in a group predict the average
Self-Efficacy of the pair (r2 = .12, p < .01).
4 Computational Model
We know that our coding scheme is useful for mak-
ing predictions about speakers. We now judge
whether it can be reproduced fully automatically.
Our model must select, for each contribution ci in a
dialogue, the most likely classification label li from
{K1,K2, A1, A2, o, ch}. We also build in paral-
lel a segmentation model to select si from the set
{new, same}. Our baseline approach to both prob-
lems is to use a bag-of-words model of the contribu-
tion, and use machine learning for classification.
Certain types of interactions, explored in section
4.1, are difficult or impossible to classify without
context. We build a contextual feature space, de-
scribed in section 4.2, to enhance our baseline bag-
of-words model. We can also describe patterns that
appear in discourse segments, as detailed in section
3.1. In our coding manual, these instructions are
given as rules for how segments should be coded by
humans. Our hypothesis is that by enforcing these
rules in the output of our automatic classifier, per-
formance will increase. In section 4.3 we formalize
these constraints using Integer Linear Programming.
4.1 Challenging cases
We want to distinguish between phenomena such as
in the following two examples.
f K2 so I?m like on the bank on the
bank of the east lake
g K1 yeah
In this case, a one-token contribution is indis-
putably a K1 move, answering a yes/no question.
However, in the dialogue below, it is equally inar-
guable that the same move is an A1:
g A2 go almost to the edge of the lake
f A1 yeah
Without this context, these moves would be indis-
tinguishable to a model. With it, they are both easily
classified correctly.
We also observed that markers for segmentation
of a segment vary between contentful initiations and
non-contentful ones. For instance, filler noises can
often initiate segments:
g o hmm...
g K2 do you have a farmer?s gate?
f K1 no
Situations such as this are common. This is also a
challenge for segmentation, as demonstrated below:
g K1 oh oh it?s on the right-hand side
of my great viewpoint
f o okay yeah
g o right eh
g A2 go almost to the edge of the lake
f A1 yeah
A long statement or instruction from one speaker
is followed up with a terse response (in the same
segment) from the listener. However, after that back-
channel move, a short floor-grabbing move is often
made to start the next segment. This is a distinc-
tion that a bag-of-words model would have difficulty
with. This is markedly different from contentful seg-
ment initiations:
g A2 come directly down below the
stone circle and we come up
f ch I don?t have a stone circle
g o you don?t have a stone circle
All three of these lines look like statements, which
often initiate new segments. However, only the first
should be marked as starting a new segment. The
other two are topically related, in the second line by
contradicting the instruction, and in the third by re-
peating the previous person?s statement.
4.2 Contextual Feature Space Additions
To incorporate the insights above into our model, we
append features to our bag-of-words model. First,
in our classification model we include both lexical
bigrams and part-of-speech bigrams to encode fur-
ther lexical knowledge and some notion of syntac-
tic structure. To account for restatements and topic
shifts, we add a feature based on cosine similarity
(using term vectors weighted by TF-IDF calculated
1022
over training data). We then add a feature for the
predicted label of the previous contribution - after
each contribution is classified, the next contribution
adds a feature for the automatic label. This requires
our model to function as an on-line classifier.
We build two segmentation models, one trained
on contributions of less than four tokens, and an-
other trained on contributions of four or more to-
kens, to distinguish between characteristics of con-
tentful and non-contentful contributions. To the
short-contribution model, we add two additional fea-
tures. The first represents the ratio between the
length of the current contribution and the length of
the previous contribution. The second represents
whether a change in speaker has occurred between
the current and previous contribution.
4.3 Constraints using Integer Linear
Programming
We formulate our constraints using Integer Linear
Programming (ILP). This formulation has an ad-
vantage over other sequence labelling formulations,
such as Viterbi decoding, in its ability to enforce
structure through constraints. We then enhance this
classifier by adding constraints, which allow expert
knowledge of discourse structure to be enforced in
classification. We can use these constraints to elim-
inate label options which would violate the rules for
a segment outlined in our coding manual.
Each classification decision is made at the contri-
bution level, jointly optimizing the Negotiation la-
bel and segmentation label for a single contribution,
then treating those labels as given for the next con-
tribution classification.
To define our objective function for optimization,
for each possible label, we train a one vs. all SVM,
and use the resulting regression for each label as
a score, giving us six values ~li for our Negotiation
label and two values ~si for our segmentation label.
Then, subject to the constraints below, we optimize:
arg max
l?~li,s?~si
l + s
Thus, at each contribution, if the highest-scoring
Negotiation label breaks a constraint, the model can
optimize whether to drop to the next-most-likely la-
bel, or start a new segment.
Recall from section 3.1 that our discourse seg-
ments follow strict rules related to ordering and rep-
etition of contributions. Below, we list the con-
straints that we used in our model to enforce that
pattern, along with a brief explanation of the intu-
ition behind each.
?ci ? s, (li = K2)?
?j < i, cj ? t? (lj 6= K1)
(1)
?ci ? s, (li = A2)?
?j < i, cj ? t? (lj 6= A1)
(2)
The first constraints enforce the rule that a pri-
mary move cannot occur before a secondary move
in the same segment. For instance, a question must
initiate a new segment if it follows a statement.
?ci ? s, (li ? {A1, A2})?
?j < i, cj ? s? (lj /? {K1,K2})
(3)
?ci ? s, (li ? {K1,K2})?
?j < i, cj ? s? (lj /? {A1, A2})
(4)
These constraints specify that A moves and K
moves cannot cooccur in a segment. An instruc-
tion for action and a question requesting information
must be considered separate segments.
?ci ? s, (li = A1)? ((li?1 = A1) ?
?j < i, cj ? s? (lj 6= A1))
(5)
?ci ? s, (li = K1)? ((li?1 = K1) ?
?j < i, cj ? s? (lj 6= K1))
(6)
This pair states that two primary moves cannot oc-
cur in the same segment unless they are contiguous,
in rapid succession.
?ci ? s, (li = A1)?
?j < i, cj ? s, (lj = A2)? (Si 6= Sj)
(7)
?ci ? s, (li = K1)?
?j < i, cj ? s, (lj = K2)? (Si 6= Sj)
(8)
The last set of constraints enforce the intuitive
notion that a speaker cannot follow their own sec-
ondary move with a primary move in that segment
(such as answering their own question).
1023
Computationally, an advantage of these con-
straints is that they do not extend past the current
segment in history. This means that they usually
are only enforced over the past few moves, and do
not enforce any global constraint over the structure
of the whole dialogue. This allows the constraints
to be flexible to various conversational styles, and
tractable for fast computation independent of the
length of the dialogue.
5 Evaluation
We test our models on a twenty conversation sub-
set of the MapTask corpus detailed in Table 1. We
compare the use of four models in our results.
? Baseline: This model uses a bag-of-words fea-
ture space as input to an SVM classifier. No
segmentation model is used and no ILP con-
straints are enforced.
? Baseline+ILP: This model uses the baseline
feature space as input to both classification and
segmentation models. ILP constraints are en-
forced between these models.
? Contextual: This model uses our enhanced
feature space from section 4.2, with no segmen-
tation model and no ILP constraints enforced.
? Contextual+ILP: This model uses the en-
hanced feature spaces for both Negotiation la-
bels and segment boundaries from section 4.2
to enforce ILP constraints.
For segmentation, we evaluate our models using
exact-match accuracy. We use multiple evaluation
metrics to judge classification. The first and most
basic is accuracy - the percentage of accurately cho-
sen Negotiation labels. Secondly, we use Cohen?s
Kappa (Cohen, 1960) to judge improvement in ac-
curacy over chance. The final evaluation is the r2
coefficient computed between predicted and actual
Authoritativeness ratios per speaker. This represents
how much variance in authoritativeness is accounted
for in the predicted ratios. This final metric is the
most important for measuring reproducibility of hu-
man analyses of speaker authority in conversation.
We use SIDE for feature extraction (Mayfield
and Rose?, 2010), SVM-Light for machine learning
Model Accuracy Kappa r2
Baseline 59.7% 0.465 0.354
Baseline+ILP 61.6% 0.488 0.663
Segmentation 72.3%
Contextual 66.7% 0.565 0.908
Contextual+ILP 68.4% 0.584 0.947
Segmentation 74.9%
Table 2: Performance evaluation for our models. Each
line is significantly improved in both accuracy and r2 er-
ror from the previous line (p < .01).
(Joachims, 1999), and Learning-Based Java for ILP
inference (Rizzolo and Roth, 2010). Performance
is evaluated by 20-fold cross-validation, where each
fold is trained on 19 conversations and tested on the
remaining one. Statistical significance was calcu-
lated using a student?s paired t-test. For accuracy
and kappa, n = 20 (one data point per conversation)
and for r2, n = 40 (one data point per speaker).
5.1 Results
All classification results are given in Table 2 and
charts showing correlation between predicted and
actual speaker Authoritativeness ratios are shown in
Figure 1. We observe that the baseline bag-of-words
model performs well above random chance (kappa
of 0.465); however, its accuracy is still very low
and its ability to predict Authoritativeness ratio of
a speaker is not particularly high (r2 of 0.354 with
ratios from manually labelled data). We observe a
significant improvement when ILP constraints are
applied to this model.
The contextual model described in section 4.2
performs better than our baseline constrained model.
However, the gains found in the contextual model
are somewhat orthogonal to the gains from using
ILP constraints, as applying those constraints to
the contextual model results in further performance
gains (and a high r2 coefficient of 0.947).
Our segmentation model was evaluated based on
exact matches in boundaries. Switching from base-
line to contextual features, we observe an improve-
ment in accuracy of 2.6%.
5.2 Error Analysis
An error analysis of model predictions explains the
large effect on correlation despite relatively smaller
1024
Figure 1: Plots of predicted (x axis) and actual (y axis) Authoritativeness ratios for speakers across 20 conversations,
for the Baseline (left), Baseline+Constraints (center), and Contextual+Constraints (right) models.
changes in accuracy. Our Authoritativeness ratio
does not take into account moves labelled o or
ch. What we find is that the most advanced model
still makes many mistakes at determining whether a
move should be labelled as o or a core move. This er-
ror rate is, however, fairly consistent across the four
core move codes. When a move is determined (cor-
rectly) to not be an o move, the system is highly ac-
curate in distinguishing between the four core labels.
The one systematic confusion that continues to
appear most frequently in our results is the inabil-
ity to distinguish between a segment containing an
A2 move followed by an A1 move, and a segment
containing a K1 move followed by an o move. The
surface structure of these types of exchanges is very
similar. Consider the following two exchanges:
g A2 if you come down almost to the
bottom of the map that I?ve got
f A1 uh-huh
f K1 but the meadow?s below my bro-
ken gate
g o right yes
These two exchanges on a surface level are highly
similar. Out of context, making this distinction is
very hard even for human coders, so it is not surpris-
ing then that this pattern is the most difficult one to
recognize in this corpus. It contributes most of the
remaining confusion between the four core codes.
6 Conclusions
In this work we have presented one formulation of
authority in dialogue. This formulation allows us
to describe positioning in discourse in a way that
is complementary to prior work in mixed-initiative
dialogue systems and analysis of speaker certainty.
Our model includes a simple understanding of dis-
course structure while also encoding information
about the types of moves used, and the certainty of a
speaker as a source of information. This formulation
is reproducible by human coders, with an inter-rater
reliability of 0.71.
We have then presented a computational model
for automatically applying these codes per contribu-
tion. In our best model, we see a good 68.4% accu-
racy on a six-way individual contribution labelling
task. More importantly, this model replicates human
analyses of authoritativeness very well, with an r2
coefficient of 0.947.
There is room for improvement in our model in
future work. Further use of contextual features will
more thoroughly represent the information we want
our model to take into account. Our segmentation
accuracy is also fairly low, and further examination
of segmentation accuracy using a more sophisticated
evaluation metric, such as WindowDiff (Pevzner and
Hearst, 2002), would be helpful.
In general, however, we now have an automated
model that is reliable in reproducing human judg-
ments of authoritativeness. We are now interested in
how we can apply this to the larger questions of po-
sitioning we began this paper by asking, especially
in describing speaker positioning at various instants
throughout a single discourse. This will be the main
thrust of our future work.
Acknowledgements
This research was supported by NSF grants SBE-
0836012 and HCC-0803482.
1025
References
Anne Anderson, Miles Bader, Ellen Bard, Elizabeth
Boyle, Gwyneth Doherty, Simon Garrod, et al 1991.
The HCRC Map Task Corpus. In Language and
Speech.
Albert Bandura. 1997. Self-efficacy: The Exercise of
Control
Margaret Berry. 1981. Towards Layers of Exchange
Structure for Directive Exchanges. In Network 2.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis.
Jennifer Chu-Carroll and Michael Brown. 1998. An Ev-
idential Model for Tracking Initiative in Collaborative
Dialogue Interactions. In User Modeling and User-
Adapted Interaction.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. In Educational and Psychological
Measurement.
Mark Core and Johanna Moore and Claus Zinn. 2003.
The Role of Initiative in Tutorial Dialogue. In Pro-
ceedings of EACL.
Kate Forbes-Riley and Diane Litman. 2009. Adapting to
Student Uncertainty Improves Tutoring Dialogues. In
Proceedings of Artificial Intelligence in Education.
Barbara Grosz and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. In Compu-
tational Linguistics.
Iris Howley and Elijah Mayfield and Carolyn Penstein
Rose?. 2011. Missing Something? Authority in Col-
laborative Learning. In Proceedings of Computer-
Supported Collaborative Learning.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. In Advances in Kernel Methods
- Support Vector Learning.
Pamela Jordan and Barbara Di Eugenio. 1997. Control
and Initiative in Collaborative Problem Solving Dia-
logues. In Proceedings of AAAI Spring Symposium
on Computational Models for Mixed Initiative Inter-
actions.
Stephen Levinson. 2000. Pragmatics.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting Certainness in Spoken Tutorial
Dialogues. In Proceedings of Interspeech.
Diane Litman and Kate Forbes-Riley. 2006. Correlations
betweeen Dialogue Acts and Learning in Spoken Tu-
toring Dialogue. In Natural Language Engineering.
Diane Litman, Mihai Rotaru, and Greg Nicholas. 2009.
Classifying Turn-Level Uncertainty Using Word-Level
Prosody. In Proceedings of Interspeech.
James Martin. 1992. English Text: System and Structure.
James Martin. 2000. Factoring out Exchange: Types of
Structure. In Working with Dialogue.
James Martin and David Rose. 2003. Working with Dis-
course: Meaning Beyond the Clause.
James Martin, Michele Zappavigna, and Paul Dwyer.
2008. Negotiating Shame: Exchange and Genre Struc-
ture in Youth Justice Conferencing. In Proceedings of
European Systemic Functional Linguistics.
Elijah Mayfield and Carolyn Penstein Rose?. 2010. An
Interactive Tool for Supporting Error Analysis for Text
Mining. In Proceedings of Demo Session at NAACL.
Julia Peltason and Britta Wrede. 2010. Modeling
Human-Robot Interaction Based on Generic Interac-
tion Patterns. In AAAI Report on Dialog with Robots.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. In Computational Linguistics.
Heather Pon-Barry and Stuart Shieber. 2010. Assessing
Self-awareness and Transparency when Classifying a
Speakers Level of Certainty. In Speech Prosody.
Andrei Popescu-Belis. 2008. Dimensionality of Dia-
logue Act Tagsets: An Empirical Analysis of Large
Corpora. In Language Resources and Evaluation.
Nick Rizzolo and Dan Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In Language
Resources and Evaluation.
Dan Roth and Wen-Tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural Lan-
guage Tasks. In Proceedings of CoNLL.
Emanuel Schegloff. 2007. Sequence Organization in In-
teraction: A Primer in Conversation Analysis.
Ethan Selfridge and Peter Heeman. 2010. Importance-
Driven Turn-Bidding for Spoken Dialogue Systems.
In Proceedings of ACL.
Ronnie Smith. 1992. A computational model of
expectation-driven mixed-initiative dialog processing.
Ph.D. Dissertation.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, et al 2000.
Dialogue Act Modeling for Automatic Tagging and
Recognition of Conversational Speech. In Computa-
tional Linguistics.
Robert Veel. 1999. Language, Knowledge, and Author-
ity in School Mathematics. In Pedagogy and the Shap-
ing of Consciousness: Linguistics and Social Pro-
cesses
Marilyn Walker and Steve Whittaker. 1990. Mixed Ini-
tiative in Dialogue: An Investigation into Discourse
Structure. In Proceedings of ACL.
Steve Whittaker and Phil Stenton. 1988. Cues and Con-
trol in Expert-Client Dialogues. In Proceedings of
ACL.
Britta Wrede and Elizabeth Shriberg. 2003. The Re-
lationship between Dialogue Acts and Hot Spots in
Meetings. In IEEE Workshop on Automatic Speech
Recognition and Understanding.
1026
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 115?120,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
SciSumm: A Multi-Document Summarization System for Scientific Articles
Nitin Agarwal
Language Technologies Institute
Carnegie Mellon University
nitina@cs.cmu.edu
Ravi Shankar Reddy
Language Technologies Resource Center
IIIT-Hyderabad, India
krs reddy@students.iiit.ac.in
Kiran Gvr
Language Technologies Resource Center
IIIT-Hyderabad, India
kiran gvr@students.iiit.ac.in
Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
cprose@cs.cmu.edu
Abstract
In this demo, we present SciSumm, an inter-
active multi-document summarization system
for scientific articles. The document collec-
tion to be summarized is a list of papers cited
together within the same source article, oth-
erwise known as a co-citation. At the heart
of the approach is a topic based clustering of
fragments extracted from each article based on
queries generated from the context surround-
ing the co-cited list of papers. This analy-
sis enables the generation of an overview of
common themes from the co-cited papers that
relate to the context in which the co-citation
was found. SciSumm is currently built over
the 2008 ACL Anthology, however the gen-
eralizable nature of the summarization tech-
niques and the extensible architecture makes it
possible to use the system with other corpora
where a citation network is available. Evalu-
ation results on the same corpus demonstrate
that our system performs better than an exist-
ing widely used multi-document summariza-
tion system (MEAD).
1 Introduction
We present an interactive multi-document summa-
rization system called SciSumm that summarizes
document collections that are composed of lists of
papers cited together within the same source arti-
cle, otherwise known as a co-citation. The inter-
active nature of the summarization approach makes
this demo session ideal for its presentation.
When users interact with SciSumm, they request
summaries in context as they read, and that context
determines the focus of the summary generated for
a set of related scientific articles. This behaviour is
different from some other non-interactive summa-
rization systems that might appear as a black box
and might not tailor the result to the specific infor-
mation needs of the users in context. SciSumm cap-
tures a user?s contextual needs when a user clicks on
a co-citation. Using the context of the co-citation in
the source article, we generate a query that allows
us to create a summary in a query-oriented fash-
ion. The extracted portions of the co-cited articles
are then assembled into clusters that represent the
main themes of the articles that relate to the context
in which they were cited. Our evaluation demon-
strates that SciSumm achieves higher quality sum-
maries than a state-of-the-art multidocument sum-
marization system (Radev, 2004).
The rest of the paper is organized as follows. We
first describe the design goals for SciSumm in 2 to
motivate the need for the system and its usefulness.
The end-to-end summarization pipeline has been de-
scribed in Section 3. Section 4 presents an evalua-
tion of summaries generated from the system. We
present an overview of relevant literature in Section
5. We end the paper with conclusions and some in-
teresting further research directions in Section 6.
2 Design Goals
Consider that as a researcher reads a scientific arti-
cle, she/he encounters numerous citations, most of
them citing the foundational and seminal work that
is important in that scientific domain. The text sur-
rounding these citations is a valuable resource as
it allows the author to make a statement about her
115
viewpoint towards the cited articles. However, to re-
searchers who are new to the field, or sometimes just
as a side-effect of not being completely up-to-date
with related work in a domain, these citations may
pose a challenge to readers. A system that could
generate a small summary of the collection of cited
articles that is constructed specifically to relate to
the claims made by the author citing them would be
incredibly useful. It would also help the researcher
determine if the cited work is relevant for her own
research.
As an example of such a co-citation consider the
following citation sentence:
Various machine learning approaches have been
proposed for chunking (Ramshaw and Marcus,
1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et
al. , 2000; Tjong Kim Sang, 2000b; Sassano and
Utsuro, 2000; van Halteren, 2000).
Now imagine the reader trying to determine about
widely used machine learning approaches for noun
phrase chunking. He would probably be required
to go through these cited papers to understand what
is similar and different in the variety of chunking
approaches. Instead of going through these individ-
ual papers, it would be quicker if the user could get
the summary of the topics in all those papers that
talk about the usage of machine learning methods
in chunking. SciSumm aims to automatically dis-
cover these points of comparison between the co-
cited papers by taking into consideration the con-
textual needs of a user. When the user clicks on a
co-citation in context, the system uses the text sur-
rounding that co-citation as evidence of the informa-
tion need.
3 System Overview
A high level overview of our system?s architecture
is presented in Figure 1. The system provides a web
based interface for viewing and summarizing re-
search articles in the ACL Anthology corpus, 2008.
The summarization proceeds in three main stages as
follows:
? A user may retrieve a collection of articles
of interest by entering a query. SciSumm re-
sponds by returning a list of relevant articles,
including the title and a snippet based sum-
mary. For this SciSumm uses standard retrieval
from a Lucene index.
? A user can use the title, snippet summary and
author information to find an article of inter-
est. The actual article is rendered in HTML af-
ter the user clicks on one of the search results.
The co-citations in the article are highlighted in
bold and italics to mark them as points of inter-
est for the user.
? If a user clicks on one, SciSumm responds by
generating a query from the local context of the
co-citation. That query is then used to select
relevant portions of the co-cited articles, which
are then used to generate the summary.
An example of a summary for a particular topic is
displayed in Figure 2. This figure shows one of
the clusters generated for the citation sentence ?Var-
ious machine learning approaches have been pro-
posed for chunking (Ramshaw and Marcus, 1995;
Tjong Kim Sang, 2000a; Tjong Kim Sang et al ,
2000; Tjong Kim Sang, 2000b; Sassano and Utsuro,
2000; van Halteren, 2000)?. The cluster has a la-
bel Chunk, Tag, Word and contains fragments from
two of the papers discussing this topic. A ranked
list of such clusters is generated, which allows for
swift navigation between topics of interest for a user
(Figure 3). This summary is tremendously useful as
it informs the user of the different perspectives of
co-cited authors towards a shared problem (in this
case ?Chunking?). More specifically, it informs the
user as to how different or similar approaches are
that were used for this research problem (which is
?Chunking?).
3.1 System Description
SciSumm has four primary modules that are central
to the functionality of the system, as displayed in
Figure 1. First, the Text Tiling module takes care
of obtaining tiles of text relevant to the citation con-
text. Next, the clustering module is used to generate
labelled clusters using the text tiles extracted from
the co-cited papers. The clusters are ordered accord-
ing to relevance with respect to the generated query.
This is accomplished by the Ranking Module.
In the following sections, we discuss each of the
main modules in detail.
116
Figure 1: SciSumm summarization pipeline
3.2 Texttiling
The Text Tiling module uses the TextTiling algo-
rithm (Hearst, 1997) for segmenting the text of each
article. We have used text tiles as the basic unit
for our summary since individual sentences are too
short to stand on their own. This happens as a side-
effect of the length of scientific articles. Sentences
picked from different parts of several articles assem-
bled together would make an incoherent summary.
Once computed, text tiles are used to expand on the
content viewed within the context associated with a
co-citation. The intuition is that an embedded co-
citation in a text tile is connected with the topic dis-
tribution of its context. Thus, we can use a computa-
tion of similarity between tiles and the context of the
co-citation to rank clusters generated using Frequent
Term based text clustering.
3.3 Frequent Term Based Clustering
The clustering module employs Frequent Term
Based Clustering (Beil et al, 2002). For each co-
citation, we use this clustering technique to cluster
all the of the extracted text tiles generated by seg-
menting each of the co-cited papers. We settled on
this clustering approach for the following reasons:
? Text tile contents coming from different papers
constitute a sparse vector space, and thus the
centroid based approaches would not work very
well for integrating content across papers.
? Frequent Term based clustering is extremely
fast in execution time as well as and relatively
efficient in terms of space requirements.
? A frequent term set is generated for each clus-
ter, which gives a comprehensible description
that can be used to label the cluster.
Frequent Term Based text clustering uses a group
of frequently co-occurring terms called a frequent
term set. We use a measure of entropy to rank these
frequent term sets. Frequent term sets provide a
clean clustering that is determined by specifying the
number of overlapping documents containing more
than one frequent term set. The algorithm uses the
first k term sets if all the documents in the document
collection are clustered. To discover all the possi-
ble candidates for clustering, i.e., term sets, we used
the Apriori algorithm (Agrawal et al, 1994), which
identifies the sets of terms that are both relatively
frequent and highly correlated with one another.
3.4 Cluster Ranking
The ranking module uses cosine similarity between
the query and the centroid of each cluster to rank all
the clusters generated by the clustering module. The
context of a co-citation is restricted to the text of the
segment in which the co-citation is found. In this
way we attempt to leverage the expert knowledge of
the author as it is encoded in the local context of the
co-citation.
4 Evaluation
We have taken great care in the design of the evalu-
ation for the SciSumm summarization system. In a
117
Figure 2: Example of a summary generated by our system. We can see that the clusters are cross cutting across
different papers, thus giving the user a multi-document summary.
typical evaluation of a multi-document summariza-
tion system, gold standard summaries are created by
hand and then compared against fixed length gen-
erated summaries. It was necessary to prepare our
own evaluation corpus, consisting of gold standard
summaries created for a randomly selected set of co-
citations because such an evaluation corpus does not
exist for this task.
4.1 Experimental Setup
An important target user population for multi-
document summarization of scientific articles is
graduate students. Hence to get a measure of how
well the summarization system is performing, we
asked 2 graduate students who have been working
in the computational linguistics community to create
gold standard summaries of a fixed length (8 sen-
tences ? 200 words) for 10 randomly selected co-
citations. We obtained two different gold standard
summaries for each co-citation (i.e., 20 gold stan-
dard summaries total). Our evaluation is designed
to measure the quality of the content selection. In
future work, we will evaluate the usability of the
SciSumm system using a task based evaluation.
In the absence of any other multi-document sum-
marization system in the domain of scientific ar-
ticle summarization, we used a widely used and
freely available multi-document summarization sys-
tem called MEAD (Radev, 2004) as our baseline.
MEAD uses centroid based summarization to cre-
ate informative clusters of topics. We use the de-
fault configuration of MEAD in which MEAD uses
length, position and centroid for ranking each sen-
tence. We did not use query focussed summarization
with MEAD. We evaluate its performance with the
same gold standard summaries we use to evaluate
SciSumm. For generating a summary from our sys-
tem we used sentences from the tiles that are clus-
tered in the top ranked cluster. Once all of the ex-
tracts included in that entire cluster are exhausted,
we move on to the next highly ranked cluster. In this
way we prepare a summary comprising of 8 highly
relevant sentences.
4.2 Results
For measuring performance of the two summariza-
tion systems (SciSumm and MEAD), we compute
the ROUGE metric based on the 2 * 10 gold standard
summaries that were manually created. ROUGE has
been traditionally used to compute the performance
based on the N-gram overlap (ROUGE-N) between
the summaries generated by the system and the tar-
get gold standard summaries. For our evaluation
we used two different versions of the ROUGE met-
ric, namely ROUGE-1 and ROUGE-2, which corre-
spond to measures of the unigram and bigram over-
lap respectively. We computed four metrics in order
to get a complete picture of how SciSumm performs
in relation to the baseline, namely ROUGE-1 F-
measure, ROUGE-1 Recall, ROUGE-2 F-measure,
and ROUGE-2 Recall.
From the results presented in Figure 4 and 5, we
can see that our system performs well on average in
comparison to the baseline. Especially important is
118
Figure 3: Clusters generated in response to a user click on the co-citation. The list of clusters in the left pane gives a
bird-eye view of the topics which are present in the co-cited papers
Table 1: Average ROUGE results. * represents improve-
ment significant at p < .05, ? at p < .01.
Metric MEAD SciSumm
ROUGE-1 F-measure 0.3680 0.5123 ?
ROUGE-1 Recall 0.4168 0.5018
ROUGE-1 Precision 0.3424 0.5349 ?
ROUGE-2 F-measure 0.1598 0.3303 *
ROUGE-2 Recall 0.1786 0.3227 *
ROUGE-2 Precision 0.1481 0.3450 ?
the performance of the system on recall measures,
which shows the most dramatic advantage over the
baseline. To measure the statistical significance of
this result, we carried out a Student T-Test, the re-
sults of which are presented in the results section
in Table 1. It is apparent from the p-values gener-
ated by T-Test that our system performs significantly
better than MEAD on three of the metrics on which
both the systems were evaluated using (p < 0.05)
as the criterion for statistical significance. This sup-
ports the view that summaries perceived as higher in
value are generated using a query focused technique,
where the query is generated automatically from the
context of the co-citation.
5 Previous Work
Surprisingly, not many approaches to the problem of
summarization of scientific articles have been pro-
posed in the past. Qazvinian et al (2008) present
a summarization approach that can be seen as the
converse of what we are working to achieve. Rather
than summarizing multiple papers cited in the same
source article, they summarize different viewpoints
expressed towards the same paper from different pa-
pers that cite it. Nanba et al (1999) argue in their
work that a co-citation frequently implies a consis-
tent viewpoint towards the cited articles. Another
approach that uses bibliographic coupling has been
used for gathering different viewpoints from which
to summarize a document (Kaplan et al, 2008). In
our work we make use of this insight by generating
a query to focus our multi-document summary from
the text closest to the citation.
6 Conclusion And Future Work
In this demo, we present SciSumm, which is an in-
teractive multi-document summarization system for
scientific articles. Our evaluation shows that the
SciSumm approach to content selection outperforms
another widely used multi-document summarization
system for this summarization task.
Our long term goal is to expand the capabilities
of SciSumm to generate literature surveys of larger
document collections from less focused queries.
This more challenging task would require more con-
trol over filtering and ranking in order to avoid gen-
erating summaries that lack focus. To this end, a
future improvement that we plan to use is a vari-
ant on MMR (Maximum Marginal Relevance) (Car-
bonell et al, 1998), which can be used to optimize
the diversity of selected text tiles as well as the rel-
evance based ordering of clusters, i.e., so that more
diverse sets of extracts from the co-cited articles will
be placed at the ready fingertips of users.
Another important direction is to refine the inter-
action design through task-based user studies. As
we collect more feedback from students and re-
searchers through this process, we will used the in-
sights gained to achieve a more robust and effective
implementation.
119
Figure 4: ROUGE-1 Recall Figure 5: ROUGE-2 Recall
7 Acknowledgements
This research was supported in part by NSF grant
EEC-064848 and ONR grant N00014-10-1-0277.
References
Agrawal R. and Srikant R. 1994. Fast Algorithm for
Mining Association Rules In Proceedings of the 20th
VLDB Conference Santiago, Chile, 1994
Baxendale, P. 1958. Machine-made index for technical
literature - an experiment. IBM Journal of Research
and Development
Beil F., Ester M. and Xu X 2002. Frequent-Term based
Text Clustering In Proceedings of SIGKDD ?02 Ed-
monton, Alberta, Canada
Carbonell J. and Goldstein J. 1998. The Use of MMR,
Diversity-Based Reranking for Reordering Documents
and Producing Summaries In Research and Develop-
ment in Information Retrieval, pages 335?336
Councill I. G. , Giles C. L. and Kan M. 2008. ParsCit:
An open-source CRF reference string parsing pack-
age INTERNATIONAL LANGUAGE RESOURCES
AND EVALUATION European Language Resources
Association
Edmundson, H.P. 1969. New methods in automatic ex-
tracting. Journal of ACM.
Hearst M.A. 1997 TextTiling: Segmenting text into
multi-paragraph subtopic passages In proceedings of
LREC 2004, Lisbon, Portugal, May 2004
Joseph M. T. and Radev D. R. 2007. Citation analysis,
centrality, and the ACL Anthology
Kupiec J. , Pedersen J. , Chen F. 1995. A training doc-
ument summarizer. In Proceedings SIGIR ?95, pages
68-73, New York, NY, USA. 28(1):114?133.
Luhn, H. P. 1958. IBM Journal of Research Develop-
ment.
Mani I. , Bloedorn E. 1997. Multi-Document Summa-
rization by graph search and matching In AAAI/IAAI,
pages 622-628. [15, 16].
Nanba H. , Okumura M. 1999. Towards Multi-paper
Summarization Using Reference Information In Pro-
ceedings of IJCAI-99, pages 926?931 .
Paice CD. 1990. Constructing Literature Abstracts by
Computer: Techniques and Prospects Information
Processing and Management Vol. 26, No.1, pp, 171-
186, 1990
Qazvinian V. , Radev D.R 2008. Scientific Paper
summarization using Citation Summary Networks In
Proceedings of the 22nd International Conference on
Computational Linguistics, pages 689?696 Manch-
ester, August 2008
Radev D. R . , Jing H. and Budzikowska M. 2000.
Centroid-based summarization of multiple documents:
sentence extraction, utility based evaluation, and user
studies In NAACL-ANLP 2000 Workshop on Auto-
matic summarization, pages 21-30, Morristown, NJ,
USA. [12, 16, 17].
Radev, Dragomir. 2004. MEAD - a platform for mul-
tidocument multilingual text summarization. In pro-
ceedings of LREC 2004, Lisbon, Portugal, May 2004.
Teufel S. , Moens M. 2002. Summarizing Scientific
Articles - Experiments with Relevance and Rhetorical
Status In Journal of Computational Linguistics, MIT
Press.
Hal Daume III , Marcu D. 2006. Bayesian query-
focussed summarization. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics, ACL.
Eisenstein J , Barzilay R. 2008. Bayesian unsupervised
topic segmentation In EMNLP-SIGDAT.
Barzilay R , Lee L. 2004. Catching the drift: Probabilis-
tic content models, with applications to generation and
summarization In Proceedings of 3rd Asian Semantic
Web Conference (ASWC 2008), pp.182-188,.
Kaplan D , Tokunaga T. 2008. Sighting citation sights:
A collective-intelligence approach for automatic sum-
marization of research papers using C-sites In HLT-
NAACL.
120
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 131?139,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Sentiment Classification using Automatically Extracted Subgraph Features
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose? and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
{shilpaa, emayfiel, cprose, ehn}@cs.cmu.edu
Abstract
In this work, we propose a novel representa-
tion of text based on patterns derived from lin-
guistic annotation graphs. We use a subgraph
mining algorithm to automatically derive fea-
tures as frequent subgraphs from the annota-
tion graph. This process generates a very large
number of features, many of which are highly
correlated. We propose a genetic program-
ming based approach to feature construction
which creates a fixed number of strong classi-
fication predictors from these subgraphs. We
evaluate the benefit gained from evolved struc-
tured features, when used in addition to the
bag-of-words features, for a sentiment classi-
fication task.
1 Introduction
In recent years, the topic of sentiment analysis has
been one of the more popular directions in the field
of language technologies. Recent work in super-
vised sentiment analysis has focused on innovative
approaches to feature creation, with the greatest im-
provements in performance with features that in-
sightfully capture the essence of the linguistic con-
structions used to express sentiment, e.g. (Wilson et
al., 2004), (Joshi and Rose?, 2009)
In this spirit, we present a novel approach that
leverages subgraphs automatically extracted from
linguistic annotation graphs using efficient subgraph
mining algorithms (Yan and Han, 2002). The diffi-
culty with automatically deriving complex features
comes with the increased feature space size. Many
of these features are highly correlated and do not
provide any new information to the model. For ex-
ample, a feature of type unigram POS (e.g. ?cam-
era NN?) doesn?t provide any additional informa-
tion beyond the unigram feature (e.g. ?camera?),
for words that are often used with the same part of
speech. However, alongside several redundant fea-
tures, there are also features that provide new infor-
mation. It is these features that we aim to capture.
In this work, we propose an evolutionary ap-
proach that constructs complex features from sub-
graphs extracted from an annotation graph. A con-
stant number of these features are added to the un-
igram feature space, adding much of the represen-
tational benefits without the computational cost of a
drastic increase in feature space size.
In the remainder of the paper, we review prior
work on features commonly used for sentiment anal-
ysis. We then describe the annotation graph rep-
resentation proposed by Arora and Nyberg (2009).
Following this, we describe the frequent subgraph
mining algorithm proposed in Yan and Han (2002),
and used in this work to extract frequent subgraphs
from the annotation graphs. We then introduce our
novel feature evolution approach, and discuss our
experimental setup and results. Subgraph features
combined with the feature evolution approach gives
promising results, with an improvement in perfor-
mance over the baseline.
2 Related Work
Some of the recent work in sentiment analysis has
shown that structured features (features that capture
syntactic patterns in text), such as n-grams, depen-
dency relations, etc., improve performance beyond
131
the bag of words approach. Arora et al (2009) show
that deep syntactic scope features constructed from
transitive closure of dependency relations give sig-
nificant improvement for identifying types of claims
in product reviews. Gamon (2004) found that using
deep linguistic features derived from phrase struc-
ture trees and part of speech annotations yields sig-
nificant improvements on the task of predicting sat-
isfaction ratings in customer feedback data. Wilson
et al (2004) use syntactic clues derived from depen-
dency parse tree as features for predicting the inten-
sity of opinion phrases1.
Structured features that capture linguistic patterns
are often hand crafted by domain experts (Wilson
et al, 2005) after careful examination of the data.
Thus, they do not always generalize well across
datasets and domains. This also requires a signif-
icant amount of time and resources. By automati-
cally deriving structured features, we might be able
to learn new annotations faster.
Matsumoto et al (2005) propose an approach that
uses frequent sub-sequence and sub-tree mining ap-
proaches (Asai et al, 2002; Pei et al, 2004) to derive
structured features such as word sub-sequences and
dependency sub-trees. They show that these features
outperform bag-of-words features for a sentiment
classification task and achieve the best performance
to date on a commonly-used movie review dataset.
Their approach presents an automatic procedure for
deriving features that capture long distance depen-
dencies without much expert intervention.
However, their approach is limited to sequences
or tree annotations. Often, features that combine
several annotations capture interesting characteris-
tics of text. For example, Wilson et al (2004), Ga-
mon (2004) and Joshi and Rose? (2009) show that
a combination of dependency relations and part of
speech annotations boosts performance. The anno-
tation graph representation proposed by Arora and
Nyberg (2009) is a formalism for representing sev-
eral linguistic annotations together on text. With an
annotation graph representation, instances are rep-
resented as graphs from which frequent subgraph
patterns may be extracted and used as features for
learning new annotations.
1Although, in this work we are classifying sentences and not
phrases, similar clues may be used for sentiment classification
in sentences as well
In this work, we use an efficient frequent sub-
graph mining algorithm (gSpan) (Yan and Han,
2002) to extract frequent subgraphs from a linguis-
tic annotation graph (Arora and Nyberg, 2009). An
annotation graph is a general representation for ar-
bitrary linguistic annotations. The annotation graph
and subgraph mining algorithm provide us a quick
way to test several alternative linguistic representa-
tions of text. In the next section, we present a formal
definition of the annotation graph and a motivating
example for subgraph features.
3 Annotation Graph Representation and
Feature Subgraphs
Arora and Nyberg (2009) define the annotation
graph as a quadruple: G = (N,E,?, ?), where
N is the set of nodes, E is the set of edges, s.t.
E ? N ? N , and ? = ?N ? ?E is the set of la-
bels for nodes and edges. ? : N ? E ? ? is the
labeling function for nodes and edges. Examples of
node labels (?N ) are tokens (unigrams) and annota-
tions such as part of speech, polarity etc. Examples
of edge labels (?E) are leftOf, dependency type etc.
The leftOf relation is defined between two adjacent
nodes. The dependency type relation is defined be-
tween a head word and its modifier.
Annotations may be represented in an annotation
graph in several ways. For example, a dependency
triple annotation ?good amod movie?, may be repre-
sented as a d amod relation between the head word
?movie? and its modifier ?good?, or as a node d amod
with edges ParentOfGov and ParentOfDep to the
head and the modifier words. An example of an an-
notation graph is shown in Figure 1.
The instance in Figure 1 describes a movie review
comment, ?interesting, but not compelling.?. The
words ?interesting? and ?compelling? both have pos-
itive prior polarity, however, the phrase expresses
negative sentiment towards the movie. Heuristics for
special handling of negation have been proposed in
the literature. For example, Pang et al (2002) ap-
pend every word following a negation, until a punc-
tuation, with a ?NOT? . Applying a similar technique
to our example gives us two sentiment bearing fea-
tures, one positive (?interesting?) and one negative
(?NOT-compelling?), and the model may not be as
sure about the predicted label, since there is both
132
positive and negative sentiment present.
In Figure 2, we show three discriminating sub-
graph features derived from the annotation graph in
Figure 1. These subgraph features capture the nega-
tive sentiment in our example phrase. The first fea-
ture in 2(a) captures the pattern using dependency
relations between words. A different review com-
ment may use the same linguistic construction but
with a different pair of words, for example ?a pretty
good, but not excellent story.? This is the same lin-
guistic pattern but with different words the model
may not have seen before, and hence may not clas-
sify this instance correctly. This suggests that the
feature in 2(a) may be too specific.
In order to mine general features that capture the
rhetorical structure of language, we may add prior
polarity annotations to the annotation graph, us-
ing a lexicon such as Wilson et al (2005). Fig-
ure 2(b) shows the subgraph in 2(a) with polar-
ity annotations. If we want to generalize the pat-
tern in 2(a) to any positive words, we may use the
feature subgraph in Figure 2(c) with X wild cards
on words that are polar or negating. This feature
subgraph captures the negative sentiment in both
phrases ?interesting, but not compelling.? and ?a
pretty good, but not excellent story.?. Similar gener-
alization using wild cards on words may be applied
with other annotations such as part of speech anno-
tations as well. By choosing where to put the wild
card, we can get features similar to, but more pow-
erful than, the dependency back-off features in Joshi
and Rose? (2009).
 
U_interesting U_, U_but U_not U_compelling U_. 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
posQ 
P_VBN 
posQ 
P_, 
posQ 
P_CC 
posQ 
P_RB 
posQ 
P_JJ 
posQ 
P_. 
Figure 1: Annotation graph for sentence ?interesting, but not
compelling.? . Prefixes: ?U? for unigrams (tokens), ?L? for po-
larity, ?D? for dependency relation and ?P? for part of speech.
Edges with no label encode the ?leftOf? relation between words.
4 Subgraph Mining Algorithms
In the previous section, we demonstrated that sub-
graphs from an annotation graph can be used to iden-
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
(a)
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(b)
 
X X X 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(c)
Figure 2: Subgraph features from the annotation graph in
Figure 1
tify the rhetorical structure used to express senti-
ment. The subgraph patterns that represent general
linguistic structure will be more frequent than sur-
face level patterns. Hence, we use a frequent sub-
graph mining algorithm to find frequent subgraph
patterns, from which we construct features to use in
the supervised learning algorithm.
The goal in frequent subgraph mining is to find
frequent subgraphs in a collection of graphs. A
graph G? is a subgraph of another graph G if there
exists a subgraph isomorphism2 from G? to G, de-
noted by G? ? G.
Earlier approaches in frequent subgraph mining
(Inokuchi et al, 2000; Kuramochi and Karypis,
2001) used a two-step approach of first generating
the candidate subgraphs and then testing their fre-
quency in the graph database. The second step in-
volves a subgraph isomorphism test, which is NP-
complete. Although efficient isomorphism testing
algorithms have been developed making it practical
to use, with lots of candidate subgraphs to test, it can
2http://en.wikipedia.org/wiki/Subgraph_
isomorphism_problem
133
still be very expensive for real applications.
gSpan (Yan and Han, 2002) uses an alternative
pattern growth based approach to frequent subgraph
mining, which extends graphs from a single sub-
graph directly, without candidate generation. For
each discovered subgraph G, new edges are added
recursively until all frequent supergraphs of G have
been discovered. gSpan uses a depth first search tree
(DFS) and restricts edge extension to only vertices
on the rightmost path. However, there can be multi-
ple DFS trees for a graph. gSpan introduces a set of
rules to select one of them as representative. Each
graph is represented by its unique canonical DFS
code, and the codes for two graphs are equivalent if
the graphs are isomorphic. This reduces the compu-
tational cost of the subgraph mining algorithm sub-
stantially, making gSpan orders of magnitude faster
than other subgraph mining algorithms. With sev-
eral implementations available 3, gSpan has been
commonly used for mining frequent subgraph pat-
terns (Kudo et al, 2004; Deshpande et al, 2005). In
this work, we use gSpan to mine frequent subgraphs
from the annotation graph.
5 Feature Construction using Genetic
Programming
A challenge to overcome when adding expressive-
ness to the feature space for any text classification
problem is the rapid increase in the feature space
size. Among this large set of new features, most
are not predictive or are very weak predictors, and
only a few carry novel information that improves
classification performance. Because of this, adding
more complex features often gives no improvement
or even worsens performance as the feature space?s
signal is drowned out by noise.
Riloff et al (2006) propose a feature subsump-
tion approach to address this issue. They define a
hierarchy for features based on the information they
represent. A complex feature is only added if its
discriminative power is a delta above the discrimi-
native power of all its simpler forms. In this work,
we use a Genetic Programming (Koza, 1992) based
approach which evaluates interactions between fea-
3http://www.cs.ucsb.edu/?xyan/software/
gSpan.htm, http://www.kyb.mpg.de/bs/people/
nowozin/gboost/
tures and evolves complex features from them. The
advantage of the genetic programing based approach
over feature subsumption is that it allows us to eval-
uate a feature using multiple criteria. We show that
this approach performs better than feature subsump-
tion.
A lot of work has considered this genetic pro-
gramming problem (Smith and Bull, 2005). The
most similar approaches to ours are taken by Kraw-
iec (2002) and Otero et al (2002), both of which use
genetic programming to build tree feature represen-
tations. None of this work was applied to a language
processing task, though there has been some sim-
ilar work to ours in that community, most notably
(Hirsch et al, 2007), which built search queries for
topic classification of documents. Our prior work
(Mayfield and Rose?, 2010) introduced a new feature
construction method and was effective when using
unigram features; here we extend our approach to
feature spaces which are even larger and thus more
problematic.
The Genetic Programming (GP) paradigm is most
advantageous when applied to problems where there
is not a correct answer to a problem, but instead
there is a gradient of partial solutions which incre-
mentally improve in quality. Potential solutions are
represented as trees consisting of functions (non-leaf
nodes in the tree, which perform an action given
their child nodes as input) and terminals (leaf nodes
in the tree, often variables or constants in an equa-
tion). The tree (an individual) can then be inter-
preted as a program to be executed, and the output
of that program can be measured for fitness (a mea-
surement of the program?s quality). High-fitness in-
dividuals are selected for reproduction into a new
generation of candidate individuals through a breed-
ing process, where parts of each parent are combined
to form a new individual.
We apply this design to a language processing
task at the stage of feature construction - given many
weakly predictive features, we would like to com-
bine them in a way which produces a better feature.
For our functions we use boolean statements AND
and XOR, while our terminals are selected randomly
from the set of all unigrams and our new, extracted
subgraph features. Each leaf?s value, when applied
to a single sentence, is equal to 1 if that subgraph is
present in the sentence, and 0 if the subgraph is not
134
present.
The tree in Figure 3 is a simplified example of our
evolved features. It combines three features, a uni-
gram feature ?too? (centre node) and two subgraph
features: 1) the subgraph in the leftmost node oc-
curs in collocations containing ?more than? (e.g.,
?nothing more than? or ?little more than?), 2) the
subgraph in the rightmost node occurs in negative
phrases such as ?opportunism at its most glaring?
(JJS is a superlative adjective and PRP$ is a pos-
sessive pronoun). A single feature combining these
weak indicators can be more predictive than any part
alone.
!"#$
!"#$
%&'(($
%&)(*+$
,&-*+-&'./0$
%&1'2$
3"4&3#35$
3"4&664$
,&-(22$
Figure 3: A tree constructed using subgraph features and GP
(Simplified for illustrative purposes)
In the rest of this section, we first describe the
feature construction process using genetic program-
ming. We then discuss how fitness of an individual
is measured for our classification task.
5.1 Feature Construction Process
We divide our data into two sets, training and test.
We again divide our training data in half, and train
our GP features on only one half of this data4 This is
to avoid overfitting the final SVM model to the GP
features. In a single GP run, we produce one feature
to match each class value. For a sentiment classifica-
tion task, a feature is evolved to be predictive of the
positive instances, and another feature is evolved to
be predictive of the negative documents. We repeat
this procedure a total of 15 times (using different
seeds for random selection of features), producing
a total of 30 new features to be added to the feature
space.
4For genetic programming we used the ECJ toolkit
(http://cs.gmu.edu/?eclab/projects/ecj/).
5.2 Defining Fitness
Our definition of fitness is based on the concepts
of precision and recall, borrowed from informa-
tion retrieval. We define our set of documents
as being comprised of a set of positive documents
P0, P1, P2, ...Pu and a set of negative documents
N0, N1, N2, ...Nv. For a given individual I and doc-
ument D, we define hit(I,D) to equal 1 if the state-
ment I is true of that document and 0 otherwise. Pre-
cision and recall of an individual feature for predict-
ing positive documents5 is then defined as follows:
Prec(I) =
u
?
i=0
hit(I, Pi)
u
?
i=0
hit(I, Pi) +
v
?
i=0
hit(I,Ni)
(1)
Rec(I) =
u
?
i=0
hit(I, Pi)
u
(2)
We then weight these values to give significantly
more importance to precision, using the F? measure,
which gives the harmonic mean between precision
and recall:
F?(I) =
(1 + ?2)? (Prec(I)?Rec(I))
(?2 ? Prec(I)) +Rec(I) (3)
In addition to this fitness function, we add two
penalties to the equation. The first penalty applies to
prevent trees from becoming overly complex. One
option to ensure that features remain moderately
simple is to simply have a maximum depth beyond
which trees cannot grow. Following the work of
Otero et al (2002), we penalize trees based on the
number of nodes they contain. This discourages
bloat, i.e. sections of trees which do not contribute to
overall accuracy. This penalty, known as parsimony
pressure, is labeled PP in our fitness function.
The second penalty is based on the correlation be-
tween the feature being constructed, and the sub-
graphs and unigrams which appear as nodes within
that individual. Without this penalty, a feature may
5Negative precision and recall are defined identically, with
obvious adjustments to test for negative documents instead of
positive.
135
often be redundant, taking much more complexity
to represent the same information that is captured
with a simple unigram. We measure correlation us-
ing Pearson?s product moment, defined for two vec-
tors X , Y as:
?x,y =
E[(X ? ?X)(Y ? ?Y )]
?X?Y
(4)
This results in a value from 1 (for perfect align-
ment) to -1 (for inverse alignment). We assign a
penalty for any correlation past a cutoff. This func-
tion is labeled CC (correlation constraint) in our fit-
ness function.
Our fitness function therefore is:
Fitness = F 1
8
+ PP + CC (5)
6 Experiments and Results
We evaluate our approach on a sentiment classifi-
cation task, where the goal is to classify a movie
review sentence as expressing positive or negative
sentiment towards the movie.
6.1 Data and Experimental Setup
Data: The dataset consists of snippets from Rot-
ten Tomatoes (Pang and Lee, 2005) 6. It consists
of 10662 snippets/sentences total with equal num-
ber positive and negative sentences (5331 each).
This dataset was created and used by Pang and Lee
(2005) to train a classifier for identifying positive
sentences in a full length review. We use the first
8000 (4000 positive, 4000 negative) sentences as
training data and evaluate on remaining 2662 (1331
positive, 1331 negative) sentences. We added part
of speech and dependency triple annotations to this
data using the Stanford parser (Klein and Manning,
2003).
Annotation Graph: For the annotation graph rep-
resentation, we used Unigrams (U), Part of Speech
(P) and Dependency Relation Type (D) as labels for
the nodes, and ParentOfGov and ParentOfDep as la-
bels for the edges. For a dependency triple such as
?amod good movie?, five nodes are added to the an-
notation graph as shown in Figure 4(a). ParentOf-
Gov and ParentOfDep edges are added from the
6http://www.cs.cornell.edu/people/pabo/
movie-review-data/rt-polaritydata.tar.gz
D_amod
U_good
P_JJ
P_NN
U_movie
ParentofGov
ParentofGovParentofDep
ParentofDep
(a)
D_amod
U_good
P_NN
ParentofGov
ParentofDep
(b)
D_amod
X
P_JJ
P_NN
X
posQ
ParentofGov
ParentofDep
posQ
(c)
Figure 4: Annotation graph and a feature subgraph for
dependency triple annotation ?amod good camera?. (c)
shows an alternative representation with wild cards
dependency relation node D amod to the unigram
nodes U good and U movie. These edges are also
added for the part of speech nodes that correspond
to the two unigrams in the dependency relation, as
shown in Figure 4(a). This allows the algorithm to
find general patterns, based on a dependency rela-
tion between two part of speech nodes, two unigram
nodes or a combination of the two. For example,
a subgraph in Figure 4(b) captures a general pat-
tern where good modifies a noun. This feature ex-
ists in ?amod good movie?, ?amod good camera?
and other similar dependency triples. This feature is
similar to the the dependency back-off features pro-
posed in Joshi and Rose? (2009).
The extra edges are an alternative to putting wild
cards on words, as proposed in section 3. On the
other hand, putting a wild card on every word in
the annotation graph for our example (Figure 4(c)),
will only give features based on dependency rela-
tions between part of speech annotations. Thus, the
wild card based approach is more restrictive than
136
adding more edges. However, with lots of edges, the
complexity of the subgraph mining algorithm and
the number of subgraph features increases tremen-
dously.
Classifier: For our experiments we use Support
Vector Machines (SVM) with a linear kernel. We
use the SVM-light7 implementation of SVM with
default settings.
Parameters: The gSpan algorithm requires setting
the minimum support threshold (minsup) for the
subgraph patterns to extract. Support for a subgraph
is the number of graphs in the dataset that contain
the subgraph. We experimented with several values
for minimum support and minsup = 2 gave us the
best performance.
For Genetic Programming, we used the same pa-
rameter settings as described in Mayfield and Rose?
(2010), which were tuned on a different dataset8
than one used in this work, but it is from the same
movie review domain. We also consider one alter-
ation to these settings. As we are introducing many
new and highly correlated features to our feature
space through subgraphs, we believe that a stricter
constraint must be placed on correlation between
features. To accomplish this, we can set our correla-
tion penalty cutoff to 0.3, lower than the 0.5 cutoff
used in prior work. Results for both settings are re-
ported.
Baselines: To the best of our knowledge, there is
no supervised machine learning result published on
this dataset. We compare our results with the fol-
lowing baselines:
? Unigram-only Baseline: In sentiment analysis,
unigram-only features have been a strong base-
line (Pang et al, 2002; Pang and Lee, 2004).
We only use unigrams that occur in at least
two sentences of the training data same as Mat-
sumoto et al (2005). We also filter out stop
words using a small stop word list9.
? ?2 Baseline: For our training data, after filter-
ing infrequent unigrams and stop words, we get
7http://svmlight.joachims.org/
8Full movie review data by Pang et al (2002)
9http://nlp.stanford.edu/
IR-book/html/htmledition/
dropping-common-terms-stop-words-1.html
(with one modification: removed ?will?, added ?this?)
8424 features. Adding subgraph features in-
creases the total number of features to 44, 161,
a factor of 5 increase in size. Feature selec-
tion can be used to reduce this size by select-
ing the most discriminative features. ?2 feature
selection (Manning et al, 2008) is commonly
used in the literature. We compare two methods
of feature selection with ?2, one which rejects
features if their ?2 score is not significant at the
0.05 level, and one that reduces the number of
features to match the size of our feature space
with GP.
? Feature Subsumption (FS): Following the idea
in Riloff et al (2006), a complex feature
C is discarded if IG(S) ? IG(C) ? ?,
where IG is Information Gain and S is
a simple feature that representationally sub-
sumes C, i.e. the text spans that match S
are a superset of the text spans that match
C. In our work, complex features are sub-
graph features and simple features are uni-
gram features contained in them. For example,
(D amod) Edge ParentOfDep (U bad) is
a complex feature for which U bad is a sim-
ple feature. We tried same values for ? ?
{0.002, 0.001, 0.0005}, as suggested in Riloff
et al (2006). Since all values gave us same
number of features, we only report a single re-
sult for feature subsumption.
? Correlation (Corr): As mentioned earlier,
some of the subgraph features are highly corre-
lated with unigram features and do not provide
new knowledge. A correlation based filter for
subgraph features can be used to discard a com-
plex feature C if its absolute correlation with its
simpler feature (unigram feature) is more than
a certain threshold. We use the same threshold
as used in the GP criterion, but as a hard filter
instead of a penalty.
6.2 Results and Discussion
In Table 1, we present our results. As can be
seen, subgraph features when added to the unigrams,
without any feature selection, decrease the perfor-
mance. ?2 feature selection with fixed feature space
size provides a very small gain over unigrams. All
other feature selection approaches perform worse
137
Settings #Features Acc. ?
Uni 8424 75.66 -
Uni + Sub 44161 75.28 -0.38
Uni + Sub, ?2 sig. 3407 74.68 -0.98
Uni + Sub, ?2 size 8454 75.77 +0.11
Uni + Sub, (FS) 18234 75.47 -0.19
Uni + Sub, (Corr) 18980 75.24 -0.42
Uni + GP (U) ? 8454 76.18 +0.52
Uni + GP (U+S) ? 8454 76.48 +0.82
Uni + GP (U+S) ? 8454 76.93 +1.27
Table 1: Experimental results for feature spaces with un-
igrams, with and without subgraph features. Feature se-
lection with 1) fixed significance level (?2 sig.), 2) fixed
feature space size (?2 size), 3) Feature Subsumption (FS)
and 4) Correlation based feature filtering (Corr)). GP fea-
tures for unigrams only {GP(U)}, or both unigrams and
subgraph features {GP(U+S)}. Both the settings from
Mayfield and Rose? (2010) (?) and more stringent correla-
tion constraint (?) are reported. #Features is the num-
ber of features in the training data. Acc is the accuracy
and ? is the difference from unigram only baseline. Best
performing feature configuration is highlighted in bold.
than the unigram-only approach. With GP, we ob-
serve a marginally significant gain (p < 0.1) in per-
formance over unigrams, calculated using one-way
ANOVA. Benefit from GP is more when subgraph
features are used in addition to the unigram features,
for constructing more complex pattern features. Ad-
ditionally, our performance is improved when we
constrain the correlation more severely than in previ-
ously published research, supporting our hypothesis
that this is a helpful way to respond to the problem
of redundancy in subgraph features.
A problem that we see with ?2 feature selection is
that several top ranked features may be highly cor-
related. For example, the top 5 features based on ?2
score are shown in Table 2; it is immediately obvi-
ous that the features are highly redundant.
With GP based feature construction, we can con-
sider this relationship between features, and con-
struct new features as a combination of selected un-
igram and subgraph features. With the correlation
criterion in the evolution process, we are able to
build combined features that provide new informa-
tion compared to unigrams.
The results we present are for the best perform-
(D advmod) Edge ParentOfDep (U too)
U too
U bad
U movie
(D amod) Edge ParentOfDep (U bad)
Table 2: Top features based on ?2 score
ing parameter configuration that we tested, after a
series of experiments. We realize that this places us
in danger of overfitting to the particulars of this data
set, however, the data set is large enough to partially
mitigate this concern.
7 Conclusion and Future Work
We have shown that there is additional information
to be gained from text beyond words, and demon-
strated two methods for increasing this information -
a subgraph mining approach that finds common syn-
tactic patterns that capture sentiment-bearing rhetor-
ical structure in text, and a feature construction
technique that uses genetic programming to com-
bine these more complex features without the redun-
dancy, increasing the size of the feature space only
by a fixed amount. The increase in performance that
we see is small but consistent.
In the future, we would like to extend this work to
other datasets and other problems within the field of
sentiment analysis. With the availability of several
off-the-shelf linguistic annotators, we may add more
linguistic annotations to the annotation graph and
richer subgraph features may be discovered. There
is also additional refinement that can be performed
on our genetic programming fitness function, which
is expected to improve the quality of our features.
Acknowledgments
This work was funded in part by the DARPA Ma-
chine Reading program under contract FA8750-09-
C-0172, and in part by NSF grant DRL-0835426.
We would like to thank Dr. Xifeng Yan and Marisa
Thoma for the gSpan code.
References
Shilpa Arora, Mahesh Joshi and Carolyn P. Rose?. 2009.
Identifying Types of Claims in Online Customer Re-
138
views. Proceedings of the HLT/NAACL.
Shilpa Arora and Eric Nyberg. 2009. Interactive Anno-
tation Learning with Indirect Feature Voting. Proceed-
ings of the HLT/NAACL (Student Research Work-
shop).
Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hiroshi
Sakamoto and Setsuo Arikawa. 2002. Efficient sub-
structure discovery from large semi-structured data.
Proceedings of SIAM Int. Conf. on Data Mining
(SDM).
Mukund Deshpande , Michihiro Kuramochi , Nikil Wale
and George Karypis. 2005. Frequent Substructure-
Based Approaches for Classifying Chemical Com-
pounds. IEEE Transactions on Knowledge and Data
Engineering.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vec-
tors, and the role of linguistic analysis, Proceedings
of COLING.
Laurence Hirsch, Robin Hirsch and Masoud Saeedi.
2007. Evolving Lucene Search Queries for Text Clas-
sification. Proceedings of the Genetic and Evolution-
ary Computation Conference.
Mahesh Joshi and Carolyn P. Rose?. 2009. Generalizing
Dependency Features for Opinion Mining. Proceed-
ings of the ACL-IJCNLP Conference (Short Papers).
Akihiro Inokuchi, Takashi Washio and Hiroshi Motoda.
2000. An Apriori-based Algorithm for Mining Fre-
quent Substructures from Graph Data. Proceedings
of PKDD.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the main con-
ference of the ACL.
John Koza. 1992. Genetic Programming: On the Pro-
gramming of Computers by Means of Natural Selec-
tion. MIT Press.
Krzysztof Krawiec. 2002. Genetic programming-based
construction of features for machine learning and
knowledge discovery tasks. Genetic Programming and
Evolvable Machines.
Taku Kudo, Eisaku Maeda and Yuji Matsumoto. 2004.
An Application of Boosting to Graph Classification.
Proceedings of NIPS.
Michihiro Kuramochi and George Karypis. 2002. Fre-
quent Subgraph Discovery. Proceedings of ICDM.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Proceedings of PAKDD.
Shotaro Matsumoto, Hiroya Takamura and Manabu Oku-
mura. 2005. Sentiment Classification Using Word
Sub-sequences and Dependency Sub-trees. Proceed-
ings of PAKDD.
Elijah Mayfield and Carolyn Penstein-Rose?. 2010.
Using Feature Construction to Avoid Large Feature
Spaces in Text Classification. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Fernando Otero, Monique Silva, Alex Freitas and Julio
Nievola. 2002. Genetic Programming for Attribute
Construction in Data Mining. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classication using Ma-
chine Learning Techniques. Proceedings of EMNLP.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. Proceedings of the
main conference of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. Proceedings of the main con-
ference of ACL.
Jian Pei, Jiawei Han, Behzad Mortazavi-asl, Jianyong
Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal
and Mei-chun Hsu. 2004. Mining Sequential Pat-
terns by Pattern-Growth: The PrefixSpan Approach.
Proceedings of IEEE Transactions on Knowledge and
Data Engineering.
Ellen Riloff, Siddharth Patwardhan and Janyce Wiebe.
2006. Feature Subsumption for Opinion Analysis.
Proceedings of the EMNLP.
Matthew Smith and Larry Bull. 2005. Genetic Program-
ming with a Genetic Algorithm for Feature Construc-
tion and Selection. Genetic Programming and Evolv-
able Machines.
Theresa Wilson, Janyce Wiebe and Rebecca Hwa. 2004.
Just How Mad Are You? Finding Strong and Weak
Opinion Clauses. Proceedings of AAAI.
Theresa Wilson, Janyce Wiebe and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity
in Phrase-Level Sentiment Analysis. Proceedings of
HLT/EMNLP.
Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-
Based Substructure Pattern Mining. UIUC Techni-
cal Report, UIUCDCS-R-2002-2296 (shorter version
in ICDM?02).
139
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 8?15,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Towards Multi-Document Summarization of Scientific Articles:Making
Interesting Comparisons with SciSumm
Nitin Agarwal
Language Technologies Institute
Carnegie Mellon University
nitina@cs.cmu.edu
Ravi Shankar Reddy
Language Technologies Resource Center
IIIT-Hyderabad, India
krs reddy@students.iiit.ac.in
Kiran Gvr
Language Technologies Resource Center
IIIT-Hyderabad, India
kiran gvr@students.iiit.ac.in
Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
cprose@cs.cmu.edu
Abstract
We present a novel unsupervised approach to
the problem of multi-document summariza-
tion of scientific articles, in which the doc-
ument collection is a list of papers cited to-
gether within the same source article, other-
wise known as a co-citation. At the heart of
the approach is a topic based clustering of
fragments extracted from each co-cited arti-
cle and relevance ranking using a query gen-
erated from the context surrounding the co-
cited list of papers. This analysis enables the
generation of an overview of common themes
from the co-cited papers that relate to the con-
text in which the co-citation was found. We
present a system called SciSumm that em-
bodies this approach and apply it to the 2008
ACL Anthology. We evaluate this summa-
rization system for relevant content selection
using gold standard summaries prepared on
principle based guidelines. Evaluation with
gold standard summaries demonstrates that
our system performs better in content selec-
tion than an existing summarization system
(MEAD). We present a detailed summary of
our findings and discuss possible directions
for future research.
1 Introduction
In this paper we present a novel, unsupervised ap-
proach to multi-document summarization of scien-
tific articles. While the field of multi-document sum-
marization has achieved impressive results with col-
lections of news articles, summarization of collec-
tions of scientific articles is a strikingly different
problem. Multi-document summarization of news
articles amounts to synthesizing details about the
same story as it has unfolded over a variety of re-
ports, some of which contain redundant information.
In contrast, each scientific article tells its own re-
search story. Even with papers that address similar
research questions, the argument being made is dif-
ferent. Instead of collecting and arranging details
into a single, synthesized story, the task is to abstract
away from the specific details of individual papers
and to find the common threads that unite them and
make sense of the document collection as a whole.
Another challenge with summarization of scien-
tific literature becomes clear as one compares alter-
native reviews of the same literature. Each review
author brings their own unique perspective and ques-
tions to bear in their reading and presentation of that
literature. While this is true of other genres of doc-
uments that have been the target of multi-document
summarization work in the past, we don?t find query
oriented approaches to multi-document summariza-
tion of scientific articles. One contribution of this
work is a technical approach to query oriented multi-
document summarization of scientific articles that
has been evaluated in comparison with a competi-
tive baseline that is not query oriented. The evalu-
ation demonstrates the advantage of the query ori-
ented approach for this type of summarization.
We present a system called SciSumm that sum-
marizes document collections that are composed of
lists of papers cited together within the same source
article, otherwise known as a co-citation. Using the
context of the co-citation in the source article, we
generate a query that allows us to generate a sum-
mary in a query-oriented fashion. The extracted por-
8
tions of the co-cited articles are then assembled into
clusters that represent the main themes of the arti-
cles that relate to the context in which they were
cited. Our evaluation demonstrates that SciSumm
achieves higher quality summaries than the MEAD
summarization system (Radev, 2004).
The rest of the paper is organized as follows. We
present an overview of relevant literature in Section
2. The end-to-end summarization pipeline has been
described in Section 3 . Section 4 presents an eval-
uation of summaries generated from the system. We
end the paper with conclusions and some interesting
further research directions in Section 5.
2 Literature Review
We begin our literature review by thinking about
some common use cases for multi-document sum-
marization of scientific articles.
First consider that as a researcher reads a scien-
tific article, she/he encounters numerous citations,
most of them citing the foundational and seminal
work that is important in that scientific domain. The
text surrounding these citations is a valuable re-
source as it allows the author to make a statement
about her viewpoint towards the cited articles. A
tool that could provide a small summary of the col-
lection of cited articles that is constructed specifi-
cally to relate to the claims made by the author cit-
ing them would be useful. It might also help the
researcher determine if the cited work is relevant for
her own research.
As an example of such a co-citation consider the
following citation sentence
Various machine learning approaches have been
proposed for chunking (Ramshaw and Marcus,
1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et
al. , 2000; Tjong Kim Sang, 2000b; Sassano and
Utsuro, 2000; van Halteren, 2000).
Now imagine the reader trying to determine about
widely used machine learning approaches for noun
phrase chunking. Instead of going through these
individual papers, it would be more useful to get
the summary of the topics in all those papers that
talk about the usage of machine learning methods in
chunking.
2.1 Overview of Multi-Document
Summarization
An exhaustive summary of recent work in summa-
rization is out of the scope for this paper. Hence, we
review only the most relevant approaches in summa-
rization to our current work. As most recent work in
multi-document summarization has been extractive,
and in our observation, scientific articles contain the
type of information that we would want in a sum-
mary, we follow this convention. This allows us to
avoid the complexities of natural language genera-
tion based approaches in abstractive summarization.
Multi-document summarization is an extension of
single document summarization in which the the-
matically important textual fragments are extracted
from multiple comparable documents, e.g., news ar-
ticles describing the same event. The techniques not
only need to address identification and removal of
redundant information but also inclusion of unique
and novel contributions. Various graph based (Mani
et al, 1997) and centroid clustering based meth-
ods (Radev et al, 2000) have been proposed to
address the problem of multi-document summa-
rization. Both of these methods identify common
themes present in a document collection using a sen-
tence similarity metric.
2.2 Summarization of Scientific Articles
Surprisingly, not many approaches to the problem of
summarization of scientific articles have been pro-
posed in the past. One exception is Teufel and
Moens (2002), who view summarization as a clas-
sification task in which they use a Naive Bayes clas-
sifier to assign a rhetorical status to each sentence
in an article and thus divide the whole article into
regions with a specific argumentation status (e.g.
categories such as AIM, CONTRAST and BACK-
GROUND). In our proposed approach, we are trying
to identify reoccurring topic themes that are com-
mon across the articles and may appear under a va-
riety of rhetorical headings.
Nanba and colleagues (1999) argue in their work
that a co-citation frequently implies a consistent
viewpoint towards the cited articles. Similarly, for
articles cited within different sentences, textual sim-
ilarity between the articles is inversely proportional
to the size of the sentential gap between the citations.
9
Figure 1: SciSumm summarization pipeline
In our work we make use of this insight by gen-
erating a query to focus our multi-document sum-
mary from the text closest to the citation. Qazvinian
and colleagues (2008) present a summarization ap-
proach that can be seen as the converse of what we
are working to achieve. Rather than summarizing
multiple papers cited in the same source article, they
summarize different viewpoints expressed towards
the same article from different citing articles. Some
of the insights they use in their work also apply to
our problem. They used a clustering approach over
different citations for the same target article for dis-
covery of different ways of thinking about that ar-
ticle. Citation text has been already shown to con-
tain important concepts about the article that might
be absent from other important sections of an ar-
ticle e.g. an Abstract (Mohammad et al, 2009) .
Template based generation of summaries possess-
ing similar hierarchical topic structure as the Related
Work section in an article has also been proposed
(Hoang et al, 2010). In our work, we consider a flat
topic structure in the form of topic clusters. More
specifically, we discover the comparable attributes
of the co-cited articles using Frequent Term Based
Clustering (Beil et al, 2002). The clusters gener-
ated in this process contain a set of topically related
text fragments called tiles, which are extracted from
the set of co-cited articles. Each cluster is indexed
with a label, which is a frequent term set present in
the tile. We take this to be an approximation of a
description for the topic represented by the cluster.
3 System Overview of the SciSumm
Summarization System
A high level overview of our system?s architecture is
presented in Figure 1 . The system provides a web
based interface for viewing and summarizing re-
search articles in the ACL Anthology corpus, 2008.
The summarization proceeds in three main stages.
First, a user may retrieve a collection of articles of
interest by entering a query. SciSumm responds by
returning a list of relevant articles. The user can con-
tinue to read an article of interest as shown in Figure
2. The co-citations in the paper are highlighted in
bold and italics to mark them as points of interest for
the user. If a user clicks on a co-citation, SciSumm
responds by generating a query from the local con-
text of the co-citation and uses it to rank the clusters
generated.
As an example consider the following citation
sentence ?Various machine learning approaches
have been proposed for chunking (Ramshaw and
Marcus, 1995; Tjong Kim Sang, 2000a; Tjong Kim
Sang et al , 2000; Tjong Kim Sang, 2000b; Sassano
and Utsuro, 2000; van Halteren, 2000)?. If the user
clicks on this co-citation, SciSumm generates a list
of clusters and ranks them for relevance. Most of the
top ranked cluster labels thus generated are shown in
Figure 3 along with the cluster content of the highest
ranked cluster labelled as Phrase, Noun. The labels
index into the corresponding cluster. An example
of such cluster is displayed in Figure 4. The clus-
ter has a label Chunk and contains tiles from two
of the three papers discussing about a topic identi-
10
Figure 2: Interface to read a paper. The sentences containing co-citations are automatically highlighted and contain a
?More? button beside them letting the user elaborate on the sentence.
fied by this label. In this specific example the topic
was not shared by tiles present in the third paper.
The words highlighted are interesting terms which
are either part of the label of the cluster or show
a low IDF (Inverse Document Frequency) amongst
the tiles generated from the co-cited papers. These
words are presented as hyper-links to the search in-
terface and can be further used as search queries for
finding articles on related topics.
3.1 System Description
SciSumm has four primary modules that are central
to the functionality of the system, as displayed in
Figure 1. First, the Text Tiling module takes care of
obtaining tiles of text relevant to the citation context.
It uses the Texttiling algorithm (Hearst, 1997), to
segment the co-cited papers into text tiles based on
topic shifts identified using a term overlap measure
computed between fixed-length blocks of text. Next,
the clustering module is used to generate labelled
clusters using the text tiles extracted from the co-
cited papers. The labels provide a conveniently com-
prehensible and yet terse description of each cluster.
We have used a Frequent Term Based Clustering al-
gorithm (Beil et al, 2002) for clustering. The clus-
ters are ordered according to relevance with respect
to the generated query. This is accomplished by the
Ranking Module. Finally, the summary presenta-
tion module is used to display the ranked clusters
obtained from the ranking module. Alongside the
clusters, an HTML pane also shows the labels of all
the clusters. Having such a bird?s-eye view of all the
cluster labels helps the user to quickly navigate to an
interesting topic. The entire pipeline is used in real-
time to generate topic clusters which are useful for
generating snippet summary and more exploratory
analysis.
In the following sections, we discuss each of the
main modules in detail.
3.2 Texttiling
The Text Tiling module uses the TextTiling algo-
rithm (Hearst, 1997) for segmenting the text of each
article. Each such segment obtained by the TextTil-
ing algorithm has been referred as a text tile. We
have used these text tiles as the basic unit for our
summary since individual sentences are too short
to stand on their own. Once computed, text tiles
are used to identify the context associated with a
co-citation. The intuition is that an embedded co-
citation in a text tile is connected with the topic dis-
tribution of the tile. We use important text from this
tile to rank the text clusters generated using Frequent
Term based text clustering.
11
Figure 3: Clusters generated in response to a user click on the co-citation. The list of clusters in the left pane gives a
bird-eye view of the topics which are present in the co-cited papers
3.3 Frequent Term Based Clustering
The clustering module employs Frequent Term
Based Clustering (Beil et al, 2002). For each co-
citation, we use this clustering technique to cluster
all the of the extracted text tiles generated by seg-
menting each of the co-cited papers. We settled on
this clustering approach for the following reasons:
? Text tile contents coming from different papers
constitute a sparse vector space, and thus the
centroid based approaches would not work very
well.
? Frequent Term based clustering is extremely
fast in execution time as well as and relatively
efficient in terms of space requirements.
? A frequent term set is generated for each clus-
ter which gives a comprehensible description of
the cluster.
Frequent Term Based text clustering uses a group
of frequently co-occurring terms called a frequent
term set. Each frequent term set indexes to a cor-
responding cluster. The frequent term set has the
property that it occurs at least once in each of the
documents present in the cluster. The algorithm uses
the first k term sets if all the documents in the doc-
ument collections are clustered.To discover all the
possible candidates for clustering, i.e., term sets, we
used the Apriori algorithm (Agrawal et al, 1994),
which identifies the sets of terms that are relatively
frequent. We use entropy measure to score each fre-
quent term set as discovered from the Apriori algo-
rithm. The entropy overlap of a cluster Ci, EO(Ci)
is calculated as follows:
EO(Ci) =
?
DjCi
?
1
fj
.ln(
1
fj
)
where Dj is the jth document which gets binned
in the cluster Ci, fj is the number of clusters which
contain Dj . A smaller value means that the doc-
ument Dj is contained in few other clusters Ci.
EO(Ci) increases monotonically as fj increases.
We thus rank the clusters with their corresponding
EO(Ci) and then pick a cluster with the smallest
entropy overlap EO(Ci) . Once a cluster is chosen
to be included in the final clustering, we remove the
documents present in chosen cluster from other can-
didate clusters. This results in a hard clustering of
documents. We also remove term set correspond-
ing to Ci from the list of candidate frequent term
sets and then again recompute the EO(Ci) ?s for the
clusters. We continue this re-scoring and selecting
a candidate cluster until the final clustering does not
completely exhaust the entire document collection.
3.4 Cluster Ranking
The ranking module uses cosine similarity between
the query and the centroid of each cluster to rank all
the clusters generated by the clustering module. The
context of a co-citation is restricted to the text of the
tile in which the co-citation is found. In this way
we attempt to leverage the expert knowledge of the
author as it is encoded in the local context of the co-
citation in our process of automatically ranking the
clusters in terms of importance.
12
Figure 4: Example of a summary generated by our system. We can see that the clusters are cross cutting across
different papers, thus giving the user a multi-document summary.
4 Evaluation
In a typical evaluation of a multi-document sum-
marization system, gold standard summaries are
evaluated against fixed length generated summaries.
Summarization conferences such as DUC have com-
petitions where different summarization systems
compete on a standard task of generating summaries
for a publicly available dataset. The summaries gen-
erated using each individual summarization system
are then evaluated against the summaries prepared
by human annotators. Summarization of scientific
article is a novel task and hence no test collection of
gold standard summaries exist. Thus, it was neces-
sary to prepare our own evaluation corpus, consist-
ing of gold standard multi-document summaries for
a set of randomly selected co-citations.
4.1 Experimental Setup
An important target user population for multi-
document summarization of scientific articles is
graduate students. Hence to get a measure of how
well the summarization system is performing, we
asked 2 graduate students who have been working
in the computational linguistics community to cre-
ate gold standard summaries of a fixed length (8 sen-
tences ? 200 words) for ten different randomly se-
lected co-citations. The students were given guide-
lines to prepare summaries based on the design goals
of the SciSumm system, but not any of its technical
details. Thus, for 10 co-citations, we obtained two
different gold standard summaries. For ROUGE-1
the average score between each pair of gold standard
summaries was 0.518 (Min = 0.388, Max = 0.686).
Similarly for ROUGE-2 the average score was 0.242
(Min = 0.119, Max=0.443). While these scores do
not have a well-calibrated meaning to them, they
give an indication of the complexity of the task.
Since the annotators were creating extractive sum-
maries which could justify the co-citation, they had
to pay special attention to the section where the co-
citation came from. One can consider this similar to
the sense making process a reader might go through
when using the citing paper as a lens through which
to interpret the cited literature.
Note that while SciSumm provides users with
an interactive interface that supports navigation be-
tween documents, the gold standard summaries are
static. Thus, our evaluation is designed to measure
the quality of the content selection when taking into
consideration the citation context. This would also
13
help us to evaluate the influence exerted by the ci-
tation context in the gold standard summaries. In
future work, we will evaluate the usability of the
SciSumm system using a task based evaluation.
In the absence of any other multi-document sum-
marization systems in the domain of scientific ar-
ticle summarization, we used a widely used and
freely available multi-document summarization sys-
tem called MEAD (Radev, 2004) as our baseline.
MEAD uses centroid based summarization to cre-
ate informative clusters of topics. We use the de-
fault configuration of MEAD in which MEAD uses
length, position and centroid for ranking each sen-
tence. We did not use query focussed summariza-
tion with MEAD. We evaluate its performance with
the same gold standard summaries we use to evalu-
ate SciSumm. For generating a summary from our
system we used sentences from the tiles which gets
clustered in the top ranked cluster. When that entire
cluster is exhausted we move on to the next highly
ranked cluster. In this way we prepare a summary
comprising of 8 sentences.
4.2 Results
For measuring performance of the two summariza-
tion systems (SciSumm and MEAD), we compute
the ROUGE metric based on the 2 * 10 gold standard
summaries that were manually created. ROUGE
has been traditionally used to compute the perfor-
mance based on the N-gram overlap (ROUGE-N)
between the summaries generated by the system and
the target gold summaries. For our evaluation we
used two different versions of the ROUGE met-
ric, namely ROUGE-1 and ROUGE-2, which cor-
respond to measures of the unigram and bigram
overlap respectively. We computed four metrics in
order to measure SciSumm?s performance, namely
ROUGE-1 F-measure, ROUGE-1 Recall, ROUGE-
2 F-measure, and ROUGE-2 Recall. To measure the
statistical significance of this result, we carried out
a Student T-Test, the results of which are presented
in the results section. The t-test results displayed
in Table 1 show that our systems performs signif-
icantly better than MEAD on three of the metrics
(p < .05). On two additional metrics, SciSumm
performs marginally better (p < .1).
This shows that using the query generated out
of the co-citation is useful for content selection
Table 1: Average ROUGE results. * represents improve-
ment significant at p < .05, ? at p < .01.
Metric MEAD SciSumm
ROUGE-1 F-measure 0.3680 0.5123 ?
ROUGE-1 Recall 0.4168 0.5018
ROUGE-1 Precision 0.3424 0.5349 ?
ROUGE-2 F-measure 0.1598 0.3303 *
ROUGE-2 Recall 0.1786 0.3227 *
ROUGE-2 Precision 0.1481 0.3450 ?
from cited papers. Intuitively, this makes sense as
each researcher would have a unique perspective
when reviewing scientific literature. Co-citations
can be considered as micro-reviews which summa-
rizes the thread unifying the research presented in
each of the cited papers. This provides evidence that
the co-citation context provides useful information
for forming an effective query to focus the multi-
document summary to reflect the perspective of the
author of the citing paper.
5 Conclusions and Future Work
In this work, we proposed the first unsupervised ap-
proach to the problem of multi-document summa-
rization of scientific articles that we know of. In
this approach, the document collection is a list of
papers cited together within the same source arti-
cle, otherwise known as a co-citation. The summary
is presented in the form of topic labeled clusters,
which provide easy navigation based on the user?s
topic of interest. Another contribution is a techni-
cal approach to query oriented multi-document sum-
marization of scientific articles that has been evalu-
ated in comparison with a competitive baseline that
is not query oriented. Our evaluation shows that the
SciSumm approach to content selection outperforms
another multi-document summarization system for
this summarization task.
Our long term goal is to expand the capabilities
of SciSumm to generate literature surveys of larger
document collections from less focused queries.
This more challenging task would require more con-
trol over filtering and ranking in order to avoid gen-
erating summaries that lack focus. To this end, a
future improvement that we plan to use a variant on
MMR (Maximum Marginal Relevance) (Carbonell
14
et al, 1998), which can be used to optimize the di-
versity of selected text tiles as well as the relevance
based ordering of clusters in order to put a more di-
verse set of observations from the co-cited articles
at the fingertips of users. A natural extension would
also be to discover the nature of citations to gen-
erate improved summaries. Non-explicit citations
(Qazvinian et al, 2010) which could be used to gen-
erate similar topic clusters.
Another important direction is to refine the inter-
action design through task-based user studies. As we
collect more feedback from students and researchers
through this process, we will use the insights gained
to achieve a more robust and effective implementa-
tion. We also plan to leverage research in informa-
tion visualization to enhance the usability of the sys-
tem.
6 Acknowledgements
This work was supported by NSF EEC-064848 and
NSF EEC-0935127.
References
Agrawal R. and Srikant R. 1994. Fast Algorithm for
Mining Association Rules In Proceedings of the 20th
VLDB Conference Santiago, Chile, 1994
Baxendale, P. 1958. Machine-made index for technical
literature - an experiment. IBM Journal of Research
and Development
Beil F., Ester M. and Xu X 2002. Frequent-Term based
Text Clustering In Proceedings of SIGKDD ?02 Ed-
monton, Alberta, Canada
Carbonell J. and Goldstein J. 1998. The Use of MMR,
Diversity-Based Reranking for Reordering Documents
and Producing Summaries In Research and Develop-
ment in Information Retrieval, pages 335?336
Councill I. G. , Giles C. L. and Kan M. 2008. ParsCit:
An open-source CRF reference string parsing pack-
age INTERNATIONAL LANGUAGE RESOURCES
AND EVALUATION European Language Resources
Association
Edmundson, H.P. 1969. New methods in automatic ex-
tracting. Journal of ACM.
Hearst M.A. 1997 TextTiling: Segmenting text into
multi-paragraph subtopic passages In proceedings of
LREC 2004, Lisbon, Portugal, May 2004
Joseph M. T. and Radev D. R. 2007. Citation analysis,
centrality, and the ACL Anthology
Kupiec J. , Pedersen J. , Chen F. 1995. A training doc-
ument summarizer. In Proceedings SIGIR ?95, pages
68-73, New York, NY, USA. 28(1):114?133.
Luhn, H. P. 1958. IBM Journal of Research Develop-
ment.
Mani I. , Bloedorn E. 1997. Multi-Document Summa-
rization by graph search and matching In AAAI/IAAI,
pages 622-628. [15, 16].
Nanba H. , Okumura M. 1999. Towards Multi-paper
Summarization Using Reference Information In Pro-
ceedings of IJCAI-99, pages 926?931 .
Paice CD. 1990. Constructing Literature Abstracts by
Computer: Techniques and Prospects Information
Processing and Management Vol. 26, No.1, pp, 171-
186, 1990
Qazvinian V. , Radev D.R 2008. Scientific Paper
summarization using Citation Summary Networks In
Proceedings of the 22nd International Conference on
Computational Linguistics, pages 689?696 Manch-
ester, August 2008
Radev D. R . , Jing H. and Budzikowska M. 2000.
Centroid-based summarization of multiple documents:
sentence extraction, utility based evaluation, and user
studies In NAACL-ANLP 2000 Workshop on Auto-
matic summarization, pages 21-30, Morristown, NJ,
USA. [12, 16, 17].
Radev, Dragomir. 2004. MEAD - a platform for multi-
document multilingual text summarization. In pro-
ceedings of LREC 2004, Lisbon, Portugal, May 2004.
Teufel S. , Moens M. 2002. Summarizing Scientific
Articles - Experiments with Relevance and Rhetorical
Status In Journal of Computational Linguistics, MIT
Press.
Mohammad, Saif and Dorr, Bonnie and Egan, Melissa
and Hassan, Ahmed and Muthukrishan, Pradeep and
Qazvinian, Vahed and Radev, Dragomir and Zajic,
David 2009. Using citations to generate surveys of
scientific paradigms In Proceedings of Human Lan-
guage Technologies:The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics
Qazvinian, Vahed and Radev, Dragomir R. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics
Hoang, Cong Duy Vu and Kan, Min-Yen 2010. Towards
automated related work summarization In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Posters
15
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 76?85,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Language use as a reflection of socialization in online communities
Dong Nguyen
Carnegie Mellon University
Language Technologies Institute
Pittsburgh, PA 15213
dongn@cs.cmu.edu
Carolyn P. Rose?
Carnegie Mellon University
Language Technologies Institute
Pittsburgh, PA 15213
cprose@cs.cmu.edu
Abstract
In this paper we investigate the connection be-
tween language and community membership
of long time community participants through
computational modeling techniques. We re-
port on findings from an analysis of language
usage within a popular online discussion fo-
rum with participation of thousands of users
spanning multiple years. We find community
norms of long time participants that are char-
acterized by forum specific jargon and a style
that is highly informal and shows familiarity
with specific other participants and high emo-
tional involvement in the discussion. We also
find quantitative evidence of persistent shifts
in language usage towards these norms across
users over the course of the first year of com-
munity participation. Our observed patterns
suggests language stabilization after 8 or 9
months of participation.
1 Introduction
In this paper we use text mining and machine
learning methodologies as lenses through which to
understand the connection between language use
and community membership in online communi-
ties. Specifically we examine an online medical sup-
port community called breastcancer.org. We present
analyses of data from an active online community
with the goal of uncovering the connection between
language and online community membership. In
particular, we will look at language changes that oc-
cur over time as people continue to participate in an
online community. Consistent with the Communi-
ties of Practice theory of participation within a com-
munity (Lave and Wenger, 1991), we find increas-
ing conformity to community norms within the first
year of participation that then stabilizes as partici-
pants continue their involvement in the community.
Within the Communities of Practice view, social-
ization into a community begins with peripheral par-
ticipation, during which individuals have the op-
portunity to observe community norms. Lave and
Wenger?s theory has been applied to both online
and face-to-face communities. In an online commu-
nity, observing community norms begins with lurk-
ing and reading messages before an initial post. This
is termed legitimate peripheral participation, and it
is during this stage that potential new members ob-
serve community norms in action. With an initial
post, a user embarks upon the path of centripetal
participation, as they are taking steps towards core
participation.
Becoming a core member of a community means
adopting community norms. Persistent language
changes occur as an accumulation of local accom-
modation effects (Labov, 2010a; Labov, 2010b).
The extent of the adoption reflects the commitment
to community membership. Thus, as an individual
progressively moves from the periphery of a com-
munity towards the core, their behavior will progres-
sively grow towards conformity with these norms,
although total conformity very rarely occurs. The
quantitative analysis we present in the form of a
regression model is consistent with this theoretical
perspective and allows us to see what centripetal par-
ticipation and core participation look like within the
breastcancer.org community. We are able to test the
robustness of these observations by using the extent
76
of conformity to community norms as a predictor
of how long a member has been actively participat-
ing in an online community. We will present results
from this predictive analysis as part of the quantita-
tive evidence we provide in support of this model of
community participation.
Patterns of local accommodation and of long time
language change within communities have been ex-
tensively studied in the field of variationist sociolin-
guistics. However, with respect to online commu-
nities in particular, recent research has looked at
accommodation (Danescu-Niculescu-Mizil et al,
2011; Nguyen et al, 2010) and some shorter term
language changes (i.e., over a period of a few
months). However, longitudinal analyses of lan-
guage change spanning long time periods (i.e., more
than a few months) in online communities as we
present in this paper have been largely absent from
the literature. Typically, long term language change
in sociolinguistics requires reconstructing the past
from the present using age grading techniques, since
a comprehensive historical record is typically absent
(Labov, 2010a; Labov, 2010b). Online communi-
ties present a unique opportunity to study long term
language change from a much more comprehensive
historical record of a community?s development.
In the remainder of the paper, we first review prior
work on computational models of accommodation
and language change. We then present a qualitative
view of communication within the breastcancer.org
community. We then present two quantitative analy-
ses, one that explores language change in the aggre-
gate, and another that tests the robustness of findings
from the first analysis with a regression model that
allows us to predict how long a member has been
active within the community. We conclude with dis-
cussion and future work.
2 Related work
For decades, research under the heading of Social
Accommodation Theory (Giles et al, 1973) has at-
tempted to layer a social interpretation on patterns of
linguistic variation. This extensive line of research
has provided ample quantitative evidence that peo-
ple adjust their language within interactions, some-
times to build solidarity or liking, and other times
to differentiate themselves from others (Eckert and
Rickford, 2001).
In this line of work, people have often looked
at accommodation in small discussion groups and
dyadic conversation pairs. For example, Gonza-
les et al (2010) analyzed style matching in small
group discussions, and used it to predict cohesive-
ness and task performance in the groups. Scis-
sors et al (2009) analyzed conversational pairs play-
ing a social dilemma game and interacting through
an instant messenger. They found that certain pat-
terns of high linguistic similarity characterize high
trusting pairs. Niederhoffer and Pennebaker (2002)
found linguistic style matching both at the conver-
sation level and locally at a turn-by-turn level in
dyadic conversations. Paolillo (2001) looked at the
connection between linguistic variation and strong
and weak ties in an Internet Relay Chat channel.
Nguyen et al (2010) found accommodation effects
in an online political forum that contains discus-
sions between people with different political view-
points. Recently, Danescu-Niculescu-Mizil et al
(2011) showed that accommodation was also present
in Twitter conversations.
Lam (2008) gives an overview of work on lan-
guage socialization in online communities. We
know that persistent language changes over long
time periods are the accumulated result of local ac-
commodations that occur within short-term contexts
for social reasons (Labov, 2010a; Labov, 2010b).
However, the process through which individuals
adopt the language practices of online communi-
ties has been barely explored so far. One exam-
ple of investigation within this scope is the work
of Postmes et al (2000), in which we find analy-
sis of the formation of group norms in a computer-
mediated communication setting. Specifically, they
found that small groups were formed during the pro-
cess and communication norms including language
usage patterns were present within those groups.
Over time, conformity to these norms increased.
Similarly, Cassell and Tversky (2005) looked at evo-
lution of language patterns in an online community.
In this work, the participants were students from
around the world participating in the Junior Summit
forum ?98. Cassell and Tversky found that partic-
ipants converged on style, topics, goals and strate-
gies. Analyses were computed using word frequen-
cies of common classes (such as self references) and
77
Table 1: Statistics dataset.
Posts 1,562,590
Threads 68,226
Users (at least one post) 31,307
Time-span Oct 2002 - Jan 2011
manual coding. Huffaker et al (2006) examined a
subset of the same data. When comparing consec-
utive weeks over a 6 week time period, they found
that the language diverged. They hypothesized that
this was caused by external events leading to the in-
troduction of new words.
Our research differs from the research by Cas-
sell and Tversky (2005), Huffaker et al (2006) and
Postmes et al (2000) in several respects. For ex-
ample, in all of this work, participants joined the
community simultaneously at the inception of the
community. In contrast, our community of inquiry
has evolved over time, with members joining inter-
mittently throughout the history of the community.
Additionally, our analysis spans much more time,
specifically 2 years of data rather than 3 or 4 months.
Thus, this research addresses a different question
from the way community norms are first established
at the inception of a community. In contrast, what
we investigate is how new users are socialized into
an existing community in which norms have already
been established prior to their arrival.
We are not the first researchers to study our com-
munity of inquiry (Jha and Elhadad, 2010). How-
ever, prior work on data from this forum was focused
on predicting the cancer stage of a patient rather than
issues related to language change that we investi-
gate.
3 Data description
We analyze one of the largest breast cancer forums
on the web (http://community.breastcancer.org/).
All posts and user profiles of the forum were crawled
in January 2011.
The forum serves as a platform for many differ-
ent kinds of interactions, and serving the needs of a
variety of types of users. For example, a large pro-
portion of users only join to ask some medical ques-
tions, and therefore do not stay active long. In fact,
we find that a lot of users (12,349) only post in the
first week after their registration. The distribution of
number of weeks between a user?s last post and reg-
istration date follows a power law. However, besides
these short-term users, we also find a large number
of users who appear to be looking for more social in-
volvement and continue to participate for years, even
after their disease is in remission.
This distinction in types of users is reflected in the
forum structure. The forum is well organized, con-
taining over 60 subforums targeting different topics.
Besides specific subforums targeting medical topics
(such as ?Stage I and II Breast Cancer? and ?Radi-
ation Therapy - Before, During and After?), there
are subforums for certain population groups (such
as ?Canadian Breast Cancer Survivors? and ?Sin-
gles with breast cancer?), for social purposes (such
as ?Growing our Friendships After Treatment?, ?Get
Togethers?, and ?CyberSisters Photo Album?) and
non cancer related purposes (such as ?Humor and
Games?). In many of the subforums there are spe-
cific threads that foster the formation of small sub
communities, for example threads for people who
started chemotherapy in a certain month.
In the data we find community norms of long time
participants that are characterized by forum specific
jargon and a style that is highly informal and shows
familiarity with specific other participants and high
emotional involvement in the discussion. We infer
that the forum specific jargon is distinct from what
we would find in those users outside of it, in that that
there are places in the forum explaining commonly
used abbreviations to new users. We also observe
posts within threads where users ask about certain
abbreviations used in previous posts. Some of these
abbreviations are cancer related and also used in
places other than the forum, such as dx (diagnosis),
and rads (radiation, radiotherapy). Thus, they may
be reflective of identification with a broader commu-
nity of cancer patients who are internet users. Other
often used abbreviations are dh (dear husband), dd
(dear daughter), etc. We also observed that users fre-
quently refer to members of the community by name
and even as sister(s).
Now let us look at some examples illustrating
these patterns of language change. We take as an ex-
ample a specific long-time user. We start with a post
from early in her participation, specifically from a
couple of days after her registration:
78
I am also new to the forum, but not new
to bc, diagnosed last yr, [..] My follow-
up with surgeon for reports is not until
8/9 over a week later. My husband too is
so wonderful, only married a yr in May,
1 month before bc diagnosed, I could not
get through this if it weren?t for him, never
misses an appointment, [...] I wish every-
one well. We will all survive.
The next two posts1 are from the same user, 2 to
4 years after her registration date. Both posts are di-
rected to other forum members, very informal, and
contain a lot of abbreviations (e.g. ?DH? (Dear Hus-
band), ?DD? (Dear Daughter), ?SIL? (Son in Law)).
Gee Ann I think we may have shared the
same ?moment in time? boy I am getting
paid back big time for my fun in the sun.
Well Rose enjoy your last day of freedom
- LOL. Have lots of fun with DH ?The
Harley?. Ride long and hard ( either one
you choose - OOPS ).
Oh Kim- sorry you have so much going
on - and an idiot DH on top of it all.
[..] Steph- vent away - that sucks - [..]
XOXOXOXOXOXOXOX [..], quiet week-
end kids went to DD?s & SIL on Fri-
day evening, they take them to school [..],
made an AM pop in as I am supposed to,
SIL is an idiot but then you all know that.
This anecdotal evidence illustrates the linguistic
shift we will now provide quantitative evidence for.
4 Patterns of language change
4.1 Approach
In this section we aggregate data across long time
participants and look at global patterns of language
change. Specifically, we will analyze patterns of
change in the first year after registration of these
members, and show how language patterns consis-
tently become more different from the first week of
participation and more similar to the stable pattern
found within the second year of data. Furthermore,
when comparing consecutive weeks we find that the
1Names are replaced in example
difference increases and then stabilizes by the end
of the first year. The unit of analysis is one week
of data. Because there are multiple ways to mea-
sure the similarity or difference between two distri-
butions, we explore the use of two different meth-
ods. The first metric we use is the Kullback-Leibler
(KL) divergence. Larger values indicate bigger dif-
ferences in distribution. P represents the true distri-
bution. Note that this metric is asymmetric.
KL(P,Q) =
?
i
P (i) log
P (i)
Q(i)
We also explore using the Spearman?s Rank Corre-
lation Coefficient (SRCC), which measures the sim-
ilarity of two rankings:
SRCC = 1?
6
?
i d
2
i
n(n2 ? 1)
Where di is the difference between the ranks of word
i in the two rankings and n is the total number of
words.
4.2 Sampling
In this analysis, we begin by aligning the data of ev-
ery member by registration date. We then aggregate
posts of all users by week. Thus, in week 1, we have
the posts from all users during the first week after
their registration. Note that the actual week in time
would not be the same for each of these users since
they did not all register at the same time. In this way,
a week worth of data represents the way users talk
after the corresponding number of weeks after regis-
tering with the community rather than representing
a specific period of time. Because our dataset spans
a large time period of time (i.e. more than 8 years),
it is very unlikely that patterns we find in the data
reflect external events from any specific time period.
As discussed before, a large proportion of mem-
bers only post in their first week after registration.
These short time members might already initially
differ from members who tend to participate longer
in the forum. Therefore, it might confuse the model
if we take these short time members into account.
We may observe apparent changes in language that
are artifacts of the difference in distribution of users
across weeks. Thus, because we are interested in
language change specifically, we only consider posts
of long-term participants.
79
In addition, we have limited our focus to the ini-
tial two-year period of participation, because it is
for this length of participation that we have enough
users and enough posts to make a computational
model feasible. We have also limited ourselves to
examining high frequency words, because we have
a large vocabulary but only a limited amount of data
per week. Two weeks can look artificially similar
if they both have a lot of non-occurring words. In
summary, taking above considerations into account,
we applied the following procedure:
? We only look at the first 2 years, for which we
still have a large amount of data for every week.
? We only look at members who are long-term
participants (2 years or longer), this leaves us
with 3,012 users.
? For every week, we randomly sample an equal
number of posts (i.e., 600 from each week). All
posts are taken into account (i.e. both responses
as well as thread openings).
? We only look at the distribution change of
high frequency words (words occurring at least
1,000 times), this leaves us with 1,540 unique
words. No stemming or stop word removal was
done.
4.3 Comparison with early and late
distributions
Using the dataset described in the previous section,
we compare the language of each week during the
first year after registration with language in the very
first week and with language in the second year.
First we analyze whether language in the first year
becomes more similar to language used by members
in their second year as time progresses. We there-
fore compare the word distributions of the weeks of
the first year with the overall word distribution of
the second year. We apply KL divergence where
we consider the distribution of the second year as
the ?true distribution?. The result is shown in Fig-
ure 1. We see that the KL divergence decreases,
which means that as time progresses, the word dis-
tributions look more like the distribution of the sec-
ond year. Fitting a Least Squares (LS) model, we
get an intercept of 0.121033 and slope of -0.001080
Figure 1: KL divergence between weeks in first year and
overall second year.
Figure 2: KL divergence between weeks in first year and
first week.
(r2 = 0.5528). Using the Spearman Rank Correla-
tion (SRCC) and fitting a LS model, we observe the
same pattern (r2 = 0.6435).
Our second analysis involves comparing the dis-
tributions of the first year (excluding the first week),
with the distribution of the first week. The result is
shown in Figure 2. We see that the KL divergence
increases, meaning that as time progresses, the word
distributions become less similar with the first week.
(KL: r2 = 0.6643, SRCC: r2 = 0.7962).
4.4 Comparing consecutive distributions
We now compare the distributions of consecutive
weeks to see how much language change occurs in
different time periods. For KL divergence we use the
symmetric version. Results are presented in Figure
3 and show a divergence pattern throughout the first
year that stabilizes towards the end of that first year
of participation. (KL: r2 = 0.4726, SRCC: r2 =
80
Figure 3: KL divergence between consecutive weeks.
0.8178). The divergence pattern was also observed
by Huffaker et al (2006) (related, but not equiva-
lent setting, as mentioned in the literature review).
We hypothesize that the divergence occurs because
users tend to talk about a progressively broader set
of topics as they become more involved in the com-
munity. To confirm this hypothesis, we compare the
distributions of each week with the uniform distri-
bution. We indeed find that as time progresses, the
distributions for each week become more uniform.
(KL: r2 = 0.3283, SRCC: r2 = 0.6435).
5 Predicting membership duration
In the previous section we found strong patterns of
language change in our data. We are interested in the
extent to which we can automatically predict how
many weeks the user has been a member, using only
text or meta-features from that specific week. Iden-
tifying which features predict how long a member
has been active can give more detailed insight into
the social language that characterizes the commu-
nity. In addition, it tells us how prominent the pat-
tern is among other sources of language variation.
5.1 Dataset
For this analysis, we set up the data slightly differ-
ently. Now, rather than combine data across users,
we keep the data from each user for each week sep-
arate so we can make a separate prediction for each
user during each week of their participation. Thus,
for each person, we aggregate all posts per week.
We only consider weeks in the first two years after
the registration in which there were at least 10 posts
with at least 10 tokens from that user.
Table 2: Statistics dataset.
#Docs #Persons #Posts
Training 13,273 1,591 380,143
Development 4,617 548 122,489
Test 4,571 548 134,141
5.2 Approach
Given an input vector x ? Rm containing the fea-
tures, we aim to find a prediction y? ? R for the num-
ber of weeks the person has been a member of the
community y ? R using a linear regression model:
y? = ?0 + x>? where ?0 and ? are the parameters
to estimate. Usually, the parameters are learned by
minimizing the sum of squared errors.
In order to strive for a model with high explana-
tory value, we use Linear Regression, with L1 reg-
ularization (Tibshirani, 1996). This minimizes the
sum of squared errors, but in addition adds a penalty
term ?
?m
j=1 |?j |, the sum of absolute values of the
coefficients. ? is a constant and can be found by
optimizing over the development data. As a re-
sult, this method delivers sparse models. We use
Orthant-Wise Limited-memory Quasi-Newton Op-
timizer (Andrew and Gao, 2007) as our optimiza-
tion method. This method has proven to establish
competitive performances with other optimization
methods, while producing sparse models (Gao et al,
2007).
Because our observations suggest that language
change decreases as members have been active
longer, we also experimented with applying a log
transformation on the number of weeks.
5.3 Features
For all features, we only use information that has
been available for that particular week. We explore
different types of features related to the qualitative
differences in language we discussed in Section 3:
textual, behavioral, subforum and meta-features.
5.3.1 Textual features
We explore the following textual features:
? Unigrams and bigrams.
? Part of Speech (POS) bigrams. Text was tagged
using the Stanford POS tagger (Toutanova et
al., 2003).
81
? LIWC (Pennebaker et al, 2001), a word count-
ing program that captures word classes and
stylistic features.
? Usernames. Because some of the usernames
are common words, we only consider user-
names of users active in the same thread.
? Proper names. We obtained a list containing
common female names. We ranked them ac-
cording to frequency in our dataset, and manu-
ally deleted common words in our dataset, such
as happy, hope, tuesday and may, from our list.
? Slang words. We manually compile a list of
common abbreviations and their whole words
counterpart. We then count the number of ab-
breviations and the number of whole words
used in the post. The feature value then
is (#abbrev?#wholewords)/#totalwords.
Because in some contexts no abbreviations
can be used, this feature takes into account if
the user actually chose to use the abbrevia-
tion/whole word, or if there was no need for
it.
No stemming or stopword removal is used. Fre-
quencies are normalized by length.
5.3.2 Behavioral features
We also explore additional features that indicate
the behavior of the user:
? Ratio (posts starting threads) / (total number of
posts).
? Number of posts.
5.3.3 Subforum features
We include as features the distribution of subfo-
rums the member has posted in. This captures two
intuitions. First, it is an approximation of the current
phase in the cancer process for that member. For ex-
ample, we noticed that most of the new users have
just been diagnosed, while long term users have al-
ready finished treatment. Because the subforums are
very specific (such as ?Not Diagnosed with a Re-
currence or Metastases but Concerned?), we expect
these features to give a good approximation of the
phase the user is currently in. In addition, these sub-
forums also give an indication of the user?s interest.
Table 3: Results reported with Pearsons correlation (r).
Run # Features Raw (r) Log (r)
Unigrams + Bigrams 43,126 0.547 0.621
POS 1,258 0.409 0.437
LIWC 88 0.494 0.492
Proper names 1 0.185 0.186
Usernames 1 0.150 0.102
Slang 1 0.092 0.176
Behavior 2 0.139 0.243
Subforum 65 0.404 0.419
All above 44,542 0.581 0.649
All above + Person 46,133 0.586 0.656
For example, whether the user posts mostly in med-
ical forums, or mostly in the social orientated subfo-
rums.
5.3.4 Other features
Most of the persons appear multiple times in our
dataset (e.g. multiple weeks). To help the model
control for idiosyncratic features of individual users,
we include for every person a dummy variable asso-
ciated with that user?s unique identity. This helps
the model at training time to separate variance in
language usage across users from general effects re-
lated to length of participation. Note that we do not
use these features as test time.
5.4 Results
We experimented with individual types of features
as well as all of them aggregated. The results (corre-
lations) can be found in Table 3. The features having
the most weight for long time participants in our best
model (All incl. Person, Log) are presented in Table
4. We see that for most features the performance
was higher when applying the log transformation.
This was especially the case with the unigrams and
bigrams features. For some features the difference
was less, such as for proper names and the subforum
features. This could indicate that these features have
a more linear pattern as time progresses, while word
patterns such as unigrams tend to stabilize earlier.
We find that both stylistic patterns (such as POS) as
well as patterns indicating conformity (social behav-
ior, slang words) are individually already very pre-
dictive.
In our best performing model, we find that both
82
Table 5: Qualitative grouping of textual features.
Type Short time members Long time members
Abbreviations Husband My DD (Dear Daughter), Your PS (Plastic Surgeon)
Social networks Facebook, fb
Greetings Hi all Hi girls, Hi gals
I versus other LIWC-I, My, Me LIWC-other, We, Sisters
Social support Hugs, Condolences, So sorry
Thanking Thanks, Thanx, Thx
Forum Bc org, On bco
Introducing Newbie, New here, Am new
Asking information Info, LIWC-qmarks
Table 4: Top 10 features of long term users.
Feature Weight
META - slang 0.058362195
META -propername 0.052984915
year 0.050872918
META - [person1] 0.050708718
META - [person2] 0.040548104
months 0.040400583
META - [person3] 0.039806096
LIWC - Othref 0.036080545
META - [person4] 0.035605996
POS - nnp prp 0.035033650
the slang and proper name features get a high weight
for long time participants. Furthermore, we observe
that a lot of the person meta features are included
in the model when it is trained, although as men-
tioned we do not use these features at testing time.
The fact that the model assigns them weight indi-
cates that idiosyncratic features of users explain a lot
of variance in the data. Our best performing model
has 3,518 non zero features. In Table 5 we qual-
itatively grouped and contrasted features that were
more associated with short-term or long-term mem-
bers. We see that long-term members show much
more social behavior and familiarity with each other.
This is shown to references to each other, more so-
cial support, references to social networks and ways
of greeting. They furthermore talk about the forum
itself more often by using the abbreviation ?bco?.
Short term members are characterized by words that
are used when they introduce themselves to others.
Thus we find that long time participants are char-
acterized by informal language, containing many fo-
rum specific jargon, as well as showing emotional
involvement with other forum members. Our best
run obtained a correlation of r = 0.656, giving an
r2 value of 0.430. This means that 0.43 of the vari-
ation can be explained by our model. Since there
are many other factors that influence the writing of
users, it is understandable that our model does not
explain all the variance.
6 Discussion
As discussed widely in previous literature, peo-
ple become socialized into communities over time
through their interactions with community mem-
bers. The extent of conformity to group norms re-
flects commitment to the group. Our first study
showed evidence of increasing conformity to com-
munity norms through changes in simple word dis-
tributions. The second study then tested the robust-
ness of these findings through a prediction task and
extended the language features of the first study.
Since community members tend to conform in-
creasingly to community norms over time, although
the target class for our predictive model is time, it
is reasonable to assume that what the model really
learns to predict is how long average community
members have been around by the time they sound
like that. In other words, one can think about its time
prediction as a measure of how long it sounds like
that person has been in the community. The model
would therefore overpredict for members who move
from the periphery to the core of a community faster
than average while underpredicting for those who do
so more gradually. This would be consistent with the
83
idea that rate of commitment making and conformity
is person specific.
There are two limitations that need to be ad-
dressed regarding the present studies. First, there
are certain factors that influence the rate of adop-
tion to the forum that we are not able to take into
account. For example, some people might have al-
ready been reading the forum for a while, before
they actually decide to join the community. These
people are already exposed to the community prac-
tices, and therefore might already show more con-
formity in the beginning than others.
Second, our experiments involved one online
community targeting a very specific topic. Due to
the nature of the topic, most of the active users come
from a small subpopulation (mostly women between
40-60 years). Therefore, it is a question how well
these results can be applied to other online commu-
nities.
As a future application, a model that can capture
these changes could be used in research related to
commitment in online communities.
7 Conclusion
It is widely accepted that persistent language change
in individuals occurs over time as a result of the
accumulation of local processes of accommodation.
Although previous research has looked at accommo-
dation within short periods of time, including recent
research on social media data, persistent language
change as a result of longer term involvement in an
online community is still an understudied area.
In this paper we have presented research aiming to
close this gap. We have analyzed data from a large
online breast cancer forum. Analyzing data of long
time members, we found strong patterns indicating
language changes as these members participated in
the community, especially over the course of their
first year of participation.
We then presented a regression approach to pre-
dict how long a person has been a member of the
community. Long time participants were character-
ized by showing more social behavior. Furthermore,
they used more forum specific language, such as cer-
tain abbreviations and ways of greeting. Due to the
nature of our dataset, language was also influenced
by external factors such as changes in the cancer pro-
cess of individuals.
Although our observations are intuitive and agree
with observations in previous, related literature re-
garding socialization in communities, it is still a
question whether our observations generalize to
other online communities.
In our current work we have looked at changes
across users and across contexts. However, it is well
known that individuals adapt their language depend-
ing on local interactions. Thus, a next step would
be to model the process by which local accommoda-
tion accumulates and results in long term language
change.
Acknowledgments
The authors would like to thank Michael Heilman
for the regression code and Noah Smith for ideas for
the regression experiments. This work was funded
by NSF grant IIS-0968485.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of the 24th international conference on Machine
learning, ICML ?07, pages 33?40, New York, NY,
USA. ACM.
Justine Cassell and Dona Tversky. 2005. The language
of online intercultural community formation. Journal
of Computer-Mediated Communication, 10:16?33.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! linguistic
style accommodation in social media. In Proceedings
of WWW.
Penelope Eckert and John R. Rickford. 2001. Style and
Sociolinguistic Variation. Cambridge: University of
Cambridge Press.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 824?831, Prague, Czech Republic, June.
Association for Computational Linguistics.
Howard Giles, Donald M. Taylor, and Richard Bourhis.
1973. Towards a theory of interpersonal accommoda-
tion through language: some canadian data. Language
in Society, 2(02):177?192.
Amy L. Gonzales, Jeffrey T. Hancock, and James W. Pen-
nebaker. 2010. Language style matching as a predic-
tor of social dynamics in small groups. Communica-
tion Research, 37(1):3?19, February.
84
David Huffaker, Joseph Jorgensen, Francisco Iacobelli,
Paul Tepper, and Justine Cassell. 2006. Computa-
tional measures for language similarity across time
in online communities. In Proceedings of the HLT-
NAACL 2006 Workshop on Analyzing Conversations
in Text and Speech, ACTS, pages 15?22, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Mukund Jha and Noe?mie Elhadad. 2010. Cancer stage
prediction based on patient online discourse. In Pro-
ceedings of the 2010 Workshop on Biomedical Natu-
ral Language Processing, BioNLP ?10, pages 64?71,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
William Labov. 2010a. Principles of Linguistic Change,
Volume I, Internal Factors. Wiley-Blackwell.
William Labov. 2010b. Principles of Linguistic Change,
Volume I, Social Factors. Wiley-Blackwell.
Wan S. E. Lam. 2008. Language socialization in on-
line communities. In Nancy H. Hornberger, editor, En-
cyclopedia of Language and Education, pages 2859?
2869. Springer US.
Jean Lave and Etienne Wenger. 1991. Situated Learn-
ing. Legitimate peripheral participation. Cambridge:
University of Cambridge Press.
Dong Nguyen, Elijah Mayfield, and Carolyn P. Rose.
2010. An analysis of perspectives in interactive set-
tings. In Proceedings of the 2010 KDD Workshop on
Social Media Analytics.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction.
John C. Paolillo. 2001. Language variation on internet
relay chat: A social network approach. Journal of So-
ciolinguistics, 5:180?213.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis, 2001. Linguistic Inquiry and Word Count
(LIWC): A Computerized Text Analysis Program.
Tom Postmes, Russell Spears, and Martin Lea. 2000.
The formation of group norms in computer-mediated
communication. Human Communication Research,
26(3):341?371.
Lauren E. Scissors, Alastair J. Gill, Kathleen Geraghty,
and Darren Gergle. 2009. In cmc we trust: the role
of similarity. In Proceedings of the 27th international
conference on Human factors in computing systems,
CHI ?09, pages 527?536, New York, NY, USA. ACM.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
85
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115?123,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Author Age Prediction from Text using Linear Regression
Dong Nguyen Noah A. Smith Carolyn P. Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dongn,nasmith,cprose}@cs.cmu.edu
Abstract
While the study of the connection between
discourse patterns and personal identification
is decades old, the study of these patterns us-
ing language technologies is relatively recent.
In that more recent tradition we frame author
age prediction from text as a regression prob-
lem. We explore the same task using three
very different genres of data simultaneously:
blogs, telephone conversations, and online fo-
rum posts. We employ a technique from do-
main adaptation that allows us to train a joint
model involving all three corpora together as
well as separately and analyze differences in
predictive features across joint and corpus-
specific aspects of the model. Effective fea-
tures include both stylistic ones (such as POS
patterns) as well as content oriented ones. Us-
ing a linear regression model based on shallow
text features, we obtain correlations up to 0.74
and mean absolute errors between 4.1 and 6.8
years.
1 Introduction
A major thrust of research in sociolinguistics is to
understand the connection between the way peo-
ple use language and their community membership,
where community membership can be construed
along a variety of dimensions, including age, gen-
der, socioeconomic status and political affiliation. A
person is a member of a multiplicity of communi-
ties, and thus the person?s identity and language are
influenced by many factors.
In this paper we focus on the relationship between
age and language use. Recently, machine learning
methods have been applied to determine the age of
persons based on the language that they utter. Stud-
ies of the stylistic and content-based features that
predict age or other personal characteristics yield
new insights into the connection between discourse
and identity. However, that connection is known to
be highly contextual, such as whether the data were
collected synchronously or asynchronously, through
typed or spoken interaction, or whether participants
can see one another or not. Recent work in the area
of domain adaptation raises awareness about the ef-
fect of contextual factors on the generality of text
prediction models.
Our first contribution to this literature is an in-
vestigation of age prediction using a multi-corpus
approach. We present results and analysis across
three very different corpora: a blog corpus (Schler
et al, 2006), a transcribed telephone speech corpus
(Cieri et al, 2004) and posts from an online forum
on breast cancer. By using the domain adaptation
approach of Daume? III (2007), we train a model on
all these corpora together and separate the global
features from corpus-specific features that are asso-
ciated with age.
A second contribution is the investigation of age
prediction with age modeled as a continuous vari-
able rather than as a categorical variable. Most
prior research on age prediction has framed this as a
two-class or three-class classification problem (e.g.,
Schler et al, 2006 and Garera and Yarowsky, 2009).
In our work, modeling age as a continuous variable
is interesting not only as a more realistic representa-
tion of age, but also for practical benefits of joint
modeling of age across corpora since the bound-
115
aries for discretizing age into a categorical variable
in prior work have been chosen heuristically and in
a corpus-dependent way, making it hard to compare
performance across different kinds of data.
In the remainder of the paper, we first discuss re-
lated work and present and compare the different
datasets. We then outline our approach and results.
We conclude with discussion and future work.
2 Related work
Time is an important factor in sociolinguistic analy-
sis of language variation. While a thorough review
of this work is beyond the scope of this paper, Eckert
(1996) gives an overview of the literature on age as
a sociolinguistic variable. Linguistic variation can
occur as an individual moves through life, or as a re-
sult of changes in the community itself as it moves
through time. As an added complexity, Argamon et
al. (2007) found connections between language vari-
ation and age and gender. Features that were used
with increasing age were also used more by males
for any age. Features that were used with decreas-
ing age were used more by females. In other work,
the same features that distinguish male and female
writing also distinguish non-fiction and fiction (Arg-
amon et al, 2003). Thus, the separate effects of age,
time period, gender, topic, and genre may be diffi-
cult to tease apart in naturalistic data where many of
these variables are unknown.
Recently, machine learning approaches have been
explored to estimate the age of an author or speaker
using text uttered or written by the person. This
has been modeled as a classification problem, in a
similar spirit to sociolinguistic work where age has
been investigated in terms of differences in distri-
butions of characteristics between cohorts. In the
sociolinguistic literature, cohorts such as these are
determined either etically (arbitrary, but equal age
spans such as decades) or emically (related to life
stage, such as adolescence etc.). In machine learn-
ing research, these cohorts have typically been deter-
mined for practical reasons relating to distribution of
age groups within a corpus, although the boundaries
sometimes have also made sense from a life stage
perspective. For example, researchers have mod-
eled age as a two-class classification problem with
boundaries at age 40 (Garera and Yarowsky, 2009)
or 30 (Rao et al, 2010). Another line of work has
looked at modeling age estimation as a three-class
classification problem (Schler et al, 2006; Goswami
et al, 2009), with age groups of 13-17, 23-27 and
33-42. In addition to machine learning experiments,
other researchers have published statistical analyses
of differences in distribution related to age and lan-
guage and have found similar patterns.
As an example of one of these studies, Pen-
nebaker and Stone (2003) analyzed the relationship
between language use and aging by collecting data
from a large number of previous studies. They
used LIWC (Pennebaker et al, 2001) for analysis.
They found that with increasing age, people tend to
use more positive and fewer negative affect words,
more future-tense and less past-tense, and fewer
self-references. Furthermore, a general pattern of
increasing cognitive complexity was seen. Barbieri
(2008) uses key word analysis to analyze language
and age. Two groups (15?25 and 35?60) were com-
pared. Analysis showed that younger speakers? talk
is characterized by slang and swear words, indica-
tors of speaker stance and emotional involvement,
while older people tend to use more modals.
Age classification experiments have been con-
ducted on a wide range of types of data, in-
cluding blogs (Schler et al, 2006; Goswami et
al., 2009), telephone conversations (Garera and
Yarowsky, 2009), and recently Twitter (Rao et al,
2010). Effective features were both content fea-
tures (such as unigrams, bigrams and word classes)
as well as stylistic features (such as part-of-speech,
slang words and average sentence length). These
separate published studies present some common-
alities of findings. However, based on these re-
sults from experiments conducted on very different
datasets, it is not possible to determine how gener-
alizable the models are. Thus, there is a need for an
investigation of generalizability specifically in the
modeling of linguistic variation related to age, which
we present in this paper.
Age classification from speech data has been of
interest for many years. Recently, age regression us-
ing speech features has been explored (Spiegl et al,
2009). Spiegel?s system obtained a mean absolute
error of approximately 10 years using support vec-
tor regression. Van Heerden et al (2010) explore
combining regression estimates to improve age clas-
116
sification. As far as we are aware, we are the first to
publish results from a regression model that directly
predicts age using textual features.
3 Data description
We explore three datasets with different characteris-
tics. The data was divided into a training, develop-
ment and test set. Statistics are listed in Table 1.
3.1 Blog corpus
In August 2004 Schler et al (2006) crawled blogs
from blogger.com. Information such as gen-
der and age were provided by the users in their re-
spective profiles. Users were divided into three age
groups, and each group had an equal number of fe-
male and male bloggers. In our experiments, ev-
ery document consists of all posts from a particular
blogger.
3.2 Fisher telephone corpus
The Fisher corpus (Cieri et al, 2004) contains tran-
scripts of telephone conversations. People were ran-
domly assigned to pairs, and for (almost) every per-
son, characteristics such as gender and age were
recorded. Furthermore, for each conversation a topic
was assigned. The data was collected beginning De-
cember 2002 and continued for nearly one year. In
our experiments, we aggregate the data for each per-
son.
3.3 Breast cancer forum
We drew data from one of the most active online fo-
rums for persons with breast cancer.1 All posts and
user profiles of the forum were crawled in January
2011. Only a small proportion of users had indicated
their age in their profile. We manually annotated the
age of approximately 200 additional users with less
common ages by looking manually at their posts. An
author?s age can often be annotated because users
tend to make references to their age when they intro-
duce themselves or when telling their treatment his-
tory (e.g., I was diagnosed 2 years ago when I was
just 38). Combining this with the date of the specific
post, a birth year can be estimated. Because a per-
son?s data can span multiple years, we aggregate all
the data per year for each person. Each person was
1http://community.breastcancer.org
Figure 1: Comparison of age frequency in datasets.
0
500
1000
1500
2000
2500
10 20 30 40 50 60 70 80 90
F
re
qu
en
cy
Age
Blogs
Fisher
Cancer
assigned randomly to one of the data splits, to make
sure all documents representing the same person ap-
peared in only one split. The dataset contains posts
from October 2002 until January 2011.
3.4 Dataset comparison and statistics
The datasets differ in several respects: specificity
(general topics versus breast cancer), modality of in-
teraction (telephone conversations versus online fo-
rum versus blog post), age distribution, and amount
of data per person. The blog and Fisher dataset con-
tain approximately equal amounts of males and fe-
males, while the breast cancer dataset is heavily bi-
ased towards women.
A comparison of the age distributions of the three
corpora is given in Figure 1. The Fisher dataset
has the most uniform distribution across the ages,
while the blog data has a lot of young persons and
the breast cancer forum has a lot of older people.
The youngest person in our dataset is 13 years old
and the oldest is 88. Note that our blog corpus con-
tains gaps between different age categories, which
is an artifact of the experimental approach used by
the people who released this dataset (Schler et al,
2006).
Because all datasets were created between 2002
and 2011, we are less likely to observe results due to
cohort effects (changes that occur because of collec-
tive changes in culture, such as use of the Internet).
117
Table 1: Datasets statistics.
Blogs Fisher Cancer
Data #docs avg #tokens #docs avg #tokens #docs avg #tokens #persons
Training 9,660 13,042 5,957 3,409 2,330 22,719 1,269
Development 4,830 13,672 2,977 3,385 747 32,239 360
Test 4,830 13,206 2,980 3,376 797 26,952 368
4 Experimental setup
4.1 Linear regression
Given an input vector x ? Rm, where x1, . . . , xm
represent features (also called independent variables
or predictors), we find a prediction y? ? R for the age
of a person y ? R using a linear regression model:
y? = ?0 + x>? where ?0 and ? are the parame-
ters to estimate. Usually, the parameters are learned
by minimizing the sum of squared errors. In order
to strive for a model with high explanatory value,
we use a linear regression model with Lasso (also
called L1) regularization (Tibshirani, 1996). This
minimizes the sum of squared errors, but in addition
adds a penalty term ?
?m
j=1 |?j |. ? is a constant and
can be found by optimizing over the development
data. As a result, this method delivers sparse mod-
els. We use OWLQN to optimize the regularized
empirical risk (Andrew and Gao, 2007; Gao et al,
2007). We evaluate the models by reporting the cor-
relation and mean absolute error (MAE).
4.2 Joint model
To discover which features are important across
datasets and which are corpus-specific, we train a
model on the data of all corpora using the feature
representation proposed by Daume? III (2007). Using
this model, the original feature space is augmented
by representing each individual feature as 4 new fea-
tures: a global feature and three corpus-specific fea-
tures, specifically one for each dataset. Thus for ev-
ery feature f , we now have fglobal , fblogs , ffisher and
fcancer . For every instance, only the global and the
one specific corpus feature are set. For example for
a particular feature value xj for the blog dataset we
would have ?xj , xj , 0, 0?. If it would appear in the
cancer dataset we would have ?xj , 0, 0, xj?. Because
the resulting model using L1 regression only selects
a small subset of the features, some features may
only appear either as global features or as corpus-
specific features in the final model.
4.3 Overview different models
Besides experimenting with the joint model, we are
also interested in the performance using only the dis-
covered global features. This can be achieved by ap-
plying the weights for the global features directly as
learned by the joint model, or retraining the model
on the individual datasets using only the global fea-
tures. In summary, we have the following models:
? INDIV: Models trained on the three corpora in-
dividually.
? JOINT: Model trained on all three corpora with
features represented as in Daume? III (2007).
? JOINT-Global: Using the learned JOINT
model but only keeping the global features.
? JOINT-Global-Retrained: Using the discov-
ered global features by the JOINT model, but
retrained on each specific dataset.
4.4 Features
4.4.1 Textual features
We explore the following textual features; all fea-
tures are frequency counts normalized by the length
(number of tokens) of the document.
? Unigrams.
? POS unigrams and bigrams. Text is tagged us-
ing the Stanford POS tagger (Toutanova et al,
2003).
? LIWC (Pennebaker et al, 2001). This is a word
counting program that captures word classes
such as inclusion words (LIWC-incl: ?with,?
?and,? ?include,? etc.), causation words (LIWC-
cause: ?because,? ?hence,? etc.), and stylis-
tic characteristics such as percentage of words
longer than 6 letters (LIWC-Sixltr).
118
Figure 2: Scatterplot of true and predicted age.
-20
-10
0
10
20
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90
P
re
di
ct
ed
ag
e
True age
4.4.2 Gender
Because the gender of a person also influences
how age is reflected in a person?s text or speech (e.g.
Argamon et al (2007) ), we add a binary feature for
the gender of the person (Male = 1, Female = 0).
This feature is only known for the blog and Fisher
dataset. For the breast cancer dataset the gender is
not known, but we assume they are all women.
5 Results and discussion
As discussed, we experiment with four different
models. We explore three different feature sets: only
unigrams, only POS, and the full feature set. The re-
sults are presented in Table 2. The most important
features using the JOINT model with the full feature
set (condition 10) are presented in Table 3.
5.1 Quantitative analysis
Overall, similar performance is obtained on the
Fisher and blog datasets. The highest correlations
were achieved on the Fisher dataset, with a best cor-
relation of r = 0.742. This gives an r2 value of
0.551, indicating that 55% of the variance can be
explained by the model. However, a higher mean
absolute error (MAE) was observed compared to
the blog dataset. This may be caused by the larger
spread in distribution of ages in the Fisher dataset.
The lowest correlations were observed on the cancer
dataset. This is probably caused by the small amount
of training instances, the noisy text, and the fact that
the ages lie very close to each other.
Overall, the joint model using all features per-
formed best (condition 10). In Figure 2 a plot is
presented that relates the true and predicted ages for
this condition. We find that for the high ages there
are more instances with high errors, probably caused
by the small amount of training data for the extreme
ages.
We find the correlation metric to be very sensitive
to the amount of data. For example, when comput-
ing the correlation over the aggregated results of all
corpora, we get a much higher correlation (0.830),
but the MAE (5.345) is closer to that computed over
the individual datasets. However, the MAE is de-
pendent on the age distributions in the corpus, which
can be observed by contrasting the MAE on the runs
of the Fisher and cancer dataset. This thus suggests
that these two measures are complementary and both
are useful as evaluation metrics for this task.
For most experiments the joint models show im-
provement over the individual models. Returning
to our question of generality, we can make several
observations. First, performance decreases signif-
icantly when only using the global features (com-
paring JOINT and JOINT-Global-retrained), con-
firming that corpus-specific features are important.
Second, learned weights of global features are rea-
sonably generalizable. When using the full feature
set, retraining the global features on the corpora di-
rectly only gives a slight improvement (e.g. com-
pare conditions 11 and 12). Third, the bias term
(?0) is very corpus-specific and has a big influence
on the MAE. For example, when comparing condi-
tions 11 and 12, the correlations are very similar but
the MAEs are much lower when the model is re-
trained. This is a result of adjusting the bias term
to the specific dataset. For example the bias term of
the model trained on only the blog dataset is 22.45,
compared to the bias of 46.11 when trained on the
cancer dataset.
In addition, we observe better performance in the
cancer dataset when retraining the model using only
the global features compared to the initial feature
set. This suggests that using the global features
might have been an effective method for feature se-
lection to prevent overfitting on this small dataset.
119
Table 2: Results on the test set, reported with Pearson?s correlation (r) and mean absolute error (MAE).
Blogs Fisher Cancer
ID Model #Features r MAE r MAE r MAE
Unigrams
1 INDIV 56,440 0.644 4.236 0.715 7.145 0.426 7.085
2 JOINT 56,440 0.694 4.232 0.723 7.066 0.530 6.537
3 JOINT-Global 656 0.605 5.800 0.628 10.370 0.461 16.632
4 JOINT-Global-retrained 656 0.658 4.409 0.675 7.529 0.498 6.797
POS
5 INDIV 4,656 0.519 5.095 0.553 8.635 0.150 7.699
6 JOINT 4,656 0.563 4.899 0.549 8.657 0.035 8.449
7 JOINT-Global 110 0.495 6.332 0.390 12.232 0.151 19.454
8 JOINT-Global-retrained 110 0.519 5.095 0.475 9.187 0.150 7.699
All features
9 INDIV 61,416 0.699 4.144 0.731 6.926 0.462 6.943
10 JOINT 61,416 0.696 4.227 0.742 6.835 0.535 6.545
11 JOINT-Global 510 0.625 5.295 0.650 11.982 0.459 17.472
12 JOINT-Global-retrained 510 0.629 4.633 0.651 7.862 0.490 6.876
5.2 Feature analysis
The most important features using the JOINT model
with the full feature set (condition 10) are presented
in Table 3. Features associated with a young age
have a negative weight, while features associated
with old age have a positive weight. For almost all
runs and evaluation metrics the full feature set gives
the best performance. However, looking at the per-
formance increase, we observe that the unigram only
baseline gives strong results. Overall, both stylistic
as well as content features are important. For con-
tent features, we see that references to family (e.g.,
?granddaughter? versus ?son?) as well as to daily
life (e.g., ?school? versus ?job?) are very predictive.
Although the performance using only POS tags
is lower, reasonable correlations are obtained using
only POS tags. In Table 3 we see many POS features
associated with old age. This is confirmed when an-
alyzing the whole feature set selected by the JOINT
model (condition 10). In this model 510 features are
nonzero, 161 of which are POS patterns. Of these,
43 have a negative weight, and 118 have a positive
weight. This thus again suggests that old age is char-
acterized more by syntactic effects than young age.
Most important features are consistent with obser-
vations from previous research. For example, in the
Fisher dataset, similar to findings from classification
experiments by Garera and Yarowsky (2009), the
word ?well? is most predictive of older age. ?Like?
has the highest association with younger age. This
agrees with observations by Barbieri (2008). As
was also observed by others, ?just? is highly associ-
ated with young persons. Consistent with literature
that males generally ?sound older? than they truly
are (Argamon et al, 2007, and others), our male
speaker feature has a high negative weight. And, in
agreement with previous observations, younger peo-
ple use more swear words and negative emotions.
The differences between the corpora are reflected
in the features that have the most weight. The effec-
tive features in the Fisher dataset are more typical
of conversational settings and effective features in
the cancer dataset are about being pregnant and hav-
ing kids. Features associated with the blog dataset
are typical of the story telling nature of many blog
posts.
Comparing the extracted corpus-specific features
with the features selected when training on the indi-
vidual corpora, we do see evidence that the JOINT
model separates general versus specific features.
For example, the most important features associ-
ated with young people in the cancer dataset when
only training on the cancer dataset (condition 9)
are: LIWC - Emoticons, LIWC - Pronoun, definitely,
120
Table 3: Most important features in the JOINT model with all features (condition 10).
(a) Features for younger people.
Global Blogs Fisher Cancer
like -1.295 you -0.387 actually -0.457 LIWC-Emotic. -0.188
gender-male -0.539 went -0.310 mean -0.343 young -0.116
LIWC-School -0.442 fun -0.216 everyone -0.273 history -0.092
just -0.354 school -0.192 definitely -0.273 mom -0.087
LIWC-Anger -0.303 but -0.189 mom -0.230 ultrasound -0.083
LIWC-Cause -0.290 LIWC-Comma -0.152 student -0.182 kids -0.071
mom -0.290 go -0.142 pretty -0.137 age -0.069
so -0.271 POS-vbp nn -0.116 POS-lrb cd -0.135 mum -0.069
definitely -0.263 thats -0.115 LIWC-Swear -0.134 POS-sym rrb -0.069
LIWC-Negemo -0.256 well -0.112 huge -0.126 discharge -0.063
(b) Features for older people.
Global Blogs Fisher Cancer
years 0.601 LIWC - Job 0.514 well 1.644 POS - dt 0.713
POS - dt 0.485 son 0.267 LIWC - WC 0.855 POS - md vb 0.450
LIWC - Incl 0.483 kids 0.228 POS - uh prp 0.504 POS - nn 0.369
POS - prp vbp 0.337 years 0.178 retired 0.492 LIWC - Negate 0.327
granddaughter 0.332 work 0.147 POS - prp vbp 0.430 POS - nn vbd 0.321
grandchildren 0.293 wife 0.142 said 0.404 POS - nnp 0.304
had 0.277 husband 0.137 POS - cc fw 0.358 us 0.287
daughter 0.272 meds 0.112 son 0.353 all 0.266
grandson 0.245 dealing 0.096 subject 0.319 good 0.248
ah 0.243 weekend 0.094 POS - cc cc 0.316 POS - cc nn 0.222
mom, mum, really, LIWC - Family, LIWC - Humans,
thank, and she. The difference in age distribution is
reflected in the feature weights. In the JOINT model,
the bias term is 24.866. Because most of the persons
in the cancer dataset are older, the features associ-
ated with young age in the cancer dataset have much
lower weights compared to the other datasets.
Because our goal is to compare features across
the corpora, we have not exploited corpus-specific
features. For example, thread or subforum features
could be used for the breast cancer corpus, and for
the Fisher dataset, one could add features that ex-
ploit the conversational setting of the data.
5.3 Examples
We present examples of text of younger and older
persons and connect them to the learned model.
The examples are manually selected to illustrate
strengths and weaknesses of the model.
5.3.1 Younger people
We first present some examples of text by young
persons. The following is an example of a 17-year
old in the blog dataset, the system predicted this to
be from a 16.48-year-old:
I can?t sleep, but this time I have school
tommorow, so I have to try I guess. My
parents got all pissed at me today because
I forgot how to do the homework [...]. Re-
ally mad, I ended it pissing off my mom
and [...] NOTHING! Damn, when I?m at
my cousin?s I have no urge to use the com-
puter like I do here, [...].
This example matches with important features de-
termined by the system, containing references to
school and parents, and usage of swearing and anger
words.
121
The following are selected turns (T) by a 19-year
old (system prediction: 17.37 years) in a conversa-
tion in the Fisher dataset.
T: yeah it?s too i just just freaked out [...]
T: that kinda sucks for them
T: they were they were like going crazy
[...]
T: it?s like against some law to like
The text has many informal words such as ?kinda?
and well as many occurrences of the word ?like.?
This example is from a 19-year old from the can-
cer dataset. The system?s prediction was far off, es-
timating an age of 35.48.
Im very young and an athlete and I really
do not want to look disfigured, especially
when I work so hard to be fit. I know it
sounds shallow, but Im young and hope
to [...] my husband one day :) [...] My
grandmother died of breast cancer at 51,
and my mother is currently dealing with a
cancerous tumor on her ovaries.
Besides explicit references to being ?very young,?
the text is much more formal than typical texts, mak-
ing it a hard example.
5.3.2 Older people
The following is a snippet from a 47-year-old
(system prediction: 34.42 years) in the blog dataset.
[...]In the weeks leading up to this meet-
ing certain of the managers repeatedly as-
serted strong positions. [...] their previous
(irresponsible yet non-negotiable) opin-
ions[...] Well, today?s my first Father?s
day [...]. Bringing a child into this world
is quite a responsibility especially with all
the fears and challenges we face. [...]
This matches some important features such as ref-
erences to jobs, as well as having kids. The many
references to the word ?father? in the whole text
might have confused the model. The following are
selected turns (T) by a 73-year old (system predic-
tion: 73.26 years) in a conversation in the Fisher
dataset.
T: ah thoughts i?m retired right now
T: i i really can?t ah think of anyth- think
of i would ah ah change considerably ah
i?m i?m very i?ve been very happily mar-
ried and i have ah three children and six
grandchildren
T: yeah that?s right well i i think i would do
things more differently fair- fairly recently
than a long time ago
This example contains references to being retired
and having grandchildren, as well as many usages
of ?ah?. The following is an example of a 70-year
old (system prediction: 71.53 years) in the cancer
dataset.
[...] I was a little bit fearful of having
surgery on both sides at once (reduction
and lift on the right, tissue expander on
the left) [...] On the good side, my son
and family live near the plastic surgeon?s
office and the hospital, [...], at least from
my son and my granddaughter [...]
6 Conclusion
We presented linear regression experiments to pre-
dict the age of a text?s author. As evaluation metrics,
we found correlation as well as mean absolute er-
ror to be complementary and useful measures. We
obtained correlations up to 0.74 and mean absolute
errors between 4.1 and 6.8 years. In three different
corpora, we found both content features and stylis-
tic features to be strong indicators of a person?s age.
Even a unigram only baseline already gives strong
performance and many POS patterns are strong in-
dicators of old age. By learning jointly from all of
the corpora, we were able to separate generally ef-
fective features from corpus-dependent ones.
Acknowledgments
The authors would like to thank the anonymous review-
ers for feedback, Michael Heilman for the regression
code, and other members of the ARK group for help run-
ning the experiments. This work was funded by NSF
grants CAREER IIS-1054319 to N.A.S. and IIS-0968485
to C.P.R.
122
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proc. of
ICML.
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat R. Shimoni. 2003. Gender, genre, and writing
style in formal written texts. Text, 23(3):321?346.
Shlomo Argamon, Moshe Koppel, James Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
age, gender, and the varieties of self-expression.
Federica Barbieri. 2008. Patterns of age-based linguistic
variation in American English. Journal of Sociolin-
guistics, 12(1):58?88.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proc. of LREC, pages
69?71.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proc. of ACL.
Penelope Eckert. 1996. Age as a sociolinguistic variable.
In The Handbook of Sociolinguistics. Oxford: Black-
well.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proc. of ACL.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. of ACL-IJCNLP.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers? age and gen-
der. In Proc. of ICWSM.
James W. Pennebaker and Lori D. Stone. 2003. Words
of wisdom: Language use over the lifespan. Journal
of Personality and Social Psychology, 85:291?301.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis, 2001. Linguistic Inquiry and Word Count
(LIWC): A Computerized Text Analysis Program.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. of SMUC.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James Pennebaker. 2006. Effects of age and gender
on blogging. In Proceedings of the AAAI Spring Sym-
posia on Computational Approaches to Analyzing We-
blogs.
Werner Spiegl, Georg Stemmer, Eva Lasarcyk, Varada
Kolhatkar, Andrew Cassidy, Blaise Potard, Stephen
Shum, Young Chol Song, Puyang Xu, Peter Beyer-
lein, James Harnsberger, and Elmar No?th. 2009. Ana-
lyzing features for automatic age estimation on cross-
sectional data. In Proc. of INTERSPEECH.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society Series B (Methodological), 58(1):267?288.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
NAACL-HLT.
Charl van Heerden, Etienne Barnard, Marelie Davel,
Christiaan van der Walt, Ewald van Dyk, Michael
Feld, and Christian Muller. 2010. Combining re-
gression and classification methods for improving au-
tomatic speaker age recognition. In Proc. of ICASSP.
123
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 227?238,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Comparing Triggering Policies for Social Behaviors 
 
 
Rohit Kumar, Carolyn P. Ros? 
Language Technologies Institute, Carnegie Mellon University 
Gates Hillman Center, 5000 Forbes Avenue, 
Pittsburgh, PA, USA 15213 
rohitk , cprose @ cs.cmu.edu 
 
 
 
 
 
 
Abstract 
Instructional efficacy of automated Con-
versational Agents designed to help small 
groups of students achieve higher learning 
outcomes can be improved by the use of 
social interaction strategies. These strate-
gies help the tutor agent manage the atten-
tion of the students while delivering useful 
instructional content. Two technical chal-
lenges involving the use of social interac-
tion strategies include determining the 
appropriate policy for triggering these 
strategies and regulating the amount of so-
cial behavior performed by the tutor. In this 
paper, a comparison of six different trigger-
ing policies is presented. We find that a 
triggering policy learnt from human beha-
vior in combination with a filter that keeps 
the amount of social behavior comparable 
to that performed by human tutors offers 
the most effective solution to the these 
challenges. 
1 Introduction 
While Conversational Agents have been shown to 
be an effective technology for delivering instruc-
tional content to students in a variety of learning 
domains and situations (Grasser et. al., 2005; Ku-
mar et. al., 2007; Arnott et. al., 2008), it has been 
observed that students are more likely to ignore 
and abuse the tutor in a collaborative learning set-
ting (with 2 or more students) compared to the case 
of one-on-one tutoring (Bhatt et. al., 2004; Kumar 
et. al., 2007). In our prior work (Kumar et. al., 
2010a), we have addressed this problem by em-
ploying agents that are capable of performing both 
instructional behavior as well as social behavior. In 
our initial implementation, the social behavior per-
formed by these agents was composed of eleven 
social interaction strategies that were triggered by 
a set of hand crafted rules (Kumar and Ros?, 
2010b). Section 2 provides additional details about 
these strategies. 
Comparison between the social behavior trig-
gered by our hand crafted rules and that triggered 
by a human tutor revealed significant perception 
benefits (more likeable, higher task satisfaction, 
etc.) for the human triggering policy. Also, the stu-
dents in a wizard-of-oz condition who interacted 
with the tutors whose social behaviors were trig-
gered by humans had better learning outcomes 
(0.93?) with respect to a No social behavior base-
line. The condition where students interacted with 
the rule-based automated tutors was also signifi-
cantly better (0.71?) than the No social behavior 
baseline in terms of learning outcomes. While the 
learning outcomes of the rule-based tutors was not 
significantly worse than the human tutor, in com-
bination with the perception outcomes, we see the 
potential for further improvement of conversation-
al agents by employing a better triggering policy. 
Building on these prior results, in this paper we 
explore a way to improve the effectiveness of so-
cially capable tutor agents that uses a triggering 
policy learnt from a corpus of human behavior. 
The underlying hypothesis of this approach is that 
a human-like triggering policy would lead to im-
provements in the agent?s performance and percep-
227
tion ratings compared to a rule-based triggering 
policy. As a first step towards verifying this hypo-
thesis, we learnt a collection of triggering policies 
from a corpus of human behavior. While the focus 
of this paper is to evaluate the most human-like 
triggering policy learnt from data in terms of its 
perception benefits and learning outcomes, Section 
4 summarizes our efforts on learning triggering 
policies. 
Before we discuss the details of the evaluation 
we conducted, Section 3 presents an analysis of 
mediating factors that provides insights into the 
reasons behind the effectiveness of social behavior. 
The design and procedure of the user study we 
conducted to evaluate the learnt triggering policies 
is described in Section 5. Finally, Section 6 dis-
cusses the results of this evaluation. 
2 Social Interaction Strategies 
In our prior work (Kumar et. al., 2010; Ai et. al., 
2010; Kumar et. al., 2011), we have developed and 
evaluated automated tutors for two different educa-
tional domains equipped with eleven social interac-
tion strategies. These strategies, listed in Table 1, 
correspond to three positive socio-emotional inte-
raction categories identified by Bales (1950): 
Showing Solidarity, Showing Tension Release and 
Agreeing. 
Appendix A shows excerpts of an interaction 
between three students and a tutor during a college 
freshmen mechanical engineering learning activity.  
The shaded turns demonstrate realizations of some 
of the eleven social interaction strategies. 
Turns 7-12 shows the tutor initiating and partic-
ipating in group formation using Strategy 1a (Do 
Introductions) by greeting the students and asking 
for their names. In turn 53, the tutor is employing 
Strategy 3b (Show Comprehension / Approval) in 
response to a student opinion expressed in turn 52. 
When one of the students becomes inactive in the 
interaction, the tutor uses strategy 1e (Encourage) 
realized as a targeted prompt shown in turn 122 to 
elicit a response from the inactive student. Turn 
148 demonstrates Strategy 1d (Complement / 
Praise) to appreciate student participation in a con-
ceptual tutoring episode that concluded at turn 147. 
Finally, turn 152 shows a realization of Strategy 2c 
(Express Enthusiasm, Elation, Satisfaction) which 
is tied to either the start or the end of lengthy prob-
lem solving steps in the learning activity such as 
calculating the outcome of certain design choices 
made by the students during the learning activity. 
 
1. Showing Solidarity 
Raises other's status, gives help, reward 
1a. Do Introductions 
Introduce and ask names of all participants 
1b. Be Protective & Nurturing 
Discourage teasing 
1c. Give Re-assurance 
When student is discontent, asking for help 
1d. Complement / Praise 
To acknowledge student contributions 
1e. Encourage 
When group or members are inactive 
1f. Conclude Socially 
 
2. Showing Tension Release 
Jokes, laughs, shows satisfaction 
2a. Expression of feeling better 
After periods of tension, work pressure 
2b. Be cheerful 
2c. Express Enthusiasm, Elation, Satisfaction 
On completing significant steps of the task 
 
3. Agreeing 
Shows passive acceptance, understands, 
concurs, complies 
3a. Show Attention 
To student ideas as encouragement 
3b. Show Comprehension / Approval 
To student opinions and orientations 
Table 1. Social Interaction Strategies for three  
social-emotional interaction categories 
3 Effect of Social Behaviors 
We have observed the the use of social behavior in 
collaborative learning settings can help in improv-
ing learning outcomes (Kumar et. al., 2010a). This 
observation is not necessarily consistent with intui-
tion. On the one hand, we could attribute this to the 
ability to manage the user?s (student) attention to-
wards the agent (tutor) by displaying the social 
behavior. At the same time, we could argue against 
the use of social behavior by considering it as a 
potential distraction from the task. In an earlier 
experiment (Kumar et. al., 2010c), we found that it 
is important to keep the amount of social behavior 
at an optimal level in order to observe the learning 
228
benefits of the social behavior and to avoid its dis-
traction costs. 
 
1 Tutor Are you paying attention to this? 
2 S006 of course 
3 S001 yes 
4 Tutor 
What would happen if the stress in 
our Wrench is too high? 
5 S006 fail 
6 S001 it'd fail 
7 S014 it would break. 
8 Tutor That's right. The wrench can break! 
9 Tutor 
A wrench that can break in normal 
use is not safe! 
10 S006 :( 
11 Tutor 
In general, to keep a wrench safe, 
we want to keep the stress in the 
wrench high or low? 
12 S006 lowwww 
13 S001 low 
14 Tutor Right, stress should be kept low. 
15 Tutor 
Which material property deter-
mines when the stress is too high? 
16 S001 yield strength? 
 
Table 2. Excerpt of a Conceptual Tutoring Episode 
 
In this section, we present a corpus analysis to 
discover an underlying model of how the social 
behavior affects learning outcomes. We used a 
corpus of collaborative learning interactions be-
tween teams of three or more students and a tutor. 
The interactions involve the teams performing a 
learning activity which lasts approximately 40 mi-
nutes. During this time, the tutors provide direc-
tions, hints and conceptual tutoring. Each team 
undergoes up to four conceptual tutoring episodes. 
An episode is a system-initiated conversation dur-
ing which the tutor leads the students through a 
directed line of reasoning to help them reflect upon 
a concept related to the learning activity. An ex-
cerpt of a tutoring episode discussing the relation-
ship between stress and safety is shown in Table 2. 
3.1 Coding Tutoring Episodes 
Each turn in all the tutoring episodes of the 32 inte-
ractions between a team of students and an auto-
mated tutor were annotated using a coding scheme 
described here. The tutor turns were categorized as 
either Respondable (TR) if the students were ex-
pected to the respond to that tutor turn or Not Res-
pondable (TU) otherwise. In Table 2, all the 
shaded turns are labeled as Respondable. 
 
 
Figure 1. Venn Diagram of Episode Turn Annotations 
 
Student Turns are categorized into one of three 
categories. Good turns (SG) identifies turns where 
the students are showing attention to a respondable 
tutor turn (e.g. Turn 2 & 3 in Table 2) or the stu-
dents are giving a correct or an incorrect response 
to a direct question by the tutor (e.g. Turns 5, 6, 7, 
12, 13 & 16). Counterproductive (Bad) student 
turns (SB) include students abusing the tutor or 
ignoring the tutor (e.g. talking to another student 
when the students are expected to respond to a tu-
tor turn). Student turns that are not categorized as 
Good or Bad are labeled as Other (SO). Turn 10 is 
an example of SO because it is a response to a tu-
tor turn (9) where no student response is expected. 
Figure 1 shows a Venn diagram of the different 
annotations. All five categories are mutually exclu-
sive. 
3.2 Structural Equation Modeling 
In order to discover an underlying model of how 
the use of social behavior affects student learning, 
we used a structural equation modeling (SEM) 
technique (Scheines et. al., 1994). 
Data: To measure learning outcomes, our data 
comprised of scores from pre-test and post-test 
administered to 88 students who were part of the 
32 teams whose data was annotated for this analy-
sis. We normalized the number of Good (SG) and 
Bad (SB) student turns by the number of Respond-
able (TR) tutor turns and included normalized SG 
(nSG) and normalized SB (nSB) as measures of 
interaction characteristics of each student in our 
dataset. Total number of social turns performed by 
the tutor in each interaction was included as a cha-
racteristic of social behavior displayed by the tutor. 
Finally, the total amount of time (in seconds) that 
229
the students spent on the tutoring episodes was 
included as a characteristic of the interaction quali-
ty during the tutoring episodes. 
Prior Knowledge: The only prior knowledge 
input to the model stated that the pre-test occurs 
before the post-test. 
Discovered Models: We used Tetrad IV to dis-
cover a structural equation model in the data com-
prising of 6 fields (PreTest, PostTest, nSG, nSB, 
SocialTurns, EpisodeDuration) for each of the 88 
students. Figure 2 shows the structural equation 
model discovered by Tetrad using the dataset de-
scribed above. p-Value of 0.46 for this model con-
firms the hypothesis used by Tetrad for its 
statistical analysis i.e. the model was not discov-
ered randomly. Note that unlike other statistical 
tests, SEM models built using Tetrad are evaluated 
as significant if the p-Value is greater than 0.05. 
The numbers on the arrows are correlation coeffi-
cients and the numbers on the boxes indicate mean 
values for each variable. 
 
 
 
Figure 2. SEM discovered using all 6 variables in our 
dataset 
 
Besides the obvious causal effect of PreTest 
score on PostTest score, we find that as the dura-
tion of the tutoring episodes (EpisodeDuration) 
increases, the learning outcomes deteriorate. We 
notice that an increase in the normalized number of 
Bad student turns increases EpisodeDuration indi-
cating that students who abuse or ignore the tutor 
are likely to not pay attention to the learning con-
tent presented during the tutoring episodes, hence 
prolonging the tutoring episode as the tutor tries to 
get the students through the instructional content. 
Furthermore, we observe that social behavior helps 
in counteracting the negative learning effect of Bad 
interaction behaviors of the students. Tutors that 
perform social behavior are capable of managing 
the student?s attention and get the students through 
the tutoring episode faster. 
3.3 Discussion 
The SEM analysis discussed in the previous sec-
tion helps us better understand the relationship be-
tween the use of social behavior and student 
learning in a collaborative learning setting. Let?s 
consider the duration of the tutoring episodes as an 
indicator of the students? attention to the tutor 
(higher duration lower attention). We see that 
social behavior helps in managing the students? 
attention, which may be affected negatively by 
counterproductive/bad interaction behavior from 
the students. 
Besides suggesting that social behavior could be 
a useful strategy for directing student attention, it 
also suggests that social behavior may not serve 
this function where counterproductive student be-
havior is not present or where it does not occur 
enough to negatively impact task behavior. This is 
because a minimum amount of time needs to be 
spent on each tutoring episode to deliver the in-
structional of the concept being discussed. In the 
absence of counterproductive student behavior, 
episode duration may be close to that minimum. 
Also, in an earlier analysis (Hua et. al., 2010) in 
a different learning domain where the social beha-
viors described in Section 2 were employed, we 
have observed that the number of abusive/negative 
comment made by the students about the tutor dur-
ing the interaction were significantly higher in a 
condition where the tutors performed a high 
amount of social behavior. This suggests that the 
relationship between the SocialTurns and Episo-
deDuration variables may not be linear in extreme 
cases and emphasizes the importance of perform-
ing an optimal amount of social behavior. 
4 Triggering Social Behavior 
Aside from designing, implementing and regulat-
ing the amount of social behavior performed by 
automated tutors, one of the challenges involved in 
the appropriate use of social interaction strategies 
is that of triggering these strategies only at the 
most appropriate moments during the interaction. 
Our initial implementation of these strategies 
(Kumar & Ros?, 2010b) achieved this using a set 
230
of hand crafted rules that used features such as re-
cent student turns, state of the tutoring plan, etc. 
Here we will summarize our efforts on building 
a better triggering policy using a data-driven ap-
proach that models the behavior of human tutors at 
triggering the social interaction strategies listed in 
Table 1. Using a corpus of 10 interactions between 
a group of students and partially automated tutors 
whose social behaviors were triggered by human 
tutors, we attempt to learn a triggering policy that 
predicts when the human tutors will trigger a social 
strategies. Currently, we focus on only learning a 
triggering policy that determines if a social beha-
vior should be performed. The choice of which 
behavior is performed when triggered by the policy 
is still based on the rules used in our earlier im-
plementation as discussed in Section 5.3. 
In order to compare the triggers generated by a 
policy, we use a binary sequence comparison me-
tric called kKappa (Neikrasz & Moore, 2010) de-
veloped for evaluating discourse segmentation 
approaches. The metric allows a soft penalty for 
misplacing a trigger (or a segment boundary) with-
in a window of k turns. 
We developed a large margin learning algo-
rithm following McDonald et. al. (2005) that itera-
tively learns the coefficients of a linear function in 
the feature space that separates turns where human 
tutors decided to trigger a social behavior from the 
rest of the turns. Instead of using an instance-based 
objective function (like square-loss), our algorithm 
maximizes the kKappa metric over a provided 
training set. The function learnt this way can be 
used as a triggering policy by using it at every turn 
during an interaction to predict if a human tutor 
would trigger a social behavior. We used a collec-
tion of automatically extractable features that 
represent the lexical and semantic content of recent 
student and tutor turns, current discourse state and 
activity levels of the students. 
While details of the objective evaluation of the 
various learnt triggering policies is beyond the 
scope of this paper, we found that the best per-
forming strategy (k-? = 0.13) was significantly bet-
ter than a random baseline (k-? = 0.01) as well as 
the rule based triggering policy (k-? = -0.09) used 
in our initial implementation. Also, the policy 
learnt by our algorithm outperformed policies 
learnt by algorithms such as Linear Regression (k-
? = 0.00) and Logistic Regression (k-? = 0.05) that 
use instance-based loss metrics (Hall et. al., 2009). 
5 User Study 
Here we will present an experiment we conducted 
to evaluate the effectiveness of various ways to 
trigger social behavior discussed in Section 4. This 
experiment is a step towards verifying the hypo-
thesis that a human-like triggering policy could 
outperform a rule-based triggering policy that was 
used in our earlier experiments (Kumar et. al., 
2010a). We use the same interactive situation for 
the experiment presented here as in our earlier 
work. Freshmen mechanical engineering students 
enrolled at an American university participate in a 
computer-aided engineering lab that is divided into 
three parts, i.e., Computer-Aided Design (CAD), 
Computer-Aided Analysis (CAA) and Computer-
Aided Manufacturing (CAM). Students practice 
the use of various engineering software packages 
for all three parts as they design, analyze and man-
ufacture an Aluminum wrench. Our experiment is 
conducted during the second part (CAA) of the lab. 
5.1 Procedure & Materials 
The Computer-Aided Analysis lab comprises of 
two activities. The first activity involves analyzing 
a wrench design given to the students by specify-
ing certain loading conditions and simulating the 
stresses and deformations in the wrench. Students 
are led by a teaching assistant during this activity. 
They spend approximately 25 minutes performing 
this activity. At the end of the analysis activity, the 
students see a simulation of the stress distribution 
in the body of the wrench. 
After the analysis activity, a pre-test is adminis-
tered. Each student spends 10 minutes working on 
the pre-test individually. The pre-test comprises of 
11 questions, 8 of which are multiple-choice ques-
tions and the other 3 are short essay type questions. 
The second activity of the CAA lab is a colla-
borative design activity. During this activity, stu-
dents work in teams of three. Student in the same 
team are seated in separate parts of the lab and can 
only communicate using a text-based chatroom 
application (M?hlpfordt and Wessner, 2005). The 
chatroom application also provides a shared work-
space in the form of a whiteboard. 
After the pre-test, students are given written in-
structions describing the collaborative design ac-
tivity. The instructions ask the students to design a 
better wrench in terms of ease of use, cost of mate-
rials and safety compared to the wrench they ana-
231
lyzed earlier. The students are expected to come up 
with three new designs in 40 minutes by varying 
parameters like dimensions and materials of the 
wrench. The instructions also include various for-
mulae and data that the students might need to use 
for their designs. Besides course credit, the instruc-
tions mention an additional giftcard for the team 
that comes up with the best design ($10 for each 
member of the winning team). 
Students are asked to log in to their respective 
team?s chatroom. They spend the next 40 minutes 
working on the collaborative design activity. Be-
sides the three students, the chatroom for each 
team includes an automated tutor. The tutor guides 
the students through the first two designs suggest-
ing potential choices for dimension and materials 
for each design. As the design activity progresses, 
the tutor initiates four conceptual tutoring episodes 
to help the students reflect upon underlying me-
chanical engineering concepts like stress, force, 
moment, safety, etc., that are relevant to the design 
activity. 
Our experimental manipulation happens during 
this 40 minute segment. The tutor in each team?s 
chatroom is configured to perform social behavior 
using different triggering policies as specified by 
the condition assigned to the team. The conditions 
are discussed in the next section. Irrespective of 
the condition, each team receives the 4 conceptual 
tutoring episodes. Every student performs all the 
steps of this procedure like all other students. 
At the end of the collaborative design activity, a 
post-test and a survey are administered. Students 
are asked to spend 15 minutes to first complete the 
test and then the survey. The post-test is the same 
test used for pre-test. The survey comprises of 15 
items shown in Appendix B. The students are 
asked to rate each item on a 7-point Likert scale 
ranging from Strongly Disagree (1) to Strongly 
Agree (7). The 15 items on the survey include 11 
items eliciting perception of the tutor. 9 of the 11 
items state positive aspects of the tutor (e.g. ?tutor 
was friendly?). The other 2 items stated negative 
aspects about the tutor (e.g. ?tutor?s responses got 
in the way?). Besides the items about the tutor, 2 
items elicited the student?s rating about the colla-
borative design activity. The last 2 items were 
about the student?s satisfaction with their perfor-
mance on the design task. 
In total, both the activities that are part of the 
CAA lab take approximately 1 hour 40 minutes. 
5.2 Experimental Design 
The teams participating in the experiment de-
scribed here were divided into six conditions. 
These conditions determined the triggering policy 
and the amount of social behavior performed by 
the automated tutors. Tutors in the None condition 
did not perform any social behavior. Tutors in the 
Rules condition used the same hand crafted rule-
based triggering policy employed in our earlier 
experiment (Kumar et. al., 2010a). Following the 
results from another experiment (Kumar & Ros?, 
2010c), the automated tutors in the Rules condition 
performed a moderate amount of social behavior 
(atmost 20% of all tutor turns). On average, the 
Rules policy triggered 25 social turns per interac-
tion. 
The RandomLow and RandomHigh condi-
tions used a random triggering policy with a social 
ratio filter to regulate the amount of social beha-
vior. In both the random conditions, the tutor 
would trigger social behavior using a random 
number generator to generate the confidence of 
triggering a social behavior after every turn (by a 
student or a tutor). In the RandomLow condition, a 
behavior would be triggered if the confidence was 
above 0.91. In the RandomHigh condition, a beha-
vior would be triggered if the confidence was 
above 0.85. On average, the RandomLow condi-
tion had 23 behaviors triggered per interaction. 
About 37 behaviors were triggered in the Ran-
domHigh condition. 
The LearntLow and LearntHigh conditions 
used the best triggering policy learnt from a corpus 
of human triggering of social behavior as discussed 
in Section 4. The same social ratio filter used in the 
random conditions was used in these two condi-
tions also. As in the case with RandomLow and 
RandomHigh, different values of a confidence pa-
rameter were used for the LearntLow and Learn-
tHigh conditions to control the number of social 
behaviors triggered. On average, the LearntLow 
condition had 22 triggers and the LearntHigh con-
dition had 28 triggers. 
5.3 Generating Behaviors 
The various triggering policies described above for 
each of our experimental conditions only deter-
mine when a tutor agent will perform a social be-
havior. In order to perform the social behavior in 
actual use, the agent must not only determine when 
232
a behavior should be triggered, but also determine 
which behavior should be performed when a trig-
ger is received. Our implementation of the tutor 
agent used in this experiment provides a conti-
nuous stream of scores for each of the eleven so-
cial interaction strategies that the tutor can 
perform. The scores are computed using hand-
crafted functions that use the same features used in 
our rule-based triggering policy (Kumar et. al., 
2010b). When a social behavior is triggered, a rou-
lette wheel selection is used to determine the strat-
egy to be performed. The circumference of the 
wheel assigned to each strategy is proportional to 
the score of each strategy. If the score of all the 
strategies is zero, a generic social prompt is per-
formed. 
6 Results 
126 students enrolled in an introductory mechani-
cal engineering course at an American university 
participated in the experiment described in this 
paper. The experiment was conducted on two sepa-
rate days separated by one week. On each day, four 
sessions of the Computer-Aided Analysis lab were 
conducted, and students attended only one as-
signed session. Session assignment was made 
based on an alphabetic split. The 126 students were 
divided into 42 teams. 20 teams participated on the 
first day of the experiment. They were evenly split 
into four conditions (None, Rules, RandomHigh & 
LearntHigh). The remaining 22 teams participated 
on the second day. Out of these, 5 teams each were 
assigned to the None and RandomLow condition. 6 
teams each were assigned to the Rules and 
LearntLow conditions. 
The rest of this section presents detailed results 
and analysis of this experiment. To summarize, we 
found that out of the six evaluated policies only the 
LearntLow policy that uses a triggering model 
learnt from human triggering data and generates a 
moderate amount of social behavior is consistently 
better than the other policies in terms of both per-
formance as well as perception outcomes. Also, the 
LearntLow policy is found to be most efficient at 
delivering the instructional content as indicated by 
the smallest EpisodeDuration in Table 5. 
6.1 Learning Outcomes 
The learning outcomes analysis presented here 
shows the advantage of using a triggering policy 
learnt from a corpus of human triggering behavior 
along with a filtering technique that regulates the 
amount of social behavior as shown in Table 3. 
We first verified that there was no significant 
difference between the six conditions on the pre-
test scores. As in the case of previous experiments 
using this learning activity, we saw that the learn-
ing activity was pedagogically beneficial to the 
students irrespective of the condition. There was a 
significant improvement in test scores between 
pre-test and post-test { p < 0.0001, F(1,250) = 
26.01, effect-size = 0.58? }. 
There was no significant effect of the condition 
assigned to each team on the total test scores. 
However, there was a significant effect on the test 
scores of short-essay type questions using the pre-
test score as a covariate and the condition as a fac-
tor { p < 0.05, F(5, 119) = 2.88 }. The adjusted 
post test scores for the short essay type questions 
and their standard deviations are shown in Table 3. 
Post-hoc analysis showed that the LearntLow con-
dition was significantly better than LearntHigh 
condition { effect-size = 0.65? }. Also, Random-
Low condition was marginally better than Learn-
tHigh condition { p < 0.07, effect-size = 0.62? }. 
 
 
Mean St.Dev. 
LearntLow 5.12 0.54 
RandomLow 5.06 0.67 
None 4.75 1.13 
RandomHigh 4.59 1.09 
Rules 4.38 0.89 
LearntHigh 3.98 1.74 
 
Table 3. Mean and Standard Deviation of Adjusted Post 
Test Scores for Short Essay Type Questions 
 
This result further supports the observation 
from our earlier experiment (Kumar & Ros?, 
2010c) which demonstrated that importance of per-
forming the right amount of social behavior. Both 
RandomLow and LearntLow conditions employ 
the non-linear social ratio filter which keeps the 
amount of allowed social behavior at a level com-
parable to the amount of social behavior performed 
by human tutors. 
Since the primary objective of the experiment 
described here was to evaluate a learnt triggering 
policy with respect to a rule-based triggering poli-
cy, we repeated the ANCOVA for the short essay 
type question using data from only the Rules, 
233
LearntLow and LearntHigh conditions. We found a 
significant effect of condition on the post-test score 
using pre-test score as a covariate { p = 0.01, 
F(2,62) = 4.98 }. A post-hoc analysis showed that 
the LearntLow condition was significantly better 
than the LearntHigh condition as above and  the 
LearntLow condition was marginally better than 
the Rules condition { p ? 0.08, effect-size = 0.84? 
}. We observe that a triggering policy learnt from 
human triggering behavior can achieve a marginal 
improvement on learning outcomes compared to 
our existing rule-based triggering policy. This is 
consistent with our hypothesis. 
6.2 Perception Ratings 
We averaged the student?s rating for the 11 items 
about the tutor into a single tutor rating measure 
used here. Rating on the two negative statements 
about the tutor were inverted (7?1, 6?2, and so 
on) for this calculation.   
 
 
Mean St.Dev. 
Rules 4.74 1.45 
LearntLow 4.56 1.58 
None 4.42 1.49 
RandomHigh 3.74 1.63 
LearntHigh 3.55 1.26 
RandomLow 3.18 0.91 
 
Table 4. Mean and Standard Deviation of Tutor Ratings 
 
We found a significant effect of condition on 
the tutor ratings { p < 0.01, F(5,120) = 3.83 }. Ta-
ble 4 shows the mean and standard deviations of 
tutor ratings for each condition. Post-hoc analysis 
showed that only the Rules condition was signifi-
cantly better than the RandomLow condition. Also, 
we found that Rules was marginally better than 
LearntHigh condition { p < 0.08 } and both Learnt-
Low and None conditions was marginally better 
than RandomLow condition { p < 0.08 }. 
While we did not see a significant improvement 
in perception due the use of a learnt triggering pol-
icy when compared to a rule-based triggering poli-
cy, we find an advantage over using a random 
triggering policy (RandomLow) which was as 
good as a learnt policy on the learning outcomes. 
The results from the tutor?s perception ratings fur-
ther support the importance of timing and regulat-
ing the amount of social behavior. 
We did not find any significant effect of condi-
tion on the ratings about the design activity or stu-
dent?s task satisfaction. 
6.3 Analysis of Tutoring Episodes 
In order to understand the results from the experi-
ment presented in this paper, we applied the struc-
tural equation model discussed earlier (Figure 2) to 
the data collected from our current experiment. 
Figure 3 shows the model for our current experi-
ment (p=0.4492). Only four variables were used 
because the annotations of good and bad student 
behavior are not available at this time. 
 
 
 
Figure 3. SEM applied to data from this experiment 
 
 
Mean St.Dev. 
RandomHigh 540.80 49.50 
LearntHigh 534.80 61.00 
None 523.88 41.54 
Rules 519.80 102.70 
RandomLow 519.20 74.40 
LearntLow 484.00 69.80 
 
Table 5. Mean and Standard Deviation of Duration 
of Tutoring Episodes 
 
We see that most of the model parameters (p-
Value, means & correlations) are similar to para-
meters for the model shown in Figure 2. However, 
the correlation between SocialTurns and Episode-
Duration is much smaller. Also, note that the mean 
of EpisodeDuration is smaller compared to that in 
Figure 2 which indicates that lesser counterproduc-
tive behavior was displayed by the students in this 
experiment. The conceptual tutoring episodes are 
operating closer to the minimum episode duration 
which leaves a smaller room for improvement by 
234
the use of social interaction strategies. As dis-
cussed in Section 3.3, this explains the smaller cor-
relation between SocialTurns and EpisodeDuration 
in Figure 3. 
Table 5 shows the mean and standard deviations 
of the duration of tutoring episodes for each condi-
tion. Even though the differences are not signifi-
cant, the LearntLow policy has the lowest duration 
indicating higher student attention than the other 
conditions. 
7 Discussion 
Prior work in the field of human-human interaction 
and human-machine interaction in the form of di-
alog systems has emphasized the importance of 
timing the display of behavior to achieve natural 
and/or productive interactions. In general, timing 
of interactive behaviors (verbal as well as non-
verbal) has been studied in the context of joint ac-
tivities being performed by the participants. Beha-
viors are timed to achieve and maintain 
coordination between the participants (Clark, 
2005). Specifically, among other topics, timing of 
low-level (signal) interaction like turn-taking has 
been the subject of several investigations (Raux & 
Eskenazi, 2008; Takeuchi et. al., 2004). 
On the other hand, the use of social behavior by 
conversational agents to support students has been 
proposed (Veletsianos et. al., 2009; Gulz et. al., 
2010). Work in the area of affective computing and 
its application to tutorial dialog has focused on 
identification of student?s emotional states and us-
ing those to improve choice of behavior performed 
by tutors (D?Mello et. al., 2005). Our prior work 
(Kumar et. al., 2010; Kumar et. al., 2007) has 
shown that social behavior motivated from empiri-
cal research in small group communication (Bales, 
1950) can help in effectively supporting students in 
collaborative learning settings. Use of social inte-
raction in other applications of conversational 
agents besides education has been investigated 
(Bickmore et. al., 2009; Dybala et. al., 2009; Doh-
saka et. al., 2009). 
The experiments presented here bridges these 
two tracks of research specifically proposing a so-
lution to the challenge of timing social behavior in 
the context of a supporting collaborative learning. 
Compared to the work on timing signal-level joint 
activities like turn-taking, this work focuses on the 
timing of joint activities at the conversation level. 
The success of our algorithm at learning a model 
of timing conversational behaviors in the context 
of an interactive task could potentially offer a gen-
eral approach for realizing such behaviors in other 
conversational agents. 
8 Conclusion  
In this paper, we presented an experiment that 
compared the effectiveness of several social beha-
vior triggering policies. Specifically, we compared 
a triggering policy learnt from a corpus of human 
triggering behavior to a rule-based policy which 
has previously been shown to be successful at trig-
gering effective social behavior in a collaborative 
learning activity. 
The presented experiment provides further evi-
dence in support of the intuition that timing of so-
cial behavior and regulating the amount of social 
behavior are critical to improving performance and 
perception outcomes. A triggering policy based on 
human-like timing in combination with a filter that 
attempts to keep amount of social behavior at the 
same level as human tutors was shown to be mar-
ginally better than the rule-based policy on learn-
ing outcomes. Also, on perception measures, we 
found that the human-like policy is marginally bet-
ter than a random triggering policy which uses the 
same filter to control the amount of social beha-
vior.  Only the learned model provides a win both 
on learning and on perception measures. 
In order to better understand the effect of use of 
social behavior by automated tutors on student?s 
learning outcomes, we presented a structured mod-
el which suggests that social behavior helps in 
achieving higher learning outcomes by allowing 
the tutor to better manage the student?s attention. 
Following this model, we saw that a human-like 
triggering policy is able to achieve higher student 
attention as indicated by the smaller duration of 
tutoring episodes. 
We found a significant negative correlation { 
coefficient = -0.20, p < 0.05 } between the tutor?s 
perception rating and number of social behaviors 
triggered when none of the social interaction strat-
egies were applicable. As next steps, our best trig-
gering policy could be potentially further refined 
by achieving a closer integration of the triggering 
model with the social behavior generation mechan-
ism to prevent triggering when none of the eleven 
strategies could be generated. 
235
References  
Hua Ai, Rohit Kumar, Dong Nguyen, Amrut Nagasund-
er and Carolyn P. Ros?, 2010, Exploring the Effec-
tiveness of Social Capabilities and Goal Alignment in 
Computer Supported Collaborative Learning, Intelli-
gent Tutoring Systems, Pittsburgh, PA 
Elizabeth Arnott, Peter Hastings and David Allbritton, 
2008, Research Methods Tutor: Evaluation of a di-
alogue-based tutoring system in the classroom, Be-
havior Research Methods, 40 (3), 694-698 
Robert F. Bales, 1950, Interaction process analysis: A 
method for the study of small groups, Addison-
Wesley, Cambridge, MA 
Khelan Bhatt, Martha Evens, Shlomo Argamon, 2004, 
Hedged responses and expressions of affect in hu-
man/human and human/computer tutorial interac-
tions, CogSci, Chicago, IL 
Timothy Bickmore, Daniel Schulman and Langxuan 
Yin, 2009, Engagement vs. Deceit: Virtual Humans 
with Human Autobiographies, Proc. of Intelligent 
Virtual Agents, Amsterdam, Netherlands 
Herbert H. Clark, 2005, Coordinating with each other in 
a material world, Discourse Studies, 7 (4-5), 507-525 
Sidney K. D?Mello, Scotty D. Craig, Barry Gholson, 
Stan Frankin, Rosalind Picard, Arthur C. Graesser, 
2005, Integrating Affect Sensors in an Intelligent Tu-
toring System, Wksp on Affective Interactions: The 
Computer in the Affective Loop, IUI, San Diego, CA 
Pawel Dybala, Michal Ptaszynski, Rafal Rzepka and 
Kenji Araki, 2009, Humoroids: Conversational 
Agents that induce positive emotions with humor, 
AAMAS, Budapest, Hungary 
Kohji Dohsaka, Ryoto Asai, Ryichiro Higashinaka, Ya-
suhiro Minami and Eisaku Maeda, 2009, Effects of 
Conversational Agents on Human Communication in 
Though Evoking Multi-Party dialogues, SIGDial 
2009, London, UK 
Agneta Gulz, Annika Silvervarg and Bj?rn Sj?d?n, 
2010, Design for off-task interaction - Rethinking 
pedagogy in technology enhanced learning, Intl. 
Conf. on Advanced Learning Technologies, Tunisia 
Arthur C. Graesser, Patrick Chipman, Brian C. Haynes, 
and Andrew Olney, 2005, AutoTutor: An Intelligent 
Tutoring System with Mixed-initiative Dialogue, 
IEEE Transactions in Education, 48, 612-618 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann and Ian H. Witten, 
2009, The WEKA Data Mining Software: An Up-
date; SIGKDD Explorations, Volume 11, Issue 1 
Rohit Kumar, Jack L. Beuth and Carolyn P. Ros?, 2011, 
Conversational Strategies that Support Idea Genera-
tion Productivity in Groups, 9th Intl. Conf. on Com-
puter Supported Collaborative Learning, Hong Kong 
Rohit Kumar, Hua Ai, Jack Beuth and Carolyn P. Ros?, 
2010a, Socially-capable Conversational Tutors can 
be Effective in Collaborative-Learning situations, In-
telligent Tutoring Systems, Pittsburgh, PA 
Rohit Kumar and Carolyn P. Ros?, 2010b, Engaging 
learning groups using Social Interaction Strategies, 
NAACL-HLT, Los Angeles, CA 
Rohit Kumar and Carolyn P. Ros?, 2010c, Conversa-
tional Tutors with Rich Interactive Behaviors that 
support Collaborative Learning, Workshop on Op-
portunities for intelligent and adaptive behavior in 
collaborative learning systems, ITS 2010, Pittsburgh, 
PA 
Rohit Kumar, Gahgene Gweon, Mahesh Joshi, Yue Cui 
and Carolyn Ros?, Supporting students working to-
gether on Math with Social Dialogue, Workshop on 
Speech and Language Technology in Education, 
Farmington, PA, 2007 
Ryan McDonald, Koby Crammer and Fernando Pereira, 
2005a, Online large-margin training of dependency 
parsers, Proc. of ACL, 91-98, Ann Arbor, MI 
Martin M?hlpfordt and Martin Wessner, 2005, Explicit 
referencing in chat supports collaborative learning, 
Proc. Computer Support for Collaborative Learning 
(CSCL) 
John Niekrasz and Johanna D. Moore, 2010, Unbiased 
Discourse Segmentation Evaluation, Proc. of SLT, 
Berkeley, CA 
Antoine Raux and Maxine Eskenazi, 2008, Optimizing 
Endpointing Thresholds using Dialogue Features in a 
Spoken Dialogue System, SIGDial 2008, Columbus, 
OH 
Richard Scheines, Peter Spirtes, Clark Glymour, and 
Christopher Meek, 1994, TETRAD II: Tools for Dis-
covery, Lawrence Erlbaum Associates, Hillsdale, NJ 
 Masashi Takeuchi, Norihide Kitaoka and Seiichi Naka-
gawaM, 2004, Timing detection for realtime dialog 
systems using prosodic and linguistic information, 
Intl Conf. on Speech Prosody 2004, Nara, Japan 
Tetrad IV, 
http://www.phil.cmu.edu/projects/tetrad/tetrad4.html 
George Veletsianos, Charles Miller and Aaron Doering, 
Veletsianos, 2009, EnALI: A Research and Design 
Framework for Virtual Characters and Pedagogical 
Agents, Journal of Educational Computing Research, 
41(2), 171-194 
236
Appendix A. Excerpts of an interaction between three students and an automated tutor demonstrating 
some of the social interaction strategies listed in Table 1. 
 
7 Tutor Hi, I am your tutor for today's lab. 
8 Tutor Lets introduce ourselves briefly. I am Avis. 
9 S083 Hey Avis! I'm _name1_ 
10 S073 Im _name2_ 
11 S089 i'm _name3_ 
12 Tutor Its nice to meet you all. :) 
13 Tutor Let's get started by reviewing the base design in your worksheet. 
 
Strategy 1a: Do Introductions 
 
48 S083 R we using the same material? 
49 S073 I assume so 
50 S073 just changing the length 
51 S089 yeah we have multiple design steps 
52 S089 so probably for now 
53 Tutor cool :) 
54 S083 O ok 
 
Strategy 3b: Show Comprehension / Approval 
 
119 Tutor Is this a safe wrench? 
120 S073 and then is the same for design 2 
121 S073 so yes the wrench is safe 
122 Tutor _name1_ ... any thoughts you'd like to contribute? 
123 S083 its a safer wrench if its in steel 
 
Strategy 1e: Encourage (inactive members) 
 
143 S073 high 
144 Tutor Right, higher yield stress is better. 
145 S089 so steel or titanium 
146 S089 but have fun paying for that... 
147 Tutor All other things being equal, you want to choose a stronger material. 
148 Tutor It's good to have your contributions to the discussion :-) 
149 S073 yay 
 
Strategy 1d: Complement / Praise 
 
150 S073 :) 
151 Tutor Let's improve design 1 by using Steel for our 2nd _truncated_ 
152 Tutor I am happy to be working with our team 
153 S083 thanks :-) 
154 Tutor You can start calculating the fields in the worksheet _truncated_ 
155 S089 woo... 
 
Strategy 2c: Express Enthusiasm, Elation, Satisfaction 
237
Appendix B. Survey administered to the participants at the end of the Collaborative Design Activity 
 
Using the following scale, Indicate to what extent you agree with each of the following items. 
 
1 2 3 4 5 6 7 
Strongly 
Disagree 
Mostly 
Disagree 
Somewhat 
Disagree 
Neutral Somewhat 
Agree 
Mostly 
Agree 
Strongly 
Agree 
 
 
 
The tutor was part of my team. 1 2 3 4 5 6 7 
The tutor provided good ideas for the discussion. 1 2 3 4 5 6 7 
The tutor received my contributions positively. 1 2 3 4 5 6 7 
The tutor was friendly during the discussion. 1 2 3 4 5 6 7 
The tutor responded to my contributions. 1 2 3 4 5 6 7 
The tutor helped in lowering the tension in my group. 1 2 3 4 5 6 7 
The tutor was paying attention to our conversation. 1 2 3 4 5 6 7 
Overall, I liked the tutor very much. 1 2 3 4 5 6 7 
I think the tutor was as good as a human tutor. 1 2 3 4 5 6 7 
I often ignored what the tutor was saying. 1 2 3 4 5 6 7 
The tutor's responses got in the way of our conversation. 1 2 3 4 5 6 7 
The design challenge was exciting. 1 2 3 4 5 6 7 
I did my best to come up with good designs. 1 2 3 4 5 6 7 
I am happy with the discussion I had with my group. 1 2 3 4 5 6 7 
Overall, we were successful at meeting our goals during the design challenge. 1 2 3 4 5 6 7 
 
 
238
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 49?59,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
 
 
Modeling of Stylistic Variation in Social Media with Stretchy Patterns 
   Philip Gianfortoni David Adamson Carolyn P. Ros? Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University Language Technologies Institute Language Technologies Institute Language Technologies Institute Pittsburgh, PA Pittsburgh, PA Pittsburgh, PA pwg@cs.cmu.edu dadamson@cs.cmu.edu cprose@cs.cmu.edu      Abstract In this paper we describe a novel feature discovery technique that can be used to model stylistic variation in sociolects. While structural features offer much in terms of expressive power over simpler features used more frequently in machine learning approaches to modeling linguistic variation, they frequently come at an excessive cost in terms of feature space size expansion.  We propose a novel form of structural features referred to as ?stretchy patterns? that strike a balance between expressive power and compactness in order to enable modeling stylistic variation with reasonably small datasets.  As an example we focus on the problem of modeling variation related to gender in personal blogs.  Our evaluation demonstrates a significant improvement over standard baselines. 1 Introduction The contribution of this paper is a novel approach to feature induction seeking to model stylistic variation at a level that not only achieves high performance, but generalizes across domains better than alternative techniques. Building on an earlier template based approach for modeling sarcasm (Tsur et al, 2010), we investigate the use of what we have termed ?stretchy? features to model 
stylistic variation related to sociolects, which can be thought of as a form of dialect.  Specifically, we focus on the problem of gender based classification.  Gender classification and age classification have both received increased attention in the social media analysis community in recent years (Goswami et al, 2009; Barbieri, 2008; Cieri et al, 2004), most likely because large data sets annotated with these variables have recently become available.  Machine learning technology provides a lens with which to explore linguistic variation that complements earlier statistical techniques used by variationist sociolinguists in their work mapping out the space of dialect variation and its accompanying social interpretation (Labov, 2010a; Labov, 2010b; Eckert & Rickford, 2001).  These complementary approaches share a common foundation in numerical methods, however while descriptive statistics and inferential statistics mainly serve the purpose of describing non-random differences in distributions between communities, machine learning work in the area of social media analysis asks the more challenging question of whether the differences described are big enough to enable identification of community membership by means of those differences. In the remainder of the paper, we first introduce prior work in a variety of related areas that both demonstrates why generalizable models characterizing sociolects within social media contexts are challenging to create and motivates our novel approach.  Next we describe our 
49
  
technical approach for inducing ?stretchy patterns?.  We then present a series of experiments that demonstrate that our stretchy patterns provide advantages over alternative feature spaces in terms of avoiding overfitting to irrelevant content-based features as evidenced both in terms of achieving higher performance with smaller amounts of training data and in terms of generalizing better across subpopulations that share other demographic and individual difference variables.  2 Prior Work Analysis of social media has grown in popularity over the past decade.  Nevertheless, results on problems such as gender classification (Argamon et al, 2003), age classification (Argamon et al, 2007), political affiliation classification (Jiang & Argamon, 2008), and sentiment analysis (Wiebe et al, 2004) demonstrate how difficult stylistic classification tasks can be, and even more so when the generality is evaluated by testing models trained in one domain on examples from another domain. Prior work on feature engineering has attempted to address this generalization difficulty.  Here we motivate our ?stretchy pattern? approach to feature engineering for modeling sociolects, using gender analysis as a lens through which to understand the problem. 2.1 Variation Analysis and Gender Since the earliest work in the area of variationist sociolinguistics, gender has been a variable of interest, which explains interesting differences in communication style that have been the topic of discussion both in academic circles (Holmes & Meyerhoff, 2003) and in the popular press (Tannen, 2001). The immense significance that has been placed on these differences, whether they are viewed as essentially linked to inherent traits, learned cultural patterns, or socially situated identities that are constructed within interaction, warrants attention to gender based differences within the scope of dialect variation. While one may view gender differences in communication from multiple angles, including topic, stance, and style, we focus specifically on linguistic style in our work.  Numerous attempts to computationally model gender based language variation have been published in the past decade (Corney et al, 2002; 
Argamon et al, 2003; Schler et al, 2005; Schler, 2006; Yan & Yan, 2006; Zhang et al, 2009; Mukherjee & Liu, 2010). Gender based language variation arises from multiple sources. For example, within a single corpus comprised of samples of male and female language that the two genders do not speak or write about the same topics. This has been reported to be the case with blog corpora such as the one used in this paper. Even in cases where pains have been taken to control for the distribution of topics associated with each gender within a corpus (Argamon et al, 2003), it?s still not clear the extent to which that distribution is completely controlled. For example, if one is careful to have equal numbers of writing samples related to politics from males and females, it may still be the case that males and females are discussing different political issues or are addressing political issues from a different role based angle. While these differences are interesting, they do not fit within the purview of linguistic style variation.  Word based features such as unigrams and bigrams are highly likely to pick up on differences in topic (Schler, 2006) and possibly perspective. Thus, in cases where linguistic style variation is specifically of interest, these features are not likely to be included in the set of features used to model the variation even if their use leads to high performance within restricted domains. Typical kinds of features that are used instead include part-of-speech (POS) n-grams (Koppel, 2002; Argamon et al, 2003), word structure features that cluster words according to endings that indicate part of speech (Zhang et al, 2009), features that indicate the distribution of word lengths within a corpus (Corney et al, 2002), usage of punctuation, and features related to usage of jargon (Schler et al, 2005). In Internet-based communication, additional features have been investigated such as usage of internet specific features including ?internet speak? (e.g., lol, wtf, etc.), emoticons, and URLs (Yan & Yan, 2006). In addition to attention to feature space design issues, some work on computational modeling of gender based language variation has included the development of novel feature selection techniques, which have also had a significant impact on success (Mukherjee & Liu, 2010; Zhang, Dang, & Chen, 2009).  Of these features, the only ones that capture stylistic elements that extend beyond individual 
50
  
words at a time are the POS ngram features. The inclusion of these features has been motivated by their hypothesized generality, although in practice, the generality of gender prediction models has not been formally evaluated in the gender prediction literature. 2.2 Domain Adaptation in Social Media Recent work in the area of domain adaptation (Arnold et al, 2008; Daum? III, 2007; Finkel & Manning, 2009) raises awareness of the difficulties with generality of trained models and offers insight into the reasons for the difficulty with generalization. We consider these issues specifically in connection with the problem of modeling gender based variation. One problem, also noted by variationist sociolinguists, is that similar language variation is associated with different variables (McEnery, 2006).  For example, linguistic features associated with older age are also more associated with male communication style than female communication style for people of the same age (Argamon et al, 2007).  Another problem is that style is not exhibited by different words than those that serve the purpose of communicating content.  Thus, there is much about style that is expressed in a topic specific way.   What exacerbates these problems in text processing approaches is that texts are typically represented with features that are at the wrong level of granularity for what is being modeled.  Specifically, for practical reasons, the most common types of features used in text classification tasks are still unigrams, bigrams, and part-of-speech bigrams.  While relying heavily on these relatively simple features has computational advantages in terms of keeping the feature space size manageable, which aids in efficient model learning, in combination with the complicating factors just mentioned, these text classification approaches are highly prone to over-fitting.  Specifically, when text is represented with features that operate at too fine grained of a level, features that truly model the target style are not present within the model.  Thus, the trained models are not able to capture the style itself and instead capture features that merely correlate with that style within the data.  Thus, in cases where the data is not independent and identically distributed (IID), 
and where instances that belong to different subpopulations within the non-IID data have different class value distributions, the model will tend to give weight to features that indicate the subpopulation rather than features that model the style.  This may lead to models that perform well within datasets that contain the same distribution of subpopulations, but will not generalize to different subpopulations, or even datasets composed of different proportions of the same subpopulations. Models employing primarly unigrams and bigrams as features are particularly problematic in this respect.  2.3 Automatic Feature Engineering In recent years, a variety of manual and automatic feature engineering techniques have been developed in order to construct feature spaces that are adept at capturing interesting language variation without overfitting to content based variation, with the hope of leading to more generalizable models.  POS n-grams, which have frequently been utilized in genre analysis models (Argamon et al, 2003), are a strategic balance between informativity and simplicity. They are able to estimate syntactic structure and style without modeling it directly. In an attempt to capture syntactic structure more faithfully, there has been experimentation within the area of sentiment analysis on using syntactic dependency features (Joshi & Ros?, 2009; Arora, Joshi, & Ros?, 2009). However, results have been mixed. In practice, the added richness of the features comes at a tremendous cost in terms of dramatic increases in feature space size. What has been more successful in practice is templatizing the dependency features in order to capture the same amount of structure without creating features that are so specific.  Syntactic dependency based features are able to capture more structure than POS bigrams, however, they are still limited to representing relationships between pairs of words within a text. Thus, they still leave much to be desired in terms of representation power. Experimentation with graph mining from dependency parses has also been used for generating rich feature spaces (Arora et al, 2010). However, results with these features has also been disappointing. In practice, the rich features with real predictive power end up being 
51
  
difficult to find amidst myriads of useless features that simply add noise to the model. One direction that has proven successful at exceeding the representational power and performance of POS bigrams with only a very modest increase in feature space size has been a genetic programming based approach to learning to build a strategic set of rich features so that the benefits of rich features can be obtained without the expense in terms of feature space expansion. Successful experiments with this technique have been conducted in the area of sentiment analysis, with terminal symbols including unigrams in one case (Mayfield & Ros?, 2010) and graph features extracted from dependency parses in another (Arora et al, 2010). Nevertheless, improvements using these strategic sets of evolved features have been very small even where statistically significant, and thus it is difficult to justify adding so much machinery for such a small improvement.  Another direction is to construct template based features that combine some aspects of POS n-grams in that they are a flat representation, and the backoff version of dependency features, in that the symbols represent sets of words, which may be POS tags, learned word classes, distribution based word classes (such as high frequency words or low frequency words), or words. Such types of features have been used alone or in combination with sophisticated feature selection techniques or bootstrapping techniques, and have been applied to problems such as detection of sarcasm (Tsur et al, 2010), detection of causal connections between events (Girju, 2010), or machine translation (Gimpel et al, 2011). Our work is most similar to this class of approaches.  3  Technical Approach: Stretchy Patterns Other systems have managed to extract and employ patterns containing gaps with some success.  For example, Gimpel (2011) uses Gibbs sampling to collect patterns containing single-word gaps, and uses them among other features in a machine translation system.  Our patterns are more like the ones described in Tsur (2010), which were applied to the task of identifying sarcasm in sentences.  We predicted that a similar method would show promise in extracting broader stylistic features indicative of the author?s group-aligned dialect. We have chosen 
the classification of an author?s gender as the task to which we can apply our patterns. 3.1    Pattern-Based Features To extract their sarcasm-detecting patterns, Tsur (2010) first defined two sets of words: High Frequency Words (HFW) and Content Words (CW).  The HFW set contained all words that occurred more than 100 times per million, and the CW set contained all words in the corpus that occurred fewer than 1000 times per million.  Thus, a word could be contained in the HFW set, the CW set, or both. Such patterns must begin and end with words in the HFW set, and (as in our implementation) are constrained in the number of words drawn from each set. Additionally, as a preprocessing step, in their approach they made an attempt to replace phrases belonging to several categories of domain-specific phrases, such as product and manufacturer names with a label string, which was then added to the HFW set, indicating membership.  For example, given an input such as ?Garmin apparently does not care much about product quality or customer support?, a number of patterns would be produced, including ?[company] CW does not CW much?.  3.2   Stretchy Patterns Tsur?s patterns were applied as features to classify sentences as sarcastic (or not), within the domain of online product reviews. Here our implementation and application diverge from Tsur?s ? the blog corpus features large multi-sentence documents, and span a diverse set of topics and authors. We aim to use these patterns not to classify sentiment or subtlety, but to capture the style and structure employed by subsets of the author-population.  We define a document as an ordered list of tokens.  Each token is composed of a surface-form lexeme and any additional syntactic or semantic information about the word at this position (in our case this is simply the POS tag, but other layers such as Named Entity might be included). We refer to any of the available forms of a token as a type. A category is a set of word-types. Each type must belong to at least one category.  All categories have a corresponding label, by which they?ll be referred to within the patterns to come. Gap is a 
52
  
special category, containing all types that aren?t part of any other category. The types belonging to any defined category may also be explicitly added to the Gap category.   A stretchy pattern is defined as a sequence of categories, which must not begin or end with a Gap category.  We designate any number of adjacent Gap instances in a pattern by the string ?GAP+?1 and every other category instance by its label.  As a convention, the label of a singleton category is the name of the type contained in the category (thus "writes" would be the label of a category containing only surface form "writes" and "VBZ" would be the label of the a category containing only the POS tag "VBZ"). The overall number of Gap and non-Gap category instances comprising a pattern is restricted - following Tsur (2010), we allow no more than six tokens of either category. In the case of Gap instances, this restriction is placed on the number of underlying tokens, and not the collapsed GAP+ form.   A sequence of tokens in a document matches a pattern if there is some expansion where each token corresponds in order to the pattern?s categories. A given instance of GAP+ will match between zero and six tokens, provided the total number of Gap instances in the pattern do not exceed six2.  By way of example, two patterns follow, with two strings that match each. Tokens that match as Gaps are shown in parenthesis. [cc] (GAP+) [adj] [adj] ?and (some clients were) kinda popular...? ?from (our) own general election...?  for (GAP+) [third-pron] (GAP+) [end] [first-pron] ?ready for () them (to end) . I am...? ?for (murdering) his (prose) . i want??  Although the matched sequences vary in length and content, the stretchy patterns preserve information about the proximity and ordering of particular words and categories. They focus on the relationship between key (non-Gap) words, and allow a wide array of sequences to be matched  by 
                                                           1 This is actually an extractor parameter, but we collapse all adjacent gaps for all our experiments. 2 The restrictions on gaps are extractor parameters, but we picked zero to six gaps for our experiments. 
a single pattern in a way that traditional word-class n-grams would not. Our ?stretchy pattern? formalism strictly subsumes Tsur?s approach in terms of representational power.  In particular, we could generate the same patterns described in Tsur (2010) by creating a singleton surface form category for each word in Tsur?s HFW and then creating a category called [CW] that contains all of the words in the Tsur CW set, in addition to the domain-specific product/manufacturer categories Tsur employed.  Label Category Members adj JJ, JJR, JJS cc CC, IN md MD end <period>, <comma>, <question>, <exclamation> first-pron I, me, my, mine, im, I?m second-pron you, your, youre, you?re, yours, y?all third-pron he, him emotional feel, hurt, lonely, love time hour, hours, late, min, minute, minutes, months, schedule, seconds, time, years,  male_curse fucking, fuck, jesus, cunt, fucker female_curse god, bloody, pig, hell, bitch, pissed, assed, shit Table 1. Word Categories 3.3   Word Categories With the aim of capturing general usage patterns, and motivated by the results of corpus linguists and discourse analysts, a handful token categories were defined, after the fashion of the LIWC categories as discussed in Gill (2009). Tokens belonging to categories may be replaced with their category label as patterns are extracted from each document. As a token might belong to multiple categories, the same token sequence may generate, and therefore match multiple patterns.   Words from a list of 800 common prepositions, conjunctions, adjectives, and adverbs were included as singleton surface-form categories. Determiners in particular are absent from this list (and from the POS categories that follow), as their absence or presence in a noun phrase is one of the primary variations the stretchy gaps of our patterns were intended to smooth over.  A handful of POS categories were selected, reflecting previous research and predictions about gender differences in language usage. For example, to capture the ?hedging? discussed in Holmes (2003) as more common in female speech, the modal tag MD was included as a singleton 
53
  
category. A category comprising the coordinating conjunction and preposition tags (CC, IN) was included to highlight transitions in complicated or nested multi-part sentences.  Additionally, where previous results suggested variation within a category based on gender (e.g. swearing, as in McEnery (2006)), two categories were added, with the words most discriminative for each gender. However, even those words most favored by male authors might appear in contexts where males would never use them - it is our hope that by embedding these otherwise-distinguishing features within the structure afforded by gap patterns we can extract more meaningful patterns that more accurately and expressively capture the style of each gender. 3.4   Extraction and Filtering Patterns are extracted from the training set, using a sliding window over the token stream to generate all allowable combinations of category-gap sequences within the window. This generates an exponential number of patterns - we initially filter this huge set based on each pattern?s accuracy and coverage as a standalone classifier, discarding those with less than a minimum precision or number of instances within the training set. In the experiments that follow, these thresholds were set to a minimum of 60% per-feature precision, and at least 15 document-level hits. 4 Evaluation We have motivated the design of our stretchy patterns by the desire to balance expressive power and compactness. The evidence of our success should be demonstrated along two dimensions: first, that these compact features allow our models to achieve a higher performance when trained on small datasets and second, that models trained with our stretchy patterns generalize better between domains. Thus, in this section, we present two evaluations of our approach in comparison to three baseline approaches. 4.1   Dataset We chose to use the Blog Authorship Corpus for our evaluation, which has been used in earlier work related to gender classification (Schler 2006), 
and which is available for web download3. Each instance contains a series of personal blog entries from a single author. For each blog, we have metadata indicating the gender, age, occupation, and astrological sign of the author. From this corpus, for each experiment, we randomly selected a subset in which we have balanced the distribution of gender and occupation. In particular, we selected 10 of the most common occupations in the dataset, specifically Science, Law, Non-Profit, Internet, Engineering, Media, Arts, Education, Technology, and Student. We randomly select the same number of blogs from each of these occupations, and within occupation based sets, we maintain an even distribution of male and female authors. We treat the occupation variable as a proxy for topic since bloggers typically make reference to their work in their posts. We make use of this proxy for topic in our evaluation of domain generality below. 4.2   Baseline Approaches We can find in the literature a variety of approaches to modeling gender based linguistic variation, as outlined in our prior work discussion above. If our purpose was to demonstrate that our stretchy patterns beat the state-of-the-art at the predictive task of gender classification, it would be essential to implement one of these approaches as our baseline. However, our purpose here is instead to address two more specific research questions instead, and for that we argue that we can learn something from comparing with three more simplistic baselines, which differ only in terms of feature extraction. The three baseline models we tested included a unigram model, a unigram+bigram model, and a Part-of-Speech bigram model. For part-of-speech tagging we used the Stanford part-of-speech tagger4 (Toutanova et al, 2003).  Our three baseline feature spaces have been very commonly used in the language technologies community for a variety of social media analysis tasks, the most common of which in recent years has been sentiment analysis. While these feature spaces are simple, they have remained surprisingly strong baseline approaches when testing is done 
                                                           3  http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm 4  http://nlp.stanford.edu/software/tagger.shtml 
54
  
within domain, and with large enough training sets. However, these relatively weak, low level features are notorious for low performance when datasets are too small and for low generalizability when evaluated in a cross-domain setting. Because of this, we expect to see our baseline approaches perform well when both training and testing data match in terms of topic distribution and when we use our largest amount of training data. However, we expect performance to degrade as training data set size decreases as well as when we test in a cross-domain setting. We expect to see degradation also with our proposed stretchy patterns. However, we will consider our claims to have been supported if we see less degradation with our stretchy patterns than with the baseline approaches.  We did minimal preprocessing on the textual data prior to feature extraction for all approaches. Specifically, all numbers in the text were replaced with a <number> symbol. Punctuation was separated from words and treated as a separate symbol. All tokens were downcased so that we generalize across capitalization options. In all cases, we use a support vector machine approach to training the model, using the SMO implementation found in Weka (Witten & Frank, 2005), using a linear polynomial kernel and default settings. For each model, we first use a Chi-Squared filter for attribute selection over the training data, retaining only the top 3,000 features prior to training.  4.3  Study 1: Learning on Small Datasets The purpose of Study 1 was to test the claim that our stretchy patterns achieve higher performance when we train using a small amount of data. For this evaluation, we constructed a test set of 3,000 instances that we use consistently across training configurations. Specifically, we selected 300 blogs from each of the 10 occupations listed above such that 150 of them were from male authors and 150 from female authors. We constructed also a set of training sets of size 300, 800, 1500, 2000, and 3000 randomly selected blogs respectively, in which we maintain the same occupation and gender distribution as in the test set. To compensate for sampling eccentricities, two samples of each training size were extracted, and their results averaged for each experiment. In all cases, from each blog, we randomly selected one 
blog entry that was at least 100 words long. For each baseline approach as well as the stretchy feature approach, we build a model using each training set, which we then test using the common test set. Thus, for each approach, we can examine how performance increases as amount of training data increases, and we can compare this growth curve between approaches.  Training Set Size Unigram Unigram + Bigram POS Bigram Stretchy Patterns 300 49.9  (-.002) 49.85(-.002) 51.6   ( .032) 48.65(-.027) 800 51.65( .029) 50.15 (.003) 50.55 ( .014) 53.15 ( .072) 1500 48.6  (-.028) 49.98     (0) 48.63 (-.028) 53.95 ( .066) 2000 50.55( .011) 51.7   (.034) 51.82 ( .063) 53.98 ( .079) 3000 49.48(-.010) 50.8   (.016) 49.88 ( .0025) 59.05 ( .181) Table 2 Classification accuracy for varying data sizes (with kappa in parentheses)  The dramatic mediocrity of the baselines? performance highlights the difficulty of the selected data set, confirming the sense that most of what these n-gram models pick up is not truly gender-specific usage, but shadows of the distribution of topics (here, occupations) between the genders. At all sizes except the smallest (where no approach is significantly better than random), our approach outperforms the baselines. At size 800, this difference is marginal (p < .1), and at the larger sizes, it is a significant increase (p < .05). 4.4  Study 2: Evaluation of Domain Generality For our evaluation of domain generality, we randomly selected 200 blogs from each of the 10 most common occupations in the corpus, 100 of which were by male authors and 100 by female authors. As in the evaluation above, from each blog, we randomly selected one blog entry that was at least 100 words long. In order to test domain generality, we perform a leave-one-occupation-out cross validation experiment, which we refer to as a Cross Domain evaluation setting. In this setting, on each fold, we always test on blogs from an occupation that was not represented within the training data. Thus, indicators of gender that are specific to an occupation will not generalize from training to test.  Table 3 displays the results from the comparison of our stretchy feature approach with each of the baseline approaches. On average, stretchy patterns generalized better to new domains 
55
  
than the other approaches. The stretchy feature approach beat the baseline approaches in a statistically significant way (p < .05). Occupation Unigram Unigram + Bigram POS Bigram Stretchy Patterns Engineering 49.5    (-.01) 53      ( .06) 49    (-.02) 50.5    ( .01) Education 49       (-.02) 52      ( .04) 54.5  ( .09) 51       ( .02) Internet 55.5    ( .11) 47.5   (-.05) 55.5 ( .11) 56.5    ( .13) Law 51.5    ( .03) 46.5   (-.07) 46.5 (-.07) 50.5    ( .01) Non-Profit 50         ( 0 ) 54      ( .08) 49    (-.02) 51.      ( .02) Technology 50         ( 0 ) 53.5   ( .07) 50     ( 0 )  51.5    ( .03) Arts 48       (-.04) 46.5   (-.07) 51    ( .02) 55.4    ( .11) Media 53       ( .06) 50        ( 0 ) 45    (-.10) 51.5    ( .02) Science 52       ( .04) 48      (-.04) 40.5 (-.19) 59.5    ( .19) Student 51       ( .02) 46      (-.09) 55    ( .10) 62       ( .24) Average 50.95  (.002) 49.7 (-.007) 49.6  ( .01) 53.94  ( .08) Random CV 61.05    (.22) 59.65  (.19) 57.95 (.16) 62.8    ( .26) Table 3 Accuracy from leave-one-occupation-out cross-validation (with kappa in parentheses)  For random cross-validation, our approach performed marginally better than the unigram baseline, and again significantly exceeds the performance of the other two baselines. Note that for all approaches, there is a significant drop in performance from Random CV to the cross-domain setting, showing that all approaches, including ours, suffer from domain specificity to some extent.  However, while all of the baselines drop down to essentially random performance in the cross-domain setting, and stretchy patterns remain significantly higher than random, we show that our approach has more domain generality, although it still leaves room for improvement on that score. 5 Qualitative Analysis of Results Here we present a qualitative analysis of the sorts of patterns extracted by our method. Although we cannot draw broad conclusions from a qualitative investigation of such a small amount of data, we did observe some interesting trends.   As our features do not so much capture syntactic structure as the loose proximity and order of classes of words, we?ll say less about structure and more about what sort of words show up in each others? neighborhood. In particular, a huge proportion of the top-ranked patterns feature instances of the [end] and [first-pron] categories, 
suggesting that much of the gender distinction captured by our patterns is to be found around sentence boundaries and self-references. It?s believable and encouraging that ?the way I talk about myself? is an important element in distinguishing style between genders.  The Chi-squared ranking of the stretchy patterns gives us a hint as to the predictive effect of each as a feature. In the discussion and examples that follow, we?ll draw from the highest-ranked features, and refer to the weights? signs to label each pattern as ?male? or ?female?.  In these features the discourse analyst or dialectician can find fodder for their favorite framework, or support for popularly held views on gender and language. For example, we find that about twice as many of the patterns containing either [third-pron] or [second-pron] in the neighborhood of [first-pron] are weighted toward female, supporting earlier findings that women are more concerned with considering interpersonal relationships in their discourse than are men, as  in Kite (2002). For example,   [first-pron] (GAP+) [third-pron]  ?i (have time for) them?  Supporting the notion that distinctively female language is ?deviant,? and viewed as a divergence from a male baseline, as discussed in Eckert & McConnell-Ginet (1992), we note that more of the top-ranked patterns are weighted toward female. This might suggest that the ?female? style is less standard and therefore harder to detect. Additionally, we only find adjacent [end] markers, capturing repeated punctuation, in our female-weighted patterns. For instance,    [adj] (GAP+) [end] (GAP+) [end] [end]  ?new (songs) ! ( :-) see yas ) . .?  This divergence from the standard sentence form, while more common overall in informal electronic communications, does occur more frequently among female authors in the data. Further analysis of the data suggests that emoticons like :-) would have formed a useful category for our patterns, as they occur roughly twice as often in female posts, and often in the context of end-of-sentence punctuation.    We provide a rough numerical overview of the features extracted during the random cross-validation experiment. Samples of high-ranking 
56
  
stretchy patterns appear in Tables 4 and 5. Note that sequences may match more than one pattern, and that GAP+ expansions can have zero length.  [first-pron] (female) and i have time for... (female) a freshman , my brother is... (male) and i overcame my fear ...  [end] (GAP+) [first-pron] (female) no ! ! ! (i just guess) i... (female) all year . (. .) i am so... (male) the internet . () i ask only... [end] (GAP+) [end] and (female) positives . (gotta stay positive) . and hey... (female) at the park ..(. sitting at teh bench alone ..).  and walking down on my memory line... (male) sunflower . (she has a few photo galleries ..).   and i would like... like (GAP+) [first-pron] (female) well like (anywho . . . I got) my picture back? (female) it?s times like (these that I miss) my friends... (male) with something like (that in the air ,) i don't...	
 Table 4. Female Patterns.  [adj] (GAP+) [end] (GAP+) [first-pron] (male) her own digital (camera) . (what  enlightens) me is... (male) a few (photo galleries ..) . (and) i would... (female) money all (year) . (..) i am so much... [first-pron] (GAP+) [end] (male) again . i (ate so well today , too) . lots of ... (male) movie i ('d already seen once before) .  (female) a junior and i (have the top locker) . lol [end] (GAP+) [first-pron] (GAP+) [cc] (male) food ! () i ('m so hooked) on this delicious... (male) galleries .(.. and) i (would like) for you to... (female) alot better . () i (have a locker right) above... so (GAP+) [end] (male) was it ? so (cousins , stay posted) . remember... (male) experience you've gained so (far) . if... (female) , its been so (damn crappy out) . ok bye Table 5. Male Patterns.  Although our patterns capture much more than the unigram frequencies of categories, a glance at such among the extracted patterns will prove enlightening. Of the 3000 patterns considered, 1407 were weighted to some degree toward male, and 1593 toward female. Overall, female patterns include more of our chosen categories than their male counterparts. Many of these imbalances matched our initial predictions, in particular the greater number of female patterns with [first-pron] 
(772 vs. 497), [second-pron] (47 vs 27), [third-pron] (286 vs. 203), and [end] (851 vs. 618), [emotion] (36 vs. 20).   Contrary to our expectations, [md] appeared only slightly more frequently in female patterns (73 vs. 66), and [time] appeared in only a few male patterns (22 female vs. 7 male) - of these time-patterns, most of the matching segments included the word ?time? itself, instead of any other time-related words. No patterns containing the divided curse categories were among the top-ranked features.  6 Conclusions and Current Directions In this paper we described a novel template based feature creation approach that we refer to as stretchy patterns. We have evaluated our approach two ways, once to show that with this approach we are able to achieve higher performance than baseline approaches when small amounts of training data are used, and one in which we demonstrated that we are able to achieve better performance in a cross domain evaluation setting.  While the results of our experiments have shown promising results, we acknowledge that we have scratched the surface of the problem we are investigating. First, our comparison was limited to just a couple of strategically selected baselines. However, there have been many variations in the literature on gender classification specifically, and genre analysis more generally, that we could have included in our evaluations, and that would likely offer additional insights. For example, we have tested our approach against POS bigrams, but we have not utilized longer POS sequences, which have been used in the literature on gender classification with mixed results. In practice, longer POS sequences have only been more valuable than POS bigrams when sophisticated feature selection techniques have been used (Mukherje & Liu, 2010). Attention may also be directed to the selection or generation of word categories better suited to stretchy patterns. Alternative approaches to selecting or clustering these features should also be explored. 7 Acknowledgements This research was funded by ONR grant N000141110221 and NSF DRL-0835426. 
57
  
References  Argamon, S., Koppel, M., Fine, J., & Shimoni, A. (2003). Gender, genre, and writing style in formal written texts, Text, 23(3), pp 321-346. Argamon, S., Koppel, M., Pennebaker, J., & Schler, J. (2007). Mining the blogosphere: age, gender, and the varieties of self-expression. First Monday 12(9). Arnold, A. (2009). Exploiting Domain And Task Regularities For Robust Named Entity Recognition. PhD thesis, Carnegie Mellon University, 2009. Arora, S., Joshi, M., Ros?, C. P. (2009). Identifying Types of Claims in Online Customer Reviews, Proceedings of the North American Chapter of the Association for Computational Linguistics.  Arora, S., Mayfield, E., Ros?, C. P., & Nyberg, E. (2010). Sentiment Classification using Automatically Extracted Subgraph Features, Proceedings of the NAACL HLT Workshop on Emotion in Text. Barbieri, F. (2008). Patterns of age-based linguistic variation in American English. Journal of Sociolinguistics 12(1), pp 58-88.  Cieri, C., Miller, D., & Walker, K. (2004). The fisher corpus: a resource for the next generations of speech-to-text. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pp 69-71. Corney, M., de Vel, O., Anderson, A., Mohay, G. (2002). Gender-preferential text mining of e-mail discourse, in the Proceedings of the 18th Annual Computer Security Applications Conference. Daum? III, H. (2007). Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256-263. Eckert, P. & Rickford, J. (2001). Style and Sociolinguistic Variation, Cambridge: University of Cambridge Press. Eckert, P. & McConnell-Ginet, S. (1992). Think Practically and Look Locally: Language and Gender as Community- Based Practice. In  the Annual Review of Anthropology, Vol. 21, pages 461-490. Finkel, J. & Manning, C. (2009). Hierarchical Bayesian Domain Adaptation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Gill, A., Nowson, S. & Oberlander, J. (2009). What Are They Blogging About? Personality, Topic and 
Motivation in Blogs. In Proceedings of the Third International ICWSM Conference. Gimpel, K., Smith, N. A. (2011). Unsupervised Feature Induction for Modeling Long-Distance Dependencies in Machine Translation, Forthcoming. Girju, R. (2010). Towards Social Causality: An Analysis of Interpersonal Relationships in Online Blogs and Forums, Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media. Goswami, S., Sarkar, S. & Rustagi, M. (2009). Stylometric analysis of bloggers? age and gender. In Proceedings of the Third International ICWSM Conference.  Holmes, J. & Meyerhoff, M. (2003). The Handbook of Language and Gender, Blackwell Publishing. Jiang, M. & Argamon, S. (2008). Political leaning categorization by exploring subjectivities in political blogs. In Proceedings of the 4th International Conference on Data Mining, pages 647-653. Joshi, M. & Ros?, C. P. (2009). Generalizing Dependency Features for Opinion Mining, Proceedings of the Association for Computational Linguistics. Kite, M. (2002) Gender Stereotypes, in the Encyclopedia of Women and Gender: Sex Similarities and DIfferences, Volume 1, Academic Press. Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259.  Labov, W. (2010a). Principles of Linguistic Change: Internal Factors (Volume 1), Wiley-Blackwell. Labov, W. (2010b). Principles of Linguistic Change: Social Factors (Volume 2), Wiley-Blackwell. Mayfield, E. & Ros?, C. P. (2010). Using Feature Construction to Avoid Large Feature Spaces in Text Classification, in Proceedings of the Genetic and Evolutionary Computation Conference. McEnery, T. (2006). Swearing in English: Bad language, purity and power from 1586 to the present, Routledge. Mukherjee, A. & Liu, B. (2010). Improved Gender Classification of Blog Authors, Proceedings of EMNLP 2010. Schler, J., Koppel, M., Argamon, S., Pennebaker, J. (2005). Effects of Age and Gender on Blogging, Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs. 
58
  
Schler, J. (2006). Effects of Age and Gender on Blogging. Artificial Intelligence, 86, 82-84. Tannen, D. (2001). You Just Don?t Understand: Women and Men in Conversation, First Quill. Tsur, O., Davidov, D., & Rappoport, A. (2010). ICWSM ? A Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Product Reviews, Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media. Wiebe, J., Bruce, R., Martin, M., Wilson, T., & Ball, M. (2004). Learning Subjective Language, Computational Linguistics, 30(3). Witten, I. & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and Techniques, second edition, Elsevier, San Francisco. Yan, X., & Yan, L. (2006). Gender classification of weblog authors. AAAI Spring Symposium Series Computational Approaches to Analyzing Weblogs (p. 228?230). Zhang, Y., Dang, Y., Chen, H. (2009). Gender Difference Analysis of Political Web Forums : An Experiment on International Islamic Women?s Forum, Proceedings of the 2009 IEEE international conference on Intelligence and security informatics, pp 61-64.    
59
