Automatic Segmentation of Multiparty Dialogue
Pei-Yun Hsueh
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
p.hsueh@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
J.Moore@ed.ac.uk
Steve Renals
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
s.renals@ed.ac.uk
Abstract
In this paper, we investigate the prob-
lem of automatically predicting segment
boundaries in spoken multiparty dialogue.
We extend prior work in two ways. We
first apply approaches that have been pro-
posed for predicting top-level topic shifts
to the problem of identifying subtopic
boundaries. We then explore the impact
on performance of using ASR output as
opposed to human transcription. Exam-
ination of the effect of features shows
that predicting top-level and predicting
subtopic boundaries are two distinct tasks:
(1) for predicting subtopic boundaries,
the lexical cohesion-based approach alone
can achieve competitive results, (2) for
predicting top-level boundaries, the ma-
chine learning approach that combines
lexical-cohesion and conversational fea-
tures performs best, and (3) conversational
cues, such as cue phrases and overlapping
speech, are better indicators for the top-
level prediction task. We also find that
the transcription errors inevitable in ASR
output have a negative impact on models
that combine lexical-cohesion and conver-
sational features, but do not change the
general preference of approach for the two
tasks.
1 Introduction
Text segmentation, i.e., determining the points at
which the topic changes in a stream of text, plays
an important role in applications such as topic
detection and tracking, summarization, automatic
genre detection and information retrieval and ex-
traction (Pevzner and Hearst, 2002). In recent
work, researchers have applied these techniques
to corpora such as newswire feeds, transcripts of
radio broadcasts, and spoken dialogues, in order
to facilitate browsing, information retrieval, and
topic detection (Allan et al, 1998; van Mulbregt
et al, 1999; Shriberg et al, 2000; Dharanipragada
et al, 2000; Blei and Moreno, 2001; Christensen
et al, 2005). In this paper, we focus on segmenta-
tion of multiparty dialogues, in particular record-
ings of small group meetings. We compare mod-
els based solely on lexical information, which are
common in approaches to automatic segmentation
of text, with models that combine lexical and con-
versational features. Because tasks as diverse as
browsing, on the one hand, and summarization, on
the other, require different levels of granularity of
segmentation, we explore the performance of our
models for two tasks: hypothesizing where ma-
jor topic changes occur and hypothesizing where
more subtle nested topic shifts occur.
In addition, because we do not wish to make the
assumption that high quality transcripts of meet-
ing records, such as those produced by human
transcribers, will be commonly available, we re-
quire algorithms that operate directly on automatic
speech recognition (ASR) output.
2 Previous Work
Prior research on segmentation of spoken ?docu-
ments? uses approaches that were developed for
text segmentation, and that are based solely on
textual cues. These include algorithms based on
lexical cohesion (Galley et al, 2003; Stokes et
al., 2004), as well as models using annotated fea-
tures (e.g., cue phrases, part-of-speech tags, coref-
erence relations) that have been determined to cor-
relate with segment boundaries (Gavalda et al,
1997; Beeferman et al, 1999). Blei et al (2001)
273
and van Mulbregt et al (1999) use topic lan-
guage models and variants of the hidden Markov
model (HMM) to identify topic segments. Recent
systems achieve good results for predicting topic
boundaries when trained and tested on human
transcriptions. For example, Stokes et al (2004)
report an error rate (Pk) of 0.25 on segmenting
broadcast news stories using unsupervised lexical
cohesion-based approaches. However, topic seg-
mentation of multiparty dialogue seems to be a
considerably harder task. Galley et al (2003) re-
port an error rate (Pk) of 0.319 for the task of pre-
dicting major topic segments in meetings.1
Although recordings of multiparty dialogue
lack the distinct segmentation cues commonly
found in text (e.g., headings, paragraph breaks,
and other typographic cues) or news story segmen-
tation (e.g., the distinction between anchor and
interview segments), they contain conversation-
based features that may be of use for automatic
segmentation. These include silence, overlap rate,
speaker activity change (Galley et al, 2003), and
cross-speaker linking information, such as adja-
cency pairs (Zechner and Waibel, 2000). Many
of these features can be expected to be compli-
mentary. For segmenting spontaneous multiparty
dialogue into major topic segments, Galley et
al. (2003) have shown that a model integrating lex-
ical and conversation-based features outperforms
one based on solely lexical cohesion information.
However, the automatic segmentation models
in prior work were developed for predicting top-
level topic segments. In addition, compared to
read speech and two-party dialogue, multi-party
dialogues typically exhibit a considerably higher
word error rate (WER) (Morgan et al, 2003).
We expect that incorrectly recognized words will
impair the robustness of lexical cohesion-based
approaches and extraction of conversation-based
discourse cues and other features. Past research
on broadcast news story segmentation using ASR
transcription has shown performance degradation
from 5% to 38% using different evaluation metrics
(van Mulbregt et al, 1999; Shriberg et al, 2000;
Blei and Moreno, 2001). However, no prior study
has reported directly on the extent of this degra-
dation on the performance of a more subtle topic
segmentation task and in spontaneous multiparty
dialogue. In this paper, we extend prior work by
1For the definition of Pk and Wd, please refer to section
3.4.1
investigating the effect of using ASR output on the
models that have previously been proposed. In ad-
dition, we aim to find useful features and models
for the subtopic prediction task.
3 Method
3.1 Data
In this study, we used the ICSI meeting corpus
(LDC2004S02). Seventy-five natural meetings of
ICSI research groups were recorded using close-
talking far field head-mounted microphones and
four desktop PZM microphones. The corpus in-
cludes human transcriptions of all meetings. We
added ASR transcriptions of all 75 meetings which
were produced by Hain (2005), with an average
WER of roughly 30%.
The ASR system used a vocabulary of 50,000
words, together with a trigram language model
trained on a combination of in-domain meeting
data, related texts found by web search, conver-
sational telephone speech (CTS) transcripts and
broadcast news transcripts (about 109 words in to-
tal), resulting in a test-set perplexity of about 80.
The acoustic models comprised a set of context-
dependent hidden Markov models, using gaussian
mixture model output distributions. These were
initially trained on CTS acoustic training data, and
were adapted to the ICSI meetings domain using
maximum a posteriori (MAP) adaptation. Further
adaptation to individual speakers was achieved us-
ing vocal tract length normalization and maximum
likelihood linear regression. A four-fold cross-
validation technique was employed: four recog-
nizers were trained, with each employing 75% of
the ICSI meetings as acoustic and language model
training data, and then used to recognize the re-
maining 25% of the meetings.
3.2 Fine-grained and coarse-grained topics
We characterize a dialogue as a sequence of top-
ical segments that may be further divided into
subtopic segments. For example, the 60 minute
meeting Bed003, whose theme is the planning of
a research project on automatic speech recognition
can be described by 4 major topics, from ?open-
ing? to ?general discourse features for higher lay-
ers? to ?how to proceed? to ?closing?. Depending
on the complexity, each topic can be further di-
vided into a number of subtopics. For example,
?how to proceed? can be subdivided to 4 subtopic
segments, ?segmenting off regions of features?,
274
?ad-hoc probabilities?, ?data collection? and ?ex-
perimental setup?.
Three human annotators at our site used a tai-
lored tool to perform topic segmentation in which
they could choose to decompose a topic into
subtopics, with at most three levels in the resulting
hierarchy. Topics are described to the annotators
as what people in a meeting were talking about.
Annotators were asked to provide a free text la-
bel for each topic segment; they were encour-
aged to use keywords drawn from the transcrip-
tion in these labels, and we provided some stan-
dard labels for non-content topics, such as ?open-
ing? and ?chitchat?, to impose consistency. For
our initial experiments with automatic segmenta-
tion at different levels of granularity, we flattened
the subtopic structure and consider only two levels
of segmentation?top-level topics and all subtopics.
To establish reliability of our annotation proce-
dure, we calculated kappa statistics between the
annotations of each pair of coders. Our analy-
sis indicates human annotators achieve ? = 0.79
agreement on top-level segment boundaries and
? = 0.73 agreement on subtopic boundaries. The
level of agreement confirms good replicability of
the annotation procedure.
3.3 Probabilistic models
Our goal is to investigate the impact of ASR er-
rors on the selection of features and the choice of
models for segmenting topics at different levels of
granularity. We compare two segmentation mod-
els: (1) an unsupervised lexical cohesion-based
model (LM) using solely lexical cohesion infor-
mation, and (2) feature-based combined models
(CM) that are trained on a combination of lexical
cohesion and conversational features.
3.3.1 Lexical cohesion-based model
In this study, we use Galley et al?s (2003)
LCSeg algorithm, a variant of TextTiling (Hearst,
1997). LCSeg hypothesizes that a major topic
shift is likely to occur where strong term repeti-
tions start and end. The algorithm works with two
adjacent analysis windows, each of a fixed size
which is empirically determined. For each utter-
ance boundary, LCSeg calculates a lexical cohe-
sion score by computing the cosine similarity at
the transition between the two windows. Low sim-
ilarity indicates low lexical cohesion, and a sharp
change in lexical cohesion score indicates a high
probability of an actual topic boundary. The prin-
cipal difference between LCSeg and TextTiling is
that LCSeg measures similarity in terms of lexical
chains (i.e., term repetitions), whereas TextTiling
computes similarity using word counts.
3.3.2 Integrating lexical and
conversation-based features
We also used machine learning approaches that
integrate features into a combined model, cast-
ing topic segmentation as a binary classification
task. Under this supervised learning scheme, a
training set in which each potential topic bound-
ary2 is labelled as either positive (POS) or neg-
ative (NEG) is used to train a classifier to pre-
dict whether each unseen example in the test set
belongs to the class POS or NEG. Our objective
here is to determine whether the advantage of in-
tegrating lexical and conversational features also
improves automatic topic segmentation at the finer
granularity of subtopic levels, as well as when
ASR transcriptions are used.
For this study, we trained decision trees (c4.5)
to learn the best indicators of topic boundaries.
We first used features extracted with the optimal
window size reported to perform best in Galley et
al. (2003) for segmenting meeting transcripts into
major topical units. In particular, this study uses
the following features: (1) lexical cohesion fea-
tures: the raw lexical cohesion score and proba-
bility of topic shift indicated by the sharpness of
change in lexical cohesion score, and (2) conver-
sational features: the number of cue phrases in
an analysis window of 5 seconds preceding and
following the potential boundary, and other inter-
actional features, including similarity of speaker
activity (measured as a change in probability dis-
tribution of number of words spoken by each
speaker) within 5 seconds preceding and follow-
ing each potential boundary, the amount of over-
lapping speech within 30 seconds following each
potential boundary, and the amount of silence be-
tween speaker turns within 30 seconds preceding
each potential boundary.
3.4 Evaluation
To compare to prior work, we perform a 25-
fold leave-one-out cross validation on the set of
25 ICSI meetings that were used in Galley et
2In this study, the end of each speaker turn is a potential
segment boundary. If there is a pause of more than 1 second
within a single speaker turn, the turn is divided at the begin-
ning of the pause creating a potential segment boundary.
275
al. (2003). We repeated the procedure to eval-
uate the accuracy using the lexical cohesion and
combined models on both human and ASR tran-
scriptions. In each evaluation, we trained the au-
tomatic segmentation models for two tasks: pre-
dicting subtopic boundaries (SUB) and predicting
only top-level boundaries (TOP).
3.4.1 Evaluation metrics
In order to be able to compare our results di-
rectly with previous work, we first report our re-
sults using the standard error rate metrics of Pk
and Wd. Pk (Beeferman et al, 1999) is the prob-
ability that two utterances drawn randomly from a
document (in our case, a meeting transcript) are in-
correctly identified as belonging to the same topic
segment. WindowDiff (Wd) (Pevzner and Hearst,
2002) calculates the error rate by moving a sliding
window across the meeting transcript counting the
number of times the hypothesized and reference
segment boundaries are different.
3.4.2 Baseline
To compute a baseline, we follow Kan (2003)
and Hearst (1997) in using Monte Carlo simu-
lated segments. For the corpus used as training
data in the experiments, the probability of a poten-
tial topic boundary being an actual one is approxi-
mately 2.2% for all subtopic segments, and 0.69%
for top-level topic segments. Therefore, the Monte
Carlo simulation algorithm predicts that a speaker
turn is a segment boundary with these probabilities
for the two different segmentation tasks. We exe-
cuted the algorithm 10,000 times on each meeting
and averaged the scores to form the baseline for
our experiments.
3.4.3 Topline
For the 24 meetings that were used in training,
we have top-level topic boundaries annotated by
coders at Columbia University (Col) and in our lab
at Edinburgh (Edi). We take the majority opinion
on each segment boundary from the Col annota-
tors as reference segments. For the Edi annota-
tions of top-level topic segments, where multiple
annotations exist, we choose one randomly. The
topline is then computed as the Pk score compar-
ing the Col majority annotation to the Edi annota-
tion.
4 Results
4.1 Experiment 1: Predicting top-level and
subtopic segment boundaries
The meetings in the ICSI corpus last approxi-
mately 1 hour and have an average of 8-10 top-
level topic segments. In order to facilitate meet-
ing browsing and question-answering, we believe
it is useful to include subtopic boundaries in or-
der to narrow in more accurately on the portion
of the meeting that contains the information the
user needs. Therefore, we performed experiments
aimed at analysing how the LM and CM seg-
mentation models behave in predicting segment
boundaries at the two different levels of granular-
ity.
All of the results are reported on the test set.
Table 1 shows the performance of the lexical co-
hesion model (LM) and the combined model (CM)
integrating the lexical cohesion and conversational
features discussed in Section 3.3.2.3 For the task
of predicting top-level topic boundaries from hu-
man transcripts, CM outperforms LM. LM tends
to over-predict on the top-level, resulting in a
higher false alarm rate. However, for the task of
predicting subtopic shifts, LM alone is consider-
ably better than CM.
Error Rate Transcript ASR
Models Pk Wd Pk Wd
LM SUB 32.31% 38.18% 32.91% 37.13%
(LCSeg) TOP 36.50% 46.57% 38.02% 48.18%
CM SUB 36.90% 38.68% 38.19% n/a
(C4.5) TOP 28.35% 29.52% 28.38% n/a
Table 1: Performance comparison of probabilistic
segmentation models.
In order to support browsing during the meeting
or shortly thereafter, automatic topic segmentation
will have to operate on the transcriptions produced
by ASR. First note from Table 1 that the prefer-
ence of models for segmentation at the two differ-
ent levels of granularity is the same for ASR and
human transcriptions. CM is better for predicting
top-level boundaries and LM is better for predict-
ing subtopic boundaries. This suggests that these
3We do not report Wd scores for the combined model
(CM) on ASR output because this model predicted 0 segment
boundaries when operating on ASR output. In our experi-
ence, CM routinely underpredicted the number of segment
boundaries, and due to the nature of the Wd metric, it should
not be used when there are 0 hypothesized topic boundaries.
276
are two distinct tasks, regardless of whether the
system operates on human produced transcription
or ASR output. Subtopics are better characterized
by lexical cohesion, whereas top-level topic shifts
are signalled by conversational features as well as
lexical-cohesion based features.
4.1.1 Effect of feature combinations:
predicting from human transcripts
Next, we wish to determine which features in
the combined model are most effective for predict-
ing topic segments at the two levels of granularity.
Table 2 gives the average Pk for all 25 meetings
in the test set, using the features described in Sec-
tion 3.3.2. We group the features into four classes:
(1) lexical cohesion-based features (LF): including
lexical cohesion value (LCV) and estimated pos-
terior probability (LCP); (2) interaction features
(IF): the amount of overlapping speech (OVR),
the amount of silence between speaker segments
(GAP), similarity of speaker activity (ACT); (3)
cue phrase feature (CUE); and (4) all available fea-
tures (ALL). For comparison we also report the
baseline (see Section 3.4.2) generated by Monte
Carlo algorithm (MC-B). All of the models us-
ing one or more features from these classes out-
perform the baseline model. A one-way ANOVA
revealed this reliable effect on the top-level seg-
mentation (F (7, 192) = 17.46, p < 0.01) as well
as on the subtopic segmentation task (F (7, 192) =
5.862, p < 0.01).
TRANSCRIPT Error Rate(Pk)
Feature set SUB TOP
MC-B 46.61% 48.43%
LF(LCV+LCP) 38.13% 29.92%
IF(ACT+OVR+GAP) 38.87% 30.11%
IF+CUE 38.87% 30.11%
LF+ACT 38.70% 30.10%
LF+OVR 38.56% 29.48%
LF+GAP 38.50% 29.87%
LF+IF 38.11% 29.61%
LF+CUE 37.46% 29.18%
ALL(LF+IF+CUE) 36.90% 28.35%
Table 2: Effect of different feature combinations
for predicting topic boundaries from human tran-
scripts. MC-B is the randomly generated baseline.
As shown in Table 2, the best performing model
for predicting top-level segments is the one us-
ing all of the features (ALL). This is not surpris-
ing, because these were the features that Galley
et al (2003) found to be most effective for pre-
dicting top-level segment boundaries in their com-
bined model. Looking at the results in more de-
tail, we see that when we begin with LF features
alone and add other features one by one, the only
model (other than ALL) that achieves significant4
improvement (p < 0.05) over LF is LF+CUE,
the model that combines lexical cohesion features
with cue phrases.
When we look at the results for predicting
subtopic boundaries, we again see that the best
performing model is the one using all features
(ALL). Models using lexical-cohesion features
alone (LF) and lexical cohesion features with cue
phrases (LF+CUE) both yield significantly better
results than using interactional features (IF) alone
(p < 0.01), or using them with cue phrase features
(IF+CUE) (p < 0.01). Again, none of the interac-
tional features used in combination with LF sig-
nificantly improves performance. Indeed, adding
speaker activity change (LF+ACT) degrades the
performance (p < 0.05).
Therefore, we conclude that for predicting both
top-level and subtopic boundaries from human
transcriptions, the most important features are the
lexical cohesion based features (LF), followed
by cue phrases (CUE), with interactional features
contributing to improved performance only when
used in combination with LF and CUE.
However, a closer look at the Pk scores in Ta-
ble 2, adds further evidence to our hypothesis that
predicting subtopics may be a different task from
predicting top-level topics. Subtopic shifts oc-
cur more frequently, and often without clear con-
versational cues. This is suggested by the fact
that absolute performance on subtopic prediction
degrades when any of the interactional features
are combined with the lexical cohesion features.
In contrast, the interactional features slightly im-
prove performance when predicting top-level seg-
ments. Moreover, the fact that the feature OVR
has a positive impact on the model for predicting
top-level topic boundaries, but does not improve
the model for predicting subtopic boundaries re-
veals that having less overlapping speech is a more
prominent phenomenon in major topic shifts than
4Because we do not wish to make assumptions about the
underlying distribution of error rates, and error rates are not
measured on an interval level, we use a non-parametric sign
test throughout these experiments to compute statistical sig-
nificance.
277
in subtopic shifts.
4.1.2 Effect of feature combinations:
predicting from ASR output
Features extracted from ASR transcripts are dis-
tinct from those extracted from human transcripts
in at least three ways: (1) incorrectly recognized
words incur erroneous lexical cohesion features
(LF), (2) incorrectly recognized words incur erro-
neous cue phrase features (CUE), and (3) the ASR
system recognizes less overlapping speech (OVR).
In contrast to the finding that integrating conver-
sational features with lexical cohesion features is
useful for prediction from human transcripts, Ta-
ble 3 shows that when operating on ASR output,
neither adding interactional nor cue phrase fea-
tures improves the performance of the model using
only lexical cohesion features. In fact, the model
using all features (ALL) is significantly worse than
the model using only lexical cohesion based fea-
tures (LF). This suggests that we must explore new
features that can lessen the perplexity introduced
by ASR outputs in order to train a better model.
ASR Error Rate(Pk)
Feature set SUB TOP
MC-B 43.41% 45.22%
LF(LCV+LCP) 36.83% 25.27%
IF(ACT+OVR+GAP) 36.83% 25.27%
IF+CUE 36.83% 25.27%
LF+GAP 36.67% 24.62%
LF+IF 36.83% 28.24%
LF+CUE 37.42% 25.27%
ALL(LF+IF+CUE) 38.19% 28.38%
Table 3: Effect of different feature combinations
for predicting topic boundaries from ASR output.
4.2 Experiment 2: Statistically learned cue
phrases
In prior work, Galley et al (2003) empirically
identified cue phrases that are indicators of seg-
ment boundaries, and then eliminated all cues that
had not previously been identified as cue phrases
in the literature. Here, we conduct an experiment
to explore how different ways of identifying cue
phrases can help identify useful new features for
the two boundary prediction tasks.
In each fold of the 25-fold leave-one-out cross
validation, we use a modified5 Chi-square test to
5In order to satisfy the mathematical assumptions under-
calculate statistics for each word (unigram) and
word pair (bi-gram) that occurred in the 24 train-
ing meetings. We then rank unigrams and bigrams
according to their Chi-square scores, filtering out
those with values under 6.64, the threshold for the
Chi-square statistic at the 0.01 significance level.
The unigrams and bigrams in this ranked list are
the learned cue phrases. We then use the occur-
rence counts of cue phrases in an analysis window
around each potential topic boundary in the test
meeting as a feature.
Table 4 shows the performance of models that
use statistically learned cue phrases in their feature
sets compared with models using no cue phrase
features and Galley?s model, which only uses cue
phrases that correspond to those identified in the
literature (Col-cue). We see that for predicting
subtopics, models using the cue word features
(1gram) and the combination of cue words and bi-
grams (1+2gram) yield a 15% and 8.24% improve-
ment over models using no cue features (NOCUE)
(p < 0.01) respectively, while models using only
cue phrases found in the literature (Col-cue) im-
prove performance by just 3.18%. In contrast, for
predicting top-level topics, the model using cue
phrases from the literature (Col-cue) achieves a
4.2% improvement, and this is the only model that
produces statistically significantly better results
than the model using no cue phrases (NOCUE).
The superior performance of models using statis-
tically learned cue phrases as features for predict-
ing subtopic boundaries suggests there may exist a
different set of cue phrases that serve as segmen-
tation cues for subtopic boundaries.
5 Discussion
As observed in the corpus of meetings, the lack
of macro-level segment units (e.g., story breaks,
paragraph breaks) makes the task of segmenting
spontaneous multiparty dialogue, such as meet-
ings, different from segmenting text or broadcast
news. Compared to the task of segmenting expos-
itory texts reported in Hearst (1997) with a 39.1%
chance of each paragraph end being a target topic
boundary, the chance of each speaker turn be-
ing a top-level or sub-topic boundary in our ICSI
corpus is just 2.2% and 0.69%. The imbalanced
class distribution has a negative effect on the per-
lying the test, we removed cases with an expected value that
is under a threshold (in this study, we use 1), and we apply
Yate?s correction, (|ObservedV alue?ExpectedV alue| ?
0.5)2/ExpectedV alue.
278
NOCUE Col-cue 1gram 2gram 1+2gram MC-B Topline
SUB 38.11% 36.90% 32.39% 36.86% 34.97% 46.61% n/a
TOP 29.61% 28.35% 28.95% 29.20% 29.27% 48.43% 13.48%
Table 4: Performance of models trained with cue phrases from the literature (Col-cue) and cue phrases
learned from statistical tests, including cue words (1gram), cue word pairs (2gram), and cue phrases
composed of both words and word pairs (1+2gram). NOCUE is the model using no cue phrase features.
The Topline is the agreement of human annotators on top-level segments.
0 10 20 30 40 50 60 70 80
0.26
0.28
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
Training Set Size (In meetings)
E
rr
or
 R
at
e 
(P
k)
TRAN?ALL
TRAN?TOP
ASR?ALL
ASR?TOP
Figure 1: Performance of the combined model
over the increase of the training set size.
formance of machine learning approaches. In a
pilot study, we investigated sampling techniques
that rebalance the class distribution in the train-
ing set. We found that sampling techniques pre-
viously reported in Liu et al(2004) as useful for
dealing with an imbalanced class distribution in
the task of disfluency detection and sentence seg-
mentation do not work for this particular data set.
The implicit assumption of some classifiers (such
as pruned decision trees) that the class distribution
of the test set matches that of the training set, and
that the costs of false positives and false negatives
are equivalent, may account for the failure of these
sampling techniques to yield improvements in per-
formance, when measured using Pk and Wd.
Another approach that copes with the im-
balanced class prediction problem but does not
change the natural class distribution is to increase
the size of the training set. We conducted an ex-
periment in which we incrementally increased the
training set size by randomly choosing ten meet-
ings each time until all meetings were selected.
We executed the process three times and averaged
the scores to obtain the results shown in Figure 1.
However, increasing training set size adds to the
perplexity in the training phase. We see that in-
creasing the size of the training set only improves
the accuracy of segment boundary prediction for
predicting top-level topics on ASR output. The
figure also indicates that training a model to pre-
dict top-level boundaries requires no more than fif-
teen meetings in the training set to reach a reason-
able level of performance.
6 Conclusions
Discovering major topic shifts and finding nested
subtopics are essential for the success of speech
document browsing and retrieval. Meeting records
contain rich information, in both content and con-
versation behavioral form, that enable automatic
topic segmentation at different levels of granular-
ity. The current study demonstrates that the two
tasks ? predicting top-level and subtopic bound-
aries ? are distinct in many ways: (1) for pre-
dicting subtopic boundaries, the lexical cohesion-
based approach achieves results that are com-
petitive with the machine learning approach that
combines lexical and conversational features; (2)
for predicting top-level boundaries, the machine
learning approach performs the best; and (3) many
conversational cues, such as overlapping speech
and cue phrases discussed in the literature, are
better indicators for top-level topic shifts than
for subtopic shifts, but new features such as cue
phrases can be learned statistically for the subtopic
prediction task. Even in the presence of a rela-
tively higher word error rate, using ASR output
makes no difference to the preference of model for
the two tasks. The conversational features also did
not help improve the performance for predicting
from ASR output.
In order to further identify useful features for
automatic segmentation of meetings at different
levels of granularity, we will explore the use of
279
multimodal, i.e., acoustic and visual, cues. In ad-
dition, in the current study, we only extracted fea-
tures from within the analysis windows immedi-
ately preceding and following each potential topic
boundary; we will explore models that take into
account features of longer range dependencies.
7 Acknowledgements
Many thanks to Jean Carletta for her invaluable
help in managing the data, and for advice and
comments on the work reported in this paper.
Thanks also to the AMI ASR group for produc-
ing the ASR transcriptions, and to the anonymous
reviewers for their helpful comments. This work
was supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811).
References
J. Allan, J.G. Carbonell, G. Doddington, J. Yamron,
and Y. Yang. 1998. Topic detection and tracking pi-
lot study: Final report. In Proceedings of the DARPA
Broadcast News Transcription and Understanding
Workshop.
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statis-
tical models for text segmentation. Machine Learn-
ing, 34:177?210.
D. M. Blei and P. J. Moreno. 2001. Topic segmentation
with an aspect hidden Markov model. In Proceed-
ings of the 24th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. ACM Press.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2005. Maximum entropy segmentation of broad-
cast news. In Proceedings of the IEEE International
Conference on Acoustic, Speech, and Signal Pro-
cessing, Philadelphia, USA.
S. Dharanipragada, M. Franz, J.S. McCarley, K. Pap-
ineni, S. Roukos, T. Ward, and W. J. Zhu. 2000.
Statistical methods for topic segmentation. In Pro-
ceedings of the International Conference on Spoken
Language Processing, pages 516?519.
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
M. Gavalda, K. Zechner, and G. Aist. 1997. High per-
formance segmentation of spontaneous speech using
part of speech and trigger word information. In Pro-
ceedings of the Fifth ANLP Conference, pages 12?
15.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, and S. Renals. 2005. Tran-
scription of conference room meetings: an investi-
gation. In Proceedings of Interspeech.
M. Hearst. 1997. Texttiling: Segmenting text into mul-
tiparagraph subtopic passages. Computational Lin-
guistics, 25(3):527?571.
M. Kan. 2003. Automatic text summarization as
applied to information retrieval: Using indicative
and informative summaries. Ph.D. thesis, Columbia
University, New York USA.
Y. Liu, E. Shriberg, A. Stolcke, and M. Harper. 2004.
Using machine learning to cope with imbalanced
classes in natural sppech: Evidence from sentence
boundary and disfluency detection. In Proceedings
of the Intl. Conf. Spoken Language Processing.
N. Morgan, D. Baron, S. Bhagat, H. Carvey,
R. Dhillon, J. Edwards, D. Gelbart, A. Janin,
A. Krupski, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, , and C. Wooters. 2003. Meetings about meet-
ings: research at icsi on speech in multiparty conver-
sations. In Proceedings of the IEEE International
Conference on Acoustic, Speech, and Signal Pro-
cessing.
L. Pevzner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36.
E. Shriberg, A. Stolcke, D. Hakkani-tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Commu-
nications, 31(1-2):127?254.
N. Stokes, J. Carthy, and A.F. Smeaton. 2004. Se-
lect: a lexical cohesion based news story segmenta-
tion system. AI Communications, 17(1):3?12, Jan-
uary.
P. van Mulbregt, J. Carp, L. Gillick, S. Lowe, and
J. Yamron. 1999. Segmentation of automatically
transcribed broadcast news text. In Proceedings of
the DARPA Broadcast News Workshop, pages 77?
80. Morgan Kaufman Publishers.
Klaus Zechner and Alex Waibel. 2000. DIASUMM:
Flexible summarization of spontaneous dialogues in
unrestricted domains. In Proceedings of COLING-
2000, pages 968?974.
280
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 367?374,
New York, June 2006. c?2006 Association for Computational Linguistics
Incorporating Speaker and Discourse Features into Speech
Summarization
Gabriel Murray, Steve Renals,
Jean Carletta, Johanna Moore
University of Edinburgh, School of Informatics
Edinburgh EH8 9LW, Scotland
gabriel.murray@ed.ac.uk, s.renals@ed.ac.uk,
jeanc@inf.ed.ac.uk, j.moore@ed.ac.uk
Abstract
We have explored the usefulness of incorporat-
ing speech and discourse features in an automatic
speech summarization system applied to meeting
recordings from the ICSI Meetings corpus. By an-
alyzing speaker activity, turn-taking and discourse
cues, we hypothesize that such a system can out-
perform solely text-based methods inherited from
the field of text summarization. The summariza-
tion methods are described, two evaluation meth-
ods are applied and compared, and the results
clearly show that utilizing such features is advanta-
geous and efficient. Even simple methods relying
on discourse cues and speaker activity can outper-
form text summarization approaches.
1. Introduction
The task of summarizing spontaneous spoken di-
alogue from meetings presents many challenges:
information is sparse; speech is disfluent and frag-
mented; automatic speech recognition is imper-
fect. However, there are numerous speech-specific
characteristics to be explored and taken advantage
of. Previous research on summarizing speech has
concentrated on utilizing prosodic features [1, 2].
We have examined the usefulness of additional
speech-specific characteristics such as discourse
cues, speaker activity, and listener feedback. This
speech features approach is contrasted with a sec-
ond summarization approach using only textual
features?a centroid method [3] using a latent se-
mantic representation of utterances. These indi-
vidual approaches are compared to a combined ap-
proach as well as random baseline summaries.
This paper also introduces a new evalua-
tion scheme for automatic summaries of meeting
recordings, using a weighted precision score based
on multiple human annotations of each meeting
transcript. This evaluation scheme is described
in detail below and is motivated by previous find-
ings [4] suggesting that n-gram based metrics like
ROUGE [5] do not correlate well in this domain.
2. Previous Work
In the field of speech summarization in general, re-
search investigating speech-specific characteristics
has focused largely on prosodic features such as F0
mean and standard deviation, pause information,
syllable duration and energy. Koumpis and Re-
nals [1] investigated prosodic features for summa-
rizing voicemail messages in order to send voice-
mail summaries to mobile devices. Hori et al [6]
have developed an integrated speech summariza-
tion approach, based on finite state transducers, in
which the recognition and summarization compo-
nents are composed into a single finite state trans-
ducer, reporting results on a lecture summariza-
tion task. In the Broadcast News domain, Maskey
and Hirschberg [7] found that the best summariza-
tion results utilized prosodic, lexical, and structural
features, while Ohtake et al [8] explored using
only prosodic features for summarization. Maskey
and Hirschberg similarly found that prosodic fea-
tures alone resulted in good quality summaries of
367
Broadcast News.
In the meetings domain (using the ICSI cor-
pus), Murray et al [2] compared text summariza-
tion approaches with feature-based approaches us-
ing prosodic features, with human judges favoring
the feature-based approaches. Zechner [9] inves-
tigated summarizing several genres of speech, in-
cluding spontaneous meeting speech. Though rel-
evance detection in his work relied largely on tf.idf
scores, Zechner also explored cross-speaker infor-
mation linking and question/answer detection, so
that utterances could be extracted not only accord-
ing to high tf.idf scores, but also if they were linked
to other informative utterances.
Similarly, this work aims to detect important
utterances that may not be detectable according
to lexical features or prosodic prominence, but
are nonetheless linked to high speaker activity,
decision-making, or meeting structure.
3. Summarization Approaches
The following subsections give detailed descrip-
tions of our two summarization systems, one of
which focuses on speech and discourse features
while the other utilizes text summarization tech-
niques and latent semantic analysis.
3.1. Speech and Discourse Features
In previous summarization work on the ICSI cor-
pus [2, 4], Murray et al explored multiple ways
of applying latent semantic analysis (LSA) to a
term/document matrix of weighted term frequen-
cies from a given meeting, a development of the
method in [10]. A central insight to the present
work is that additional features beyond simple term
frequencies can be included in the matrix before
singular value decomposition (SVD) is carried out.
We can use SVD to project this matrix of features
to a lower dimensionality space, subsequently ap-
plying the same methods as used in [2] for extract-
ing sentences.
The features used in these experiments in-
cluded features of speaker activity, discourse cues,
listener feedback, simple keyword spotting, meet-
ing location and dialogue act length (in words).
For each dialogue act, there are features indi-
cating which speaker spoke the dialogue act and
whether the same speaker spoke the preceding and
succeeding dialogue acts. Another set of features
indicates how many speakers are active on either
side of a given dialogue act: specifically, how
many speakers were active in the preceding and
succeeding five dialogue acts. To further gauge
speaker activity, we located areas of high speaker
interaction and indicated whether or not a given
dialogue act immediately preceded this region of
activity, with the motivation being that informa-
tive utterances are often provocative in eliciting re-
sponses and interaction. Additionally, we included
a feature indicating which speakers most often ut-
tered dialogue acts that preceded high levels of
speaker interaction, as one way of gauging speaker
status in the meeting. Another feature relating to
speaker activity gives each dialogue act a score ac-
cording to how active the speaker is in the meeting
as a whole, based on the intuition that the most ac-
tive speakers will tend to utter the most important
dialogue acts.
The features for discourse cues, listener feed-
back, and keyword spotting were deliberately su-
perficial, all based simply on detecting informative
words. The feature for discourse cues indicates the
presence or absence of words such as decide, dis-
cuss, conclude, agree, and fragments such as we
should indicating a planned course of action. Lis-
tener feedback was based on the presence or ab-
sence of positive feedback cues following a given
dialogue act; these include responses such as right,
exactly and yeah. Keyword spotting was based
on frequent words minus stopwords, indicating the
presence or absence of any of the top twenty non-
stopword frequent words. The discourse cues of
interest were derived from a manual corpus analy-
sis rather than being automatically detected.
A structural feature scored dialogue acts ac-
cording to their position in the meeting, with di-
alogue acts from the middle to later portion of the
meeting scoring higher and dialogue acts at the be-
ginning and very end scoring lower. This is a fea-
ture that is well-matched to the relatively unstruc-
tured ICSI meetings, as many meetings would be
expected to have informative proposals and agen-
das at the beginning and perhaps summary state-
ments and conclusions at the end.
Finally, we include a dialogue act length fea-
ture motivated by the fact that informative utter-
ances will tend to be longer than others.
The extraction method follows [11] by rank-
ing sentences using an LSA sentence score. The
368
matrix of features is decomposed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n?n matrix of right-singular vectors.
Using sub-matrices S and V T , the LSA sentence
scores are obtained using:
ScLSAi =
?
?
?
?
n
?
k=1
v(i, k)2 ? ?(k)2 ,
where v(i, k) is the kth element of the ith sen-
tence vector and ?(k) is the corresponding singular
value.
Experiments on a development set of 55 ICSI
meetings showed that reduction to between 5?15
dimension was optimal. These development ex-
periments also showed that weighting some fea-
tures slightly higher than others resulted in much
improved results; specifically, the discourse cues
and listener feedback cues were weighted slightly
higher.
3.2. LSA Centroid
The second summarization method is a textual ap-
proach incorporating LSA into a centroid-based
system [3]. The centroid is a pseudo-document
representing the important aspects of the docu-
ment as a whole; in the work of [3], this pseudo-
document consists of keywords and their modi-
fied tf.idf scores. In the present research, we take
a different approach to constructing the centroid
and to representing sentences in the document.
First, tf.idf scores are calculated for all words in
the meeting. Using these scores, we find the top
twenty keywords and choose these as the basis for
our centroid. We then perform LSA on a very large
corpus of Broadcast News and ICSI data, using the
Infomap tool1. Infomap provides a query language
with which we can retrieve word vectors for our
twenty keywords, and the centroid is thus repre-
sented as the average of its constituent keyword
vectors [12] [13].
Dialogue acts from the meetings are repre-
sented in much the same fashion. For each dia-
logue act, the vectors of its constituent words are
1http://infomap.stanford.edu
retrieved, and the dialogue act as a whole is the av-
erage of its word vectors. Extraction then proceeds
by finding the dialogue act with the highest cosine
similarity with the centroid, adding this to the sum-
mary, then continuing until the desired summary
length is reached.
3.3. Combined
The third summarization method is simply a com-
bination of the first two. Each system produces a
ranking and a master ranking is derived from these
two rankings. The hypothesis is that the strength
of one system will differ from the other and that
the two will complement each other and produce
a good overall ranking. The first system would be
expected to locate areas of high activity, decision-
making, and planning, while the second would lo-
cate information-rich utterances. This exempli-
fies one of the challenges of summarizing meeting
recordings: namely, that utterances can be impor-
tant in much different ways. A comprehensive sys-
tem that relies on more than one idea of importance
is ideal.
4. Experimental Setup
All summaries were 350 words in length, much
shorter than the compression rate used in [2] (10%
of dialogue acts). The ICSI meetings themselves
average around 10,000 words in length. The rea-
sons for choosing a shorter length for summaries
are that shorter summaries are more likely to be
useful to a user wanting to quickly overview and
browse a meeting, they present a greater summa-
rization challenge in that the summarizer must be
more exact in pinpointing the important aspects of
the meeting, and shorter summaries make it more
feasible to enlist human evaluators to judge the nu-
merous summaries on various criteria in the future.
Summaries were created on both manual tran-
scripts and speech recognizer output. The unit of
extraction for these summaries was the dialogue
act, and these experiments used human segmented
and labeled dialogue acts rather than try to detect
them automatically. In future work, we intend to
incorporate dialogue act detection and labeling as
part of one complete automatic summarization sys-
tem.
369
4.1. Corpus Description
The ICSI Meetings corpus consists of 75 meetings,
lasting approximately one hour each. Our test set
consists of six meetings, each with multiple hu-
man annotations. Annotators were given access
to a graphical user interface (GUI) for browsing
an individual meeting that included earlier human
annotations: an orthographic transcription time-
synchronized with the audio, and a topic segmen-
tation based on a shallow hierarchical decompo-
sition with keyword-based text labels describing
each topic segment. The annotators were told to
construct a textual summary of the meeting aimed
at someone who is interested in the research being
carried out, such as a researcher who does similar
work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for
each heading, and told that there must be text for
the general abstract, but that the other headings
may have null annotations for some meetings. An-
notators who were new to the data were encour-
aged to listen to a meeting straight through before
beginning to author the summary.
Immediately after authoring a textual sum-
mary, annotators were asked to create an extractive
summary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding [14]. Annotators were told to ex-
tract dialogue acts that together would convey the
information in the textual summary, and could be
used to support the correctness of that summary.
They were given no specific instructions about the
number or percentage of acts to extract or about
redundant dialogue acts. For each dialogue act ex-
tracted, they were then required in a second pass
to choose the sentences from the textual summary
supported by the dialogue act, creating a many-
to-many mapping between the recording and the
textual summary. Although the expectation was
that each extracted dialogue act and each summary
sentence would be linked to something in the op-
posing resource, we told the annotators that under
some circumstances dialogue acts and summary
sentences could stand alone.
We created summaries using both manual tran-
scripts as well as automatic speech recognition
(ASR) output. The AMI-ASR system [15] is de-
scribed in more detail in [4] and the average word
error rate (WER) for the corpus is 29.5%.
4.2. Evaluation Frameworks
The many-to-many mapping of dialogue acts to
summary sentences described in the previous sec-
tion allows us to evaluate our extractive summaries
according to how often each annotator linked a
given extracted dialogue act to a summary sen-
tence. This is somewhat analogous to Pyramid
weighting [16], but with dialogue acts as the SCUs.
In fact, we can calculate weighted precision, recall
and f-score using these annotations, but because
the summaries created are so short, we focus on
weighted precision as our central metric. For each
dialogue act that the summarizer extracts, we count
the number of times that each annotator links that
dialogue act to a summary sentence. For a given
dialogue act, it may be that one annotator links it
0 times, one annotator links it 1 time, and the third
annotator links it two times, resulting in an aver-
age score of 1 for that dialogue act. The scores for
all of the summary dialogue acts can be calculated
and averaged to create an overall summary score.
ROUGE scores, based on n-gram overlap be-
tween human abstracts and automatic extracts,
were also calculated for comparison [5]. ROUGE-
2, based on bigram overlap, is considered the most
stable as far as correlating with human judgments,
and this was therefore our ROUGE metric of inter-
est. ROUGE-SU4, which evaluates bigrams with
intervening material between the two elements of
the bigram, has recently been shown in the con-
text of the Document Understanding Conference
(DUC)2 to bring no significant additional informa-
tion as compared with ROUGE-2. Results from
[4] and from DUC 2005 also show that ROUGE
does not always correlate well with human judg-
ments. It is therefore included in this research in
the hope of further determining how reliable the
2http://duc.nist.gov
370
ROUGE metric is for our domain of meeting sum-
marization.
5. Results
The experimental results are shown in figure 1
(weighted precision) and figure 2 (ROUGE-2) and
are discussed below.
5.1. Weighted Precision Results
For weighted precision, the speech features ap-
proach was easily the best and scored significantly
better than the centroid and random approaches
(ANOVA,p<0.05), attaining an averaged weighted
precision of 0.52. The combined approach did
not improve upon the speech features approach
but was not significantly worse either. The ran-
domly created summaries scored much lower than
all three systems.
The superior performance of the speech fea-
tures approach compared to the LSA centroid
method closely mirrors results on the ICSI devel-
opment set, where the centroid method scored 0.23
and the speech features approach scored 0.42. For
the speech features approach on the test set, the
best feature by far was dialogue act length. Re-
moving this feature resulted in the precision score
being nearly halved. This mirrors results from
Maskey and Hirschberg [7], who found that the
length of a sentence in seconds and its length in
words were the two best features for predicting
summary sentences. Both the simple keyword
spotting and the discourse cue detection features
caused a lesser decline in precision when removed,
while other features of speaker activity had a neg-
ligible impact on the test results.
Interestingly, the weighted precision scores on
ASR were not significantly worse for any of the
summarization approaches. In fact, the centroid
approach scored very slightly higher on ASR out-
put than on manual transcripts. In [17] and [2] it
was similarly found that summarizing with ASR
output did not cause great deterioration in the qual-
ity of the summaries. It is not especially surpris-
ing that the speech features approach performed
similarly on both manual and ASR transcripts, as
many of its features based on speaker exchanges
and speaker activity would be unaffected by ASR
errors. The speech features approach is still signif-
icantly better than the random and centroid sum-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
PRECISION-MAN
PRECISION-ASR
Figure 1: Weighted Precision Results on Test Set
maries, and is not significantly better than the com-
bined approach on ASR.
5.2. ROUGE Results
The ROUGE results greatly differed from the
weighted precision results in several ways. First,
the centroid method was considered to be the best,
with a ROUGE-2 score of 0.047 compared with
0.041 for the speech features approach. Second,
there were not as great of differences between the
four systems according to ROUGE as there were
according to weighted precision. In fact, the ran-
dom summaries of manual transcripts are not sig-
nificantly worse than the other approaches, accord-
ing to ROUGE-2. Neither the combined approach
nor the speech features approach is significantly
worse than the centroid system, with the combined
approach generally scoring on par with the cen-
troid scores.
The third difference relates to summarization
on ASR output. ROUGE-2 has the random system
and the combined system showing sharp declines
when applied to ASR transcripts. The speech fea-
tures and centroid approaches do not show de-
clines. Random summaries are significantly worse
than both the centroid summaries (p<0.1) and
speech features summaries (p<0.05). Though the
combined approach declines on ASR output, it is
not significantly worse than the other systems.
To get an idea of a ROUGE-2 upper bound, for
each meeting in the test set we left one human ab-
stract out and compared it with the remaining ab-
stracts. The result was an average ROUGE-2 score
of .086.
371
 0.02
 0.04
 0.06
 0.08
 0.1
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
ROUGE2-MAN
ROUGE2-ASR
UPPER BOUND
Figure 2: ROUGE-2 Results on Test Set
ROUGE-1 and ROUGE-SU4 show no signif-
icant differences between the centroid and speech
features approaches.
5.3. Correlations
There is no significant correlation between
macroaveraged ROUGE and weighted precision
scores across the meeting set, on both ASR and
manual transcripts. The Pearson correlation is
0.562 with a significance of p < 0.147. The Spear-
man correlation is 0.282 with a significance of p <
0.498. The correlation of scores across each test
meeting is worse yet, with a Pearson correlation
of 0.185 (p<0.208) and a Spearman correlation of
0.181 (p<0.271).
5.4. Sample Summary
The following is the text of a summary of meeting
Bed004 using the speech features approach:
-so its possible that we could do something like a summary
node of some sort that
-and then the question would be if if those are the things that you
care about uh can you make a relatively compact way of getting from
the various inputs to the things you care about
-this is sort of th the second version and i i i look at this maybe just
as a you know a a whatever uml diagram or you know as just a uh
screen shot not really as a bayes net as john johno said
-and um this is about as much as we can do if we dont w if we want
to avoid uh uh a huge combinatorial explosion where we specify ok if
its this and this but that is not the case and so forth it just gets really
really messy
-also it strikes me that we we m may want to approach the point
where we can sort of try to find a uh a specification for some interface
here that um takes the normal m three l looks at it
-so what youre trying to get out of this deep co cognitive linguistics is
the fact that w if you know about source source paths and goals and
nnn all this sort of stuff that a lot of this is the same for different tasks
-what youd really like of course is the same thing youd always like
which is that you have um a kind of intermediate representation
which looks the same o over a bunch of inputs and a bunch of outputs
-and pushing it one step further when you get to construction
grammar and stuff what youd like to be able to do is say you have
this parser which is much fancier than the parser that comes with uh
smartkom
-in independent of whether it about what is this or where is it or
something that you could tell from the construction you could pull
out deep semantic information which youre gonna use in a general
way
6. Discussion
Though the speech features approach was consid-
ered the best system, it is unclear why the com-
bined approach did not yield improvement. One
possibility relates to the extreme brevity of the
summaries: because the summaries are only 350
words in length, it is possible to have two sum-
maries of the same meeting which are equally
good but completely non-overlapping in content.
In other words, they both extract informative dia-
logue acts, but not the same ones. Combining the
rankings of two such systems might create a third
system which is comparable but not any better than
either of the first two systems alone. However, it
is still possible that the combined system will be
better in terms of balancing the two types of im-
portance discussed above: utterances that contain a
lot of informative content and keywords and utter-
ances that relate to decision-making and meeting
structure.
ROUGE did not correlate well with the
weighted precision scores, a result that adds to the
previous evidence that this metric may not be reli-
able in the domain of meeting summarization.
It is very encouraging that the summarization
approaches in general seem immune to the WER
of the ASR output. This confirms previous find-
ings such as [17] and [2], and the speech and
structural features used herein are particularly un-
affected by a moderately high WER. The reason
for the random summarizaton system not suffering
372
a sharp decline when applied to ASR may be due
to the fact that its scores were already so low that
it couldn?t deteriorate any further.
7. Future Work
The above results show that even a relatively small
set of speech, discourse, and structural features can
outperform a text summarization approach on this
data, and there are many additional features to be
explored. Of particular interest to us are features
relating to speaker status, i.e. features that help us
determine who is leading the meeting and who it is
that others are deferring to. We would also like to
more closely investigate the relationship between
areas of high speaker activity and informative ut-
terances.
In the immediate future, we will incorporate
these features into a machine-learning framework,
building support vector models trained on the ex-
tracted and non-extracted classes of the training
set.
Finally, we will apply these methods to the
AMI corpus [18] and create summaries of compa-
rable length for that meeting set. There are likely
to be differences regarding usefulness of certain
features due to the ICSI meetings being relatively
unstructured and informal and the AMI hub meet-
ings being more structured with a higher informa-
tion density.
8. Conclusion
The results presented above show that using fea-
tures related to speaker activity, listener feedback,
discourse cues and dialogue act length can outper-
form the lexical methods of text summarization ap-
proaches. More specifically, the fact that there are
multiple types of important utterances requires that
we use multiple methods of detecting importance.
Lexical methods and prosodic features are not nec-
essarily going to detect utterances that are relevant
to agreement, decision-making or speaker activity.
This research also provides further evidence that
ROUGE does not correlate well with human judg-
ments in this domain. Finally, it has been demon-
strated that high WER for ASR output does not
significantly decrease summarization quality.
9. Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication AMI-
150).
10. References
[1] K. Koumpis and S. Renals, ?Automatic sum-
marization of voicemail messages using lex-
ical and prosodic features,? ACM Transac-
tions on Speech and Language Processing,
vol. 2, pp. 1?24, 2005.
[2] G. Murray, S. Renals, and J. Carletta, ?Ex-
tractive summarization of meeting record-
ings,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[3] D. Radev, S. Blair-Goldensohn, and
Z. Zhang, ?Experiments in single and multi-
document summarization using mead,? in
The Proceedings of the First Document
Understanding Conference, New Orleans,
LA, September 2001.
[4] G. Murray, S. Renals, J. Carletta, and
J. Moore, ?Evaluating automatic summaries
of meeting recordings,? in Proceedings of
the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop on Machine Translation and Summa-
rization Evaluation (MTSE), Ann Arbor, MI,
USA, June 2005.
[5] C.-Y. Lin and E. H. Hovy, ?Automatic
evaluation of summaries using n-gram co-
occurrence statistics,? in Proceedings of
HLT-NAACL 2003, Edmonton, Calgary,
Canada, May 2003.
[6] T. Hori, C. Hori, and Y. Minami, ?Speech
summarization using weighted finite-state
transducers,? in Proceedings of the 8th Eu-
ropean Conference on Speech Communica-
tion and Technology, Geneva, Switzerland,
September 2003.
373
[7] S. Maskey and J. Hirschberg, ?Compar-
ing lexial, acoustic/prosodic, discourse and
structural features for speech summariza-
tion,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[8] K. Ohtake, K. Yamamoto, Y. Toma, S. Sado,
S. Masuyama, and S. Nakagawa, ?Newscast
speech summarization via sentence shorten-
ing based on prosodic features,? in Proceed-
ings of the ISCA and IEEE Workshop on
Spontaneous Speech Processing and Recog-
nition, Tokyo, Japan, April 2003,.
[9] K. Zechner, ?Automatic summarization of
open-domain multiparty dialogues in diverse
genres,? Computational Linguistics, vol. 28,
no. 4, pp. 447?485, 2002.
[10] Y. Gong and X. Liu, ?Generic text sum-
marization using relevance measure and la-
tent semantic analysis,? in Proceedings of
the 24th Annual International ACM SI-
GIR Conference on Research and Develop-
ment in Information Retrieval, New Orleans,
Louisiana, USA, September 2001, pp. 19?25.
[11] J. Steinberger and K. Jez?ek, ?Using latent
semantic analysis in text summarization and
summary evaluation,? in Proceedings of ISIM
2004, Roznov pod Radhostem, Czech Repub-
lic, April 2004, pp. 93?100.
[12] P. Foltz, W. Kintsch, and T. Landauer, ?The
measurement of textual coherence with la-
tent semantic analysis,? Discourse Processes,
vol. 25, 1998.
[13] B. Hachey, G. Murray, and D. Reitter, ?The
embra system at duc 2005: Query-oriented
multi-document summarization with a very
large latent semantic space,? in Proceedings
of the Document Understanding Conference
(DUC) 2005, Vancouver, BC, Canada, Octo-
ber 2005.
[14] E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, ,
and H. Carvey, ?The ICSI meeting recorder
dialog act (MRDA) corpus,? in Proceedings
of the 5th SIGdial Workshop on Discourse
and Dialogue, Cambridge, MA, USA, April-
May 2004, pp. 97?100.
[15] T. Hain, J. Dines, G. Garau, M. Karafiat,
D. Moore, V. Wan, R. Ordelman,
I.Mc.Cowan, J.Vepa, and S.Renals, ?An
investigation into transcription of conference
room meetings,? Proceedings of the 9th
European Conference on Speech Commu-
nication and Technology, Lisbon, Portugal,
September 2005.
[16] A. Nenkova and B. Passonneau, ?Evaluat-
ing content selection in summarization: The
pyramid method,? in Proceedings of HLT-
NAACL 2004, Boston, MA, USA, May 2004.
[17] R. Valenza, T. Robinson, M. Hickey, and
R. Tucker, ?Summarization of spoken audio
through information extraction,? in Proceed-
ings of the ESCA Workshop on Accessing In-
formation in Spoken Audio, Cambridge UK,
April 1999, pp. 111?116.
[18] J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, W. Kraaij, M. Kronen-
thal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and
P. Wellner, ?The AMI meeting corpus:
A pre-announcement,? in Proceedings of
MLMI 2005, Edinburgh, UK, June 2005.
374
NAACL HLT Demonstration Program, pages 9?10,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Automatic Segmentation and Summarization of Meeting Speech
Gabriel Murray, Pei-Yun Hsueh, Simon Tucker
Jonathan Kilgour, Jean Carletta, Johanna Moore, Steve Renals
University of Edinburgh
Edinburgh, Scotland
fgabriel.murray,p.hsuehg@ed.ac.uk
1 Introduction
AMI Meeting Facilitator is a system that per-
forms topic segmentation and extractive sum-
marisation. It consists of three components: (1)
a segmenter that divides a meeting into a num-
ber of locally coherent segments, (2) a summa-
rizer that selects the most important utterances
from the meeting transcripts. and (3) a com-
pression component that removes the less im-
portant words from each utterance based on the
degree of compression the user specied. The
goal of the AMI Meeting Facilitator is two-fold:
rst, we want to provide sucient visual aids for
users to interpret what is going on in a recorded
meeting; second, we want to support the devel-
opment of downstream information retrieval and
information extraction modules with the infor-
mation about the topics and summaries in meet-
ing segments.
2 Component Description
2.1 Segmentation
The AMI Meeting Segmenter is trained using a
set of 50 meetings that are seperate from the in-
put meeting. We rst extract features from the
audio and video recording of the input meeting
in order to train the Maximum Entropy (Max-
Ent) models for classifying topic boundaries and
non-topic boundaries. Then we test each utter-
ance in the input meeting on the Segmenter to
see if it is a topic boundary or not. The features
we use include the following ve categories: (1)
Conversational Feature: These include a set
of seven conversational features, including the
amount of overlapping speech, the amount of
silence between speaker segments, the level of
similarity of speaker activity, the number of cue
words, and the predictions of LCSEG (i.e., the
lexical cohesion statistics, the estimated poste-
rior probability, the predicted class). (2) Lex-
ical Feature: Each spurt is represented as a
vector space of uni-grams, wherein a vector is 1
or 0 depending on whether the cue word appears
in the spurt. (3) Prosodic Feature: These
include dialogue-act (DA) rate-of-speech, max-
imum F0 of the DA, mean energy of the DA,
amount of silence in the DA, precedent and sub-
sequent pauses, and duration of the DA. (4)
Motion Feature: These include the average
magnitude of speaker movements, which is mea-
sured by the number of pixels changed, over the
frames of 40 ms within the spurt. (5) Contex-
tual Feature: These include the dialogue act
types and the speaker role (e.g., project man-
ager, marketing expert). In the dialogue act an-
notations, each dialogue act is classied as one
of the 15 types.
2.2 Summarization
The AMI summarizer is trained using a set of
98 scenario meetings. We train a support vec-
tor machine (SVM) on these meetings, using 26
features relating to the following categories: (1)
Prosodic Features: These include dialogue-
act (DA) rate-of-speech, maximum F0 of the
DA, mean energy of the DA, amount of silence
in the DA, precedent and subsequent pauses,
9
and duration of the DA. (2) Speaker Fea-
tures: These features relate to how dominant
the speaker is in the meeting as a whole, and
they include percentage of the total dialogue
acts which each speaker utters, percentage of
total words which speaker utters, and amount
of time in meeting that each person is speak-
ing. (3) Structural Features: These features
include the DA position in the meeting, and the
DA position in the speaker's turn. (4) Term
Weighting Features: We use two types of
term weighting: tf.idf, which is based on words
that are frequent in the meeting but rare across
a set of other meetings or documents, and a sec-
ond weighting feature which relates to how word
usage varies between the four meeting partici-
pants.
After training the SVM, we test on each meet-
ing of the 20 meeting test set in turn, ranking
the dialogue acts from most probable to least
probable in terms of being extract-worthy. Such
a ranking allows the user to create a summary
of whatever length she desires.
2.3 Compression
Each dialogue act has its constituent words
scored using tf.idf, and as the user compresses
the meeting to a greater degree the browser
gradually removes the less important words from
each dialogue act, leaving only the most infor-
mative material of the meeting.
3 Related Work
Previous work has explored the eect of lexi-
cal cohesion and conversational features on char-
acterizing topic boundaries, following Galley et
al.(2003). In previous work, we have also studied
the problem of predicting topic boundaries at
dierent levels of granularity and showed that a
supervised classication approach performs bet-
ter on predicting a coarser level of topic segmen-
tation (Hsueh et al, 2006).
The amount of work being done on speech
summarization has accelerated in recent years.
Maskey and Hirschberg(September 2005) have
explored speech summarization in the domain
of Broadcast News data, nding that combin-
ing prosodic, lexical and structural features yield
the best results. On the ICSI meeting corpus,
Murray et al(September 2005) compared apply-
ing text summarization approaches to feature-
based approaches including prosodic features,
while Galley(2006) used skip-chain Conditional
Random Fields to model pragmatic dependen-
cies between meeting utterances, and ranked
meeting dialogue acts using a combination or
prosodic, lexical, discourse and structural fea-
tures.
4 acknowledgement
This work was supported by the European
Union 6th FWP IST Integrated Project AMI
(Augmented Multi- party Interaction, FP6-
506811)
References
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
M. Galley. 2006. A skip-chain conditional ran-
dom eld for ranking meeting utterances by im-
portance. In Proceedings of EMNLP-06, Sydney,
Australia.
P. Hsueh, J. Moore, and S. Renals. 2006. Automatic
segmentation of multiparty dialogue. In the Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
S. Maskey and J. Hirschberg. September 2005. Com-
paring lexial, acoustic/prosodic, discourse and
structural features for speech summarization. In
Proceedings of the 9th European Conference on
Speech Communication and Technology, Lisbon,
Portugal.
G. Murray, S. Renals, and J. Carletta. Septem-
ber 2005. Extractive summarization of meeting
recordings. In Proceedings of the 9th European
Conference on Speech Communication and Tech-
nology, Lisbon, Portugal.
10
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 33?40, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating Automatic Summaries of Meeting Recordings
Gabriel Murray
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Steve Renals
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Jean Carletta
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Johanna Moore
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Abstract
The research below explores schemes for
evaluating automatic summaries of busi-
ness meetings, using the ICSI Meeting
Corpus (Janin et al, 2003). Both au-
tomatic and subjective evaluations were
carried out, with a central interest be-
ing whether or not the two types of eval-
uations correlate with each other. The
evaluation metrics were used to compare
and contrast differing approaches to au-
tomatic summarization, the deterioration
of summary quality on ASR output ver-
sus manual transcripts, and to determine
whether manual extracts are rated signifi-
cantly higher than automatic extracts.
1 Introduction
In the field of automatic summarization, it is widely
agreed upon that more attention needs to be paid
to the development of standardized approaches to
summarization evaluation. For example, the cur-
rent incarnation of the Document Understanding
Conference is putting its main focus on the de-
velopment of evaluation schemes, including semi-
automatic approaches to evaluation. One semi-
automatic approach to evaluation is ROUGE (Lin
and Hovy, 2003), which is primarily based on n-
gram co-occurrence between automatic and human
summaries. A key question of the research con-
tained herein is how well ROUGE correlates with
human judgments of summaries within the domain
of meeting speech. If it is determined that the two
types of evaluations correlate strongly, then ROUGE
will likely be a valuable and robust evaluation tool in
the development stage of a summarization system,
when the cost of frequent human evaluations would
be prohibitive.
Three basic approaches to summarization are
evaluated and compared below: Maximal Marginal
Relevance, Latent Semantic Analysis, and feature-
based classification. The other major comparisons
in this paper are between summaries on ASR ver-
sus manual transcripts, and between manual and au-
tomatic extracts. For example, regarding the for-
mer, it might be expected that summaries on ASR
transcripts would be rated lower than summaries on
manual transcripts, due to speech recognition errors.
Regarding the comparison of manual and automatic
extracts, the manual extracts can be thought of as
a gold standard for the extraction task, represent-
ing the performance ceiling that the automatic ap-
proaches are aiming for.
More detailed descriptions of the summarization
approaches and experimental setup can be found in
(Murray et al, 2005). That work relied solely on
ROUGE as an evaluation metric, and this paper pro-
ceeds to investigate whether ROUGE alone is a reli-
able metric for our summarization domain, by com-
paring the automatic scores with recently-gathered
human evaluations. Also, it should be noted that
while we are at the moment only utilizing intrinsic
evaluation methods, our ultimate plan is to evalu-
ate these meeting summaries extrinsically within the
context of a meeting browser (Wellner et al, 2005).
33
2 Description of the Summarization
Approaches
2.1 Maximal Marginal Relevance (MMR)
MMR (Carbonell and Goldstein, 1998) uses the
vector-space model of text retrieval and is particu-
larly applicable to query-based and multi-document
summarization. The MMR algorithm chooses
sentences via a weighted combination of query-
relevance and redundancy scores, both derived using
cosine similarity. The MMR score ScMMR(i)for a
given sentence Si in the document is given by
ScMMR(i) =
?(Sim(Si, D))? (1? ?)(Sim(Si, Summ)) ,
where D is the average document vector, Summ
is the average vector from the set of sentences al-
ready selected, and ? trades off between relevance
and redundancy. Sim is the cosine similarity be-
tween two documents.
This implementation of MMR uses lambda an-
nealing so that relevance is emphasized while the
summary is still short and minimizing redundancy is
prioritized more highly as the summary lengthens.
2.2 Latent Semantic Analysis (LSA)
LSA is a vector-space approach which involves pro-
jecting the original term-document matrix to a re-
duced dimension representation. It is based on the
singular value decomposition (SVD) of an m ? n
term-document matrix A, whose elements Aij rep-
resent the weighted term frequency of term i in doc-
ument j. In SVD, the term-document matrix is de-
composed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n ? n matrix of right-singular vectors.
The rows of V T may be regarded as defining top-
ics, with the columns representing sentences from
the document. Following Gong and Liu (Gong and
Liu, 2001), summarization proceeds by choosing,
for each row in V T , the sentence with the highest
value. This process continues until the desired sum-
mary length is reached.
Two drawbacks of this method are that dimen-
sionality is tied to summary length and that good
sentence candidates may not be chosen if they do
not ?win? in any dimension (Steinberger and Jez?ek,
2004). The authors in (Steinberger and Jez?ek, 2004)
found one solution, by extracting a single LSA-
based sentence score, with variable dimensionality
reduction.
We address the same concerns, following the
Gong and Liu approach, but rather than extracting
the best sentence for each topic, the n best sentences
are extracted, with n determined by the correspond-
ing singular values from matrix S. The number of
sentences in the summary that will come from the
first topic is determined by the percentage that the
largest singular value represents out of the sum of all
singular values, and so on for each topic. Thus, di-
mensionality reduction is no longer tied to summary
length and more than one sentence per topic can be
chosen. Using this method, the level of dimension-
ality reduction is essentially learned from the data.
2.3 Feature-Based Approaches
Feature-based classification approaches have been
widely used in text and speech summarization, with
positive results (Kupiec et al, 1995). In this work
we combined textual and prosodic features, using
Gaussian mixture models for the extracted and non-
extracted classes. The prosodic features were the
mean and standard deviation of F0, energy, and du-
ration, all estimated and normalized at the word-
level, then averaged over the utterance. The two lex-
ical features were both TFIDF-based: the average
and the maximum TFIDF score for the utterance.
For our second feature-based approach, we de-
rived single LSA-based sentence scores (Steinberger
and Jez?ek, 2004) to complement the six features de-
scribed above, to determine whether such an LSA
sentence score is beneficial in determining sentence
importance. We reduced the original term-document
matrix to 300 dimensions; however, Steinberger and
Jez?ek found the greatest success in their work by re-
ducing to a single dimension (Steinberger, personal
communication). The LSA sentence score was ob-
tained using:
ScLSAi =
?
?
?
?
n?
k=1
v(i, k)2 ? ?(k)2 ,
34
where v(i, k) is the kth element of the ith sentence
vector and ?(k) is the corresponding singular value.
3 Experimental Setup
We used human summaries of the ICSI Meeting cor-
pus for evaluation and for training the feature-based
approaches. An evaluation set of six meetings was
defined and multiple human summaries were created
for these meetings, with each test meeting having ei-
ther three or four manual summaries. The remaining
meetings were regarded as training data and a single
human summary was created for these. Our sum-
maries were created as follows.
Annotators were given access to a graphical user
interface (GUI) for browsing an individual meeting
that included earlier human annotations: an ortho-
graphic transcription time-synchronized with the au-
dio, and a topic segmentation based on a shallow hi-
erarchical decomposition with keyword-based text
labels describing each topic segment. The annota-
tors were told to construct a textual summary of the
meeting aimed at someone who is interested in the
research being carried out, such as a researcher who
does similar work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for each
heading, and told that there must be text for the gen-
eral abstract, but that the other headings may have
null annotations for some meetings.
Immediately after authoring a textual summary,
annotators were asked to create an extractive sum-
mary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding (Shriberg et al, 2004) (The dialogue
act categories themselves were not displayed, just
the segmentation). Annotators were told to extract
dialogue acts that together would convey the infor-
mation in the textual summary, and could be used to
support the correctness of that summary. They were
given no specific instructions about the number or
percentage of acts to extract or about redundant dia-
logue act. For each dialogue act extracted, they were
then required in a second pass to choose the sen-
tences from the textual summary supported by the
dialogue act, creating a many-to-many mapping be-
tween the recording and the textual summary.
The MMR and LSA approaches are both unsuper-
vised and do not require labelled training data. For
both feature-based approaches, the GMM classifiers
were trained on a subset of the training data repre-
senting approximately 20 hours of meetings.
We performed summarization using both the hu-
man transcripts and speech recognizer output. The
speech recognizer output was created using base-
line acoustic models created using a training set
consisting of 300 hours of conversational telephone
speech from the Switchboard and Callhome cor-
pora. The resultant models (cross-word triphones
trained on conversational side based cepstral mean
normalised PLP features) were then MAP adapted
to the meeting domain using the ICSI corpus (Hain
et al, 2005). A trigram language model was em-
ployed. Fair recognition output for the whole corpus
was obtained by dividing the corpus into four parts,
and employing a leave one out procedure (training
the acoustic and language models on three parts of
the corpus and testing on the fourth, rotating to ob-
tain recognition results for the full corpus). This
resulted in an average word error rate (WER) of
29.5%. Automatic segmentation into dialogue acts
or sentence boundaries was not performed: the dia-
logue act boundaries for the manual transcripts were
mapped on to the speech recognition output.
3.1 Description of the Evaluation Schemes
A particular interest in our research is how automatic
measures of informativeness correlate with human
judgments on the same criteria. During the devel-
opment stage of a summarization system it is not
feasible to employ many hours of manual evalua-
tions, and so a critical issue is whether or not soft-
ware packages such as ROUGE are able to measure
informativeness in a way that correlates with subjec-
tive summarization evaluations.
35
3.1.1 ROUGE
Gauging informativeness has been the focus
of automatic summarization evaluation research.
We used the ROUGE evaluation approach (Lin
and Hovy, 2003), which is based on n-gram co-
occurrence between machine summaries and ?ideal?
human summaries. ROUGE is currently the stan-
dard objective evaluation measure for the Document
Understanding Conference 1; ROUGE does not as-
sume that there is a single ?gold standard? summary.
Instead it operates by matching the target summary
against a set of reference summaries. ROUGE-1
through ROUGE-4 are simple n-gram co-occurrence
measures, which check whether each n-gram in the
reference summary is contained in the machine sum-
mary. ROUGE-L and ROUGE-W are measures of
common subsequences shared between two sum-
maries, with ROUGE-W favoring contiguous com-
mon subsequences. Lin (Lin and Hovy, 2003) has
found that ROUGE-1 and ROUGE-2 correlate well
with human judgments.
3.1.2 Human Evalautions
The subjective evaluation portion of our research
utilized 5 judges who had little or no familiarity with
the content of the ICSI meetings. Each judge eval-
uated 10 summaries per meeting, for a total of sixty
summaries. In order to familiarize themselves with
a given meeting, they were provided with a human
abstract of the meeting and the full transcript of the
meeting with links to the audio. The human judges
were instructed to read the abstract, and to consult
the full transcript and audio as needed, with the en-
tire familiarization stage not to exceed 20 minutes.
The judges were presented with 12 questions at
the end of each summary, and were instructed that
upon beginning the questionnaire they should not re-
consult the summary itself. 6 of the questions re-
garded informativeness and 6 involved readability
and coherence, though our current research concen-
trates on the informativeness evaluations. The eval-
uations used a Likert scale based on agreement or
disagreement with statements, such as the following
Informativeness statements:
1. The important points of the meeting are repre-
sented in the summary.
1http://duc.nist.gov/
2. The summary avoids redundancy.
3. The summary sentences on average seem rele-
vant.
4. The relationship between the importance of
each topic and the amount of summary space
given to that topic seems appropriate.
5. The summary is repetitive.
6. The summary contains unnecessary informa-
tion.
Statements such as 2 and 5 above are measuring
the same impressions, with the polarity of the state-
ments merely reversed, in order to better gauge the
reliability of the answers. The readability/coherence
portion consisted of the following statements:
1. It is generally easy to tell whom or what is be-
ing referred to in the summary.
2. The summary has good continuity, i.e. the sen-
tences seem to join smoothly from one to an-
other.
3. The individual sentences on average are clear
and well-formed.
4. The summary seems disjointed.
5. The summary is incoherent.
6. On average, individual sentences are poorly
constructed.
It was not possible in this paper to gauge how
responses to these readability statements correlate
with automatic metrics, for the reason that auto-
matic metrics of readability and coherence have not
been widely discussed in the field of summariza-
tion. Though subjective evaluations of summaries
are often divided into informativeness and readabil-
ity questions, only automatic metrics of informative-
ness have been investigated in-depth by the summa-
rization community. We believe that the develop-
ment of automatic metrics for coherence and read-
ability should be a high priority for researchers in
summarization evaluation and plan on pursuing this
avenue of research. For example, work on coher-
ence in NLG (Lapata, 2003) could potentially in-
form summarization evaluation. Mani (Mani et al,
36
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
ROUGE-1-MANROUGE-2-MANROUGE-L-MANROUGE-1-ASRROUGE-2-ASRROUGE-L-ASR
Figure 1: ROUGE Scores for the Summarization Ap-
proaches
1999) is one of the few papers to have discussed
measuring summary readability automatically.
4 Results
The results of these experiments can be analyzed
in various ways: significant differences of ROUGE
results across summarization approaches, deterio-
ration of ROUGE results on ASR versus manual
transcripts, significant differences of human eval-
uations across summarization approaches, deterio-
ration of human evaluations on ASR versus man-
ual transcripts, and finally, the correlation between
ROUGE and human evaluations.
4.1 ROUGE results across summarization
approaches
All of the machine summaries were 10% of the orig-
inal document length, in terms of the number of di-
alogue acts contained. Of the four approaches to
summarization used herein, the latent semantic anal-
ysis method performed the best on every meeting
tested for every ROUGE measure with the excep-
tion of ROUGE-3 and ROUGE-4. This approach
was significantly better than either feature-based ap-
proach (p<0.05), but was not a significant improve-
ment over MMR. For ROUGE-3 and ROUGE-4,
none of the summarization approaches were signifi-
cantly different from each other, owing to data spar-
sity. Figure 1 gives the ROUGE-1, ROUGE-2 and
ROUGE-L results for each of the summarization ap-
proaches, on both manual and ASR transcripts.
4.1.1 ASR versus Manual
The results of the four summarization approaches
on ASR output were much the same, with LSA and
MMR being comparable to each other, and each of
them outperforming the feature-based approaches.
On ASR output, LSA again consistently performed
the best.
Interestingly, though the LSA approach scored
higher when using manual transcripts than when
using ASR transcripts, the difference was small and
insignificant despite the nearly 30% WER of the
ASR. All of the summarization approaches showed
minimal deterioration when used on ASR output
as compared to manual transcripts, but the LSA
approach seemed particularly resilient, as evidenced
by Figure 1. One reason for the relatively small
impact of ASR output on summarization results is
that for each of the 6 meetings, the WER of the
summaries was lower than the WER of the meeting
as a whole. Similarly, Valenza et al(Valenza et
al., 1999) and Zechner and Waibel (Zechner and
Waibel, 2000) both observed that the WER of
extracted summaries was significantly lower than
the overall WER in the case of broadcast news. The
table below demonstrates the discrepancy between
summary WER and meeting WER for the six
meetings used in this research.
Meeting Summary WER Meeting WER
Bed004 27.0 35.7
Bed009 28.3 39.8
Bed016 39.6 49.8
Bmr005 23.9 36.1
Bmr019 28.0 36.5
Bro018 25.9 35.6
WER% for Summaries and Meetings
There was no improvement in the second feature-
based approach (adding an LSA sentence score) as
compared with the first feature-based approach. The
sentence score used here relied on a reduction to 300
dimensions, which may not have been ideal for this
data.
The similarity between the MMR and LSA ap-
proaches here mirrors Gong and Liu?s findings, giv-
ing credence to the claim that LSA maximizes rele-
vance and minimizes redundancy, in a different and
more opaque manner then MMR, but with similar
37
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 5.03 4.53 4.67 4.83
NO REDUN. 4.33 2.60 3.00 3.77
RELEVANT 4.83 4.07 4.33 4.53
TOPIC SPACE 4.43 3.83 3.87 4.30
REPETITIVE 3.37 4.70 4.60 3.83
UNNEC. INFO. 4.70 6.00 5.83 5.00
Table 1: Human Scores for 4 Approaches on Manual
Transcripts
results. Regardless of whether or not the singular
vectors of V T can rightly be thought of as topics or
concepts (a seemingly strong claim), the LSA ap-
proach was as successful as the more popular MMR
algorithm.
4.2 Human results across summarization
approaches
Table 1 presents average ratings for the six state-
ments across four summarization approaches on
manual transcripts. Interestingly, the first feature-
based approach is given the highest marks on each
criterion. For statements 2, 5 and 6 FB1 is signif-
icantly better than the other approaches. It is par-
ticularly surprising that FB1 would score well on
statement 2, which concerns redundancy, given that
MMR and LSA explicitly aim to reduce redundancy
while the feature-based approaches are merely clas-
sifying utterances as relevant or not. The second
feature-based approach was not significantly worse
than the first on this score.
Considering the difficult task of evaluating ten ex-
tractive summaries per meeting, we are quite satis-
fied with the consistency of the human judges. For
example, statements that were merely reworded ver-
sions of other statements were given consistent rat-
ings. It was also the case that, with the exception
of evaluating the sixth statement, judges were able
to tell that the manual extracts were superior to the
automatic approaches.
4.2.1 ASR versus Manual
Table 2 presents average ratings for the six state-
ments across four summarization approaches on
ASR transcripts. The LSA and MMR approaches
performed better in terms of having less deteri-
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 3.53 4.13 3.73 3.50
NO REDUN. 3.40 2.97 2.63 3.57
RELEVANT 3.47 3.57 3.00 3.47
TOPIC SPACE 3.27 3.33 3.00 3.20
REPETITIVE 4.43 4.73 4.70 4.20
UNNEC. INFO. 5.37 6.00 6.00 5.33
Table 2: Human Scores for 4 Approaches on ASR
Transcripts
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-1-MANHUMAN-1-ASR
Figure 2: INFORMATIVENESS-1 Scores for the
Summarization Approaches
oration of scores when used on ASR output in-
stead of manual transcripts. LSA-ASR was not
significantly worse than LSA on any of the 6 rat-
ings. MMR-ASR was significantly worse than
MMR on only 3 of the 6. In contrast, FB1-
ASR was significantly worse than FB1 for 5 of
the 6 approaches, reinforcing the point that MMR
and LSA seem to favor extracting utterances with
fewer errors. Figures 2, 3 and 4 depict the
how the ASR and manual approaches affect the
INFORMATIVENESS-1, INFORMATIVENESS-4
and INFORMATIVENESS-6 ratings, respectively.
Note that for Figure 6, a higher score is a worse rat-
ing.
4.3 ROUGE and Human correlations
According to (Lin and Hovy, 2003), ROUGE-
1 correlates particularly well with human judg-
ments of informativeness. In the human eval-
uation survey discussed here, the first statement
(INFORMATIVENESS-1) would be expected to
correlate most highly with ROUGE-1, as it is ask-
38
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-4-MANHUMAN-4-ASR
Figure 3: INFORMATIVENESS-4 Scores for the
Summarization Approaches
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-6-MANHUMAN-6-ASR
Figure 4: INFORMATIVENESS-6 Scores for the
Summarization Approaches
ing whether the summary contains the important
points of the meeting. As could be guessed from the
discussion above, there is no significant correlation
between ROUGE-1 and human evaluations when
analyzing only the 4 summarization approaches
on manual transcripts. However, when looking
at the 4 approaches on ASR output, ROUGE-1
and INFORMATIVENESS-1 have a moderate and
significant positive correlation (Spearman?s rho =
0.500, p < 0.05). This correlation on ASR out-
put is strong enough that when ROUGE-1 and
INFORMATIVENESS-1 scores are tested for corre-
lation across all 8 summarization approaches, there
is a significant positive correlation (Spearman?s rho
= 0.388, p < 0.05).
The other significant correlations for ROUGE-
1 across all 8 summarization approaches are with
INFORMATIVENESS-2, INFORMATIVENESS-5
and INFORMATIVENESS-6. However, these are
negative correlations. For example, with regard to
INFORMATIVENESS-2, summaries that are rated
as having a high level of redundancy are given high
ROUGE-1 scores, and summaries with little redun-
dancy are given low ROUGE-1 scores. Similary,
with regard to INFORMATIVENESS-6, summaries
that are said to have a great deal of unnecessary in-
formation are given high ROUGE-1 scores. It is
difficult to interpret some of these negative correla-
tions, as ROUGE does not measure redundancy and
would not necessarily be expected to correlate with
redundancy evaluations.
5 Discussion
In general, ROUGE did not correlate well with the
human evaluations for this data. The MMR and
LSA approaches were deemed to be significantly
better than the feature-based approaches according
to ROUGE, while these findings were reversed ac-
cording to the human evaluations. An area of agree-
ment, however, is that the LSA-ASR and MMR-
ASR approaches have a small and insignificant de-
cline in scores compared with the decline of scores
for the feature-based approaches. One of the most
interesting findings of this research is that MMR and
LSA approaches used on ASR tend to select utter-
ances with fewer ASR errors.
ROUGE has been shown to correlate well with
human evaluations in DUC, when used on news cor-
pora, but the summarization task here ? using con-
versational speech from meetings ? is quite different
from summarizing news articles. ROUGE may sim-
ply be less applicable to this domain.
6 Future Work
It remains to be determined through further ex-
perimentation by researchers using various corpora
whether or not ROUGE truly correlates well with
human judgments. The results presented above are
mixed in nature, but do not present ROUGE as being
sufficient in itself to robustly evaluate a summariza-
tion system under development.
We are also interested in developing automatic
metrics of coherence and readability. We now have
human evaluations of these criteria and are ready to
39
begin testing for correlations between these subjec-
tive judgments and potential automatic metrics.
7 Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for the speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication).
References
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. ACM SIGIR,
pages 335?336.
Y. Gong and X. Liu. 2001. Generic text summarization
using relevance measure and latent semantic analysis.
In Proc. ACM SIGIR, pages 19?25.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, I.Mc.Cowan, J.Vepa, and
S.Renals. 2005. An investigation into transcription of
conference room meetings. Submitted to Eurospeech.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. IEEE ICASSP.
J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In ACM SIGIR ?95, pages 68?
73.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In ACL, pages 545?
552.
C.-Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL.
Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999.
Improving summaries by revising them. In Proceed-
ings of the 37th conference on Association for Compu-
tational Linguistics, pages 558?565, Morristown, NJ,
USA. Association for Computational Linguistics.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. Submitted to
Eurospeech.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proc. 5th SIGdial Workshop on
Discourse and Dialogue, pages 97?100.
J. Steinberger and K. Jez?ek. 2004. Using latent semantic
analysis in text summarization and summary evalua-
tion. In Proc. ISIM ?04, pages 93?100.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proc. ESCA Workshop on Ac-
cessing Information in Spoken Audio, pages 111?116.
Pierre Wellner, Mike Flynn, Simon Tucker, and Steve
Whittaker. 2005. A meeting browser evaluation test.
In CHI ?05: CHI ?05 extended abstracts on Human
factors in computing systems, pages 2021?2024, New
York, NY, USA. ACM Press.
K. Zechner and A. Waibel. 2000. Minimizing word error
rate in textual summaries of spoken language. In Proc.
NAACL-2000.
40
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 1?7,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Prosodic Correlates of Rhetorical Relations
Gabriel Murray
Centre for Speech Technology Research
University of Edinburgh
Edinburgh EH8 9LW
gabriel.murray@ed.ac.uk
Maite Taboada
Dept. of Linguistics
Simon Fraser University
Vancouver V5A 1S6
mtaboada@sfu.ca
Steve Renals
Centre for Speech Technology Research
University of Edinburgh
Edinburgh EH8 9LW
s.renals@ed.ac.uk
Abstract
This paper investigates the usefulness of
prosodic features in classifying rhetori-
cal relations between utterances in meet-
ing recordings. Five rhetorical relations
of contrast, elaboration, summary, ques-
tion and cause are explored. Three train-
ing methods - supervised, unsupervised,
and combined - are compared, and classi-
fication is carried out using support vector
machines. The results of this pilot study
are encouraging but mixed, with pairwise
classification achieving an average of 68%
accuracy in discerning between relation
pairs using only prosodic features, but
multi-class classification performing only
slightly better than chance.
1 Introduction
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) attempts to describe a given text
in terms of its coherence, i.e. how it is that the parts
of the text are related to one another and how each
part plays a role. Two adjacent text spans will of-
ten exhibit a nucleus-satellite relationship, where the
satellite plays a role that is relative to the nucleus.
For example, one sentence might make a claim and
the following sentence give evidence for the claim,
with the second sentence being a satellite and the
evidence relation existing between the two spans.
In a text containing many sentences, these nucleus-
satellite pairs can be built up to produce a document-
wide rhetorical tree. Figure 1 gives an example of a
rhetorical tree for a three-sentence text1.
Theories such as RST have been popular for some
time as a way of describing the multi-levelled rhetor-
ical relations that exist in text, with relevant appli-
cations such as automatic summarization (Marcu,
1997) and natural language generation (Knott and
Dale, 1996). However, implementing automatic
rhetorical parsers has been a problematic area of
research. Techniques that rely heavily on explicit
signals, such as discourse markers, are of limited
use both because only a small percentage of rhetori-
cal relations are signalled explicitly and because ex-
plicit markers can be ambiguous. RST trees are bi-
nary branching trees distinguishing between nuclei
and satellites, and automatically determining nucle-
arity is also far from trivial. Furthermore, there
are some documents which are simply not amenable
to being described by a document-wide rhetorical
tree (Mann and Thompson, 1988). Finally, some-
times more than one relation can hold between two
given units (Moore and Pollack, 1992). Given the
problems of automatically parsing text for rhetori-
cal relations, it seems prohibitively difficult to at-
tempt rhetorical parsing of speech documents - data
which are marked by disfluencies, low information
density, and sometimes little cohesion. For that rea-
son, this pilot study sets out a comparatively mod-
est task: to determine whether one of five relations
holds between two adjacent dialogue acts in meet-
ing speech. All relations are of the form nucleus-
satellite, and the five relation types are contrast,
1Contrast is in fact often realized with a multi-nuclear struc-
ture
1
 	





 ffHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Recognition and Understanding of Meetings
Steve Renals
Centre for Speech Technology Research, University of Edinburgh
Informatics Forum, 10 Crichton Street, Edinburgh EH8 9AB, UK
s.renals@ed.ac.uk homepages.inf.ed.ac.uk/srenals/
Abstract
This paper is about interpreting human com-
munication in meetings using audio, video and
other signals. Automatic meeting recognition
and understanding is extremely challenging,
since communication in a meeting is sponta-
neous and conversational, and involves mul-
tiple speakers and multiple modalities. This
leads to a number of significant research prob-
lems in signal processing, in speech recog-
nition, and in discourse interpretation, tak-
ing account of both individual and group be-
haviours. Addressing these problems requires
an interdisciplinary effort. In this paper, I
discuss the capture and annotation of multi-
modal meeting recordings?resulting in the
AMI meeting corpus?and how we have built
on this to develop techniques and applications
for the recognition and interpretation of meet-
ings.
1 Introduction
On the face of it, meetings do not seem to form a
compelling research area. Although many people
spend a substantial fraction of their time in meetings
(e.g. the 1998 3M online survey at http://www.
3m.com/meetingnetwork/), for most people
they are not the most enjoyable aspect of their work.
However, for all the time that is spent in meet-
ings, technological support for the meeting process
is scant. Meeting records usually take the form of
brief minutes, personal notes, and more recent use
of collaborative web 2.0 software. Such records are
labour intensive to produce?because they are man-
ually created?and usually fail to capture much of
the content of a meeting, for example the factors
that led to a particular decision and the different sub-
jective attitudes displayed by the meeting attendees.
For all the time invested in meetings, very little of
the wealth of information that is exchanged is ex-
plicitly preserved.
To preserve the information recorded in meet-
ings, it is necessary to capture it. Obviously this
involves recording the speech of the meeting partic-
ipants. However, human communication is a mul-
timodal activity with information being exchanged
via gestures, handwritten diagrams, and numerous
social signals. The creation of a rich meeting record
involves the capture of data across several modal-
ities. It is a key engineering challenge to capture
such multimodal signals in a reliable, unobtrusive
and flexible way, but the greater challenges arise
from unlocking the multimodal recordings. If such
recordings are not transcribed and indexed (at the
least), then access merely corresponds to replay.
And it is rare that people will have the time, or the
inclination, to replay a meeting.
There is a long and interesting thread of research
which is concerned to better understand the dynam-
ics of meetings and the way that groups function
(Bales, 1951; McGrath, 1991; Stasser and Taylor,
1991). The types of analyses and studies carried
out by these authors is still someway beyond what
we can do automatically. The first significant work
on automatic processing of meetings, coupled with
an exploration of how people might interact with
an archive of recorded meetings, was performed in
the mid 1990s (Kazman et al, 1996). This work
was limited by the fact that it was not possible at
1
that time to transcribe meeting speech automatically.
Other early work in the area concentrated on the
multimodal capture and broadcast of meetings (Roy
and Luz, 1999; Cutler et al, 2002; Yong et al,
2001).
Three groups further developed approaches to
automatically index the content of meetings. A
team at Fuji Xerox PARC used video retrieval tech-
niques such as keyframing to automatically gener-
ate manga-style summaries of meetings (Uchihashi
et al, 1999), Waibel and colleagues at CMU used
speech recognition and video tracking for meet-
ings (Waibel et al, 2001), and Morgan and col-
leagues at ICSI focused on audio-only capture and
speech recognition (Morgan et al, 2003). Since
2003 research in the recognition and understand-
ing of meetings has developed substantially, stim-
ulated by evaluation campaigns such as the NIST
Rich Transcription (RT)1 and CLEAR2 evaluations,
as well as some large multidisciplinary projects such
as AMI/AMIDA3, CHIL4 and CALO5.
This paper is about the work we have carried out
in meeting capture, recognition and interpretation
within the AMI and AMIDA projects since 2004.
One of the principal outputs of these projects was
a multimodal corpus of meeting recordings, anno-
tated at a number of different levels. In section 2 we
discuss collection of meeting data, and the construc-
tion of the AMI corpus. The remainder of the pa-
per discusses the automatic recognition (section 3)
and interpretation (section 4) of multimodal meeting
recordings, application prototypes (section 5) and is-
sues relating to evaluation (section 6).
2 The AMI corpus
Ideally it would not be necessary to undertake a large
scale data collection and annotation exercise, every
time we address a new domain. However unsuper-
vised adaptation techniques are still rather imma-
ture, and prior to the collection of the AMI corpus,
there had not been a controlled collection and multi-
level annotation of multiparty interactions, recorded
across multiple modalities.
1www.itl.nist.gov/iad/mig/tests/rt/
2clear-evaluation.org/
3www.amiproject.org/
4chil.server.de/
5caloproject.sri.com/
Figure 1: AMI instrumented meeting room: four co-
located participants, one joined by video conference. In
this case two microphone arrays and seven cameras were
used.
One of our key motivations is the development
of automatic approaches to recognise and interpret
group interactions, using information spread across
multiple modalities, but collected as unobtrusively
as possible. This led to the design and construction
of the AMI Instrumented Meeting Rooms (figure 1)
at the University of Edinburgh, Idiap Research In-
stitute, and TNO Human Factors. These rooms con-
tained a set of standardised recording equipment in-
cluding six or seven cameras (four of which would
be used for close-up views in meeting of up to four
people), an 8-element microphone array, a close-
talking microphone for each participant (used to
guarantee a clean audio signal for each speaker),
as well capture of digital pens, whiteboards, shared
laptop spaces, data projector and videoconferencing
if used. A considerable amount of hardware was
necessary for ensuring frame-level synchronisation.
More recently we have used a lighter weight setup,
that uses a high resolution spherical digital video
camera system, and a single microphone array (7?
20 elements, depending on meeting size) synchro-
nised using software. We have also constructed a
prototype system using a low-cost, flexible array of
digital MEMS microphones (Zwyssig et al, 2010).
We used these instrumented meeting rooms to
record the AMI Meeting Corpus (Carletta, 2007).
This corpus contains over 100 hours of meeting
recordings, with the different recording streams syn-
chronised to a common timeline. The corpus con-
tains a number of manually created and automatic
annotations, synchronised to the same timeline. This
2
includes a high-quality manual word-level transcrip-
tion of the complete corpus, as well as reference au-
tomatic speech recognition output, using the speech
recognition system discussed in section 3 (using 5-
fold cross-validation). In addition to word-level
transcriptions, the corpus includes manual annota-
tions that describe the behaviour of meeting partici-
pants at a number of levels. These include dialogue
acts, topic segmentation, extractive and abstractive
summaries, named entities, limited forms of head
and hand gestures, gaze direction, movement around
the room, and head pose information. Some of these
annotations, in particular video annotation, are ex-
pensive to perform: about 10 hours of meetings have
been completely annotated at all these levels; over
70% of the corpus has been fully annotated with
the linguistic annotations. NXT?the NITE XML
Toolkit6?an XML-based open source software in-
frastructure for multimodal annotation was used to
carry out and manage the annotations.
About 70% of the AMI corpus consists of meet-
ings based on a design scenario, in which four par-
ticipants play roles in a design team. The scenario
involves four team meetings, between which the par-
ticipants had tasks to accomplish. The participant
roles were stimulated in real-time by email and web
content. Although the use of a scenario reduces the
overall realism of the meetings, we adopted this ap-
proach for several reasons, most importantly: (1)
there were some preferred design outcomes, mak-
ing it possible to define some objective group out-
come measures; (2) the knowledge and motivation
of the participants was controlled, thus removing the
serious confounding factors that would arise from
the long history and context found in real organ-
isations; and (3) allowing the meeting scenario to
be replicated, thus enabling system-level evaluations
(as discussed in section 6). We recorded and anno-
tated thirty replicates of the scenario: this provides
an unparalleled resource for system evaluation, but
also reduces the variability of the corpus (for ex-
ample in terms of the language used). The remain-
ing 30% of the corpus contains meetings that would
have occurred anyway; these are meetings with a
lot less control than the scenario meetings, but with
greater linguistic variability.
6sourceforge.net/projects/nite/
All the meetings in the AMI corpus are spoken
in English, but over half the participants are non-
native speakers. This adds realism in a European
context, as well as providing an additional speech
recognition challenge. The corpus is publicly avail-
able7, and is released under a licence that is based on
the Creative Commons Attribution NonCommercial
ShareAlike 2.5 Licence. This includes all the signals
and manual annotations, plus a number of automatic
annotations (e.g. speech recognition) made available
to lower the startup cost of performing research on
the corpus.
3 Multimodal recognition
The predominant motivation behind the collection
and annotation of the AMI corpus was to enable the
development of multimodal recognisers to address
issues such as speech recognition, speaker diarisar-
tion (Wooters and Huijbregts, 2007), gesture recog-
nition (Al-Hames et al, 2007) and focus of attention
(Ba and Odobez, 2008). Although speech recog-
nition is based on the (multichannel) audio signal,
the other problems can be successfully addressed by
combining modalities. (There is certainly informa-
tion in other modalities that has the potential to make
speech recognition more accurate, but so far we have
not been able to use it consistently and robustly.)
Speech recognition: The automatic transcription
of what is spoken in a meeting is an essential pre-
requisite to interpreting a meeting. Morgan et al
(2003) described the speech recognition of meetings
as an ?ASR-complete? problem. Developing an ac-
curate system for meeting recognition involves the
automatic segmentation of the recording into utter-
ances from a single talker, robustness to reverbera-
tion and competing acoustic sources, handling over-
lapping talkers, exploitation of multiple microphone
recordings, as well as the core acoustic and language
modelling problems that arise when attempting to
recognise spontaneous, conversational speech.
Our initial systems for meeting recognition used
audio recorded with close-talking microphones, in
order to develop the core acoustic modelling tech-
niques. More recently our focus has been on recog-
nising speech obtained using tabletop microphone
7corpus.amiproject.org/
3
arrays, which are less intrusive but have a lower
signal-to-noise ratio. Multiple microphone sys-
tems are based on microphone array beamforming
in which the individual microphone signals are fil-
tered and summed to enhance signals coming from
a particular direction, while suppressing signals
from competing locations (Wo?lfel and McDonough,
2009).
The core acoustic and language modelling com-
ponents for meeting speech recognition correspond
quite closely to the state-of-the-art systems used in
other domains. Acoustic modelling techniques in-
clude vocal tract length normalisation, speaker adap-
tation based on maximum likelihood linear trans-
forms, and further training using a discriminative
minimum Bayes risk criterion such as minimum
phone error rate (Gales and Young, 2007; Renals
and Hain, 2010). In addition we have employed a
number of novel acoustic parameterisations includ-
ing approaches based on local posterior probability
estimation (Grezl et al, 2007) and pitch adaptive
features (Garau and Renals, 2008), the automatic
construction of domain-specific language models
using documents obtained from the web by search-
ing with n-grams obtained from meeting transcripts
(Wan and Hain, 2006; Bulyko et al, 2007), and au-
tomatic approaches to acoustic segmentation opti-
mised for meetings (Wrigley et al, 2005; Dines et
al., 2006).
A feature of the systems developed for meeting
recognition is the use of multiple recognition passes,
cross-adaptation and model combination (Hain et
al., 2007). In particular successive passes make use
of more detailed?and more diverse?acoustic and
language models. Different acoustic models trained
on different feature representations (e.g. standard
PLP features and posterior probability-based fea-
tures) are cross-adapted, and different feature repre-
sentations are also combined using linear transforms
such as heteroscedastic linear discriminant analysis
(Kumar and Andreou, 1998).
These systems have been evaluated in successive
NIST RT evaluations: the core microphone array
based system has a word error rate of about 40%;
after adaptation and feature combination steps, this
error rate can be reduced to about 30%. The equiv-
alent close-talking microphone system has baseline
word error rate of about 35%, reduced to less than
25% after further recognition passes (Hain et al,
2007). The core system runs about five times slower
than real-time, and the full system is about fourteen
times slower than real-time, on current commodity
hardware. We have developed a low-latency real-
time system (with an error rate of about 41% for mi-
crophone array input) (Garner et al, 2009), based on
an open source runtime system8.
4 Meeting interpretation
One of the interdisciplinary joys of working on
meetings is that researchers with different ap-
proaches are able to build collaborations through
working on common problems and common data.
The automatic interpretation of meetings is a very
good example: meetings form an exciting challenge
for work in things such as topic identification, sum-
marisation, dialogue act recognition and the recog-
nition of subjective content. Although text-based
approaches (using the output of a speech recogni-
tion system) form strong baselines, it is often the
case that systems can be improved through the in-
corporation of information characteristic of spoken
communication, such as prosody and speaker turn
patterns, as well video information such as head or
hand movements.
Segmentation: We have explored multistream
statistical models to automatically segment meeting
recordings. Meetings can be usefully segmented at
many different levels, for example into speech and
non-speech (an essential pre-processing for speech
recognition), into utterances spoken by a single
talker, into dialogue acts, into topics, and into ?meet-
ing phases?. The latter was the subject of our first in-
vestigations in using multimodal multistream mod-
els to segment meetings.
Meetings are group events, characterised by both
individual actions and group actions. To obtain
structure at the group level, we and colleagues in
the M4 and AMI projects investigated segmenting
a meeting into a sequence of group actions such as
monologue, discussion and presentation (McCowan
et al, 2005). We used a number of feature streams
for this segmentation and labelling task including
speaker turn dynamics, prosody, lexical information,
8juicer.amiproject.org/
4
and participant head and hand movements (Diel-
mann and Renals, 2007). Our initial experiments
used an HMM to model the feature streams with a
single hidden state space, and resulted in an ?action
error rate? of over 40% (action error rate is analo-
gous to word error rate, but defined over meeting
actions, presumed not to overlap). The HMM was
then substituted by a richer DBN multistream model
in which each feature stream was processed inde-
pendently at a lower level of the model. These par-
tial results were then combined at a higher level,
thus providing hierarchical integration of the multi-
modal feature streams. This multistream approach
enabled a later integration of feature streams and
increased flexibility in modelling the interdepen-
dences between the different streams, enabling some
accommodation for asynchrony and multiple time
scales. Thus use of the richer DBN multistream
model resulted in a significant lowering of the ac-
tion error rate to around 13%.
We extended this approach to look at a much finer
grained segmentation: dialogue acts. A dialogue act
can be viewed as a segment of speech labelled so as
to roughly categorise the speaker?s intention. In the
AMI corpus each dialogue act in a meeting is given
one of 15 labels, which may be categorised as infor-
mation exchange, making or eliciting suggestions or
offers, commenting on the discussion, social acts,
backchannels, or ?other?. The segmentation prob-
lem is non-trivial, since a single stretch of speech
(with no pauses) from a speaker may comprise sev-
eral dialogue acts?and conversely a single dialogue
act may contain pauses. To address the tasks of auto-
matically segmenting the speech into dialogue acts,
and assigning a label to each segment, we employed
a switching dynamic Bayesian network architecture,
which modelled a set of features related to lexical
content and prosody and incorporates a weighted in-
terpolated factored language model (Dielmann and
Renals, 2008). The switching DBN coordinated the
recognition process by integrating all the available
resources. This approach was able to leverage addi-
tional corpora of conversational data by using them
as training data for a factored language model which
was used in conjunction with additional task spe-
cific language models. We followed this joint gener-
ative model, with a discriminative approach, based
on conditional random fields, which performed a re-
classification of the segmented dialogue acts.
Our experiments on dialogue act recognition used
both automatic and manual transcriptions of the
AMI corpus. The degradation when moving from
manual transcriptions to the output of a speech
recogniser was less than 10% absolute for both di-
alogue act classification and segmentation. Our ex-
periments indicated that it is possible to perform au-
tomatic segmentation into dialogue acts with a rel-
atively low error rate. However the operations of
tagging and recognition into fifteen imbalanced DA
categories have a relatively high error rate, even after
discriminative reclassification, indicating that this
remains a challenging task.
Summarisation: The automatic generation of
summaries provides a natural way to succinctly de-
scribe the content of a meeting, and can be an effi-
cient way for users to obtain information. We have
focussed on extractive techniques to construct sum-
maries, in which the most relevant parts of a meeting
are located, and concatenated together to provide a
?cut-and-paste? summary, which may be textual or
multimodal.
Our approach to extractive summarisation is
based on automatically extracting relevant dialogue
acts (or alternatively ?spurts?, segments spoken by
a single speaker and delimited by silence) from a
meeting (Murray et al, 2006). This requires (as a
minimum) the automatic speech transcription and,
if spurts are not used, dialogue act segmentation.
Lexical information is clearly extremely important
for summarisation, but we have also found speaker
features (relating to activity, dominance and over-
lap), structural features (the length and position of
dialogue acts), prosody, and discourse cues (phrases
which signal likely relevance) to be important for
the development of accurate methods for extractive
summarisation of meetings. Furthermore we have
explored reduced dimension representations of text,
based on latent semantic analysis, which we found
added precision to the summarisation. Using an
evaluation measure referred to as weighted preci-
sion, we discovered that it is possible to reliably
extract the most relevant dialogue acts, even in the
presence of speech recognition errors.
5
5 Application prototypes
We have incorporated these meeting recognition and
interpretation components in a number of applica-
tions. Our basic approach to navigating meeting
archives centres on the notion of meeting browsers,
in which media files, transcripts and segmentations
are synchronised to a common time line. Figure 2
(a) gives an example of such a browser, which also
enables a user to pan and zoom within the captured
spherical video stream.
We have explored (and, as discussed below, eval-
uated) a number of ways of including automatically
generated summaries in meeting browsers. The
browser illustrated in figure 2 (b) enables navigation
by the summarised transcript or via the topic seg-
mentation. In this case the degree of summarisation
is controlled by a slider, which removes those speech
segments that do no contribute to the summary. We
have also explored real-time (with a few utterances
latency) approaches to summarisation, to facilitate
meeting ?catchup? scenarios, including the genera-
tion of audio only summaries, with about 60% of
the speech removed (Tucker et al, 2010). Visual-
isations of summaries include a comic book layout
(Castronovo et al, 2008), illustrated in figure 3. This
is related to ?VideoManga? (Uchihashi et al, 1999),
but driven by transcribed speech rather than visually
identified keyframes.
The availability of real-time meeting speech
recognition, with phrase-level latency (Garner et al,
2009), enables a new class of applications. Within
AMIDA we developed a software architecture re-
ferred to as ?The Hub? to support real-time ap-
plications. The Hub is essentially a real-time an-
notation server, mediating between annotation pro-
ducers, such as speech recognition, and annotation
consumers, such as a real-time catchup browser.
Of course many applications will be both produc-
ers and consumers: for instance topic segmenta-
tion consumes transcripts and speaker turn informa-
tion and produces time aligned topic segments. A
good example of an application made possible by
real-time recognition components and the Hub is the
AMIDA Content Linking Device (Popescu-Belis et
al., 2008). Content linking is essentially a continual
real-time search in which a repository is searched
using a query constructed from the current conver-
(a) Basic web-based browser
(b) Summary browser
Figure 2: Two examples of meeting browsers, both in-
clude time synchronisation with a searchable ASR tran-
script and speaker activities. (a) is a basic web-based
browser; (b) also employs extractive summarisation and
topic segmentation components.
sational context. In this case the context is obtained
from a speech recognition transcript of the past 30
seconds of the conversation, and a query is con-
structed using tf ?idf or a similar measure, combined
with predefined keywords or topic weightings. The
repository to be searched may be the web, or a por-
tion of the web, or it may be an organisational doc-
ument repository, including transcribed, structured
and indexed recordings of previous meetings. Figure
4 shows a basic interface to content linking. We have
constructed live content-linking systems, driven by
microphone array based real-time speech recogni-
tion, with the aim of presenting?without explicit
query?potentially relevant documents to meeting
participants.
6
yeah they like 
spongy 
material
like yeah a 
sponge-ball
okay like 
this
yeah
yeah
okay our secondary 
audience people 
above a forty years 
in age they like the 
dark traditional 
colours
yeah materials 
like wood that
well i figure if we 
go for l_ l_c_d_ 
we should have the 
advanced
yeah
yeah
okay that's 
my
yeah do your 
thing tim
okay
yeah which 
buttons do you 
want to in it
because you can build 
in a back-forward 
button and some 
somebody would just 
want to watch two 
channels
you we could 
choose what 
what's better 
plastic or 
rubber
yeah i mean 
plastic is
so you could go 
for plastic but i 
figured
yeah
yeah
it isn't i think 
yeah
well i don't 
know no
Materials
LCD screen
Buttons
materials
Figure 3: Comic book display of autom tically generated
meeting summary.
6 Evaluation
The multiple streams of data and multiple layers of
annotations that make up the AMI corpus enable it to
be used for evaluations of specific recognition com-
ponents. The corpus has been used to evaluate many
different things including voice activity detection,
speaker diarisation and speech recognition (in the
NIST RT evaluations), and head pose recognition
(in the CLEAR evaluation). In the spoken language
processing domain, the AMI corpus has been used
to evaluate meeting summarisation, topic segmen-
tation, dialogue act recognition and cross-language
retrieval.
In addition to intrinsic component-level evalu-
ations, it is valuable to evaluate complete sys-
tems, and components in a system context. In the
AMI/AMIDA projects, we investigated a number of
extrinsic evaluation frameworks for browsing and
accessing meeting archives. The Browser Evalua-
tion Test (BET) (Wellner et al, 2005) provides a
framework for the comparison of arbitrary meet-
ing browser setups, which may differ in terms of
which content extraction or abstraction components
are employed. In the BET test subjects have to an-
swer true/false questions about a number of ?obser-
vations of interest? relating to a recorded meeting,
using the browser under test with a specified time
limit (typically half the meeting length).
We devel ped of a variant of the BET to specifi-
Figure 4: Demonstration screenshot of the AMI auto-
matic content linking device. The subpanels show (clock-
wise from top left) the ASR transcript, relevant docu-
ments from the meeting document base, relevant web hits
and a a tag cloud.
cally evaluate different summarisation approaches.
In the Decision Audit evaluation (Murray et al,
2009) the user?s task is to ascertain the factors across
a number of meetings that lead to a particular deci-
sion being made. A set of browsers were constructed
differing in the summarisation approach employed
(manual vs. ASR transcripts; extractive vs. abstrac-
tive vs. human vs. keyword-based summarisation),
and the test subjects used them to perform the deci-
sion audit. Like the BET this evaluation is labour-
intensive, but the results can be analysed using a
battery of objective and subjective measures. Con-
clusi ns from carrying out this evaluation indicated
that the task itself was quite challenging for users
(even with human transcripts and summaries, most
users could not find many factors involved in the de-
cision), that automatic extractive summaries outper-
formed reasonably competitive baseline approaches,
and that although subjects reported ASR transcripts
to be unsatisfactory (due to the error rate) browsing
using the ASR transcript still resulted in users? be-
ing generally able to find the relevant parts of the
meeting archive.
7
7 Conclusions
In this paper I have given an overview of our inves-
tigations into automatic meeting recognition and in-
terpretation. Multiparty communication is a chal-
lenging problem at many levels, from signal pro-
cessing to discourse modelling. A major part of
our attempt to address this problem, in an interdisci-
plinary way, was the collection, annotation, and dis-
tribution of the AMI meeting corpus. The AMI cor-
pus has been at the basis of nearly all the work that
we have carried out in the area, from speech recog-
nition to summarisation. Multiparty speech recog-
nition remains a difficult task, with a typical error
rate of over 20%, however the accuracy is enough to
enable various components to build on top of it. A
major achievement has been the development of pro-
totype applications that can use phrase-level latency
real-time speech recognition.
Many of the automatic approaches to meeting
recognition and characterisation are characterised by
extensive combination at the feature stream, model
and system level. In our experience, such ap-
proaches offer consistent improvements in accuracy
for these complex, multimodal tasks.
Meetings serve a social function, and much of
this has been ignored in our work, so far. We have
focussed principally on understanding meetings in
terms of their lexical content, augmented by vari-
ous multimodal streams. However in many inter-
actions, the social signals are at least as important
as the propositional content of the words (Pentland,
2008); it is a major challenge to develop meeting in-
terpretation components that can infer and take ad-
vantage of such social cues. We have made initial
attempts to do this, by attempting to include aspects
such as social role (Huang and Renals, 2008).
The AMI corpus involved a substantial effort from
many individuals, and provides an invaluable re-
source. However, we do not wish to do this again,
even if we are dealing with a domain that is sig-
nificantly different, such as larger groups, or family
?meetings?. However, our recognisers rely strongly
on annotated in-domain data. It is a major chal-
lenge to develop algorithms that are unsupervised
and adaptive to free us from the need to collect and
annotate large amount of data each time we are in-
terested in a new domain.
Acknowledgments
This paper has arisen from a collaboration involving
several laboratories. I have benefitted, in particular,
from long-term collaborations with Herve? Bourlard,
Jean Carletta, Thomas Hain, and Mike Lincoln, and
from a number of fantastic PhD students. This work
was supported by the European IST/ICT Programme
Projects IST-2001-34485 (M4), FP6-506811 (AMI),
FP6-033812 (AMIDA), and FP7-231287 (SSPNet).
This paper only reflects the author?s views and fund-
ing agencies are not liable for any use that may be
made of the information contained herein.
References
M. Al-Hames, C. Lenz, S. Reiter, J. Schenk, F. Wallhoff,
and G. Rigoll. 2007. Robust multi-modal group action
recognition in meetings from disturbed videos with the
asynchronous hidden Markov model. In Proc IEEE
ICIP.
S. O. Ba and J. M. Odobez. 2008. Multi-party focus of
attention recognition in meetings from head pose and
multimodal contextual cues. In Proc. IEEE ICASSP.
R. F. Bales. 1951. Interaction Process Analysis. Addi-
son Wesley, Cambridge MA, USA.
I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and
O. Cetin. 2007. Web resources for language modeling
in conversational speech recognition. ACM Transac-
tions on Speech and Language Processing, 5(1):1?25.
J. Carletta. 2007. Unleashing the killer corpus: expe-
riences in creating the multi-everything AMI Meet-
ing Corpus. Language Resources and Evaluation,
41:181?190.
S. Castronovo, J. Frey, and P. Poller. 2008. A generic
layout-tool for summaries of meetings in a constraint-
based approach. In Machine Learning for Multimodal
Interaction (Proc. MLMI ?08). Springer.
R. Cutler, Y. Rui, A. Gupta, J. Cadiz, I. Tashev, L. He,
A. Colburn, Z. Zhang, Z. Liu, and S. Silverberg. 2002.
Distributed meetings: a meeting capture and broad-
casting system. In Proc. ACM Multimedia, pages 503?
512.
A. Dielmann and S. Renals. 2007. Automatic meet-
ing segmentation using dynamic Bayesian networks.
IEEE Transactions on Multimedia, 9(1):25?36.
A. Dielmann and S. Renals. 2008. Recognition of di-
alogue acts in multiparty meetings using a switching
DBN. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 16(7):1303?1314.
J. Dines, J. Vepa, and T. Hain. 2006. The segmenta-
tion of multi-channel meeting recordings for automatic
speech recognition. In Proc. Interspeech.
8
M. J. F. Gales and S. J. Young. 2007. The application of
hidden Markov models in speech recognition. Foun-
dations and Trends in Signal Processing, 1(3):195?
304.
G. Garau and S. Renals. 2008. Combining spectral rep-
resentations for large vocabulary continuous speech
recognition. IEEE Transactions on Audio, Speech and
Language Processing, 16(3):508?518.
P. Garner, J. Dines, T. Hain, A. El Hannani, M. Karafiat,
D. Korchagin, M. Lincoln, V. Wan, and L. Zhang.
2009. Real-time ASR from meetings. In Proc. In-
terspeech.
F. Grezl, M. Karafiat, S. Kontar, and J. Cernocky. 2007.
Probabilistic and bottle-neck features for lvcsr of
meetings. In Acoustics, Speech and Signal Process-
ing, 2007. ICASSP 2007. IEEE International Confer-
ence on, volume 4, pages IV?757?IV?760.
T. Hain, L. Burget, J. Dines, G. Garau, M. Karafiat,
M. Lincoln, J. Vepa, and V. Wan. 2007. The ami
system for the transcription of speech in meetings. In
Proc. IEEE ICASSP?07.
S. Huang and S. Renals. 2008. Unsupervised language
model adaptation based on topic and role information
in multiparty meetings. In Proc. Interspeech ?08.
R. Kazman, R. Al-Halimi, W. Hunt, and M. Mantei.
1996. Four paradigms for indexing video conferences.
Multimedia, IEEE, 3(1):63?73.
N. Kumar and A. G. Andreou. 1998. Heteroscedastic
discriminant analysis and reduced rank HMMs for im-
proved recognition. Speech Communication, 26:283?
297.
I. McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud,
M. Barnard, and D. Zhang. 2005. Automatic analysis
of multimodal group actions in meetings. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
27(3):305?317.
J. E. McGrath. 1991. Time, interaction, and performance
(TIP): A theory of groups. Small Group Research,
22(2):147.
N. Morgan, D. Baron, S. Bhagat, H. Carvey, R. Dhillon,
J. Edwards, D. Gelbart, A. Janin, A. Krupski, B. Pe-
skin, T. Pfau, E. Shriberg, A. Stolcke, and C. Woot-
ers. 2003. Meetings about meetings: research at ICSI
on speech in multiparty conversations. In Proc. IEEE
ICASSP.
G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. In-
corporating speaker and discourse features into speech
summarization. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
367?374.
G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals,
and J. Kilgour. 2009. Extrinsic summarization eval-
uation: A decision audit task. ACM Transactions on
Speech and Language Processing, 6(2):1?29.
A.S. Pentland. 2008. Honest signals: how they shape
our world. The MIT Press.
A. Popescu-Belis, E. Boertjes, J. Kilgour, P. Poller,
S. Castronovo, T. Wilson, A. Jaimes, and J. Car-
letta. 2008. The amida automatic content linking de-
vice: Just-in-time document retrieval in meetings. In
Machine Learning for Multimodal Interaction (Proc.
MLMI ?08).
S. Renals and T. Hain. 2010. Speech recognition. In
A. Clark, C. Fox, and S. Lappin, editors, Handbook
of Computational Linguistics and Natural Language
Processing. Wiley Blackwell.
D. M. Roy and S. Luz. 1999. Audio meeting history
tool: Interactive graphical user-support for virtual au-
dio meetings. In Proc. ESCA Workshop on Accessing
Information in Spoken Audio, pages 107?110.
G. Stasser and LA Taylor. 1991. Speaking turns in face-
to-face discussions. Journal of Personality and Social
Psychology, 60(5):675?684.
S. Tucker, O. Bergman, A. Ramamoorthy, and S. Whit-
taker. 2010. Catchup: a useful application of time-
travel in meetings. In Proc. ACM CSCW, pages 99?
102.
S. Uchihashi, J. Foote, A. Girgensohn, and J. Boreczky.
1999. Video manga: generating semantically mean-
ingful video summaries. In Proc. ACM Multimedia,
pages 383?392.
A. Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf,
T. Schultz, H. Soltau, H. Yu, and K. Zechner. 2001.
Advances in automatic meeting record creation and ac-
cess. In Proc IEEE ICASSP.
V. Wan and T. Hain. 2006. Strategies for language model
web-data collection. In Proc IEEE ICASSP.
P. Wellner, M. Flynn, S. Tucker, and S. Whittaker. 2005.
A meeting browser evaluation test. In Proc. ACMCHI,
pages 2021?2024.
M. Wo?lfel and J. McDonough. 2009. Distant Speech
Recognition. Wiley.
C. Wooters and M. Huijbregts. 2007. The ICSI RT07s
speaker diarization system. In Multimodal Technolo-
gies for Perception of Humans. International Evalu-
ation Workshops CLEAR 2007 and RT 2007, volume
4625 of LNCS, pages 509?519. Springer.
S. Wrigley, G. Brown, V. Wan, and S. Renals. 2005.
Speech and crosstalk detection in multichannel audio.
IEEE Transactions on Speech and Audio Processing,
13(1):84?91.
R. Yong, A. Gupta, and J. Cadiz. 2001. Viewing meet-
ings captured by an omni-directional camera. ACM
Transactions on Computing Human Interaction.
E. Zwyssig, M. Lincoln, and S. Renals. 2010. A digital
microphone array for distant speech recognition. In
Proc. IEEE ICASSP?10.
9
